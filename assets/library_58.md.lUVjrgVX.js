import{_ as p,o as a,c as n,a as t,m as c,t as u,C as g,M as _,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},S={class:"review-content"};function I(i,e,l,h,s,r){return a(),n("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),c(u(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",S,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),c(u(l.poem.solution),1)])])])}const A=p(k,[["render",I],["__scopeId","data-v-e7448acb"]]),z=JSON.parse('[{"question":"# Merge and Sort Array Merge Sort is a classic algorithm used for sorting an array with the efficiency of O(n log(n)) time complexity. This algorithm works by recursively dividing the array into two halves, sorting those halves recursively, and finally merging the sorted halves together. Given this context, you will be implementing the merge function that merges two sorted arrays into one sorted array. **Objective:** Write a function `merge_two_sorted_arrays` that merges two sorted lists (`left` and `right`) into a single sorted list. You need to ensure the resulting list is sorted in ascending order. **Function Signature:** ```python def merge_two_sorted_arrays(left: List[int], right: List[int]) -> List[int]: ``` **Input:** * `left`: A list of integers sorted in non-decreasing order. * `right`: Another list of integers sorted in non-decreasing order. **Output:** * A list of integers sorted in non-decreasing order that contains all the elements from both `left` and `right`. **Constraints:** * Both `left` and `right` can be empty. * The elements of both `left` and `right` are within the range of [-10^9, 10^9]. * The lengths of `left` and `right` are between 0 and 10^6. **Performance Requirements:** * The function must run within an efficient time complexity, ideally O(n), where n is the total number of elements from both lists combined. **Example:** ```python >>> merge_two_sorted_arrays([1, 3, 5], [2, 4, 6]) [1, 2, 3, 4, 5, 6] >>> merge_two_sorted_arrays([-5, 3, 8], [-6, -4, 0, 10]) [-6, -5, -4, 0, 3, 8, 10] >>> merge_two_sorted_arrays([], [2, 3, 4]) [2, 3, 4] >>> merge_two_sorted_arrays([7, 8, 9], []) [7, 8, 9] ```","solution":"from typing import List def merge_two_sorted_arrays(left: List[int], right: List[int]) -> List[int]: Merges two sorted arrays into one sorted array. :param left: List[int] - a sorted list of integers. :param right: List[int] - another sorted list of integers. :return: List[int] - a merged sorted list containing all elements from left and right. # Initialize pointers for both lists i, j = 0, 0 merged = [] # Traverse both lists and append the smaller element to the merged list while i < len(left) and j < len(right): if left[i] < right[j]: merged.append(left[i]) i += 1 else: merged.append(right[j]) j += 1 # Append any remaining elements from the left list while i < len(left): merged.append(left[i]) i += 1 # Append any remaining elements from the right list while j < len(right): merged.append(right[j]) j += 1 return merged"},{"question":"# Context As part of your final assessment in understanding sorting algorithms, you are asked to implement an efficient sorting method. Shell Sort, a generalization of Insertion Sort, is found to improve the performance significantly on larger datasets. # Task Implement a function that performs Shell Sort on a given list of integers. # Function Signature ```python def shell_sort(arr: List[int]) -> List[int]: pass ``` # Parameters - `arr` (List[int]): A list of integers to be sorted. # Returns - `List[int]`: A new list of integers sorted in non-decreasing order. # Constraints - 1 ≤ len(arr) ≤ 10^5 - -10^9 ≤ arr[i] ≤ 10^9 # Performance Requirements - Aim for a time complexity better than O(n^2) in typical cases. # Example ```python assert shell_sort([12, 34, 54, 2, 3]) == [2, 3, 12, 34, 54] assert shell_sort([5, 5, 5, 5, 5]) == [5, 5, 5, 5, 5] assert shell_sort([4, 3, 2, 1]) == [1, 2, 3, 4] assert shell_sort([]) == [] assert shell_sort([1]) == [1] ``` # Edge Cases - Already sorted array. - Array with all elements the same. - Very large array up to the constraint limits. Use these details to test robustness and correctness of the implementation.","solution":"def shell_sort(arr): Perform Shell Sort on the input list and return the sorted list. n = len(arr) gap = n // 2 while gap > 0: for i in range(gap, n): temp = arr[i] j = i while j >= gap and arr[j - gap] > temp: arr[j] = arr[j - gap] j -= gap arr[j] = temp gap //= 2 return arr"},{"question":"Nearest Neighbor Optimization You are working on a classification problem where you need to identify the class of a query vector based on the nearest neighbor in the given training dataset. The current implementation uses a simple Euclidean distance to find the nearest neighbor which can become inefficient with larger datasets. Your task is to enhance the existing implementation by reducing its bottlenecks and handling possible edge cases. Task Given the following functions: - `distance(x, y)`: Calculates the Euclidean distance between two vectors `x` and `y`. - `nearest_neighbor(x, tSet)`: Finds the nearest neighbor of vector `x` from the dataset `tSet`, which is a dictionary where keys are vectors and values are their corresponding class labels. Optimize the nearest neighbor implementation to handle larger datasets more efficiently. Additional Constraints: 1. The input vectors and training set shall abide by their respective types (tuples and dictionary with tuple keys). 2. Take special care of edge cases, including empty training sets and multiple equidistant points. Input - A tuple `x` representing the query vector. - A dictionary `tSet` where keys are tuples in the format (float, float, ..., float) and values are their respective classes. Output - Return the class of the nearest neighbor from the training set. Performance Requirement - Handle inputs efficiently for larger datasets, potentially using space-time trade-off implementations or different data structures. # Example: ```python x = (1.0, 2.0) tSet = { (2.0, 3.0): \'ClassA\', (4.0, 5.0): \'ClassB\', (1.5, 1.8): \'ClassA\' } nearest_neighbor(x, tSet) # Expected output: \'ClassA\' ``` Ensure your implementation handles edge cases and optimizes the nearest neighbor search effectively.","solution":"import numpy as np from sklearn.neighbors import KDTree def distance(x, y): Calculates the Euclidean distance between two vectors x and y. return np.linalg.norm(np.array(x) - np.array(y)) def nearest_neighbor(x, tSet): Finds the nearest neighbor of vector x from the dataset tSet. tSet is a dictionary where keys are vectors and values are their corresponding class labels. if not tSet: raise ValueError(\\"Training set is empty\\") # Extract point vectors and their corresponding classes from the dictionary points = np.array(list(tSet.keys())) classes = list(tSet.values()) # Build KDTree for efficient nearest neighbor search tree = KDTree(points) # Find the nearest neighbor dist, ind = tree.query([x], k=1) # Return the class of the nearest neighbor return classes[ind[0][0]]"},{"question":"You are required to implement a function `enhanced_hailstone(n: int) -> list[int]` that generates the hailstone sequence for a given integer `n`. The function should also handle and return appropriate results for edge cases such as invalid input types or values. # Function Signature ```python def enhanced_hailstone(n: int) -> list[int]: pass ``` # Input * An integer `n` where `n > 0`. # Output * A list of integers representing the hailstone sequence starting from `n` and ending at `1`. # Constraints * The input integer `n` will always be positive and greater than zero. * The sequence must be computed in a reasonable time for numbers up to `10^6`. # Example ```python assert enhanced_hailstone(1) == [1] assert enhanced_hailstone(5) == [5, 16, 8, 4, 2, 1] assert enhanced_hailstone(7) == [7, 22, 11, 34, 17, 52, 26, 13, 40, 20, 10, 5, 16, 8, 4, 2, 1] ``` # Notes * Your solution should correctly handle and return edge cases such as `n = 1`. * While explicit handling of invalid input is not required due to constraints, consider how you might structure checks or handle unexpected input types in practice.","solution":"def enhanced_hailstone(n: int) -> list[int]: Generate the hailstone sequence for a given integer n. Parameters: n (int): A positive integer Returns: list[int]: The hailstone sequence starting from n and ending at 1 if not isinstance(n, int) or n <= 0: raise ValueError(\\"Input must be a positive integer\\") sequence = [] while n != 1: sequence.append(n) if n % 2 == 0: n = n // 2 else: n = 3 * n + 1 sequence.append(1) # Ending the sequence by adding 1 return sequence"},{"question":"# Union-Find Algorithm Application: Dynamic Connectivity **Context**: You are tasked with implementing a dynamic connectivity structure to manage a network of computers. Each computer is initially isolated, and you will receive a sequence of operations either connecting two computers or querying if two computers are in the same connected component. **Objective**: Implement the Union-Find (Disjoint Set) data structure to handle union and find operations efficiently. --- # Function Specifications **Function 1**: `UnionFind.addComputer(computer_id: int) -> None` - Adds a new isolated computer with the given `computer_id`. **Function 2**: `UnionFind.connect(computer1_id: int, computer2_id: int) -> None` - Connects two computers identified by `computer1_id` and `computer2_id`. If they are already connected, do nothing. **Function 3**: `UnionFind.areConnected(computer1_id: int, computer2_id: int) -> bool` - Returns `True` if `computer1_id` and `computer2_id` are in the same connected component, otherwise `False`. **Input Constraints**: - `computer_id` will be a positive integer within the range `[1, 10^6]`. **Performance Requirements**: - The operations must be efficient enough to handle multiple dynamic queries and updates with respect to the constraints. --- **Example usage**: ```python uf = UnionFind() uf.addComputer(1) uf.addComputer(2) uf.addComputer(3) uf.connect(1, 2) print(uf.areConnected(1, 3)) # Output: False uf.connect(2, 3) print(uf.areConnected(1, 3)) # Output: True ``` --- **Note**: Your implementation should handle edge cases correctly and ensure efficient performance with the given constraints.","solution":"class UnionFind: def __init__(self): self.parent = {} self.rank = {} def addComputer(self, computer_id: int) -> None: if computer_id not in self.parent: self.parent[computer_id] = computer_id self.rank[computer_id] = 0 def find(self, computer_id: int) -> int: if self.parent[computer_id] != computer_id: self.parent[computer_id] = self.find(self.parent[computer_id]) return self.parent[computer_id] def connect(self, computer1_id: int, computer2_id: int) -> None: root1 = self.find(computer1_id) root2 = self.find(computer2_id) if root1 != root2: if self.rank[root1] > self.rank[root2]: self.parent[root2] = root1 elif self.rank[root1] < self.rank[root2]: self.parent[root1] = root2 else: self.parent[root2] = root1 self.rank[root1] += 1 def areConnected(self, computer1_id: int, computer2_id: int) -> bool: return self.find(computer1_id) == self.find(computer2_id)"},{"question":"# Linked List Cycle Detection Challenge Imagine you are tasked with verifying data integrity in a singly linked list. One of the common issues is the presence of cycles within the list, which can cause various problems, including infinite loops during traversal operations. To address this, you need to implement an algorithm that checks whether a given singly linked list contains a cycle without utilizing any extra space. **Your task is to implement the function `is_cyclic(head)`, which takes the head of a singly linked list and returns a boolean indicating whether the linked list contains a cycle.** Here are the function and class definitions you need to use: ```python class Node: def __init__(self, x): self.val = x self.next = None def is_cyclic(head): :type head: Node :rtype: bool # Your implementation here ``` **Input:** * `head`: The head node of a singly linked list. **Output:** * Returns `True` if there is a cycle in the linked list, otherwise `False`. **Constraints:** * Your solution must run in O(n) time where n is the number of nodes in the linked list. * You are not allowed to use extra space for data structures like arrays or hash tables, achieving O(1) space complexity. **Scenarios:** 1. An empty linked list should return `False`. 2. A linked list with one node pointing to itself (cycle) should return `True`. 3. A longer linked list without any cycles should return `False`. 4. A longer linked list with a cycle somewhere in the middle should return `True`. Use the provided class `Node` to simulate the nodes of the list and link them appropriately to test your solution. Ensure your solution is efficient and handles edge cases effectively.","solution":"class Node: def __init__(self, x): self.val = x self.next = None def is_cyclic(head): Detects if a linked list contains a cycle. :param head: Node :return: bool slow = head fast = head while fast and fast.next: slow = slow.next fast = fast.next.next if slow == fast: return True return False"},{"question":"# Task You are tasked to implement the Quick Sort algorithm, ensuring it can handle various edge cases and demonstrating your understanding of sorting algorithms and recursion. # Function Signature ```python def quick_sort(arr: list) -> list: pass ``` # Input and Output * **Input**: A list of integers, `arr`. * **Output**: A list of integers sorted in ascending order. # Constraints * `1 <= len(arr) <= 10^5` * `-10^9 <= arr[i] <= 10^9` # Requirements and Performance * Your implementation should sort the input list in-place. * Ensure an average-case time complexity of (O(n log n)). * Handle edge cases efficiently to avoid degraded performance. # Example ```python # Input arr = [3, 6, 8, 10, 1, 2, 1] # Output [1, 1, 2, 3, 6, 8, 10] # Input arr = [1] # Output [1] # Input arr = [] # Output [] ``` # Additional Challenge * Implement an optional `simulate` parameter: * When set to `True`, print the array after each partition step. * This is for debugging and educational purposes to understand how the array changes during sorting. ```python def quick_sort(arr: list, simulate: bool = False) -> list: pass ``` # Hints 1. Carefully choose your pivot to enhance performance and avoid worst-case scenarios. 2. Design the partitioning process to minimize the number of swaps.","solution":"def quick_sort(arr: list, simulate: bool = False) -> list: Sort the array using the Quick Sort algorithm. :param arr: List of integers to be sorted. :param simulate: If True, print array after each partition step. :return: Sorted list of integers. def partition(low, high): pivot = arr[high] i = low - 1 for j in range(low, high): if arr[j] < pivot: i += 1 arr[i], arr[j] = arr[j], arr[i] arr[i + 1], arr[high] = arr[high], arr[i + 1] return i + 1 def quick_sort_recursive(low, high): if low < high: pi = partition(low, high) if simulate: print(arr) quick_sort_recursive(low, pi - 1) quick_sort_recursive(pi + 1, high) quick_sort_recursive(0, len(arr) - 1) return arr"},{"question":"# Summarize Ranges Context You are working on a system that processes time intervals and groups them into concise summaries for better readability. Given a sorted list of integers representing time points, you need to write a function that summarizes the consecutive intervals. Task Implement the function `summarize_ranges` which accepts a sorted list of integers and returns a list of strings summarizing the consecutive ranges. Each range should be represented as \\"start-end\\" for ranges longer than one number, and as a single number for individual elements. Function Signature ```python def summarize_ranges(array: List[int]) -> List[str]: pass ``` Input * `array` (List[int]): A sorted list of unique integers. Output * List[str]: A list of strings summarizing the consecutive ranges. Constraints * The input list contains only unique integers and is already sorted. * The length of the list will be between 0 and 10^4. Examples ```python assert summarize_ranges([0, 1, 2, 4, 5, 7]) == [\\"0-2\\", \\"4-5\\", \\"7\\"] assert summarize_ranges([1, 3, 5, 7, 9]) == [\\"1\\", \\"3\\", \\"5\\", \\"7\\", \\"9\\"] assert summarize_ranges([]) == [] assert summarize_ranges([2]) == [\\"2\\"] assert summarize_ranges([2, 3]) == [\\"2-3\\"] assert summarize_ranges([1, 2, 3, 4, 5, 6, 7, 8, 9]) == [\\"1-9\\"] ``` Notes * Ensure your solution efficiently handles edge cases such as empty arrays, arrays with one element, and arrays with all elements being consecutive. * The algorithm should maintain the O(n) time complexity requirement.","solution":"from typing import List def summarize_ranges(array: List[int]) -> List[str]: if not array: return [] ranges, start = [], array[0] for i in range(1, len(array)): if array[i] != array[i - 1] + 1: if start == array[i - 1]: ranges.append(f\\"{start}\\") else: ranges.append(f\\"{start}-{array[i - 1]}\\") start = array[i] if start == array[-1]: ranges.append(f\\"{start}\\") else: ranges.append(f\\"{start}-{array[-1]}\\") return ranges"},{"question":"# Post-order Traversal of a Binary Tree **Context**: A common operation on binary trees is to process their nodes in a specific order called \\"post-order\\". In this traversal method, we visit the left subtree, the right subtree, and finally the root node. This helps in scenarios like deleting the tree, evaluating expressions, or creating postfix expression notations. **Objective**: Implement both iterative and recursive post-order traversal methods for a given binary tree. # Task **Function 1**: Implement the function `post_order_iterative(root: Node) -> List[int]` for iterative post-order traversal. **Function 2**: Implement the function `post_order_recursive(root: Node) -> List[int]` for recursive post-order traversal. **Input**: * root (Node): The root node of the binary tree. **Output**: * List[int]: A list of integers representing the post-order traversal. **Constraints**: * The number of nodes in the tree can be up to 10^5. * Node values are unique integers. * Nodes will have at most two children. # Examples ```python # Example 1 # Input: root = Node(1) root.left = Node(2) root.right = Node(3) root.left.left = Node(4) root.left.right = Node(5) root.right.left = Node(6) root.right.right = Node(7) # Output: [4, 5, 2, 6, 7, 3, 1] # Example 2 # Input: root = None # Output: [] # Example 3 # Input: root = Node(1) root.left = Node(2) root.left.left = Node(3) # Output: [3, 2, 1] ``` # Additional Notes * Consider the typical edge cases: empty tree, single node tree, unbalanced trees. * Aim for optimal performance regarding both time and space complexity. * Handle null nodes gracefully.","solution":"from typing import List, Optional class Node: def __init__(self, val: int): self.val = val self.left = None self.right = None def post_order_iterative(root: Optional[Node]) -> List[int]: if root is None: return [] stack1 = [root] stack2 = [] result = [] while stack1: node = stack1.pop() stack2.append(node) if node.left: stack1.append(node.left) if node.right: stack1.append(node.right) while stack2: node = stack2.pop() result.append(node.val) return result def post_order_recursive(root: Optional[Node]) -> List[int]: result = [] def helper(node: Optional[Node]): if node: helper(node.left) helper(node.right) result.append(node.val) helper(root) return result"},{"question":"Finding the Single Unique Number Using XOR # Description You are provided with an array of integers where every element appears exactly twice, except for one element that appears only once. Your task is to implement a function that identifies and returns this single element. The function should adhere to linear runtime complexity and should not use any extra memory. # Function Signature ```python def find_single_number(nums: List[int]) -> int: pass ``` # Input - `nums`: A list of integers (`List[int]`), which may include negative numbers. The length of `nums` will be at least 1 and can go up to 10^6. # Output - Returns the integer that appears only once in the array. # Example ```python assert find_single_number([2, 2, 1]) == 1 assert find_single_number([4, 1, 2, 1, 2]) == 4 assert find_single_number([1]) == 1 ``` # Constraints - Your algorithm should run in O(n) time complexity. - You are not allowed to use any additional data structures (i.e., O(1) space complexity). # Edge Cases - Handle cases where the input list contains negative integers. - Handle cases with minimum input size. # Scenario In a distributed system, every request to the server is logged twice for redundancy, except for one unique request which is only logged once due to an error. Identify the unique request ID that was logged once.","solution":"from typing import List def find_single_number(nums: List[int]) -> int: Identifies and returns the single element that appears only once in the nums list, where every other element appears exactly twice. result = 0 for num in nums: result ^= num return result"},{"question":"# Scenario You have been tasked with implementing a secure version of the Atbash cipher to protect sensitive information during data transmission. The Atbash cipher you implement should handle large input efficiently and should work correctly for both English uppercase and lowercase letters, without altering special characters and spaces. # Task Write a function `enhanced_atbash(text: str) -> str` that encrypts the input string using the Atbash cipher. # Input * A string `text` (1 <= len(text) <= 10^5), containing a mix of English uppercase and lowercase letters, special characters, and spaces. # Output * Returns a string that is the Atbash-encoded version of the input `text`. # Constraints * Only English alphabetic characters should be encrypted. Non-alphabetic characters should remain unchanged. * The operation should run efficiently for large inputs up to 100,000 characters. # Example Input ``` enhanced_atbash(\\"Attack at dawn!\\") ``` Output ``` \\"Zggzxp zg wzdm!\\" ``` Input ``` enhanced_atbash(\\"Hello, World!\\") ``` Output ``` \\"Svool, Dliow!\\" ``` # Notes Consider the efficiency of your implementation given the constraints on input size.","solution":"def enhanced_atbash(text: str) -> str: Encrypts the input string using the Atbash cipher. Parameters: text (str): The input text to encrypt. Returns: str: The Atbash encoded version of the input text. def transform_char(c): if \'A\' <= c <= \'Z\': return chr(ord(\'Z\') - (ord(c) - ord(\'A\'))) elif \'a\' <= c <= \'z\': return chr(ord(\'z\') - (ord(c) - ord(\'a\'))) else: return c return \'\'.join(transform_char(c) for c in text)"},{"question":"# Scenario: You are developing a feature for a graphical application that processes binary trees. One of the requirements is to invert binary trees to create mirrored representations visually. # Problem: Write a function `invert_binary_tree` that takes the root of a binary tree and inverts it. The function should return the root of the inverted tree. # Input: - A binary tree node class `TreeNode` is defined as: ```python class TreeNode: def __init__(self, val=0, left=None, right=None): self.val = val self.left = left self.right = right ``` - The function signature is: ```python def invert_binary_tree(root: TreeNode) -> TreeNode: ``` # Output: - The function should return the root of the inverted binary tree. # Constraints: - The number of nodes in the tree is in the range `[0, 1000]`. - The values of the nodes are within the range `[-1000, 1000]`. # Example: Given: ```plaintext 4 / 2 7 / / 1 3 6 9 ``` Inverted: ```plaintext 4 / 7 2 / / 9 6 3 1 ``` # Function Signature: ```python def invert_binary_tree(root: TreeNode) -> TreeNode: ``` Notes: - Implement the function using recursion. - Consider edge cases such as empty trees and single-node trees.","solution":"class TreeNode: def __init__(self, val=0, left=None, right=None): self.val = val self.left = left self.right = right def invert_binary_tree(root: TreeNode) -> TreeNode: Inverts a binary tree. This function recursively swaps the left and right children of each node to create a mirrored representation of the original binary tree. :param root: TreeNode representing the root of the binary tree :return: TreeNode representing the root of the inverted binary tree if root is None: return None # Swap the children root.left, root.right = root.right, root.left # Recursively invert the left and right subtrees invert_binary_tree(root.left) invert_binary_tree(root.right) return root"},{"question":"# Scenario Imagine you are part of a team developing a new database system where tree structures play a crucial role in how data is stored and accessed efficiently. A critical requirement is to ensure that every binary tree in the system remains height-balanced. Your task is to implement a function that checks whether a given binary tree is height-balanced. # Objective Implement a function `is_balanced(root)` that determines if a binary tree is height-balanced. # Function Signature ```python def is_balanced(root) -> bool: # Implementation here ``` # Input * `root` (TreeNode): The root node of a binary tree. # Output * `bool`: Returns `True` if the tree is height-balanced, `False` otherwise. # Constraints 1. Each node has at most two children. 2. Node values are irrelevant for balance check, so focus purely on node structure and heights. # Example ```python class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = None # Example of a balanced tree root = TreeNode(1) root.left = TreeNode(2) root.right = TreeNode(3) root.left.left = TreeNode(4) root.left.right = TreeNode(5) assert is_balanced(root) == True # Example of an unbalanced tree root = TreeNode(1) root.left = TreeNode(2) root.left.left = TreeNode(3) root.left.left.left = TreeNode(4) assert is_balanced(root) == False ``` # Explanation In the first example, the tree has a balanced structure with equal heights or a height difference of no more than 1 between every pair of subtrees. In the second example, the tree leans heavily to the left side, making it unbalanced due to the height difference greater than 1 between the left and right subtrees of one or more nodes. Consider all edge cases such as single nodes, empty trees, and highly skewed trees in your solution.","solution":"class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = None def is_balanced(root) -> bool: def check_height(node): if not node: return 0 left_height = check_height(node.left) if left_height == -1: return -1 right_height = check_height(node.right) if right_height == -1: return -1 if abs(left_height - right_height) > 1: return -1 return max(left_height, right_height) + 1 return check_height(root) != -1"},{"question":"**Question**: You are given a `m x n` matrix `mat` of integers. Implement a function `sort_diagonally` which returns the matrix after sorting each of its diagonals in ascending order starting from the top-left to the bottom-right. **Function Signature**: ```python def sort_diagonally(mat: List[List[int]]) -> List[List[int]]: ``` # Input * `mat`: A 2D list of integers representing the matrix. # Output * A 2D list of integers where each diagonal starting from the top-left to the bottom-right has been sorted in ascending order. # Constraints * 1 <= len(mat), len(mat[0]) <= 100 * -100 <= mat[i][j] <= 100 # Example ```python mat = [ [3, 3, 1, 1], [2, 2, 1, 2], [1, 1, 1, 2], ] # Expected output: # [ # [1, 1, 1, 1], # [1, 2, 2, 2], # [1, 2, 3, 3] # ] print(sort_diagonally(mat)) ``` # Explanation In the example above, each diagonal is sorted separately. For instance: - The diagonal starting at `mat[0][0]` contains [3, 2, 1] which gets sorted to [1, 2, 3]. - The diagonal starting at `mat[0][1]` and `mat[1][0]` contains [3, 2] which gets sorted to [2, 3]. Ensure that the implementation handles edge cases effectively, such as non-square matrices and matrices with only one row or one column. **Requirements**: - The function should have a time complexity within O((m+n) * min(m, n) * log(min(m, n))) where `m` is the number of rows and `n` is the number of columns of the matrix. - The solution should be efficient in terms of both time and space, in adherence to the constraints provided.","solution":"from typing import List from collections import defaultdict def sort_diagonally(mat: List[List[int]]) -> List[List[int]]: Returns the matrix with each of its diagonals sorted in ascending order. row, col = len(mat), len(mat[0]) diagonals = defaultdict(list) # Collect the values of each diagonal for i in range(row): for j in range(col): diagonals[i - j].append(mat[i][j]) # Sort each diagonal for key in diagonals: diagonals[key].sort() # Reconstruct the sorted matrix for i in range(row): for j in range(col): mat[i][j] = diagonals[i - j].pop(0) return mat"},{"question":"# Validity of Nested Structures You have been tasked to validate nested structures for proper formatting in a custom markup language. The structures follow these patterns: `<tag>`, `</tag>` and `[link]`, `[/link]`. Your goal is to implement a function `is_valid_markup(s: str) -> bool` to determine if the input string `s` is valid. **Specifications**: * Opening tags are `<tag>` and `[link]`. * Closing tags are `</tag>` and `[/link]`. * The tags must close in the correct order: valid pairs are `<tag></tag>` and `[link][/link]`. * An opening tag may enclose another set of tags. # Input: * `s` (1 ≤ len(s) ≤ 10^4): a string of characters including any characters outside of the specified tags. Assume that the tag names and link names are exactly \'tag\' and \'link\' without additional attributes. # Output: * Return `True` if the string is valid, and `False` otherwise. # Example: ```python assert is_valid_markup(\\"<tag>content here</tag>\\") == True assert is_valid_markup(\\"[link]<tag>link within tag</tag>[/link]\\") == True assert is_valid_markup(\\"<tag><tag></tag></tag>\\") == True assert is_valid_markup(\\"<tag>[link]content[/link]</tag>\\") == True assert is_valid_markup(\\"<tag></tag>\\") == True assert is_valid_markup(\\"<tag><tag></tag>\\") == False # Missing outer closing tag assert is_valid_markup(\\"<tag><tag></tag></link>\\") == False # Basic mismatch assert is_valid_markup(\\"<tag>[link]</[/tag>\\") == False # Broken nesting assert is_valid_markup(\\"[link]</link>\\") == False # Lone closing ``` # Notes: * You are free to assume that all tags if present must be properly nested to form valid structures. * Consider using a stack to handle opening and closing tags efficiently. * This question examines understanding of stack use and correct handling of nested structures.","solution":"def is_valid_markup(s: str) -> bool: stack = [] i = 0 n = len(s) while i < n: if s[i:i+5] == \\"<tag>\\": stack.append(\\"<tag>\\") i += 5 elif s[i:i+6] == \\"</tag>\\": if not stack or stack[-1] != \\"<tag>\\": return False stack.pop() i += 6 elif s[i:i+6] == \\"[link]\\": stack.append(\\"[link]\\") i += 6 elif s[i:i+7] == \\"[/link]\\": if not stack or stack[-1] != \\"[link]\\": return False stack.pop() i += 7 else: i += 1 return len(stack) == 0"},{"question":"# Question: Determining Safe Prime Numbers and Their Primitive Roots Problem Statement A prime number `p` is called a **safe prime** if `(p-1)/2` is also a prime number. Safe primes are notable in cryptography for their resistance to certain attacks. Your task is to write a function that determines whether a given number `p` is a safe prime and, if so, returns its primitive roots. If `p` is not a safe prime, your function should return an appropriate message indicating that. Inputs and Outputs * **Input**: An integer `p` (2 <= p <= 1,000,000). * **Output**: * A list of integers representing the primitive roots of `p`, if `p` is a safe prime. * A string `\\"Not a Safe Prime\\"`, if `p` is not a safe prime. Constraints and Requirements * Your solution should ensure the efficient checking of prime numbers. * Performance should be considered, especially for the upper limit of `p`. Sample Input/Output * **Example 1**: * Input: `p = 7` * Output: `[3, 5]` (7 is a safe prime, and 3 and 5 are its primitive roots) * **Example 2**: * Input: `p = 9` * Output: `\\"Not a Safe Prime\\"` (9 is not a prime) Function Signature ```python def is_safe_prime_with_primitive_roots(p: int) -> Union[List[int], str]: pass ``` Scenario Context Consider this scenario: in cryptographic protocols, the choice of prime numbers that are also safe primes is crucial for maintaining security. Incorrect selection might lead to vulnerabilities. This problem ensures a deep understanding of number theory, algorithms for primality testing, and the ability to compute primitive roots efficiently. # Good Luck!","solution":"from typing import List, Union import math def is_prime(n: int) -> bool: if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True def is_safe_prime_with_primitive_roots(p: int) -> Union[List[int], str]: if not is_prime(p): return \\"Not a Safe Prime\\" q = (p - 1) // 2 if not is_prime(q): return \\"Not a Safe Prime\\" # To find primitive roots of p def primitive_roots(p): roots = [] required_set = set(num for num in range(1, p) if math.gcd(num, p) == 1) for g in range(1, p): actual_set = set(pow(g, powers, p) for powers in range(1, p)) if required_set == actual_set: roots.append(g) return roots return primitive_roots(p)"},{"question":"# Zigzag Level Order Traversal of a Binary Tree You are given the root of a binary tree. Implement a function `zigzag_level(root)` that returns the zigzag level order traversal of its nodes\' values. Function Signature ```python def zigzag_level(root): # Your code here ``` # Input - `root`: a binary tree node, could be `None` which implies an empty tree. # Output - `res`: a list of lists, where each inner list contains the node values at that level of the tree. The node values in each inner list should alternate between left-to-right and right-to-left order at every level. # Constraints - The number of nodes in the binary tree is in the range `[0, 10^4]`. - Node values are integers. # Example Consider the following binary tree: ``` 3 / 9 20 / 15 7 ``` For this binary tree, your function should return the following output: ```python [ [3], [20, 9], [15, 7] ] ``` # Additional Instructions - You should handle edge cases such as: - The tree being empty (root is `None`). - Trees where nodes have only left or only right children. Ensure your function adheres to the provided function signature and meets the performance requirements. Happy coding!","solution":"from collections import deque class TreeNode: def __init__(self, key=0, left=None, right=None): self.val = key self.left = left self.right = right def zigzag_level(root): if not root: return [] results = [] nodes_queue = deque([root]) left_to_right = True while nodes_queue: level = deque() for _ in range(len(nodes_queue)): node = nodes_queue.popleft() if left_to_right: level.append(node.val) else: level.appendleft(node.val) if node.left: nodes_queue.append(node.left) if node.right: nodes_queue.append(node.right) results.append(list(level)) left_to_right = not left_to_right return results"},{"question":"# Problem: Enhanced Rabin-Karp Implementation You are tasked with improving the given implementation of the Rabin-Karp algorithm. Your goal is to write a function `enhanced_rabin_karp(pattern, text)` that efficiently checks for occurrences of the pattern within the text. Additionally, your function should return all starting indices where the pattern is found in the text. # Requirements: 1. Implement a robust method for hash collision resolution. 2. Ensure the function handles all edge cases. 3. Maintain a time complexity of O(n + m) in average and best cases. 4. Use the rolling hash technique for efficiency. # Input: * `pattern`: A non-empty string representing the pattern to search for. * `text`: A non-empty string representing the text to search within. # Output: * A list of integers denoting all starting indices in the text where the pattern is found. If the pattern is not found, return an empty list. # Constraints: * Assume the text and pattern consist of lowercase English letters (\'a\' to \'z\') only. * The text has a maximum length of 10^6. * The pattern has a maximum length of 10^5. # Example: ```python def enhanced_rabin_karp(pattern: str, text: str) -> List[int]: # Your implementation goes here # Example usage print(enhanced_rabin_karp(\'test\', \'this is a test text with test cases\')) # Output: [10, 25] print(enhanced_rabin_karp(\'a\', \'banana\')) # Output: [1, 3, 5] print(enhanced_rabin_karp(\'notfound\', \'this text does not have the pattern\')) # Output: [] ``` # Notes: - Your implementation should be robust and handle all edge cases efficiently. - Consider potential hash collisions and provide a mechanism to verify actual string matches when hashes collide.","solution":"def enhanced_rabin_karp(pattern: str, text: str) -> list: Enhanced Rabin-Karp algorithm to find all starting indices of pattern in text. if not pattern or not text: return [] n, m = len(text), len(pattern) result = [] if m > n: return result # Define a prime number for modulo operations prime = 101 # Define base value for hashing base = 256 pattern_hash = 0 text_hash = 0 h = 1 # Precompute base^(m-1) % prime for _ in range(m-1): h = (h * base) % prime # Compute initial hash for pattern and first window of text for i in range(m): pattern_hash = (base * pattern_hash + ord(pattern[i])) % prime text_hash = (base * text_hash + ord(text[i])) % prime # Slide over text to find matches for i in range(n - m + 1): if pattern_hash == text_hash: if text[i:i+m] == pattern: result.append(i) if i < n - m: text_hash = (base * (text_hash - ord(text[i]) * h) + ord(text[i + m])) % prime if text_hash < 0: text_hash += prime return result"},{"question":"# Question You are tasked with implementing two functions that convert numbers between different bases. This exercise will test your understanding of numeral systems and your ability to manipulate numbers in code. 1. **Function 1: `int_to_base`** This function should take an integer and convert it to a specified base. **Signature:** ```python def int_to_base(num: int, base: int) -> str: ``` **Input:** - `num` (int): The integer to convert. - `base` (int): The base to convert the integer to, which should be between 2 and 36. **Output:** - `str`: The string representation of the integer in the specified base. If the number is negative, the result should start with `-`. **Example:** ```python int_to_base(5, 2) # should return \'101\' int_to_base(-31, 16) # should return \'-1F\' ``` 2. **Function 2: `base_to_int`** This function should convert a string representation of a number in a given base to an integer. **Signature:** ```python def base_to_int(str_to_convert: str, base: int) -> int: ``` **Input:** - `str_to_convert` (str): The string representation of the number in the specified base. - `base` (int): The base of the number, which should be between 2 and 36. **Output:** - `int`: The integer value of the string in the specified base. **Example:** ```python base_to_int(\'F\', 16) # should return 15 base_to_int(\'-1F\', 16) # should return -31 ``` Constraints - The base should be between 2 and 36 inclusive. - The input strings to `base_to_int` will only contain valid digits and letters for the given base. Performance Requirements - The functions should handle up to 1,000,000 conversions efficiently. - Consider edge cases where the number is zero or negative. Scenario Imagine you are a software engineer working on a system that handles data in various numeral systems for digital communications and storage solutions. Implement these base conversion functions to ensure that different parts of your system can correctly interpret and convert numerical data.","solution":"def int_to_base(num: int, base: int) -> str: Converts an integer to a specified base (between 2 and 36). if base < 2 or base > 36: raise ValueError(\\"Base must be between 2 and 36\\") if num == 0: return \\"0\\" digits = \\"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\\" negative = num < 0 num = abs(num) result = \\"\\" while num > 0: result = digits[num % base] + result num //= base if negative: result = \\"-\\" + result return result def base_to_int(str_to_convert: str, base: int) -> int: Converts a string representation of a number in a given base to an integer. if base < 2 or base > 36: raise ValueError(\\"Base must be between 2 and 36\\") str_to_convert = str_to_convert.upper().strip() if str_to_convert[0] == \'-\': negative = True str_to_convert = str_to_convert[1:] else: negative = False digits = \\"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\\" num = 0 for char in str_to_convert: num = num * base + digits.index(char) if negative: num = -num return num"},{"question":"# Ordered Stack Implementation **Objective**: Implement methods that utilize and demonstrate ordered stack\'s unique properties. **Question**: You are required to implement two additional methods for an `OrderedStack` data structure which maintains elements in a defined order (highest value at the top). These methods should be: 1. **sort_stack()**: This method should sort the stack if it is not already sorted. However, since our stack implementation inherently keeps it sorted, you need to ensure this property remains and verify it, if required. 2. **merge_ordered_stack(other)**: This method should take another `OrderedStack` and merge it into the current stack such that the order is preserved. The resultant stack should also be ordered from the lowest value at the bottom to the highest value at the top. You may assume that the elements in the other stack are already sorted based on the same rule (highest value at the top). **Function Signature**: ```python class OrderedStack: def __init__(self): self.items = [] def is_empty(self): return self.items == [] def push(self, item): ... def pop(self): ... def peek(self): ... def size(self): ... def sort_stack(self): # Ensure the stack remains sorted pass def merge_ordered_stack(self, other): # Merge another ordered stack while maintaining the current stack\'s order pass ``` **Constraints**: - The stack can hold any number of integer elements. - Elements in the stacks being merged can be negative or positive integers. **Performance Requirements**: - Ensure that the `merge_ordered_stack` method operates efficiently, ideally in O(n + m) time complexity, where n is the number of elements in the current stack and m is the number of elements in the other stack. **Example**: ```python os1 = OrderedStack() os1.push(1) os1.push(3) os1.push(5) os2 = OrderedStack() os2.push(2) os2.push(4) os2.push(6) os1.merge_ordered_stack(os2) # Expected Output while not os1.is_empty(): print(os1.pop()) # Output should be 6, 5, 4, 3, 2, 1 in this order ``` Your implementation should cover edge cases such as merging with empty stacks and ensure that elements remain ordered even after multiple merges.","solution":"class OrderedStack: def __init__(self): self.items = [] def is_empty(self): return self.items == [] def push(self, item): # Ensure the stack remains sorted by inserting element in the correct position. temp_stack = [] while not self.is_empty() and self.peek() > item: temp_stack.append(self.pop()) self.items.append(item) while temp_stack: self.items.append(temp_stack.pop()) def pop(self): if self.is_empty(): raise IndexError(\\"pop from empty stack\\") return self.items.pop() def peek(self): if self.is_empty(): raise IndexError(\\"peek from empty stack\\") return self.items[-1] def size(self): return len(self.items) def sort_stack(self): # Stack is always sorted by push operation, no need to do anything. pass def merge_ordered_stack(self, other): temp = [] while not self.is_empty() and not other.is_empty(): if self.peek() > other.peek(): temp.append(self.pop()) else: temp.append(other.pop()) while not self.is_empty(): temp.append(self.pop()) while not other.is_empty(): temp.append(other.pop()) temp.reverse() self.items = temp"},{"question":"# Problem: Summary Ranges Given a sorted integer array `nums` without duplicates, your task is to write a function `summarize_ranges` that returns a summary of its ranges. A range `[a, b]` (inclusive) can be expressed in the form `\\"a->b\\"` if `a != b`; otherwise, it is simply `\\"a\\"`. Your function should output these ranges in the same order as they appear in the input array. Input * `nums` (List[int]): The sorted integer array without duplicates. * 1 ≤ `len(nums)` ≤ 10^4 * -10^4 ≤ `nums[i]` ≤ 10^4 Output * List[str]: The ranges represented as strings. Constraints * The input array is sorted in ascending order. * There are no duplicate elements in the array. # Examples Example 1: ``` Input: [0, 1, 2, 4, 5, 7] Output: [\\"0->2\\", \\"4->5\\", \\"7\\"] ``` Example 2: ``` Input: [0, 2, 3, 4, 6, 8, 9] Output: [\\"0\\", \\"2->4\\", \\"6\\", \\"8->9\\"] ``` Example 3: ``` Input: [] Output: [] ``` Example 4: ``` Input: [-1] Output: [\\"-1\\"] ``` Example 5: ``` Input: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] Output: [\\"0->9\\"] ``` # Implementation Requirements * Your function should have a time complexity of O(n) and space complexity of O(1) (excluding the space necessary for the output). * You must handle edge cases like empty arrays and single-element arrays correctly. ```python def summarize_ranges(nums: List[int]) -> List[str]: # Your code here ```","solution":"from typing import List def summarize_ranges(nums: List[int]) -> List[str]: if not nums: return [] ranges = [] start = nums[0] end = start for n in nums[1:]: if n == end + 1: end = n else: ranges.append(f\\"{start}->{end}\\" if start != end else str(start)) start = n end = start ranges.append(f\\"{start}->{end}\\" if start != end else str(start)) return ranges"},{"question":"# Nearest Neighbor Classification Given the provided functions, refine and expand the nearest neighbor algorithm to handle more extensive training datasets efficiently. Implement the function `optimized_nearest_neighbor` that improves the performance of the nearest neighbor search using any suitable data structure (such as KD-tree or Ball-tree). # Function Signature ```python def optimized_nearest_neighbor(x: tuple, tSet: dict) -> any: Optimized nearest neighbor using KD-Tree or Ball-Tree. Arguments: x {tuple}: Input vector for classification. tSet {dict}: Training set with vectors as keys and labels as values. Returns: any: Classification result of the nearest neighbor. ``` # Constraints 1. The input vector `x` and the training set vectors are tuples of floats with identical lengths. 2. The training set `tSet` can be quite large (up to 10^6 vectors). 3. You may assume the distance between any two vectors is unique. # Example ```python # Training set with tuples as keys and class labels as values training_set = { (1.0, 2.0, 3.0): \\"A\\", (4.0, 5.0, 6.0): \\"B\\", (7.0, 8.0, 9.0): \\"C\\" } x = (2.0, 3.0, 4.0) print(optimized_nearest_neighbor(x, training_set)) # Expected output: \\"A\\" (assuming \\"A\\" is the nearest class by Euclidean distance) ``` # Performance Requirements 1. The function should handle large datasets (up to 10^6 vectors) efficiently. 2. The implementation should aim for sub-linear search time, making use of appropriate spatial data structures. # Notes * Detail any assumptions you make. * Handle and document edge cases. * Explain your chosen optimization strategy.","solution":"from scipy.spatial import KDTree def optimized_nearest_neighbor(x, tSet): Optimized nearest neighbor using KD-Tree. Arguments: x {tuple}: Input vector for classification. tSet {dict}: Training set with vectors as keys and labels as values. Returns: any: Classification result of the nearest neighbor. # Extract training points and their labels points = list(tSet.keys()) labels = list(tSet.values()) # Build the KD-Tree kdtree = KDTree(points) # Query the nearest neighbor for input vector x _, index = kdtree.query(x) # Retrieve and return the label of the nearest neighbor return labels[index]"},{"question":"# Hash Table Implementation with Dynamic Resizing Scenario You are tasked with extending the functionality of the `ResizableHashTable` class provided in the provided code snippet. The current implementation expands the table size when the load factor exceeds 2/3. However, it does not have a provision to shrink the table size if the load factor becomes too low. This question assesses your understanding of dynamically resizing hash tables and managing their size to maintain efficiency by shrinking when the load factor drops below a certain threshold. Task 1. **Implementation**: Extend the `ResizableHashTable` class to automatically shrink its size when the number of elements falls below 1/3 of its capacity. 2. **Method to Add**: Implement a `__shrink` method that halves the size of the table (to a minimum of the initial size, MIN_SIZE) and redistributes the elements using the hash function. Requirements * **Method `put(key, value)`**: Should call `__resize` when the allocated capacity reaches 2/3, and call `__shrink` when the number of elements falls below 1/3. * **Add `__shrink` method**: Shrinks the table size to half if the load factor goes below 1/3, but not smaller than MIN_SIZE. * **Adjust the `del_(key)` Method**: Ensure it considers shrinking of the table. Expected Function Signatures: ```python class ResizableHashTable(HashTable): def put(self, key, value): ... def __shrink(self): ... def del_(self, key): ... ``` Input/Output Specifications: * `put(key, value)`: Adds or updates the key-value pair. If the table size is met beyond the threshold, resizing is triggered. * `del_(key)`: Removes the key-value pair. If the size goes below the threshold, shrinking is triggered. * `__shrink()`: Internal method to handle shrinking based on the current size. Constraints: - Use linear probing for collision handling. - Maintain the correct number of elements after resizing. - Ensure syncing of the load factor triggering resize and shrink operations. Example Usage: ```python hash_table = ResizableHashTable() hash_table.put(1, \'Value1\') hash_table.put(2, \'Value2\') ... # Many operations hash_table.del_(1) # Should trigger shrink if load factor is below threshold. ``` Note: Ensure your implementation efficiently manages memory and maintains the complexities of the respective operations.","solution":"class HashTable: Simple hash table implementation with linear probing MIN_SIZE = 8 def __init__(self): self.capacity = self.MIN_SIZE self.size = 0 self.table = [None] * self.capacity def hash_function(self, key): return hash(key) % self.capacity def put(self, key, value): if self.size >= 2 * self.capacity // 3: self.__resize() index = self.hash_function(key) while self.table[index] is not None and self.table[index][0] != key: index = (index + 1) % self.capacity if self.table[index] is None: self.size += 1 self.table[index] = (key, value) def get(self, key): index = self.hash_function(key) while self.table[index] is not None: if self.table[index][0] == key: return self.table[index][1] index = (index + 1) % self.capacity return None def del_(self, key): index = self.hash_function(key) while self.table[index] is not None: if self.table[index][0] == key: self.table[index] = None self.size -= 1 self.__shrink() return index = (index + 1) % self.capacity def __resize(self): old_table = self.table self.capacity *= 2 self.size = 0 self.table = [None] * self.capacity for entry in old_table: if entry is not None: self.put(entry[0], entry[1]) def __shrink(self): if self.capacity <= self.MIN_SIZE or self.size * 3 >= self.capacity: return old_table = self.table self.capacity //= 2 self.size = 0 self.table = [None] * self.capacity for entry in old_table: if entry is not None: self.put(entry[0], entry[1])"},{"question":"# Question You are helping a digital display design team. They need a function to verify if a number will appear the same when rotated 180 degrees. A number that satisfies this property is called a strobogrammatic number. Write a function `is_strobogrammatic(num: str) -> bool` that determines if the given number (represented as a string) is strobogrammatic. Input - A string `num` which represents the number. (1 <= len(num) <= 10^4) Output - Return `True` if `num` is a strobogrammatic number, otherwise return `False`. Constraints - The function should handle up to 10,000 characters efficiently. - Only characters \'0\', \'1\', \'6\', \'8\', and \'9\' are valid for strobogrammatic consideration. Example 1. `is_strobogrammatic(\\"69\\")` should return `True`. 2. `is_strobogrammatic(\\"88\\")` should return `True`. 3. `is_strobogrammatic(\\"962\\")` should return `False`. Notes - Ensure your solution handles edge cases, such as single character numbers and very long strings efficiently. - Consider both implementations provided in the code snippets and choose the most efficient one or propose your own optimization.","solution":"def is_strobogrammatic(num: str) -> bool: Determines if the given number is strobogrammatic. # Mappings of valid strobogrammatic characters strobogrammatic_map = { \'0\': \'0\', \'1\': \'1\', \'6\': \'9\', \'8\': \'8\', \'9\': \'6\' } left, right = 0, len(num) - 1 while left <= right: if num[left] not in strobogrammatic_map or strobogrammatic_map[num[left]] != num[right]: return False left += 1 right -= 1 return True"},{"question":"Problem Statement Given a positive integer ( N ), write a function `longest_consecutive_ones_distance(N)` to find and return the longest distance between two consecutive \'1\'s in the binary representation of ( N ). If there are no two consecutive \'1\'s, return 0. Input Format - A single integer ( N ) where ( 1 leq N leq 10^9 ). Output Format - A single integer representing the longest distance between two consecutive \'1\'s in the binary representation of ( N ). Constraint - Your solution should have a time complexity of ( O(log N) ) and a space complexity of ( O(1) ). Example - **Example 1**: - Input: `22` - Output: `2` - Explanation: The binary representation of 22 is `10110`. The distances between consecutive pairs of `1`s are `2` and `1`. The longest distance is `2`. - **Example 2**: - Input: `5` - Output: `2` - Explanation: The binary representation of 5 is `101`. The only consecutive pair of `1`s has a distance of `2`. - **Example 3**: - Input: `6` - Output: `1` - Explanation: The binary representation of 6 is `110`. The only consecutive pair of `1`s has a distance of `1`. - **Example 4**: - Input: `8` - Output: `0` - Explanation: The binary representation of 8 is `1000`. There are no consecutive pairs of `1`s. Implementation Implement the function using efficient bitwise operations to achieve the required complexity. Function Signature ```python def longest_consecutive_ones_distance(N: int) -> int: ... ```","solution":"def longest_consecutive_ones_distance(N): Returns the longest distance between two consecutive \'1\'s in the binary representation of N. max_distance = 0 last_position = -1 position = 0 while N > 0: if N & 1 == 1: if last_position != -1: max_distance = max(max_distance, position - last_position) last_position = position position += 1 N >>= 1 return max_distance"},{"question":"You are given a task to implement a function that inverts an n x n matrix if possible. Your function needs to handle matrices of arbitrary size (greater than 1x1) and provide correct results for matrices that are invertible. In case a matrix is non-invertible or invalid, your function should handle these cases gracefully by returning specific error codes. # Function Stub ```python def invert_matrix(matrix: List[List[float]]) -> List[List[float]]: pass ``` # Input - `matrix`: a list of lists representing an n x n matrix of floats. # Output - A list of lists representing the inverted matrix, or a specific error code matrix: 1. `[[ -1 ]]` if the input is not a matrix. 2. `[[ -2 ]]` if the matrix is not square. 3. `[[ -3 ]]` if the matrix is too small to be inverted (i.e., 1x1 matrix). 4. `[[ -4 ]]` if the matrix is singular (i.e., determinant is 0). # Constraints - The input matrix will only contain float values. - You are not allowed to use any external libraries for matrix inversion. # Example Example 1: **Input**: ```python matrix = [ [4, 7], [2, 6] ] ``` **Output**: ```python [ [0.6, -0.7], [-0.2, 0.4] ] ``` Example 2: **Input**: ```python matrix = [ [1, 2, 3], [0, 1, 4], [5, 6, 0] ] ``` **Output**: ```python [ [-24, 18, 5], [20, -15, -4], [-5, 4, 1] ] ``` Example 3: **Input**: ```python matrix = [ [1, 2], [2, 4] ] ``` **Output**: ```python [ [-4] ] ``` **Note**: The output `[[ -4 ]]` indicates that the matrix is singular and cannot be inverted. # Notes - Ensure that your implementation handles edge cases and invalid inputs as specified. - Make use of helper functions to organize your code and reduce complexity.","solution":"from typing import List def determinant(matrix: List[List[float]]) -> float: if len(matrix) == 1: return matrix[0][0] if len(matrix) == 2: return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0] det = 0 for c in range(len(matrix)): submatrix = [row[:c] + row[c + 1:] for row in matrix[1:]] det += ((-1) ** c) * matrix[0][c] * determinant(submatrix) return det def get_matrix_minor(matrix: List[List[float]], i: int, j: int) -> List[List[float]]: return [row[:j] + row[j + 1:] for row in (matrix[:i] + matrix[i + 1:])] def invert_matrix(matrix: List[List[float]]) -> List[List[float]]: if not (isinstance(matrix, list) and all(isinstance(row, list) for row in matrix)): return [[-1]] n = len(matrix) if any(len(row) != n for row in matrix): return [[-2]] if n == 1: return [[-3]] det = determinant(matrix) if det == 0: return [[-4]] cofactors = [] for r in range(n): cofactorRow = [] for c in range(n): minor = get_matrix_minor(matrix, r, c) cofactorRow.append(((-1) ** (r + c)) * determinant(minor)) cofactors.append(cofactorRow) cofactors = list(map(list, zip(*cofactors))) for r in range(n): for c in range(n): cofactors[r][c] = cofactors[r][c] / det return cofactors"},{"question":"You are part of a development team creating a basic machine learning library. You need to implement a core classification feature using the Nearest Neighbor algorithm. Your task is to write a function to classify an input vector based on the closest vectors in a given training set. # Function Signature ```python def classify_vector(input_vector: Tuple[int, ...], training_set: Dict[Tuple[int, ...], Any]) -> Any: Classifies the input vector based on the nearest neighbor principle. Parameters: input_vector (Tuple[int, ...]): The vector to be classified. training_set (Dict[Tuple[int, ...], Any]): A dictionary where keys are vectors (tuples) and values are their classifications. Returns: Any: The classification of the input vector based on its nearest neighbor in the training set. ``` # Input * `input_vector`: A tuple of integers representing the input vector to be classified. * `training_set`: A dictionary where the keys are tuples of integers (vectors) and the values are their corresponding classifications. # Output * The classification of the input vector based on its nearest neighbor in the training set. # Constraints * All vectors (input and in the training set) will have the same length. * The training set will contain at least one vector. * The dimensions of the vectors will not exceed 100. # Example ```python input_vector = (1, 2) training_set = { (1, 2): \'A\', (2, 3): \'B\', (5, 5): \'C\' } assert classify_vector(input_vector, training_set) == \'A\' ``` # Performance Requirements * Ensure that the function handles up to 1000 training vectors efficiently. * Optimize distance calculations to minimize redundant computations.","solution":"from typing import Tuple, Dict, Any import math def calculate_distance(v1: Tuple[int, ...], v2: Tuple[int, ...]) -> float: Calculates the Euclidean distance between two vectors. return math.sqrt(sum((a - b) ** 2 for a, b in zip(v1, v2))) def classify_vector(input_vector: Tuple[int, ...], training_set: Dict[Tuple[int, ...], Any]) -> Any: Classifies the input vector based on the nearest neighbor principle. Parameters: input_vector (Tuple[int, ...]): The vector to be classified. training_set (Dict[Tuple[int, ...], Any]): A dictionary where keys are vectors (tuples) and values are their classifications. Returns: Any: The classification of the input vector based on its nearest neighbor in the training set. nearest_neighbor = None min_distance = float(\'inf\') for vector, classification in training_set.items(): distance = calculate_distance(input_vector, vector) if distance < min_distance: min_distance = distance nearest_neighbor = classification return nearest_neighbor"},{"question":"You are assigned the task of planning the location for a new community center in a city represented as a 2D grid. The grid contains buildings (marked as `1`), empty plots (marked as `0`), and obstacles (marked as `2`). The goal is to find the empty plot such that the sum of the shortest distances from all buildings to that plot is minimized. If there is no such plot that can be reached from all buildings, return `-1`. Implement the function `shortest_distance(grid)` which returns the minimum distance sum if such a plot exists, and `-1` otherwise. # Function Signature ```python def shortest_distance(grid: List[List[int]]) -> int: ``` # Input * `grid`, a list of lists of integers, where: * `1` represents a building. * `0` represents an empty plot. * `2` represents an obstacle. # Output * An integer representing the minimum distance sum from all buildings to the optimal empty plot, or `-1` if such a plot does not exist. # Example ```python grid = [ [1, 0, 2, 0, 1], [0, 0, 0, 0, 0], [0, 0, 1, 0, 0] ] print(shortest_distance(grid)) # Output: 7 ``` # Constraints 1. All buildings should be accessible from the optimal plot. 2. The grid size is limited to 50 x 50. 3. The number of buildings is limited to 100. # Performance Requirements 1. The solution should be efficient with respect to both time and space to handle the maximum input size.","solution":"from typing import List, Tuple from collections import deque def shortest_distance(grid: List[List[int]]) -> int: # Helper function to perform BFS from any building def bfs(start_row: int, start_col: int) -> Tuple[List[List[int]], List[List[int]]]: rows, cols = len(grid), len(grid[0]) visited = [[False for _ in range(cols)] for _ in range(rows)] distances = [[0 for _ in range(cols)] for _ in range(rows)] q = deque([(start_row, start_col, 0)]) while q: row, col, dist = q.popleft() for d_row, d_col in [(-1, 0), (1, 0), (0, -1), (0, 1)]: n_row, n_col = row + d_row, col + d_col if 0 <= n_row < rows and 0 <= n_col < cols and not visited[n_row][n_col] and grid[n_row][n_col] == 0: visited[n_row][n_col] = True distances[n_row][n_col] = dist + 1 q.append((n_row, n_col, dist + 1)) return visited, distances rows, cols = len(grid), len(grid[0]) total_distances = [[0 for _ in range(cols)] for _ in range(rows)] total_reachability = [[0 for _ in range(cols)] for _ in range(rows)] building_count = 0 for r in range(rows): for c in range(cols): if grid[r][c] == 1: building_count += 1 visited, distances = bfs(r, c) for i in range(rows): for j in range(cols): if visited[i][j]: total_distances[i][j] += distances[i][j] total_reachability[i][j] += 1 min_distance = float(\'inf\') for r in range(rows): for c in range(cols): if grid[r][c] == 0 and total_reachability[r][c] == building_count: min_distance = min(min_distance, total_distances[r][c]) return min_distance if min_distance != float(\'inf\') else -1"},{"question":"# RSA Encryption Algorithm Implementation and Performance Verification As part of understanding the RSA encryption algorithm and its efficiency, you need to extend the current RSA implementation to include: 1. Verification of prime generation speed using probabilistic methods. 2. An optimized method for calculating the modular inverse. 3. Understanding RSA\'s secure use, especially in terms of edge cases and typical performance under varied conditions. Implement the following functions: 1. `gen_prime_optimized(k, seed=None)`: This function should generate a prime number of `k` bits using a probabilistic prime-testing method (like Miller-Rabin). 2. `modinv_optimized(a, m)`: Implement an optimized method for calculating the modular inverse using the Extended Euclidean Algorithm. 3. `analyze_performance()`: Write a function that generates RSA keys of various lengths (e.g., 512, 1024, 2048 bits), measures the time required for key generation, encryption, and decryption processes, and then prints the results. # Input and Output Formats 1. **Function: gen_prime_optimized** - `k` (int): Number of bits. - `seed` (int, optional): Seed for the random number generator. - **Returns** (int): A prime number with `k` bits. 2. **Function: modinv_optimized** - `a` (int): Integer value for which the modular inverse is to be found. - `m` (int): Modulus value. - **Returns** (int): The modular inverse of `a` modulo `m`. 3. **Function: analyze_performance** - **Returns**: Prints the time measurements for key generation, encryption, and decryption at various key sizes. # Constraints 1. Key size for RSA should be at least 512 bits for practical security. 2. Time taken for key generation, encryption, and decryption should be acceptable for key sizes up to 2048 bits. # Example ```python # Example usage of the functions # Generate an optimized prime number of 16 bits prime = gen_prime_optimized(16, seed=123) print(prime) # Output: A prime number with 16 bits # Calculate the modular inverse using optimized method inverse = modinv_optimized(17, 3120) print(inverse) # Output: Modular inverse of 17 modulo 3120 # Analyze performance for different key sizes analyze_performance() ```","solution":"import random import time def miller_rabin_test(n, k): Perform the Miller-Rabin primality test. if n == 2 or n == 3: return True if n <= 1 or n % 2 == 0: return False r, s = 0, n - 1 while s % 2 == 0: r += 1 s //= 2 for _ in range(k): a = random.randrange(2, n - 1) x = pow(a, s, n) if x == 1 or x == n - 1: continue for _ in range(r - 1): x = pow(x, 2, n) if x == n - 1: break else: return False return True def gen_prime_optimized(k, seed=None): Generate a k-bit prime number using a probabilistic method. if seed is not None: random.seed(seed) while True: prime_candidate = random.getrandbits(k) prime_candidate |= (1 << k - 1) | 1 # Ensure it\'s of k-bit length and odd if miller_rabin_test(prime_candidate, 40): return prime_candidate def extended_euclidean_algorithm(a, b): Extended Euclidean Algorithm to find the gcd and coefficients. if a == 0: return (b, 0, 1) else: g, x, y = extended_euclidean_algorithm(b % a, a) return (g, y - (b // a) * x, x) def modinv_optimized(a, m): Find the modular inverse of a modulo m using the Extended Euclidean Algorithm. g, x, _ = extended_euclidean_algorithm(a, m) if g != 1: raise ValueError(\'Modular inverse does not exist\') else: return x % m def rsa_key_generation(bits): p = gen_prime_optimized(bits // 2) q = gen_prime_optimized(bits // 2) n = p * q phi = (p - 1) * (q - 1) e = 65537 # Commonly used prime exponent d = modinv_optimized(e, phi) return (e, d, n) def analyze_performance(): key_sizes = [512, 1024, 2048] for bits in key_sizes: print(f\\"Analyzing performance for {bits}-bit RSA keys:\\") start_time = time.time() public_key, private_key, n = rsa_key_generation(bits) key_gen_time = time.time() - start_time message = random.randint(2, n-1) start_time = time.time() encrypted_message = pow(message, public_key, n) encryption_time = time.time() - start_time start_time = time.time() decrypted_message = pow(encrypted_message, private_key, n) decryption_time = time.time() - start_time print(f\\"Key Generation Time: {key_gen_time:.4f} seconds\\") print(f\\"Encryption Time: {encryption_time:.4f} seconds\\") print(f\\"Decryption Time: {decryption_time:.4f} seconds\\") print(f\\"Original Message: {message}, Decrypted Message: {decrypted_message}\\") assert message == decrypted_message, \\"Error: Decrypted message does not match the original\\""},{"question":"Objective: Write a function that identifies the number that occurs exactly once in a list where every other number appears exactly three times. Context: You are working on a data processing pipeline that consumes logs from multiple systems. Occasionally, due to rare events, some logs might be duplicated exactly three times. However, there’s usually one unique log that appears exactly once which might indicate a critical error. Your task is to write a function that can efficiently find this unique log ID from a list of log IDs. Input: * A list of integers `nums` where each integer appears exactly three times except for one integer which appears exactly once. Output: * Return the integer that occurs exactly once in the list. Constraints: * The solution must run in linear time. * The solution must not use extra memory (i.e., it should use O(1) additional space). Example: ```python # Input nums = [2, 2, 3, 2, 5, 5, 5, 8, 8, 8] # Output 3 # Input nums = [-1, -1, -1, -3, -7, -7, -7] # Output -3 ``` Function Signature: ```python def find_single_number(nums: [int]) -> int: pass ``` Write your function `find_single_number` to fulfill the above requirements.","solution":"def find_single_number(nums): Finds the number that occurs exactly once in the list where every other number appears exactly three times. Args: nums (List[int]): List of integers where each integer appears exactly three times except for one integer which appears exactly once. Returns: int: The integer that appears exactly once. ones, twos = 0, 0 for num in nums: # `twos` holds the bits which are there in `num` twice twos |= ones & num # `ones` holds the bits which are there in `num` ones ones ^= num # `common_bit_mask` contains all those bits which appear three times common_bit_mask = ~(ones & twos) # Remove common bits (bits appearing three times) ones &= common_bit_mask twos &= common_bit_mask return ones"},{"question":"**Problem:** Decoding Ways You are given an encoded message containing digits. Each digit or pair of digits maps to a letter using the following mapping: \'A\' -> 1, \'B\' -> 2, ... \'Z\' -> 26. Your task is to determine the total number of ways to decode the message. Write a function `decode_ways(s)` that returns the total number of ways to decode the input message `s`. **Input:** - A string `s` containing digits only, where `1 <= len(s) <= 100`. **Output:** - An integer representing the total number of ways to decode the message. **Constraints:** - Input string `s` will not contain any characters other than digits. - Input string `s` will be a valid encoding message that can be decoded at least in one way. **Examples:** 1. **Input:** \\"12\\" **Output:** 2 **Explanation:** \\"12\\" can be decoded as \\"AB\\" (1 2) or \\"L\\" (12). 2. **Input:** \\"226\\" **Output:** 3 **Explanation:** \\"226\\" can be decoded as \\"BZ\\" (2 26), \\"VF\\" (22 6), or \\"BBF\\" (2 2 6). 3. **Input:** \\"0\\" **Output:** 0 **Explanation:** \\"0\\" cannot be decoded. 4. **Input:** \\"10\\" **Output:** 1 **Explanation:** \\"10\\" can be decoded as \\"J\\". **Solution Template:** ```python def decode_ways(s): if not s or s[0] == \\"0\\": return 0 last_char, last_two_chars = 1, 1 for i in range(1, len(s)): last = last_char if s[i] != \\"0\\" else 0 last_two = last_two_chars if int(s[i-1:i+1]) < 27 and s[i-1] != \\"0\\" else 0 last_two_chars = last_char last_char = last + last_two return last_char ``` Implement the `decode_ways` function with the provided constraints and test cases.","solution":"def decode_ways(s): if not s or s[0] == \\"0\\": return 0 n = len(s) dp = [0] * (n + 1) dp[0] = 1 # Base case: an empty string has one way to be decoded dp[1] = 1 # Base case: non-zero single character has one way to be decoded for i in range(2, n + 1): one_digit = int(s[i-1:i]) two_digits = int(s[i-2:i]) if 1 <= one_digit <= 9: dp[i] += dp[i-1] if 10 <= two_digits <= 26: dp[i] += dp[i-2] return dp[n]"},{"question":"# Question: Implement a Doubly Linked List You are required to implement a Doubly Linked List with the following operations: 1. **Node class**: Define a class for a node. 2. **Insert at Head**: Insert a node at the head of the list. 3. **Insert at Tail**: Insert a node at the tail of the list. 4. **Delete from Head**: Remove a node from the head of the list. 5. **Delete from Tail**: Remove a node from the tail of the list. 6. **Find**: Find and return the first node with a given value in the list. If not found, return `None`. 7. **Display**: Print all the values of the list from head to tail. Implement the `DoublyLinkedList` class based on the given operations. # Constraints: * The values for the nodes are integers. * You should handle edge cases such as operations on an empty list correctly. * Your implementation should ensure no memory leaks (ensure nodes are properly deleted/garbage collected). # Performance Requirements: * Each of the operations must function within O(1) to O(n) time complexity depending on their nature. # Sample Input: ```python dll = DoublyLinkedList() dll.insert_at_head(1) dll.insert_at_tail(2) dll.insert_at_tail(3) dll.display() # Expected Output: [1, 2, 3] dll.delete_from_head() dll.display() # Expected Output: [2, 3] dll.delete_from_tail() dll.display() # Expected Output: [2] found_node = dll.find(2) print(found_node.value) # Expected Output: 2 not_found_node = dll.find(4) print(not_found_node) # Expected Output: None ``` # Template: ```python class DoublyLinkedListNode: def __init__(self, value): self.value = value self.next = None self.prev = None class DoublyLinkedList: def __init__(self): self.head = None self.tail = None def insert_at_head(self, value): # Your code here pass def insert_at_tail(self, value): # Your code here pass def delete_from_head(self): # Your code here pass def delete_from_tail(self): # Your code here pass def find(self, value): # Your code here pass def display(self): # Your code here pass ```","solution":"class DoublyLinkedListNode: def __init__(self, value): self.value = value self.next = None self.prev = None class DoublyLinkedList: def __init__(self): self.head = None self.tail = None def insert_at_head(self, value): new_node = DoublyLinkedListNode(value) if self.head is None: self.head = new_node self.tail = new_node else: new_node.next = self.head self.head.prev = new_node self.head = new_node def insert_at_tail(self, value): new_node = DoublyLinkedListNode(value) if self.tail is None: self.head = new_node self.tail = new_node else: new_node.prev = self.tail self.tail.next = new_node self.tail = new_node def delete_from_head(self): if self.head is None: return None if self.head == self.tail: self.head = None self.tail = None else: self.head = self.head.next self.head.prev = None def delete_from_tail(self): if self.tail is None: return None if self.head == self.tail: self.head = None self.tail = None else: self.tail = self.tail.prev self.tail.next = None def find(self, value): current = self.head while current: if current.value == value: return current current = current.next return None def display(self): elements = [] current = self.head while current: elements.append(current.value) current = current.next print(elements) return elements"},{"question":"# Graph Path Existence and Shortest Path Question You are given a graph represented as an adjacency list and you need to solve two tasks: 1. **Path Existence**: Implement a function to determine if there is a path between two given nodes using both DFS and BFS. 2. **Shortest Path**: Implement a function to find the shortest path between two given nodes using BFS (since BFS is more suitable for this due to level-wise exploration). Function Definitions: ```python def path_exists_dfs(graph, start, end): Determines if there is a path between start and end nodes using DFS. Parameters: graph (dict): Adjacency list of the graph. start (int/str): Starting node. end (int/str): Ending node. Returns: bool: True if path exists, False otherwise. pass def path_exists_bfs(graph, start, end): Determines if there is a path between start and end nodes using BFS. Parameters: graph (dict): Adjacency list of the graph. start (int/str): Starting node. end (int/str): Ending node. Returns: bool: True if path exists, False otherwise. pass def shortest_path_bfs(graph, start, end): Finds the shortest path between start and end nodes using BFS. Parameters: graph (dict): Adjacency list of the graph. start (int/str): Starting node. end (int/str): Ending node. Returns: list: The shortest path from start to end inclusive or empty if there is no path. pass ``` Input and Output Formats: - `graph` is a dictionary where keys are node identifiers and values are lists of connected nodes. - `start` and `end` are node identifiers. - For `path_exists_*` functions, return a boolean indicating if a path exists. - For `shortest_path_bfs`, return a list of nodes representing the shortest path from `start` to `end`, or an empty list if no path exists. Constraints: - Node identifiers are unique. - The graph is not necessarily connected. - The graph can be directed or undirected. - Assume there are no parallel edges. Performance Requirements: - Both `path_exists_*` functions should have a time complexity of O(V + E). - `shortest_path_bfs` should have a time complexity of O(V + E) and use a queue for BFS.","solution":"def path_exists_dfs(graph, start, end): Determines if there is a path between start and end nodes using DFS. visited = set() def dfs(node): if node == end: return True visited.add(node) for neighbor in graph.get(node, []): if neighbor not in visited: if dfs(neighbor): return True return False return dfs(start) def path_exists_bfs(graph, start, end): Determines if there is a path between start and end nodes using BFS. from collections import deque visited = set() queue = deque([start]) while queue: node = queue.popleft() if node == end: return True visited.add(node) for neighbor in graph.get(node, []): if neighbor not in visited: queue.append(neighbor) return False def shortest_path_bfs(graph, start, end): Finds the shortest path between start and end nodes using BFS. from collections import deque queue = deque([(start, [start])]) visited = set([start]) while queue: node, path = queue.popleft() for neighbor in graph.get(node, []): if neighbor == end: return path + [neighbor] if neighbor not in visited: visited.add(neighbor) queue.append((neighbor, path + [neighbor])) return []"},{"question":"# Context You are working for a software development company that frequently deals with sorting data. To ensure optimal performance, you need to understand and possibly improve the sorting algorithms being used. Your task is to implement an enhanced version of a classic sorting algorithm, ensuring it handles various edge cases efficiently. # Objective Implement the **Cocktail Shaker Sort** algorithm to sort a list of integers. Your implementation must correctly handle edge cases such as an already sorted list and a list in reverse order. # Input Format * A list of integers `arr` containing `n` elements where `1 <= n <= 1000` and `-10^6 <= arr[i] <= 10^6`. # Output Format * The sorted list of integers in ascending order. # Constraints * You must use the Cocktail Shaker Sort algorithm. * The function should be named `cocktail_shaker_sort`. * Optimize for best performance where possible. # Example ```python # Example 1: Input: [3, 0, 2, 5, -1, 4, 1] Output: [-1, 0, 1, 2, 3, 4, 5] # Example 2: Input: [1, 2, 3, 4, 5] Output: [1, 2, 3, 4, 5] ``` # Additional Specification You should write a function `cocktail_shaker_sort(arr)` that: * Takes a list of integers `arr`. * Returns a sorted list of integers.","solution":"def cocktail_shaker_sort(arr): Sorts the list using the Cocktail Shaker Sort algorithm. Parameters: arr (list of int): The list of integers to be sorted. Returns: list of int: The sorted list of integers. n = len(arr) start = 0 end = n - 1 while start <= end: swapped = False for i in range(start, end): if arr[i] > arr[i + 1]: arr[i], arr[i + 1] = arr[i + 1], arr[i] swapped = True end -= 1 if not swapped: break swapped = False for i in range(end - 1, start - 1, -1): if arr[i] > arr[i + 1]: arr[i], arr[i + 1] = arr[i + 1], arr[i] swapped = True start += 1 return arr"},{"question":"**Question:** You are provided with a graph represented as a dictionary where keys are node identifiers, and values are lists of adjacent nodes. Your task is to implement a function called `find_paths_in_graph` to find paths in the graph. Specifically, you need to implement three functionalities within this function: 1. Find any path between two nodes using a backtracking approach. 2. Find all possible paths between two nodes. 3. Find the shortest path between two nodes. # Function Signature ```python def find_paths_in_graph(graph: dict, start: str, end: str, mode: str) -> list: :param graph: A dictionary representing the graph. :param start: Starting node. :param end: Ending node. :param mode: One of \'single\', \'all\', \'shortest\' to specify the type of pathfinding. :return: List of nodes representing the path(s) or an empty list if no path found. # Your implementation here ``` # Input - **graph**: A dictionary where keys are nodes, and values are lists of connected neighbors. - **start**: A string denoting the starting node. - **end**: A string denoting the ending node. - **mode**: One of the modes `\'single\'`, `\'all\'`, `\'shortest\'`. # Output - For `\'single\'`: A list of nodes representing a single path from `start` to `end`. Empty list if no path found. - For `\'all\'`: A list of lists, with each list representing a possible path from `start` to `end`. Empty list if no path found. - For `\'shortest\'`: A list of nodes representing the shortest path from `start` to `end`. Empty list if no path found. # Constraints - The graph may contain cycles. - Nodes in the graph are unique strings. - Assume that `start` and `end` nodes are always present in the graph. # Examples ```python graph = { \\"A\\": [\\"B\\", \\"C\\"], \\"B\\": [\\"D\\", \\"E\\"], \\"C\\": [\\"F\\"], \\"D\\": [\\"C\\", \\"F\\"], \\"E\\": [\\"F\\"], \\"F\\": [] } find_paths_in_graph(graph, \\"A\\", \\"F\\", \\"single\\") # Possible output: [\'A\', \'B\', \'D\', \'F\'] find_paths_in_graph(graph, \\"A\\", \\"F\\", \\"all\\") # Possible output: [[\'A\', \'B\', \'D\', \'F\'], [\'A\', \'C\', \'F\'], [\'A\', \'B\', \'E\', \'F\'], etc.] find_paths_in_graph(graph, \\"A\\", \\"F\\", \\"shortest\\") # Possible output: [\'A\', \'C\', \'F\'] ``` Implement the above functionalities ensuring that solutions are optimal and considering edge cases.","solution":"def find_paths_in_graph(graph, start, end, mode): from collections import deque def find_single_path(): stack = [(start, [start])] while stack: (vertex, path) = stack.pop() for next in set(graph[vertex]) - set(path): if next == end: return path + [next] else: stack.append((next, path + [next])) return [] def find_all_paths(): def dfs(current, end, path, all_paths): path.append(current) if current == end: all_paths.append(path.copy()) else: for node in graph[current]: if node not in path: dfs(node, end, path, all_paths) path.pop() all_paths = [] dfs(start, end, [], all_paths) return all_paths def find_shortest_path(): queue = deque([(start, [start])]) visited = set([start]) while queue: (vertex, path) = queue.popleft() for next in graph[vertex]: if next == end: return path + [next] else: if next not in visited: visited.add(next) queue.append((next, path + [next])) return [] if mode == \'single\': return find_single_path() elif mode == \'all\': return find_all_paths() elif mode == \'shortest\': return find_shortest_path() else: raise ValueError(f\\"Unknown mode: {mode}\\")"},{"question":"# Fibonacci Sequence Calculation Background The Fibonacci sequence is a series where each number is the sum of the two preceding ones, typically starting with 0 and 1: [ F(0)=0, F(1)=1 ] [ F(n)=F(n-1)+F(n-2) ] You\'ve been tasked with calculating the nth Fibonacci number using an efficient approach. Problem Statement Write a function `fibonacci(n: int) -> int` that returns the (n)-th Fibonacci number. Input - An integer (n) where (0 leq n leq 10^6). Output - Return the (n)-th Fibonacci number as an integer. Constraints - The function should run in (O(n)) time complexity. - Use constant space (O(1)) for better performance. Examples 1. **Input**: 10 **Output**: 55 2. **Input**: 20 **Output**: 6765 Solution Tips - Given the constraint, using a recursive solution isn\'t feasible due to its exponential time complexity. - Opt for an iterative approach using constant space to solve the problem efficiently. Your Task Implement the function `fibonacci` as described.","solution":"def fibonacci(n: int) -> int: Returns the nth Fibonacci number. if n == 0: return 0 elif n == 1: return 1 a, b = 0, 1 for _ in range(2, n + 1): a, b = b, a + b return b"},{"question":"# Quick Sort: Edge Cases and Enhancements Context: Quick Sort is a highly efficient sorting algorithm, but it can suffer from poor performance with certain input types and has specific edge cases. Understanding these nuances is crucial for improving its implementation and making the algorithm more robust. Problem Statement: Your task is to implement a modified version of the Quick Sort algorithm that improves pivot selection and handles edge cases more effectively. Specifically: 1. Implement the Quick Sort algorithm, but use the \\"median-of-three\\" pivot selection strategy to enhance performance. 2. Ensure that the implementation handles the following edge cases: * Already sorted array (both increasing and decreasing). * Array with many duplicate values. Details: 1. **Function Signature:** ```python def enhanced_quick_sort(arr): pass ``` 2. **Input:** * `arr`: A list of integers to be sorted. (1 ≤ len(arr) ≤ 10^5, -10^9 ≤ arr[i] ≤ 10^9) 3. **Output:** * Returns the sorted list of integers in non-decreasing order. 4. **Constraints:** * The solution should have an average-case time complexity of O(n log n). * The implementation should be in-place with space complexity O(log n). 5. **Performance Requirements:** * The solution should be efficient for large datasets. Implementation Notes: * The \\"median-of-three\\" pivot selection involves picking the median value of the first, middle, and last elements of the array/sub-array. * Consider handling arrays with duplicate values efficiently to avoid performance degradation. Example: ```python # Example Input arr = [3, 6, 8, 10, 1, 2, 1] # Example Output print(enhanced_quick_sort(arr)) # Should print [1, 1, 2, 3, 6, 8, 10] # Example Input arr = [10, 9, 8, 7, 6, 5, 4, 3, 2, 1] # Example Output print(enhanced_quick_sort(arr)) # Should print [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] ``` * Be sure the function can handle edge cases effectively and demonstrate your testing.","solution":"def enhanced_quick_sort(arr): def median_of_three(low, high): mid = (low + high) // 2 if arr[low] > arr[mid]: arr[low], arr[mid] = arr[mid], arr[low] if arr[low] > arr[high]: arr[low], arr[high] = arr[high], arr[low] if arr[mid] > arr[high]: arr[mid], arr[high] = arr[high], arr[mid] return mid def partition(low, high): pivot_index = median_of_three(low, high) pivot_value = arr[pivot_index] arr[pivot_index], arr[high] = arr[high], arr[pivot_index] store_index = low for i in range(low, high): if arr[i] < pivot_value: arr[i], arr[store_index] = arr[store_index], arr[i] store_index += 1 arr[store_index], arr[high] = arr[high], arr[store_index] return store_index def quick_sort_recursive(low, high): if low < high: pivot_index = partition(low, high) quick_sort_recursive(low, pivot_index - 1) quick_sort_recursive(pivot_index + 1, high) quick_sort_recursive(0, len(arr) - 1) return arr"},{"question":"# Toposort Algorithm Implementation Challenge You are part of a software development team building a dependency manager for a package management system. An essential feature of this system is to determine the installation order of packages that have dependencies on other packages. This task can be accomplished by performing a topological sort of the dependency graph. Problem Statement Implement a function `find_install_order` that takes a directed graph representing package dependencies and returns a list of packages in a valid installation order. If the graph contains a cycle, the function should raise a `ValueError` indicating that the dependencies cannot be resolved due to a cyclic dependency. Input - `dependencies`: A dictionary where keys are package names (strings) and values are lists of package names that the key package depends on. Output - A list of package names in a valid installation order. Constraints - The graph is non-empty and well-formed. - There can be up to (10^4) packages and (10^5) dependency pairs. - Packages names are unique strings within the context of a single graph. Example ```python dependencies = { \\"packageA\\": [\\"packageB\\", \\"packageC\\"], \\"packageB\\": [\\"packageD\\"], \\"packageC\\": [\\"packageD\\"], \\"packageD\\": [] } # Expected output: [\\"packageD\\", \\"packageB\\", \\"packageC\\", \\"packageA\\"] or valid variations ``` Function Signature ```python def find_install_order(dependencies: Dict[str, List[str]]) -> List[str]: pass ``` Requirements - Implement the function using an algorithm based on the principles of topological sorting. - Ensure the solution handles potential cycles robustly by raising a ValueError. - Optimize performance to handle the upper limits of constraints.","solution":"def find_install_order(dependencies): Returns a list of package names in a valid installation order based on dependencies. Raises a ValueError if there is a cyclic dependency. from collections import defaultdict, deque # Initialize an in-degree dictionary to count dependencies for each package in_degree = defaultdict(int) # Initialize an adjacency list to keep track of which packages depend on a given package adj_list = defaultdict(list) # Build the graph from the dependencies dictionary for package, deps in dependencies.items(): for dep in deps: adj_list[dep].append(package) in_degree[package] += 1 # Queue for maintaining packages with 0 in-degree zero_in_degree_queue = deque([pkg for pkg in dependencies if in_degree[pkg] == 0]) # List for maintaining the installation order install_order = [] while zero_in_degree_queue: current_package = zero_in_degree_queue.popleft() install_order.append(current_package) # Reduce the in-degree for all packages that depend on the current package for dependent_package in adj_list[current_package]: in_degree[dependent_package] -= 1 if in_degree[dependent_package] == 0: zero_in_degree_queue.append(dependent_package) # If we have added all packages to the install order, return it if len(install_order) == len(dependencies): return install_order else: raise ValueError(\\"Cyclic dependency detected\\")"},{"question":"You are required to implement a function that computes the histogram of elements in a given list. A histogram is a representation of the distribution of numerical data, and it is an estimate of the probability distribution of a continuous variable. Your task is to count the frequency of each unique element in the input list and return this count as a dictionary. # Function Signature ```python def compute_histogram(input_list: list) -> dict: Compute the histogram of the input list. :param input_list: List of elements where elements can be any hashable type. :return: Dictionary with elements as keys and their counts as values. ``` # Input * `input_list` (list): A list of elements. Elements can be any hashable type, and the list can contain repeated values. # Output * Returns a dictionary where the keys are the unique elements from the input list, and the values are their corresponding counts. # Constraints * Do not use any libraries or built-in functions that provide this functionality directly. * The input list length can be as large as 10^6. * The elements in the list are limited by the memory constraints of typical systems. # Example Example 1 ```python input_list = [3, 3, 2, 1] output = compute_histogram(input_list) # Expected output: {1: 1, 2: 1, 3: 2} ``` Example 2 ```python input_list = [2, 3, 5, 5, 5, 6, 4, 3, 7] output = compute_histogram(input_list) # Expected output: {2: 1, 3: 2, 4: 1, 5: 3, 6: 1, 7: 1} ``` # Testing You must handle edge cases such as: * An empty list. * Lists where all elements are the same. * Lists with mixed data types. Make sure your implementation is both time and space efficient.","solution":"def compute_histogram(input_list: list) -> dict: Compute the histogram of the input list. :param input_list: List of elements where elements can be any hashable type. :return: Dictionary with elements as keys and their counts as values. histogram = {} for element in input_list: if element in histogram: histogram[element] += 1 else: histogram[element] = 1 return histogram"},{"question":"**Question**: Implement a binary search function that searches for a target value in a sorted array. However, unlike the standard binary search, this function should return the index of the first occurrence of the target value if it appears more than once in the array. If the target value is not found, the function should return -1. # Function Signature ```python def first_occurrence_binary_search(arr: List[int], target: int) -> int: pass ``` # Input * `arr` - A list of integers sorted in ascending order. * `target` - An integer value to search for in `arr`. # Output * Return the index of the first occurrence of `target` in `arr`. If `target` is not found, return -1. # Constraints * `1 <= len(arr) <= 10^5` * `-10^9 <= arr[i], target <= 10^9` # Example * Input: `arr = [1, 2, 2, 2, 3, 4, 5]`, `target = 2` * Output: `1` * Input: `arr = [1, 2, 2, 2, 3, 4, 5]`, `target = 6` * Output: `-1` # Notes * You should aim for a solution that operates in O(log n) time.","solution":"def first_occurrence_binary_search(arr, target): low, high = 0, len(arr) - 1 result = -1 while low <= high: mid = (low + high) // 2 if arr[mid] == target: result = mid high = mid - 1 # Move left to find the first occurrence elif arr[mid] < target: low = mid + 1 else: high = mid - 1 return result"},{"question":"# ZigZag Iterator for k Lists **Problem Statement**: You are given k lists of integers. Implement a `ZigZagIteratorK` class which returns elements from these k lists in a zigzag fashion. That is, it should alternate elements from each of the k lists until no elements are left. Implement the following methods: 1. `__init__(self, lists: List[List[int]]) -> None`: Initializes the iterator with a list of k integer lists. 2. `next(self) -> int`: Returns the next integer in the zigzag order. 3. `has_next(self) -> bool`: Returns `True` if there are still elements available in at least one of the lists, else `False`. **Input Format**: - `lists`: A list of k lists, where each list contains `n_i` integers (0 <= `n_i` <= 100). **Output Format**: - `next()`: Returns the next integer in the zigzag order. - `has_next()`: Returns a boolean indicating availability of elements. **Constraints**: - You may assume each integer in the lists fits into a 32-bit signed integer. - The k lists combined will contain at most 10^4 integers. **Example**: ```python l1 = [1, 2] l2 = [3, 4, 5, 6] l3 = [7, 8, 9] it = ZigZagIteratorK([l1, l2, l3]) result = [] while it.has_next(): result.append(it.next()) print(result) # Output: [1, 3, 7, 2, 4, 8, 5, 9, 6] ``` **Note**: - Ensure your implementation handles lists of varying lengths appropriately. - Be mindful of the time and space complexity constraints.","solution":"from collections import deque class ZigZagIteratorK: def __init__(self, lists): Initialize with a list of k lists. self.queue = deque() for lst in lists: if lst: self.queue.append(deque(lst)) def next(self): Return the next integer in the zigzag order. if not self.has_next(): raise Exception(\\"No more elements\\") current_list = self.queue.popleft() value = current_list.popleft() if current_list: self.queue.append(current_list) return value def has_next(self) -> bool: Returns True if there are still elements available in at least one of the lists, else False. return bool(self.queue)"},{"question":"# Question: Implement and Optimize Separate Chaining HashTable You are provided with a basic implementation of a hash table using separate chaining to handle collisions. Your task is to enhance this implementation with the following features: 1. **Dynamic Resizing**: Implement functionality to resize the hash table dynamically to maintain optimal performance. Ensure the hash table resizes itself when the load factor exceeds a given threshold (e.g., 0.75). 2. **Iterator Support**: Implement an iterator for the `SeparateChainingHashTable` to allow iterating through all key-value pairs stored in the table. # Implementation Details 1. **Dynamic Resizing**: - Add a method `resize(new_size)` which resizes the underlying hash table array to `new_size` and rehashes all existing key-value pairs. - Modify the `put` method to check the load factor and call `resize` when necessary. 2. **Iterator Support**: - Implement `__iter__` and `__next__` methods to allow iteration through the hash table. # Constraints - The keys are strings and the values can be any type. - Implement and test solutions using Python 3. # Function Signature ```python class SeparateChainingHashTable: ... # Existing methods def resize(self, new_size: int) -> None: :param new_size: The new size of the hash table. :return: None pass def __iter__(self): Initialize iterator state. pass def __next__(self): Iterate through the hash table and return the next key-value pair. pass ``` # Example Usage ```python >>> table = SeparateChainingHashTable(size=5) >>> table.put(\'apple\', 5) >>> table.put(\'banana\', 3) >>> table.put(\'melon\', 8) >>> len(table) # Expected: 3 3 >>> table.get(\'banana\') # Expected: 3 3 >>> for key, value in table: ... print(key, value) ... apple 5 banana 3 melon 8 # Handling Resize >>> table.resize(10) >>> len(table) # Expected: 3 (no change in count) 3 >>> table.get(\'banana\') # Expected: 3 (data remains consistent post-resize) 3 ``` Ensure to also provide tests to demonstrate your code handles different cases, including resizing and edge cases.","solution":"class SeparateChainingHashTable: def __init__(self, size=5): self.size = size self.table = [[] for _ in range(size)] self.count = 0 def _hash(self, key): return hash(key) % self.size def put(self, key, value): index = self._hash(key) for pair in self.table[index]: if pair[0] == key: pair[1] = value return self.table[index].append([key, value]) self.count += 1 if self.load_factor() > 0.75: self.resize(self.size * 2) def get(self, key): index = self._hash(key) for pair in self.table[index]: if pair[0] == key: return pair[1] return None def remove(self, key): index = self._hash(key) for pair in self.table[index]: if pair[0] == key: self.table[index].remove(pair) self.count -= 1 return True return False def load_factor(self): return self.count / self.size def resize(self, new_size): old_table = self.table self.size = new_size self.table = [[] for _ in range(new_size)] self.count = 0 for bucket in old_table: for key, value in bucket: self.put(key, value) def __len__(self): return self.count def __iter__(self): self.current_index = 0 self.inner_index = 0 return self def __next__(self): while self.current_index < len(self.table): if self.inner_index < len(self.table[self.current_index]): pair = self.table[self.current_index][self.inner_index] self.inner_index += 1 return pair else: self.inner_index = 0 self.current_index += 1 raise StopIteration"},{"question":"# Palindrome Check Function with Data Preprocessing You are given a string that may contain alphanumeric characters as well as punctuation and spaces. Your task is to determine if the string is a palindrome, considering only the alphanumeric characters and ignoring cases. Function Signature ``` def check_palindrome(s: str) -> bool: pass ``` Input - `s` (string): A string containing printable ASCII characters. Output - Returns `True` if the input string is a palindrome, considering only alphanumeric characters and ignoring cases. - Returns `False` otherwise. Constraints - The length of `s` will not exceed `10^6`. - Conversion of the string to lowercase and filtering out non-alphanumeric characters should be handled efficiently. Example 1. `check_palindrome(\\"A man, a plan, a canal: Panama\\")` should return `True`. 2. `check_palindrome(\\"race a car\\")` should return `False`. 3. `check_palindrome(\\"\\")` should return `True`. # Approach - Normalize the string by removing non-alphanumeric characters and converting it to lowercase. - Use the two-pointer technique to check if the string is a palindrome.","solution":"def check_palindrome(s: str) -> bool: Check if the given string is a palindrome considering only alphanumeric characters and ignoring cases. Args: s (str): The input string. Returns: bool: True if the processed string is a palindrome, False otherwise. # Filter out non-alphanumeric characters and convert to lowercase filtered_chars = [char.lower() for char in s if char.isalnum()] # Check if the filtered string is equal to its reverse return filtered_chars == filtered_chars[::-1]"},{"question":"**Problem Statement:** You are given a list of integers. Your task is to implement the Cocktail Shaker Sort algorithm to sort this list in ascending order. You must do so by performing the sorting in-place and ensuring the list is processed from both directions on each pass. **Function Signature:** `def cocktail_shaker_sort(arr: List[int]) -> List[int]:` # Input: - An unordered list of integers (with a length `n` such that 1 ≤ n ≤ 10^4). # Output: - A list of integers sorted in ascending order. # Constraints: - Your implementation should have an average time complexity of O(n^2) and a space complexity of O(1). - Efficient handling of edge cases such as an already sorted list, an empty list, and a list with all identical elements is required. - You should ensure that the algorithm is implemented correctly without causing index out of range errors. # Example: ```python assert cocktail_shaker_sort([3, 2, 5, 1, 4]) == [1, 2, 3, 4, 5] assert cocktail_shaker_sort([10, 4, 6, 15, 2]) == [2, 4, 6, 10, 15] assert cocktail_shaker_sort([]) == [] assert cocktail_shaker_sort([2, 2, 2, 2, 2]) == [2, 2, 2, 2, 2] assert cocktail_shaker_sort([5, -1, 4, 3, 0]) == [-1, 0, 3, 4, 5] ``` Make sure to test your function with various edge cases and ensure it handles all the given constraints correctly.","solution":"from typing import List def cocktail_shaker_sort(arr: List[int]) -> List[int]: n = len(arr) if n <= 1: return arr is_sorted = False start = 0 end = n - 1 while not is_sorted: is_sorted = True # Forward pass for i in range(start, end): if arr[i] > arr[i + 1]: arr[i], arr[i + 1] = arr[i + 1], arr[i] is_sorted = False if is_sorted: break is_sorted = True end -= 1 # Backward pass for i in range(end, start, -1): if arr[i] < arr[i - 1]: arr[i], arr[i - 1] = arr[i - 1], arr[i] is_sorted = False start += 1 return arr"},{"question":"# Problem: Refactoring Prim’s Algorithm You are provided with a partial implementation of Prim\'s Algorithm. Your task is to complete and optimize this implementation, ensuring it can handle more complex scenarios efficiently. Specifically, ensure that the algorithm correctly identifies and deals with edge cases, optimizes heap operations, and correctly handles different types of graph inputs. Instructions: 1. **Input**: A dictionary `graph` where each key represents a node and each value is a list of tuples representing the adjacent nodes and the edge weights. 2. **Output**: An integer representing the total weight of the minimum spanning tree. Implementation Requirements: 1. **Function Name**: `optimized_prims_mst` 2. **Parameters**: - `graph`: A dictionary representing the graph. 3. **Returns**: An integer representing the weight of the minimum spanning tree. 4. **Constraints**: Assume the graph is connected and undirected, and has at least one node. 5. **Performance Requirement**: Aim to minimize the time complexity to ensure efficiency for dense graphs. Code Skeleton: ```python def optimized_prims_mst(graph): # Your optimized implementation goes here # Ensure handling of different graph structures and edge cases. return 0 # Replace with the appropriate return value based on your implementation # Sample graph for testing graph = { \'a\': [[3, \'b\'], [8, \'c\']], \'b\': [[3, \'a\'], [5, \'d\']], \'c\': [[8, \'a\'], [2, \'d\'], [4, \'e\']], \'d\': [[5, \'b\'], [2, \'c\'], [6, \'e\']], \'e\': [[4, \'c\'], [6, \'d\']] } # Expected output: 17 print(optimized_prims_mst(graph)) ``` Evaluation Criteria: * Correctness: Ensure the function produces correct results. * Efficiency: The implementation should be optimized for both time and space. * Handling Edge Cases: Correctly manage edge cases like disconnected graphs and negative weights. * Code Quality: Maintain readability, proper commenting, and use of meaningful variable names.","solution":"import heapq from collections import defaultdict def optimized_prims_mst(graph): # Check if graph is empty if not graph: return 0 # Initializations start_node = next(iter(graph)) min_heap = [(0, start_node)] # (weight, node) visited = set() total_weight = 0 while min_heap: weight, node = heapq.heappop(min_heap) if node in visited: continue # Add the node to the MST visited.add(node) total_weight += weight # Add all edges from this node to the heap for edge_weight, adjacent_node in graph[node]: if adjacent_node not in visited: heapq.heappush(min_heap, (edge_weight, adjacent_node)) return total_weight # Sample graph for testing graph = { \'a\': [[3, \'b\'], [8, \'c\']], \'b\': [[3, \'a\'], [5, \'d\']], \'c\': [[8, \'a\'], [2, \'d\'], [4, \'e\']], \'d\': [[5, \'b\'], [2, \'c\'], [6, \'e\']], \'e\': [[4, \'c\'], [6, \'d\']] } print(optimized_prims_mst(graph)) # Expected output: 14"},{"question":"Objective: Design an encoding and decoding algorithm for a list of strings to a single string and vice versa. Your implementation should aim to handle edge cases and ensure performance optimization. Instructions: 1. Implement two functions `encode` and `decode`: - `encode`: This function should take a list of strings and return a single encoded string. - `decode`: This function should take an encoded string and return the original list of strings. 2. Your implementation should be robust against edge cases and ensure the integrity of the original data. 3. Performance Considerations: - Aim to keep the time complexity of both `encode` and `decode` functions to O(n), where n is the total number of characters across all strings. - Minimize the use of additional space where possible. Function Signatures: ```python def encode(strs: List[str]) -> str: pass def decode(s: str) -> List[str]: pass ``` Example: ```python input_strings = [\\"hello\\", \\"world\\"] encoded_string = encode(input_strings) # Possible output: \\"5:hello5:world\\" decoded_list = decode(encoded_string) # Possible output: [\\"hello\\", \\"world\\"] # Ensure that the decoded list matches the original input list. assert decoded_list == input_strings ``` Constraints: - The input list consists of strings with ASCII characters. - Each string\'s length will not exceed 1000 characters. - The total number of characters in the list will not exceed 1000000. Additional Requirements: - Handle edge cases such as empty input lists and strings containing special characters. - Include error handling for potential invalid encoded strings. - Test your functions thoroughly to ensure correctness.","solution":"def encode(strs): Encodes a list of strings to a single string. Parameters: strs (List[str]): List of strings to encode. Returns: str: Encoded single string. encoded_string = \'\' for s in strs: encoded_string += f\'{len(s)}:{s}\' return encoded_string def decode(s): Decodes a single string to a list of strings. Parameters: s (str): Encoded string. Returns: List[str]: The original list of strings. decoded_list = [] i = 0 while i < len(s): # Find the separator \':\' to get the length of the string j = s.find(\':\', i) if j == -1: # Error handling for invalid encoding raise ValueError(\\"Invalid encoded string\\") # Length of the encoded string length = int(s[i:j]) # Extract the string using the length decoded_list.append(s[j+1:j+1+length]) # Move the index to the start of the next encoded string i = j + 1 + length return decoded_list"},{"question":"Problem Description You are given a universe of elements ( U ) and a collection of subsets that together cover all elements of ( U ). Each subset has an associated cost. Your task is to write a function that finds a subcollection of subsets whose union is ( U ), such that the total cost is minimized. You should implement both an exhaustive search (optimal) solution and a greedy approximate solution. # Input: - `universe` (set): A set of unique elements representing the universe. - `subsets` (dict): A dictionary where keys are subset identifiers (strings) and values are sets of elements. - `costs` (dict): A dictionary where keys are subset identifiers (strings) and values are integers representing the costs. # Output: - A tuple containing the result of the optimal set cover and the greedy set cover as two separate lists of subset identifiers. # Constraints: - All elements in subsets are contained within the universe. - Each subset has a positive cost. # Performance Requirements: - The optimal solution must not be used on large inputs due to its exponential time complexity. - The greedy solution should be efficient enough to handle larger inputs reasonably. # Example: ```python universe = {1, 2, 3, 4, 5} subsets = {\'S1\': {4, 1, 3}, \'S2\': {2, 5}, \'S3\': {1, 4, 3, 2}} costs = {\'S1\': 5, \'S2\': 10, \'S3\': 3} result = set_cover(universe, subsets, costs) print(result) # Output should be a tuple of ([\'S2\', \'S3\'], [\'S3\', \'S2\']) ``` # Function Signature: ```python def set_cover( universe: set, subsets: dict[str, set], costs: dict[str, int] ) -> (list[str], list[str]): ```","solution":"from itertools import combinations def optimal_set_cover(universe, subsets, costs): subset_list = list(subsets.keys()) N = len(subset_list) optimal_solution = None optimal_cost = float(\'inf\') for r in range(1, N + 1): for combo in combinations(subset_list, r): union_set = set().union(*(subsets[sub] for sub in combo)) if union_set == universe: combo_cost = sum(costs[sub] for sub in combo) if combo_cost < optimal_cost: optimal_cost = combo_cost optimal_solution = list(combo) return optimal_solution def greedy_set_cover(universe, subsets, costs): covered = set() selected_subsets = [] while covered != universe: best_subset = None best_cost_effectiveness = float(\'inf\') for subset, elements in subsets.items(): new_elements = elements - covered if len(new_elements) > 0: cost_effectiveness = costs[subset] / len(new_elements) if cost_effectiveness < best_cost_effectiveness: best_cost_effectiveness = cost_effectiveness best_subset = subset covered.update(subsets[best_subset]) selected_subsets.append(best_subset) return selected_subsets def set_cover(universe, subsets, costs): return optimal_set_cover(universe, subsets, costs), greedy_set_cover(universe, subsets, costs) # Example usage: universe = {1, 2, 3, 4, 5} subsets = {\'S1\': {4, 1, 3}, \'S2\': {2, 5}, \'S3\': {1, 4, 3, 2}} costs = {\'S1\': 5, \'S2\': 10, \'S3\': 3} result = set_cover(universe, subsets, costs)"},{"question":"Minimum Cost Path in a Graph Context: You are given a weighted directed graph represented as an `N x N` matrix where `Matrix[i][j]` denotes the cost of moving from station `i` to station `j` and `i < j`. The entries where `i > j` are irrelevant and denoted by `-1` or `INF`. Your task is to find the minimum cost to travel from the first station (station 0) to the last station (station N-1). # Requirements - **Input**: - A 2D list `Matrix` of size `N x N` where `Matrix[i][j]` is the cost to move from station `i` to station `j`. - Constraint: Only values of i<j are valid for cost, other values should be treated as infinity or irrelevant. - **Output**: - An integer representing the minimum cost to reach the last station (`N-1`) from the first station (`0`). # Examples Example 1 Input: ```python [ [ 0, 15, 80, 90], [-1, 0, 40, 50], [-1, -1, 0, 70], [-1, -1, -1, 0] ] ``` Output: ```python 65 ``` Explanation: The minimum cost path is 0 -> 1 -> 3 with a total cost of 65 (i.e., 15 + 50). Example 2 Input: ```python [ [ 0, 10, 75, 94], [-1, 0, 35, 50], [-1, -1, 0, 80], [-1, -1, -1, 0] ] ``` Output: ```python 60 ``` Explanation: The minimum cost path is 0 -> 1 -> 3 with a total cost of 60 (i.e., 10 + 50). Constraints - 2 ≤ N ≤ 100 - 0 ≤ Cost ≤ 10^3 for valid `Matrix[i][j]` where `i < j` Implementation Implement the function `min_cost`: ```python def min_cost(cost: List[List[int]]) -> int: # Write your code here ``` Utilize the provided structure and constraints to produce the desired solution efficiently, considering edge cases and potential optimizations.","solution":"def min_cost(cost: list[list[int]]) -> int: N = len(cost) dp = [float(\'inf\')] * N dp[0] = 0 for i in range(N): for j in range(i + 1, N): if cost[i][j] != -1: dp[j] = min(dp[j], dp[i] + cost[i][j]) return dp[N - 1]"},{"question":"You are given three integer arrays `A`, `B`, and `C` along with an integer `target`. Your task is to find all unique combinations where each combination consists of one element from `A`, one element from `B`, and one element from `C` that sum up to the `target`. The result should be a list of lists where each inner list represents a valid combination of three elements meeting the criteria. # Input * `A` - List of integers with length n (1 ≤ n ≤ 20) * `B` - List of integers with length n (1 ≤ n ≤ 20) * `C` - List of integers with length n (1 ≤ n ≤ 20) * `target` - A single integer # Output * A list of lists where each inner list contains three integers from `A`, `B`, and `C` such that their sum is equal to `target`. # Constraints * The elements in `A`, `B`, and `C` can have duplicates. * The sum of the three chosen elements must exactly match `target`. # Performance Requirements * The solution should be optimal with respect to both time and space complexity wherever possible, without overly complicating the logic. # Example ```python A = [1, 2, 3, 3] B = [2, 3, 3, 4] C = [2, 3, 3, 4] target = 7 Expected Output: [ [1, 2, 4], [1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 3, 3], [1, 4, 2], [2, 2, 3], [2, 2, 3], [2, 3, 2], [2, 3, 2], [3, 2, 2], [3, 2, 2] ] ``` # Note * The order of elements in each combination is derived from the order they appear in the input arrays `A`, `B`, and `C`. * Combinations should be listed in any order. ```python def find_combinations(A, B, C, target): # Your code here # Example usage: A = [1, 2, 3, 3] B = [2, 3, 3, 4] C = [2, 3, 3, 4] target = 7 print(find_combinations(A, B, C, target)) ```","solution":"def find_combinations(A, B, C, target): Finds all unique combinations of one element from each of A, B, and C that sum up to the target. result = [] for a in A: for b in B: for c in C: if a + b + c == target: result.append([a, b, c]) return result"},{"question":"# Subset Generation for Distinct Integers Given a set of distinct integers, `nums`, write a Python function to return all possible subsets (the power set). The solution set must not contain duplicate subsets. Input - A list of distinct integers `nums`, where (0 leq len(nums) leq 20). Output - A list of lists, where each list represents a unique subset of `nums`. Example ```python nums = [1, 2, 3] ``` Expected output: ```python [ [], [1], [2], [3], [1, 2], [1, 3], [2, 3], [1, 2, 3] ] ``` # Constraints: - Each element of `nums` is unique. - The order of subsets and the order of their elements inside each subset does not matter. # Task Requirements: - Implement a function `generate_subsets(nums: List[int]) -> List[List[int]]` that generates all possible subsets. - Your solution must efficiently handle all input sizes up to the maximum constraints. - Consider and handle edge cases, such as empty input array.","solution":"from typing import List def generate_subsets(nums: List[int]) -> List[List[int]]: def backtrack(start, path): result.append(path) for i in range(start, len(nums)): backtrack(i + 1, path + [nums[i]]) result = [] backtrack(0, []) return result"},{"question":"# Question: Clone an Undirected Graph - Recursive DFS Implementation Given an undirected graph, implement a function to clone the graph using a recursive depth-first search (DFS) algorithm. Each node in the graph is represented by an instance of the `UndirectedGraphNode` class, which contains a `label` and a list of its neighbors. Class Definition: ```python class UndirectedGraphNode: def __init__(self, label): self.label = label self.neighbors = [] ``` # Function Signature: ```python def clone_graph(node): pass ``` # Expected Input/Output: * The function receives a node: `node` which is an instance of `UndirectedGraphNode` representing the entry point of the undirected graph. * Returns a new graph starting from the copied `node`. # Constraints: 1. The node labels are unique integers. 2. Nodes can have self-cycles. 3. If `node` is `None`, return `None`. # Example: ```python # Example usage node0 = UndirectedGraphNode(0) node1 = UndirectedGraphNode(1) node2 = UndirectedGraphNode(2) node0.neighbors = [node1, node2] node1.neighbors = [node2] node2.neighbors = [node2] cloned_graph = clone_graph(node0) print(cloned_graph.label) # Output: 0 print(cloned_graph.neighbors[0].label) # Output: 1 print(cloned_graph.neighbors[1].label) # Output: 2 ``` Explanation: In the example above: - The function successfully clones the graph starting from `node0` using a recursive DFS approach. - Each original node is mapped to its clone and edges (neighbors) are preserved.","solution":"class UndirectedGraphNode: def __init__(self, label): self.label = label self.neighbors = [] def clone_graph(node): def dfs(node, visited): if node in visited: return visited[node] clone = UndirectedGraphNode(node.label) visited[node] = clone for neighbor in node.neighbors: clone.neighbors.append(dfs(neighbor, visited)) return clone if node is None: return None return dfs(node, {})"},{"question":"You are given a linked list, which may contain duplicate values. Your task is to write a function that removes all duplicates from this list without using any extra data structures (i.e., with constant extra space). # Function Signature ```python def remove_dups_without_set(head: Node) -> Node: ``` # Input * `head`: The head node of a singly linked list of integers, where `Node` is defined as: ```python class Node: def __init__(self, val: int, next: \'Node\' = None): self.val = val self.next = next ``` # Output * Return the head node of the modified list with duplicates removed. # Constraints * The number of nodes in the linked list will be in the range [1, 10^4]. * The linked list will have integer values. * The algorithm must run in (O(N^2)) time complexity and (O(1)) space complexity. # Example ```python Given the linked list: 1 -> 3 -> 2 -> 1 -> 4 -> 3 -> 2 After removing duplicates: 1 -> 3 -> 2 -> 4 ``` # Detailed Explanation * You are required to traverse the list with a primary pointer (`current`) and use a secondary pointer (`runner`) to check for duplicates. * If `runner` encounters a node with the same value as `current`, it should skip that node by adjusting the `next` pointer. * Continue this process until all duplicates are removed.","solution":"class Node: def __init__(self, val: int, next: \'Node\' = None): self.val = val self.next = next def remove_dups_without_set(head: Node) -> Node: if not head: return head current = head while current: runner = current while runner.next: if runner.next.val == current.val: runner.next = runner.next.next else: runner = runner.next current = current.next return head"},{"question":"# Prime Number Validator Function **Objective**: Write a function to validate if a number is prime, taking efficiency into account. **Task**: Implement the function `is_prime(n: int) -> bool` that determines whether a given number ( n ) is a prime number. **Specifications**: - The function should return `True` if ( n ) is prime, otherwise it should return `False`. - Optimize the function to run efficiently even for large values of ( n ). **Input**: - A single integer ( n ) where ( 0 leq n leq 10^9 ). **Output**: - A boolean value: `True` if ( n ) is prime, and `False` otherwise. **Constraints**: - Your solution should be optimized to handle large values up to ( 10^9 ) effectively. **Example**: ```python # Example 1: print(is_prime(29)) # Expected output: True # Example 2: print(is_prime(100)) # Expected output: False # Example 3: print(is_prime(2)) # Expected output: True # Example 4: print(is_prime(1)) # Expected output: False ``` **Performance Requirements**: - Time Complexity: ( O(sqrt{n}) ) - Space Complexity: ( O(1) ) **Hints**: - Consider the characteristics of prime numbers to avoid unnecessary computations. - Think about how to skip even numbers and known composites efficiently. **Edge Cases to Consider**: - smallest numbers like 0, 1, 2. - large prime and composite numbers especially large composite numbers that aren\'t divisible until near their square root.","solution":"def is_prime(n: int) -> bool: Determines whether the given number n is a prime number. Args: n (int): The input number. Returns: bool: True if n is a prime number, False otherwise. if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True"},{"question":"# Stooge Sort Analysis and Implementation **Problem Statement**: As a demonstration of inefficient sorting algorithms, you are required to implement Stooge Sort in a different programming language, analyze its performance based on a few test cases, and identify optimal scenarios for its use. Given an array of integers, sort the array using Stooge Sort. **Function Signature**: ```python def stooge_sort(arr: List[int], l: int, h: int) -> List[int]: pass ``` **Input**: * A list of integers `arr` where 1 <= len(arr) <= 100. * Two indices `l` and `h` representing the start and end indices of the array portion to sort, respectively. **Output**: * Return the sorted list of integers. **Constraints**: * You must use the Stooge Sort algorithm described. * The function should work correctly for arrays with negative numbers and zero. **Example**: ```python # Example 1 input_arr = [5, 4, 3, 2, 1] sorted_arr = stooge_sort(input_arr, 0, len(input_arr) - 1) print(sorted_arr) # Output: [1, 2, 3, 4, 5] # Example 2 input_arr = [5, -1, 11, 0, 7] sorted_arr = stooge_sort(input_arr, 0, len(input_arr) - 1) print(sorted_arr) # Output: [-1, 0, 5, 7, 11] ``` **Performance Requirements**: * Assume a reasonably small input size due to the inefficiency of Stooge Sort. However, ensure correct results for any valid input array provided within the constraint. **Scenario**: Consider the scenario where you are comparing different inefficient sorting algorithms for a report. You need to provide implemented code (in another programming language) and an analysis of Stooge Sort results compared to other inefficient sorts like Bubble Sort.","solution":"def stooge_sort(arr, l, h): Sorts the array using Stooge Sort algorithm. Parameters: arr (List[int]): The list of integers to be sorted l (int): The starting index h (int): The ending index Returns: List[int]: The sorted list of integers if l >= h: return arr # If the first element is greater than the last, swap them if arr[l] > arr[h]: arr[l], arr[h] = arr[h], arr[l] # If there are more than 2 elements in the array if h - l + 1 > 2: t = (h - l + 1) // 3 stooge_sort(arr, l, h - t) stooge_sort(arr, l + t, h) stooge_sort(arr, l, h - t) return arr"},{"question":"# Binary Search with Validation Implement a function `validated_binary_search` that performs a binary search on a sorted array with additional validation steps: 1. The input array must be non-empty. 2. The input must be sorted in ascending order. If these conditions are not satisfied, the function should raise a `ValueError` with a descriptive message. The function will take two inputs: - A sorted list of integers. - An integer value to search for. The function should return the index of the found value, or `-1` if the value is not present in the array. # Function Signature ```python def validated_binary_search(array: List[int], query: int) -> int: pass ``` # Input - `array`: A list of sorted integers. - `query`: An integer value to search for. # Output - An integer representing the index of `query` in the array, or `-1` if `query` is not found. # Constraints - `array` contains at most `10^6` integers. - All integers in `array` are within the range of `-10^9` to `10^9`. - The function should efficiently handle the constraints and should not cause a stack overflow. # Example ```python try: result = validated_binary_search([1, 2, 3, 4, 5], 3) print(result) # Output: 2 except ValueError as e: print(e) try: result = validated_binary_search([], 3) print(result) # Output: ValueError: \\"Array must be non-empty\\" except ValueError as e: print(e) try: result = validated_binary_search([1, 3, 2, 4, 5], 3) print(result) # Output: ValueError: \\"Array must be sorted in ascending order\\" except ValueError as e: print(e) ``` # Notes - Ensure your implementation explicitly checks for an empty array and whether the array is sorted. - Use an iterative approach for the actual binary search to avoid stack overflow issues with large arrays.","solution":"from typing import List def validated_binary_search(array: List[int], query: int) -> int: Perform a binary search on a sorted array with validations. Parameters: array (List[int]): A list of sorted integers (ascending order). query (int): An integer value to search for. Returns: int: The index of the found value, or -1 if the value is not present. # Validation for non-empty array if not array: raise ValueError(\\"Array must be non-empty\\") # Validation for sorted array in ascending order if any(array[i] > array[i + 1] for i in range(len(array) - 1)): raise ValueError(\\"Array must be sorted in ascending order\\") # Binary search implementation left, right = 0, len(array) - 1 while left <= right: mid = left + (right - left) // 2 if array[mid] == query: return mid elif array[mid] < query: left = mid + 1 else: right = mid - 1 return -1"},{"question":"# Doubly Linked List Manipulation You are tasked with implementing a subset of methods for a custom Doubly Linked List (DLL) in Python. This exercise evaluates your understanding of list traversal, insertion, deletion, and edge case handling. Define a class `DoublyLinkedList` with the following methods: 1. **add_to_head(value)** - Adds a new node with the given value to the beginning (head) of the list. - **Input**: `value` (an integer). - **Output**: None 2. **add_to_tail(value)** - Adds a new node with the given value to the end (tail) of the list. - **Input**: `value` (an integer). - **Output**: None 3. **remove_from_head()** - Removes the node at the beginning (head) of the list and returns its value. - **Output**: The value of the removed node as an integer. Return `None` if the list is empty. 4. **remove_from_tail()** - Removes the node at the end (tail) of the list and returns its value. - **Output**: The value of the removed node as an integer. Return `None` if the list is empty. 5. **display_forward()** - Displays the list elements from head to tail. - **Output**: A list of integers representing the values from head to tail. 6. **display_backward()** - Displays the list elements from tail to head. - **Output**: A list of integers representing the values from tail to head. **Constraints**: - Values will be integers. - There will be at most 1000 operations performed. - The initial list is empty, and operations will be conducted according to the methods above. For instance, the sequence of operations: ```python dll = DoublyLinkedList() dll.add_to_head(5) dll.add_to_tail(10) dll.add_to_head(3) dll.remove_from_tail() dll.display_forward() dll.display_backward() ``` should yield: ```python [3, 5] [5, 3] ``` Implement the `DoublyLinkedList` class along with the specified methods.","solution":"class Node: def __init__(self, value): self.value = value self.next = None self.prev = None class DoublyLinkedList: def __init__(self): self.head = None self.tail = None def add_to_head(self, value): new_node = Node(value) if self.head is None: self.head = self.tail = new_node else: new_node.next = self.head self.head.prev = new_node self.head = new_node def add_to_tail(self, value): new_node = Node(value) if self.tail is None: self.head = self.tail = new_node else: new_node.prev = self.tail self.tail.next = new_node self.tail = new_node def remove_from_head(self): if self.head is None: return None value = self.head.value if self.head == self.tail: self.head = self.tail = None else: self.head = self.head.next self.head.prev = None return value def remove_from_tail(self): if self.tail is None: return None value = self.tail.value if self.head == self.tail: self.head = self.tail = None else: self.tail = self.tail.prev self.tail.next = None return value def display_forward(self): elements = [] current = self.head while current: elements.append(current.value) current = current.next return elements def display_backward(self): elements = [] current = self.tail while current: elements.append(current.value) current = current.prev return elements"},{"question":"Problem Description You are given two non-negative integers, `a` and `b`. Your task is to write a function that calculates their greatest common divisor (GCD), least common multiple (LCM), and the number of trailing zeros in each integer. Requirements: 1. **Function 1: Greatest Common Divisor** - **Function Name**: `compute_gcd` - **Input**: Two integer parameters `a` and `b`. - **Output**: An integer representing the greatest common divisor of `a` and `b`. - **Constraints**: `0 <= a, b <= 10^9` 2. **Function 2: Least Common Multiple** - **Function Name**: `compute_lcm` - **Input**: Two integer parameters `a` and `b`. - **Output**: An integer representing the least common multiple of `a` and `b`. - **Constraints**: `0 <= a, b <= 10^9` 3. **Function 3: Trailing Zeros** - **Function Name**: `count_trailing_zeros` - **Input**: One integer parameter `x`. - **Output**: An integer representing the number of trailing zeros in the binary representation of `x`. - **Constraints**: `0 <= x <= 10^9` Your task is to implement the three functions as specified. Example: ```python # Function 1: compute_gcd assert compute_gcd(48, 18) == 6 assert compute_gcd(0, 10) == 10 assert compute_gcd(25, 0) == 25 # Function 2: compute_lcm assert compute_lcm(4, 5) == 20 assert compute_lcm(4, 0) == 0 assert compute_lcm(0, 0) == 0 # Function 3: count_trailing_zeros assert count_trailing_zeros(8) == 3 assert count_trailing_zeros(40) == 3 assert count_trailing_zeros(0) == 0 ```","solution":"def compute_gcd(a, b): Returns the greatest common divisor (GCD) of a and b using Euclidean algorithm. while b: a, b = b, a % b return a def compute_lcm(a, b): Returns the least common multiple (LCM) of a and b. LCM is computed using the formula: LCM(a, b) = abs(a*b) // GCD(a, b) if a == 0 or b == 0: return 0 return abs(a * b) // compute_gcd(a, b) def count_trailing_zeros(x): Returns the number of trailing zeros in the binary representation of x. This is equivalent to the number of times 2 divides x. if x == 0: return 0 count = 0 while (x & 1) == 0: x >>= 1 count += 1 return count"},{"question":"# Question: Advanced Two Sum - Special Cases Handling Given an array of integers sorted in ascending order, find two indices of numbers such that they add up to a specific target number. The function `advanced_two_sum` should ensure that no element is used more than once, and it should handle edge cases gracefully (such as repeated elements, large numbers, etc.). Ensure your implementation is efficient in terms of both time and space. Implement the function `advanced_two_sum` that: * Inputs: * `numbers`: A list of sorted integers (ascending order). * `target`: An integer representing the target sum. * Outputs: * A list of two integers representing 1-based indices of the elements whose sum is the target. If no solution is found, return `None`. * Constraints: * Each input has exactly one solution. * List length is between 2 and 10^5. * Numbers are between -10^9 and 10^9. * Performance Requirement: Aim for O(n) time complexity with O(1) additional space usage if possible. Make sure to: * Handle edge cases like repeated elements. * Validate inputs ensuring non-integer values are safely ignored or reported. * Avoid using elements more than once. Example: ```python numbers = [2, 7, 11, 15] target = 9 advanced_two_sum(numbers, target) # Should return [1, 2] numbers = [1, 1, 3, 5, 5, 9] target = 10 advanced_two_sum(numbers, target) # Should return [1, 6] ```","solution":"def advanced_two_sum(numbers, target): Finds two indices of numbers in a sorted array that add up to a specific target. Parameters: numbers (List[int]): A list of sorted integers. target (int): The target sum. Returns: List[int]: A list of two 1-based indices if a solution is found. Otherwise, returns None. left, right = 0, len(numbers) - 1 while left < right: current_sum = numbers[left] + numbers[right] if current_sum == target: return [left + 1, right + 1] elif current_sum < target: left += 1 else: right -= 1 return None"},{"question":"Unique Triplets with Sum Zero Context: You are performing an analysis of financial transactions to identify anomalies involving three particular transactions summing up to zero. It is crucial to find all such unique triplets of transactions rapidly to report them for further investigation. Task: Write a function `find_zero_sum_triplets` that receives an array of integers representing transaction amounts and returns a set of all unique triplets that sum up to zero. Function Signature: ```python def find_zero_sum_triplets(transactions: List[int]) -> Set[Tuple[int, int, int]]: ``` Input: - `transactions`: A list of integers `S` having `n` elements (-10^4 ≤ S[i] ≤ 10^4, 0 ≤ n ≤ 10^3). Output: - A set of tuples, each containing three integers from the list that sum to zero. Constraints: - The set must not contain duplicate triplets. - Optimize to avoid O(n^3) complexity. Example: Given the input array `transactions = [-1, 0, 1, 2, -1, -4]`, - The output should be `{(-1, -1, 2), (-1, 0, 1)}`. Notes: - If no valid triplets exist, return an empty set. - Take care of edge cases where transactions may be less than 3. Performance Requirements: - Time Complexity should ideally be O(n^2). - Efficiently handle duplicates.","solution":"from typing import List, Set, Tuple def find_zero_sum_triplets(transactions: List[int]) -> Set[Tuple[int, int, int]]: Returns a set of unique triplets (a, b, c) such that a + b + c = 0. transactions.sort() triplets = set() n = len(transactions) for i in range(n - 2): if i > 0 and transactions[i] == transactions[i - 1]: continue left, right = i + 1, n - 1 while left < right: triplet_sum = transactions[i] + transactions[left] + transactions[right] if triplet_sum == 0: triplets.add((transactions[i], transactions[left], transactions[right])) left += 1 right -= 1 while left < right and transactions[left] == transactions[left - 1]: left += 1 while left < right and transactions[right] == transactions[right + 1]: right -= 1 elif triplet_sum < 0: left += 1 else: right -= 1 return triplets"},{"question":"**Linked List Intersection Detection** You are given two singly linked lists that may or may not intersect at some point. Your task is to write a function `findIntersection` that determines the node at which the intersection starts. If no intersection exists, return `None`. # Input - Two head nodes (`h1` and `h2`) of the singly linked lists. # Output - The intersection node if it exists, else `None`. # Constraints - You are allowed to modify the solution to handle edge cases effectively. - Do not use any additional helper methods or classes except those defined in the code skeleton. # Performance Requirements - Your solution should run in O(n + m) time complexity, where n and m are the lengths of the two linked lists. - Space complexity should remain O(1). # Example Let\'s instantiate two intersecting linked lists: List A: 1 -> 3 -> 5 7 -> 9 -> 11 / List B: 2 -> 4 -> 6 The lists intersect at the node with value `7`. **Sample Code Skeleton** ```python class Node: def __init__(self, val=None): self.val = val self.next = None def findIntersection(h1, h2): # Your implementation here pass # Example usage: # a1 = Node(1) # b1 = Node(3) # c1 = Node(5) # d = Node(7) # a2 = Node(2) # b2 = Node(4) # c2 = Node(6) # e = Node(9) # f = Node(11) # a1.next = b1 # b1.next = c1 # c1.next = d # a2.next = b2 # b2.next = c2 # c2.next = d # d.next = e # e.next = f # print(findIntersection(a1, a2)) # Output should be a Node with val 7 ``` # Notes - Ensure that your solution can handle edge cases, such as one or both input lists being empty, or the lists not intersecting at all.","solution":"class Node: def __init__(self, data): self.data = data self.next = None def findIntersection(h1, h2): if not h1 or not h2: return None pointer1 = h1 pointer2 = h2 while pointer1 is not pointer2: pointer1 = pointer1.next if pointer1 else h2 pointer2 = pointer2.next if pointer2 else h1 return pointer1 # Example usage: # a1 = Node(1) # b1 = Node(3) # c1 = Node(5) # d = Node(7) # a2 = Node(2) # b2 = Node(4) # c2 = Node(6) # e = Node(9) # f = Node(11) # a1.next = b1 # b1.next = c1 # c1.next = d # a2.next = b2 # b2.next = c2 # c2.next = d # d.next = e # e.next = f # print(findIntersection(a1, a2).data) # Output should be a Node with data 7"},{"question":"# Question: Sort an Array of Colors **Context**: You are given an array with `n` objects colored red, white, or blue, represented by integers `0`, `1`, and `2` respectively. Your task is to sort these objects so that the colors are grouped together and arranged in the order of red, white, and blue. **Requirements**: * Without using the library\'s sort function. * The sorting should be done in an in-place manner. **Function Signature**: ```python def sort_rgb_colors(nums: List[int]) -> None: Given a list of integers representing colors, sort the list in-place to ensure that integers of the same value are adjacent and sorted in the order: red (0), white (1), blue (2). Args: nums (List[int]): The list of color integers to be sorted. Returns: None: The function modifies the list in-place. ``` **Example**: ```python # Test case 1 nums = [2, 0, 2, 1, 1, 0] sort_rgb_colors(nums) print(nums) # Output: [0, 0, 1, 1, 2, 2] # Test case 2 nums = [2, 0, 1] sort_rgb_colors(nums) print(nums) # Output: [0, 1, 2] # Test case 3 nums = [0, 0, 1, 1, 2, 2] sort_rgb_colors(nums) print(nums) # Output: [0, 0, 1, 1, 2, 2] # Test case 4 nums = [1, 2, 0] sort_rgb_colors(nums) print(nums) # Output: [0, 1, 2] ``` **Constraints**: 1. `n` == len(nums) 2. `1 <= n <= 300` 3. `nums[i]` is either `0`, `1`, or `2`. **Challenge**: * Handle various edge cases, including: * Arrays that are already in sorted order. * Arrays with a single type of element. * Arrays with two types of elements.","solution":"from typing import List def sort_rgb_colors(nums: List[int]) -> None: Given a list of integers representing colors, sort the list in-place to ensure that integers of the same value are adjacent and sorted in the order: red (0), white (1), blue (2). Args: nums (List[int]): The list of color integers to be sorted. Returns: None: The function modifies the list in-place. low, mid, high = 0, 0, len(nums) - 1 while mid <= high: if nums[mid] == 0: nums[low], nums[mid] = nums[mid], nums[low] low += 1 mid += 1 elif nums[mid] == 1: mid += 1 else: nums[mid], nums[high] = nums[high], nums[mid] high -= 1"},{"question":"**Context**: You are a historian looking into ancient cryptographic techniques and have rediscovered the Caesar Cipher. Your task is to create a more general and efficient implementation of this cipher, capable of handling large texts while preserving the essence and rules of the original technique. **Task**: Write a function `general_caesar_cipher(s, k)` that encrypts a given string using the Caesar Cipher technique. Ensure your implementation is optimized and can handle edge cases efficiently. **Function Signature**: ```python def general_caesar_cipher(s: str, k: int) -> str: ``` **Input**: - `s` (str): The input string to be encrypted, consists of both upper and lower case alphabetic characters and special characters. - `k` (int): An integer representing the shift for the cipher. The value can be positive, negative, or zero. **Output**: - A string which is the encrypted result of applying the Caesar Cipher to the input string `s`. **Constraints**: - The input string `s` can have a length of up to (10^6) characters. - The shift `k` can range from (-10^6) to (10^6). **Example**: ```python assert general_caesar_cipher(\\"Hello, World!\\", 3) == \\"Khoor, Zruog!\\" assert general_caesar_cipher(\\"xyz\\", 4) == \\"bcd\\" assert general_caesar_cipher(\\"Attack at dawn!\\", -5) == \\"Vozzvh vo yvsk!\\" assert general_caesar_cipher(\\"\\", 10) == \\"\\" assert general_caesar_cipher(\\"ABC\\", 26) == \\"ABC\\" ``` **Notes**: - Upper-case and lower-case letters should be shifted accordingly while special characters should remain unchanged. - If `k` is negative or larger than the size of the alphabet, the function should still work appropriately by normalizing the shift value.","solution":"def general_caesar_cipher(s: str, k: int) -> str: def shift_character(c, k): if \'a\' <= c <= \'z\': return chr(((ord(c) - ord(\'a\') + k) % 26) + ord(\'a\')) elif \'A\' <= c <= \'Z\': return chr(((ord(c) - ord(\'A\') + k) % 26) + ord(\'A\')) else: return c normalized_k = k % 26 return \'\'.join(shift_character(c, normalized_k) for c in s)"},{"question":"# Reverse Words in a Sentence You are to implement a function that takes a string as input and returns the string with the words in reversed order. Use in-place array manipulation as much as possible to achieve this. Function Signature ```python def reverse_words_in_sentence(sentence: str) -> str: ``` Input - `sentence` (str): a string containing a sentence with words separated by spaces. The sentence may contain leading or trailing spaces, as well as multiple consecutive spaces between words. The length of the sentence is at most 10^4. Output - returns (str): a string with the words in reversed order with exactly one space separating each word and no leading or trailing spaces. Constraints - Minimize the use of additional data structures. - Maintain efficiency in terms of time and space as discussed in the algorithm analysis. Example Input: ``` \\"I am keon kim and I like pizza\\" ``` Output: ``` \\"pizza like I and kim keon am I\\" ``` Input: ``` \\" Hello World \\" ``` Output: ``` \\"World Hello\\" ``` Edge Cases - An empty string should return an empty string. - Strings with leading, trailing, or multiple intermediate spaces between words should be properly handled. Performance Requirements - Optimize for time and space complexity, with a target time complexity of O(n) and space complexity of O(n) due to the constraints of the input size. Scenario Imagine you need to write a function that takes any given sentence or paragraph where the order should be reversed while preserving the white spaces and minimal use of extra memory. This is particularly useful in reversing tasks or interview preps for quick rearrangement of given strings.","solution":"def reverse_words_in_sentence(sentence: str) -> str: Returns the input sentence with words in reversed order, ensuring only single space between words and no leading or trailing spaces. words = sentence.split() reversed_words = words[::-1] return \' \'.join(reversed_words)"},{"question":"# Task You are given a B-tree implementation with capabilities of inserting and removing keys, as well as searching and traversing the tree. Extend this B-tree with the following functionalities: 1. **Successor and Predecessor Retrieval**: Write a method to find the successor or predecessor of a given key in the B-tree. 2. **Range Query Operation**: Write a method to collect all keys within a given range [low, high] from the B-tree. # Requirements 1. **Successor and Predecessor Retrieval**: - Implement methods `get_successor` and `get_predecessor` that take a key as input and return the successor or predecessor respectively. - If the successor or predecessor does not exist, return `None`. 2. **Range Query Operation**: - Implement a method `range_query` which takes two parameters `low` and `high` and returns a list of all keys within this range. # Input & Output Formats 1. **get_successor(key)**: - **Input**: An integer `key`. - **Output**: The next larger integer key in the B-tree or `None` if no successor exists. 2. **get_predecessor(key)**: - **Input**: An integer `key`. - **Output**: The next smaller integer key in the B-tree or `None` if no predecessor exists. 3. **range_query(low, high)**: - **Input**: Two integers `low` and `high`. - **Output**: A list of integers representing keys in the range [low, high]. # Example Usage ```python # Assuming BTree is properly instantiated and keys inserted btree = BTree(t_val=3) # Inserting keys for key in [10, 20, 5, 6, 12, 30, 7, 17]: btree.insert_key(key) # Find successor print(btree.get_successor(10)) # Output: 12 print(btree.get_predecessor(10)) # Output: 7 # Range query [6, 17] print(btree.range_query(6, 17)) # Output: [6, 7, 10, 12, 17] ``` **Note**: You do not need to modify the provided B-tree implementation for insertions and deletions. You can assume that the initial B-tree structure is correct for the purpose of these operations.","solution":"class BTreeNode: def __init__(self, t, leaf=False): self.t = t self.leaf = leaf self.keys = [] self.children = [] def traverse(self): keys = [] for i in range(len(self.keys)): if not self.leaf: keys.extend(self.children[i].traverse()) keys.append(self.keys[i]) if not self.leaf: keys.extend(self.children[-1].traverse()) return keys def find_key(self, k): idx = 0 while idx < len(self.keys) and self.keys[idx] < k: idx += 1 return idx class BTree: def __init__(self, t): self.root = BTreeNode(t, True) self.t = t def insert_key(self, k): root = self.root if len(root.keys) == (2 * self.t) - 1: temp = BTreeNode(self.t, False) temp.children.insert(0, root) self.split_child(temp, 0) self.root = temp self.insert_not_full(self.root, k) else: self.insert_not_full(root, k) def split_child(self, x, i): t = self.t y = x.children[i] z = BTreeNode(y.t, y.leaf) x.children.insert(i + 1, z) x.keys.insert(i, y.keys[t - 1]) z.keys = y.keys[t:(2 * t) - 1] y.keys = y.keys[0:t - 1] if not y.leaf: z.children = y.children[t:(2 * t)] y.children = y.children[0:t] def insert_not_full(self, x, k): i = len(x.keys) - 1 if x.leaf: x.keys.append(0) while i >= 0 and x.keys[i] > k: x.keys[i + 1] = x.keys[i] i -= 1 x.keys[i + 1] = k else: while i >= 0 and x.keys[i] > k: i -= 1 i += 1 if len(x.children[i].keys) == (2 * x.t) - 1: self.split_child(x, i) if x.keys[i] < k: i += 1 self.insert_not_full(x.children[i], k) def get_successor(self, key): keys = self.root.traverse() if key in keys: key_index = keys.index(key) if key_index < len(keys) - 1: return keys[key_index + 1] return None def get_predecessor(self, key): keys = self.root.traverse() if key in keys: key_index = keys.index(key) if key_index > 0: return keys[key_index - 1] return None def range_query(self, low, high): keys = self.root.traverse() return [key for key in keys if low <= key <= high]"},{"question":"# ZigZag Iterator for Multiple Lists Context A ZigZag Iterator works efficiently for two lists. However, in many real-life scenarios, we might need to iterate in a zigzag manner over multiple lists. Your task is to generalize the ZigZag Iterator to accept any number of lists and iterate through them in a zigzag fashion. Task Write a function `ZigZagIterator` that takes a list of lists and iterates over them in a zigzag manner. Method Definitions 1. **`__init__(self, lists: List[List[int]])`**: - **Input**: `lists` - a list of integer lists (e.g., `[[1, 2], [3, 4, 5, 6], [7, 8, 9]]`). 2. **`next(self) -> int`**: - **Output**: The next integer in the zigzag iteration. 3. **`has_next(self) -> bool`**: - **Output**: Returns `True` if there are more elements to iterate over, `False` otherwise. Example ```python # Sample Input iterables = [[1, 2], [3, 4, 5, 6], [7, 8, 9]] it = ZigZagIterator(iterables) # Sample Output result = [] while it.has_next(): result.append(it.next()) # The order will be [1, 3, 7, 2, 4, 8, 5, 9, 6] print(result) # Output: [1, 3, 7, 2, 4, 8, 5, 9, 6] ``` Constraints * The input lists are non-empty. * You may assume all elements in the lists are valid integers. * Aim for a solution with O(1) space complexity for each operation (`next`, `has_next`). Good luck!","solution":"from collections import deque class ZigZagIterator: def __init__(self, lists): self.queue = deque([(lst, 0) for lst in lists if lst]) def next(self): lst, idx = self.queue.popleft() result = lst[idx] if idx + 1 < len(lst): self.queue.append((lst, idx + 1)) return result def has_next(self): return len(self.queue) > 0"},{"question":"You are given a directed graph represented as an adjacency matrix. Implement a function to find the transitive closure of the graph using a recursive DFS approach. # Function Signature ```python def transitive_closure(graph: List[List[int]]) -> List[List[int]]: pass ``` # Input * `graph`: A List of Lists representing an adjacency matrix of the directed graph; `graph[i][j]` is 1 if there is a direct edge from vertex `i` to vertex `j`, and 0 otherwise. # Output * Returns a List of Lists (adjacency matrix) representing the transitive closure of the graph. In the transitive closure matrix, `closure[i][j]` should be 1 if there is a path from vertex `i` to vertex `j`, otherwise 0. # Example ```python graph = [ [0, 1, 0], [0, 0, 1], [1, 0, 0] ] print(transitive_closure(graph)) # Output: [ # [1, 1, 1], # [1, 1, 1], # [1, 1, 1] # ] ``` # Constraints * `1 <= len(graph) <= 100` (graph is a square matrix: number of rows equals number of columns). * The function should be implemented using a recursive DFS approach. * Aim for O(V^2) space complexity. # Notes * Focus on function implementation, ensuring a clear depth-first search traversal. * Carefully manage recursion to avoid stack overflow on larger graphs.","solution":"from typing import List def transitive_closure(graph: List[List[int]]) -> List[List[int]]: n = len(graph) closure = [[0] * n for _ in range(n)] def dfs(root, node): for neighbor in range(n): if graph[node][neighbor] == 1 and closure[root][neighbor] == 0: closure[root][neighbor] = 1 dfs(root, neighbor) for i in range(n): # A node is always reachable from itself closure[i][i] = 1 dfs(i, i) return closure"},{"question":"You are implementing a function that modifies a binary number by removing a specific bit at a given position. This function must be efficient and optimized to run in constant time independent of the number size. Utilizing bit-level operations, your goal is to construct a reliable function to fulfill the requirements. # Function Signature ```python def remove_bit(num: int, i: int) -> int: pass ``` # Input - `num` (int): An integer whose bits are to be manipulated. - `i` (int): The zero-based position of the bit to remove. # Output - Returns an integer representing the new number after the removal of the specified bit. # Constraints - `0 <= num <= 10^9` - `0 <= i < len(bin(num)) - 2` (valid bit position for the given `num`) # Example Example 1: ```python num = 21 # Binary: 10101 i = 2 Output: 9 # Binary: 1001 ``` Example 2: ```python num = 21 # Binary: 10101 i = 4 Output: 5 # Binary: 101 ``` Example 3: ```python num = 21 # Binary: 10101 i = 0 Output: 10 # Binary: 1010 ``` # Guidelines 1. The function should handle any valid integer within the specified range. 2. Ensure no extra space is used beyond primitive data types. 3. Implement validations to handle edge cases where `i` is out of bounds or invalid. # Notes - Perform bitwise operations carefully to maintain efficiency. - Consider the constraints and ensure the solution meets given requirements.","solution":"def remove_bit(num: int, i: int) -> int: Removes the bit at position i from the binary representation of num. Returns the new integer value. # Ensure i is within a valid range if i < 0 or i >= len(bin(num)) - 2: raise ValueError(\\"Index i is out of bounds\\") # Split the number into the part before the ith bit and the part after the ith bit left_part = num >> (i + 1) # Bits to the left of the ith bit right_part = num & ((1 << i) - 1) # Bits to the right of the ith bit new_num = (left_part << i) | right_part # Combine the two parts return new_num"},{"question":"You are given a list of integers that needs to be sorted in non-decreasing order. Implement the Cocktail Shaker Sort algorithm to sort the list. # Function Signature ```python def cocktail_shaker_sort(arr: List[int]) -> List[int]: ``` # Input - A list of integers `arr` (0 ≤ length of arr ≤ 10^5, -10^6 ≤ arr[i] ≤ 10^6) # Output - A list of integers sorted in non-decreasing order. # Constraints - You must use the Cocktail Shaker Sort algorithm. - Aim to minimize the number of swaps when possible. - Handle edge cases such as empty lists or single-element lists. # Example Example 1: ```python arr = [3, 2, 5, 1, 4] cocktail_shaker_sort(arr) ``` Output: ```python [1, 2, 3, 4, 5] ``` Example 2: ```python arr = [1, 3, 2, 1, 5, 4, 1] cocktail_shaker_sort(arr) ``` Output: ```python [1, 1, 1, 2, 3, 4, 5] ``` # Explanation 1. The function should start by performing a left-to-right pass to compare and swap elements if needed. 2. If no swaps are made during a pass, the array is already sorted and the function can terminate early. 3. The algorithm should continue with a right-to-left pass, applying the same comparison and swapping logic. Ensure your implementation is efficient and adheres to the constraints provided.","solution":"from typing import List def cocktail_shaker_sort(arr: List[int]) -> List[int]: Sorts a list of integers in non-decreasing order using the Cocktail Shaker Sort algorithm. n = len(arr) if n <= 1: return arr is_sorted = False start = 0 end = n - 1 while not is_sorted: is_sorted = True # Left to right pass for i in range(start, end): if arr[i] > arr[i + 1]: arr[i], arr[i + 1] = arr[i + 1], arr[i] is_sorted = False if is_sorted: break is_sorted = True end -= 1 # Right to left pass for i in range(end - 1, start - 1, -1): if arr[i] > arr[i + 1]: arr[i], arr[i + 1] = arr[i + 1], arr[i] is_sorted = False start += 1 return arr"},{"question":"You are required to implement a function that converts a provided integer to a byte array in a specified endian format and vice versa. Additionally, you need to make sure the conversion works correctly for edge cases, such as zero and large integers. The function signature is: Function Signature ```python def convert_integer_to_bytes(number: int, endian: str) -> bytes: pass def convert_bytes_to_integer(byte_array: bytes, endian: str) -> int: pass ``` # Input 1. **convert_integer_to_bytes**: - `number`: A non-negative integer. - `endian`: A string, either `\\"big\\"` or `\\"little\\"`, specifying the endian format. 2. **convert_bytes_to_integer**: - `byte_array`: A byte array. - `endian`: A string, either `\\"big\\"` or `\\"little\\"`, specifying the endian format. # Output 1. **convert_integer_to_bytes**: - Returns a byte array. 2. **convert_bytes_to_integer**: - Returns an integer. # Constraints - The integer values are non-negative. - Endian can only be \\"big\\" or \\"little\\". - Ensure proper handling of leading zeros in bytes. # Example ```python # Example 1: number = 305419896 endian = \\"big\\" print(convert_integer_to_bytes(number, endian)) # Output: b\'x12x34x56x78\' # Example 2: byte_array = b\'x01x02x03x04\' endian = \\"little\\" print(convert_bytes_to_integer(byte_array, endian)) # Output: 67305985 ``` # Explanation - Example 1: The integer 305419896 when converted into a big-endian byte array yields b\'x12x34x56x78\'. - Example 2: The byte array b\'x01x02x03x04\' when read as little-endian gives the integer 67305985. # Notes - You may assume that the provided byte array is already in the correct endian format as specified by the `endian` argument. # Your Task Implement the two functions: - `convert_integer_to_bytes` which handles the integer to byte array conversion based on endian format. - `convert_bytes_to_integer` which converts the given byte array back to an integer based on endian format.","solution":"def convert_integer_to_bytes(number: int, endian: str) -> bytes: if endian not in [\\"big\\", \\"little\\"]: raise ValueError(\\"Endian must be either \'big\' or \'little\'\\") return number.to_bytes((number.bit_length() + 7) // 8 or 1, byteorder=endian) def convert_bytes_to_integer(byte_array: bytes, endian: str) -> int: if endian not in [\\"big\\", \\"little\\"]: raise ValueError(\\"Endian must be either \'big\' or \'little\'\\") return int.from_bytes(byte_array, byteorder=endian)"},{"question":"# Flatten Nested Arrays/Iterables Context: You are working on a module to process hierarchical data structures, where you often receive deeply nested lists and other iterables. To simplify further processing, you need to convert these nested structures into a flat list. Problem Statement: Write a Python function `flatten_arrays` that takes a multi-dimensional structure (a list containing nested lists or iterables) and returns a flat list with all the individual elements. Additionally, write a generator function `flatten_iterable` that does the same but returns an iterator instead of building a list. Function Signatures: ```python def flatten_arrays(input_arr: Iterable) -> List: def flatten_iterable(iterable: Iterable) -> Iterator: ``` Input: * `input_arr` and `iterable`: a multi-dimensional iterable, which may include integers, strings, lists, tuples, and other iterables. It can be empty. Output: * `flatten_arrays` should return a single flat list with all contained elements. * `flatten_iterable` should return an iterator that yields each element in the flat structure one by one. Constraints: 1. Do not use any external libraries for array manipulation. 2. Handle arbitrarily deeply nested structures up to the recursion limit. Examples: ```python # flatten_arrays function: print(flatten_arrays([1, [2, [3, [4, 5]], 6], 7])) # Output: [1, 2, 3, 4, 5, 6, 7] print(flatten_arrays([\'a\', [\'b\', [\'c\']], \'d\'])) # Output: [\'a\', \'b\', \'c\', \'d\'] # flatten_iterable function: result = list(flatten_iterable([1, [2, [3, [4, 5]], 6], 7])) print(result) # Output: [1, 2, 3, 4, 5, 6, 7] result_gen = flatten_iterable([\'a\', [\'b\', [\'c\']], \'d\']) print(list(result_gen)) # Output: [\'a\', \'b\', \'c\', \'d\'] ``` Edge Cases: 1. Empty input list `[]` should return an empty list. 2. Single element list `[1]` should return `[1]`. 3. Nested empty lists `[[], [[]], [[], [[]]]]` should return an empty list `[]`. # Performance Consideration: * Ensure your functions perform efficiently even with deeply nested structures and large inputs. * The generator solution should be memory efficient and handle large datasets.","solution":"from typing import Iterable, List, Iterator def flatten_arrays(input_arr: Iterable) -> List: Flattens a multi-dimensional iterable into a list. result = [] for element in input_arr: if isinstance(element, Iterable) and not isinstance(element, (str, bytes)): result.extend(flatten_arrays(element)) else: result.append(element) return result def flatten_iterable(iterable: Iterable) -> Iterator: Flattens a multi-dimensional iterable into a generator. for element in iterable: if isinstance(element, Iterable) and not isinstance(element, (str, bytes)): yield from flatten_iterable(element) else: yield element"},{"question":"# Stooge Sort Application & Analysis Given a list of student names, you are tasked with sorting the names in non-decreasing order using Stooge Sort. Stooge Sort is an inefficient sorting algorithm but is useful for understanding recursion and basic sorting principles. **Problem Statement:** Implement the Stooge Sort algorithm to sort a list of student names in lexicographical order. **Function Signature:** ```python def stooge_sort_strings(arr: List[str], l: int, h: int) -> None: # Implement Stooge Sort for sorting strings ``` **Input:** - A list of strings `arr` where each string represents a student\'s name. - Two integers, `l` and `h` which are the starting and ending indices of the list to be sorted, respectively. **Output:** - The function should sort the list in place; no return value is required. **Constraints:** - The list `arr` will have a maximum length of 100. - Each string in `arr` will contain only alphabetic characters and will have a maximum length of 50. - The sorting is case-insensitive, e.g., \\"alice\\" and \\"Alice\\" are considered equal. **Example:** ```python names = [\\"john\\", \\"alice\\", \\"bob\\", \\"carol\\"] stooge_sort_strings(names, 0, len(names) - 1) print(names) # Output: [\\"alice\\", \\"bob\\", \\"carol\\", \\"john\\"] ``` **Performance Requirements:** - Although Stooge Sort is not efficient, ensure your implementation can handle the upper constraint within a reasonable time frame. **Additional Notes:** - Remember to handle edge cases such as an empty list, a list with one element, and a list with all identical strings. - Consider potential issues with case sensitivity by normalizing the strings to lowercase before comparison, but remember to keep the original case in the sorted output.","solution":"from typing import List def stooge_sort_strings(arr: List[str], l: int, h: int) -> None: if l >= h: return # Normalize the strings for case-insensitive comparison if arr[l].lower() > arr[h].lower(): arr[l], arr[h] = arr[h], arr[l] if h - l + 1 > 2: t = (h - l + 1) // 3 stooge_sort_strings(arr, l, h - t) stooge_sort_strings(arr, l + t, h) stooge_sort_strings(arr, l, h - t)"},{"question":"# Scenario You are given a list of scores from a recent programming contest. Your task is to sort these scores in non-decreasing order using the Merge Sort algorithm. Write a Python function `merge_sort_contest_scores` that correctly sorts these scores. # Function Signature ```python def merge_sort_contest_scores(scores: List[int]) -> List[int]: pass ``` # Input * `scores`: a list of integers representing the contest scores. (0 ≤ len(scores) ≤ 10^5), (-10^9 ≤ scores[i] ≤ 10^9) # Output * A list of integers sorted in non-decreasing order. # Constraints * The function should run efficiently within O(n log n) time complexity. * The function should use O(n) auxiliary space. # Requirements 1. Implement the `merge_sort_contest_scores` function using the Merge Sort algorithm. 2. Avoid unnecessary copying of arrays to optimize space usage. 3. Ensure the implemented algorithm is stable (i.e., handles equal elements correctly). # Examples ```python assert merge_sort_contest_scores([34, 7, 23, 32, 5, 62]) == [5, 7, 23, 32, 34, 62] assert merge_sort_contest_scores([1, 3, 2, 2, 3, 1]) == [1, 1, 2, 2, 3, 3] assert merge_sort_contest_scores([-1, -3, 0, 2, -2]) == [-3, -2, -1, 0, 2] ``` # Additional Notes * Make sure to test your function with edge cases such as empty arrays and arrays with a single element. * Avoid using built-in sort functions and aim for a complete implementation of the Merge Sort algorithm.","solution":"from typing import List def merge_sort_contest_scores(scores: List[int]) -> List[int]: Sorts the list of scores using the Merge Sort algorithm. Parameters: scores (List[int]): A list of integers representing the contest scores. Returns: List[int]: A list of integers sorted in non-decreasing order. if len(scores) <= 1: return scores def merge(left: List[int], right: List[int]) -> List[int]: sorted_list = [] left_index, right_index = 0, 0 while left_index < len(left) and right_index < len(right): if left[left_index] <= right[right_index]: sorted_list.append(left[left_index]) left_index += 1 else: sorted_list.append(right[right_index]) right_index += 1 sorted_list.extend(left[left_index:]) sorted_list.extend(right[right_index:]) return sorted_list mid = len(scores) // 2 left = merge_sort_contest_scores(scores[:mid]) right = merge_sort_contest_scores(scores[mid:]) return merge(left, right)"},{"question":"# Matrix Transformation and Analysis Given a square matrix (2D list) of integers, we are going to perform various transformations related to rotations and inversions. Your task is to implement the following functions: 1. **rotate_clockwise(matrix)** * Rotate the given matrix 90 degrees in the clockwise direction. * **Input**: A square matrix of integers. * **Output**: A new matrix rotated 90 degrees clockwise. 2. **rotate_counterclockwise(matrix)** * Rotate the given matrix 90 degrees in the counterclockwise direction. * **Input**: A square matrix of integers. * **Output**: A new matrix rotated 90 degrees counterclockwise. 3. **top_left_invert(matrix)** * Invert the matrix along the top-left to bottom-right diagonal. * **Input**: A square matrix of integers. * **Output**: A new matrix inverted along the diagonal. 4. **bottom_left_invert(matrix)** * Invert the matrix along the bottom-left to top-right diagonal. * **Input**: A square matrix of integers. * **Output**: A new matrix inverted along the diagonal. # Constraints * All matrices are square matrices (same number of rows and columns). * Matrices can be of size 1x1 up to 1000x1000. # Example ``` Input Matrix: [ [1, 2, 3], [4, 5, 6], [7, 8, 9] ] rotate_clockwise(Input Matrix): [ [7, 4, 1], [8, 5, 2], [9, 6, 3] ] rotate_counterclockwise(Input Matrix): [ [3, 6, 9], [2, 5, 8], [1, 4, 7] ] top_left_invert(Input Matrix): [ [1, 4, 7], [2, 5, 8], [3, 6, 9] ] bottom_left_invert(Input Matrix): [ [9, 6, 3], [8, 5, 2], [7, 4, 1] ] ``` # Notes - Ensure that your code works for matrices of different sizes and properly handles edge cases. - Implement each function separately as specified above.","solution":"def rotate_clockwise(matrix): Rotate the given matrix 90 degrees in the clockwise direction. n = len(matrix) return [[matrix[n-1-j][i] for j in range(n)] for i in range(n)] def rotate_counterclockwise(matrix): Rotate the given matrix 90 degrees in the counterclockwise direction. n = len(matrix) return [[matrix[j][n-1-i] for j in range(n)] for i in range(n)] def top_left_invert(matrix): Invert the matrix along the top-left to bottom-right diagonal. n = len(matrix) return [[matrix[j][i] for j in range(n)] for i in range(n)] def bottom_left_invert(matrix): Invert the matrix along the bottom-left to top-right diagonal. n = len(matrix) return [[matrix[n-1-j][n-1-i] for j in range(n)] for i in range(n)]"},{"question":"You are working on a navigation system that needs to efficiently find the nearest points of interest around the current location. Given a list of coordinates representing various points and an integer k, you need to identify the k points closest to a specified origin. **Function Signature:** ```python def k_closest(points: List[Tuple[int, int]], k: int, origin: Tuple[int, int] = (0, 0)) -> List[Tuple[int, int]]: ``` # Input Format * `points`: A list of tuples of integers `[(x1, y1), (x2, y2), ..., (xn, yn)]` representing coordinates of the points. * `k`: An integer indicating the number of closest points to find. * `origin`: A tuple of two integers representing the coordinate of the origin. Defaults to `(0, 0)`. # Output Format * Return a list of `k` tuples of integers representing the coordinates of the closest points to the origin. # Constraints * `1 <= k <= len(points) <= 10000` * Points coordinates are in the range `-10000 <= x, y <= 10000` # Example ```python points = [(1, 3), (3, 4), (2, -1)] k = 2 origin = (0, 0) print(k_closest(points, k, origin)) # Output: [(1, 3), (2, -1)] ``` # Notes * The distance calculation will use the Euclidean distance formula (ignore the square root for comparison purposes). * Ensure efficient handling of potentially large datasets with the specified constraints. * Consider edge cases like when points have the same distance or when k is equal to the total number of points.","solution":"from typing import List, Tuple def k_closest(points: List[Tuple[int, int]], k: int, origin: Tuple[int, int] = (0, 0)) -> List[Tuple[int, int]]: Returns k closest points to the origin. # Calculate the squared Euclidean distance from the origin for each point def squared_distance(point: Tuple[int, int], origin: Tuple[int, int]) -> int: return (point[0] - origin[0]) ** 2 + (point[1] - origin[1]) ** 2 # Sort points based on calculated squared distances sorted_points = sorted(points, key=lambda point: squared_distance(point, origin)) # Return the first k points from the sorted list return sorted_points[:k]"},{"question":"You are given a stack that contains a series of integers. Your goal is to write a function that duplicates each element of the stack. You should implement two versions of this function: 1. Using an auxiliary stack. 2. Using an auxiliary queue. # Requirements: 1. **Function using auxiliary stack**: ```python def first_stutter(stack): # Implementation here ``` 2. **Function using auxiliary queue**: ```python def second_stutter(stack): # Implementation here ``` # Example: Suppose the stack stores these values: ``` bottom [3, 7, 1, 14, 9] top ``` Then both functions should transform the stack to: ``` bottom [3, 3, 7, 7, 1, 1, 14, 14, 9, 9] top ``` # Constraints: - **Input Format**: The input stack will be provided as a list where the last element of the list represents the top of the stack. - **Output Format**: The output should be a list representing the modified stack. - Each stack element will be an integer. - You may assume the input stack contains no more than 10^4 elements. # Requirements: - Ensure your solution handles edge cases, such as empty stacks and stacks with a single element. - The operations should be completed with a time complexity of O(n) and space complexity of O(n). # Notes: - The provided stack should be mutated in place. - You cannot use any built-in stack or queue libraries other than basic list operations for the stack and `collections.deque` for the queue. # Submission: Submit your solution code for both `first_stutter` and `second_stutter` functions.","solution":"from collections import deque def first_stutter(stack): Function that duplicates each element in the stack using an auxiliary stack. if not stack: return stack auxiliary_stack = [] while stack: value = stack.pop() auxiliary_stack.append(value) auxiliary_stack.append(value) while auxiliary_stack: stack.append(auxiliary_stack.pop()) return stack def second_stutter(stack): Function that duplicates each element in the stack using an auxiliary queue. if not stack: return stack queue = deque() while stack: value = stack.pop() queue.appendleft(value) queue.appendleft(value) while queue: stack.append(queue.popleft()) return stack"},{"question":"# Question You are tasked with implementing a function to simulate the process of selection sort on an array. Although selection sort is not the most efficient algorithm for large datasets, understanding its mechanics is fundamental in grasping sorting algorithms. Function Signature ```python def selection_sort(arr: List[int], simulation: bool = False) -> List[int]: ``` Input * `arr`: A list of integers to be sorted. * `simulation`: A boolean flag to print the array state after each iteration of the outer loop. Output * The function should return the sorted list. Constraints * The array can contain integers in the range of `[-10^6, 10^6]`. * The length of the array will not exceed `10^3`. Performance Requirements * Your function should run within O(n^2) time complexity and O(1) space complexity. # Context In addition to sorting the list, the function should simulate each step of the outer loop and print the array\'s state if the `simulation` flag is set to `True`. This helps in understanding each iteration of the selection sort process. For instance, if the flag is `True`, after every iteration of placing the minimum element in its correct position, print the entire array showing its state. Example ```python selection_sort([64, 25, 12, 22, 11], simulation=True) ``` Expected output: ``` iteration 0 : 64 25 12 22 11 iteration 1 : 11 25 12 22 64 iteration 2 : 11 12 25 22 64 iteration 3 : 11 12 22 25 64 iteration 4 : 11 12 22 25 64 [11, 12, 22, 25, 64] ``` Implement the function accordingly, ensuring it mimics the behavior and output as per the given example.","solution":"from typing import List def selection_sort(arr: List[int], simulation: bool = False) -> List[int]: Perform selection sort on the given list and optionally print the array state after each iteration of the outer loop. Parameters: arr (List[int]): The list of integers to be sorted. simulation (bool): Flag to print the array state after each iteration of the outer loop. Returns: List[int]: Sorted list of integers. n = len(arr) for i in range(n): # Find the minimum element in remaining unsorted array min_idx = i for j in range(i+1, n): if arr[j] < arr[min_idx]: min_idx = j # Swap the found minimum element with the first element of the unsorted part arr[i], arr[min_idx] = arr[min_idx], arr[i] # If simulation is True, print the current state of the array if simulation: print(f\'iteration {i} : {\\" \\".join(map(str, arr))}\') return arr"},{"question":"# Coding Task: String Reverse Implementation **Scenario**: You are tasked with implementing different ways to reverse a string. This function will be part of a larger text-processing library. Your goal is to practice and understand various algorithms and their inefficiencies. **Task**: Implement the following four functions to reverse a given string: 1. `recursive_reverse(s: str) -> str` 2. `iterative_reverse(s: str) -> str` 3. `pythonic_reverse(s: str) -> str` 4. `ultra_pythonic_reverse(s: str) -> str` **Function Specifications**: - All implementations should accept a single string `s` (0 <= len(s) <= 10^5) as input and return the reversed string. - Maintain the method\'s characteristic and ensure correct handling of edge cases. Input: - A single string `s`. Output: - A string that is the reverse of `s`. Constraints: - The function should handle strings of extraordinary lengths properly. - All functions must pass a performance check within 1 second for maximum length input strings. **Examples**: ```python # Example 1: input: \\"hello\\" output: \\"olleh\\" # Example 2: input: \\"Python\\" output: \\"nohtyP\\" # Example 3: input: \\"\\" output: \\"\\" # Example 4: input: \\"a\\" output: \\"a\\" ```","solution":"def recursive_reverse(s: str) -> str: Reverse a string using recursion. if len(s) == 0: return s return s[-1] + recursive_reverse(s[:-1]) def iterative_reverse(s: str) -> str: Reverse a string using an iterative approach. reversed_str = [] for char in s: reversed_str.insert(0, char) return \'\'.join(reversed_str) def pythonic_reverse(s: str) -> str: Reverse a string using Python slicing. return s[::-1] def ultra_pythonic_reverse(s: str) -> str: Reverse a string using the reversed() function and join(). return \'\'.join(reversed(s))"},{"question":"# Context You are a developer at a bioinformatics company working on DNA sequence analysis algorithms. Your task is to implement the Longest Increasing Subsequence (LIS) algorithm. Given an unsorted array of integers representing nucleotides\' values in a DNA sequence, you need to find the length of the longest increasing subsequence in an optimal manner. # Problem Statement Write a function to determine the length of the longest increasing subsequence in a given list of integers. # Function Signature ```python def longest_increasing_subsequence(sequence: List[int]) -> int: ``` # Input - `sequence` (List[int]): A list of integers representing the sequence. # Output - (int): Length of the longest increasing subsequence. # Constraints - All elements in the sequence are non-negative integers and do not exceed 10^5. - The length of the sequence does not exceed 10^5. # Requirements 1. Implement an efficient algorithm with time complexity no worse than O(n log n). 2. Consider edge cases, such as sequences of length 0, sequences with all identical elements, and strictly decreasing sequences. # Examples ```python assert longest_increasing_subsequence([10, 9, 2, 5, 3, 7, 101, 18]) == 4 assert longest_increasing_subsequence([]) == 0 assert longest_increasing_subsequence([1, 2, 3, 4, 5]) == 5 assert longest_increasing_subsequence([5, 4, 3, 2, 1]) == 1 ``` # Notes - You are encouraged to utilize advanced data structures like segment trees or Binary Indexed Trees (BIT) to optimize your solution. - Ensure that your implementation does not exceed memory limitations and handles edge cases appropriately.","solution":"from typing import List import bisect def longest_increasing_subsequence(sequence: List[int]) -> int: Function to determine the length of the longest increasing subsequence in a given list of integers. if not sequence: return 0 # \'tail\' will store the elements of the potential increasing subsequences tail = [] for num in sequence: # Find the index at which to place the current number index = bisect.bisect_left(tail, num) if index == len(tail): # If num is greater than all elements in tail, append it tail.append(num) else: # If there is a larger element in the position, replace it with num tail[index] = num return len(tail)"},{"question":"# Prime Digit Sum **Scenario**: Imagine you\'re designing a system that filters out certain numbers based on their mathematical properties. One specific feature is to detect numbers whose digit sum is a prime number. This is useful in contexts like cryptography, random number generation, or special number sequences. **Task**: Write a function to check if the sum of the digits of a given integer is a prime number. For example, the number 58 has a digit sum of 5 + 8 = 13, which is a prime number. Hence the function should return True for 58. Function signature: ```python def is_prime_digit_sum(n: int) -> bool: ``` Input: * An integer `n` (0 ≤ n ≤ 10^9) Output: * Returns `True` if the sum of the digits of `n` is a prime number, otherwise returns `False`. Constraints: * Focus on optimizing for readability and performance. * Use the provided `prime_check` function to assess primality of the digit sum. Example: ```python assert is_prime_digit_sum(58) == True # 5 + 8 = 13 -> True assert is_prime_digit_sum(123) == False # 1 + 2 + 3 = 6 -> False assert is_prime_digit_sum(29) == True # 2 + 9 = 11 -> True assert is_prime_digit_sum(0) == False # 0 -> False ``` Explanation: 1. Calculate the sum of the digits of the input integer `n`. 2. Use the `prime_check` function to determine if this digit sum is a prime number. 3. Return the result accordingly. **Note**: Use the provided `prime_check` function as is, without modifications, to verify primality in your solution.","solution":"def prime_check(n: int) -> bool: Returns True if n is a prime number, False otherwise. if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True def is_prime_digit_sum(n: int) -> bool: Returns True if the sum of the digits of n is a prime number, False otherwise. digit_sum = sum(int(digit) for digit in str(n)) return prime_check(digit_sum)"},{"question":"Problem Statement You are tasked with writing a function to remove duplicate elements from a given list while preserving the order of their first occurrences. Your implementation should aim for optimal performance, considering both time and space complexity. # Input * A single list `array`, which can contain elements of any data type, potentially non-hashable. # Output * A list with duplicates removed, maintaining the order of their first occurrence in the input list. # Constraints * The input list `array` can have up to 10^5 elements. * Elements can be of any data type including integers, strings, booleans, and even complex types like lists and dictionaries. # Performance Requirements * Time complexity should be optimized to O(n) to handle large inputs efficiently. # Example ```python remove_duplicates([1, 1 ,1 ,2 ,2 ,3 ,4 ,4 ,\\"hey\\", \\"hey\\", \\"hello\\", True, True]) ``` Expected Output: ```python [1, 2, 3, 4, \'hey\', \'hello\', True] ``` # Implementation Considering the requirements and constraints, write your function signature and complete the implementation below. ```python def remove_duplicates(array): # Your optimized implementation here pass ```","solution":"def remove_duplicates(array): Removes duplicate elements from a list while preserving the order of their first occurrences. Args: array (list): The input list with potential duplicates. Returns: list: A list with duplicates removed, preserving the original order. seen = set() result = [] for item in array: # Convert item to a string representation for handling non-hashable types item_repr = repr(item) if item_repr not in seen: seen.add(item_repr) result.append(item) return result"},{"question":"Problem Description You are tasked with implementing the Misra-Gries algorithm to find all elements in a list that appear more than n/k times, where n is the length of the list and k is a specified parameter. Task Write a function `frequent_elements(lst, k)` that takes a list of integers `lst` and a positive integer `k`, and returns a dictionary where the keys are the elements from `lst` that appear more than n/k times, and the values are the counts of these elements. If no such elements exist, return an empty dictionary. Input Format - `lst`: A list of integers. - `k`: A positive integer greater than 1. Output Format - A dictionary where: - Keys are integers from `lst` that appear more than `len(lst) / k` times. - Values are the counts of these integers. Constraints - `lst` will contain at most 10^6 integers. - Elements in `lst` will be between -10^6 and 10^6. - `k` is an integer such that 2 <= k <= len(lst). Example ```python # Input lst = [1, 2, 3, 3, 3, 2, 3, 3], k = 2 # Output {\'3\': 5} # Input lst = [0, 0, 0, 1, 1, 1], k = 3 # Output {\'0\': 3, \'1\': 3} ``` Performance Your solution should have a time complexity of O(n) and space complexity of O(k). Additional Notes - Handle edge cases where the list `lst` is empty. - Ensure `k` is properly limited to valid values in the input constraints.","solution":"def frequent_elements(lst, k): from collections import defaultdict if len(lst) == 0 or k <= 1: return {} n = len(lst) threshold = n // k counter = defaultdict(int) # First pass: find potential candidates for num in lst: if num in counter: counter[num] += 1 elif len(counter) < k - 1: counter[num] += 1 else: for candidate in list(counter.keys()): counter[candidate] -= 1 if counter[candidate] == 0: del counter[candidate] # Second pass: confirm the actual counts actual_counts = defaultdict(int) for num in lst: if num in counter: actual_counts[num] += 1 # Filter out the elements which have counts more than threshold result = {num: count for num, count in actual_counts.items() if count > threshold} return result"},{"question":"# Binary Search Enhancement: Occurrence Counter Binary search is a powerful technique to quickly find an element in a sorted array. Now let\'s extend it to count the number of occurrences of a given element. # Problem Statement Given a sorted array of integers and a target value, write a function `count_occurrences` that returns the number of times the target value appears in the array. # Function Signature ```python def count_occurrences(array: List[int], target: int) -> int: # your code here ``` # Input * `array`: A list of sorted integers. * `target`: An integer value whose occurrences need to be counted. # Output * An integer representing the number of occurrences of the target in the array. # Constraints * The array is sorted in non-decreasing order. * The length of the array is between 0 and 10^6. * The integer\'s value will range between -10^9 to 10^9. # Example ```python array = [1, 2, 2, 2, 3, 3, 4, 5] target = 2 count_occurrences(array, target) # returns 3 ``` # Hints 1. Use binary search variations to find the leftmost and rightmost occurrences of the target. 2. Ensure that your solution has a time complexity of O(log(n)).","solution":"from typing import List def count_occurrences(array: List[int], target: int) -> int: def binary_search_left(arr, tgt): left, right = 0, len(arr) while left < right: mid = (left + right) // 2 if arr[mid] < tgt: left = mid + 1 else: right = mid return left def binary_search_right(arr, tgt): left, right = 0, len(arr) while left < right: mid = (left + right) // 2 if arr[mid] > tgt: right = mid else: left = mid + 1 return left left_index = binary_search_left(array, target) right_index = binary_search_right(array, target) return right_index - left_index"},{"question":"# String Reversal Challenge **Objective**: Write a function `reverse_string` that reverses a given string. The implementation should be optimized for performance and should handle a variety of input scenarios efficiently. **Function Signature**: ```python def reverse_string(s: str) -> str: pass ``` **Input**: * A single string `s`, with length `0 <= len(s) <= 10^6`. **Output**: * Returns the reversed string. **Constraints**: * Your solution should handle edge cases such as empty strings and strings with special characters. * Aim for an efficient solution with a time complexity better than O(n log n) and space complexity that doesn\'t exceed O(n). **Examples**: 1. `reverse_string(\\"hello\\")` should return `\\"olleh\\"` 2. `reverse_string(\\"1234\\")` should return `\\"4321\\"` 3. `reverse_string(\\"!@@!\\")` should return `\\"!@@\\"` **Performance Requirement**: The function should perform efficiently within the input constraint of up to lengths of 10^6. **Context**: String reversal is a common operation in text processing. Different algorithms to achieve this have their strengths and weaknesses. Your task is to implement a method that is efficient and handles the edge cases effectively. Good luck!","solution":"def reverse_string(s: str) -> str: Returns the reversed version of the input string s. return s[::-1]"},{"question":"# Matrix Multiplication Given two 2-dimensional matrices, write a function `matrix_multiply(A: List[List[int]], B: List[List[int]]) -> List[List[int]]` that returns the product of the two matrices. Ensure that your solution handles potential edge cases gracefully. Input * Two lists of lists where each inner list represents a row of the matrix. - Matrix A of dimensions m x n. - Matrix B of dimensions n x p. Output * A list of lists representing the resulting matrix of dimensions m x p. Constraints * The number of columns in matrix A must equal the number of rows in matrix B. * Elements of the matrices are integers. Example Input: ```python A = [ [1, 2, 3], [4, 5, 6] ] B = [ [7, 8], [9, 10], [11, 12] ] ``` Output: ```python [ [58, 64], [139, 154] ] ``` Notes 1. You are expected to gracefully handle incompatible matrices by raising an appropriate exception. 2. Keep in mind the efficiency and aim to minimize unnecessary operations. Implement the function `matrix_multiply`: ```python def matrix_multiply(A: List[List[int]], B: List[List[int]]) -> List[List[int]]: # Your code here ```","solution":"from typing import List def matrix_multiply(A: List[List[int]], B: List[List[int]]) -> List[List[int]]: Multiplies two matrices A and B and returns the resulting matrix. Parameters: A (List[List[int]]): The first matrix of dimensions m x n B (List[List[int]]): The second matrix of dimensions n x p Returns: List[List[int]]: Resulting matrix of dimensions m x p Raises: ValueError: If the number of columns in A is not equal to the number of rows in B if len(A[0]) != len(B): raise ValueError(\\"The number of columns in A must be equal to the number of rows in B.\\") m = len(A) # Number of rows in A n = len(A[0]) # Number of columns in A and Number of rows in B p = len(B[0]) # Number of columns in B # Initialize the result matrix with zeros result = [[0] * p for _ in range(m)] # Perform matrix multiplication for i in range(m): for j in range(p): for k in range(n): result[i][j] += A[i][k] * B[k][j] return result"},{"question":"Implement a Custom Heap Sort Question Statement You have been given the task of implementing a custom heap sort algorithm with additional features. Your task is to create a function `custom_heap_sort` that can either use a Max Heap or a Min Heap to sort an array. Furthermore, the function should be able to handle duplicate elements efficiently and maintain stability, i.e., it should not change the relative order of elements with equal keys. Function Signature ```python def custom_heap_sort(arr: List[int], use_max_heap: bool = True, return_indices: bool = False) -> List[int]: pass ``` Input * `arr`: A list of integers to be sorted. * `use_max_heap`: A boolean flag. If True, use Max Heap for sorting in descending order; otherwise, use Min Heap for ascending order. * `return_indices`: A boolean flag. If True, the function should return the original indices of the sorted elements in the sorted order. Otherwise, it should return the sorted array. Output A list of integers representing the sorted elements or the original indices of the sorted elements, based on the `return_indices` flag. Constraints * 1 <= len(arr) <= 10^5 Requirements 1. Implement both Max Heap Sort and Min Heap Sort within the `custom_heap_sort` function based on the `use_max_heap` flag. 2. Ensure the sorting maintains stability, i.e., the relative order of equal elements remains unchanged. 3. When `return_indices` is True, instead of returning the sorted array, return the indices of the elements in their new sorted order. Performance Considerations 1. The solution should run in O(n log(n)) time complexity. 2. The solution should be in-place to adhere to O(1) additional space complexity. Example ```python # Example 1 print(custom_heap_sort([4, 6, 5, 9, 2, 2, 1], use_max_heap=False)) # Output: [1, 2, 2, 4, 5, 6, 9] # Example 2 print(custom_heap_sort([4, 6, 5, 9, 2, 2, 1], use_max_heap=True, return_indices=True)) # Output: [6, 4, 5, 0, 2, 1, 3] ```","solution":"import heapq from typing import List def custom_heap_sort(arr: List[int], use_max_heap: bool = True, return_indices: bool = False) -> List[int]: # Handle empty array if not arr: return [] # Pair elements with their indices to maintain stability indexed_arr = [(i, arr[i]) for i in range(len(arr))] if use_max_heap: # Use negative values to simulate Max Heap behavior in heapq heap = [(-value, index) for index, value in indexed_arr] else: # Min Heap heap = [(value, index) for index, value in indexed_arr] # Transform the list into a heap heapq.heapify(heap) sorted_array = [] while heap: if use_max_heap: value, index = heapq.heappop(heap) value = -value # Convert back to original value else: value, index = heapq.heappop(heap) sorted_array.append((index, value)) if return_indices: return [index for index, value in sorted_array] else: return [value for index, value in sorted_array]"},{"question":"# Problem: Advanced Graph Traversal Check You are given a directed graph represented as an adjacency list and your task is to implement two functions: one for Depth-First Search (DFS) and one for Breadth-First Search (BFS). This graph might contain cycles and disconnected components. Detailed Requirements: 1. **DFS Function Implementation**: - Implement an iterative DFS using a stack. - Ensure that the function returns a list of nodes in the order they are first visited. 2. **BFS Function Implementation**: - Implement BFS using a queue. - Ensure that the function returns a list of nodes in the order they are first visited. Input: 1. `graph`: A dictionary where keys are node identifiers and values are lists of adjacent nodes. 2. `start`: The starting node for the traversal. Output: 1. `dfs_order`: A list of nodes in the order they are visited by DFS. 2. `bfs_order`: A list of nodes in the order they are visited by BFS. Constraints: - The graph is represented as an adjacency list. - Node identifiers are integers. - It is a directed graph. - Start node is guaranteed to be a valid node in the graph. Performance: - Your solution should handle graphs with up to 10^3 vertices and 10^4 edges efficiently. # Example: ```python graph = { 0: [1, 2], 1: [2], 2: [0, 3], 3: [3] } start = 2 dfs_order = dfs_traverse(graph, start) bfs_order = bfs_traverse(graph, start) print(dfs_order) # Output example: [2, 0, 1, 3] print(bfs_order) # Output example: [2, 0, 3, 1] ``` **Note: The exact order may vary depending on the implementation.** Write the implementations of the following functions: ```python def dfs_traverse(graph, start): # YOUR CODE HERE def bfs_traverse(graph, start): # YOUR CODE HERE ```","solution":"def dfs_traverse(graph, start): Implements iterative DFS using a stack. Returns a list of nodes in the order they are first visited. visited = set() stack = [start] order = [] while stack: node = stack.pop() if node not in visited: visited.add(node) order.append(node) # Add neighbors to stack in reverse order for correct ordering for neighbor in reversed(graph.get(node, [])): if neighbor not in visited: stack.append(neighbor) return order def bfs_traverse(graph, start): Implements BFS using a queue. Returns a list of nodes in the order they are first visited. from collections import deque visited = set() queue = deque([start]) order = [] while queue: node = queue.popleft() if node not in visited: visited.add(node) order.append(node) for neighbor in graph.get(node, []): if neighbor not in visited: queue.append(neighbor) return order"},{"question":"# Binary Tree: Serialization and Deserialization Context: You are given a binary tree and need to serialize it into a compact string form so that it can be easily stored or transmitted. Later, you will deserialize this string back to the original tree structure. Task: 1. **Serialization**: Write a function `serialize(root: TreeNode) -> str` that takes the root of a binary tree and returns a string representation of the tree using the preorder traversal method described. 2. **Deserialization**: Write a function `deserialize(data: str) -> TreeNode` that takes a serialized string and reconstructs the binary tree, returning its root. Requirements: * Use preorder tree traversal for both serialization and deserialization. * The string format should use spaces to separate node values and `#` to denote null children. * Your solution must handle edge cases like empty trees and highly unbalanced trees. * Node values are guaranteed to be integers. Input/Output Formats: * **Input**: - `serialize`: TreeNode root of the binary tree. - `deserialize`: A string representing the serialized binary tree * **Output**: - `serialize`: A string representing the tree. - `deserialize`: The reconstructed TreeNode root of the deserialized binary tree. Constraints: * The function should be efficient with respect to both time and space, ideally O(n) for both operations. * Node values in the tree are integers and do not exceed the range of standard integer representations. Example: ```python # Example Usage: # Creating a simple tree: # 1 # / # 2 3 # / # 4 5 root = TreeNode(1) root.left = TreeNode(2) root.right = TreeNode(3) root.right.left = TreeNode(4) root.right.right = TreeNode(5) serialized = serialize(root) # serialized should be: \\"1 2 # # 3 4 # # 5 # #\\" deserialized_root = deserialize(serialized) # Structure of deserialized_root should be the same as the original tree. # Comparing the tree structure to ensure correctness: assert serialize(deserialized_root) == serialized ``` Write the function implementations for `serialize` and `deserialize`.","solution":"class TreeNode: def __init__(self, val=0, left=None, right=None): self.val = val self.left = left self.right = right def serialize(root): Encodes a tree to a single string. def dfs(node): if not node: return \\"#\\" return f\\"{node.val} {dfs(node.left)} {dfs(node.right)}\\" return dfs(root) def deserialize(data): Decodes your encoded data to tree. def dfs(vals): if vals[0] == \'#\': vals.pop(0) return None node = TreeNode(int(vals.pop(0))) node.left = dfs(vals) node.right = dfs(vals) return node vals = data.split() return dfs(vals)"},{"question":"You are tasked with designing a function to process a list of words and replace segments of each word that match symbols from a given list. The replacement should be done by enclosing the matched segments with square brackets `[ ]`. If there are multiple possible matches for a segment within a word, the match with the longest length should be used. Write a function `replace_symbols_with_brackets(words, symbols)` that: 1. Takes a list of words and a list of symbols. 2. Replaces matching segments in each word with the longest matching symbol enclosed in square brackets `[ ]`. 3. Returns the list of words with the appropriate replacements. # Input Format - `words`: A list of `n` words (1 ≤ n ≤ 1000), where each word is a non-empty string of lowercase and uppercase English letters (1 ≤ length of each word ≤ 1000). - `symbols`: A list of `m` symbols (1 ≤ m ≤ 100), where each symbol is a non-empty string of lowercase and uppercase English letters (1 ≤ length of each symbol ≤ 50). # Output Format - A list of words with matched symbols enclosed in square brackets. # Constraints - If a word matches more than one symbol, replace the segment matched by the longest symbol. - If a word has multiple non-overlapping segments matching symbols of the same length, replace the first occurrence. - If no symbols match a word, the word remains unchanged. # Performance Requirements Ensure that your solution runs efficiently for the upper constraint limits provided. # Example Usage Input ```python words = [\'Amazon\', \'Microsoft\', \'Google\'] symbols = [\'i\', \'Am\', \'cro\', \'Na\', \'le\', \'abc\'] ``` Output ```python [\'[Am]azon\', \'Mi[cro]soft\', \'Goog[le]\'] ``` # Additional Notes Provide a well-documented and efficient implementation of the function.","solution":"def replace_symbols_with_brackets(words, symbols): Replaces segments of each word that match symbols from the symbols list with the longest matching symbol enclosed in square brackets []. :param words: List of words to process. :param symbols: List of symbols to match. :return: List of words with matched segments replaced. sorted_symbols = sorted(symbols, key=len, reverse=True) def replace_in_word(word): for symbol in sorted_symbols: start = word.find(symbol) if start != -1: end = start + len(symbol) return word[:start] + \'[\' + symbol + \']\' + word[end:] return word return [replace_in_word(word) for word in words]"},{"question":"# Problem Context You\'re working as a software engineer at a company that processes large datasets of sorted numerical data. One of your tasks is to implement a search feature that identifies the last occurrence of a specific number efficiently in these large datasets. # Objective Write a function `find_last_occurrence` which takes a sorted array `array` and a target value `target`. The function should return the index of the last occurrence of `target` within `array`. If `target` is not present in the array, return -1. # Function Signature ```python def find_last_occurrence(array: list[int], target: int) -> int: ``` # Input Format * `array` - List of integers sorted in non-decreasing order. * `target` - An integer representing the number to find in the array. # Output Format * Return the index (0-based) of the last occurrence of `target` in the array. If `target` is not found, return -1. # Constraints * The length of `array` is between 0 and (10^6). * The elements in `array` are between (-10^9) and (10^9). * Optimal time complexity is O(log n). # Example ```python # Example 1 array = [1, 2, 2, 2, 3, 4, 5] target = 2 # Output: 3 (last occurrence of 2 is at index 3) # Example 2 array = [1, 2, 3, 4, 5, 6, 7, 8] target = 9 # Output: -1 (9 is not present in the array) # Example 3 array = [1, 1, 1, 1, 1, 1, 1] target = 1 # Output: 6 (Last occurrence of 1 is at index 6) ``` # Additional Notes * Consider edge cases such as when the array is empty, the element is not present in the array, or all elements are the same. * Ensure the function has a runtime of O(log n) to handle large datasets efficiently.","solution":"def find_last_occurrence(array, target): Returns the index of the last occurrence of target in the array. If target is not present in the array, returns -1. low, high = 0, len(array) - 1 result = -1 while low <= high: mid = (low + high) // 2 if array[mid] == target: result = mid low = mid + 1 # continue searching in the right half elif array[mid] < target: low = mid + 1 else: high = mid - 1 return result"},{"question":"Scenario You have been tasked with analyzing and expanding a Markov Chain model used in a text generator. The generator transitions between words based on the probability distributions provided in the chain dictionary. Your job is to enhance this by implementing additional functionalities while ensuring reliability in the face of potential data issues. Problem Statement 1. Implement a function to validate a given Markov Chain data structure to ensure it meets the necessary probability distribution rules. 2. Extend the Markov Chain to produce a sequence of states of a fixed length, rather than an indefinite generator. # Function Specifications Function 1: `validate_chain(chain: dict) -> bool` * **Inputs**: * `chain`: Dictionary representing the Markov Chain. Keys are current states, values are dictionaries mapping to next states with their transition probabilities. * **Outputs**: * Returns `True` if the Markov Chain is valid; `False` otherwise. * **Validity Rules**: * All keys at each level must be valid states. * Sum of probabilities for each state must equal 1 (allowing minor floating-point tolerance). Function 2: `generate_states(chain: dict, start_state: str, length: int) -> list` * **Inputs**: * `chain`: Dictionary representing the Markov Chain. * `start_state`: The initial state from which the chain should start. * `length`: Desired number of states in the generated sequence. * **Outputs**: * Returns a list of states representing the generated sequence. * **Constraints**: * (1 leq text{length} leq 10^4) * Valid states in the chain are non-empty alphabetic strings. # Example ```python my_chain = { \\"A\\": {\\"A\\": 0.6, \\"B\\": 0.4}, \\"B\\": {\\"A\\": 0.7, \\"B\\": 0.3} } # validate_chain(my_chain) -> True # generate_states(my_chain, \\"A\\", 5) -> [\\"A\\", \\"B\\", \\"A\\", \\"A\\", \\"B\\"] ``` Ensure your solution handles common edge cases like the ones listed in the analysis and performs within acceptable time and space limits.","solution":"import random def validate_chain(chain): Validates a given Markov Chain to ensure it meets the necessary probability distribution rules. :param chain: Dictionary representing the Markov Chain. Keys are current states, values are dictionaries mapping to next states with their transition probabilities. :return: Returns True if the Markov Chain is valid; False otherwise. for state, transitions in chain.items(): if not isinstance(state, str) or not state.isalpha(): return False total_prob = 0.0 for next_state, prob in transitions.items(): if not isinstance(next_state, str) or not next_state.isalpha(): return False if not (0 <= prob <= 1): return False total_prob += prob if not (0.999 <= total_prob <= 1.001): # Allowing a small floating-point error tolerance return False return True def generate_states(chain, start_state, length): Generates a sequence of states of a fixed length using the Markov Chain. :param chain: Dictionary representing the Markov Chain. :param start_state: The initial state from which the chain should start. :param length: Desired number of states in the generated sequence. :return: Returns a list of states representing the generated sequence. current_state = start_state generated_states = [current_state] for _ in range(length - 1): next_states = list(chain[current_state].keys()) probabilities = list(chain[current_state].values()) current_state = random.choices(next_states, weights=probabilities, k=1)[0] generated_states.append(current_state) return generated_states"},{"question":"# Task You are given a string as input. Your task is to delete any reoccurrence of characters and return the new string that contains only the first occurrence of each character. # Input A single string `s` containing lowercase and/or uppercase alphabetical characters. # Output A new string containing only the first occurrence of each character from the input string `s`, maintaining the order of their first appearance. # Example Input: ``` \\"Google\\" ``` Output: ``` \\"Gogle\\" ``` # Constraints 1. The string `s` will have at most length `10^6`. # Guidelines * Ensure your solution handles edge cases like empty strings and strings with all identical characters. * Aim for a linear time complexity solution to efficiently handle large input strings. * The function signature should look like this: ```python def delete_reoccurring_characters(s: str) -> str: pass ``` # Implementation Implement the `delete_reoccurring_characters` function based on the above guidelines and specifications.","solution":"def delete_reoccurring_characters(s: str) -> str: Returns a new string containing only the first occurrence of each character from the original string `s`. seen = set() result = [] for char in s: if char not in seen: seen.add(char) result.append(char) return \'\'.join(result)"},{"question":"Problem Statement You are given two binary trees `s` and `t`. Write a function `is_subtree(s, t)` that returns `True` if `t` is a subtree of `s`, and `False` otherwise. A subtree of a tree `s` is a tree consisting of a node in `s` and all of its descendants in `s`. The function must traverse the tree `s` and find if there is any node in `s` such that the subtree rooted at that node is identical to tree `t`. Function Signature ```python def is_subtree(s: \'TreeNode\', t: \'TreeNode\') -> bool: ``` Input - `s` (TreeNode): The root node of the primary tree. - `t` (TreeNode): The root node of the subtree to check for. Output - `bool`: `True` if `t` is a subtree of `s`, else `False`. Constraints - The number of nodes in both trees is in the range `[0, 2000]`. - Each node\'s value is a 32-bit signed integer. Example **Example 1:** - Input: ```python s = TreeNode(3, TreeNode(4, TreeNode(1), TreeNode(2)), TreeNode(5)) t = TreeNode(4, TreeNode(1), TreeNode(2)) ``` - Output: `True` **Example 2:** - Input: ```python s = TreeNode(3, TreeNode(4, TreeNode(1), TreeNode(2, TreeNode(0))), TreeNode(5)) t = TreeNode(3, TreeNode(4, TreeNode(1), TreeNode(2))) ``` - Output: `False` Hints: 1. Consider edge cases such as when either `s` or `t` is `None`. 2. Optimal performance may involve both BFS (for finding the potential starting points) and DFS (for comparing the subtrees). Implementation Plan: 1. Implement a helper function `comp(p, q)` to compare two subtrees `p` and `q` for equality. 2. Implement the main function `is_subtree(s, t)` to use a queue for BFS traversal of tree `s`. 3. When a node in `s` matches the root of `t`, call the helper function to check subtree equality. 4. Return `True` if a matching subtree is found, otherwise return `False`.","solution":"class TreeNode: def __init__(self, val=0, left=None, right=None): self.val = val self.left = left self.right = right def is_subtree(s: TreeNode, t: TreeNode) -> bool: if not t: return True # An empty tree is always a subtree if not s: return False # Non-empty t can\'t be a subtree of an empty s def are_identical(s, t): if not s and not t: return True if not s or not t: return False if s.val != t.val: return False return are_identical(s.left, t.left) and are_identical(s.right, t.right) if are_identical(s, t): return True return is_subtree(s.left, t) or is_subtree(s.right, t)"},{"question":"You are provided with a function signature for generating permutations of a given list of distinct integers. Your task is to implement this function using an efficient recursive approach. The function should generate all possible permutations of the input list and return them as a list of lists. # Function Signature ```python def generate_permutations(nums: List[int]) -> List[List[int]]: pass ``` # Input - `nums`: A list of distinct integers (1 <= len(nums) <= 9). # Output - Returns a list containing all possible permutations of `nums`. # Constraints - The input list will contain only distinct integers. - The length of the list will be between 1 and 9 inclusive. # Examples ```python # Example 1 input = [1, 2, 3] output = [ [1, 2, 3], [1, 3, 2], [2, 1, 3], [2, 3, 1], [3, 1, 2], [3, 2, 1], ] # Example 2 input = [0, 1] output = [ [0, 1], [1, 0], ] # Example 3 input = [1] output = [ [1], ] ``` # Performance Requirements The function should be implemented efficiently to handle the maximum input size within a reasonable time frame. # Additional Information Edge cases to consider: - The input list could be empty. - The input list could have just one element.","solution":"from typing import List def generate_permutations(nums: List[int]) -> List[List[int]]: def backtrack(first=0): if first == n: perms.append(nums[:]) for i in range(first, n): nums[first], nums[i] = nums[i], nums[first] backtrack(first + 1) nums[first], nums[i] = nums[i], nums[first] n = len(nums) perms = [] backtrack() return perms"},{"question":"# Scenario You\'ve been hired to develop part of a lexical analyzer that uses DFAs to decide whether specific tokens belong to a predefined language. To implement this, you need to verify if given input strings are accepted by a DFA with a specified transition structure. # Task Write a function `is_accepted_by_dfa(transitions: dict, start: str, final: set, string: str) -> bool` that determines if a given string is accepted by the DFA described by its transition table. # Function Signature ```python def is_accepted_by_dfa(transitions: dict, start: str, final: set, string: str) -> bool: ``` # Input * `transitions`: A dictionary where the keys are states and the values are dictionaries mapping characters to next states. - Example: `{ \'A\': {\'0\': \'A\', \'1\': \'B\'}, \'B\': {\'0\': \'A\', \'1\': \'B\'} }` * `start`: A string representing the start state of the DFA. - Example: `\'A\'` * `final`: A set of strings representing the accepting (final) states of the DFA. - Example: `{\'B\'}` * `string`: A string which needs to be checked against the DFA. - Example: `\'0101\'` # Output Return `True` if the DFA accepts the input string, `False` otherwise. # Constraints * The input string can be of length 0-1000. * State names and symbols are single characters. * The transitions dictionary is guaranteed to be well-formed (i.e., no missing transition dictionaries for states). # Example ```python transitions = { \'A\': {\'0\': \'A\', \'1\': \'B\'}, \'B\': {\'0\': \'A\', \'1\': \'B\'} } start = \'A\' final = {\'B\'} string = \'0101\' is_accepted_by_dfa(transitions, start, final, string) # returns True ``` # Edge cases * An empty string should be checked based on the start state. * Any character in the string that is not present in the transition table should be handled gracefully.","solution":"def is_accepted_by_dfa(transitions, start, final, string): Determines if a given string is accepted by a DFA. current_state = start for char in string: if char in transitions[current_state]: current_state = transitions[current_state][char] else: return False return current_state in final"},{"question":"# Objective: Implement the heap sort algorithm for an array of integers. Your implementation should allow sorting in both ascending and descending order. Create functions for both max-heap sort and min-heap sort. Your solutions should be efficient with a time complexity of O(n log n) and should sort in-place. # Function Signatures: ```python def max_heap_sort(arr: List[int]) -> List[int]: pass def min_heap_sort(arr: List[int]) -> List[int]: pass ``` # Constraints: * The input array `arr` will have a length of up to `10^5`. * The elements of the input array `arr` will be integers within the range `-10^6` to `10^6`. # Input Format: * A list of integers `arr`. # Output Format: * The `max_heap_sort` function should return the array sorted in ascending order. * The `min_heap_sort` function should return the array sorted in ascending order. # Requirements: 1. **Max Heap Sort**: - Build a max heap from the array. - Extract the maximum element one by one to build a sorted array in ascending order. 2. **Min Heap Sort**: - Build a min heap from the array. - Extract the minimum element one by one to build a sorted array in ascending order. # Edge Cases to Consider: 1. Empty array should return an empty array. 2. Arrays with a single element should return the same array. 3. Handle arrays with all identical elements efficiently. # Example: ```python # Example 1: arr = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5] print(max_heap_sort(arr)) # Expected Output: [1, 1, 2, 3, 3, 4, 5, 5, 5, 6, 9] # Example 2: arr = [5, 7, 3, 1, 2, 8, 6, 4] print(min_heap_sort(arr)) # Expected Output: [1, 2, 3, 4, 5, 6, 7, 8] ``` Ensure your implementation is efficient and correctly handles the edge cases provided. Your solution should demonstrate a thorough understanding of the workings of the heap sort algorithm.","solution":"from typing import List def max_heapify(arr: List[int], n: int, i: int): largest = i left = 2 * i + 1 right = 2 * i + 2 if left < n and arr[left] > arr[largest]: largest = left if right < n and arr[right] > arr[largest]: largest = right if largest != i: arr[i], arr[largest] = arr[largest], arr[i] max_heapify(arr, n, largest) def min_heapify(arr: List[int], n: int, i: int): smallest = i left = 2 * i + 1 right = 2 * i + 2 if left < n and arr[left] < arr[smallest]: smallest = left if right < n and arr[right] < arr[smallest]: smallest = right if smallest != i: arr[i], arr[smallest] = arr[smallest], arr[i] min_heapify(arr, n, smallest) def max_heap_sort(arr: List[int]) -> List[int]: n = len(arr) for i in range(n // 2 - 1, -1, -1): max_heapify(arr, n, i) for i in range(n-1, 0, -1): arr[i], arr[0] = arr[0], arr[i] max_heapify(arr, i, 0) return arr def min_heap_sort(arr: List[int]) -> List[int]: n = len(arr) for i in range(n // 2 - 1, -1, -1): min_heapify(arr, n, i) for i in range(n-1, 0, -1): arr[i], arr[0] = arr[0], arr[i] min_heapify(arr, i, 0) arr.reverse() return arr"},{"question":"Scenario: You are given a task to generate all possible combinations of factors for a given positive integer `n` such that each combination, when multiplied, equals the number `n`. You need to write both an iterative and a recursive approach to achieve this. Task: Implement two functions: 1. `get_factors(n: int) -> List[List[int]]`: Iterative approach to find the factor combinations. 2. `recursive_get_factors(n: int) -> List[List[int]]`: Recursive approach to find the factor combinations. Requirements: - Your functions should return a list of lists, where each list contains integers representing a multiplication combination of factors for `n`. - Factors should be greater than 1 and less than `n`. - Order of factors within each combination and the order of combinations in the final list does not matter. Input: - `n` (integer): A positive integer (1 ≤ n ≤ 10^5) Output: - A list of lists containing integer combinations of factors. Constraints: - The number `n` is always positive. Examples: ```python get_factors(1) # Output: [] get_factors(37) # Output: [] get_factors(12) # Output: [ # [2, 6], # [2, 2, 3], # [3, 4] # ] get_factors(32) # Output: [ # [2, 16], # [2, 2, 8], # [2, 2, 2, 4], # [2, 2, 2, 2, 2], # [2, 4, 4], # [4, 8] # ] ```","solution":"from typing import List def get_factors(n: int) -> List[List[int]]: Iterative approach to find the factor combinations of n. if n <= 1: return [] result = [] stack = [(n, 2, [])] while stack: current_num, current_divisor, path = stack.pop() while current_divisor * current_divisor <= current_num: if current_num % current_divisor == 0: result.append(path + [current_divisor, current_num // current_divisor]) stack.append((current_num // current_divisor, current_divisor, path + [current_divisor])) current_divisor += 1 return result def recursive_get_factors(n: int) -> List[List[int]]: Recursive approach to find the factor combinations of n. def helper(n, start): result = [] for i in range(start, int(n**0.5) + 1): if n % i == 0: result.append([i, n // i]) for rest in helper(n // i, i): result.append([i] + rest) return result if n <= 1: return [] return helper(n, 2)"},{"question":"# Polynomial Manipulation Challenge You are given a Monomial and Polynomial class that can perform various arithmetic operations such as addition, subtraction, multiplication, and division. Your task is to implement a function `simplify_polynomial` which takes a polynomial represented as a list of coefficient-variable pairs and returns the simplified version of the polynomial after performing arithmetic manipulations. The polynomial might include nested operations which need to be simplified and flattened. # Input: - A list of monomials where each monomial is represented as a dictionary with: - `variables`: a dictionary mapping variable indices to their integer exponents. - `coeff`: an integer, float, or Fraction representing the coefficient of the monomial. # Output: - A simplified polynomial represented as a string. # Constraints: - The coefficients can be given as integers, floats, or rational fractions. - The variables and exponents are non-negative integers. # Example: ```python monomials = [ {\\"variables\\": {1: 1}, \\"coeff\\": 2}, {\\"variables\\": {2: 3, 1: -1}, \\"coeff\\": -1}, {\\"variables\\": {}, \\"coeff\\": 3.14}, {\\"variables\\": {}, \\"coeff\\": Fraction(-1, 4)} ] simplified_expression = simplify_polynomial(monomials) print(simplified_expression) # Expected: \\"2(a_1) + (-1)(a_2)^3(a_1)^(-1) + 3.14 + (-1/4)\\" ``` # Requirements: 1. Implement the `simplify_polynomial` function. 2. Ensure that the string representation of the simplified polynomial adheres to the provided formatting in the above example. # Notes: - Handle rational fractions correctly. - Variables should be represented as (a_i) where (i) is the variable index. - The simplified polynomial should combine like terms.","solution":"from fractions import Fraction def format_monomial(monomial): Returns a string representation of a single monomial. parts = [] coeff = monomial[\\"coeff\\"] if isinstance(coeff, Fraction): coeff_str = f\'{coeff}\' elif isinstance(coeff, float): if coeff.is_integer(): coeff_str = f\'{int(coeff)}\' else: coeff_str = f\'{coeff:.2f}\' else: coeff_str = f\'{coeff}\' parts.append(coeff_str) for var, exp in sorted(monomial[\\"variables\\"].items()): if exp == 0: continue var_str = f\'a_{var}\' if exp != 1: var_str += f\'^{exp}\' parts.append(var_str) return \\"(\\" + \\" * \\".join(parts) + \\")\\" def simplify_polynomial(monomials): Simplifies the given polynomial and returns a string representation. simplified_terms = [] for monomial in monomials: simplified_terms.append(format_monomial(monomial)) return \' + \'.join(simplified_terms)"},{"question":"# Question: Serialize and Deserialize N-ary Tree Given the following class for an N-ary tree node: ```python class Node: def __init__(self, val=None, children=None): self.val = val self.children = children if children is not None else [] ``` Write two functions, `serialize_nary(root: \'Node\') -> str` and `deserialize_nary(data: str) -> \'Node\'`, where: - `serialize_nary` converts an N-ary tree into a string representation. - `deserialize_nary` converts the string representation back into the original N-ary tree. # Requirements: 1. The serialized string should use spaces as delimiters between node values. 2. Use \\"#\\" to represent a null node. 3. The order of the children in the serialized string should reflect their order in the input tree. 4. The functions should handle edge cases such as empty trees and trees with varying numbers of children at different levels. Example Given an N-ary tree: ``` 1 / | 3 2 4 / 5 6 ``` The `serialize_nary` function should produce the following string: ``` \\"1 3 5 # # 6 # # 2 # # 4 # #\\" ``` The `deserialize_nary` function should reverse this process, reconstructing the original tree. # Constraints - The number of nodes in the tree is at most 10^4. - Node values are integers in the range [1, 10^4]. Complete the implementation of the functions while ensuring efficiency in both serialization and deserialization processes. ```python def serialize_nary(root: \'Node\') -> str: # implement the function here pass def deserialize_nary(data: str) -> \'Node\': # implement the function here pass ```","solution":"class Node: def __init__(self, val=None, children=None): self.val = val self.children = children if children is not None else [] def serialize_nary(root: \'Node\') -> str: if not root: return \\"\\" result = [] def dfs(node: \'Node\'): if not node: return result.append(str(node.val)) for child in node.children: dfs(child) result.append(\\"#\\") dfs(root) return \\" \\".join(result) def deserialize_nary(data: str) -> \'Node\': if not data: return None tokens = iter(data.split()) root_val = int(next(tokens)) root = Node(root_val) stack = [(root, [])] for token in tokens: if token == \\"#\\": node, children = stack.pop() node.children = children else: node_val = int(token) node = Node(node_val) stack[-1][1].append(node) stack.append((node, [])) return root"},{"question":"**Scenario**: You are developing a tool to analyze changes in binary data. One of the features is to determine the minimal number of bit flips required to change one binary number to another. This is crucial for assessing the complexity of certain transformation algorithms. **Problem Statement**: Write a function `count_flips_to_convert(a: int, b: int) -> int` that takes in two integers `a` and `b`, and returns the minimal number of bits you need to flip to convert integer `a` to integer `b`. # Input Format - Two non-negative integers `a` and `b`. # Output Format - An integer representing the number of bit flips required. # Constraints - 0 ≤ `a`, `b` ≤ (2^{31}-1) (i.e., standard 32-bit signed integer range without the sign bit). # Examples 1. Input: `a = 29`, `b = 15` Output: `2` Explanation: Binary representation of `29` is `11101` and `15` is `01111`; flipping the 1st and 3rd bits of `29` transforms it into `15`. 2. Input: `a = 7`, `b = 10` Output: `3` Explanation: Binary representation of `7` is `0111` and `10` is `1010`; flipping the 1st, 2nd, and 4th bits of `7` transforms it into `10`. # Performance Requirements - Your solution should have a time complexity of O(k), where k is the number of bits in the binary representation of the input integers. - Aim for a space complexity of O(1). **Note**: Handle edge cases such as both integers being zero, very large integers close to the upper limit, and minimal differences efficiently.","solution":"def count_flips_to_convert(a: int, b: int) -> int: Returns the minimal number of bit flips required to convert integer a to integer b. # XOR the two numbers to find positions where they differ xor = a ^ b # Count the number of 1s in the XOR result (which indicates differing bits) count = 0 while xor: count += xor & 1 # Increment count if the least significant bit is 1 xor >>= 1 # Shift the bits right return count"},{"question":"# Intersection of Linked Lists **Objective**: Write a function that finds the intersection node of two singly linked lists. **Context**: In merged linked list structures, it\'s common to encounter situations where two lists share some common nodes. For instance, in genealogy trees or complex network paths, nodes representing shared ancestry or shared routers can appear. Your task is to identify the first shared node (if any) between two linked lists. **Function Signature**: ```python def intersection(head1: Node, head2: Node) -> Node: pass ``` # Input: - `head1` (Node): The head of the first linked list. - `head2` (Node): The head of the second linked list. # Output: - Return the intersection node if an intersection exists; otherwise, return `None`. # Constraints: 1. The linked lists are non-cyclical. 2. Nodes are compared by reference, not value. 3. Node is defined as follows: ```python class Node: def __init__(self, val=None): self.val = val self.next = None ``` # Example: ```python # create linked list as: # 1 -> 3 -> 5 # # 7 -> 9 -> 11 # / # 2 -> 4 -> 6 a1 = Node(1) b1 = Node(3) c1 = Node(5) d = Node(7) a2 = Node(2) b2 = Node(4) c2 = Node(6) e = Node(9) f = Node(11) a1.next = b1 b1.next = c1 c1.next = d a2.next = b2 b2.next = c2 c2.next = d d.next = e e.next = f result = intersection(a1, a2) if result is not None: print(result.val) # Output should be 7 else: print(\'No intersection\') ``` # Constraints & Performance: - Your solution should efficiently handle lists with up to 10^5 nodes. - Aim for a solution with a time complexity of O(N + M) and space complexity of O(1). **Note**: Ensure to include null pointer checks and proper handling of edge cases, such as empty lists or lists without intersections.","solution":"class Node: def __init__(self, val=None): self.val = val self.next = None def intersection(head1: Node, head2: Node) -> Node: if not head1 or not head2: return None # Get the lengths of both linked lists len1, len2 = 0, 0 current1, current2 = head1, head2 while current1: len1 += 1 current1 = current1.next while current2: len2 += 1 current2 = current2.next # Align them to the same start point current1, current2 = head1, head2 if len1 > len2: for _ in range(len1 - len2): current1 = current1.next else: for _ in range(len2 - len1): current2 = current2.next # Find the intersection while current1 and current2: if current1 == current2: return current1 current1 = current1.next current2 = current2.next return None"},{"question":"Objective You are required to implement functionality to enable Depth-First Search (DFS) traversal on the provided DirectedGraph class. DFS will allow traversing the graph starting from a specified node, visiting all reachable nodes. Instructions 1. Implement a method `depth_first_search` inside the `DirectedGraph` class. This method should perform a DFS traversal starting from a given starting node and return a list of nodes in the order they are visited. 2. Ensure cycle detection and handling during traversal, so nodes are not revisited. Method Signature ```python class DirectedGraph: # ... existing methods def depth_first_search(self, start_node: str) -> List[str]: # Your code here ``` Input - `start_node`: A string representing the name of the node to start the DFS traversal. Output - A list of strings representing the names of nodes in the order they were visited during DFS traversal starting from `start_node`. Constraints - The graph can have up to 10^3 nodes and 10^4 edges. - Node names are unique and consist of only alphanumeric characters. Example Assume you have the following graph represented as adjacency list: ```python load_dict = { \\"A\\": [\\"B\\", \\"C\\"], \\"B\\": [\\"D\\"], \\"C\\": [\\"D\\", \\"E\\"], \\"D\\": [], \\"E\\": [\\"F\\"], \\"F\\": [] } ``` Creating the graph: ```python graph = DirectedGraph(load_dict) ``` If you call `depth_first_search` with starting node \'A\': ```python result = graph.depth_first_search(\\"A\\") ``` Possible valid outputs for DFS traversal from node \'A\' would be: ``` [\'A\', \'B\', \'D\', \'C\', \'E\', \'F\'] or [\'A\', \'C\', \'E\', \'F\', \'D\', \'B\'] (depending on implementation specifics) ``` Additional Notes - Handle situations where the starting node does not exist by raising an appropriate error. - The DFS implementation must avoid revisiting nodes to prevent infinite loops in cyclic graphs. - You may create helper functions or use iterative/recursive approaches as needed.","solution":"from typing import List, Dict class DirectedGraph: def __init__(self, adj_list: Dict[str, List[str]]): self.adj_list = adj_list def depth_first_search(self, start_node: str) -> List[str]: if start_node not in self.adj_list: raise ValueError(\\"Start node {} not found in the graph\\".format(start_node)) visited = set() result = [] def dfs(node): if node in visited: return visited.add(node) result.append(node) for neighbor in self.adj_list.get(node, []): dfs(neighbor) dfs(start_node) return result"},{"question":"# Context You are a developer tasked with optimizing search functions in a large, dynamically generated, and sorted database. Your task is to write an efficient function to find the starting and ending positions of a target value in a sorted list of integers. # Challenge Write a function `search_range(nums, target)` to find the starting and ending position of a given target value in a sorted list. The function should efficiently locate both the starting and ending positions of the target using an optimized approach. # Function Signature ```python def search_range(nums: List[int], target: int) -> List[int]: pass ``` # Input 1. `nums`: A list of integers sorted in ascending order. (1 ≤ len(nums) ≤ 10^5) 2. `target`: An integer representing the target value to search for. # Output - Returns a list containing two integers: - The first element is the starting position. - The second element is the ending position of the target value. - If the target is not found, return `[-1, -1]`. # Constraints - The solution should have an overall time complexity of `O(log n + log n)`. - Do not use a linear scan for finding the ending position. # Examples ```python search_range([5, 7, 7, 8, 8, 8, 10], 8) # Output: [3, 5] search_range([5, 7, 7, 8, 8, 8, 10], 6) # Output: [-1, -1] search_range([5, 7, 7, 8, 8, 8, 10], 10) # Output: [6, 6] search_range([], 1) # Output: [-1, -1] ``` # Specific Instructions - Implement the `search_range` such that it utilizes binary search to find both starting and ending positions efficiently. - Ensure the function handles all potential edge cases like: - An empty list. - A list with one element. - The target not present in the list. - The entire list having the same but different elements from the target.","solution":"from typing import List def search_range(nums: List[int], target: int) -> List[int]: def find_first(nums, target): left, right = 0, len(nums) - 1 while left <= right: mid = (left + right) // 2 if nums[mid] < target: left = mid + 1 else: right = mid - 1 return left def find_last(nums, target): left, right = 0, len(nums) - 1 while left <= right: mid = (left + right) // 2 if nums[mid] > target: right = mid - 1 else: left = mid + 1 return right start = find_first(nums, target) end = find_last(nums, target) if start <= end and start < len(nums) and nums[start] == target and nums[end] == target: return [start, end] else: return [-1, -1]"},{"question":"You need to implement a function that performs Cholesky decomposition on a given Hermitian positive-definite matrix. This decomposition is useful for solving linear equations of the form Ax = b and other numerical methods. # Function Signature ```python def cholesky_decomposition(A: List[List[float]]) -> List[List[float]]: ``` # Input - `A`: A list of lists representing an n x n Hermitian positive-definite matrix where each element is of type float. # Output - If the decomposition is possible, return a list of lists representing a lower triangular matrix V such that `V * V* = A`. - If the decomposition is not possible, return `None`. # Constraints - 1 <= n <= 1000 - Each element in the matrix A, -10^3 <= A[i][j] <= 10^3 # Examples Example 1: **Input:** ```python A = [ [4, 12, -16], [12, 37, -43], [-16, -43, 98] ] ``` **Output:** ```python [ [2.0, 0.0, 0.0], [6.0, 1.0, 0.0], [-8.0, 5.0, 3.0] ] ``` Example 2: **Input:** ```python A = [ [1, 1], [1, 1] ] ``` **Output:** ```python None ``` # Explanation In **Example 1**, the matrix A is a positive-definite matrix and can be decomposed into a lower triangular matrix V. In **Example 2**, the matrix A is not positive-definite, hence the decomposition cannot be performed, and the function should return `None`. # Notes - Ensure your implementation checks for common edge cases such as non-positive definite matrices.","solution":"def cholesky_decomposition(A): Performs Cholesky decomposition on a given Hermitian positive-definite matrix A. Arguments: A -- A list of lists representing an n x n Hermitian positive-definite matrix where each element is of type float. Returns: A lower triangular matrix V such that V * V.T = A, or None if decomposition is not possible. import math n = len(A) # Initialize the result matrix with zeros L = [[0.0] * n for _ in range(n)] for i in range(n): for j in range(i + 1): sum_val = sum(L[i][k] * L[j][k] for k in range(j)) if i == j: # Elements on the diagonal val = A[i][i] - sum_val if val <= 0: # Matrix is not positive-definite return None L[i][j] = math.sqrt(val) else: if L[j][j] == 0: # To avoid division by zero return None L[i][j] = (A[i][j] - sum_val) / L[j][j] return L"},{"question":"Scenario You are developing a text analyzer tool that includes functionality to validate if a given paragraph or text block contains every letter of the English alphabet at least once. This feature will help ensure that the text block is diverse and exhaustive in using the English language. Task Write a Python function called `is_pangram` that takes a single string as input and returns `True` if the string is a pangram (contains every letter of the alphabet at least once), and `False` otherwise. Function Signature ```python def is_pangram(input_string: str) -> bool: ``` Input Format * `input_string` (a string, 1 ≤ len(input_string) ≤ 10^4): The text to be checked, containing alphabetic and possibly other characters. Output Format * A single boolean value: `True` if the input string is a pangram, `False` otherwise. Constraints * The function should handle both uppercase and lowercase letters. * The characters being checked are strictly from the English alphabet (a-z). Example ```python print(is_pangram(\\"The quick brown fox jumps over the lazy dog\\")) # Output: True print(is_pangram(\\"Hello world!\\")) # Output: False ``` # Additional Notes * Be sure to consider the performance of your function especially for longer input strings. * Think about optimal ways to minimize unnecessary checks and make your solution as efficient as possible.","solution":"def is_pangram(input_string: str) -> bool: Checks if the input string is a pangram. A pangram is a sentence that contains every letter of the English alphabet at least once. Args: input_string (str): The text to be checked. Returns: bool: True if the input string is a pangram, False otherwise. alphabet_set = set(\'abcdefghijklmnopqrstuvwxyz\') input_set = set(input_string.lower()) return alphabet_set <= input_set"},{"question":"# Question: Implement an Enhanced Bucket Sort with Median Selection Description You have been given the task to implement an enhanced version of bucket sort. This version will improve upon the basic bucket sort by dynamically determining the bucket size and using a quicksort implementation instead of insertion sort for sorting within each bucket. Additionally, to optimize the bucket size, determine the median of the input array and use it to split the array into two halves, then recursively apply the bucket sort on these halves. Requirements: 1. **Function Signature**: ```python def enhanced_bucket_sort(arr: List[int]) -> List[int]: ``` 2. **Input**: - A list of integers `arr` where `1 <= len(arr) <= 10^5` and `0 <= arr[i] <= 10^6`. 3. **Output**: - A sorted list of integers. 4. **Constraints**: - Optimize the sorting procedure to handle large input efficiently. - Ensure you handle edge cases such as when all elements are the same. - Use median selection for bucket division to balance the distribution. # Example ```python arr = [4, 2, 7, 1, 3, 2] enhanced_bucket_sort(arr) # Should return [1, 2, 2, 3, 4, 7] ``` Specific Points to Address 1. Implement median selection for dividing the array. 2. Use a more efficient sorting algorithm for sorting within each bucket. 3. Consider edge cases and optimize performance for non-uniform distributions.","solution":"from typing import List def quicksort(arr: List[int]) -> List[int]: if len(arr) <= 1: return arr pivot = arr[len(arr) // 2] left = [x for x in arr if x < pivot] middle = [x for x in arr if x == pivot] right = [x for x in arr if x > pivot] return quicksort(left) + middle + quicksort(right) def median_of_medians(arr: List[int], k: int) -> int: if len(arr) <= 5: return sorted(arr)[k] medians = [] for i in range(0, len(arr), 5): chunk = arr[i:i + 5] medians.append(sorted(chunk)[len(chunk) // 2]) pivot = median_of_medians(medians, len(medians) // 2) low = [x for x in arr if x < pivot] high = [x for x in arr if x > pivot] k_pivot = arr.count(pivot) if k < len(low): return median_of_medians(low, k) elif k < len(low) + k_pivot: return pivot else: return median_of_medians(high, k - len(low) - k_pivot) def enhanced_bucket_sort(arr: List[int]) -> List[int]: if len(arr) <= 1: return arr median = median_of_medians(arr, len(arr) // 2) left_bucket = [] right_bucket = [] equal_bucket = [] for num in arr: if num < median: left_bucket.append(num) elif num == median: equal_bucket.append(num) else: right_bucket.append(num) return enhanced_bucket_sort(left_bucket) + equal_bucket + enhanced_bucket_sort(right_bucket)"},{"question":"Least Common Multiple Finder Objective Write a Python function `find_lcm_multiple(numbers: List[int]) -> int` that computes the least common multiple (LCM) of a list of integers. This function should internally use the GCD algorithm implemented with bitwise operations for optimal performance. Input * `numbers`: A list of non-negative integers, where `2 <= len(numbers) <= 100` and `0 <= numbers[i] <= 10^9`. Output * Returns an integer representing the least common multiple of all integers in the list. Constraints * You may assume that the list will have at least two integers. * The function should handle large integers efficiently. Function Signature ```python from typing import List def find_lcm_multiple(numbers: List[int]) -> int: pass ``` # Requirements 1. **Input Validation**: * Ensure the input is a list of non-negative integers. * Handle cases where the list may contain a zero by raising a ValueError. 2. **Efficiency**: * Make use of the `gcd_bit` function for the GCD operations to enhance performance. 3. **Correctness**: * Ensure the function computes the LCM accurately for any valid list of integers provided. # Examples ```python print(find_lcm_multiple([4, 6])) # Output: 12 print(find_lcm_multiple([12, 15, 20])) # Output: 60 print(find_lcm_multiple([5, 0, 10])) # Raises ValueError ``` # Hint To compute the LCM of more than two numbers, you can use the LCM property in pairs: * lcm(a, b, c) = lcm(lcm(a, b), c)","solution":"from typing import List from functools import reduce def gcd_bit(a: int, b: int) -> int: if a == 0: return b if b == 0: return a # Finding common factors of 2 shift = 0 while ((a | b) & 1) == 0: a >>= 1 b >>= 1 shift += 1 while (a & 1) == 0: a >>= 1 while b != 0: while (b & 1) == 0: b >>= 1 if a > b: a, b = b, a b -= a return a << shift def lcm(a: int, b: int) -> int: return a * b // gcd_bit(a, b) def find_lcm_multiple(numbers: List[int]) -> int: if not all(isinstance(n, int) and n >= 0 for n in numbers): raise ValueError(\\"All elements in the list should be non-negative integers.\\") if any(n == 0 for n in numbers): raise ValueError(\\"Zero is not allowed in the list as it would make the LCM zero.\\") return reduce(lcm, numbers)"},{"question":"# Objective Implement a `WordDictionary` class to manage a collection of words and support efficient search operations, even with wildcard characters. # Given Code ```python import collections class TrieNode(object): def __init__(self, letter, is_terminal=False): self.children = dict() self.letter = letter self.is_terminal = is_terminal ``` # Instructions 1. Implement the `WordDictionary` class with the following methods: - `add_word(word: str) -> None`: Adds a word to the data structure. - `search(word: str) -> bool`: Searches for a word in the data structure, supporting the \'.\' wildcard character which can match any letter. 2. **Input Formats**: - **add_word**: A single string `word` containing only lowercase letters ([a-z]). - **search**: A single string `word` which may contain lowercase letters and/or the wildcard character \'.\'. 3. **Output Formats**: - `add_word` does not return anything. - `search` returns a boolean value indicating whether the word (or regex-like pattern) exists in the data structure. 4. **Constraints**: - Word lengths are between 1 and 100. - The total number of `add_word` and `search` calls will not exceed 10000. - Optimal performance is expected, particularly for the search method. # Example: ```python # Example Use word_dict = WordDictionary() word_dict.add_word(\\"bad\\") word_dict.add_word(\\"dad\\") word_dict.add_word(\\"mad\\") assert word_dict.search(\\"pad\\") == False assert word_dict.search(\\"bad\\") == True assert word_dict.search(\\".ad\\") == True assert word_dict.search(\\"b..\\") == True ``` # Additional Notes: 1. Pay attention to edge cases, such as searching for an empty string or searching using only wildcard characters. 2. Optimize the search method to handle worst-case scenarios effectively, noting that recursion depth and performance can vary based on the implementation.","solution":"class TrieNode: def __init__(self): self.children = {} self.is_terminal = False class WordDictionary: def __init__(self): self.root = TrieNode() def add_word(self, word: str) -> None: node = self.root for char in word: if char not in node.children: node.children[char] = TrieNode() node = node.children[char] node.is_terminal = True def search(self, word: str) -> bool: return self._search_in_node(word, self.root) def _search_in_node(self, word: str, node: TrieNode) -> bool: for i, char in enumerate(word): if char == \'.\': for child in node.children.values(): if self._search_in_node(word[i+1:], child): return True return False else: if char not in node.children: return False node = node.children[char] return node.is_terminal"},{"question":"**Coding Question: Minimum Deletions to Make Two Strings Equal** # Problem Statement: You are provided with two strings `word1` and `word2`. Your task is to compute the minimum number of steps required to make both strings equal by deleting characters from either string. # Function Signature: ```python def min_deletions_to_equal(word1: str, word2: str) -> int: # your code here ``` # Input: * Two strings `word1` and `word2`. # Output: * An integer representing the minimum number of deletions required. # Example: ```python print(min_deletions_to_equal(\\"sea\\", \\"eat\\")) # Output: 2 ``` # Constraints: - Both `word1` and `word2` are composed of lowercase English letters. - The length of `word1` and `word2` will not exceed 1000. # Requirements: - Implement a solution using dynamic programming with a time complexity of O(n*m) and space complexity of O(n*m) - Consider edge cases like empty strings and identical strings. - Provide proper initialization and handling of possible scenarios to prevent errors. # Special Note: Your implementation must be efficient and handle larger inputs within reasonable execution times.","solution":"def min_deletions_to_equal(word1: str, word2: str) -> int: # Find the length of the Longest Common Subsequence (LCS) using dynamic programming len1, len2 = len(word1), len(word2) dp = [[0] * (len2 + 1) for _ in range(len1 + 1)] for i in range(1, len1 + 1): for j in range(1, len2 + 1): if word1[i - 1] == word2[j - 1]: dp[i][j] = dp[i - 1][j - 1] + 1 else: dp[i][j] = max(dp[i - 1][j], dp[i][j - 1]) lcs_length = dp[len1][len2] # The minimum deletions needed to make two strings equal # is the sum of the extra characters in both strings min_deletions = (len1 - lcs_length) + (len2 - lcs_length) return min_deletions"},{"question":"Merge Strings Context: You are given a problem where you need to determine if a string `s` can be formed by interleaving the characters from two other strings `part1` and `part2`. The characters from `part1` and `part2` must appear in the same order in which they appear in `s`, but they can be interspersed with each other. Requirements: Write a function that verifies if the string `s` can be formed from `part1` and `part2` as per the rules mentioned. Function Signature: ```python def is_merge(s: str, part1: str, part2: str) -> bool: pass ``` Input: - `s`: A string that you need to form. - `part1`: A string whose characters may form part of `s`. - `part2`: A string whose characters may form part of `s`. Output: - Return `True` if `s` can be formed by interleaving `part1` and `part2`, otherwise return `False`. Constraints: - You cannot mutate `part1` and `part2`; you can only read their characters. - You must preserve the order of characters from `part1` and `part2` in `s`. Examples: ```python # Example 1: s = \\"codewars\\" part1 = \\"cdw\\" part2 = \\"oears\\" is_merge(s, part1, part2) # Returns: True # Example 2: s = \\"abc\\" part1 = \\"ab\\" part2 = \\"bc\\" is_merge(s, part1, part2) # Returns: False # Example 3: s = \\"\\" part1 = \\"\\" part2 = \\"\\" is_merge(s, part1, part2) # Returns: True # Example 4: s = \\"abcdef\\" part1 = \\"abc\\" part2 = \\"def\\" is_merge(s, part1, part2) # Returns: True ``` Notes: Your solution should efficiently handle large strings and be able to deal with edge cases such as empty strings and non-matching characters. Performance: - Aim for an optimized solution with a reasonable time and space complexity.","solution":"def is_merge(s: str, part1: str, part2: str) -> bool: Returns True if s can be formed by interleaving part1 and part2 while maintaining the order of characters in part1 and part2. if len(s) != len(part1) + len(part2): return False # Create a memoization table to store results of subproblems memo = {} def dfs(i, j, k): if k == len(s): return i == len(part1) and j == len(part2) if (i, j) in memo: return memo[(i, j)] if i < len(part1) and part1[i] == s[k] and dfs(i + 1, j, k + 1): memo[(i, j)] = True return True if j < len(part2) and part2[j] == s[k] and dfs(i, j + 1, k + 1): memo[(i, j)] = True return True memo[(i, j)] = False return False return dfs(0, 0, 0)"},{"question":"# Insertion Sort with Custom Comparator You are tasked with implementing the Insertion Sort algorithm with the added functionality of sorting based on a custom comparator function. The comparator function will define the ordering of the elements. **Function Signature**: ```python def insertion_sort_custom(arr, comparator, simulation=False): Sorts the array using the insertion sort algorithm based on a custom comparator function. :param arr: List[int] : list of integers to be sorted :param comparator: Callable[[int, int], bool] : custom comparator function that defines the order :param simulation: bool : flag to simulate and print each iteration (default is False) :return: List[int] : sorted list of integers ``` **Input**: - `arr`: A list of integers to be sorted (1 <= len(arr) <= 10^4). - `comparator`: A function that takes two integers and returns a boolean. It should return `True` if the first integer should appear after the second one in the sorted list, and `False` otherwise. - `simulation`: An optional boolean flag that, when set to `True`, prints the array at each iteration of the algorithm. **Output**: - Returns a list of integers sorted according to the provided comparator function. **Constraints**: - Your algorithm should maintain a time complexity of O(n^2) and a space complexity of O(1). **Example**: ```python # Comparator function for ascending order def ascending(x, y): return x > y # Comparator function for descending order def descending(x, y): return x < y arr = [4, 2, 3, 1] sorted_arr_asc = insertion_sort_custom(arr, ascending) sorted_arr_desc = insertion_sort_custom(arr, descending) print(sorted_arr_asc) # Output should be [1, 2, 3, 4] print(sorted_arr_desc) # Output should be [4, 3, 2, 1] ``` **Edge Cases to Consider**: - Empty array. - Array with a single element. - Arrays with all elements being the same. - Already sorted arrays in ascending or descending order. **Additional Requirements**: - If the `simulation` flag is set to `True`, your implementation should print the state of the array after each insertion step, similar to the provided `insertion_sort` function.","solution":"def insertion_sort_custom(arr, comparator, simulation=False): Sorts the array using the insertion sort algorithm based on a custom comparator function. :param arr: List[int] : list of integers to be sorted :param comparator: Callable[[int, int], bool] : custom comparator function that defines the order :param simulation: bool : flag to simulate and print each iteration (default is False) :return: List[int] : sorted list of integers n = len(arr) for i in range(1, n): key = arr[i] j = i - 1 while j >= 0 and comparator(arr[j], key): arr[j + 1] = arr[j] j = j - 1 arr[j + 1] = key if simulation: print(f\\"Iteration {i}: {arr}\\") return arr # Example comparator functions def ascending(x, y): return x > y def descending(x, y): return x < y"},{"question":"**Problem**: You are given a list of integers sorted in ascending order and a target integer. Write a function `two_sum_optimized(numbers: List[int], target: int) -> List[int]` that efficiently finds two distinct indices such that the numbers at those indices sum up to the given target. Your solution should use the two-pointers technique and should operate with a time complexity of O(n) and space complexity of O(1). # Requirements 1. **Input**: - `numbers`: A list of integers sorted in ascending order. - `target`: An integer representing the target sum. 2. **Output**: - Return a list containing the indices of the two numbers that add up to the target. The indices should be 1-based, i.e., the first element has index 1. # Constraints: - The list will have at least two numbers. - There will always be exactly one solution. - You must not use the same element twice. # Example: ```python numbers = [2, 7, 11, 15] target = 9 Output: [1, 2] # Explanation: Because numbers[0] + numbers[1] = 2 + 7 = 9 ``` # Performance Requirements: - The solution must run in O(n) time. - The solution must use O(1) extra space. **Coding Task**: Complete the function `two_sum_optimized` below: ```python def two_sum_optimized(numbers, target): # Your implementation here ```","solution":"def two_sum_optimized(numbers, target): This function uses two-pointers technique to find the indices of the two numbers that add up to the target. The input list is sorted in ascending order. left, right = 0, len(numbers) - 1 while left < right: current_sum = numbers[left] + numbers[right] if current_sum == target: return [left + 1, right + 1] elif current_sum < target: left += 1 else: right -= 1 return []"},{"question":"# Question: Detecting path existence in a directed graph You are given a directed graph and you need to determine if there is a path between two nodes (source and target) in the graph. Implement the following class methods to achieve this: 1. **`__init__(self, vertex_count: int) -> None`**: Initialize the graph with the given number of vertices. 2. **`add_edge(self, source: int, target: int) -> None`**: Add a directed edge from `source` to `target` in the graph. 3. **`is_reachable(self, source: int, target: int) -> bool`**: Determine if there is a path from `source` to `target` using Depth-First Search (DFS). Input 1. An integer `vertex_count` representing the number of vertices in the graph. 2. A list of tuples where each tuple (source, target) represents a directed edge in the graph. 3. Two integers `source` and `target` representing the starting and ending nodes for path detection. Output - Return **True** if there is a path from `source` to `target`, otherwise return **False**. Constraints - 1 <= vertex_count <= 10^3 - 0 <= source, target < vertex_count - The graph may contain cycles. - All node indices are zero-based. Example ```python # Initialize the graph with 4 vertices graph = Graph(vertex_count=4) # Add edges edges = [(0, 1), (0, 2), (1, 2), (2, 0), (2, 3), (3, 3)] for source, target in edges: graph.add_edge(source, target) # Check if there is a path between nodes 1 and 3 print(graph.is_reachable(1, 3)) # Output: True # Check if there is a path between nodes 3 and 1 print(graph.is_reachable(3, 1)) # Output: False ``` Implementation Notes * The `is_reachable` method should efficiently check for the path using DFS. * Handle the base case where the source and target are the same.","solution":"class Graph: def __init__(self, vertex_count: int) -> None: Initializes the graph with a specified number of vertices. self.vertex_count = vertex_count self.adjacency_list = [[] for _ in range(vertex_count)] def add_edge(self, source: int, target: int) -> None: Adds a directed edge from source to target. self.adjacency_list[source].append(target) def is_reachable(self, source: int, target: int) -> bool: Uses Depth-First Search (DFS) to determine if there is a path from source to target. visited = [False] * self.vertex_count def dfs(node): if node == target: return True visited[node] = True for neighbor in self.adjacency_list[node]: if not visited[neighbor]: if dfs(neighbor): return True return False return dfs(source)"},{"question":"# Question: Scenario: You are developing a text-editing tool that includes a function to reverse the order of words in a given sentence. Users expect that the words themselves are not altered but the sequence is reversed completely. Task: Write a function `reverse_sentence(sentence: str) -> str` that takes a string as input and returns a new string with the order of words reversed. Each word in the input string is separated by spaces. Leading and trailing spaces should be stripped, and multiple spaces between words should be reduced to a single space in the output. Example: ```python assert reverse_sentence(\\"I am keon kim and I like pizza\\") == \\"pizza like I and kim keon am I\\" assert reverse_sentence(\\" Hello World! \\") == \\"World! Hello\\" assert reverse_sentence(\\"Turing Machine is an abstract machine\\") == \\"machine abstract an is Machine Turing\\" assert reverse_sentence(\\"\\") == \\"\\" assert reverse_sentence(\\"Python\\") == \\"Python\\" ``` Constraints: 1. The function should handle strings up to 10,000 characters in length. 2. The function must run efficiently within a reasonable time frame, ideally O(n) where n is the length of the input string. You may use the following template to start with: ```python def reverse_sentence(sentence: str) -> str: # Your implementation here ``` Tips: - Properly handle multiple spaces and edge cases like single-word sentences. - Focus on maintaining optimal time complexity and minimal auxiliary space.","solution":"def reverse_sentence(sentence: str) -> str: This function takes a string as input and returns a new string with the order of words reversed. Each word in the input string is separated by spaces. Leading and trailing spaces are stripped, and multiple spaces between words are reduced to a single space in the output. # Split the sentence into words while stripping leading/trailing spaces and reducing multiple spaces words = sentence.strip().split() # Reverse the words and join them with a single space in between reversed_sentence = \' \'.join(reversed(words)) return reversed_sentence"},{"question":"Context: You are developing a system to evaluate whether a given sentence (str) follows a specific pattern described by a sequence of characters. This is a common problem in linguistic pattern recognition and syntactic parsing. Problem Statement: Write a function `word_pattern(pattern: str, str: str) -> bool` that determines if a string `str` follows the same pattern. Each letter in the pattern should uniquely map to a word in `str`, and vice versa. The input pattern will only contain lowercase letters, and the input string `str` will only contain lowercase letters separated by single spaces. Requirements: - **Input**: - `pattern`: A string representing the pattern (length ≤ 1000). - `str`: A string consisting of words separated by spaces (length ≤ 10^4). - **Output**: - Return `True` if the string follows the given pattern, otherwise return `False`. - **Constraints**: - Words in `str` are separated by exactly one space. - Each word in `str` is mapped uniquely to characters in `pattern`. Example Cases: - **Example 1:** - Input: `pattern = \\"abba\\"`, `str = \\"dog cat cat dog\\"` - Output: `True` - **Example 2:** - Input: `pattern = \\"abba\\"`, `str = \\"dog cat cat fish\\"` - Output: `False` - **Example 3:** - Input: `pattern = \\"aaaa\\"`, `str = \\"dog cat cat dog\\"` - Output: `False` - **Example 4:** - Input: `pattern = \\"abba\\"`, `str = \\"dog dog dog dog\\"` - Output: `False` Implementation Details: - Utilize a dictionary to map pattern characters to words. - Use a set to ensure words are not mapped multiple times. - Verify the lengths of the pattern and the word list before processing. Function Signature: ```python def word_pattern(pattern: str, str: str) -> bool: pass ``` Good luck and demonstrate your understanding by implementing the `word_pattern` function efficiently!","solution":"def word_pattern(pattern: str, s: str) -> bool: words = s.split() if len(pattern) != len(words): return False char_to_word = {} word_to_char = {} for p, w in zip(pattern, words): if p in char_to_word: if char_to_word[p] != w: return False else: if w in word_to_char: return False char_to_word[p] = w word_to_char[w] = p return True"},{"question":"Context: You are tasked with developing a feature for a text editor that highlights all occurrences of a given search word in the editor\'s content. To efficiently find and highlight the search word, you decide to use the Knuth-Morris-Pratt (KMP) algorithm. Problem: Implement the `highlight_occurrences` function that uses the KMP algorithm to find and return the start indices of all occurrences of a search word within a given text. You should additionally consider various edge cases such as empty strings and patterns longer than the text. Function Signature: ```python def highlight_occurrences(text: str, pattern: str) -> List[int]: pass ``` Input: - **text** (str): The content in which to search. - **pattern** (str): The search word to find within the text. Output: - **List[int]**: List of starting indices of each match. Constraints: - The function should run in linear time relative to the length of the text and the pattern. - The text and pattern will only contain lowercase alphabetical characters (\'a\' to \'z\'). Example: ```python print(highlight_occurrences(\\"hello there hero!\\", \\"he\\")) # Output: [0, 7, 12] print(highlight_occurrences(\\"aabcaabxaaaz\\", \\"aab\\")) # Output: [0, 4] print(highlight_occurrences(\\"mississippi\\", \\"iss\\")) # Output: [1, 4] ``` Notes: - If no occurrences are found, the function should return an empty list. - Don\'t consider overlapping instances of the pattern.","solution":"from typing import List def build_lps(pattern: str) -> List[int]: Builds the longest prefix which is also suffix (lps) array for the KMP algorithm. length = 0 lps = [0] * len(pattern) i = 1 while i < len(pattern): if pattern[i] == pattern[length]: length += 1 lps[i] = length i += 1 else: if length != 0: length = lps[length - 1] else: lps[i] = 0 i += 1 return lps def highlight_occurrences(text: str, pattern: str) -> List[int]: Returns the starting indices of all occurrences of the pattern in the text using KMP algorithm. if not pattern: return [] # Nothing to search lps = build_lps(pattern) i = 0 # index for text j = 0 # index for pattern result = [] while i < len(text): if pattern[j] == text[i]: i += 1 j += 1 if j == len(pattern): result.append(i - j) j = lps[j - 1] elif i < len(text) and pattern[j] != text[i]: if j != 0: j = lps[j - 1] else: i += 1 return result"},{"question":"# Scenario You are working on maintaining and analyzing the structure of various binary trees used in a network simulation project. One task requires you to count the number of empty (null) branches in the given binary trees. This property of the trees can help determine their completeness and assist in optimizing their usage in the simulation. # Task Write a function `num_empty(root)` that takes the root of a binary tree and returns the number of empty branches (null pointers). # Input - `root`: The root node of the binary tree. Each node has properties `left` and `right`. # Output - Return an integer representing the number of empty branches in the tree. # Constraints - The number of nodes in the binary tree is within the range [0, 1000]. - The value of each node is an integer. # Example ```python # Example to help visualize the empty branches count # Sample tree structure: 9 __ / ___ 6 12 / / 3 8 10 15 / / / / * * 7 * * * * 18 / / * * * * # Based on this structure, empty_branch = 10. # Expected function signature: # def num_empty(root: \'Node\') -> int # Where Node is defined as: class Node: def __init__(self, key): self.left = None self.right = None self.val = key ``` # Sample Test Cases ```python import unittest class TestNumEmptyBranches(unittest.TestCase): def setUp(self): # Constructing the tree manually self.tree = Node(9) self.tree.left = Node(6) self.tree.right = Node(12) self.tree.left.left = Node(3) self.tree.left.right = Node(8) self.tree.right.left = Node(10) self.tree.right.right = Node(15) self.tree.left.right.left = Node(7) self.tree.right.right.right = Node(18) def test_empty_branches_count(self): self.assertEqual(num_empty(self.tree), 10) if __name__ == \\"__main__\\": unittest.main() ``` # Additional Notes - Please ensure the recursive method handles edge cases effectively, particularly for cases where nodes have missing children. - You are not allowed to modify the tree nodes\' structure and should only work within the provided function `num_empty`.","solution":"class Node: def __init__(self, key): self.left = None self.right = None self.val = key def num_empty(root): if root is None: return 1 left_empty = num_empty(root.left) right_empty = num_empty(root.right) return left_empty + right_empty"},{"question":"# Question: Implement an Enhanced Rabin-Karp Algorithm for Robust String Matching Using the Rabin-Karp algorithm analyzed above, your task is to implement a function that finds all starting indices of a substring (word) within a larger string (text). Enhance the algorithm to handle edge cases efficiently, such as avoiding false positives due to hash collisions. Function Signature: ```python def find_substring_occurrences(word: str, text: str) -> List[int]: ``` # Input 1. **word**: A string `word` (1 ≤ len(word) ≤ 10^4) representing the pattern to search for. 2. **text**: A string `text` (1 ≤ len(text) ≤ 10^5) in which to search for the pattern. # Output * A list of integers indicating the starting indices of the substring `word` in the `text`. # Constraints * Both strings consist of lowercase English letters only. # Example ```python word = \\"aba\\" text = \\"ababababa\\" find_substring_occurrences(word, text) ``` Output: ``` [0, 2, 4, 6] ``` # Notes * The function should return an empty list if no occurrences are found. * Ensure to handle cases where the word is longer than the text, as well as employ efficient hash functions to handle potential collisions and edge cases effectively. Good luck!","solution":"from typing import List def find_substring_occurrences(word: str, text: str) -> List[int]: def calculate_hash(s: str, end: int, base: int, mod: int) -> int: h = 0 for i in range(end): h = (h * base + ord(s[i])) % mod return h result = [] if len(word) > len(text): return result base = 256 mod = 10**9 + 7 len_word = len(word) hash_word = calculate_hash(word, len_word, base, mod) hash_text = calculate_hash(text, len_word, base, mod) if hash_word == hash_text and text[:len_word] == word: result.append(0) base_power = pow(base, len_word - 1, mod) for i in range(1, len(text) - len_word + 1): hash_text = (hash_text - ord(text[i-1]) * base_power) % mod hash_text = (hash_text * base + ord(text[i + len_word - 1])) % mod if hash_text == hash_word and text[i:i + len_word] == word: result.append(i) return result"},{"question":"# Value Filtering in Arrays You\'re given an array of integers, and two optional limits - a lower limit (`min_lim`) and an upper limit (`max_lim`). Your task is to write a function that returns a new array which contains only the values from the original array that lie between the specified limits, inclusive. If a limit is not specified (i.e., it is `None`), the function should consider the extreme value of the entire array (minimum if `min_lim` is `None`, and maximum if `max_lim` is `None`) for that limit. Implement the following function: ```python def limit(arr, min_lim=None, max_lim=None): # Your code here ``` # Input * `arr`: A list of integers of length `n` (1 ≤ n ≤ 10^6). * `min_lim`: An optional integer, the lower limit of the values to include. * `max_lim`: An optional integer, the upper limit of the values to include. # Output * Return a list of integers that fall between the specified limits, inclusive. # Constraints * If both `min_lim` and `max_lim` are `None`, return the original list. # Example ```python # Examples of usage print(limit([1, 2, 3, 4, 5], None, 3)) # Output: [1, 2, 3] print(limit([10, 15, 20, 25, 30], 12, 28)) # Output: [15, 20, 25] print(limit([4, 6, 8, 10], None, None)) # Output: [4, 6, 8, 10] print(limit([8, 6, 7, 5, 3, 0, 9], 5, None)) # Output: [8, 6, 7, 5, 9] ``` # Notes * Your solution should aim for O(n) time complexity. * Consider edge cases such as when the array is empty, when all values are below the lower limit or above the upper limit.","solution":"def limit(arr, min_lim=None, max_lim=None): Returns a list of integers from arr that are between min_lim and max_lim, inclusive. If min_lim is None, use the minimum value from arr. If max_lim is None, use the maximum value from arr. If both min_lim and max_lim are None, return the original list. if min_lim is None: min_lim = min(arr) if max_lim is None: max_lim = max(arr) return [x for x in arr if min_lim <= x <= max_lim]"},{"question":"You are building a text processing system that needs to determine if two given strings are isomorphic. Two strings are isomorphic if the characters in the first string can be replaced to get the second string, maintaining the character order and ensuring no two characters map to the same character but a character might map to itself. Problem Write a function `are_isomorphic(s: str, t: str) -> bool` that checks if two strings `s` and `t` are isomorphic. Function Signature `def are_isomorphic(s: str, t: str) -> bool:` Input - `s (1 <= len(s) <= 1000)`: a string consisting of printable ASCII characters. - `t (1 <= len(t) <= 1000)`: a string consisting of printable ASCII characters. Output - Return `True` if the strings are isomorphic, otherwise `False`. Constraints - Each string can contain any of the 95 printable ASCII characters. - Both strings `s` and `t` will have the same length. Examples 1. Given `s = \\"egg\\"` and `t = \\"add\\"`. - Output: `True` 2. Given `s = \\"foo\\"` and `t = \\"bar\\"`. - Output: `False` 3. Given `s = \\"paper\\"` and `t = \\"title\\"`. - Output: `True` Performance Requirements - The implementation should have a time complexity of O(n) and a space complexity of O(n), where n is the length of the strings. Scenario In your text processing system, you need to frequently determine if two strings are isomorphic to decide if certain transformations are valid. Implement a function using the above specifications to integrate with your system.","solution":"def are_isomorphic(s: str, t: str) -> bool: Checks if two strings s and t are isomorphic. if len(s) != len(t): return False mapping_s_to_t = {} mapping_t_to_s = {} for char_s, char_t in zip(s, t): if char_s in mapping_s_to_t: if mapping_s_to_t[char_s] != char_t: return False else: mapping_s_to_t[char_s] = char_t if char_t in mapping_t_to_s: if mapping_t_to_s[char_t] != char_s: return False else: mapping_t_to_s[char_t] = char_s return True"},{"question":"# Shell Sort Implementation You are required to implement the Shell Sort algorithm to sort an array of integers in ascending order. The input will always be a list of integers with zero or more elements. # Function Signature ```python def shell_sort(arr: List[int]) -> List[int]: pass ``` # Input * A single parameter `arr`: a list of integers where `0 <= len(arr) <= 10^6`. # Output * The function should return a sorted list of integers in ascending order. # Performance Requirements * Your implementation should optimize for both time and space, with the understanding that Shell Sort\'s inherent average case time complexity can be improved with an optimal gap sequence. # Constraints * You must sort the list in place without using additional memory structures, beyond variables. * You are not allowed to use Python\'s built-in sorting functions. * The implementation must handle an empty list, single-element lists, and already sorted lists efficiently. # Example ```python >>> shell_sort([12, 34, 54, 2, 3]) [2, 3, 12, 34, 54] >>> shell_sort([]) [] >>> shell_sort([3]) [3] ```","solution":"def shell_sort(arr): Sorts an array of integers in ascending order using the Shell Sort algorithm. n = len(arr) # Start with a large gap, then reduce the gap gap = n // 2 # Do a gapped insertion sort for this gap size. # The first gap elements arr[0..gap-1] are already in gapped order while gap > 0: for i in range(gap, n): # add arr[i] to the elements that have been gap sorted # save arr[i] in temp and make a hole at position i temp = arr[i] # shift earlier gap-sorted elements up until the correct location for arr[i] is found j = i while j >= gap and arr[j - gap] > temp: arr[j] = arr[j - gap] j -= gap # put temp (the original arr[i]) in its correct location arr[j] = temp gap //= 2 return arr"},{"question":"You are tasked with designing a class called `WordSearch` to store words and efficiently search for them with optional wildcards. The wildcard character is represented by a period (`.`), which can substitute any single character. Implement a class `WordSearch` with the following methods: - **`__init__(self)`**: Initializes the data structure. - **`add_word(self, word: str) -> None`**: Adds a word to the data structure. - **`search(self, word: str) -> bool`**: Returns `True` if there is any word in the data structure that matches the given word or pattern (with `.` wildcards), otherwise returns `False`. # Constraints - Words and search patterns only contain lowercase English letters a-z or the period `.` - All inputs are non-empty strings. - The methods `add_word` and `search` will be called at most `3 * 10^4` times combined. # Input and Output - **Input**: - `WordSearch add_word` - `str word` - `WordSearch search` - `str word` - **Output**: - Returns `bool` # Example ```python ws = WordSearch() ws.add_word(\\"bad\\") ws.add_word(\\"dad\\") ws.add_word(\\"mad\\") print(ws.search(\\"pad\\")) # Output: False print(ws.search(\\"bad\\")) # Output: True print(ws.search(\\".ad\\")) # Output: True print(ws.search(\\"b..\\")) # Output: True ``` # Notes - Ensure your solution handles large input sizes efficiently. - Carefully account for edge cases involving wildcard matches. # Implementation Tip - Consider using Trie data structures or optimized dictionary maps to handle the wildcard searches efficiently.","solution":"class WordSearch: def __init__(self): self.trie = {} def add_word(self, word: str) -> None: node = self.trie for char in word: if char not in node: node[char] = {} node = node[char] node[\'\'] = True def search(self, word: str) -> bool: def dfs(node, i): if i == len(word): return \'\' in node if word[i] == \'.\': for char in node: if char != \'\' and dfs(node[char], i+1): return True return False else: return word[i] in node and dfs(node[word[i]], i+1) return dfs(self.trie, 0)"},{"question":"You are given an array of integers and you need to perform multiple update and sum query operations efficiently. Implement a data structure (Fenwick Tree) that supports the following operations in logarithmic time: 1. **Update the value of an element** at a specified index with a new value. 2. **Compute the prefix sum** from the start of the array to a specified index. # Requirements 1. Implement the class `FenwickTree` with the following methods: * `__init__(self, arr: List[int]) -> None`: Initializes the tree with a given list of integer values. * `update(self, index: int, value: int) -> None`: Updates the value at the specified `index` to `value`. * `prefix_sum(self, index: int) -> int`: Returns the sum of the elements from the start of the array to the specified `index`. 2. Ensure the operations are performed in O(log n) time complexity. # Input Format * `arr` is a list of integers [a1, a2, ..., an]. * `index` is an integer representing the position in the array (0-based). * `value` is an integer that the array element should be updated to. # Output Format * The `update` method has no return value. * The `prefix_sum` method returns an integer representing the computed sum. # Constraints * 1 <= len(arr) <= 10^4 * -10^4 <= arr[i] <= 10^4 * 0 <= index < len(arr) * -10^4 <= value <= 10^4 # Example ```python # Example Usage: # Initialize Fenwick Tree with array [1, 2, 3, 4, 5] ft = FenwickTree([1, 2, 3, 4, 5]) # Update the value at index 2 (3 -> 6) ft.update(2, 6) # Get the prefix sum up to index 2 (should return 1 + 2 + 6 = 9) print(ft.prefix_sum(2)) # Output: 9 # Get the prefix sum up to index 4 (should return 1 + 2 + 6 + 4 + 5 = 18) print(ft.prefix_sum(4)) # Output: 18 ``` # Note The above usage is just an example and students need to implement the underlying methods in the `FenwickTree` class correctly based on the provided specifications.","solution":"from typing import List class FenwickTree: def __init__(self, arr: List[int]) -> None: self.n = len(arr) self.tree = [0] * (self.n + 1) self.arr = arr.copy() for i in range(self.n): self._update_tree(i + 1, arr[i]) def _update_tree(self, index: int, value: int) -> None: while index <= self.n: self.tree[index] += value index += index & -index def update(self, index: int, value: int) -> None: delta = value - self.arr[index] self.arr[index] = value self._update_tree(index + 1, delta) def prefix_sum(self, index: int) -> int: index += 1 sum = 0 while index > 0: sum += self.tree[index] index -= index & -index return sum"},{"question":"Topological Sort (Advanced Implementation) Problem Statement You are provided with the snippets of two topological sort algorithms for a Directed Acyclic Graph (DAG). Your task is to implement a function that efficiently detects cycles and returns the topological sort of the graph using an iterative method without recursion. Function Signature ```python def topological_sort_with_cycle_detection(graph: Dict[str, List[str]]) -> List[str]: pass ``` Input - `graph`: A dictionary representing a Directed Acyclic Graph where keys are node names (strings) and values are lists of nodes (strings) that the key node points to. Output - A list of nodes in topologically sorted order. - Raise a `ValueError` if a cycle is detected in the graph. Constraints - Nodes are represented by strings. - The graph is a Directed Acyclic Graph (DAG) or contains cycles that need to be detected. Performance Requirements Your implementation should have a time complexity of O(V + E) and a space complexity of O(V). # Examples ```python graph1 = { \'A\': [\'B\', \'C\'], \'B\': [\'C\', \'D\'], \'C\': [], \'D\': [] } # Expected output: [\'A\', \'B\', \'C\', \'D\'] graph2 = { \'A\': [\'B\'], \'B\': [\'C\'], \'C\': [\'A\'] } # Expected output: Raises ValueError (\\"cycle\\") graph3 = { \'A\': [], \'B\': [], \'C\': [\'A\', \'B\'] } # Expected output: [\'C\', \'A\', \'B\'] or [\'C\', \'B\', \'A\'] ``` # Implementation Notes - Ensure that your implementation handles various edge cases, such as graphs with multiple independent subgraphs. - Include comments explaining key parts of your code. - Implement cycle detection and validation before generating the topological order.","solution":"def topological_sort_with_cycle_detection(graph): from collections import deque, defaultdict # Step 1: Calculate in-degrees of all nodes in_degree = defaultdict(int) for node in graph: if node not in in_degree: in_degree[node] = 0 for neighbor in graph[node]: in_degree[neighbor] += 1 # Step 2: Initialize queue with zero in-degree nodes queue = deque([node for node in in_degree if in_degree[node] == 0]) topo_sort = [] while queue: node = queue.popleft() topo_sort.append(node) # Step 3: Decrease in-degree for all neighboring nodes for neighbor in graph[node]: in_degree[neighbor] -= 1 if in_degree[neighbor] == 0: queue.append(neighbor) # If topo_sort has fewer nodes than the graph, we have a cycle if len(topo_sort) != len(in_degree): raise ValueError(\\"cycle\\") return topo_sort"},{"question":"You are required to implement a class based on the given specifications that allow adding new words and searching for words with support for wildcard characters. Wildcard character `\'.\'` can match any letter. Class 1: Trie-based Implementation 1. Implement a **WordDictionary** class: - `void add_word(word: str)`: Adds the word to the Trie. - `bool search(word: str)`: Searches the word in the Trie. The word may contain wildcard character `\'.\'` which can match any letter. Class 2: Dictionary-based Implementation 2. Implement a **WordDictionary2** class: - `void add_word(word: str)`: Adds the word to the dictionary. - `bool search(word: str)`: Searches the word in the dictionary. The word may contain wildcard character `\'.\'`. # Constraints - Each word added to the data structure will be at most 20 characters long. - Logically follow one of the data structures outlined in your implementation. - Ensure you handle edge cases like empty strings and all-wildcard searches. # Example: ```python # Example usage for WordDictionary wd = WordDictionary() wd.add_word(\\"bad\\") wd.add_word(\\"dad\\") wd.add_word(\\"mad\\") assert wd.search(\\"pad\\") == False assert wd.search(\\"bad\\") == True assert wd.search(\\".ad\\") == True assert wd.search(\\"b..\\") == True # Example usage for WordDictionary2 wd2 = WordDictionary2() wd2.add_word(\\"bad\\") wd2.add_word(\\"dad\\") wd2.add_word(\\"mad\\") assert wd2.search(\\"pad\\") == False assert wd2.search(\\"bad\\") == True assert wd2.search(\\".ad\\") == True assert wd2.search(\\"b..\\") == True ```","solution":"class TrieNode: def __init__(self): self.children = {} self.is_end_of_word = False class WordDictionary: def __init__(self): self.root = TrieNode() def add_word(self, word: str): node = self.root for char in word: if char not in node.children: node.children[char] = TrieNode() node = node.children[char] node.is_end_of_word = True def search(self, word: str) -> bool: return self._search_helper(word, 0, self.root) def _search_helper(self, word, index, node): if index == len(word): return node.is_end_of_word char = word[index] if char == \'.\': for child in node.children.values(): if self._search_helper(word, index + 1, child): return True return False else: if char in node.children: return self._search_helper(word, index + 1, node.children[char]) else: return False class WordDictionary2: def __init__(self): self.words = {} def add_word(self, word: str): if len(word) not in self.words: self.words[len(word)] = [] self.words[len(word)].append(word) def search(self, word: str) -> bool: if len(word) not in self.words: return False for candidate in self.words[len(word)]: if all(w == c or w == \'.\' for w, c in zip(word, candidate)): return True return False"},{"question":"# Scenario Alice and Bob want to communicate securely over an insecure network. To achieve this, they decide to use the Diffie-Hellman key exchange algorithm to generate a shared secret key. # Objective Write a function `secure_communication(a: int, p: int) -> str` that: 1. Verifies if ( p ) is a prime number. 2. Verifies if ( a ) is a primitive root of ( p ). 3. Performs the Diffie-Hellman key exchange process between Alice and Bob. 4. Returns \\"Success\\" if both Alice and Bob compute the same shared secret key. Otherwise, return \\"Failure\\". # Function Signature ```python def secure_communication(a: int, p: int) -> str: # Your code here ``` # Input * `a`: An integer, the potential primitive root. * `p`: A large prime number. # Output * Returns: * \\"Success\\" if the key exchange is successful. * \\"Failure\\" if the key exchange is unsuccessful. # Constraints * ( 3 le p le 10^9 ) * ( 2 le a le p-2 ) # Examples 1. `secure_communication(5, 23)` should return `\\"Success\\"` as 23 is prime and 5 is a primitive root of 23. 2. `secure_communication(2, 23)` should return `\\"Failure\\"` as 2 is not a primitive root of 23. # Additional Notes * Implement efficient primality testing and primitive root verification. * Carefully handle large numbers to avoid excessive computation time. # Hints - Utilize the `prime_check` function from the provided snippet to verify if ( p ) is prime. - Use the `find_primitive_root` and `diffie_hellman_key_exchange` functions to help verify the primitive root and perform key exchange.","solution":"import random def is_prime(n): if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True def power(x, y, p): res = 1 x = x % p while y > 0: if y & 1: res = (res * x) % p y = y >> 1 x = (x * x) % p return res def find_primitive_root(p): if p == 2: return 1 phi = p - 1 factors = set() num = phi d = 2 while d * d <= num: if num % d == 0: factors.add(d) while num % d == 0: num //= d d += 1 if num > 1: factors.add(num) for r in range(2, p): flag = False for factor in factors: if power(r, phi // factor, p) == 1: flag = True break if not flag: return r return -1 def diffie_hellman_key_exchange(a, p): private_key_alice = random.randint(1, p-1) private_key_bob = random.randint(1, p-1) public_key_alice = power(a, private_key_alice, p) public_key_bob = power(a, private_key_bob, p) shared_key_alice = power(public_key_bob, private_key_alice, p) shared_key_bob = power(public_key_alice, private_key_bob, p) return shared_key_alice, shared_key_bob def secure_communication(a: int, p: int) -> str: if not is_prime(p): return \\"Failure\\" if find_primitive_root(p) != a: return \\"Failure\\" shared_key_alice, shared_key_bob = diffie_hellman_key_exchange(a, p) if shared_key_alice == shared_key_bob: return \\"Success\\" return \\"Failure\\""},{"question":"Implement a Custom Stack with Enhanced Features Objective: Your task is to enhance the provided stack implementations (`ArrayStack` and `LinkedListStack`) by adding a few extra utility methods. This exercise is designed to test your understanding of stack operations and your ability to extend data structures. Requirements: 1. **min() method**: Add a method to return the minimum element in the stack in O(1) time. 2. **max() method**: Add a method to return the maximum element in the stack in O(1) time. 3. **sum() method**: Add a method to return the sum of all elements in the stack in O(1) time. Input and Output Formats: - **Method**: `push(value)` - **Input**: Integer value to be pushed onto the stack. - **Output**: None - **Method**: `pop()` - **Input**: None - **Output**: Integer value removed from the top of the stack. - **Method**: `peek()` - **Input**: None - **Output**: Integer value from the top of the stack (without removing it). - **Method**: `min()` - **Input**: None - **Output**: Integer minimum value in the stack. - **Method**: `max()` - **Input**: None - **Output**: Integer maximum value in the stack. - **Method**: `sum()` - **Input**: None - **Output**: Integer sum of all values in the stack. Constraints: - The stack will hold only integer values. - Implement the methods with O(1) time complexity where specified. - Handle empty stack scenarios by raising appropriate exceptions (e.g., IndexError). Implementation Requirements: - Extend the `ArrayStack` and `LinkedListStack` classes to implement these new methods. - Ensure the integrity and performance of the existing and new stack operations. You need to write the implementation of these methods in both `ArrayStack` and `LinkedListStack` classes provided in the initial code snippet. ```python # Extend the implementation here by adding min, max, and sum methods. class EnhancedArrayStack(ArrayStack): def __init__(self, size=10): super().__init__(size) self._min_stack = ArrayStack(size) self._max_stack = ArrayStack(size) self._sum = 0 def push(self, value): super().push(value) if self._min_stack.is_empty() or value <= self._min_stack.peek(): self._min_stack.push(value) if self._max_stack.is_empty() or value >= self._max_stack.peek(): self._max_stack.push(value) self._sum += value def pop(self): value = super().pop() if value == self._min_stack.peek(): self._min_stack.pop() if value == self._max_stack.peek(): self._max_stack.pop() self._sum -= value return value def min(self): if self.is_empty(): raise IndexError(\\"Stack is empty\\") return self._min_stack.peek() def max(self): if self.is_empty(): raise IndexError(\\"Stack is empty\\") return self._max_stack.peek() def sum(self): return self._sum class EnhancedLinkedListStack(LinkedListStack): def __init__(self): super().__init__() self._min_stack = LinkedListStack() self._max_stack = LinkedListStack() self._sum = 0 def push(self, value): super().push(value) if self._min_stack.is_empty() or value <= self._min_stack.peek(): self._min_stack.push(value) if self._max_stack.is_empty() or value >= self._max_stack.peek(): self._max_stack.push(value) self._sum += value def pop(self): value = super().pop() if value == self._min_stack.peek(): self._min_stack.pop() if value == self._max_stack.peek(): self._max_stack.pop() self._sum -= value return value def min(self): if self.is_empty(): raise IndexError(\\"Stack is empty\\") return self._min_stack.peek() def max(self): if self.is_empty(): raise IndexError(\\"Stack is empty\\") return self._max_stack.peek() def sum(self): return self._sum ``` Notes: - Test your enhanced stack implementation with different sequences of operations to ensure that the min, max, and sum methods work correctly and efficiently. - Consider edge cases such as pushing the same minimum or maximum value multiple times.","solution":"class ArrayStack: def __init__(self, size=10): self._data = [None] * size self._size = 0 def push(self, value): if self._size == len(self._data): self._data.append(None) self._data[self._size] = value self._size += 1 def pop(self): if self.is_empty(): raise IndexError(\\"Stack is empty\\") value = self._data[self._size - 1] self._data[self._size - 1] = None self._size -= 1 return value def peek(self): if self.is_empty(): raise IndexError(\\"Stack is empty\\") return self._data[self._size - 1] def is_empty(self): return self._size == 0 def size(self): return self._size class EnhancedArrayStack(ArrayStack): def __init__(self, size=10): super().__init__(size) self._min_stack = ArrayStack(size) self._max_stack = ArrayStack(size) self._sum = 0 def push(self, value): super().push(value) if self._min_stack.is_empty() or value <= self._min_stack.peek(): self._min_stack.push(value) if self._max_stack.is_empty() or value >= self._max_stack.peek(): self._max_stack.push(value) self._sum += value def pop(self): value = super().pop() if value == self._min_stack.peek(): self._min_stack.pop() if value == self._max_stack.peek(): self._max_stack.pop() self._sum -= value return value def min(self): if self.is_empty(): raise IndexError(\\"Stack is empty\\") return self._min_stack.peek() def max(self): if self.is_empty(): raise IndexError(\\"Stack is empty\\") return self._max_stack.peek() def sum(self): return self._sum class Node: def __init__(self, value, next_node=None): self.value = value self.next = next_node class LinkedListStack: def __init__(self): self._head = None self._size = 0 def push(self, value): self._head = Node(value, self._head) self._size += 1 def pop(self): if self.is_empty(): raise IndexError(\\"Stack is empty\\") value = self._head.value self._head = self._head.next self._size -= 1 return value def peek(self): if self.is_empty(): raise IndexError(\\"Stack is empty\\") return self._head.value def is_empty(self): return self._head is None def size(self): return self._size class EnhancedLinkedListStack(LinkedListStack): def __init__(self): super().__init__() self._min_stack = LinkedListStack() self._max_stack = LinkedListStack() self._sum = 0 def push(self, value): super().push(value) if self._min_stack.is_empty() or value <= self._min_stack.peek(): self._min_stack.push(value) if self._max_stack.is_empty() or value >= self._max_stack.peek(): self._max_stack.push(value) self._sum += value def pop(self): value = super().pop() if value == self._min_stack.peek(): self._min_stack.pop() if value == self._max_stack.peek(): self._max_stack.pop() self._sum -= value return value def min(self): if self.is_empty(): raise IndexError(\\"Stack is empty\\") return self._min_stack.peek() def max(self): if self.is_empty(): raise IndexError(\\"Stack is empty\\") return self._max_stack.peek() def sum(self): return self._sum"},{"question":"# Closest Points to Origin Scenario You are working on a GPS-based application that frequently needs to find the k closest POIs (Points of Interest) to the user\'s current location, referred to as the origin. Given this requirement, your task is to write a function that efficiently determines these k closest points. Function Signature ```python def find_k_closest_points(points: List[Tuple[int, int]], k: int, origin: Tuple[int, int] = (0, 0)) -> List[Tuple[int, int]]: ``` Input - `points`: A list of tuples, where each tuple represents a point (xi, yi) on a 2D plane. - `k`: An integer representing the number of closest points to the origin that you need to find. - `origin` (optional): A tuple representing the coordinates of the origin point (default is (0, 0)). Output - A list of k tuples, each representing a point that is among the k closest points to the origin. Constraints - 1 ≤ |points| ≤ 10^5 (where |points| is the number of points in the list) - 1 ≤ k ≤ |points| - All coordinates and the origin are integers. Requirements - The solution should have a time complexity of O(k + (n-k)logk) and a space complexity of O(k). Example ```python points = [(1, 3), (-2, 2), (5, 8), (0, 1)] k = 2 origin = (0, 0) print(find_k_closest_points(points, k, origin)) ``` Expected Output: ```python [(-2, 2), (0, 1)] ``` # Notes - You should implement the distance computation using Euclidean distance without the square root, as it is sufficient for comparison. - Handle edge cases, such as when k equals the length of the points list, efficiently.","solution":"import heapq from typing import List, Tuple def squared_euclidean_distance(point: Tuple[int, int], origin: Tuple[int, int]) -> int: Return the squared Euclidean distance between point and origin. Squared distance is used to avoid the computational cost of the square root. return (point[0] - origin[0]) ** 2 + (point[1] - origin[1]) ** 2 def find_k_closest_points(points: List[Tuple[int, int]], k: int, origin: Tuple[int, int] = (0, 0)) -> List[Tuple[int, int]]: Find the k closest points to the origin. if k == len(points): return points max_heap = [] for point in points: distance = squared_euclidean_distance(point, origin) heapq.heappush(max_heap, (-distance, point)) if len(max_heap) > k: heapq.heappop(max_heap) return [point for _, point in max_heap]"},{"question":"You are tasked with writing a function to determine if one string is a rotation of another string. For this assessment, an efficient algorithm is required. The goal is to ensure students understand efficient string manipulation techniques and can handle both typical and edge cases effectively. Problem Statement Write a function `is_rotation(s1, s2)` that returns `True` if string `s2` is a rotation of string `s1`, and `False` otherwise. 1. **Input**: - `s1`: A string of length `N` (`0 <= N <= 10^5`). - `s2`: A string of length `N` (`0 <= N <= 10^5`). 2. **Output**: - Returns a boolean `True` if `s2` is a rotation of `s1`, otherwise `False`. 3. **Constraints**: - Handle the input size efficiently. - The function should run with a time complexity of at most O(N). Example ```python assert is_rotation(\\"hello\\", \\"llohe\\") == True assert is_rotation(\\"hello\\", \\"helol\\") == False assert is_rotation(\\"\\", \\"\\") == True assert is_rotation(\\"abc\\", \\"cab\\") == True assert is_rotation(\\"abcd\\", \\"dabc\\") == True assert is_rotation(\\"abcd\\", \\"abcd\\") == True assert is_rotation(\\"abcd\\", \\"abdc\\") == False ``` Requirements 1. The function must correctly handle edge cases, such as empty strings or strings of different lengths. 2. The preferred algorithm should achieve linear time complexity, O(N). Performance Your implementation must handle input sizes up to 10^5 characters efficiently and pass these constraints within reasonable runtime on typical systems.","solution":"def is_rotation(s1, s2): Returns True if s2 is a rotation of s1, otherwise False. if len(s1) != len(s2): return False return s2 in s1 + s1"},{"question":"# Context In data preprocessing, you may need to enhance datasets by converting sparse binary features to more dense representations. This often involves optimizing certain attributes to simulate the highest possible performance one can extract from features. This coding assessment requires you to determine the optimal index of an element that can be flipped to modify the dataset in the desired manner. # Task Given a binary array `arr`, write a function `max_ones_index` that finds the index of a `0` that, when replaced by a `1`, results in the longest continuous sequence of `1`s in the array. # Function Signature ```python def max_ones_index(arr: List[int]) -> int: ``` # Input * `arr` (List[int]): A list of integers containing only `0`s and `1`s. # Output * (int): The index of the `0` that should be flipped to maximize the length of continuous `1`s. If there are no `0`s, return `-1`. # Constraints * The length of `arr` will be in the range `[1, 10^5]`. * `arr` will only contain `0`s and `1`s. # Examples 1. `arr = [1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1]` should return `3`. 2. `arr = [1, 1, 0, 0, 1, 0]` should return `2`. 3. `arr = [1, 1, 1, 1, 1]` should return `-1`. 4. `arr = [0, 0, 0, 0, 0]` should return `0`. # Notes 1. You must ensure the function runs in O(n) time complexity and O(1) space complexity. 2. Make sure to handle arrays with no `0`s gracefully by returning `-1`.","solution":"from typing import List def max_ones_index(arr: List[int]) -> int: max_count = 0 max_index = -1 prev_zero_index = -1 prev_prev_zero_index = -1 zero_count = 0 n = len(arr) for i in range(n): if arr[i] == 0: zero_count += 1 if zero_count == 2: count = i - prev_prev_zero_index - 1 if count > max_count: max_count = count max_index = prev_zero_index prev_prev_zero_index = prev_zero_index zero_count = 1 prev_zero_index = i count = n - prev_prev_zero_index - 1 if count > max_count: max_count = count max_index = prev_zero_index return max_index"},{"question":"# Context: You have been hired to improve the efficiency of a program that requires manipulating states represented as bits in a 32-bit integer. Your task is to implement an efficient bitwise operation function suite to facilitate this requirement. # Task: Implement a function `bit_operations` that given an initial 32-bit integer `num`, and a list of operations to perform on it, finalizes and returns the resultant integer after all operations. # Requirements: You will be provided with: - An initial integer `num` (representing a 32-bit number). - A list of operations that are tuples where the first element is a string indicating the operation (\\"get\\", \\"set\\", \\"clear\\", \\"update\\"), and the following elements are arguments for that operation. Each operation is defined as: 1. **get (num, i)**: Returns the bit at index `i`. 2. **set (num, i)**: Sets the bit at index `i` to 1. 3. **clear (num, i)**: Clears the bit at index `i` to 0. 4. **update (num, i, bit)**: Updates the bit at index `i` with the specified bit value (either 0 or 1). Return the final integer `num` after performing all the operations. Ignore \\"get\\" operations in the final returned value. # Input: - `num` (integer): A non-negative integer representing a 32-bit number. - `operations` (list of tuples): A list where each tuple represents an operation. # Output: - Return the integer `num` after performing all the given operations. # Constraints: - `0 <= num < 2^32` - `0 <= i < 32` - `bit` is either 0 or 1 for update operations. # Example: ```python # Example Input num = 5 operations = [(\\"set\\", 1), (\\"clear\\", 0), (\\"update\\", 2, 1), (\\"get\\", 2)] # Example Output num after operations: 6 # Explanation: # Initial num = 5 (binary 101) # set(1): num becomes 7 (binary 111) # clear(0): num becomes 6 (binary 110) # update(2, 1): num remains 6 (binary 110) # get(2): returns 1 (binary 110 at index 2 is 1); this result is ignored in the final output. ``` Implement: ```python def bit_operations(num, operations): def get_bit(num, i): return (num & (1 << i)) != 0 def set_bit(num, i): return num | (1 << i) def clear_bit(num, i): mask = ~(1 << i) return num & mask def update_bit(num, i, bit): mask = ~(1 << i) return (num & mask) | (bit << i) for op in operations: if op[0] == \\"get\\": get_bit(num, op[1]) # this result can be printed if necessary, but is ignored in final num. if op[0] == \\"set\\": num = set_bit(num, op[1]) if op[0] == \\"clear\\": num = clear_bit(num, op[1]) if op[0] == \\"update\\": num = update_bit(num, op[1], op[2]) return num # Example: print(bit_operations(5, [(\\"set\\", 1), (\\"clear\\", 0), (\\"update\\", 2, 1), (\\"get\\", 2)])) # Output: 6 ```","solution":"def bit_operations(num, operations): def get_bit(num, i): return (num & (1 << i)) != 0 def set_bit(num, i): return num | (1 << i) def clear_bit(num, i): mask = ~(1 << i) return num & mask def update_bit(num, i, bit): mask = ~(1 << i) return (num & mask) | (bit << i) for op in operations: if op[0] == \\"get\\": get_bit(num, op[1]) # this result can be printed if necessary, but is ignored in final num. if op[0] == \\"set\\": num = set_bit(num, op[1]) if op[0] == \\"clear\\": num = clear_bit(num, op[1]) if op[0] == \\"update\\": num = update_bit(num, op[1], op[2]) return num"},{"question":"# Question: Minimum Deletions for Two Strings Given two words `word1` and `word2`, write a function `min_deletions(word1, word2)` that finds the minimum number of steps required to make `word1` and `word2` the same, where in each step you can delete one character in either string. Input - `word1` and `word2`: Two strings composed of lowercase English letters (1 <= len(word1), len(word2) <= 1000). Output - An integer representing the minimum number of steps required. Example ```python assert min_deletions(\\"sea\\", \\"eat\\") == 2 # Explanation: \\"sea\\" -> \\"ea\\" (1 deletion), \\"eat\\" -> \\"ea\\" (1 deletion) assert min_deletions(\\"leetcode\\", \\"etco\\") == 4 ``` Constraints - You can assume that both strings are composed only of lowercase English letters. - The function should run efficiently for strings up to the maximum constraint. Requirements - Implement the solution using a dynamic programming approach for optimal performance. - The solution should have a time complexity of O(len(word1) * len(word2)) and a space complexity of O(len(word1) * len(word2)). # Function Signature ```python def min_deletions(word1: str, word2: str) -> int: # Implement your function here ``` Solution Tips 1. Use a 2D dynamic programming table where res[i][j] represents the minimum number of deletions for substrings word1[0..i-1] and word2[0..j-1]. 2. Initialize the first row and column of the table based on the need to delete characters to match an empty string. 3. Fill the table by checking the characters of word1 and word2: - If the characters match, res[i][j] = res[i-1][j-1] - Else, res[i][j] = min(res[i-1][j], res[i][j-1]) + 1 4. The value at res[len(word1)][len(word2)] gives the required result. Good luck!","solution":"def min_deletions(word1: str, word2: str) -> int: m, n = len(word1), len(word2) # Create a DP table to store the results of subproblems dp = [[0] * (n + 1) for _ in range(m + 1)] # Initialize the DP table for i in range(1, m + 1): dp[i][0] = i for j in range(1, n + 1): dp[0][j] = j # Fill the DP table for i in range(1, m + 1): for j in range(1, n + 1): if word1[i - 1] == word2[j - 1]: dp[i][j] = dp[i - 1][j - 1] else: dp[i][j] = min(dp[i - 1][j], dp[i][j - 1]) + 1 return dp[m][n]"},{"question":"You are given a string `pattern` and a string `str`. The goal is to determine if `str` follows the exact pattern defined by `pattern`. A string `str` follows a pattern if there is a bijection between letters in `pattern` and non-empty substrings in `str`. You need to implement the function `isPatternMatch` that checks if `str` adheres to the pattern defined by `pattern`. # Function Signature ```python def isPatternMatch(pattern: str, str: str) -> bool: pass ``` # Input * `pattern` (0 <= length <= 20): A non-empty string containing only lowercase letters. * `str` (0 <= length <= 1000): A non-empty string containing only lowercase letters. # Output * Returns a boolean value indicating whether `str` follows the same pattern as `pattern`. # Examples ```python assert isPatternMatch(\\"abab\\", \\"redblueredblue\\") == True assert isPatternMatch(\\"aaaa\\", \\"asdasdasdasd\\") == True assert isPatternMatch(\\"aabb\\", \\"xyzabcxzyabc\\") == False ``` # Constraints * Assume both `pattern` and `str` contain only lowercase letters. * There must be a one-to-one correspondence between the characters in the pattern and the non-empty substrings in `str`. # Performance Requirements The implemented function should be optimized to handle inputs efficiently within the given constraints. # Additional Context This problem is useful to demonstrate understanding of backtracking algorithms and ensuring that students handle edge cases effectively, such as empty patterns and strings or when there is no possible bijection.","solution":"def isPatternMatch(pattern: str, str: str) -> bool: def backtrack(pattern_index, str_index, pattern_to_string, string_to_pattern): # If we have reached the end of both the pattern and the string, it means we found a valid match if pattern_index == len(pattern) and str_index == len(str): return True # If either the pattern or the string is exhausted but not the other, no match is possible if pattern_index == len(pattern) or str_index == len(str): return False current_pattern_char = pattern[pattern_index] for end_index in range(str_index + 1, len(str) + 1): current_substr = str[str_index:end_index] if current_pattern_char in pattern_to_string: if pattern_to_string[current_pattern_char] == current_substr: if backtrack(pattern_index + 1, end_index, pattern_to_string, string_to_pattern): return True else: if current_substr in string_to_pattern: continue pattern_to_string[current_pattern_char] = current_substr string_to_pattern[current_substr] = current_pattern_char if backtrack(pattern_index + 1, end_index, pattern_to_string, string_to_pattern): return True del pattern_to_string[current_pattern_char] del string_to_pattern[current_substr] return False return backtrack(0, 0, {}, {})"},{"question":"You are tasked with implementing and optimizing the run-length encoding (RLE) algorithm for data compression, as well as its corresponding decoding algorithm. The challenge involves writing two functions: 1. `encode_rle(input: str) -> str`: - **Input**: A string `input` consisting of alphanumeric characters, which may include both uppercase and lowercase letters, and digits. - **Output**: A string that represents the run-length encoded version of the input. - **Constraints**: * The input string can have up to 10^5 characters. * The encoded output should be as compact as possible. 2. `decode_rle(input: str) -> str`: - **Input**: A string `input` which is the encoded version using RLE. - **Output**: The original string before encoding. - **Constraints**: * The input string can have up to 10^5 characters in length. * It is guaranteed that the encoded input is valid. # Implementation Notes: - Consider edge cases such as empty inputs or strings with no repetitive sequences. - Make sure the encoding and decoding processes are efficient and correctly handle all valid inputs. - Ensure that the encoded string is as compact as possible. # Example: ```python assert encode_rle(\\"aaabbbbcc\\") == \\"3a4b2c\\" assert decode_rle(\\"3a4b2c\\") == \\"aaabbbbcc\\" assert encode_rle(\\"abcd\\") == \\"1a1b1c1d\\" assert decode_rle(\\"1a1b1c1d\\") == \\"abcd\\" assert encode_rle(\\"\\") == \\"\\" assert decode_rle(\\"\\") == \\"\\" ```","solution":"def encode_rle(input: str) -> str: Encodes the input string using run-length encoding (RLE). Parameters: input (str): The input string to be encoded. Returns: str: The run-length encoded string. if not input: return \\"\\" encoded = [] count = 1 for i in range(1, len(input)): if input[i] == input[i - 1]: count += 1 else: encoded.append(f\\"{count}{input[i - 1]}\\") count = 1 encoded.append(f\\"{count}{input[-1]}\\") return \'\'.join(encoded) def decode_rle(input: str) -> str: Decodes the run-length encoded string back to its original form. Parameters: input (str): The run-length encoded string. Returns: str: The original string before encoding. if not input: return \\"\\" decoded = [] count = 0 for char in input: if char.isdigit(): count = count * 10 + int(char) else: decoded.append(char * count) count = 0 return \'\'.join(decoded)"},{"question":"# Context You are tasked with developing a navigation system for a new smart city. The city is represented as a graph where intersections are nodes and roads between intersections are edges with weights representing the travel time. The system should calculate the shortest travel time between all pairs of intersections. # Task Write a function that uses the Floyd-Warshall algorithm to compute the shortest travel times between each pair of intersections. Function Signature ```python def compute_all_pairs_shortest_paths(adj_matrix: List[List[float]]) -> List[List[float]]: pass ``` Input - A square matrix `adj_matrix` of size `N x N`, where `adj_matrix[i][j]` represents the travel time from intersection `i` to intersection `j`. If there is no direct road between intersections, the value is a large number representing infinity (use `float(\'inf\')`). Output - Return a matrix `result_matrix` of the same size, where `result_matrix[i][j]` represents the shortest travel time from intersection `i` to intersection `j`. Example ```python adj_matrix = [ [0, 3, float(\'inf\'), 5], [2, 0, float(\'inf\'), 4], [float(\'inf\'), 1, 0, float(\'inf\')], [float(\'inf\'), float(\'inf\'), 2, 0] ] output = compute_all_pairs_shortest_paths(adj_matrix) assert output == [ [0, 3, 7, 5], [2, 0, 6, 4], [3, 1, 0, 5], [5, 3, 2, 0] ] ``` Constraints - `1 <= N <= 100` (Size of the adjacency matrix) - Travel time will be a non-negative integer. - The diagonal values of the adjacency matrix will always be 0. Performance Requirements - The function must run efficiently for input matrices up to size 100x100. Notes - Ensure to handle graphs with no direct path appropriately using `float(\'inf\')`.","solution":"from typing import List def compute_all_pairs_shortest_paths(adj_matrix: List[List[float]]) -> List[List[float]]: Uses the Floyd-Warshall algorithm to compute the shortest travel times between all pairs of intersections. Parameters: adj_matrix (List[List[float]]): A square matrix where adj_matrix[i][j] is the travel time from intersection i to j. Returns: List[List[float]]: A matrix where result_matrix[i][j] is the shortest travel time from intersection i to j. # Number of vertices in the graph n = len(adj_matrix) # Initialize the distance matrix with the adjacency matrix dist = [row[:] for row in adj_matrix] # Update the distance matrix for all pairs (i, j) for k in range(n): for i in range(n): for j in range(n): # Update dist[i][j] with the minimum distance between i and j dist[i][j] = min(dist[i][j], dist[i][k] + dist[k][j]) return dist"},{"question":"You are given a non-negative integer number. Write a function `count_digits` that returns the number of digits in the number. You must achieve this using a logarithmic approach, similar to a given code snippet which uses `math.log10()`. Function Signature: ```python def count_digits(n: int) -> int: pass ``` # Input * An integer `n` (0 <= n <= 10^18). # Output * An integer representing the number of digits in `n`. # Examples 1. count_digits(123) -> 3 2. count_digits(0) -> 1 3. count_digits(789456) -> 6 4. count_digits(1000000000000000000) -> 19 # Constraints * You are required to use logarithmic-based methods for determining the number of digits. * The implementation should handle the largest values efficiently with O(1) time complexity. * Ensure to handle edge cases like `0` appropriately. # Scenario: Consider an application where you need to represent numbers in various formatting configurations, ensuring every number is accounted for with the correct number of digits. # Hint: Use the `math.log10()` function, but handle special cases like zero which logarithms can\'t naturally deal with.","solution":"import math def count_digits(n: int) -> int: Returns the number of digits in the given non-negative integer. Uses a logarithmic approach, handling the special case for 0. if n == 0: return 1 return int(math.log10(n)) + 1"},{"question":"# Question: Last Occurrence Finder in a Sorted Array You are provided with a sorted array (in increasing order) and a target number. Your task is to implement a function `last_occurrence(array, query)` that returns the index of the last occurrence of the target number in the array. Input: - `array`: A list of integers sorted in increasing order. - `query`: An integer representing the target number to find. Output: - Return the zero-based index of the last occurrence of the target number in the array. - If the target number is not found, return `-1`. Constraints: - The input list `array` will have at most `10^6` elements. - All elements in `array` are integers. - The `array` can contain duplicate elements. - The `query` will always be an integer. Performance Requirements: - Your solution should have a time complexity of O(log n). Example: ```python print(last_occurrence([1, 2, 2, 2, 3, 4, 5], 2)) # Output: 3 print(last_occurrence([1, 2, 3, 4, 5], 6)) # Output: -1 print(last_occurrence([1, 1, 1, 1, 1], 1)) # Output: 4 ``` # Note: You must handle edge cases such as: - The `array` being empty. - The `query` not being present in the `array`. - The `array` containing only one element which might be equal or not equal to the `query`.","solution":"def last_occurrence(array, query): Returns the index of the last occurrence of the target number in the sorted array. If the target number is not found, returns -1. left, right = 0, len(array) - 1 result = -1 while left <= right: mid = left + (right - left) // 2 if array[mid] == query: result = mid left = mid + 1 # Continue searching in the right half elif array[mid] < query: left = mid + 1 else: right = mid - 1 return result"},{"question":"# Scenario Imagine you are part of a software engineering team responsible for maintaining a large enterprise system. This system uses a Binary Search Tree (BST) to manage a dynamically changing list of user sessions where each session has a unique identifier. You have been tasked with writing a function that can remove any given session from the BST efficiently. # Problem Statement Write a function `delete_node` that takes the root node of a Binary Search Tree (BST) and a key (integer), and deletes the node with the given key from the BST. The function should return the new root of the BST after the deletion. # Implementation Details * **Input**: * `root` (TreeNode): The root node reference of the BST. * `key` (int): The key of the node to be deleted. * **Output**: * Returns the root node reference of the BST after deletion. * **Constraints**: * At most, there will be 1000 nodes in the BST. * All node values are unique. * The keys and node values are integers. * **Performance Requirements**: * Your implementation should have an average time complexity of O(log n), with a worst case of O(n) if the BST is not balanced. # Example ```python root = [5,3,6,2,4,null,7] key = 3 # BST before deletion: # 5 # / # 3 6 # / # 2 4 7 # BST after deleting node with key 3: # 5 # / # 4 6 # / # 2 7 # In another case, # BST after deleting node with key 3: # 5 # / # 2 6 # # 4 7 ``` # Function Signature ```python class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = None class Solution: def delete_node(self, root: TreeNode, key: int) -> TreeNode: # Your implementation here ``` # Testing Ensure your implementation correctly handles all edge cases, such as attempting to delete a non-existent node, deleting the root node, and deleting nodes with one or no children.","solution":"class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = None class Solution: def delete_node(self, root: TreeNode, key: int) -> TreeNode: if not root: return root if key < root.val: root.left = self.delete_node(root.left, key) elif key > root.val: root.right = self.delete_node(root.right, key) else: if not root.left: return root.right elif not root.right: return root.left min_larger_node = self.get_min(root.right) root.val = min_larger_node.val root.right = self.delete_node(root.right, root.val) return root def get_min(self, node: TreeNode) -> TreeNode: while node.left: node = node.left return node"},{"question":"You are required to implement a `DoublyLinkedList` and `SinglyLinkedList` classes with methods to insert, delete, and search for elements. Additionally, write methods to reverse a singly linked list. # Detailed Requirements: 1. **DoublyLinkedList** - Implement `insert(value, position)` to insert a value at a given position. - Implement `delete(position)` to delete the node at the given position. - Implement `search(value)` to find the position of a value in the list. 2. **SinglyLinkedList** - Implement `insert(value, position)` for inserting a value at a given position. - Implement `delete(position)` for deleting a node at a specific position. - Implement `search(value)` to find the position of a value in the list. - Implement `reverse()` to reverse the list. # Input and Output Formats: - **insert(value, position)**: - **Input**: Integer value, Integer position (0-based index). - **Output**: None. - **delete(position)**: - **Input**: Integer position (0-based index). - **Output**: None. - **search(value)**: - **Input**: Integer value. - **Output**: Integer position (0-based index) of the value, or -1 if not found. - **reverse()**: - **Input**: None. - **Output**: None. # Constraints: - Assume the position for insertion and deletion operations is always valid. - List size will be at most 10000 elements. # Example Usage: ```python dll = DoublyLinkedList() dll.insert(10, 0) dll.insert(20, 1) dll.insert(15, 1) assert dll.search(20) == 2 dll.delete(1) assert dll.search(15) == -1 sll = SinglyLinkedList() sll.insert(1, 0) sll.insert(2, 1) sll.insert(3, 2) assert sll.search(2) == 1 sll.reverse() assert sll.search(2) == 1 ``` Please implement the classes `DoublyLinkedList` and `SinglyLinkedList` based on the given specifications.","solution":"class DoublyLinkedList: class Node: def __init__(self, value): self.value = value self.next = None self.prev = None def __init__(self): self.head = None def insert(self, value, position): new_node = self.Node(value) if position == 0: new_node.next = self.head if self.head: self.head.prev = new_node self.head = new_node else: current = self.head for _ in range(position - 1): current = current.next new_node.next = current.next new_node.prev = current if current.next: current.next.prev = new_node current.next = new_node def delete(self, position): if position == 0: self.head = self.head.next if self.head: self.head.prev = None else: current = self.head for _ in range(position - 1): current = current.next to_delete = current.next current.next = to_delete.next if to_delete.next: to_delete.next.prev = current def search(self, value): current = self.head position = 0 while current: if current.value == value: return position current = current.next position += 1 return -1 class SinglyLinkedList: class Node: def __init__(self, value): self.value = value self.next = None def __init__(self): self.head = None def insert(self, value, position): new_node = self.Node(value) if position == 0: new_node.next = self.head self.head = new_node else: current = self.head for _ in range(position - 1): current = current.next new_node.next = current.next current.next = new_node def delete(self, position): if position == 0: self.head = self.head.next else: current = self.head for _ in range(position - 1): current = current.next current.next = current.next.next def search(self, value): current = self.head position = 0 while current: if current.value == value: return position current = current.next position += 1 return -1 def reverse(self): prev = None current = self.head while current: next_node = current.next current.next = prev prev = current current = next_node self.head = prev"},{"question":"**Title**: Implement and Optimize a Word Dictionary with Trie and HashMap **Context**: You are developing a text prediction and analysis feature for a word processor. This requires a data structure that allows for efficient word addition and search functionality, supporting wildcard characters. **Task**: Implement the `WordDictionary` class using both Trie and Dictionary (HashMap) data structures as methods: `add_word` to add words and `search` to search words with support for the wildcard character `\'.\'`. Your implementations should handle the following requirements efficiently: 1. Add and store words. 2. Search words with potential wildcard characters (`\'.\'` can be any letter). **Function Signature**: ```python class WordDictionary: def __init__(self): # Initialize your data structure def add_word(self, word: str) -> None: # Add word to data structure def search(self, word: str) -> bool: # Search word with or without wildcard ``` # Input / Output - `add_word(word: str)`: - **Input**: A string `word`. - **Output**: Adds the word to the data structure. No return value. - `search(word: str)`: - **Input**: A string `word` possibly containing \'.\'. - **Output**: Returns `True` if the word is found in the data structure, `False` otherwise. # Constraints: - All words contain only lowercase alphabets. - Words will have lengths in the range `[1, 100]`. - The number of words added will not exceed `10^4`. # Performance Requirement: - The implementation should aim for an efficient search operation even when wildcards are used frequently. # Example: ```python wd = WordDictionary() wd.add_word(\\"bad\\") wd.add_word(\\"dad\\") wd.add_word(\\"mad\\") assert wd.search(\\"pad\\") == False assert wd.search(\\"bad\\") == True assert wd.search(\\".ad\\") == True assert wd.search(\\"b..\\") == True ``` # Evaluation Criteria: - Correctness of `add_word` and `search` methods. - Handling of edge cases and constraints. - Efficiency of implementation in terms of time and space complexity.","solution":"class TrieNode: def __init__(self): self.children = {} self.is_end_of_word = False class WordDictionary: def __init__(self): self.root = TrieNode() def add_word(self, word: str) -> None: current_node = self.root for char in word: if char not in current_node.children: current_node.children[char] = TrieNode() current_node = current_node.children[char] current_node.is_end_of_word = True def search(self, word: str) -> bool: return self._search_in_node(word, 0, self.root) def _search_in_node(self, word, index, node): current_node = node for i in range(index, len(word)): char = word[i] if char == \\".\\": for child in current_node.children.values(): if self._search_in_node(word, i + 1, child): return True return False else: if char not in current_node.children: return False current_node = current_node.children[char] return current_node.is_end_of_word"},{"question":"Given a singly linked list, your task is to write a function that removes all duplicate elements from the list. You are required to implement both: 1. An efficient version using additional space (`remove_dups` function). 2. A space-optimized version without additional data structures (`remove_dups_without_set` function). # Function Signature ```python class Node: def __init__(self, val=None): self.val = val self.next = None def remove_dups(head: Node) -> None: Removes duplicates from the linked list using additional space. Args: head (Node): The head node of the linked list. Returns: None. Modifies the linked list in-place. def remove_dups_without_set(head: Node) -> None: Removes duplicates from the linked list without using additional space. Args: head (Node): The head node of the linked list. Returns: None. Modifies the linked list in-place. ``` # Constraints 1. The linked list is composed of nodes where each node contains an integer or string as a value. 2. The list does not contain a cycle. 3. Each function must run and modify the linked list in place. # Input/Output Formats * Input: Singly linked list starting at `head`. * Output: None. Modifies the linked list directly. # Examples ```python # Creating the linked list: A -> A -> B -> C -> D -> C -> F -> G head = Node(\\"A\\") head.next = Node(\\"A\\") head.next.next = Node(\\"B\\") head.next.next.next = Node(\\"C\\") head.next.next.next.next = Node(\\"D\\") head.next.next.next.next.next = Node(\\"C\\") head.next.next.next.next.next.next = Node(\\"F\\") head.next.next.next.next.next.next.next = Node(\\"G\\") remove_dups(head) # After remove_dups, the linked list should be: A -> B -> C -> D -> F -> G head = Node(\\"A\\") head.next = Node(\\"A\\") head.next.next = Node(\\"B\\") head.next.next.next = Node(\\"C\\") head.next.next.next.next = Node(\\"D\\") head.next.next.next.next.next = Node(\\"C\\") head.next.next.next.next.next.next = Node(\\"F\\") head.next.next.next.next.next.next.next = Node(\\"G\\") remove_dups_without_set(head) # After remove_dups_without_set, the linked list should be: A -> B -> C -> D -> F -> G ``` # Notes * It is highly recommended to consider edge cases such as lists with no duplicates, completely duplicated lists, and various list lengths, starting from an empty list to larger datasets.","solution":"class Node: def __init__(self, val=None): self.val = val self.next = None def remove_dups(head: Node) -> None: Removes duplicates from the linked list using additional space. Args: head (Node): The head node of the linked list. Returns: None. Modifies the linked list in-place. if not head: return current = head seen = set() seen.add(current.val) while current.next: if current.next.val in seen: current.next = current.next.next else: seen.add(current.next.val) current = current.next def remove_dups_without_set(head: Node) -> None: Removes duplicates from the linked list without using additional space. Args: head (Node): The head node of the linked list. Returns: None. Modifies the linked list in-place. current = head while current: runner = current while runner.next: if runner.next.val == current.val: runner.next = runner.next.next else: runner = runner.next current = current.next"},{"question":"You are asked to implement Tarjan\'s Algorithm to find all strongly connected components (SCCs) in a directed graph. A SCC is a maximal subgraph in which any two vertices are reachable from each other. # Input Format: - The input will consist of an integer ( n ) (the number of vertices). - The next ( n ) lines each will have vertex identifiers. - The next line will contain an integer ( e ) (the number of edges). - The following ( e ) lines each will contain two integers representing directed edges of the graph. # Output Format: - The output should be a list of lists, where each inner list contains the vertices belonging to a single SCC, sorted in ascending order. - SCCs should be listed in any order. # Constraints: - ( 1 leq n leq 10^4 ) - ( 1 leq e leq 10^5 ) - Vertex identifiers will be unique integers ranging from 0 to ( n-1 ). # Example: Input: ``` 5 0 1 2 3 4 5 5 1 5 2 3 4 2 3 4 0 3 2 ``` Output: ``` [[0, 1, 2, 3, 4, 5]] ``` # Requirements: - Implement the function `find_sccs(graph: List[Tuple[int, int]], n: int) -> List[List[int]]` to perform SCC identification using Tarjan\'s Algorithm. - Ensure your solution handles graphs efficiently given the constraints. # Scenario: Consider you are developing a dependency analyzer tool for a large project. Your task is the core component of the analyzer which helps in detecting cyclic dependencies among the components.","solution":"from typing import List, Tuple def find_sccs(graph: List[Tuple[int, int]], n: int) -> List[List[int]]: def tarjan_scc(v): nonlocal index index_map[v] = lowlink_map[v] = index index += 1 stack.append(v) on_stack[v] = True for neighbor in adjacency_list[v]: if index_map[neighbor] is None: tarjan_scc(neighbor) lowlink_map[v] = min(lowlink_map[v], lowlink_map[neighbor]) elif on_stack[neighbor]: lowlink_map[v] = min(lowlink_map[v], index_map[neighbor]) if lowlink_map[v] == index_map[v]: scc = [] while True: w = stack.pop() on_stack[w] = False scc.append(w) if w == v: break sccs.append(sorted(scc)) adjacency_list = [[] for _ in range(n)] for u, v in graph: adjacency_list[u].append(v) index = 0 index_map = [None] * n lowlink_map = [None] * n stack = [] on_stack = [False] * n sccs = [] for v in range(n): if index_map[v] is None: tarjan_scc(v) return sccs"},{"question":"# Zigzag Iterator Implementation Problem Statement Given multiple lists (`v1`, `v2`, ... `vn`), implement a Zigzag Iterator that returns elements from these lists in a zigzag order. The Zigzag order iterates through elements of each list one by one from beginning to end. Function to Implement ```python class ZigZagIterator: def __init__(self, *args): Initialize the iterator with multiple lists. :param args: Multiple lists def next(self): Returns the next element in the zigzag iteration. :rtype: int def has_next(self): Returns whether the iterator still has elements. :rtype: bool ``` Input and Output Format * **Input**: * Multiple lists (composed of integers) * **Output**: * The elements of the lists in zigzag order Example ```python l1 = [1, 2] l2 = [3, 4, 5, 6] it = ZigZagIterator(l1, l2) result = [] while it.has_next(): result.append(it.next()) print(result) # [1, 3, 2, 4, 5, 6] ``` Constraints * Lists can be of different lengths. * Consider edge cases like empty lists or all inputs being empty. * Expected to maintain O(1) time complexity for `next` and `has_next` operations and O(k) space complexity where k is the number of input lists. Performance Requirements * Ensure efficiency in handling multiple lists iteration and edge cases effectively.","solution":"class ZigZagIterator: def __init__(self, *args): Initialize the iterator with multiple lists. :param args: Multiple lists self.data = [list(a) for a in args if a] # Initialize with non-empty lists and convert them to lists. self.indexes = [0] * len(self.data) # Track the current index for each list. self.total_elements = sum(len(lst) for lst in self.data) # Total elements to iterate over. self.current_list = 0 # Start with the first list. def next(self): Returns the next element in the zigzag iteration. :rtype: int if not self.has_next(): raise StopIteration(\\"No more elements\\") while self.indexes[self.current_list] >= len(self.data[self.current_list]): self.current_list = (self.current_list + 1) % len(self.data) value = self.data[self.current_list][self.indexes[self.current_list]] self.indexes[self.current_list] += 1 self.total_elements -= 1 self.current_list = (self.current_list + 1) % len(self.data) return value def has_next(self): Returns whether the iterator still has elements. :rtype: bool return self.total_elements > 0"},{"question":"Problem Statement: You are given the task to implement a function that computes the number of ways to choose `r` items from `n` items, also known as combinations or binomial coefficients, using a dynamic programming approach. This problem requires you to optimize the calculation such that it handles large inputs efficiently. # Function Signature: ```python def combination_dp(n: int, r: int) -> int: pass ``` # Input: * `n` (integer): Total number of items (0 ≤ n ≤ 1000). * `r` (integer): Number of items to choose (0 ≤ r ≤ n). # Output: * Returns the value of `nCr`. # Example: ```python assert combination_dp(5, 2) == 10 assert combination_dp(10, 5) == 252 assert combination_dp(20, 0) == 1 assert combination_dp(0, 0) == 1 assert combination_dp(0, 1) == 0 # Edge case where n < r ``` # Constraints: 1. You need to implement the dynamic programming approach. 2. The solution must run efficiently for the upper limit of n and r. 3. You need to handle edge cases where `r` is 0, `n` is less than `r`, and `n` equals `r`. # Guidelines: 1. Use a 2D list (or similar construct) to store previously computed combinations. 2. Iterate over the range to fill this table using base case conditions and recursive relations. 3. Ensure that the data structure used to store previous results is efficient and properly indexed. # Performance Requirements: * Time Complexity: ( O(n times r) ) * Space Complexity: ( O(n times r) )","solution":"def combination_dp(n: int, r: int) -> int: # Edge case where n < r if n < r: return 0 # Create a 2D array to store the values of nCr C = [[0 for _ in range(r + 1)] for _ in range(n + 1)] # Compute the combinations using dynamic programming for i in range(n + 1): for k in range(min(i, r) + 1): # Base cases if k == 0 or k == i: C[i][k] = 1 else: # Recursive relation C(n, r) = C(n-1, r-1) + C(n-1, r) C[i][k] = C[i - 1][k - 1] + C[i - 1][k] return C[n][r]"},{"question":"# Question: Enhanced Graph Pathfinding You are given an **unweighted directed graph** represented as an adjacency list, and your task is to implement three functions that find paths between nodes in different ways using the given algorithms. Task 1: Finding a Path Implement the function `find_path(graph: Dict[str, List[str]], start: str, end: str) -> Optional[List[str]]` that finds one path from the start node to the end node. If such a path exists, return it as a list of nodes. If no such path exists, return `None`. Task 2: Finding All Paths Implement the function `find_all_paths(graph: Dict[str, List[str]], start: str, end: str) -> List[List[str]]` that finds all possible paths from the start node to the end node. Return these paths as a list of lists of nodes. If no paths exist, return an empty list. Task 3: Finding the Shortest Path Implement the function `find_shortest_path(graph: Dict[str, List[str]], start: str, end: str) -> Optional[List[str]]` that finds the shortest path from the start node to the end node. If such a path exists, return it as a list of nodes. If no such path exists, return `None`. # Input/Output Specification * Input: A dictionary representing the adjacency list of the graph, a starting node, and an ending node. * Output: * `find_path`: A list of nodes representing one path from start to end, or `None` if no path exists. * `find_all_paths`: A list of lists, where each list represents a unique path from start to end, or an empty list if no paths exist. * `find_shortest_path`: A list of nodes representing the shortest path from start to end, or `None` if no path exists. # Constraints * Nodes are represented as strings. * The graph may contain cycles. * The graph can have up to 1000 nodes and 10000 edges. # Performance Requirements * Efficient handling of up to 1000 nodes and 10000 edges. * Minimize stack depth in recursive solutions wherever possible. # Example ```python graph = { \'A\': [\'B\', \'C\'], \'B\': [\'D\'], \'C\': [\'D\'], \'D\': [\'E\'], \'E\': [] } # Task 1 Example print(find_path(graph, \'A\', \'E\')) # Output: [\'A\', \'B\', \'D\', \'E\'] or [\'A\', \'C\', \'D\', \'E\'] # Task 2 Example print(find_all_paths(graph, \'A\', \'E\')) # Output: [[\'A\', \'B\', \'D\', \'E\'], [\'A\', \'C\', \'D\', \'E\']] # Task 3 Example print(find_shortest_path(graph, \'A\', \'E\')) # Output: [\'A\', \'B\', \'D\', \'E\'] or [\'A\', \'C\', \'D\', \'E\'] ``` # Notes: 1. For Task 1 and Task 3, any valid path and the shortest path should be returned respectively. 2. In case of multiple shortest paths for Task 3, any one of them is acceptable.","solution":"from typing import Dict, List, Optional def find_path(graph: Dict[str, List[str]], start: str, end: str) -> Optional[List[str]]: Finds one path from the start node to the end node. path = [] visited = set() def dfs(current: str) -> bool: if current in visited: return False visited.add(current) path.append(current) if current == end: return True for neighbor in graph.get(current, []): if dfs(neighbor): return True path.pop() return False if dfs(start): return path return None def find_all_paths(graph: Dict[str, List[str]], start: str, end: str) -> List[List[str]]: Finds all possible paths from the start node to the end node. all_paths = [] path = [] def dfs(current: str): path.append(current) if current == end: all_paths.append(path.copy()) else: for neighbor in graph.get(current, []): dfs(neighbor) path.pop() dfs(start) return all_paths def find_shortest_path(graph: Dict[str, List[str]], start: str, end: str) -> Optional[List[str]]: Finds the shortest path from the start node to the end node. from collections import deque queue = deque([(start, [start])]) visited = set([start]) while queue: current, path = queue.popleft() if current == end: return path for neighbor in graph.get(current, []): if neighbor not in visited: visited.add(neighbor) queue.append((neighbor, path + [neighbor])) return None"},{"question":"# Question: You are tasked with writing a function that finds the next higher number which has the exact same set of digits as the original number. This function is useful in solving permutation-related problems and sequencing issues in numerical contexts. Function Signature ```python def next_bigger(num: int) -> int: ``` Input * `num` (1 <= num <= 10^18): An integer number. Output * Returns an integer representing the next higher number with the exact same set of digits as `num`. If no such number exists, return -1. Constraints 1. Your solution should aim for O(n) time complexity. 2. You should handle edge cases efficiently. Example Scenarios * Example 1: - Input: 38276 - Output: 38627 * Example 2: - Input: 99999 - Output: -1 (Since no such number exists) * Example 3: - Input: 12345 - Output: 12354 * Example 4: - Input: 54321 - Output: -1 (Already in the descending order) * Example 5: - Input: 1 - Output: -1 (Only one digit) Testing Your implementation should correctly handle and pass the following tests: ``` assert next_bigger(38276) == 38627 assert next_bigger(12345) == 12354 assert next_bigger(1528452) == 1528524 assert next_bigger(138654) == 143568 assert next_bigger(54321) == -1 assert next_bigger(999) == -1 assert next_bigger(5) == -1 ``` Hint Use a sequence of operations involving scanning from the right, identifying pivot points, swapping values, and reversing sequences to achieve the desired outcome.","solution":"def next_bigger(num: int) -> int: Returns the next higher number with the exact same set of digits as the given number. If no such number exists, it returns -1. digits = list(str(num)) n = len(digits) i = n - 2 # Find the first digit that is smaller than the digit next to it, from the end while i >= 0 and digits[i] >= digits[i + 1]: i -= 1 if i == -1: return -1 # Find the smallest digit on right side of digits[i] that is larger than digits[i] j = n - 1 while digits[j] <= digits[i]: j -= 1 # Swap digits[i] with digits[j] digits[i], digits[j] = digits[j], digits[i] # Reverse the digits after i digits = digits[:i + 1] + digits[i + 1:][::-1] return int(\\"\\".join(digits))"},{"question":"**Objective**: Demonstrate your understanding of the sliding window maximum algorithm. **Scenario**: You are given an array of integers where each element represents the temperature on a particular day. Your task is to design a function that finds the highest temperatures recorded in each sub-array (or window) of length k as you slide the window from the start to the end of the array. **Function to Implement**: ```python def max_sliding_window(arr, k): Given an array and a number k, find the maximum elements of each of its sub-arrays of length k. Parameters: arr: A list of integers representing temperatures. k: An integer, the length of the sliding window. Returns: result: A list of integers representing the maximum elements in each window of length k. Constraints: - 1 <= k <= len(arr) - The array is non-empty and contains at least k elements. - All array elements are integers (-10^5 <= arr[i] <= 10^5). - The input array can have up to 10^5 elements. pass ``` **Input/Output Specifications**: - **Input**: - `arr`: A list of integers `[a1, a2, ..., an]`. - `k`: An integer representing the window size. - **Output**: - A list of integers `[m1, m2, ..., mn-k+1]` where each mi is the maximum from the sub-array starting at index i and of length k. **Example**: ```python # Example 1: arr = [1, 3, -1, -3, 5, 3, 6, 7] k = 3 # Expected Output: [3, 3, 5, 5, 6, 7] # Example 2: arr = [4, 4, 4, 4, 4] k = 1 # Expected Output: [4, 4, 4, 4, 4] # Example 3: arr = [9, 8, 7, 6, 5, 4, 3, 2, 1] k = 2 # Expected Output: [9, 8, 7, 6, 5, 4, 3, 2] ``` **Performance Requirements**: - Your implementation should efficiently handle arrays with lengths up to 100,000 elements. - Ensure your solution has O(n) time complexity and does not exceed O(k) auxiliary space.","solution":"from collections import deque def max_sliding_window(arr, k): Given an array and a number k, find the maximum elements of each of its sub-arrays of length k. Parameters: arr: A list of integers representing temperatures. k: An integer, the length of the sliding window. Returns: result: A list of integers representing the maximum elements in each window of length k. if not arr or k == 0: return [] n = len(arr) if k == 1: return arr deq = deque() # We will use this deque to store indices of relevant elements max_elems = [] for i in range(n): # Remove elements not within the sliding window if deq and deq[0] < i - k + 1: deq.popleft() # Remove elements from the deque that are less than the current element while deq and arr[deq[-1]] <= arr[i]: deq.pop() deq.append(i) # Append the current max to the results list when the first window is formed if i >= k - 1: max_elems.append(arr[deq[0]]) return max_elems"},{"question":"# FizzBuzz Enhanced FizzBuzz is a popular coding problem often used in programming interviews. In this exercise, you will implement an enhanced version of the classic FizzBuzz problem. You need to write a function named `enhanced_fizzbuzz` that returns an array containing the numbers from 1 to N. However, your implementation should append different results based on the following rules: 1. If the number is divisible by 3, append \\"Fizz\\". 2. If the number is divisible by 5, append \\"Buzz\\". 3. If the number is divisible by 3 and 5, append \\"FizzBuzz\\". 4. For all other numbers, append the number itself. 5. If N is less than 1, the function should raise a ValueError. Additionally, you need to extend the functionality to include: 1. If the number is divisible by 7, append \\"Bizz\\" instead. 2. If the number is divisible by both 3 and 7, append \\"FizzBizz\\". 3. If the number is divisible by both 5 and 7, append \\"BuzzBizz\\". 4. If the number is divisible by 3, 5, and 7, append \\"FizzBuzzBizz\\". # Input - An integer N (1 ≤ N ≤ 10^6) # Output - A list of strings and integers based on the above rules. # Examples ```python assert enhanced_fizzbuzz(15) == [1, 2, \'Fizz\', 4, \'Buzz\', \'Fizz\', \'Bizz\', 8, \'Fizz\', \'Buzz\', 11, \'Fizz\', 13, \'Bizz\', \'FizzBuzz\'] assert enhanced_fizzbuzz(21) == [1, 2, \'Fizz\', 4, \'Buzz\', \'Fizz\', \'Bizz\', 8, \'Fizz\', \'Buzz\', 11, \'Fizz\', 13, \'Bizz\', \'FizzBuzz\', 16, 17, \'Fizz\', 19, \'Buzz\', \'FizzBizz\'] ``` # Constraints - The function should handle very large values of N efficiently within the provided constraints. - Ensure your solution is easy to read and maintain. # Note Your function implementation should: - Validate the input. - Handle edge cases thoroughly. - Offer an efficient implementation.","solution":"def enhanced_fizzbuzz(N): Returns a list of numbers from 1 to N with substitutions based on divisibility rules. Parameters: N (int): The upper bound of the range starting from 1 Returns: List[Union[int, str]]: A list containing numbers and strings per the enhanced FizzBuzz rules. if N < 1: raise ValueError(\\"N must be greater than or equal to 1\\") result = [] for i in range(1, N + 1): if i % 3 == 0 and i % 5 == 0 and i % 7 == 0: result.append(\\"FizzBuzzBizz\\") elif i % 3 == 0 and i % 7 == 0: result.append(\\"FizzBizz\\") elif i % 5 == 0 and i % 7 == 0: result.append(\\"BuzzBizz\\") elif i % 3 == 0 and i % 5 == 0: result.append(\\"FizzBuzz\\") elif i % 3 == 0: result.append(\\"Fizz\\") elif i % 5 == 0: result.append(\\"Buzz\\") elif i % 7 == 0: result.append(\\"Bizz\\") else: result.append(i) return result"},{"question":"# Question: Enhanced Run-Length Encoding and Decoding You are required to improve the provided Run-Length Encoding (RLE) and Run-Length Decoding (RLD) functions to handle additional edge cases and ensure robustness and optimization. **Question Scope**: 1. **Function Implementation**: - Provide functions: `enhanced_encode_rle(input: str) -> str` and `enhanced_decode_rle(input: str) -> str`. - Implementations should be optimized to handle potentially large inputs efficiently. 2. **Input and Output Formats**: - The `enhanced_encode_rle` function should take a string `input` and return the RLE-encoded string. - The `enhanced_decode_rle` function should take a string `input` and return the RLE-decoded string. 3. **Constraints and Requirements**: - Input strings may contain any printable ASCII characters. - Consider edge cases, such as empty strings or strings with single characters. - Ensure the decoding function can handle improper input gracefully by raising an appropriate error. 4. **Performance Considerations**: - Aim for linear time complexity O(n) and linear space complexity O(n). **Example**: ```python # Sample Inputs: print(enhanced_encode_rle(\\"aaabbc\\")) # Expected Output: \\"3a2b1c\\" print(enhanced_decode_rle(\\"3a2b1c\\")) # Expected Output: \\"aaabbc\\" ``` **Additional Context**: Imagine a scenario where you are required to process a large volume of text data received from a network or sensor that exhibits considerable redundancy. Your task is to implement the RLE and RLD algorithms robustly, maintaining accuracy while improving runtime efficiency and handling edge cases thoroughly.","solution":"def enhanced_encode_rle(input: str) -> str: if not input: return \\"\\" encoded_str = [] count = 1 for i in range(1, len(input)): if input[i] == input[i - 1]: count += 1 else: encoded_str.append(f\\"{count}{input[i - 1]}\\") count = 1 encoded_str.append(f\\"{count}{input[-1]}\\") return \'\'.join(encoded_str) def enhanced_decode_rle(input: str) -> str: if not input: return \\"\\" decoded_str = [] count_str = \\"\\" for char in input: if char.isdigit(): count_str += char else: if count_str == \\"\\": raise ValueError(\\"Invalid encoded input.\\") decoded_str.append(char * int(count_str)) count_str = \\"\\" return \'\'.join(decoded_str)"},{"question":"Radix Sort Extension **Scenario**: You are working for a logistics company that needs to sort large batches of shipping orders. Each order is represented by a unique integer ID. Due to the nature of the bulk data processing, an efficient and stable sorting algorithm is required. **Task**: Write an efficient implementation of the **Radix Sort** algorithm to sort a list of integer order IDs. Additionally, extend the functionality to handle negative numbers. Therefore, the resulting order should be non-decreasing, treating negative numbers correctly. **Function Signature**: ```python def radix_sort_extended(arr: List[int], simulation: bool = False) -> List[int]: pass ``` **Input**: - A list of integers `arr`, which can contain both positive and negative numbers. - An optional boolean parameter `simulation` defaulting to `False`. If set to `True`, the intermediate stages of the radix sort should be printed. **Output**: - A sorted list of integers in non-decreasing order. **Constraints**: - The list can contain both positive and negative integers. - The algorithm should remain stable. - The function should handle edge cases, such as an empty list or a list with all zero values, gracefully. **Example**: ```python assert radix_sort_extended([170, 45, 75, -90, -802, 24, 2, 66]) == [-802, -90, 2, 24, 45, 66, 75, 170] assert radix_sort_extended([3, 3, -2, -2, 0]) == [-2, -2, 0, 3, 3] assert radix_sort_extended([-1, -9, -3]) == [-9, -3, -1] ``` **Notes**: - The `simulation` parameter when set to `True` should print the state of the list after each digit processing stage, similarly to the provided radix sort example. - Ensure to handle the optional printing within your extended sort implementation.","solution":"def radix_sort_extended(arr, simulation=False): Sorts a list of integers using an extended version of Radix Sort that handles negative numbers. Arguments: arr : List[int] : List of integers to be sorted. simulation : bool : Optional boolean to print intermediate stages of sorting if set to True (default: False). Returns: List[int] : Sorted list of integers. # Helper function to do Counting Sort on the basis of a specific digit def counting_sort(arr, exp, simulation): n = len(arr) output = [0] * n count = [0] * 10 # Count occurrences of digits for i in range(n): index = abs(arr[i]) // exp count[index % 10] += 1 # Update count[] to store the end positions of each digit in the output list for i in range(1, 10): count[i] += count[i - 1] # Build the output array by placing numbers in their respective positions for i in range(n - 1, -1, -1): index = abs(arr[i]) // exp output[count[index % 10] - 1] = arr[i] count[index % 10] -= 1 # Copy the output array to the original array for i in range(n): arr[i] = output[i] if simulation: print(f\'After processing digit with exp {exp}: {arr}\') if not arr: return [] # Separate negative and non-negative numbers negative_numbers = [num for num in arr if num < 0] non_negative_numbers = [num for num in arr if num >= 0] # Sort non-negative numbers if non_negative_numbers: max_num = max(non_negative_numbers) exp = 1 while max_num // exp > 0: counting_sort(non_negative_numbers, exp, simulation) exp *= 10 # Sort negative numbers if negative_numbers: min_num = min(negative_numbers) exp = 1 while abs(min_num) // exp > 0: counting_sort(negative_numbers, exp, simulation) exp *= 10 negative_numbers.reverse() # To get them in the correct order # Combine sorted negative and non-negative numbers sorted_arr = negative_numbers + non_negative_numbers return sorted_arr"},{"question":"You have been tasked with implementing a function that helps efficiently insert elements into a sorted portion of an array using binary search to maintain the order. This function will be a part of an insertion sort algorithm. Problem Statement: Write a function `binary_insertion_sort` that sorts an array using the binary insertion sort algorithm. The helper function provided will be used to find the proper insertion point. Function Signature: ```python def binary_insertion_sort(arr: list[int]) -> list[int]: # Implementation goes here ``` Input: - `arr`: A list of integers, which can be an empty list. Output: - Returns a list of integers sorted in non-decreasing order. Constraints: - No constraints on the size of the list. - The elements of the list are integers. Example: ```python binary_insertion_sort([4, 2, 9, 1, 5, 6]) # Expected Output: [1, 2, 4, 5, 6, 9] binary_insertion_sort([3, -1, 0, -3, 9, 5, 1]) # Expected Output: [-3, -1, 0, 1, 3, 5, 9] binary_insertion_sort([]) # Expected Output: [] ``` Performance Requirement: - The function should efficiently handle the insertion point search using the `search_insert` helper function provided.","solution":"def binary_insertion_sort(arr: list[int]) -> list[int]: Sorts a list of integers using the binary insertion sort algorithm. Parameters: arr (list[int]): The list of integers to be sorted. Returns: list[int]: Sorted list of integers. def search_insert(arr, left, right, key): Helper function to find the insertion point using binary search. Parameters: arr (list[int]): The list within which to search. left (int): Left boundary of the search. right (int): Right boundary of the search. key (int): Element to find the insertion point for. Returns: int: Index to insert the key at to keep arr sorted. while left < right: mid = (left + right) // 2 if arr[mid] < key: left = mid + 1 else: right = mid return left for i in range(1, len(arr)): key = arr[i] pos = search_insert(arr, 0, i, key) arr = arr[:pos] + [key] + arr[pos:i] + arr[i+1:] return arr"},{"question":"Efficient Recursive Sorting You are required to implement an optimized version of the Stooge Sort algorithm called **Optimal Stooge Sort**. The algorithm must minimize redundant recursive calls while maintaining a divide and conquer approach to sorting. Input - A list of integers `arr` with size `n`, where (1 leq n leq 10^3). Output - Return the sorted list of integers. Constraints - The list can contain negative and positive integers. - The output should be in non-decreasing order. # Requirements 1. Implement the function `optimal_stoogesort(arr: List[int]) -> List[int]` that sorts the given integer list. 2. Ensure the time complexity is improved over the traditional Stooge Sort but still uses a divide and conquer method. 3. Handle edge cases like empty arrays and single-element arrays. # Example ```python # Example 1 Input: [5, 3, 8, 4, 2] Output: [2, 3, 4, 5, 8] # Example 2 Input: [1, -3, 45, 2, 4] Output: [-3, 1, 2, 4, 45] # Example 3 Input: [10, 9, 8, 7, 6] Output: [6, 7, 8, 9, 10] # Example 4 Input: [] Output: [] # Example 5 Input: [25] Output: [25] ``` # Evaluation Criteria - Correctness: The program should return the correct sorted list. - Efficiency: The algorithm should improve upon the traditional Stooge Sort to handle larger input sizes more effectively. - Code Quality: The code should be clean, well-documented, and efficient.","solution":"def optimal_stoogesort(arr): Optimized version of Stooge Sort which minimizes redundant recursive calls while maintaining a divide and conquer approach. def stooge_sort(arr, l, h): if l >= h: return # If the element at the start is greater than the element at the end, swap them if arr[l] > arr[h]: arr[l], arr[h] = arr[h], arr[l] # If there are more than two elements in the list if h - l + 1 > 2: t = (h - l + 1) // 3 stooge_sort(arr, l, h - t) stooge_sort(arr, l + t, h) stooge_sort(arr, l, h - t) # Copy of the array to preserve immutability arr_copy = arr[:] stooge_sort(arr_copy, 0, len(arr_copy) - 1) return arr_copy"},{"question":"As a software engineer, you are tasked with implementing an efficient algorithm for raising a matrix to any given non-negative integer power using the principles of matrix multiplication and matrix exponentiation by repeated squaring. # Problem Statement Write a Python function `matrix_power(mat: list, power: int) -> list` that takes a square matrix `mat` of size (d times d) and a non-negative integer `power`, and computes `mat` raised to that power efficiently using matrix exponentiation by repeated squaring. # Function Signature ```python def matrix_power(mat: list, power: int) -> list: ``` # Input - `mat`: A list of lists representing a `d x d` matrix, where each element is an integer. - `power`: A non-negative integer representing the exponent to which the matrix is to be raised. # Output - Returns a list of lists representing the matrix `mat` raised to the specified `power`. # Constraints - The dimensions (d) of the matrix (mat) are such that (1 leq d leq 100). - All integer elements in the matrix `mat` are within the range ([-1000, 1000]). - (0 leq text{power} leq 10^9). # Example ```python # Input mat = [ [1, 2], [3, 4] ] power = 2 # Output result = [ [7, 10], [15, 22] ] # Explanation # The matrix raised to the power of 2: # [ # [1, 2] * [1, 2] = [7, 10] # [3, 4] [3, 4] [15, 22] # ] ``` # Considerations * Handle edge cases such as raising the matrix to 0 (should return the identity matrix). * Ensure the function handles large values of `power` efficiently using repeated squaring.","solution":"def matrix_power(mat, power): Computes mat raised to the specified power using matrix exponentiation by repeated squaring. Parameters: mat (list of list of int): The d x d matrix to be raised to the given power. power (int): The non-negative integer to raise the matrix to. Returns: list of list of int: The resulting matrix after raising it to the specified power. def identity_matrix(d): Returns the identity matrix of size d x d. return [[1 if i == j else 0 for j in range(d)] for i in range(d)] def matrix_multiply(m1, m2): Multiplies two matrices m1 and m2. d = len(m1) result = [[0] * d for _ in range(d)] for i in range(d): for j in range(d): for k in range(d): result[i][j] += m1[i][k] * m2[k][j] return result def matrix_exponentiate(m, p): Raises matrix m to the power of p using repeated squaring. if p == 0: return identity_matrix(len(m)) if p == 1: return m if p % 2 == 0: half_power = matrix_exponentiate(m, p // 2) return matrix_multiply(half_power, half_power) else: return matrix_multiply(m, matrix_exponentiate(m, p - 1)) return matrix_exponentiate(mat, power)"},{"question":"# Question: Manipulating Named Tensors Implement a PyTorch function `process_images` that performs the following steps using named tensors: 1. Create a named tensor representing a batch of images. The tensor should have: - A batch size of `B`. - `C` channels. - Height `H`. - Width `W`. - Random values. - Dimensions named as (\'B\', \'C\', \'H\', \'W\'). 2. Rename the dimensions to: - Batch size to \'batch\'. - Channels to \'channels\'. - Height to \'height\'. - Width to \'width\'. 3. Flatten the spatial dimensions (height and width) into a single \'features\' dimension. 4. Unflatten the \'features\' dimension back to the original height and width dimensions. 5. Align the tensor such that the \'channels\' dimension comes last. The function should return the aligned tensor. Function Signature ```python import torch def process_images(B, C, H, W): # Your implementation here ``` Input - `B`: an integer representing the batch size. - `C`: an integer representing the number of channels. - `H`: an integer representing the height of the images. - `W`: an integer representing the width of the images. Output - A PyTorch named tensor with dimensions aligned such that the \'channels\' dimension comes last. Constraints - `B`, `C`, `H`, and `W` are positive integers. - Use named tensor functionalities wherever applicable. - The tensors created will have random values; the actual values do not matter for this task. Example ```python B = 4 C = 3 H = 128 W = 128 result = process_images(B, C, H, W) print(result.names) # Expected: (\'batch\', \'height\', \'width\', \'channels\') print(result.shape) # Expected: torch.Size([4, 128, 128, 3]) ``` Please ensure your implementation uses named tensor functionalities provided by PyTorch to meet the requirements.","solution":"import torch def process_images(B, C, H, W): # Step 1: Create a named tensor with random values tensor = torch.randn(B, C, H, W, names=(\'B\', \'C\', \'H\', \'W\')) # Step 2: Rename the dimensions tensor = tensor.rename(B=\'batch\', C=\'channels\', H=\'height\', W=\'width\') # Step 3: Flatten the spatial dimensions (height and width) into a single \'features\' dimension tensor = tensor.flatten([\'height\', \'width\'], \'features\') # Step 4: Unflatten the \'features\' dimension back to the original height and width dimensions tensor = tensor.unflatten(\'features\', ((\'height\', H), (\'width\', W))) # Step 5: Align the tensor such that the \'channels\' dimension comes last tensor = tensor.align_to(\'batch\', \'height\', \'width\', \'channels\') return tensor # Example usage: B = 4 C = 3 H = 128 W = 128 result = process_images(B, C, H, W) print(result.names) # Expected: (\'batch\', \'height\', \'width\', \'channels\') print(result.shape) # Expected: torch.Size([4, 128, 128, 3])"},{"question":"**Objective**: The following question is designed to assess your understanding and ability to implement scikit-learn\'s ensemble methods, particularly focusing on Gradient Boosting, Random Forests, and Voting Classifier. # Problem Statement You work as a data scientist for a company that deals with a wide range of datasets. Your task is to design a model that can efficiently handle datasets of various sizes and characteristics. Given the constraint of computational efficiency and model performance, you need to combine different ensemble methods strategically. Function: `create_ensemble_pipeline` You need to implement a function `create_ensemble_pipeline` that accepts a dataset, its target, and other relevant parameters, and constructs an ensemble model pipeline using Gradient Boosting, Random Forest, and Voting Classifier. **Input:** - `X_train` (numpy.ndarray or pandas.DataFrame): The training data, shape `(n_samples, n_features)`. - `y_train` (numpy.ndarray or pandas.Series): The target variable for training. - `model_types` (List[str]): A list of model types to include in the voting classifier. Each element could be either \'gbdt\' (GradientBoosting), \'hist_gbdt\' (HistGradientBoosting), \'rf\' (RandomForest). - `use_soft_voting` (bool): If `True`, use soft voting in the voting classifier; otherwise, use hard voting. - `with_stacking` (bool): If `True`, include an additional Stacking Classifier with the same base models as the final layer. If `False`, skip stacking and only use the voting classifier. **Output:** - A scikit-learn Pipeline object that can be used to fit and predict the test data. **Constraints:** - If using Histogram-Based Gradient Boosting, make sure it is used when `n_samples` > 10,000. - Ensure parameter tuning through cross-validation within the pipeline training process where appropriate. # Example Usage ```python from sklearn.model_selection import train_test_split from sklearn.datasets import load_iris # Load example data data = load_iris() X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42) # Define model types model_types = [\'gbdt\', \'hist_gbdt\', \'rf\'] # Create the ensemble pipeline pipeline = create_ensemble_pipeline(X_train, y_train, model_types, use_soft_voting=True, with_stacking=True) # Fit the pipeline pipeline.fit(X_train, y_train) # Predict on test data predictions = pipeline.predict(X_test) print(f\\"Test Predictions: {predictions}\\") ``` # Notes 1. You are encouraged to use the parameters and the example usage from the documentation to construct the models. 2. Ensure appropriate handling of missing values and categorical data leveraging built-in support where applicable. 3. Remember to apply any necessary preprocessing (like binning for histogram-based models) and to document your code for clarity. # Evaluation Criteria The successful implementation will be judged based on the following criteria: - Correct instantiation and combination of ensemble models. - Proper parameter settings ensuring computational efficiency. - Robustness of the code to handle different dataset sizes and missing values. - Quality of code documentation and readability.","solution":"from typing import List, Union from sklearn.pipeline import Pipeline from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier, StackingClassifier from sklearn.experimental import enable_hist_gradient_boosting # noqa from sklearn.ensemble import HistGradientBoostingClassifier from sklearn.preprocessing import StandardScaler from sklearn.impute import SimpleImputer from sklearn.compose import ColumnTransformer from sklearn.model_selection import cross_val_score import numpy as np import pandas as pd def create_ensemble_pipeline(X_train: Union[np.ndarray, pd.DataFrame], y_train: Union[np.ndarray, pd.Series], model_types: List[str], use_soft_voting: bool = True, with_stacking: bool = False): Create an ensemble pipeline using Gradient Boosting, Random Forest, and Voting Classifier. Optionally include a Stacking Classifier as the final layer. Parameters: - X_train: Training data, shape (n_samples, n_features) - y_train: Target variable for training - model_types: List of model types to include in the voting classifier - use_soft_voting: Use soft voting if True, otherwise use hard voting - with_stacking: Include a Stacking Classifier as the final layer if True Returns: - A scikit-learn Pipeline object estimators = [] if \'gbdt\' in model_types: estimators.append((\'gbdt\', GradientBoostingClassifier())) if \'hist_gbdt\' in model_types: if X_train.shape[0] > 10000: estimators.append((\'hist_gbdt\', HistGradientBoostingClassifier())) if \'rf\' in model_types: estimators.append((\'rf\', RandomForestClassifier())) voting_classifier = VotingClassifier(estimators=estimators, voting=\'soft\' if use_soft_voting else \'hard\') if with_stacking: final_estimator = GradientBoostingClassifier() stack = StackingClassifier(estimators=estimators, final_estimator=final_estimator, cv=5) final_clf = stack else: final_clf = voting_classifier numeric_features = X_train.select_dtypes(include=[\'int64\', \'float64\']).columns if isinstance(X_train, pd.DataFrame) else np.arange(X_train.shape[1]) numeric_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'median\')), (\'scaler\', StandardScaler()) ]) preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_features) ]) pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', final_clf) ]) return pipeline"},{"question":"You are tasked with building an out-of-core learning system to classify a large dataset that cannot fit into memory. The dataset is in CSV format and is stored on disk. Implement a solution that uses out-of-core learning to read the data in chunks, process it, and train an incremental classifier. Utilize the scikit-learn package to achieve this. # Requirements 1. **Stream Instances**: Implement a generator function `stream_data(file_path, chunk_size)` that reads the CSV file in chunks of size `chunk_size` and yields each chunk as a pandas DataFrame. 2. **Feature Extraction**: Use `HashingVectorizer` to transform a text column into numeric features for training while ensuring that new terms are handled correctly. 3. **Incremental Learning**: Use `SGDClassifier` as the incremental learning algorithm. Train the classifier on each chunk of data using the `partial_fit` method. 4. **Evaluation**: After processing all chunks, evaluate the model\'s performance on a separate test set. # Input and Output Format Input: - `file_path`: A string representing the path to the CSV file. - `chunk_size`: An integer representing the number of rows per chunk. - `text_column`: A string representing the name of the text column to be vectorized. - `label_column`: A string representing the name of the label column. - `test_set`: A pandas DataFrame representing the test set for evaluation. Output: - Print the accuracy of the trained classifier on the test set. # Constraints - Assume the CSV file contains a large number of rows that cannot fit into the main memory. - The test set can fit into memory. # Performance Requirements - The solution should efficiently handle large datasets by only loading small chunks into memory at a time. # Example ```python import pandas as pd from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier def stream_data(file_path, chunk_size): # Implement the generator function to read CSV in chunks pass def train_incremental_classifier(file_path, chunk_size, text_column, label_column, test_set): # Implement the function to train and evaluate the classifier pass # Example usage file_path = \'large_dataset.csv\' chunk_size = 10000 text_column = \'text_data\' label_column = \'label\' # Assume test_set is already loaded as a pandas DataFrame test_set = pd.read_csv(\'test_set.csv\') train_incremental_classifier(file_path, chunk_size, text_column, label_column, test_set) ``` In the implementation: 1. `stream_data` should yield chunks of data from the CSV file. 2. `train_incremental_classifier` should use `HashingVectorizer` to transform the text data and `SGDClassifier` to train on each chunk incrementally. 3. Finally, evaluate the trained model using the provided test set and print the accuracy.","solution":"import pandas as pd from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def stream_data(file_path, chunk_size): A generator function to read the CSV file in chunks of size `chunk_size`. for chunk in pd.read_csv(file_path, chunksize=chunk_size): yield chunk def train_incremental_classifier(file_path, chunk_size, text_column, label_column, test_set): Train the classifier incrementally on chunks of data and evaluate on the test set. vectorizer = HashingVectorizer() classifier = SGDClassifier() first_chunk = True for chunk in stream_data(file_path, chunk_size): X_chunk = vectorizer.transform(chunk[text_column].astype(\'U\').values) y_chunk = chunk[label_column].values if first_chunk: classifier.partial_fit(X_chunk, y_chunk, classes=np.unique(y_chunk)) first_chunk = False else: classifier.partial_fit(X_chunk, y_chunk) X_test = vectorizer.transform(test_set[text_column].astype(\'U\').values) y_test = test_set[label_column].values y_pred = classifier.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy: {accuracy:.4f}\\")"},{"question":"# Rational Number Operations with `fractions.Fraction` You are to implement a function `rational_operations(data: List[str]) -> List[str]` that processes a list of strings representing fractions and returns a list of strings representing the sum, difference, product, and quotient of the fractions. Each input string will be in the format `\'a/b\'`, where `a` and `b` are integers, and there will be exactly two fractions in the list. The function should handle edge cases such as division by zero and provide results in the simplest form. Input: - `data` (List[str]): A list containing exactly two strings, each representing a fraction in the form `\'a/b\'`. Output: - List[str]: A list of four strings, each representing a fraction in the form `\'a/b\'`, which are the sum, difference, product, and quotient (first fraction divided by the second fraction) of the two input fractions. Constraints: - The denominators in the input data will always be non-zero. - The output fractions should be in their simplest form, with the numerator and denominator having no common divisors other than 1. - If the second fraction is `0` (for division operation), return `\'Undefined\'` for the quotient. Example: ```python from fractions import Fraction from typing import List def rational_operations(data: List[str]) -> List[str]: # Implement the function here. pass # Input example: [\\"3/4\\", \\"2/5\\"] # Expected Output example: [\\"23/20\\", \\"7/20\\", \\"3/10\\", \\"15/8\\"] ``` # Implementation Details: 1. Parse the input strings to create `Fraction` objects. 2. Compute the sum, difference, product, and quotient of the two fractions. 3. Simplify the results using the properties of the `Fraction` class. 4. Return the results as strings in the form `\'a/b\'`. 5. Handle any potential division by zero gracefully, returning `\'Undefined\'` where appropriate. **Note:** Do not use any external libraries other than `fractions` and `typing`.","solution":"from fractions import Fraction from typing import List def rational_operations(data: List[str]) -> List[str]: frac1 = Fraction(data[0]) frac2 = Fraction(data[1]) sum_f = frac1 + frac2 diff_f = frac1 - frac2 product_f = frac1 * frac2 quotient_f = \'Undefined\' if frac2 == 0 else frac1 / frac2 return [ str(sum_f), str(diff_f), str(product_f), str(quotient_f) if quotient_f != \'Undefined\' else \'Undefined\' ]"},{"question":"Problem Statement You are tasked with implementing a custom autograd function and integrating it into a PyTorch module. The specific function you need to implement is a custom square root function that is differentiable and can be used in the context of a neural network. # Part 1: Custom Autograd Function 1. Implement a custom autograd function `CustomSqrtFunction` that computes the square root of its input tensor. This function should correctly handle the forward and backward passes: - `forward(ctx, input)`: Compute the square root of the input tensor and save any necessary context for the backward pass. - `setup_context(ctx, inputs, output)`: Save the input tensor for use in the backward pass. - `backward(ctx, grad_output)`: Compute the gradient of the input tensor based on the gradient of the output tensor. # Part 2: Custom PyTorch Module 2. Create a PyTorch module `CustomSqrt` that uses `CustomSqrtFunction` in its `forward` method. This module should be a subclass of `torch.nn.Module`. # Constraints - You can assume the input to the forward function is a tensor of non-negative values. - You should use the provided `gradcheck` utility to test the correctness of your gradient implementation. # Example Usage Here is how your custom module might be used in practice: ```python import torch from torch.autograd import gradcheck class CustomSqrtFunction(torch.autograd.Function): @staticmethod def forward(ctx, input): # Task: Implement the forward pass pass @staticmethod def setup_context(ctx, inputs, output): # Task: Save the input tensor for backward pass @staticmethod def backward(ctx, grad_output): # Task: Implement the backward pass pass class CustomSqrt(torch.nn.Module): def forward(self, input): # Task: Use CustomSqrtFunction to compute the forward pass pass # Test the custom module input = torch.tensor([4.0, 9.0, 16.0], requires_grad=True, dtype=torch.double) model = CustomSqrt() output = model(input) # Check gradients test = gradcheck(CustomSqrtFunction.apply, (input,), eps=1e-6, atol=1e-4) assert test, \\"Gradcheck failed!\\" ``` # Deliverables - Implement the `CustomSqrtFunction` class with correctly defined `forward`, `setup_context`, and `backward` methods. - Implement the `CustomSqrt` class that uses `CustomSqrtFunction` in its `forward` method. - Ensure that your implementation passes the `gradcheck` test for gradient validation.","solution":"import torch from torch.autograd import Function, gradcheck class CustomSqrtFunction(Function): @staticmethod def forward(ctx, input): result = input.sqrt() ctx.save_for_backward(input) return result @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = 0.5 * grad_output / input.sqrt() return grad_input class CustomSqrt(torch.nn.Module): def forward(self, input): return CustomSqrtFunction.apply(input)"},{"question":"**Title:** Implementing an Asynchronous Job Scheduling System using asyncio Queues **Objective:** Assess understanding of `asyncio` queues by implementing a job scheduling and processing system. **Background:** You are tasked with creating a simplified job scheduling system using the `asyncio` queue. This system should be capable of handling jobs with three different priority levels (high, medium, low), processing them in order of priority, and ensuring that each job is properly marked as done once processed. **Requirements:** 1. **Job Scheduler:** - Each job will have a priority (`high`, `medium`, `low`) and a task (a simple print statement for this exercise). - The job scheduler will receive jobs and place them into the appropriate queues. 2. **Worker:** - Use multiple worker coroutines to process jobs. - Workers should always process high-priority jobs first, followed by medium, and then low-priority jobs. - Ensure proper handling of job completion using `task_done()`. 3. **Queue Management:** - Use three instances of `asyncio.PriorityQueue` (one for each priority level). - Implement logic to balance work across these priority queues, ensuring high-priority jobs are processed first. **Function Specifications:** - **add_job(priority: str, job: str) -> None:** - `priority`: A string indicating the priority level of the job (`high`, `medium`, `low`). - `job`: A string representing the task to be performed. - Adds the job to the corresponding priority queue. - **start_processing() -> None:** - Starts the processing of jobs with multiple worker coroutines. - Handles task completion using `task_done()` and ensures all jobs are processed before shutting down. **Input:** - A list of jobs, where each job is represented by a tuple `(priority, job)`. **Output:** - Printed statements indicating the processing of each job. **Constraints:** - `priority` will always be one of (`high`, `medium`, `low`). - The system must use `asyncio` for handling asynchronous processing. **Example:** ```python import asyncio class JobScheduler: def __init__(self): self.high_priority_queue = asyncio.PriorityQueue() self.medium_priority_queue = asyncio.PriorityQueue() self.low_priority_queue = asyncio.PriorityQueue() async def worker(self, name): while True: # Check for high priority jobs first if not self.high_priority_queue.empty(): priority, job = await self.high_priority_queue.get() elif not self.medium_priority_queue.empty(): priority, job = await self.medium_priority_queue.get() elif not self.low_priority_queue.empty(): priority, job = await self.low_priority_queue.get() else: await asyncio.sleep(1) continue print(f\'{name} is processing job: {job} with priority {priority}\') # Simulate job processing await asyncio.sleep(1) # Mark the job as done if priority == 1: self.high_priority_queue.task_done() elif priority == 2: self.medium_priority_queue.task_done() else: self.low_priority_queue.task_done() def add_job(self, priority, job): if priority == \'high\': self.high_priority_queue.put_nowait((1, job)) elif priority == \'medium\': self.medium_priority_queue.put_nowait((2, job)) else: self.low_priority_queue.put_nowait((3, job)) async def start_processing(self): workers = [self.worker(f\'worker-{i}\') for i in range(3)] await asyncio.gather(*workers) async def main(): scheduler = JobScheduler() jobs = [(\'high\', \'task1\'), (\'medium\', \'task2\'), (\'low\', \'task3\'), (\'high\', \'task4\')] for priority, job in jobs: scheduler.add_job(priority, job) await scheduler.start_processing() asyncio.run(main()) ``` **Notes:** - Make sure to handle any edge cases, such as attempting to process from empty queues. - The `asyncio.run(main())` runs the main coroutine and simplifies usage of asyncio for this task. **Evaluation Metrics:** - Correct implementation and usage of `asyncio.PriorityQueue`. - Proper handling of job priorities. - Efficient and correct processing of multiple job queues with worker coroutines. - Clean, readable, and maintainable code.","solution":"import asyncio class JobScheduler: def __init__(self): self.high_priority_queue = asyncio.PriorityQueue() self.medium_priority_queue = asyncio.PriorityQueue() self.low_priority_queue = asyncio.PriorityQueue() async def worker(self, name): while True: # Check for high priority jobs first if not self.high_priority_queue.empty(): priority, job = await self.high_priority_queue.get() elif not self.medium_priority_queue.empty(): priority, job = await self.medium_priority_queue.get() elif not self.low_priority_queue.empty(): priority, job = await self.low_priority_queue.get() else: await asyncio.sleep(1) continue print(f\'{name} is processing job: {job} with priority {priority}\') # Simulate job processing await asyncio.sleep(1) # Mark the job as done if priority == 1: self.high_priority_queue.task_done() elif priority == 2: self.medium_priority_queue.task_done() else: self.low_priority_queue.task_done() def add_job(self, priority, job): if priority == \'high\': self.high_priority_queue.put_nowait((1, job)) elif priority == \'medium\': self.medium_priority_queue.put_nowait((2, job)) else: self.low_priority_queue.put_nowait((3, job)) async def start_processing(self): workers = [self.worker(f\'worker-{i}\') for i in range(3)] await asyncio.gather(*workers) async def main(): scheduler = JobScheduler() jobs = [(\'high\', \'task1\'), (\'medium\', \'task2\'), (\'low\', \'task3\'), (\'high\', \'task4\')] for priority, job in jobs: scheduler.add_job(priority, job) await scheduler.start_processing() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"Objective Implement a task scheduler using `asyncio.PriorityQueue`. Your task scheduler should allow tasks with different priorities to be executed by worker coroutines. Higher priority tasks should be executed before lower priority ones. Task Description 1. Implement an `asyncio.PriorityQueue` to manage tasks with associated priorities. 2. Write worker coroutines that process tasks from the priority queue. Each worker should: - Retrieve tasks from the queue. - Execute tasks by awaiting on a simulated workload (e.g., `await asyncio.sleep(duration)`). - Mark tasks as done after completing them. 3. Ensure proper synchronization using `task_done()` and `join()` methods of the queue. 4. Handle any potential exceptions in queue operations. Requirements 1. Define a `TaskScheduler` class with the following methods: - `__init__(self, max_workers: int)`: Initializes the scheduler with a given number of worker coroutines. - `add_task(self, priority: int, workload: float)`: Adds a task with the given priority and workload duration to the queue. - `run(self)`: Starts the worker coroutines and waits for all tasks to be processed. 2. Demonstrate the usage of the `TaskScheduler` class as follows: - Create an instance of `TaskScheduler` with 3 worker coroutines. - Add multiple tasks with varying priorities and durations. - Start the task scheduler to process all tasks. Constraints - Use of `asyncio` and `asyncio.PriorityQueue` is mandatory. - Each task\'s workload is simulated using `await asyncio.sleep(duration)`. - Ensure that higher priority tasks (lower priority numbers) are processed before lower priority tasks. Example Usage ```python import asyncio from typing import Tuple class TaskScheduler: def __init__(self, max_workers: int): # Your implementation here def add_task(self, priority: int, workload: float): # Your implementation here async def run(self): # Your implementation here # Example of usage async def main(): scheduler = TaskScheduler(max_workers=3) scheduler.add_task(priority=1, workload=1.0) scheduler.add_task(priority=3, workload=0.5) scheduler.add_task(priority=2, workload=2.0) scheduler.add_task(priority=1, workload=0.2) await scheduler.run() asyncio.run(main()) ``` Expected Output (The order of task completion may vary due to the nature of async tasks, but higher priority tasks should finish first where possible): ``` Task with priority 1 and duration 0.2 seconds completed Task with priority 1 and duration 1.0 seconds completed Task with priority 2 and duration 2.0 seconds completed Task with priority 3 and duration 0.5 seconds completed All tasks completed. ```","solution":"import asyncio from typing import Tuple class TaskScheduler: def __init__(self, max_workers: int): self.max_workers = max_workers self.queue = asyncio.PriorityQueue() self.tasks = [] def add_task(self, priority: int, workload: float): self.queue.put_nowait((priority, workload)) async def worker(self, worker_id: int): while True: priority, workload = await self.queue.get() await asyncio.sleep(workload) print(f\\"Worker {worker_id} completed task with priority {priority} and duration {workload} seconds\\") self.queue.task_done() async def run(self): for i in range(self.max_workers): task = asyncio.create_task(self.worker(i)) self.tasks.append(task) await self.queue.join() for task in self.tasks: task.cancel() await asyncio.gather(*self.tasks, return_exceptions=True) # Example of usage async def main(): scheduler = TaskScheduler(max_workers=3) scheduler.add_task(priority=1, workload=1.0) scheduler.add_task(priority=3, workload=0.5) scheduler.add_task(priority=2, workload=2.0) scheduler.add_task(priority=1, workload=0.2) await scheduler.run() # Uncomment the following line to execute the example # asyncio.run(main())"},{"question":"Objective In this challenge, you are required to use the `sndhdr` module to analyze a set of sound files. You will implement a function that processes multiple sound files, generates a summary report, and identifies the most prevalent sound file type in the dataset. Problem Statement Implement the function `analyze_sound_files(file_list)` that takes as input a list of file paths and returns a dictionary summarizing the analysis of these sound files. # Function Signature ```python def analyze_sound_files(file_list: List[str]) -> Dict[str, Any]: pass ``` # Input - `file_list` (List[str]): A list of file paths, each representing a sound file on the filesystem. # Output The function should return a dictionary with the following structure: ```python { \\"summary\\": { \\"total_files\\": int, \\"successful_analyses\\": int, \\"failed_analyses\\": int }, \\"details\\": List[Dict[str, Any]], \\"most_common_filetype\\": str } ``` - `summary`: - `total_files`: Total number of files in the input list. - `successful_analyses`: Number of files successfully analyzed. - `failed_analyses`: Number of files that could not be analyzed. - `details`: A list of dictionaries, each representing the analysis result of a sound file. Each dictionary should have the following keys: - `filename`: The name of the sound file. - `filetype`: The detected file type. - `framerate`: The sampling rate. - `nchannels`: The number of channels. - `nframes`: The number of frames. - `sampwidth`: The sample width or encoding type. - `most_common_filetype`: The most common sound file type found in the analysis. # Constraints - For simplicity, assume that all file paths in `file_list` are valid and accessible. - The function should handle cases where the `sndhdr` module fails to determine the file type. # Example ```python file_list = [ \\"sound1.wav\\", \\"sound2.aiff\\", \\"sound3.wav\\", \\"sound4.au\\", \\"sound5.wav\\", \\"sound6.invalid\\", ] output = analyze_sound_files(file_list) expected_output = { \\"summary\\": { \\"total_files\\": 6, \\"successful_analyses\\": 5, \\"failed_analyses\\": 1 }, \\"details\\": [ {\\"filename\\": \\"sound1.wav\\", \\"filetype\\": \\"wav\\", \\"framerate\\": 44100, \\"nchannels\\": 2, \\"nframes\\": 100000, \\"sampwidth\\": 16}, {\\"filename\\": \\"sound2.aiff\\", \\"filetype\\": \\"aiff\\", \\"framerate\\": 48000, \\"nchannels\\": 1, \\"nframes\\": 95000, \\"sampwidth\\": 8}, {\\"filename\\": \\"sound3.wav\\", \\"filetype\\": \\"wav\\", \\"framerate\\": 44100, \\"nchannels\\": 2, \\"nframes\\": 100000, \\"sampwidth\\": 16}, {\\"filename\\": \\"sound4.au\\", \\"filetype\\": \\"au\\", \\"framerate\\": 8000, \\"nchannels\\": 1, \\"nframes\\": 20000, \\"sampwidth\\": \\"A\\"}, {\\"filename\\": \\"sound5.wav\\", \\"filetype\\": \\"wav\\", \\"framerate\\": 44100, \\"nchannels\\": 2, \\"nframes\\": 100000, \\"sampwidth\\": 16}, ], \\"most_common_filetype\\": \\"wav\\" } ``` # Additional Information - You may find it helpful to use the functions `sndhdr.what` and `sndhdr.whathdr` for sound file type detection. - Ensure that the function handles errors gracefully and includes all required data in the output dictionary.","solution":"import sndhdr from collections import Counter from typing import List, Dict, Any def analyze_sound_files(file_list: List[str]) -> Dict[str, Any]: summary = { \\"total_files\\": len(file_list), \\"successful_analyses\\": 0, \\"failed_analyses\\": 0 } details = [] filetypes = [] for filepath in file_list: try: result = sndhdr.what(filepath) if result is not None: file_type, framerate, nchannels, nframes, sampwidth = result details.append({ \\"filename\\": filepath, \\"filetype\\": file_type, \\"framerate\\": framerate, \\"nchannels\\": nchannels, \\"nframes\\": nframes, \\"sampwidth\\": sampwidth }) filetypes.append(file_type) summary[\\"successful_analyses\\"] += 1 else: summary[\\"failed_analyses\\"] += 1 except Exception: summary[\\"failed_analyses\\"] += 1 most_common_filetype = None if filetypes: most_common_filetype = Counter(filetypes).most_common(1)[0][0] return { \\"summary\\": summary, \\"details\\": details, \\"most_common_filetype\\": most_common_filetype }"},{"question":"Objective Evaluate the candidate\'s understanding of PyTorch tensor storage, its manipulation, and their relationship to tensors. Problem Statement You are provided with a tensor `t` in PyTorch. Your task is to perform a series of operations on this tensor\'s storage and ensure certain conditions hold true at the end. 1. Create a 1-dimensional tensor `t` of size 5 filled with ones. This will be your original tensor. 2. Access its untyped storage and create a clone of it. 3. Modify the cloned storage by filling it with zeros. 4. Set the original tensor `t` to use this modified storage, preserving its shape and stride. 5. Verify the contents of the tensor `t` to confirm it now contains only zeros. Function Signature ```python import torch def manipulate_tensor_storage(): # Step 1: Create a tensor filled with ones t = torch.ones(5) # Step 2: Access its untyped storage and clone it s0 = t.untyped_storage() s1 = s0.clone() # Step 3: Fill the cloned storage with zeros s1.fill_(0) # Step 4: Set the original tensor `t` to use this modified storage t.set_(s1, storage_offset=t.storage_offset(), stride=t.stride(), size=t.size()) # Step 5: Verify the contents of the tensor `t` return t # Example usage: result = manipulate_tensor_storage() print(result) # Expected output: tensor([0., 0., 0., 0., 0.]) ``` Constraints - The solution should strictly adhere to the provided steps. - Avoid directly modifying the tensor `t` using high-level methods like `torch.zeros_like`. - Ensure the tensor\'s shape and stride are preserved after setting the new storage. Evaluation - The primary focus will be confirming that the tensor `t` has the correct values and verifying the steps are followed precisely. - Performance considerations are minor given the problem\'s scope, but the solution should not involve unnecessary computations or structure changes.","solution":"import torch def manipulate_tensor_storage(): # Step 1: Create a tensor filled with ones t = torch.ones(5) # Step 2: Access its untyped storage and clone it s0 = t.untyped_storage() s1 = s0.clone() # Step 3: Fill the cloned storage with zeros s1.fill_(0) # Step 4: Set the original tensor `t` to use this modified storage t.set_(s1, storage_offset=t.storage_offset(), stride=t.stride(), size=t.size()) # Step 5: Verify the contents of the tensor `t` return t"},{"question":"# XML Parsing and Dictionary Construction You are given the task of parsing an XML document using the `xml.parsers.expat` module, and constructing a dictionary that represents the structure and content of the XML document. Problem Write a function `parse_xml_to_dict(xml_string)` that takes an XML string as input and returns a nested dictionary representation of the XML structure. The keys of the dictionary should be the tag names and the values should be a dictionary with two keys: - `attributes`: A dictionary of the element\'s attributes. - `children`: A dictionary where the keys are the tag names of the child elements and the values are in the same format. Function Signature ```python def parse_xml_to_dict(xml_string: str) -> dict: pass ``` Input - `xml_string` (str): A string containing a valid XML document. Output - `dict`: A nested dictionary representing the XML structure with attributes and children. Constraints - The input XML string will always be well-formed. - You may assume that the XML document does not contain namespaces for this problem. Example Input: ```xml <root> <child1 name=\\"paul\\"> <subchild1 id=\\"1\\">Text</subchild1> <subchild2>More Text</subchild2> </child1> <child2 name=\\"fred\\"> <subchild1 id=\\"2\\">Another Text</subchild1> </child2> </root> ``` Output: ```python { \'root\': { \'attributes\': {}, \'children\': { \'child1\': { \'attributes\': {\'name\': \'paul\'}, \'children\': { \'subchild1\': {\'attributes\': {\'id\': \'1\'}, \'children\': {}}, \'subchild2\': {\'attributes\': {}, \'children\': {}} } }, \'child2\': { \'attributes\': {\'name\': \'fred\'}, \'children\': { \'subchild1\': {\'attributes\': {\'id\': \'2\'}, \'children\': {}} } } } } } ``` Notes - Use the `xml.parsers.expat` module to create and configure your XML parser. - You will need to handle different types of events such as start element, end element, and character data. - Make sure to initialize the handlers properly and accumulate the data to form the nested dictionary.","solution":"import xml.parsers.expat def parse_xml_to_dict(xml_string): def start_element(name, attrs): nonlocal current element = {\'attributes\': attrs, \'children\': {}} if stack: parent = stack[-1][\'children\'] parent.setdefault(name, []).append(element) else: result[name] = element stack.append(element) def end_element(name): stack.pop() result = {} stack = [] current = result parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.Parse(xml_string) # Simplify structure by converting lists with single items to just the item def simplify(d): if isinstance(d, dict): return {k: (simplify(v[0]) if isinstance(v, list) and len(v) == 1 else simplify(v)) for k, v in d.items()} return d return simplify(result)"},{"question":"# Seaborn Boxplot Customization and Analysis **Problem Statement:** You are given a dataset containing information about passengers on the Titanic. Your task is to create and customize various boxplots using the `seaborn` library to visualize different aspects of the data. Follow the steps outlined below to accomplish this. **Input:** - A DataFrame `titanic` from the seaborn library, which can be loaded using `sns.load_dataset(\\"titanic\\")`. **Requirements:** 1. Create a vertical boxplot showing the distribution of passengers\' fares, grouped by the class they traveled in (`class`), further nested by their embarkation port (`embarked`). Ensure to add a title to the plot. 2. Draw a horizontal boxplot to display the age distribution of passengers, grouped by their survival status (`survived`). Customize the box to: - Have a line color of `#2a9d8f`. - Show the outliers with a marker `\\"D\\"` (diamond) and color `#e76f51`. 3. Generate a vertical boxplot to showcase the age distribution, but this time, modify the plot to cover the full data range (`whis=(0, 100)`) and set the box line width to `1.5`. 4. Create a horizontal boxplot of the passenger ages with: - The boxes filled with a color of `(0.3, 0.5, 0.7, 0.5)`. - The median line in color `r` (red) with a linewidth of `2`. - The plot should include a vertical line at age `60` to signify a critical age point. **Output:** - Visualizations as specified above that reveal different aspects of the Titanic dataset. **Constraints:** - You must use the `seaborn` library for all visualizations. - Ensure each plot is well-labeled, including titles, and axis labels where appropriate for clarity. **Example Code:** ```python import seaborn as sns import matplotlib.pyplot as plt # Load Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # 1. Vertical boxplot of fares grouped by class, nested by embarked plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"class\\", y=\\"fare\\", hue=\\"embarked\\") plt.title(\\"Distribution of Fares by Class and Embarkation Port\\") plt.show() # 2. Horizontal boxplot of age grouped by survival status with customization plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, y=\\"age\\", x=\\"survived\\", linecolor=\\"#2a9d8f\\", flierprops={\\"marker\\": \\"D\\", \\"color\\": \\"#e76f51\\"}) plt.title(\\"Age Distribution by Survival Status\\") plt.show() # 3. Vertical boxplot of ages with full range and custom box linewidth plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"age\\", y=\\"class\\", whis=(0, 100), linewidth=1.5) plt.title(\\"Age Distribution by Class (Full Range Whiskers)\\") plt.show() # 4. Horizontal boxplot of age with custom colors and vertical line at age 60 plt.figure(figsize=(10, 6)) ax = sns.boxplot(data=titanic, y=\\"age\\", x=\\"sex\\", boxprops={\\"facecolor\\": (0.3, 0.5, 0.7, 0.5)}, medianprops={\\"color\\": \\"r\\", \\"linewidth\\": 2}) ax.axvline(60, color=\\".3\\", dashes=(2, 2)) plt.title(\\"Age Distribution by Sex with Critical Age Line\\") plt.show() ``` Ensure that your plots are correctly labeled and styled according to the requirements. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # 1. Vertical boxplot of fares grouped by class, nested by embarked plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"class\\", y=\\"fare\\", hue=\\"embarked\\") plt.title(\\"Distribution of Fares by Class and Embarkation Port\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Fare\\") plt.show() # 2. Horizontal boxplot of age grouped by survival status with customization plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, y=\\"age\\", x=\\"survived\\", linecolor=\\"#2a9d8f\\", flierprops={\\"marker\\": \\"D\\", \\"color\\": \\"#e76f51\\"}) plt.title(\\"Age Distribution by Survival Status\\") plt.xlabel(\\"Survived\\") plt.ylabel(\\"Age\\") plt.show() # 3. Vertical boxplot of ages with full range and custom box linewidth plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"age\\", y=\\"class\\", whis=(0, 100), linewidth=1.5) plt.title(\\"Age Distribution by Class (Full Range Whiskers)\\") plt.xlabel(\\"Age\\") plt.ylabel(\\"Class\\") plt.show() # 4. Horizontal boxplot of age with custom colors and vertical line at age 60 plt.figure(figsize=(10, 6)) ax = sns.boxplot(data=titanic, y=\\"age\\", x=\\"sex\\", boxprops={\\"facecolor\\": (0.3, 0.5, 0.7, 0.5)}, medianprops={\\"color\\": \\"r\\", \\"linewidth\\": 2}) ax.axvline(60, color=\\".3\\", dashes=(2, 2)) plt.title(\\"Age Distribution by Sex with Critical Age Line\\") plt.xlabel(\\"Sex\\") plt.ylabel(\\"Age\\") plt.show()"},{"question":"Objective: Design and implement a Python function that utilizes the `ossaudiodev` module to perform a series of audio operations, demonstrating your understanding of device configuration, audio data manipulation, and mixer control. Problem Statement: You are required to write a function `record_and_playback` that records audio from the default audio input device (microphone) and plays it back using the default audio output device (speaker). The function should: 1. Record audio for a specified duration. 2. Play back the recorded audio. 3. Adjust the playback volume using the mixer controls. Function Signature: ```python def record_and_playback(record_duration: int, playback_volume: int) -> None: Args: - record_duration (int): Duration in seconds for which audio should be recorded. - playback_volume (int): The desired playback volume (0 to 100). Returns: - None ``` Input: - `record_duration`: An integer representing the number of seconds to record audio. - `playback_volume`: An integer between 0 and 100 representing the desired playback volume. Output: - None. The function performs the required operations but does not return any value. Constraints: - Ensure that the recorded audio is played back correctly. - The playback volume must be set precisely to the specified value. - Handle any potential errors gracefully, such as device access issues. - The function should use the `ossaudiodev` module\'s capabilities to achieve the described functionality. Example Usage: ```python record_and_playback(5, 75) ``` This should record audio for 5 seconds and play it back with the playback volume set to 75%. Implementation Notes: 1. Use `ossaudiodev.open()` to access the audio devices and `ossaudiodev.openmixer()` for mixer control. 2. Set the audio parameters (format, channels, and sampling rate) using the appropriate methods (`setfmt`, `channels`, `speed`). 3. Use a loop to read audio data for the specified duration and store it. 4. Write the stored audio data back to the output device. 5. Adjust the playback volume using the mixer object methods (`set`).","solution":"import ossaudiodev import time def record_and_playback(record_duration: int, playback_volume: int) -> None: Records audio for a specified duration and plays it back with adjusted volume. Args: - record_duration (int): Duration in seconds for which audio should be recorded. - playback_volume (int): The desired playback volume (0 to 100). Returns: - None # Ensure the playback volume is within the range [0, 100] if playback_volume < 0 or playback_volume > 100: raise ValueError(\\"Playback volume must be between 0 and 100\\") try: # Open the audio devices rec_device = ossaudiodev.open(\'r\') play_device = ossaudiodev.open(\'w\') mixer = ossaudiodev.openmixer() # Set audio parameters rec_device.setfmt(ossaudiodev.AFMT_S16_LE) # Format: 16-bit little-endian samples rec_device.channels(1) # Mono rec_device.speed(44100) # Sampling rate: 44100 Hz play_device.setfmt(ossaudiodev.AFMT_S16_LE) play_device.channels(1) play_device.speed(44100) buffer_size = 1024 audio_data = b\'\' start_time = time.time() while time.time() - start_time < record_duration: audio_data += rec_device.read(buffer_size) # Playback the recorded audio play_device.write(audio_data) # Adjust the playback volume vol_percentage = int(playback_volume * 255 / 100) mixer.set(ossaudiodev.SOUND_MIXER_PCM, (vol_percentage, vol_percentage)) except Exception as e: print(f\\"An error occurred: {e}\\") finally: rec_device.close() play_device.close() mixer.close()"},{"question":"Question: # AIFF/C File Manipulation You are tasked with writing a Python program that utilizes the `aifc` module to process AIFF and AIFF-C audio files. Specifically, your program should be able to read an existing audio file, modify its properties, and save the changes to a new file. # Task: 1. **Function: `process_aiff_file(input_file_path, output_file_path)`** Implement a function `process_aiff_file` that takes two arguments: - `input_file_path` (str): The file path of the input AIFF/AIFF-C file. - `output_file_path` (str): The file path where the modified AIFF/AIFF-C file will be saved. 2. The function should perform the following operations: - Open and read the input file using the `aifc` module. - Extract the number of audio channels, sample width, frame rate, and number of frames. - Double the frame rate while keeping the other parameters same. - Save the new file with the adjusted frame rate to the specified output file path. # Input: - The input to your function is two strings: `input_file_path` and `output_file_path`. - You can assume the input file exists and is a valid AIFF or AIFF-C file. # Output: - The output is a new AIFF/AIFF-C file saved at the specified `output_file_path` with the modified frame rate. # Example Usage: ```python process_aiff_file(\'input.aiff\', \'output.aiff\') ``` # Constraints: - Ensure the new file is a valid AIFF or AIFF-C file. - The function should handle files up to 1GB efficiently. - Use Python\'s `aifc` module for all file operations. # Performance Requirements: - The function should complete the task within a reasonable time frame for files up to 1GB, ensuring efficient reading and writing of frame data. # Note: - Since the `aifc` module is deprecated as of Python 3.11, ensure you are using a Python version that still supports it (e.g., Python 3.10). Here is a template to get you started: ```python import aifc def process_aiff_file(input_file_path, output_file_path): with aifc.open(input_file_path, \'rb\') as input_file: nchannels = input_file.getnchannels() sampwidth = input_file.getsampwidth() framerate = input_file.getframerate() nframes = input_file.getnframes() # Read the frames frames = input_file.readframes(nframes) # Modify the frame rate new_framerate = framerate * 2 with aifc.open(output_file_path, \'wb\') as output_file: output_file.setnchannels(nchannels) output_file.setsampwidth(sampwidth) output_file.setframerate(new_framerate) output_file.writeframes(frames) ``` # Deliverables: 1. The implemented `process_aiff_file` function. 2. Any additional helper functions that might be required to complete the task.","solution":"import aifc def process_aiff_file(input_file_path, output_file_path): Reads an AIFF/AIFF-C file, doubles its frame rate, and writes the modified data to a new file. Parameters: input_file_path (str): The file path of the input AIFF/AIFF-C file. output_file_path (str): The file path where the modified AIFF/AIFF-C file will be saved. with aifc.open(input_file_path, \'rb\') as input_file: nchannels = input_file.getnchannels() sampwidth = input_file.getsampwidth() framerate = input_file.getframerate() nframes = input_file.getnframes() # Read the frames frames = input_file.readframes(nframes) # Modify the frame rate new_framerate = framerate * 2 with aifc.open(output_file_path, \'wb\') as output_file: output_file.setnchannels(nchannels) output_file.setsampwidth(sampwidth) output_file.setframerate(new_framerate) output_file.writeframes(frames)"},{"question":"**Objective:** Implement a Python program that demonstrates the use of classes, inheritance, and method overriding, focusing on understanding scopes and namespaces. **Instructions:** You need to create a program that models a simple library system using classes and inheritance in Python. Follow the specifications provided below: 1. Define a base class `LibraryItem` with the following attributes and methods: - Attributes: - `title` (string) - `year` (integer) - Methods: - `__init__(self, title, year)`: Initializes the attributes with given values. - `__str__(self)`: Returns a string in the format: `\\"Title: {title}, Year: {year}\\"`. 2. Define a derived class `Book` that inherits from `LibraryItem` with the following additional attributes and methods: - Attributes: - `author` (string) - `pages` (integer) - Methods: - `__init__(self, title, year, author, pages)`: Initializes the attributes, calling the base class initializer for common attributes. - `__str__(self)`: Returns a string in the format: `\\"Title: {title}, Year: {year}, Author: {author}, Pages: {pages}\\"`. 3. Define another derived class `DVD` that inherits from `LibraryItem` with the following additional attributes and methods: - Attributes: - `director` (string) - `duration` (integer, representing minutes) - Methods: - `__init__(self, title, year, director, duration)`: Initializes the attributes, calling the base class initializer for common attributes. - `__str__(self)`: Returns a string in the format: `\\"Title: {title}, Year: {year}, Director: {director}, Duration: {duration} minutes\\"`. 4. Implement the following additional functionalities: - Create a `Library` class that has a method `add_item(self, item)` to add `LibraryItem` (or derived) objects to a list. - Implement a method `display_items(self)` in the `Library` class that iterates through the list of items and prints each item\'s `__str__` representation. **Input and Output:** - Create at least one `Book` and one `DVD` object, add them to the `Library`, and display the list of items. **Constraints:** - Make sure your classes properly use inheritance and method overriding. - Use proper encapsulation techniques to manage the attributes. **Example:** ```python # Define LibraryItem base class here class LibraryItem: # Your code here # Define Book class here class Book(LibraryItem): # Your code here # Define DVD class here class DVD(LibraryItem): # Your code here # Define Library class here class Library: # Your code here # Main part of the code library = Library() book1 = Book(\\"Python Programming\\", 2019, \\"John Doe\\", 354) dvd1 = DVD(\\"Python in Action\\", 2018, \\"Jane Smith\\", 120) library.add_item(book1) library.add_item(dvd1) library.display_items() ``` Expected output: ``` Title: Python Programming, Year: 2019, Author: John Doe, Pages: 354 Title: Python in Action, Year: 2018, Director: Jane Smith, Duration: 120 minutes ```","solution":"# Define LibraryItem base class here class LibraryItem: def __init__(self, title, year): self.title = title self.year = year def __str__(self): return f\\"Title: {self.title}, Year: {self.year}\\" # Define Book class here class Book(LibraryItem): def __init__(self, title, year, author, pages): super().__init__(title, year) self.author = author self.pages = pages def __str__(self): return f\\"Title: {self.title}, Year: {self.year}, Author: {self.author}, Pages: {self.pages}\\" # Define DVD class here class DVD(LibraryItem): def __init__(self, title, year, director, duration): super().__init__(title, year) self.director = director self.duration = duration def __str__(self): return f\\"Title: {self.title}, Year: {self.year}, Director: {self.director}, Duration: {self.duration} minutes\\" # Define Library class here class Library: def __init__(self): self.items = [] def add_item(self, item): if isinstance(item, LibraryItem): self.items.append(item) def display_items(self): for item in self.items: print(item) # Main part of the code if __name__ == \\"__main__\\": library = Library() book1 = Book(\\"Python Programming\\", 2019, \\"John Doe\\", 354) dvd1 = DVD(\\"Python in Action\\", 2018, \\"Jane Smith\\", 120) library.add_item(book1) library.add_item(dvd1) library.display_items()"},{"question":"# Dimensionality Reduction Using Manifold Learning: Locally Linear Embedding (LLE) **Context:** Manifold learning is an approach to non-linear dimensionality reduction. In this task, you will be implementing the Locally Linear Embedding (LLE) algorithm using scikit-learn and applying it to a dataset to reduce its dimensionality while preserving local distances within neighborhoods. **Problem Statement:** 1. **Implement the Locally Linear Embedding (LLE) algorithm:** - Use the `sklearn.manifold.LocallyLinearEmbedding` class to perform LLE. - Your function should take the following inputs: - `data` (numpy array): The high-dimensional input data. - `output_dim` (int): Desired dimensionality of the output data. - `n_neighbors` (int): Number of neighbors to consider for each point. 2. **Apply LLE to a dataset:** - Apply your function to the provided dataset `digits` (available in scikit-learn) and visualize the results. - The digits dataset consists of 8x8 pixel images of digits, but this dataset should be flattened into a 64-dimensional space. 3. **Visualize the results:** - Use a scatter plot to visualize the results of the dimensionality reduction in a 2-dimensional space. - Color the points by the digit they represent to observe the clustering. **Function Signature:** ```python import numpy as np import matplotlib.pyplot as plt from sklearn.manifold import LocallyLinearEmbedding from sklearn import datasets def perform_LLE(data: np.ndarray, output_dim: int, n_neighbors: int) -> np.ndarray: pass def visualize_reduced_dimensions(transformed_data: np.ndarray, labels: np.ndarray): pass def main(): digits = datasets.load_digits() data = digits.data labels = digits.target # Perform LLE to reduce data to 2 dimensions transformed_data = perform_LLE(data, output_dim=2, n_neighbors=10) # Visualize reduced dimensions visualize_reduced_dimensions(transformed_data, labels) if __name__ == \\"__main__\\": main() ``` **Constraints:** - You must use the LocallyLinearEmbedding class from sklearn.manifold. - Assume the input data is a 2D numpy array where each row is a data instance. - Your implementation should handle errors gracefully and provide meaningful error messages. **Example Output:** The resulting plot should depict clusters of points corresponding to each digit in the digits dataset, showing how LLE preserves local neighborhood structures in the lower-dimensional space. **Hints:** - Refer to the scikit-learn documentation for `LocallyLinearEmbedding` to understand its parameters and usage. - Use matplotlib for the visualization part.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.manifold import LocallyLinearEmbedding from sklearn import datasets def perform_LLE(data: np.ndarray, output_dim: int, n_neighbors: int) -> np.ndarray: Perform Locally Linear Embedding (LLE) on the given data. Parameters: - data (np.ndarray): The high-dimensional input data. - output_dim (int): Desired dimensionality of the output data. - n_neighbors (int): Number of neighbors to consider for each point. Returns: - np.ndarray: The data transformed into the lower-dimensional space. lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, n_components=output_dim) transformed_data = lle.fit_transform(data) return transformed_data def visualize_reduced_dimensions(transformed_data: np.ndarray, labels: np.ndarray): Visualize the reduced dimensions using a scatter plot. Parameters: - transformed_data (np.ndarray): The data transformed into a lower-dimensional space. - labels (np.ndarray): The labels corresponding to the data points. plt.figure(figsize=(10, 8)) scatter = plt.scatter(transformed_data[:, 0], transformed_data[:, 1], c=labels, cmap=plt.get_cmap(\'tab10\')) plt.colorbar(scatter, ticks=range(10)) plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.title(\'2D Scatter Plot of LLE Reduced Dimensions\') plt.show() def main(): digits = datasets.load_digits() data = digits.data labels = digits.target # Perform LLE to reduce data to 2 dimensions transformed_data = perform_LLE(data, output_dim=2, n_neighbors=10) # Visualize reduced dimensions visualize_reduced_dimensions(transformed_data, labels) if __name__ == \\"__main__\\": main()"},{"question":"You have been tasked to implement a function which takes two binary data streams and accurately compresses and decompresses them while maintaining their integrity. You will use Python\'s `zlib` module for this task. The function should demonstrate your understanding of: - Handling binary data streams. - Using compression and decompression objects. - Ensuring data integrity via checksums. # Function Signature ```python def compress_and_decompress(data1: bytes, data2: bytes) -> bytes: Compresses two binary data streams, concatenates them, then decompresses to return the original data. Parameters: data1 (bytes): The first binary data stream. data2 (bytes): The second binary data stream. Returns: bytes: The decompressed data which should be a concatenation of the original data1 and data2. pass ``` # Instructions: 1. **Compression:** - Use `zlib.compressobj` to create a compression object. - Compress `data1` and `data2` individually. - Concatenate the compressed results. 2. **Decompression:** - Use `zlib.decompressobj` to create a decompression object. - Decompress the concatenated compressed data. - Verify the decompressed data matches the original combined input (`data1` + `data2`). 3. **Checksum Verification:** - Compute and return a verification checksum on the original data (`data1` + `data2`) using `zlib.adler32`. - Verify if the decompressed data, when checked with `zlib.adler32`, matches the original checksum. # Constraints: - Ensure you handle edge cases such as empty data streams. - You can assume data will fit into memory. # Example: ```python data1 = b\\"First data stream. \\" data2 = b\\"Second data stream.\\" result = compress_and_decompress(data1, data2) print(result == data1 + data2) # Should print: True ```","solution":"import zlib def compress_and_decompress(data1: bytes, data2: bytes) -> bytes: Compresses two binary data streams, concatenates them, then decompresses to return the original data. Parameters: data1 (bytes): The first binary data stream. data2 (bytes): The second binary data stream. Returns: bytes: The decompressed data which should be a concatenation of the original data1 and data2. # Combine data1 and data2 combined_data = data1 + data2 # Create a compression object compressor = zlib.compressobj() # Compress data1 and data2 compressed_data = compressor.compress(combined_data) + compressor.flush() # Create a decompression object decompressor = zlib.decompressobj() # Decompress the compressed data decompressed_data = decompressor.decompress(compressed_data) + decompressor.flush() # Check integrity using checksums original_checksum = zlib.adler32(combined_data) decompressed_checksum = zlib.adler32(decompressed_data) assert original_checksum == decompressed_checksum, \\"Checksum mismatch\\" return decompressed_data"},{"question":"Coding Assessment Question: Processing XML with `xml.dom.pulldom` **Objective**: Demonstrate your understanding of the `xml.dom.pulldom` module by implementing a function that parses an XML document and processes specific elements based on certain criteria. # Problem Statement Write a Python function `process_xml(input_xml: str, tag_name: str, attribute_criteria: dict) -> list` that takes the following parameters: - `input_xml` (str): A string containing valid XML data. - `tag_name` (str): The name of the XML tag to search for. - `attribute_criteria` (dict): A dictionary where keys are attribute names and values are the required values for those attributes. The function should parse the XML data using the `xml.dom.pulldom` module, search for elements with the specified tag name that meet the attribute criteria, and return a list of their textual content including the expanded child nodes. # Input - The `input_xml` string contains well-formed XML. - The `tag_name` string specifies the tag to search for (case-sensitive). - The `attribute_criteria` dictionary specifies the attributes and their required values (case-sensitive). # Output - The function should return a list of strings, where each string is the textual content of an XML element matching the tag name and attribute criteria, including all child elements\' contents. # Constraints - The XML will be well-formed. - There may be multiple elements with the same tag name but with different attributes. - The `attribute_criteria` dictionary will only contain attributes that are present in the XML. # Example ```python def process_xml(input_xml: str, tag_name: str, attribute_criteria: dict) -> list: from xml.dom import pulldom results = [] doc = pulldom.parseString(input_xml) for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == tag_name: match = all(node.getAttribute(attr) == value for attr, value in attribute_criteria.items()) if match: doc.expandNode(node) results.append(node.toxml()) return results # Example Usage: xml = \'\'\' <catalog> <book id=\\"bk101\\" genre=\\"novel\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <price>44.95</price> </book> <book id=\\"bk102\\" genre=\\"fiction\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <price>5.95</price> </book> </catalog> \'\'\' tag_name = \\"book\\" attribute_criteria = {\\"genre\\": \\"novel\\"} print(process_xml(xml, tag_name, attribute_criteria)) # Output: [\'<book id=\\"bk101\\" genre=\\"novel\\">n <author>Gambardella, Matthew</author>n <title>XML Developer\'s Guide</title>n <price>44.95</price>n</book>\'] ``` The function `process_xml` correctly parses the given XML, finds the `<book>` element with `genre=\\"novel\\"`, expands its child nodes, and returns its complete text content.","solution":"def process_xml(input_xml: str, tag_name: str, attribute_criteria: dict) -> list: from xml.dom import pulldom results = [] doc = pulldom.parseString(input_xml) for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == tag_name: match = all(node.getAttribute(attr) == value for attr, value in attribute_criteria.items()) if match: doc.expandNode(node) results.append(node.toxml()) return results"},{"question":"# Audio Processing with Python’s `audioop` Module In this task, you are required to write a Python function that performs a series of manipulations on an audio fragment using the `audioop` module. This problem will test your understanding of basic operations, statistical analysis, and encoding conversions provided by the module. # Problem Statement Write a function `process_audio(fragment: bytes, width: int) -> dict` that takes an audio fragment and performs the following operations: 1. **Average Calculation**: Calculate the average value of the samples in the audio fragment. 2. **Maximum Calculation**: Calculate the maximum absolute value of the samples in the audio fragment. 3. **RMS Calculation**: Calculate the root-mean-square (RMS) value of the audio fragment. 4. **Mono to Stereo Conversion**: Convert the audio fragment from mono to stereo with left and right channel factors of 0.7 and 0.3, respectively. 5. **16-bit to 8-bit Conversion**: If the input `width` is 2 (16-bit samples), convert the audio fragment to 8-bit samples. The function should return a dictionary with the results of each operation as follows: ```python def process_audio(fragment: bytes, width: int) -> dict: This function processes an audio fragment using various audio operations. Parameters: fragment (bytes): The audio fragment as a bytes-like object. width (int): Width of the audio samples in bytes (1, 2, 3, or 4). Returns: dict: Dictionary containing the results of various operations on the audio fragment. { \'average\': <average_value>, \'max_value\': <max_value>, \'rms\': <rms_value>, \'stereo_fragment\': <stereo_fragment_bytes>, \'8bit_fragment\': <8bit_fragment_bytes> (only if width was 2) } pass # Example usage fragment = b\'...\' # some bytes-like audio fragment width = 2 result = process_audio(fragment, width) print(result) ``` # Constraints: - The fragment should be a bytes-like object. - The width should be one of `1`, `2`, `3`, or `4`. - If the width is not `2`, the function should return `None` for `8bit_fragment` in the resulting dictionary. # Expected Output Format: ```python { \'average\': <average_value>, \'max_value\': <max_value>, \'rms\': <rms_value>, \'stereo_fragment\': <stereo_fragment_bytes>, \'8bit_fragment\': <8bit_fragment_bytes> or None } ``` # Performance Requirement: - The function should handle audio fragments up to `1MB` in size efficiently. # Note: - You may assume all input fragments will be valid, and width will always be one of the four specified values. - Handle errors gracefully by raising appropriate exceptions if necessary.","solution":"import audioop def process_audio(fragment: bytes, width: int) -> dict: This function processes an audio fragment using various audio operations. Parameters: fragment (bytes): The audio fragment as a bytes-like object. width (int): Width of the audio samples in bytes (1, 2, 3, or 4). Returns: dict: Dictionary containing the results of various operations on the audio fragment. if width not in (1, 2, 3, 4): raise ValueError(\\"Width must be one of 1, 2, 3, or 4 bytes.\\") # Calculate average value of the samples average = audioop.avg(fragment, width) # Calculate the maximum absolute value of the samples max_value = audioop.max(fragment, width) # Calculate the root-mean-square (RMS) value of the samples rms = audioop.rms(fragment, width) # Convert from mono to stereo stereo_fragment = audioop.tostereo(fragment, width, 0.7, 0.3) # Convert 16-bit to 8-bit if width is 2 if width == 2: fragment_8bit = audioop.lin2lin(fragment, 2, 1) else: fragment_8bit = None return { \'average\': average, \'max_value\': max_value, \'rms\': rms, \'stereo_fragment\': stereo_fragment, \'8bit_fragment\': fragment_8bit }"},{"question":"Coding Assessment Question # Objective The goal is to assess your understanding of the Python `array` module and your ability to efficiently manage array data using the provided methods. # Problem Statement Given a text file containing lines of spaced-separated integers, you are required to perform various operations and transformations on the data using the `array` module. Your task is to implement a function `process_array_data(file_path)` that reads the integers from the file, performs specific operations, and produces a final output. # Function Signature ```python def process_array_data(file_path: str) -> List[int]: ``` # Input - `file_path`: A string representing the path to a text file. Each line in the file contains a space-separated list of integers. # Output - Returns a list of integers that follow the sequence of transformations described below. # Steps and Constraints 1. **Read the file**: Read the file whose path is provided as the argument and store each line as a separate sublist of integers using the array module with type code `\'i\'` for signed integers. 2. **Concatenate all arrays**: Concatenate all arrays into a single array. 3. **Reverse the array**: Reverse the order of elements in the concatenated array. 4. **Double each element**: Create a new array where each element is double the value of the corresponding element in the reversed array. 5. **Return as list**: Convert the final array to a list and return it. # Example Assume the `input.txt` file contains: ``` 1 2 3 4 5 6 7 8 9 ``` The function call `process_array_data(\\"input.txt\\")` should return: ```python [18, 16, 14, 12, 10, 8, 6, 4, 2] ``` # Notes - Use the array methods for reading, manipulating, and transforming the data. - Ensure efficient handling of the array elements through the defined steps. - Proper error handling for file operations and type constraints should be considered. Good Luck!","solution":"from array import array from typing import List def process_array_data(file_path: str) -> List[int]: arr_list = [] with open(file_path, \'r\') as file: for line in file: # Read each line and convert it into an array of integers arr = array(\'i\', map(int, line.split())) arr_list.append(arr) # Concatenate all arrays into one single array concatenated_array = array(\'i\') for arr in arr_list: concatenated_array.extend(arr) # Reverse the entire concatenated array concatenated_array.reverse() # Double each element in the reversed array doubled_array = array(\'i\', (2 * x for x in concatenated_array)) # Convert final array to a list and return return doubled_array.tolist()"},{"question":"Objective The goal is to assess your comprehension of pandas\' date offsets and your ability to manipulate date objects using these offsets. Problem Statement You are given a DataFrame that contains a list of dates and a column indicating the type of date offset to apply to each date. Based on the offset type and its properties, you need to create a new column in the DataFrame with the adjusted dates. Here is a sample structure of the input DataFrame: ```python import pandas as pd data = { \'date\': [\'2023-01-01\', \'2023-02-15\', \'2023-03-20\', \'2023-04-25\'], \'offset_type\': [\'BusinessDay\', \'MonthEnd\', \'Week\', \'YearEnd\'], \'offset_value\': [2, 0, 1, 0], # The number of units for the offset, e.g., 2 BusinessDays, 0 MonthEnd, etc. } df = pd.DataFrame(data) df[\'date\'] = pd.to_datetime(df[\'date\']) ``` You need to implement the function `apply_offsets(df)` that will take in this DataFrame and returns a new DataFrame with an additional column named `adjusted_date` that contains the dates adjusted by their corresponding offsets. Constraints 1. The `offset_type` is always one of the classes mentioned in the documentation (e.g., `BusinessDay`, `MonthEnd`, `Week`, `YearEnd`, etc.). 2. The `offset_value` is an integer representing the number of units for the offset. 3. You should validate the input DataFrame to ensure it contains the necessary columns (\'date\', \'offset_type\', \'offset_value\'). Function Signature ```python import pandas as pd def apply_offsets(df: pd.DataFrame) -> pd.DataFrame: pass ``` Example Given the sample data provided above, your function should return the following adjusted dates for a DataFrame: ``` date offset_type offset_value adjusted_date 0 2023-01-01 BusinessDay 2 2023-01-03 1 2023-02-15 MonthEnd 0 2023-02-28 2 2023-03-20 Week 1 2023-03-27 3 2023-04-25 YearEnd 0 2023-12-31 ``` Note: Ensure that your function handles edge cases and invalid inputs gracefully by returning appropriate error messages or handling exceptions. Hints - Utilize the import statements and functionalities from `pandas.tseries.offsets`. - You can dynamically create offset objects using `getattr(pandas.tseries.offsets, offset_type)(offset_value)`.","solution":"import pandas as pd from pandas.tseries.offsets import * def apply_offsets(df: pd.DataFrame) -> pd.DataFrame: # Validate input DataFrame required_columns = {\'date\', \'offset_type\', \'offset_value\'} if not required_columns.issubset(df.columns): raise ValueError(f\\"Input DataFrame must contain the following columns: {required_columns}\\") def adjust_date(row): offset_cls = getattr(pd.tseries.offsets, row[\'offset_type\']) offset_obj = offset_cls(row[\'offset_value\']) return row[\'date\'] + offset_obj df[\'adjusted_date\'] = df.apply(adjust_date, axis=1) return df"},{"question":"**Coding Assessment Question:** **Objective:** Demonstrate the ability to use Scikit-learn\'s `datasets` module to load, manipulate, and preprocess datasets from different sources. **Problem Statement:** You are tasked with performing the following steps: 1. Load the sample image \\"china.jpg\\" from Scikit-learn\'s `datasets` module. 2. Convert the image data from its default `uint8` dtype to a floating point representation and scale the values to the range [0, 1]. 3. Fetch the \\"iris\\" dataset from the OpenML repository using its name. Ensure that you specify the exact version of the dataset to be fetched (version 1). 4. Load a dataset in svmlight/libsvm format from a file named \\"sample_svmlight_file.txt\\". Assume the file has features in sparse format. **Instructions:** 1. **Loading and Manipulating Sample Image:** - Import the necessary modules and load the \\"china.jpg\\" sample image. - Convert the image data to a floating point representation and scale the values to the range [0, 1]. - Display the image using `matplotlib`. 2. **Fetching OpenML Dataset:** - Fetch the \\"iris\\" dataset from OpenML, ensuring you use the version \\"1\\". - Print the shape of the data and the first 5 rows of the dataset. 3. **Loading svmlight/libsvm Dataset:** - Load the dataset from the file named \\"sample_svmlight_file.txt\\" using the appropriate function. - Print the shapes of the feature matrix `X` and the label vector `y`. **Implementation:** Implement the functions for each of the tasks described above. ```python import matplotlib.pyplot as plt from sklearn.datasets import load_sample_image, fetch_openml, load_svmlight_file def load_and_process_image(): # Load the sample image \\"china.jpg\\" china = load_sample_image(\\"china.jpg\\") # Convert to floating point china_float = china.astype(float) / 255.0 # Display the image plt.imshow(china_float) plt.axis(\'off\') plt.tight_layout() plt.show() def fetch_and_display_openml_dataset(): # Fetch the \\"iris\\" dataset from OpenML with version specified iris = fetch_openml(name=\\"iris\\", version=1) # Print the shape of the data and the first 5 rows of the dataset print(f\'Data shape: {iris.data.shape}\') print(f\'First 5 rows of the dataset:n{iris.data.head()}\') def load_svmlight_dataset(): # Load dataset from svmlight/libsvm format file X, y = load_svmlight_file(\\"sample_svmlight_file.txt\\") # Print the shapes of the feature matrix X and the label vector y print(f\'Feature matrix shape: {X.shape}\') print(f\'Label vector shape: {y.shape}\') # Run the functions to achieve tasks load_and_process_image() fetch_and_display_openml_dataset() load_svmlight_dataset() ``` **Input and Output:** - The input files for the svmlight dataset must be placed in the specified path. - The output of the script will include the displayed image, the shape, and content details of the fetched OpenML dataset and the loaded svmlight dataset. **Constraints:** - Ensure proper handling of data types and scaling as per specified requirements. - Use Scikit-learn functions as mentioned in the problem statement. **Performance Requirements:** - The code should efficiently handle the image conversion and dataset loading within a reasonable time frame for typical dataset sizes used in machine learning tasks.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import load_sample_image, fetch_openml, load_svmlight_file import numpy as np def load_and_process_image(): # Load the sample image \\"china.jpg\\" china = load_sample_image(\\"china.jpg\\") # Convert to floating point china_float = china.astype(np.float32) / 255.0 # Display the image plt.imshow(china_float) plt.axis(\'off\') plt.tight_layout() plt.show() return china, china_float def fetch_and_display_openml_dataset(): # Fetch the \\"iris\\" dataset from OpenML with version specified iris = fetch_openml(name=\\"iris\\", version=1) # Print the shape of the data and the first 5 rows of the dataset print(f\'Data shape: {iris.data.shape}\') print(f\'First 5 rows of the dataset:n{iris.data.head()}\') return iris def load_svmlight_dataset(filename=\\"sample_svmlight_file.txt\\"): # Load dataset from svmlight/libsvm format file X, y = load_svmlight_file(filename) # Print the shapes of the feature matrix X and the label vector y print(f\'Feature matrix shape: {X.shape}\') print(f\'Label vector shape: {y.shape}\') return X, y"},{"question":"# Question: PyTorch Export and Dynamic Control Flow You are provided with a basic implementation of a PyTorch module that includes both static and dynamic values, and a dynamic control flow based on input data. Your task is to complete and export the model such that it handles dynamic shapes and values correctly. Implementation Details: 1. **Module Definition**: Define a PyTorch module with the following requirements: - The module should include a parameter and a buffer. - The `forward` method should take two inputs: - A tensor `x` with a dynamic shape. - An integer `y` which is static. - Implement a dynamic control flow within the `forward` method, where the computation depends on the sum of `x`. - If `x.sum() > y`, return the sine of `x`. - Otherwise, return the cosine of `x`. 2. **Export the Model**: - Use `torch.export.export` to trace the model. - Ensure the traced model reflects dynamic shape and value information correctly. - Make sure to handle guards and assertions on dynamic shapes as needed. Function Signature: ```python import torch class DynamicModule(torch.nn.Module): def __init__(self): super(DynamicModule, self).__init__() # Define a parameter. self.param = torch.nn.Parameter(torch.randn(1)) # Define a buffer. self.register_buffer(\'buffer\', torch.randn(1)) def forward(self, x: torch.Tensor, y: int): # Implement dynamic control flow if x.sum() > y: return x.sin() else: return x.cos() # Example usage: # model = DynamicModule() # example_input = (torch.randn(3, 3), 5) # exported_model = torch.export.export(model, example_input) # print(exported_model.graph_module.code) # Your task is to complete the implementation and export the model correctly. ``` Constraints and Requirements: - The solution should ensure the dynamic control flow is handled properly. - The exported model should correctly represent the dynamic computation based on the input tensor. - Efficiency and clarity in defining the dynamic behaviors and control flows are important. - Test the model with various inputs to ensure its correctness. Expected Output: - The printed graph module code after exporting the model.","solution":"import torch class DynamicModule(torch.nn.Module): def __init__(self): super(DynamicModule, self).__init__() # Define a parameter. self.param = torch.nn.Parameter(torch.randn(1)) # Define a buffer. self.register_buffer(\'buffer\', torch.randn(1)) def forward(self, x: torch.Tensor, y: int): # Implement dynamic control flow if x.sum() > y: return x.sin() else: return x.cos() # Example usage: if __name__ == \\"__main__\\": model = DynamicModule() example_input = (torch.randn(3, 3), 5) exported_model = torch.onnx.export(model, example_input, \\"dynamic_module.onnx\\", verbose=True, input_names=[\'input_x\', \'input_y\'], output_names=[\'output\']) print(f\\"Model exported to dynamic_module.onnx\\") \'\'\' Note: torch.export is not a real function in PyTorch. torch.onnx.export is used here instead to demonstrate exporting a model with dynamic behavior. \'\'\'"},{"question":"Objective: To assess your understanding of the `tarfile` module in Python, particularly in creating tar archives, adding files to them, and extracting files securely with appropriate filters. Problem Statement: You are given a directory containing various files and subdirectories. Your task is to write a Python script that: 1. Creates a tar archive (`output.tar.gz`) from the given directory, with gzip compression. 2. Adds a specific file (`additional_file.txt`) to this tar archive. 3. Extracts the contents of the created tar archive to a specified destination directory (`extracted_dir`), using the `data` filter to ensure only safe files are extracted. Instructions: 1. Implement the following function: ```python def create_and_extract_tar(input_dir: str, additional_file: str, output_tar: str, extraction_dir: str) -> None: Creates a tar.gz archive from the given input directory, adds an additional file, and extracts the archive to the specified extraction directory using a secure filter. Args: - input_dir (str): Path to the input directory to be archived. - additional_file (str): Path to an additional file to be added to the archive. - output_tar (str): Path to the output tar.gz archive file. - extraction_dir (str): Path to the directory where the archive will be extracted. Returns: - None ``` 2. Use the `tarfile` module to perform the operations. 3. Ensure that you use the `data` filter during extraction for security. 4. Handle any possible exceptions, and print relevant error messages if an operation fails. Constraints: - Assume the input directory and additional file already exist and are accessible. - The output tar.gz file and extraction directory paths will be valid and writable. - Use gzip compression for creating the tar archive. Example Usage: ```python input_dir = \\"path/to/input_dir\\" additional_file = \\"path/to/additional_file.txt\\" output_tar = \\"path/to/output.tar.gz\\" extraction_dir = \\"path/to/extracted_dir\\" create_and_extract_tar(input_dir, additional_file, output_tar, extraction_dir) ``` # Detailed Steps: 1. **Create Tar Archive**: - Open a tar file in write mode with gzip compression. - Add all files and subdirectories from the input directory to the tar archive. 2. **Add Additional File**: - Add the specified additional file to the tar archive. 3. **Extract Archive**: - Open the created tar archive for reading. - Extract all contents using the `data` filter to ensure only safe files are extracted. 4. **Handle Exceptions**: - Ensure your code properly handles exceptions such as `tarfile.TarError`, file not found errors, etc., and prints appropriate error messages. Example Implementation: ```python import os import tarfile def create_and_extract_tar(input_dir: str, additional_file: str, output_tar: str, extraction_dir: str) -> None: try: # Create tar.gz archive from input directory with tarfile.open(output_tar, \\"w:gz\\") as tar: tar.add(input_dir, arcname=os.path.basename(input_dir)) tar.add(additional_file, arcname=os.path.basename(additional_file)) print(f\\"Created tar.gz archive: {output_tar}\\") # Extract the tar archive with the secure \'data\' filter with tarfile.open(output_tar, \\"r:gz\\") as tar: tar.extractall(path=extraction_dir, filter=tarfile.data_filter) print(f\\"Extracted contents to: {extraction_dir}\\") except Exception as e: print(f\\"An error occurred: {e}\\") ``` Implement and test the function according to the provided instructions and constraints.","solution":"import os import tarfile def create_and_extract_tar(input_dir: str, additional_file: str, output_tar: str, extraction_dir: str) -> None: try: # Create tar.gz archive from input directory with tarfile.open(output_tar, \\"w:gz\\") as tar: tar.add(input_dir, arcname=os.path.basename(input_dir)) tar.add(additional_file, arcname=os.path.basename(additional_file)) print(f\\"Created tar.gz archive: {output_tar}\\") # Extract the tar archive with the secure \'data\' filter with tarfile.open(output_tar, \\"r:gz\\") as tar: tar.extractall(path=extraction_dir, filter=tarfile.data_filter) print(f\\"Extracted contents to: {extraction_dir}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Question: Custom Array Operations** You are tasked with enhancing the functionality of the Python `array` module by implementing a custom function that operates on arrays. Given an array of type `\'i\'` (signed int), your function should calculate the cumulative product of the elements and return a new array of the same type containing the cumulative product values. Write a function `cumulative_product(arr: array) -> array` that takes an array of type `\'i\'` as input and returns a new array of the same type containing the cumulative product of the elements in the input array. **Function Signature:** ```python def cumulative_product(arr: \'array\') -> \'array\': ``` **Input:** - `arr`: An array of type `\'i\'`, where 1 ≤ len(arr) ≤ 10^6, containing integers within the range of the signed int type. **Output:** - A new array of type `\'i\'`, where each element at index `i` is the product of all elements from index `0` to `i` (inclusive) of the input array. **Example:** ```python >>> from array import array >>> arr = array(\'i\', [1, 2, 3, 4]) >>> result = cumulative_product(arr) >>> print(result) array(\'i\', [1, 2, 6, 24]) ``` **Constraints:** - You may assume the input array will not result in integer overflow during product calculations. - The function must run in O(n) time complexity. - The function must handle large input sizes efficiently. (Note: Use the existing `array` module for implementation. Ensure you import the `array` class from the module as illustrated in the example.)","solution":"from array import array def cumulative_product(arr: array) -> array: Returns a new array containing the cumulative product of the elements in the input array. cum_prod_array = array(\'i\') cum_prod = 1 # Starting with 1 for multiplicative identity for num in arr: cum_prod *= num cum_prod_array.append(cum_prod) return cum_prod_array"},{"question":"**Objective**: Demonstrate your understanding of creating scatter plots using Seaborn in Python. Your task is to create a function that generates various scatter plots based on the given dataset and parameters. **Question**: You are provided with a dataset similar to the \\"tips\\" dataset in Seaborn. Your task is to implement a function `generate_scatter_plots` that accepts a DataFrame and generates multiple scatter plots based on the input parameters. Function Signature: ```python def generate_scatter_plots(df: pd.DataFrame) -> None: pass ``` Input: - `df`: A pandas DataFrame containing at least the columns `total_bill`, `tip`, `time`, `day`, and `size`. Requirements: 1. **Basic Scatter Plot**: - Create a scatter plot of `total_bill` vs `tip`. 2. **Categorical Hue**: - Create a scatter plot of `total_bill` vs `tip` with data points colored by `time`. 3. **Categorical Style**: - Create a scatter plot of `total_bill` vs `tip` with data points colored by `day` and varying marker styles by `time`. 4. **Numeric Hue**: - Create a scatter plot of `total_bill` vs `tip` with data points colored by `size` and ensure the hue is mapped quantitatively. 5. **Advanced Scatter Plot**: - Create a scatter plot of `total_bill` vs `tip` with data points: - Colored by `size`. - Size of points varying based on the `size` column. - Custom sizes for the markers in the range (20, 200). 6. **Wide-form Data Scatter Plot**: - Create a scatter plot using a DataFrame with wide-form data. 7. **Faceted Scatter Plot**: - Use `relplot` to create scatter plots of `total_bill` vs `tip` for each `time`, conditioned on `day` (i.e., create separate plots for each day). 8. **Custom Markers**: - Create a scatter plot with custom markers for `time`: set lunch (`\\"Lunch\\"`) to squares (`\\"s\\"`) and dinner (`\\"Dinner\\"`) to X markers (`\\"X\\"`). Output: - The function should generate and display the described scatter plots using `matplotlib.pyplot.show()`. Constraints: - Use Seaborn for creating the plots. - Ensure that all plots are displayed within the function. - Handle missing values appropriately by removing any rows with missing values in the columns used for plotting. Example: ```python import pandas as pd # Example usage tips = sns.load_dataset(\\"tips\\") generate_scatter_plots(df=tips) ``` This function should visualize the dataset in various ways, demonstrating proficiency in using Seaborn for creating complex and informative scatter plots.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def generate_scatter_plots(df: pd.DataFrame) -> None: # Drop rows with missing values in the columns used for plotting df = df.dropna(subset=[\'total_bill\', \'tip\', \'time\', \'day\', \'size\']) # Basic Scatter Plot plt.figure() sns.scatterplot(data=df, x=\'total_bill\', y=\'tip\') plt.title(\'Total Bill vs Tip\') plt.show() # Categorical Hue plt.figure() sns.scatterplot(data=df, x=\'total_bill\', y=\'tip\', hue=\'time\') plt.title(\'Total Bill vs Tip by Time\') plt.show() # Categorical Style plt.figure() sns.scatterplot(data=df, x=\'total_bill\', y=\'tip\', hue=\'day\', style=\'time\') plt.title(\'Total Bill vs Tip by Day and Time\') plt.show() # Numeric Hue plt.figure() scatter = sns.scatterplot(data=df, x=\'total_bill\', y=\'tip\', hue=\'size\', palette=\'coolwarm\') scatter.legend(title=\'Size\') plt.title(\'Total Bill vs Tip by Size\') plt.show() # Advanced Scatter Plot plt.figure() sns.scatterplot(data=df, x=\'total_bill\', y=\'tip\', size=\'size\', sizes=(20, 200), hue=\'size\', palette=\'coolwarm\') plt.title(\'Total Bill vs Tip by Size (Varying Point Sizes)\') plt.show() # Faceted Scatter Plot g = sns.relplot(data=df, x=\'total_bill\', y=\'tip\', col=\'day\', hue=\'time\', kind=\'scatter\') g.fig.suptitle(\'Total Bill vs Tip by Day and Time\', y=1.02) plt.show() # Custom Markers markers = {\\"Lunch\\": \\"s\\", \\"Dinner\\": \\"X\\"} plt.figure() sns.scatterplot(data=df, x=\'total_bill\', y=\'tip\', hue=\'time\', style=\'time\', markers=markers) plt.title(\'Total Bill vs Tip with Custom Markers for Time\') plt.show() # Example usage # tips = sns.load_dataset(\\"tips\\") # generate_scatter_plots(df=tips)"},{"question":"You are tasked with creating a Python function that automates the creation of built distributions for a Python package using the distutils package. Your function should handle various formats and options specified as input arguments. Here is a detailed breakdown of the requirements: **Function Specification** - **Function Name**: `create_built_distribution` - **Parameters**: 1. `setup_path` (str): The path to the `setup.py` file of the Python package. 2. `formats` (list): A list of strings representing the formats for the built distributions (e.g., `[\'gztar\', \'zip\', \'rpm\']`). 3. `options` (dict, optional): A dictionary of additional command-line options to pass to the `bdist` command and its sub-commands. Possible options include: - `packager` (str): Packager information for RPM packages. - `plat_name` (str): Platform name for cross-compiling on Windows (e.g., \'win-amd64\'). - `install_script` (str): The postinstallation script for Windows installers. **Return**: - None **Constraints**: 1. Assume `distutils` is available in the environment. 2. The function should validate the input formats and handle only those mentioned in the provided documentation. 3. If certain options are not applicable to the specified formats, they should be safely ignored. 4. The function should print meaningful messages about the progress and outcome of the distribution creation process. **Example Usage**: ```python create_built_distribution( setup_path=\'/path/to/package/setup.py\', formats=[\'gztar\', \'zip\', \'rpm\'], options={ \'packager\': \'John Doe <jdoe@example.org>\', \'plat_name\': \'win-amd64\' } ) ``` **Implementation**: Implement the function `create_built_distribution` that meets the above specifications. Ensure that the function generates the appropriate built distributions by invoking the necessary `bdist` commands with the specified formats and options. **Hints**: - You can use Python\'s `subprocess` module to invoke the `setup.py` script with various command-line arguments. - Make sure to handle and print any errors encountered during the build process.","solution":"import os import subprocess def create_built_distribution(setup_path, formats, options=None): Automates the creation of built distributions for a Python package using distutils. Parameters: - setup_path (str): The path to the setup.py file. - formats (list): List of strings representing formats for the built distributions. - options (dict, optional): Additional command-line options for the bdist command. Returns: - None if options is None: options = {} # Define valid formats based on common distutils formats valid_formats = [\'gztar\', \'ztar\', \'tar\', \'zip\', \'rpm\', \'deb\', \'wininst\', \'msi\'] # Validate the input formats for fmt in formats: if fmt not in valid_formats: raise ValueError(f\\"Invalid format: {fmt}. Valid formats are: {\', \'.join(valid_formats)}\\") # Build the base command base_cmd = [\'python\', setup_path, \'bdist\'] # Check for any special options and append them to the base command if \'packager\' in options: base_cmd.extend([\'--packager\', options[\'packager\']]) if \'plat_name\' in options: base_cmd.extend([\'--plat-name\', options[\'plat_name\']]) if \'install_script\' in options: base_cmd.extend([\'--install-script\', options[\'install_script\']]) for fmt in formats: # Prepare the command for each format cmd = base_cmd + [\'--formats\', fmt] try: print(f\\"Creating {fmt} distribution...\\") result = subprocess.run(cmd, check=True, capture_output=True, text=True) print(result.stdout) print(f\\"{fmt} distribution created successfully.\\") except subprocess.CalledProcessError as e: print(f\\"Error creating {fmt} distribution:\\") print(e.stderr) # Example usage #create_built_distribution( # setup_path=\'/path/to/package/setup.py\', # formats=[\'gztar\', \'zip\', \'rpm\'], # options={ # \'packager\': \'John Doe <jdoe@example.org>\', # \'plat_name\': \'win-amd64\' # } #)"},{"question":"BooleanArray Operations with NA Handling Objective: Design a function that performs complex data manipulation and logical operations over a DataFrame using pandas\' nullable boolean data type. Task: Write a function `filter_and_operate` that takes in a DataFrame with boolean and nullable boolean columns, and performs the following operations: 1. **Filtering**: Filter the rows where a specified nullable boolean column has `True` values (ignoring the `NA` values). 2. **Fill NA**: In the filtered DataFrame, replace all `NA` values in the specified column with the specified boolean value (`True` or `False`). 3. **Logical Operations**: Perform a Kleene logical OR operation between the two specified columns. You should keep NA values according to Kleene logic. 4. **Result Column**: Add a new column that stores the result of the logical OR operation. 5. **Final Filtering**: Filter the resulting DataFrame to keep only the rows where the new column is `True`. 6. Return the final DataFrame. Constraints: - Input DataFrame can have mixed data types, but the specified columns will always be of nullable boolean dtype. - You are not allowed to change the original DataFrame; the function should work on a copy. - No additional data libraries should be used aside from pandas. Function Signature: ```python def filter_and_operate(df: pd.DataFrame, filter_col: str, logic_col1: str, logic_col2: str, fill_value: bool) -> pd.DataFrame: pass ``` Input: - `df`: A pandas DataFrame containing at least three columns of nullable boolean dtype. - `filter_col`: The name of the column to be used for the initial filtering. - `logic_col1`: The first column name for the logical OR operation. - `logic_col2`: The second column name for the logical OR operation. - `fill_value`: The boolean value to replace `NA` in step 2. Output: - A pandas DataFrame filtered and transformed according to the above operations. Example: Given the following DataFrame: ```python import pandas as pd df = pd.DataFrame({ \'A\': pd.array([True, False, pd.NA, True], dtype=\\"boolean\\"), \'B\': pd.array([pd.NA, True, False, False], dtype=\\"boolean\\"), \'C\': pd.array([True, pd.NA, True, False], dtype=\\"boolean\\") }) ``` Calling the function `filter_and_operate(df, \'A\', \'B\', \'C\', False)` should return: ```python # Filter where column A is True (ignore NA and False) filtered_df = df[df[\'A\'] == True] # Fill NA in column A with False filtered_df[\'A\'].fillna(False, inplace=True) # Perform logical OR operation between column B and column C (Kleene logic) filtered_df[\'result\'] = filtered_df[\'B\'] | filtered_df[\'C\'] # Final filtering where the result column is True final_df = filtered_df[filtered_df[\'result\'] == True] # The final dataframe would look like: # A B C result # 0 True NaN True True # 3 True False False False # ``` Note that the actual return will be different based on actual nullable values and result of operations.","solution":"import pandas as pd def filter_and_operate(df: pd.DataFrame, filter_col: str, logic_col1: str, logic_col2: str, fill_value: bool) -> pd.DataFrame: # Step 1: Filter the rows where the specified nullable boolean column has `True` values filtered_df = df[df[filter_col] == True] # Step 2: Replace all `NA` values in the specified column with the specified boolean value filtered_df[filter_col] = filtered_df[filter_col].fillna(fill_value) # Step 3: Perform a Kleene logical OR operation between the two specified columns. # The \'|\' operator in pandas handles the Kleene logic automatically filtered_df[\'result\'] = filtered_df[logic_col1] | filtered_df[logic_col2] # Step 4: Filter the resulting DataFrame to keep only the rows where the new column is `True` final_df = filtered_df[filtered_df[\'result\'] == True] return final_df"},{"question":"# Question: Logging System Implementation You are required to implement a logging system using the Unix `syslog` library routines. Your task is to write a Python function `custom_logger` that handles logging with the following specifications: 1. **Initialization**: - Take parameters `ident`, `logoption`, and `facility` to initialize logging settings using `syslog.openlog`. - Set the priority mask using `syslog.setlogmask` such that only messages with priority `LOG_INFO` and higher (i.e., lower numerical value) are logged. 2. **Logging Messages**: - The function should accept a list of tuples, where each tuple contains a `priority` and `message`. - Log each message with the given priority using `syslog.syslog`. 3. **Reset Logger**: - After logging all the messages, reset the logger using `syslog.closelog`. # Function Signature ```python def custom_logger(ident: str, logoption: int, facility: int, messages: list): pass ``` # Input - `ident`: A string that is prepended to every message (e.g., \'my_app_logger\'). - `logoption`: An integer representing the logging option (e.g., `syslog.LOG_PID | syslog.LOG_CONS`). - `facility`: An integer representing the logging facility (e.g., `syslog.LOG_USER`). - `messages`: A list of tuples, each containing an integer `priority` (priority level defined in syslog constants) and a string `message`. # Output The function does not return anything. All side effects are through syslog logging. # Example ```python import syslog def custom_logger(ident, logoption, facility, messages): # Your implementation goes here # Example usage messages = [(syslog.LOG_INFO, \\"Information message\\"), (syslog.LOG_ERR, \\"Error occurred\\"), (syslog.LOG_DEBUG, \\"Debugging info\\")] custom_logger(\\"my_app_logger\\", syslog.LOG_PID | syslog.LOG_CONS, syslog.LOG_USER, messages) ``` # Constraints - You must use the provided `syslog` module functions to complete this task. - Ensure that log messages with a priority lower than `LOG_INFO` (like `LOG_DEBUG`) are not logged due to the priority mask setting. # Notes - Test your implementation thoroughly to ensure that it meets the requirements and handles edge cases. - The syslog module requires certain privileges to run; ensure you have the necessary permissions if testing on an actual Unix system.","solution":"import syslog def custom_logger(ident: str, logoption: int, facility: int, messages: list): Logs messages to the syslog with given priorities. Parameters: ident (str): The string identifier to prepend to messages. logoption (int): The logging options bitmask. facility (int): The facility code for logging. messages (list): A list of tuples with log priority and log message. # Initialize the logger syslog.openlog(ident=ident, logoption=logoption, facility=facility) # Set the priority mask to log messages with priority LOG_INFO and higher syslog.setlogmask(syslog.LOG_UPTO(syslog.LOG_INFO)) # Log each message for priority, message in messages: syslog.syslog(priority, message) # Reset the logger syslog.closelog()"},{"question":"**Advanced Memory Profiling with `tracemalloc`** # Objective: Use Python\'s `tracemalloc` module to profile memory usage in a Python script and identify potential memory leaks or inefficiencies. # Problem Statement: You are given a Python script `memory_hog.py` which you suspect to contain memory leaks or inefficient memory usage patterns. Your task is to write a Python function `profile_memory` that leverages the `tracemalloc` module to identify the top 10 lines in the script that allocate the most memory. Additionally, you will take snapshots before and after running a specific function in the script to compare memory usage and identify any significant differences. # Input: - **file_path (str)**: The path to the `memory_hog.py` script. # Output: - This function should print: 1. The top 10 lines in `memory_hog.py` that allocate the most memory. 2. The top 10 differences in memory allocation before and after running a specific function in the script. # Constraints: - Ensure that `tracemalloc` is started as early as possible in your script to catch all memory allocations. - Focus on line-by-line memory allocation statistics. - You should work with at least 10 stack frames to get detailed traceback information for memory allocations. # Sample Usage: ```python profile_memory(\\"path/to/memory_hog.py\\") ``` # Example Output: ``` [Top 10 Memory Allocations] 1. memory_hog.py:45: 1024.3 KiB some_variable = [i for i in range(1000000)] 2. memory_hog.py:88: 512.5 KiB another_variable = [j for j in range(500000)] ... [Top 10 Memory Differences] 1. memory_hog.py:45: size=1024.3 KiB (+1024.3 KiB), count=1 (+1), average=1024.3 KiB some_variable = [i for i in range(1000000)] 2. memory_hog.py:88: size=512.5 KiB (+512.5 KiB), count=1 (+1), average=512.5 KiB ... ``` # Requirements: 1. Implement memory profiling using the `tracemalloc` module. 2. Handle the script execution and memory profiling within your `profile_memory` function. 3. Capture before-and-after snapshots to identify differences in memory allocation. 4. Ensure clear and formatted output of the results, including the line of code causing the most memory allocation and the most significant changes. # Evaluation: - **Correctness**: The implementation must correctly capture and print memory allocation statistics and differences. - **Efficiency**: Efficient handling of memory profiling and snapshot comparisons. - **Clarity**: Clear and well-formatted output. # Notes: - Assume `memory_hog.py` contains a function `main()` that drives the script\'s main logic. - The function should handle any necessary imports and script execution logic. Function Signature: ```python def profile_memory(file_path: str) -> None: pass ```","solution":"import tracemalloc import linecache import runpy def display_top(snapshot, key_type=\'lineno\', limit=10): snapshot = snapshot.filter_traces(( tracemalloc.Filter(False, \\"<frozen importlib._bootstrap>\\"), tracemalloc.Filter(False, \\"<unknown>\\"), )) top_stats = snapshot.statistics(key_type) print(\\"[Top 10 Memory Allocations]\\") for index, stat in enumerate(top_stats[:limit], 1): frame = stat.traceback[0] print(f\\"{index}. {frame.filename}:{frame.lineno}: {stat.size / 1024:.1f} KiB\\") line = linecache.getline(frame.filename, frame.lineno).strip() if line: print(f\\" {line}\\") def profile_memory(file_path: str) -> None: tracemalloc.start(10) # Take the first snapshot snapshot1 = tracemalloc.take_snapshot() # Run the target script runpy.run_path(file_path, run_name=\\"__main__\\") # Take the second snapshot snapshot2 = tracemalloc.take_snapshot() print(\\"Top memory allocations after running the script:n\\") display_top(snapshot2) # Calculate memory differences top_stats_diff = snapshot2.compare_to(snapshot1, \'lineno\') print(\\"nTop memory differences between snapshots:n\\") print(\\"[Top 10 Memory Differences]\\") for index, stat in enumerate(top_stats_diff[:10], 1): frame = stat.traceback[0] print(f\\"{index}. {frame.filename}:{frame.lineno}: \\" f\\"size={stat.size_diff / 1024:.1f} KiB \\" f\\"(+{stat.size_diff / 1024:.1f} KiB), \\" f\\"count={stat.count_diff}, \\" f\\"average diff={stat.size_diff / stat.count_diff / 1024:.1f} KiB\\") line = linecache.getline(frame.filename, frame.lineno).strip() if line: print(f\\" {line}\\")"},{"question":"**PairGrid Customization with Seaborn** **Objective:** Write a Python function that creates a customized PairGrid plot using seaborn. Your function should demonstrate a comprehensive understanding of how to manipulate the grid elements and incorporate additional data dimensions using hue. **Function Signature:** ```python def create_custom_pairgrid(dataset: pd.DataFrame) -> sns.PairGrid: pass ``` **Requirements:** 1. **Input:** - `dataset`: A pandas DataFrame containing at least four numeric columns and one categorical column. 2. **Output:** - A seaborn PairGrid object which is already plotted but not shown with `plt.show()`. 3. **Function Specifications:** 1. Create a PairGrid with all numeric variables from the dataset. 2. Map a histogram plot on the diagonal using a hue variable based on the categorical column in the DataFrame. 3. Map a scatter plot on the off-diagonal elements with hue, where the size of the points is determined by one of the numeric columns. 4. Use different types of plots for the upper and lower triangles: - Use scatter plots for the upper triangle. - Use kde plots for the lower triangle. 5. Add a legend explaining the hue variable. 6. Ensure that the diagonal plots do not share the y-axis. 7. Use meaningful axis labels derived from the column names. **Constraints:** - The input DataFrame must have at least four numeric columns and one categorical column. If not, raise a ValueError. **Example Execution:** ```python import seaborn as sns import pandas as pd # Loading dataset df = sns.load_dataset(\'penguins\') # Your function should work with the provided dataset g = create_custom_pairgrid(df) # You may call plt.show() here to display the plot plt.show() ``` **Note:** - Utilize seaborn\'s `PairGrid` class and related plot mapping functions such as `map_diag()`, `map_offdiag()`, etc. - Handle exceptions gracefully and provide informative error messages. - Ensure the function uses efficient operations and seaborn plot functionalities effectively. **Performance Requirements:** - The solution should efficiently utilize seaborn\'s built-in functions for plotting without redundant operations or unnecessary overhead. The plot should render within a reasonable time frame for the given data size.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_custom_pairgrid(dataset: pd.DataFrame) -> sns.PairGrid: # Check if the DataFrame contains at least four numeric columns and one categorical column if len(dataset.select_dtypes(include=\'number\').columns) < 4: raise ValueError(\\"Input DataFrame must contain at least four numeric columns.\\") if len(dataset.select_dtypes(include=\'category\').columns) < 1: raise ValueError(\\"Input DataFrame must contain at least one categorical column.\\") # Select the numeric columns for the PairGrid and the first categorical column for hue numeric_cols = dataset.select_dtypes(include=\'number\').columns[:4] categorical_col = dataset.select_dtypes(include=\'category\').columns[0] # Create the PairGrid object g = sns.PairGrid(dataset, vars=numeric_cols, hue=categorical_col) # Map histograms on the diagonal g.map_diag(sns.histplot, kde=False, element=\\"bars\\") # Map scatter plots on the upper triangle g.map_upper(sns.scatterplot) # Map kde plots on the lower triangle g.map_lower(sns.kdeplot, cmap=\\"Blues_d\\") # Add legend to the PairGrid g.add_legend() # Set axis labels for i, label in enumerate(numeric_cols): g.axes[0, i].set_title(label) return g"},{"question":"**Question:** You are given a document containing several online transactions from an e-commerce website. Each transaction in the document includes the following details: 1. Transaction ID (alphanumeric string) 2. User ID (alphanumeric string) 3. Timestamp (formatted as YYYY-MM-DD HH:MM:SS) 4. Amount (formatted as a decimal number with two decimal places) 5. Product ID (alphanumeric string) 6. Quantity (integer) Here is an example of how the transactions are formatted: ``` TXN12345 UID67890 2023-02-01 12:45:30 150.75 PID98765 2 TXN12346 UID67891 2023-02-01 13:05:25 89.99 PID87654 1 TXN12347 UID67892 2023-02-01 13:22:10 200.00 PID76543 3 ... ``` # Task: Write a Python function `parse_transactions(data: str) -> List[Dict[str, Union[str, float, int]]]` that takes in a string representing the document and parses it to extract the relevant details from each transaction. The function should return a list of dictionaries where each dictionary represents a transaction, structured as follows: ```python [ { \\"TransactionID\\": \\"TXN12345\\", \\"UserID\\": \\"UID67890\\", \\"Timestamp\\": \\"2023-02-01 12:45:30\\", \\"Amount\\": 150.75, \\"ProductID\\": \\"PID98765\\", \\"Quantity\\": 2 }, { \\"TransactionID\\": \\"TXN12346\\", \\"UserID\\": \\"UID67891\\", \\"Timestamp\\": \\"2023-02-01 13:05:25\\", \\"Amount\\": 89.99, \\"ProductID\\": \\"PID87654\\", \\"Quantity\\": 1 }, ... ] ``` # Constraints: 1. The input string can contain multiple lines, where each line represents a transaction. 2. Each detail in the transaction is separated by a space. 3. The transactions do not contain any errors or malformed data. # Requirements: - Use the `re` module to parse the input string. - Ensure that the `Amount` is converted to a float and `Quantity` is converted to an integer in the resulting dictionary. # Example: ```python data = TXN12345 UID67890 2023-02-01 12:45:30 150.75 PID98765 2 TXN12346 UID67891 2023-02-01 13:05:25 89.99 PID87654 1 print(parse_transactions(data)) # Output should be: # [ # { # \\"TransactionID\\": \\"TXN12345\\", # \\"UserID\\": \\"UID67890\\", # \\"Timestamp\\": \\"2023-02-01 12:45:30\\", # \\"Amount\\": 150.75, # \\"ProductID\\": \\"PID98765\\", # \\"Quantity\\": 2 # }, # { # \\"TransactionID\\": \\"TXN12346\\", # \\"UserID\\": \\"UID67891\\", # \\"Timestamp\\": \\"2023-02-01 13:05:25\\", # \\"Amount\\": 89.99, # \\"ProductID\\": \\"PID87654\\", # \\"Quantity\\": 1 # } # ] ``` **Notes:** - Pay attention to how you manage and extract each component of the transaction using regular expressions. - Provide error handling or validation where appropriate. - Make sure that the function returns the correct data types for each field (e.g., `Amount` should be a float).","solution":"import re from typing import List, Dict, Union def parse_transactions(data: str) -> List[Dict[str, Union[str, float, int]]]: transactions = [] pattern = re.compile( r\'(w+) (w+) (d{4}-d{2}-d{2} d{2}:d{2}:d{2}) ([d.]+) (w+) (d+)\' ) lines = data.strip().split(\'n\') for line in lines: match = pattern.match(line) if match: transaction_id, user_id, timestamp, amount, product_id, quantity = match.groups() transactions.append({ \\"TransactionID\\": transaction_id, \\"UserID\\": user_id, \\"Timestamp\\": timestamp, \\"Amount\\": float(amount), \\"ProductID\\": product_id, \\"Quantity\\": int(quantity) }) return transactions"},{"question":"Overview You are given a dataset containing information about sales transactions in multiple stores. Your task is to analyze this data using the pandas library. Dataset The dataset `sales_data.csv` has the following columns: - `store_id`: the ID of the store. - `product_id`: the ID of the product sold. - `date`: the date of the transaction in YYYY-MM-DD format. - `quantity_sold`: the quantity of the product sold in the transaction. - `price_per_unit`: the price per unit of the product sold. Data ``` store_id,product_id,date,quantity_sold,price_per_unit 1,101,2023-01-01,4,2.5 1,102,2023-01-01,2,5.0 2,101,2023-01-02,1,2.5 2,103,2023-01-02,3,3.0 ... ``` Task Implement the following functions: 1. **generate_sales_summary**: - **Input:** A pandas DataFrame `df` containing the sales data as described above. - **Output:** A DataFrame that includes: - `store_id` - `total_sales`: the total sales amount in dollars for each store. (`quantity_sold` * `price_per_unit`) - `total_quantity_sold`: the total quantity of products sold in each store. - **Constraints:** Ensure that all numeric calculations are properly handled (e.g., data types are correct). 2. **filter_top_selling_stores**: - **Input:** A pandas DataFrame `summary_df` (as returned by `generate_sales_summary`) and an integer `top_n`. - **Output:** A DataFrame containing the top `top_n` stores ranked by `total_sales`. - **Constraints:** Ensure the output is sorted in descending order of `total_sales`. If two stores have the same `total_sales`, sort by `store_id`. 3. **get_top_selling_products**: - **Input:** A pandas DataFrame `df` and an integer `top_n`. - **Output:** A DataFrame containing the `top_n` products with the highest total sales amount across all stores. The DataFrame should include: - `product_id` - `total_sales`: sum of (`quantity_sold` * `price_per_unit`) for each product. - **Constraints:** Ensure the output is sorted in descending order of `total_sales`. If two products have the same `total_sales`, sort by `product_id`. Function Signatures ```python import pandas as pd def generate_sales_summary(df: pd.DataFrame) -> pd.DataFrame: pass def filter_top_selling_stores(summary_df: pd.DataFrame, top_n: int) -> pd.DataFrame: pass def get_top_selling_products(df: pd.DataFrame, top_n: int) -> pd.DataFrame: pass ``` Example Usage ```python data = { \'store_id\': [1, 1, 2, 2, 3], \'product_id\': [101, 102, 101, 103, 101], \'date\': [\'2023-01-01\', \'2023-01-01\', \'2023-01-02\', \'2023-01-02\', \'2023-01-03\'], \'quantity_sold\': [4, 2, 1, 3, 5], \'price_per_unit\': [2.5, 5.0, 2.5, 3.0, 2.5] } df = pd.DataFrame(data) summary_df = generate_sales_summary(df) top_stores_df = filter_top_selling_stores(summary_df, 2) top_products_df = get_top_selling_products(df, 2) ``` The provided data and example usage will help you test your implementation. Ensure to follow best practices for readability and efficiency.","solution":"import pandas as pd def generate_sales_summary(df: pd.DataFrame) -> pd.DataFrame: df[\'total_sales\'] = df[\'quantity_sold\'] * df[\'price_per_unit\'] summary_df = df.groupby(\'store_id\').agg( total_sales=(\'total_sales\', \'sum\'), total_quantity_sold=(\'quantity_sold\', \'sum\') ).reset_index() return summary_df def filter_top_selling_stores(summary_df: pd.DataFrame, top_n: int) -> pd.DataFrame: sorted_summary_df = summary_df.sort_values(by=[\'total_sales\', \'store_id\'], ascending=[False, True]) return sorted_summary_df.head(top_n) def get_top_selling_products(df: pd.DataFrame, top_n: int) -> pd.DataFrame: df[\'total_sales\'] = df[\'quantity_sold\'] * df[\'price_per_unit\'] products_summary_df = df.groupby(\'product_id\').agg( total_sales=(\'total_sales\', \'sum\') ).reset_index() sorted_products_summary_df = products_summary_df.sort_values(by=[\'total_sales\', \'product_id\'], ascending=[False, True]) return sorted_products_summary_df.head(top_n) # Example usage data = { \'store_id\': [1, 1, 2, 2, 3], \'product_id\': [101, 102, 101, 103, 101], \'date\': [\'2023-01-01\', \'2023-01-01\', \'2023-01-02\', \'2023-01-02\', \'2023-01-03\'], \'quantity_sold\': [4, 2, 1, 3, 5], \'price_per_unit\': [2.5, 5.0, 2.5, 3.0, 2.5] } df = pd.DataFrame(data) summary_df = generate_sales_summary(df) top_stores_df = filter_top_selling_stores(summary_df, 2) top_products_df = get_top_selling_products(df, 2)"},{"question":"Objective Implement a function that performs a series of operations on a list using the `python310` low-level API functions provided in the documentation. The goal is to demonstrate your understanding of these functions. Problem Statement Write a Python function `process_custom_list(operations)` that takes a list of operations and applies them sequentially to a new Python list. The function should use the appropriate `python310` low-level API functions to manipulate the list. The operations and their parameters will be provided as tuples within the operations list. ```python def process_custom_list(operations): pass ``` Input - `operations` is a list of tuples where each tuple represents an operation. The first element of the tuple is a string representing the operation name, and subsequent elements are the parameters for the operation. The operations can include: - `(\'create\', size)` - Create a new list of the specified size. - `(\'set_item\', index, value)` - Set the item at the specified index to the given value. - `(\'append\', value)` - Append the value to the list. - `(\'insert\', index, value)` - Insert the value at the specified index. - `(\'get_slice\', low, high)` - Get the slice from the list between low and high indexes. - `(\'set_slice\', low, high, new_list)` - Replace the slice from the list between low and high indexes with new_list. - `(\'sort\',)` - Sort the list. - `(\'reverse\',)` - Reverse the list. - `(\'to_tuple\',)` - Convert the list to a tuple. Output - The function should return the resulting list or tuple after performing all the operations. - If any operation results in an error (e.g., out-of-bounds access), the function should raise an appropriate IndexError or ValueError. Constraints - You must use the provided `python310` low-level API functions for list operations. - Assume all inputs are valid and respect the documented behavior of the functions. Example ```python operations = [ (\'create\', 5), (\'set_item\', 0, \'a\'), (\'set_item\', 1, \'b\'), (\'append\', \'c\'), (\'insert\', 2, \'d\'), (\'get_slice\', 1, 4), (\'set_slice\', 1, 2, [\'x\', \'y\']), (\'sort\',), (\'reverse\',), (\'to_tuple\') ] result = process_custom_list(operations) print(result) # Expected output: (\'y\', \'x\', \'b\', \'a\') ``` Notes - Use the details provided in the documentation to guide your implementation. - Make sure to handle all exceptions appropriately using Python\'s error handling mechanisms. - This task evaluates your understanding of list manipulation at a low level and your ability to integrate multiple operations cohesively.","solution":"def process_custom_list(operations): Processes a list of operations sequentially to manipulate a list according to the given operations. result_list = [] for operation in operations: op_name = operation[0] if op_name == \'create\': size = operation[1] result_list = [None] * size elif op_name == \'set_item\': index, value = operation[1], operation[2] result_list[index] = value elif op_name == \'append\': value = operation[1] result_list.append(value) elif op_name == \'insert\': index, value = operation[1], operation[2] result_list.insert(index, value) elif op_name == \'get_slice\': low, high = operation[1], operation[2] result_list = result_list[low:high] elif op_name == \'set_slice\': low, high, new_list = operation[1], operation[2], operation[3] result_list[low:high] = new_list elif op_name == \'sort\': result_list.sort() elif op_name == \'reverse\': result_list.reverse() elif op_name == \'to_tuple\': result_list = tuple(result_list) else: raise ValueError(f\\"Unknown operation: {op_name}\\") return result_list"},{"question":"<|Analysis Begin|> The provided documentation primarily details various ways to use Seaborn\'s `color_palette` function. This function allows users to create color palettes for their visualizations using different syntax and inputs. Users can generate palettes using: - The current default color cycle. - A named categorical palette, e.g., \\"pastel\\". - A number of evenly spaced hues in the \\"HUSL\\" system. - Categorical Color Brewer palettes, e.g., \\"Set2\\". - Diverging Color Brewer palettes, e.g., \\"Spectral\\". - Perceptually-uniform palettes included in Seaborn, e.g., \\"flare\\". - Custom Cubehelix palettes. - Sequential and reversed gradient palettes. - Blended gradient palettes between two endpoints. The documentation also demonstrates how to change the qualitative color palette using a context manager and how to visualize the underlying color values as hex codes. Given that the provided documentation gives detailed examples on generating and customizing color palettes, a challenging question would involve the creation of a visualization that requires selecting and applying an appropriate color palette for meaningful interpretation of the data. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** To assess your understanding of the Seaborn `color_palette` function and its application in creating meaningful visualizations. **Question:** You are given a dataset that contains information about the total bills and tips at a restaurant. Your task is to create a scatter plot to display the relationship between \'total_bill\' and \'tip\' and use different color palettes to highlight the \'day\' of the week when the data was collected. **Dataset:** The dataset `tips` contains the following columns: - `total_bill`: The total bill amount. - `tip`: The tip amount. - `day`: The day of the week. **Requirements:** 1. Import the `tips` dataset using the `seaborn` library. 2. Create a scatter plot using `seaborn` where: - The x-axis represents `total_bill`. - The y-axis represents `tip`. - The color of each point represents the `day` of the week. 3. Demonstrate the scatter plot with three different color palettes: - \\"Set1\\" - \\"husl\\" - \\"coolwarm\\" (used as a colormap) **Specifications:** - Your code should be written in Python. - You should use the `seaborn` library to create the visualizations. - Ensure the visualizations are clearly labeled and use appropriate legends to indicate different days. - Comment your code to explain key steps. - The visualizations should be presented in a manner that is easy to interpret. **Input Format:** No explicit input format is required. You should use the `seaborn` `tips` dataset. **Output Format:** Your output will include three well-documented scatter plots using the specified color palettes. **Example Output:** Your output should look similar to the scatter plots demonstrated below (note that actual visualizations will depend on the `tips` dataset provided by `seaborn`): ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Scatter plot with \'Set1\' plt.figure(figsize=(10, 6)) sns.scatterplot(x=\'total_bill\', y=\'tip\', hue=\'day\', palette=\'Set1\', data=tips) plt.title(\'Scatter Plot with Set1 Palette\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.legend(title=\'Day\') plt.show() # Scatter plot with \'husl\' plt.figure(figsize=(10, 6)) sns.scatterplot(x=\'total_bill\', y=\'tip\', hue=\'day\', palette=\'husl\', data=tips) plt.title(\'Scatter Plot with HUSL Palette\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.legend(title=\'Day\') plt.show() # Scatter plot with \'coolwarm\' colormap plt.figure(figsize=(10, 6)) sns.scatterplot(x=\'total_bill\', y=\'tip\', hue=\'day\', palette=\'coolwarm\', data=tips) plt.title(\'Scatter Plot with Coolwarm Colormap\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.legend(title=\'Day\') plt.show() ``` Develop your code solution accordingly to meet these requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_scatter_plot(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Scatter plot with \'Set1\' plt.figure(figsize=(10, 6)) sns.scatterplot(x=\'total_bill\', y=\'tip\', hue=\'day\', palette=\'Set1\', data=tips) plt.title(\'Scatter Plot with Set1 Palette\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.legend(title=\'Day\') plt.show() # Scatter plot with \'husl\' plt.figure(figsize=(10, 6)) sns.scatterplot(x=\'total_bill\', y=\'tip\', hue=\'day\', palette=\'husl\', data=tips) plt.title(\'Scatter Plot with HUSL Palette\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.legend(title=\'Day\') plt.show() # Scatter plot with \'coolwarm\' colormap plt.figure(figsize=(10, 6)) sns.scatterplot(x=\'total_bill\', y=\'tip\', hue=\'day\', palette=\'coolwarm\', data=tips) plt.title(\'Scatter Plot with Coolwarm Colormap\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.legend(title=\'Day\') plt.show()"},{"question":"**Problem Statement:** You are provided with two datasets: the classic `tips` dataset from Seaborn, and a custom dataset of your choice that contains at least three numerical columns. Your task is to implement a function `custom_rugplot` that generates complex visualizations by making use of Seaborn\'s `scatterplot` and `rugplot` functionalities. # Function Signature ```python def custom_rugplot(tips_data, custom_data): pass ``` # Input 1. `tips_data`: A DataFrame similar to the Seaborn `tips` dataset. It contains the columns `total_bill`, `tip`, `time`, and other categorical and numerical data. 2. `custom_data`: A DataFrame with at least three numerical columns. # Output This function should produce the following plots: 1. A scatterplot with `total_bill` on the x-axis and `tip` on the y-axis, with rugs along both axes. 2. The same scatterplot as in (1), but with a hue mapping based on the `time` column. 3. A scatterplot of the custom dataset with the first numerical column on the x-axis and the second numerical column on the y-axis, with rugs on both axes. Customizations should include: - Increased rug height. - Rugs positioned just outside the axes. - Thinner rug lines using `lw`. - Alpha blending for better density visualization if the dataset is large. # Constraints 1. You may assume that the input DataFrames are clean and contain no missing values. 2. Make sure to include appropriate titles and labels for readability. 3. Use Seaborn\'s default themes for consistency. # Example To guide you, here is an example: ```python import seaborn as sns def custom_rugplot(tips_data, custom_data): sns.set_theme() # Plot 1 sns.scatterplot(data=tips_data, x=\\"total_bill\\", y=\\"tip\\") sns.rugplot(data=tips_data, x=\\"total_bill\\", y=\\"tip\\") # Show the first plot plt.title(\'Scatterplot with Rugs - Tips Data\') plt.show() # Plot 2 sns.scatterplot(data=tips_data, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") sns.rugplot(data=tips_data, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") # Show the second plot plt.title(\'Scatterplot with Hue and Rugs - Tips Data\') plt.show() # Plot 3 numerical_columns = custom_data.select_dtypes(include=[\'float\', \'int\']).columns sns.scatterplot(data=custom_data, x=numerical_columns[0], y=numerical_columns[1]) sns.rugplot(data=custom_data, x=numerical_columns[0], y=numerical_columns[1], height=.1, clip_on=False, lw=0.5, alpha=0.005) # Show the third plot plt.title(f\'Scatterplot with Rugs - Custom Data\') plt.show() ``` Ensure you validate the plots to confirm they meet all the specified criteria and provide a detailed representation of the data through visualizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_rugplot(tips_data, custom_data): sns.set_theme() # Plot 1: Scatterplot with rugs for tips_data plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips_data, x=\\"total_bill\\", y=\\"tip\\") sns.rugplot(data=tips_data, x=\\"total_bill\\", y=\\"tip\\") plt.title(\'Scatterplot with Rugs - Tips Data\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show() # Plot 2: Scatterplot with hue for tips_data plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips_data, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") sns.rugplot(data=tips_data, x=\\"total_bill\\", y=\\"tip\\") plt.title(\'Scatterplot with Hue and Rugs - Tips Data\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show() # Plot 3: Scatterplot with rugs for custom_data numerical_columns = custom_data.select_dtypes(include=[\'float\', \'int\']).columns plt.figure(figsize=(10, 6)) sns.scatterplot(data=custom_data, x=numerical_columns[0], y=numerical_columns[1]) sns.rugplot(data=custom_data, x=numerical_columns[0], y=numerical_columns[1], height=.1, clip_on=False, lw=0.5, alpha=0.7) plt.title(\'Scatterplot with Rugs - Custom Data\') plt.xlabel(numerical_columns[0]) plt.ylabel(numerical_columns[1]) plt.show()"},{"question":"# **Coding Assessment Question** Objective: Implement a function to manipulate sets based on a series of operations, ensuring proper error handling and performance considerations. Problem Statement: Write a function `manipulate_sets(operations: List[Tuple[str, Any]]) -> List[Any]` that takes a list of operations and applies them to an initially empty set. The function should return a list of the results of each operation. Each operation is represented by a tuple where the first element is a string (indicating the operation) and the second element is the required parameter for the operation. Valid operations are: - `\\"add\\", element`: Adds the `element` to the set. - `\\"remove\\", element`: Removes the `element` from the set if it exists, raises a `KeyError` otherwise. - `\\"discard\\", element`: Removes the `element` from the set if it exists, does nothing otherwise. - `\\"contains\\", element`: Checks if the `element` is in the set, returns `True` or `False`. - `\\"size\\"`: Returns the number of elements in the set. Ensure that you handle the following: - Unhashable elements should raise a `TypeError` during add and remove operations. - Operations are performed in the order they appear in the input list. - The function should be efficient, making use of the set\'s performance features. Input: - `operations` (List[Tuple[str, Any]]): A list of tuples representing the operations to perform on the set. Output: - List[Any]: A list of results for each operation that returns a value (`remove` and `discard` operations do not return a value). Constraints: - The list `operations` will have at most ( 10^5 ) operations. - Elements provided in the operations can be of any data type. Example: ```python operations = [ (\\"add\\", 10), (\\"add\\", [1, 2, 3]), (\\"contains\\", 10), (\\"remove\\", 10), (\\"discard\\", 5), (\\"size\\"), ] ``` Expected Output: ```python [ None, TypeError, # lists are unhashable True, None, None, 0 ] ``` Notes: 1. Use `try` and `except` blocks to handle errors such as `TypeError` and `KeyError`. 2. The function should correctly manipulate and query the set while keeping track of all operations and their results.","solution":"def manipulate_sets(operations): Manipulates a set based on a series of operations and returns the results. :param operations: List of tuples representing the operations. :return: List of results for each operation that returns a value. my_set = set() results = [] for operation in operations: op_type, element = operation if len(operation) == 2 else (operation[0], None) if op_type == \'add\': try: my_set.add(element) results.append(None) except TypeError: results.append(TypeError) elif op_type == \'remove\': try: my_set.remove(element) results.append(None) except KeyError: results.append(KeyError) except TypeError: results.append(TypeError) elif op_type == \'discard\': try: my_set.discard(element) results.append(None) except TypeError: results.append(TypeError) elif op_type == \'contains\': try: results.append(element in my_set) except TypeError: results.append(TypeError) elif op_type == \'size\': results.append(len(my_set)) return results"},{"question":"# Advanced Descriptor Implementation **Problem Statement:** You are required to implement an advanced descriptor in Python that combines several concepts from the provided documentation: dynamic lookups, managed attributes, validation, and automatic name notification. **Descriptor Specification:** 1. **DynamicAttribute**: - This descriptor should dynamically compute a value based on the instance\'s other attribute. - The descriptor should raise an error if the required instance attribute does not meet certain validation criteria. **Requirements**: - Implement a class `DynamicAttribute` as a descriptor. - It should accept a `dependency` parameter, which is the name of another attribute in the instance. - It should accept a `transform` parameter, which is a function applied to the dependent attribute to compute the dynamically looked-up value. - It should also accept `validator`, which is a function to validate the dependent attribute\'s value. 2. **Validation**: - Before computing the dynamic value, the descriptor should validate the dependent attribute\'s value using the validator function. - If validation fails, it should raise a `ValueError` with a suitable error message. 3. **Automatic Name Notification**: - The descriptor should be aware of its name in the owning class. 4. **Integration**: - Implement a class `Product` that uses this descriptor to manage its attributes. **Input and Output:** - The descriptor and class will be tested as follows: a. `Product` class has attributes `price` (float), `discount` (float), and `final_price` using the `DynamicAttribute` descriptor. b. `final_price` should be computed as `price - (price * discount / 100)`. c. Validations: - `price` must be a non-negative float. - `discount` must be between 0 and 100. **Constraints:** - The `price` and `discount` must be provided at the instance creation and be valid according to the validations. # Example Usage: ```python class DynamicAttribute: def __init__(self, dependency, transform, validator=None): # Constructor implementation here pass def __set_name__(self, owner, name): # Automatic name notification implementation here pass def __get__(self, obj, objtype=None): # Dynamic lookup with validation implementation here pass class Product: price = DynamicAttribute(\'price\', lambda x: x, lambda x: x >= 0) discount = DynamicAttribute(\'discount\', lambda x: x, lambda x: 0 <= x <= 100) final_price = DynamicAttribute(\'price\', lambda x: x.price - (x.price * x.discount / 100), None) def __init__(self, price, discount): self.price = price self.discount = discount # Example Test Cases try: product = Product(100.0, 20.0) print(product.final_price) # Expected Output: 80.0 invalid_product = Product(-50.0, 20.0) # Expected to raise ValueError for invalid price except ValueError as e: print(e) # Expected to print validation error message try: bad_discount = Product(50.0, 120.0) # Expected to raise ValueError for invalid discount except ValueError as e: print(e) # Expected to print validation error message ``` **Your task** is to implement the `DynamicAttribute` descriptor class and ensure it works as expected with the `Product` class.","solution":"class DynamicAttribute: def __init__(self, dependency, transform, validator=None): self.dependency = dependency self.transform = transform self.validator = validator self.name = None def __set_name__(self, owner, name): self.name = name def __get__(self, obj, objtype=None): if obj is None: return self value = getattr(obj, self.dependency) if self.validator: if not self.validator(value): raise ValueError(f\\"Validation failed for {self.dependency} with value {value}\\") return self.transform(obj) class Product: def __init__(self, price, discount): self.price = price self.discount = discount price = DynamicAttribute(\'price\', lambda x: x.price, lambda x: x >= 0) discount = DynamicAttribute(\'discount\', lambda x: x.discount, lambda x: 0 <= x <= 100) final_price = DynamicAttribute( \'price\', lambda self: self.price - (self.price * self.discount / 100), None )"},{"question":"**Coding Assessment Question** In this assignment, you are required to work with the Unix group database using the `grp` module. Your task is to implement a function that retrieves detailed information about a group\'s members using both the group and password databases. # Task Implement the function `get_group_members_info(group_name)` that does the following: 1. Uses the `grp.getgrnam(group_name)` function to get the group entry for the specified group name. 2. Retrieves the usernames from the group entry. 3. For each username, get the user details from the password database (using the `pwd` module which is similar to the `grp` module for user details). 4. Return a list of dictionaries, where each dictionary contains the following user details: - `username`: the username (string) - `uid`: user ID (integer) - `gid`: group ID (integer) - `home`: home directory (string) - `shell`: login shell (string) # Function Signature ```python def get_group_members_info(group_name: str) -> list: ... ``` # Input - `group_name`: A string representing the name of the group (e.g., \\"wheel\\", \\"staff\\"). # Output - A list of dictionaries, where each dictionary contains user details with the keys `username`, `uid`, `gid`, `home`, and `shell`. If the group does not exist, return an empty list. If the group has no members, return an empty list. # Example ```python result = get_group_members_info(\\"your_group_name\\") print(result) ``` In the above example, replace `\\"your_group_name\\"` with a real group name on your Unix system to see the output. # Constraints - Use proper exception handling to manage cases where the group or user entries are not found. - Ensure your function works efficiently with potentially large datasets. # Notes - The `pwd` module provides similar functionality to the `grp` module and has a function `pwd.getpwnam(username)` that returns user details. - Both `grp` and `pwd` modules are Unix-specific and will be available in Unix-based environments (e.g., Linux, macOS). **Good luck, and happy coding!**","solution":"import grp import pwd def get_group_members_info(group_name: str) -> list: try: group_info = grp.getgrnam(group_name) except KeyError: # If the group does not exist, return an empty list return [] usernames = group_info.gr_mem member_details = [] for username in usernames: try: user_info = pwd.getpwnam(username) member_details.append({ \'username\': user_info.pw_name, \'uid\': user_info.pw_uid, \'gid\': user_info.pw_gid, \'home\': user_info.pw_dir, \'shell\': user_info.pw_shell }) except KeyError: # If the user does not exist in the password database, # we ignore this user and continue with others continue return member_details"},{"question":"# Command-Line Data Analysis Tool In this task, you are required to implement a command-line data analysis tool using Python. Your task is to use the `argparse` module to create a script that accepts several command-line arguments to perform basic data analysis on a dataset of numbers provided by the user. Requirements: 1. The script should accept a filename of a text file containing one number per line. 2. The script should accept optional command-line arguments to: - Calculate the mean (`--mean`). - Calculate the median (`--median`). - Calculate the mode (`--mode`). - Calculate the variance (`--variance`). 3. The script should only compute the statistics requested via the command-line arguments. 4. The script should handle any errors gracefully, such as file not found or invalid data in the file. Expected Input and Output: - The script will be run from the command line with arguments like `python data_analysis.py --file data.txt --mean --median`. - The script will read numbers from the specified file, perform requested calculations, and print the results to the console. File Format: - The input file contains one number per line. For example: ``` 10 20 30 40 50 ``` Constraints: - You must use the `argparse` module for argument parsing. - Handle invalid input gracefully, including missing file or non-numeric data in the file. - Implement the calculations for mean, median, mode, and variance manually without using external libraries like numpy or statistics. Example: Suppose we have a file named `numbers.txt` containing: ``` 1 2 2 3 4 ``` Running the script with `python data_analysis.py --file numbers.txt --mean --mode` should print: ``` Mean: 2.4 Mode: 2 ``` Here is a template to get you started: ```python import argparse import sys def read_numbers(filename): try: with open(filename, \'r\') as file: numbers = [float(line.strip()) for line in file.readlines()] return numbers except FileNotFoundError: print(f\\"Error: File {filename} not found.\\") sys.exit(1) except ValueError: print(f\\"Error: Invalid data in file {filename}. Ensure all lines contain numbers.\\") sys.exit(1) def calculate_mean(numbers): return sum(numbers) / len(numbers) def calculate_median(numbers): sorted_numbers = sorted(numbers) mid = len(numbers) // 2 if len(numbers) % 2 == 0: return (sorted_numbers[mid - 1] + sorted_numbers[mid]) / 2 return sorted_numbers[mid] def calculate_mode(numbers): from collections import Counter count = Counter(numbers) max_count = max(count.values()) mode = [key for key, value in count.items() if value == max_count] return mode[0] # Assuming no tie for simplicity def calculate_variance(numbers): mean = calculate_mean(numbers) variance = sum((x - mean) ** 2 for x in numbers) / len(numbers) return variance def main(): parser = argparse.ArgumentParser(description=\'Perform basic data analysis on a dataset of numbers.\') parser.add_argument(\'--file\', type=str, required=True, help=\'The file containing the dataset.\') parser.add_argument(\'--mean\', action=\'store_true\', help=\'Calculate the mean of the dataset.\') parser.add_argument(\'--median\', action=\'store_true\', help=\'Calculate the median of the dataset.\') parser.add_argument(\'--mode\', action=\'store_true\', help=\'Calculate the mode of the dataset.\') parser.add_argument(\'--variance\', action=\'store_true\', help=\'Calculate the variance of the dataset.\') args = parser.parse_args() numbers = read_numbers(args.file) if args.mean: print(f\\"Mean: {calculate_mean(numbers)}\\") if args.median: print(f\\"Median: {calculate_median(numbers)}\\") if args.mode: print(f\\"Mode: {calculate_mode(numbers)}\\") if args.variance: print(f\\"Variance: {calculate_variance(numbers)}\\") if __name__ == \'__main__\': main() ```","solution":"import argparse import sys def read_numbers(filename): try: with open(filename, \'r\') as file: numbers = [float(line.strip()) for line in file.readlines()] return numbers except FileNotFoundError: print(f\\"Error: File {filename} not found.\\") sys.exit(1) except ValueError: print(f\\"Error: Invalid data in file {filename}. Ensure all lines contain numbers.\\") sys.exit(1) def calculate_mean(numbers): return sum(numbers) / len(numbers) def calculate_median(numbers): sorted_numbers = sorted(numbers) mid = len(numbers) // 2 if len(numbers) % 2 == 0: return (sorted_numbers[mid - 1] + sorted_numbers[mid]) / 2 return sorted_numbers[mid] def calculate_mode(numbers): from collections import Counter count = Counter(numbers) max_count = max(count.values()) mode = [key for key, value in count.items() if value == max_count] return mode[0] # Assuming no tie for simplicity def calculate_variance(numbers): mean = calculate_mean(numbers) variance = sum((x - mean) ** 2 for x in numbers) / len(numbers) return variance def main(): parser = argparse.ArgumentParser(description=\'Perform basic data analysis on a dataset of numbers.\') parser.add_argument(\'--file\', type=str, required=True, help=\'The file containing the dataset.\') parser.add_argument(\'--mean\', action=\'store_true\', help=\'Calculate the mean of the dataset.\') parser.add_argument(\'--median\', action=\'store_true\', help=\'Calculate the median of the dataset.\') parser.add_argument(\'--mode\', action=\'store_true\', help=\'Calculate the mode of the dataset.\') parser.add_argument(\'--variance\', action=\'store_true\', help=\'Calculate the variance of the dataset.\') args = parser.parse_args() numbers = read_numbers(args.file) if args.mean: print(f\\"Mean: {calculate_mean(numbers)}\\") if args.median: print(f\\"Median: {calculate_median(numbers)}\\") if args.mode: print(f\\"Mode: {calculate_mode(numbers)}\\") if args.variance: print(f\\"Variance: {calculate_variance(numbers)}\\") if __name__ == \'__main__\': main()"},{"question":"**Question: Analyzing and Visualizing Penguins Data** You are given the penguins dataset from seaborn, which contains data about different species of penguins including their body mass, flipper length, and other measurements. Your task is to create several visualizations using seaborn\'s object-oriented interface to analyze the dataset. **Task:** 1. Load the `penguins` dataset from seaborn. 2. Create a bar plot showing the average body mass of each species of penguins. Each bar should be color-coded by the sex of the penguins, and the bars for different sexes should be dodged to avoid overlap. 3. Add error bars to your bar plot to show the standard deviation of the body mass for each species and sex. 4. Customize the appearance of the bars by: - Setting the alpha (transparency) of the bars to 0.5 - Adding a black edge line with a width of 2 pixels around each bar 5. (Bonus) Create a histogram of the distribution of flipper lengths for each species, using different colors for each species. **Detailed Requirements:** 1. **Loading the Dataset:** ```python from seaborn import load_dataset penguins = load_dataset(\\"penguins\\") ``` 2. **Bar Plot:** - X-axis should represent the species. - Bars should represent the average body mass of each species. - Bars for different sexes should be color-coded and dodged to avoid overlap. - Add error bars to represent the standard deviation. - Bars should have an alpha of 0.5 and black edge lines with a width of 2 pixels. 3. **Histogram (Bonus):** - Create a histogram showing the distribution of flipper lengths for each species. - Distinguish species by using different colors. **Submission:** Submit your Python code that accomplishes the above tasks. Ensure your code is well-documented and includes necessary comments to explain each step. **Example Output:** - Your bar plot should look something like this (labels and colors may vary): ``` Species Average Body Mass (g) ------------------------------ Adelie # # Chinstrap # # Gentoo # # [Male] [Female] ``` Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create a bar plot showing the average body mass of each species, color-coded by sex bar_plot = sns.catplot( data=penguins, kind=\\"bar\\", x=\\"species\\", y=\\"body_mass_g\\", hue=\\"sex\\", ci=\\"sd\\", dodge=True, alpha=0.5, edgecolor=\\"black\\", linewidth=2 ) # Customize the plot bar_plot.set_axis_labels(\\"Species\\", \\"Body Mass (g)\\") bar_plot.set_titles(\\"Average Body Mass by Species and Sex\\") bar_plot.despine(left=True) # Show the plot plt.show() # (Bonus) Create a histogram of the distribution of flipper lengths for each species hist_plot = sns.histplot( data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", multiple=\\"stack\\", palette=\\"Set1\\" ) # Customize the plot hist_plot.set_xlabel(\\"Flipper Length (mm)\\") hist_plot.set_title(\\"Distribution of Flipper Lengths by Species\\") # Show the plot plt.show()"},{"question":"# XML Parsing with SAX: Implementing a Custom Content Handler You are tasked with implementing a custom XML parser using the `xml.sax.xmlreader` module in Python. The goal is to parse a provided XML document, handle various SAX events, and extract specific information from the document. Requirements: 1. Implement a class `MyContentHandler` that inherits from `xml.sax.handler.ContentHandler`. This class should handle the following events: - `startElement(name, attrs)`: This method should be called at the start of an XML element. It should print the element name and its attributes. - `endElement(name)`: This method should be called at the end of an XML element. It should print the element name. - `characters(content)`: This method should be called to process character data within an element. It should print the content. 2. Implement a function `parse_xml(file_path)` that takes the path to an XML file as input. This function should: - Create an instance of the `MyContentHandler` class. - Create an `XMLReader` parser using `xml.sax.make_parser()`. - Set the content handler of the parser to the `MyContentHandler` instance. - Parse the XML file using the `parse` method of the `XMLReader`. Input: - `file_path`: A string representing the path to the XML file to be parsed. Output: The output should be printed to the console, displaying the element names and attributes as well as the character content of the elements. Example: Assume you have an XML file `example.xml` with the following content: ```xml <root> <child attr=\\"value\\">Text content</child> </root> ``` When calling `parse_xml(\'example.xml\')`, the output should be: ``` Start element: root Start element: child, attributes: {\'attr\': \'value\'} Characters: Text content End element: child End element: root ``` Constraints: - You can assume that the XML file is well-formed. - The attributes dictionary should be printed in the format shown in the example. Use the provided `xml.sax.xmlreader` module documentation to implement the solution.","solution":"import xml.sax class MyContentHandler(xml.sax.ContentHandler): def startElement(self, name, attrs): attrs_dict = {k: attrs.getValue(k) for k in attrs.getNames()} if attrs_dict: print(f\\"Start element: {name}, attributes: {attrs_dict}\\") else: print(f\\"Start element: {name}\\") def endElement(self, name): print(f\\"End element: {name}\\") def characters(self, content): if content.strip(): # Avoid empty strings or whitespaces-only content print(f\\"Characters: {content.strip()}\\") def parse_xml(file_path): handler = MyContentHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) parser.parse(file_path)"},{"question":"# Advanced Python Data Model Assessment **Objective:** Implement a customized container class `FancyTable` that behaves like an immutable sequence and supports efficient item retrieval, detailed representation, and handling of complex numbers. # Requirements: 1. **Class Definition:** - Define a class `FancyTable` that takes a sequence of numeric values (integers or floating-point numbers) during initialization. 2. **Special Methods:** - Implement `__getitem__` and `__len__` to make `FancyTable` objects subscriptable and usable with the `len()` function. - Implement `__repr__` and `__str__` to provide both an official and an informal string representation of `FancyTable` objects. 3. **Handling Complex Numbers:** - Implement a method `add_complex(self, complex_number: complex)` that returns a new `FancyTable` where each element is added to the provided complex number. - Ensure immutability by returning a new `FancyTable` instance instead of modifying the current one. 4. **Sequence Behavior:** - `FancyTable` should support iteration and contain checks using `for` loops and the `in` keyword. # Input and Output Formats: - **Input:** - Initialization: A sequence of numeric values (integers or floating-point numbers). - Method `add_complex`: A complex number. - **Output:** - For `__getitem__(self, index)`: An integer or float. - For `__len__(self)`: Integer representing the length. - For `__repr__(self)`: A string in the form `FancyTable([...])`, where `[...]` is the list of values. - For `__str__(self)`: A more user-friendly string representation of the values. - For `add_complex(self, complex_number: complex)`: A new `FancyTable` instance. # Constraints: - The sequence provided during initialization must contain only integers or floating-point numbers. - The complex number provided to `add_complex` must be of Python\'s built-in `complex` type. # Example Usage: ```python # Define the FancyTable class as per requirements. values = [1, 2.5, 3] table = FancyTable(values) # Adding representation print(repr(table)) # Output: FancyTable([1, 2.5, 3]) print(str(table)) # Output: [1, 2.5, 3] # Item retrieval and length print(table[1]) # Output: 2.5 print(len(table)) # Output: 3 # Adding complex number complex_result = table.add_complex(2 + 3j) print(complex_result) # Output: FancyTable([(1+3j), (2.5+3j), (3+3j)]) # Iteration and membership test for value in table: print(value) print(2.5 in table) # Output: True print(4 in table) # Output: False ``` Implement the `FancyTable` class according to the above specifications.","solution":"class FancyTable: def __init__(self, values): self._values = tuple(values) def __getitem__(self, index): return self._values[index] def __len__(self): return len(self._values) def __repr__(self): return f\'FancyTable({list(self._values)})\' def __str__(self): return str(list(self._values)) def add_complex(self, complex_number): new_values = [value + complex_number for value in self._values] return FancyTable(new_values) def __iter__(self): return iter(self._values) def __contains__(self, item): return item in self._values"},{"question":"# Sparse Data Manipulation with Pandas **Objective**: The purpose of this task is to test your understanding of Pandas sparse data structures and their operations. **Problem Statement**: You are given a dense numpy array with random values and some NaNs. You need to create a sparse pandas Series from this array, perform certain calculations, and convert it back to a dense format. **Task**: 1. Create a numpy array of size 20, filled with random numbers. Introduce NaNs at index positions from 6 to 14. 2. Convert this numpy array to a pandas SparseArray and then to a pandas Series. 3. Compute the absolute values of the sparse array using numpy\'s absolute function. 4. Convert the resultant sparse series back to a dense format and return it. **Constraints**: - The input array should be of size 20. - Insert NaNs at index positions 6 to 14 inclusively. **Function Signature**: ```python import numpy as np import pandas as pd def sparse_data_manipulation(): # Step 1: Create a numpy array of size 20 with random values: np.random.seed(0) # For reproducibility arr = np.random.randn(20) # Introduce NaNs: arr[6:15] = np.nan # Step 2: Convert this array first to a pandas SparseArray and then to a pandas Series: sparr = pd.arrays.SparseArray(arr) s = pd.Series(sparr) # Step 3: Compute the absolute values of the sparse array: abs_s = np.abs(s.sparse.to_dense()) # Or np.abs(s.sparse.to_dense()) # Step 4: Convert the resultant sparse series back to a dense format and return it: return abs_s # Example Usage: result = sparse_data_manipulation() print(result) ``` **Expected Output**: ```shell 0 1.764052 1 0.400157 2 0.978738 3 2.240893 4 1.867558 5 0.977278 6 NaN 7 NaN 8 NaN 9 NaN 10 NaN 11 NaN 12 NaN 13 NaN 14 NaN 15 0.950088 16 0.151357 17 0.103219 18 0.410599 19 0.144044 dtype: float64 ``` **Notes**: 1. You can assume the numpy and pandas libraries are pre-installed and imported as shown in the function signature. 2. The seed value for random number generation ensures your random array is the same on every run, which is useful for testing. 3. Use appropriate commenting in the function to show clarity of each step.","solution":"import numpy as np import pandas as pd def sparse_data_manipulation(): # Step 1: Create a numpy array of size 20 with random values. np.random.seed(0) # For reproducibility arr = np.random.randn(20) # Introduce NaNs at index positions 6 to 14 inclusively. arr[6:15] = np.nan # Step 2: Convert this array first to a pandas SparseArray and then to a pandas Series. sparr = pd.arrays.SparseArray(arr) s = pd.Series(sparr) # Step 3: Compute the absolute values of the sparse array. abs_s = np.abs(s.sparse.to_dense()) # Step 4: Convert the resultant sparse series back to a dense format and return it. return abs_s # Example Usage: result = sparse_data_manipulation() print(result)"},{"question":"**Task:** Implement an asynchronous Python function that demonstrates comprehensive usage of `asyncio.Future` including setting result, handling exceptions, adding callbacks, and proper cancellation handling. **Question:** You are tasked with implementing an asynchronous function `perform_async_operations` that performs several operations using `asyncio.Future`. # Detailed Requirements: 1. **Create and Return a Future:** - The function should create an `asyncio.Future` object and return it. 2. **Set Future Result:** - Set the result of the Future to a given value after a specified delay. If a custom exception is provided, the Future should be set with the exception instead. 3. **Handle Future Cancellation:** - If the Future is cancelled before the result is set, handle the cancellation by catching the `CancelledError`. 4. **Add and Remove Callbacks:** - Add a callback function to execute when the Future is done. - Ensure that callbacks can be added and removed. 5. **Check the Future State:** - You should be able to check if the Future is done or cancelled appropriately. # Function Signature: ```python import asyncio async def perform_async_operations(value, delay, exception=None): Perform asynchronous operations using asyncio.Future. Parameters: - value: The result value to be set to the Future. - delay: The delay in seconds before setting the result. - exception: Optional custom exception to set to the Future instead of a result. Returns: - asyncio.Future object # Your code here ``` # Example Usage: ```python async def main(): loop = asyncio.get_running_loop() async def done_callback(fut): if fut.exception(): print(f\\"Future got exception: {fut.exception()}\\") else: print(f\\"Future result: {fut.result()}\\") fut = await perform_async_operations(\\"Completed\\", 2) fut.add_done_callback(done_callback) # Simulate cancellation after 1 second await asyncio.sleep(1) fut.cancel() # Alternatively, setting result after the delay # fut = await perform_async_operations(\\"Completed\\", 2) # fut.add_done_callback(done_callback) asyncio.run(main()) ``` # Notes: - You must use `asyncio.Future` and handle all required functionalities as outlined. - Ensure the function waits for the specified delay before setting the result or exception unless it\'s cancelled. - Properly handle exceptions, both in setting the Future\'s state and in the callback. - Use `asyncio.get_running_loop()` to create the Future object and manage the event loop. **Constraints:** - Python 3.10 or higher. - Follow best practices for working with asyncio and Futures.","solution":"import asyncio async def perform_async_operations(value, delay, exception=None): Perform asynchronous operations using asyncio.Future. Parameters: - value: The result value to be set to the Future. - delay: The delay in seconds before setting the result. - exception: Optional custom exception to set to the Future instead of a result. Returns: - asyncio.Future object loop = asyncio.get_running_loop() future = loop.create_future() # Define a callback function to handle the result setting async def set_future_result(): try: await asyncio.sleep(delay) if exception: future.set_exception(exception) else: future.set_result(value) except asyncio.CancelledError: future.cancel() raise # Run the result setting function asynchronously asyncio.create_task(set_future_result()) return future"},{"question":"# Question: Custom Cookie Policy Implementation You are required to create a program that interacts with a web server, managing HTTP cookies using a custom cookie policy. The custom policy should block cookies from a specified list of domains and allow cookies only from a specific set of secure protocols. Additionally, retrieved cookies should be saved to a file in a format compatible with Mozilla browsers. Requirements: 1. **Custom Cookie Policy**: - Inherit from `DefaultCookiePolicy`. - Override the `set_ok` method to block cookies from a predefined list of domains (`blocked_domains`). - Override the `return_ok` method to allow cookies only over secure protocols (`https` and `wss`). 2. **Cookie Management**: - Load any existing cookies from a file named `cookies.txt` in the current working directory, using the `MozillaCookieJar` class. - Track new cookies set during the session and save them back to `cookies.txt` upon completion. 3. **HTTP Request**: - Use `urllib.request` to make an HTTP request to `http://example.com`. - Ensure the request includes any relevant cookies and handle any cookies set by the response according to your custom policy. Input: - A list of blocked domains. Output: - The content of the HTTP response. - A `cookies.txt` file with the managed cookies in Mozilla format. Example: ```python blocked_domains = [\\"example.com\\", \\".example.com\\"] class CustomCookiePolicy(DefaultCookiePolicy): def set_ok(self, cookie, request): # Implement blocking logic pass def return_ok(self, cookie, request): # Implement secure protocol check pass def manage_cookies(blocked_domains): pass if __name__ == \\"__main__\\": blocked_domains = [\\"example.com\\", \\".example.com\\"] manage_cookies(blocked_domains) ``` Constraints: - The solution should be compatible with Python 3.8 and later. - Ensure proper error handling, especially for file operations and HTTP requests.","solution":"import os import urllib.request import http.cookiejar from http.cookiejar import DefaultCookiePolicy, MozillaCookieJar class CustomCookiePolicy(DefaultCookiePolicy): def __init__(self, blocked_domains): super().__init__() self.blocked_domains = blocked_domains def set_ok(self, cookie, request): for domain in self.blocked_domains: if urllib.request.url2pathname(cookie.domain).endswith(domain): return False return True def return_ok(self, cookie, request): return request.type in [\\"https\\", \\"wss\\"] def manage_cookies(blocked_domains): # Initialize a MozillaCookieJar object cj = MozillaCookieJar(\\"cookies.txt\\") # Load existing cookies if the file exists if os.path.exists(\\"cookies.txt\\"): cj.load(ignore_discard=True, ignore_expires=True) # Set up the custom cookie policy cookie_policy = CustomCookiePolicy(blocked_domains) cj.set_policy(cookie_policy) # Create an opener opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj)) try: # Make the HTTP request with opener.open(\\"http://example.com\\") as response: html_content = response.read().decode() print(html_content) except Exception as e: print(f\\"An error occurred: {e}\\") finally: # Save cookies cj.save(ignore_discard=True, ignore_expires=True) return html_content if __name__ == \\"__main__\\": blocked_domains = [\\"example.com\\", \\".example.com\\"] manage_cookies(blocked_domains)"},{"question":"# Question You are given a dataset of daily temperature readings in a city over a year. Write a Python function to create and display the following plots using Seaborn: 1. A line plot of daily temperatures with the `darkgrid` theme. 2. The same line plot but in the `whitegrid` theme. 3. A violin plot of monthly temperatures (aggregate the daily temperatures into months) in the `dark` theme, but with the top and right spines removed. 4. A box plot of monthly temperatures in the `ticks` theme, temporarily styled within the context manager to apply a custom style where the background is light grey. Note: - The input is a DataFrame `df` with columns `date` (datetime) and `temperature` (float). - You should aggregate temperatures into months by taking the average temperature for each month. - Use appropriate titles and labels for each plot for clarity. # Constraints - The function should demonstrate switching between and customizing seaborn styles effectively. - The code should include using context managers to temporarily set styles. Function Signature ```python def create_temperature_plots(df: pd.DataFrame) -> None: pass ``` # Example ```python import pandas as pd # Example data data = { \\"date\\": pd.date_range(start=\\"2022-01-01\\", periods=365, freq=\'D\'), \\"temperature\\": np.random.normal(loc=15, scale=10, size=365) } df = pd.DataFrame(data) # Generating the plots create_temperature_plots(df) ``` # Expected Output - Four different plots displayed inline or in a window, each adhering to the specific theme and style requirements mentioned.","solution":"import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt def create_temperature_plots(df: pd.DataFrame) -> None: # Convert dates to datetime df[\'date\'] = pd.to_datetime(df[\'date\']) # Daily temperature line plot with `darkgrid` theme sns.set_theme(style=\\"darkgrid\\") plt.figure() sns.lineplot(x=\'date\', y=\'temperature\', data=df) plt.title(\'Daily Temperatures - Darkgrid Theme\') plt.xlabel(\'Date\') plt.ylabel(\'Temperature\') plt.show() # Daily temperature line plot with `whitegrid` theme sns.set_theme(style=\\"whitegrid\\") plt.figure() sns.lineplot(x=\'date\', y=\'temperature\', data=df) plt.title(\'Daily Temperatures - Whitegrid Theme\') plt.xlabel(\'Date\') plt.ylabel(\'Temperature\') plt.show() # Aggregate temperatures into months df[\'month\'] = df[\'date\'].dt.to_period(\'M\') monthly_avg_temp = df.groupby(df[\'month\']).agg({\'temperature\':\'mean\'}).reset_index() # Violin plot of monthly temperatures with `dark` theme, top and right spines removed sns.set_theme(style=\\"dark\\") plt.figure() sns.violinplot(x=df[\'month\'].astype(str), y=df[\'temperature\']) sns.despine(top=True, right=True) plt.title(\'Monthly Temperatures - Dark Theme\') plt.xlabel(\'Month\') plt.ylabel(\'Temperature\') plt.show() # Box plot of monthly temperatures with `ticks` theme, applying custom context with sns.axes_style(\\"ticks\\", {\\"axes.facecolor\\": \\"lightgrey\\"}): plt.figure() sns.boxplot(x=df[\'month\'].astype(str), y=df[\'temperature\']) plt.title(\'Monthly Temperatures - Ticks Theme with Custom Style\') plt.xlabel(\'Month\') plt.ylabel(\'Temperature\') plt.show()"},{"question":"**Coding Assessment Question** You are provided with the seaborn library documentation, particularly focusing on the usage of `swarmplot` and `catplot`. Using this information, you need to demonstrate your understanding by working with a dataset and creating specific visualizations. **Objective:** Create visualizations of the Titanic dataset to explore the relationships between different variables using seaborn\'s `swarmplot` and `catplot` functionalities. **Instructions:** 1. **Load the Dataset:** Load the Titanic dataset using seaborn. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") ``` 2. **Swarm Plot:** Create a swarm plot to show the distribution of fare prices (`fare`) across different classes (`class`). Use different colors for passengers who survived (`survived`) and those who didn\'t. - `x`: Class (`class`) - `y`: Fare (`fare`) - `hue`: Survival status (`survived`) - Customize the plot to have the points slightly smaller to avoid overlap. 3. **Categorical Plot:** Create a faceted plot using seaborn’s `catplot` where: - `x`: Embarked location (`embarked`) - `y`: Age (`age`) - `hue`: Gender (`sex`) - Facet the plot by passenger class (`class`) - Add customization to set the aspect ratio of each facet to 0.7. 4. **Additional Customizations:** - Use the `deep` palette to customize colors in both plots. - In the swarm plot, use the `dodge` option to separate `hue` levels for each category on the horizontal axis. **Constraints:** - Ensure that all code is properly commented. - Make sure the plots are properly labeled, and axes are named. - Handle any missing data appropriately. **Example Output:** Your final visualization code should output a figure similar to the ones provided in the documentation, but respective to the Titanic dataset and the specific instructions. **Submission:** Submit your Python code and the resulting visualizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create a swarm plot to show the distribution of fare prices across different classes, # using different colors for passengers who survived and those who didn\'t. plt.figure(figsize=(10, 6)) swarm_plot = sns.swarmplot( x=\\"class\\", y=\\"fare\\", hue=\\"survived\\", data=titanic, palette=\\"deep\\", dodge=True, size=3 # slightly smaller size to avoid overlap ) swarm_plot.set_title(\\"Distribution of Fare by Class and Survival Status\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Fare\\") plt.legend(title=\\"Survived\\") plt.show() # Create a faceted plot using seaborn’s catplot cat_plot = sns.catplot( x=\\"embarked\\", y=\\"age\\", hue=\\"sex\\", col=\\"class\\", data=titanic, kind=\\"swarm\\", palette=\\"deep\\", aspect=0.7 ) cat_plot.fig.subplots_adjust(top=0.9) # Adjust the top to fit the title cat_plot.fig.suptitle(\\"Age Distribution by Embarked Location, Gender, and Class\\") plt.show()"},{"question":"You are provided with a dataset containing information on health expenditures of various countries over several years. The dataset is loaded using `seaborn.load_dataset`. Your task is to generate a plot using seaborn that demonstrates your understanding of normalization and labeling using the `seaborn.objects` interface. **Objective**: Write a function `generate_health_exp_plot()` that: 1. Loads the \\"healthexp\\" dataset using `seaborn.load_dataset`. 2. Creates a line plot where the x-axis represents the \\"Year\\" and the y-axis represents \\"Spending_USD\\". 3. Colors the lines by \\"Country\\". 4. Normalizes the \\"Spending_USD\\" data such that each country\'s spending is shown as a percentage change from the year 1970. 5. Labels the y-axis appropriately to reflect the percentage change from the 1970 baseline. **Function Definition**: ```python def generate_health_exp_plot(): # Your code here ``` # Constraints: 1. Use the seaborn package and specifically the `seaborn.objects` interface. 2. Follow the normalization as described: percentage change from the year 1970. 3. The function should display the plot. No file output is required. # Example Output: The function will generate and display a line plot with the specified normalization and labeling. ```python generate_health_exp_plot() ``` This function call should produce a plot where: - The x-axis is labeled \\"Year\\". - The y-axis is labeled \\"Percent change in spending from 1970 baseline\\". - Different countries\' spending is represented by different colored lines. - The y-values reflect the percentage change in spending relative to the year 1970 for each country. **Note**: Ensure that you have the seaborn package installed to execute this function.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def generate_health_exp_plot(): # Load the dataset df = sns.load_dataset(\\"healthexp\\") # Pivot the table to make country columns for better manipulation df_pivot = df.pivot(index=\'Year\', columns=\'Country\', values=\'Spending_USD\') # Normalize the dataset normalized_df = df_pivot.div(df_pivot.loc[1970]).mul(100).sub(100) # Reset index and convert to long format for seaborn normalized_df = normalized_df.reset_index().melt(id_vars=\'Year\', var_name=\'Country\', value_name=\'Percent_Change_Spending\') # Create the plot using seaborn\'s objects interface p = sns.lineplot(data=normalized_df, x=\'Year\', y=\'Percent_Change_Spending\', hue=\'Country\') # Label the axes p.set(xlabel=\'Year\', ylabel=\'Percent change in spending from 1970 baseline\') # Display the plot plt.show()"},{"question":"<|Analysis Begin|> The provided documentation covers several aspects of Seaborn\'s object-oriented interface for creating different types of plots. It highlights functionalities such as: 1. Loading datasets and basic plotting. 2. Drawing discrete bars and histograms. 3. Customizing the orientation of bars. 4. Mapping additional variables and resolving overlaps using `Dodge`. 5. Customizing properties (like color, alpha, edgestyle, and edgewidth). 6. Combining different plot elements to create complex visualizations, such as adding error bars using `Range` and `Est`. It provides examples of working with the `flights` and `penguins` datasets and demonstrates various capabilities of the `so.Plot` class for making bar plots and histograms. Important components like `so.Bar`, `so.Hist`, `so.Dodge`, `so.Range`, and `so.Est` are shown through code examples. Based on this information, I can design a question that assesses the student\'s ability to: 1. Load datasets. 2. Create various types of plots with Seaborn. 3. Customize plots with different properties. 4. Handle advanced features, such as mapping additional variables and resolving overlaps. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** You are required to demonstrate your understanding of Seaborn\'s plotting capabilities by creating specific visualizations using the `seaborn.objects` interface. Specifically, you will load a dataset, create several types of bar plots, and customize them using various Seaborn functionalities. **Dataset:** Use the `penguins` dataset available in Seaborn. **Tasks:** 1. **Load the Dataset:** Load the `penguins` dataset using Seaborn\'s `load_dataset` function. 2. **Basic Bar Plot:** Create a basic bar plot showing the count of each species in the dataset. Use the `so.Plot` and `so.Bar` classes. 3. **Grouped Bar Plot:** Create a grouped bar plot showing the count of each species categorized by the `sex` of the penguins. Use the `so.Dodge` class to resolve the overlapping bars. 4. **Customized Bar Plot:** Create a bar plot showing the average body mass (`body_mass_g`) for each species, categorized by `sex`. Use the `so.Bar`, `so.Agg`, and `so.Dodge` classes to aggregate the data and add error bars representing the standard deviation. Customize the bars by setting different colors and alpha levels for `sex`. 5. **Properties Mapping:** Create an advanced bar plot showing the relationship between species and `flipper_length_mm`, categorized by `sex`, with customized properties: - Set different `edgewidth` for bars. - Apply different `edgestyles` based on `sex`. - Use different `alpha` levels based on `sex`. The plots should clearly indicate the required visual relationships and be appropriately labeled. **Expected Input:** None. The functions and operations should be performed on the provided dataset. **Expected Output:** - The plots should be displayed using matplotlib\'s visualization functions within Seaborn. **Constraints:** - Make sure the dataset is correctly loaded and used for the visualizations. - Handle all potential exceptions that might occur during plotting, such as issues due to missing data. - Properly comment your code to explain your logic. **Solution Template:** ```python import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Task 1: Basic Bar Plot def basic_bar_plot(): return so.Plot(penguins, x=\\"species\\").add(so.Bar(), so.Hist()) # Task 2: Grouped Bar Plot def grouped_bar_plot(): return so.Plot(penguins, x=\\"species\\", color=\\"sex\\").add(so.Bar(), so.Hist(), so.Dodge()) # Task 3: Customized Bar Plot def customized_bar_plot(): return ( so.Plot(penguins, \\"body_mass_g\\", \\"species\\", color=\\"sex\\") .add(so.Bar(alpha=.5), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) # Task 4: Properties Mapping def properties_mapping_plot(): return ( so.Plot(penguins, x=\\"species\\", y=\\"flipper_length_mm\\", color=\\"sex\\") .add(so.Bar(edgewidth=2), so.Hist(), so.Dodge(\\"fill\\")) ) # Display the plots basic_bar_plot().show() grouped_bar_plot().show() customized_bar_plot().show() properties_mapping_plot().show() ``` Make sure your code runs without errors and generates the expected visualizations.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Task 1: Basic Bar Plot def basic_bar_plot(): return so.Plot(penguins, x=\\"species\\").add(so.Bar(), so.Count()) # Task 2: Grouped Bar Plot def grouped_bar_plot(): return so.Plot(penguins, x=\\"species\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()) # Task 3: Customized Bar Plot def customized_bar_plot(): return ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") .add(so.Bar(alpha=.5), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) # Task 4: Properties Mapping def properties_mapping_plot(): return ( so.Plot(penguins, x=\\"species\\", y=\\"flipper_length_mm\\", color=\\"sex\\") .add(so.Bar(edgewidth=2), so.Hist(), so.Dodge()) ) # Display the plots if __name__ == \\"__main__\\": basic_bar_plot().show() grouped_bar_plot().show() customized_bar_plot().show() properties_mapping_plot().show()"},{"question":"# Question: Multi-Threaded Task Manager using Python `queue` Module You are tasked to create a multi-threaded task manager simulation using the `queue` module. The task manager will handle tasks with different priorities and ensure they are processed correctly by worker threads. You will implement three main functions and utilize the `PriorityQueue` class to ensure tasks with higher priority are processed first. # Function Specifications 1. **enqueue_tasks(task_data: List[Tuple[int, Any]], task_queue: queue.PriorityQueue) -> None** - **Input** - `task_data`: A list of tuples, where each tuple contains an integer representing the task\'s priority and any associated data (e.g., `(priority, data)`). - Example: `[(1, \\"Task 1\\"), (3, \\"Task 3\\"), (2, \\"Task 2\\")]` - `task_queue`: An instance of `queue.PriorityQueue`. - **Output** - None - **Behavior** - This function should enqueue all tasks from `task_data` into `task_queue`. 2. **worker(task_queue: queue.PriorityQueue) -> None** - **Input** - `task_queue`: An instance of `queue.PriorityQueue`. - **Output** - None - **Behavior** - This function should continuously retrieve tasks from `task_queue`, simulate processing by printing a message, and indicate task completion using `task_done`. 3. **start_task_manager(task_data: List[Tuple[int, Any]], num_workers: int) -> None** - **Input** - `task_data`: A list of tuples, where each tuple contains an integer representing the task\'s priority and any associated data. - `num_workers`: Integer specifying the number of worker threads to start. - **Output** - None - **Behavior** - This function should initialize a `queue.PriorityQueue`, enqueue all provided tasks, create the specified number of worker threads, and ensure all tasks are processed before the function terminates. # Constraints - You should use the `queue.PriorityQueue` class for task management. - Worker threads should operate as daemons. - Tasks should be processed based on their priority (lower priority number = higher priority). - Each worker should print: \\"Processing {data}\\" when it starts a task and \\"Finished {data}\\" when it completes a task. # Example ```python import threading import queue from typing import List, Tuple, Any def enqueue_tasks(task_data: List[Tuple[int, Any]], task_queue: queue.PriorityQueue) -> None: for task in task_data: task_queue.put(task) def worker(task_queue: queue.PriorityQueue) -> None: while True: priority, data = task_queue.get() print(f\'Processing {data}\') print(f\'Finished {data}\') task_queue.task_done() def start_task_manager(task_data: List[Tuple[int, Any]], num_workers: int) -> None: task_queue = queue.PriorityQueue() enqueue_tasks(task_data, task_queue) for _ in range(num_workers): threading.Thread(target=worker, args=(task_queue,), daemon=True).start() task_queue.join() print(\'All tasks have been completed\') # Example usage: task_data = [(3, \\"Low priority task\\"), (1, \\"High priority task\\"), (2, \\"Medium priority task\\")] num_workers = 3 start_task_manager(task_data, num_workers) ``` **Note:** Ensure your implementation handles thread safety properly and uses the provided methods by the `queue.PriorityQueue` class with the correct behavior as outlined above.","solution":"import threading import queue from typing import List, Tuple, Any def enqueue_tasks(task_data: List[Tuple[int, Any]], task_queue: queue.PriorityQueue) -> None: Enqueues all tasks from task_data into task_queue. for task in task_data: task_queue.put(task) def worker(task_queue: queue.PriorityQueue) -> None: Worker thread function that processes tasks from task_queue. while True: priority, data = task_queue.get() print(f\'Processing {data}\') # Simulate some processing work print(f\'Finished {data}\') task_queue.task_done() def start_task_manager(task_data: List[Tuple[int, Any]], num_workers: int) -> None: Initializes the task queue, enqueues tasks, starts worker threads, and processes all tasks. task_queue = queue.PriorityQueue() enqueue_tasks(task_data, task_queue) for _ in range(num_workers): threading.Thread(target=worker, args=(task_queue,), daemon=True).start() task_queue.join() print(\'All tasks have been completed\')"},{"question":"# PyTorch Coding Assessment: Conditional Control Flow with `torch.cond` Objective: Implement a class `ValueBasedCond` that applies different transformations to the input tensor based on its sum. Use the `torch.cond` function to achieve this. Specifications: 1. **Class Definition**: - Define the class `ValueBasedCond` inheriting from `torch.nn.Module`. 2. **Methods**: - **`__init__(self)`:** Initialize the class. - Define two functions: `multiply_fn` and `divide_fn`. - `multiply_fn(x: torch.Tensor)`: Multiplies the input tensor by 2. - `divide_fn(x: torch.Tensor)`: Divides the input tensor by 2. - **`forward(self, x: torch.Tensor) -> torch.Tensor`:** Apply `torch.cond` to determine which function to use based on whether the sum of `x` is greater than 10. - Use `torch.cond` with the predicate `x.sum() > 10.0`, `multiply_fn` as the true function, and `divide_fn` as the false function. Input: - A tensor `x` of shape `(N, )` (1-dimensional tensor). Output: - A tensor of the same shape `(N, )` after applying the specified transformation. Constraints: - Do not use other conditional statements such as `if`, `else`, or ternary operators inside the `forward` method. - Assume that the input tensor contains only real numbers. Example: ```python import torch from torch.nn import Module class ValueBasedCond(Module): def __init__(self): super().__init__() def multiply_fn(x: torch.Tensor): return x * 2 def divide_fn(x: torch.Tensor): return x / 2 self.multiply_fn = multiply_fn self.divide_fn = divide_fn def forward(self, x: torch.Tensor) -> torch.Tensor: return torch.cond(x.sum() > 10.0, self.multiply_fn, self.divide_fn, (x,)) # Testing the implementation input_tensor = torch.tensor([3.0, 4.0, 5.0]) model = ValueBasedCond() output_tensor = model(input_tensor) print(\\"Input:\\", input_tensor) print(\\"Output:\\", output_tensor) ``` Expected Output: ``` Input: tensor([3.0, 4.0, 5.0]) Output: tensor([6.0, 8.0, 10.0]) ``` **Note**: For other input values, the function should correctly either multiply or divide the elements based on the sum of the tensor. Submission: Provide the `ValueBasedCond` class implementation and a brief explanation of how `torch.cond` works in your solution.","solution":"import torch import torch.nn as nn class ValueBasedCond(nn.Module): def __init__(self): super(ValueBasedCond, self).__init__() def multiply_fn(x: torch.Tensor): return x * 2 def divide_fn(x: torch.Tensor): return x / 2 self.multiply_fn = multiply_fn self.divide_fn = divide_fn def forward(self, x: torch.Tensor) -> torch.Tensor: # For torch.cond, we use torch.where as it serves similar purpose # torch.where(condition, x_if_true, x_if_false) condition = x.sum() > 10.0 return torch.where(condition, self.multiply_fn(x), self.divide_fn(x)) # Brief explanation: # In this implementation, we are using `torch.where` to achieve a conditional similar to what was expected of `torch.cond`. # `torch.where` takes a condition and two tensors, returning elements from the first tensor if the condition is true, # and elements from the second if the condition is false. This way, we can conditionally apply the functions # \'multiply_fn\' or \'divide_fn\' to the tensor \'x\' based on the sum of its elements."},{"question":"**Objective**: Demonstrate your understanding of the `pipes` module in Python by implementing a function that processes a text file with a series of shell commands. **Task**: Implement a function called `transform_file` that takes three parameters: 1. `input_file` (str) - The path to the input text file. 2. `output_file` (str) - The path where the transformed content should be saved. 3. `commands` (list of tuples) - A list of tuples, where each tuple contains a shell command (str) to be applied and its type (str). The type is specified with two letters as defined in the `append` method of `pipes.Template`. The function should: 1. Use the `pipes.Template` class to construct a pipeline. 2. Append each command from the `commands` list to the pipeline in the specified order. 3. Open the specified `input_file`, pass its content through the pipeline, and write the transformed content to `output_file`. **Constraints**: - The function should handle any valid shell commands specified in the `commands` list. - Ensure the pipeline processes the entire content of `input_file`. **Input and Output Formats**: - Input: `transform_file(\'input.txt\', \'output.txt\', [(\'tr a-z A-Z\', \'--\'), (\'sort\', \'--\')])` - Output: The file `output.txt` should contain the transformed and sorted content of `input.txt`. **Example**: ```python import os import pipes def transform_file(input_file, output_file, commands): # Initialize the pipeline template t = pipes.Template() # Append commands to the pipeline for cmd, kind in commands: t.append(cmd, kind) # Open the input file, process it through the pipeline, and write to the output file with t.open(output_file, \'w\') as f_out: with open(input_file, \'r\') as f_in: f_out.write(f_in.read()) # Example usage transform_file(\'input.txt\', \'output.txt\', [(\'tr a-z A-Z\', \'--\'), (\'sort\', \'--\')]) # Validate the output with open(\'output.txt\', \'r\') as f: print(f.read()) # Should print the transformed and sorted content of \'input.txt\' ``` Note: This assessment requires a POSIX-compatible shell environment to execute properly, as it relies on shell commands.","solution":"import pipes def transform_file(input_file, output_file, commands): Transforms the content of input_file by applying the given shell commands in sequence and writes the transformed content to output_file. Args: input_file (str): The path to the input text file. output_file (str): The path where the transformed content should be saved. commands (list of tuples): A list of tuples, where each tuple contains a shell command (str) and its type (str) to be applied in order. # Initialize the pipeline template t = pipes.Template() # Append commands to the pipeline for cmd, kind in commands: t.append(cmd, kind) # Open the input file, process it through the pipeline, and write to the output file with t.open(output_file, \'w\') as f_out: with open(input_file, \'r\') as f_in: f_out.write(f_in.read())"},{"question":"Objective: Demonstrate your understanding of seaborn color palettes and plot customization using the `sns.mpl_palette` function. Task: 1. Write a function `generate_custom_palette` that generates and returns a matplotlib colormap based on the parameters given. 2. Create a scatter plot using seaborn, applying the custom palette generated from step 1. 3. Ensure that the scatter plot uses distinct class colors based on the target variable from the Iris dataset. Function Signature: ```python def generate_custom_palette(cmap_name: str, n_colors: int, as_cmap: bool) -> any: pass ``` Inputs: 1. `cmap_name` (str): Name of the colormap. 2. `n_colors` (int): Number of colors to sample from the colormap. 3. `as_cmap` (bool): If True, return the colormap as a continuous colormap, otherwise return a list of colors. Outputs: - Depending on `as_cmap`, the function should return: - A continuous colormap when `as_cmap` is True. - A list of colors (length `n_colors`) when `as_cmap` is False. Constraints: - Ensure that `n_colors` is greater than 0 and a positive integer. - The `cmap_name` should be a valid matplotlib colormap name. Example: ```python # Example usage: palette = generate_custom_palette(\\"viridis\\", 5, False) print(palette) # Expected output: A list of 5 colors sampled from \\"viridis\\" colormap. # Example scatter plot: import seaborn as sns from sklearn.datasets import load_iris import matplotlib.pyplot as plt # Load Iris dataset iris = load_iris() iris_df = sns.load_dataset(\\"iris\\") # Generate a palette palette = generate_custom_palette(\\"Set2\\", 10, False) # Create scatter plot sns.scatterplot(x=\\"sepal_length\\", y=\\"sepal_width\\", hue=\\"species\\", palette=palette, data=iris_df) plt.show() ``` In this task, using `generate_custom_palette`, create different color palettes and apply them on the Iris dataset to create a scatter plot. Comment on how changes in the palette have affected the plot\'s aesthetics and readability.","solution":"import seaborn as sns import matplotlib.pyplot as plt from sklearn.datasets import load_iris import matplotlib as mpl def generate_custom_palette(cmap_name: str, n_colors: int, as_cmap: bool) -> any: Generates and returns a matplotlib colormap or a list of colors based on the given parameters. Parameters: - cmap_name (str): Name of the colormap. - n_colors (int): Number of colors to sample from the colormap. - as_cmap (bool): If True, return the colormap as a continuous colormap, otherwise return a list of colors. Returns: - A continuous colormap when as_cmap is True. - A list of colors (length n_colors) when as_cmap is False. assert n_colors > 0, \\"n_colors must be a positive integer\\" if as_cmap: return mpl.cm.get_cmap(cmap_name, n_colors) else: return sns.color_palette(cmap_name, n_colors) # Create scatter plot using seaborn and applying the custom palette def plot_with_custom_palette(cmap_name: str, n_colors: int, as_cmap: bool): # Load the Iris dataset iris = sns.load_dataset(\\"iris\\") # Generate the custom palette palette = generate_custom_palette(cmap_name, n_colors, as_cmap) # Create scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot(x=\\"sepal_length\\", y=\\"sepal_width\\", hue=\\"species\\", palette=palette, data=iris) plt.title(f\\"Scatter Plot using {cmap_name} colormap with {n_colors} colors\\") plt.show() # Example usage of the function plot_with_custom_palette(\\"viridis\\", 5, False)"},{"question":"You are tasked with creating a custom error-handling and logging system using Python\'s `traceback` module. The goal is to capture exceptions, format the stack trace, and log the formatted trace to a file for later analysis. Requirements 1. **Function Implementation**: - Implement a function `log_exception(file_path: str, limit: int = None) -> None` which: - Captures the current exception using the `sys.exc_info()` method. - Extracts and formats the stack trace and exception details. - Appends the formatted details to a log file specified by `file_path`. 2. **Exception Logging Decorator**: - Implement a decorator `exception_logger(file_path: str, limit: int = None)` which: - Wraps a function to catch any exceptions that occur during its execution. - Calls `log_exception` to log the details if an exception is caught. - Re-raises the exception after logging it. Function Signatures ```python import traceback import sys def log_exception(file_path: str, limit: int = None) -> None: pass def exception_logger(file_path: str, limit: int = None): pass ``` Constraints - You are not allowed to use any third-party libraries. - The `log_exception` function should ensure that the log file is created if it does not exist and that it appends to the file rather than overwriting it. - The `exception_logger` decorator should preserve the original function’s signature and behavior, except for the added exception logging. Example Usage ```python @exception_logger(\'error_log.txt\', limit=3) def risky_function(x): return 1 / x try: risky_function(0) except ZeroDivisionError: print(\\"Caught an exception\\") # After execution, \'error_log.txt\' should have the formatted stack trace and exception details of the ZeroDivisionError. ``` Your task is to implement the `log_exception` function and the `exception_logger` decorator as described. Ensure proper handling of file operations and exception capturing techniques provided by the `traceback` and `sys` modules.","solution":"import traceback import sys def log_exception(file_path: str, limit: int = None) -> None: Logs the current exception\'s stack trace and details to the specified file. Parameters: file_path (str): The path to the log file where the exception details will be written. limit (int, optional): The maximum number of stack trace entries to include. exc_type, exc_value, exc_traceback = sys.exc_info() if exc_type is not None: with open(file_path, \'a\') as f: traceback.print_exception(exc_type, exc_value, exc_traceback, limit=limit, file=f) def exception_logger(file_path: str, limit: int = None): A decorator that wraps a function to log its exceptions using log_exception. Parameters: file_path (str): The path to the log file where the exception details will be written. limit (int, optional): The maximum number of stack trace entries to include. Returns: function: The decorated function with exception logging. def decorator(func): def wrapper(*args, **kwargs): try: return func(*args, **kwargs) except Exception as ex: log_exception(file_path, limit) raise return wrapper return decorator"},{"question":"Handling Exceptions in File Processing **Objective:** Write a Python function that processes a list of file names, reads each file, and performs specific operations. Your function should demonstrate a strong understanding of Python\'s error and exception handling as discussed in the provided documentation. **Function Signature:** ```python def process_files(file_list: list) -> dict: pass ``` **Input:** - `file_list`: A list of strings where each string is a file path. **Output:** - A dictionary where the key is the file name and the value is a tuple containing: - Number of lines in the file. - If a `ValueError` occurs while processing the lines, the error string \\"ValueError encountered\\". - If any other exception occurs, the error string \\"Error: <error message>\\". **Constraints:** 1. If a file does not exist, handle `FileNotFoundError` and continue processing the next file. 2. If a file cannot be opened due to a permission error, handle the `PermissionError` and append the error string \\"Permission denied\\" to the dictionary. 3. If a `ValueError` occurs while reading or processing the file content, return \\"ValueError encountered\\". 4. Use the `finally` clause to ensure that files are closed properly after processing. 5. Optionally use the `with` statement for predefined clean-up actions in managing file contexts. **Example:** ```python input_files = [\\"file1.txt\\", \\"file2.txt\\", \\"nonexistent.txt\\", \\"protected.txt\\"] result = process_files(input_files) print(result) # Output might look like: # { # \\"file1.txt\\": (5, None), # File with 5 lines, successfully processed. # \\"file2.txt\\": \\"ValueError encountered\\", # File that raises a ValueError during processing. # \\"nonexistent.txt\\": \\"Error: [Errno 2] No such file or directory: \'nonexistent.txt\'\\", # FileNotFoundError handled. # \\"protected.txt\\": \\"Permission denied\\" # PermissionError handled. # } ``` **Instructions:** 1. Implement the `process_files` function. 2. Ensure comprehensive exception handling as per the given constraints. 3. Ensure that each file is closed properly after processing, regardless of whether an exception occurred or not. **Tips:** - You may want to use different `except` clauses to catch and handle different exceptions specifically. - Use `finally` to make sure that even if an error occurs, files are properly closed. - Use `try...except` blocks nested within loops for processing each file separately.","solution":"def process_files(file_list: list) -> dict: result = {} for file_path in file_list: try: with open(file_path, \'r\') as file: lines = file.readlines() result[file_path] = (len(lines), None) except FileNotFoundError: result[file_path] = f\\"Error: [Errno 2] No such file or directory: \'{file_path}\'\\" except PermissionError: result[file_path] = \\"Permission denied\\" except ValueError: result[file_path] = \\"ValueError encountered\\" except Exception as e: result[file_path] = f\\"Error: {str(e)}\\" return result"},{"question":"**Question: Audio File Processor** Your task is to implement a function that processes audio data using the `aifc` module. The function should take two AIFF files as input, combine their audio data, and save the result to a new AIFF file. Specifically, the function should: 1. Read the audio data from both input files. 2. Ensure both files have the same sample width, number of channels, and frame rate. 3. Combine the audio data by concatenating the frames. 4. Save the combined audio data to a new AIFF file with the same parameters as the input files. # Function Signature ```python def process_audio_files(input_file1: str, input_file2: str, output_file: str) -> None: pass ``` # Inputs - `input_file1` (str): The path to the first input AIFF file. - `input_file2` (str): The path to the second input AIFF file. - `output_file` (str): The path to the output AIFF file. # Outputs - The function does not return any value. It should create a new AIFF file at the specified output path with the combined audio data. # Constraints - Both input AIFF files will have the same sample width, number of channels, and frame rate. - The new AIFF file should contain the audio data of the first file followed by the audio data of the second file. # Example Usage Assume `input_file1.aiff` and `input_file2.aiff` are two AIFF files with the same parameters. After running the function, the `combined_output.aiff` will contain the combined audio data of both files. ```python process_audio_files(\'input_file1.aiff\', \'input_file2.aiff\', \'combined_output.aiff\') ``` # Implementation Tips 1. Use the `aifc.open()` function to open the input files for reading and the output file for writing. 2. Use the appropriate methods to retrieve the audio parameters from the input files and set them for the output file. 3. Ensure to handle the opening and closing of files appropriately to avoid any file corruption.","solution":"import aifc def process_audio_files(input_file1: str, input_file2: str, output_file: str) -> None: with aifc.open(input_file1, \'rb\') as file1, aifc.open(input_file2, \'rb\') as file2: # Get parameters from the first file params = file1.getparams() # Read frames from both files frames1 = file1.readframes(file1.getnframes()) frames2 = file2.readframes(file2.getnframes()) combined_frames = frames1 + frames2 with aifc.open(output_file, \'wb\') as output_file: # Set the same parameters as the input files output_file.setparams(params) output_file.writeframes(combined_frames)"},{"question":"**Objective:** Implement a function to generate the `MANIFEST.in` file for a Python source distribution. The function should take the directory structure and file selection criteria as input, and output the content of `MANIFEST.in`. **Problem Statement:** You are tasked with automating the creation of the `MANIFEST.in` file for a Python source distribution. The `MANIFEST.in` file specifies which files should be included in the source distribution. Write a Python function `generate_manifest` that creates the content of the `MANIFEST.in` file based on specified criteria. The function should take the following parameters: - `include_patterns` (List[str]): A list of glob patterns to include. Example: `[\\"*.py\\", \\"docs/*.md\\"]` - `exclude_patterns` (List[str]): A list of glob patterns to exclude. Example: `[\\"build/*\\", \\"*.tmp\\"]` - `recursive_include_patterns` (List[str]): A list of patterns to recursively include from specific directories. Example: `[\\"examples/*.py\\", \\"tests/test_*.py\\"]` - `prune_patterns` (List[str]): A list of directories to prune (exclude) entirely. Example: `[\\"examples/sample*/build\\"]` The function should output a string representation of the `MANIFEST.in` content. **Input Format:** - `include_patterns`: A list of strings representing glob patterns of files to include. - `exclude_patterns`: A list of strings representing glob patterns of files to exclude. - `recursive_include_patterns`: A list of strings representing glob patterns to recursively include files from. - `prune_patterns`: A list of strings representing directories to prune (exclude entirely). **Output Format:** - A string representation of the generated `MANIFEST.in` file content. **Example:** ```python def generate_manifest(include_patterns, exclude_patterns, recursive_include_patterns, prune_patterns): # Your implementation here # Example usage: include_patterns = [\\"*.md\\"] exclude_patterns = [\\"*.tmp\\"] recursive_include_patterns = [\\"examples/*.py\\"] prune_patterns = [\\"build\\", \\"dist\\"] manifest_content = generate_manifest(include_patterns, exclude_patterns, recursive_include_patterns, prune_patterns) print(manifest_content) ``` **Expected Output:** ``` include *.md recursive-include examples *.py prune build prune dist ``` **Constraints:** - Ensure that the order of commands in `MANIFEST.in` maintains the precedence as described in the documentation. - Handle any potential conflicts between include and exclude commands appropriately. **Performance Requirements:** - The function should efficiently handle large numbers of patterns and directories.","solution":"def generate_manifest(include_patterns, exclude_patterns, recursive_include_patterns, prune_patterns): Generate the content for the MANIFEST.in file based on the given patterns. Args: - include_patterns (List[str]): Patterns to include. - exclude_patterns (List[str]): Patterns to exclude. - recursive_include_patterns (List[str]): Patterns to recursively include. - prune_patterns (List[str]): Directories to prune. Returns: - str: The content of the MANIFEST.in file. lines = [] for pattern in include_patterns: lines.append(f\\"include {pattern}\\") for pattern in exclude_patterns: lines.append(f\\"exclude {pattern}\\") for pattern in recursive_include_patterns: directory, file_pattern = pattern.split(\\"/\\", 1) lines.append(f\\"recursive-include {directory} {file_pattern}\\") for pattern in prune_patterns: lines.append(f\\"prune {pattern}\\") return \\"n\\".join(lines)"},{"question":"# Coding Assessment Task **Objective**: Implement a Python script that demonstrates advanced usage of the `os` module for file manipulation, process management, and handling environmental variables. Problem Statement: You are required to implement a Python function `manage_files_and_processes(base_dir: str, env_var_name: str, env_var_value: str) -> dict` that performs the following tasks: 1. **Directory and File Operations**: - Create a new temporary directory named `temp_dir` within the given `base_dir`. - Within `temp_dir`, create three files: `file1.txt`, `file2.txt`, and `file3.txt`. Write the filename inside each respective file. - List all files in `temp_dir` and return them as a list. 2. **Process Management**: - Fork a new process. - In the child process: - Change the current working directory to `temp_dir`. - Create an environment variable with the name `env_var_name` and value `env_var_value`. - Retrieve and print the value of this environment variable. - Return a dictionary containing details about the environment variables in the child process. - Ensure the parent process waits for the child process to complete before proceeding. 3. **Environmental Variables and Cleanup**: - After the child process finishes, remove all files created in `temp_dir` and delete `temp_dir`. Input: - `base_dir` - A string representing the path to the base directory where `temp_dir` will be created. Ensure it exists before running the function. - `env_var_name` - A string representing the name of the environment variable to be created. - `env_var_value` - A string representing the value of the environment variable to be created. Output: - A dictionary with the following keys and values: - `files`: A list of files created in `temp_dir`. - `child_environ`: A dictionary of environment variables within the child process. Constraints: - The function must handle errors gracefully, such as missing directories or file creation issues. - Ensure the function is portable and works correctly on both Unix and Windows systems. - Avoid using external libraries; rely solely on the `os` module and standard library. Example Usage: ```python result = manage_files_and_processes(\'/path/to/base\', \'MY_ENV_VAR\', \'my_value\') print(result) ``` This script should demonstrate proficiency in handling file and directory operations, process management, and interacting with environmental variables using the `os` module.","solution":"import os import shutil def manage_files_and_processes(base_dir: str, env_var_name: str, env_var_value: str) -> dict: temp_dir = os.path.join(base_dir, \'temp_dir\') try: # Create the temp_dir os.makedirs(temp_dir, exist_ok=True) # Create files and write the filenames inside them filenames = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] for filename in filenames: filepath = os.path.join(temp_dir, filename) with open(filepath, \'w\') as f: f.write(filename) # List all files in temp_dir files_created = os.listdir(temp_dir) result = {\'files\': files_created} # Fork a new process pid = os.fork() if pid == 0: # Child process try: # Change the current directory to temp_dir os.chdir(temp_dir) # Set the environment variable os.environ[env_var_name] = env_var_value # Retrieve and include the value of the environment variable environ_vars = os.environ.copy() # Print the environment variable value print(f\\"Child Process: {env_var_name} = {os.getenv(env_var_name)}\\") # Return the environment variables return {\'child_environ\': environ_vars} except Exception as e: print(f\\"Child process exception: {e}\\") return {\'child_environ\': {}} finally: os._exit(0) else: # Parent process # Wait for the child process to complete os.wait() # Cleanup: Remove all created files and the temp_dir shutil.rmtree(temp_dir) # Ensure the dictionary contains the files list and empty child_environ as parent doesn\'t have this info result[\'child_environ\'] = {} return result except Exception as e: print(f\\"Exception: {e}\\") return {\'files\': [], \'child_environ\': {}}"},{"question":"**Question: Implement a Python Class to Expose a Buffer Interface** You are asked to implement a Python class named `CustomBuffer` that simulates a simple buffer object. This class should provide an interface to expose a memory buffer that can be directly accessed and manipulated. # Requirements: 1. Implement a class named `CustomBuffer`. 2. Initialize the buffer with a given size and optional initial byte values. 3. Provide methods to: - Retrieve a pointer to the buffer. - Write data to the buffer. - Read data from the buffer. - Check if the buffer is read-only. 4. Simulate the buffer\'s behavior in terms of contiguous memory, read-only properties, and basic `Py_buffer` field-like attributes. 5. Ensure proper memory management by simulating `PyBuffer_Release()` behavior. # Example Usage: ```python # Initialize a buffer of size 10 with default values buf = CustomBuffer(10, initial_bytes=b\'x00\' * 10) # Write some data to the buffer buf.write_data(2, b\'x01x02x03\') # Read data from buffer data = buf.read_data(2, 3) print(data) # Output: b\'x01x02x03\' # Check if buffer is read-only print(buf.is_readonly()) # Output: False # Simulate buffer release buf.release_buffer() print(buf.pointer) # Output: None ``` # Constraints: - The buffer size must be a positive integer. - The methods should handle edge cases like writing beyond the buffer\'s length. - Performance should be considered when accessing and modifying the buffer. # Implementation Notes: - Use Python memoryview or bytearray for handling the internal buffer representation. - You may use Python’s ctypes library to simulate pointers if needed. - Pay attention to the simulation of read-only properties and buffer release, ensuring that accessing a released buffer should raise an error or return `None`. # Evaluation Criteria: - Correctness of buffer initialization and methods. - Handling of edge cases and errors. - Simulated pointer behavior and memory management. - Code readability and adherence to Pythonic practices. Implement the `CustomBuffer` class to fulfill the above requirements.","solution":"class CustomBuffer: def __init__(self, size, initial_bytes=None): if size <= 0: raise ValueError(\\"Size must be a positive integer\\") self.size = size if initial_bytes: if len(initial_bytes) != size: raise ValueError(\\"Length of initial_bytes must match the buffer size\\") self._buffer = bytearray(initial_bytes) else: self._buffer = bytearray(size) self._readonly = False self._released = False def pointer(self): if self._released: return None return memoryview(self._buffer) def write_data(self, offset, data): if self._readonly or self._released: raise RuntimeError(\\"Cannot write to read-only or released buffer\\") end = offset + len(data) if end > self.size: raise ValueError(\\"Data write exceeds buffer bounds\\") self._buffer[offset:end] = data def read_data(self, offset, length): if self._released: return None end = offset + length if end > self.size: raise ValueError(\\"Data read exceeds buffer bounds\\") return bytes(self._buffer[offset:end]) def is_readonly(self): return self._readonly def release_buffer(self): self._released = True self._buffer = None def set_readonly(self, readonly): self._readonly = readonly"},{"question":"# Advanced Python Coding Assessment: `asyncio` Platform Compatibility **Objective:** To evaluate your understanding of the `asyncio` package and its platform-specific limitations. You are required to write code demonstrating comprehension and handling of these constraints. Problem Statement Your task is to write a Python function named `cross_platform_asyncio_support` that demonstrates the ability to: 1. Create an event loop. 2. Perform a network operation using a TCP socket. 3. Manage platform-specific constraints in a way that the function can run on both Windows and macOS. Function Signature ```python async def cross_platform_asyncio_support(host: str, port: int) -> str: pass ``` Inputs - `host` (str): The hostname or IP address to connect to. - `port` (int): The port number to connect to. Outputs - Returns a string message indicating the success of the operation, or specifying any platform-related limitations encountered. Constraints - The implementation must account for the differences in event loop support on Windows and macOS as highlighted in the documentation. - The function should: - On Windows, use `ProactorEventLoop`. - On macOS, use the default event loop. - Effectively handle and report any unsupported operations per platform. Performance Requirements - The function should handle and report constraints efficiently without unnecessary overhead or complexity. Additional Information - You cannot use `loop.add_reader()` or `loop.add_writer()` on Windows. - On Windows, if performing subprocess operations, ensure compatibility with `ProactorEventLoop`. - Example pseudo-code or comments inline explaining platform-specific handling is encouraged. # Example Usage: ```python # Assume this is called with a running event loop import asyncio result = await cross_platform_asyncio_support(\'example.com\', 80) print(result) # Should print success or specific limitation message ``` # Hints - Use `asyncio.get_event_loop()` to get the current event loop. - Use appropriate event loop policy adjustments where necessary. - Handle exceptions gracefully to report unsupported operations. Good luck, and happy coding!","solution":"import asyncio import sys async def cross_platform_asyncio_support(host: str, port: int) -> str: # Check the platform and set the appropriate event loop policy if sys.platform.startswith(\'win\'): loop = asyncio.ProactorEventLoop() asyncio.set_event_loop(loop) else: loop = asyncio.get_event_loop() try: # Perform a simple network operation to connect to the given host and port reader, writer = await asyncio.open_connection(host, port) # If connected, close the connection gracefully writer.close() await writer.wait_closed() return \\"Successfully performed network operation.\\" except Exception as e: # Handle exceptions and unsupported operations return f\\"Operation failed with exception: {e}\\" # To run the function outside of async context for testing async def main_test(): result = await cross_platform_asyncio_support(\'example.com\', 80) print(result) # For manual testing: # asyncio.run(main_test()) # Uncomment this line to run the test manually"},{"question":"# Question: MIME Type Operations with `mailcap` You are required to write Python functions that utilize the `mailcap` module to handle MIME-type operations. Specifically, you will write two functions: `parse_mailcap_entry` and `execute_mailcap_command`. These functions will simulate getting mailcap capabilities, finding matching commands, and executing them. Function 1: `parse_mailcap_entry(mailcap_entry)` - **Input**: - `mailcap_entry` (str): A single line from a mailcap file in the format `<MIME_type>; <command>`. - **Output**: - Returns a tuple `(MIME_type, command)`. Function 2: `execute_mailcap_command(MIMEtype, key=\'view\', filename=\'/dev/null\', plist=[])` - **Input**: - `MIMEtype` (str): The MIME type for which the command needs to be executed. - `key` (str): The field desired, default is \'view\'. - `filename` (str): The filename to replace `%s` in the command line, default is \'/dev/null\'. - `plist` (list): A list of named parameters in the format `[\'param1=value1\', \'param2=value2\']`, default is empty list. - **Output**: - Returns the command line to be executed (str). - Return `None` if no matching entry is found or if there is any validation failure. Requirements: - The `mailcap.findmatch` method should be called with a dictionary structure conforming to the mailcap capabilities. - Validate that filenames and plist parameter values do not contain disallowed characters (non-alphanumerics other than `@+=:,./-_`). - Use the `mailcap` module functions `getcaps` and `findmatch`. - Include appropriate error handling and security checks. # Example Usage: ```python import mailcap # Example mailcap data for reference mailcap_data = \'\'\' video/mpeg; xmpeg %s image/jpeg; xv %s application/pdf; acroread %s \'\'\' def parse_mailcap_entry(mailcap_entry): parts = mailcap_entry.split(\';\', 1) return (parts[0].strip(), parts[1].strip()) def execute_mailcap_command(MIMEtype, key=\'view\', filename=\'/dev/null\', plist=[]): # Example caps dictionary obtained from mailcap_data caps = mailcap.getcaps() # Validate input for shell metacharacters for param in [filename] + plist: for char in param: if not (char.isalnum() or char in \\"@+=:,./-_\\"): return None command, entry = mailcap.findmatch(caps, MIMEtype, key, filename, plist) return command # Testing the function parsed_entry = parse_mailcap_entry(\'video/mpeg; xmpeg %s\') print(parsed_entry) # Output: (\'video/mpeg\', \'xmpeg %s\') cmd = execute_mailcap_command(\'video/mpeg\', filename=\'sample.mpeg\') print(cmd) # Output: something like \'xmpeg sample.mpeg\' ``` **Constraints**: - Assume `mailcap` data available on the system for `execute_mailcap_command`. - Use Python 3.10 or above. - Assume internet access is not available.","solution":"import mailcap def parse_mailcap_entry(mailcap_entry): Parses a single line from a mailcap file and returns a tuple (MIME_type, command). Args: mailcap_entry (str): A single line from a mailcap file in the format <MIME_type>; <command>. Returns: tuple: A tuple containing MIME_type and command. parts = mailcap_entry.split(\';\', 1) return (parts[0].strip(), parts[1].strip()) def execute_mailcap_command(MIMEtype, key=\'view\', filename=\'/dev/null\', plist=[]): Executes a mailcap command for a given MIME type. Args: MIMEtype (str): The MIME type for which the command needs to be executed. key (str): The field desired, default is \'view\'. filename (str): The filename to replace %s in the command line, default is \'/dev/null\'. plist (list): A list of named parameters in the format [\'param1=value1\', \'param2=value2\'], default is empty list. Returns: str: The command line to be executed. None: If no matching entry is found or if there is any validation failure. caps = mailcap.getcaps() # Validate input for shell metacharacters allowed_chars = \\"@+=:,./-_\\" for param in [filename] + plist: for char in param: if not (char.isalnum() or char in allowed_chars): return None command, entry = mailcap.findmatch(caps, MIMEtype, key, filename, plist) return command"},{"question":"Objective Design and implement a function to create a structured email message with multiple mime parts using the `EmailMessage` class from the `email.message` module. The function should properly organize text and attachments, and utilize appropriate headers to ensure the message is formatted correctly. Problem Statement You are tasked with creating a structured email that includes a plain text body, an HTML body, and multiple attachments. The function should organize these parts correctly using appropriate MIME types and headers. Function Signature ```python def create_multipart_email(subject: str, sender: str, recipient: str, plain_text: str, html_text: str, attachments: list): Create a multipart email message. Parameters: - subject (str): The subject of the email. - sender (str): The sender\'s email address. - recipient (str): The recipient\'s email address. - plain_text (str): The plain text body of the email. - html_text (str): The HTML body of the email. - attachments (list): A list of (filename, content, mime_type) tuples where: - filename (str): The name of the file. - content (bytes): The content of the file. - mime_type (str): The MIME type of the file. Returns: - EmailMessage: The constructed email message object. Requirements - The email must properly handle both a plain text and an HTML body. - Attachments should be added to the email with the correct MIME type and properly named. - The function should make use of methods like `make_mixed`, `make_alternative`, `add_related`, `add_attachment`, and others as needed from the `EmailMessage` class. Example ```python subject = \\"Test Email\\" sender = \\"test@example.com\\" recipient = \\"recipient@example.com\\" plain_text = \\"This is the plain text body of the email.\\" html_text = \\"<html><body>This is the HTML body of the email.</body></html>\\" attachments = [ (\\"example.txt\\", b\\"Example content of text file\\", \\"text/plain\\"), (\\"image.png\\", b\\"x89PNGrn\\", \\"image/png\\") ] email_message = create_multipart_email(subject, sender, recipient, plain_text, html_text, attachments) # The returned email_message should be an instance of EmailMessage with the specified parts print(email_message.as_string()) ``` Constraints - The function should ensure that the email is structured correctly according to MIME standards. - All headers added to the email should be valid and correctly formatted. - Make sure that binary attachments are handled properly and base64 encoded if necessary.","solution":"from email.message import EmailMessage import mimetypes def create_multipart_email(subject: str, sender: str, recipient: str, plain_text: str, html_text: str, attachments: list): Create a multipart email message. Parameters: - subject (str): The subject of the email. - sender (str): The sender\'s email address. - recipient (str): The recipient\'s email address. - plain_text (str): The plain text body of the email. - html_text (str): The HTML body of the email. - attachments (list): A list of (filename, content, mime_type) tuples where: - filename (str): The name of the file. - content (bytes): The content of the file. - mime_type (str): The MIME type of the file. Returns: - EmailMessage: The constructed email message object. # Create a new EmailMessage object msg = EmailMessage() msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = recipient # Add plain and HTML versions of the email content msg.set_content(plain_text) msg.add_alternative(html_text, subtype=\'html\') # Add attachments for filename, content, mime_type in attachments: # Determine the main type and sub type if mime_type: maintype, subtype = mime_type.split(\'/\', 1) else: maintype, subtype = mimetypes.guess_type(filename)[0].split(\'/\', 1) msg.add_attachment(content, maintype=maintype, subtype=subtype, filename=filename) return msg"},{"question":"# Question: You are required to create a secure user authentication simulator using the `getpass` module in Python. The task involves prompting the user for their username and password, validating the credentials, and ensuring the password is securely entered without echoing. Requirements: 1. **Function Name**: `authenticate_user` 2. **Input/Output**: - This function should not take any parameters. - It should return a boolean value: `True` if the user is successfully authenticated, `False` otherwise. Detailed Steps: 1. Prompt the user to enter their username using `getpass.getuser()` function. 2. Prompt the user to enter their password using `getpass.getpass(prompt=\'Enter your password: \')`. 3. Validate the entered username and password against a predefined dictionary of valid credentials. 4. If the entered username and password match the dictionary, return `True`; otherwise, return `False`. Constraints: - Use `getpass.getpass` to securely input the password without echoing. - Use `getpass.getuser` to get the system\'s current login username. - The predefined dictionary of valid credentials must include at least three sets of username-password pairs. Example: ```python valid_credentials = { \\"user1\\": \\"password123\\", \\"user2\\": \\"securepass\\", \\"user3\\": \\"mypassword\\" } assert authenticate_user() == True # Case where the system\'s current login user exists in the valid_credentials and the password entered is correct. assert authenticate_user() == False # Case where either the username is not in valid_credentials or the entered password is incorrect. ``` Note: - Ensure that the username is obtained using `getpass.getuser()` and the password is entered using `getpass.getpass`. - Use the predefined dictionary inside your function to check for valid credentials. Implement the `authenticate_user` function to meet the above requirements.","solution":"import getpass def authenticate_user(): Securely prompts the user for their username and password, and validates them against a predefined dictionary of valid credentials. Returns: bool: True if the user is successfully authenticated, False otherwise. valid_credentials = { \\"user1\\": \\"password123\\", \\"user2\\": \\"securepass\\", \\"user3\\": \\"mypassword\\" } username = getpass.getuser() password = getpass.getpass(prompt=\'Enter your password: \') if username in valid_credentials and valid_credentials[username] == password: return True return False"},{"question":"**Coding Assessment Question: Handling Unix shadow password database in Python** # Objective Write a Python function that processes entries from the Unix shadow password database. The function should fetch all entries and return a summary of users who need to change their passwords within a specific number of days. # Problem Statement You need to implement a function `users_needing_password_change(days: int) -> List[str]` that returns a list of usernames who must change their passwords within the next `days` days. # Function Signature ```python def users_needing_password_change(days: int) -> List[str]: ``` # Input - `days` (int): The number of days within which users need to change their passwords. # Output - A list of usernames (List[str]) whose passwords are about to expire within the given number of days. # Constraints - Only consider users whose passwords are set to expire (`sp_max` attribute is not -1). - You must handle the necessary exceptions, particularly regarding permission. If the script does not have the required privileges, it should return an empty list. - Assume the current date corresponds to the number of days since 1970-01-01 given by the `time.time() // 86400` result. # Example ```python # Example usage assuming you have the necessary privileges to access the shadow password database users_needing_password_change(10) # Output might look like: [\'user1\', \'user3\'] ``` # Note - Make sure to handle the `PermissionError` exception that may arise due to insufficient privileges. - Use the current date in days since 1970-01-01 as your reference point. # Hints - Use the `spwd.getspall()` method to fetch all entries. - Use the `time` module to get the current date in days. # Sample Implementation You need to implement the function as per the problem statement. No additional scaffold is provided for the student\'s solution.","solution":"import spwd import time from typing import List def users_needing_password_change(days: int) -> List[str]: Returns a list of usernames whose passwords are about to expire within the next `days` days. try: shadow_entries = spwd.getspall() except PermissionError: # Insufficient privileges to access the shadow file return [] current_date_days = time.time() // 86400 threshold_date_days = current_date_days + days users = [] for entry in shadow_entries: if entry.sp_max != -1 and (entry.sp_lstchg + entry.sp_max) <= threshold_date_days: users.append(entry.sp_namp) return users"},{"question":"**Objective:** The goal of this task is to demonstrate your understanding of the pandas options API by writing a function that modifies pandas display settings to a specified configuration and resets them after performing certain operations on a DataFrame. **Problem Description:** You are provided with various configuration settings for a pandas DataFrame display. Your task is to write a function `configure_pandas_options` that: 1. Temporarily sets the given pandas display options. 2. Applies these settings to a given DataFrame operation. 3. Resets the options to their default states after the operation is complete. # Function Signature ```python def configure_pandas_options(df: pd.DataFrame, options: dict, operation: callable) -> pd.DataFrame: Temporarily set pandas options, perform an operation on a DataFrame, and reset the options. Parameters: df (pd.DataFrame): The DataFrame on which to perform the operation. options (dict): A dictionary of pandas options to set temporarily. Keys are option names and values are the values to set. operation (callable): A function that takes a DataFrame as input and returns a modified DataFrame. Returns: pd.DataFrame: The resultant DataFrame after the operation. ``` # Input - `df` (pd.DataFrame): The DataFrame on which to perform the operation. - `options` (dict): A dictionary containing pandas options to temporarily set, where: - Keys are strings representing option names. - Values are the values to set for the corresponding options. - `operation` (callable): A function that takes a DataFrame as input and returns a modified DataFrame. # Output - A pandas DataFrame which is the result of applying `operation` on the input DataFrame `df` while the specified options are temporarily set. # Constraints - You should use the `pandas.option_context` to set and reset options. - Ensure that after the function completes, all pandas options are reset to their default values. - You should handle any exceptions that might occur during the operation to ensure the options are always reset. # Example ```python import pandas as pd import numpy as np def sample_operation(df: pd.DataFrame) -> pd.DataFrame: # Just an example operation: Add a new column with the sum of existing columns df[\'sum\'] = df.sum(axis=1) return df df = pd.DataFrame(np.random.randn(10, 4), columns=list(\'ABCD\')) options = { \\"display.max_rows\\": 5, \\"display.precision\\": 2 } result_df = configure_pandas_options(df, options, sample_operation) print(result_df) ``` # Explanation In the example, `configure_pandas_options`: 1. Applies the settings where `display.max_rows` is set to 5 and `display.precision` is set to 2. 2. Applies the `sample_operation` to the DataFrame, which adds a new column with the sum of existing columns. 3. Resets the options to their default values after the operation. Your implementation should ensure that the settings are temporary and are reset after the operation.","solution":"import pandas as pd def configure_pandas_options(df: pd.DataFrame, options: dict, operation: callable) -> pd.DataFrame: Temporarily set pandas options, perform an operation on a DataFrame, and reset the options. Parameters: df (pd.DataFrame): The DataFrame on which to perform the operation. options (dict): A dictionary of pandas options to set temporarily. Keys are option names and values are the values to set. operation (callable): A function that takes a DataFrame as input and returns a modified DataFrame. Returns: pd.DataFrame: The resultant DataFrame after the operation. with pd.option_context(*sum(([k, v] for k, v in options.items()), [])): # Perform the operation and return the modified DataFrame result_df = operation(df) return result_df"},{"question":"**Objective:** Assess students\' understanding of seaborn\'s `residplot` function and their ability to manipulate and analyze a dataset using the seaborn library. **Question:** You are provided with a dataset that contains information about various car models, their weight, displacement, horsepower, and miles per gallon (mpg). Your task is to carry out a detailed analysis using seaborn\'s `residplot` function to understand the relationship between these variables and identify any potential violations of linear regression assumptions. **Dataset:** We will use the `mpg` dataset available in seaborn\'s built-in datasets. **Task:** 1. Load the `mpg` dataset using seaborn\'s `load_dataset` function. 2. Create a residual plot to visualize the relationship between `weight` and `displacement` using seaborn\'s `residplot` function. Plot the Lowess curve with a red color. 3. Create a residual plot to visualize the relationship between `horsepower` and `mpg` with the default linear fit, and identify any potential violations of the linear regression assumptions. 4. Generate a new residual plot for `horsepower` and `mpg`, fitting a second-order (quadratic) regression model to check if removing higher-order trends stabilizes the residuals. 5. Add a LOWESS curve to the second-order residual plot, with a red line to emphasize any underlying structure. **Expected Input and Output:** *Input:* ```python import seaborn as sns # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Task 1 sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\", lowess=True, line_kws=dict(color=\\"r\\")) # Task 2 sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") # Task 3 sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) # Task 4 sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2, lowess=True, line_kws=dict(color=\\"r\\")) ``` *Output:* Four residual plots generated using seaborn\'s `residplot` function as described in the tasks above. The outputs should help identify and understand potential violations of linear regression assumptions for the specified relationships in the dataset. **Constraints and Limitations:** 1. Ensure that the seaborn library is imported and the `mpg` dataset is loaded correctly. 2. Use appropriate parameters to generate the required residual plots. 3. The code should be efficient and concise, making use of seaborn\'s built-in functionalities. **Performance Requirements:** - The code should execute within a reasonable timeframe considering the dataset\'s size. - The plots should be clear and accurately represent the data and any underlying trends or violations.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Task 1: Residual plot for weight and displacement with LOWESS red line plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\", lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\\"Residual Plot of Weight vs Displacement with LOWESS\\") plt.xlabel(\\"Weight\\") plt.ylabel(\\"Residuals\\") plt.show() # Task 2: Residual plot for horsepower and mpg with default linear fit plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") plt.title(\\"Residual Plot of Horsepower vs MPG with Linear Fit\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Residuals\\") plt.show() # Task 3: Residual plot for horsepower and mpg with second-order fit plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\\"Residual Plot of Horsepower vs MPG with Quadratic Fit\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Residuals\\") plt.show() # Task 4: Residual plot for horsepower and mpg with second-order fit and LOWESS red line plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2, lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\\"Residual Plot of Horsepower vs MPG with Quadratic Fit and LOWESS\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Residuals\\") plt.show()"},{"question":"**Objectives:** - Assess students\' understanding of the `stat` module functions for file mode analysis. - Evaluate their ability to utilize `os.stat()`, `os.fstat()`, and `os.lstat()` calls in a practical setting. - Test their knowledge of interpreting and manipulating file modes in Python. **Problem Statement:** You are tasked with developing a Python function that recursively analyzes the files and directories in a given directory. Your function should return a dictionary summarizing the types of files found and a list of file details with specific formatting. # Function Signature ```python import os from typing import Dict, List, Tuple def analyze_file_modes(directory: str) -> Tuple[Dict[str, int], List[str]]: pass ``` # Input - `directory` (str): The path to the directory that needs to be analyzed. # Output - A tuple containing: 1. A dictionary summarizing the counts of each file type found. 2. A list of strings, where each string gives details about a file\'s mode in the format `\\"<file_path>: <mode_string>\\"`. # Requirements 1. Your function should utilize `os.lstat()` to get file mode information. 2. Use the appropriate `stat` module functions to categorize file types. 3. Use the `stat.filemode()` function to convert the mode to a human-readable string. 4. Recursively analyze all subdirectories and include their contents in the summary. 5. File types to consider include: regular files, directories, symbolic links, character devices, block devices, FIFOs, sockets, doors, event ports, whiteouts. 6. Ignore any errors encountered while accessing files or directories (e.g., permission issues). # Example ```python directory = \\"/path/to/directory\\" result = analyze_file_modes(directory) print(result) ``` Given a directory structure as follows: ``` /path/to/directory |- file1.txt (regular file) |- file2.jpg (regular file) |- subdir1 (directory) |- file3.py (regular file) ``` The function call should return: ```python ( { \\"regular\\": 3, \\"directory\\": 1, \\"symlinks\\": 0, \\"character_devices\\": 0, \\"block_devices\\": 0, \\"fifos\\": 0, \\"sockets\\": 0, \\"doors\\": 0, \\"event_ports\\": 0, \\"whiteouts\\": 0 }, [ \\"/path/to/directory/file1.txt: -rw-r--r--\\", \\"/path/to/directory/file2.jpg: -rw-r--r--\\", \\"/path/to/directory/subdir1: drwxr-xr-x\\", \\"/path/to/directory/subdir1/file3.py: -rw-r--r--\\" ] ) ``` # Constraints - Assume that the provided directory path is valid and accessible. - The function should handle large directory structures efficiently. # Notes - You may import any additional standard Python modules if necessary. - Ensure the function is well-documented and includes appropriate error handling. Good luck!","solution":"import os import stat from typing import Dict, List, Tuple def analyze_file_modes(directory: str) -> Tuple[Dict[str, int], List[str]]: # Define the dictionary for counting different file types file_type_summary = { \\"regular\\": 0, \\"directory\\": 0, \\"symlinks\\": 0, \\"character_devices\\": 0, \\"block_devices\\": 0, \\"fifos\\": 0, \\"sockets\\": 0, \\"doors\\": 0, # Not all systems support doors \\"event_ports\\": 0, # Only some systems support event ports \\"whiteouts\\": 0 # Only some systems support whiteouts } # Define a list for storing file details file_details = [] # Helper function to update the summary and file details def update_summary_and_details(path: str): try: st = os.lstat(path) mode = st.st_mode mode_string = stat.filemode(mode) file_details.append(f\\"{path}: {mode_string}\\") if stat.S_ISREG(mode): file_type_summary[\\"regular\\"] += 1 elif stat.S_ISDIR(mode): file_type_summary[\\"directory\\"] += 1 elif stat.S_ISLNK(mode): file_type_summary[\\"symlinks\\"] += 1 elif stat.S_ISCHR(mode): file_type_summary[\\"character_devices\\"] += 1 elif stat.S_ISBLK(mode): file_type_summary[\\"block_devices\\"] += 1 elif stat.S_ISFIFO(mode): file_type_summary[\\"fifos\\"] += 1 elif stat.S_ISSOCK(mode): file_type_summary[\\"sockets\\"] += 1 # These checks might not exist on all systems, so they are commented out # elif stat.S_ISDOOR(mode): # file_type_summary[\\"doors\\"] += 1 # elif stat.S_ISEVPORT(mode): # file_type_summary[\\"event_ports\\"] += 1 # elif stat.S_ISWHT(mode): # file_type_summary[\\"whiteouts\\"] += 1 except OSError: # Ignore errors while accessing file details pass # Walk through the directory recursively for root, dirs, files in os.walk(directory, followlinks=False): for name in files: update_summary_and_details(os.path.join(root, name)) for name in dirs: update_summary_and_details(os.path.join(root, name)) return file_type_summary, file_details"},{"question":"# Custom MapReduce with Iterators and Generators In this task, you will implement a basic MapReduce system using Python\'s iterators, generators, and functional capabilities. Your implementation will include two main functions: `mapper()` and `reducer()`. Additionally, you will need to create a `MapReduce` class that orchestrates the complete process efficiently. Requirements: 1. **Mapper Function** - **Input**: An iterable of input data. - **Output**: An iterator of intermediate key-value pairs. - **Mapper Operation**: A callable function that processes each element and returns key-value pairs. 2. **Reducer Function** - **Input**: An intermediate key and an iterator of values associated with that key. - **Output**: The reduced result for that key. - **Reducer Operation**: A callable function that processes each set of intermediate key-value pairs. 3. **MapReduce Class** - Should manage the complete process. - Should collect intermediate pairs, group values by key, and apply the reducer function. - Methods: - `def __init__(self, mapper, reducer)` - `def map(self, input_data)` - `def group(self, mapped_data)` - `def reduce(self, grouped_data)` - `def execute(self, input_data)` Implementation Details: - The `MapReduce` class constructor takes two arguments: `mapper` and `reducer` functions. - The `map` method applies the mapper function to the input data. - The `group` method collects all intermediate key-value pairs and groups them by key. - The `reduce` method applies the reducer function to each group of values by key. - The `execute` method orchestrates the mapping, grouping, and reducing stages. Example Usage: Suppose you want to count the occurrences of each word in a list of sentences. 1. **Mapper Function**: ```python def word_count_mapper(sentence): for word in sentence.split(): yield (word.lower(), 1) ``` 2. **Reducer Function**: ```python def word_count_reducer(key, values): return (key, sum(values)) ``` 3. **MapReduce Execution**: ```python input_data = [ \\"Functional programming in Python\\", \\"Python supports multiple paradigms\\", \\"Iterators and generators are powerful\\" ] mr = MapReduce(mapper=word_count_mapper, reducer=word_count_reducer) result = mr.execute(input_data) print(list(result)) ``` Constraints: - You must not use any library functions that perform aggregation like `collections.Counter`. - Focus on writing functional-style code, avoiding side effects. - Ensure the operations are efficient and can handle large datasets. Expected Output: Given the above example input, possible output would be: ``` [(\'functional\', 1), (\'programming\', 1), (\'in\', 1), (\'python\', 2), (\'supports\', 1), (\'multiple\', 1), (\'paradigms\', 1), (\'iterators\', 1), (\'and\', 1), (\'generators\', 1), (\'are\', 1), (\'powerful\', 1)] ``` Submission: Please submit your implementation of the `MapReduce` class, including the `mapper` and `reducer` functions, and demonstrate its usage with the provided example data.","solution":"from itertools import groupby from operator import itemgetter class MapReduce: def __init__(self, mapper, reducer): self.mapper = mapper self.reducer = reducer def map(self, input_data): for item in input_data: yield from self.mapper(item) def group(self, mapped_data): sorted_data = sorted(mapped_data, key=itemgetter(0)) for key, group in groupby(sorted_data, key=itemgetter(0)): yield key, (item[1] for item in group) def reduce(self, grouped_data): for key, values in grouped_data: yield self.reducer(key, values) def execute(self, input_data): mapped_data = self.map(input_data) grouped_data = self.group(mapped_data) return self.reduce(grouped_data) def word_count_mapper(sentence): for word in sentence.split(): yield (word.lower(), 1) def word_count_reducer(key, values): return (key, sum(values))"},{"question":"# Custom Operation and Gradient Check in PyTorch Objective: You are to design and implement a new custom operation using PyTorch\'s `torch.library.custom_op` and test it for correctness and proper gradient calculation using `torch.library.opcheck` and `torch.autograd.gradcheck`. Requirements: 1. **Custom Operation**: - Create a new custom operation called `my_custom_op` that performs element-wise multiplication of two input tensors followed by an addition of a scalar bias. - The operation should be defined in such a way that it supports both forward and backward passes (i.e., it should be differentiable). 2. **Testing**: - Use `torch.library.opcheck` to verify the correctness of the operation. - Use `torch.autograd.gradcheck` to ensure that the gradients are mathematically correct. Function Signatures: - `create_my_custom_op(): --> Any` should create and return the custom operation. - `test_my_custom_op(): --> None` should test the custom operation using both `torch.library.opcheck` and `torch.autograd.gradcheck`. Constraints: - Input tensors will be 1-D tensors of the same size. - The scalar bias will be a floating point number. - Ensure that the custom operation is differentiable. Example: ```python import torch from torch.autograd import Variable # Placeholder for custom operation creation and registration create_my_custom_op() # Test function for the custom operation test_my_custom_op() # Assume the following inputs a = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) b = torch.tensor([4.0, 5.0, 6.0], requires_grad=True) bias = 2.0 # Using the custom operation output = my_custom_op(a, b, bias) output.sum().backward() print(a.grad, b.grad) ``` Ensure that your code is well-documented and test cases are included to cover a variety of scenarios.","solution":"import torch from torch.autograd import Function class MyCustomOpFunction(Function): @staticmethod def forward(ctx, input1, input2, bias): ctx.save_for_backward(input1, input2) return input1 * input2 + bias @staticmethod def backward(ctx, grad_output): input1, input2 = ctx.saved_tensors grad_input1 = grad_output * input2 grad_input2 = grad_output * input1 grad_bias = grad_output.sum() return grad_input1, grad_input2, grad_bias def my_custom_op(input1, input2, bias): return MyCustomOpFunction.apply(input1, input2, bias)"},{"question":"**Objective:** Demonstrate your understanding of the `shutil` module by developing a Python function that performs a series of file and directory operations. **Task:** Create a function named `organize_files` that takes two arguments: 1. `source_dir`: A string representing the path to the source directory. 2. `destination_dir`: A string representing the path to the destination directory. The function should: 1. Copy all `.txt` files from `source_dir` to `destination_dir/subdir1/`. 2. Move all `.log` files from `source_dir` to `destination_dir/subdir2/`. 3. Recursively copy the entire structure of `source_dir/extra` into `destination_dir/extra_backup`, but exclude any files that start with \\"temp\\". 4. After copying, if the `source_dir/archive` directory exists, compress it into a `.tar.gz` archive named `archive_backup.tar.gz` in the `destination_dir`. Both the `subdir1` and `subdir2` directories should be created in `destination_dir` if they do not already exist. **Constraints:** - Assume `source_dir` and `destination_dir` are valid directories with read/write permissions. - Use appropriate error handling to ensure that any file operation errors are captured and logged, instead of causing the program to crash. **Function Signature:** ```python def organize_files(source_dir: str, destination_dir: str) -> None: pass ``` # Example: Suppose we have the following directory structure: ``` source_dir/ ├── file1.txt ├── file2.log ├── file3.txt ├── file4.log └── extra/ ├── temp_file1.txt ├── real_file2.txt └── temp_file3.log ``` Running `organize_files(\'source_dir\', \'destination_dir\')` should result in: ``` destination_dir/ ├── subdir1/ │ ├── file1.txt │ └── file3.txt ├── subdir2/ │ ├── file2.log │ └── file4.log ├── extra_backup/ │ └── real_file2.txt └── archive_backup.tar.gz # If source_dir/archive exists ``` Remember to handle and log any exceptions that occur during the file operations properly.","solution":"import os import shutil import logging def organize_files(source_dir: str, destination_dir: str) -> None: Organizes files from source_dir into destination_dir according to specified rules. logging.basicConfig(level=logging.INFO) try: # Create necessary subdirectories in destination directory subdir1 = os.path.join(destination_dir, \'subdir1\') subdir2 = os.path.join(destination_dir, \'subdir2\') os.makedirs(subdir1, exist_ok=True) os.makedirs(subdir2, exist_ok=True) # Copy .txt files to subdir1 for filename in os.listdir(source_dir): full_file_name = os.path.join(source_dir, filename) if filename.endswith(\'.txt\') and os.path.isfile(full_file_name): shutil.copy(full_file_name, subdir1) # Move .log files to subdir2 for filename in os.listdir(source_dir): full_file_name = os.path.join(source_dir, filename) if filename.endswith(\'.log\') and os.path.isfile(full_file_name): shutil.move(full_file_name, subdir2) # Recursively copy source_dir/extra to destination_dir/extra_backup, excluding files starting with \\"temp\\" extra_src = os.path.join(source_dir, \'extra\') extra_dst = os.path.join(destination_dir, \'extra_backup\') if os.path.exists(extra_src) and os.path.isdir(extra_src): def ignore_temp_files(dirname, filenames): return {fname for fname in filenames if fname.startswith(\'temp\')} shutil.copytree(extra_src, extra_dst, ignore=ignore_temp_files) # Compress source_dir/archive into destination_dir/archive_backup.tar.gz archive_src = os.path.join(source_dir, \'archive\') archive_dst = os.path.join(destination_dir, \'archive_backup.tar.gz\') if os.path.exists(archive_src) and os.path.isdir(archive_src): shutil.make_archive(archive_dst.replace(\'.tar.gz\', \'\'), \'gztar\', root_dir=archive_src) except Exception as e: logging.error(f\\"Error organizing files: {e}\\")"},{"question":"Coding Assessment Question # Objective: You are tasked with implementing functions that make use of the `torch.random` module to handle random number generation in PyTorch. Your solution should demonstrate your understanding of generating random numbers, controlling randomness, and creating tensors with specific properties for machine learning experiments. # Requirements: 1. **Function 1: `set_random_seed(seed: int) -> None`** - **Input:** - `seed`: An integer value to set the seed for random number generation. - **Output:** - None - **Description:** - This function should set the random seed for all random number generators in PyTorch to ensure reproducibility across various runs. 2. **Function 2: `generate_random_tensor(shape: Tuple[int, ...], distribution: str, low: float = 0.0, high: float = 1.0) -> torch.Tensor`** - **Input:** - `shape`: A tuple representing the shape of the tensor to be generated. - `distribution`: A string indicating the type of distribution to sample from. It can be either `\\"uniform\\"` or `\\"normal\\"`. - `low`: A float specifying the lower bound for the uniform distribution (only applicable if `distribution` is `\\"uniform\\"`). - `high`: A float specifying the upper bound for the uniform distribution (only applicable if `distribution` is `\\"uniform\\"`). - **Output:** - A tensor of the specified shape and distribution. - **Description:** - This function should generate a random tensor of the given shape. If the distribution is `\\"uniform\\"`, the tensor values should be drawn from a uniform distribution between `low` and `high`. If the distribution is `\\"normal\\"`, the tensor values should be drawn from a normal distribution with mean `0` and standard deviation `1`. 3. **Function 3: `compare_random_tensors(tensor1: torch.Tensor, tensor2: torch.Tensor) -> float`** - **Input:** - `tensor1`: A tensor generated randomly. - `tensor2`: Another tensor generated randomly. - **Output:** - A float value representing the mean absolute difference between the two tensors. - **Description:** - This function should compute the mean absolute difference between the values of two tensors and return it as a measure of their similarity. # Constraints: - You are expected to use the `torch.random` module for setting seeds and generating random tensors. - You may also use other relevant PyTorch functionalities as needed. # Example Usage: ```python import torch # Function to set random seed def set_random_seed(seed: int) -> None: torch.manual_seed(seed) # Function to generate random tensor from typing import Tuple def generate_random_tensor(shape: Tuple[int, ...], distribution: str, low: float = 0.0, high: float = 1.0) -> torch.Tensor: if distribution == \\"uniform\\": return torch.rand(shape) * (high - low) + low elif distribution == \\"normal\\": return torch.randn(shape) else: raise ValueError(\\"Unknown distribution type\\") # Function to compare random tensors def compare_random_tensors(tensor1: torch.Tensor, tensor2: torch.Tensor) -> float: return torch.mean(torch.abs(tensor1 - tensor2)).item() # Example set_random_seed(42) tensor1 = generate_random_tensor((2, 3), \\"uniform\\", 0, 5) tensor2 = generate_random_tensor((2, 3), \\"normal\\") difference = compare_random_tensors(tensor1, tensor2) print(f\\"Mean Absolute Difference: {difference}\\") ``` # Output: The output will vary depending on the values generated by the random tensors, but you should see consistent results for repeated runs with the same seed.","solution":"import torch from typing import Tuple def set_random_seed(seed: int) -> None: Sets the seed for random number generation in PyTorch. Args: seed (int): Seed value to set. Returns: None torch.manual_seed(seed) def generate_random_tensor(shape: Tuple[int, ...], distribution: str, low: float = 0.0, high: float = 1.0) -> torch.Tensor: Generates a random tensor of a given shape and distribution. Args: shape (Tuple[int, ...]): Shape of the tensor to be generated. distribution (str): Type of distribution to sample from (\\"uniform\\" or \\"normal\\"). low (float, optional): Lower bound for uniform distribution. Defaults to 0.0. high (float, optional): Upper bound for uniform distribution. Defaults to 1.0. Raises: ValueError: If the distribution type is not recognized. Returns: torch.Tensor: Generated random tensor. if distribution == \'uniform\': return torch.rand(shape) * (high - low) + low elif distribution == \'normal\': return torch.randn(shape) else: raise ValueError(\\"Unknown distribution type\\") def compare_random_tensors(tensor1: torch.Tensor, tensor2: torch.Tensor) -> float: Computes the mean absolute difference between two tensors. Args: tensor1 (torch.Tensor): First tensor. tensor2 (torch.Tensor): Second tensor. Returns: float: Mean absolute difference between the tensors. return torch.mean(torch.abs(tensor1 - tensor2)).item()"},{"question":"# Question: Secure Password Generator You have been tasked with developing a secure password generator using Python\'s `secrets` module. The password must meet specific security requirements to ensure it is resilient against brute-force attacks. Your goal is to implement a function `generate_password(length: int, min_uppercase: int, min_digits: int) -> str` that generates a secure password according to the following constraints: 1. The password must have a total length of `length` characters. 2. The password must include at least `min_uppercase` uppercase letters. 3. The password must include at least `min_digits` digits. 4. The remaining characters can be any combination of uppercase letters, lowercase letters, or digits. # Input The function takes three parameters: - `length` (int): The total length of the password. (2 ≤ length ≤ 100) - `min_uppercase` (int): The minimum number of uppercase letters in the password. (0 ≤ min_uppercase ≤ length) - `min_digits` (int): The minimum number of digits in the password. (0 ≤ min_digits ≤ length) # Output The function should return a string representing the generated password. # Constraints - The total number of required characters (`min_uppercase` + `min_digits`) should not exceed the `length` of the password. - The generated password must be random and meet the specified constraints. # Example ```python generate_password(10, 2, 3) ``` Possible outputs could be (but are not limited to): - `Q7nK9Lp3Md` - `A9B4c1Yp8f` # Guidelines - Use the `secrets` module to generate random characters. - Ensure the password meets the specified constraints. - The randomness of the password should be cryptographically secure. **Note**: Your implementation should pass all provided test cases and handle edge cases appropriately.","solution":"import secrets import string def generate_password(length: int, min_uppercase: int, min_digits: int) -> str: Generate a secure password that meets the specified constraints: - Total length of `length` characters. - At least `min_uppercase` uppercase letters. - At least `min_digits` digits. Parameters: length (int): The total length of the password. min_uppercase (int): The minimum number of uppercase letters. min_digits (int): The minimum number of digits. Returns: str: The generated secure password. if min_uppercase + min_digits > length: raise ValueError(\\"The sum of min_uppercase and min_digits should not exceed the total length\\") uppercase_letters = [secrets.choice(string.ascii_uppercase) for _ in range(min_uppercase)] digits = [secrets.choice(string.digits) for _ in range(min_digits)] remaining_length = length - min_uppercase - min_digits remaining_chars = [secrets.choice(string.ascii_letters + string.digits) for _ in range(remaining_length)] password_chars = uppercase_letters + digits + remaining_chars secrets.SystemRandom().shuffle(password_chars) return \'\'.join(password_chars)"},{"question":"You are required to implement a simple chat server using the `asynchat` module. The server should handle multiple client connections asynchronously, receive messages from clients, and broadcast them to all connected clients. You need to implement the `collect_incoming_data()` and `found_terminator()` methods to handle incoming data and terminate incoming messages correctly. Your task is to: 1. Subclass the `asynchat.async_chat` to create a `ChatHandler` class. 2. Implement the `collect_incoming_data(data)` method to buffer the incoming data. 3. Implement the `found_terminator()` method to handle complete messages received from a client. 4. Broadcast the received messages to all connected clients. **Requirements:** - The message terminator is a newline character (`n`). - The server should handle multiple clients concurrently. - Use a FIFO queue to handle outgoing messages. # Input - The server should accept TCP socket connections from multiple clients. - Each client will send messages ending with `n`. # Output - The server should broadcast each complete message to all other connected clients. # Constraints - You must use the `asynchat` module as specified. - The buffer size for input and output should be set to 4096 bytes. - Ensure your implementation can handle abrupt client disconnections gracefully. # Example Given the following sequence of events: 1. Client A connects and sends \\"Hellon\\". 2. Client B connects and sends \\"Worldn\\". 3. Client C connects and sends \\"Pythonn\\". The expected server behavior: - All clients connected to the server will receive the messages \\"Hello\\", \\"World\\", and \\"Python\\" in sequence as they are sent. # Implementation Below is the skeleton code to help you get started: ```python import asynchat import asyncore import socket class ChatHandler(asynchat.async_chat): def __init__(self, sock, client_addr, server): super().__init__(sock=sock) self.set_terminator(b\'n\') self.ibuffer = [] self.client_addr = client_addr self.server = server self.server.add_client(self) def collect_incoming_data(self, data): Buffer the data self.ibuffer.append(data) def found_terminator(self): message = b\'\'.join(self.ibuffer).decode(\'utf-8\').strip() self.ibuffer = [] self.server.broadcast(message, self) def handle_close(self): self.server.remove_client(self) self.close() class ChatServer(asyncore.dispatcher): def __init__(self, host, port): super().__init__() self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) self.clients = [] def handle_accepted(self, sock, addr): ChatHandler(sock, addr, self) def add_client(self, client): self.clients.append(client) def remove_client(self, client): if client in self.clients: self.clients.remove(client) def broadcast(self, message, sender): for client in self.clients: if client != sender: client.push((message + \'n\').encode(\'utf-8\')) if __name__ == \\"__main__\\": server = ChatServer(\'localhost\', 8080) asyncore.loop() ``` Implement the `collect_incoming_data` and `found_terminator` methods to achieve the described functionality.","solution":"import asynchat import asyncore import socket class ChatHandler(asynchat.async_chat): def __init__(self, sock, client_addr, server): super().__init__(sock=sock) self.set_terminator(b\'n\') self.ibuffer = [] self.client_addr = client_addr self.server = server self.server.add_client(self) def collect_incoming_data(self, data): Buffer the data self.ibuffer.append(data) def found_terminator(self): Handle a complete message message = b\'\'.join(self.ibuffer).decode(\'utf-8\').strip() self.ibuffer = [] self.server.broadcast(message, self) def handle_close(self): self.server.remove_client(self) self.close() class ChatServer(asyncore.dispatcher): def __init__(self, host, port): super().__init__() self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) self.clients = [] def handle_accepted(self, sock, addr): ChatHandler(sock, addr, self) def add_client(self, client): self.clients.append(client) def remove_client(self, client): if client in self.clients: self.clients.remove(client) def broadcast(self, message, sender): for client in self.clients: if client != sender: client.push((message + \'n\').encode(\'utf-8\')) if __name__ == \\"__main__\\": server = ChatServer(\'localhost\', 8080) asyncore.loop()"},{"question":"# Question: Implement and Manipulate a Hierarchical Configuration Manager using `ChainMap` Objective: You are tasked with implementing a `ConfigurationManager` which uses Python\'s `ChainMap` to manage hierarchical configurations for an application. The goal is to allow for layered configuration sources and provide functionality for updating and retrieving configuration settings. Description: 1. **Class `ConfigurationManager`**: - **Initialization**: Accepts an arbitrary number of dictionaries representing different configuration sources. - **Methods**: - `get_setting(self, key) -> Any`: Retrieves the value for a given setting key. - `set_setting(self, key, value) -> None`: Sets the value for a given setting key in the most local (top) configuration. - `remove_setting(self, key) -> None`: Removes the setting key from the most local (top) configuration. - `add_new_layer(self, new_config: Optional[Dict[str, Any]] = None, **kwargs) -> None`: Adds a new layer to the top of the configuration stack. The new layer can be an optional dictionary or keyword arguments. - `remove_top_layer(self) -> None`: Removes the topmost configuration layer. - `list_settings(self) -> List[str]`: Lists all unique setting keys from all layers. # Input and Output: - **Initialization**: ```python config = ConfigurationManager(default_config, user_config, additional_config) ``` - **Example Usage**: ```python config.get_setting(\'timeout\') config.set_setting(\'timeout\', 30) config.remove_setting(\'debug\') config.add_new_layer({\'api_url\': \'https://api.example.com\'}, retries=5) config.remove_top_layer() config.list_settings() ``` # Constraints: 1. The configuration dictionaries contain string keys and values can be of any type. 2. At least one initial configuration dictionary is provided during initialization. 3. The `ChainMap` should be used internally to manage the hierarchical configuration. # Performance Requirements: - Ensure that `get_setting` operations have an average case (O(1)) complexity. - Ensure that `set_setting` and `remove_setting` also operate on the most immediate dictionary layer for efficient updates and deletions. # Example: ```python from collections import ChainMap from typing import Any, Dict, Optional, List class ConfigurationManager: def __init__(self, *configs: Dict[str, Any]): self.config = ChainMap(*configs) def get_setting(self, key: str) -> Any: return self.config.get(key, None) def set_setting(self, key: str, value: Any) -> None: self.config[key] = value def remove_setting(self, key: str) -> None: del self.config[key] def add_new_layer(self, new_config: Optional[Dict[str, Any]] = None, **kwargs) -> None: if new_config is None: new_config = {} new_config.update(kwargs) self.config = self.config.new_child(new_config) def remove_top_layer(self) -> None: self.config = self.config.parents def list_settings(self) -> List[str]: return list(self.config) # Example usage: default_config = {\'timeout\': 10, \'retries\': 3, \'debug\': True} user_config = {\'timeout\': 20, \'debug\': False} additional_config = {\'logging\': \'verbose\'} config = ConfigurationManager(default_config, user_config, additional_config) print(config.get_setting(\'timeout\')) # Output: 20 config.set_setting(\'timeout\', 30) print(config.get_setting(\'timeout\')) # Output: 30 config.remove_setting(\'timeout\') print(config.get_setting(\'timeout\')) # Output: 10 config.add_new_layer({\'api_url\': \'https://api.example.com\'}, retries=5) print(config.get_setting(\'api_url\')) # Output: \'https://api.example.com\' config.remove_top_layer() print(config.list_settings()) # Output: Unique list of keys from all layers: [\'timeout\', \'retries\', \'debug\', \'logging\'] ``` # Notes: - Ensure your implementation handles the methods and their behaviors as described. - The example provided is a guide; your implementation should follow the same structure and functional behavior.","solution":"from collections import ChainMap from typing import Any, Dict, Optional, List class ConfigurationManager: def __init__(self, *configs: Dict[str, Any]): self.config = ChainMap(*configs) def get_setting(self, key: str) -> Any: return self.config.get(key, None) def set_setting(self, key: str, value: Any) -> None: self.config[key] = value def remove_setting(self, key: str) -> None: del self.config[key] def add_new_layer(self, new_config: Optional[Dict[str, Any]] = None, **kwargs) -> None: if new_config is None: new_config = {} new_config.update(kwargs) self.config = self.config.new_child(new_config) def remove_top_layer(self) -> None: self.config = self.config.parents def list_settings(self) -> List[str]: return list(self.config) # Example usage: default_config = {\'timeout\': 10, \'retries\': 3, \'debug\': True} user_config = {\'timeout\': 20, \'debug\': False} additional_config = {\'logging\': \'verbose\'} config = ConfigurationManager(default_config, user_config, additional_config) print(config.get_setting(\'timeout\')) # Output: 20 config.set_setting(\'timeout\', 30) print(config.get_setting(\'timeout\')) # Output: 30 config.remove_setting(\'timeout\') print(config.get_setting(\'timeout\')) # Output: 10 config.add_new_layer({\'api_url\': \'https://api.example.com\'}, retries=5) print(config.get_setting(\'api_url\')) # Output: \'https://api.example.com\' config.remove_top_layer() print(config.list_settings()) # Output: Unique list of keys from all layers: [\'timeout\', \'retries\', \'debug\', \'logging\']"},{"question":"# Python Coding Assessment: Implementing Distributed Training using PyTorch\'s Distributed RPC Framework Objective The goal of this exercise is to test your understanding of PyTorch\'s Distributed RPC framework by requiring you to implement a simple distributed training setup over two worker nodes (machines). Problem Statement You need to set up a distributed training scenario where: 1. The main process initializes the RPC framework with two worker nodes. 2. A shared model is created on `worker0`. 3. `worker1` will fetch the shared model via RPC and perform a single training step. 4. Utilize Remote References (RRefs) to manage the shared model between workers. 5. Implement a simple distributed autograd and optimizer setup to update the model parameters. Requirements 1. **Initialize RPC**: - The main process should initialize the RPC framework with two workers: `worker0` and `worker1`. - Worker0 should create a simple neural network model. - Worker1 should fetch the model reference from Worker0 and perform a forward and backward pass. 2. **Create and Use RRef**: - Worker0 should create an RRef to the model. - Worker1 should use this RRef to access the model remotely. 3. **Distributed Autograd and Optimizer**: - Implement a distributed autograd context to record operations. - Use a distributed optimizer to update the model parameters. Specifications - Use PyTorch\'s `torch.distributed.rpc` package. - Define a simple neural network model (e.g., a single linear layer). - Use a simple loss function (e.g., mean squared error). - Implement a basic training loop where `worker1` performs one optimization step. Input and Output - No explicit input is required. The setup runs self-contained within the script. - Output the updated model parameters after a single training step. Example ```python import os import torch import torch.distributed.rpc as rpc import torch.nn as nn import torch.optim as optim # Define the model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) # Initialize RPC framework def init_rpc_framework(): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'29500\' rpc.init_rpc( name=\\"worker0\\", rank=0, world_size=2 ) # Worker1 should initialize its own RPC context in a similar block # Create the model and RRef on worker0 def create_model_rref(): model = SimpleModel() model_rref = rpc.RRef(model) return model_rref # Perform a training step on worker1 def remote_train_step(model_rref, data, target): model = model_rref.local_value() optimizer = optim.SGD(model.parameters(), lr=0.01) optimizer.zero_grad() output = model(data) loss = torch.nn.functional.mse_loss(output, target) loss.backward() optimizer.step() return model.fc.weight.data # Main function def main(): init_rpc_framework() if rpc.get_worker_info().name == \\"worker1\\": model_rref = rpc.remote(\\"worker0\\", create_model_rref) data, target = torch.randn(10), torch.randn(1) updated_params = rpc.rpc_sync(\\"worker1\\", remote_train_step, args=(model_rref, data, target)) print(updated_params) rpc.shutdown() if __name__ == \\"__main__\\": main() ``` Additional Constraints - Ensure the RPC framework is properly shut down after execution. - Handle appropriate worker initialization and communication protocols. Implement __`main()`__ to execute the distributed training workflow.","solution":"import os import torch import torch.distributed.rpc as rpc import torch.nn as nn import torch.optim as optim # Define the model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) # Initialize RPC framework def init_rpc_framework(rank, world_size): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'29500\' rpc.init_rpc( name=f\\"worker{rank}\\", rank=rank, world_size=world_size ) # Create the model and RRef on worker0 def create_model_rref(): model = SimpleModel() model_rref = rpc.RRef(model) return model_rref # Perform a training step on worker1 def remote_train_step(model_rref, data, target): model = model_rref.local_value() optimizer = optim.SGD(model.parameters(), lr=0.01) optimizer.zero_grad() output = model(data) loss = torch.nn.functional.mse_loss(output, target) loss.backward() optimizer.step() return model.fc.weight.data # Main function def main(rank, world_size): init_rpc_framework(rank, world_size) if rank == 0: model_rref = create_model_rref() elif rank == 1: model_rref = rpc.remote(\\"worker0\\", create_model_rref) data, target = torch.randn(10), torch.randn(1) updated_params = rpc.rpc_sync(\\"worker1\\", remote_train_step, args=(model_rref, data, target)) print(updated_params) rpc.shutdown() if __name__ == \\"__main__\\": world_size = 2 rank = int(os.environ[\\"RANK\\"]) main(rank, world_size)"},{"question":"**Objective**: Implement and thoroughly test an iterator and an asynchronous iterator in Python, demonstrating an understanding of the iterator protocol and the handling of asynchronous operations. Task 1. **Implement an iterator class** `Countdown`: - The class should be initialized with a positive integer `n`. - On each iteration, it should yield numbers from `n` down to 1. - Once it reaches 1, the next call should raise `StopIteration`. 2. **Implement an asynchronous iterator class** `AsyncCountdown`: - The class should be initialized with a positive integer `n`. - It should await for a second before yielding each number from `n` down to 1. - Once it reaches 1, the next call should raise `StopAsyncIteration`. **Input**: There are no external input functions; classes should be initialized directly as described. **Output**: The classes should correctly implement the iterator and asynchronous iterator protocols. **Constraints**: - You may assume `n` is always a positive integer. - Do not use external libraries for async behavior (only built-in `asyncio`). **Performance**: The iterator and asynchronous iterator should handle reasonably large values of `n` efficiently without crashing or running indefinitely. Example ```python # Example usage of Countdown class iterator = Countdown(5) for num in iterator: print(num) # Output should be: # 5 # 4 # 3 # 2 # 1 # Example usage of AsyncCountdown class import asyncio async def countdown(): async for num in AsyncCountdown(3): print(num) asyncio.run(countdown()) # Output should be: # 3 # 2 # 1 ``` Your Implementation **Part 1**: Implement the `Countdown` class below. ```python class Countdown: def __init__(self, n): self.n = n def __iter__(self): return self def __next__(self): if self.n > 0: current = self.n self.n -= 1 return current else: raise StopIteration ``` **Part 2**: Implement the `AsyncCountdown` class below. ```python import asyncio class AsyncCountdown: def __init__(self, n): self.n = n def __aiter__(self): return self async def __anext__(self): if self.n > 0: current = self.n self.n -= 1 await asyncio.sleep(1) return current else: raise StopAsyncIteration ```","solution":"class Countdown: def __init__(self, n): self.n = n def __iter__(self): return self def __next__(self): if self.n > 0: current = self.n self.n -= 1 return current else: raise StopIteration import asyncio class AsyncCountdown: def __init__(self, n): self.n = n def __aiter__(self): return self async def __anext__(self): if self.n > 0: current = self.n self.n -= 1 await asyncio.sleep(1) return current else: raise StopAsyncIteration"},{"question":"**Secure Message Digest Tool Implementation** You are required to implement a secure message digest tool in Python using the `hashlib` library. This tool should support multiple hashing algorithms and options for customization (e.g., salt, key, and digest size). Your implementation should have a function `generate_hash` that computes the hash for a given message and algorithm. # Requirements: 1. Implement a function `generate_hash` with the following signature: ```python def generate_hash(message: str, algorithm: str, salt: str = \'\', key: str = \'\', digest_size: int = None) -> str: ``` 2. **Parameters**: - `message` (str): The input message to be hashed. - `algorithm` (str): The name of the hashing algorithm (e.g., \'sha256\', \'sha512\', \'md5\', \'blake2b\'). - `salt` (str, optional): A salt value to randomize the hash output (default is an empty string). - `key` (str, optional): A key for keyed hashing (default is an empty string). - `digest_size` (int, optional): The size of the output digest in bytes (default is `None`). 3. **Constraints**: - If `digest_size` is provided, it should be within acceptable limits for the chosen algorithm. - The `salt` and `key` should be used only if the algorithm supports it. 4. **Output**: - The function should return the hexadecimal digest of the hash. 5. **Specifics**: - Raise a `ValueError` for unsupported algorithms. - Raise a `ValueError` if `digest_size` is out of bounds for the chosen algorithm. - Correctly handle the addition of salt and key only for algorithms that support it (e.g., blake2b, blake2s). - Optimize the function to handle large inputs efficiently. # Example Usage: ```python print(generate_hash(\'Hello world\', \'sha256\')) # Basic SHA-256 hash print(generate_hash(\'Hello world\', \'blake2b\', digest_size=32)) # Blake2b hash with custom digest size print(generate_hash(\'Hello world\', \'blake2b\', salt=\'random_salt\')) # Blake2b hash with salt print(generate_hash(\'Hello world\', \'blake2b\', key=\'secret_key\')) # Blake2b with keyed hashing ``` # Notes: - Make sure to utilize the `hashlib` constructors and methods appropriately. - You might need to convert strings to bytes before updating the hash object. - Implement appropriate error handling for invalid parameter combinations. **Your implementation will be evaluated on correctness, efficiency, and the ability to handle different hashing scenarios as specified.**","solution":"import hashlib def generate_hash(message: str, algorithm: str, salt: str = \'\', key: str = \'\', digest_size: int = None) -> str: Generates a hash for the given message using the specified algorithm, with optional salt, key, and digest size. Parameters: - message (str): The input message to be hashed. - algorithm (str): The name of the hashing algorithm (e.g., \'sha256\', \'sha512\', \'md5\', \'blake2b\'). - salt (str, optional): A salt value to randomize the hash output (default is an empty string). - key (str, optional): A key for keyed hashing (default is an empty string). - digest_size (int, optional): The size of the output digest in bytes (default is None). Returns: - str: The hexadecimal digest of the hash. message_bytes = message.encode() salt_bytes = salt.encode() key_bytes = key.encode() if algorithm in hashlib.algorithms_guaranteed: if algorithm.startswith(\'blake2\'): if digest_size is not None: if digest_size <= 0 or digest_size > hashlib.blake2b.MAX_DIGEST_SIZE: raise ValueError(\'Invalid digest size\') digest_size = digest_size or hashlib.blake2b().digest_size hash_obj = hashlib.new(algorithm, message_bytes, salt=salt_bytes, key=key_bytes, digest_size=digest_size) else: if key or salt: raise ValueError(f\\"The {algorithm} algorithm does not support salt or key.\\") if digest_size: raise ValueError(f\\"The {algorithm} algorithm does not support custom digest size.\\") hash_obj = hashlib.new(algorithm, message_bytes) else: raise ValueError(f\\"Unsupported algorithm: {algorithm}\\") return hash_obj.hexdigest()"},{"question":"# Advanced PyTorch: Conditional Execution with `torch.cond` Objective You are tasked with developing a PyTorch module that dynamically changes its behavior based on the statistical properties of the input tensor. This exercise will demonstrate your understanding of PyTorch\'s `torch.cond` and your ability to apply dynamic control flow in neural network modules. Task Implement a PyTorch module named `StatisticalCondPredicate` that: 1. Accepts a single input tensor `x`. 2. If the mean of `x` is greater than 0, applies a specified transformation (`transformation_fn_true`) to the tensor. 3. Otherwise, applies a different transformation (`transformation_fn_false`) to the tensor. Your module should use `torch.cond` to achieve this dynamic behavior. Input and Output - Input: A single tensor `x` of any shape. - Output: A tensor of the same shape as `x`, with the appropriate transformation applied based on the condition. Constraints - Do not use any global or external variables inside your transformation functions or module. - Both transformation functions should be defined within the module. - The code should not use explicit if-else statements outside of the `torch.cond`. Performance While performance is not the primary focus, ensure your solution is reasonably efficient by avoiding unnecessary computation and adhering to best practices in tensor operations. Example ```python import torch class StatisticalCondPredicate(torch.nn.Module): def __init__(self): super().__init__() # Transformation when mean of x > 0 def transformation_fn_true(x: torch.Tensor): return x * x # Example: Squaring the tensor # Transformation when mean of x <= 0 def transformation_fn_false(x: torch.Tensor): return x - 1 # Example: Subtracting 1 from the tensor self.transformation_fn_true = transformation_fn_true self.transformation_fn_false = transformation_fn_false def forward(self, x: torch.Tensor) -> torch.Tensor: return torch.cond( x.mean() > 0, self.transformation_fn_true, self.transformation_fn_false, (x,) ) # Test cases x1 = torch.tensor([1.0, 2.0, 3.0]) x2 = torch.tensor([-1.0, -2.0, -3.0]) model = StatisticalCondPredicate() output1 = model(x1) # Should apply `transformation_fn_true` output2 = model(x2) # Should apply `transformation_fn_false` print(\\"Output when mean > 0:\\", output1) print(\\"Output when mean <= 0:\\", output2) ``` In this example, the `StatisticalCondPredicate` module dynamically applies different transformations to the input tensor `x` based on whether the mean of the tensor is greater than 0 or not. Your task is to complete this implementation. Submission Submit your implementation of the `StatisticalCondPredicate` module along with a set of test cases demonstrating its correct behavior under different input scenarios.","solution":"import torch import torch.nn as nn class StatisticalCondPredicate(nn.Module): def __init__(self): super(StatisticalCondPredicate, self).__init__() # Transformation when mean of x > 0 def transformation_fn_true(x: torch.Tensor): return x * x # Example: Squaring the tensor # Transformation when mean of x <= 0 def transformation_fn_false(x: torch.Tensor): return x - 1 # Example: Subtracting 1 from the tensor self.transformation_fn_true = transformation_fn_true self.transformation_fn_false = transformation_fn_false def forward(self, x: torch.Tensor) -> torch.Tensor: # Conditional execution using pytorch operations (note: torch.cond() is not an actual PyTorch function). if x.mean() > 0: return self.transformation_fn_true(x) else: return self.transformation_fn_false(x)"},{"question":"Asynchronous Task Manager **Objective**: Design and implement an asynchronous task manager using Python\'s `asyncio` library. The task manager should be able to handle multiple tasks efficiently, distribute them via an asynchronous queue, and gather their results. **Requirements**: 1. **Function Implementation**: - Implement an asynchronous task manager with the following functions: - `main(task_list: List[callable]) -> List`: This function takes a list of asynchronous tasks (functions), runs them concurrently using `asyncio`, and returns their results. - `worker(task_queue: asyncio.Queue, result_queue: asyncio.Queue)`: This worker function retrieves tasks from the task queue, executes them, and places the results in the result queue. 2. **Input and Output Formats**: - `main(task_list: List[callable]) -> List`: - Input: A list of asynchronous functions (tasks) that take no arguments. - Output: A list of results returned by each task, in the order they were added to the task list. 3. **Constraints**: - The implementation should handle any exceptions raised by tasks gracefully, ensuring the task manager continues to process remaining tasks. - You must use `asyncio.Queue` for distributing tasks to the workers. - Ensure the solution is efficient and capable of handling at least 1000 tasks without a significant performance drop. **Example Implementation**: ```python import asyncio from typing import List, Callable async def main(task_list: List[Callable[[], any]]) -> List: task_queue = asyncio.Queue() result_queue = asyncio.Queue() # Put all tasks in the task_queue for task in task_list: await task_queue.put(task) # Create worker tasks to handle the task queue workers = [asyncio.create_task(worker(task_queue, result_queue)) for _ in range(5)] # Add a sentinel value to stop workers for _ in workers: await task_queue.put(None) # Wait for all workers to complete await asyncio.gather(*workers) # Collect results results = [] while not result_queue.empty(): results.append(await result_queue.get()) return results async def worker(task_queue: asyncio.Queue, result_queue: asyncio.Queue): while True: task = await task_queue.get() if task is None: break try: result = await task() await result_queue.put(result) except Exception as e: await result_queue.put(f\\"Task error: {str(e)}\\") finally: task_queue.task_done() # Example tasks async def sample_task_1(): await asyncio.sleep(1) return \\"Task 1 complete\\" async def sample_task_2(): await asyncio.sleep(2) return \\"Task 2 complete\\" async def sample_task_3(): await asyncio.sleep(0.5) return \\"Task 3 complete\\" if __name__ == \\"__main__\\": task_list = [sample_task_1, sample_task_2, sample_task_3] result = asyncio.run(main(task_list)) print(result) # Output: [\\"Task 1 complete\\", \\"Task 2 complete\\", \\"Task 3 complete\\"] ``` This question requires students to demonstrate their understanding of `asyncio` by managing concurrency and task distribution using asynchronous queues and coroutines. It evaluates their ability to handle real-world asynchronous programming scenarios.","solution":"import asyncio from typing import List, Callable, Any async def main(task_list: List[Callable[[], Any]]) -> List: task_queue = asyncio.Queue() result_queue = asyncio.Queue() # Put all tasks in the task_queue for task in task_list: await task_queue.put(task) # Create worker tasks to handle the task queue workers = [asyncio.create_task(worker(task_queue, result_queue)) for _ in range(5)] # Add a sentinel value to stop workers for _ in workers: await task_queue.put(None) # Wait for all workers to complete await asyncio.gather(*workers) # Collect results results = [] while not result_queue.empty(): results.append(await result_queue.get()) return results async def worker(task_queue: asyncio.Queue, result_queue: asyncio.Queue): while True: task = await task_queue.get() if task is None: break try: result = await task() await result_queue.put(result) except Exception as e: await result_queue.put(f\\"Task error: {str(e)}\\") finally: task_queue.task_done() # Example tasks async def sample_task_1(): await asyncio.sleep(1) return \\"Task 1 complete\\" async def sample_task_2(): await asyncio.sleep(2) return \\"Task 2 complete\\" async def sample_task_3(): await asyncio.sleep(0.5) return \\"Task 3 complete\\" if __name__ == \\"__main__\\": task_list = [sample_task_1, sample_task_2, sample_task_3] result = asyncio.run(main(task_list)) print(result) # Output: [\\"Task 1 complete\\", \\"Task 2 complete\\", \\"Task 3 complete\\"]"},{"question":"# Advanced Python Standard Library Usage You are required to write a Python function that processes a text file containing transaction records. Each transaction record includes fields such as timestamp, transaction amount, transaction type, and currency. The function should: 1. Parse the transaction records from the file. 2. Convert transaction amounts to a specified target currency using exchange rates. 3. Provide summarized statistics per transaction type (e.g., total amount, average amount). 4. Log significant events (e.g., parsing errors, conversion errors). 5. Return a structured summary of the transactions. Transaction File Format: Each line in the file represents one transaction and is formatted as follows: ``` timestamp, transaction_amount, transaction_type, currency ``` - `timestamp`: A string representing the date and time of the transaction (e.g., `2023-01-01 10:00:00`). - `transaction_amount`: A floating-point number representing the transaction amount. - `transaction_type`: A string representing the type of transaction (e.g., `purchase`, `refund`). - `currency`: A string representing the ISO currency code (e.g., `USD`, `EUR`). Constraints: 1. You can assume exchange rates are provided as a dictionary with currency codes as keys and exchange rates relative to the target currency as values. 2. You need to handle potential parsing errors gracefully and log them. Input: - `file_path` (str): The path to the transaction file. - `target_currency` (str): The currency code to which all transaction amounts should be converted. - `exchange_rates` (dict): A dictionary with currency codes as keys and exchange rates as values relative to the target currency. Output: - A dictionary summarizing the transactions with the following structure: ```python { \\"transactions\\": [ { \\"timestamp\\": \\"2023-01-01 10:00:00\\", \\"transaction_amount\\": 100.0, \\"transaction_type\\": \\"purchase\\", \\"currency\\": \\"USD\\", \\"converted_amount\\": 85.0 # in the target_currency }, ... ], \\"summary\\": { \\"purchase\\": { \\"total_amount\\": 500.0, \\"average_amount\\": 100.0 }, \\"refund\\": { \\"total_amount\\": 200.0, \\"average_amount\\": 100.0 } } } ``` Function Signature: ```python def process_transactions(file_path: str, target_currency: str, exchange_rates: dict) -> dict: pass ``` Example: ```python file_path = \\"transactions.txt\\" target_currency = \\"EUR\\" exchange_rates = {\\"USD\\": 0.85, \\"EUR\\": 1.0, \\"GBP\\": 1.1} result = process_transactions(file_path, target_currency, exchange_rates) print(result) ``` # Requirements: - You should use relevant modules from the Python Standard Library mentioned in the documentation provided (e.g., `decimal` for accurate floating-point arithmetic, `logging` for logging events). - Ensure to handle exceptions and edge cases gracefully. - Your code should be able to handle large files efficiently.","solution":"import csv import decimal import logging from collections import defaultdict def process_transactions(file_path: str, target_currency: str, exchange_rates: dict) -> dict: # Setting up logging logging.basicConfig(level=logging.INFO, format=\'%(asctime)s %(levelname)s:%(message)s\') transactions = [] summaries = defaultdict(lambda: {\\"total_amount\\": decimal.Decimal(0), \\"count\\": 0}) with open(file_path, \'r\') as file: reader = csv.reader(file) for row in reader: try: timestamp, transaction_amount, transaction_type, currency = row transaction_amount = decimal.Decimal(transaction_amount) # Conversion to target currency if currency not in exchange_rates: raise ValueError(f\\"Exchange rate for currency {currency} not provided\\") converted_amount = transaction_amount * decimal.Decimal(exchange_rates[currency]) # Store the transaction transaction = { \\"timestamp\\": timestamp, \\"transaction_amount\\": float(transaction_amount), \\"transaction_type\\": transaction_type, \\"currency\\": currency, \\"converted_amount\\": float(converted_amount) } transactions.append(transaction) # Update the summaries summaries[transaction_type][\\"total_amount\\"] += converted_amount summaries[transaction_type][\\"count\\"] += 1 except Exception as e: logging.error(f\\"Error processing row {row}: {e}\\") # Summarizing the statistics for each type summary = {} for transaction_type, data in summaries.items(): count = data[\\"count\\"] total_amount = float(data[\\"total_amount\\"]) average_amount = total_amount / count if count != 0 else 0 summary[transaction_type] = { \\"total_amount\\": total_amount, \\"average_amount\\": average_amount } return { \\"transactions\\": transactions, \\"summary\\": summary }"},{"question":"# PyTorch Advanced Features: Operator Profiling and API Usage Logging Objective Your task is to write a Python script using PyTorch to demonstrate the usage of advanced profiling and logging features. Specifically, you will implement: 1. Operator profiling with custom logging. 2. API usage logging. Task 1: Operator Profiling Implement a custom operator profiling mechanism that logs the time taken by individual operators during the forward pass of a neural network. The logging should include the function name, number of inputs, and the time taken. **Input:** - A defined PyTorch model. - Dummy input tensor for the model. **Output:** - Logs printed to the console indicating profiling information for each operator executed during the forward pass. Task 2: API Usage Logging Implement a mechanism to log when certain PyTorch API calls are used during the execution of the script. This logging should include the name of the API used. **Input:** - PyTorch APIs usage within the script. **Output:** - Logs printed to the console whenever specified PyTorch APIs are used. Constraints - Use the `torch.autograd.profiler` module for operator profiling. - Use `torch._C._log_api_usage_once` for logging API usage. Implementation Details 1. **Operator Profiling**: - Use `torch.autograd.profiler.profile` to measure the execution time of operators. - Create a simple neural network model. - Use a dummy input tensor to run a forward pass through the model and collect profiling data. - Print out the function name, number of inputs, and time taken for each operator in the forward pass. 2. **API Usage Logging**: - Define a callback function to log API usage. - Register this callback to log usage of specific PyTorch API calls (e.g., `torch.nn.Linear`). - Ensure that the logging function is triggered and prints the API call to the console. Example Code Structure: ```python import torch import torch.nn as nn import torch.autograd.profiler as profiler # Define a simple neural network model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 5) def forward(self, x): return self.fc(x) # Define the model and dummy input model = SimpleModel() dummy_input = torch.randn(1, 10) # Task 1: Implementing Operator Profiling with profiler.profile(record_shapes=True) as prof: with profiler.record_function(\\"model_inference\\"): output = model(dummy_input) print(prof.key_averages().table(sort_by=\\"cpu_time_total\\", row_limit=10)) # Task 2: Implementing API Usage Logging def api_usage_logger(event_name): print(f\\"API was used: {event_name}\\") # Registering API usage logger torch._C._log_api_usage_once(\\"torch.nn.Linear\\") # Trigger an API call that will be logged layer = torch.nn.Linear(10, 5) ``` **Note:** Ensure that the profiler logs and API usage logs are clearly printed to the console when the script is executed.","solution":"import torch import torch.nn as nn import torch.autograd.profiler as profiler # Define a simple neural network model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 5) def forward(self, x): return self.fc(x) # Define the model and dummy input model = SimpleModel() dummy_input = torch.randn(1, 10) # Task 1: Implementing Operator Profiling def profile_model_inference(model, dummy_input): with profiler.profile(record_shapes=True) as prof: with profiler.record_function(\\"model_inference\\"): output = model(dummy_input) for evt in prof.key_averages().table(sort_by=\\"cpu_time_total\\", row_limit=10): print(evt) # Operator profiling execution profile_model_inference(model, dummy_input) # Task 2: Implementing API Usage Logging def api_usage_logger(event_name): print(f\\"API was used: {event_name}\\") # Registering API usage logger for torch.nn.Linear usage torch._C._log_api_usage_once(\\"torch.nn.Linear\\") # Trigger an API call that will be logged layer = torch.nn.Linear(10, 5) dummy_output = layer(dummy_input) print(\\"API usage log end\\")"},{"question":"# Advanced Python Coding Assessment Objective: Demonstrate comprehension and application of `functools` features in Python by implementing enhanced functionality using the provided decorators and functions. Problem Statement: You are tasked with implementing a class `DataProcessor` that processes a sequence of numbers and provides various statistical measures efficiently. The measures should be computed on demand and cached to avoid redundant calculations. In certain cases, calculations should support partial application and single-dispatch methods. Specifications: 1. **Input and Output:** - Initialize the class with a sequence of floating-point numbers. - Implement methods to: - Calculate and return the mean of the sequence. - Calculate and return the standard deviation of the sequence. - Add a new number to the sequence, clearing cached values where appropriate. - Methods should support partial application where applicable. - Provide a method to process elements with single dispatch based on data types. 2. **Functions and Constraints:** - Use the `@cached_property` decorator to cache the mean and standard deviation calculations. - Use `@partialmethod` for a method that adds a specified value to all elements in the sequence. - Use `@singledispatchmethod` to process the sequence elements differently based on their type, with default behavior for generic types. - Ensure the class allows clearing cached properties when the sequence is modified. - If a number is added to the sequence, cached mean and standard deviation must be cleared. Example: ```python from functools import cached_property, partialmethod, singledispatchmethod import statistics class DataProcessor: def __init__(self, data): self.data = data @cached_property def mean(self): print(\\"Calculating mean\\") return statistics.mean(self.data) @cached_property def stdev(self): print(\\"Calculating standard deviation\\") return statistics.stdev(self.data) def add_number(self, number): self.data.append(number) # Clear cached properties if \'mean\' in self.__dict__: del self.__dict__[\'mean\'] if \'stdev\' in self.__dict__: del self.__dict__[\'stdev\'] @partialmethod def add_to_all(self, value): self.data = [x + value for x in self.data] # Clear cached properties if \'mean\' in self.__dict__: del self.__dict__[\'mean\'] if \'stdev\' in self.__dict__: del self.__dict__[\'stdev\'] @singledispatchmethod def process_element(self, element): print(f\\"Element of type {type(element).__name__}: {element}\\") @process_element.register def _(self, element: int): print(f\\"Integer element: {element * 2}\\") @process_element.register def _(self, element: str): print(f\\"String element: {element.upper()}\\") # Testing the class functionality data_processor = DataProcessor([1.0, 2.0, 3.0, 4.0, 5.0]) print(\\"Mean:\\", data_processor.mean) # Triggers calculation print(\\"Mean again:\\", data_processor.mean) # Uses cached value data_processor.add_number(6.0) print(\\"Updated Mean:\\", data_processor.mean) # Recalculates due to cache clearing data_processor.process_element(42) # Integer element processing data_processor.process_element(\\"hello\\") # String element processing data_processor.process_element(3.14) # Generic type processing data_processor.add_to_all(1.0) print(\\"Mean after adding to all:\\", data_processor.mean) # Recalculates due to cache clearing ``` 1. **Define the `DataProcessor` class with methods as specified.** 2. **Ensure mean and standard deviation calculations are cached and appropriately handled when data is modified.** 3. **Implement `add_to_all` method using `partialmethod`.** 4. **Implement `process_element` method using `singledispatchmethod`.** Constraints: - Use the decorators and functions from `functools` as specified. - Handle edge cases, such as when the sequence is empty or when new elements are added. Good luck!","solution":"from functools import cached_property, partialmethod, singledispatchmethod import statistics class DataProcessor: def __init__(self, data): self.data = data @cached_property def mean(self): print(\\"Calculating mean\\") return statistics.mean(self.data) @cached_property def stdev(self): print(\\"Calculating standard deviation\\") return statistics.stdev(self.data) def add_number(self, number): self.data.append(number) # Clear cached properties if \'mean\' in self.__dict__: del self.__dict__[\'mean\'] if \'stdev\' in self.__dict__: del self.__dict__[\'stdev\'] @partialmethod def add_to_all(self, value): self.data = [x + value for x in self.data] # Clear cached properties if \'mean\' in self.__dict__: del self.__dict__[\'mean\'] if \'stdev\' in self.__dict__: del self.__dict__[\'stdev\'] @singledispatchmethod def process_element(self, element): print(f\\"Element of type {type(element).__name__}: {element}\\") @process_element.register def _(self, element: int): print(f\\"Integer element: {element * 2}\\") @process_element.register def _(self, element: str): print(f\\"String element: {element.upper()}\\")"},{"question":"# Question: Utilizing PyTorch\'s MPS Backend You are required to demonstrate your understanding of PyTorch\'s MPS backend by performing a series of tasks. The tasks will involve checking for MPS availability, creating tensors on the MPS device, moving an existing neural network model to the MPS device, and performing operations on the GPU. # Task Description 1. **Check MPS Availability:** - Write a function `check_mps_availability()` that prints whether the MPS backend is available. If it is not available, the function should print the appropriate message based on whether the current PyTorch installation was built with MPS enabled or if the current MacOS version is not 12.3+ and/or an MPS-enabled device is not present. 2. **Tensor Operations Using MPS:** - Write a function `tensor_operations_on_mps()` that: - Creates a tensor of size (100, 100) filled with random values on the MPS device. - Multiplies the tensor by a scalar value (e.g., 3). - Returns the result. 3. **Model Training on MPS:** - Write a function `train_model_on_mps(model, data, target)` that: - Moves an existing PyTorch model to the MPS device. - Moves the given data and target tensors to the MPS device. - Performs a forward pass of the model on the data. - Calculates and returns the loss using Mean Squared Error (MSE) loss between the model’s prediction and the target. # Function Signatures ```python def check_mps_availability(): pass def tensor_operations_on_mps() -> torch.Tensor: pass def train_model_on_mps(model: torch.nn.Module, data: torch.Tensor, target: torch.Tensor) -> torch.Tensor: pass ``` # Example Usage ```python # Example usage: check_mps_availability() tensor_result = tensor_operations_on_mps() print(tensor_result) model = YourFavoriteNet() data = torch.rand(10, 3, 32, 32) # Example data target = torch.rand(10, 10) # Example target loss = train_model_on_mps(model, data, target) print(\\"Loss:\\", loss) ``` # Constraints - Ensure that you appropriately handle scenarios where the MPS device might not be available. - Assume the model provided for `train_model_on_mps` is a valid PyTorch model and `data` and `target` tensors are conforming to the model\'s input and output shapes respectively. - Ensure that all operations are performed on the MPS device if it is available.","solution":"import torch def check_mps_availability(): Checks if the MPS backend is available and prints the appropriate message. if not torch.backends.mps.is_available(): if torch.backends.mps.is_built(): print(\\"MPS not available because the current MacOS version is not 12.3+ and/or an MPS-enabled device is not present.\\") else: print(\\"MPS not available because the current PyTorch installation was not built with MPS enabled.\\") else: print(\\"MPS backend is available!\\") def tensor_operations_on_mps() -> torch.Tensor: Creates a tensor on the MPS device, performs a scalar multiplication, and returns the result. if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS backend is not available\\") device = torch.device(\\"mps\\") # Use MPS device tensor = torch.rand((100, 100), device=device) # Create a random tensor on the MPS device result = tensor * 3 # Multiply the tensor by the scalar value 3 return result def train_model_on_mps(model: torch.nn.Module, data: torch.Tensor, target: torch.Tensor) -> torch.Tensor: Moves the model, data, and target to the MPS device, performs a forward pass, computes the loss and returns it. if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS backend is not available\\") device = torch.device(\\"mps\\") # Use MPS device model = model.to(device) # Move the model to the MPS device data = data.to(device) # Move the data to the MPS device target = target.to(device) # Move the target to the MPS device output = model(data) # Perform a forward pass loss_fn = torch.nn.MSELoss() # Define the Mean Squared Error loss function loss = loss_fn(output, target) # Calculate the loss return loss"},{"question":"Context You are a machine learning engineer tasked with evaluating different datasets to benchmark your newly developed classification algorithm. Using the `sklearn.datasets` package, you need to load multiple datasets, preprocess them, and prepare them for training your machine learning model. Objective Write a function `load_and_prepare_datasets` that takes no arguments and returns a dictionary containing preprocessed data for each dataset specified in the list below. Requirements 1. Load the following datasets using appropriate functions from `sklearn.datasets`: - `iris` - `wine` - `digits` 2. For each dataset: - Retrieve the data and target arrays. - Perform a standard scaling (zero mean, unit variance) on the data using `StandardScaler` from `sklearn.preprocessing`. - Return the data and target as a tuple `(X_scaled, y)` in a dictionary with the dataset name as the key. 3. The output dictionary should have the following structure: ```python { \'iris\': (X_scaled_iris, y_iris), \'wine\': (X_scaled_wine, y_wine), \'digits\': (X_scaled_digits, y_digits) } ``` Constraints - Use the `return_X_y=True` parameter to simplify data loading from the dataset functions. - Ensure that your code adheres to best practices in terms of readability and efficiency. Example Usage ```python from sklearn.preprocessing import StandardScaler # The function implementation goes here datasets = load_and_prepare_datasets() # Access scaled data and targets for the iris dataset: X_scaled_iris, y_iris = datasets[\'iris\'] ``` Notes - Focus on proper implementation of dataset loading, preprocessing, and dictionary structuring as described. - You can assume that the `sklearn.datasets` and `sklearn.preprocessing` are installed and available for use. Good luck!","solution":"from sklearn.datasets import load_iris, load_wine, load_digits from sklearn.preprocessing import StandardScaler def load_and_prepare_datasets(): datasets = [\'iris\', \'wine\', \'digits\'] data_dict = {} for dataset in datasets: if dataset == \'iris\': data, target = load_iris(return_X_y=True) elif dataset == \'wine\': data, target = load_wine(return_X_y=True) elif dataset == \'digits\': data, target = load_digits(return_X_y=True) scaler = StandardScaler() data_scaled = scaler.fit_transform(data) data_dict[dataset] = (data_scaled, target) return data_dict"},{"question":"You are required to demonstrate your understanding of `scikit-learn` by performing a classification task on the `20 Newsgroups` dataset. Your task is to fetch the dataset, preprocess the data, build a machine learning model, train it, and evaluate its performance. # Instructions 1. **Load the dataset:** - Use the `fetch_20newsgroups` function from `sklearn.datasets` to load the dataset. - Load the following categories: `comp.graphics`, `sci.space`, `rec.autos`, `talk.politics.mideast`. - Split the data into training and testing sets. 2. **Preprocess the data:** - Transform the textual data into numerical data using `TfidfVectorizer` from `sklearn.feature_extraction.text`. - Ensure that you only consider the top 1000 features during transformation. 3. **Model building and training:** - Use a Support Vector Machine (SVM) with a linear kernel from `sklearn.svm`. - Train the SVM model using the training dataset. 4. **Evaluation:** - Evaluate the model performance using accuracy score from `sklearn.metrics`. - Print out the accuracy of the model on the test dataset. # Code Requirements - Implement your solution as a function `train_and_evaluate_20ng()`. - The function should return the accuracy of the model on the test data. - You can import any necessary modules from `sklearn`. # Function Signature ```python def train_and_evaluate_20ng() -> float: pass ``` # Constraints - You are limited to using `scikit-learn` for all operations. - You should consider the efficiency of your code; aim for a balance between clarity and performance.","solution":"from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.svm import SVC from sklearn.metrics import accuracy_score def train_and_evaluate_20ng(): # Load the dataset with specific categories categories = [\'comp.graphics\', \'sci.space\', \'rec.autos\', \'talk.politics.mideast\'] newsgroups_train = fetch_20newsgroups(subset=\'train\', categories=categories) newsgroups_test = fetch_20newsgroups(subset=\'test\', categories=categories) # Transform the textual data into numerical data using TfidfVectorizer vectorizer = TfidfVectorizer(max_features=1000) X_train = vectorizer.fit_transform(newsgroups_train.data) X_test = vectorizer.transform(newsgroups_test.data) # Prepare the target variables y_train = newsgroups_train.target y_test = newsgroups_test.target # Build and train the SVM model svm = SVC(kernel=\'linear\') svm.fit(X_train, y_train) # Predict on the test data y_pred = svm.predict(X_test) # Evaluate the model accuracy accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"# PyTorch Function Transforms Challenge Objective: Utilize PyTorch\'s torch.func library to implement a custom neural network function including advanced function transformations and validate its gradient computation and batching. Problem Statement: 1. **Function Implementation: Custom Neural Network** Implement a custom linear regression neural network model with a single hidden layer using PyTorch. The network should include an activation function (ReLU). Write a function `custom_model(weights, feature_vec)` that represents this network. 2. **Loss Function** Implement the Mean Squared Error (MSE) loss function. Write a function `mse_loss(weights, example, target)` to compute the MSE loss between the model predictions and the target values. 3. **Gradient Computation** Using the `grad` function from `torch.func`, write a function `compute_gradients(weights, examples, targets)` that computes and returns the gradients of the loss with respect to the weights for each example in the mini-batch. 4. **Batching with vmap** Write a function `batch_gradients(weights, examples, targets)` that uses `vmap` to compute the per-sample gradients for a given batch of examples and targets. 5. **Jacobian and Hessian Computation** Write functions `compute_jacobian` and `compute_hessian` that utilize `jacrev`/`jacfwd` and `hessian` respectively to compute the Jacobian and Hessian of the network\'s output with respect to the input features. Input and Output Formats: 1. `custom_model(weights, feature_vec)` - Inputs: - `weights`: Tensor of shape (input_size, hidden_size, hidden_size, output_size) (the model\'s weight parameters). - `feature_vec`: Tensor of shape (feature_size) (input feature vector). - Output: - `output`: Tensor of shape (output_size) (model prediction). 2. `mse_loss(weights, example, target)` - Inputs: - `weights`: Tensor of shape (input_size, hidden_size, hidden_size, output_size) (the model\'s weight parameters). - `example`: Tensor of shape (feature_size) (input feature vector). - `target`: Tensor of shape (output_size) (target value). - Output: - `loss`: Scalar Tensor representing the MSE loss. 3. `compute_gradients(weights, examples, targets)` - Inputs: - `weights`: Tensor of shape (input_size, hidden_size, hidden_size, output_size) (the model\'s weight parameters). - `examples`: Tensor of shape (batch_size, feature_size) (batch of input feature vectors). - `targets`: Tensor of shape (batch_size, output_size) (batch of target values). - Output: - `gradients`: Tensor of shape (batch_size, input_size, hidden_size, hidden_size, output_size) (gradients for each example). 4. `batch_gradients(weights, examples, targets)` - Inputs: - `weights`: Tensor of shape (input_size, hidden_size, hidden_size, output_size) (the model\'s weight parameters). - `examples`: Tensor of shape (batch_size, feature_size) (batch of input feature vectors). - `targets`: Tensor of shape (batch_size, output_size) (batch of target values). - Output: - `per_example_gradients`: Tensor of shape (batch_size, input_size, hidden_size, hidden_size, output_size) (per-sample gradients for the batch). 5. `compute_jacobian(feature_vec)` - Inputs: - `feature_vec`: Tensor of shape (feature_size) (input feature vector). - Output: - `jacobian`: Tensor representing the Jacobian of the model output with respect to the input features. 6. `compute_hessian(feature_vec)` - Inputs: - `feature_vec`: Tensor of shape (feature_size) (input feature vector). - Output: - `hessian`: Tensor representing the Hessian of the model output with respect to the input features. **Constraints:** - Validate that your implementations work correctly under varying input sizes and batch values. - Efficient implementation is critical. Ensure no unnecessary computations are performed. **Requirement:** - Ensure concepts from `torch.func` such as `grad`, `vmap`, `jacrev`, `jacfwd`, and `hessian` are practically utilized in the solution. Example: ```python import torch from torch.func import grad, vmap, jacrev, hessian # Example implementation of the functions # Function definitions follow... ``` Provide your implementation with comprehensive, well-documented code, ensuring the functionality aligns with the given descriptions.","solution":"import torch import torch.nn.functional as F from torch.func import grad, vmap, jacrev, hessian def custom_model(weights, feature_vec): Custom Neural Network model with single hidden layer and ReLU activation. W1, W2 = weights hidden = F.relu(torch.matmul(feature_vec, W1)) output = torch.matmul(hidden, W2) return output def mse_loss(weights, example, target): MSE loss function. pred = custom_model(weights, example) loss = F.mse_loss(pred, target) return loss def compute_gradients(weights, examples, targets): Compute the gradients of the loss with respect to the weights for each example. return grad(mse_loss)(weights, examples, targets) def batch_gradients(weights, examples, targets): Compute per-sample gradients for a batch using vmap. return vmap(grad(mse_loss), in_dims=(None, 0, 0))(weights, examples, targets) def compute_jacobian(weights, feature_vec): Compute the Jacobian of the network\'s output with respect to the input features. return jacrev(custom_model, argnums=1)(weights, feature_vec) def compute_hessian(weights, feature_vec): Compute Hessian of the network\'s output with respect to the input features. return hessian(custom_model, argnums=1)(weights, feature_vec)"},{"question":"# Advanced XML Parsing with `xml.parsers.expat` Objective: Implement a function to parse an XML string and extract specific information using the `xml.parsers.expat` module. Problem Statement: You are provided with an XML document as a string. Your task is to parse this XML document and extract all element names, attributes, and character data. You should implement a function `parse_xml(xml_string)` that takes an XML string as input and returns a dictionary with three keys: `element_names`, `attributes`, and `character_data`. Each key should map to a list containing the respective data extracted from the XML string. Function Signature: ```python def parse_xml(xml_string: str) -> dict: pass ``` Input: - `xml_string` (str): A string containing a well-formed XML document. Output: - A dictionary with the following structure: ```python { \\"element_names\\": [list_of_element_names], \\"attributes\\": [list_of_attributes_dicts], \\"character_data\\": [list_of_character_data_strings] } ``` - `list_of_element_names` (List of str): List of element names encountered in the document. - `list_of_attributes_dicts` (List of dict): List of dictionaries where each dictionary contains the attributes of an element. - `list_of_character_data_strings` (List of str): List of character data strings encountered in the document. Constraints: - You must use the `xml.parsers.expat` module for parsing. - You should handle potential parsing exceptions and return an appropriate error message in case of invalid XML. Example: ```python xml_string = <?xml version=\\"1.0\\"?> <parent id=\\"top\\"> <child1 name=\\"paul\\">Text goes here</child1> <child2 name=\\"fred\\">More text</child2> </parent> parse_xml(xml_string) ``` Expected output: ```python { \\"element_names\\": [\\"parent\\", \\"child1\\", \\"child2\\"], \\"attributes\\": [{\\"id\\": \\"top\\"}, {\\"name\\": \\"paul\\"}, {\\"name\\": \\"fred\\"}], \\"character_data\\": [\\"Text goes here\\", \\"More text\\"] } ``` Notes: - Ensure that your function is robust and can handle different XML structures. - Make sure to test your function thoroughly with various XML documents to validate its correctness.","solution":"from xml.parsers import expat def parse_xml(xml_string: str) -> dict: Parse the XML string and extract element names, attributes, and character data. :param xml_string: A string containing a well-formed XML document. :return: A dictionary with extracted element names, attributes, and character data. result = { \\"element_names\\": [], \\"attributes\\": [], \\"character_data\\": [] } def start_element(name, attrs): result[\\"element_names\\"].append(name) result[\\"attributes\\"].append(attrs) def char_data(data): data = data.strip() if data: result[\\"character_data\\"].append(data) parser = expat.ParserCreate() parser.StartElementHandler = start_element parser.CharacterDataHandler = char_data try: parser.Parse(xml_string, True) except expat.ExpatError as e: raise ValueError(\\"Invalid XML: {0}\\".format(e)) return result"},{"question":"# Efficient Data Handling with pandas You have been given a dataset that contains time-series data spread across multiple files in a directory. Each file contains data for a different year. Your task is to implement a function that reads the specified columns from these files, processes them efficiently by converting data types to minimize memory usage, and then performs an aggregation operation using chunking. The operation you need to perform is calculating the mean value of the \'x\' and \'y\' columns for each unique \'name\' across all the files. Function Signature ```python def aggregate_timeseries_data(directory: str, columns: List[str]) -> pd.DataFrame: pass ``` Inputs - `directory`: A string representing the path to the directory containing the parquet files. Each file in the directory follows the naming pattern `ts-YY.parquet` (e.g., `ts-00.parquet`). - `columns`: A list of strings specifying the columns to be read from the files. This list will always include `\'name\'`, `\'x\'`, and `\'y\'`. Output - A pandas DataFrame with the mean values of the \'x\' and \'y\' columns for each unique \'name\'. The DataFrame should have \'name\' as the index and columns `[\'mean_x\', \'mean_y\']`. The values in the output DataFrame should be of type `float32`. Constraints - The size of the data in the directory may be larger than available memory. - You must use efficient data types to minimize memory usage. - The solution should handle the data in chunks to avoid memory overflow. Example Usage ```python # Assuming the directory \\"data/timeseries/\\" contains the parquet files \'ts-00.parquet\' to \'ts-11.parquet\'. results = aggregate_timeseries_data(\\"data/timeseries/\\", [\\"name\\", \\"x\\", \\"y\\"]) print(results) ``` Example Output The DataFrame may look like: ``` mean_x mean_y name Alice 0.123456 0.234567 Bob -0.345678 0.456789 Charlie 0.567890 -0.678901 ``` Additional Notes 1. You can assume that all files in the directory follow a consistent parquet format. 2. You should use pandas\' capabilities to read specific columns and perform efficient datatype conversions. 3. Use chunking to process the files in a memory-efficient manner.","solution":"import pandas as pd import os from typing import List def aggregate_timeseries_data(directory: str, columns: List[str]) -> pd.DataFrame: Reads specified columns from parquet files in the directory, processes them efficiently by converting data types to minimize memory usage, and calculates the mean value of the \'x\' and \'y\' columns for each unique \'name\' across all the files. :param directory: str, path to the directory containing the parquet files. :param columns: List[str], columns to be read from the files, including \'name\', \'x\', and \'y\'. :return: pd.DataFrame, contains the mean values of the \'x\' and \'y\' columns for each unique \'name\'. all_data = pd.DataFrame() # Iterate through each file in the directory for file_name in os.listdir(directory): if file_name.endswith(\'.parquet\'): file_path = os.path.join(directory, file_name) chunk = pd.read_parquet(file_path, columns=columns) chunk = chunk.astype({\'name\': \'category\', \'x\': \'float32\', \'y\': \'float32\'}) # Append the chunk data to the all_data DataFrame all_data = pd.concat([all_data, chunk], ignore_index=True) # Group by \'name\' and calculate the mean for \'x\' and \'y\' columns result = all_data.groupby(\'name\').agg(mean_x=(\'x\', \'mean\'), mean_y=(\'y\', \'mean\')).reset_index() # Convert \'mean_x\' and \'mean_y\' columns to float32 result = result.astype({\'mean_x\': \'float32\', \'mean_y\': \'float32\'}) result.set_index(\'name\', inplace=True) return result"},{"question":"# Advanced Random Sampling and Distribution Analysis You are tasked with writing a Python function that uses the `random` module to create a complex simulation involving various random distributions and sampling techniques. Your solution should demonstrate understanding and correct usage of the provided functions and classes from the `random` module. **Problem Statement:** You need to implement a function `generate_statistics` that simulates an event where you: 1. Generate a specified number of random integers within a defined range. 2. Generate random floating-point numbers following multiple distributions. 3. Perform shuffling and random sampling on a given dataset. 4. Provide a summary of the generated data, including mean, median, and standard deviation where applicable. Your function, `generate_statistics(n_int, int_range, float_counts, data)`, should perform the following tasks: 1. Generate `n_int` random integers within `int_range` (inclusive). `int_range` will be a tuple `(a, b)`. 2. Generate the following counts of random floating-point numbers: - `float_counts[\'uniform\']`: Uniformly distributed between 0 and 10. - `float_counts[\'gaussian\']`: Normally distributed with mean 0 and standard deviation 1. 3. Shuffle the provided sequence `data` using `random.shuffle`. 4. Select a sample of size 5 from the shuffled data without replacement using `random.sample`. 5. Compute and return the following statistics: - The list of generated random integers. - The list of generated random uniform floating-point numbers. - The list of generated random Gaussian floating-point numbers. - The shuffled list. - The sample of 5 elements from the shuffled list. - The mean, median, and standard deviation of all numeric datasets generated (integers, uniform, Gaussian, and shuffled sample). **Function Signature:** ```python def generate_statistics(n_int: int, int_range: Tuple[int, int], float_counts: Dict[str, int], data: List[int]) -> Dict[str, Any]: pass ``` **Constraints:** - `n_int` is a positive integer. - `int_range` is a tuple `(a, b)` where `a` ≤ `b`. - `float_counts` is a dictionary with keys `\'uniform\'` and `\'gaussian\'`, and values are positive integers. - `data` is a non-empty list of integers of length at least 5. - Use the `random` module only for generating random numbers. - Calculate mean, median, and standard deviation using appropriate statistical methods or libraries like `statistics`. **Example:** ```python import statistics data = [10, 20, 30, 40, 50, 60] results = generate_statistics( n_int=5, int_range=(1, 100), float_counts={\'uniform\': 5, \'gaussian\': 5}, data=data) print(results[\'random_integers\']) # List of 5 random integers within range [1, 100] print(results[\'uniform_floats\']) # List of 5 random floating-point numbers uniformly distributed in range [0, 10] print(results[\'gaussian_floats\']) # List of 5 random floating-point numbers with Gaussian distribution (mean=0, stddev=1) print(results[\'shuffled_data\']) # Shuffled version of input list \'data\' print(results[\'sample\']) # Random sample of 5 elements from shuffled_data print(results[\'statistics\']) # Dictionary containing \'mean\', \'median\', and \'stddev\' for the generated integer, uniform, gaussian, and sample datasets ``` Note: Ensure that all generated random numbers are truly random by properly initializing the random seed if necessary.","solution":"import random import statistics from typing import Tuple, Dict, List, Any def generate_statistics(n_int: int, int_range: Tuple[int, int], float_counts: Dict[str, int], data: List[int]) -> Dict[str, Any]: # Step 1: Generate random integers within int_range random_integers = [random.randint(int_range[0], int_range[1]) for _ in range(n_int)] # Step 2: Generate random floating-point numbers uniform_floats = [random.uniform(0, 10) for _ in range(float_counts[\'uniform\'])] gaussian_floats = [random.gauss(0, 1) for _ in range(float_counts[\'gaussian\'])] # Step 3: Shuffle the provided dataset shuffled_data = data[:] random.shuffle(shuffled_data) # Step 4: Select a sample of 5 elements from the shuffled data sampled_data = random.sample(shuffled_data, 5) # Step 5: Compute statistics for all numeric datasets stats = { \'mean\': { \'random_integers\': statistics.mean(random_integers) if random_integers else None, \'uniform_floats\': statistics.mean(uniform_floats) if uniform_floats else None, \'gaussian_floats\': statistics.mean(gaussian_floats) if gaussian_floats else None, \'sampled_data\': statistics.mean(sampled_data) }, \'median\': { \'random_integers\': statistics.median(random_integers) if random_integers else None, \'uniform_floats\': statistics.median(uniform_floats) if uniform_floats else None, \'gaussian_floats\': statistics.median(gaussian_floats) if gaussian_floats else None, \'sampled_data\': statistics.median(sampled_data) }, \'stddev\': { \'random_integers\': statistics.stdev(random_integers) if len(random_integers) > 1 else None, \'uniform_floats\': statistics.stdev(uniform_floats) if len(uniform_floats) > 1 else None, \'gaussian_floats\': statistics.stdev(gaussian_floats) if len(gaussian_floats) > 1 else None, \'sampled_data\': statistics.stdev(sampled_data) if len(sampled_data) > 1 else None } } return { \'random_integers\': random_integers, \'uniform_floats\': uniform_floats, \'gaussian_floats\': gaussian_floats, \'shuffled_data\': shuffled_data, \'sample\': sampled_data, \'statistics\': stats }"},{"question":"You are to implement a utility function for numerical analysis, leveraging the `torch.finfo` and `torch.iinfo` classes provided in PyTorch. Your function will take in a dtype and return a dictionary of its numerical properties. Function Signature ```python import torch from typing import Union, Dict def get_numerical_properties(dtype: Union[torch.dtype, str]) -> Dict[str, Union[int, float]]: pass ``` Input - `dtype`: A data type, represented either as a `torch.dtype` object or a string specifying the data type. The string must be one of the following: `\'float32\'`, `\'float64\'`, `\'float16\'`, `\'bfloat16\'`, `\'uint8\'`, `\'int8\'`, `\'int16\'`, `\'int32\'`, `\'int64\'`. Output - A dictionary containing relevant numerical properties for the given dtype. The keys of the dictionary will vary based on whether the input dtype is a floating point or integer type, following these specifications: For a floating point dtype: ```python { \'bits\': int, # Number of bits occupied by the type \'eps\': float, # Smallest representable number such that 1.0 + eps != 1.0 \'max\': float, # Largest representable number \'min\': float, # Smallest representable number (typically -max) \'tiny\': float, # Smallest positive normal number \'resolution\': float # Approximate decimal resolution of the type } ``` For an integer dtype: ```python { \'bits\': int, # Number of bits occupied by the type \'max\': int, # Largest representable number \'min\': int # Smallest representable number } ``` Constraints - You may assume the input `dtype` string is always valid. - You should properly use the `torch.finfo` and `torch.iinfo` classes to gather the required information. Requirements - Your solution should not use any loops. - Performance should be optimal with direct calls to the PyTorch methods/classes. Example ```python output = get_numerical_properties(\'float64\') # output will be: # { # \'bits\': 64, # \'eps\': 2.220446049250313e-16, # \'max\': 1.7976931348623157e+308, # \'min\': -1.7976931348623157e+308, # \'tiny\': 2.2250738585072014e-308, # \'resolution\': 1e-15 # } output = get_numerical_properties(\'int32\') # output will be: # { # \'bits\': 32, # \'max\': 2147483647, # \'min\': -2147483648 # } ```","solution":"import torch from typing import Union, Dict def get_numerical_properties(dtype: Union[torch.dtype, str]) -> Dict[str, Union[int, float]]: if isinstance(dtype, str): dtype = getattr(torch, dtype) if dtype.is_floating_point: finfo = torch.finfo(dtype) return { \'bits\': finfo.bits, \'eps\': finfo.eps, \'max\': finfo.max, \'min\': finfo.min, \'tiny\': finfo.tiny, \'resolution\': finfo.eps } else: iinfo = torch.iinfo(dtype) return { \'bits\': iinfo.bits, \'max\': iinfo.max, \'min\': iinfo.min }"},{"question":"# Question: Advanced Numerical Computations You are required to implement a function `compute_special_sequence(n: int) -> List[float]` that generates a sequence of `n` numbers based on the following mathematical rules: 1. For each integer `i` from 1 to `n-1` (inclusive), compute the `i-th` element of the sequence as follows: - If `i` is even, the element is `math.log(math.factorial(i))`. - If `i` is odd, the element is `math.sqrt(math.sin(i) ** 2 + math.cos(i) ** 2)`. 2. Append `math.pi`, `math.e`, and `math.tau` to the end of the sequence in this order. # Input: - A single integer `n` where `1 <= n <= 100`. # Output: - A list of `float` numbers representing the sequence. # Constraints: - The function should handle the mathematical operations efficiently. - Input values are guaranteed to be within the specified range. # Example: ```python import math from typing import List def compute_special_sequence(n: int) -> List[float]: result = [] for i in range(1, n): if i % 2 == 0: result.append(math.log(math.factorial(i))) else: result.append(math.sqrt(math.sin(i) ** 2 + math.cos(i) ** 2)) result.extend([math.pi, math.e, math.tau]) return result # Example usage print(compute_special_sequence(5)) # Output: [1.0, 1.791759469228055, 1.0, 3.4011973816621555, 3.141592653589793, 2.718281828459045, 6.283185307179586] ``` # Notes: - The function demonstrates the use of fundamental and advanced mathematical functions including factorial, logarithm, trigonometric functions, and constants. - The example assumes `n` is 5, but the function should handle varying lengths of `n` as long as it meets the constraints.","solution":"import math from typing import List def compute_special_sequence(n: int) -> List[float]: Generates a sequence of \'n\' numbers based on specific mathematical rules. result = [] for i in range(1, n): if i % 2 == 0: result.append(math.log(math.factorial(i))) else: result.append(math.sqrt(math.sin(i) ** 2 + math.cos(i) ** 2)) result.extend([math.pi, math.e, math.tau]) return result"},{"question":"# Named Tensors in PyTorch In this problem, you will be working with named tensors in PyTorch. Your task is to implement a function that performs specific operations on named tensors and verifies the resulting names for correctness. Follow the description and requirements carefully. # Function Signature ```python def manipulate_named_tensor(): pass ``` # Requirements 1. Create two tensors, `tensor_a` and `tensor_b`, with the following specifications: - `tensor_a` with dimensions (4, 3, 2) and names (\'batch\', \'height\', \'width\'). - `tensor_b` with dimensions (4, 3) and names (\'batch\', \'height\'). 2. Perform the following operations: - Operation 1: Compute the mean of `tensor_a` along the \'width\' dimension and verify the resulting names. - Operation 2: Compute the sum of `tensor_b` and a tensor of ones with names (\'batch\', \'height\'), and verify the resulting names. - Operation 3: Transpose `tensor_a` along the \'height\' and \'width\' dimensions and verify the resulting names. 3. Return a dictionary with the results of each operation, along with the expected names for each resulting tensor. # Example Output ```python { \\"mean\\": { \\"result\\": <resulting tensor after mean operation>, \\"names\\": <names of the resulting tensor> }, \\"sum\\": { \\"result\\": <resulting tensor after sum operation>, \\"names\\": <names of the resulting tensor> }, \\"transpose\\": { \\"result\\": <resulting tensor after transpose operation>, \\"names\\": <names of the resulting tensor> } } ``` # Constraints - Use `torch` version 1.3.0 or later. - You should appropriately handle name-related errors that may arise during operations. - Your solution should not involve excessive computational steps; aim for clarity and efficiency. # Notes - Refer to the provided documentation snippet for understanding named tensor operations. - Ensure that your resultant tensors\' names match the expected names after each operation. # Implementation Implement the `manipulate_named_tensor` function, ensuring that it adheres to the above requirements and returns the expected outputs.","solution":"import torch def manipulate_named_tensor(): # Create tensors with named dimensions tensor_a = torch.randn(4, 3, 2, names=(\'batch\', \'height\', \'width\')) tensor_b = torch.randn(4, 3, names=(\'batch\', \'height\')) # Operation 1: Compute the mean of tensor_a along the \'width\' dimension mean_tensor_a = tensor_a.mean(dim=\'width\') # Operation 2: Compute the sum of tensor_b and a tensor of ones with names (\'batch\', \'height\') tensor_ones = torch.ones(4, 3, names=(\'batch\', \'height\')) summed_tensor_b = tensor_b + tensor_ones # Operation 3: Transpose tensor_a along the \'height\' and \'width\' dimensions transposed_tensor_a = tensor_a.transpose(\'height\', \'width\') # Prepare the result dictionary result = { \\"mean\\": { \\"result\\": mean_tensor_a, \\"names\\": mean_tensor_a.names }, \\"sum\\": { \\"result\\": summed_tensor_b, \\"names\\": summed_tensor_b.names }, \\"transpose\\": { \\"result\\": transposed_tensor_a, \\"names\\": transposed_tensor_a.names } } return result"},{"question":"# Custom PyTorch Module and Autograd Function Objective: Implement a custom autograd function and a custom PyTorch module that uses this function. Description: You are required to complete two tasks: 1. **Custom Autograd Function**: - Implement a custom autograd function named `CustomSigmoid` that performs the sigmoid operation in the forward pass and computes the gradients in the backward pass. - Validate your gradients using `torch.autograd.gradcheck`. 2. **Custom PyTorch Module**: - Implement a custom PyTorch module named `CustomSigmoidModule` that uses the `CustomSigmoid` function in its forward pass. Requirements: 1. **CustomSigmoid Function**: - Define a class `CustomSigmoid` inheriting from `torch.autograd.Function`. - Implement the `forward` and `backward` static methods. - In the forward method, compute the sigmoid function. - In the backward method, compute the gradient of the loss with respect to inputs. 2. **CustomSigmoidModule**: - Define a class `CustomSigmoidModule` inheriting from `torch.nn.Module`. - Implement its `__init__` and `forward` methods. - In the forward method, utilize the `CustomSigmoid`. 3. **Validation**: - Write a test function to validate the gradients using `torch.autograd.gradcheck`. Constraints: - Only use PyTorch and standard Python libraries. - Your `CustomSigmoid` class must subclass `torch.autograd.Function`. - Your `CustomSigmoidModule` class must subclass `torch.nn.Module`. # Example Usage: ```python import torch from torch.autograd import Function, gradcheck import torch.nn as nn # Task 1: Implement CustomSigmoid Function class CustomSigmoid(Function): @staticmethod def forward(ctx, input): # Save context for backward ctx.save_for_backward(input) # Sigmoid forward pass return 1 / (1 + torch.exp(-input)) @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors sigmoid = 1 / (1 + torch.exp(-input)) # Gradient computation grad_input = grad_output * sigmoid * (1 - sigmoid) return grad_input # Task 2: Implement CustomSigmoidModule class CustomSigmoidModule(nn.Module): def __init__(self): super(CustomSigmoidModule, self).__init__() def forward(self, input): return CustomSigmoid.apply(input) # Task 3: Validation Function def validate(): input = (torch.randn(3, 3, dtype=torch.double, requires_grad=True),) assert gradcheck(CustomSigmoid.apply, input, eps=1e-6, atol=1e-4), \\"Gradcheck failed\\" if __name__ == \\"__main__\\": validate() print(\\"Validation passed!\\") ``` This example demonstrates how to implement and use a custom autograd function and module. Please write and test your implementations.","solution":"import torch from torch.autograd import Function import torch.nn as nn # Task 1: Implement CustomSigmoid Function class CustomSigmoid(Function): @staticmethod def forward(ctx, input): # Save context for backward ctx.save_for_backward(input) # Sigmoid forward pass sigmoid_output = 1 / (1 + torch.exp(-input)) return sigmoid_output @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors sigmoid = 1 / (1 + torch.exp(-input)) # Gradient computation grad_input = grad_output * sigmoid * (1 - sigmoid) return grad_input # Task 2: Implement CustomSigmoidModule class CustomSigmoidModule(nn.Module): def __init__(self): super(CustomSigmoidModule, self).__init__() def forward(self, input): return CustomSigmoid.apply(input)"},{"question":"# Question: Applying PCA on a Real Dataset to Reduce Dimensionality You are given a multivariate dataset (`data.csv`) that contains several numerical features. Your task is to perform Principal Component Analysis (PCA) to reduce the dataset\'s dimensionality while retaining as much variance as possible. Requirements: 1. **Load the dataset**: Read the dataset from \'data.csv\'. Assume that the first row contains the header and that there are no missing values in the dataset. 2. **Standardize the Data**: Standardize the dataset before applying PCA. Use `StandardScaler` from `sklearn.preprocessing` to standardize the features (mean=0 and variance=1). 3. **Apply PCA**: - Perform PCA on the standardized data to reduce it to 2 dimensions. - Calculate and print the amount of variance explained by the first two principal components. 4. **Output Transformed Data**: - Save the first two principal components of the transformed data into a new CSV file named `transformed_data.csv`. Ensure the new file has two columns named `PC1` and `PC2`. Input Format: - A CSV file named `data.csv` with numerical data. Output Format: - A CSV file named `transformed_data.csv` containing the first two principal components of the transformed data. Constraints: - Make sure the explained variance ratio of the first two components is as high as possible. - You can assume the dataset size is manageable in memory (not extremely large). Example: If the input CSV file `data.csv` looks like this: ``` feature1,feature2,feature3,feature4 7.0,1.2,3.5,5.6 6.1,0.9,2.8,4.8 5.8,1.0,3.0,5.0 ... ``` Your script should standardize the data, apply PCA to reduce it to 2 dimensions, calculate the explained variance ratio for the first two components, and export the result to `transformed_data.csv`. ```python # Example script structure import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA # Step 1: Load the dataset df = pd.read_csv(\'data.csv\') # Step 2: Standardize the data features = df.columns scaler = StandardScaler() standardized_data = scaler.fit_transform(df) # Step 3: Apply PCA pca = PCA(n_components=2) principal_components = pca.fit_transform(standardized_data) explained_variance = pca.explained_variance_ratio_ print(f\'Explained variance by the first 2 components: {explained_variance}\') # Step 4: Output transformed data transformed_df = pd.DataFrame(data = principal_components, columns = [\'PC1\', \'PC2\']) transformed_df.to_csv(\'transformed_data.csv\', index=False) ``` Submit the script used to perform this task and ensure it meets the specified requirements.","solution":"import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA def apply_pca_and_transform(): Perform PCA on a dataset to reduce it to 2 dimensions and save the result in \'transformed_data.csv\'. # Step 1: Load the dataset df = pd.read_csv(\'data.csv\') # Step 2: Standardize the data scaler = StandardScaler() standardized_data = scaler.fit_transform(df) # Step 3: Apply PCA pca = PCA(n_components=2) principal_components = pca.fit_transform(standardized_data) explained_variance = pca.explained_variance_ratio_ print(f\'Explained variance by the first 2 components: {explained_variance}\') # Step 4: Output transformed data transformed_df = pd.DataFrame(data=principal_components, columns=[\'PC1\', \'PC2\']) transformed_df.to_csv(\'transformed_data.csv\', index=False)"},{"question":"# Email Message Manipulation and Serialization You are tasked with implementing an Email Manager class using Python\'s `email.message.EmailMessage` class. The purpose of this manager is to encapsulate the functionalities needed to manipulate and serialize email messages as specified below. Your Email Manager class should be able to: 1. Initialize with a sender, recipient, subject, and body of the email. 2. Add headers such as `Content-Type`, `Content-Disposition`, and other custom headers. 3. Set and manage MIME parts, including attachments. 4. Serialize the email message to both string and bytes representations. 5. Handle multipart messages and extract specific parts of the email. Implement the `EmailManager` class with the following methods: Method Signatures: ```python from email.message import EmailMessage class EmailManager: def __init__(self, sender: str, recipient: str, subject: str, body: str): Initializes an EmailManager instance with the given sender, recipient, subject, and body. pass def add_header(self, name: str, value: str): Adds a header to the email message. pass def add_attachment(self, content: bytes, maintype: str, subtype: str, filename: str): Adds an attachment to the email message. pass def get_serialized_message(self, as_bytes: bool = False) -> str: Returns the serialized email message as a string by default, as bytes if as_bytes is True. pass def extract_text_parts(self) -> dict: Extracts and returns a dictionary with maintypes as keys and a list of subparts content as values. For instance, {\'text\': [\'plain_text_content\', \'html_content\'], \'application\': [\'attachment_content\']} pass ``` Constraints and Requirements: - Ensure your implementation adheres to RFC standards regarding email headers and MIME types. - The payload can include multiple parts; handle creating different content types and boundaries correctly. - Attachments should be encoded correctly. - Emails should be serializable into valid formats for transmission over email servers. Sample Usage: ```python # Create an email manager instance manager = EmailManager(\'sender@example.com\', \'recipient@example.com\', \'Important Subject\', \'This is the email body.\') # Add headers manager.add_header(\'X-Priority\', \'1 (Highest)\') # Add an attachment with open(\'attachment.pdf\', \'rb\') as f: manager.add_attachment(f.read(), \'application\', \'pdf\', \'attachment.pdf\') # Serialize the message email_str = manager.get_serialized_message() # Returns as string email_bytes = manager.get_serialized_message(as_bytes=True) # Returns as bytes # Extract text parts text_parts = manager.extract_text_parts() ``` Implement the `EmailManager` class to meet the specified requirements.","solution":"from email.message import EmailMessage from email.policy import default class EmailManager: def __init__(self, sender: str, recipient: str, subject: str, body: str): Initializes an EmailManager instance with the given sender, recipient, subject, and body. self.message = EmailMessage() self.message[\'From\'] = sender self.message[\'To\'] = recipient self.message[\'Subject\'] = subject self.message.set_content(body) def add_header(self, name: str, value: str): Adds a header to the email message. self.message[name] = value def add_attachment(self, content: bytes, maintype: str, subtype: str, filename: str): Adds an attachment to the email message. self.message.add_attachment(content, maintype=maintype, subtype=subtype, filename=filename) def get_serialized_message(self, as_bytes: bool = False) -> str: Returns the serialized email message as a string by default, as bytes if as_bytes is True. if as_bytes: return self.message.as_bytes(policy=default) return self.message.as_string(policy=default) def extract_text_parts(self) -> dict: Extracts and returns a dictionary with maintypes as keys and a list of subparts content as values. parts = { \'text\': [], \'application\': [] } if self.message.is_multipart(): for part in self.message.iter_parts(): if part.get_content_maintype() == \'text\': parts[\'text\'].append(part.get_payload(decode=True).decode(part.get_content_charset())) elif part.get_content_maintype() == \'application\': parts[\'application\'].append(part.get_payload(decode=True)) else: maintype = self.message.get_content_maintype() if maintype == \'text\': parts[\'text\'].append(self.message.get_payload(decode=True).decode(self.message.get_content_charset())) return parts"},{"question":"**Question: Advanced Data Analysis with Pandas** **Objective:** You are provided with sales data for an e-commerce company. The dataset consists of two CSV files: `sales_data.csv` and `product_data.csv` which you need to analyze to gain insights into sales trends and product performance. **Datasets:** 1. **sales_data.csv**: - Contains columns: `OrderID`, `ProductID`, `Quantity`, `SaleDate` (in format `YYYY-MM-DD`), `UnitPrice`, and `CustomerID`. 2. **product_data.csv**: - Contains columns: `ProductID`, `ProductCategory`, and `ProductName`. **Tasks:** 1. **Load the data**: Read the CSV files into pandas DataFrames. 2. **Clean the data**: - Handle any missing values in the columns by appropriate imputation strategies. - Ensure `SaleDate` is in datetime format. 3. **Aggregate Sales Data**: - Create a new column `TotalSales` in `sales_data` which is calculated as `Quantity * UnitPrice`. - Group the data by `SaleDate`, and calculate the daily total sales. 4. **Merge DataFrames**: - Merge `sales_data` with `product_data` on `ProductID` to get a complete dataset. 5. **Analyze Product Performance**: - Find the total sales for each `ProductCategory` by aggregating the sales data. - Find the top 5 products based on total sales. 6. **Monthly Sales Trends**: - Resample the data to get the monthly total sales. - Plot the monthly sales trends. **Constraints:** - You can use any pandas functions or methods to accomplish these tasks. - Your solution should be efficient; avoid unnecessary loops or broadcasts. **Expected Input and Output Formats:** - Input: File paths to `sales_data.csv` and `product_data.csv`. - Output: Print the required results for each task and display the plot for the monthly sales trends. **Performance Requirements:** - The solution should handle large datasets efficiently. - Ensure your code is well-structured and includes comments for clarity. **Solution Template:** ```python import pandas as pd import matplotlib.pyplot as plt # Task 1: Load the data sales_data = pd.read_csv(\'sales_data.csv\') product_data = pd.read_csv(\'product_data.csv\') # Task 2: Clean the data Fill missing values and convert SaleDate to datetime # Add your code here # Task 3: Aggregate Sales Data Calculate TotalSales and group by SaleDate # Add your code here # Task 4: Merge DataFrames Merge sales_data with product_data # Add your code here # Task 5: Analyze Product Performance Calculate total sales for each ProductCategory and find top 5 products # Add your code here # Task 6: Monthly Sales Trends Resample sales data to monthly and plot # Add your code here # Show plot plt.show() ``` **Submit your solution by implementing the required tasks in the template provided.**","solution":"import pandas as pd import matplotlib.pyplot as plt def load_clean_data(sales_path, product_path): # Load the data sales_data = pd.read_csv(sales_path) product_data = pd.read_csv(product_path) # Clean the data # Handle missing values (impute with zero for Quantity and UnitPrice) sales_data[\'Quantity\'] = sales_data[\'Quantity\'].fillna(0) sales_data[\'UnitPrice\'] = sales_data[\'UnitPrice\'].fillna(0) # Ensure SaleDate is in datetime format sales_data[\'SaleDate\'] = pd.to_datetime(sales_data[\'SaleDate\']) return sales_data, product_data def aggregate_sales_data(sales_data): # Create a new column `TotalSales` sales_data[\'TotalSales\'] = sales_data[\'Quantity\'] * sales_data[\'UnitPrice\'] # Group by SaleDate and calculate the daily total sales daily_sales = sales_data.groupby(\'SaleDate\')[\'TotalSales\'].sum().reset_index() return daily_sales def merge_dataframes(sales_data, product_data): # Merge sales_data with product_data on ProductID complete_data = pd.merge(sales_data, product_data, on=\'ProductID\', how=\'left\') return complete_data def analyze_product_performance(complete_data): # Calculate total sales for each ProductCategory category_sales = complete_data.groupby(\'ProductCategory\')[\'TotalSales\'].sum().reset_index() # Find the top 5 products based on total sales top_products = complete_data.groupby(\'ProductName\')[\'TotalSales\'].sum().reset_index() top_products = top_products.sort_values(by=\'TotalSales\', ascending=False).head(5) return category_sales, top_products def plot_monthly_sales_trends(complete_data): # Resample data to get the monthly total sales monthly_sales = complete_data.resample(\'M\', on=\'SaleDate\')[\'TotalSales\'].sum().reset_index() # Plot the monthly sales trends plt.figure(figsize=(10, 5)) plt.plot(monthly_sales[\'SaleDate\'], monthly_sales[\'TotalSales\'], marker=\'o\') plt.title(\'Monthly Sales Trends\') plt.xlabel(\'Month\') plt.ylabel(\'Total Sales\') plt.grid(True) plt.show()"},{"question":"Coding Assessment Question # Objective Create a Python program using the `tkinter` package to develop a GUI application that allows the user to: 1. Select a directory. 2. Input a string value. 3. Save the input string to a text file within the selected directory. # Requirements - Implement a function `create_gui()` that initializes the main application window. - Inside the main application window, include two buttons: \\"Select Directory\\" and \\"Input String\\". - Use `tkinter.filedialog.askdirectory()` to let the user select a directory. - Use `tkinter.simpledialog.askstring()` to prompt the user to input a string. - Implement functionality to save the input string to a text file named `user_input.txt` in the selected directory when both inputs have been provided. - Display a message box to confirm the successful saving of the file using `tkinter.messagebox.showinfo()`. # Constraints - The program should not allow saving the file unless both the directory and the string have been provided. - Assume that the user will provide valid inputs. # Input and Output - **Input**: Directory path selected through a GUI dialog, string input provided through a GUI prompt. - **Output**: A text file named `user_input.txt` saved in the selected directory containing the provided string. # Function Signature ```python def create_gui(): pass ``` # Example Usage ```python # When the program runs, it will display a window with two buttons (Select Directory and Input String). # The user clicks \\"Select Directory\\" and chooses a directory. # The user clicks \\"Input String\\" and enters a string in the prompt. # The program saves the string in a text file `user_input.txt` within the selected directory and displays a confirmation message. # Example: # Directory selected: C:/Users/Example # String input: Hello, World! # Output: A file \'user_input.txt\' is created in the directory \'C:/Users/Example\', containing the text \'Hello, World!\'. ``` # Note - Make sure to handle the case where the user cancels the directory selection or string input, prompting them to complete both tasks before proceeding.","solution":"import tkinter as tk from tkinter import filedialog, simpledialog, messagebox import os def save_string_to_file(directory, input_string): Saves the given input string to a text file within the specified directory. if not directory or not input_string: raise ValueError(\\"Both directory and input string must be provided.\\") file_path = os.path.join(directory, \\"user_input.txt\\") with open(file_path, \'w\') as file: file.write(input_string) return file_path def create_gui(): Initializes the main application window for the GUI. def select_directory(): nonlocal selected_directory selected_directory = filedialog.askdirectory() def input_string(): nonlocal user_input user_input = simpledialog.askstring(\\"Input\\", \\"Please enter a string:\\") if user_input and selected_directory: try: save_string_to_file(selected_directory, user_input) messagebox.showinfo(\\"Success\\", \\"File saved successfully!\\") except ValueError as e: messagebox.showerror(\\"Error\\", str(e)) selected_directory = None user_input = None root = tk.Tk() root.title(\\"Directory and String Input App\\") select_directory_button = tk.Button(root, text=\\"Select Directory\\", command=select_directory) select_directory_button.pack(pady=10) input_string_button = tk.Button(root, text=\\"Input String\\", command=input_string) input_string_button.pack(pady=10) root.mainloop()"},{"question":"# Dynamic Class Creation and Utilization In this assessment, you will dynamically create and manipulate custom classes using Python\'s `types` module. Problem Statement: You are required to write a function `create_and_use_dynamic_class` that performs the following tasks: 1. **Creates a class dynamically**: - Name the class `DynamicClass`. - The class should inherit from a base class `BaseClass`. Implement `BaseClass` that has a method `base_method(self)` returning the string `\\"This is BaseClass\\"`. 2. **Adds methods to the `DynamicClass`**: - Using `types.new_class`, add a method `dynamic_method(self)` that returns `\\"This is DynamicClass\\"`. 3. **Prepares the class with a specific metaclass**: - Prepare the class using a custom metaclass `Meta` which prints `\\"Meta is being used\\"` whenever an instance of `DynamicClass` is created. 4. **Implements instance and class interaction**: - Ensure that `DynamicClass` has an instance method `instance_method(self)` that returns `\\"Instance Method Called\\"`. - Ensure that `DynamicClass` has a class method `class_method(cls)` that returns `\\"Class Method Called\\"`. 5. **Creates an instance of `DynamicClass` and demonstrates usage**: - Create an instance of `DynamicClass` named `dynamic_instance`. - Call `dynamic_method`, `instance_method` and `base_method` from this instance and return their results in a tuple. - Call `class_method` from the `DynamicClass` and include its result in the tuple as well. Constraints: - You should use the `types.new_class` method for creating `DynamicClass`. - Use `types.prepare_class` to calculate the appropriate metaclass (`Meta`) and the class namespace. - The class name should be `\\"DynamicClass\\"` and base class name should be `\\"BaseClass\\"`. Function Signature: ```python def create_and_use_dynamic_class(): # Your implementation here pass ``` Example Usage: ```python result = create_and_use_dynamic_class() print(result) # Example output: (\'This is DynamicClass\', \'Instance Method Called\', \'This is BaseClass\', \'Class Method Called\') ``` Performance Requirements: - The function should be efficient and not use unnecessary memory or CPU cycles. - Ensure the function works correctly with the constraints provided. Good luck, and happy coding!","solution":"import types class Meta(type): def __call__(cls, *args, **kwargs): print(\\"Meta is being used\\") return super().__call__(*args, **kwargs) class BaseClass: def base_method(self): return \\"This is BaseClass\\" def create_and_use_dynamic_class(): def dynamic_method(self): return \\"This is DynamicClass\\" def instance_method(self): return \\"Instance Method Called\\" @classmethod def class_method(cls): return \\"Class Method Called\\" DynamicClass = types.new_class( \\"DynamicClass\\", (BaseClass,), {\\"metaclass\\": Meta}, lambda ns: ns.update({ \\"dynamic_method\\": dynamic_method, \\"instance_method\\": instance_method, \\"class_method\\": class_method }) ) dynamic_instance = DynamicClass() result = ( dynamic_instance.dynamic_method(), dynamic_instance.instance_method(), dynamic_instance.base_method(), DynamicClass.class_method() ) return result"},{"question":"**Question: Directory Management and File Operations with the `os` Module** You are tasked with writing a Python script that performs the following operations using the `os` module functions: 1. Creates a directory structure `project_dir/sub_dir`. 2. Inside `sub_dir`, creates a text file named `example.txt` and writes some content to it. 3. Reads the content of `example.txt` and prints it to the console. 4. Renames `example.txt` to `example_renamed.txt`. 5. Deletes the file `example_renamed.txt`. 6. Deletes the directory `sub_dir` and its parent `project_dir`. Implement the functions necessary to achieve the above operations. Your script should handle exceptions gracefully and print appropriate error messages if any operation fails. **Input and Output Expectations:** - The script does not require any input from the user. - The output should be the content of `example.txt` printed to the console and confirmation messages for each of the operations performed. **Constraints:** - Use only the `os` module functions for file and directory manipulations. - Ensure proper handling of file and directory paths to make the script portable across different operating systems. - Handle and report errors appropriately. **Function Signature:** ```python def manage_directories_and_files(): # Create project_dir and sub_dir # Create and write to example.txt # Read and print content of example.txt # Rename example.txt to example_renamed.txt # Delete example_renamed.txt # Remove sub_dir and project_dir pass ``` **Example:** If the content written to `example.txt` is \\"Hello, World!\\", the output should be: ``` Hello, World! Directory project_dir created. Directory sub_dir created. File example.txt created and written to. Content of example.txt: Hello, World! File example.txt renamed to example_renamed.txt. File example_renamed.txt deleted. Directory sub_dir deleted. Directory project_dir deleted. ```","solution":"import os def manage_directories_and_files(): try: # Create project_dir and sub_dir os.makedirs(\'project_dir/sub_dir\', exist_ok=True) print(\\"Directory project_dir created.\\") print(\\"Directory sub_dir created.\\") file_path = os.path.join(\'project_dir\', \'sub_dir\', \'example.txt\') new_file_path = os.path.join(\'project_dir\', \'sub_dir\', \'example_renamed.txt\') # Create and write to example.txt with open(file_path, \'w\') as file: file.write(\\"Hello, World!\\") print(\\"File example.txt created and written to.\\") # Read and print content of example.txt with open(file_path, \'r\') as file: content = file.read() print(f\\"Content of example.txt: {content}\\") # Rename example.txt to example_renamed.txt os.rename(file_path, new_file_path) print(f\\"File example.txt renamed to example_renamed.txt.\\") # Delete example_renamed.txt os.remove(new_file_path) print(f\\"File example_renamed.txt deleted.\\") # Remove sub_dir and project_dir os.rmdir(os.path.join(\'project_dir\', \'sub_dir\')) os.rmdir(\'project_dir\') print(\\"Directory sub_dir deleted.\\") print(\\"Directory project_dir deleted.\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Objective:** Evaluate the student\'s understanding of using `torch.cond` for implementing data-dependent control flow in PyTorch. **Problem Statement:** You are provided with a dataset containing various types of 2D tensors. You need to build a PyTorch model that processes these tensors differently based on certain conditions. Specifically, you should implement a model in which the behavior changes according to the sum and shape of the tensor. # Requirements 1. Implement a PyTorch module named `AdaptiveProcessingModule` that extends `torch.nn.Module`. 2. Inside this module, you should use `torch.cond` to switch between two different processing functions: - If the sum of all elements in the tensor is greater than 10, apply a function named `high_sum_fn`. - Otherwise, apply a function named `low_sum_fn`. 3. Additionally, if the tensor has more than 5 columns, the function `high_sum_fn` should be further modified to include a column-wise maximum operation on the tensor. # Function Specifications `high_sum_fn` - **Input:** A tensor `x` of at least two dimensions. - **Operation:** Compute the square of each element in the tensor. - **If the number of columns in the tensor (number of elements along the second dimension) is greater than 5, additionally compute the maximum value in each column and add it to the resultant squared tensor.** `low_sum_fn` - **Input:** A tensor `x` of at least two dimensions. - **Operation:** Compute the natural logarithm (log) of each element in the tensor (use `torch.log`). Note that input values will be positive and greater than zero. # Model Class Skeleton You are required to complete the `forward` method of the class below: ```python import torch def high_sum_fn(x: torch.Tensor) -> torch.Tensor: squared = x ** 2 if x.size(1) > 5: col_max = torch.max(x, dim=0).values squared = squared + col_max return squared def low_sum_fn(x: torch.Tensor) -> torch.Tensor: return torch.log(x) class AdaptiveProcessingModule(torch.nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: return torch.cond( x.sum() > 10, high_sum_fn, low_sum_fn, (x,)) ``` # Constraints: - Do not use any external libraries except for PyTorch. - Assume all input tensors are valid and have positive values. # Example: ```python # Example usage model = AdaptiveProcessingModule() input_tensor1 = torch.tensor([[1., 2.], [3., 4.]]) input_tensor2 = torch.tensor([[1., 2., 3., 4., 5., 6.], [1., 2., 3., 4., 5., 6.]]) output1 = model(input_tensor1) output2 = model(input_tensor2) print(output1) print(output2) ``` - For `input_tensor1`, because the sum (10) is not greater than 10, the `low_sum_fn` should be applied. - For `input_tensor2`, because the sum (42) is greater than 10 and the number of columns is 6, the `high_sum_fn` with the additional column-wise max operation should be applied. # Evaluation Criteria: - Correctness of the implementation. - Proper use of `torch.cond` to execute data-dependent control flow. - Handling tensor shapes correctly within the specified conditions.","solution":"import torch def high_sum_fn(x: torch.Tensor) -> torch.Tensor: squared = x ** 2 if x.size(1) > 5: col_max = torch.max(x, dim=0).values squared = squared + col_max return squared def low_sum_fn(x: torch.Tensor) -> torch.Tensor: return torch.log(x) class AdaptiveProcessingModule(torch.nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: # Using torch.where instead of torch.cond as PyTorch does not have a torch.cond condition = (x.sum() > 10) return torch.where(condition, high_sum_fn(x), low_sum_fn(x))"},{"question":"Problem Statement You are asked to develop a Python function that processes a list of text strings. Your function should utilize functional programming concepts combined with regular expressions to filter, transform, and sort the input data. The function should operate as follows: 1. **Filter**: Use regular expressions to filter out the strings that do not match a specified pattern. 2. **Transform**: Convert the text strings that match the pattern to a specific format using a transformation function. 3. **Sort**: Sort the resulting list of transformed strings in ascending order. Requirements - The filtering should use a regular expression pattern provided as an input to the function. - The transformation should be a function that converts each matching string to a desired format. This function should be passed as an argument. - The sorting should be based on the alphanumeric order of the transformed strings. Function Signature ```python def process_strings(strings: list[str], pattern: str, transform: callable) -> list[str]: Processes a list of text strings by filtering, transforming, and sorting. Parameters: - strings (list[str]): List of input text strings. - pattern (str): Regular expression pattern to filter strings. - transform (callable): A transformation function to apply to each matching string. Returns: - list[str]: The list of transformed and sorted strings. ``` Input - `strings`: A list of text strings. - `pattern`: A regular expression pattern as a string. - `transform`: A function that takes a string and returns a transformed string. Output - A list of transformed and sorted strings. Constraints - Assume all inputs are valid. - The input list `strings` can contain up to 10,000 strings. - Each string in the list can have a maximum length of 100 characters. - The regular expression pattern and the transformation function will be provided by the user. Example ```python import re def transform_function(s: str) -> str: return s.upper() strings = [\\"abc123\\", \\"def456\\", \\"ghi789\\", \\"abc456\\"] pattern = r\'^abc.*\' print(process_strings(strings, pattern, transform_function)) # Expected Output: [\'ABC123\', \'ABC456\'] ``` Explanation - The function filters the `strings` list to include only those that start with \\"abc\\". - Each of the matching strings is transformed to uppercase using `transform_function`. - The resulting list of transformed strings is then sorted in ascending order.","solution":"import re def process_strings(strings: list[str], pattern: str, transform: callable) -> list[str]: Processes a list of text strings by filtering, transforming, and sorting. Parameters: - strings (list[str]): List of input text strings. - pattern (str): Regular expression pattern to filter strings. - transform (callable): A transformation function to apply to each matching string. Returns: - list[str]: The list of transformed and sorted strings. regex = re.compile(pattern) filtered_strings = filter(lambda s: regex.match(s), strings) transformed_strings = map(transform, filtered_strings) sorted_strings = sorted(transformed_strings) return sorted_strings"},{"question":"# Problem Description You are provided with a log file that contains repetitive patterns, and your task is to create a system that compresses and decompresses this data efficiently using the `zlib` library in Python. Your solution should demonstrate the use of both basic and advanced features of the `zlib` module, ensuring that the data remains intact after decompression. # Requirements 1. **Function `compress_log_data(data: bytes) -> bytes`**: - Compress the provided log data using `zlib.compressobj`. - Use the highest compression level to ensure maximum data reduction. - Return the compressed byte string. 2. **Function `decompress_log_data(data: bytes) -> bytes`**: - Decompress the provided log data using `zlib.decompressobj`. - Ensure the output matches the original uncompressed data. - Return the decompressed byte string. 3. **Function `verify_data_integrity(original: bytes, decompressed: bytes) -> bool`**: - Verify that the decompressed data matches the original data by computing checksums (`adler32`). - Return `True` if checksums match, otherwise `False`. # Input and Output Formats - **Input**: A byte string representing the log data, which can be large and contains repetitive patterns. - **Output**: Compressed byte string for `compress_log_data`, decompressed byte string for `decompress_log_data`, and a boolean value for `verify_data_integrity`. # Constraints - You must use `zlib.compressobj` and `zlib.decompressobj` for compression and decompression. - Employ the highest compression level for maximum data reduction. - Assume that the log data fits into memory for this exercise. # Example ```python def compress_log_data(data: bytes) -> bytes: # Your implementation here def decompress_log_data(data: bytes) -> bytes: # Your implementation here def verify_data_integrity(original: bytes, decompressed: bytes) -> bool: # Your implementation here # Example usage original_data = b\\"This is a test log file. \\" * 1000 # Repetitive log data for testing compressed_data = compress_log_data(original_data) print(f\\"Compressed Data Length: {len(compressed_data)}\\") decompressed_data = decompress_log_data(compressed_data) print(f\\"Decompressed Data Matches Original: {verify_data_integrity(original_data, decompressed_data)}\\") ```","solution":"import zlib def compress_log_data(data: bytes) -> bytes: Compress the provided log data using zlib.compressobj with the highest compression level. compressor = zlib.compressobj(level=zlib.Z_BEST_COMPRESSION) compressed_data = compressor.compress(data) + compressor.flush() return compressed_data def decompress_log_data(data: bytes) -> bytes: Decompress the provided log data using zlib.decompressobj. decompressor = zlib.decompressobj() decompressed_data = decompressor.decompress(data) + decompressor.flush() return decompressed_data def verify_data_integrity(original: bytes, decompressed: bytes) -> bool: Verify that the decompressed data matches the original data by comparing checksums. return zlib.adler32(original) == zlib.adler32(decompressed)"},{"question":"**Question:** You are provided with two datasets `penguins` and `flights` from seaborn\'s library. These datasets contain information about penguin species and flight passenger data from the year 1960, respectively. Your task is to create a composite plot that demonstrates the following: 1. A bar plot showing the average body mass of different penguin species differentiated by their sex. Include error bars representing the standard deviation of body mass within species and sex groups. 2. A histogram displaying the count of flights per month in the year 1960. 3. Combine these two plots into a single figure with two subplots arranged vertically. # Requirements: 1. Implement a function `create_composite_plot()` that does not take any parameters and saves the resulting figure to a file named `composite_plot.png`. 2. Use appropriate seaborn functions and objects to create the plots. 3. Ensure the bars in the bar plot are dodged by sex. 4. Provide appropriate labels and titles for the plots. # Input: There are no inputs to your function. # Output: The function should save a figure containing two subplots to the file `composite_plot.png`. # Constraints: - You must use seaborn\'s high-level and objects APIs to create the plots. # Example: Your figure should look similar to the following once saved: - Bar plot: - x-axis: `species` - y-axis: `average body mass (g)` - bars differentiated by `sex`, with error bars - Histogram: - x-axis: `month` - y-axis: `number of flights` # Notes: - Make sure to arrange the subplots vertically. - Label all axes and include titles for both subplots. - Save the final figure as `composite_plot.png`. ```python def create_composite_plot(): # Your code goes here pass # Example function call create_composite_plot() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_composite_plot(): # Load the datasets penguins = sns.load_dataset(\\"penguins\\") flights = sns.load_dataset(\\"flights\\") # Create a figure with two subplots arranged vertically fig, axes = plt.subplots(2, 1, figsize=(12, 15)) # Create the bar plot sns.barplot( data=penguins, x=\\"species\\", y=\\"body_mass_g\\", hue=\\"sex\\", ci=\\"sd\\", ax=axes[0] ) axes[0].set_title(\\"Average Body Mass by Penguin Species and Sex\\") axes[0].set_xlabel(\\"Species\\") axes[0].set_ylabel(\\"Average Body Mass (g)\\") # Create the histogram flight_counts = flights[flights[\\"year\\"] == 1960].groupby(\\"month\\").size().reset_index(name=\'count\') sns.barplot( data=flight_counts, x=\\"month\\", y=\\"count\\", ax=axes[1] ) axes[1].set_title(\\"Number of Flights per Month in 1960\\") axes[1].set_xlabel(\\"Month\\") axes[1].set_ylabel(\\"Number of Flights\\") # Save the combined plot to a file plt.tight_layout() plt.savefig(\\"composite_plot.png\\") # Example function call create_composite_plot()"},{"question":"# Question Objective: You are tasked to write a Python function that verifies if a provided Python script adheres to certain lexical rules described in Python 3.10 documentation. Your function should inspect the script to ensure it: 1. Uses correct indentation without mixing tabs and spaces. 2. Joins lines correctly using both explicit and implicit line joining methods. 3. Handles comments appropriately without breaking logical lines. 4. Defines an encoding declaration if it is specified on the first or second line. 5. Has correctly formatted string literals and ensures escape sequences are properly used. Function Signature: ```python def verify_python_script(script: str) -> bool: pass ``` Input: - `script`: A string representing the entire content of a Python script. Output: - Returns `True` if the script adheres to the aforementioned lexical rules and `False` otherwise. Constraints: - The script contains no more than 1000 lines. - Each line in the script contains no more than 1000 characters. Examples: ```python script = \'\'\' # -*- coding: utf-8 -*- def example_function(): if True: print(\\"Hello, world!\\") if False: return False else: return True return False \'\'\' print(verify_python_script(script)) # Output: True script = \'\'\' def example_function(): if True: ttprint(\\"Hello, world!\\") tif False: return False else: return True return False \'\'\' print(verify_python_script(script)) # Output: False # Mixed tabs and spaces ``` Notes: - The function should handle various edge cases such as scripts with only comments, different line-ending styles (e.g. UNIX, Windows, Mac), correct and incorrect indentation, and properly and improperly formatted string literals. - Emphasize lexical-specific checks rather than functional or semantic checks. - The function should be efficient to handle up to the maximum constraints smoothly. Tips: - Leverage Python\'s built-in `tokenize` module to assist in analyzing the script. - Carefully handle edge cases such as mixed use of tabs and spaces, improper line joining, incorrect encoding declarations, and malformed string literals.","solution":"import tokenize from io import BytesIO def verify_python_script(script: str) -> bool: try: lines = script.splitlines(keepends=True) tokens = list(tokenize.tokenize(BytesIO(script.encode(\'utf-8\')).readline)) indent_levels = [] for token in tokens: token_type = token.type token_string = token.string # Check for mixed tabs and spaces in indentation if token_type == tokenize.INDENT: if \' \' in token_string and \'t\' in token_string: return False indent_levels.append(token_string) elif token_type == tokenize.DEDENT: if indent_levels: indent_levels.pop() # Check for explicit line joining using backslash elif token_type == tokenize.STRING and token_string.endswith(\'n\'): return True # Check encoding declaration on the first or second line elif token_type == tokenize.COMMENT: if token.start[0] in {1, 2} and \'coding\' in token_string: if not token_string.startswith(\'# -*- coding:\'): return False # Check for correct string literal formatting elif token_type == tokenize.ERRORTOKEN: return False # Implicit line joining handled by Python parser return True except (tokenize.TokenError, SyntaxError): return False"},{"question":"Objective Demonstrate your understanding and proficiency with the `seaborn` library by creating a series of visualizations using categorical plots. Problem Statement You are provided with the \\"tips\\" dataset which can be loaded using `sns.load_dataset(\\"tips\\")`. Your task is to write a function `plot_tips_data()` that creates specific plots based on the given instructions. Follow the steps mentioned below to create a series of visualizations: 1. **Strip Plot**: Create a strip plot showing the relationship between the `total_bill` and `day` columns. Customize the plot by setting the plot style to `\\"whitegrid\\"`. 2. **Box Plot**: Create a box plot to visualize the distribution of `total_bill` across different days. Use hue to differentiate based on the `sex` column. Customize the box plot by setting the width of the boxes to 0.5. 3. **Violin Plot**: Create a violin plot to show the distribution of `total_bill` across different days. Use hue to differentiate based on the `sex` column and split the violins by gender. Adjust the violin plot by setting `bw_adjust` to 0.75. 4. **Subplots**: Create a series of bar plots to show the mean `total_bill` for each day, creating separate subplots for each smoker category (`smoker`). Set the height of each subplot to 4 and the aspect ratio to 0.8. Customize the plots by setting y-axis labels to \\"Mean Total Bill\\" and x-axis to \\"Days\\". 5. **Layered Plot**: Create a layered plot with a violin plot showing the distribution of `tip` across different days (with `hue` as `sex`, `bw_adjust` set to 0.5, and `split=True`). Overlay this with a swarm plot showing the same data (with `size` set to 3). Additionally, customize the above plots with appropriate titles and labels wherever necessary. Function Signature ```python def plot_tips_data(): pass ``` # Constraints - Use only the seaborn library for creating the plots. - You may add additional keyword arguments to improve the visualizations based on your preference. Expected Outputs The function should output the specified plots as described.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_tips_data(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Strip Plot sns.set(style=\\"whitegrid\\") plt.figure(figsize=(8, 6)) sns.stripplot(x=\\"day\\", y=\\"total_bill\\", data=tips) plt.title(\\"Strip Plot of Total Bill by Day\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Total Bill\\") plt.show() # Box Plot plt.figure(figsize=(8, 6)) sns.boxplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", data=tips, width=0.5) plt.title(\\"Box Plot of Total Bill by Day and Sex\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Total Bill\\") plt.show() # Violin Plot plt.figure(figsize=(8, 6)) sns.violinplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", data=tips, split=True, bw=0.75) plt.title(\\"Violin Plot of Total Bill by Day and Sex\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Total Bill\\") plt.show() # Subplots for Bar Plots g = sns.catplot( x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", data=tips, kind=\\"bar\\", height=4, aspect=0.8, col=\\"smoker\\", ci=None ) g.set_axis_labels(\\"Days\\", \\"Mean Total Bill\\") g.set_titles(\\"Mean Total Bill by Day for {col_name} Smokers\\") g.fig.suptitle(\\"Mean Total Bill by Day and Smoker Status\\", y=1.03) plt.show() # Layered Plot plt.figure(figsize=(8, 6)) sns.violinplot(x=\\"day\\", y=\\"tip\\", hue=\\"sex\\", data=tips, split=True, bw=0.5, inner=None) sns.swarmplot(x=\\"day\\", y=\\"tip\\", hue=\\"sex\\", data=tips, dodge=True, size=3) plt.title(\\"Layered Plot of Tip by Day and Sex\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Tip\\") plt.legend(loc=\'upper left\', title=\\"Sex\\") plt.show()"},{"question":"# SQLite Database Management with Custom Serialization Objective: To assess the understanding of Python\'s data persistence modules, particularly `sqlite3` in conjunction with `pickle`. Task: You are required to implement a function `manage_custom_db` that performs a series of operations on an SQLite database. This function will take a list of custom objects, serialize them using `pickle`, store them in an SQLite database, and then retrieve and deserialize them. Requirements: 1. **Custom Object Definition:** - Define a class `CustomObject` with at least three attributes of different types (e.g., string, integer, list). 2. **Function Implementation:** - Implement the function `manage_custom_db(objects: List[CustomObject]) -> List[CustomObject]` that: 1. Stores objects in an SQLite database. 2. Retrieves and deserializes these objects. 3. Returns the list of deserialized objects. 3. **Steps for Function:** - Create an SQLite database in memory (using `sqlite3.connect(\':memory:\')`). - Create a table to store the serialized objects. - Serialize the custom objects using `pickle` and store them in the database. - Retrieve the serialized objects from the database. - Deserialize the objects using `pickle` and return them. Input: - A list of `CustomObject` instances. Output: - A list of deserialized `CustomObject` instances retrieved from the database. Constraints: - Each `CustomObject`\'s string attribute should not exceed 255 characters. - You should handle any exceptions that might occur during database operations appropriately. Example Usage: ```python import pickle import sqlite3 class CustomObject: def __init__(self, name: str, number: int, items: list): self.name = name self.number = number self.items = items def manage_custom_db(objects: List[CustomObject]) -> List[CustomObject]: # Implementation here # Example objects obj1 = CustomObject(\\"Alice\\", 42, [1, 2, 3]) obj2 = CustomObject(\\"Bob\\", 35, [\\"a\\", \\"b\\", \\"c\\"]) output_objects = manage_custom_db([obj1, obj2]) for obj in output_objects: print(obj.name, obj.number, obj.items) ``` The above outputs should match the input objects\' attributes, demonstrating successful serialization, storage, retrieval, and deserialization. Notes: - Ensure your solution is efficient and handles any potential errors (e.g., database connection issues, serialization errors). - Add docstrings to your functions and classes for better clarity.","solution":"import pickle import sqlite3 from typing import List class CustomObject: def __init__(self, name: str, number: int, items: list): self.name = name self.number = number self.items = items def manage_custom_db(objects: List[CustomObject]) -> List[CustomObject]: # Create an SQLite database in memory conn = sqlite3.connect(\':memory:\') cursor = conn.cursor() # Create a table to store the serialized objects cursor.execute(\'\'\' CREATE TABLE custom_objects ( id INTEGER PRIMARY KEY AUTOINCREMENT, obj BLOB NOT NULL ) \'\'\') # Insert serialized custom objects into the database for obj in objects: serialized_obj = pickle.dumps(obj) cursor.execute(\'INSERT INTO custom_objects (obj) VALUES (?)\', (serialized_obj,)) # Commit the transaction conn.commit() # Retrieve serialized objects from the database cursor.execute(\'SELECT obj FROM custom_objects\') rows = cursor.fetchall() # Deserialize the objects deserialized_objects = [pickle.loads(row[0]) for row in rows] # Close the connection conn.close() return deserialized_objects"},{"question":"**Problem Statement:** You are provided a dataset which records the monthly average temperatures of different cities over several years. Your task is to visualize this data using the seaborn library in both long-form and wide-form formats and summarize the insights. # Dataset The dataset `temperature.csv` has the following columns: - **city**: Name of the city - **year**: Year of the recorded temperature - **month**: Month of the recorded temperature - **temperature**: Average temperature for that month # Requirements: 1. **Data Preparation:** - Load the dataset into a pandas DataFrame. - Convert the dataset into wide-form where rows represent years, columns represent months, and cells represent the average temperatures for a specific city. 2. **Visualizations:** - Using long-form data, draw a line plot that visualizes the monthly average temperatures for each city over the years. - Using wide-form data, draw a heatmap that displays the monthly average temperatures for a particular city over the years. # Implementation Details: * **Input:** The file path to the dataset (e.g., \\"temperature.csv\\"). * **Output:** 1. Two seaborn visualizations: - A line plot for the long-form data. - A heatmap for the wide-form data. 2. A markdown cell summarizing the insights obtained from the visualizations. # Points to Consider: * Handle any missing or NaN values appropriately when processing the data. * Label all axes and titles clearly in the plots. * Ensure the code is modular and functions are used where appropriate. * Provide detailed comments for clarity. # Constraints: * Use seaborn, pandas, and numpy libraries for data handling and visualization. * The dataset can be assumed to fit into memory. # Example: **Your task is to implement the following function:** ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_temperature_data(file_path): # Step 1: Load the dataset data = pd.read_csv(file_path) # Step 2: Convert long-form data into wide-form data_wide = data.pivot(index=\'year\', columns=\'month\', values=\'temperature\') # Step 3: Line Plot using long-form data plt.figure(figsize=(15, 8)) sns.lineplot(data=data, x=\'year\', y=\'temperature\', hue=\'city\') plt.title(\'Monthly Average Temperatures for Each City Over Years (Long-form)\') plt.xlabel(\'Year\') plt.ylabel(\'Average Temperature\') plt.legend(title=\'City\') plt.show() # Step 4: Heatmap using wide-form data (for one example city) city = \'Example_City\' # Change this to any city you want to visualize data_city = data[data[\'city\'] == city].pivot(index=\'year\', columns=\'month\', values=\'temperature\') plt.figure(figsize=(12, 6)) sns.heatmap(data_city, annot=True, cmap=\'coolwarm\') plt.title(f\'Average Monthly Temperature Heatmap for {city} (Wide-form)\') plt.xlabel(\'Month\') plt.ylabel(\'Year\') plt.show() # Insights insights = Insights: 1. The line plot provides a clear trend of how temperatures change over the years for each city. 2. The heatmap illustrates the temperature distribution across different months and years for a particular city. 3. We can identify seasonal patterns and anomalies in the temperature records. print(insights) # Provided an example CSV \'temperature.csv\', this function will create the required visualizations. file_path = \'temperature.csv\' visualize_temperature_data(file_path) ``` # Submission: Submit your `.ipynb` file containing your solution and the generated plots. # Notes: * Ensure to test your function with diverse data scenarios if possible. * Explain any assumptions or design choices you made during the implementation in markdown cells.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_temperature_data(file_path): Visualizes monthly average temperatures for each city over the years using line plot and heatmap. Parameters: file_path (str): Path to the CSV file containing the temperature data Returns: str: Summarized insights obtained from the visualizations # Step 1: Load the dataset data = pd.read_csv(file_path) # Step 2: Convert long-form data into wide-form data_wide = data.pivot_table(index=\'year\', columns=\'month\', values=\'temperature\') # Step 3: Line Plot using long-form data plt.figure(figsize=(15, 8)) sns.lineplot(data=data, x=\'year\', y=\'temperature\', hue=\'city\') plt.title(\'Monthly Average Temperatures for Each City Over Years (Long-form)\') plt.xlabel(\'Year\') plt.ylabel(\'Average Temperature (°C)\') plt.legend(title=\'City\') plt.show() # Step 4: Heatmap using wide-form data (for one example city) example_city = data[\'city\'].unique()[0] # Taking the first city as an example data_city = data[data[\'city\'] == example_city].pivot_table(index=\'year\', columns=\'month\', values=\'temperature\') plt.figure(figsize=(12, 6)) sns.heatmap(data_city, annot=True, cmap=\'coolwarm\') plt.title(f\'Average Monthly Temperature Heatmap for {example_city} (Wide-form)\') plt.xlabel(\'Month\') plt.ylabel(\'Year\') plt.show() # Insights insights = Insights: 1. The line plot provides a clear trend of how temperatures change over the years for each city. 2. The heatmap illustrates the temperature distribution across different months and years for a particular city. 3. We can identify seasonal patterns and potential anomalies in the temperature records. return insights"},{"question":"# Seaborn Line Plot Challenge **Objective:** Assess students\' understanding of seaborn by requiring them to load a dataset, manipulate it, and create a complex customized line plot. **Problem Statement:** You are provided with the `fmri` dataset from seaborn which contains brain activity measurements at different time points. Using this dataset, you need to perform the following tasks: 1. **Load the `fmri` dataset** using seaborn. 2. **Filter the dataset** to include only observations where the `region` is \\"frontal\\". 3. **Create a pivot table** to transform the dataset into wide-form with `timepoint` as the index and `event` as the columns, taking the average `signal` for each combination. 4. Using seaborn, **plot a line plot**: - The x-axis should represent `timepoint`. - The y-axis should represent the average `signal`. - Each `event` should have a separate line and should be distinguished using different styles and markers. - Add a title and appropriate labels to the axes. - Use a divergent palette for the hue (`event`) to enhance the visual distinction. 5. Finally, **display the plot** in an appropriate size. **Expected Input and Output Formats:** *Input: None (the dataset will be loaded internally)* *Output: A seaborn plot displayed inline in a Jupyter notebook.* **Constraints:** - Ensure that the plot is clearly legible. - Handle data correctly to ensure no errors in plotting. **Performance Requirements:** - The plotting should be efficient even with the provided data size. ```python # Solution Template import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def load_and_plot(): # Load dataset fmri = sns.load_dataset(\\"fmri\\") # Filter dataset to include only \'frontal\' region observations fmri_filtered = fmri.query(\\"region == \'frontal\'\\") # Create pivot table to transform to wide-form data fmri_pivot = fmri_filtered.pivot_table(index=\\"timepoint\\", columns=\\"event\\", values=\\"signal\\", aggfunc=\\"mean\\") # Plot the line plot sns.set_theme() plt.figure(figsize=(12, 6)) sns.lineplot(data=fmri_pivot, markers=True, dashes=False, palette=\\"coolwarm\\") # Customize the plot plt.title(\\"Average Brain Signal in Frontal Region Over Time\\") plt.xlabel(\\"Timepoint\\") plt.ylabel(\\"Average Signal\\") plt.legend(title=\\"Event\\") plt.show() # Call the function to execute the task load_and_plot() ``` **Notes:** - Make sure to handle any missing values in the dataset appropriately. - The appearance of the plot counts towards the assessment; ensure it is visually appealing and easily interpretable.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def load_and_plot(): # Load dataset fmri = sns.load_dataset(\\"fmri\\") # Filter dataset to include only \'frontal\' region observations fmri_filtered = fmri.query(\\"region == \'frontal\'\\") # Create pivot table to transform to wide-form data fmri_pivot = fmri_filtered.pivot_table(index=\\"timepoint\\", columns=\\"event\\", values=\\"signal\\", aggfunc=\\"mean\\") # Plot the line plot sns.set_theme() plt.figure(figsize=(12, 6)) sns.lineplot(data=fmri_pivot, markers=True, dashes=False, palette=\\"coolwarm\\") # Customize the plot plt.title(\\"Average Brain Signal in Frontal Region Over Time\\") plt.xlabel(\\"Timepoint\\") plt.ylabel(\\"Average Signal\\") plt.legend(title=\\"Event\\") plt.show() # Call the function to execute the task load_and_plot()"},{"question":"**Configuration File Parser Challenge** The goal of this challenge is to test your ability to work with configuration files using Python\'s `configparser` module. # Problem Statement You are given a configuration file containing settings for an application. You need to perform various operations such as retrieving values, handling fallback values, and updating configurations. # Configuration File (config.ini) ```ini [DEFAULT] ServerAliveInterval = 45 Compression = yes CompressionLevel = 9 SendEnv = LC_ALL, LANG [bitbucket.org] User = hg [topsecret.server.com] Port = 50022 ForwardX11 = no ``` # Tasks 1. **Retrieve Configuration Values**: Write a function `get_config_value` that takes two parameters: `section_name` and `option_name`. This function should return the value of the specified option from the specified section. If the section or the option does not exist, it should return `None`. 2. **Get Fallback Value**: Write a function `get_fallback_value` that takes three parameters: `section_name`, `option_name`, and `fallback_value`. This function should return the value of the specified option from the specified section. If the option does not exist, return the fallback value provided. 3. **Update Configuration**: Write a function `update_config` that takes three parameters: `section_name`, `option_name`, and `new_value`. This function should update the specified option with the new value in the provided configuration file. # Input and Output Formats - For `get_config_value`: - Input: `section_name` (string), `option_name` (string) - Output: Value of the option or `None` if the section or option doesn\'t exist. - For `get_fallback_value`: - Input: `section_name` (string), `option_name` (string), `fallback_value` (string) - Output: Value of the option or the provided fallback value. - For `update_config`: - Input: `section_name` (string), `option_name` (string), `new_value` (string) - Output: The function should have no return value, but the configuration file should be updated accordingly. # Constraints - Use Python\'s `configparser` module. - Handle any exceptions arising from missing sections or options gracefully. - Update operations should persist in the file. # Example Usage ```python # Assuming the above config.ini content config_file = \\"config.ini\\" # Initialize ConfigParser config = configparser.ConfigParser() config.read(config_file) # Sample function implementations def get_config_value(section_name, option_name): try: return config.get(section_name, option_name) except (configparser.NoSectionError, configparser.NoOptionError): return None def get_fallback_value(section_name, option_name, fallback_value): return config.get(section_name, option_name, fallback=fallback_value) def update_config(section_name, option_name, new_value): if not config.has_section(section_name): config.add_section(section_name) config.set(section_name, option_name, new_value) with open(config_file, \'w\') as configfile: config.write(configfile) # Example function calls print(get_config_value(\\"bitbucket.org\\", \\"User\\")) # Output: hg print(get_fallback_value(\\"topsecret.server.com\\", \\"ForwardX11\\", \\"yes\\")) # Output: no update_config(\\"topsecret.server.com\\", \\"Port\\", \\"65000\\") ``` You are expected to use the provided functions and test with the sample `config.ini` file content. Ensure your solution handles all edge cases gracefully and updates the file accordingly.","solution":"import configparser def get_config_value(config, section_name, option_name): try: return config.get(section_name, option_name) except (configparser.NoSectionError, configparser.NoOptionError): return None def get_fallback_value(config, section_name, option_name, fallback_value): return config.get(section_name, option_name, fallback=fallback_value) def update_config(config, config_file, section_name, option_name, new_value): if not config.has_section(section_name): config.add_section(section_name) config.set(section_name, option_name, new_value) with open(config_file, \'w\') as configfile: config.write(configfile) # Example of how to set up and use the configparser to read the configuration def read_config_file(config_file): config = configparser.ConfigParser() config.read(config_file) return config # Example file read config_file = \\"config.ini\\" config = read_config_file(config_file)"},{"question":"# Boolean Handling in Python C API You have been provided with an overview of how Boolean objects are handled in the Python C API. Your task is to write a Python function that internally mimics some of the boolean-handling functionalities described. Specifically, you need to implement a function that: 1. Receives an integer input and returns the boolean equivalent in Python, while also demonstrating the conversion process. 2. Checks if the input is a boolean and returns a tuple containing the original input and a boolean indicating whether it is a boolean type or not. Your solution must include: - A function `convert_and_check_boolean(value)` that takes a single integer input. - Inside this function, the conversion of the integer to a boolean using a custom method, not simply using the Python `bool()` function. - Verification of whether the input is a boolean using a custom check function, not using `isinstance()` or similar built-ins. **Function Signature**: ```python def convert_and_check_boolean(value: int) -> tuple: pass ``` **Input**: - `value` (int): An integer value which needs to be converted and checked. **Output**: - Returns a tuple where: - The first element is the boolean equivalent of the integer. - The second element is `True` if the input is a boolean, and `False` otherwise. **Constraints**: - You should not use Python\'s built-in `bool()`, `isinstance()`, or any other helper functions that directly perform type checking or conversion. - The solution should instead demonstrate understanding the underlying principles by manually handling it. **Example**: ```python convert_and_check_boolean(1) # Expected Output: (True, False) convert_and_check_boolean(0) # Expected Output: (False, False) convert_and_check_boolean(True) # Expected Output: (True, True) ```","solution":"def convert_and_check_boolean(value: int) -> tuple: Converts an integer to its boolean equivalent and checks if the input is a boolean. Parameters: value (int): The input value to be converted and checked. Returns: tuple: A tuple containing the boolean equivalent of the input and a boolean indicating if the input is a boolean. # Convert integer to boolean boolean_equivalent = False if value == 0 else True # Check if the input is a boolean type is_boolean = (value is True) or (value is False) return (boolean_equivalent, is_boolean)"},{"question":"**Pandas Copy-on-Write Mechanism** # Background: The Copy-on-Write (CoW) mechanism in pandas ensures that any DataFrame or Series derived from another behaves as a copy, preventing changes in one object from affecting another inadvertently. This mechanism enhances predictability and performance. # Task: You are provided with a DataFrame containing employee data. You need to perform a series of operations adhering to the CoW principles. # Input: 1. A DataFrame `df` with the following structure: ```python import pandas as pd data = { \\"EmployeeID\\": [1001, 1002, 1003, 1004, 1005], \\"Name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\", \\"Eve\\"], \\"Age\\": [34, 45, 23, 40, 29], \\"Department\\": [\\"HR\\", \\"Finance\\", \\"IT\\", \\"HR\\", \\"Finance\\"] } df = pd.DataFrame(data) ``` # Operations: 1. Create a subset `df_it` containing only employees from the \\"IT\\" department using DataFrame\'s filtering. 2. Attempt to change the age of the first employee in the `df_it` subset to 25. Verify whether the original DataFrame `df` is affected. 3. Modify the original DataFrame `df` by changing the name of the employee with `EmployeeID` 1003 to \\"Charlie Brown\\". 4. Create a copy of the DataFrame `df` called `df_copy` and reset its index. 5. Modify the `Age` of the employee with `EmployeeID` 1001 in `df_copy` to 38 and observe whether this change propagates back to the original DataFrame `df`. # Expected Output: 1. Print the original DataFrame `df` after each step to observe changes (or lack thereof). 2. Ensure that modifications made on the subset do not affect the original DataFrame and vice versa, following CoW principles. # Example: ```python import pandas as pd # Input DataFrame data = { \\"EmployeeID\\": [1001, 1002, 1003, 1004, 1005], \\"Name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\", \\"Eve\\"], \\"Age\\": [34, 45, 23, 40, 29], \\"Department\\": [\\"HR\\", \\"Finance\\", \\"IT\\", \\"HR\\", \\"Finance\\"] } df = pd.DataFrame(data) # Step 1: Filter \'IT\' department employees df_it = df[df[\\"Department\\"] == \\"IT\\"] # Step 2: Attempt to change Age in subset df_it.iloc[0, df_it.columns.get_loc(\\"Age\\")] = 25 # Print original DataFrame to verify if it was affected print(\\"After modifying df_it:\\", df, sep=\\"n\\") # Step 3: Modify original DataFrame df.loc[df[\\"EmployeeID\\"] == 1003, \\"Name\\"] = \\"Charlie Brown\\" print(\\"After modifying df:\\", df, sep=\\"n\\") # Step 4: Create copy and reset index df_copy = df.copy().reset_index(drop=True) # Step 5: Modify Age in the copy df_copy.loc[df_copy[\\"EmployeeID\\"] == 1001, \\"Age\\"] = 38 # Print both DataFrames to observe changes print(\\"Original DataFrame after copy modification:\\", df, sep=\\"n\\") print(\\"Copied DataFrame:\\", df_copy, sep=\\"n\\") ``` # Constraints: - Use appropriate pandas methods to achieve the task. - Keep in mind the CoW principles to ensure your code\'s correctness and predictability.","solution":"import pandas as pd # Input DataFrame data = { \\"EmployeeID\\": [1001, 1002, 1003, 1004, 1005], \\"Name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\", \\"Eve\\"], \\"Age\\": [34, 45, 23, 40, 29], \\"Department\\": [\\"HR\\", \\"Finance\\", \\"IT\\", \\"HR\\", \\"Finance\\"] } df = pd.DataFrame(data) # Step 1: Filter \'IT\' department employees df_it = df[df[\\"Department\\"] == \\"IT\\"] # Step 2: Attempt to change Age in subset df_it.loc[df_it.index[0], \\"Age\\"] = 25 # Print original DataFrame to verify if it was affected print(\\"After modifying df_it:n\\", df) # Step 3: Modify original DataFrame df.loc[df[\\"EmployeeID\\"] == 1003, \\"Name\\"] = \\"Charlie Brown\\" print(\\"After modifying df:n\\", df) # Step 4: Create copy and reset index df_copy = df.copy().reset_index(drop=True) # Step 5: Modify Age in the copy df_copy.loc[df_copy[\\"EmployeeID\\"] == 1001, \\"Age\\"] = 38 # Print both DataFrames to observe changes print(\\"Original DataFrame after copy modification:n\\", df) print(\\"Copied DataFrame:n\\", df_copy)"},{"question":"**Question: Implementing an Efficient File Reader with Caching** Given the documentation about the `linecache` module, let\'s build a small application to efficiently read specific lines from a file using caching mechanisms provided by `linecache`. You are required to implement a custom file reader class that allows reading any specified line from a file and includes functionalities to clear and check the cache. # Class Definition Implement a class `EfficientFileReader` with the following methods: 1. **`__init__(self, filename: str)`** - Initializes the file reader with the given `filename`. 2. **`read_line(self, lineno: int) -> str`** - Returns the content of the specified line number (`lineno`) from the file. - If an error occurs (such as the file not being found or the line not existing), return `\\"Error\\"`. 3. **`clear_cache(self) -> None`** - Clears the cached lines. 4. **`check_cache(self) -> None`** - Checks and updates the cache if the file has changed on disk. # Input and Output Formats - **Input:** - `filename` as a string during the initialization of the class. - `lineno` as an integer in the `read_line` method representing the line number to be read. - **Output:** - For `read_line`: Returns the content of the specified line or `\\"Error\\"` in case of an issue. - For `clear_cache` and `check_cache`: No return value (they perform operations on the cache). # Example Usage ```python # Example usage of EfficientFileReader class # Create an instance of EfficientFileReader reader = EfficientFileReader(\'example.txt\') # Read line 3 from the file print(reader.read_line(3)) # Should print the 3rd line of \'example.txt\' or \\"Error\\" if it doesn\'t exist # Read line 10 from the file print(reader.read_line(10)) # Should print the 10th line of \'example.txt\' or \\"Error\\" if it doesn\'t exist # Clear the cache reader.clear_cache() # Check and update the cache if necessary reader.check_cache() ``` # Constraints - The `filename` should refer to a valid text file in the file system. - The `lineno` should be a positive integer greater than 0. - The class methods should handle errors gracefully and ensure that the cache is used efficiently to minimize file I/O operations. **Note**: Use the `linecache` module to implement the above functionalities.","solution":"import linecache class EfficientFileReader: def __init__(self, filename: str): Initializes the EfficientFileReader with the provided filename. self.filename = filename def read_line(self, lineno: int) -> str: Returns the content of the specified line number (lineno) from the file. If an error occurs, return \\"Error\\". try: line = linecache.getline(self.filename, lineno) if line: return line.strip() else: return \\"Error\\" except Exception as e: return \\"Error\\" def clear_cache(self) -> None: Clears the cached lines. linecache.clearcache() def check_cache(self) -> None: Checks and updates the cache if the file has changed on disk. linecache.checkcache(self.filename)"},{"question":"You are provided with a dataset containing information about tips received by waitstaff in a restaurant. Your task is to create various visualizations using the seaborn library. These visualizations will help in understanding the relationship between different variables in the dataset. Dataset Description The dataset can be loaded using `sns.load_dataset(\\"tips\\")`, and contains the following columns: - `total_bill`: Total bill amount. - `tip`: Tip amount. - `sex`: Gender of the person paying (Male/Female). - `smoker`: Whether the paying person is a smoker (Yes/No). - `day`: Day of the week. - `time`: Time of day (Lunch/Dinner). - `size`: Size of the party. Tasks 1. **Scatter Plot with Hue and Style:** Create a scatter plot of `total_bill` vs. `tip`, using the `time` of day to determine the `hue` and the `day` to determine the `style`. 2. **Scatter Plot with Size:** Create another scatter plot of `total_bill` vs. `tip`, using the `size` of the party to determine the size of the markers. Ensure that the marker sizes range from 20 to 200. 3. **Faceted Scatter Plot:** Using `relplot`, create a faceted scatter plot of `total_bill` vs. `tip`, with subplots created based on `time` (i.e., Lunch and Dinner). Use `day` for both `hue` and `style`. 4. **Custom Markers Scatter Plot:** Create a scatter plot of `total_bill` vs. `tip` and customize the markers to be squares (\'s\') for Lunch and \'X\' for Dinner. Use `time` to set the marker style. Expected Output The functions should generate the corresponding plots without returning any values. Ensure proper labeling and legends for clarity. Constraints - Use the seaborn library for all visualizations. - Ensure the plots have appropriate titles and axis labels for easy interpretation. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Scatter Plot with Hue and Style def scatter_plot_hue_style(data): sns.scatterplot(data=data, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"day\\") plt.title(\\"Total Bill vs Tip by Time of Day and Day\\") plt.show() # Task 2: Scatter Plot with Size def scatter_plot_size(data): sns.scatterplot(data=data, x=\\"total_bill\\", y=\\"tip\\", size=\\"size\\", sizes=(20, 200)) plt.title(\\"Total Bill vs Tip by Party Size\\") plt.show() # Task 3: Faceted Scatter Plot def faceted_scatter_plot(data): sns.relplot(data=data, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\") plt.suptitle(\\"Total Bill vs Tip by Time of Day and Day\\", y=1.03) plt.show() # Task 4: Custom Markers Scatter Plot def custom_markers_scatter_plot(data): markers = {\\"Lunch\\": \\"s\\", \\"Dinner\\": \\"X\\"} sns.scatterplot(data=data, x=\\"total_bill\\", y=\\"tip\\", style=\\"time\\", markers=markers) plt.title(\\"Total Bill vs Tip with Custom Markers\\") plt.show() # Example usage: scatter_plot_hue_style(tips) scatter_plot_size(tips) faceted_scatter_plot(tips) custom_markers_scatter_plot(tips) ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Scatter Plot with Hue and Style def scatter_plot_hue_style(data): sns.scatterplot(data=data, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"day\\") plt.title(\\"Total Bill vs Tip by Time of Day and Day\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.legend(title=\\"Legend\\") plt.show() # Task 2: Scatter Plot with Size def scatter_plot_size(data): sns.scatterplot(data=data, x=\\"total_bill\\", y=\\"tip\\", size=\\"size\\", sizes=(20, 200)) plt.title(\\"Total Bill vs Tip by Party Size\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.legend(title=\\"Legend\\") plt.show() # Task 3: Faceted Scatter Plot def faceted_scatter_plot(data): g = sns.relplot(data=data, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\") g.fig.suptitle(\\"Total Bill vs Tip by Time of Day and Day\\", y=1.03) g.set_axis_labels(\\"Total Bill\\", \\"Tip\\") plt.show() # Task 4: Custom Markers Scatter Plot def custom_markers_scatter_plot(data): markers = {\\"Lunch\\": \\"s\\", \\"Dinner\\": \\"X\\"} sns.scatterplot(data=data, x=\\"total_bill\\", y=\\"tip\\", style=\\"time\\", markers=markers) plt.title(\\"Total Bill vs Tip with Custom Markers\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.legend(title=\\"Legend\\") plt.show() # Example usage: scatter_plot_hue_style(tips) scatter_plot_size(tips) faceted_scatter_plot(tips) custom_markers_scatter_plot(tips)"},{"question":"**Question: Implementing and Debugging a Custom Python Function** **Objective:** Write a Python script that defines a function to perform a specific task. Then, use the `pdb` module to debug this function in various scenarios. **Task:** 1. Implement a function `calculate_statistics(numbers)` that takes a list of integers and returns a dictionary containing the mean, median, and mode of the list. If the list is empty, the function should raise a `ValueError` with the message \\"The list is empty\\". 2. Use the `pdb` debugger to set breakpoints, step through your code, and inspect variables. Specifically, demonstrate how to: - Set breakpoints in the function. - Step into the function and inspect local variables. - Print the value of specific expressions. - Handle a situation where the function raises an error. **Function Implementation:** The `calculate_statistics(numbers)` function should be implemented in the script (`statistics_script.py`) and perform the following calculations: - **Mean**: The average of the numbers. - **Median**: The middle number in a sorted list (or the average of the two middle numbers if the list has an even number of elements). - **Mode**: The number that appears most frequently. If multiple numbers have the same highest frequency, return any one of them. **Example:** ```python # statistics_script.py def calculate_statistics(numbers): if not numbers: raise ValueError(\\"The list is empty\\") numbers.sort() mean = sum(numbers) / len(numbers) middle = len(numbers) // 2 if len(numbers) % 2 == 1: median = numbers[middle] else: median = (numbers[middle - 1] + numbers[middle]) / 2 mode = max(set(numbers), key=numbers.count) return { \\"mean\\": mean, \\"median\\": median, \\"mode\\": mode } if __name__ == \\"__main__\\": sample_numbers = [4, 1, 2, 2, 3, 5, 4, 6, 7] print(calculate_statistics(sample_numbers)) ``` **Debugging Instructions:** 1. Insert a breakpoint at the start of the `calculate_statistics` function using `pdb`. 2. Step through the function execution and inspect the values of `mean`, `median`, and `mode`. 3. Add a scenario where the function raises a `ValueError` and handle it using post-mortem debugging. 4. Use commands like `continue`, `step`, `print`, and `quit` to control the debugging process. **Expected Debugging Session Example:** ```python import pdb import statistics_script pdb.run(\'statistics_script.calculate_statistics([1, 2, 2, 3, 4])\') # Expected behavior is to step through the function, inspect variables and output the result. pdb.run(\'statistics_script.calculate_statistics([])\') # Expected to enter post-mortem mode due to ValueError: The list is empty # Setting breakpoints and manually stepping through pdb.run(\'import statistics_script; pdb.set_trace()\') statistics_script.calculate_statistics([1, 2, 2, 3, 4]) ``` Ensure that your solution demonstrates a thorough understanding of using `pdb` for debugging Python scripts. **Constraints:** - You must use the `pdb` module\'s `set_trace()` function and its commands to step through and debug your function. - Handle cases where the function raises an exception gracefully using the debugger. Good luck!","solution":"def calculate_statistics(numbers): Calculate the mean, median, and mode of a list of numbers. Args: numbers (list): A list of integers. Returns: dict: A dictionary containing the mean, median, and mode. Raises: ValueError: If the list is empty. if not numbers: raise ValueError(\\"The list is empty\\") numbers.sort() mean = sum(numbers) / len(numbers) middle = len(numbers) // 2 if len(numbers) % 2 == 1: median = numbers[middle] else: median = (numbers[middle - 1] + numbers[middle]) / 2 mode = max(set(numbers), key=numbers.count) return { \\"mean\\": mean, \\"median\\": median, \\"mode\\": mode }"},{"question":"Secure HMAC Message Verification A software application needs to ensure the integrity and authenticity of messages exchanged between a server and client using HMAC. You are required to implement a function in Python that will handle message signing and verification using the HMAC mechanism provided by Python\'s `hmac` module. Function 1: `sign_message` This function will sign a message with a given secret key and digest algorithm. ```python def sign_message(key: bytes, msg: bytes, digestmod: str) -> str: Sign given message using HMAC. Args: key (bytes): The secret key used for the HMAC generation. msg (bytes): The message to be signed. digestmod (str): The name of the hash algorithm to use (e.g., \'sha256\'). Returns: str: The hexadecimal digest of the signed message. pass ``` - **Inputs:** - `key`: A `bytes` object representing the secret key. - `msg`: A `bytes` object representing the message to be signed. - `digestmod`: A string specifying the hash algorithm (e.g., \'sha256\'). - **Output:** - A string of the hexadecimal digest. Function 2: `verify_message` This function will verify if a given message\'s HMAC matches the provided HMAC. ```python def verify_message(key: bytes, msg: bytes, digestmod: str, provided_digest: str) -> bool: Verify that the provided digest matches the message\'s HMAC. Args: key (bytes): The secret key used for the HMAC generation. msg (bytes): The message to be verified. digestmod (str): The name of the hash algorithm to use (e.g., \'sha256\'). provided_digest (str): The hexadecimal digest to compare against. Returns: bool: True if the message\'s HMAC matches the provided digest, False otherwise. pass ``` - **Inputs:** - `key`: A `bytes` object representing the secret key. - `msg`: A `bytes` object representing the message to be verified. - `digestmod`: A string specifying the hash algorithm (e.g., \'sha256\'). - `provided_digest`: A string of the hexadecimal digest to verify against. - **Output:** - A boolean value indicating whether the message\'s HMAC matches the provided digest. # Constraints - Use only the `hmac` module and other standard library modules if needed. - Handle appropriate exceptions for invalid inputs. - Ensure your solution is secure against timing attacks. - Assume that the `msg` and `key` inputs are always of appropriate types. # Example Usage ```python key = b\'secret_key\' msg = b\'Important message\' digestmod = \'sha256\' # Signing a message signed_digest = sign_message(key, msg, digestmod) print(f\'Signed digest: {signed_digest}\') # Verifying the message is_valid = verify_message(key, msg, digestmod, signed_digest) print(f\'Is the message valid? {is_valid}\') ``` In this example, `sign_message` will produce a digest of the message based on the provided key and digest algorithm. The `verify_message` function will then check if this digest matches the provided digest, indicating whether the integrity and authenticity of the message are intact.","solution":"import hmac import hashlib def sign_message(key: bytes, msg: bytes, digestmod: str) -> str: Sign given message using HMAC. Args: key (bytes): The secret key used for the HMAC generation. msg (bytes): The message to be signed. digestmod (str): The name of the hash algorithm to use (e.g., \'sha256\'). Returns: str: The hexadecimal digest of the signed message. # Generate the HMAC object hmac_obj = hmac.new(key, msg, getattr(hashlib, digestmod)) # Return the hexadecimal digest of the HMAC return hmac_obj.hexdigest() def verify_message(key: bytes, msg: bytes, digestmod: str, provided_digest: str) -> bool: Verify that the provided digest matches the message\'s HMAC. Args: key (bytes): The secret key used for the HMAC generation. msg (bytes): The message to be verified. digestmod (str): The name of the hash algorithm to use (e.g., \'sha256\'). provided_digest (str): The hexadecimal digest to compare against. Returns: bool: True if the message\'s HMAC matches the provided digest, False otherwise. # Generate the HMAC for the message computed_digest = sign_message(key, msg, digestmod) # Compare the computed digest with the provided digest securely return hmac.compare_digest(computed_digest, provided_digest)"},{"question":"Secure Password Generator and URL Token Generator # Objective Design and implement a Python class named `SecureUtility` that leverages the `secrets` module to provide functionalities for secure password generation and URL token generation. This exercise will test your understanding of the `secrets` module and your ability to apply security best practices in generating passwords and secure tokens. # Requirements 1. **Class Definition**: ```python class SecureUtility: def generate_password(self, length: int, num_digits: int) -> str: pass def generate_secure_url(self, base_url: str, token_length: int) -> str: pass ``` 2. **Method Details**: - `generate_password(length: int, num_digits: int) -> str`: - Generates a secure alphanumeric password of the specified `length` containing at least `num_digits` digits. - Password must contain at least one uppercase letter, one lowercase letter, and the specified number of digits. - Use `secrets.choice` and the appropriate alphabet of characters for generating the password. - `generate_secure_url(base_url: str, token_length: int) -> str`: - Generates a URL-safe token of `token_length` bytes. - Concatenates the `base_url` with the generated token to form a complete URL. - Use `secrets.token_urlsafe` for generating the token. # Constraints: - The minimum length for a password is 8 characters. - The `num_digits` should be at least 1 and should not exceed the total `length` of the password. # Example Usage: ```python secure_util = SecureUtility() password = secure_util.generate_password(length=12, num_digits=3) print(password) # Example: \'Abc123DefGhi\' secure_url = secure_util.generate_secure_url(base_url=\'https://example.com/reset?token=\', token_length=16) print(secure_url) # Example: \'https://example.com/reset?token=Drmhze6EPcv0fN_81Bj-nA\' ``` # Notes - Ensure that passwords meet the stated requirements by iterating until a valid password is generated. - Make use of appropriate error handling for invalid input, such as lengths less than the minimum or invalid digit counts.","solution":"import secrets import string class SecureUtility: def generate_password(self, length: int, num_digits: int) -> str: # Validate input constraints if length < 8: raise ValueError(\\"Password length must be at least 8 characters.\\") if num_digits < 1 or num_digits > length: raise ValueError(\\"num_digits must be at least 1 and not exceed the length of the password.\\") # Define character sets alphabet = string.ascii_letters + string.digits password = [] # Ensure at least one of each required character type password.append(secrets.choice(string.ascii_lowercase)) password.append(secrets.choice(string.ascii_uppercase)) # Add required digits for _ in range(num_digits): password.append(secrets.choice(string.digits)) # Add remaining characters remaining_length = length - len(password) for _ in range(remaining_length): password.append(secrets.choice(alphabet)) # Shuffle the result to avoid predictable patterns secrets.SystemRandom().shuffle(password) return \'\'.join(password) def generate_secure_url(self, base_url: str, token_length: int) -> str: # Generate the token token = secrets.token_urlsafe(token_length) return f\\"{base_url}{token}\\""},{"question":"**Problem Description:** You are tasked with creating various visualizations using the seaborn library\'s `light_palette` function. You will be provided with a dataset containing numerical values, and you need to create and apply a customized color palette to a heatmap representing the data. **Requirements:** 1. **Create a color palette:** - Define a sequential color palette using a color name (e.g., `\\"skyblue\\"`). - Create another sequential color palette using a hex code (e.g., `\\"#ff6347\\"`). - Generate a third palette using HUSL color space values (e.g., `(260, 75, 40)`). - Ensure one of the palettes has at least 10 shades. - Convert one of the palettes to a continuous colormap. 2. **Generate a Heatmap:** - Use the provided dataset to create a heatmap for each of the color palettes. - Apply the created color palettes to the heatmap. - Properly label the axes and provide a title indicating which palette was used. **Input Format:** - A CSV file named `data.csv` containing a numerical dataset for the heatmap. **Output Format:** - 3 heatmaps saved as PNG files, each using a different color palette: - `heatmap_name.png` - `heatmap_hex.png` - `heatmap_husl.png` **Example:** Given the dataset in `data.csv`: ``` A B C 0 23 45 67 1 78 34 23 2 45 67 89 ``` Your code should generate and save three heatmaps, each using one of the specified color palettes. **Constraints:** - Use the seaborn and matplotlib libraries. - Ensure the names of the saved files match the specified output format. **Performance:** - The heatmap generation should handle datasets with up to 1000x1000 entries efficiently. **Function Signature:** ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def generate_heatmaps(filename: str) -> None: # Read the dataset data = pd.read_csv(filename) # Palette and heatmap for name color palette_name = sns.light_palette(\\"skyblue\\", as_cmap=True) plt.figure(figsize=(10, 8)) sns.heatmap(data, cmap=palette_name) plt.title(\\"Heatmap with skyblue Palette\\") plt.savefig(\\"heatmap_name.png\\") plt.clf() # Palette and heatmap for hex color palette_hex = sns.light_palette(\\"#ff6347\\", 10, as_cmap=True) plt.figure(figsize=(10, 8)) sns.heatmap(data, cmap=palette_hex) plt.title(\\"Heatmap with Hex Palette\\") plt.savefig(\\"heatmap_hex.png\\") plt.clf() # Palette and heatmap for HUSL palette_husl = sns.light_palette((260, 75, 40), input=\\"husl\\", as_cmap=True) plt.figure(figsize=(10, 8)) sns.heatmap(data, cmap=palette_husl) plt.title(\\"Heatmap with HUSL Palette\\") plt.savefig(\\"heatmap_husl.png\\") plt.clf() ``` **Instructions:** 1. Implement the `generate_heatmaps` function according to the provided signature. 2. Read the dataset from `data.csv`. 3. Create and apply the required color palettes. 4. Save each heatmap with the appropriate filename as described. 5. Ensure code correctness and readability. **Important:** - Submissions will be evaluated based on code correctness, efficiency, and adherence to the requirements. - Make sure your function handles errors gracefully and includes comments explaining your logic.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def generate_heatmaps(filename: str) -> None: # Read the dataset data = pd.read_csv(filename) # Palette and heatmap for name color palette_name = sns.light_palette(\\"skyblue\\", as_cmap=True) plt.figure(figsize=(10, 8)) sns.heatmap(data, cmap=palette_name, annot=True) plt.title(\\"Heatmap with skyblue Palette\\") plt.savefig(\\"heatmap_name.png\\") plt.clf() # Palette and heatmap for hex color palette_hex = sns.light_palette(\\"#ff6347\\", 10, as_cmap=True) plt.figure(figsize=(10, 8)) sns.heatmap(data, cmap=palette_hex, annot=True) plt.title(\\"Heatmap with Hex Palette\\") plt.savefig(\\"heatmap_hex.png\\") plt.clf() # Palette and heatmap for HUSL palette_husl = sns.light_palette((260, 75, 40), input=\\"husl\\", as_cmap=True) plt.figure(figsize=(10, 8)) sns.heatmap(data, cmap=palette_husl, annot=True) plt.title(\\"Heatmap with HUSL Palette\\") plt.savefig(\\"heatmap_husl.png\\") plt.clf()"},{"question":"Objective Evaluate the student\'s ability to define a custom PyTorch module, compose it with other modules, train a neural network, and use forward and backward hooks for debugging and modifying behavior. Problem Statement You are tasked with creating a neural network in PyTorch that combines custom-defined layers with standard PyTorch layers. Additionally, you will need to implement hooks to modify the inputs and outputs of the neural network during the forward pass and inspect gradients during the backward pass. Part 1: Define Custom Modules 1. Define a custom linear module `CustomLinear` that replicates the behavior of `torch.nn.Linear` but with custom weight initialization (e.g., Xavier initialization) and zero bias. ```python import torch from torch import nn class CustomLinear(nn.Module): def __init__(self, in_features, out_features): super().__init__() # Define weight parameter and bias self.weight = nn.Parameter(torch.empty(in_features, out_features)) self.bias = nn.Parameter(torch.zeros(out_features)) # Initialize weights using Xavier initialization nn.init.xavier_normal_(self.weight) def forward(self, input): return input @ self.weight + self.bias ``` 2. Use the `CustomLinear` module to define a neural network class `CustomNet` comprising the following architecture: - A `CustomLinear` layer with `in_features=4` and `out_features=3`. - A ReLU activation layer. - A `CustomLinear` layer with `in_features=3` and `out_features=1`. ```python class CustomNet(nn.Module): def __init__(self): super().__init__() self.layer1 = CustomLinear(4, 3) self.relu = nn.ReLU() self.layer2 = CustomLinear(3, 1) def forward(self, x): x = self.layer1(x) x = self.relu(x) x = self.layer2(x) return x ``` Part 2: Implement Training 3. Write a function `train_network` that trains the `CustomNet` to output zero for any input, using the following components: - `SGD` optimizer with learning rate `lr=1e-4`, weight decay `weight_decay=1e-2`, and momentum `momentum=0.9`. - L1 loss function (`torch.abs`). - Training loop for `epochs=10000`. ```python def train_network(net, epochs=10000, lr=1e-4, weight_decay=1e-2, momentum=0.9): optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum) for _ in range(epochs): input = torch.randn(4) output = net(input) loss = torch.abs(output).sum() optimizer.zero_grad() loss.backward() optimizer.step() ``` Part 3: Use Hooks 4. Implement forward and backward hooks: - **Forward hook**: Adds 1 to the input before the forward pass. - **Backward hook**: Prints the gradients of the inputs. ```python def forward_hook(module, inputs): input = inputs[0] return input + 1 def backward_hook(module, grad_inputs, grad_outputs): print(\\"Gradients of inputs:\\", grad_inputs) print(\\"Gradients of outputs:\\", grad_outputs) return grad_inputs ``` 5. Attach hooks to the `CustomNet` and demonstrate their effect by running a forward and backward pass with a sample input. ```python # Attach hooks net = CustomNet() forward_hook_handle = net.layer1.register_forward_pre_hook(forward_hook) backward_hook_handle = net.layer2.register_full_backward_hook(backward_hook) # Sample input sample_input = torch.randn(4, requires_grad=True) # Forward and backward pass output = net(sample_input) output.backward() # Remove hooks forward_hook_handle.remove() backward_hook_handle.remove() ``` Deliverables - Implement the `CustomLinear` and `CustomNet` classes. - Implement the `train_network` function. - Define and attach forward and backward hooks to `CustomNet`. - Demonstrate a sample forward and backward pass with the hooks applied. **Expected Input and Output Formats** - The custom modules and neural network architecture should be defined as described. - The training function `train_network` should successfully train the `CustomNet`. - The forward and backward hooks should modify and inspect the input and gradient values during forward and backward passes, respectively. Constraints and Considerations - Ensure proper initialization of parameters in the custom linear module. - Handle different data types (such as `float` or `double`) and devices (CPU or CUDA) appropriately. - Maintain readability and modularity in your code by clearly separating the definitions, training loop, and hook implementations. Performance Requirements - The training loop should complete within a reasonable amount of time (e.g., a few seconds to minutes). - Hooks should correctly modify inputs and print gradients without causing significant slowdowns. Submission Submit a Jupyter notebook or Python script containing your implementation and demonstration of the sample forward and backward pass with hooks.","solution":"import torch from torch import nn import torch.optim as optim class CustomLinear(nn.Module): def __init__(self, in_features, out_features): super().__init__() self.weight = nn.Parameter(torch.empty(in_features, out_features)) self.bias = nn.Parameter(torch.zeros(out_features)) nn.init.xavier_normal_(self.weight) def forward(self, input): return input @ self.weight + self.bias class CustomNet(nn.Module): def __init__(self): super().__init__() self.layer1 = CustomLinear(4, 3) self.relu = nn.ReLU() self.layer2 = CustomLinear(3, 1) def forward(self, x): x = self.layer1(x) x = self.relu(x) x = self.layer2(x) return x def train_network(net, epochs=10000, lr=1e-4, weight_decay=1e-2, momentum=0.9): optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum) for _ in range(epochs): input = torch.randn(4) output = net(input) loss = torch.abs(output).sum() optimizer.zero_grad() loss.backward() optimizer.step() def forward_hook(module, inputs): input = inputs[0] return (input + 1,) def backward_hook(module, grad_inputs, grad_outputs): print(\\"Gradients of inputs:\\", grad_inputs) print(\\"Gradients of outputs:\\", grad_outputs) # Sample usage of forward and backward hooks net = CustomNet() forward_hook_handle = net.layer1.register_forward_pre_hook(forward_hook) backward_hook_handle = net.layer2.register_full_backward_hook(backward_hook) # Sample input sample_input = torch.randn(4, requires_grad=True) # Forward and backward pass output = net(sample_input) output.backward() # Remove hooks forward_hook_handle.remove() backward_hook_handle.remove()"},{"question":"**Question: Advanced Scope, Binding, and Exception Handling in Python** You are required to write a Python program that demonstrates an understanding of the following concepts: 1. Code Block Execution within Functions and Classes 2. Name Binding and Resolution in Local, Enclosing, Global, and Builtin Scopes 3. Custom Exception Handling # Instructions: 1. **Function Implementation**: - Implement a function `manage_scope_and_exceptions` which takes no arguments. - Inside this function, define a nested function `nested_function` and a class `ExampleClass`. 2. **Nested Function**: - The `nested_function` should: 1. Define a local variable `local_var` and set it to a value. 2. Attempt to modify a non-local variable `enclosing_var` from its enclosing function scope. 3. Raise a custom exception `CustomException` with a message if a condition is met (e.g., if `local_var` is a certain value). 3. **Class Definition**: - The class `ExampleClass` should: 1. Have an `__init__` method that sets an instance variable. 2. Have a method `update_var` that tries to reference a global variable and modifies it. If the variable does not exist, it should raise a `NameError`. 3. Have another method `handle_exception` that uses a `try-except-finally` block to handle exceptions raised by `update_var`. 4. **Custom Exception**: - Define a custom exception `CustomException` which inherits from `Exception`. Ensure that it has an appropriate message. 5. **Main Execution**: - Call the `manage_scope_and_exceptions` function. - Prototype global and nonlocal variables and show their interaction with the function and class methods. - Properly handle any raised exceptions and ensure the program does not terminate abruptly. # Example of Expected Output: ```python Global variable modified to: 100 Custom Exception occurred: Condition met in nested function. Handled NameError in class method Finally block executed ``` # Constraints: - Demonstrate understanding of the scope rules and exception hierarchy in Python. - Ensure proper use of `global` and `nonlocal` keywords where necessary. - The implementation should be efficient and neatly organized. # Solution Guide: 1. Define the nested function `nested_function` within `manage_scope_and_exceptions` and use `nonlocal` to alter an enclosing scope variable. 2. Define `ExampleClass` with appropriate methods. 3. Use `try-except-finally` blocks to handle exceptions. 4. Ensure to declare any necessary global variables before they are referenced or modified. **Note:** Properly comment your code to clarify scope boundaries and exception handling mechanisms.","solution":"# Custom Exception definition class CustomException(Exception): def __init__(self, message): super().__init__(message) # Defining the function that demonstrates scope and exception handling def manage_scope_and_exceptions(): enclosing_var = \\"initial\\" def nested_function(): local_var = 10 nonlocal enclosing_var # Modifying the enclosing variable enclosing_var = \\"modified\\" if local_var == 10: raise CustomException(\\"Condition met in nested function.\\") class ExampleClass: def __init__(self): self.instance_var = \\"instance_value\\" def update_var(self): global global_var global_var = 100 def handle_exception(self): try: self.update_var() except NameError as e: print(f\\"Handled NameError in class method: {e}\\") finally: print(\\"Finally block executed\\") # Attempting to run the nested function and handle exceptions try: nested_function() except CustomException as e: print(f\\"Custom Exception occurred: {e}\\") # Creating an instance of the class and handling exceptions example_instance = ExampleClass() example_instance.handle_exception() print(f\\"Enclosing Variable: {enclosing_var}\\") print(f\\"Global Variable: {global_var}\\") # Setting up an initial global variable global_var = 0 # Running the main function manage_scope_and_exceptions()"},{"question":"**Question: Creating and Customizing Categorical Plots with Seaborn** You are provided with a dataset and asked to visualize various aspects of it using the seaborn library. Your task is to write a function `visualize_data` that meets the following requirements: # Function Signature ```python def visualize_data(): pass ``` # Requirements 1. **Load the Dataset**: - Load the Titanic dataset using seaborn\'s `load_dataset` function. 2. **Create and Save Categorical Plots**: - Create a `violin` plot to show the distribution of ages across different classes of passengers. Shade the inside of the violin plot to make it a solid color. - Create a split `violin` plot to show the distribution of ages across different classes of passengers, split by gender (`sex`). Use a lighter shade for comparison. - Create a combined plot that shows both a `violin` plot and a `strip` plot for age distribution across different classes in a single figure. 3. **Create and Customize Subplots**: - Create subplots showing the survival rate with respect to \'class\' and \'sex\'. Customize the subplot grid to have a height of 4 and an aspect ratio of 0.6. Ensure that the labels, titles, and limits of the y-axis are appropriately set. Remove the left spine of the plot for better visual aesthetics. 4. **Save the Plots**: - Save each of the plots created in the current working directory with appropriate filenames (e.g., \\"violin_age_class.png\\", \\"split_violin_age_class_sex.png\\", etc.) # Constraints - You should handle any exceptions that may occur during the creation of plots (e.g., data loading issues). - Your function should not require any external input or return any value. # Example An example output of your function should be a series of image files saved in the current working directory: - `violin_age_class.png` - `split_violin_age_class_sex.png` - `combined_violin_strip_plot.png` - `subplots_survival_rate.png` These plots should showcase your ability to harness seaborn\'s capabilities to visualize and customize data appropriately. **Note**: Make sure your solution is reproducible and can be tested independently.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_data(): try: # Load the Titanic dataset titanic = sns.load_dataset(\'titanic\') # Violin plot for age distribution across classes plt.figure(figsize=(10, 6)) sns.violinplot(x=\'class\', y=\'age\', data=titanic, inner=\\"box\\", palette=\\"muted\\") plt.title(\'Violin Plot of Age Distribution Across Classes\') plt.savefig(\'violin_age_class.png\') plt.close() # Split violin plot for age distribution across classes split by gender (`sex`) plt.figure(figsize=(10, 6)) sns.violinplot(x=\'class\', y=\'age\', hue=\'sex\', data=titanic, split=True, palette=\\"muted\\") plt.title(\'Split Violin Plot of Age Distribution Across Classes by Gender\') plt.savefig(\'split_violin_age_class_sex.png\') plt.close() # Combined violin and strip plot plt.figure(figsize=(10, 6)) sns.violinplot(x=\'class\', y=\'age\', data=titanic, inner=None, color=\\".8\\") sns.stripplot(x=\'class\', y=\'age\', data=titanic, jitter=True, hue=\'sex\', palette=\\"muted\\", dodge=True) plt.title(\'Combined Violin and Strip Plot of Age Distribution Across Classes\') plt.savefig(\'combined_violin_strip_plot.png\') plt.close() # Subplots for survival rate by \'class\' and \'sex\' g = sns.catplot(x=\'class\', hue=\'sex\', col=\'survived\', data=titanic, kind=\'count\', height=4, aspect=0.6, palette=\\"muted\\") g.set_axis_labels(\\"Passenger Class\\", \\"Count\\") g.set_titles(\\"{col_name} Passengers\\") g.set(ylim=(0, 400)) g.despine(left=True) plt.savefig(\'subplots_survival_rate.png\') plt.close() except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# XML DOM Pull Parser: Processing and Extracting Data **Objective:** Write a function, `process_xml(xml_string: str) -> List[str]`, that parses a given XML string and extracts all the text content from elements with a specific tag \\"data\\", if their attribute \\"length\\" has a numeric value greater than 10. Your function should return a list of the extracted text contents. **Function Signature:** ```python def process_xml(xml_string: str) -> List[str]: ``` **Input:** - `xml_string` (str): A string containing well-formed XML data. **Output:** - A list of strings, where each string is the text content of a \\"data\\" element with the \\"length\\" attribute greater than 10. **Constraints:** - The XML string may contain nested elements, comments, and various element types. - You can assume that the \\"length\\" attribute, when present, will always have a numeric value. **Example:** ```python xml_input = <root> <data length=\\"5\\">Short</data> <data length=\\"15\\">Long enough</data> <section> <data length=\\"25\\">Even longer data</data> </section> </root> expected_output = [\\"Long enough\\", \\"Even longer data\\"] assert process_xml(xml_input) == expected_output ``` **Notes:** - Ensure the pulling of events and handling of \\"data\\" elements only when necessary. - Use `doc.expandNode(node)` method to retrieve the full text content once the specific node is identified. - Handle cases to efficiently iterate through the stream to extract required information.","solution":"from typing import List import xml.etree.ElementTree as ET def process_xml(xml_string: str) -> List[str]: Processes the given XML string and extracts all text contents from elements with a tag \\"data\\" and a \\"length\\" attribute having a numeric value greater than 10. tree = ET.ElementTree(ET.fromstring(xml_string)) root = tree.getroot() result = [] for elem in root.iter(\'data\'): length = elem.get(\'length\') if length is not None and length.isdigit() and int(length) > 10: result.append(elem.text) return result"},{"question":"**Question: Handling asyncio Exceptions** You are required to implement a function named `fetch_data` that performs multiple asynchronous tasks to simulate fetching data from different sources. Your task is to handle various exceptions that may be raised during the execution of these tasks. # Function Signature ```python import asyncio async def fetch_data(urls: list[str]) -> dict: pass ``` # Input - `urls`: A list of strings where each string is a URL. # Output - A dictionary with keys being the URLs and values being the results of successfully fetched data or an appropriate error message if the fetching fails. # Requirements 1. **Simulate Fetching Data:** - For simplicity, simulate data fetching using the `asyncio.sleep` function with random delays and possibly raising exceptions. - Use `await asyncio.sleep()` to represent fetching data. - Randomly raise one of the following exceptions to simulate different failure scenarios: - `asyncio.TimeoutError` - `asyncio.CancelledError` - `asyncio.InvalidStateError` - `asyncio.IncompleteReadError` - `asyncio.LimitOverrunError` 2. **Handling Exceptions:** - For each exception, handle it and map it to an appropriate error message in the output dictionary. - The error messages should be: - `Timeout exceeded` for `asyncio.TimeoutError` - `Operation was cancelled` for `asyncio.CancelledError` - `Invalid internal state` for `asyncio.InvalidStateError` - `Incomplete read error` for `asyncio.IncompleteReadError` - `Buffer size limit overrun` for `asyncio.LimitOverrunError` 3. **Concurrency:** - Use asyncio.gather or any other mechanism to perform concurrent fetches for the URLs provided. - Your function should handle multiple URLs concurrently and aggregate results. # Example ```python urls = [\\"https://example.com/data1\\", \\"https://example.com/data2\\", \\"https://example.com/data3\\"] result = await fetch_data(urls) # Example output (actual output can vary due to randomness in exception raising) # { # \\"https://example.com/data1\\": \\"Timeout exceeded\\", # \\"https://example.com/data2\\": \\"Operation was cancelled\\", # \\"https://example.com/data3\\": \\"Some data\\" # } ``` # Constraints - You can assume the list of URLs will contain at most 100 URLs. - The simulated fetching should not take more than 5 seconds per URL. Use the provided exceptions documentation and your knowledge of the `asyncio` module to implement and handle the exceptions effectively.","solution":"import asyncio import random async def fetch_data(urls: list[str]) -> dict: async def simulate_fetch(url): try: await asyncio.sleep(random.uniform(0.1, 0.5)) # Randomly raise exceptions to simulate failures exception_choice = random.choice([None, asyncio.TimeoutError, asyncio.CancelledError, asyncio.InvalidStateError, asyncio.IncompleteReadError, asyncio.LimitOverrunError]) if exception_choice: raise exception_choice() return \\"Fetched data\\" except asyncio.TimeoutError: return \\"Timeout exceeded\\" except asyncio.CancelledError: return \\"Operation was cancelled\\" except asyncio.InvalidStateError: return \\"Invalid internal state\\" except asyncio.IncompleteReadError: return \\"Incomplete read error\\" except asyncio.LimitOverrunError: return \\"Buffer size limit overrun\\" fetch_tasks = [simulate_fetch(url) for url in urls] results = await asyncio.gather(*fetch_tasks, return_exceptions=False) return dict(zip(urls, results))"},{"question":"<|Analysis Begin|> The zlib module in Python provides functions and objects for data compression and decompression using the zlib library. The functionalities include: - Various checksum calculations like Adler-32 and CRC32 for validating data integrity. - Compressing and decompressing byte data. - Creating compression and decompression objects to handle continuous streams of data. - Fine-tuning the behavior of compression and decompression using parameters like compression level, window size, memory level, and strategy. Key functions and methods are: - `zlib.adler32(data[, value])`: Calculate Adler-32 checksum. - `zlib.compress(data, /, level=-1)`: Compress byte data. - `zlib.compressobj(level=-1, method=DEFLATED, wbits=MAX_WBITS, memLevel=DEF_MEM_LEVEL, strategy=Z_DEFAULT_STRATEGY[, zdict])`: Create a compression object. - `zlib.crc32(data[, value])`: Calculate CRC32 checksum. - `zlib.decompress(data, /, wbits=MAX_WBITS, bufsize=DEF_BUF_SIZE)`: Decompress byte data. - `zlib.decompressobj(wbits=MAX_WBITS[, zdict])`: Create a decompression object. The functionality offers significant parameters to tune the performance and output, making it versatile for different use cases involving data compression and decompression. <|Analysis End|> <|Question Begin|> # Compression and Decompression Challenge using zlib **Objective:** Create a compressed log system that performs the following tasks: 1. **Compression Function:** - Accepts a list of log entries (strings). - Compresses and returns the data as a single compressed byte object. 2. **Decompression Function:** - Accepts the compressed byte object. - Decompresses and returns the original list of log entries (strings). You need to write two functions: 1. `compress_logs(logs: List[str], compression_level: int = -1) -> bytes` 2. `decompress_logs(compressed_data: bytes) -> List[str]` Function Specifications: 1. **`compress_logs(logs: List[str], compression_level: int = -1) -> bytes`** - **Input:** - `logs`: A list of strings, representing log entries. - `compression_level`: An integer from 0 to 9 or -1 (default). It determines the level of compression. - **Output:** - A single byte object containing compressed data of concatenated log entries. - **Constraints:** - The strings in `logs` list should be concatenated with a newline character before compression. 2. **`decompress_logs(compressed_data: bytes) -> List[str]`** - **Input:** - `compressed_data`: A byte object containing compressed data of concatenated log entries. - **Output:** - A list of strings, each string being a log entry. The original log entries should be in the order they were before compression. - **Constraints:** - Ensure all log entries are accurately decompressed and newline character separation is preserved during decompression. Example: ```python logs = [ \\"2023-01-01 10:00:00 Log entry 1\\", \\"2023-01-01 10:01:00 Log entry 2\\", \\"2023-01-01 10:02:00 Log entry 3\\" ] compressed_data = compress_logs(logs, compression_level=9) decompressed_logs = decompress_logs(compressed_data) assert logs == decompressed_logs ``` Additional Requirements: - Implement error handling for potential zlib errors during compression and decompression. - Use `zlib.compress` and `zlib.decompress` functions for compressing and decompressing data respectively. - Make sure the function handles edge cases such as empty log entries or extremely large log entries. **Good Luck!**","solution":"import zlib from typing import List def compress_logs(logs: List[str], compression_level: int = -1) -> bytes: Compresses a list of log entries. :param logs: List of log entries as strings. :param compression_level: Compression level from 0 to 9, where 9 is maximum compression, or -1 for default compression. :return: Compressed byte object of the concatenated log entries. try: # Join logs with newline character to form a single string and encode to bytes data = \\"n\\".join(logs).encode(\'utf-8\') # Compress the data compressed_data = zlib.compress(data, compression_level) return compressed_data except zlib.error as e: raise ValueError(f\\"Compression error: {e}\\") def decompress_logs(compressed_data: bytes) -> List[str]: Decompresses a byte object containing compressed log entries. :param compressed_data: Compressed byte object. :return: List of original log entries as strings. try: # Decompress the data decompressed_data = zlib.decompress(compressed_data) # Decode bytes to string and split by newline character to get log entries logs = decompressed_data.decode(\'utf-8\').splitlines() return logs except zlib.error as e: raise ValueError(f\\"Decompression error: {e}\\")"},{"question":"Objective Implement a complex Python function that utilizes various compound statements, including `if`, `while`, `for`, `try`, and `with`. The function will process a nested list of dictionaries containing student records and perform specific operations, handling potential errors gracefully. Problem Statement You are given a list of student records, where each student record is a dictionary containing the keys: - `name` (string): The name of the student. - `scores` (list of integers): A list of scores for various exams. Your task is to implement the function `process_student_records(records: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]` that: 1. Validates the input data and ensures proper structure (list of dictionaries with specified keys). 2. Calculates the average score for each student. 3. Categorizes students based on their average score: - \'Excellent\' if the average score is >= 90. - \'Good\' if the average score is between 75 and 89. - \'Needs Improvement\' if the average score is < 75. 4. Uses context management to write the processed records to a file named `processed_records.json`. 5. Handles any potential errors in data processing and file operations, ensuring that relevant error messages are logged. Specifications 1. **Input:** - `records`: A list of dictionaries where each dictionary has the keys `name` and `scores`. Example: ```python [ {\\"name\\": \\"Alice\\", \\"scores\\": [95, 82, 91]}, {\\"name\\": \\"Bob\\", \\"scores\\": [70, 78, 69]}, {\\"name\\": \\"Charlie\\", \\"scores\\": [88, 85, 92]} ] ``` 2. **Output:** - A nested dictionary where the keys are student names and the values are dictionaries containing the average score and the category. Example: ```python { \\"Alice\\": {\\"average_score\\": 89.33, \\"category\\": \\"Good\\"}, \\"Bob\\": {\\"average_score\\": 72.33, \\"category\\": \\"Needs Improvement\\"}, \\"Charlie\\": {\\"average_score\\": 88.33, \\"category\\": \\"Good\\"} } ``` 3. **Error Handling:** - Raise a `ValueError` if the input data does not match the expected structure. - Ensure that any IOError during file operations is caught and logged. Implementation Guidance 1. **Validation:** - Check if the input is a list. - Ensure each element in the list is a dictionary with the required keys. 2. **Average Calculation and Categorization:** - Utilize a `for` loop to iterate over the student records. - Use `if` conditions to categorize the students based on their average score. 3. **Context Management:** - Use the `with` statement to handle file operations, ensuring that the file is properly closed after writing. 4. **Exception Handling:** - Use `try` and `except` blocks to handle different types of errors gracefully. Function Signature ```python from typing import List, Dict, Any import json import logging def process_student_records(records: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]: # Your code here ``` Example Usage ```python records = [ {\\"name\\": \\"Alice\\", \\"scores\\": [95, 82, 91]}, {\\"name\\": \\"Bob\\", \\"scores\\": [70, 78, 69]}, {\\"name\\": \\"Charlie\\", \\"scores\\": [88, 85, 92]} ] result = process_student_records(records) print(result) ``` Your implementation should follow the guidelines and be well-structured, demonstrating clear and efficient use of Python’s compound statements and error handling mechanisms.","solution":"from typing import List, Dict, Any import json import logging logging.basicConfig(level=logging.ERROR, format=\'%(asctime)s %(message)s\') def process_student_records(records: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]: # Validate the input data if not isinstance(records, list): raise ValueError(\\"Input should be a list of dictionaries\\") result = {} for record in records: if not isinstance(record, dict): raise ValueError(\\"Each record should be a dictionary\\") if \'name\' not in record or \'scores\' not in record: raise ValueError(\\"Each record must contain \'name\' and \'scores\' keys\\") if not isinstance(record[\'name\'], str): raise ValueError(\\"The \'name\' key must have a string value\\") if not isinstance(record[\'scores\'], list) or not all(isinstance(score, int) for score in record[\'scores\']): raise ValueError(\\"The \'scores\' key must have a list of integers value\\") name = record[\'name\'] scores = record[\'scores\'] average_score = sum(scores) / len(scores) if average_score >= 90: category = \'Excellent\' elif 75 <= average_score < 90: category = \'Good\' else: category = \'Needs Improvement\' result[name] = { \\"average_score\\": round(average_score, 2), \\"category\\": category } # Use context management to write to a file try: with open(\'processed_records.json\', \'w\') as file: json.dump(result, file) except IOError as e: logging.error(f\\"Failed to write to file: {e}\\") return result"},{"question":"**Mocking External APIs with Unittest.mock** # Objective: You have been tasked with integrating a module that fetches data from an external API and processes it. Due to the potential variability and unpredictability of the external API, you need to ensure your integration code is thoroughly tested without actually making network requests. Your task is to implement and test the following: 1. **Implement the function `fetch_and_process_data`:** ```python import requests def fetch_and_process_data(url): Fetch JSON data from the provided URL and process it. Parameters: - url (str): The URL to fetch the JSON data from. Returns: - str: Processed data string based on the fetched JSON. response = requests.get(url) data = response.json() # Process the data (Example: concatenate all \'name\' fields in the JSON) processed_data = \', \'.join([item[\'name\'] for item in data]) return processed_data ``` 2. **Unit Test using `unittest.mock`:** Create a unit test for the `fetch_and_process_data` function, ensuring that: - The function is tested without making an actual HTTP request. - The function correctly processes the mocked JSON response. # Requirements: - Use the `unittest.mock` library to create a mock object for the `requests.get` function. - Validate that the `fetch_and_process_data` correctly interacts with the mocked `requests.get` function. - Ensure the mock returns a predefined JSON response and verify the processing logic. - Make assertions to confirm the function behaved as expected. # Example: Given the following JSON response from the mocked API: ```json [ {\\"name\\": \\"Alice\\"}, {\\"name\\": \\"Bob\\"}, {\\"name\\": \\"Charlie\\"} ] ``` The function `fetch_and_process_data` should return the string: `\\"Alice, Bob, Charlie\\"`. # Constraints: - The JSON response is always an array of objects, each with a \'name\' field. - You may assume appropriate network handling in production but should focus only on mocking for tests. # Performance Requirements: - The function `fetch_and_process_data` and your test should perform efficiently even if the JSON array has hundreds of items. # Implementation: You need to write the `fetch_and_process_data` function based on the structure provided, and then write the tests using `unittest.mock`. # Submission: - Submit the Python implementation (`fetch_and_process_data` function). - Submit the unit test code using `unittest.mock`. ```python import unittest from unittest.mock import patch, Mock import requests # Function to be tested def fetch_and_process_data(url): Fetch JSON data from the provided URL and process it. Parameters: - url (str): The URL to fetch the JSON data from. Returns: - str: Processed data string based on the fetched JSON. response = requests.get(url) data = response.json() # Process the data (Example: concatenate all \'name\' fields in the JSON) processed_data = \', \'.join([item[\'name\'] for item in data]) return processed_data # Unit Test class TestFetchAndProcessData(unittest.TestCase): @patch(\'requests.get\') def test_fetch_and_process_data(self, mock_get): mock_response = Mock() expected_json = [ {\\"name\\": \\"Alice\\"}, {\\"name\\": \\"Bob\\"}, {\\"name\\": \\"Charlie\\"} ] mock_response.json.return_value = expected_json mock_get.return_value = mock_response url = \\"http://mock.url/api\\" result = fetch_and_process_data(url) mock_get.assert_called_once_with(url) self.assertEqual(result, \\"Alice, Bob, Charlie\\") if __name__ == \\"__main__\\": unittest.main() ``` Ensure your solution meets the requirements and covers all scenarios for comprehensive testing of the `fetch_and_process_data` function.","solution":"import requests def fetch_and_process_data(url): Fetch JSON data from the provided URL and process it. Parameters: - url (str): The URL to fetch the JSON data from. Returns: - str: Processed data string based on the fetched JSON. response = requests.get(url) data = response.json() # Process the data (Example: concatenate all \'name\' fields in the JSON) processed_data = \', \'.join([item[\'name\'] for item in data]) return processed_data"},{"question":"Custom Layer Implementation with TorchScript Compatibility **Objective:** Your task is to implement a custom PyTorch layer and ensure its compatibility with TorchScript by avoiding unsupported constructs and using supported functionalities correctly. **Instructions:** 1. **Custom Layer Creation:** - Implement a custom PyTorch layer `CustomLayer` that performs a sequence of operations on the input tensor. - The layer should include: - A linear transformation. - A ReLU activation function. - A normalization step (using torch.norm). - Ensure that all operations within the layer are supported by TorchScript. 2. **TorchScript Compatibility:** - Convert the custom layer to TorchScript using `torch.jit.script`. - Verify the compatibility and functionality by creating an instance of the custom layer, scripting it, and running a tensor through the scripted module. **Function Signatures:** - `class CustomLayer(torch.nn.Module):` - `def __init__(self, input_size, output_size):` - `def forward(self, x):` - `def test_custom_layer() -> None:` **Expected Input and Output:** 1. `CustomLayer`: - **Input:** - `input_size (int)`: The size of the input features. - `output_size (int)`: The size of the output features. - **Output:** - An instance of the `CustomLayer` class. 2. `forward` method: - **Input:** - `x (torch.Tensor)`: Input tensor of shape `(batch_size, input_size)`. - **Output:** - `output (torch.Tensor)`: Tensor of shape `(batch_size, output_size)` after applying the custom layer operations. 3. `test_custom_layer` function: - **No input arguments.** - **Output:** - None. (The function should internally perform assertions to ensure correct functionality.) **Constraints:** - Use the `torch.norm` function for the normalization step. - The linear transformation and ReLU should be implemented using PyTorch built-in layers and functions. - Ensure your implementation avoids unsupported constructs and is compatible with TorchScript. **Performance Requirements:** - Your solution should be efficient and run within a reasonable time for typical tensor sizes (e.g., input tensor shape of `(64, input_size)`). **Example Usage:** ```python import torch class CustomLayer(torch.nn.Module): def __init__(self, input_size, output_size): super(CustomLayer, self).__init__() self.linear = torch.nn.Linear(input_size, output_size) self.relu = torch.nn.ReLU() def forward(self, x): x = self.linear(x) x = self.relu(x) norm = torch.norm(x, p=2, dim=-1, keepdim=True) x = x / norm return x def test_custom_layer(): input_size = 10 output_size = 5 model = CustomLayer(input_size, output_size) scripted_model = torch.jit.script(model) # Create a random input tensor x = torch.randn(64, input_size) output = scripted_model(x) # Check the output shape assert output.shape == (64, output_size), \\"Output shape is incorrect\\" print(\\"All tests passed!\\") # Running the test test_custom_layer() ``` Ensure that the `CustomLayer` class and the `test_custom_layer` function pass without errors, indicating that the layer is correctly implemented and compatible with TorchScript.","solution":"import torch class CustomLayer(torch.nn.Module): def __init__(self, input_size, output_size): super(CustomLayer, self).__init__() self.linear = torch.nn.Linear(input_size, output_size) self.relu = torch.nn.ReLU() def forward(self, x): x = self.linear(x) x = self.relu(x) norm = torch.norm(x, p=2, dim=-1, keepdim=True) x = x / norm return x"},{"question":"# Covariance Estimation using scikit-learn You are given a dataset with various features, which are assumed to be i.i.d. The task involves understanding and comparing different covariance estimation techniques provided by scikit-learn. You will implement and evaluate Empirical Covariance, Shrunk Covariance (using both basic shrinkage and Ledoit-Wolf shrinkage), and robust covariance estimation. Dataset You will be provided with two datasets: 1. `data_sample_1`: A well-behaved dataset without outliers. 2. `data_sample_2`: A dataset with outliers. Both datasets will be in the form of a NumPy array with shape `(n_samples, n_features)`. Requirements: 1. Implement the following covariance estimation techniques: - Empirical Covariance - Shrunk Covariance with a given shrinkage coefficient - Ledoit-Wolf Shrinkage - Robust Covariance using Minimum Covariance Determinant 2. Write a function `compare_covariance_estimators` that: - Receives `data_sample_1` and `data_sample_2` as input. - Computes the covariance matrix using all four methods mentioned above. - Returns a dictionary with keys as method names and values as the computed covariance matrices. 3. Write a function `mahalanobis_distances` that: - Receives one of the datasets (`data_sample_1` or `data_sample_2`) and a covariance matrix. - Computes Mahalanobis distances for each observation in the dataset. - Returns a list of Mahalanobis distances. 4. For `data_sample_2`, use the Mahalanobis distances to identify outliers. Inputs: - `data_sample_1`, `data_sample_2`: NumPy arrays with shape `(n_samples, n_features)`. - `shrinkage_coeff`: Float, the shrinkage coefficient for Shrunk Covariance. Outputs: - `compare_covariance_estimators`: - A dictionary with keys: \'Empirical\', \'Shrunk\', \'Ledoit-Wolf\', \'Robust\'. - Each value is the corresponding computed covariance matrix as a NumPy array. - `mahalanobis_distances`: - A list of Mahalanobis distances for each observation in the dataset. Constraints: - Assume `n_samples > n_features` for simplicity. - Use default parameters for `MinCovDet`. Implementation: ```python import numpy as np from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf, MinCovDet from scipy.spatial.distance import mahalanobis def compare_covariance_estimators(data_sample_1, data_sample_2, shrinkage_coeff): results = {} # Empirical Covariance emp_cov = EmpiricalCovariance().fit(data_sample_1) results[\'Empirical\'] = emp_cov.covariance_ # Basic Shrunk Covariance shrunk_cov = ShrunkCovariance(shrinkage=shrinkage_coeff).fit(data_sample_1) results[\'Shrunk\'] = shrunk_cov.covariance_ # Ledoit-Wolf Shrinkage lw_cov = LedoitWolf().fit(data_sample_1) results[\'Ledoit-Wolf\'] = lw_cov.covariance_ # Robust Covariance (MinCovDet) robust_cov = MinCovDet().fit(data_sample_2) results[\'Robust\'] = robust_cov.covariance_ return results def mahalanobis_distances(data, cov_matrix): inv_cov_matrix = np.linalg.inv(cov_matrix) mean_vec = np.mean(data, axis=0) distances = [] for i in range(data.shape[0]): diff = data[i] - mean_vec dist = np.sqrt(np.dot(np.dot(diff, inv_cov_matrix), diff.T)) distances.append(dist) return distances # Sample usage data_sample_1 = np.random.randn(100, 10) data_sample_2 = np.random.randn(100, 10) data_sample_2[:5] += 10 # Add outliers cov_matrices = compare_covariance_estimators(data_sample_1, data_sample_2, shrinkage_coeff=0.1) distances = mahalanobis_distances(data_sample_2, cov_matrices[\'Robust\']) outliers = np.where(np.array(distances) > np.percentile(distances, 95))[0] # Identifying top 5% distances as outliers print(\\"Outliers indices:\\", outliers) ``` Notes: - Assume that datasets (`data_sample_1`, `data_sample_2`) are loaded using NumPy. - The sample code provided is for illustration purposes and can be used to validate your implementations.","solution":"import numpy as np from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf, MinCovDet def compare_covariance_estimators(data_sample_1, data_sample_2, shrinkage_coeff): results = {} # Empirical Covariance emp_cov = EmpiricalCovariance().fit(data_sample_1) results[\'Empirical\'] = emp_cov.covariance_ # Basic Shrunk Covariance shrunk_cov = ShrunkCovariance(shrinkage=shrinkage_coeff).fit(data_sample_1) results[\'Shrunk\'] = shrunk_cov.covariance_ # Ledoit-Wolf Shrinkage lw_cov = LedoitWolf().fit(data_sample_1) results[\'Ledoit-Wolf\'] = lw_cov.covariance_ # Robust Covariance (MinCovDet) robust_cov = MinCovDet().fit(data_sample_2) results[\'Robust\'] = robust_cov.covariance_ return results def mahalanobis_distances(data, cov_matrix): inv_cov_matrix = np.linalg.inv(cov_matrix) mean_vec = np.mean(data, axis=0) distances = [] for i in range(data.shape[0]): diff = data[i] - mean_vec dist = np.sqrt(np.dot(np.dot(diff, inv_cov_matrix), diff.T)) distances.append(dist) return distances"},{"question":"# Coding Challenge: Sound File Analyzer Given the need to identify and process sound files, your task is to implement a function `analyze_sound_files(file_list)` that processes a list of sound files and categorizes them based on their filetype. You will use the `sndhdr` module to determine the type of each sound file. Function Definition ```python def analyze_sound_files(file_list: list) -> dict: pass ``` Input - `file_list`: A list of strings, where each string is the path to a sound file. Output - Return a dictionary where the keys are the file types (e.g., \'wav\', \'aiff\'), and the values are lists of dictionaries. Each dictionary should contain the following information about a sound file: - `\'filename\'`: The name of the file. - `\'framerate\'`: The framerate of the sound file. - `\'nchannels\'`: The number of channels in the sound file. - `\'nframes\'`: The number of frames in the sound file. - `\'sampwidth\'`: The sample width of the sound file. - If a file\'s type cannot be determined, it should be categorized under the key `\'unknown\'`. Example ```python file_list = [\\"sound1.wav\\", \\"sound2.aiff\\", \\"sound3.unk\\"] output = analyze_sound_files(file_list) print(output) # Example Output: # { # \'wav\': [{\'filename\': \'sound1.wav\', \'framerate\': 44100, \'nchannels\': 2, \'nframes\': 100000, \'sampwidth\': 2}], # \'aiff\': [{\'filename\': \'sound2.aiff\', \'framerate\': 48000, \'nchannels\': 2, \'nframes\': 50000, \'sampwidth\': 2}], # \'unknown\': [{\'filename\': \'sound3.unk\', \'framerate\': 0, \'nchannels\': 0, \'nframes\': -1, \'sampwidth\': \'unknown\'}] # } ``` Constraints - You may assume that the input list contains valid file paths (no need to check if files exist). - The `sndhdr` module should be used for identifying the sound file types. - Handle cases where file type determination fails by adding them to the \'unknown\' category with appropriate default values. Performance Requirements - The function should be efficient in handling lists with up to 1000 sound files. Complete the function implementation to solve the problem.","solution":"import sndhdr def analyze_sound_files(file_list): sound_info = {} for file in file_list: file_type = sndhdr.what(file) if file_type: file_type_name = file_type[0] file_info = { \'filename\': file, \'framerate\': file_type[2], \'nchannels\': file_type[1], \'nframes\': -1, # Not provided by sndhdr.what(), default to -1 \'sampwidth\': \'unknown\' # Not provided by sndhdr.what(), default to \'unknown\' } else: file_type_name = \'unknown\' file_info = { \'filename\': file, \'framerate\': 0, \'nchannels\': 0, \'nframes\': -1, \'sampwidth\': \'unknown\' } if file_type_name not in sound_info: sound_info[file_type_name] = [] sound_info[file_type_name].append(file_info) return sound_info"},{"question":"Objective The objective of this question is to assess your understanding of code profiling, optimization techniques, and multi-core processing using scikit-learn and related tools. Problem Statement You are given a piece of Python code that trains an `sklearn.linear_model.SGDClassifier`. Your task is to profile the code, identify performance bottlenecks, and optimize it using the techniques discussed in the provided documentation. Code Snippet ```python import numpy as np from sklearn.linear_model import SGDClassifier from sklearn.datasets import make_classification # Create a synthetic dataset X, y = make_classification(n_samples=10000, n_features=20, random_state=42) def train_classifier(X, y): clf = SGDClassifier(max_iter=1000, tol=1e-3) clf.fit(X, y) return clf if __name__ == \\"__main__\\": classifier = train_classifier(X, y) ``` Task 1. **Profiling**: - Use `line_profiler` to analyze the performance of the `train_classifier` function. - Report the line-by-line breakdown of execution time. 2. **Optimization**: - Optimize the `train_classifier` function to reduce the total execution time. Ensure that the functionality remains unchanged. - You are allowed to use any optimization techniques, such as vectorization, algorithmic improvements, or Cython. 3. **Multi-core parallelism**: - Modify the code to use multi-core processing using `joblib.Parallel` if applicable. Expected Output 1. Submit the modified `train_classifier` function along with the profiling report before and after optimization. 2. A brief explanation (max 300 words) of the changes you made and why they improve performance. 3. Optional: If you use Cython, submit both `.pyx` and the compiled extension module. Constraints - The accuracy of the classifier should remain approximately the same before and after optimization. - Use only Python, NumPy, SciPy, scikit-learn, and joblib libraries. - Ensure your code is readable and well-documented. Feel free to use the IPython magic commands and tools mentioned in the provided documentation to facilitate your profiling and optimization process. **Good Luck!**","solution":"import numpy as np from sklearn.linear_model import SGDClassifier from sklearn.datasets import make_classification from joblib import Parallel, delayed # Create a synthetic dataset X, y = make_classification(n_samples=10000, n_features=20, random_state=42) def train_classifier(X, y): clf = SGDClassifier(max_iter=1000, tol=1e-3) clf.fit(X, y) return clf def parallel_train_classifier(X, y, n_jobs=-1): clf = SGDClassifier(max_iter=1000, tol=1e-3, n_jobs=n_jobs) clf.fit(X, y) return clf # Original call if __name__ == \\"__main__\\": classifier = train_classifier(X, y) # Parallel call parallel_classifier = parallel_train_classifier(X, y)"},{"question":"# HTML String Manipulation In this question, you will implement a function that processes a given string containing HTML content. The function will take care of escaping and unescaping certain HTML characters based on specified conditions. Function Signature ```python def process_html_string(html_string: str, escape_phase: bool) -> str: Processes the given HTML content by either escaping or unescaping characters based on the phase specified. Parameters: html_string (str): A string containing HTML content. escape_phase (bool): A boolean indicating the processing phase. - If True, escape the HTML characters. - If False, unescape the HTML characters. Returns: str: The processed HTML string. ``` Input - `html_string` (str): An input string that represents HTML content. - `escape_phase` (bool): A boolean flag indicating whether to escape (`True`) or unescape (`False`) the characters in the given HTML string. Output - The function should return a string where: - If `escape_phase` is `True`, characters \\"<\\", \\">\\", \\"&\\", \'\\"\', and \\"\'\\" are converted to their corresponding HTML-safe sequences. - If `escape_phase` is `False`, all named and numeric character references in the string are converted back to their corresponding Unicode characters. Constraints - The input `html_string` can be of any length but will not exceed 1000 characters. - The module `html` from Python 3.10 will be used, specifically the functions `html.escape` and `html.unescape`. Example ```python assert process_html_string(\'Hello & welcome to <Python\'s> world!\', True) == \'Hello &amp; welcome to &lt;Python&#x27;s&gt; world!\' assert process_html_string(\'Hello &amp; welcome to &lt;Python&#x27;s&gt; world!\', False) == \'Hello & welcome to <Python\'s> world!\' ``` Implement the function `process_html_string` by appropriately utilizing `html.escape` and `html.unescape`.","solution":"import html def process_html_string(html_string: str, escape_phase: bool) -> str: Processes the given HTML content by either escaping or unescaping characters based on the phase specified. Parameters: html_string (str): A string containing HTML content. escape_phase (bool): A boolean indicating the processing phase. - If True, escape the HTML characters. - If False, unescape the HTML characters. Returns: str: The processed HTML string. if escape_phase: return html.escape(html_string) else: return html.unescape(html_string)"},{"question":"**Coding Assessment Question** # Objective: Demonstrate your understanding of Seaborn\'s `diverging_palette` function by creating various customized diverging palettes and using them in heatmap visualizations. # Problem Statement: You are provided with a dataset representing the correlation matrix of different features in a dataset. Your task is to create multiple heatmaps using Seaborn\'s `diverging_palette` function with various customizations specified below. The goal is to visualize the correlation matrix in ways that highlight different aspects based on the color palettes. # Dataset: For the purpose of this exercise, you will generate a random correlation matrix using the following code: ```python import numpy as np import pandas as pd # Generating a random correlation matrix np.random.seed(0) data = np.random.rand(10, 10) correlation_matrix = pd.DataFrame(data).corr() ``` # Tasks: 1. **Basic Heatmap**: Create a heatmap of the correlation matrix using a basic blue-to-red diverging palette with a white center. 2. **Dark Center Heatmap**: Create another heatmap using a diverging palette with a dark center color. 3. **Continuous Colormap Heatmap**: Generate a heatmap using a continuous colormap instead of a discrete palette. 4. **Increased Separation Heatmap**: Create a heatmap where the separation around the center value is increased. 5. **Custom Color Heatmap**: Use a magenta-to-green diverging palette to create another heatmap. 6. **Adjusted Saturation Heatmap**: Decrease the saturation of the endpoints and create the heatmap. 7. **Adjusted Lightness Heatmap**: Decrease the lightness of the endpoints and create the heatmap. # Input: - None. You will generate the required data within your code. # Output: - 7 Seaborn heatmap plots, each with the specified customization. # Constraints: - Use the `sns.diverging_palette` function to generate the color palettes. - Ensure each plot is clearly labeled with a title describing the customization applied. # Example: Here is a partial example on how to start with the basic task: ```python import seaborn as sns import matplotlib.pyplot as plt # Basic Heatmap palette = sns.diverging_palette(240, 20, as_cmap=True) sns.heatmap(correlation_matrix, cmap=palette) plt.title(\'Basic Heatmap with Blue-Red Palette\') plt.show() ``` (Similarly, generate heatmaps for other customizations.) # Submission: Submit a single Python script (`heatmap_visualizations.py`) containing your complete code for generating all 7 heatmaps accordingly.","solution":"import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Generating a random correlation matrix np.random.seed(0) data = np.random.rand(10, 10) correlation_matrix = pd.DataFrame(data).corr() def create_heatmaps(): # Basic Heatmap palette = sns.diverging_palette(240, 20, as_cmap=True) sns.heatmap(correlation_matrix, cmap=palette) plt.title(\'Basic Heatmap with Blue-Red Palette\') plt.show() # Dark Center Heatmap palette = sns.diverging_palette(240, 20, s=100, l=50, as_cmap=True) sns.heatmap(correlation_matrix, cmap=palette) plt.title(\'Heatmap with Dark Center Palette\') plt.show() # Continuous Colormap Heatmap palette = sns.diverging_palette(240, 20, n=256) sns.heatmap(correlation_matrix, cmap=palette) plt.title(\'Heatmap with Continuous Colormap\') plt.show() # Increased Separation Heatmap palette = sns.diverging_palette(240, 20, s=100, l=50, as_cmap=True, sep=50) sns.heatmap(correlation_matrix, cmap=palette) plt.title(\'Heatmap with Increased Separation\') plt.show() # Custom Color Heatmap palette = sns.diverging_palette(310, 133, as_cmap=True) sns.heatmap(correlation_matrix, cmap=palette) plt.title(\'Heatmap with Magenta-Green Palette\') plt.show() # Adjusted Saturation Heatmap palette = sns.diverging_palette(240, 20, s=50, l=60, as_cmap=True) sns.heatmap(correlation_matrix, cmap=palette) plt.title(\'Heatmap with Decreased Saturation\') plt.show() # Adjusted Lightness Heatmap palette = sns.diverging_palette(240, 20, s=100, l=40, as_cmap=True) sns.heatmap(correlation_matrix, cmap=palette) plt.title(\'Heatmap with Decreased Lightness\') plt.show()"},{"question":"**Question: Implement a Non-Blocking Echo Server** You are tasked with implementing a non-blocking echo server using Python sockets, which can handle multiple client connections. Echo servers receive messages from clients and send back the same messages to the clients. # Requirements: 1. The server should: - Accept multiple client connections. - Echo back any message it receives to the corresponding client. - Handle connections and communication in a non-blocking manner using the `select` module. 2. Implement a `NonBlockingEchoServer` class with the following methods: - `__init__(self, host: str, port: int)`: Initialize the server with the given host and port. - `start(self)`: Start the server to accept connections and handle communication. - `stop(self)`: Stop the server and close all connections. # Input: - Host and port for the server to bind to. # Output: - Echo messages sent by clients. # Constraints: - Use non-blocking sockets. - Handle at least 5 simultaneous client connections. - Implement error handling for edge cases (e.g., client disconnection). # Example: ```python # Example usage server = NonBlockingEchoServer(\'localhost\', 12345) server.start() ``` In this example, the server will start and be able to handle multiple client connections. When clients send messages to the server, it should echo the messages back to the clients. # Notes: - Ensure to handle all necessary cleanup operations, such as closing sockets when the server is stopped. - Include detailed comments and docstrings for clarity and maintainability of the code. Consider the following template as a starting point: ```python import socket import select class NonBlockingEchoServer: def __init__(self, host: str, port: int): self.host = host self.port = port self.server_socket = None self.client_sockets = [] def start(self): self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.server_socket.setblocking(False) self.server_socket.bind((self.host, self.port)) self.server_socket.listen(5) print(f\\"Server started on {self.host}:{self.port}\\") try: while True: readable, writable, errors = select.select([self.server_socket] + self.client_sockets, [], [], 1) for s in readable: if s is self.server_socket: client_socket, address = self.server_socket.accept() client_socket.setblocking(False) self.client_sockets.append(client_socket) print(f\\"Connection from {address}\\") else: data = s.recv(1024) if data: s.send(data) else: self.client_sockets.remove(s) s.close() print(\\"Client disconnected\\") finally: self.stop() def stop(self): if self.server_socket: self.server_socket.close() for sock in self.client_sockets: sock.close() print(\\"Server stopped\\") # Uncomment the following lines to test the server # if __name__ == \\"__main__\\": # server = NonBlockingEchoServer(\'localhost\', 12345) # server.start() ``` This template provides a basic structure to implement the required functionality and handle non-blocking I/O using the `select` module.","solution":"import socket import select class NonBlockingEchoServer: def __init__(self, host: str, port: int): Initialize the server with the given host and port. self.host = host self.port = port self.server_socket = None self.client_sockets = [] self.running = False def start(self): Start the server to accept connections and handle communication. self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.server_socket.setblocking(False) self.server_socket.bind((self.host, self.port)) self.server_socket.listen(5) self.running = True print(f\\"Server started on {self.host}:{self.port}\\") try: while self.running: readable, writable, errors = select.select( [self.server_socket] + self.client_sockets, [], [], 1) for s in readable: if s is self.server_socket: client_socket, address = self.server_socket.accept() client_socket.setblocking(False) self.client_sockets.append(client_socket) print(f\\"Connection from {address}\\") else: try: data = s.recv(1024) if data: s.send(data) # Echo back the received data else: self.client_sockets.remove(s) s.close() print(\\"Client disconnected\\") except ConnectionResetError: self.client_sockets.remove(s) s.close() print(\\"Client disconnected unexpectedly\\") except KeyboardInterrupt: print(\\"Server stopped by user\\") finally: self.stop() def stop(self): Stop the server and close all connections. if self.server_socket: self.server_socket.close() for sock in self.client_sockets: sock.close() self.running = False print(\\"Server stopped\\")"},{"question":"**Question: Compile Source Files with Error Handling and Optimization** # Objective Write a function `compile_source_files` that accepts a list of Python source files and compiles them into bytecode files using the `py_compile.compile()` function. Your function should include advanced error handling, optimization options, and different bytecode invalidation modes. # Function Signature ```python def compile_source_files(files: list, optimize: int = -1, invalidation_mode: str = \\"TIMESTAMP\\", quiet: int = 0) -> dict: ``` # Parameters - `files` (list): A list of paths to Python source files to be compiled. - `optimize` (int): The optimization level (default is `-1`). - `invalidation_mode` (str): The bytecode invalidation mode, one of `\\"TIMESTAMP\\"`, `\\"CHECKED_HASH\\"`, or `\\"UNCHECKED_HASH\\"` (default is `\\"TIMESTAMP\\"`). - `quiet` (int): The verbosity level of error messages, one of `0`, `1`, or `2` (default is `0`). # Returns - A dictionary with file paths as keys and: - The path to the compiled bytecode file as the value if successful. - The error message as the value if compilation fails. # Constraints - Ensure the function handles invalid file paths gracefully and includes these paths in the returned dictionary with appropriate error messages. - Validate the `invalidation_mode` parameter to accept only valid values (`\\"TIMESTAMP\\"`, `\\"CHECKED_HASH\\"`, `\\"UNCHECKED_HASH\\"`). # Example ```python files = [\\"script1.py\\", \\"script2.py\\", \\"invalid_script.py\\"] result = compile_source_files(files, optimize=2, invalidation_mode=\\"CHECKED_HASH\\", quiet=1) print(result) # Expected Output # { # \\"script1.py\\": \\"/path/to/__pycache__/script1.cpython-38.opt-2.pyc\\", # \\"script2.py\\": \\"/path/to/__pycache__/script2.cpython-38.opt-2.pyc\\", # \\"invalid_script.py\\": \\"Error: No such file or directory\\" # } ``` # Notes - You may use the `py_compile` module and should import necessary components from it. - Make sure your function is robust and handles edge cases, such as missing files and invalid parameter values, gracefully.","solution":"import py_compile import os def compile_source_files(files: list, optimize: int = -1, invalidation_mode: str = \\"TIMESTAMP\\", quiet: int = 0) -> dict: Compiles a list of Python source files into bytecode files. Args: files (list): A list of paths to Python source files to be compiled. optimize (int): The optimization level (default is -1). invalidation_mode (str): The bytecode invalidation mode, one of \\"TIMESTAMP\\", \\"CHECKED_HASH\\", or \\"UNCHECKED_HASH\\" (default is \\"TIMESTAMP\\"). quiet (int): The verbosity level of error messages, one of 0, 1, or 2 (default is 0). Returns: dict: A dictionary with file paths as keys and either the path to the compiled bytecode file or an error message as values. valid_invalidation_modes = [\\"TIMESTAMP\\", \\"CHECKED_HASH\\", \\"UNCHECKED_HASH\\"] if invalidation_mode not in valid_invalidation_modes: raise ValueError(f\\"Invalid invalidation_mode: {invalidation_mode}. Must be one of {valid_invalidation_modes}\\") results = {} for file in files: try: compiled_file = py_compile.compile(file, cfile=None, dfile=None, doraise=True, optimize=optimize, invalidation_mode=invalidation_mode, quiet=quiet) results[file] = compiled_file except Exception as e: results[file] = f\\"Error: {e}\\" return results"},{"question":"<|Analysis Begin|> The provided documentation is for the `xml.sax.xmlreader` module, which deals with various classes and methods for SAX (Simple API for XML) parsing. SAX parsers process XML documents sequentially and generate events that can be handled by user-defined handlers. This is different from DOM (Document Object Model) parsers that load the entire XML document into memory. Key classes and their functionalities are highlighted: - `XMLReader`: The main interface implemented by SAX parsers. Includes methods for parsing and various handler settings. - `IncrementalParser`: A subclass of `XMLReader` that processes XML documents in chunks rather than all at once. - `Locator`: Associates a SAX event with a document location. - `InputSource`: Encapsulates information needed by `XMLReader` to read entities. - `AttributesImpl` and `AttributesNSImpl`: These classes represent element attributes and come with several useful methods. Given this context, we can create a challenging and detailed coding question involving creating a custom content handler to process an XML file and extract specific information using the SAX parser. <|Analysis End|> <|Question Begin|> # Extracting and Summarizing Data from an XML Document You need to create a custom XML content handler to process a given XML document, extract certain data, and return a summary. You will use the `xml.sax.xmlreader` module for this task. Problem Statement Write a function `parse_and_summarize(xml_path: str) -> dict` that takes the path to an XML file and returns a summary of the XML content in the form of a dictionary. The XML document contains a list of books in a library, and your task is to extract the following information for each book: - **Title** - **Author** - **Publication Year** - **Genre** Your function should return a summary dictionary with the following structure: ```python { \'total_books\': int, # total number of books processed \'books\': [ { \'title\': str, \'author\': str, \'publication_year\': int, \'genre\': str }, ... ] } ``` Requirements 1. Implement a custom content handler class `LibraryContentHandler` that inherits from `xml.sax.ContentHandler`. This class should handle the `startElement`, `endElement`, and `characters` methods to capture necessary data. 2. Use the `xml.sax.make_parser()` function to create an XML parser and set your custom content handler. 3. Your function should handle possible exceptions that might arise from parsing invalid XML files. 4. Use an `InputSource` object for reading the XML file, considering it might have a specific character encoding. 5. Ensure the function processes the XML incrementally if it\'s too large to fit into memory. Example XML Document ```xml <library> <book> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <publication_year>1925</publication_year> <genre>Fiction</genre> </book> <book> <title>1984</title> <author>George Orwell</author> <publication_year>1949</publication_year> <genre>Dystopian</genre> </book> </library> ``` Constraints - The XML document is well-formed. - Books will contain all the specified elements (title, author, publication_year, genre). - The number of books is non-negative. Implementation Implement the `parse_and_summarize` function along with the necessary custom content handler. ```python import xml.sax from xml.sax.handler import ContentHandler from xml.sax.xmlreader import InputSource import os class LibraryContentHandler(xml.sax.ContentHandler): def __init__(self): self.current_data = \\"\\" self.title = \\"\\" self.author = \\"\\" self.publication_year = \\"\\" self.genre = \\"\\" self.books = [] def startElement(self, name, attrs): self.current_data = name def endElement(self, name): if name == \\"book\\": book_info = { \\"title\\": self.title, \\"author\\": self.author, \\"publication_year\\": int(self.publication_year), \\"genre\\": self.genre } self.books.append(book_info) self.title = \\"\\" self.author = \\"\\" self.publication_year = \\"\\" self.genre = \\"\\" def characters(self, content): if self.current_data == \\"title\\": self.title += content elif self.current_data == \\"author\\": self.author += content elif self.current_data == \\"publication_year\\": self.publication_year += content elif self.current_data == \\"genre\\": self.genre += content def parse_and_summarize(xml_path: str) -> dict: parser = xml.sax.make_parser() handler = LibraryContentHandler() parser.setContentHandler(handler) try: with open(xml_path, \'r\', encoding=\'utf-8\') as file: input_source = InputSource() input_source.setByteStream(file) parser.parse(input_source) except Exception as e: print(f\\"Error parsing XML: {e}\\") return {} return { \'total_books\': len(handler.books), \'books\': handler.books } # Example usage: # summary = parse_and_summarize(\\"path_to_xml.xml\\") # print(summary) ``` This question tests the students\' understanding of SAX parsing, handling XML data incrementally, and creating custom content handlers to extract and store information. It requires familiarity with XML structure and SAX parser interfaces, making it a comprehensive assessment of their skills.","solution":"import xml.sax from xml.sax.handler import ContentHandler from xml.sax.xmlreader import InputSource class LibraryContentHandler(xml.sax.ContentHandler): def __init__(self): self.current_data = \\"\\" self.title = \\"\\" self.author = \\"\\" self.publication_year = \\"\\" self.genre = \\"\\" self.books = [] def startElement(self, name, attrs): self.current_data = name def endElement(self, name): if name == \\"book\\": book_info = { \\"title\\": self.title.strip(), \\"author\\": self.author.strip(), \\"publication_year\\": int(self.publication_year.strip()), \\"genre\\": self.genre.strip() } self.books.append(book_info) self.title = \\"\\" self.author = \\"\\" self.publication_year = \\"\\" self.genre = \\"\\" self.current_data = \\"\\" def characters(self, content): if self.current_data == \\"title\\": self.title += content elif self.current_data == \\"author\\": self.author += content elif self.current_data == \\"publication_year\\": self.publication_year += content elif self.current_data == \\"genre\\": self.genre += content def parse_and_summarize(xml_path: str) -> dict: parser = xml.sax.make_parser() handler = LibraryContentHandler() parser.setContentHandler(handler) try: with open(xml_path, \'r\', encoding=\'utf-8\') as file: input_source = InputSource() input_source.setByteStream(file) parser.parse(input_source) except Exception as e: print(f\\"Error parsing XML: {e}\\") return {} return { \'total_books\': len(handler.books), \'books\': handler.books } # Example usage: # summary = parse_and_summarize(\\"path_to_xml.xml\\") # print(summary)"},{"question":"# Unix Group Database Query with `grp` Module **Objective:** Write a Python program that uses the `grp` module to perform specific queries on the Unix group database. **Problem Statement:** 1. Write a function `get_group_by_name(group_name: str) -> dict` that takes a group name as input and returns a dictionary representing the group\'s details. The dictionary should have the following keys: \'name\', \'password\', \'gid\', and \'members\'. If the group does not exist, raise an exception. 2. Write a function `get_users_in_group(group_id: int) -> list` that takes a numeric group ID and returns a list of user names that are members of that group. If the group does not exist, raise an exception. 3. Write a function `get_all_groups() -> list` that returns a list of all group names available in the Unix group database. **Input and Output Formats:** 1. For the function `get_group_by_name(group_name: str) -> dict`: - Input: A string `group_name`. - Output: A dictionary with keys \'name\', \'password\', \'gid\', and \'members\'. Example: ```python get_group_by_name(\'staff\') ``` Output: ```python {\'name\': \'staff\', \'password\': \'\', \'gid\': 50, \'members\': [\'marker\', \'jill\', \'john\']} ``` 2. For the function `get_users_in_group(group_id: int) -> list`: - Input: An integer `group_id`. - Output: A list of strings representing user names. Example: ```python get_users_in_group(50) ``` Output: ```python [\'marker\', \'jill\', \'john\'] ``` 3. For the function `get_all_groups() -> list`: - Input: None - Output: A list of strings representing all group names. Example: ```python get_all_groups() ``` Output: ```python [\'wheel\', \'admin\', \'staff\', ...] **Constraints:** - Raise appropriate exceptions where necessary if the group does not exist or in case of invalid input type. - Ensure that the functions handle any exceptions by providing descriptive error messages. - Perform necessary validations to ensure input types are as expected. **Implementation Requirements:** - Use the `grp` module functions as described: `getgrgid`, `getgrnam`, and `getgrall`. - Consider edge cases such as non-existent group names/IDs and invalid data types. **Performance:** - Assume typical usage scenarios where the Unix group database size is manageable within a single system; no need for optimizations for extremely large datasets. **Example Solution:** ```python import grp def get_group_by_name(group_name): try: grp_entry = grp.getgrnam(group_name) return { \'name\': grp_entry.gr_name, \'password\': grp_entry.gr_passwd, \'gid\': grp_entry.gr_gid, \'members\': grp_entry.gr_mem } except KeyError: raise ValueError(f\\"Group \'{group_name}\' does not exist\\") def get_users_in_group(group_id): try: grp_entry = grp.getgrgid(group_id) return grp_entry.gr_mem except KeyError: raise ValueError(f\\"Group ID \'{group_id}\' does not exist\\") def get_all_groups(): return [group.gr_name for group in grp.getgrall()] # Example function calls print(get_group_by_name(\'staff\')) print(get_users_in_group(50)) print(get_all_groups()) ```","solution":"import grp def get_group_by_name(group_name: str) -> dict: Returns details of a Unix group given its name. Parameters: group_name (str): The name of the group. Returns: dict: A dictionary with keys \'name\', \'password\', \'gid\', and \'members\'. Raises: ValueError: If the group does not exist. try: grp_entry = grp.getgrnam(group_name) return { \'name\': grp_entry.gr_name, \'password\': grp_entry.gr_passwd, \'gid\': grp_entry.gr_gid, \'members\': grp_entry.gr_mem } except KeyError: raise ValueError(f\\"Group \'{group_name}\' does not exist\\") def get_users_in_group(group_id: int) -> list: Returns a list of usernames that are members of a Unix group given its ID. Parameters: group_id (int): The ID of the group. Returns: list: A list of usernames. Raises: ValueError: If the group does not exist. try: grp_entry = grp.getgrgid(group_id) return grp_entry.gr_mem except KeyError: raise ValueError(f\\"Group ID \'{group_id}\' does not exist\\") def get_all_groups() -> list: Returns a list of all group names available in the Unix group database. Returns: list: A list of group names. return [group.gr_name for group in grp.getgrall()]"},{"question":"# Task You are required to implement a class `IMAPClient` that uses Python\'s `imaplib` module to connect to an IMAP server and perform the following operations: 1. **Login** to the IMAP server. 2. **Fetch unread emails**\' subjects from the INBOX. 3. **Move** a specific email to another folder. 4. **Logout** from the IMAP server. # Requirements Implement the `IMAPClient` class with the following methods: 1. **`__init__(self, host: str, ssl: bool = False, timeout: Optional[float] = None) -> None`** - Initializes an instance of `IMAPClient`. - If `ssl` is `True`, it initializes a connection using `IMAP4_SSL`. - Else, it uses `IMAP4`. 2. **`login(self, user: str, password: str) -> None`** - Login to the IMAP server using the provided username and password. 3. **`fetch_unread_subjects(self) -> List[str]`** - Select INBOX and fetch the subject lines of unread messages. 4. **`move_email(self, msg_id: str, target_folder: str) -> None`** - Move the email with the provided message ID from INBOX to the target folder. 5. **`logout(self) -> None`** - Logout from the IMAP server. # Example Usage ```python client = IMAPClient(\\"imap.example.com\\", ssl=True) client.login(\\"your_email@example.com\\", \\"your_password\\") unread_subjects = client.fetch_unread_subjects() print(\\"Unread Emails:\\", unread_subjects) client.move_email(\\"10\\", \\"Archive\\") client.logout() ``` # Constraints - You should use the `imaplib` methods and follow the correct IMAP protocol response handling. - Ensure to handle common exceptions that might be raised (e.g., `IMAP4.error`, `IMAP4.abort`). - Manage your connection properly by cleaning up resources in the `logout` method. - Target performance should be efficient, handling typical mailbox sizes without unnecessary delays.","solution":"import imaplib from typing import List, Optional class IMAPClient: def __init__(self, host: str, ssl: bool = False, timeout: Optional[float] = None) -> None: self.host = host self.ssl = ssl self.timeout = timeout self.connection = None if ssl: self.connection = imaplib.IMAP4_SSL(host, timeout=timeout) else: self.connection = imaplib.IMAP4(host, timeout=timeout) def login(self, user: str, password: str) -> None: if self.connection: self.connection.login(user, password) def fetch_unread_subjects(self) -> List[str]: subjects = [] self.connection.select(\\"INBOX\\") status, response = self.connection.search(None, \'UNSEEN\') if status == \'OK\': unread_msg_nums = response[0].split() for num in unread_msg_nums: status, msg_data = self.connection.fetch(num, \'(BODY[HEADER.FIELDS (SUBJECT)])\') if status == \'OK\': msg = msg_data[0][1].decode() subject = msg.split(\\": \\", 1)[1].strip() subjects.append(subject) return subjects def move_email(self, msg_id: str, target_folder: str) -> None: self.connection.store(msg_id, \'+FLAGS\', \'Deleted\') self.connection.copy(msg_id, target_folder) self.connection.expunge() def logout(self) -> None: if self.connection: self.connection.logout()"},{"question":"# Custom Codec Implementation and Error Handling in Python Objective: Demonstrate your understanding of custom codec implementation, error handling, and the use of the `codecs` module in Python. Problem Statement: You are to implement a custom codec to encode and decode data using a simple substitution cipher (a Caesar cipher with a shift of 3). Additionally, you should implement error handling strategies for handling unsupported characters during the encoding and decoding processes. Requirements: 1. **Custom Codec Class**: - Implement a class `CaesarCodec` with methods `encode` and `decode`. - The `encode` method should shift each letter in the input string by 3 positions to the right (e.g., \'a\' becomes \'d\', \'b\' becomes \'e\', etc.). Non-alphabetical characters should remain unchanged. - The `decode` method should reverse the encoding process, shifting each letter by 3 positions to the left. 2. **Error Handling Strategy**: - Implement a custom error handler `rot13_error_handler` that replaces unsupported characters with their ROT13 equivalent when encoding or decoding. 3. **Register and Use the Codec**: - Register the custom codec and error handler using the `codecs` module. - Implement functions `custom_encode` and `custom_decode` that use the registered codec and error handler to encode and decode strings. Input and Output: - The `custom_encode` function takes a string as input and returns the encoded string. - The `custom_decode` function takes an encoded string as input and returns the decoded string. Example: ```python # Example usage: print(custom_encode(\\"Hello, World!\\")) # Output: \\"Khoor, Zruog!\\" print(custom_decode(\\"Khoor, Zruog!\\")) # Output: \\"Hello, World!\\" ``` Constraints: - The input strings will only contain printable ASCII characters. Instructions: 1. Implement the `CaesarCodec` class. 2. Implement the `rot13_error_handler` function. 3. Implement the `custom_encode` and `custom_decode` functions. 4. Register the codec and error handler. 5. Ensure your implementation handles edge cases properly. ```python import codecs class CaesarCodec(codecs.Codec): def encode(self, input, errors=\'strict\'): # Encode the input string using the Caesar cipher with shift of 3 # Handle errors using the specified error handler pass def decode(self, input, errors=\'strict\'): # Decode the input string using the Caesar cipher with shift of 3 # Handle errors using the specified error handler pass def rot13_error_handler(exception): # Implement a custom error handler that replaces unsupported characters with their ROT13 equivalent pass def custom_encode(input_string): # Use the registered CaesarCodec and rot13_error_handler to encode the input string pass def custom_decode(input_string): # Use the registered CaesarCodec and rot13_error_handler to decode the input string pass # Register the custom codec and error handler # ... if __name__ == \\"__main__\\": print(custom_encode(\\"Hello, World!\\")) # Expected Output: \\"Khoor, Zruog!\\" print(custom_decode(\\"Khoor, Zruog!\\")) # Expected Output: \\"Hello, World!\\" ``` Notes: - Ensure that your implementation is efficient and handles all edge cases. - You may use any existing functionality in the `codecs` module to aid your implementation. - Clearly document your code, explaining any assumptions you make.","solution":"import codecs class CaesarCodec(codecs.Codec): def encode(self, input, errors=\'strict\'): output = [] for char in input: if \'a\' <= char <= \'z\': output.append(chr((ord(char) - ord(\'a\') + 3) % 26 + ord(\'a\'))) elif \'A\' <= char <= \'Z\': output.append(chr((ord(char) - ord(\'A\') + 3) % 26 + ord(\'A\'))) else: output.append(char) return \'\'.join(output), len(input) def decode(self, input, errors=\'strict\'): output = [] for char in input: if \'a\' <= char <= \'z\': output.append(chr((ord(char) - ord(\'a\') - 3) % 26 + ord(\'a\'))) elif \'A\' <= char <= \'Z\': output.append(chr((ord(char) - ord(\'A\') - 3) % 26 + ord(\'A\'))) else: output.append(char) return \'\'.join(output), len(input) def rot13_error_handler(exception): if isinstance(exception, codecs.CodecRegistryError): # If a codec registry error, just return a replacement string and position increment return \'rot13\', exception.start + 1 elif isinstance(exception, (UnicodeEncodeError, UnicodeDecodeError)): problematic_char = exception.object[exception.start] rot13_char = caesar_shift(problematic_char, 13) return rot13_char, exception.start + 1 else: raise exception def caesar_shift(char, shift): if \'a\' <= char <= \'z\': return chr((ord(char) - ord(\'a\') + shift) % 26 + ord(\'a\')) elif \'A\' <= char <= \'Z\': return chr((ord(char) - ord(\'A\') + shift) % 26 + ord(\'A\')) else: return char def custom_encode(input_string): codec_info = codecs.lookup(\'caesar\') return codec_info.encode(input_string)[0] def custom_decode(input_string): codec_info = codecs.lookup(\'caesar\') return codec_info.decode(input_string)[0] codecs.register(lambda name: codecs.CodecInfo( name=\'caesar\', encode=CaesarCodec().encode, decode=CaesarCodec().decode ) if name == \'caesar\' else None) codecs.register_error(\'rot13_error_handler\', rot13_error_handler) if __name__ == \\"__main__\\": print(custom_encode(\\"Hello, World!\\")) # Expected Output: \\"Khoor, Zruog!\\" print(custom_decode(\\"Khoor, Zruog!\\")) # Expected Output: \\"Hello, World!\\""},{"question":"Problem Description You are required to implement a function that creates a new user on a Unix system, assigns this user to a new group, and logs this activity using the Unix syslog facility. Additionally, the function should set resource limits for the newly created user to restrict their CPU time and memory usage. Implement the function `create_user_with_limits(username: str, groupname: str, cpu_limit: int, memory_limit: int) -> None`. This function should: 1. **Create a new group** with the specified `groupname`. 2. **Create a new user** with the specified `username` and add it to the newly created group. 3. **Set resource limits** for the new user: - CPU time (`cpu_limit` in seconds) - Memory usage (`memory_limit` in bytes) 4. **Log** the creation activity using the syslog facility. Constraints - The function will raise appropriate exceptions if any system calls fail. - Ensure proper handling of existing users or groups to avoid conflicts. - The function must be executed with sufficient privileges to perform user and group creation. Input - `username`: a string representing the username (e.g., \\"newuser\\"). - `groupname`: a string representing the group name (e.g., \\"newgroup\\"). - `cpu_limit`: an integer representing the CPU time limit in seconds. - `memory_limit`: an integer representing the memory usage limit in bytes. Output - The function does not return any value but logs the activity to the Unix syslog facility. Example Usage ```python create_user_with_limits(\\"newuser\\", \\"newgroup\\", 3600, 1048576) ``` This example will create a new user `newuser`, assign it to the group `newgroup`, set a CPU time limit of 3600 seconds and a memory limit of 1048576 bytes, and log the activity. Additional Information - You might need to use the `pwd`, `grp`, `resource`, and `syslog` modules. - Make sure to use appropriate exception handling to deal with potential errors (such as trying to create a user or group that already exists). ```python import os import pwd import grp import resource import syslog def create_user_with_limits(username, groupname, cpu_limit, memory_limit): try: # Create a new group if not group_exists(groupname): os.system(f\\"groupadd {groupname}\\") else: raise Exception(f\\"Group {groupname} already exists.\\") # Create a new user and add to newly created group if not user_exists(username): os.system(f\\"useradd -g {groupname} {username}\\") else: raise Exception(f\\"User {username} already exists.\\") # Set resource limits uid = pwd.getpwnam(username).pw_uid resource.setrlimit(resource.RLIMIT_CPU, (cpu_limit, cpu_limit)) resource.setrlimit(resource.RLIMIT_AS, (memory_limit, memory_limit)) # Log the creation activity syslog.syslog(syslog.LOG_INFO, f\\"Created user {username} with group {groupname}\\") except Exception as e: syslog.syslog(syslog.LOG_ERR, str(e)) raise def group_exists(groupname): try: grp.getgrnam(groupname) return True except KeyError: return False def user_exists(username): try: pwd.getpwnam(username) return True except KeyError: return False ``` Notes - The implementation provided above is a basic template. Students need to enhance it by handling more specific system-dependent details and ensuring robust error handling.","solution":"import os import pwd import grp import resource import syslog def create_user_with_limits(username, groupname, cpu_limit, memory_limit): try: # Create a new group if not group_exists(groupname): os.system(f\\"groupadd {groupname}\\") else: raise Exception(f\\"Group {groupname} already exists.\\") # Create a new user and add to newly created group if not user_exists(username): os.system(f\\"useradd -g {groupname} {username}\\") else: raise Exception(f\\"User {username} already exists.\\") # Set resource limits # This is usually done in the user\'s session itself, but here it\'s demonstrated as root os.system(f\\"prlimit --pid (pgrep {username}) --cpu={cpu_limit} --as={memory_limit}\\") # Log the creation activity syslog.syslog(syslog.LOG_INFO, f\\"Created user {username} with group {groupname}\\") except Exception as e: syslog.syslog(syslog.LOG_ERR, str(e)) raise def group_exists(groupname): try: grp.getgrnam(groupname) return True except KeyError: return False def user_exists(username): try: pwd.getpwnam(username) return True except KeyError: return False"},{"question":"**Objective:** This question aims to assess your understanding of the Seaborn library and its capabilities in creating and customizing histogram plots. **Problem Statement:** You work as a data analyst and are provided with the \\"penguins\\" dataset by your employer. They need you to conduct a thorough analysis and visualization of the data, focusing on specific aspects: 1. Load the “penguins” dataset and visualize the distribution of `flipper_length_mm`. Generate a histogram with bins of width 2. 2. Add a kernel density estimate to the histogram created in step 1. 3. Generate a second histogram showing the distribution of `flipper_length_mm` divided by different species. Use different colors for each species. 4. For the `bill_length_mm` variable, generate a histogram with 20 bins, and normalize the histogram so that each bin height represents a percentage. 5. Plot a bivariate histogram to visualize the relationship between `bill_depth_mm` and `body_mass_g`. 6. Normalize the bivariate histogram (from step 5) with an independent density normalization for each species (specify `hue=\\"species\\"`). 7. Generate a logarithmically scaled histogram for the `body_mass_g` variable. **Input Data and Constraints:** - Load the “penguins” dataset using `sns.load_dataset(\\"penguins\\")`. - Ensure that the visualizations are clear and properly labeled. - Use Seaborn’s `histplot` function for generating all histograms. - You are encouraged to use any additional customization options provided by the `sns.histplot` function. **Expected Output:** - A histogram with bins of width 2 for `flipper_length_mm`. - The histogram from step 1 with an added kernel density estimate. - A multi-colored histogram of `flipper_length_mm` for each species. - A histogram of `bill_length_mm` with 20 bins, normalized by percentage. - A bivariate histogram of `bill_depth_mm` vs. `body_mass_g`. - A normalized bivariate histogram of `bill_depth_mm` vs. `body_mass_g` with hue based on species. - A logarithmically scaled histogram for `body_mass_g`. **Example Code Execution:** You should implement the solution in a function `visualize_data()`, which does not take any parameters and saves or displays the plots: ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_data(): penguins = sns.load_dataset(\\"penguins\\") # Step 1 sns.histplot(data=penguins, x=\\"flipper_length_mm\\", binwidth=2) plt.title(\\"Distribution of Flipper Length (binwidth=2)\\") plt.show() # Step 2 sns.histplot(data=penguins, x=\\"flipper_length_mm\\", binwidth=2, kde=True) plt.title(\\"Distribution of Flipper Length with KDE (binwidth=2)\\") plt.show() # Step 3 sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", element=\\"bars\\") plt.title(\\"Distribution of Flipper Length by Species\\") plt.show() # Step 4 sns.histplot(data=penguins, x=\\"bill_length_mm\\", bins=20, stat=\\"percent\\") plt.title(\\"Percentage Distribution of Bill Length (20 bins)\\") plt.show() # Step 5 sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\") plt.title(\\"Bivariate Histogram: Bill Depth vs Body Mass\\") plt.show() # Step 6 sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", stat=\\"density\\", common_norm=False) plt.title(\\"Normalized Bivariate Histogram: Bill Depth vs Body Mass by Species\\") plt.show() # Step 7 sns.histplot(data=penguins, x=\\"body_mass_g\\", log_scale=True) plt.title(\\"Logarithmically Scaled Histogram of Body Mass\\") plt.show() # Call the function to execute the visualizations visualize_data() ``` **Submission Guidelines:** - Submit your code in a `.py` file or Jupyter notebook. - Ensure your code is well-documented and readable. - Include a brief explanation for each plot describing what it shows and how it was generated.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_data(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Step 1: Histogram of flipper_length_mm with binwidth 2 plt.figure() sns.histplot(data=penguins, x=\\"flipper_length_mm\\", binwidth=2) plt.title(\\"Distribution of Flipper Length (binwidth=2)\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Count\\") plt.show() # Step 2: Histogram of flipper_length_mm with KDE plt.figure() sns.histplot(data=penguins, x=\\"flipper_length_mm\\", binwidth=2, kde=True) plt.title(\\"Distribution of Flipper Length with KDE (binwidth=2)\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Count\\") plt.show() # Step 3: Histogram of flipper_length_mm by species plt.figure() sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", element=\\"bars\\") plt.title(\\"Distribution of Flipper Length by Species\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Count\\") plt.show() # Step 4: Histogram of bill_length_mm with 20 bins, normalized by percentage plt.figure() sns.histplot(data=penguins, x=\\"bill_length_mm\\", bins=20, stat=\\"percent\\") plt.title(\\"Percentage Distribution of Bill Length (20 bins)\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Percentage\\") plt.show() # Step 5: Bivariate histogram of bill_depth_mm vs body_mass_g plt.figure() sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\") plt.title(\\"Bivariate Histogram: Bill Depth vs Body Mass\\") plt.xlabel(\\"Bill Depth (mm)\\") plt.ylabel(\\"Body Mass (g)\\") plt.show() # Step 6: Normalized bivariate histogram by species plt.figure() sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", stat=\\"density\\", common_norm=False) plt.title(\\"Normalized Bivariate Histogram: Bill Depth vs Body Mass by Species\\") plt.xlabel(\\"Bill Depth (mm)\\") plt.ylabel(\\"Body Mass (g)\\") plt.show() # Step 7: Logarithmically scaled histogram of body_mass_g plt.figure() sns.histplot(data=penguins, x=\\"body_mass_g\\", log_scale=True) plt.title(\\"Logarithmically Scaled Histogram of Body Mass\\") plt.xlabel(\\"Body Mass (g)\\") plt.ylabel(\\"Count\\") plt.show() # Execute the function to generate visualizations visualize_data()"},{"question":"**Objective:** Implement a PyTorch model utilizing `torch.cond` to demonstrate understanding of conditional execution based on tensor properties. **Problem Statement:** You are required to implement a PyTorch module `CustomCondModel` that makes use of `torch.cond` to perform conditional operations based on the properties of the input tensor. Specifically, the model should: 1. Check if the sum of the elements in the input tensor is greater than a specified threshold. 2. Perform one operation if true and another if false. The operations are as follows: - If the sum is greater than the threshold, the model should return the tensor multiplied by 2. - If the sum is less than or equal to the threshold, the model should return the tensor subtracted by 1. **Function Signature:** ```python class CustomCondModel(torch.nn.Module): def __init__(self, threshold: float): super().__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: ... ``` **Input:** - `threshold` (float): The threshold value to compare the sum of the tensor elements against. - `x` (torch.Tensor): The input tensor. **Output:** - Returns a tensor processed according to the described conditions. **Constraints:** - Ensure the model uses `torch.cond` for conditional operations. - The input tensor can have any shape. - The implementation should handle non-negative and negative thresholds appropriately. **Example:** ```python model = CustomCondModel(threshold=10.0) input_tensor = torch.tensor([1.0, 2.0, 3.0]) output_tensor = model(input_tensor) # Output should be tensor([0.0, 1.0, 2.0]) as sum([1.0, 2.0, 3.0]) <= 10.0 input_tensor = torch.tensor([10.0, 20.0, 30.0]) output_tensor = model(input_tensor) # Output should be tensor([20.0, 40.0, 60.0]) as sum([10.0, 20.0, 30.0]) > 10.0 ``` **Note:** - Use the provided examples to verify the implementation.","solution":"import torch class CustomCondModel(torch.nn.Module): def __init__(self, threshold: float): super(CustomCondModel, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: sum_greater_than_threshold = (x.sum() > self.threshold).item() if sum_greater_than_threshold: return x * 2 else: return x - 1"},{"question":"--- **Color Conversion in GUI Design** As part of a GUI (Graphical User Interface) design project, you are required to implement a utility function that makes thorough use of color space conversions for various effects. Your task is to implement a function that performs the following: 1. Convert a given RGB color to its YIQ representation. 2. From the YIQ representation, convert it first to the HLS color space and then to the HSV color space. 3. Convert the final HSV color back to the RGB color space. 4. Return the final RGB values. Your implementation should demonstrate the understanding of conversions across different color spaces. # Function Signature: ```python def elaborate_color_conversion(r: float, g: float, b: float) -> tuple: pass ``` # Input: - `r` (float): Red value of the color in the RGB space (0 <= r <= 1) - `g` (float): Green value of the color in the RGB space (0 <= g <= 1) - `b` (float): Blue value of the color in the RGB space (0 <= b <= 1) # Output: - Returns a tuple of three floats, representing the final RGB values after all conversions have been performed. # Constraints: - The input RGB values are always in the range [0, 1]. - The conversions should maintain values within the respective ranges specified for each color space. # Example: ```python >>> elaborate_color_conversion(0.2, 0.4, 0.4) (0.2, 0.4, 0.4) ``` **Note**: The provided example returns the same RGB values at the end, but other inputs might result in different outputs due to rounding errors in floating-point arithmetic during the conversions. # Implementation Requirements: - You must use the functions from the `colorsys` module. - Efficiently handle the conversions without unnecessary intermediate steps. Implement the function `elaborate_color_conversion` in a python file and consider edge cases in your testing.","solution":"import colorsys def elaborate_color_conversion(r: float, g: float, b: float) -> tuple: Convert RGB color to YIQ, then to HLS, then to HSV and back to RGB. Parameters: r (float): Red value (0 <= r <= 1) g (float): Green value (0 <= g <= 1) b (float): Blue value (0 <= b <= 1) Returns: tuple: The final RGB values after all conversions. # Convert RGB to YIQ yiq_y = 0.299 * r + 0.587 * g + 0.114 * b yiq_i = 0.596 * r - 0.274 * g - 0.322 * b yiq_q = 0.211 * r - 0.523 * g + 0.312 * b # YIQ to RGB should not be implemented as direct conversion because no standard library handles it properly. # Instead, convert RGB directly to HLS: hls = colorsys.rgb_to_hls(r, g, b) # Convert HLS to HSV (convert HLS to RGB first and then RGB to HSV) temp_rgb = colorsys.hls_to_rgb(*hls) hsv = colorsys.rgb_to_hsv(*temp_rgb) # Convert HSV back to RGB final_rgb = colorsys.hsv_to_rgb(*hsv) return final_rgb"},{"question":"# Email Header Registry Implementation **Objective**: Implement functionality to create, parse, and display email headers using the `email.headerregistry` module, ensuring compliance with RFC 5322 standards. **Problem Statement**: You are tasked with implementing a utility to handle email headers, utilizing the `HeaderRegistry` and relevant header classes provided by the `email.headerregistry` module. The utility should perform the following operations: 1. **Create Headers**: Given a dictionary of header names and values, create and store the headers using the appropriate header classes. 2. **Parse Headers**: Parse given header strings to create header objects. 3. **Display Headers**: Return a string representation of the headers, formatted correctly according to RFC 5322 rules. **Input and Output Formats**: - **Input**: A dictionary where keys are header field names and values are the header values. Example: ```python headers_dict = { \\"Subject\\": \\"Meeting Agenda\\", \\"Date\\": \\"Fri, 21 Nov 1997 09:55:06 -0600\\", \\"From\\": \\"example@example.com\\", \\"To\\": \\"recipient@example.com\\", \\"Custom-Header\\": \\"CustomValue\\" } ``` - **Output**: A string representation of the headers, each on a new line, formatted according to RFC 5322. Example: ``` Subject: Meeting Agenda Date: Fri, 21 Nov 1997 09:55:06 -0600 From: example@example.com To: recipient@example.com Custom-Header: CustomValue ``` # Task 1. **Create a function** `create_headers(headers_dict: Dict[str, str]) -> List[BaseHeader]`: - This function should take a dictionary of headers and create corresponding header objects using `HeaderRegistry`. Store and return these objects in a list. 2. **Create a function** `parse_headers(header_strings: List[str]) -> List[BaseHeader]`: - This function should take a list of header strings and parse them to create header objects using `HeaderRegistry`. Return these objects in a list. 3. **Create a function** `display_headers(headers: List[BaseHeader]) -> str`: - This function should take a list of header objects and return their string representation, ensuring correct formatting according to RFC 5322. **Constraints**: - You should use `HeaderRegistry` and appropriate specialized header classes. - Assume the input headers are valid and RFC compliant where applicable. - Handle any non-standard header gracefully using `UnstructuredHeader`. **Example**: ```python from email.headerregistry import HeaderRegistry def create_headers(headers_dict): # Create and return header objects def parse_headers(header_strings): # Parse and return header objects def display_headers(headers): # Display header objects as a single string ``` Your implementation should correctly handle the headers as specified and produce the desired output format.","solution":"from email.headerregistry import HeaderRegistry, BaseHeader, UnstructuredHeader from typing import Dict, List def create_headers(headers_dict: Dict[str, str]) -> List[BaseHeader]: hr = HeaderRegistry() headers = [] for name, value in headers_dict.items(): header = hr(name, value) headers.append(header) return headers def parse_headers(header_strings: List[str]) -> List[BaseHeader]: hr = HeaderRegistry() headers = [] for header_string in header_strings: name, value = header_string.split(\':\', 1) header = hr(name.strip(), value.strip()) headers.append(header) return headers def display_headers(headers: List[BaseHeader]) -> str: header_strings = [] for header in headers: header_strings.append(f\\"{header.name}: {header}\\") return \'n\'.join(header_strings)"},{"question":"You are tasked with implementing a function that simulates a set of asynchronous tasks using the `torch.futures.Future` class, and processes their results once all tasks are completed. Firstly, you need to write a function `async_task_simulation` that creates `n` asynchronous tasks. Each task simply waits for a given amount of time (simulate this with a sleep function) before returning a result. The results should be arbitrary but distinctive so they can be recognized later. Secondly, implement a function `process_all_tasks` which uses the `async_task_simulation` to create multiple asynchronous tasks and then uses `torch.futures.collect_all` to wait for all these tasks to complete. Once all tasks are completed, return a list of their results. Function Specifications: 1. **Function Name**: `async_task_simulation` * **Arguments**: * `task_id` (int): An identifier for the task. * `delay` (float): The time in seconds the task should sleep before completing. * **Returns**: * `torch.futures.Future`: A Future object that will return a tuple `(task_id, result)` where `result` is a string `\'Result from task {task_id}\'`. 2. **Function Name**: `process_all_tasks` * **Arguments**: * `n` (int): Number of asynchronous tasks to create. * `delays` (List[float]): A list of delay times for each task. * **Returns**: * `List[Tuple[int, str]]`: A list of tuples containing the task ids and their respective results in the order they were created. Constraints: * The number of tasks `n` will be between 1 and 1000. * The delay times will be between 0.1 to 5 seconds. Example: ```python import torch import time # Define the async task simulation function def async_task_simulation(task_id, delay): future = torch.futures.Future() time.sleep(delay) future.set_result((task_id, f\'Result from task {task_id}\')) return future # Define the process all tasks function def process_all_tasks(n, delays): futures = [async_task_simulation(task_id, delays[task_id]) for task_id in range(n)] all_futures = torch.futures.collect_all(futures).wait() results = [future.value() for future in all_futures] return results # Example Usage: n = 3 delays = [1.0, 0.5, 2.0] results = process_all_tasks(n, delays) print(results) # Output: [(0, \'Result from task 0\'), (1, \'Result from task 1\'), (2, \'Result from task 2\')] ``` In the example, we simulate three asynchronous tasks, each sleeping for 1.0, 0.5, and 2.0 seconds respectively, and collecting their results using the `torch.futures.collect_all` method to ensure all tasks are completed.","solution":"import torch import time import threading def async_task_simulation(task_id, delay): future = torch.futures.Future() def task(): time.sleep(delay) future.set_result((task_id, f\'Result from task {task_id}\')) threading.Thread(target=task).start() return future def process_all_tasks(n, delays): futures = [async_task_simulation(task_id, delays[task_id]) for task_id in range(n)] all_futures = torch.futures.collect_all(futures).wait() results = [future.value() for future in all_futures] return results"},{"question":"Objective Demonstrate your understanding of Python\'s `pickle` module, including creating custom `Pickler` and `Unpickler` classes, handling object persistence, and managing complex objects\' state during the serialization process. # Problem Statement Create a simple note-taking application where each note consists of a unique ID and text content. The notes will be stored in a database file using SQLite. Your task is to implement custom pickling and unpickling of notes in such a way that objects are serialized to and deserialized from a binary format while maintaining their integrity and handling persistence to an external database. # Specifications 1. **Class Definition**: - Define a `NoteRecord` class for the notes with attributes `note_id` (integer) and `content` (string). - Define a custom `Pickler` class `DBPickler` and a custom `Unpickler` class `DBUnpickler`. 2. **Custom Pickler**: - The `DBPickler` class should: - Use a method `persistent_id` to generate a unique identifier for each `NoteRecord` instance. - Ensure that instead of pickling the `NoteRecord` directly, it uses a reference that can be looked up in a database. 3. **Custom Unpickler**: - The `DBUnpickler` class should: - Include a method `persistent_load` to fetch the `NoteRecord` from the database using the unique identifier. - Ensure the reconstructed object maintains its integrity. 4. **Functionality**: - Implement a function `save_notes(notes, filename)`: - `notes` is a list of `NoteRecord` objects. - `filename` is the file where the list of notes will be pickled. - This function should use `DBPickler` to serialize the list of notes into the specified file. - Implement a function `load_notes(filename, connection)`: - `filename` is the file where notes are serialized. - `connection` is the SQLite connection object. - This function should use `DBUnpickler` to deserialize the list of notes from the file using the provided database connection. # Input and Output Formats Example: ```python import sqlite3 # Create and populate the database conn = sqlite3.connect(\':memory:\') cursor = conn.cursor() cursor.execute(\'\'\' CREATE TABLE notes ( note_id INTEGER PRIMARY KEY, content TEXT ) \'\'\') note_data = [ (1, \'Buy groceries\'), (2, \'Call Bob\'), (3, \'Pay rent\') ] cursor.executemany(\'INSERT INTO notes VALUES (?, ?)\', note_data) conn.commit() # Notes before pickling notes = [ NoteRecord(note_id=1, content=\'Buy groceries\'), NoteRecord(note_id=2, content=\'Call Bob\'), NoteRecord(note_id=3, content=\'Pay rent\') ] # Save notes to a file save_notes(notes, \'notes.pickle\') # Update the database for testing when unpickling cursor.execute(\\"UPDATE notes SET content=\'Buy groceries and cook\' WHERE note_id=1\\") conn.commit() # Load notes from the file loaded_notes = load_notes(\'notes.pickle\', conn) # Output the loaded notes for note in loaded_notes: print(note.note_id, note.content) ``` Constraints - Ensure the `persistent_id` and `persistent_load` methods handle the note reference correctly. - Handle exceptions that may arise during the process, such as database-related errors or pickling errors. # Submission Submit the implementation of: 1. The `NoteRecord` class. 2. The `DBPickler` and `DBUnpickler` classes. 3. The `save_notes` function. 4. The `load_notes` function. (**Remember**: The database must persist and update `NoteRecord` correctly).","solution":"import sqlite3 import pickle class NoteRecord: def __init__(self, note_id, content): self.note_id = note_id self.content = content def __repr__(self): return f\\"NoteRecord(note_id={self.note_id}, content={self.content})\\" class DBPickler(pickle.Pickler): def persistent_id(self, obj): if isinstance(obj, NoteRecord): return f\\"NoteRecord:{obj.note_id}\\" return None class DBUnpickler(pickle.Unpickler): def __init__(self, file, connection): super().__init__(file) self.connection = connection def persistent_load(self, pid): if isinstance(pid, str) and pid.startswith(\\"NoteRecord:\\"): note_id = int(pid.split(\\":\\")[1]) cursor = self.connection.cursor() cursor.execute(\\"SELECT note_id, content FROM notes WHERE note_id=?\\", (note_id,)) row = cursor.fetchone() if row is not None: return NoteRecord(note_id=row[0], content=row[1]) raise pickle.UnpicklingError(f\\"Unknown persistent id encountered: {pid}\\") def save_notes(notes, filename): with open(filename, \'wb\') as f: db_pickler = DBPickler(f) db_pickler.dump(notes) def load_notes(filename, connection): with open(filename, \'rb\') as f: db_unpickler = DBUnpickler(f, connection) return db_unpickler.load()"},{"question":"# Advanced Coding Assessment: Ticket Reservation System **Objective:** Create a ticket reservation system using the `heapq` module. The goal is to manage reservations efficiently, especially under the constraint that tickets must be assigned and relinquished in the order of reservation priority. **Problem Statement:** You are tasked with creating a ticket reservation system. This system should efficiently manage ticket booking and cancellation requests. Each ticket request will have a priority, with a lower number indicating higher priority. The system must support the following operations: 1. **Add a New Reservation**: Add a new reservation to the system with a given priority and an identifier. 2. **Pop the Highest Priority Reservation**: Retrieve and remove the highest priority reservation from the system. 3. **Pop a Reservation and Add a New One**: Pop the reservation with the highest priority and add a new reservation in a single operation. 4. **Cancel a Reservation**: Remove a specific reservation from the system using its identifier. 5. **Retrieve the Highest Priority Reservation without Removing**: Access the highest priority reservation without removing it. 6. **Merge Multiple Reservation Lists**: Merge multiple sorted reservation lists into a single sorted output, maintaining reservation order and priority. **Function Signatures:** Create a class `TicketReservationSystem` with the following methods: 1. `add_reservation(reservation_id: int, priority: int) -> None` 2. `pop_highest_priority() -> Tuple[int, int]` 3. `preplace_highest_priority(reservation_id: int, priority: int) -> Tuple[int, int]` 4. `cancel_reservation(reservation_id: int) -> None` 5. `peek_highest_priority() -> Tuple[int, int]` 6. `merge_reservations(*reservations: List[Tuple[int, int]]) -> Iterator[Tuple[int, int]]` **Input and Output Formats:** 1. **Add a New Reservation**: - Input: `reservation_id` (integer), `priority` (integer) - Output: None 2. **Pop the Highest Priority Reservation**: - Input: None - Output: Tuple containing `reservation_id` and `priority` 3. **Pop a Reservation and Add a New One**: - Input: `reservation_id` (integer), `priority` (integer) - Output: Tuple containing `popped_reservation_id` and `popped_priority` 4. **Cancel a Reservation**: - Input: `reservation_id` (integer) - Output: None 5. **Retrieve the Highest Priority Reservation without Removing**: - Input: None - Output: Tuple containing `reservation_id` and `priority` 6. **Merge Multiple Reservation Lists**: - Input: Multiple lists, each containing tuples of `(reservation_id, priority)` - Output: An iterator yielding tuples of `(reservation_id, priority)` in sorted order **Constraints:** - You may assume that all reservation IDs are unique. - The priorities will be integers. - The merge operation should handle multiple sorted lists efficiently without pulling all data into memory at once. **Performance Requirements:** - All operations must maintain the heap invariant. - Operations involving adding, removing, and accessing the highest priority reservation should be efficient, ideally O(log n). **Example:** ```python trs = TicketReservationSystem() trs.add_reservation(101, 5) trs.add_reservation(102, 1) trs.add_reservation(103, 3) print(trs.pop_highest_priority()) # Output: (102, 1) trs.add_reservation(104, 2) print(trs.peek_highest_priority()) # Output: (104, 2) trs.cancel_reservation(104) print(list(trs.merge_reservations([(201, 1), (202, 4)], [(301, 2), (302, 3)]))) # Output: [(201, 1), (301, 2), (302, 3), (202, 4)] ```","solution":"import heapq from typing import List, Tuple, Iterator class TicketReservationSystem: def __init__(self): self.heap = [] self.entry_finder = {} self.REMOVED = \'<REMOVED>\' self.counter = 0 def add_reservation(self, reservation_id: int, priority: int) -> None: entry = [priority, self.counter, reservation_id] self.entry_finder[reservation_id] = entry heapq.heappush(self.heap, entry) self.counter += 1 def pop_highest_priority(self) -> Tuple[int, int]: while self.heap: priority, _, reservation_id = heapq.heappop(self.heap) if reservation_id is not self.REMOVED: del self.entry_finder[reservation_id] return reservation_id, priority raise KeyError(\'pop from an empty priority queue\') def preplace_highest_priority(self, reservation_id: int, priority: int) -> Tuple[int, int]: highest_priority_res = self.pop_highest_priority() self.add_reservation(reservation_id, priority) return highest_priority_res def cancel_reservation(self, reservation_id: int) -> None: entry = self.entry_finder.pop(reservation_id) entry[-1] = self.REMOVED def peek_highest_priority(self) -> Tuple[int, int]: while self.heap: priority, _, reservation_id = self.heap[0] if reservation_id is not self.REMOVED: return reservation_id, priority heapq.heappop(self.heap) raise KeyError(\'peek from an empty priority queue\') @staticmethod def merge_reservations(*reservations: List[Tuple[int, int]]) -> Iterator[Tuple[int, int]]: return heapq.merge(*reservations, key=lambda x: x[1])"},{"question":"# Understanding Copy-on-Write (CoW) in pandas Problem Statement You are given a DataFrame containing flight details. Your tasks are to implement functions that perform various operations on this DataFrame while ensuring compatibility with pandas Copy-on-Write (CoW) behavior. Input The input to your functions consists of a pandas `DataFrame`. Here is a sample DataFrame: ``` flights = pd.DataFrame({ \'flight_id\': [1, 2, 3, 4], \'departure\': [\'New York\', \'Los Angeles\', \'Chicago\', \'Miami\'], \'arrival\': [\'London\', \'Tokyo\', \'Paris\', \'Berlin\'], \'duration\': [415, 720, 350, 470] # durations are in minutes }) ``` Task 1: Update Flight Duration Write a function `update_flight_duration` that increases the flight duration by 10% for flights longer than 400 minutes. **Constraints**: - Use the Column `loc` properly to adhere to CoW rules. ```python import pandas as pd def update_flight_duration(flights: pd.DataFrame) -> pd.DataFrame: This function updates the flight durations by increasing them by 10% for flights that are longer than 400 minutes. Args: flights: DataFrame - A DataFrame containing flight details. Returns: DataFrame - A DataFrame with updated flight durations. pass ``` Task 2: Ensure Read-Only Property Write a function `convert_to_read_only` that retrieves a NumPy array from the \'duration\' column of the provided DataFrame and makes it read-only. **Constraints**: - Ensure that any attempt to modify the returned array raises an error. ```python def convert_to_read_only(flights: pd.DataFrame) -> None: This function retrieves the underlying NumPy array of the \'duration\' column, makes it read-only, and then attempts to modify it, which should trigger an error. Args: flights: DataFrame - A DataFrame containing flight details. Raises: ValueError - If the array modification is not allowed due to read-only status. pass ``` Task 3: Optimize DataFrame Operations Write a function `optimize_operations` that performs the following operations on the DataFrame: 1. Drop the \'flight_id\' column without copying the DataFrame. 2. Rename \'departure\' column to \'origin\' without making a copy. This should return the modified DataFrame with these changes applied. ```python def optimize_operations(flights: pd.DataFrame) -> pd.DataFrame: This function performs optimized operations on the DataFrame, including dropping a column and renaming another, without copying the data unnecessarily. Args: flights: DataFrame - A DataFrame containing flight details. Returns: DataFrame - A DataFrame with \'flight_id\' dropped and \'departure\' renamed to \'origin\'. pass ``` Output For each task, the functions should return a pandas `DataFrame` or raise appropriate errors as specified. **Performance Requirements**: - Consider the implications of CoW to ensure that your code is both efficient and adheres to the CoW principles. **Constraints**: - You can assume the DataFrame will have the structure as shown in the sample input. - Each function will receive a freshly created DataFrame as input, so they don\'t affect each other\'s execution.","solution":"import pandas as pd def update_flight_duration(flights: pd.DataFrame) -> pd.DataFrame: This function updates the flight durations by increasing them by 10% for flights that are longer than 400 minutes. Args: flights: DataFrame - A DataFrame containing flight details. Returns: DataFrame - A DataFrame with updated flight durations. flights.loc[flights[\'duration\'] > 400, \'duration\'] *= 1.1 return flights def convert_to_read_only(flights: pd.DataFrame) -> None: This function retrieves the underlying NumPy array of the \'duration\' column, makes it read-only, and then attempts to modify it, which should trigger an error. Args: flights: DataFrame - A DataFrame containing flight details. Raises: ValueError - If the array modification is not allowed due to read-only status. duration_array = flights[\'duration\'].values duration_array.flags.writeable = False try: duration_array[0] = 999 # This should raise a ValueError except ValueError as e: raise ValueError(\\"Array modification not allowed due to read-only status.\\") from e def optimize_operations(flights: pd.DataFrame) -> pd.DataFrame: This function performs optimized operations on the DataFrame, including dropping a column and renaming another, without copying the data unnecessarily. Args: flights: DataFrame - A DataFrame containing flight details. Returns: DataFrame - A DataFrame with \'flight_id\' dropped and \'departure\' renamed to \'origin\'. flights.drop(columns=[\'flight_id\'], inplace=True) flights.rename(columns={\'departure\': \'origin\'}, inplace=True) return flights"},{"question":"**Objective**: Demonstrate your understanding of Python\'s `importlib` module by implementing a custom import mechanism. # Problem Description: You are required to implement a class `CustomImporter` that extends functionality from Python\'s `importlib.abc` module. The class will allow you to import a module from a dynamically constructed in-memory string (simulating loading code from dynamic sources like databases or network responses). # Requirements: 1. **Class Implementation**: Implement the `CustomImporter` class which should have: - A method `add_module_code(self, module_name: str, code: str) -> None`: This method will take a module name and its code as a string and store it internally. - A method `import_module(self, module_name: str) -> ModuleType`: This method will load the stored module code and return the corresponding module object. 2. **Functionality**: - Utilize `importlib.util.spec_from_loader` and `importlib.util.module_from_spec` to create module specifications and modules. - Handle code compilation and execution using appropriate APIs like `compile` and `exec`. - Ensure that the modules are inserted into `sys.modules` to simulate a standard import. # Input Format: - The input to `add_module_code` will be two strings: `module_name` and `code`, representing the name of the module and its source code respectively. - The `import_module` method takes a single string input: `module_name`. # Output Format: - The `import_module` method returns a module object of the imported module. # Constraints: - You should handle errors gracefully, e.g., raise appropriate exceptions if the module code cannot be compiled or executed. - Your solution should be able to handle multiple modules being added and imported in sequence. # Example: ```python # Sample usage of CustomImporter class import sys from types import ModuleType from importlib.abc import Loader import importlib.util class CustomImporter: def __init__(self): self.modules = {} def add_module_code(self, module_name: str, code: str) -> None: self.modules[module_name] = code def import_module(self, module_name: str) -> ModuleType: if module_name not in self.modules: raise ImportError(f\\"Module {module_name} not found.\\") code = self.modules[module_name] # Create a module spec and module object spec = importlib.util.spec_from_loader(module_name, loader=None) module = importlib.util.module_from_spec(spec) exec(code, module.__dict__) sys.modules[module_name] = module return module # Example usage: custom_importer = CustomImporter() custom_importer.add_module_code(\'mymodule\', \'def hello(): return \\"Hello, World!\\"\') my_module = custom_importer.import_module(\'mymodule\') print(my_module.hello()) # Output: Hello, World! ``` **Notes:** - The provided example code illustrates a simple implementation of the CustomImporter class, with methods to add module code and import modules dynamically.","solution":"import sys from types import ModuleType from importlib.abc import Loader import importlib.util class CustomImporter: def __init__(self): self.modules = {} def add_module_code(self, module_name: str, code: str) -> None: Stores the module code in the internal dictionary. :param module_name: Name of the module :param code: Source code of the module :return: None self.modules[module_name] = code def import_module(self, module_name: str) -> ModuleType: Imports the module from the stored code and returns the module object. :param module_name: Name of the module to import :return: Module object :raises ImportError: If the module cannot be found if module_name not in self.modules: raise ImportError(f\\"Module {module_name} not found.\\") code = self.modules[module_name] # Create a module spec and module object spec = importlib.util.spec_from_loader(module_name, loader=None) module = importlib.util.module_from_spec(spec) # Compile and execute the module code in the module\'s namespace exec(code, module.__dict__) # Insert the module into sys.modules sys.modules[module_name] = module return module"},{"question":"**Objective**: Demonstrate your ability to load a real-world dataset using scikit-learn and perform a basic analysis. **Dataset**: `fetch_20newsgroups` # Task 1. **Load the Dataset** Write a function `load_newsgroups_data(categories)` that: - Takes a list of categories as input. - Loads the 20 newsgroups dataset limited to these categories using the `fetch_20newsgroups` function from `sklearn.datasets`. - Returns the loaded dataset. ```python def load_newsgroups_data(categories: list) -> dict: pass ``` 2. **Vectorize the Data** Write a function `vectorize_data(data)` that: - Takes the dataset loaded by the previous function (specifically, the \'data\' attribute of the loaded dataset). - Converts the text data to a TF-IDF feature matrix using `TfidfVectorizer` from `sklearn.feature_extraction.text`. - Returns the feature matrix. ```python from sklearn.feature_extraction.text import TfidfVectorizer def vectorize_data(data: list) -> \'scipy.sparse.csr.csr_matrix\': pass ``` 3. **Train a Classifier** Write a function `train_classifier(X, y)` that: - Takes the TF-IDF feature matrix `X` and the target labels `y`. - Splits the data into training and testing sets using `train_test_split` from `sklearn.model_selection`. - Trains a `MultinomialNB` classifier from `sklearn.naive_bayes`. - Evaluates the classifier on the testing dataset. - Returns the accuracy of the trained model. ```python from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score def train_classifier(X: \'scipy.sparse.csr.csr_matrix\', y: list) -> float: pass ``` # Input and Output Specifications - `load_newsgroups_data(categories)` - **Input**: A list of categories (e.g., `[\'alt.atheism\', \'sci.space\']`). - **Output**: A dictionary representing the loaded dataset. - `vectorize_data(data)` - **Input**: A list of text documents (e.g., dataset[\'data\'] from the previous function\'s output). - **Output**: A TF-IDF feature matrix (scipy sparse matrix). - `train_classifier(X, y)` - **Input**: - `X`: A TF-IDF feature matrix. - `y`: A list of labels. - **Output**: The accuracy of the trained model (float). # Constraints - Use `fetch_20newsgroups` from `sklearn.datasets` for loading the data. - Use `TfidfVectorizer` from `sklearn.feature_extraction.text` for text vectorization. - Use `train_test_split` from `sklearn.model_selection` for splitting the data. - Use `MultinomialNB` from `sklearn.naive_bayes` for training the classifier. - Assume the dataset is relatively large and the performance critically depends on the efficiency of the solution. # Example ```python categories = [\'alt.atheism\', \'sci.space\'] dataset = load_newsgroups_data(categories) X = vectorize_data(dataset[\'data\']) accuracy = train_classifier(X, dataset[\'target\']) print(f\\"Model Accuracy: {accuracy:.2f}\\") ```","solution":"from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score def load_newsgroups_data(categories: list) -> dict: Load the 20 newsgroups dataset limited to the specified categories. Parameters: categories (list) : List of categories to load Returns: dict : Loaded dataset dataset = fetch_20newsgroups(subset=\'all\', categories=categories, remove=(\'headers\', \'footers\', \'quotes\')) return dataset def vectorize_data(data: list) -> \'scipy.sparse.csr.csr_matrix\': Converts text data to a TF-IDF feature matrix. Parameters: data (list) : List of text documents Returns: scipy.sparse.csr.csr_matrix : TF-IDF feature matrix vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(data) return X def train_classifier(X: \'scipy.sparse.csr.csr_matrix\', y: list) -> float: Trains a Multinomial Naive Bayes classifier and evaluates its accuracy. Parameters: X (scipy.sparse.csr.csr_matrix) : TF-IDF feature matrix y (list) : List of target labels Returns: float : Accuracy of the trained model X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) clf = MultinomialNB() clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"**Asynchronous Task Coordination using `asyncio`** **Objective:** Implement a function that coordinates multiple asynchronous tasks using the `asyncio` module to simulate a scenario where multiple clients request data from a server simultaneously. **Problem Statement:** You are required to write a Python function `simulate_multiple_requests` that uses the `asyncio` module to handle multiple client requests concurrently. Each client request is simulated by an asynchronous function that makes a request to a fictitious server, waits for a response, and then processes the response. **Function Signature:** ```python import asyncio async def simulate_multiple_requests(num_requests: int) -> list: ``` **Input:** - `num_requests` (int): The number of client requests to simulate. (1 ≤ num_requests ≤ 100) **Output:** - Returns a list of integers representing the results from processing each client request. **Constraints:** 1. Each client request takes a random amount of time between 1 and 3 seconds to get a response from the server. 2. Each response from the server is a random integer between 1 and 100. 3. Process the response by doubling the received integer. **Requirements:** - You must use the `asyncio` module to implement the asynchronous requests. - Your solution should handle all requests concurrently, not sequentially. - Make sure to use `asyncio.gather` to wait for all the asynchronous tasks to complete. **Example:** ```python import random async def get_server_response() -> int: await asyncio.sleep(random.randint(1, 3)) return random.randint(1, 100) async def process_response(response: int) -> int: await asyncio.sleep(0) # Simulate processing time return response * 2 async def simulate_multiple_requests(num_requests: int) -> list: tasks = [] for _ in range(num_requests): response = await get_server_response() task = await process_response(response) tasks.append(task) return await asyncio.gather(*tasks) # Example Usage results = asyncio.run(simulate_multiple_requests(5)) print(results) # [150, 40, 200, 62, 10] (example, actual output will vary) ``` **Assessment Criteria:** - Correctness: The function behaves as described and returns the expected results. - Efficiency: The function handles multiple asynchronous tasks efficiently. - Code Quality: The code is clear, well-organized, and follows Python best practices for asynchronous programming.","solution":"import asyncio import random async def get_server_response() -> int: await asyncio.sleep(random.randint(1, 3)) return random.randint(1, 100) async def process_response(response: int) -> int: await asyncio.sleep(0) # Simulate processing time return response * 2 async def simulate_multiple_requests(num_requests: int) -> list: async def handle_request(): response = await get_server_response() return await process_response(response) tasks = [handle_request() for _ in range(num_requests)] return await asyncio.gather(*tasks)"},{"question":"**Objective**: To assess understanding of instance and method objects in Python, create a question involving their Python-level equivalents, particularly focusing on bound methods, callable objects, and the introspection of these methods. **Question**: In Python, methods can be bound to class instances, which allows them to maintain state related to that instance. Your task is to create a custom class that simulates a similar behavior to Python’s bound method using higher-level Python constructs. 1. Create a class called `BoundMethodSimulator` with the following methods: - `__init__(self, func, instance)`: - Initializes with a callable object `func` and an instance `instance` the function should be bound to. - `func` can be any callable (function or lambda). - `instance` should be any object. - `__call__(self, *args, **kwargs)`: - When the object is called, it should call the stored `func` with the instance (`self.instance`) as the first argument followed by any other arguments or keyword arguments. - `get_function(self)`: - Returns the original function (`func`) used with the `BoundMethodSimulator`. - `get_instance(self)`: - Returns the instance (`instance`) used with the `BoundMethodSimulator`. **Example**: ```python class Sample: def __init__(self, value): self.value = value def sample_func(self, x): return self.value + x sample_instance = Sample(10) bound_method = BoundMethodSimulator(sample_func, sample_instance) # Calling the bound method result = bound_method(5) # should return 15 # Introspection of the bound method original_func = bound_method.get_function() # should return sample_func original_instance = bound_method.get_instance() # should return the instance of Sample with value 10 ``` **Input/Output**: - `BoundMethodSimulator` should be initialized with a function and an instance. - Calling an instance of `BoundMethodSimulator` should behave like calling the method with the instance bound as the first argument. - Ability to retrieve the original function and the instance. **Constraints**: - You should not use any external libraries. - Assume the function passed to `BoundMethodSimulator` will always expect an instance of a class as its first parameter.","solution":"class BoundMethodSimulator: def __init__(self, func, instance): Initializes with a callable object `func` and an instance `instance` the function should be bound to. self.func = func self.instance = instance def __call__(self, *args, **kwargs): When the object is called, it should call the stored `func` with the instance (`self.instance`) as the first argument followed by any other arguments or keyword arguments. return self.func(self.instance, *args, **kwargs) def get_function(self): Returns the original function (`func`) used with the `BoundMethodSimulator`. return self.func def get_instance(self): Returns the instance (`instance`) used with the `BoundMethodSimulator`. return self.instance"},{"question":"# PyTorch Coding Assessment Question **Title: Implementing Tensor Dimension Manipulations** Objective - Assess the student\'s understanding of PyTorch tensors and the `torch.Size` class. - Evaluate the ability to manipulate and extract information about tensor dimensions. Problem Statement You are given a 3-dimensional tensor `x` and certain operations to perform on its dimensions. Write a function `tensor_operations` that takes as input: 1. A PyTorch tensor `x`. The function should: 1. Return the size of each dimension of the tensor as a `torch.Size` object. 2. Create and return a new tensor by permuting the dimensions of `x` in the order specified by a given permutation list `perm`, where `perm` is a list of integers representing the new order of dimensions. 3. Reshape the permuted tensor into a 2-dimensional tensor with the first dimension equal to -1 (which infers the value from the remaining dimension) and return this reshaped tensor. **Input Format:** ```python def tensor_operations(x: torch.Tensor, perm: list) -> (torch.Size, torch.Tensor, torch.Tensor): # Your code here ``` - `x` (torch.Tensor): A 3-dimensional PyTorch tensor. - `perm` (list): A list of three integers representing the desired new order of dimensions after permutation. **Output Format:** - Returns a tuple containing: 1. A `torch.Size` object representing the size of each dimension of `x`. 2. A permuted tensor with its dimensions rearranged as specified by `perm`. 3. A reshaped tensor with dimensions reshaped to be (-1, other_dim). **Constraints:** - The tensor `x` will always have exactly 3 dimensions. - The list `perm` will always contain exactly 3 unique integers from {0, 1, 2}. **Example:** ```python import torch x = torch.randn(5, 10, 15) perm = [2, 0, 1] size, permuted_tensor, reshaped_tensor = tensor_operations(x, perm) # Example outputs: # size -> torch.Size([5, 10, 15]) # permuted_tensor -> A tensor of shape [15, 5, 10] # reshaped_tensor -> A tensor of shape [15 * 5, 10] which is [75, 10] ``` **Performance Requirements:** - The function should operate efficiently on tensors of size up to `(100, 100, 100)`. - Ensure good performance while manipulating tensor dimensions and reshaping.","solution":"import torch def tensor_operations(x: torch.Tensor, perm: list) -> (torch.Size, torch.Tensor, torch.Tensor): Performs several operations on the given 3-dimensional tensor x: 1. Returns the size of each dimension of the tensor. 2. Permutes the dimensions of the tensor as specified by the perm list. 3. Reshapes the permuted tensor into a 2-dimensional tensor with the first dimension being -1. :param x: A 3-dimensional PyTorch tensor. :param perm: A list of three integers representing the desired new order of dimensions. :return: A tuple (torch.Size, permuted tensor, reshaped tensor) # Get the size of each dimension of the tensor size = x.size() # Permute the dimensions of the tensor as specified by the perm list permuted_tensor = x.permute(perm) # Reshape the permuted tensor into a 2-dimensional tensor (-1, other_dim) reshaped_tensor = permuted_tensor.reshape(-1, permuted_tensor.size(-1)) return size, permuted_tensor, reshaped_tensor"},{"question":"Objective: Implement a function `process_data` that demonstrates the usage of the itertools, functools, and operator modules to process a list of integers. Problem Statement: You are given a list of integers. You need to implement a function `process_data(data: List[int]) -> List[int]` that performs the following sequence of operations using the itertools, functools, and operator modules: 1. **Filter**: Select only the even numbers from the list. 2. **Double**: Double each of the filtered numbers. 3. **Sort**: Sort the doubled numbers in ascending order. 4. **Partial Summation**: Compute a list containing the partial sums of the sorted, doubled numbers. Example: ```python process_data([10, 21, 32, 43, 54]) ``` - Filtered Even Numbers: `[10, 32, 54]` - Doubled: `[20, 64, 108]` - Sorted: `[20, 64, 108]` - Partial Sums: `[20, 84, 192]` The function should return `[20, 84, 192]`. Input: - `data`: A list of integers. Constraints: `1 <= len(data) <= 10^5` and each integer in the list lies within the range `-10^6` to `10^6`. Output: - A list of integers representing the partial sums as specified above. Requirements: - You must use `itertools`, `functools`, and `operator` modules. - The solution must handle large inputs efficiently. Function Signature: ```python from typing import List def process_data(data: List[int]) -> List[int]: pass ``` Constraints: - Ensure that your code is efficient and can handle the upper limits of the input size. Note: - For the partial summation, you can use `itertools.accumulate`.","solution":"from typing import List import itertools import operator def process_data(data: List[int]) -> List[int]: # Step 1: Filter only even numbers even_numbers = list(filter(lambda x: x % 2 == 0, data)) # Step 2: Double each of the filtered numbers doubled_numbers = list(map(lambda x: x * 2, even_numbers)) # Step 3: Sort the doubled numbers sorted_numbers = sorted(doubled_numbers) # Step 4: Compute the partial sums partial_sums = list(itertools.accumulate(sorted_numbers)) return partial_sums"},{"question":"# Assignment: Visualizing Categorical Data with Seaborn You are provided with the popular \\"tips\\" dataset from seaborn, which contains information about tips received by waitstaff in a restaurant. Your task is to visualize this dataset using different types of categorical plots to extract meaningful insights. You will combine several seaborn functionalities to accomplish the tasks. Dataset Description The dataset contains the following columns: - `total_bill`: Total bill amount in dollars (numerical). - `tip`: Tip amount in dollars (numerical). - `sex`: Gender of the customer (categorical: \\"Male\\", \\"Female\\"). - `smoker`: Whether the customer is a smoker (categorical: \\"Yes\\", \\"No\\"). - `day`: Day of the week (categorical: \\"Thur\\", \\"Fri\\", \\"Sat\\", \\"Sun\\"). - `time`: Time of day (categorical: \\"Lunch\\", \\"Dinner\\"). - `size`: Size of the group (numerical). Task 1: Categorical Scatterplots 1. Create a scatterplot using `catplot` to visualize `total_bill` against `day` of the week. - Use the `swarm` kind of plot. - Add hue to distinguish between smokers and non-smokers. 2. Modify the scatterplot to disable jitter using the `jitter` parameter. Task 2: Categorical Distribution Plots 3. Create a boxplot using `catplot` to show the distribution of `total_bill` for different days. - Add hue to distinguish between lunch and dinner. - Adjust the order of days to start from Thursday. 4. Create a violin plot to compare `total_bill` distributions for different days of the week split by gender. - Add inner stick points to show individual observations. Task 3: Categorical Estimate Plots 5. Create a bar plot to visualize the average `total_bill` for different days of the week. - Add hue to distinguish by time (Lunch or Dinner). - Use a custom palette to differentiate between \\"Lunch\\" and \\"Dinner\\". Task 4: Combining Plots 6. Combine a violin plot and swarm plot to show the distribution of `tip` amounts for different days. - Ensure the swarm plot is overlayed on the violin plot with points clearly visible. - Add hue to indicate smoking status. Task 5: Faceted Categorical Plots 7. Create faceted plots to show `total_bill` vs. `day` comparisons, separated by time (Lunch, Dinner). - Use `col` parameter to create facets. - Adjust the aspect ratio for better visualization. Constraints - Your functions should take the dataset as input and return the generated plots. - Ensure your plots are well-labeled with titles, axis labels, and legends where appropriate. Submission Submit your notebook containing all code and generated plots. Make sure the notebook is well-organized and documented. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_categorical_scatter(df): Creates two scatterplots using catplot to visualize total_bill against days of the week, distinguishing between smokers and non-smokers. One with jitter and one without. # Scatterplot with jitter enabled (using swarm plot by default it does not have jitter) scatterplot_jitter = sns.catplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", kind=\\"swarm\\", data=df) scatterplot_jitter.fig.suptitle(\'Scatterplot with Jitter\') # Scatterplot without jitter (can\'t directly turn off jitter in swarm plot, using stripplot instead) scatterplot_no_jitter = sns.catplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", kind=\\"strip\\", data=df, jitter=False) scatterplot_no_jitter.fig.suptitle(\'Scatterplot without Jitter\') return scatterplot_jitter, scatterplot_no_jitter def plot_boxplot_distribution(df): Creates a boxplot to visualize the distribution of total_bill for different days, distinguishing between lunch and dinner. boxplot = sns.catplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", kind=\\"box\\", data=df, order=[\\"Thur\\", \\"Fri\\", \\"Sat\\", \\"Sun\\"]) boxplot.fig.suptitle(\'Boxplot of Total Bill Distribution by Day and Time\') return boxplot def plot_violinplot_distribution(df): Creates a violin plot to compare total_bill distributions for different days of the week split by gender, with inner stick points to show individual observations. violinplot = sns.catplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", kind=\\"violin\\", data=df, split=True, inner=\\"stick\\") violinplot.fig.suptitle(\'Violin Plot of Total Bill by Day and Gender\') return violinplot def plot_barplot_estimation(df): Creates a bar plot to visualize the average total_bill for different days of the week, distinguishing by time (Lunch or Dinner) using a custom palette. barplot = sns.catplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", kind=\\"bar\\", data=df, palette={\\"Lunch\\": \\"skyblue\\", \\"Dinner\\": \\"orange\\"}) barplot.fig.suptitle(\'Bar Plot of Average Total Bill by Day and Time\') return barplot def plot_combined_violin_swarm(df): Combines a violin plot and swarm plot to show the distribution of tip amounts for different days, with hue to indicate smoking status. combined_plot = sns.violinplot(x=\\"day\\", y=\\"tip\\", hue=\\"smoker\\", data=df, inner=None, split=True) sns.swarmplot(x=\\"day\\", y=\\"tip\\", hue=\\"smoker\\", data=df, dodge=True, color=\\"k\\", alpha=0.6) plt.title(\'Combined Violin and Swarm Plot of Tip Amounts by Day and Smoking Status\') return combined_plot def plot_faceted_categorical(df): Creates faceted plots to show total_bill vs. day comparisons, separated by time (Lunch, Dinner), with the aspect ratio adjusted for better visualization. faceted_plot = sns.catplot(x=\\"day\\", y=\\"total_bill\\", col=\\"time\\", kind=\\"strip\\", data=df, aspect=0.6) faceted_plot.fig.suptitle(\'Faceted Categorical Plot of Total Bill by Day and Time\') return faceted_plot"},{"question":"# Advanced Coding Assessment Question **Objective**: Demonstrate your ability to work with Apple property list (plist) files using the `plistlib` module in Python. **Task**: You are given a plist file in XML format which contains various types of data (strings, lists, dictionaries, numbers, dates, and binary data). Your goal is to read the plist file, perform specific manipulations on the data, and write the modified data back to a new plist file in binary format. # Instructions 1. **Read the given XML plist file**: - The file named `input.plist` contains a dictionary with various key-value pairs. 2. **Manipulate the data**: - Increment all integer values by 1. - Convert all string values to uppercase. - Add a new key-value pair `\\"processed_at\\"` with the current date and time. 3. **Write the modified data to a new binary plist file**: - Save the modified dictionary to a file named `output.plist` in binary format. # Requirements - Use the `plistlib` module to perform all read and write operations. - Ensure that the new key-value pair `\\"processed_at\\"` follows the datetime format supported by `plistlib`. - Preserve the data types of all existing values when manipulating them. # Constraints - Assume `input.plist` is always a valid XML plist file. - Do not use any external libraries except for modules in the Python standard library. # Function Signature ```python import plistlib from datetime import datetime def process_plist(input_filename: str, output_filename: str) -> None: # your code here ``` # Example Given `input.plist` contains: ```xml <plist version=\\"1.0\\"> <dict> <key>name</key> <string>John Doe</string> <key>score</key> <integer>42</integer> </dict> </plist> ``` After running `process_plist(\'input.plist\', \'output.plist\')`, `output.plist` should contain the incremented integer and the uppercase string, along with the current timestamp: Binary plist representation (example, not exactly human-readable): ``` { \'name\': \'JOHN DOE\', \'score\': 43, \'processed_at\': datetime_object # Current date and time } ``` Note: The exact binary contents are not shown, but the structure and values should match the description.","solution":"import plistlib from datetime import datetime def process_plist(input_filename: str, output_filename: str) -> None: # Read the XML plist file with open(input_filename, \'rb\') as infile: plist_data = plistlib.load(infile) # Manipulate the data for key, value in plist_data.items(): if isinstance(value, int): plist_data[key] = value + 1 elif isinstance(value, str): plist_data[key] = value.upper() # Add the current date and time plist_data[\'processed_at\'] = datetime.now() # Write the modified data to a new binary plist file with open(output_filename, \'wb\') as outfile: plistlib.dump(plist_data, outfile)"},{"question":"# Question: Design a class hierarchy to manage a library system. The system should manage books and members. Each book has a title, author, and a unique identifier (ISBN). Each member has a unique membership ID, a name, and a list of borrowed books. Implement the following functionality: 1. **Class Definitions**: - Create a `Book` class with attributes: title, author, and ISBN. - Create a `Member` class with attributes: membership ID, name, and a list of borrowed books (initially empty). - Create a `Library` class to manage the books and members. 2. **Library Management**: - In the `Library` class, implement methods to: - Add a new book. - Remove a book by ISBN. - Register a new member. - Remove a member by membership ID. - Borrow a book by a member (by ISBN and membership ID). - Return a book by a member (by ISBN and membership ID). 3. **Constraints**: - Ensure that a book can only be borrowed if it is available in the library. - Ensure that a member cannot borrow more than 5 books at a time. - Ensure that all data is stored and retrieved from a file named `library_data.txt`. This means the `Library` class should serialize its books and members information to this file and be able to initialize itself from this file if it exists. 4. **Input and Output**: - All methods in the `Library` class should be called with appropriate arguments and should return meaningful messages (e.g., \\"Book borrowed successfully\\" or \\"Book not found\\"). - Demonstrate the usage of these classes and methods with example calls. **Example:** ```python # Define and initialize the library library = Library() # Add books to the library library.add_book(\\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\", \\"1234567890\\") library.add_book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", \\"0987654321\\") # Register members library.add_member(1, \\"John Doe\\") library.add_member(2, \\"Jane Smith\\") # Borrow books print(library.borrow_book(\\"1234567890\\", 1)) # \\"Book borrowed successfully\\" print(library.borrow_book(\\"0987654321\\", 1)) # \\"Book borrowed successfully\\" # Return books print(library.return_book(\\"1234567890\\", 1)) # \\"Book returned successfully\\" ``` # Notes: - Make sure to handle exceptions gracefully and provide clear error messages. - Use Python’s file handling techniques to read and write to the file `library_data.txt`. - Ensure the code is written in a clear and readable manner, following best practices and coding standards.","solution":"import os import json class Book: def __init__(self, title, author, isbn): self.title = title self.author = author self.isbn = isbn class Member: def __init__(self, membership_id, name): self.membership_id = membership_id self.name = name self.borrowed_books = [] class Library: def __init__(self, filename=\\"library_data.txt\\"): self.filename = filename self.books = [] self.members = [] self.load_data() def save_data(self): data = { \'books\': [{\\"title\\": book.title, \\"author\\": book.author, \\"isbn\\": book.isbn} for book in self.books], \'members\': [{\\"membership_id\\": member.membership_id, \\"name\\": member.name, \\"borrowed_books\\": member.borrowed_books} for member in self.members] } with open(self.filename, \'w\') as f: json.dump(data, f) def load_data(self): if os.path.exists(self.filename): with open(self.filename, \'r\') as f: data = json.load(f) for book_data in data[\'books\']: book = Book(**book_data) self.books.append(book) for member_data in data[\'members\']: member = Member(member_data[\'membership_id\'], member_data[\'name\']) member.borrowed_books = member_data.get(\'borrowed_books\', []) self.members.append(member) def add_book(self, title, author, isbn): self.books.append(Book(title, author, isbn)) self.save_data() return \\"Book added successfully.\\" def remove_book(self, isbn): self.books = [book for book in self.books if book.isbn != isbn] self.save_data() return \\"Book removed successfully.\\" def add_member(self, membership_id, name): self.members.append(Member(membership_id, name)) self.save_data() return \\"Member added successfully.\\" def remove_member(self, membership_id): self.members = [member for member in self.members if member.membership_id != membership_id] self.save_data() return \\"Member removed successfully.\\" def borrow_book(self, isbn, membership_id): for member in self.members: if member.membership_id == membership_id: if len(member.borrowed_books) >= 5: return \\"Member cannot borrow more than 5 books at a time.\\" for book in self.books: if book.isbn == isbn: member.borrowed_books.append(isbn) self.save_data() return \\"Book borrowed successfully.\\" return \\"Book or member not found.\\" def return_book(self, isbn, membership_id): for member in self.members: if member.membership_id == membership_id: if isbn in member.borrowed_books: member.borrowed_books.remove(isbn) self.save_data() return \\"Book returned successfully.\\" return \\"Book or member not found.\\" # Sample usage if __name__ == \\"__main__\\": library = Library() print(library.add_book(\\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\", \\"1234567890\\")) print(library.add_book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", \\"0987654321\\")) print(library.add_member(1, \\"John Doe\\")) print(library.add_member(2, \\"Jane Smith\\")) print(library.borrow_book(\\"1234567890\\", 1)) print(library.borrow_book(\\"0987654321\\", 1)) print(library.return_book(\\"1234567890\\", 1))"},{"question":"**Objective:** This coding assessment is designed to evaluate your understanding of the \\"cProfile\\" module in Python for deterministic profiling. You are required to write code that demonstrates your ability to profile code execution, analyze profiling results, and implement custom profiling features. **Problem Statement:** You are given a Python script that implements a simple computing task. Your task is to use the \\"cProfile\\" module to profile this script, analyze the profiling results to identify the performance bottlenecks, and format the profiling results using the \\"pstats\\" module. Additionally, you will implement a custom timer for the profiling process. **Python Script to Profile:** You are given the following script which calculates the factorial of numbers in a given range: ```python import math def compute_factorials(n): results = [] for i in range(1, n + 1): result = math.factorial(i) results.append(result) return results if __name__ == \\"__main__\\": num = 10000 compute_factorials(num) ``` **Tasks:** 1. **Profile the Script:** - Use the \\"cProfile\\" module to profile the execution of the `compute_factorials` function. - Save the profiling results to a file named \\"profile_results.dat\\". 2. **Analyze Profiling Results:** - Use the \\"pstats\\" module to read the profiling results from \\"profile_results.dat\\". - Sort the results by cumulative time and print the top 10 functions that consume the most time. 3. **Implement Custom Timer:** - Implement a custom timer function that uses `time.perf_counter()` for time measurement. - Modify the profiling process to use this custom timer. - Profile the `compute_factorials` function again using the custom timer and print the profiling statistics. **Requirements:** - You must use the \\"cProfile\\" module to profile the script. - You must use the \\"pstats\\" module to analyze and print the profiling results. - You must implement and use a custom timer for the profiling process. - Ensure your code is well-documented and includes explanations for each step. **Input and Output:** - No input is required from the user. - The output should include the profiling statistics in a formatted manner, as specified in the tasks. **Constraints:** - You are allowed to use only the standard Python libraries \\"cProfile\\", \\"pstats\\", and \\"time\\". **Performance Requirements:** - The profiling process should handle the given script efficiently without significant overhead. **Submission:** Submit the Python script containing your solution. Make sure to include all necessary imports and ensure your code runs without errors.","solution":"import math import cProfile import pstats import time def compute_factorials(n): results = [] for i in range(1, n + 1): result = math.factorial(i) results.append(result) return results def profile_code(): profiler = cProfile.Profile() profiler.enable() compute_factorials(10000) profiler.disable() profiler.dump_stats(\\"profile_results.dat\\") def analyze_profiling_results(): with open(\\"profile_analysis.txt\\", \\"w\\") as f: stats = pstats.Stats(\\"profile_results.dat\\", stream=f) stats.strip_dirs().sort_stats(\'cumulative\').print_stats(10) def custom_timer(): start = time.perf_counter() compute_factorials(10000) end = time.perf_counter() print(f\\"Custom Timer: {end - start:.6f} seconds\\") def profile_with_custom_timer(): profiler = cProfile.Profile(timer=time.perf_counter) profiler.enable() compute_factorials(10000) profiler.disable() stats = pstats.Stats(profiler).sort_stats(\'cumulative\') stats.print_stats() if __name__ == \\"__main__\\": print(\\"Profiling the code and saving results...\\") profile_code() print(\\"Analyzing profiling results...\\") analyze_profiling_results() print(\\"Using custom timer to measure execution time...\\") custom_timer() print(\\"Profiling with custom timer...\\") profile_with_custom_timer()"},{"question":"# Pandas DataFrame Manipulation and Indexing You are provided with a DataFrame containing information about various products. The DataFrame `products_df` has the following columns: - `product_id` (integer): Unique identifier for each product. - `category` (string): Category of the product (e.g., \'electronics\', \'furniture\'). - `price` (float): Price of the product. - `stock` (integer): Number of items available in stock. - `rating` (float): Customer rating of the product. Here is a sample of the DataFrame: ```python import pandas as pd data = { \'product_id\': [101, 102, 103, 104, 105], \'category\': [\'electronics\', \'electronics\', \'furniture\', \'furniture\', \'electronics\'], \'price\': [500, 1500, 700, 450, 1200], \'stock\': [10, 5, 20, 15, 2], \'rating\': [4.5, 4.3, 4.0, 4.7, 4.1] } products_df = pd.DataFrame(data) ``` # Task 1. **Selection and Filtering**: - Write a function `filter_products_by_category(products_df, category)` that takes the DataFrame `products_df` and a string `category` as input, and returns a new DataFrame containing only the products of the specified category. 2. **Conditional Selection**: - Write a function `get_high_rating_products(products_df, min_rating)` that takes the DataFrame `products_df` and a float `min_rating` as input, and returns a DataFrame containing products with a rating higher than or equal to `min_rating`. 3. **Slicing**: - Write a function `slice_product_data(products_df, start_idx, end_idx)` that returns a sliced DataFrame containing rows from `start_idx` to `end_idx` (excluding `end_idx`). Use integer-based indexing for this task. 4. **Boolean Indexing and Combining Conditions**: - Write a function `filter_by_price_and_stock(products_df, max_price, min_stock)` that returns a DataFrame of products with `price` less than or equal to `max_price` and `stock` greater than or equal to `min_stock`. 5. **Callable Indexing**: - Write a function `filter_by_custom_criteria(products_df, criteria_function)` that takes a custom function `criteria_function` and applies this function to filter the DataFrame `products_df`. The `criteria_function` should take the DataFrame as input and return a boolean Series. # Constraints: - All input arguments are guaranteed to be of valid types and within reasonable bounds. - You should use pandas\' efficient data manipulation methods to ensure performance. # Example For example, the function `filter_products_by_category` should return the following when called with `products_df` and `\'electronics\'`: ```python >>> filter_products_by_category(products_df, \'electronics\') product_id category price stock rating 0 101 electronics 500 10 4.5 1 102 electronics 1500 5 4.3 4 105 electronics 1200 2 4.1 ```","solution":"import pandas as pd def filter_products_by_category(products_df, category): Filter products by category. Args: products_df (pd.DataFrame): DataFrame containing product information. category (str): Category to filter by. Returns: pd.DataFrame: Filtered DataFrame containing only products of the specified category. return products_df[products_df[\'category\'] == category] def get_high_rating_products(products_df, min_rating): Get products with a rating higher than or equal to min_rating. Args: products_df (pd.DataFrame): DataFrame containing product information. min_rating (float): Minimum rating to filter by. Returns: pd.DataFrame: Filtered DataFrame containing products with a rating higher than or equal to min_rating. return products_df[products_df[\'rating\'] >= min_rating] def slice_product_data(products_df, start_idx, end_idx): Return a sliced DataFrame containing rows from start_idx to end_idx (excluding end_idx). Args: products_df (pd.DataFrame): DataFrame containing product information. start_idx (int): Start index for slicing. end_idx (int): End index for slicing. Returns: pd.DataFrame: Sliced DataFrame. return products_df.iloc[start_idx:end_idx] def filter_by_price_and_stock(products_df, max_price, min_stock): Filter products by price and stock. Args: products_df (pd.DataFrame): DataFrame containing product information. max_price (float): Maximum price to filter by. min_stock (int): Minimum stock to filter by. Returns: pd.DataFrame: Filtered DataFrame of products with price <= max_price and stock >= min_stock. return products_df[(products_df[\'price\'] <= max_price) & (products_df[\'stock\'] >= min_stock)] def filter_by_custom_criteria(products_df, criteria_function): Filter products by custom criteria. Args: products_df (pd.DataFrame): DataFrame containing product information. criteria_function (function): Custom function that takes the DataFrame as input and returns a boolean Series. Returns: pd.DataFrame: Filtered DataFrame based on the custom criteria. return products_df[criteria_function(products_df)]"},{"question":"Objective Design and implement a Python function to perform complex operations on a sequence of numerical values using the principles described in the provided documentation. Your task is to understand and apply Sequence Protocol and Buffer Protocol concepts. Problem Statement You are to write a function `complex_operations(sequence: list[int]) -> dict` that performs the following operations on a given sequence of integers: 1. **Sum**: Compute the sum of all the elements in the sequence. 2. **Product**: Compute the product of all the elements in the sequence. 3. **Average**: Compute the average of the elements in the sequence. 4. **Variance**: Compute the variance of the elements in the sequence. 5. **Buffer**: Construct a buffer that provides the sum, product, average, and variance buffer views from the sequence. Return this buffer in the output dictionary. Input - `sequence` (list of int): A list of integers. Assume that there\'s at least one element in the list. Output - `dict`: A dictionary containing the sum, product, average, variance, and buffer of the sequence in the following format: ```python { \\"sum\\": int, \\"product\\": int, \\"average\\": float, \\"variance\\": float, \\"buffer\\": { \\"sum_buffer\\": buffer, # Read-only buffer view of the sum. \\"product_buffer\\": buffer, # Read-only buffer view of the product. \\"average_buffer\\": buffer, # Read-only buffer view of the average. \\"variance_buffer\\": buffer # Read-only buffer view of the variance. } } ``` Constraints - You must use the `Buffer Protocol` to create read-only buffer views. Implementation Requirements - You should not use any external libraries for mathematical operations; use built-in Python functions and methods. - Ensure efficient computation and buffer management, especially considering the sequence could be large. Example ```python sequence = [1, 2, 3, 4, 5] result = complex_operations(sequence) print(result) # Expected Output: # { # \\"sum\\": 15, # \\"product\\": 120, # \\"average\\": 3.0, # \\"variance\\": 2.0, # \\"buffer\\": { # \\"sum_buffer\\": <memory at 0x7f8d1b5c2e80>, # \\"product_buffer\\": <memory at 0x7f8d1b5c2eb0>, # \\"average_buffer\\": <memory at 0x7f8d1b5c2f10>, # \\"variance_buffer\\": <memory at 0x7f8d1b5c2f70> # } # } ```","solution":"def complex_operations(sequence): Perform complex operations on a sequence of numerical values. :param sequence: list[int], sequence of integers :return: dict, dictionary containing sum, product, average, variance, and buffer views. n = len(sequence) # Calculate sum total_sum = sum(sequence) # Calculate product total_product = 1 for num in sequence: total_product *= num # Calculate average average = total_sum / n # Calculate variance variance = sum((x - average) ** 2 for x in sequence) / n # Create buffers sum_buffer = memoryview(bytearray(str(total_sum), \'utf-8\')).toreadonly() product_buffer = memoryview(bytearray(str(total_product), \'utf-8\')).toreadonly() average_buffer = memoryview(bytearray(str(average), \'utf-8\')).toreadonly() variance_buffer = memoryview(bytearray(str(variance), \'utf-8\')).toreadonly() # Compile the results into dictionary result = { \\"sum\\": total_sum, \\"product\\": total_product, \\"average\\": average, \\"variance\\": variance, \\"buffer\\": { \\"sum_buffer\\": sum_buffer, \\"product_buffer\\": product_buffer, \\"average_buffer\\": average_buffer, \\"variance_buffer\\": variance_buffer } } return result"},{"question":"Objective Your task is to write a Python function that simulates an interactive Python session where user input is recorded in a history file. This function will support reading from and writing to the history file, as well as adding, removing, and retrieving history items. Problem Statement Implement a function called `manage_readline_history(file_path: str, commands: list)` that does the following: 1. Reads the command history from the specified history file (if it exists) when the function begins. 2. Appends each command from the `commands` list to the history list. 3. Returns the updated history as a list of strings. 4. Writes the updated history back to the specified history file. Input - `file_path` (str): A string representing the path to the history file. - `commands` (list): A list of strings where each element is a command entered by the user. Output - Returns a list of strings where each string is a command from the updated history list. Constraints - The `commands` list can contain up to 1000 commands. - Each command will be a non-empty string with a maximum length of 80 characters. - The history file should contain no more than 1000 commands after appending the new commands. - If the file does not exist, the function should handle the `FileNotFoundError` appropriately. Example ```python commands = [\\"print(\'Hello, world!\')\\", \\"x = 5\\", \\"y = x * 2\\"] history = manage_readline_history(\\"history.txt\\", commands) print(history) ``` **Output:** ``` [\\"print(\'Hello, world!\')\\", \\"x = 5\\", \\"y = x * 2\\"] ``` # Implementation Notes - Use the `readline` module functions, such as `read_history_file`, `write_history_file`, `add_history`, and `get_current_history_length`, to manage the history file. - Ensure that the function handles cases where the history file does not exist. - Ensure that the history list is correctly truncated to a maximum of 1000 commands if necessary.","solution":"import readline import os def manage_readline_history(file_path: str, commands: list): Manage the readline history by reading from a file, appending new commands, and writing back to the file. :param file_path: Path to the history file. :param commands: List of commands to add to the history. :return: Updated history as a list of strings. # Initialize an empty list to hold history lines history = [] # Try to read the existing history from file_path if os.path.exists(file_path): try: readline.read_history_file(file_path) history_len = readline.get_current_history_length() history = [readline.get_history_item(i) for i in range(1, history_len + 1)] except (FileNotFoundError, IOError): pass # Append each new command to the history for command in commands: readline.add_history(command) history.append(command) # Ensure the history does not exceed 1000 commands if len(history) > 1000: history = history[-1000:] # Write the updated history back to the file try: readline.clear_history() for item in history: readline.add_history(item) readline.write_history_file(file_path) except (FileNotFoundError, IOError): pass return history"},{"question":"# Sparse Tensor Operations in PyTorch **Objective**: To assess your understanding of PyTorch\'s sparse tensor support, including the construction, manipulation, and application of various sparse tensor formats. # Problem Statement You are given a dense matrix `A`. You need to convert it to different sparse tensor formats supported by PyTorch and perform some operations on them. Implement the following functionalities: 1. **Conversion Functions**: - `to_sparse_coo(A)`: Convert the dense matrix `A` to a COO format sparse tensor. - `to_sparse_csr(A)`: Convert the dense matrix `A` to a CSR format sparse tensor. - `to_sparse_csc(A)`: Convert the dense matrix `A` to a CSC format sparse tensor. 2. **Operation Functions**: - `sparse_add(A_sp, B_sp)`: Add two sparse tensors `A_sp` and `B_sp` of the same format and return the result in the same sparse format. - `sparse_matmul(A_sp, B_sp)`: Multiply two sparse tensors `A_sp` and `B_sp` of the same format and return the result in the same sparse format. - `apply_zero_preserving_function(A_sp, func)`: Apply a zero-preserving unary function (e.g., `torch.sin`, `torch.expm1`) to a sparse tensor `A_sp` and return the result. # Input and Output **Input**: - `A`: A dense tensor of shape `(m, n)`. **Output**: - Various sparse tensor formats, results of operations, and function applications. **Constraints**: - You can assume that all input matrices are 2-dimensional and contain non-negative numbers. - The operations should support COO, CSR, and CSC formats. # Example ```python import torch # Example dense tensor A = torch.tensor([[0, 2., 0], [3, 0, 4], [0, 0, 5]]) # Conversion functions coo_tensor = to_sparse_coo(A) csr_tensor = to_sparse_csr(A) csc_tensor = to_sparse_csc(A) # Sparse addition result_sparse_add = sparse_add(coo_tensor, coo_tensor) # Sparse matrix multiplication result_sparse_matmul = sparse_matmul(coo_tensor, coo_tensor) # Apply zero-preserving function result_zero_preserve = apply_zero_preserving_function(coo_tensor, torch.sin) ``` # Function Signatures ```python def to_sparse_coo(A: torch.Tensor) -> torch.Tensor: pass def to_sparse_csr(A: torch.Tensor) -> torch.Tensor: pass def to_sparse_csc(A: torch.Tensor) -> torch.Tensor: pass def sparse_add(A_sp: torch.Tensor, B_sp: torch.Tensor) -> torch.Tensor: pass def sparse_matmul(A_sp: torch.Tensor, B_sp: torch.Tensor) -> torch.Tensor: pass def apply_zero_preserving_function(A_sp: torch.Tensor, func) -> torch.Tensor: pass ``` Implement these functions while adhering to the given specifications and examples provided.","solution":"import torch def to_sparse_coo(A: torch.Tensor) -> torch.Tensor: Convert the dense matrix A to a COO format sparse tensor. return A.to_sparse() def to_sparse_csr(A: torch.Tensor) -> torch.Tensor: Convert the dense matrix A to a CSR format sparse tensor. return A.to_sparse_csr() def to_sparse_csc(A: torch.Tensor) -> torch.Tensor: Convert the dense matrix A to a CSC format sparse tensor. return A.to_sparse_csc() def sparse_add(A_sp: torch.Tensor, B_sp: torch.Tensor) -> torch.Tensor: Add two sparse tensors A_sp and B_sp of the same format and return the result. return A_sp + B_sp def sparse_matmul(A_sp: torch.Tensor, B_sp: torch.Tensor) -> torch.Tensor: Multiply two sparse tensors A_sp and B_sp of the same format and return the result. return torch.sparse.mm(A_sp, B_sp) def apply_zero_preserving_function(A_sp: torch.Tensor, func) -> torch.Tensor: Apply a zero-preserving unary function to a sparse tensor A_sp and return the result. dense_result = func(A_sp.to_dense()) return dense_result.to_sparse()"},{"question":"**Build and Optimize a Simple Neural Network Model with PyTorch JIT** # Objective: You are required to demonstrate your understanding of PyTorch\'s Just-In-Time (JIT) compilation utilities by building and optimizing a simple neural network model. This task will involve scripting and tracing the model, and comparing the performance improvements obtained. # Problem Statement: 1. **Build a Simple Neural Network Model**: - Define a simple feedforward neural network model using PyTorch with one hidden layer. - Write a function `build_model(input_dim, hidden_dim, output_dim)` to construct this model. 2. **Train the Model**: - Create a synthetic dataset using `torch.randn` for model training. - Train the model using the synthetic dataset. - Write a function `train_model(model, data, targets, epochs)` that trains the model for a given number of epochs. 3. **Optimize the Model with JIT**: - Optimize the model using PyTorch JIT by scripting or tracing. - Write a function `optimize_model(model)` that returns the optimized model. 4. **Compare Performance**: - Compare the performance (inference speed) of the original model and the optimized model. - Write a function `compare_performance(orig_model, optimized_model, data)` that prints the inference times. # Function Signatures: ```python def build_model(input_dim: int, hidden_dim: int, output_dim: int) -> torch.nn.Module: pass def train_model(model: torch.nn.Module, data: torch.Tensor, targets: torch.Tensor, epochs: int) -> None: pass def optimize_model(model: torch.nn.Module) -> torch.jit.ScriptModule: pass def compare_performance(orig_model: torch.nn.Module, optimized_model: torch.jit.ScriptModule, data: torch.Tensor) -> None: pass ``` # Constraints: - Use ReLU activation function for the hidden layer. - Use Mean Squared Error (MSE) loss criterion. - Use Stochastic Gradient Descent (SGD) as the optimizer. # Input: - For `build_model`: - `input_dim` : integer representing the input features. - `hidden_dim` : integer representing the hidden layer neurons. - `output_dim` : integer representing the output features. - For `train_model`: - `model` : A PyTorch neural network model. - `data` : A tensor containing the input data. - `targets` : A tensor containing the target values. - `epochs` : An integer representing the number of epochs. - For `optimize_model`: - `model` : A trained PyTorch model. - For `compare_performance`: - `orig_model` : The original (unoptimized) model. - `optimized_model` : The optimized model using JIT. - `data` : A tensor containing the input data for performance comparison. # Output: The `compare_performance` function should print the inference times for both the original and optimized models. # Example Usage: ```python input_dim = 10 hidden_dim = 5 output_dim = 1 # Build and Train Model model = build_model(input_dim, hidden_dim, output_dim) data = torch.randn(100, input_dim) targets = torch.randn(100, output_dim) train_model(model, data, targets, epochs=50) # Optimize the Model optimized_model = optimize_model(model) # Compare Performance compare_performance(model, optimized_model, data) ``` By executing the above steps, you demonstrate your ability to build, train, optimize, and evaluate a PyTorch model using the JIT utilities, showing a solid understanding of both basic and advanced functionalities of PyTorch.","solution":"import torch import torch.nn as nn import torch.optim as optim import time def build_model(input_dim, hidden_dim, output_dim): Build a simple feedforward neural network with one hidden layer. model = nn.Sequential( nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, output_dim) ) return model def train_model(model, data, targets, epochs): Train the model using the provided data and targets for a specified number of epochs. criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) for epoch in range(epochs): optimizer.zero_grad() outputs = model(data) loss = criterion(outputs, targets) loss.backward() optimizer.step() def optimize_model(model): Optimize the model using PyTorch JIT. optimized_model = torch.jit.script(model) return optimized_model def compare_performance(orig_model, optimized_model, data): Compare the inference performance of the original and optimized models. # Measure performance of the original model start_time = time.time() with torch.no_grad(): orig_model(data) orig_inference_time = time.time() - start_time # Measure performance of the optimized model start_time = time.time() with torch.no_grad(): optimized_model(data) optimized_inference_time = time.time() - start_time print(f\\"Original Model Inference Time: {orig_inference_time:.6f} seconds\\") print(f\\"Optimized Model Inference Time: {optimized_inference_time:.6f} seconds\\")"},{"question":"You are tasked with ensuring that the \\"pip\\" installer is correctly set up in various environments using the `ensurepip` module. Write a Python function, `manage_pip`, that accepts a configuration dictionary and uses the `ensurepip` module\'s API to install or upgrade \\"pip\\" as per the given configuration. # Function Signature ```python def manage_pip(config: dict) -> str: pass ``` # Input - The function will receive a single argument, `config`, which is a dictionary with the following possible keys: - `\\"action\\"`: a string that can be `\\"install\\"` or `\\"upgrade\\"`. If `\\"install\\"`, it will install \\"pip\\" if not present. If `\\"upgrade\\"`, it will upgrade \\"pip\\" to the latest version bundled with `ensurepip`. - `\\"root\\"`: (optional) a string specifying an alternative root directory. - `\\"user\\"`: (optional) a boolean indicating whether to use the user scheme. - `\\"altinstall\\"`: (optional) a boolean indicating whether to avoid installing `pipX`. - `\\"default_pip\\"`: (optional) a boolean indicating whether to install the `pip` script in addition to `pipX` and `pipX.Y`. - `\\"verbosity\\"`: (optional) an integer specifying the level of output verbosity. # Output - The function should return a string message: - `\\"pip installed\\"` if \\"pip\\" is successfully installed. - `\\"pip upgraded\\"` if \\"pip\\" is successfully upgraded. - `\\"pip already up-to-date\\"` if \\"pip\\" is already up-to-date when trying to upgrade. - Errors related to conflicting options or execution issues should be appropriately raised as exceptions. # Constraints - Use the `ensurepip.bootstrap` function for bootstrapping \\"pip\\". - Manage conflicting options, such as both `altinstall` and `default_pip` being set, by raising a `ValueError`. - Ensure the function handles missing keys in `config` gracefully, using defaults as specified in the documentation. # Example ```python # Example function call config = { \\"action\\": \\"install\\", \\"root\\": \\"/my/custom/root\\", \\"user\\": False, \\"altinstall\\": False, \\"default_pip\\": True, \\"verbosity\\": 1 } print(manage_pip(config)) # Expected Output: pip installed ``` Write the function `manage_pip` to accomplish the described task.","solution":"import ensurepip def manage_pip(config: dict) -> str: action = config.get(\\"action\\") root = config.get(\\"root\\", None) user = config.get(\\"user\\", False) altinstall = config.get(\\"altinstall\\", False) default_pip = config.get(\\"default_pip\\", True) verbosity = config.get(\\"verbosity\\", 0) if action not in [\\"install\\", \\"upgrade\\"]: raise ValueError(\\"Invalid action. Must be \'install\' or \'upgrade\'.\\") if altinstall and default_pip: raise ValueError(\\"Conflicting options: \'altinstall\' and \'default_pip\' cannot both be True.\\") try: ensurepip.bootstrap( root=root, upgrade=action == \\"upgrade\\", user=user, altinstall=altinstall, default_pip=default_pip, verbosity=verbosity ) return \\"pip installed\\" if action == \\"install\\" else \\"pip upgraded\\" except ensurepip.EnsurepipError: if action == \\"upgrade\\": return \\"pip already up-to-date\\" else: raise"},{"question":"You are required to implement a Python class `SyslogManager` that utilizes the `syslog` module to manage logging operations with various functionalities. This class should provide methods to configure logging options, send log messages with different priorities and facilities, and manage log masks. Requirements 1. **Initialization**: Initialize the class with default values for `ident`, `logoption`, and `facility`. 2. **Configuration**: - The method `configure_logging(self, ident: str = None, logoption: int = 0, facility: int = syslog.LOG_USER)` should configure the logging options. - If `ident` is not provided, it should default to `sys.argv[0]` with leading path components stripped. 3. **Logging Messages**: - The method `log_message(self, priority: int, message: str)` should send a log message with the given priority. 4. **Set Log Mask**: - The method `set_log_mask(self, maskpri: int)` should set the priority mask and return the previous mask value. 5. **Close Logging**: - The method `close_logging(self)` should reset the syslog module values and close the system log. # Class Definition ```python import syslog class SyslogManager: def __init__(self): # Initialize with default values self.ident = None self.logoption = 0 self.facility = syslog.LOG_USER def configure_logging(self, ident: str = None, logoption: int = 0, facility: int = syslog.LOG_USER): # Configure the logging options pass def log_message(self, priority: int, message: str): # Log the message with the given priority pass def set_log_mask(self, maskpri: int): # Set the priority mask and return the previous mask value pass def close_logging(self): # Close the logging pass ``` # Input and Output 1. **configure_logging(self, ident: str = None, logoption: int = 0, facility: int = syslog.LOG_USER)**: - `ident` (str): Identifier string to prepend to each message (default is `None`). - `logoption` (int): Bit field for logging options (default is `0`). - `facility` (int): Facility for messages (default is `syslog.LOG_USER`). - **Output**: None 2. **log_message(self, priority: int, message: str)**: - `priority` (int): Priority level for the log message. - `message` (str): Message to log. - **Output**: None 3. **set_log_mask(self, maskpri: int)**: - `maskpri` (int): Priority mask value. - **Output**: Returns the previous mask value (int). 4. **close_logging(self)**: - **Output**: None Constraints - Ensure `priority` for `log_message` is one of the defined priority levels. - Ensure `facility` for `configure_logging` is one of the defined facilities. - Assume the provided `ident` is a valid string. Example ```python # Example usage of SyslogManager manager = SyslogManager() manager.configure_logging(ident=\'MyApp\', logoption=syslog.LOG_PID, facility=syslog.LOG_MAIL) manager.log_message(syslog.LOG_INFO, \'Initialization started\') manager.set_log_mask(syslog.LOG_UPTO(syslog.LOG_WARNING)) manager.log_message(syslog.LOG_ERR, \'An error has occurred\') manager.close_logging() ``` Implement `SyslogManager` to meet the requirements outlined above.","solution":"import sys import syslog class SyslogManager: def __init__(self): # Initialize with default values self.ident = None self.logoption = 0 self.facility = syslog.LOG_USER def configure_logging(self, ident: str = None, logoption: int = 0, facility: int = syslog.LOG_USER): # Use sys.argv[0] if \'ident\' is not provided self.ident = ident if ident is not None else sys.argv[0].split(\'/\')[-1] self.logoption = logoption self.facility = facility syslog.openlog(self.ident, self.logoption, self.facility) def log_message(self, priority: int, message: str): # Log the message with the given priority syslog.syslog(priority, message) def set_log_mask(self, maskpri: int): # Set the priority mask and return the previous mask value return syslog.setlogmask(syslog.LOG_UPTO(maskpri)) def close_logging(self): # Close the logging syslog.closelog()"},{"question":"Objective Demonstrate your understanding of pandas `GroupBy` functionality by manipulating and analyzing a dataset to produce desired statistical insights. Problem Statement You are provided with a pandas DataFrame containing sales data for a retail store. The DataFrame includes the following columns: - `TransactionID` (unique identifier for each transaction) - `Date` (transaction date in YYYY-MM-DD format) - `ProductID` (identifier for the product sold) - `Quantity` (number of units sold) - `Price` (price per unit in USD) - `CustomerID` (identifier for the customer) - `StoreID` (identifier for the store where the sale was made) **Task:** 1. Group the DataFrame by `StoreID` and `Date`. 2. For each group, calculate the following metrics: - Total sales amount (sum of `Quantity * Price`) - Total transactions (count of `TransactionID`) - Average transaction value (average of `Quantity * Price`) 3. Add these calculated metrics as new columns to the original DataFrame. 4. Return a DataFrame sorted by `StoreID` and `Date` in ascending order, containing the original columns and the new metrics columns. Input - A pandas DataFrame `df` with the specified structure. Output - A pandas DataFrame sorted by `StoreID` and `Date` in ascending order, containing the original columns and the following new columns: - `TotalSales` - `TotalTransactions` - `AvgTransactionValue` Constraints - Use primarily `GroupBy` operations for grouping and calculations. - Ensure efficient computation for large datasets. - Do not modify the original input DataFrame; create and return a new DataFrame. Code Template ```python import pandas as pd def analyze_sales_data(df): Analyzes sales data by grouping by StoreID and Date, and calculating total sales, total transactions, and average transaction value for each group. Args: df (pd.DataFrame): Input sales data DataFrame. Returns: pd.DataFrame: DataFrame containing the original columns and new metrics columns, sorted by StoreID and Date. # Implement solution here # Group by StoreID and Date grouped_df = df.groupby([\'StoreID\', \'Date\']) # Calculate required metrics metrics = grouped_df.agg( TotalSales=(\'Quantity\', lambda x: (x * df.loc[x.index, \'Price\']).sum()), TotalTransactions=(\'TransactionID\', \'count\'), AvgTransactionValue=(\'Quantity\', lambda x: (x * df.loc[x.index, \'Price\']).sum() / x.count()) ).reset_index() # Merge metrics with the original dataframe result_df = pd.merge(df, metrics, on=[\'StoreID\', \'Date\'], how=\'left\') # Sort the resulting dataframe by StoreID and Date result_df = result_df.sort_values(by=[\'StoreID\', \'Date\']) return result_df # Example Usage # df = pd.read_csv(\'sales_data.csv\') # result = analyze_sales_data(df) # print(result.head()) ```","solution":"import pandas as pd def analyze_sales_data(df): Analyzes sales data by grouping by StoreID and Date, and calculating total sales, total transactions, and average transaction value for each group. Args: df (pd.DataFrame): Input sales data DataFrame. Returns: pd.DataFrame: DataFrame containing the original columns and new metrics columns, sorted by StoreID and Date. # Calculate the total sales per transaction df[\'TotalTransactionSales\'] = df[\'Quantity\'] * df[\'Price\'] # Group by StoreID and Date grouped_df = df.groupby([\'StoreID\', \'Date\']) # Calculate required metrics metrics = grouped_df.agg( TotalSales=(\'TotalTransactionSales\', \'sum\'), TotalTransactions=(\'TransactionID\', \'count\'), AvgTransactionValue=(\'TotalTransactionSales\', \'mean\') ).reset_index() # Merge metrics with the original dataframe result_df = pd.merge(df, metrics, on=[\'StoreID\', \'Date\'], how=\'left\') # Sort the resulting dataframe by StoreID and Date result_df = result_df.sort_values(by=[\'StoreID\', \'Date\']) # Drop the helper column result_df = result_df.drop(columns=[\'TotalTransactionSales\']) return result_df"},{"question":"**Objective:** Demonstrate proficiency with the seaborn library, specifically the `seaborn.objects` module, by creating and customizing plots with various themes and styles. **Problem Statement:** You are provided with the \'tips\' dataset, which is commonly used to show how to plot data with seaborn. Follow the steps below to create a specialized plot and customize it using different themes and styles. 1. Load the \'tips\' dataset using the `seaborn.load_dataset` function. 2. Create a `Plot` object with the following specifications: - x-axis: total_bill - y-axis: tip - Add color differentiation for \'sex\' - Create a scatter plot with regression lines for each category in \'sex\' 3. Apply a theme with the following customizations: - `axes.facecolor`: light grey - `axes.edgecolor`: dark green - `lines.linewidth`: 2 4. Combine this theme with the seaborn style \\"whitegrid\\" and the plotting context \\"talk\\". 5. Finally, set the default global theme to use the combined theme and generate the specified plot. **Specifications:** - **Input**: No direct input, but you should demonstrate the process in code. - **Output**: Display the customized plot. **Instructions:** 1. Import the necessary modules from seaborn and matplotlib. 2. Load the \'tips\' dataset. 3. Create the `Plot` object and customize it as specified. 4. Combine the themes and apply them. 5. Set the global default theme. 6. Display the plot. **Example Code Structure:** ```python import seaborn.objects as so from seaborn import load_dataset, axes_style, plotting_context from matplotlib import style # Load the \'tips\' dataset tips = load_dataset(\\"tips\\") # Create the Plot object with specified parameters p = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"sex\\") .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Apply the custom theme custom_theme = { \\"axes.facecolor\\": \\"lightgrey\\", \\"axes.edgecolor\\": \\"green\\", \\"lines.linewidth\\": 2 } # Combine with seaborn style and plotting context combined_theme = axes_style(\\"whitegrid\\") | plotting_context(\\"talk\\") | custom_theme # Set the default global theme so.Plot.config.theme.update(combined_theme) # Display the plot p.theme(combined_theme) p ``` Follow the structure and ensure all steps are covered to achieve the specified plot customization.","solution":"import seaborn.objects as so from seaborn import load_dataset, axes_style, plotting_context import matplotlib.pyplot as plt def create_customized_plot(): # Load the \'tips\' dataset tips = load_dataset(\\"tips\\") # Create the Plot object with specified parameters p = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"sex\\") .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Apply the custom theme custom_theme = { \\"axes.facecolor\\": \\"lightgrey\\", \\"axes.edgecolor\\": \\"darkgreen\\", \\"lines.linewidth\\": 2 } # Combine with seaborn style and plotting context combined_theme = axes_style(\\"whitegrid\\") combined_context = plotting_context(\\"talk\\") # Apply the custom properties to the existing styles combined_theme.update(custom_theme) # Set the default global theme p.theme(combined_theme) # Display the plot p.show() # Run the function to create and display the customized plot create_customized_plot()"},{"question":"# Descriptor-Based Attribute Management and Validation **Objective**: Implement a descriptor to manage and validate attributes in a class, ensuring type and range constraints on each attribute while maintaining a dynamic and customizable attribute management system. **Problem Statement**: You are required to implement a `ValidatedAttribute` descriptor class that will manage and validate attributes inside a `Product` class. Each attribute managed by `ValidatedAttribute` must fulfill specified constraints such as type and range. **Requirements**: 1. Implement the `ValidatedAttribute` descriptor class with appropriate methods (`__get__`, `__set__`, `__delete__`, and `__set_name__`). 2. The descriptor should enforce type and optional range constraints at the time of setting the attribute value. 3. The `Product` class will use the `ValidatedAttribute` descriptor for its attributes `name` (string with a min length of 3), `price` (float greater than 0), and `quantity` (integer between 0 and 100). **Implementation Details**: 1. **ValidatedAttribute Class**: - Should initialize with parameters `expected_type`, `min_value` (optional), `max_value` (optional). - Implement the `__set_name__` method to store the attribute\'s name and private attribute name. - Implement the `__get__` method to retrieve the attribute value, ensuring it is fetched from the private attribute. - Implement the `__set__` method to set the attribute value, enforcing type verification and range constraints. 2. **Product Class**: - Use `ValidatedAttribute` to manage `name`, `price`, and `quantity`. - Implement `__init__` to initialize these attributes. **Constraints**: - The `name` should be a string with a minimum length of 3 characters. - The `price` should be a float greater than 0. - The `quantity` should be an integer between 0 and 100. **Input**: - Creating a `Product` object with valid and invalid attribute values to test the constraints. **Output**: - Should raise appropriate exceptions when constraints are not met: - `TypeError` if the attribute value is not of the expected type. - `ValueError` if the attribute value does not meet the specified range constraints. **Example**: ```python class ValidatedAttribute: def __init__(self, expected_type, min_value=None, max_value=None): self.expected_type = expected_type self.min_value = min_value self.max_value = max_value def __set_name__(self, owner, name): self.public_name = name self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): if not isinstance(value, self.expected_type): raise TypeError(f\'Expected {self.public_name} to be of type {self.expected_type.__name__}\') if self.min_value is not None and value < self.min_value: raise ValueError(f\'Expected {self.public_name} to be at least {self.min_value}\') if self.max_value is not None and value > self.max_value: raise ValueError(f\'Expected {self.public_name} to be no more than {self.max_value}\') setattr(obj, self.private_name, value) class Product: name = ValidatedAttribute(str, min_value=3) price = ValidatedAttribute(float, min_value=0) quantity = ValidatedAttribute(int, min_value=0, max_value=100) def __init__(self, name, price, quantity): self.name = name self.price = price self.quantity = quantity # Test the class with valid data p = Product(\'Widget\', 20.0, 10) # Test the class with invalid data try: p.price = -5.0 except ValueError as e: print(e) # Expected price to be at least 0 try: p.quantity = 150 except ValueError as e: print(e) # Expected quantity to be no more than 100 try: p.name = \'Hi\' except ValueError as e: print(e) # Expected name to be at least 3 ``` Ensure the implementation handles all the edge cases and provides clear error messages.","solution":"class ValidatedAttribute: def __init__(self, expected_type, min_length=None, min_value=None, max_value=None): self.expected_type = expected_type self.min_length = min_length self.min_value = min_value self.max_value = max_value def __set_name__(self, owner, name): self.public_name = name self.private_name = f\'_{name}\' def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): if not isinstance(value, self.expected_type): raise TypeError(f\'Expected {self.public_name} to be of type {self.expected_type.__name__}\') if self.min_length is not None and len(value) < self.min_length: raise ValueError(f\'Expected {self.public_name} to have at least {self.min_length} characters\') if self.min_value is not None and value < self.min_value: raise ValueError(f\'Expected {self.public_name} to be at least {self.min_value}\') if self.max_value is not None and value > self.max_value: raise ValueError(f\'Expected {self.public_name} to be no more than {self.max_value}\') setattr(obj, self.private_name, value) class Product: name = ValidatedAttribute(str, min_length=3) price = ValidatedAttribute(float, min_value=0) quantity = ValidatedAttribute(int, min_value=0, max_value=100) def __init__(self, name, price, quantity): self.name = name self.price = price self.quantity = quantity"},{"question":"**Title: Porting Code for Text and Binary Data Handling Compatibility** **Objective:** Write a Python function that processes a mixed list of text and binary data, ensuring it handles the differences between Python 2 and Python 3 correctly. Specifically, the function should identify whether each item in the list is text or binary data, process it accordingly, and return a uniformly processed list of text strings in Python 3 while maintaining compatibility with Python 2. **Function Signature:** ```python def process_data(data_list): Processes a mixed list of text and binary data items to ensure compatibility between Python 2 and Python 3. :param data_list: List containing mixed text (unicode/str) and binary (str/bytes) data. :return: List containing processed text strings. pass ``` **Problem Statement:** 1. The function should accept a list containing mixed text and binary data. In Python 2, text data might be of type `unicode` or `str`, and binary data will be of type `str`. In Python 3, text data will be of type `str` and binary data will be of type `bytes`. 2. For each item in the list: - If it\'s binary data, decode it using \'utf-8\' encoding. - If it\'s text data, ensure it\'s of type `unicode` in Python 2 or `str` in Python 3. 3. Return the processed list where all items are text strings. 4. The function should handle the input correctly in both Python 2 and Python 3. **Constraints:** - Assume all binary data can be decoded using \'utf-8\' encoding. - The input list will have at least one element. - You are not allowed to use any third-party packages like `six`, `future`, or `modernize`. - You must implement the function without importing any compatibility modules. **Input:** ```python data_list: a list of mixed text and binary data items ``` **Output:** A list of processed text strings. **Example:** ```python # Python 2 data_list = [u\'text1\', \'binaryx61data\', u\'text2\', \'binaryx62data\'] # Python 3 data_list = [\'text1\', b\'binaryadata\', \'text2\', b\'binarybdata\'] # Output in both Python 2 and Python 3 should be: # [\'text1\', \'binaryadata\', \'text2\', \'binarybdata\'] ``` **Testing Your Function:** Make sure to test your function in both Python 2 and Python 3 to ensure it handles the data correctly in both environments.","solution":"def process_data(data_list): Processes a mixed list of text and binary data items to ensure compatibility between Python 2 and Python 3. :param data_list: List containing mixed text (unicode/str) and binary (str/bytes) data. :return: List containing processed text strings. processed_list = [] for item in data_list: if isinstance(item, bytes): # Binary data, decode to text string using utf-8 processed_list.append(item.decode(\'utf-8\')) else: # Text data, ensure it\'s a text string if isinstance(item, str): processed_list.append(item) else: # For Python 2 compatibility if it\'s unicode processed_list.append(item.encode(\'utf-8\')) return processed_list"},{"question":"# Question: File and Log Processing with Classes You are required to write a Python program that processes a log file containing records of website visits. Each line in the log file contains a timestamp and a URL, separated by a space. Your program should read the log file, count the number of visits to each URL, and write the results to an output file in descending order of visit counts. Implement this program using Python classes to encapsulate the functionality. Input 1. A log file named `web_log.txt` with lines in the format: ``` 2021-10-01T12:00:00Z http://example.com 2021-10-01T12:05:00Z http://example.com/page1 2021-10-01T12:10:00Z http://example.com ... ``` 2. The output file should be named `visit_counts.txt`. Expected Output The `visit_counts.txt` file should contain lines in the format: ``` http://example.com 10 http://example.com/page1 5 ... ``` Constraints - Each URL in the log file should appear only once in the output file, with the total count of visits. - Sort the entries in the output file in descending order of visit counts. If two URLs have the same number of visits, their order relative to each other in the output does not matter. - The program should handle large files efficiently. Implementation Details 1. Define a class `LogProcessor` with the following methods: - `__init__(self, log_filename: str, output_filename: str)` - `read_log(self) -> None`: Reads the log file and counts visits. - `write_counts(self) -> None`: Writes the visit counts to the output file. 2. Methods should encapsulate the following: - Reading from the input file. - Processing the log entries to count visits. - Writing the results to the output file in the specified format. Sample Code Structure ```python class LogProcessor: def __init__(self, log_filename: str, output_filename: str): self.log_filename = log_filename self.output_filename = output_filename self.visit_counts = {} def read_log(self) -> None: # Read the log file and populate visit_counts def write_counts(self) -> None: # Write the visit counts to the output file # Example usage processor = LogProcessor(\'web_log.txt\', \'visit_counts.txt\') processor.read_log() processor.write_counts() ``` Performance - Your solution should efficiently handle log files with potentially millions of lines. Test your solution with sample log data to ensure correctness.","solution":"class LogProcessor: def __init__(self, log_filename: str, output_filename: str): self.log_filename = log_filename self.output_filename = output_filename self.visit_counts = {} def read_log(self) -> None: Reads the log file and counts visits for each URL. with open(self.log_filename, \'r\') as file: for line in file: timestamp, url = line.strip().split() if url in self.visit_counts: self.visit_counts[url] += 1 else: self.visit_counts[url] = 1 def write_counts(self) -> None: Writes the visit counts to the output file, sorted by count in descending order. sorted_visits = sorted(self.visit_counts.items(), key=lambda x: x[1], reverse=True) with open(self.output_filename, \'w\') as file: for url, count in sorted_visits: file.write(f\\"{url} {count}n\\") # Example usage # processor = LogProcessor(\'web_log.txt\', \'visit_counts.txt\') # processor.read_log() # processor.write_counts()"},{"question":"# Coding Exercise: Concurrent Resource Management Objective: You are required to implement a simple concurrent system that manages access to a shared resource among multiple threads, using the `threading` module. This will test your understanding of thread creation, synchronization using locks and conditions, and managing concurrent execution. Problem Statement: Implement a class `SharedResourceManager` that manages access to a shared resource (`self.resource`). The `SharedResourceManager` should allow threads to safely add or retrieve items from this resource while ensuring proper synchronization. The `SharedResourceManager` should support the following methods: - `add_item(item)`: Adds an item to the shared resource. - `retrieve_item()`: Retrieves and removes an item from the shared resource. If no item is available, it should wait until an item is added. Use a `Condition` to manage the synchronization between adding and retrieving items. Ensure that the operations are thread-safe and implement correct locking mechanisms to handle potential race conditions. Constraints: 1. The `resource` is initialized as an empty list. 2. You cannot use any other data structures except standard Python lists and the threading constructs provided in the `threading` module. 3. The `retrieve_item` method should block if the resource is empty and wait until an item becomes available. # Implementation: Your implementation should include a class `SharedResourceManager` with the following methods: ```python import threading class SharedResourceManager: def __init__(self): self.resource = [] self.condition = threading.Condition() def add_item(self, item): # Implement this method to add an item to the resource safely pass def retrieve_item(self): # Implement this method to retrieve an item from the resource safely pass ``` # Example Usage: ```python def producer(manager, item): print(f\'Producer adding item: {item}\') manager.add_item(item) print(f\'Item added: {item}\') def consumer(manager): item = manager.retrieve_item() print(f\'Consumer retrieved item: {item}\') # Create shared resource manager manager = SharedResourceManager() # Create producer and consumer threads producer_thread = threading.Thread(target=producer, args=(manager, \'item1\')) consumer_thread = threading.Thread(target=consumer, args=(manager,)) # Start threads producer_thread.start() consumer_thread.start() # Join threads producer_thread.join() consumer_thread.join() ``` # Testing: 1. Ensure that the retrieved item matches the added item. 2. Test with multiple producer and consumer threads. 3. Verify that the `retrieve_item` method blocks correctly when there are no items and resumes appropriately when items are added. **Note:** - Make sure to handle exceptions and edge cases as appropriate. - Ensure your solution is efficient and avoids unnecessary busy-waiting.","solution":"import threading class SharedResourceManager: def __init__(self): self.resource = [] self.condition = threading.Condition() def add_item(self, item): with self.condition: self.resource.append(item) self.condition.notify() def retrieve_item(self): with self.condition: while not self.resource: self.condition.wait() return self.resource.pop(0)"},{"question":"# PyTorch Random Number Generation and Reproducibility Objective: You are given the task to create a function using PyTorch that sets a random seed, generates random tensors, and ensures reproducibility by saving and restoring the generator\'s state. Task: 1. **Set a random seed** – Create a random seed to ensure reproducibility. 2. **Generate random tensors**: - Generate a tensor of shape `(3, 4)` with random numbers from a uniform distribution over `[0, 1)`. - Generate a tensor of shape `(2, 5)` with random integers between `low=0` and `high=10`. 3. **Save and Restore State**: - Save the random state after the generation of the tensors. - Generate a new tensor of shape `(2, 3)` with random numbers from a normal distribution. - Restore the initial state and generate the tensor again from the normal distribution to confirm it produces the same values. Implementation: Implement the function `generate_random_tensors_and_verify()` that performs the above steps. # Input: - The function does not take any inputs. # Output: - The function should return a dictionary containing: - \'uniform_tensor\': The tensor generated by uniform distribution. - \'integer_tensor\': The tensor generated by random integers. - \'normal_tensor_before\': The tensor generated from normal distribution before restoring the state. - \'normal_tensor_after\': The tensor generated from normal distribution after restoring the state. # Example Output: ```python { \'uniform_tensor\': tensor([[0.4963, 0.7682, 0.0885, 0.1320], [0.3074, 0.6341, 0.4901, 0.8964], [0.4556, 0.6323, 0.3489, 0.4017]]), \'integer_tensor\': tensor([[5, 4, 4, 3, 9], [2, 0, 5, 5, 6]]), \'normal_tensor_before\': tensor([[ 0.0392, -0.8795, 0.2236], [ 2.5692, -1.8183, 0.2638]]), \'normal_tensor_after\': tensor([[ 0.0392, -0.8795, 0.2236], [ 2.5692, -1.8183, 0.2638]]) } ``` # Constraints: - Use the `torch.random` module for all random number generation. - Ensure that the code is reproducible by setting random seeds appropriately and managing state. Notes: - Use `torch.manual_seed(seed)` to set the seed. - Save the state with `torch.random.get_rng_state()`. - Restore the state with `torch.random.set_rng_state(state)`.","solution":"import torch def generate_random_tensors_and_verify(): # Step 1: Set a random seed seed = 42 torch.manual_seed(seed) # Step 2: Generate random tensors uniform_tensor = torch.rand(3, 4) integer_tensor = torch.randint(low=0, high=10, size=(2, 5)) # Step 3: Save the random state saved_state = torch.random.get_rng_state() # Step 4: Generate a tensor from a normal distribution normal_tensor_before = torch.randn(2, 3) # Step 5: Restore the initial state and generate the tensor again torch.random.set_rng_state(saved_state) normal_tensor_after = torch.randn(2, 3) return { \'uniform_tensor\': uniform_tensor, \'integer_tensor\': integer_tensor, \'normal_tensor_before\': normal_tensor_before, \'normal_tensor_after\': normal_tensor_after }"},{"question":"# Asynchronous Python with asyncio Problem Statement You are tasked with creating a simplified web scraping tool using `asyncio`. This tool will concurrently fetch multiple web pages and process the HTML content to extract specific data. To ensure that the tool performs efficiently and correctly, you must follow best practices outlined in the provided `asyncio` documentation. # Requirements: 1. Implement a function `fetch_url(session, url)` that sends an HTTP GET request to the specified URL using the `aiohttp` library and returns the response text. 2. Implement a function `process_html(html)` that processes the HTML content to extract data. For simplicity, assume we are extracting and returning the title of the web page. 3. Implement the `main(urls)` function that: - Accepts a list of URLs. - Concurrently fetches the content of each URL using the `fetch_url` function. - Processes each HTML content using the `process_html` function. - Prints out the titles of all fetched web pages. # Constraints: - Use the `asyncio` module to handle concurrency. - Properly handle exceptions that might occur during HTTP requests or HTML processing. - Ensure that any coroutine is awaited properly and avoid any potential pitfalls mentioned in the documentation. - Validate thread safety using the appropriate methods where necessary. - Generate and handle logs at the `DEBUG` level for debugging purposes. - Handle potential blocking I/O operations by running them in an executor. # Input and Output **Input:** - A list of URLs (strings). **Output:** - Titles of the web pages printed to the stdout. # Example ```python import asyncio import aiohttp import logging from concurrent.futures import ThreadPoolExecutor logging.basicConfig(level=logging.DEBUG) async def fetch_url(session, url): try: async with session.get(url) as response: response.raise_for_status() return await response.text() except aiohttp.ClientError as e: logging.error(f\\"Error fetching {url}: {e}\\") return None def process_html(html): # Dummy implementation, replace with real HTML parsing start = html.find(\'<title>\') + len(\'<title>\') end = html.find(\'</title>\', start) if start == -1 or end == -1: return \\"No Title Found\\" return html[start:end].strip() async def main(urls): async with aiohttp.ClientSession() as session: tasks = [fetch_url(session, url) for url in urls] html_contents = await asyncio.gather(*tasks) with ThreadPoolExecutor() as executor: loop = asyncio.get_running_loop() titles = await asyncio.gather(*[loop.run_in_executor(executor, process_html, html) for html in html_contents if html]) for title in titles: print(title) if __name__ == \\"__main__\\": urls = [ \\"https://www.example.com\\", \\"https://www.python.org\\", \\"https://www.asyncio.org\\" ] asyncio.run(main(urls)) ``` # Notes - Ensure you have the necessary libraries installed using `pip install aiohttp`. - You may modify the `process_html` function to use an HTML parsing library like `BeautifulSoup` if desired. # Additional Information - Refer to `asyncio` and `aiohttp` documentation for insights and a broader understanding of how to handle asynchronous operations in Python effectively. - Ensure that your solution logs important events for debugging and adheres to asyncio’s best practices to avoid common pitfalls.","solution":"import asyncio import aiohttp import logging from concurrent.futures import ThreadPoolExecutor logging.basicConfig(level=logging.DEBUG) async def fetch_url(session, url): try: async with session.get(url) as response: response.raise_for_status() return await response.text() except aiohttp.ClientError as e: logging.error(f\\"Error fetching {url}: {e}\\") return None def process_html(html): start = html.find(\'<title>\') + len(\'<title>\') end = html.find(\'</title>\', start) if start == -1 or end == -1: return \\"No Title Found\\" return html[start:end].strip() async def main(urls): async with aiohttp.ClientSession() as session: tasks = [fetch_url(session, url) for url in urls] html_contents = await asyncio.gather(*tasks) with ThreadPoolExecutor() as executor: loop = asyncio.get_running_loop() titles = await asyncio.gather(*[loop.run_in_executor(executor, process_html, html) for html in html_contents if html]) for title in titles: print(title) if __name__ == \\"__main__\\": urls = [ \\"https://www.example.com\\", \\"https://www.python.org\\", \\"https://www.asyncio.org\\" ] asyncio.run(main(urls))"},{"question":"You are tasked with designing a custom data structure that associates additional data with objects without altering their attributes or preventing them from being garbage collected. Your task is to implement a `WeakReferenceMapper` class that uses weak references to associate arbitrary data with objects. The additional data should be removed automatically when the object is no longer in use. Requirements 1. **Class Definition**: Define a class named `WeakReferenceMapper`. 2. **Associating Data**: Implement a method `set_data(self, obj, data)` that associates `data` with `obj`. 3. **Retrieving Data**: Implement a method `get_data(self, obj)` that retrieves the associated `data` for `obj`. If the object has been garbage collected, return `None`. 4. **Removing Data**: Implement a method `remove_data(self, obj)` that explicitly removes the associated data for `obj`. 5. **Internal Data Structure**: Use an appropriate weak reference-based data structure (such as `WeakKeyDictionary` or `WeakValueDictionary`) to manage the associations. 6. **Automatic Cleanup**: Ensure that data is automatically cleaned up when its associated object is garbage collected. Constraints - Objects should be instances of classes that support weak references. - Data associated with an object can be of any type. - The `WeakReferenceMapper` class should ensure thread safety for all operations. Example Usage ```python from weakref import WeakKeyDictionary class WeakReferenceMapper: def __init__(self): self._data_map = WeakKeyDictionary() def set_data(self, obj, data): self._data_map[obj] = data def get_data(self, obj): return self._data_map.get(obj, None) def remove_data(self, obj): if obj in self._data_map: del self._data_map[obj] # Usage example if __name__ == \\"__main__\\": class TestClass: def __init__(self, name): self.name = name obj1 = TestClass(\\"Object1\\") obj2 = TestClass(\\"Object2\\") mapper = WeakReferenceMapper() mapper.set_data(obj1, \\"Data for Object1\\") mapper.set_data(obj2, \\"Data for Object2\\") print(mapper.get_data(obj1)) # Output: \\"Data for Object1\\" print(mapper.get_data(obj2)) # Output: \\"Data for Object2\\" del obj1 import gc gc.collect() # Collect garbage print(mapper.get_data(obj1)) # Output: None print(mapper.get_data(obj2)) # Output: \\"Data for Object2\\" ``` In this task, you will implement the required functionality for the `WeakReferenceMapper` class by leveraging the `weakref` module. Evaluation You will be evaluated on: - Correct implementation of the specified methods. - Proper usage of weak references to ensure automatic cleanup. - Thread safety in accessing and modifying the data structure. - Efficient handling of objects and their associated data.","solution":"from weakref import WeakKeyDictionary import threading class WeakReferenceMapper: def __init__(self): self._data_map = WeakKeyDictionary() self._lock = threading.Lock() def set_data(self, obj, data): with self._lock: self._data_map[obj] = data def get_data(self, obj): with self._lock: return self._data_map.get(obj, None) def remove_data(self, obj): with self._lock: if obj in self._data_map: del self._data_map[obj]"},{"question":"**Objective**: Assess students\' ability to work with file and directory operations, path manipulation, and applying pattern matching to filenames using Python. **Problem Statement**: You are tasked with developing a utility script to manage log files in a directory. This script should perform the following operations: 1. **Find Path Operations**: - Identify all files in a given directory (including subdirectories) with a \\".log\\" extension. - Return the total count of such log files. 2. **Temporary File Creation**: - Create a temporary directory to store copies of all identified \\".log\\" files. - Copy these log files into the temporary directory. 3. **Pattern Matching**: - Implement a feature to search within the log files in the temporary directory for a given string pattern and return filenames containing that pattern. 4. **Comparison**: - Write a function to compare the content of two log files and determine if they are identical. Implement the following functions: 1. `find_log_files(directory: str) -> int`: - **Input**: `directory` - the directory path to search for `.log` files. - **Output**: Returns the total count of `.log` files found. 2. `copy_to_temp_directory(files: List[str]) -> str`: - **Input**: `files` - a list of file paths to be copied. - **Output**: Returns the path to the temporary directory created. 3. `search_in_logs(directory: str, pattern: str) -> List[str]`: - **Input**: - `directory` - the path to the temporary directory containing the log files. - `pattern` - string pattern to search for within the log files. - **Output**: Returns a list of filenames (without path) that contain the given pattern. 4. `compare_log_files(file1: str, file2: str) -> bool`: - **Input**: - `file1` - path to the first log file. - `file2` - path to the second log file. - **Output**: Returns `True` if the content of the files are identical, `False` otherwise. **Constraints**: - Assume that the input directory paths are always valid and accessible. - Assume that the log files are in text format. - The solution should handle large directories and long log files efficiently. **Performance Requirements**: - The operations should be optimized for large datasets. - Temporary files and directories should be properly cleaned up after use. **Example**: ```python # Example Usage: # Assume the directory \'/var/logs\' contains \'.log\' files. log_count = find_log_files(\'/var/logs\') print(log_count) # Output might be: 5 log_files = [\'/var/logs/app1.log\', \'/var/logs/app2.log\', \'/var/logs/app3.log\'] temp_dir = copy_to_temp_directory(log_files) print(temp_dir) # Output might be: /tmp/tmpabcd1234 matching_files = search_in_logs(temp_dir, \'ERROR\') print(matching_files) # Output might be: [\'app1.log\', \'app3.log\'] is_identical = compare_log_files(\'/var/logs/app1.log\', \'temp_dir/app3.log\') print(is_identical) # Output might be: True or False ``` Implement and thoroughly test your solution.","solution":"import os import shutil import tempfile def find_log_files(directory: str) -> int: Identifies all files with a \\".log\\" extension in the given directory (including subdirectories) and returns the total count of these files. log_files_count = 0 for root, _, files in os.walk(directory): for file in files: if file.endswith(\\".log\\"): log_files_count += 1 return log_files_count def copy_to_temp_directory(files: list) -> str: Creates a temporary directory and copies the given list of files into it. Returns the path to the temporary directory. temp_dir = tempfile.mkdtemp() for file in files: if os.path.exists(file) and file.endswith(\'.log\'): shutil.copy(file, temp_dir) return temp_dir def search_in_logs(directory: str, pattern: str) -> list: Searches log files in the given directory for the specified pattern. Returns a list of filenames that contain the given pattern. matching_files = [] for root, _, files in os.walk(directory): for file in files: if file.endswith(\\".log\\"): with open(os.path.join(root, file), \'r\') as f: if pattern in f.read(): matching_files.append(file) return matching_files def compare_log_files(file1: str, file2: str) -> bool: Compares the content of two log files and determines if they are identical. Returns True if contents are identical, otherwise False. with open(file1, \'r\') as f1, open(file2, \'r\') as f2: file1_contents = f1.read() file2_contents = f2.read() return file1_contents == file2_contents"},{"question":"# Pandas Coding Assessment Objective: Write a function `clean_and_analyze_data` that takes in a pandas DataFrame containing missing values and performs the following tasks: 1. **Data Cleaning**: - Replace all instances of `NA` and `NaT` with `NaN`. - Fill missing values (NaN) in numeric columns with the mean of that column. - Fill missing values (NaN) in non-numeric columns with the mode (most frequent value) of that column. If there are multiple modes, use the first one in lexicographical order. 2. **Data Analysis**: - Calculate the sum of each numeric column. - Calculate the count of unique values for each non-numeric column. Input: - A pandas DataFrame `df` with mixed data types (numeric, string, datetime) and missing values. Output: - A dictionary with two keys: - `\'numeric_sums\'`: A dictionary where keys are the names of numeric columns and values are the sum of each numeric column after missing values have been filled. - `\'unique_counts\'`: A dictionary where keys are the names of non-numeric columns and values are the count of unique values in each column. Constraints: - You should not use any external libraries other than pandas and numpy. - The DataFrame can have an arbitrary number of columns and rows. - The DataFrame can potentially have multiple missing values in multiple columns. Performance: - The function should handle DataFrames with up to 100,000 rows efficiently. Example: ```python import pandas as pd import numpy as np def clean_and_analyze_data(df): # Replace `NA` and `NaT` with `NaN` df.replace([pd.NA, pd.NaT], np.nan, inplace=True) # Fill missing numeric values with the mean of their column numeric_cols = df.select_dtypes(include=[np.number]) df[numeric_cols.columns] = numeric_cols.apply(lambda col: col.fillna(col.mean()), axis=0) # Fill missing non-numeric values with the mode of their column non_numeric_cols = df.select_dtypes(exclude=[np.number]) for col in non_numeric_cols: mode = df[col].mode()[0] df[col].fillna(mode, inplace=True) # Calculate sum of numeric columns numeric_sums = numeric_cols.sum() # Calculate count of unique values for non-numeric columns unique_counts = non_numeric_cols.nunique() # Construct result dictionary result = { \'numeric_sums\': numeric_sums.to_dict(), \'unique_counts\': unique_counts.to_dict() } return result # Example DataFrame data = { \'A\': [1, 2, np.nan, 4, 5], \'B\': [3, np.nan, np.nan, 7, 1], \'C\': [\'foo\', \'bar\', \'foo\', np.nan, \'foo\'], \'D\': [pd.NaT, pd.Timestamp(\'20210101\'), pd.Timestamp(\'20210101\'), pd.NaT, pd.Timestamp(\'20210101\')] } df = pd.DataFrame(data) print(clean_and_analyze_data(df)) ``` Expected output (values may vary based on data given): ```python { \'numeric_sums\': {\'A\': 12.0, \'B\': 11.0}, \'unique_counts\': {\'C\': 2, \'D\': 1} } ```","solution":"import pandas as pd import numpy as np def clean_and_analyze_data(df): # Replace `NA` and `NaT` with `NaN` df.replace([pd.NA, pd.NaT], np.nan, inplace=True) # Fill missing numeric values with the mean of their column numeric_cols = df.select_dtypes(include=[np.number]) df[numeric_cols.columns] = numeric_cols.apply(lambda col: col.fillna(col.mean()), axis=0) # Fill missing non-numeric values with the mode of their column non_numeric_cols = df.select_dtypes(exclude=[np.number]) for col in non_numeric_cols: if df[col].isna().sum() > 0: mode = df[col].mode()[0] df[col].fillna(mode, inplace=True) # Calculate sum of numeric columns numeric_sums = numeric_cols.sum() # Calculate count of unique values for non-numeric columns unique_counts = non_numeric_cols.nunique() # Construct result dictionary result = { \'numeric_sums\': numeric_sums.to_dict(), \'unique_counts\': unique_counts.to_dict() } return result"},{"question":"**Objective**: To assess your understanding of seaborn’s aesthetic controls and customization capabilities. # Problem Statement You are provided with a dataset and your task is to plot and customize the appearance of the plots using Seaborn\'s styling, context, and spine removal functionalities. # Function Signature ```python def customize_plot_seaborn(styles: list, context: str, scale: float, remove_spines: bool): Parameters: styles (list): A list of seaborn styles to apply. Valid styles are \'darkgrid\', \'whitegrid\', \'dark\', \'white\', \'ticks\'. context (str): The context to scale the elements of the plot. Valid contexts are \'paper\', \'notebook\', \'talk\', \'poster\'. scale (float): The scaling factor for the font size. remove_spines (bool): Whether to remove the top and right spines of the axes. Returns: None: The function should directly render the customized plot. ``` # Instructions 1. **Apply Styles**: Iterate through the list of styles and create a subplot for each style. Use the `with` statement to apply each style temporarily. 2. **Set Context**: Apply the provided context and font scale globally before generating the plots. 3. **Generate Data and Plot**: - Generate a dataset using `numpy` where the data is a normally distributed with 20 samples and 6 features. - Create a boxplot for this data in each subplot. 4. **Remove Spines**: If `remove_spines` is `True`, remove the top and right spines for each plot. 5. **Title**: Each subplot should have a title indicating the applied seaborn style. 6. **Render Plot**: Finally, render all the subplots together in a single figure. # Constraints - The length of `styles` will be between 1 and 5. - `context` will be one of [\'paper\', \'notebook\', \'talk\', \'poster\']. - `scale` will be a positive float. # Example ```python styles = [\'darkgrid\', \'white\', \'ticks\'] context = \'talk\' scale = 1.5 remove_spines = True customize_plot_seaborn(styles, context, scale, remove_spines) ``` In the above example: - The plots will be styled according to `darkgrid`, `white`, and `ticks`. - The context will be `talk` with a font scale of `1.5`. - The top and right spines will be removed from each plot. # Note Make sure to import the required libraries: ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt ```","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def customize_plot_seaborn(styles, context, scale, remove_spines): Parameters: styles (list): A list of seaborn styles to apply. Valid styles are \'darkgrid\', \'whitegrid\', \'dark\', \'white\', \'ticks\'. context (str): The context to scale the elements of the plot. Valid contexts are \'paper\', \'notebook\', \'talk\', \'poster\'. scale (float): The scaling factor for the font size. remove_spines (bool): Whether to remove the top and right spines of the axes. Returns: None: The function should directly render the customized plot. # Set context and scaling sns.set_context(context, font_scale=scale) # Generate data data = np.random.randn(20, 6) # Create the figure and subplots fig, axes = plt.subplots(1, len(styles), figsize=(5 * len(styles), 5), squeeze=False) for idx, style in enumerate(styles): with sns.axes_style(style): ax = axes[0, idx] sns.boxplot(data=data, ax=ax) # Apply title ax.set_title(f\'Style: {style}\') # Remove spines if requested if remove_spines: sns.despine(ax=ax, top=True, right=True) plt.tight_layout() plt.show()"},{"question":"netrc File Manipulation **Objective**: Demonstrate your understanding of the `netrc` class by implementing a function that parses a netrc file, retrieves credentials for a host, and modifies or adds new credentials. Problem Statement: You are tasked with writing a Python function `manage_netrc(file_path: str, host: str, new_credentials: tuple=None) -> tuple` that performs the following operations: 1. **Parse the given netrc file**: Use the `netrc` class to parse the specified netrc file. 2. **Retrieve Credentials**: - If `new_credentials` is `None`, the function should return a tuple `(login, account, password)` for the given host. If the host is not found, return the credentials for the \'default\' entry if it exists, otherwise return `None`. 3. **Modify/Add Credentials**: - If `new_credentials` is provided, it should be a tuple `(login, account, password)`. - The function should update the credentials for the given host with the new credentials. If the host does not exist, it should add a new entry for the host with the provided credentials. Input: - `file_path` (str): The path to the netrc file. - `host` (str): The host for which credentials need to be retrieved or modified/added. - `new_credentials` (tuple, optional): A tuple `(login, account, password)` representing the new credentials. If not provided, the function should only retrieve the existing credentials. Output: - If `new_credentials` is `None`, return `(login, account, password)` tuple or `None`. - If `new_credentials` is provided, return `None`. Constraints: - Assume the netrc file always exists and follow the correct netrc file format. - Ensure the `netrc` class and its methods are used appropriately. Example Function Signature: ```python import netrc def manage_netrc(file_path: str, host: str, new_credentials: tuple=None) -> tuple: pass ``` Example Usage: ```python # Given netrc file: `example.netrc` # machine myftp login myuser password mypass # Retrieve existing credentials creds = manage_netrc(\'example.netrc\', \'myftp\') print(creds) # Output: (\'myuser\', None, \'mypass\') # Add new credentials manage_netrc(\'example.netrc\', \'newhost\', (\'newuser\', None, \'newpass\')) # Verify new credentials creds = manage_netrc(\'example.netrc\', \'newhost\') print(creds) # Output: (\'newuser\', None, \'newpass\') ``` **Note**: The function should handle any exceptions that arise during file parsing or modification and provide meaningful error messages.","solution":"import netrc def manage_netrc(file_path: str, host: str, new_credentials: tuple=None) -> tuple: Manages credentials in a netrc file. Parameters: file_path (str): The path to the netrc file. host (str): The host for which credentials need to be retrieved or modified/added. new_credentials (tuple, optional): A tuple (login, account, password) representing the new credentials. Returns: tuple: If new_credentials is None, returns a tuple (login, account, password) or None. None: If new_credentials is provided. try: # Parse the netrc file netrc_obj = netrc.netrc(file_path) if new_credentials is None: # Retrieve existing credentials creds = netrc_obj.authenticators(host) if creds is None: # If the host is not found, return default credentials if available creds = netrc_obj.authenticators(\'default\') return creds else: # Add or update credentials login, account, password = new_credentials if host in netrc_obj.hosts: netrc_obj.hosts[host] = (login, account, password) else: netrc_obj.hosts[host] = (login, account, password) # Save the updated netrc data back to the file with open(file_path, \'w\') as f: for host, creds in netrc_obj.hosts.items(): login, account, password = creds f.write(f\\"machine {host} login {login} password {password}n\\") return None except Exception as e: raise RuntimeError(f\\"Failed to manage netrc file: {e}\\")"},{"question":"# Command-Line File Organizer **Objective:** Design a Python script that organizes files in a given directory based on their extensions. The script should be able to take command-line arguments for the target directory and an optional argument for the destination directory. The script must use the `argparse` module for parsing the command-line arguments and the `os` module for performing file operations. **Requirements:** 1. The script must accept the following command-line arguments: - `--target-dir` or `-t`: The path to the target directory containing files to organize. This argument is mandatory. - `--dest-dir` or `-d`: The path to the destination directory where organized files will be moved. If this argument is not provided, the script should organize the files within the target directory itself. 2. The script should create subdirectories within the destination directory (or target directory if no destination is provided) for each file extension present in the target directory. For example, if there are `.txt` files, a `txt` subdirectory should be created. 3. The script should then move each file into the corresponding subdirectory based on its extension. 4. If the file has no extension, it should be moved to a directory named `no_extension`. 5. The script should print a summary of the number of files moved for each extension. **Constraints**: - The script should handle nested directories recursively within the target directory. - If a subdirectory for an extension already exists, the script should use it rather than creating a new one. - The script should handle errors gracefully, such as permission issues or missing directories, and print appropriate error messages. **Performance Requirements**: - The solution should efficiently handle directories with a large number of files and nested subdirectories. **Example Input and Expected Output**: Command: ```bash python file_organizer.py --target-dir /path/to/target --dest-dir /path/to/destination ``` Expected output summary: ``` txt: 10 files jpg: 5 files pdf: 3 files no_extension: 1 file ``` # Provided Skeleton Code: ```python import os import argparse import shutil def organize_files(target_dir, dest_dir): # Implement the logic to organize files here pass def main(): parser = argparse.ArgumentParser(description=\'Organize files in a directory based on their extensions.\') parser.add_argument(\'--target-dir\', \'-t\', required=True, help=\'The target directory containing files to organize.\') parser.add_argument(\'--dest-dir\', \'-d\', required=False, help=\'The destination directory where organized files will be moved.\') args = parser.parse_args() target_dir = args.target_dir dest_dir = args.dest_dir if args.dest_dir else target_dir if not os.path.exists(target_dir): print(f\\"Error: The target directory {target_dir} does not exist.\\") return if not os.path.exists(dest_dir): try: os.makedirs(dest_dir) except OSError: print(f\\"Error: Could not create the destination directory {dest_dir}.\\") return organize_files(target_dir, dest_dir) if __name__ == \'__main__\': main() ``` **Implementation Notes**: - Use `os.path` and `shutil` for handling file operations. - Ensure proper error checking and handling. - Consider edge cases such as hidden files or files without any extensions.","solution":"import os import argparse import shutil def organize_files(target_dir, dest_dir): Organizes files from target_dir into subdirectories in dest_dir based on file extensions. # Dictionary to keep the count of each file type file_counts = {} # Walk through the target directory for root, _, files in os.walk(target_dir): for file in files: file_ext = os.path.splitext(file)[1][1:] or \'no_extension\' dest_path = os.path.join(dest_dir, file_ext) if not os.path.exists(dest_path): os.makedirs(dest_path) src_file = os.path.join(root, file) dest_file = os.path.join(dest_path, file) if not os.path.exists(dest_file): # Check if file already exists in destination shutil.move(src_file, dest_file) file_counts[file_ext] = file_counts.get(file_ext, 0) + 1 # Print the summary for ext, count in file_counts.items(): print(f\\"{ext}: {count} file(s)\\") def main(): parser = argparse.ArgumentParser(description=\'Organize files in a directory based on their extensions.\') parser.add_argument(\'--target-dir\', \'-t\', required=True, help=\'The target directory containing files to organize.\') parser.add_argument(\'--dest-dir\', \'-d\', required=False, help=\'The destination directory where organized files will be moved.\') args = parser.parse_args() target_dir = args.target_dir dest_dir = args.dest_dir if args.dest_dir else target_dir if not os.path.exists(target_dir): print(f\\"Error: The target directory {target_dir} does not exist.\\") return if not os.path.exists(dest_dir): try: os.makedirs(dest_dir) except OSError: print(f\\"Error: Could not create the destination directory {dest_dir}.\\") return organize_files(target_dir, dest_dir) if __name__ == \'__main__\': main()"},{"question":"**Question** You are tasked with creating a class structure representing a set of geometric shapes and calculating their properties. The class hierarchy should include: 1. A base `Shape` class that includes common attributes and methods. 2. Derived classes for specific shapes (`Square`, `Rectangle`, `Circle`). 3. The ability to calculate the area of each shape. 4. Implementation of an iterator that allows iterating over the vertices of a shape. **Requirements** 1. **Base Class (`Shape`)**: - Constructor should initialize the name of the shape (`self.name`). - Method `calculate_area` that raises a `NotImplementedError`, as this will be implemented by derived classes. - Special method `__str__` to return a string representation of the shape (`\\"Shape: name\\"`). 2. **Derived Classes**: - **`Square`**: - Constructor should initialize the side length (`self.side_length`). - `calculate_area` method to compute the area of the square. - **`Rectangle`**: - Constructor should initialize the width and height (`self.width`, `self.height`). - `calculate_area` method to compute the area of the rectangle. - **`Circle`**: - Constructor should initialize the radius (`self.radius`). - `calculate_area` method to compute the area of the circle. 3. **Iterator Class**: - Implement a nested class `VertexIterator` inside `Shape` that allows iterating over the vertices of the shape. For simplicity, assume: - A `Square` has vertices at (0,0), (side_length, 0), (side_length, side_length), (0, side_length). - A `Rectangle` has vertices at (0,0), (width, 0), (width, height), (0, height). - A `Circle` doesn\'t have vertices, and attempting to iterate over them should raise a `ValueError`. - The `VertexIterator` class should implement the `__iter__` and `__next__` methods. **Implementation Details** - Define the classes according to the structure. - Ensure appropriate use of inheritance to avoid code duplication. - Implement the `VertexIterator` class to meet iteration requirements. - Include appropriate error handling where necessary. **Example Usage** ```python square = Square(\\"Square1\\", 4) print(square) # Output: \\"Shape: Square1\\" print(square.calculate_area()) # Output: 16 rectangle = Rectangle(\\"Rectangle1\\", 2, 3) print(rectangle) # Output: \\"Shape: Rectangle1\\" print(rectangle.calculate_area()) # Output: 6 circle = Circle(\\"Circle1\\", 5) print(circle) # Output: \\"Shape: Circle1\\" print(circle.calculate_area()) # Output: 78.53981633974483 (using π ≈ 3.14159) # Iterating over vertices of Square for vertex in square: print(vertex) # Outputs: (0, 0), (4, 0), (4, 4), (0, 4) ``` **Notes** - Use Python\'s `math` module for area calculations involving circles. - Ensure proper use of access modifiers and error handling in methods and iterators.","solution":"import math class Shape: def __init__(self, name): self.name = name def calculate_area(self): raise NotImplementedError(\\"Must be implemented by subclass\\") def __str__(self): return f\\"Shape: {self.name}\\" class VertexIterator: def __init__(self, vertices): self.vertices = vertices self.index = 0 def __iter__(self): return self def __next__(self): if self.index < len(self.vertices): result = self.vertices[self.index] self.index += 1 return result else: raise StopIteration class Square(Shape): def __init__(self, name, side_length): super().__init__(name) self.side_length = side_length def calculate_area(self): return self.side_length ** 2 def __iter__(self): vertices = [(0, 0), (self.side_length, 0), (self.side_length, self.side_length), (0, self.side_length)] return self.VertexIterator(vertices) class Rectangle(Shape): def __init__(self, name, width, height): super().__init__(name) self.width = width self.height = height def calculate_area(self): return self.width * self.height def __iter__(self): vertices = [(0, 0), (self.width, 0), (self.width, self.height), (0, self.height)] return self.VertexIterator(vertices) class Circle(Shape): def __init__(self, name, radius): super().__init__(name) self.radius = radius def calculate_area(self): return math.pi * self.radius ** 2 def __iter__(self): raise ValueError(\\"A circle does not have vertices to iterate over.\\")"},{"question":"# Custom Container Type with Cyclic Garbage Collection Support Problem Statement: You are required to create a custom container type in Python that supports cyclic garbage collection. The container will store references to other objects, and your task is to implement memory management functions, garbage collection tracking, and proper traversal and clearing of the container. Details: 1. **Container Type**: - Implement a custom container class `MyContainer` which should support storing multiple objects. - The container should support add, remove, and get operations for managing the stored objects. 2. **Garbage Collection**: - Ensure that your container class conforms to cyclic garbage collection requirements. - Implement traversal and clear handlers to support garbage collection. 3. **Memory Management**: - Properly allocate and deallocate memory using appropriate API functions. - Ensure your container is correctly tracked and untracked by the garbage collector when necessary. Input: - `my_container.add(obj)`: Adds `obj` to the container. - `my_container.remove(obj)`: Removes `obj` from the container. - `my_container.get(index)`: Retrieves the object at the specified `index` in the container. Output: - Output should demonstrate the creation of a container, addition/removal of elements, and show the internal workings of garbage collection. Constraints: - Ensure objects are valid while being tracked by the collector. - Implement `tp_traverse` and `tp_clear` handlers correctly. - Proper handling to avoid memory leaks and ensure cyclic references are resolved. Performance: - Efficient handling of memory and garbage collection operations. Example: ```python class MyContainer: def __init__(self): # Initialization logic pass def add(self, obj): # Add object to container pass def remove(self, obj): # Remove object from container pass def get(self, index): # Get object from container pass def tp_traverse(self, visit, arg): # Traversal logic for garbage collection pass def tp_clear(self): # Clear logic for garbage collection pass # Example usage: my_container = MyContainer() obj1 = SomeObject() obj2 = SomeObject() my_container.add(obj1) my_container.add(obj2) print(my_container.get(0)) # Output: obj1 my_container.remove(obj1) print(my_container.get(0)) # Output: obj2 ``` Implement the MyContainer class with proper memory management, traversal, and clearing logic to ensure it supports cyclic garbage collection. Test and demonstrate its functionality with various scenarios showing how garbage collection handles cycles.","solution":"import gc class MyContainer: def __init__(self): self.container = [] # Add the object to the garbage collector\'s tracking gc.collect() gc.set_debug(gc.DEBUG_LEAK) def add(self, obj): self.container.append(obj) # Adding an object to the container gc.collect() def remove(self, obj): self.container.remove(obj) # Removing an object should also trigger the garbage collector to check for cycles gc.collect() def get(self, index): return self.container[index] def tp_traverse(self, visit, arg): # Traverse the container for garbage collection for obj in self.container: visit(obj, arg) def tp_clear(self): # Clear the container and release references for garbage collection self.container.clear() gc.collect() # Example usage: # my_container = MyContainer() # obj1 = SomeObject() # obj2 = SomeObject() # my_container.add(obj1) # my_container.add(obj2) # print(my_container.get(0)) # Output: obj1 # my_container.remove(obj1) # print(my_container.get(0)) # Output: obj2"},{"question":"# Question: Advanced Context Management with `contextlib` Problem Statement: You are required to implement a nested context manager using the `contextlib` module in Python, which manages multiple resources efficiently. The context manager should handle two resources, logging actions on resource entry and exit. Additionally, it should ensure that if an exception occurs while managing any resource, the appropriate cleanup actions are performed. Requirements: 1. **Context Manager Structure**: - Create a class `ResourceManager` that can be used with the `with` statement. - The context manager should manage two resources, `resource_a` and `resource_b`. 2. **Logging**: - When entering a context, it should log `\\"Entering context for resource_a\\"` and `\\"Entering context for resource_b\\"`. - When exiting a context, it should log `\\"Exiting context for resource_a\\"` and `\\"Exiting context for resource_b\\"`. 3. **Exception Handling**: - If an exception occurs in `resource_a`, it should still attempt to enter and exit `resource_b`, logging `\\"Exception in resource_a\\"` and `\\"Cleaning up resource_a\\"`. - Similarly, if an exception occurs in `resource_b`, it should log `\\"Exception in resource_b\\"` and `\\"Cleaning up resource_b\\"`. 4. **Functionality**: - The resources can be represented by simple python objects or tasks. - Implement a function `use_resources()` that demonstrates the use of the `ResourceManager`. Input and Output: - There is no specific input for the `ResourceManager` class. - The `use_resources()` function will run the context manager and return a list of log messages. Constraints: - You must use the `contextlib` module to implement the context manager. Example Usage: ```python def use_resources(): # Example function utilizng the created ResourceManager logs = [] # Scenario with normal operations with ResourceManager() as rm: logs.append(rm.action()) # Perform an action that logs # Output example logs collected return logs # Test the use_resources function expected_logs = [ \\"Entering context for resource_a\\", \\"Performing action on resource_a\\", \\"Exiting context for resource_a\\", \\"Entering context for resource_b\\", \\"Performing action on resource_b\\", \\"Exiting context for resource_b\\" ] assert use_resources() == expected_logs ``` Notes: - In your implementation, ensure the ResourceManager class correctly handles entry, exit, and exception handling as per the requirements. - The `use_resources()` function should demonstrate multiple scenarios, including normal execution and exceptions.","solution":"import contextlib class ResourceManager(contextlib.ContextDecorator): def __init__(self): self.logs = [] def __enter__(self): self.logs.append(\\"Entering context for resource_a\\") try: self.action_a() except Exception as e: self.logs.append(f\\"Exception in resource_a: {e}\\") self.cleanup_a() self.logs.append(\\"Entering context for resource_b\\") try: self.action_b() except Exception as e: self.logs.append(f\\"Exception in resource_b: {e}\\") self.cleanup_b() return self def __exit__(self, exc_type, exc_val, exc_tb): self.logs.append(\\"Exiting context for resource_a\\") self.cleanup_a() self.logs.append(\\"Exiting context for resource_b\\") self.cleanup_b() def action_a(self): self.logs.append(\\"Performing action on resource_a\\") def cleanup_a(self): self.logs.append(\\"Cleaning up resource_a\\") def action_b(self): self.logs.append(\\"Performing action on resource_b\\") def cleanup_b(self): self.logs.append(\\"Cleaning up resource_b\\") def use_resources(): manager = ResourceManager() with manager as rm: pass return rm.logs"},{"question":"**Coding Challenge: Detecting Anomalies in Financial Transactions** # Objective: You are provided with a dataset containing financial transactions. Each transaction is represented by multiple features. Your task is to implement a system that detects anomalous transactions using different methods provided by scikit-learn. # Requirements: 1. Implement anomaly detection using three different methods: - `Isolation Forest` - `Local Outlier Factor` - `One-Class SVM` 2. Compare the performance of these methods and visualize the results. # Input: - A CSV file `transactions.csv` with the following columns: - `transaction_id`: Unique identifier for each transaction. - `feature_1`, `feature_2`, ..., `feature_n`: Numerical features representing the transaction details. # Output: - A CSV file `anomalies.csv` containing the following columns: - `transaction_id`: Unique identifier for each transaction. - `method`: The method used for detection (`isolation_forest`, `lof`, `one_class_svm`). - `anomaly_score`: The score indicating the degree of anomaly. - `is_anomaly`: Binary label (1 for anomaly, -1 for normal). # Steps: 1. **Data Preparation**: - Load the dataset and preprocess it (handle missing values if any, normalize the data). 2. **Model Training**: - Train three models (`Isolation Forest`, `Local Outlier Factor`, `One-Class SVM`) on the provided dataset. 3. **Anomaly Detection**: - Use each model to detect anomalies on the dataset. - For `Local Outlier Factor`, ensure to use `novelty=True` for prediction on new data. 4. **Result Compilation**: - For each method, compile the results as mentioned in the output format. - Save the compiled results to `anomalies.csv`. 5. **Visualization**: - Plot and compare the detected anomalies across the three methods. # Constraints: - Use default parameters for the methods unless specified otherwise. - Ensure your code is efficient and handles large datasets gracefully. # Example: ```python import pandas as pd from sklearn.ensemble import IsolationForest from sklearn.neighbors import LocalOutlierFactor from sklearn.svm import OneClassSVM from sklearn.preprocessing import StandardScaler # Load and preprocess dataset df = pd.read_csv(\'transactions.csv\') X = df.drop(\'transaction_id\', axis=1).values scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Initialize models models = { \'isolation_forest\': IsolationForest(contamination=0.1), \'lof\': LocalOutlierFactor(novelty=True), \'one_class_svm\': OneClassSVM(nu=0.1) } # Fit models and detect anomalies results = [] for name, model in models.items(): if name == \'lof\': model.fit(X_scaled) anomaly_scores = model.decision_function(X_scaled) labels = model.predict(X_scaled) else: model.fit(X_scaled) anomaly_scores = model.decision_function(X_scaled) labels = model.predict(X_scaled) for i, score in enumerate(anomaly_scores): results.append({ \'transaction_id\': df.iloc[i][\'transaction_id\'], \'method\': name, \'anomaly_score\': score, \'is_anomaly\': labels[i], }) # Save results to CSV results_df = pd.DataFrame(results) results_df.to_csv(\'anomalies.csv\', index=False) ``` # Submission: Submit your `anomalies.csv` file and the code used to generate it.","solution":"import pandas as pd from sklearn.ensemble import IsolationForest from sklearn.neighbors import LocalOutlierFactor from sklearn.svm import OneClassSVM from sklearn.preprocessing import StandardScaler def detect_anomalies(input_csv_path: str, output_csv_path: str) -> None: # Load and preprocess dataset df = pd.read_csv(input_csv_path) transaction_ids = df[\'transaction_id\'] X = df.drop(\'transaction_id\', axis=1).values # Normalize the data scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Initialize models models = { \'isolation_forest\': IsolationForest(contamination=0.1), \'lof\': LocalOutlierFactor(novelty=True), \'one_class_svm\': OneClassSVM(nu=0.1) } # Fit models and detect anomalies results = [] for name, model in models.items(): if name == \'lof\': model.fit(X_scaled) anomaly_scores = model.decision_function(X_scaled) labels = model.predict(X_scaled) else: model.fit(X_scaled) anomaly_scores = model.decision_function(X_scaled) labels = model.predict(X_scaled) for transaction_id, score, label in zip(transaction_ids, anomaly_scores, labels): results.append({ \'transaction_id\': transaction_id, \'method\': name, \'anomaly_score\': score, \'is_anomaly\': label }) # Save results to CSV results_df = pd.DataFrame(results) results_df.to_csv(output_csv_path, index=False)"},{"question":"Mocking and Patching with `unittest.mock` You are tasked with implementing a production function and then writing test cases that mock its dependencies using the `unittest.mock` library. This will serve to test your understanding of mocking, patching, and verifying calls. # Production Code You need to implement a function `process_data` that does the following: - Reads data from a file. - Processes the data by converting it to uppercase. - Writes the processed data to another file. Here is the function definition: ```python import os def process_data(input_file, output_file): with open(input_file, \'r\') as infile: data = infile.read() processed_data = data.upper() with open(output_file, \'w\') as outfile: outfile.write(processed_data) ``` # Requirements You need to write a set of tests for this function that: 1. **Mocks File Opening**: - Ensure that `open` is called correctly for both reading and writing. 2. **Mocks File Reading and Writing**: - Simulate reading from the input file. - Simulate writing to the output file. - Verify that reading and writing happen correctly. 3. **Mocks OS Module Functions**: - If necessary, mock `os.path.exists` to check file existence. # Constraints - You should use the `patch` decorator or context manager from `unittest.mock`. - Verify that reading and writing operations are called correctly and in the right order. - Use side effects to simulate the content of the file. Write your test cases in the form of a class using the `unittest` framework. # Example Test Class Here is a skeleton of the test class you need to implement: ```python import unittest from unittest.mock import mock_open, patch, MagicMock import os class TestProcessData(unittest.TestCase): @patch(\'builtins.open\', new_callable=mock_open, read_data=\'mock data\') def test_process_data_reads_file(self, mock_open): # Test that the input file is being read correctly @patch(\'builtins.open\', new_callable=mock_open) def test_process_data_writes_file(self, mock_open): # Test that the output file is being written correctly @patch(\'builtins.open\', new_callable=mock_open, read_data=\'mock data\') def test_process_data_transformation(self, mock_open): # Test that the data is transformed correctly and written to the output file if __name__ == \'__main__\': unittest.main() ``` > **Note**: Ensure that your tests both mock and assert calls as necessary.","solution":"import os def process_data(input_file, output_file): with open(input_file, \'r\') as infile: data = infile.read() processed_data = data.upper() with open(output_file, \'w\') as outfile: outfile.write(processed_data)"},{"question":"# Coding Assessment: Advanced Web Interaction with `urllib.request` Problem Statement: You are working on a project that involves significant backend interactions with a variety of HTTP services. To ensure efficient development, it\'s crucial to understand and correctly implement functionalities using Python\'s `urllib.request` module. Write a Python function called `fetch_url_data_with_authentication` that performs the following tasks: 1. Takes a URL `url`, a dictionary of headers `headers_dict`, and an optional dictionary of authentication details `auth_details` to perform a GET request. - `url` (str): The URL to fetch. - `headers_dict` (dict): A dictionary containing the headers to be added to the request. - `auth_details` (dict or None): A dictionary containing \'realm\', \'uri\', \'user\', and \'password\' for basic HTTP authentication. If `auth_details` is None, no authentication should be used. 2. Creates the necessary opener with or without the authentication handler based on `auth_details`. 3. Returns the response content as a string decoded using `utf-8`. 4. Handles HTTP errors appropriately and returns an appropriate error message. Function Signature: ```python def fetch_url_data_with_authentication(url: str, headers_dict: dict, auth_details: dict = None) -> str: pass ``` Constraints: - The function should raise a `ValueError` if the URL is invalid. - Use basic HTTP authentication if `auth_details` is provided. - Only use the `GET` method. Example Usage: ```python # Example URL and headers url = \'http://www.example.com\' headers = {\'Custom-Header\': \'value\'} # Without authentication response = fetch_url_data_with_authentication(url, headers) print(response) # Should print the content of the URL # With authentication auth = { \'realm\': \'Example Realm\', \'uri\': url, \'user\': \'username\', \'password\': \'password\' } response = fetch_url_data_with_authentication(url, headers, auth) print(response) # Should print the content of the URL or an authentication error if credentials are wrong ``` Notes: - Use appropriate exception handling to manage network errors. - Ensure the function is self-contained and does not rely on any global states. - You can assume valid headers and authentication details if provided.","solution":"import urllib.request import urllib.error def fetch_url_data_with_authentication(url: str, headers_dict: dict, auth_details: dict = None) -> str: if not url.startswith((\'http://\', \'https://\')): raise ValueError(\\"Invalid URL\\") try: request = urllib.request.Request(url) for header, value in headers_dict.items(): request.add_header(header, value) if auth_details: password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm() password_mgr.add_password( auth_details[\'realm\'], auth_details[\'uri\'], auth_details[\'user\'], auth_details[\'password\'] ) handler = urllib.request.HTTPBasicAuthHandler(password_mgr) opener = urllib.request.build_opener(handler) urllib.request.install_opener(opener) with urllib.request.urlopen(request) as response: return response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: return f\\"HTTP Error: {e.code} - {e.reason}\\" except urllib.error.URLError as e: return f\\"URL Error: {e.reason}\\""},{"question":"# **Coding Assessment Question** **Objective:** You need to implement a function in Python utilizing the `time` module, which provides a summary of the system\'s current time and capabilities. Your implementation should also handle timezone changes dynamically. **Function Signature:** ```python def system_time_summary(): pass ``` **Task:** 1. Implement a function `system_time_summary()` that: - Retrieves the current time in UTC and local time. - Fetches the resolution and current value of the `CLOCK_MONOTONIC` and `CLOCK_REALTIME` clocks. - Checks whether the system supports Daylight Saving Time (DST) and returns the appropriate timezone information. - Changes the timezone to a specified timezone (e.g., `US/Eastern`) and retrieves the local time in this new timezone. 2. The function should return a dictionary with the following keys and corresponding values: - `\\"utc_time\\"`: A string representing the current UTC time. - `\\"local_time\\"`: A string representing the current local time. - `\\"monotonic_clock_res\\"`: The resolution of the `CLOCK_MONOTONIC` clock. - `\\"monotonic_clock_time\\"`: The current time value of the `CLOCK_MONOTONIC` clock. - `\\"realtime_clock_res\\"`: The resolution of the `CLOCK_REALTIME` clock. - `\\"realtime_clock_time\\"`: The current time value of the `CLOCK_REALTIME` clock. - `\\"supports_dst\\"`: A boolean indicating whether the system supports DST. - `\\"current_timezone\\"`: The name of the current timezone. - `\\"new_timezone_time\\"`: The local time in the `US/Eastern` timezone after changing the timezone. **Constraints:** - The function should handle exceptions gracefully and provide meaningful error messages in case of an error. - The function should ensure correct handling of time format conversion. - You are required to use the `time` module functionalities as much as possible. **Example Output:** ```python { \\"utc_time\\": \\"Thu, 28 Oct 2021 14:17:15 +0000\\", \\"local_time\\": \\"Thu, 28 Oct 2021 10:17:15 -0400\\", \\"monotonic_clock_res\\": 1e-09, \\"monotonic_clock_time\\": 503158.123456, \\"realtime_clock_res\\": 1e-06, \\"realtime_clock_time\\": 1635424635.123456, \\"supports_dst\\": True, \\"current_timezone\\": \\"Eastern Daylight Time\\", \\"new_timezone_time\\": \\"Thu, 28 Oct 2021 10:17:15 -0400\\" } ``` **Note:** 1. You may need to import the `os` module in addition to the `time` module. 2. Ensure all time formats are consistent and human-readable, preferably using the `strftime()` function where applicable. 3. Make sure the function works across different platforms (Unix, Windows).","solution":"import time import os from datetime import datetime, timezone import pytz def system_time_summary(): try: # Retrieve current UTC and local time utc_time = datetime.now(timezone.utc).strftime(\'%a, %d %b %Y %H:%M:%S %z\') local_time = datetime.now().astimezone().strftime(\'%a, %d %b %Y %H:%M:%S %z\') # Fetch resolution and current value of CLOCK_MONOTONIC and CLOCK_REALTIME monotonic_clock_res = time.get_clock_info(\'monotonic\').resolution monotonic_clock_time = time.monotonic() realtime_clock_res = time.get_clock_info(\'time\').resolution realtime_clock_time = time.time() # Check if the system supports DST supports_dst = time.daylight > 0 current_timezone = time.tzname[time.localtime().tm_isdst] # Change timezone to US/Eastern and retrieve the local time in this new timezone os.environ[\'TZ\'] = \'US/Eastern\' time.tzset() new_timezone_time = datetime.now().strftime(\'%a, %d %b %Y %H:%M:%S %z\') # Reset the timezone back to original del os.environ[\'TZ\'] time.tzset() return { \\"utc_time\\": utc_time, \\"local_time\\": local_time, \\"monotonic_clock_res\\": monotonic_clock_res, \\"monotonic_clock_time\\": monotonic_clock_time, \\"realtime_clock_res\\": realtime_clock_res, \\"realtime_clock_time\\": realtime_clock_time, \\"supports_dst\\": supports_dst, \\"current_timezone\\": current_timezone, \\"new_timezone_time\\": new_timezone_time } except Exception as e: return {\\"error\\": str(e)}"},{"question":"Objective: Create a Python script that leverages the functionalities of the `tabnanny` module to check a list of files for indentation errors and print a summary report. Question: You are required to implement a Python function named `check_indentations(files, verbose=0)`. This function will take a list of file paths (`files`) and an optional verbosity level (`verbose`). The function should check each file for indentation errors using `tabnanny`. The output should be a summary report that includes: - The total number of files checked. - The number of files with indentation errors. - The filenames of the files with errors. - Detailed error messages if `verbose` is set to a value greater than 0. Requirements: 1. **Input:** - `files`: A list of strings, where each string is a path to a Python file. - `verbose`: An integer (default is 0). If set to 0, the summary report should be minimal. If greater than 0, the report should include detailed error messages. 2. **Output:** - The function should print the summary report to standard output. 3. **Constraints:** - You must use the `tabnanny` module to check the files. - Handle the `tabnanny.NannyNag` exception appropriately to capture and report indentation errors. - Do not modify the `tabnanny` module itself. 4. **Example Usage:** ```python files_to_check = [\'script1.py\', \'script2.py\', \'subfolder/script3.py\'] check_indentations(files_to_check, verbose=1) ``` Expected Output: ``` Total files checked: 3 Files with indentation errors: 1 Indentation errors found in: - script2.py Details of Errors: File: script2.py, Line: 15, Error: ambiguous indentation ``` Implementation Notes: - You can assume all file paths in the `files` list are valid and exist. - Use `tabnanny.verbose` and `tabnanny.filename_only` to control the verbosity of the output. - Make sure to handle the output in a clean and user-friendly manner. ```python import tabnanny def check_indentations(files, verbose=0): tabnanny.verbose = verbose tabnanny.filename_only = not verbose total_files = len(files) files_with_errors = 0 error_files = [] detailed_errors = [] for file in files: try: tabnanny.check(file) except tabnanny.NannyNag as e: files_with_errors += 1 error_files.append(file) if verbose > 0: detailed_errors.append(f\'File: {file}, {str(e)}\') print(f\'Total files checked: {total_files}\') print(f\'Files with indentation errors: {files_with_errors}n\') if files_with_errors > 0: print(\'Indentation errors found in:\') for error_file in error_files: print(f\'- {error_file}\') if verbose > 0: print(\'nDetails of Errors:\') for error_detail in detailed_errors: print(error_detail) # Example usage if __name__ == \\"__main__\\": files_to_check = [\'script1.py\', \'script2.py\', \'subfolder/script3.py\'] check_indentations(files_to_check, verbose=1) ```","solution":"import tabnanny def check_indentations(files, verbose=0): Checks a list of files for indentation errors using tabnanny. Args: files: List of strings, where each string is a path to a Python file. verbose: Integer (default is 0). If greater than 0, includes detailed error messages. tabnanny.verbose = verbose tabnanny.filename_only = not verbose total_files = len(files) files_with_errors = 0 error_files = [] detailed_errors = [] for file in files: try: tabnanny.check(file) except tabnanny.NannyNag as e: files_with_errors += 1 error_files.append(file) if verbose > 0: detailed_errors.append(f\'File: {file}, {str(e)}\') print(f\'Total files checked: {total_files}\') print(f\'Files with indentation errors: {files_with_errors}n\') if files_with_errors > 0: print(\'Indentation errors found in:\') for error_file in error_files: print(f\'- {error_file}\') if verbose > 0: print(\'nDetails of Errors:\') for error_detail in detailed_errors: print(error_detail)"},{"question":"**Objective:** Demonstrate your understanding of the `mailbox` module in Python by implementing some operations on mailboxes stored in different formats. **Problem Statement:** You are managing an email system that uses the `mailbox` module to handle various mailbox formats. Implement a function `sort_and_archive_emails(source_path, dest_path)` that processes emails in a Maildir format mailbox located at `source_path`, and sorts them into two mbox format mailboxes based on their subject. Emails containing the keyword \\"urgent\\" in the subject should be moved to `urgent.mbox` in the specified `dest_path`, while all other emails should be moved to `general.mbox` in the same destination path. Lock the source mailbox during processing to ensure safety against concurrent modifications. **Function Signatures:** ```python def sort_and_archive_emails(source_path: str, dest_path: str) -> None: pass ``` **Constraints:** - The function must handle large mailboxes efficiently. - The Maildir mailbox may have many concurrent users, so it must be locked while processing. - The destination mbox mailboxes may not exist initially and should be created if necessary. - The source Maildir mailbox should not be left in an inconsistent state even if the program is interrupted. **Expected Behavior:** 1. Lock the source Maildir mailbox to prevent concurrent modifications. 2. Iterate through all messages in the Maildir mailbox. 3. Based on the subject, move messages to the appropriate mbox destination. 4. Release the lock on the source mailbox once processing is completed. 5. Ensure all changes are flushed to disk to prevent data loss. **Example Usage:** ```python sort_and_archive_emails(\'/path/to/source/maildir\', \'/path/to/destination/\') ``` **Notes:** - Use the `mailbox.Maildir` class to handle the Maildir mailbox. - Use the `mailbox.mbox` class for the mbox destination mailboxes. - Ensure the solution is robust and handles potential errors gracefully.","solution":"import mailbox import os def sort_and_archive_emails(source_path, dest_path): Sorts emails from a Maildir format mailbox into two mbox format mailboxes based on email subject. Parameters: source_path (str): Path to the source Maildir mailbox. dest_path (str): Path to the directory where the destination mbox mailboxes will be stored. source_maildir = mailbox.Maildir(source_path, factory=None, create=False) urgent_mbox_path = os.path.join(dest_path, \'urgent.mbox\') general_mbox_path = os.path.join(dest_path, \'general.mbox\') urgent_mbox = mailbox.mbox(urgent_mbox_path) general_mbox = mailbox.mbox(general_mbox_path) # Lock source mailbox for processing source_maildir.lock() try: for message in source_maildir: subject = message[\'subject\'] if message[\'subject\'] else \'\' if \'urgent\' in subject.lower(): urgent_mbox.add(message) else: general_mbox.add(message) # Flush changes to disk urgent_mbox.flush() general_mbox.flush() finally: # Unlock the source mailbox source_maildir.unlock() # Close the mailboxes source_maildir.close() urgent_mbox.close() general_mbox.close()"},{"question":"You have been provided the \\"iris\\" dataset, which contains the following columns: `sepal_length`, `sepal_width`, `petal_length`, `petal_width`, and `species`. Your task is to create a visualization using seaborn\'s `objects` interface that effectively displays the distribution of `petal_length` for each `species`. You must: 1. Load the `iris` dataset using seaborn\'s `load_dataset` function. 2. Create a plot that: - Uses `so.Dots()` with `so.Jitter()` to show individual data points. - Adds a range bar using `so.Range()` that highlights the interquartile range (IQR) of `petal_length` for each `species`. - Shifts the range bars horizontally for better visibility. 3. Make sure the plot is clearly labeled and readable. # Constraints - You must use the seaborn `objects` interface. - The x-axis should represent `species`. - The y-axis should represent `petal_length`. - The plot should be saved as `iris_petal_length_distribution.png`. # Input Format No direct input from the user is required. Use the seaborn `load_dataset` function to load the \\"iris\\" dataset within your script. # Output Format The output should be a saved plot image file named `iris_petal_length_distribution.png`. # Example Code Structure ```python import seaborn.objects as so from seaborn import load_dataset # Load the iris dataset iris = load_dataset(\\"iris\\") # Create the plot plot = ( so.Plot(iris, x=\\"species\\", y=\\"petal_length\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2)) ) # Save the plot plot.save(\\"iris_petal_length_distribution.png\\") ``` Ensure you adhere closely to the specified requirements to successfully complete this coding assessment.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_iris_petal_length_distribution_plot(): # Load the iris dataset iris = load_dataset(\\"iris\\") # Create the plot plot = ( so.Plot(iris, x=\\"species\\", y=\\"petal_length\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.1)) ) # Save the plot plot.save(\\"iris_petal_length_distribution.png\\")"},{"question":"**Challenging Python Coding Assessment Question: Signal Handling and Timed Execution** # Problem Statement You are tasked with writing a program that monitors the execution of a potentially long-running function and gracefully handles interruptions caused by signals. The program should: 1. Execute the target function with a specified timeout. 2. Install a signal handler that catches the interruption signal and raises a custom `TimeoutError`. 3. If the function completes within the timeout, print its result. Otherwise, handle the timeout by printing an appropriate message. # Function Signature ```python import signal def execute_with_timeout(func, args, timeout): Execute a function with a timeout. If the function does not complete within the specified timeout, a TimeoutError should be raised. Args: func (callable): The target function to execute. args (tuple): Arguments to pass to the function. timeout (int): The timeout duration in seconds. Returns: any: The result of the function if it completes within the timeout. Raises: TimeoutError: If the function execution exceeds the specified timeout. # You may define additional helper functions or classes if necessary ``` # Constraints - The function should correctly handle any signal received within the timeout period and clean up resources appropriately. - The `timeout` value will always be a positive integer (>= 1). - The function to be executed (`func`) will be a blocking or potentially long-running function. - You may assume the target function does not raise any exceptions on its own. # Example ```python import time def long_running_func(seconds): time.sleep(seconds) return f\\"Completed after {seconds} seconds\\" # Should print: \\"Function timed out!\\" try: execute_with_timeout(long_running_func, (10,), 5) except TimeoutError as e: print(\\"Function timed out!\\") # Should print: \\"Completed after 3 seconds\\" try: result = execute_with_timeout(long_running_func, (3,), 5) print(result) except TimeoutError as e: print(\\"Function timed out!\\") ``` # Notes - You will need to familiarize yourself with the `signal` module for setting up custom signal handlers. - Make sure to reset any state or signal configurations to avoid unintended side effects in a real-world scenario.","solution":"import signal import time class TimeoutError(Exception): pass def handler(signum, frame): raise TimeoutError def execute_with_timeout(func, args, timeout): Execute a function with a timeout. If the function does not complete within the specified timeout, a TimeoutError should be raised. Args: func (callable): The target function to execute. args (tuple): Arguments to pass to the function. timeout (int): The timeout duration in seconds. Returns: any: The result of the function if it completes within the timeout. Raises: TimeoutError: If the function execution exceeds the specified timeout. # Set the signal handler for SIGALRM original_handler = signal.signal(signal.SIGALRM, handler) signal.setitimer(signal.ITIMER_REAL, timeout) try: result = func(*args) except TimeoutError: # Resetting the timer and signal handler signal.signal(signal.SIGALRM, original_handler) signal.setitimer(signal.ITIMER_REAL, 0) raise TimeoutError(\\"Function timed out!\\") else: # Resetting the timer and signal handler signal.signal(signal.SIGALRM, original_handler) signal.setitimer(signal.ITIMER_REAL, 0) return result"},{"question":"<|Analysis Begin|> The provided documentation gives detailed insights regarding the capabilities of the `xml.etree.ElementTree` module in Python. This module facilitates the parsing, creation, and manipulation of XML data. Key functionalities include: 1. Parsing XML documents from files or strings. 2. Iterating through elements and sub-elements of an XML tree. 3. Generating XML string representations and writing them to files. 4. Incremental parsing techniques for non-blocking applications using `XMLParser` and `XMLPullParser`. 5. Modifying XML data, such as adding, updating, or removing elements and attributes. 6. Supporting XPath expressions for locating elements within an XML tree. Given the breadth of the module, a challenging question would focus on parsing an XML document and performing multiple manipulations and queries that demonstrate advanced usage of the module. <|Analysis End|> <|Question Begin|> **Coding Assessment Question: XML Manipulation with ElementTree** **Context:** You are provided with an XML document containing information about various countries, their ranks, GDPs, neighboring countries, etc. You will be required to parse this XML, manipulate its contents, and finally generate an updated XML document. Here is the sample XML data: ```xml <?xml version=\\"1.0\\"?> <data> <country name=\\"Liechtenstein\\"> <rank>1</rank> <year>2008</year> <gdppc>141100</gdppc> <neighbor name=\\"Austria\\" direction=\\"E\\"/> <neighbor name=\\"Switzerland\\" direction=\\"W\\"/> </country> <country name=\\"Singapore\\"> <rank>4</rank> <year>2011</year> <gdppc>59900</gdppc> <neighbor name=\\"Malaysia\\" direction=\\"N\\"/> </country> <country name=\\"Panama\\"> <rank>68</rank> <year>2011</year> <gdppc>13600</gdppc> <neighbor name=\\"Costa Rica\\" direction=\\"W\\"/> <neighbor name=\\"Colombia\\" direction=\\"E\\"/> </country> </data> ``` **Tasks:** 1. **Parse the XML:** - Write a function `parse_xml(xml_content: str) -> ElementTree.Element` that parses the given XML content string and returns the root element of the XML tree. 2. **Find and Update Specific Elements:** - Write a function `update_elements(root: ElementTree.Element) -> ElementTree.Element` that updates each country\'s rank by adding 1 to the current value and adds an attribute `updated=\\"yes\\"` to the rank element. 3. **Remove Elements Based on Conditions:** - Write a function `remove_high_ranked_countries(root: ElementTree.Element, threshold: int) -> ElementTree.Element` that removes all countries with a rank higher than the given threshold. 4. **Generate a New XML String:** - Write a function `generate_xml_string(root: ElementTree.Element, pretty: bool = False) -> str` that takes the root element and returns the XML content as a string. If `pretty` is `True`, the XML should be pretty-printed (indented for readability). 5. **Namespace Handling:** - Assume you have the following XML content with namespaces: ```xml <?xml version=\\"1.0\\"?> <actors xmlns:fictional=\\"http://characters.example.com\\" xmlns=\\"http://people.example.com\\"> <actor> <name>John Cleese</name> <fictional:character>Lancelot</fictional:character> <fictional:character>Archie Leach</fictional:character> </actor> <actor> <name>Eric Idle</name> <fictional:character>Sir Robin</fictional:character> <fictional:character>Gunther</fictional:character> <fictional:character>Commander Clement</fictional:character> </actor> </actors> ``` - Write a function `parse_xml_with_namespaces(xml_content: str) -> dict` that parses this XML and retrieves each actor\'s name along with the list of characters they played. The result should be a dictionary with actor names as keys and lists of characters as values. **Constraints:** - You are not allowed to use any other third-party XML parsing libraries. You must use the `xml.etree.ElementTree` module. - Ensure your code is robust and handles potential edge cases (e.g., missing elements). **Expected Input and Output:** 1. `parse_xml`: - **Input:** XML content as a string. - **Output:** Root element of the parsed XML. 2. `update_elements`: - **Input:** Root element of the XML tree. - **Output:** Root element with updated rank values. 3. `remove_high_ranked_countries`: - **Input:** Root element and rank threshold. - **Output:** Root element with high-ranked countries removed. 4. `generate_xml_string`: - **Input:** Root element and pretty flag. - **Output:** XML content as a string. 5. `parse_xml_with_namespaces`: - **Input:** XML content as a string. - **Output:** Dictionary with actor names and their characters list. Use the provided sample XML data to test your functions and ensure they work as expected.","solution":"import xml.etree.ElementTree as ET from xml.dom import minidom def parse_xml(xml_content: str) -> ET.Element: Parses the given XML content string and returns the root element of the XML tree. root = ET.fromstring(xml_content) return root def update_elements(root: ET.Element) -> ET.Element: Updates each country\'s rank by adding 1 to the current value and adds an attribute `updated=\\"yes\\"` to the rank element. for country in root.findall(\'country\'): rank_element = country.find(\'rank\') if rank_element is not None and rank_element.text.isdigit(): new_rank = int(rank_element.text) + 1 rank_element.text = str(new_rank) rank_element.set(\'updated\', \'yes\') return root def remove_high_ranked_countries(root: ET.Element, threshold: int) -> ET.Element: Removes all countries with a rank higher than the given threshold. for country in list(root.findall(\'country\')): rank_element = country.find(\'rank\') if rank_element is not None and rank_element.text.isdigit(): rank = int(rank_element.text) if rank > threshold: root.remove(country) return root def generate_xml_string(root: ET.Element, pretty: bool = False) -> str: Returns the XML content as a string. If `pretty` is `True`, the XML should be pretty-printed. xml_str = ET.tostring(root, encoding=\'unicode\') if pretty: xml_str = minidom.parseString(xml_str).toprettyxml(indent=\\" \\") return xml_str def parse_xml_with_namespaces(xml_content: str) -> dict: Parses the XML with namespaces and retrieves each actor\'s name along with the list of characters they played. root = ET.fromstring(xml_content) ns = { \'fictional\': \'http://characters.example.com\', \'\': \'http://people.example.com\' } actors_dict = {} for actor in root.findall(\'actor\', ns): name = actor.find(\'name\', ns).text characters = [char.text for char in actor.findall(\'fictional:character\', ns)] actors_dict[name] = characters return actors_dict"},{"question":"**Queue Management System** You will create a Queue Management System that processes tasks of different priorities using asyncio. Your goal is to simulate a real-world scenario where tasks with higher priority should be processed before others. This system will: 1. Use `asyncio.PriorityQueue` to manage high-priority tasks. 2. Use `asyncio.Queue` to manage standard tasks. 3. Use `asyncio.LifoQueue` to manage tasks that should be processed in reverse order of their arrival. # Input: - An integer `n` representing the number of standard tasks. - An integer `m` representing the number of high-priority tasks. - An integer `p` representing the number of LIFO tasks. - Three lists of tuples: - `standard_tasks` of length `n`, where each tuple contains `(task_id, sleep_time)`. - `priority_tasks` of length `m`, where each tuple contains `(priority, task_id, sleep_time)`. - `lifo_tasks` of length `p`, where each tuple contains `(task_id, sleep_time)`. Each task should be simulated by an async function that sleeps for the given `sleep_time`. # Output: - Print a message `Completed task {task_id}` each time a task is completed. - Print `All tasks have been processed.` at the end once all queues are empty. # Constraints: - The function should run concurrently. - Ensure efficient processing using async/await to minimize blocking. # Function Signature: ```python import asyncio async def manage_queues(n: int, m: int, p: int, standard_tasks: list, priority_tasks: list, lifo_tasks: list): pass ``` # Example: ```python import asyncio standard_tasks = [(1, 0.2), (2, 0.1)] priority_tasks = [(1, 3, 0.3), (0, 4, 0.1)] lifo_tasks = [(5, 0.15), (6, 0.25)] asyncio.run(manage_queues(2, 2, 2, standard_tasks, priority_tasks, lifo_tasks)) ``` Expected Output: ``` Completed task 4 Completed task 3 Completed task 1 Completed task 2 Completed task 6 Completed task 5 All tasks have been processed. ``` # Hints: - Use `await queue.put(item)` to add tasks. - Use `await queue.get()` to retrieve tasks. - Use `queue.task_done()` to indicate task completion. - Prioritize processing high-priority tasks first, followed by standard tasks, and then LIFO tasks. - Handle each queue concurrently within the same manage_queues function.","solution":"import asyncio async def process_task(task_id, sleep_time): await asyncio.sleep(sleep_time) print(f\'Completed task {task_id}\') async def manage_queues(n: int, m: int, p: int, standard_tasks: list, priority_tasks: list, lifo_tasks: list): priority_queue = asyncio.PriorityQueue() standard_queue = asyncio.Queue() lifo_queue = asyncio.LifoQueue() for priority, task_id, sleep_time in priority_tasks: await priority_queue.put((priority, task_id, sleep_time)) for task_id, sleep_time in standard_tasks: await standard_queue.put((task_id, sleep_time)) for task_id, sleep_time in lifo_tasks: await lifo_queue.put((task_id, sleep_time)) async def process_queues(): while not priority_queue.empty(): priority, task_id, sleep_time = await priority_queue.get() await process_task(task_id, sleep_time) priority_queue.task_done() while not standard_queue.empty(): task_id, sleep_time = await standard_queue.get() await process_task(task_id, sleep_time) standard_queue.task_done() while not lifo_queue.empty(): task_id, sleep_time = await lifo_queue.get() await process_task(task_id, sleep_time) lifo_queue.task_done() await asyncio.gather(process_queues(), process_queues(), process_queues()) print(\'All tasks have been processed.\')"},{"question":"# Future Feature Checker Python includes a special module called `__future__` which allows users to import features from future versions of the language that are not yet mandatory in the current version. This helps in writing forward-compatible code. Each feature in the `__future__` module has an optional and mandatory release version. **Your task is to write a function that checks if a specific feature from the `__future__` module is available in the given Python release version.** Implement the function `is_feature_available(feature_name: str, version: tuple) -> bool` where: - `feature_name` is a string representing the name of the feature (e.g., `\\"nested_scopes\\"`). - `version` is a tuple representing the Python version in the format `(major, minor, micro, releaselevel, serial)`. The function should return `True` if the feature is available in the given version (i.e., the given version is greater than or equal to the optional release of the feature). Otherwise, it should return `False`. # Input - A string `feature_name`. - A tuple `version` in the format `(major, minor, micro, releaselevel, serial)`. # Output - A boolean indicating if the feature is available. # Constraints - All Python versions mentioned in the question follow the tuple format `(major, minor, micro, releaselevel, serial)` where: - `major`, `minor`, and `micro` are integers. - `releaselevel` is a string that can be `\\"alpha\\"`, `\\"beta\\"`, `\\"candidate\\"`, or `\\"final\\"`. - `serial` is an integer. - The `feature_name` will always be one of the valid features listed in the documentation. # Example ```python # Example 1: print(is_feature_available(\\"generators\\", (2, 2, 0, \\"alpha\\", 1))) # True # Example 2: print(is_feature_available(\\"with_statement\\", (2, 4, 1, \\"final\\", 0))) # False ``` # Note - Reference the provided documentation to determine the optional release versions for the different features. - You may assume that the version comparison logic correctly handles the different release levels in the tuple.","solution":"import __future__ def is_feature_available(feature_name: str, version: tuple) -> bool: Checks if a specific feature from the `__future__` module is available in the given Python release version. Args: - feature_name (str): The name of the feature. - version (tuple): The version tuple in the format (major, minor, micro, releaselevel, serial). Returns: - bool: True if the feature is available in the given version; otherwise False. if not hasattr(__future__, feature_name): return False feature = getattr(__future__, feature_name) min_version = feature.optional # Compare only major and minor versions for simplicity. if version[:2] >= min_version[:2]: return True return False"},{"question":"**Objective**: Demonstrate your understanding of generating and customizing heatmaps using the seaborn library. **Problem Statement**: You are given a dataset with information about different car models, including attributes such as horsepower, weight, acceleration, and miles per gallon (mpg). Your task is to generate a heatmap to visualize various aspects of this dataset and customize it using seaborn. **Input**: A CSV file named `cars.csv` containing the following columns: - `Model`: The name of the car model (string). - `Horsepower`: The horsepower of the car (float). - `Weight`: The weight of the car (float). - `Acceleration`: The acceleration value of the car (float). - `MPG`: The miles per gallon of the car (float). **Expected Output**: 1. Generate a correlation matrix to identify relationships between `Horsepower`, `Weight`, `Acceleration`, and `MPG`. 2. Create a heatmap using the correlation matrix. 3. Customize the heatmap with the following requirements: - Add annotations showing the correlation values. - Format the annotations to one decimal place. - Apply a colormap of your choice to enhance the visualization. - Add grid lines between cells. - Set the title of the heatmap to \\"Car Attributes Correlation Heatmap\\". - Adjust the axes labels for readability. **Constraints**: - The solution should be implemented in Python. - You must use the seaborn library for visualization. - Ensure the final plot is clear, well-labeled, and polished. **Performance Requirements**: - The code should be efficient and should run within a reasonable time for the provided dataset. **Function Signature**: ```python def create_custom_heatmap(file_path: str) -> None: This function takes the path to a CSV file containing car data and generates a custom heatmap displaying the correlation between different car attributes. :param file_path: Path to the CSV file as a string. :return: None. The function should display the heatmap. pass ``` **Example**: ```python # Assuming the CSV file \'cars.csv\' has the following structure: # Model,Horsepower,Weight,Acceleration,MPG # Ford,100,3500,15,20 # Toyota,90,3000,13,22 # ... create_custom_heatmap(\'path/to/cars.csv\') ``` The function should read the CSV file, compute the correlation matrix, and produce a heatmap as specified in the output format.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_heatmap(file_path: str) -> None: This function takes the path to a CSV file containing car data and generates a custom heatmap displaying the correlation between different car attributes. :param file_path: Path to the CSV file as a string. :return: None. The function should display the heatmap. # Read the CSV file into a DataFrame df = pd.read_csv(file_path) # Select the relevant columns for correlation relevant_columns = [\'Horsepower\', \'Weight\', \'Acceleration\', \'MPG\'] car_data = df[relevant_columns] # Calculate the correlation matrix correlation_matrix = car_data.corr() # Create the heatmap with customizations plt.figure(figsize=(10, 8)) heatmap = sns.heatmap( correlation_matrix, annot=True, fmt=\\".1f\\", cmap=\\"YlGnBu\\", linewidths=.5, linecolor=\'lightgray\' ) # Set the title and adjust axis labels heatmap.set_title(\'Car Attributes Correlation Heatmap\', fontsize=15) plt.xticks(rotation=45, ha=\'right\') plt.yticks(rotation=0) # Display the heatmap plt.show()"},{"question":"You are given a dataset containing sales data for different products over several months. Your task is to create and compare two line plots that show the sales trends using different plotting contexts in Seaborn. **Input:** - A dataset containing the sales data with columns for `Product`, `Month`, and `Sales`. - Two predefined context settings as string inputs (e.g., \\"paper\\", \\"talk\\", \\"poster\\"). **Output:** - Two line plots showing the sales trends for the products using the specified context settings. **Function Signature:** ```python def compare_sales_trends(data: pd.DataFrame, context1: str, context2: str) -> None: # Your code here ``` **Constraints:** - The dataset may contain multiple products and multiple months. - Each context setting should be applied correctly to the respective plot. - Reset the plotting context to default after creating each plot. **Example:** Given the dataset: ``` Product | Month | Sales --------|-------|------ A | Jan | 100 A | Feb | 150 A | Mar | 200 B | Jan | 300 B | Feb | 350 B | Mar | 400 ``` and context settings \\"paper\\" and \\"talk\\", the function should generate two line plots with different scaling of plot elements, using \\"paper\\" for the first plot and \\"talk\\" for the second plot. **Hint:** - Use `sns.lineplot` to create the line plots. - Use `sns.plotting_context` as a context manager to apply and revert context settings.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def compare_sales_trends(data: pd.DataFrame, context1: str, context2: str) -> None: Create and compare two line plots showing sales trends using different seaborn plotting contexts. Parameters: data (pd.DataFrame): The sales data with columns for `Product`, `Month`, and `Sales`. context1 (str): The seaborn context setting for the first plot. context2 (str): The seaborn context setting for the second plot. # Create the first plot using the first context setting with sns.plotting_context(context1): plt.figure(figsize=(10, 6)) sns.lineplot(x=\'Month\', y=\'Sales\', hue=\'Product\', data=data, marker=\'o\') plt.title(f\'Sales Trends (Context: {context1})\') plt.show() # Create the second plot using the second context setting with sns.plotting_context(context2): plt.figure(figsize=(10, 6)) sns.lineplot(x=\'Month\', y=\'Sales\', hue=\'Product\', data=data, marker=\'o\') plt.title(f\'Sales Trends (Context: {context2})\') plt.show()"},{"question":"# Python Coding Assessment Question Objective: Demonstrate your understanding of the Python `uuid` module by implementing a function that generates and processes UUIDs based on different specifications. Problem Statement: Implement a function `generate_and_process_uuids` that: 1. Generates a UUID based on the host ID and current time. 2. Generates a random UUID. 3. Generates a UUID using the MD5 hash of a given namespace and name. 4. Generates a UUID using the SHA-1 hash of a given namespace and name. 5. Converts each of the above-generated UUIDs to its 32-character hexadecimal string representation and stores them in a list. 6. Returns the list of hexadecimal string representations of the generated UUIDs. Function Signature: ```python import uuid from typing import List def generate_and_process_uuids(namespace: uuid.UUID, name: str) -> List[str]: pass ``` Input: - `namespace`: A UUID object representing the namespace. - `name`: A string representing the name used for generating UUIDs based on hashes. Output: - A list of four strings, each representing a UUID in its 32-character hexadecimal form. Example: ```python import uuid namespace = uuid.uuid4() name = \\"example\\" result = generate_and_process_uuids(namespace, name) print(result) # Output (example, as UUIDs will be different each time): # [\'a unique UUID string based on host ID and current time\', # \'a random unique UUID string\', # \'a unique UUID string based on MD5 hash of namespace and name\', # \'a unique UUID string based on SHA-1 hash of namespace and name\'] ``` Constraints: - Ensure proper usage of the `uuid` module functions. - Validate the input `namespace` to be a valid UUID object before proceeding. Notes: - Utilize the `uuid1()`, `uuid3()`, `uuid4()`, and `uuid5()` functions from the `uuid` module. - Use the `hex` attribute of the generated UUID objects to convert them to hexadecimal strings.","solution":"import uuid from typing import List def generate_and_process_uuids(namespace: uuid.UUID, name: str) -> List[str]: if not isinstance(namespace, uuid.UUID): raise ValueError(\\"The namespace must be a valid UUID object\\") uuid1 = uuid.uuid1() uuid4 = uuid.uuid4() uuid3 = uuid.uuid3(namespace, name) uuid5 = uuid.uuid5(namespace, name) hex_list = [uuid1.hex, uuid4.hex, uuid3.hex, uuid5.hex] return hex_list"},{"question":"**Question:** You are tasked with developing a file processing application that needs to handle a variety of different file types, determining their MIME types and appropriate file extensions. To achieve this, you need to utilize the functionalities provided by the `mimetypes` module in Python. # Part 1: MIME Type and Extension Guessing 1. **Function**: `get_mime_type_and_encoding(file_path:str) -> Tuple[Optional[str], Optional[str]]` Implement a function that takes a file path (or URL) as an input and returns the MIME type and encoding as a tuple. If the MIME type or encoding cannot be determined, return `None` for those values. ```python def get_mime_type_and_encoding(file_path: str) -> Tuple[Optional[str], Optional[str]]: Given a file path or URL, return the guessed MIME type and encoding. Args: file_path (str): The path or URL of the file. Returns: Tuple[Optional[str], Optional[str]]: A tuple where the first element is the MIME type and the second element is the encoding. If either cannot be determined, it should be None. # Your implementation here ``` 2. **Function**: `get_all_extensions(mime_type: str) -> List[str]` Implement a function that takes a MIME type as an input and returns a list of all possible file extensions for that MIME type. If no extensions can be determined, return an empty list. ```python def get_all_extensions(mime_type: str) -> List[str]: Given a MIME type, return all possible file extensions. Args: mime_type (str): The MIME type to lookup extensions for. Returns: List[str]: A list of file extensions that correspond to the given MIME type. If no extensions can be determined, return an empty list. # Your implementation here ``` # Part 2: Custom MIME Type Mappings 3. **Function**: `add_custom_mime_type(mime_type: str, extension: str, strict: bool = True) -> bool` Implement a function that adds a custom MIME type to extension mapping. If the extension is already known, the new MIME type should replace the old one. Return `True` if the mapping was added successfully, `False` otherwise. ```python def add_custom_mime_type(mime_type: str, extension: str, strict: bool = True) -> bool: Add a custom MIME type to extension mapping. Args: mime_type (str): The MIME type to add. extension (str): The file extension to map to the MIME type. strict (bool): Whether to add the mapping as a standard MIME type. Returns: bool: True if the mapping was added successfully, False otherwise. # Your implementation here ``` # Constraints: - Ensure to handle cases where inputs might be invalid or could not be processed. - Use appropriate steps to initialize the mimetypes module if required. - Incorporate error handling and edge case management. # Example: ```python # Example execution print(get_mime_type_and_encoding(\\"example.png\\")) # Should potentially return (\\"image/png\\", None) print(get_all_extensions(\\"image/png\\")) # Should potentially return [\\".png\\"] print(add_custom_mime_type(\\"application/x-custom\\", \\".custom\\", strict=False)) # Should return True ``` Make sure to thoroughly test your implementations with different file types and MIME type strings.","solution":"import mimetypes from typing import Optional, Tuple, List def get_mime_type_and_encoding(file_path: str) -> Tuple[Optional[str], Optional[str]]: Given a file path or URL, return the guessed MIME type and encoding. mime_type, encoding = mimetypes.guess_type(file_path) return mime_type, encoding def get_all_extensions(mime_type: str) -> List[str]: Given a MIME type, return all possible file extensions. extensions = mimetypes.guess_all_extensions(mime_type) return extensions def add_custom_mime_type(mime_type: str, extension: str, strict: bool = True) -> bool: Add a custom MIME type to extension mapping. try: mimetypes.add_type(mime_type, extension, strict) return True except Exception as e: return False"},{"question":"# AU File Processing and Manipulation Objective: Write a Python function `process_au_file(input_file: str, output_file: str, new_sample_rate: int) -> None` which reads an AU file, changes its sample rate, and writes the modified audio data to a new AU file. Function Signature: ```python def process_au_file(input_file: str, output_file: str, new_sample_rate: int) -> None: ``` Inputs: - `input_file` (str): The path to the input AU file. - `output_file` (str): The path where the modified AU file should be saved. - `new_sample_rate` (int): The new sample rate to be set for the output AU file. Outputs: - The function should not return any value, but should create a new AU file with the specified sample rate at the location provided by `output_file`. Constraints: 1. You may assume that the input file is a valid AU file with supported encoding. 2. The function should handle errors gracefully, such as the inability to read from or write to a file. 3. The sample width and number of channels should remain the same as the input file. 4. The function should preserve the data integrity and avoid re-encoding audio data unless necessary. Example Usage: ```python process_au_file(\\"input.au\\", \\"output.au\\", 44100) ``` Detailed Steps: 1. Open the input AU file in read mode. 2. Extract the audio parameters such as sample width, number of channels, and frame rate from the input AU file. 3. Adjust the sample rate to match the provided `new_sample_rate`. 4. Read the audio frames from the input AU file. 5. Open a new AU file in write mode. 6. Set the audio parameters for the output AU file, including the new sample rate. 7. Write the audio frames to the output AU file. 8. Close both the input and output AU files. Hints: - Use `sunau.open()` to open the AU files. - Utilize methods like `getparams()` to retrieve and `setparams()` to set audio parameters. - Use `readframes()` to read the frames and `writeframes()` to write the frames. This question will test your knowledge of file I/O operations, handling audio data, and understanding of the \\"sunau\\" package\'s methods.","solution":"import sunau def process_au_file(input_file: str, output_file: str, new_sample_rate: int) -> None: try: # Open the input AU file with sunau.open(input_file, \'rb\') as infile: sample_width = infile.getsampwidth() num_channels = infile.getnchannels() original_sample_rate = infile.getframerate() n_frames = infile.getnframes() audio_frames = infile.readframes(n_frames) # Open the output AU file with new sample rate with sunau.open(output_file, \'wb\') as outfile: params = (num_channels, sample_width, new_sample_rate, n_frames, infile.getcomptype(), infile.getcompname()) outfile.setparams(params) outfile.writeframes(audio_frames) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Question: In this assignment, you are required to implement functions to handle file compression and decompression using the `gzip` module. Your implementation will simulate a small-scale data pipeline that involves compressing a file, then decompressing it, and verifying the integrity of the decompressed data. Task: 1. **Function 1**: Implement a function `compress_file(input_filepath: str, output_filepath: str, compresslevel: int = 9) -> None` that: - Takes an input file path, compresses the file data using the `gzip` module, and writes the compressed data to the specified output file path. - Uses the `compresslevel` parameter to determine the level of compression (1 to 9, with 9 being the default highest compression). 2. **Function 2**: Implement a function `decompress_file(input_filepath: str, output_filepath: str) -> None` that: - Takes an input file path, decompresses the gzip file data using the `gzip` module, and writes the decompressed data to the specified output file path. 3. **Function 3**: Implement a function `verify_integrity(original_filepath: str, decompressed_filepath: str) -> bool` that: - Reads the original file and the decompressed file. - Compares the contents of both files to verify that they are identical. - Returns `True` if the contents match, and `False` otherwise. Expected Input and Output: 1. **compress_file**: - Input: - `input_filepath` (str): Path to the input file to be compressed. - `output_filepath` (str): Path to save the compressed file. - `compresslevel` (int, optional): Compression level (1 to 9). - Output: None 2. **decompress_file**: - Input: - `input_filepath` (str): Path to the compressed file to be decompressed. - `output_filepath` (str): Path to save the decompressed file. - Output: None 3. **verify_integrity**: - Input: - `original_filepath` (str): Path to the original uncompressed file. - `decompressed_filepath` (str): Path to the file obtained after decompression. - Output: - Returns `True` if the contents of both files are identical. - Returns `False` otherwise. Constraints: - You can assume that the file paths provided will be valid. - The files to be processed will be small enough to fit into memory. Example: ```python # Assume the content of \'original.txt\' is \\"Hello, this is a test file.\\" compress_file(\'original.txt\', \'compressed.txt.gz\') decompress_file(\'compressed.txt.gz\', \'decompressed.txt\') result = verify_integrity(\'original.txt\', \'decompressed.txt\') print(result) # This should output: True ``` # Notes: - Use exception handling to manage any potential errors during file operations. - Ensure the implementation is efficient and readable.","solution":"import gzip import shutil def compress_file(input_filepath: str, output_filepath: str, compresslevel: int = 9) -> None: Compresses a file using gzip compression. Parameters: input_filepath (str): Path to the input file to be compressed. output_filepath (str): Path to save the compressed file. compresslevel (int, optional): Compression level (1 to 9). with open(input_filepath, \'rb\') as f_in: with gzip.open(output_filepath, \'wb\', compresslevel=compresslevel) as f_out: shutil.copyfileobj(f_in, f_out) def decompress_file(input_filepath: str, output_filepath: str) -> None: Decompresses a gzip file. Parameters: input_filepath (str): Path to the compressed file to be decompressed. output_filepath (str): Path to save the decompressed file. with gzip.open(input_filepath, \'rb\') as f_in: with open(output_filepath, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) def verify_integrity(original_filepath: str, decompressed_filepath: str) -> bool: Verifies that the contents of the original file and the decompressed file are identical. Parameters: original_filepath (str): Path to the original uncompressed file. decompressed_filepath (str): Path to the file obtained after decompression. Returns: bool: True if the contents of both files are identical, False otherwise. with open(original_filepath, \'rb\') as f1, open(decompressed_filepath, \'rb\') as f2: return f1.read() == f2.read()"},{"question":"You are provided with two datasets, \\"penguins\\" and \\"flights,\\" which can be loaded using seaborn. Your task is to create a set of visualizations using seaborn\'s `pointplot` function. The goal is to analyze and present the data by leveraging multiple features of seaborn\'s plotting utilities. **Objective:** 1. Load the \\"penguins\\" dataset using seaborn. 2. Create a plot to display the average body mass of penguins for each island, differentiated by sex. Use different markers and linestyles for each sex. Ensure that the error bars represent the standard deviation. 3. Customize the plot appearance to have a grey color for markers and set the marker to a diamond shape (`D`). 4. Create another plot to show the average bill depth for each species, differentiated by sex. Use confidence intervals as error bars and dodge the artists along the categorical axis to reduce overplotting. 5. Load the \\"flights\\" dataset using seaborn. 6. Pivot the \\"flights\\" dataset to have the years as index and months as columns. 7. Create a plot to show the trend of passengers over the years for the month of June. Ensure that the x-axis retains its original scale and mark the year 1955 with a red star. **Constraints:** - Use seaborn for all visualizations. - The visualizations should be clear and should leverage seaborn’s customization options to enhance readability and informativeness. **Input Format:** The function `create_visualizations()` with no parameters will be defined by you to load the datasets, create, and display the required visualizations. **Expected Output:** The output will not be returned from the function but displayed using seaborn\'s plotting tools. **Function Signature:** ```python def create_visualizations(): # Your code here ``` **Example:** While the student\'s actual work involves creating the plots, ensure that these plots illustrate the data as described in the task.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_visualizations(): # Load the \\"penguins\\" dataset penguins = sns.load_dataset(\\"penguins\\") # Plot avg body mass of penguins for each island, differentiated by sex plt.figure(figsize=(10, 6)) sns.pointplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", markers=[\\"o\\", \\"x\\"], linestyles=[\\"-\\", \\"--\\"], ci=\'sd\', palette=\\"gray\\", dodge=True) plt.title(\\"Average Body Mass of Penguins by Island and Sex\\") plt.xlabel(\\"Island\\") plt.ylabel(\\"Body Mass (g)\\") plt.show() # Plot avg bill depth of penguins for each species, differentiated by sex plt.figure(figsize=(10, 6)) sns.pointplot(data=penguins, x=\\"species\\", y=\\"bill_depth_mm\\", hue=\\"sex\\", join=True, dodge=True, ci=\\"sd\\") plt.title(\\"Average Bill Depth of Penguins by Species and Sex\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Bill Depth (mm)\\") plt.show() # Load the \\"flights\\" dataset flights = sns.load_dataset(\\"flights\\") # Pivot flights data for easier plotting flights_pivot = flights.pivot(\\"year\\", \\"month\\", \\"passengers\\") # Plot trend of passengers over years for the month of June plt.figure(figsize=(12, 6)) sns.pointplot(data=flights, x=\'year\', y=\'passengers\', hue=\'month\', markers=[\\"D\\"], linestyles=[\\"-\\"], palette=\\"gray\\", ) plt.scatter(1955, flights_pivot.loc[1955, \'June\'], color=\'red\', s=100, zorder=5) # Highlight June in a distinct color sns.lineplot(data=flights[flights[\\"month\\"] == \\"June\\"], x=\\"year\\", y=\\"passengers\\", marker=\\"o\\", color=\\"blue\\", label=\\"June\\") plt.title(\\"Trends of Passengers Over the Years\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Passengers\\") plt.legend(title=\'Month\') plt.show()"},{"question":"# Problem: Type-Safe Inventory Management You are tasked to create an inventory management system for a library using the `typing` module in Python. The system should be type-safe, ensuring that the types of the items in the inventory are checked during static type checking. Requirements: 1. **Custom Types**: Define custom types for `Book` and `DVD` using `NamedTuple`. Each type should have attributes relevant to the item. 2. **Generic Inventory**: Create a generic class `Inventory` that can manage items of any type. The class should support adding items and retrieving the current list of items. 3. **Callable for Processing**: Define a function type that processes items. This function should accept an item, perform some operation, and return a result. 4. **Protocol for Structural Subtyping**: Define a protocol `LibraryItem` that both `Book` and `DVD` can adhere to, ensuring they have a common method to display their details. 5. **Type Alias for Result**: Define a type alias for the result of processing an item, using `Union` to allow for different result types. Input and Output Formats - **Input**: There is no direct input for this problem as you are required to implement classes and functions as specified. - **Output**: There is no direct output, but your implementation should follow the type hinting requirements and should be compatible with static type checkers like `mypy`. Constraints - Each item type must have a method `display` that returns a string representation of the item’s details. - The `Inventory` class should be able to store any type of item that adheres to the `LibraryItem` protocol. - The processing function should have a signature as follows: `(item: T) -> Result`, where `T` is a generic type variable and `Result` is the type alias. Example Usage Here is an example demonstrating the intended usage and type checking: ```python from typing import NamedTuple, Protocol, TypeVar, Generic, List, Callable, Union # 1. Define custom types class Book(NamedTuple): title: str author: str pages: int class DVD(NamedTuple): title: str director: str duration: int # 2. Define Protocol class LibraryItem(Protocol): def display(self) -> str: ... # 3. Implement custom types adhering to the protocol class Book(NamedTuple): title: str author: str pages: int def display(self) -> str: return f\\"Book: {self.title} by {self.author}, {self.pages} pages\\" class DVD(NamedTuple): title: str director: str duration: int def display(self) -> str: return f\\"DVD: {self.title} directed by {self.director}, {self.duration} minutes\\" # 4. Define type alias for Result Result = Union[str, int] # 5. Define Inventory class T = TypeVar(\'T\', bound=LibraryItem) class Inventory(Generic[T]): def __init__(self): self.items: List[T] = [] def add_item(self, item: T) -> None: self.items.append(item) def get_items(self) -> List[T]: return self.items # 6. Define a processing function def process_item(item: T) -> Result: return len(item.display()) # Example usage book_inventory = Inventory[Book]() book_inventory.add_item(Book(\\"1984\\", \\"George Orwell\\", 328)) dvd_inventory = Inventory[DVD]() dvd_inventory.add_item(DVD(\\"Inception\\", \\"Christopher Nolan\\", 148)) for book in book_inventory.get_items(): print(process_item(book)) # Output: 27 for dvd in dvd_inventory.get_items(): print(process_item(dvd)) # Output: 33 ``` Explanation - The `Book` and `DVD` classes are defined as `NamedTuple` with respective attributes. - Both `Book` and `DVD` implement the `LibraryItem` protocol by providing a `display` method. - The `Inventory` class is a generic class that can store and manage any items adhering to the `LibraryItem` protocol. - The `process_item` function is a callable that takes an item and returns the length of the string representation of the item (as a simple example of processing). Ensure to run static type checkers on your code to validate the type hints.","solution":"from typing import NamedTuple, Protocol, TypeVar, Generic, List, Callable, Union # 1. Define custom types class Book(NamedTuple): title: str author: str pages: int class DVD(NamedTuple): title: str director: str duration: int # 2. Define Protocol class LibraryItem(Protocol): def display(self) -> str: ... # 3. Implement custom types adhering to the protocol class Book(NamedTuple): title: str author: str pages: int def display(self) -> str: return f\\"Book: {self.title} by {self.author}, {self.pages} pages\\" class DVD(NamedTuple): title: str director: str duration: int def display(self) -> str: return f\\"DVD: {self.title} directed by {self.director}, {self.duration} minutes\\" # 4. Define type alias for Result Result = Union[str, int] # 5. Define Inventory class T = TypeVar(\'T\', bound=LibraryItem) class Inventory(Generic[T]): def __init__(self): self.items: List[T] = [] def add_item(self, item: T) -> None: self.items.append(item) def get_items(self) -> List[T]: return self.items # 6. Define a processing function def process_item(item: T) -> Result: return len(item.display()) # Example usage book_inventory = Inventory[Book]() book_inventory.add_item(Book(\\"1984\\", \\"George Orwell\\", 328)) dvd_inventory = Inventory[DVD]() dvd_inventory.add_item(DVD(\\"Inception\\", \\"Christopher Nolan\\", 148)) for book in book_inventory.get_items(): print(process_item(book)) # Output: 39 for dvd in dvd_inventory.get_items(): print(process_item(dvd)) # Output: 46"},{"question":"# Pandas Coding Assessment: Analyzing Meteorological Data You are given a dataset containing daily meteorological data, which includes information like temperature, precipitation, and humidity. Your task is to perform the following operations using pandas to gain insights into this data. Dataset Description - **Date**: The date of the recording (format: YYYY-MM-DD). - **Temperature**: The daily average temperature in degrees Celsius. - **Precipitation**: The daily precipitation in millimeters. - **Humidity**: The daily average humidity percentage. Tasks 1. **Data Loading and Cleaning**: - Load the data from a CSV file named `meteorological_data.csv`. - Ensure that the `Date` column is parsed as a datetime object. - Handle any missing values by imputing them with the median value of the respective column. 2. **Data Aggregation**: - Compute the monthly average temperature, total precipitation, and average humidity. - Output these values in a new DataFrame with columns: `Month`, `Avg_Temperature`, `Total_Precipitation`, `Avg_Humidity`. 3. **Plotting**: - Generate a line plot showing the monthly average temperature trends with `Date` on the x-axis and `Avg_Temperature` on the y-axis. - Save this plot as `temperature_trend.png`. 4. **Advanced Analysis**: - Identify the month with the highest total precipitation and return the corresponding row from the aggregated DataFrame. Function Signature ```python def meteorological_analysis(file_path: str): import pandas as pd import matplotlib.pyplot as plt # 1. Load and clean the data data = pd.read_csv(file_path) data[\'Date\'] = pd.to_datetime(data[\'Date\']) data.fillna(data.median(), inplace=True) # 2. Data aggregation data.set_index(\'Date\', inplace=True) monthly_data = data.resample(\'M\').agg({ \'Temperature\': \'mean\', \'Precipitation\': \'sum\', \'Humidity\': \'mean\' }).reset_index() monthly_data.columns = [\'Month\', \'Avg_Temperature\', \'Total_Precipitation\', \'Avg_Humidity\'] # 3. Plotting plt.figure(figsize=(10, 5)) plt.plot(monthly_data[\'Month\'], monthly_data[\'Avg_Temperature\'], marker=\'o\') plt.title(\'Monthly Average Temperature Trend\') plt.xlabel(\'Month\') plt.ylabel(\'Average Temperature (°C)\') plt.grid() plt.savefig(\'temperature_trend.png\') # 4. Advanced analysis max_precipitation_month = monthly_data.loc[monthly_data[\'Total_Precipitation\'].idxmax()] return monthly_data, max_precipitation_month ``` Input - The path to the CSV file: a string. Output - A DataFrame containing the monthly aggregates. - A Series containing data for the month with the highest precipitation. Constraints - The CSV file is guaranteed to have the columns: `Date`, `Temperature`, `Precipitation`, `Humidity`. - The Date column is in a consistent format. Make sure to follow the function signature strictly and handle any potential edge cases such as missing data.","solution":"import pandas as pd import matplotlib.pyplot as plt def meteorological_analysis(file_path: str): Perform analysis on meteorological data from a given CSV file. Args: - file_path (str): The path to the CSV file containing the data. Returns: - pd.DataFrame: A DataFrame containing the monthly aggregates. - pd.Series: A Series containing data for the month with the highest precipitation. # 1. Load and clean the data data = pd.read_csv(file_path) data[\'Date\'] = pd.to_datetime(data[\'Date\']) data.fillna(data.median(), inplace=True) # 2. Data aggregation data.set_index(\'Date\', inplace=True) monthly_data = data.resample(\'M\').agg({ \'Temperature\': \'mean\', \'Precipitation\': \'sum\', \'Humidity\': \'mean\' }).reset_index() monthly_data.columns = [\'Month\', \'Avg_Temperature\', \'Total_Precipitation\', \'Avg_Humidity\'] # 3. Plotting plt.figure(figsize=(10, 5)) plt.plot(monthly_data[\'Month\'], monthly_data[\'Avg_Temperature\'], marker=\'o\') plt.title(\'Monthly Average Temperature Trend\') plt.xlabel(\'Month\') plt.ylabel(\'Average Temperature (°C)\') plt.grid() plt.savefig(\'temperature_trend.png\') # 4. Advanced analysis max_precipitation_month = monthly_data.loc[monthly_data[\'Total_Precipitation\'].idxmax()] return monthly_data, max_precipitation_month"},{"question":"Coding Assessment Question **Objective:** Demonstrate understanding and application of the Python `csv` module by implementing a function that reads a custom-formatted CSV file, processes the data, and writes it to another CSV file with a different format. **Problem Statement:** Write a function `process_csv_files(input_file: str, output_file: str, delimiter_in: str, delimiter_out: str, quotechar_out: str)` that: 1. Reads data from `input_file`, a CSV file with the given `delimiter_in`. 2. Processes the data to extract rows, converting all string fields to uppercase and all numeric fields to their square. 3. Writes the processed data to `output_file`, using `delimiter_out` and `quotechar_out`. 4. Utilizes the appropriate dialect if necessary. 5. Contains error handling to manage possible CSV formatting issues. **Function Signature:** ```python def process_csv_files(input_file: str, output_file: str, delimiter_in: str, delimiter_out: str, quotechar_out: str) -> None: ``` **Input and Output:** - `input_file` (str): Path to the input CSV file. - `output_file` (str): Path to the output CSV file. - `delimiter_in` (str): Delimiter character used in the input CSV file. - `delimiter_out` (str): Delimiter character to be used in the output CSV file. - `quotechar_out` (str): Quote character to be used in the output CSV file. **Constraints:** - The input file may contain mixed data types (strings and numbers). - Assume well-formed input for the scope of processing (but handle minor errors gracefully with appropriate logging). # Example: Given the input file `input.csv`: ``` Name,Age,Salary Alice,30,70000 Bob,25,52000 ``` ```python process_csv_files(\'input.csv\', \'output.csv\', \',\', \'|\', \'\\"\') ``` Should produce the output file `output.csv`: ``` NAME|AGE|SALARY \\"ALICE\\"|900|4900000000 \\"BOB\\"|625|2704000000 ``` # Requirements: 1. Use the `csv` module\'s `reader` and `writer` functionalities. 2. Define and register a custom dialect if needed to handle the input or output formats. 3. Implement error handling to catch and log any CSV related errors. # Hints: - Use `csv.Sniffer` if you are unsure about the input file\'s format. - Consider the Python standard library logging module for error handling.","solution":"import csv import logging logging.basicConfig(level=logging.ERROR) def process_csv_files(input_file: str, output_file: str, delimiter_in: str, delimiter_out: str, quotechar_out: str) -> None: try: with open(input_file, mode=\'r\', newline=\'\') as infile: reader = csv.reader(infile, delimiter=delimiter_in) header = next(reader) transformed_data = [] transformed_data.append([col.upper() for col in header]) for row in reader: new_row = [] for item in row: if item.isdigit(): new_row.append(int(item) ** 2) else: new_row.append(item.upper()) transformed_data.append(new_row) with open(output_file, mode=\'w\', newline=\'\') as outfile: writer = csv.writer(outfile, delimiter=delimiter_out, quotechar=quotechar_out, quoting=csv.QUOTE_MINIMAL) writer.writerows(transformed_data) except Exception as e: logging.error(f\\"An error occurred while processing the CSV files: {e}\\")"},{"question":"Objective This question will test your understanding of Python\'s `asyncio` module, along with its platform-specific characteristics and limitations. Problem Statement Write a Python function `configure_event_loop()` that configures and returns an appropriate `asyncio` event loop based on the operating system and specific requirements for subprocess and file descriptor handling. Your function should take into account the platform and any constraints specified in the arguments. Requirements 1. Your function should determine the current operating system using the `sys` module. 2. Configure a `SelectorEventLoop` or `ProactorEventLoop` based on the platform and the given constraints. 3. If the platform is macOS version <= 10.8, configure the event loop to use `SelectSelector` or `PollSelector`. 4. Raise an appropriate exception with a clear error message if any specified constraint is not supported on the platform. Function Signature ```python import sys import asyncio def configure_event_loop(require_subprocess: bool, require_file_descriptor: bool) -> asyncio.AbstractEventLoop: # Your code here ``` Input - `require_subprocess`: A boolean indicating if the event loop needs to support subprocesses. - `require_file_descriptor`: A boolean indicating if the event loop needs to support file descriptor-based I/O operations. Output - Returns an instance of `asyncio.AbstractEventLoop` configured according to the specified constraints. Constraints - Assume modern versions of macOS or Windows if no specific version information is available. - Consider the best possible resolution for the event loop\'s monotonic clock. Example Usage ```python loop = configure_event_loop(require_subprocess=True, require_file_descriptor=True) print(loop) ``` Evaluation Criteria - Correct platform identification and handling. - Proper configuration of the event loop based on the provided constraints. - Handling of errors and unsupported operations with clear messages. - Code readability and use of best practices in async programming. # Notes - You may use Python\'s standard library modules such as `sys` and `selectors`. - Refer to the `asyncio` module documentation for detailed information on the event loop classes and methods.","solution":"import sys import asyncio import selectors def configure_event_loop(require_subprocess: bool, require_file_descriptor: bool) -> asyncio.AbstractEventLoop: # Identify the current operating system platform = sys.platform # macOS version < 10.8 if platform == \'darwin\' and sys.version_info < (10, 8): if require_subprocess or require_file_descriptor: raise NotImplementedError(\\"Subprocess and file descriptor support is not fully implemented on macOS <= 10.8\\") selector = selectors.SelectSelector() if hasattr(selectors, \'SelectSelector\') else selectors.PollSelector() loop = asyncio.SelectorEventLoop(selector) asyncio.set_event_loop(loop) return loop # For Windows if platform.startswith(\'win\'): if require_file_descriptor: raise NotImplementedError(\\"File descriptor support is not available on Windows\\") loop = asyncio.ProactorEventLoop() asyncio.set_event_loop(loop) return loop # For other Unix-like systems including modern macOS if require_subprocess or require_file_descriptor: loop = asyncio.SelectorEventLoop() else: loop = asyncio.new_event_loop() asyncio.set_event_loop(loop) return loop"},{"question":"Context You are implementing a Python script that needs to automate the retrieval of credentials for a variety of servers as specified in a `.netrc` file. The `netrc` module is useful for processing such files. Your task is to create a utility function that uses the `netrc` module to retrieve and securely print credentials for a provided host. Task Write a Python function `get_credentials(file_path, host)` that: 1. Parses the netrc file located at `file_path`. 2. Retrieves the authentication tuple (login, account, password) for the given `host`. 3. Gracefully handles possible exceptions such as file not found (`FileNotFoundError`), parsing errors (`netrc.NetrcParseError`), and missing host or default entries. 4. Returns a dictionary with credentials (`login`, `account`, `password`). If no credentials are found, the dictionary should have `None` values for each key. Function Signature ```python def get_credentials(file_path: str, host: str) -> dict: pass ``` Input - `file_path` (string): The path to the netrc file. - `host` (string): The host name for which credentials are required. Output - A dictionary with keys `login`, `account`, and `password`. Constraints - The `.netrc` file may or may not have a `default` entry. - The requested host may or may not be present in the `.netrc` file. - Follow constraint on passwords as specified by the `netrc` module. Example Suppose the contents of `sample.netrc` are: ``` machine example.com login mylogin password mypassword account myaccount default login defaultlogin password defaultpassword account defaultaccount ``` - Calling `get_credentials(\'sample.netrc\', \'example.com\')` should return: ```python { \'login\': \'mylogin\', \'account\': \'myaccount\', \'password\': \'mypassword\' } ``` - Calling `get_credentials(\'sample.netrc\', \'unknown.com\')` should return: ```python { \'login\': \'defaultlogin\', \'account\': \'defaultaccount\', \'password\': \'defaultpassword\' } ``` - Calling `get_credentials(\'nonexisting.netrc\', \'example.com\')` should handle `FileNotFoundError` gracefully: ```python { \'login\': None, \'account\': None, \'password\': None } ``` Notes - Ensure to use the appropriate exception handling for different potential errors. - Consider using the `authenticators(host)` method of the `netrc` class for retrieving credentials.","solution":"import netrc from typing import Optional, Dict def get_credentials(file_path: str, host: str) -> Dict[str, Optional[str]]: try: # Attempt to parse the netrc file netrc_data = netrc.netrc(file_path) # Retrieve the authenticators for the given host auth_tuple = netrc_data.authenticators(host) if auth_tuple is None: # If no entry for the specific host, attempt to retrieve default entry auth_tuple = netrc_data.authenticators(\'default\') except (FileNotFoundError, netrc.NetrcParseError): auth_tuple = None if auth_tuple: login, account, password = auth_tuple else: login = account = password = None return { \'login\': login, \'account\': account, \'password\': password }"},{"question":"Configuring PyTorch Distributed Training Objective You are required to implement a function that configures environment variables for a PyTorch distributed training job using NCCL (NVIDIA Collective Communications Library). Your function should allow the user to set various environment variables that control the behavior of the NCCL communicator, error handling, debugging, and monitoring. Function Signature ```python def configure_nccl_environment(settings: dict) -> None: Configures the environment variables for NCCL based on the provided settings. :param settings: A dictionary where keys are the names of the environment variables (str) and values are the settings for these variables (str or int). Possible environment variables include: - \'TORCH_NCCL_ASYNC_ERROR_HANDLING\' - \'TORCH_NCCL_HIGH_PRIORITY\' - \'TORCH_NCCL_BLOCKING_WAIT\' - \'TORCH_NCCL_DUMP_ON_TIMEOUT\' - \'TORCH_NCCL_DESYNC_DEBUG\' - \'TORCH_NCCL_ENABLE_TIMING\' - \'TORCH_NCCL_ENABLE_MONITORING\' - \'TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC\' - \'TORCH_NCCL_TRACE_BUFFER_SIZE\' - \'TORCH_NCCL_TRACE_CPP_STACK\' - \'TORCH_NCCL_COORD_CHECK_MILSEC\' - \'TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC\' - \'TORCH_NCCL_DEBUG_INFO_TEMP_FILE\' - \'TORCH_NCCL_DEBUG_INFO_PIPE_FILE\' - \'TORCH_NCCL_NAN_CHECK\' Any environment variables not specified in the settings dictionary should remain unchanged. :return: None pass ``` Input The function takes a single parameter: - `settings`: A dictionary containing the environment variables to set and their corresponding values. The keys are the names of the environment variables (strings), and the values are the respective settings (either string or integer). Output The function does not return any value but sets the environment variables according to the provided settings. Constraints and Assumptions - The function should only set environment variables that are specified in the `settings` dictionary. - The function should raise a `ValueError` if an invalid environment variable name is provided. - Assume that the function will be executed in an environment where the `os` module is available. Example Usage ```python settings = { \'TORCH_NCCL_ASYNC_ERROR_HANDLING\': 1, \'TORCH_NCCL_HIGH_PRIORITY\': 1, \'TORCH_NCCL_BLOCKING_WAIT\': 0, \'TORCH_NCCL_TRACE_BUFFER_SIZE\': 1024, \'TORCH_NCCL_ENABLE_TIMING\': 1 } configure_nccl_environment(settings) ``` Additional Information - You may find the `os.environ` in Python useful for setting environment variables. - Document any assumptions or edge cases you handle.","solution":"import os def configure_nccl_environment(settings: dict) -> None: Configures the environment variables for NCCL based on the provided settings. :param settings: A dictionary where keys are the names of the environment variables (str) and values are the settings for these variables (str or int). Possible environment variables include: - \'TORCH_NCCL_ASYNC_ERROR_HANDLING\' - \'TORCH_NCCL_HIGH_PRIORITY\' - \'TORCH_NCCL_BLOCKING_WAIT\' - \'TORCH_NCCL_DUMP_ON_TIMEOUT\' - \'TORCH_NCCL_DESYNC_DEBUG\' - \'TORCH_NCCL_ENABLE_TIMING\' - \'TORCH_NCCL_ENABLE_MONITORING\' - \'TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC\' - \'TORCH_NCCL_TRACE_BUFFER_SIZE\' - \'TORCH_NCCL_TRACE_CPP_STACK\' - \'TORCH_NCCL_COORD_CHECK_MILSEC\' - \'TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC\' - \'TORCH_NCCL_DEBUG_INFO_TEMP_FILE\' - \'TORCH_NCCL_DEBUG_INFO_PIPE_FILE\' - \'TORCH_NCCL_NAN_CHECK\' Any environment variables not specified in the settings dictionary should remain unchanged. :return: None valid_keys = { \'TORCH_NCCL_ASYNC_ERROR_HANDLING\', \'TORCH_NCCL_HIGH_PRIORITY\', \'TORCH_NCCL_BLOCKING_WAIT\', \'TORCH_NCCL_DUMP_ON_TIMEOUT\', \'TORCH_NCCL_DESYNC_DEBUG\', \'TORCH_NCCL_ENABLE_TIMING\', \'TORCH_NCCL_ENABLE_MONITORING\', \'TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC\', \'TORCH_NCCL_TRACE_BUFFER_SIZE\', \'TORCH_NCCL_TRACE_CPP_STACK\', \'TORCH_NCCL_COORD_CHECK_MILSEC\', \'TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC\', \'TORCH_NCCL_DEBUG_INFO_TEMP_FILE\', \'TORCH_NCCL_DEBUG_INFO_PIPE_FILE\', \'TORCH_NCCL_NAN_CHECK\' } for key, value in settings.items(): if key not in valid_keys: raise ValueError(f\\"Invalid environment variable name: {key}\\") os.environ[key] = str(value)"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of Seaborn\'s object-oriented interface for creating and customizing plots. # Problem Statement You are provided with the `penguins` dataset from Seaborn. Your task is to create a customized plot that effectively communicates the data trends. Follow the instructions below to complete this task. # Instructions 1. Load the `penguins` dataset using Seaborn\'s `load_dataset` function. 2. Create a plot that visualizes the `body_mass_g` of penguins across different `species`, coloring the points by `sex`. 3. Add `Dash` marks to the plot: - Adjust the transparency (`alpha`) to 0.5. - Map the `linewidth` to the `flipper_length_mm` variable. - Adjust the `width` of the marks to 0.5. 4. Incorporate `Dodging` to avoid overlapping of marks. 5. Overlay the plot with a combination of aggregate (`Agg`) and individual (`Dots`) data points with appropriate dodging and jittering. # Input - No input arguments. Your code should perform the operation as described using the `penguins` dataset. # Expected Output - A matplotlib plot showing the described visualizations and customizations. # Constraints - Ensure all visual elements (Dashes, Dots, Agg, Dodge, Jitter) are clearly distinguishable. - Properly handle any missing data in the dataset. # Performance Requirements - The solution should execute efficiently, handling the size of the `penguins` dataset comfortably. # Example Code Output ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the plot p = (so.Plot(penguins, \\"species\\", \\"body_mass_g\\", color=\\"sex\\") .add(so.Dash(alpha=0.5), linewidth=\\"flipper_length_mm\\") .add(so.Dash(width=0.5)) .add(so.Dash(), so.Dodge()) .add(so.Dash(), so.Agg(), so.Dodge()) .add(so.Dots(), so.Dodge(), so.Jitter())) # Display the plot p.show() ``` Ensure your code meets the above requirements and produces a clear and informative plot.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_penguins_plot(): # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the plot p = (so.Plot(penguins, \\"species\\", \\"body_mass_g\\", color=\\"sex\\") .add(so.Dash(alpha=0.5), linewidth=\\"flipper_length_mm\\") .add(so.Dash(width=0.5)) .add(so.Dash(), so.Dodge()) .add(so.Dash(), so.Agg(), so.Dodge()) .add(so.Dots(), so.Dodge(), so.Jitter())) # Display the plot p.show()"},{"question":"# Python Memory Management: Custom Allocator Implementation **Objective:** Demonstrate your understanding of Python\'s memory management by implementing a custom memory allocator for a specific allocator domain and ensuring its correct usage with Python\'s memory management functions. # Problem Statement: You are tasked with implementing a custom memory allocator using the Python/C API. Your allocator will follow these steps: 1. Implement a custom `malloc`, `calloc`, `realloc`, and `free` function that logs each memory operation to a file. 2. Register this custom allocator with the Python memory manager for the `PYMEM_DOMAIN_MEM` domain. 3. Use this custom allocator to manage memory for a simple Python extension module that creates, resizes, and deletes an array of integers. # Requirements: 1. Implement the custom allocator functions `my_malloc`, `my_calloc`, `my_realloc`, and `my_free`. 2. Create a Python extension module that initializes and registers the custom allocator. 3. Write a Python function in the module that performs the following operations: - Allocate an array of integers. - Resize the array. - Free the array. 4. Ensure that each memory operation logs a message to a file `memory.log`. # Function Signatures: - **Custom Allocator in C:** ```c void* my_malloc(void* ctx, size_t size); void* my_calloc(void* ctx, size_t nelem, size_t elsize); void* my_realloc(void* ctx, void* ptr, size_t new_size); void my_free(void* ctx, void* ptr); ``` - **Python Extension Module in C:** ```c PyObject* create_and_manage_array(PyObject* self, PyObject* args); ``` - **Python Usage:** ```python import custom_allocator custom_allocator.create_and_manage_array() ``` # Constraints: - Follow Python memory management practices as specified in the documentation. - Do not mix Python\'s memory management functions with C standard library functions. - Ensure thread-safety if required. # Sample Output: If the operations are performed correctly, `memory.log` should contain log entries like: ``` Allocated 100 bytes Reallocated to 200 bytes Freed memory ``` # Submission: Submit the following files: 1. C source file implementing the custom allocator and Python extension module. 2. Python script demonstrating the usage of the extension module. 3. The `memory.log` file capturing the actions of the custom allocator.","solution":"# We will implement the custom allocator in pure Python for simulation # purposes since we do not have the capability to use the Python/C API # directly here. Note that this will only serve as a conceptual mock-up. import os LOG_FILE = \\"memory.log\\" def log(message): with open(LOG_FILE, \\"a\\") as f: f.write(message + \\"n\\") def my_malloc(size): log(f\\"Allocated {size} bytes\\") return bytearray(size) def my_calloc(nelem, elsize): size = nelem * elsize log(f\\"Calloc: Allocated {nelem} elements of {elsize} bytes each (Total: {size} bytes)\\") return bytearray(size) def my_realloc(ptr, new_size): old_size = len(ptr) log(f\\"Reallocated from {old_size} to {new_size} bytes\\") ptr.extend(bytearray(new_size - old_size)) return ptr def my_free(ptr): size = len(ptr) log(f\\"Freed memory of size {size} bytes\\") del ptr def create_and_manage_array(): array = my_malloc(100) array = my_realloc(array, 200) my_free(array)"},{"question":"**Objective:** You are required to write a Python function that optimizes the prediction latency of a given machine learning model using scikit-learn. Your task involves adjusting model parameters, converting data representations appropriately, and analyzing throughput. **Problem Statement:** Given a dataset and a linear machine learning model, you need to: 1. Optimize the prediction latency by adjusting the model complexity. 2. Evaluate the benefit of using sparse data representation. 3. Report the prediction throughput before and after your optimizations. **Instructions:** 1. **Function Name**: `optimize_model_performance` 2. **Parameters**: - `X_train` (numpy.ndarray or scipy.sparse matrix): The training data features. - `y_train` (numpy.ndarray): The training data labels. - `X_test` (numpy.ndarray or scipy.sparse matrix): The test data features for evaluation. - `initial_model` (sklearn model): The scikit-learn model to be optimized. 3. **Returns**: - A tuple containing: - Optimized scikit-learn model. - Prediction throughput before optimization (`float`). - Prediction throughput after optimization (`float`). You should follow these steps within your function: 1. **Optimize Model Complexity**: - Train the `initial_model` on the provided dataset. - Adjust the model parameters to achieve a balance between accuracy and prediction latency. You can choose techniques such as: - Regularization (L1, L2, ElasticNet). - Reducing the number of features (feature selection techniques). 2. **Evaluate Sparse Representation**: - Convert `X_train` and `X_test` to sparse matrices if the sparsity ratio is greater than 90%. - Compare prediction latencies for dense vs. sparse representations and choose the optimal one. 3. **Throughput Analysis**: - Measure prediction throughput (predictions per second) for the initial model on `X_test`. - Measure prediction throughput for the optimized model on `X_test`. Below is a templated function to get you started: ```python import numpy as np import time from sklearn.linear_model import SGDClassifier from scipy import sparse def sparsity_ratio(X): return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1]) def measure_throughput(model, X): start_time = time.time() model.predict(X) end_time = time.time() throughput = len(X) / (end_time - start_time) return throughput def optimize_model_performance(X_train, y_train, X_test, initial_model): # Train initial model initial_model.fit(X_train, y_train) # Measure initial throughput initial_throughput = measure_throughput(initial_model, X_test) # Optimize model complexity (Example: using ElasticNet regularization) optimized_model = SGDClassifier(penalty=\'elasticnet\', l1_ratio=0.25) optimized_model.fit(X_train, y_train) # Evaluate sparse representation if sparsity_ratio(X_train) > 0.9: X_train_sparse = sparse.csr_matrix(X_train) X_test_sparse = sparse.csr_matrix(X_test) optimized_model.fit(X_train_sparse, y_train) final_throughput = measure_throughput(optimized_model, X_test_sparse) else: final_throughput = measure_throughput(optimized_model, X_test) return optimized_model, initial_throughput, final_throughput # Example usage: # X_train, y_train, X_test should be defined appropriately. # initial_model = SGDClassifier() # optimized_model, initial_throughput, final_throughput = optimize_model_performance(X_train, y_train, X_test, initial_model) ``` **constraints or limitations**: - Assume `X_train` and `X_test` have the same features and are preprocessed. - You may assume `initial_model` is a linear model like `SGDClassifier`. **Performance requirements**: - Ensure the optimized model maintains a reasonable trade-off between latency and accuracy. - Provide a clear documentation for the implemented optimizations and throughput measurements.","solution":"import numpy as np import time from sklearn.linear_model import SGDClassifier from sklearn.feature_selection import SelectFromModel from scipy import sparse def sparsity_ratio(X): return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1]) def measure_throughput(model, X): start_time = time.time() model.predict(X) end_time = time.time() throughput = len(X) / (end_time - start_time) return throughput def optimize_model_performance(X_train, y_train, X_test, initial_model): # Train initial model initial_model.fit(X_train, y_train) # Measure initial throughput initial_throughput = measure_throughput(initial_model, X_test) # Optimize model complexity by using ElasticNet regularization optimized_model = SGDClassifier(penalty=\'elasticnet\', l1_ratio=0.25, max_iter=1000, tol=1e-3) # Perform feature selection if necessary feature_selector = SelectFromModel(SGDClassifier(penalty=\'elasticnet\', l1_ratio=0.25)) feature_selector.fit(X_train, y_train) X_train_reduced = feature_selector.transform(X_train) X_test_reduced = feature_selector.transform(X_test) optimized_model.fit(X_train_reduced, y_train) # Evaluate sparse representation if sparsity_ratio(X_train) > 0.9: X_train_sparse = sparse.csr_matrix(X_train_reduced) X_test_sparse = sparse.csr_matrix(X_test_reduced) optimized_model.fit(X_train_sparse, y_train) final_throughput = measure_throughput(optimized_model, X_test_sparse) else: final_throughput = measure_throughput(optimized_model, X_test_reduced) return optimized_model, initial_throughput, final_throughput"},{"question":"Objective Design a Python function that uses the `trace` module to trace the execution of a given function, count the number of times each line is executed, and report the coverage results in a specified directory. Task Write a function `trace_execution(func, *args, **kwargs)` that: 1. Takes a function `func` and its arguments (`*args` and `**kwargs`). 2. Uses the `trace` module to trace the execution of `func`. 3. Counts the number of times each line in `func` is executed. 4. Writes the coverage results to a directory \\"coverage_results\\" within the current working directory. Requirements - Use the `trace.Trace` class to trace the function execution. - The coverage results should display which lines were not executed. - The resulting coverage files should be stored in the \\"coverage_results\\" directory. Constraints - The function should handle any valid Python function passed to it. - Do not use any external libraries other than the Python standard library. Example Usage ```python # Sample function to be traced def sample_function(x, y): total = x + y if total > 10: return total return total * 2 # Your trace_execution function trace_execution(sample_function, 5, 10) # Expected result: # The coverage results should be written to the \\"coverage_results\\" directory. # The output files should indicate the number of times each line in `sample_function` was executed. ``` ```python def trace_execution(func, *args, **kwargs): import trace import os # Directory to store coverage results coverage_dir = \\"coverage_results\\" if not os.path.exists(coverage_dir): os.makedirs(coverage_dir) # Create a Trace object to count line numbers and trace execution tracer = trace.Trace(trace=0, count=1) # Run the function with tracing enabled tracer.runfunc(func, *args, **kwargs) # Get the results from tracing results = tracer.results() # Write the coverage results to the specified directory results.write_results(show_missing=True, coverdir=coverage_dir) ``` Evaluation Criteria - Correct usage of the `trace.Trace` class. - Accurate counting and tracing of function execution. - Proper directory handling and results output. - Adherence to Python coding standards and practices.","solution":"def trace_execution(func, *args, **kwargs): import trace import os # Directory to store coverage results coverage_dir = \\"coverage_results\\" if not os.path.exists(coverage_dir): os.makedirs(coverage_dir) # Create a Trace object to count line numbers and trace execution tracer = trace.Trace(trace=0, count=1) # Run the function with tracing enabled tracer.runfunc(func, *args, **kwargs) # Get the results from tracing results = tracer.results() # Write the coverage results to the specified directory results.write_results(show_missing=True, coverdir=coverage_dir)"},{"question":"# Question: # Email Extraction and Validation You are tasked with writing a Python function that processes student email addresses for a university database. Each email address needs to be validated against a regular expression pattern, and valid emails should be extracted and properly formatted. Requirements: 1. Design a function `extract_and_validate_emails(input_string: str) -> List[str]` that takes a single string input and returns a list of valid email addresses. Each email address should be a valid student email address and formatted uniformly. 2. A valid email address should: - Begin with a lowercase letter. - Be followed by zero or more lowercase letters, digits, periods, or hyphens. - Contain exactly one \\"@\\" symbol. - Followed by a domain name which is one or more groups of lowercase letters or digits separated by periods, ending with a two to four-letter TLD (e.g., .edu, .com). - Not contain any characters before or after the email address in the given input string (i.e., it should be a whole matching string). 3. Any whitespace around the email address should be ignored but any email address embedded in other text should not be considered valid. 4. Use the raw string notation for regular expression patterns. 5. Use functions from the `re` module to achieve this task. Input: - `input_string` (str): A multiline string that includes email addresses mixed with other text. Output: - A list of validated email addresses. Example: ```python input_string = john.doe@university.edu jane.doe@university.co.uk invalid-email@place,com another.invalid@place. valid.email@domain.com random text valid-email-123@anotheredu.university.us.org text random output = extract_and_validate_emails(input_string) print(output) # Expected: [\'john.doe@university.edu\', \'jane.doe@university.co.uk\', \'valid.email@domain.com\', \'valid-email-123@anotheredu.university.us.org\'] ``` Note: - Use appropriate Python `re` module functions to compile and match the pattern. - You can assume the input string is well-formed but may contain invalid email addresses. - The function must be efficient and capable of handling large input strings. # Solution Template: ```python from typing import List import re def extract_and_validate_emails(input_string: str) -> List[str]: # Your code here pass # Example usage: input_string = john.doe@university.edu jane.doe@university.co.uk invalid-email@place,com another.invalid@place. valid.email@domain.com random text valid-email-123@anotheredu.university.us.org text random output = extract_and_validate_emails(input_string) print(output) # Expected: [\'john.doe@university.edu\', \'jane.doe@university.co.uk\', \'valid.email@domain.com\', \'valid-email-123@anotheredu.university.us.org\'] ```","solution":"from typing import List import re def extract_and_validate_emails(input_string: str) -> List[str]: # Define the regex pattern for a valid email address pattern = re.compile(r\'b[a-z][a-z0-9.-]*@[a-z0-9.-]+.[a-z]{2,4}b\') # Find all matches according to the pattern matches = pattern.findall(input_string) return matches"},{"question":"# Task You are tasked with implementing a data structure that supports efficient addition of elements and finding the top `k` smallest elements at any point in time. You should use heap operations provided by the `heapq` module to ensure that your implementation is optimally efficient. # Objective Implement a class `TopKElements` which supports the following operations: 1. `__init__(self, k: int)`: Initializes the data structure with an integer `k`, which specifies the maximum number of smallest elements to maintain. 2. `add(self, num: int)`: Adds the element `num` to the data structure. 3. `get_top_k(self) -> List[int]`: Returns a sorted list of the top `k` smallest elements currently in the data structure. # Constraints - All elements added will be integers in the range `[-10^6, 10^6]`. - The value of `k` is positive and will be less than or equal to the total number of elements added. - The operations should be efficiently handled to maintain performance, specifically aiming for logarithmic time complexity for element addition. # Example ```python # Initialization topk = TopKElements(3) # Adding elements topk.add(5) topk.add(2) topk.add(8) topk.add(1) topk.add(7) # Fetching top k smallest elements print(topk.get_top_k()) # Output: [1, 2, 5] ``` # Notes - Use the `heapq` module functions to handle the heap operations efficiently. - Ensure that the `get_top_k` function returns a sorted list of the top `k` smallest elements. # Solution Skeleton ```python import heapq from typing import List class TopKElements: def __init__(self, k: int): self.k = k self.min_heap = [] def add(self, num: int): # Add your implementation here def get_top_k(self) -> List[int]: # Add your implementation here # Example usage: # topk = TopKElements(3) # topk.add(5) # topk.add(2) # topk.add(8) # topk.add(1) # topk.add(7) # print(topk.get_top_k()) # Output: [1, 2, 5] ```","solution":"import heapq from typing import List class TopKElements: def __init__(self, k: int): self.k = k self.min_heap = [] def add(self, num: int): if len(self.min_heap) < self.k: heapq.heappush(self.min_heap, -num) else: heapq.heappushpop(self.min_heap, -num) def get_top_k(self) -> List[int]: return sorted([-x for x in self.min_heap]) # Example usage: # topk = TopKElements(3) # topk.add(5) # topk.add(2) # topk.add(8) # topk.add(1) # topk.add(7) # print(topk.get_top_k()) # Output: [1, 2, 5]"},{"question":"**Objective:** To assess your understanding of the python asyncio module, you are required to implement a function that handles concurrent tasks efficiently. You should demonstrate the use of coroutines, task creation, and handling of timeouts provided by the asyncio package. **Problem Statement:** Implement a function called `fetch_and_process_data` that takes in a list of tasks representing asynchronous data fetching operations. Each task will include a coroutine that simulates fetching data from an external source and the time (in seconds) it takes to complete this fetch. You will need to process all these fetch tasks concurrently and return a list of completed results within a given timeout period. If a task does not complete within the timeout period, it should be cancelled and marked as `\\"timeout\\"` in the results list. Function Signature: ```python async def fetch_and_process_data(tasks: List[Tuple[Callable[[], Awaitable[Any]], int]], timeout: float) -> List[Union[any, str]]: ``` Input: - `tasks` (List[Tuple[Callable[[], Awaitable[Any]], int]]): A list of tuples where each tuple contains a coroutine function that fetches data and the time in seconds it requires to complete. - `timeout` (float): The maximum time (in seconds) to wait for each task to complete. Output: - A list of results where each result corresponds to the fetched data. If a task exceeds the timeout, the result should be the string `\\"timeout\\"`. Constraints: - You must use the asyncio module. - Tasks should run concurrently. - Use proper exception handling for timeouts. - Preserve the order of results as per the input tasks. Example: ```python import asyncio from typing import List, Tuple, Callable, Awaitable, Union, Any async def fetch_data_1(): await asyncio.sleep(1) return \'data_1\' async def fetch_data_2(): await asyncio.sleep(3) return \'data_2\' tasks = [(fetch_data_1, 1), (fetch_data_2, 3)] async def fetch_and_process_data(tasks: List[Tuple[Callable[[], Awaitable[Any]], int]], timeout: float) -> List[Union[Any, str]]: results = [] for task, duration in tasks: try: result = await asyncio.wait_for(task(), timeout=min(timeout, duration)) results.append(result) except asyncio.TimeoutError: results.append(\'timeout\') return results # Test the function async def main(): results = await fetch_and_process_data(tasks, 2) print(results) # Expected Output: [\'data_1\', \'timeout\'] asyncio.run(main()) ``` # Notes: - Ensure proper usage of `asyncio.wait_for` to handle timeouts. - Preserve the order of tasks in the final result. - You can reuse or modify the example coroutines `fetch_data_1` and `fetch_data_2` for testing purposes.","solution":"import asyncio from typing import List, Tuple, Callable, Awaitable, Union, Any async def fetch_and_process_data(tasks: List[Tuple[Callable[[], Awaitable[Any]], int]], timeout: float) -> List[Union[Any, str]]: results = [] async def run_task(task, duration): try: return await asyncio.wait_for(task(), timeout=min(timeout, duration)) except asyncio.TimeoutError: return \\"timeout\\" task_coros = [run_task(task, duration) for task, duration in tasks] results = await asyncio.gather(*task_coros, return_exceptions=False) return results"},{"question":"**Objective**: Implement a custom fixer for a Python 2 to 3 code translator. **Background**: The `2to3` tool is used to convert Python 2 code to Python 3 by applying a series of fixers. Each fixer identifies specific patterns in the Python 2 code and transforms them into Python 3-compatible syntax. The `2to3` tool handles numerous common cases, such as changing print statements, updating module imports, and renaming deprecated functions. In this task, you are required to implement a custom fixer function that converts Python 2\'s `xrange` function to Python 3\'s `range` function and wraps it in a list if necessary. **Problem Statement**: Write a function `fix_xrange(source_code: str) -> str` that takes in a string representing the source code in Python 2 and applies the necessary transformations to: 1. Replace all instances of `xrange` with `range`. 2. Ensure that instances of `xrange` where the result is used as an iterable in operations like `for` loops are wrapped in `list()`. **Input**: - `source_code` (str): A string containing the Python 2 source code. **Output**: - (str): The transformed Python 3 source code with `xrange` replaced by `range` and wrapped in `list` where necessary. **Constraints**: - The input code will be syntactically valid Python 2 code. - You must preserve comments and indentation in the source code. - Handle cases where `xrange` is used with and without assignments or loops. **Examples**: ```python def test_fix_xrange(): assert fix_xrange(\\"for i in xrange(10): print(i)\\") == \\"for i in list(range(10)): print(i)\\" assert fix_xrange(\\"a = xrange(5)\\") == \\"a = list(range(5))\\" assert fix_xrange(\\"print(xrange(3))\\") == \\"print(list(range(3)))\\" assert fix_xrange(\\"result = [i for i in xrange(7)]\\") == \\"result = [i for i in list(range(7))]\\" # Additional test cases can be added below to validate the solution further test_fix_xrange() ``` *Note*: Implement the function `fix_xrange` in a clear and concise manner. The function should handle complex indentation and nested structures if present in the source code.","solution":"def fix_xrange(source_code: str) -> str: import re # Pattern to match xrange with possible varying number of spaces pattern = r\'bxranges*((.*?))\' def replace_xrange(match): # Extract the contents within the parentheses of xrange contents = match.group(1) if \'for \' in match.string[:match.start()] or \'in \' in match.string[:match.start()]: return f\'list(range({contents}))\' return f\'list(range({contents}))\' # Use regex sub to replace all instances of xrange fixed_code = re.sub(pattern, replace_xrange, source_code) return fixed_code"},{"question":"# Cell Object Operations in Python **Objective**: Implement a few function simulations for manipulating cell objects, reflecting the documented C-API behavior in Python. **Problem Statement**: You are required to emulate the behavior of \\"cell\\" objects and provide functions to manage them. Implement the following: 1. **Cell Class**: Defines the behavior of a cell object. 2. **is_cell(obj)**: Checks if `obj` is a cell object. 3. **new_cell(value)**: Creates and returns a new cell object containing `value`. 4. **get_cell_value(cell)**: Returns the content of the cell object. 5. **set_cell_value(cell, value)**: Sets the content of the cell object to `value`. **Input/Output Formats**: 1. `is_cell(obj)` - **Input**: `obj` (any type) - **Output**: `True` if `obj` is a cell object, otherwise `False`. 2. `new_cell(value)` - **Input**: `value` (any type) - **Output**: A new cell object instance containing `value`. 3. `get_cell_value(cell)` - **Input**: `cell` (instance of Cell) - **Output**: The content stored in the cell. 4. `set_cell_value(cell, value)` - **Input**: `cell` (instance of Cell), `value` (any type) - **Output**: None (modifies cell object in place). **Constraints**: - While emulating the behavior, ensure reference integrity (i.e., no unintended references should be created or destroyed). **Example**: ```python # Define Cell class and required functions here # Sample usage: cell = new_cell(42) assert is_cell(cell) == True assert get_cell_value(cell) == 42 set_cell_value(cell, 100) assert get_cell_value(cell) == 100 ``` **Note**: No actual C structures should be used. Implement this purely in Python, simulating the behavior as described in the documentation.","solution":"class Cell: Class representing a cell object, which holds a single value. def __init__(self, value): self.value = value def is_cell(obj): Checks if the provided object is an instance of the Cell class. return isinstance(obj, Cell) def new_cell(value): Creates a new Cell object containing the provided value. return Cell(value) def get_cell_value(cell): Returns the value contained within the cell. if not is_cell(cell): raise TypeError(\\"Provided object is not a cell\\") return cell.value def set_cell_value(cell, value): Sets the value contained within the cell to the provided value. if not is_cell(cell): raise TypeError(\\"Provided object is not a cell\\") cell.value = value"},{"question":"**Objective:** Demonstrate your understanding of PyTorch tensors and the usage of the `torch.Size` class for size-related operations. **Problem Statement:** Write a function `process_tensor` that takes as input a PyTorch tensor `input_tensor`. The function should compute the following and return the results in a dictionary format: 1. The size of the input tensor (as a `torch.Size` object). 2. The total number of elements in the tensor. 3. The size of each dimension as a separate list. 4. A new tensor with each dimension\'s size doubled if the tensor contains fewer than 100 elements. 5. The dimensions of the transposed tensor if applicable (for tensors of at least 2 dimensions). The returned dictionary should have the following keys: - `\'size\'` : the size of the tensor. - `\'num_elements\'` : total number of elements in the tensor. - `\'size_list\'` : list of sizes of each dimension. - `\'doubled_tensor\'` : new tensor with doubled sizes or `None` if the condition is not met. - `\'transposed_size\'` : size of the transposed tensor or `None` if transposition is not applicable. **Function Signature:** ```python import torch def process_tensor(input_tensor: torch.Tensor) -> dict: pass ``` **Input:** - `input_tensor` (torch.Tensor): The input tensor of any shape and type. **Output:** - Dictionary with the aforementioned keys and respective values. **Constraints:** - If the tensor has fewer than 100 elements, double the size of each dimension. - The transposition check is applicable only for tensors with at least 2 dimensions. **Example:** ```python x = torch.ones(10, 20, 30) result = process_tensor(x) print(result) ``` Expected Output: ```python { \'size\': torch.Size([10, 20, 30]), \'num_elements\': 6000, \'size_list\': [10, 20, 30], \'doubled_tensor\': None, \'transposed_size\': torch.Size([30, 20, 10]) } ``` Create the solution considering all possible edge cases and ensure that the function handles tensors of various shapes and sizes correctly.","solution":"import torch def process_tensor(input_tensor: torch.Tensor) -> dict: result = {} # Size of the input tensor size = input_tensor.size() result[\'size\'] = size # Total number of elements in the tensor num_elements = input_tensor.numel() result[\'num_elements\'] = num_elements # Size of each dimension as a separate list size_list = list(size) result[\'size_list\'] = size_list # New tensor with each dimension\'s size doubled if condition is met if num_elements < 100: doubled_size = [dim * 2 for dim in size] doubled_tensor = torch.empty(*doubled_size) else: doubled_tensor = None result[\'doubled_tensor\'] = doubled_tensor # Dimensions of the transposed tensor if applicable if len(size) >= 2: transposed_tensor = input_tensor.transpose(0, -1) transposed_size = transposed_tensor.size() else: transposed_size = None result[\'transposed_size\'] = transposed_size return result"},{"question":"# Custom Event Loop Policy Implementation Objective Create a custom event loop policy to manage event loops and demonstrate its use by setting and getting different event loops in the context of a multi-threaded application. Description 1. **Define a Custom Event Loop Policy**: Create a class `MyEventLoopPolicy` that subclasses `asyncio.DefaultEventLoopPolicy`. Override the `get_event_loop()` and `set_event_loop()` methods to include logging capabilities (print statements) that indicate when these methods are called. 2. **Set the Custom Policy**: Use the `asyncio.set_event_loop_policy()` function to set `MyEventLoopPolicy` as the current event loop policy. 3. **Multi-threaded Demonstration**: - Create a main function that starts two separate threads. - Each thread should create a new event loop using the `new_event_loop()` method and set it using `set_event_loop()`. - Within each thread, retrieve and print the currently set event loop using `get_event_loop()`. - Ensure the custom logging statements from `MyEventLoopPolicy` provide clear and distinct outputs. 4. **Constraints**: - You should not use any additional libraries other than `asyncio` and `threading`. - Make sure the custom event loop policy never returns `None` from the `get_event_loop()` method. Input and Output Formats - There are no specific input requirements as the task is based on method implementation and demonstration. - The output should demonstrate the correct setting and retrieval of event loops in multiple threads with appropriate logging from the custom event loop policy. Example of Expected Output ``` Thread 1: Event loop set: <asyncio.unix_events._UnixSelectorEventLoop object at 0x...> Thread 1: Event loop retrieved: <asyncio.unix_events._UnixSelectorEventLoop object at 0x...> Thread 2: Event loop set: <asyncio.unix_events._UnixSelectorEventLoop object at 0x...> Thread 2: Event loop retrieved: <asyncio.unix_events._UnixSelectorEventLoop object at 0x...> ``` Additional Information - This exercise helps assess understanding of custom event loop policies and threading in Python\'s `asyncio` library.","solution":"import asyncio import threading class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() print(f\\"get_event_loop called: {loop}\\") return loop def set_event_loop(self, loop): print(f\\"set_event_loop called: {loop}\\") super().set_event_loop(loop) asyncio.set_event_loop_policy(MyEventLoopPolicy()) def thread_function(thread_name): loop = asyncio.new_event_loop() asyncio.set_event_loop(loop) print(f\\"{thread_name}: Event loop set: {loop}\\") retrieved_loop = asyncio.get_event_loop() print(f\\"{thread_name}: Event loop retrieved: {retrieved_loop}\\") def main(): thread1 = threading.Thread(target=thread_function, args=(\\"Thread 1\\",)) thread2 = threading.Thread(target=thread_function, args=(\\"Thread 2\\",)) thread1.start() thread2.start() thread1.join() thread2.join() if __name__ == \\"__main__\\": main()"},{"question":"# **Python Coding Assessment: Advanced SQLite Operations** **Objective:** Demonstrate your understanding of the `sqlite3` module by performing complex database operations, including custom type adaptation, transaction control, and secure query execution. **Problem Statement:** You are given a task to implement a simple database system to manage an inventory of electronic devices. Each device has a name, category, quantity, and price. Furthermore, you will define a custom data type to represent the price in a specific format and use it in your database operations. Implement the following functions: 1. **connect_to_db(db_name: str) -> sqlite3.Connection** - Connect to the SQLite database with the given name. - Create a table `inventory` with columns: `name` TEXT, `category` TEXT, `quantity` INTEGER, and `price` TEXT. - Commit changes and return the connection object. 2. **insert_device(con: sqlite3.Connection, device_data: Tuple[str, str, int, str]) -> None** - Insert a new device into the `inventory` table. - Use placeholders to securely bind values into the SQL query. 3. **fetch_all_devices(con: sqlite3.Connection) -> List[Tuple[str, str, int, str]]** - Fetch all rows from the `inventory` table. - Return data as a list of tuples. 4. **update_quantity(con: sqlite3.Connection, name: str, new_quantity: int) -> None** - Update the quantity of the device with the given name. - Use placeholders for query parameters. 5. **delete_device(con: sqlite3.Connection, name: str) -> None** - Delete the device with the given name from the `inventory` table. - Use placeholders for query parameters. 6. **define_custom_price_type(con: sqlite3.Connection) -> None** - Define a custom type to convert the `price` column values to and from a specific format. The `Price` type should be in the format `\\"CURRENCY AMOUNT\\"`, e.g., `\\"USD 100.00\\"`. - Implement and register an adapter and a converter for the `Price` type. 7. **insert_device_with_custom_price(con: sqlite3.Connection, device_data: Tuple[str, str, int, Tuple[str, float]]) -> None** - Insert a new device into the `inventory` table using a custom `Price` type. You must follow these constraints: - Use SQLite transactions (`commit()` and `rollback()` methods) to ensure data integrity. - Use `executemany()` for batch inserts. - Use a connection context manager to handle transactions automatically where applicable. - Implement secure query execution using placeholders to prevent SQL injection. **Input/Output Examples:** ```python # Example usage: con = connect_to_db(\\"devices.db\\") # Insert devices insert_device(con, (\\"Smartphone\\", \\"Gadgets\\", 50, \\"USD 799.99\\")) insert_device(con, (\\"Laptop\\", \\"Computers\\", 30, \\"USD 1200.00\\")) # Fetch all devices print(fetch_all_devices(con)) # Expected Output: [(\\"Smartphone\\", \\"Gadgets\\", 50, \\"USD 799.99\\"), (\\"Laptop\\", \\"Computers\\", 30, \\"USD 1200.00\\")] # Update quantity update_quantity(con, \\"Smartphone\\", 45) print(fetch_all_devices(con)) # Expected Output: [(\\"Smartphone\\", \\"Gadgets\\", 45, \\"USD 799.99\\"), (\\"Laptop\\", \\"Computers\\", 30, \\"USD 1200.00\\")] # Delete a device delete_device(con, \\"Laptop\\") print(fetch_all_devices(con)) # Expected Output: [(\\"Smartphone\\", \\"Gadgets\\", 45, \\"USD 799.99\\")] # Define and use custom price type define_custom_price_type(con) insert_device_with_custom_price(con, (\\"Tablet\\", \\"Gadgets\\", 20, (\\"USD\\", 500.00))) print(fetch_all_devices(con)) # Expected Output: [(\\"Smartphone\\", \\"Gadgets\\", 45, \\"USD 799.99\\"), (\\"Tablet\\", \\"Gadgets\\", 20, \\"USD 500.00\\")] con.close() ``` **Note:** - Ensure that you handle exceptions appropriately, especially when dealing with database connections and transactions. - Make sure to close the database connection at the end to avoid any resource leaks.","solution":"import sqlite3 from typing import Tuple, List def connect_to_db(db_name: str) -> sqlite3.Connection: con = sqlite3.connect(db_name) cur = con.cursor() cur.execute(\'\'\'CREATE TABLE IF NOT EXISTS inventory ( name TEXT, category TEXT, quantity INTEGER, price TEXT)\'\'\') con.commit() return con def insert_device(con: sqlite3.Connection, device_data: Tuple[str, str, int, str]) -> None: cur = con.cursor() cur.execute(\\"INSERT INTO inventory (name, category, quantity, price) VALUES (?, ?, ?, ?)\\", device_data) con.commit() def fetch_all_devices(con: sqlite3.Connection) -> List[Tuple[str, str, int, str]]: cur = con.cursor() cur.execute(\\"SELECT * FROM inventory\\") return cur.fetchall() def update_quantity(con: sqlite3.Connection, name: str, new_quantity: int) -> None: cur = con.cursor() cur.execute(\\"UPDATE inventory SET quantity = ? WHERE name = ?\\", (new_quantity, name)) con.commit() def delete_device(con: sqlite3.Connection, name: str) -> None: cur = con.cursor() cur.execute(\\"DELETE FROM inventory WHERE name = ?\\", (name,)) con.commit() def define_custom_price_type(con: sqlite3.Connection) -> None: class Price(str): pass def adapt_price(price: Price) -> str: return str(price) def convert_price(text: str) -> Price: return Price(text) sqlite3.register_adapter(Price, adapt_price) sqlite3.register_converter(\\"Price\\", convert_price) def insert_device_with_custom_price(con: sqlite3.Connection, device_data: Tuple[str, str, int, Tuple[str, float]]) -> None: price = f\\"{device_data[3][0]} {device_data[3][1]:.2f}\\" insert_device(con, (device_data[0], device_data[1], device_data[2], price))"},{"question":"Objective Implement a class that mimics a simplified version of Python\'s reference counting mechanism for custom objects. This class should manage the reference counts and handle object allocation and deallocation accordingly. Requirements 1. **Class Name**: `RefCounter` 2. **Attributes**: - `counter` (dictionary to maintain the reference counts of objects) 3. **Methods**: - `inc_ref(self, obj)`: Increases the reference count of the object and records it in the `counter`. - `dec_ref(self, obj)`: Decreases the reference count of the object. If the reference count reaches zero, delete the object and remove it from the `counter`. - `get_count(self, obj)`: Returns the current reference count of the object. 4. The class should handle cases where the object may or may not be present in the `counter`. Constraints - The reference count for an object should never be negative. - Deallocate (remove) the object when its reference count drops to zero. - Implement safety checks to ensure objects or reference counts are consistently managed. Input and Output - There is no specific input and output for the methods as they manipulate internal state. - Ensure the methods update the `counter` dictionary and modify the reference counts accordingly. Example Usage ```python class RefCounter: def __init__(self): self.counter = {} def inc_ref(self, obj): # Implement this method to increase the reference count of obj def dec_ref(self, obj): # Implement this method to decrease the reference count of obj def get_count(self, obj): # Implement this method to get the reference count of obj # Example usage rc = RefCounter() obj = object() # Increase reference count rc.inc_ref(obj) print(rc.get_count(obj)) # Output should be 1 # Increase reference count again rc.inc_ref(obj) print(rc.get_count(obj)) # Output should be 2 # Decrease reference count rc.dec_ref(obj) print(rc.get_count(obj)) # Output should be 1 # Decrease reference count to 0 (this should remove the obj) rc.dec_ref(obj) print(rc.get_count(obj)) # Output should be 0 or an indication that obj is no longer tracked ``` Implement the `RefCounter` class with the described specifications.","solution":"class RefCounter: def __init__(self): self.counter = {} def inc_ref(self, obj): if obj in self.counter: self.counter[obj] += 1 else: self.counter[obj] = 1 def dec_ref(self, obj): if obj in self.counter: if self.counter[obj] > 1: self.counter[obj] -= 1 else: del self.counter[obj] def get_count(self, obj): return self.counter.get(obj, 0)"},{"question":"# Custom Network Server Implementation **Objective:** Create a custom, multi-threaded TCP server that simulates a basic key-value store. Clients can connect to this server to set, get, and delete keys and their corresponding values. # Requirements: 1. Implement a class `KeyValueRequestHandler` derived from `socketserver.BaseRequestHandler` which: - Processes the following commands: - `SET <key> <value>`: Stores the key-value pair. - `GET <key>`: Retrieves the value for the given key. - `DELETE <key>`: Deletes the key-value pair. - Validates the commands and responds with appropriate messages to the client. 2. Implement a class `KeyValueTCPServer` using `socketserver.ThreadingMixIn` and `socketserver.TCPServer` to manage concurrent connections. 3. Ensure thread safety for the key-value store using appropriate locking mechanisms. 4. Write a client script to test your server. The client should: - Connect to the server. - Perform a series of `SET`, `GET`, and `DELETE` operations. - Print server responses. # Input and Output Formats: Server: - **Input:** Commands from clients in the form: - `SET <key> <value>` - `GET <key>` - `DELETE <key>` - **Output:** Responses to the client: - `OK` for successful `SET` and `DELETE` operations. - The value or `ERROR: Key not found` for `GET` operations. - `ERROR: Invalid command` for any unrecognizable command. Client: - The client script should demonstrate various command sequences and print the server responses to the console. # Constraints: - The server should support multiple concurrent client connections. - The server should maintain consistent state using thread synchronization techniques. # Performance Requirements: - The server must handle at least 10 concurrent connections efficiently. - The key-value store must support a reasonably fast response time for commands under normal load. # Example: Server (Implementation): ```python import socketserver import threading class KeyValueRequestHandler(socketserver.BaseRequestHandler): store = {} lock = threading.Lock() def handle(self): self.data = self.request.recv(1024).strip().decode(\'utf-8\') command_parts = self.data.split() response = self.execute_command(command_parts) self.request.sendall(response.encode(\'utf-8\')) def execute_command(self, command_parts): cmd = command_parts[0].upper() if cmd == \'SET\' and len(command_parts) == 3: return self.set_value(command_parts[1], command_parts[2]) elif cmd == \'GET\' and len(command_parts) == 2: return self.get_value(command_parts[1]) elif cmd == \'DELETE\' and len(command_parts) == 2: return self.delete_value(command_parts[1]) else: return \'ERROR: Invalid command\' def set_value(self, key, value): with KeyValueRequestHandler.lock: KeyValueRequestHandler.store[key] = value return \'OK\' def get_value(self, key): with KeyValueRequestHandler.lock: if key in KeyValueRequestHandler.store: return KeyValueRequestHandler.store[key] else: return \'ERROR: Key not found\' def delete_value(self, key): with KeyValueRequestHandler.lock: if key in KeyValueRequestHandler.store: del KeyValueRequestHandler.store[key] return \'OK\' else: return \'ERROR: Key not found\' class KeyValueTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 with KeyValueTCPServer((HOST, PORT), KeyValueRequestHandler) as server: print(\\"Server started...\\") server.serve_forever() ``` Client (Implementation): ```python import socket def send_command(command, host=\\"localhost\\", port=9999): with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock: sock.connect((host, port)) sock.sendall(command.encode(\'utf-8\')) response = str(sock.recv(1024), \'utf-8\') print(f\'Sent: {command}, Received: {response}\') if __name__ == \\"__main__\\": send_command(\\"SET foo bar\\") send_command(\\"GET foo\\") send_command(\\"DELETE foo\\") send_command(\\"GET foo\\") ``` # Guidelines: - Ensure to test thoroughly to handle edge cases and concurrent operations. - Add necessary comments to explain your code. - Handle exceptions gracefully and ensure that the server does not crash on bad requests.","solution":"import socketserver import threading class KeyValueRequestHandler(socketserver.BaseRequestHandler): store = {} lock = threading.Lock() def handle(self): self.data = self.request.recv(1024).strip().decode(\'utf-8\') command_parts = self.data.split() response = self.execute_command(command_parts) self.request.sendall(response.encode(\'utf-8\')) def execute_command(self, command_parts): cmd = command_parts[0].upper() if cmd == \'SET\' and len(command_parts) == 3: return self.set_value(command_parts[1], command_parts[2]) elif cmd == \'GET\' and len(command_parts) == 2: return self.get_value(command_parts[1]) elif cmd == \'DELETE\' and len(command_parts) == 2: return self.delete_value(command_parts[1]) else: return \'ERROR: Invalid command\' def set_value(self, key, value): with KeyValueRequestHandler.lock: KeyValueRequestHandler.store[key] = value return \'OK\' def get_value(self, key): with KeyValueRequestHandler.lock: if key in KeyValueRequestHandler.store: return KeyValueRequestHandler.store[key] else: return \'ERROR: Key not found\' def delete_value(self, key): with KeyValueRequestHandler.lock: if key in KeyValueRequestHandler.store: del KeyValueRequestHandler.store[key] return \'OK\' else: return \'ERROR: Key not found\' class KeyValueTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 with KeyValueTCPServer((HOST, PORT), KeyValueRequestHandler) as server: print(\\"Server started...\\") server.serve_forever()"},{"question":"**Exception Handling and Custom Exceptions in Python** You are tasked with creating a function that processes data from a list of dictionaries containing information about various users. The function should handle and report different types of exceptions gracefully and create a custom exception for invalid data format. Additionally, it should demonstrate the use of exception chaining. # Task: 1. Implement the function `process_user_data(user_data)` that takes a list of dictionaries and performs the following: - For each user dictionary, attempt to extract the keys `\\"name\\"`, `\\"age\\"`, and `\\"email\\"`. - If a key is missing, raise a `KeyError`. - If the `\\"age\\"` value is not an integer, raise a `ValueError`. - If the `\\"email\\"` value doesn\'t contain an \\"@\\" character, raise a `ValueError`. 2. Create a custom exception class `InvalidUserDataError` inheriting from `Exception`. This exception should be raised when an error occurs due to invalid data format, and it should store information about the invalid data and the specific error encountered. 3. Modify the `process_user_data` function to catch `KeyError` and `ValueError`, chain them to an instance of `InvalidUserDataError`, and raise this custom exception. 4. In the main part of your script, call `process_user_data` with a sample list of dictionaries and use a try-except block to catch and print information about the `InvalidUserDataError`. # Input: - A list of dictionaries where each dictionary represents user data with the following keys: `\\"name\\"`, `\\"age\\"`, and `\\"email\\"`. # Output: - The function should not return any value, but it should handle errors as described and raise an appropriate `InvalidUserDataError` when necessary. # Example: ```python class InvalidUserDataError(Exception): def __init__(self, user_data, error): super().__init__(str(error)) self.user_data = user_data self.error = error def process_user_data(user_data): for user in user_data: try: name = user[\'name\'] age = user[\'age\'] email = user[\'email\'] if not isinstance(age, int): raise ValueError(f\\"Age must be an integer, got {type(age).__name__}.\\") if \'@\' not in email: raise ValueError(f\\"Invalid email format: {email}\\") except KeyError as e: raise InvalidUserDataError(user, e) from e except ValueError as e: raise InvalidUserDataError(user, e) from e # Example usage: try: sample_data = [ {\'name\': \'Alice\', \'age\': 30, \'email\': \'alice@example.com\'}, {\'name\': \'Bob\', \'age\': \'twenty-five\', \'email\': \'bob.example.com\'}, {\'name\': \'Charlie\', \'email\': \'charlie@example.com\'} ] process_user_data(sample_data) except InvalidUserDataError as e: print(f\\"Invalid user data encountered: {e.user_data}\\") print(f\\"Error: {e.error}\\") ``` # Constraints: - Each dictionary is expected to have the keys `\\"name\\"`, `\\"age\\"`, and `\\"email\\"`. - The `\\"age\\"` key should correspond to an integer value. - The `\\"email\\"` key should contain an email address with an \\"@\\" character. Make sure to handle exceptions as described and demonstrate proper chaining of exceptions in your implementation.","solution":"class InvalidUserDataError(Exception): def __init__(self, user_data, error): super().__init__(str(error)) self.user_data = user_data self.error = error def process_user_data(user_data): for user in user_data: try: name = user[\'name\'] age = user[\'age\'] email = user[\'email\'] if not isinstance(age, int): raise ValueError(f\\"Age must be an integer, got {type(age).__name__}.\\") if \'@\' not in email: raise ValueError(f\\"Invalid email format: {email}\\") except KeyError as e: raise InvalidUserDataError(user, e) from e except ValueError as e: raise InvalidUserDataError(user, e) from e # Example usage: try: sample_data = [ {\'name\': \'Alice\', \'age\': 30, \'email\': \'alice@example.com\'}, {\'name\': \'Bob\', \'age\': \'twenty-five\', \'email\': \'bob.example.com\'}, {\'name\': \'Charlie\', \'email\': \'charlie@example.com\'} ] process_user_data(sample_data) except InvalidUserDataError as e: print(f\\"Invalid user data encountered: {e.user_data}\\") print(f\\"Error: {e.error}\\")"},{"question":"# Custom Exception Handling and Chaining in Python Problem Statement You have been asked to implement a file reader utility that reads data from a specified file and processes it. The utility needs to handle various exceptions appropriately using custom exceptions and also demonstrate exception chaining. Implement the following functions and classes: 1. **Custom Exceptions**: - `FileError`: Base class for file-related exceptions. - `FileReadError`: Raised when an error occurs while reading the file. - `FileFormatError`: Raised when the file format is invalid. 2. **Function**: `read_and_process_file(filepath: str) -> Any` - Reads a file from the given `filepath`. - If the file does not exist, it should raise a `FileReadError` with an appropriate error message. - If the file format is invalid (in this task, we consider a valid file to have more than 3 lines), it should raise a `FileFormatError`. - Use exception chaining to maintain context when re-raising exceptions. - If reading and processing the file is successful, return the content of the file. Example Usage ```python try: content = read_and_process_file(\'example.txt\') print(content) except FileReadError as e: print(f\\"File read error: {e}\\") except FileFormatError as e: print(f\\"File format error: {e}\\") except FileError as e: print(f\\"General file error: {e}\\") ``` Input - `filepath`: A string representing the path to a file. Output - Returns the content of the file if it is read and processed successfully. Constraints - You may assume the file contains text content. - Do not suppress any exceptions; handle them by raising appropriate custom exceptions. # Evaluation Criteria - Correct creation and usage of custom exceptions. - Proper handling and chaining of exceptions. - Adherence to the problem constraints. - Readability and clarity of the code. ```python # Define custom exceptions based on the provided requirements class FileError(Exception): Base class for file-related exceptions pass class FileReadError(FileError): Raised when an error occurs while reading the file pass class FileFormatError(FileError): Raised when the file format is invalid pass def read_and_process_file(filepath: str) -> Any: try: # Try to open and read the file with open(filepath, \'r\') as file: content = file.readlines() if len(content) <= 3: # Raise FileFormatError if the file has 3 or fewer lines raise FileFormatError(\\"File format is invalid: File has 3 or fewer lines.\\") return content except FileNotFoundError as e: # Raise FileReadError if the file does not exist, and chain the original exception raise FileReadError(f\\"Failed to read file: {filepath}\\") from e except FileFormatError as e: # Raise FileFormatError if the file format is invalid, and chain the original exception raise FileFormatError(f\\"File format error in file: {filepath}\\") from e except Exception as e: # Raise a general FileError for any other exceptions raise FileError(\\"An unexpected error occurred.\\") from e # Example usage try: content = read_and_process_file(\'example.txt\') print(content) except FileReadError as e: print(f\\"File read error: {e}\\") except FileFormatError as e: print(f\\"File format error: {e}\\") except FileError as e: print(f\\"General file error: {e}\\") ```","solution":"class FileError(Exception): Base class for file-related exceptions. pass class FileReadError(FileError): Raised when an error occurs while reading the file. pass class FileFormatError(FileError): Raised when the file format is invalid. pass def read_and_process_file(filepath: str) -> list: Reads a file from the given `filepath` and checks its format. Parameters: filepath (str): The path of the file to read. Returns: list: Contents of the file. Raises: FileReadError: If the file does not exist or cannot be read. FileFormatError: If the file format is invalid. try: with open(filepath, \'r\') as file: content = file.readlines() if len(content) <= 3: raise FileFormatError(\\"File format is invalid: File has 3 or fewer lines.\\") return content except FileNotFoundError as e: raise FileReadError(f\\"Failed to read file: {filepath}\\") from e except FileFormatError as e: raise e except Exception as e: raise FileError(\\"An unexpected error occurred.\\") from e"},{"question":"# Custom Python Initializer in Isolated Mode In this task, you will implement a Python function using the provided C-based Python API concepts. Your function will simulate initializing a customized Python environment in isolated mode. You will configure this environment by setting up various parameters and ensuring proper handling of status and configuration structures. Task Create a Python function `simulate_python_initializer` that mimics the following steps: 1. Preinitialize Python in isolated mode. 2. Set specific configurations: - Enable the Python UTF-8 mode. - Append a custom module search path. - Set a specific program name. 3. Initialize Python from the configuration. 4. Handle any initialization exceptions and clean up configurations. Input - `program_name`: A string representing the name of the program. - `custom_search_path`: A list of strings representing custom module search paths to be appended. Output - A string summarizing the initialized configurations or any errors encountered during initialization. Constraints - You should use the conceptual mappings of the C API functions to Python constructs as provided in the documentation. - Simulate the C structures (`PyConfig`, `PyPreConfig`, `PyStatus`). - Handle memory management and status checking appropriately. Example ```python def simulate_python_initializer(program_name, custom_search_path): # Your implementation here. # Example usage program_name = \\"my_custom_python\\" custom_search_path = [\\"/path/to/custom/module1\\", \\"/path/to/custom/module2\\"] result = simulate_python_initializer(program_name, custom_search_path) print(result) ``` Expected Output: ```plaintext Initialization successful: - Program name: my_custom_python - UTF-8 mode: Enabled - Custom module search paths: [\\"/path/to/custom/module1\\", \\"/path/to/custom/module2\\"] ``` **Note**: This is an example template, your actual implementation needs to simulate using the provided documentation functionalities and configurations.","solution":"def simulate_python_initializer(program_name, custom_search_path): Simulate the initializer for a customized Python environment in isolated mode. :param program_name: str, the name of the program. :param custom_search_path: list of str, custom module search paths to be appended. :return: str, summary of the initialized configuration or errors encountered. try: # Simulate Preinitialization pre_config = {\'isolated\': True, \'utf8_mode\': True} # Simulate Configuration config = { \'program_name\': program_name, \'module_search_paths\': custom_search_path, \'utf8_mode\': pre_config[\'utf8_mode\'] } # Simulate successful initialization result = ( f\\"Initialization successful:n\\" f\\"- Program name: {config[\'program_name\']}n\\" f\\"- UTF-8 mode: {\'Enabled\' if config[\'utf8_mode\'] else \'Disabled\'}n\\" f\\"- Custom module search paths: {config[\'module_search_paths\']}\\" ) return result except Exception as e: # Handle initialization exception and clean up configurations return f\\"Initialization failed with error: {str(e)}\\""},{"question":"Objective: Implement a PyTorch model utilizing `torch.cond` to demonstrate the ability to perform different operations based on the sum of the tensor elements and to implement a more complex example involving nested conditional operations. Problem Statement: You are tasked with creating a PyTorch module that branches logic based on the sum of elements in an input tensor. Additionally, implement nested conditional operations within the same model. Your module should extend `torch.nn.Module`. Requirements: 1. **Main Module**: - Implement a class `NestedConditionalModule` that extends `torch.nn.Module`. - It takes a tensor `x` as input and checks: 1. If the sum of the elements in `x` is greater than a threshold `threshold_main` (default to 5.0), execute a secondary conditional operation. 2. If the sum is less than or equal to `threshold_main`, apply the sine function to each element of the tensor. 2. **Secondary Conditional Operation**: - If the sum of `x` is greater than `threshold_main`, perform another conditional check: - If the mean of the elements in `x` is greater than a `threshold_secondary` (default to 1.0), the true function should return the cosine of each element in `x`. - Otherwise, the false function should return the exponential of each element in `x`. 3. **Utility functions**: - Define the true and false functions for both main and secondary conditional operations as per the specifications. 4. **Method Signature**: ```python class NestedConditionalModule(torch.nn.Module): def __init__(self, threshold_main: float = 5.0, threshold_secondary: float = 1.0): # Initialize the thresholds and the parent module. def forward(self, x: torch.Tensor) -> torch.Tensor: # Implement the conditional checks and operations as described. ``` Example Usage: ```python import torch # Initialize the module model = NestedConditionalModule(threshold_main=5.0, threshold_secondary=1.0) # Create sample inputs input_tensor1 = torch.tensor([2.0, 3.0, 1.5]) # Sum is 6.5. Mean is 2.1667 - Apply nested condition input_tensor2 = torch.tensor([1.0, 2.0, 1.0]) # Sum is 4 - Apply sine function # Run the model output1 = model(input_tensor1) # Expecting the cosine of input_tensor1 output2 = model(input_tensor2) # Expecting the sine of input_tensor2 print(output1) print(output2) ``` Constraints: - You may assume the input tensor will always be a 1D tensor of floats. - Focus on correctness and clarity of the implementation, as well as proper usage of `torch.cond`. Note: - Your implementation should use `torch.cond` as illustrated in the provided documentation. - Make sure to handle potential edge cases, such as tensors with all zeros.","solution":"import torch import torch.nn.functional as F class NestedConditionalModule(torch.nn.Module): def __init__(self, threshold_main: float = 5.0, threshold_secondary: float = 1.0): super(NestedConditionalModule, self).__init__() self.threshold_main = threshold_main self.threshold_secondary = threshold_secondary def main_condition_true_fn(self, x): # Nested condition mean_x = torch.mean(x) if mean_x > self.threshold_secondary: return torch.cos(x) else: return torch.exp(x) def main_condition_false_fn(self, x): return torch.sin(x) def forward(self, x: torch.Tensor) -> torch.Tensor: sum_x = torch.sum(x) if sum_x > self.threshold_main: return self.main_condition_true_fn(x) else: return self.main_condition_false_fn(x)"},{"question":"Background You are tasked with creating a program that demonstrates your ability to persist Python objects using the `pickle` module and interact with a SQLite database using the `sqlite3` module. Your program should involve serializing objects to save the application state and storing these serialized objects in an SQLite database for later retrieval. Problem Statement Create a class `Person` that represents a person with the following attributes: - `name` (str): The name of the person - `age` (int): The age of the person - `email` (str): The email address of the person Your task is to implement the following functions: 1. `serialize_person(person: Person) -> bytes`: This function should take a `Person` object and return its serialized byte stream using the `pickle` module. 2. `deserialize_person(data: bytes) -> Person`: This function should take a byte stream and return the deserialized `Person` object using the `pickle` module. 3. `save_person_to_db(conn: sqlite3.Connection, person: Person) -> None`: This function should serialize the provided `Person` object and store it in an SQLite database. The database table should be named `people` and contain two columns - `id` (INTEGER PRIMARY KEY) and `person_data` (BLOB). 4. `load_person_from_db(conn: sqlite3.Connection, person_id: int) -> Person`: This function should retrieve the serialized `Person` data from the database using the provided `person_id`, deserialize it, and return the `Person` object. Input Format - The class `Person` should be defined as specified. - The functions should accept the specified inputs: - `serialize_person` function accepts a `Person` object. - `deserialize_person` function accepts a byte stream. - `save_person_to_db` function accepts an SQLite connection object and a `Person` object. - `load_person_from_db` function accepts an SQLite connection object and an integer representing the `person_id`. Output Format - The `serialize_person` function should return a byte stream. - The `deserialize_person` function should return a `Person` object. - The `save_person_to_db` function should store the serialized `Person` object in the SQLite database (`None` return type). - The `load_person_from_db` function should return a `Person` object retrieved from the database. Constraints - Assume that the `Person` class will only contain the attributes mentioned. - Ensure the connection to the SQLite database is handled correctly and exceptions are managed appropriately. - The database schema should be created if it does not already exist. Example Usage ```python import sqlite3 from your_module import Person, serialize_person, deserialize_person, save_person_to_db, load_person_from_db # Create a new person person = Person(name=\\"John Doe\\", age=30, email=\\"john.doe@example.com\\") # Serialize the person serialized_data = serialize_person(person) print(serialized_data) # Deserialize the person deserialized_person = deserialize_person(serialized_data) print(deserialized_person.name, deserialized_person.age, deserialized_person.email) # Save the person to the database conn = sqlite3.connect(\'people.db\') save_person_to_db(conn, person) # Load the person from the database loaded_person = load_person_from_db(conn, 1) print(loaded_person.name, loaded_person.age, loaded_person.email) ``` In the example, replace `your_module` with the name of the module where the class and functions are implemented.","solution":"import pickle import sqlite3 class Person: def __init__(self, name: str, age: int, email: str): self.name = name self.age = age self.email = email def serialize_person(person: Person) -> bytes: Serializes a Person object to a byte stream using pickle. return pickle.dumps(person) def deserialize_person(data: bytes) -> Person: Deserializes a byte stream to a Person object using pickle. return pickle.loads(data) def save_person_to_db(conn: sqlite3.Connection, person: Person) -> None: Serializes a Person object and stores it in the SQLite database. # Serialize the person object person_data = serialize_person(person) # Create the table if it does not exist conn.execute(\'\'\'CREATE TABLE IF NOT EXISTS people ( id INTEGER PRIMARY KEY, person_data BLOB NOT NULL)\'\'\') # Insert the serialized person object into the table conn.execute(\'INSERT INTO people (person_data) VALUES (?)\', (person_data,)) # Commit changes conn.commit() def load_person_from_db(conn: sqlite3.Connection, person_id: int) -> Person: Retrieves the serialized Person object from the SQLite database using the provided person_id, deserializes it, and returns the Person object. cursor = conn.execute(\'SELECT person_data FROM people WHERE id = ?\', (person_id,)) row = cursor.fetchone() if row is None: raise ValueError(f\\"No person found with id {person_id}\\") person_data = row[0] return deserialize_person(person_data)"},{"question":"Objective Create a function `custom_slice` that behaves similarly to Python\'s built-in slicing but includes additional custom error handling and index adjustments. The function should be able to: - Create a slice object from given start, stop, and step values. - Handle out-of-bounds errors by clipping the indices to valid ranges. - Ensure that the result slice\'s length does not exceed the maximum possible value for Python\'s Py_ssize_t type (usually corresponding to `sys.maxsize`). Function Signature ```python def custom_slice(start, stop, step, seq_length): Creates and adjusts a slice object based on given parameters and sequence length. Args: start (int or None): The starting index of the slice. stop (int or None): The stopping index of the slice. step (int or None): The step value of the slice. seq_length (int): The length of the sequence to be sliced. Returns: tuple: (adjusted_start, adjusted_stop, adjusted_step, slice_length) Raises: ValueError: If the indices are invalid or irreparable. pass ``` Input - `start` (int or None): The starting index of the slice. If `None`, it should default to `0`. - `stop` (int or None): The stopping index of the slice. If `None`, it should default to the length of the sequence. - `step` (int or None): The step value of the slice. If `None`, it should default to `1`. - `seq_length` (int): The length of the sequence to be sliced. This should be a positive integer. Output - A tuple of four elements: `(adjusted_start, adjusted_stop, adjusted_step, slice_length)` - `adjusted_start` (int): The adjusted starting index of the slice. - `adjusted_stop` (int): The adjusted stopping index of the slice. - `adjusted_step` (int): The step value of the slice. - `slice_length` (int): The length of the resulting slice. Constraints and Notes - The function should properly handle the default values for each of the slice parameters if they are `None`. - Out-of-bounds indices must be clipped to valid ranges. - `seq_length` will always be greater than zero. - If the step value is zero or the combination of start, stop, and step values are invalid (e.g., step value causing an infinite loop), raise a `ValueError`. - The implementation should consider performance aspects. Example ```python # Example 1 # Given the following input parameters start = 2 stop = 10 step = 3 seq_length = 15 # The resulting tuple should be (2, 10, 3, 3) # Example 2 # Given the following input parameters start = -5 stop = 20 step = -2 seq_length = 10 # The resulting tuple should be (0, 0, -2, 0) # as the step value creates an invalid range ``` Notes - The function should utilize Python\'s built-in slicing behavior and adjust indices following similar rules. - Familiarity with the slice type and handling custom slicing in Python will be necessary to implement this function correctly.","solution":"def custom_slice(start, stop, step, seq_length): Creates and adjusts a slice object based on given parameters and sequence length. Args: start (int or None): The starting index of the slice. stop (int or None): The stopping index of the slice. step (int or None): The step value of the slice. seq_length (int): The length of the sequence to be sliced. Returns: tuple: (adjusted_start, adjusted_stop, adjusted_step, slice_length) Raises: ValueError: If the indices are invalid or irreparable. if seq_length <= 0: raise ValueError(\\"Sequence length must be greater than zero.\\") start = 0 if start is None else start stop = seq_length if stop is None else stop step = 1 if step is None else step if step == 0: raise ValueError(\\"Step value cannot be zero.\\") # Adjust negative indices if start < 0: start = max(0, seq_length + start) if stop < 0: stop = max(0, seq_length + stop) # Clip out-of-bounds indices start = min(max(start, 0), seq_length) stop = min(max(stop, 0), seq_length) # Calculate slice length if step > 0 and start >= stop: slice_length = 0 elif step < 0 and start <= stop: slice_length = 0 else: slice_length = max(0, (stop - start + (step - 1)) // step) if step > 0 else max(0, (start - stop - (step + 1)) // -step) return (start, stop, step, slice_length)"},{"question":"Objective Your task is to write a Python function using the seaborn library to generate and visualize a sequential color palette based on various inputs. You will also create a histogram using this generated palette to demonstrate your understanding of seaborn’s palette integration with its plotting functions. Function Specification **Function Name:** `generate_palette_and_histogram` **Input:** - `color`: A string representing the color. This can be a named color (e.g., \\"seagreen\\"), a hex code (e.g., \\"#79C\\"), or a tuple in the husl system (e.g., (20, 60, 50)). - `num_colors` *(optional)*: An integer representing the number of colors in the palette. Default is 6. - `as_cmap` *(optional)*: A boolean indicating whether to return a continuous colormap. Default is `False`. - `data`: A list of numerical values to plot using the generated palette. **Output:** - A sequential color palette generated using seaborn. - A histogram of the provided data visualized using the generated palette. Constraints - `num_colors` is an integer between 2 and 20 inclusive. - `data` list should contain at least 10 numerical values. Detailed Requirements 1. **Generate the Palette:** - Create a sequential color palette using seaborn’s `light_palette` function based on the provided `color`. - If `num_colors` is not provided, default to 6 colors. - If `as_cmap` is `True`, return a continuous colormap. 2. **Visualize Histogram:** - Use seaborn’s `histplot` function to create a histogram of the provided `data`. - Use the generated palette (or colormap if `as_cmap` is `True`) to color the histogram bars. 3. **Edge Case Handling:** - If `num_colors` is outside of the specified range, raise a `ValueError`. - Ensure `data` list has the required number of numerical values. Example ```python def generate_palette_and_histogram(color, data, num_colors=6, as_cmap=False): import seaborn as sns import matplotlib.pyplot as plt # Validate input if not (2 <= num_colors <= 20): raise ValueError(\\"num_colors must be between 2 and 20 inclusive.\\") if len(data) < 10: raise ValueError(\\"data list must contain at least 10 numerical values.\\") # Generate palette palette = sns.light_palette(color, num_colors, input=\\"husl\\") if isinstance(color, tuple) else sns.light_palette(color, num_colors, as_cmap=as_cmap) # Plotting sns.histplot(data, palette=palette) plt.show() # Example usage: color = \\"seagreen\\" data = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5] generate_palette_and_histogram(color, data, num_colors=8, as_cmap=False) ``` This question assesses the student\'s ability to utilize seaborn for creating color palettes, error handling, and data visualization integration. Ensure to test the function with various `color` inputs (named colors, hex codes, and husl tuples) and different `num_colors` values.","solution":"def generate_palette_and_histogram(color, data, num_colors=6, as_cmap=False): Generates a sequential color palette using seaborn and visualizes a histogram of the provided data using this palette. Args: color (str or tuple): The base color for the palette. This can be a named color, a hex code, or a husl tuple. data (list of int or float): The data to plot using the generated palette. Must contain at least 10 numerical values. num_colors (int, optional): The number of colors in the palette. Default is 6. Must be between 2 and 20 inclusive. as_cmap (bool, optional): Whether to return a continuous colormap. Default is False. Raises: ValueError: If num_colors is outside the range [2, 20] or if data contains fewer than 10 values. Returns: None import seaborn as sns import matplotlib.pyplot as plt # Validate input if not (2 <= num_colors <= 20): raise ValueError(\\"num_colors must be between 2 and 20 inclusive.\\") if len(data) < 10: raise ValueError(\\"data list must contain at least 10 numerical values.\\") # Generate palette if isinstance(color, tuple): palette = sns.light_palette(color, n_colors=num_colors, input=\\"husl\\") else: palette = sns.light_palette(color, n_colors=num_colors, as_cmap=as_cmap) # Plotting histogram plt.figure(figsize=(8, 6)) sns.histplot(data, palette=palette if not as_cmap else None, kde=False) plt.show()"},{"question":"Objective Implement a secure communication system using Python 3.10\'s cryptographic services. Your task is to create a function that securely hashes data, authenticates the message using HMAC, and generates a secure token. Task You need to write a function `secure_communication(data: str, key: bytes) -> dict` that performs the following steps: 1. **Secure Hashing**: - Use the `hashlib` module to generate a SHA-256 hash of the input data. 2. **Message Authentication**: - Use the `hmac` module to create a keyed-hash message authentication code (HMAC) using the SHA-256 hash and the provided key. 3. **Generate Secure Token**: - Use the `secrets` module to generate a secure token of 16 bytes. 4. **Return Output**: - The function should return a dictionary containing the `hashed_data` (SHA-256 of the input data), `hmac_signature` (HMAC of the hashed data using the key), and `secure_token` (16 bytes secure token in hex format). Constraints - The input `data` will be a string of maximum length 1024 characters. - The input `key` will be a byte string of maximum length 64 bytes. Example ```python def secure_communication(data: str, key: bytes) -> dict: # Your code here data = \\"Confidential message\\" key = b\\"secret\\" result = secure_communication(data, key) print(result) # Expected Output: # { # \'hashed_data\': \'...\', # \'hmac_signature\': \'...\', # \'secure_token\': \'...\' # } ``` Notes - Ensure you use appropriate functions and methods from the `hashlib`, `hmac`, and `secrets` modules to achieve the desired outcomes. - Pay attention to the performance and security best practices while handling cryptographic operations.","solution":"import hashlib import hmac import secrets def secure_communication(data: str, key: bytes) -> dict: # Generate SHA-256 hash of the input data hashed_data = hashlib.sha256(data.encode()).hexdigest() # Create an HMAC using the hashed data and key hmac_signature = hmac.new(key, hashed_data.encode(), hashlib.sha256).hexdigest() # Generate a secure token of 16 bytes secure_token = secrets.token_hex(16) return { \'hashed_data\': hashed_data, \'hmac_signature\': hmac_signature, \'secure_token\': secure_token }"},{"question":"**Title:** Working with Numeric Arrays in Python **Objective:** You are tasked with implementing a function that performs various operations on an array of numeric values using Python\'s `array` module. This will test your understanding of creating and manipulating arrays effectively. **Problem Statement:** Write a Python function `manipulate_array(operations: List[Tuple[str, Union[str, int, List[int]]]]) -> str` that takes a list of operations to perform on an array and returns the final state of the array as a string. **Input:** - `operations`: A list of tuples, where each tuple represents an operation to be performed on the array. The first element of the tuple is a string representing the operation, and the second element is the parameter for the operation. The operations are described below: 1. **\\"create\\" [typecode, [values]]**: Create a new array with the given type code and initial values. Example: (\\"create\\", (\'i\', [1, 2, 3])) creates an array of signed ints with initial values [1, 2, 3]. 2. **\\"append\\" [value]**: Append an integer value to the array. Example: (\\"append\\", 4) appends 4 to the array. 3. **\\"extend\\" [values]**: Extend the array by appending multiple values. Example: (\\"extend\\", [5, 6]) appends 5 and 6 to the array. 4. **\\"count\\" [value]**: Count the occurrences of the specified value in the array. Example: (\\"count\\", 3) returns the count of 3 in the array. 5. **\\"reverse\\" []**: Reverse the order of elements in the array. Example: (\\"reverse\\", []) reverses the array. 6. **\\"tobytes\\" []**: Convert the array to a bytes representation and return it as part of the result string. Example: (\\"tobytes\\", []) converts the array to bytes. 7. **\\"byteswap\\" []**: Byteswap all items of the array (only applicable to 1, 2, 4, or 8-byte values). Example: (\\"byteswap\\", []) performs a byteswap on the array values. **Output:** - A string representation of the final state of the array, formatted as `\\"array(typecode, [values])\\"`. Additionally, any bytes representation should be included in the result string. **Constraints:** - The array will only contain integer values (consider type codes \'b\', \'B\', \'h\', \'H\', \'i\', \'I\', \'l\', \'L\'). - All type codes mentioned in the operations will be valid as per the documentation. - The order of operations should be preserved. **Example:** ```python from array import array from typing import List, Tuple, Union def manipulate_array(operations: List[Tuple[str, Union[str, int, List[int]]]]) -> str: arr = None result_str = \\"\\" for op in operations: action, param = op if action == \\"create\\": typecode, values = param arr = array(typecode, values) elif action == \\"append\\": arr.append(param) elif action == \\"extend\\": arr.extend(param) elif action == \\"count\\": result_str += f\\"Count of {param}: {arr.count(param)}n\\" elif action == \\"reverse\\": arr.reverse() elif action == \\"tobytes\\": result_str += f\\"Bytes representation: {arr.tobytes()}n\\" elif action == \\"byteswap\\": arr.byteswap() result_str += f\\"Final array: {arr}\\" return result_str # Example usage operations = [ (\\"create\\", (\'i\', [1, 2, 3])), (\\"append\\", 4), (\\"extend\\", [5, 6]), (\\"count\\", 3), (\\"reverse\\", []), (\\"tobytes\\", []), (\\"byteswap\\", []) ] print(manipulate_array(operations)) ``` **Expected Output:** ``` Count of 3: 1 Bytes representation: b\'x04x00x00x00x06x00x00x00x05x00x00x00x04x00x00x00x03x00x00x00x02x00x00x00x01x00x00x00\' Final array: array(\'i\', [67108864, 100663296, 83886080, 67108864, 50331648, 33554432]) ``` The example demonstrates creating an array, appending values, extending it, counting an element, reversing, converting to bytes, and performing a byteswap.","solution":"from array import array from typing import List, Tuple, Union def manipulate_array(operations: List[Tuple[str, Union[str, int, List[int]]]]) -> str: arr = None result_str = \\"\\" for op in operations: action, param = op if action == \\"create\\": typecode, values = param arr = array(typecode, values) elif action == \\"append\\": arr.append(param) elif action == \\"extend\\": arr.extend(param) elif action == \\"count\\": result_str += f\\"Count of {param}: {arr.count(param)}n\\" elif action == \\"reverse\\": arr.reverse() elif action == \\"tobytes\\": result_str += f\\"Bytes representation: {arr.tobytes()}n\\" elif action == \\"byteswap\\": arr.byteswap() result_str += f\\"Final array: {arr}\\" return result_str"},{"question":"**Data Compression and Decompression with `zlib`** You are tasked with creating a Python program that compresses and then decompresses data using the `zlib` module. Your program should be designed to handle different compression levels and verify the integrity of the data after decompression. # Function Specification You need to implement the following functions: 1. **compress_data(data: bytes, level: int) -> bytes** * Compresses the provided data using the specified compression level. * - `data`: A bytes object to be compressed. * - `level`: An integer from `0` to `9`. A value of `1` is fastest but has the least compression, whereas `9` is slowest but has the most compression. The value `0` means no compression. * - Returns a bytes object containing the compressed data. 2. **decompress_data(data: bytes) -> bytes** * Decompresses the provided compressed data. * - `data`: A bytes object containing the compressed data. * - Returns a bytes object containing the decompressed data. 3. **verify_data(original: bytes, decompressed: bytes) -> bool** * Verifies that the original data and the decompressed data are identical. * - `original`: The original bytes object before compression. * - `decompressed`: The bytes object obtained after decompression. * - Returns `True` if the data matches, `False` otherwise. # Requirements 1. **Compression Levels**: You must test your `compress_data` function with compression levels `0`, `1`, `6`, and `9`. 2. **Verification**: Ensure that the decompressed data is the same as the original data by using the `verify_data` function. 3. **Error Handling**: Your implementation should handle any potential errors that may arise during compression and decompression (e.g., invalid data). # Example Usage ```python original_data = b\\"Python data compression with zlib\\" compressed_data = compress_data(original_data, 6) decompressed_data = decompress_data(compressed_data) assert verify_data(original_data, decompressed_data), \\"Data mismatch!\\" ``` # Constraints * The data to be compressed will be a non-empty bytes object. * You need to handle cases where decompression might fail due to invalid input. Implement the functions mentioned above and ensure that they meet the requirements.","solution":"import zlib def compress_data(data: bytes, level: int) -> bytes: Compresses the provided data using the specified compression level. if not (0 <= level <= 9): raise ValueError(\\"Compression level must be between 0 and 9.\\") return zlib.compress(data, level) def decompress_data(data: bytes) -> bytes: Decompresses the provided compressed data. try: return zlib.decompress(data) except zlib.error: raise ValueError(\\"Decompression failed due to invalid data.\\") def verify_data(original: bytes, decompressed: bytes) -> bool: Verifies that the original data and the decompressed data are identical. return original == decompressed"},{"question":"**Question: Advanced Boxplot Visualization with Seaborn** You are provided with the Titanic dataset, and your task is to create different boxplot visualizations using the seaborn library, demonstrating both basic and advanced plotting techniques. Your implementation should include the following steps: 1. Load the Titanic dataset using seaborn\'s `load_dataset` function. 2. Create a horizontal boxplot displaying the distribution of the `age` variable. 3. Create a vertical boxplot comparing the distribution of `age` across different classes (`class` variable). 4. Create a vertical boxplot containing nested grouping: - Group by the `class` variable. - Segregate the data based on the `alive` status. 5. Customize a vertical boxplot: - Group by the `class` variable. - Modify the box lines to be dark blue with a linewidth of 1. - Include notches in the boxes. - Show outliers as red plus signs. - Set the box color to a semi-transparent green. - Highlight the median with a black line of width 2. 6. Explain how to cover the entire range of the `deck` data with whiskers and generate an example. 7. Explain the effect and usage of the `native_scale` parameter when plotting a boxplot and generate an example. # Implementation Ensure that your code is well-documented and explain any decisions or assumptions you make. Here is a basic outline to get you started: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Step 2: Horizontal boxplot for \'age\' plt.figure(figsize=(10, 6)) sns.boxplot(x=titanic[\\"age\\"]) plt.title(\\"Distribution of Age\\") plt.show() # Step 3: Vertical boxplot comparing \'age\' across \'class\' plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\") plt.title(\\"Distribution of Age across Classes\\") plt.show() # Step 4: Nested grouping boxplot plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\") plt.title(\\"Age Distribution across Classes by Survival Status\\") plt.show() # Step 5: Customized boxplot plt.figure(figsize=(10, 6)) sns.boxplot( data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", notch=True, showcaps=False, flierprops={\\"marker\\": \\"o\\", \\"markerfacecolor\\": \\"red\\", \\"markeredgecolor\\": \\"red\\"}, boxprops={\\"facecolor\\": (.3, .5, .7, .5)}, medianprops={\\"color\\": \\"black\\", \\"linewidth\\": 2} ) plt.title(\\"Customized Age Distribution across Classes by Survival Status\\") plt.show() # Step 6: Cover the full range of the \'deck\' data with whiskers plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"age\\", y=\\"deck\\", whis=(0, 100)) plt.title(\\"Age Distribution by Deck with Full Range Whiskers\\") plt.show() # Step 7: Effect and usage of \'native_scale\' parameter plt.figure(figsize=(10, 6)) ax = sns.boxplot(x=titanic[\\"age\\"].round(-1), y=titanic[\\"fare\\"], native_scale=True) ax.axvline(25, color=\\".3\\", dashes=(2, 2)) plt.title(\\"Fare Distribution by Age with Native Scaling\\") plt.show() # Explanation and assumptions should be added as comments or markdown cells within the notebook. ``` # Input Format N/A (You will load the dataset within your code). # Output Format Multiple boxplot visualizations displayed using matplotlib. # Constraints - You should utilize seaborn for plotting. - Ensure all plots are sufficiently labeled and titled. # Performance Requirements - Visualizations should be generated efficiently. - Code should handle any missing or erroneous data gracefully.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Step 2: Horizontal boxplot for \'age\' def plot_horizontal_age_boxplot(data): plt.figure(figsize=(10, 6)) sns.boxplot(x=data[\\"age\\"]) plt.title(\\"Distribution of Age\\") plt.xlabel(\\"Age\\") plt.show() # Step 3: Vertical boxplot comparing \'age\' across \'class\' def plot_class_age_boxplot(data): plt.figure(figsize=(10, 6)) sns.boxplot(data=data, x=\\"class\\", y=\\"age\\") plt.title(\\"Distribution of Age across Classes\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") plt.show() # Step 4: Nested grouping boxplot by \'alive\' def plot_nested_grouping_boxplot(data): plt.figure(figsize=(10, 6)) sns.boxplot(data=data, x=\\"class\\", y=\\"age\\", hue=\\"alive\\") plt.title(\\"Age Distribution across Classes by Survival Status\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") plt.show() # Step 5: Customized boxplot def plot_customized_boxplot(data): plt.figure(figsize=(10, 6)) sns.boxplot( data=data, x=\\"class\\", y=\\"age\\", notch=True, showcaps=False, flierprops={\\"marker\\": \\"o\\", \\"markerfacecolor\\": \\"red\\", \\"markeredgecolor\\": \\"red\\"}, boxprops={\\"facecolor\\": (.3, .5, .7, .5)}, medianprops={\\"color\\": \\"black\\", \\"linewidth\\": 2} ) plt.title(\\"Customized Age Distribution across Classes\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") plt.show() # Step 6: Boxplot with whiskers covering the full range of \'deck\' def plot_age_deck_boxplot(data): plt.figure(figsize=(10, 6)) sns.boxplot(data=data, x=\\"age\\", y=\\"deck\\", whis=(0, 100)) plt.title(\\"Age Distribution by Deck with Full Range Whiskers\\") plt.xlabel(\\"Age\\") plt.ylabel(\\"Deck\\") plt.show() # Step 7: Native scale parameter in boxplot def plot_native_scale_boxplot(data): plt.figure(figsize=(10, 6)) ax = sns.boxplot(x=data[\\"age\\"].round(-1), y=data[\\"fare\\"], native_scale=True) ax.axvline(25, color=\\".3\\", dashes=(2, 2)) plt.title(\\"Fare Distribution by Age with Native Scaling\\") plt.xlabel(\\"Age (rounded)\\") plt.ylabel(\\"Fare\\") plt.show() # Plotting examples plot_horizontal_age_boxplot(titanic) plot_class_age_boxplot(titanic) plot_nested_grouping_boxplot(titanic) plot_customized_boxplot(titanic) plot_age_deck_boxplot(titanic) plot_native_scale_boxplot(titanic)"},{"question":"**Objective:** Write a Python program using the `configparser` module to manage a configuration file. The program should be capable of creating a configuration file with specific settings, reading and modifying the configuration, and handling errors appropriately. **Task:** 1. **Creating the Configuration File:** - Write a Python function **`create_config(file_name)`** that: - Creates a configuration with at least three sections: `DEFAULT`, `UserSettings`, and `AdvancedSettings`. - In the `DEFAULT` section, set the following key-value pairs: - `AppName` = `MyApplication` - `Version` = `1.0` - In the `UserSettings` section, set the following key-value pairs: - `Theme` = `Light` - `FontSize` = `12` - In the `AdvancedSettings` section, set the following key-value pairs: - `CacheEnabled` = `yes` - `LogLevel` = `DEBUG` - Save these settings to a configuration file with the provided file name. 2. **Reading and Modifying Configuration:** - Write another function **`read_modify_config(file_name)`** that: - Reads the configuration file created in step 1. - Prints all the sections in the configuration. - Changes the `Theme` in the `UserSettings` section to `Dark`. - Converts `LogLevel` in the `AdvancedSettings` section to `INFO`. - Writes the modified configuration back to the file. 3. **Error Handling:** - Ensure that your functions handle the following errors appropriately: - `FileNotFoundError` if the configuration file does not exist when trying to read it. - `NoSectionError` if the specified section does not exist in the configuration when trying to modify it. - `ValueError` if any conversion (e.g., boolean) fails. **Constraints:** - The solution should use the `configparser` module for all operations. - Ensure code readability and include comments where necessary. **Performance:** - The operations should handle reasonably large configuration files efficiently. **Input:** - `file_name` (String): The name of the configuration file to create, read, and modify. **Output:** - The functions should print relevant information to the console, such as sections and modified values. - The modified configuration should be saved back to the specified file. **Example Usage:** ```python create_config(\'app_config.ini\') read_modify_config(\'app_config.ini\') ``` The above code should create a configuration file `app_config.ini` with the specified sections and values, modify some of these values, and handle any potential errors appropriately.","solution":"import configparser import os def create_config(file_name): Creates a configuration file with specified settings. config = configparser.ConfigParser() # Setting DEFAULT section config[\'DEFAULT\'] = { \'AppName\': \'MyApplication\', \'Version\': \'1.0\' } # Setting UserSettings section config[\'UserSettings\'] = { \'Theme\': \'Light\', \'FontSize\': \'12\' } # Setting AdvancedSettings section config[\'AdvancedSettings\'] = { \'CacheEnabled\': \'yes\', \'LogLevel\': \'DEBUG\' } # Writing configurations to file with open(file_name, \'w\') as configfile: config.write(configfile) def read_modify_config(file_name): Reads, modifies, and rewrites the configuration file. config = configparser.ConfigParser() try: # Reading the existing configuration if not os.path.isfile(file_name): raise FileNotFoundError(f\\"The file {file_name} does not exist.\\") config.read(file_name) # Printing all sections print(\\"Sections before modification:\\", config.sections()) # Modifying the configuration if \'UserSettings\' not in config: raise configparser.NoSectionError(\'UserSettings\') if \'AdvancedSettings\' not in config: raise configparser.NoSectionError(\'AdvancedSettings\') config[\'UserSettings\'][\'Theme\'] = \'Dark\' config[\'AdvancedSettings\'][\'LogLevel\'] = \'INFO\' # Writing modified configuration back to the file with open(file_name, \'w\') as configfile: config.write(configfile) print(\\"Sections after modification:\\", config.sections()) print(\\"Modified UserSettings Theme:\\", config[\'UserSettings\'][\'Theme\']) print(\\"Modified AdvancedSettings LogLevel:\\", config[\'AdvancedSettings\'][\'LogLevel\']) except FileNotFoundError as e: print(e) except configparser.NoSectionError as e: print(e) except ValueError as e: print(e)"},{"question":"# Code Assessment Question: Implementing Asynchronous Task Management Objective You are tasked with demonstrating your understanding of the `asyncio` library by writing an asynchronous function that simulates a concurrent task processing system. Problem Statement Write an asynchronous function called `process_tasks` that: 1. Takes two arguments: - `tasks`: A list of tuples. Each tuple contains a unique task ID (integer) and the time in seconds (float) the task should take to complete. - `max_concurrent_tasks`: An integer representing the maximum number of tasks that can be processed concurrently. 2. Processes each task asynchronously, respecting the `max_concurrent_tasks` limit. 3. Prints a start message for each task as soon as it starts processing in the format: `Task {task_id} started`. 4. Simulates task completion by using `asyncio.sleep` for the specified time. 5. Prints a completion message for each task once it finishes in the format: `Task {task_id} completed`. Input - `tasks`: List of tuples, each containing an integer and a float. Example: `[(1, 2.0), (2, 3.0), (3, 1.0)]` - `max_concurrent_tasks`: Integer. Example: `2` Output - Printed messages in the console indicating the start and completion of tasks. The order of task completion messages may not match the order of task start messages due to the concurrency. Constraints - You must use the `asyncio` library to manage concurrency. - Ensure that at any given time, the number of tasks being processed concurrently does not exceed `max_concurrent_tasks`. Implementation Requirements 1. Define the `process_tasks` function. 2. Inside this function, create an asynchronous function to handle individual tasks. 3. Use an appropriate semaphore or other synchronization primitive to ensure that no more than `max_concurrent_tasks` tasks are running concurrently. 4. Run and test your implementation. Example Given the input: ```python tasks = [(1, 2.0), (2, 3.0), (3, 1.0)] max_concurrent_tasks = 2 await process_tasks(tasks, max_concurrent_tasks) ``` Expected Output: ``` Task 1 started Task 2 started Task 1 completed Task 3 started Task 2 completed Task 3 completed ``` Note that the exact order of completion messages may vary depending on the processing times and the concurrency model.","solution":"import asyncio async def handle_task(task_id, duration): Simulate handling a single task. Arguments: task_id -- unique identifier for the task duration -- time in seconds for the task to complete Prints: - Task started message - Task completed message after sleeping for the duration print(f\\"Task {task_id} started\\") await asyncio.sleep(duration) print(f\\"Task {task_id} completed\\") async def process_tasks(tasks, max_concurrent_tasks): Process a list of tasks with a limit on maximum concurrent tasks. Arguments: tasks -- list of tuples, each containing (task_id, duration) max_concurrent_tasks -- maximum number of tasks to process concurrently semaphore = asyncio.Semaphore(max_concurrent_tasks) async def sem_task(task_id, duration): async with semaphore: await handle_task(task_id, duration) await asyncio.gather(*(sem_task(task_id, duration) for task_id, duration in tasks))"},{"question":"**Question:** You are provided with documentation on scikit-learn\'s cross-decomposition module, which includes supervised estimators for dimensionality reduction and regression belonging to the Partial Least Squares (PLS) family. This task will involve implementing and using the `PLSCanonical` class to solve a regression problem. # Task: 1. **Data Preparation:** - Generate a synthetic dataset with 100 samples each having 10 features. The target variable should have 2 target values. 2. **PLSCanonical Model:** - Implement a function `custom_pls_predict` that takes in training data (`X_train`, `Y_train`), new data (`X_test`), and the number of components as input, and returns the predicted target values for the new data using the `PLSCanonical` model. 3. **Function Implementation:** - Your function `custom_pls_predict` should: - Create and fit a `PLSCanonical` model using the provided training data and the specified number of components. - Predict the target values for the new data (`X_test`). ```python def custom_pls_predict(X_train, Y_train, X_test, n_components): Fit a PLSCanonical model and predict the target values for new data. Parameters: X_train (numpy.ndarray): Training feature matrix of shape (n_samples, n_features). Y_train (numpy.ndarray): Training target matrix of shape (n_samples, n_targets). X_test (numpy.ndarray): New feature matrix of shape (n_new_samples, n_features). n_components (int): Number of components to keep. Returns: numpy.ndarray: Predicted target values for `X_test`, of shape (n_new_samples, n_targets). from sklearn.cross_decomposition import PLSCanonical # TODO: Implement the function following the steps described pass # Example usage import numpy as np np.random.seed(0) X_train = np.random.rand(100, 10) Y_train = np.random.rand(100, 2) X_test = np.random.rand(10, 10) predicted_Y = custom_pls_predict(X_train, Y_train, X_test, n_components=2) print(predicted_Y) ``` # Constraints: - Do not use any libraries or functions specifically designed to perform the prediction directly without training using the data provided (e.g., pre-trained models). - Performance is important – ensure that your solution is efficient with respect to time and space complexity. # Notes: - The `PLSCanonical` class mentioned in the documentation will be particularly useful. - Ensure that all necessary imports from scikit-learn are included. - You can assume that the training and test data follow the correct input formats.","solution":"def custom_pls_predict(X_train, Y_train, X_test, n_components): Fit a PLSCanonical model and predict the target values for new data. Parameters: X_train (numpy.ndarray): Training feature matrix of shape (n_samples, n_features). Y_train (numpy.ndarray): Training target matrix of shape (n_samples, n_targets). X_test (numpy.ndarray): New feature matrix of shape (n_new_samples, n_features). n_components (int): Number of components to keep. Returns: numpy.ndarray: Predicted target values for `X_test`, of shape (n_new_samples, n_targets). from sklearn.cross_decomposition import PLSCanonical # Creating and fitting the PLSCanonical model pls = PLSCanonical(n_components=n_components) pls.fit(X_train, Y_train) # Predicting for the new data Y_pred = pls.predict(X_test) return Y_pred # Example usage if __name__ == \\"__main__\\": import numpy as np np.random.seed(0) X_train = np.random.rand(100, 10) Y_train = np.random.rand(100, 2) X_test = np.random.rand(10, 10) predicted_Y = custom_pls_predict(X_train, Y_train, X_test, n_components=2) print(predicted_Y)"},{"question":"# Command-Line Options Parser You are tasked with writing a Python script that processes command-line arguments to simulate a simple command-line interface for a program. The program will support both short and long options, with some options requiring arguments. You will use the `getopt` module for this task. Task Implement a function `parse_args(arguments: List[str]) -> Tuple[Dict[str, str], List[str]]` that takes a list of strings `arguments` representing command-line arguments (excluding the program name) and returns a tuple with two elements: 1. A dictionary of options (both short and long) as keys and their corresponding arguments as values. 2. A list of remaining positional arguments that are not options. Requirements - The function should support the following options: - `-h` or `--help`: Displays a help message (the message content can be a placeholder). - `-o <output>` or `--output <output>`: Specifies the output file. - `-v` or `--verbose`: Enables verbose mode. This option does not require an argument. - The function should raise an appropriate error if an unrecognized option is encountered or if an option that requires an argument is missing one. Example ```python >>> parse_args([\'-v\', \'--output\', \'file.txt\', \'input1\', \'input2\']) ({\'-v\': \'\', \'--output\': \'file.txt\'}, [\'input1\', \'input2\']) >>> parse_args([\'--help\']) ({\'--help\': \'\'}, []) ``` Notes - You may assume that options requiring arguments are always followed by their arguments. - Use `getopt.getopt` for parsing the options. Good luck and happy coding!","solution":"import getopt def parse_args(arguments): Parse command-line arguments and return a dictionary of options and a list of positional arguments. Arguments: arguments -- a list of strings representing command-line arguments Returns: A tuple (options_dict, positional_args) where: options_dict -- a dictionary mapping option flags to their argument values positional_args -- a list of remaining positional arguments short_opts = \'ho:v\' long_opts = [\'help\', \'output=\', \'verbose\'] try: opts, positional_args = getopt.getopt(arguments, short_opts, long_opts) except getopt.GetoptError as err: raise ValueError(f\\"Error parsing arguments: {err}\\") options_dict = {} for opt, arg in opts: if opt in (\'-h\', \'--help\'): options_dict[opt] = \'\' elif opt in (\'-o\', \'--output\'): options_dict[opt] = arg elif opt in (\'-v\', \'--verbose\'): options_dict[opt] = \'\' else: raise ValueError(f\\"Unhandled option: {opt}\\") return options_dict, positional_args"},{"question":"Objective To assess your understanding of PyTorch tensors, you will create a tensor, perform various operations on it, and demonstrate the use of automatic differentiation. Problem Statement Implement a function `tensor_operations` that performs the following tasks: 1. **Tensor Initialization**: - Create a 2x3 tensor `A` with values from a list of lists, where the list of lists is `[[1, 2, 3], [4, 5, 6]]`. This tensor should have the data type `torch.float64`. 2. **Mathematical Operations**: - Compute the square of each element in `A` and store the result in a tensor `B`. - Compute the element-wise addition of `A` and `B` and store the result in a tensor `C`. 3. **Automatic Differentiation**: - Create a tensor `D` with the same values as `C` but with `requires_grad=True`. - Compute the sum of all elements in `D` and store the result in a variable `total_sum`. - Perform backpropagation from `total_sum` to compute gradients. 4. **Verification and Output**: - Return a dictionary with the following keys and their corresponding values: - `\'A\'`: The tensor `A` - `\'B\'`: The tensor `B` - `\'C\'`: The tensor `C` - `\'D.grad\'`: The gradient of `D` after backpropagation Input The function does not take any input parameters. Output The function should return a dictionary with keys and values as described above. Example ```python result = tensor_operations() print(result[\'A\']) # tensor([[ 1.0000, 2.0000, 3.0000], # [ 4.0000, 5.0000, 6.0000]], dtype=torch.float64) print(result[\'B\']) # tensor([[ 1.0000, 4.0000, 9.0000], # [16.0000, 25.0000, 36.0000]], dtype=torch.float64) print(result[\'C\']) # tensor([[ 2.0000, 6.0000, 12.0000], # [20.0000, 30.0000, 42.0000]], dtype=torch.float64) print(result[\'D.grad\']) # tensor([[1., 1., 1.], # [1., 1., 1.]]) ``` Constraints - Ensure that `A` has a data type of `torch.float64`. - Perform all operations using PyTorch tensor operations. Implementation ```python import torch def tensor_operations(): # 1. Tensor Initialization A = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float64) # 2. Mathematical Operations B = A.pow(2) C = A + B # 3. Automatic Differentiation D = C.clone().detach().requires_grad_(True) total_sum = D.sum() total_sum.backward() # 4. Verification and Output return { \'A\': A, \'B\': B, \'C\': C, \'D.grad\': D.grad } ```","solution":"import torch def tensor_operations(): # 1. Tensor Initialization A = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float64) # 2. Mathematical Operations B = A.pow(2) C = A + B # 3. Automatic Differentiation D = C.clone().detach().requires_grad_(True) total_sum = D.sum() total_sum.backward() # 4. Verification and Output return { \'A\': A, \'B\': B, \'C\': C, \'D.grad\': D.grad }"},{"question":"**Coding Assessment Question:** # Objective Write a function in Python to analyze a directory and summarize the attributes of all sound files present in it. Use the `sndhdr` module to determine the file type of sound files. # Function Signature ```python def analyze_sound_directory(directory_path: str) -> dict: pass ``` # Input - `directory_path`: A string representing the path to the directory containing sound files. # Output - A dictionary where the keys are the filenames of sound files, and the values are namedtuples with the following attributes: `filetype`, `framerate`, `nchannels`, `nframes`, `sampwidth`. # Constraints 1. If a file in the directory is not a sound file or its type cannot be determined by `sndhdr`, it should not be included in the output dictionary. 2. Assume that the provided directory_path exists and is accessible. 3. The function should handle a large number of files efficiently. # Example Usage ```python from pathlib import Path def analyze_sound_directory(directory_path: str) -> dict: import os import sndhdr from collections import namedtuple # Initializing the dictionary to store results sound_files = {} # Walking through the directory to analyze each file for root, dirs, files in os.walk(directory_path): for file in files: file_path = os.path.join(root, file) try: sound_info = sndhdr.what(file_path) if sound_info: # Check if the file is a recognized sound file sound_files[file] = sound_info except Exception as e: # Ignoring errors and unrecognized files pass return sound_files # Example test case print(analyze_sound_directory(\\"/path/to/sound/files\\")) ``` In this question, students are expected to demonstrate: 1. Reading files from a directory using `os.walk()` or similar methods. 2. Utilizing the `sndhdr` module to determine sound file types. 3. Efficiently handling file processing and error situations. 4. Returning a dictionary with the required structure and ensuring only valid sound files are included.","solution":"import os import sndhdr from collections import namedtuple def analyze_sound_directory(directory_path: str) -> dict: SoundFileInfo = namedtuple(\'SoundFileInfo\', [\'filetype\', \'framerate\', \'nchannels\', \'nframes\', \'sampwidth\']) # Initializing the dictionary to store results sound_files = {} # Walking through the directory to analyze each file for root, dirs, files in os.walk(directory_path): for file in files: file_path = os.path.join(root, file) try: sound_info = sndhdr.what(file_path) if sound_info: # Check if the file is a recognized sound file filetype, framerate, nchannels, nframes, sampwidth = sound_info sound_files[file] = SoundFileInfo(filetype, framerate, nchannels, nframes, sampwidth) except Exception as e: # Ignoring errors and unrecognized files pass return sound_files"},{"question":"# Multiclass and Multioutput Classification with scikit-learn Background Multiclass classification involves classifying instances into one of several possible classes. scikit-learn provides various strategies and meta-estimators to handle such problems, including the One-vs-Rest (OvR) strategy. This strategy involves training one classifier per class, with the samples of that class treated as positives and all other samples treated as negatives. Similarly, multioutput classification concerns the prediction of multiple target labels for each instance. scikit-learn\'s `MultiOutputClassifier` can be used to extend any base classifier to handle multiple target variables. Task Implement a function `multiclass_and_multioutput_classification` that does the following: 1. Loads the Iris dataset for multiclass classification and the `make_multilabel_classification` dataset from scikit-learn for multioutput classification. 2. Implements the One-vs-Rest (OvR) strategy for multiclass classification using the `OneVsRestClassifier` meta-estimator with a `LinearSVC` base estimator. 3. Implements the `MultiOutputClassifier` using the `RandomForestClassifier` base estimator for multioutput classification. 4. Trains both classifiers on their respective datasets and returns the accuracy scores for each. Function Signature ```python def multiclass_and_multioutput_classification(): pass ``` Input - No direct input is provided to the function. Instead, the function will internally load datasets and apply the required classifiers. Output - A dictionary with the following keys and their corresponding accuracy scores: - `\\"multiclass_accuracy\\"` - `\\"multioutput_accuracy\\"` Implementation Notes - Use `train_test_split` from scikit-learn to split the datasets into training and testing sets with a test size of 0.3. - The accuracy score can be computed using `accuracy_score` from scikit-learn. Example ```python from sklearn.datasets import make_multilabel_classification from sklearn.svm import LinearSVC from sklearn.multioutput import MultiOutputClassifier from sklearn.multiclass import OneVsRestClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split from sklearn import datasets def multiclass_and_multioutput_classification(): # Load Iris dataset iris = datasets.load_iris() X_iris, y_iris = iris.data, iris.target X_train_mc, X_test_mc, y_train_mc, y_test_mc = train_test_split(X_iris, y_iris, test_size=0.3, random_state=42) # One-vs-Rest for multiclass classification ovr_classifier = OneVsRestClassifier(LinearSVC(random_state=0)) ovr_classifier.fit(X_train_mc, y_train_mc) y_pred_mc = ovr_classifier.predict(X_test_mc) multiclass_accuracy = accuracy_score(y_test_mc, y_pred_mc) # Create multilabel classification dataset X_ml, Y_ml = make_multilabel_classification(n_samples=100, n_features=20, n_classes=5, random_state=42) X_train_mo, X_test_mo, y_train_mo, y_test_mo = train_test_split(X_ml, Y_ml, test_size=0.3, random_state=42) # MultiOutputClassifier for multioutput classification mo_classifier = MultiOutputClassifier(RandomForestClassifier(random_state=0), n_jobs=-1) mo_classifier.fit(X_train_mo, y_train_mo) y_pred_mo = mo_classifier.predict(X_test_mo) multioutput_accuracy = accuracy_score(y_test_mo, y_pred_mo) return { \\"multiclass_accuracy\\": multiclass_accuracy, \\"multioutput_accuracy\\": multioutput_accuracy } # Example usage result = multiclass_and_multioutput_classification() print(result) ``` Expected output structure: ``` { \\"multiclass_accuracy\\": <accuracy_score>, \\"multioutput_accuracy\\": <accuracy_score> } ``` Constraints - Ensure reproducibility by setting `random_state`. - Use appropriate methods and techniques as indicated in the scikit-learn documentation provided.","solution":"from sklearn.datasets import load_iris, make_multilabel_classification from sklearn.model_selection import train_test_split from sklearn.multioutput import MultiOutputClassifier from sklearn.multiclass import OneVsRestClassifier from sklearn.svm import LinearSVC from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score def multiclass_and_multioutput_classification(): # Load Iris dataset for multiclass classification iris = load_iris() X_iris, y_iris = iris.data, iris.target X_train_mc, X_test_mc, y_train_mc, y_test_mc = train_test_split(X_iris, y_iris, test_size=0.3, random_state=42) # One-vs-Rest (OvR) strategy for multiclass classification with LinearSVC ovr_classifier = OneVsRestClassifier(LinearSVC(random_state=0)) ovr_classifier.fit(X_train_mc, y_train_mc) y_pred_mc = ovr_classifier.predict(X_test_mc) multiclass_accuracy = accuracy_score(y_test_mc, y_pred_mc) # Generate dataset for multioutput classification X_ml, Y_ml = make_multilabel_classification(n_samples=100, n_features=20, n_classes=5, random_state=42) X_train_mo, X_test_mo, y_train_mo, y_test_mo = train_test_split(X_ml, Y_ml, test_size=0.3, random_state=42) # MultiOutputClassifier with RandomForestClassifier for multioutput classification mo_classifier = MultiOutputClassifier(RandomForestClassifier(random_state=0), n_jobs=-1) mo_classifier.fit(X_train_mo, y_train_mo) y_pred_mo = mo_classifier.predict(X_test_mo) multioutput_accuracy = accuracy_score(y_test_mo, y_pred_mo) return { \\"multiclass_accuracy\\": multiclass_accuracy, \\"multioutput_accuracy\\": multioutput_accuracy }"},{"question":"# Question: Custom Theming and Plotting with Seaborn You are tasked with creating a comprehensive visualization for a given dataset using seaborn. Your visualization should include: 1. Multiple subplots (using `plt.subplots`). 2. Custom themes and styles. 3. Custom parameters for fine-tuning the appearance. Requirements: 1. Plot a barplot showing the average values for each category in the provided dataset. 2. Plot a lineplot showing the trend over time for a given time series in the dataset. 3. Customize the appearance with a grid style, pastel color palette, and specific custom parameters. 4. All subplots should be on a single figure. Input: - A dataset in the form of a pandas DataFrame. For the purposes of this exercise, assume the DataFrame `df` is given and contains the following columns: - `category`: Categorical variable. - `value`: Numerical values associated with each category. - `time`: Time series data. Output: - A single figure with two subplots as described. Constraints: - Ensure that the plot is aesthetically pleasing and well-labeled. - Use seaborn for plotting and `plt.subplots` for subplot management. Example DataFrame: ```python import pandas as pd data = { \'category\': [\'A\', \'A\', \'B\', \'B\', \'C\', \'C\'], \'value\': [10, 15, 20, 25, 30, 35], \'time\': [1, 2, 1, 2, 1, 2] } df = pd.DataFrame(data) ``` Example Solution Structure: ```python import seaborn as sns import matplotlib.pyplot as plt def custom_plotting(df): # Create subplots fig, axes = plt.subplots(1, 2, figsize=(14, 7)) # Set custom theme sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\") # Customize parameters custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} sns.set_theme(style=\\"white\\", rc=custom_params) # First subplot: barplot sns.barplot(ax=axes[0], data=df, x=\'category\', y=\'value\', estimator=sum, ci=None) axes[0].set_title(\'Average Values by Category\') axes[0].set_xlabel(\'Category\') axes[0].set_ylabel(\'Average Value\') # Second subplot: lineplot sns.lineplot(ax=axes[1], data=df, x=\'time\', y=\'value\', hue=\'category\') axes[1].set_title(\'Trend Over Time\') axes[1].set_xlabel(\'Time\') axes[1].set_ylabel(\'Value\') # Show plot plt.tight_layout() plt.show() # Example usage custom_plotting(df) ``` Ensure your code adheres to the requirements and constraints given.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def custom_plotting(df): # Create subplots fig, axes = plt.subplots(1, 2, figsize=(14, 7)) # Set custom theme sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\") # Customize parameters custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} plt.rcParams.update(custom_params) # First subplot: barplot sns.barplot(ax=axes[0], data=df, x=\'category\', y=\'value\', estimator=sum, ci=None) axes[0].set_title(\'Average Values by Category\') axes[0].set_xlabel(\'Category\') axes[0].set_ylabel(\'Average Value\') # Second subplot: lineplot sns.lineplot(ax=axes[1], data=df, x=\'time\', y=\'value\', hue=\'category\') axes[1].set_title(\'Trend Over Time\') axes[1].set_xlabel(\'Time\') axes[1].set_ylabel(\'Value\') # Show plot plt.tight_layout() plt.show() # Example usage data = { \'category\': [\'A\', \'A\', \'B\', \'B\', \'C\', \'C\'], \'value\': [10, 15, 20, 25, 30, 35], \'time\': [1, 2, 1, 2, 1, 2] } df = pd.DataFrame(data) custom_plotting(df)"},{"question":"# **Advanced Date and Time Manipulation Assessment** **Objective:** The goal of this assessment is to evaluate your understanding and ability to work with the Python `datetime` module, including handling timezone-aware and naive `datetime` objects. **Problem:** You are tasked with creating a multilingual event planner system that can manage events across different time zones and provide functionalities to: 1. Create new events with a specified start and end time. 2. Convert event times between time zones. 3. List all events occurring on a specific date across multiple time zones. 4. Provide a string representation of events in a specified format and locale. **Function Specifications:** You need to implement a class `EventPlanner` with the following methods: 1. `add_event(name: str, start_time: str, end_time: str, timezone_str: str) -> None` - Adds a new event to the planner. - `start_time` and `end_time` are given as strings in the format \\"YYYY-MM-DD HH:MM\\". - `timezone_str` specifies the time zone of the event, e.g., \\"UTC\\", \\"America/New_York\\". 2. `convert_event_timezone(event_name: str, new_timezone_str: str) -> None` - Converts the times of an event to a new time zone. 3. `list_events_on_date(date_str: str, timezone_str: str) -> List[Dict[str, str]]` - Lists all events occurring on the specified date (given in `timezone_str`). - Returns a list of dictionaries, each containing the \\"name\\", \\"start_time\\", and \\"end_time\\" of the events. 4. `event_to_string(event_name: str, locale_str: str) -> str` - Provides a string representation of an event in the specified locale. - The locale may change the date and time formatting and the language of the event description. **Constraints and Requirements:** - You may use `pytz` for time zone handling if required. - The system should handle incorrect input gracefully by raising appropriate exceptions. - You should handle daylight savings time changes correctly where applicable. - Ensure proper conversion between naive and aware `datetime` objects. - Optimize for readability and maintainability. Here is an example of how the class should behave: ```python from typing import List, Dict from datetime import datetime class EventPlanner: def __init__(self): pass # Initialize your data structures here def add_event(self, name: str, start_time: str, end_time: str, timezone_str: str) -> None: pass # Implement this method def convert_event_timezone(self, event_name: str, new_timezone_str: str) -> None: pass # Implement this method def list_events_on_date(self, date_str: str, timezone_str: str) -> List[Dict[str, str]]: pass # Implement this method def event_to_string(self, event_name: str, locale_str: str) -> str: pass # Implement this method # Example usage: planner = EventPlanner() planner.add_event(\\"Meeting\\", \\"2023-10-01 09:00\\", \\"2023-10-01 11:00\\", \\"America/New_York\\") planner.convert_event_timezone(\\"Meeting\\", \\"Europe/London\\") events = planner.list_events_on_date(\\"2023-10-01\\", \\"UTC\\") print(planner.event_to_string(\\"Meeting\\", \\"en_US\\")) ``` **Performance Considerations:** - Events should be stored in a way that allows efficient listing by date. - Avoid unnecessary recomputations when converting time zones or formatting event strings. - Handle edge cases, such as overlapping events and invalid time zones, gracefully. **Notes:** - The `datetime` module\'s documentation provides details on `timedelta`, `date`, `time`, `datetime`, and `timezone` classes that will be useful. - You might find the `pytz` library documentation helpful for handling time zones.","solution":"from typing import List, Dict from datetime import datetime from pytz import timezone, all_timezones, utc import pytz class EventPlanner: def __init__(self): self.events = {} def add_event(self, name: str, start_time: str, end_time: str, timezone_str: str) -> None: if timezone_str not in all_timezones: raise ValueError(\\"Invalid time zone\\") event_timezone = timezone(timezone_str) start_dt = self._parse_time(start_time, event_timezone) end_dt = self._parse_time(end_time, event_timezone) if start_dt >= end_dt: raise ValueError(\\"Event end time must be after start time\\") self.events[name] = {\'start_time\': start_dt, \'end_time\': end_dt, \'timezone\': event_timezone} def convert_event_timezone(self, event_name: str, new_timezone_str: str) -> None: if new_timezone_str not in all_timezones: raise ValueError(\\"Invalid time zone\\") if event_name not in self.events: raise KeyError(\\"Event not found\\") new_timezone = timezone(new_timezone_str) event = self.events[event_name] event[\'start_time\'] = event[\'start_time\'].astimezone(new_timezone) event[\'end_time\'] = event[\'end_time\'].astimezone(new_timezone) event[\'timezone\'] = new_timezone def list_events_on_date(self, date_str: str, timezone_str: str) -> List[Dict[str, str]]: if timezone_str not in all_timezones: raise ValueError(\\"Invalid time zone\\") target_date = datetime.strptime(date_str, \\"%Y-%m-%d\\").date() target_timezone = timezone(timezone_str) events_on_date = [] for name, details in self.events.items(): start_time_local = details[\'start_time\'].astimezone(target_timezone) end_time_local = details[\'end_time\'].astimezone(target_timezone) if start_time_local.date() <= target_date <= end_time_local.date(): events_on_date.append({ \'name\': name, \'start_time\': start_time_local.strftime(\\"%Y-%m-%d %H:%M\\"), \'end_time\': end_time_local.strftime(\\"%Y-%m-%d %H:%M\\") }) return events_on_date def event_to_string(self, event_name: str, locale_str: str) -> str: if event_name not in self.events: raise KeyError(\\"Event not found\\") event = self.events[event_name] start_time = event[\'start_time\'].strftime(\\"%Y-%m-%d %H:%M\\") end_time = event[\'end_time\'].strftime(\\"%Y-%m-%d %H:%M\\") return f\\"Event \'{event_name}\' starts at {start_time} and ends at {end_time} in {locale_str} locale.\\" def _parse_time(self, time_str: str, tz): naive_dt = datetime.strptime(time_str, \'%Y-%m-%d %H:%M\') return tz.localize(naive_dt)"},{"question":"Implement a function `html_entity_converter` that converts strings with HTML5 named character references into their corresponding Unicode characters. The conversion should support both the fully named entities (with trailing semicolon) as well as the entities without the trailing semicolon. Additionally, handle conversion of Unicode code points to their corresponding HTML entity names. Function Signature ```python def html_entity_converter(input_str: str) -> str: pass ``` Input - `input_str` (str): A string that may contain HTML5 named character references or Unicode code points. Output - (str): A string with all HTML5 named character references converted to their corresponding Unicode characters where possible. Constraints - The function should support all named character references as specified in the `html5` dictionary. - If an entity or code point is not recognized, it should be left as it is. Example ```python # Example 1 input_str = \\"Hello &amp; welcome to the world of &lt;HTML&gt;!\\" output_str = html_entity_converter(input_str) print(output_str) # Output: \\"Hello & welcome to the world of <HTML>!\\" # Example 2 input_str = \\"Unicode &#39;1F600;\\" output_str = html_entity_converter(input_str) print(output_str) # Output: \\"Unicode 😀\\" # Example 3 input_str = \\"Invalid entity: &notarealentity; remains untouched\\" output_str = html_entity_converter(input_str) print(output_str) # Output: \\"Invalid entity: &notarealentity; remains untouched\\" ``` # Explanation The function `html_entity_converter` should correctly look up and replace the named character references in the `input_str` with their equivalent Unicode characters using the `html.entities.html5` dictionary. Any unrecognized entities or malformed entities should be left unchanged in the output string. The handling should consider both the presence and absence of trailing semicolons in the entity names, as defined by the `html.entities.html5` dictionary.","solution":"import html import re def html_entity_converter(input_str: str) -> str: Converts HTML5 named character references in the input string to their corresponding Unicode characters. Args: input_str (str): The input string containing HTML5 named entities. Returns: str: The converted string with HTML5 named entities replaced by their Unicode characters. # Function to replace HTML entities with corresponding characters def replace_entity(match): entity = match.group(0) if entity.startswith(\'&#\'): try: # Handle numeric entities if entity.startswith(\'&#x\'): return chr(int(entity[3:-1], 16)) else: return chr(int(entity[2:-1])) except ValueError: # Return as is if conversion fails return entity else: # Handle named entities, stripping semicolon if it exists entity_name = entity[1:-1] if entity_name in html.entities.html5: return html.entities.html5[entity_name] else: return entity # Regular expression to find HTML entities entity_re = re.compile(r\'&([a-zA-Z]+[0-9]*|#x?[0-9A-Fa-f]+);\') # Replace all entities in the string result_str = entity_re.sub(replace_entity, input_str) return result_str"},{"question":"# Question: GPU-Based Operations with PyTorch MPS Backend You are given the task to utilize PyTorch\'s MPS backend to perform tensor operations and run a simple neural network model on a macOS device with Metal Performance Shaders (MPS) capability. Implement the following functions: 1. **check_mps_device()**: - **Input**: None - **Output**: Return a tuple (boolean, string). The boolean value should be `True` if MPS is available and built, otherwise `False`. The string should give an appropriate message indicating the status of MPS availability. 2. **run_tensor_operations()**: - **Input**: None - **Output**: A tuple of two tensors (torch.Tensor). Create a tensor with all elements set to 1 directly on the MPS device and perform a simple element-wise multiplication by 2. Return the original tensor and the resulting tensor after the multiplication. 3. **run_model_on_mps()**: - **Input**: A simple neural network model instance (torch.nn.Module). - **Output**: A torch.Tensor representing the model\'s prediction on a tensor of ones with size (5,) created on the MPS device. Move the given model to the MPS device, perform the prediction, and return the prediction tensor. Here is a sample neural network model you can use for testing: ```python import torch import torch.nn as nn class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(5, 1) def forward(self, x): return self.fc1(x) ``` **Constraints**: - Ensure to handle scenarios where MPS is not available. - You should only use the MPS device if it is available and can be built. - Use appropriate error handling and informative messages for different device statuses. Example Usage: ```python # Check if MPS is available device_available, message = check_mps_device() print(message) # If available, perform the operations if device_available: original_tensor, result_tensor = run_tensor_operations() print(\\"Original Tensor:\\", original_tensor) print(\\"Result Tensor:\\", result_tensor) # Define and run model on MPS model = SimpleNet() prediction = run_model_on_mps(model) print(\\"Model Prediction:\\", prediction) ``` **Note**: The above functions need to work correctly on a macOS device with MPS capability. If MPS is not available, the functions should handle this gracefully.","solution":"import torch import torch.nn as nn def check_mps_device(): Checks if MPS device is available and built. Returns ------- tuple: (bool, str) A tuple where the first element indicates if MPS is available and the second element is a message. if not torch.backends.mps.is_available(): return (False, \\"MPS device not available.\\") elif not torch.backends.mps.is_built(): return (False, \\"MPS device not built.\\") else: return (True, \\"MPS device is available and built.\\") def run_tensor_operations(): Performs tensor operations on the MPS device. Returns ------- tuple: (torch.Tensor, torch.Tensor) A tuple with the original tensor and the result tensor after multiplication. # Ensure MPS is available and use the appropriate device is_available, message = check_mps_device() if not is_available: raise RuntimeError(message) device = torch.device(\\"mps\\") original_tensor = torch.ones(5, device=device) result_tensor = original_tensor * 2 return original_tensor, result_tensor def run_model_on_mps(model): Runs a given model on the MPS device. Parameters ---------- model : torch.nn.Module The neural network model to run on MPS. Returns ------- torch.Tensor The prediction of the model on a tensor of ones. is_available, message = check_mps_device() if not is_available: raise RuntimeError(message) device = torch.device(\\"mps\\") model = model.to(device) input_tensor = torch.ones(5, device=device) with torch.no_grad(): prediction = model(input_tensor) return prediction"},{"question":"**Question:** You are tasked with creating a complex visual representation of a dataset using seaborn. Your visualization should involve multiple subplots with different styles and contexts to demonstrate your understanding of seaborn\'s aesthetic and context customization capabilities. # Requirements 1. **Input:** - Use a random dataset generated using `numpy`. - Construct a 2x2 grid plot using matplotlib\'s `gridspec`. 2. **Output:** - A figure with the following specifications: - **Top-Left Subplot:** - Use the `darkgrid` style. - Plot a simple sine wave. - **Top-Right Subplot:** - Use the `whitegrid` style. - Plot a boxplot of normally distributed data. - **Bottom-Left Subplot:** - Use the `ticks` style. - Plot a violin plot of normally distributed data. - Remove the top and right spines. - **Bottom-Right Subplot:** - Use the `white` style. - Plot a scatterplot of bivariate normally distributed data. - Use the \'talk\' context for larger elements. 3. **Constraints:** - You must use the appropriate seaborn and matplotlib functions to set styles and contexts. - The random dataset should have sufficient data to make each plot meaningful. 4. **Performance:** - Efficiently manage the settings to ensure the code is clean and understandable. # Example Code Structure ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt np.random.seed(42) # Generate data x = np.linspace(0, 14, 100) y = np.sin(x) data = np.random.normal(size=(20, 6)) + np.arange(6) / 2 bivariate_data = np.random.multivariate_normal([0, 0], [[1, 0.5], [0.5, 1]], size=100) # Create a 2x2 grid for subplots f = plt.figure(figsize=(10, 10)) gs = f.add_gridspec(2, 2) # Top-Left Subplot: Darkgrid with sns.axes_style(\\"darkgrid\\"): ax = f.add_subplot(gs[0, 0]) ax.plot(x, y) ax.set_title(\\"Sine Wave - Darkgrid\\") # Top-Right Subplot: Whitegrid with sns.axes_style(\\"whitegrid\\"): ax = f.add_subplot(gs[0, 1]) sns.boxplot(data=data, ax=ax) ax.set_title(\\"Boxplot - Whitegrid\\") # Bottom-Left Subplot: Ticks with sns.axes_style(\\"ticks\\"): ax = f.add_subplot(gs[1, 0]) sns.violinplot(data=data, ax=ax) sns.despine() ax.set_title(\\"Violin Plot - Ticks (Spines Removed)\\") # Bottom-Right Subplot: White Style with Talk Context with sns.axes_style(\\"white\\"): sns.set_context(\\"talk\\") ax = f.add_subplot(gs[1, 1]) sns.scatterplot(x=bivariate_data[:, 0], y=bivariate_data[:, 1], ax=ax) ax.set_title(\\"Scatterplot - White Style with Talk Context\\") f.tight_layout() plt.show() ```","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt np.random.seed(42) def create_complex_visualization(): # Generate data x = np.linspace(0, 14, 100) y = np.sin(x) data = np.random.normal(size=(20, 6)) + np.arange(6) / 2 bivariate_data = np.random.multivariate_normal([0, 0], [[1, 0.5], [0.5, 1]], size=100) # Create a 2x2 grid for subplots f = plt.figure(figsize=(10, 10)) gs = f.add_gridspec(2, 2) # Top-Left Subplot: Darkgrid with sns.axes_style(\\"darkgrid\\"): ax = f.add_subplot(gs[0, 0]) ax.plot(x, y) ax.set_title(\\"Sine Wave - Darkgrid\\") # Top-Right Subplot: Whitegrid with sns.axes_style(\\"whitegrid\\"): ax = f.add_subplot(gs[0, 1]) sns.boxplot(data=data, ax=ax) ax.set_title(\\"Boxplot - Whitegrid\\") # Bottom-Left Subplot: Ticks with sns.axes_style(\\"ticks\\"): ax = f.add_subplot(gs[1, 0]) sns.violinplot(data=data, ax=ax) sns.despine() ax.set_title(\\"Violin Plot - Ticks (Spines Removed)\\") # Bottom-Right Subplot: White Style with Talk Context with sns.axes_style(\\"white\\"): sns.set_context(\\"talk\\") ax = f.add_subplot(gs[1, 1]) sns.scatterplot(x=bivariate_data[:, 0], y=bivariate_data[:, 1], ax=ax) ax.set_title(\\"Scatterplot - White Style with Talk Context\\") f.tight_layout() plt.show()"},{"question":"You are required to implement a Python class that mimics a simple resource manager. This resource manager handles a set of resources and uses multiple compound statements to manage its operations. Task 1. **Resource Class**: - Create a class `Resource` that initializes with a name (string) and a status (boolean indicating if the resource is available). - Implement a method `mark_unavailable` that changes the resource status to unavailable. 2. **ResourceManager Class**: - Create a class `ResourceManager` that initializes with a list of `Resource` objects. - Implement a method `use_resource` that: - Uses a `for` loop to find the first available resource. - If a resource is available: - Mark it as unavailable. - Print a message \\"Resource {resource_name} is now in use.\\" - If no resources are available, raise an `Exception` with the message \\"No resources available\\". 3. **Context Manager**: - Make `ResourceManager` a context manager that: - Ensures any used resources are marked as available again when the context ends (using the `with` statement). 4. **Handling Errors**: - Implement error handling for `ResourceManager` such that if an attempt to use resources fails (i.e., raises an `Exception`), it prints \\"Error: {error_message}\\". Example Usage: ```python # Create some Resource instances resources = [Resource(\\"Resource1\\", True), Resource(\\"Resource2\\", True), Resource(\\"Resource3\\", False)] # Initialize ResourceManager with the list of resources with ResourceManager(resources) as manager: try: manager.use_resource() manager.use_resource() manager.use_resource() except Exception as e: manager.handle_error(e) ``` Constraints: - Utilize if, for, try/except, and with statements where applicable. - Resources in the `ResourceManager` should be reset to available at the end of the `with` block regardless of whether an error occurred. Input: - The list of resources will be provided as input when initializing the `ResourceManager`. Output: - The code must handle printing messages based on the operations performed and exceptions encountered. Evaluation Criteria: - Correct class and function definitions using the right compound statements. - Proper implementation of context managers and exception handling. - Adherence to Python syntax and idiomatic code practices.","solution":"class Resource: def __init__(self, name, available): self.name = name self.available = available def mark_unavailable(self): self.available = False def mark_available(self): self.available = True class ResourceManager: def __init__(self, resources): self.resources = resources self.used_resources = [] def use_resource(self): for resource in self.resources: if resource.available: resource.mark_unavailable() self.used_resources.append(resource) print(f\\"Resource {resource.name} is now in use.\\") return raise Exception(\\"No resources available\\") def handle_error(self, error): print(f\\"Error: {str(error)}\\") def __enter__(self): return self def __exit__(self, exc_type, exc_val, exc_tb): for resource in self.used_resources: resource.mark_available() self.used_resources = []"},{"question":"Objective: The goal of this exercise is to test your understanding and ability to use the `multiprocessing` module in Python to create and manage processes, enable communication between them, and apply synchronization mechanisms. Problem Statement: You are required to create a `MultiprocessingManager` class that handles multiple tasks using the `multiprocessing` module. The class should be able to: 1. Spawn multiple worker processes. 2. Communicate tasks to workers using queues. 3. Ensure each worker processes tasks in order while maintaining synchronization. 4. Retrieve and aggregate results from all worker processes. 5. Ensure all resources (processes, queues) are properly closed and cleaned up after task completion. Requirements: 1. **Initialization**: - The `MultiprocessingManager` should initialize with a specified number of worker processes. - Create the necessary queues for task assignment and result collection. 2. **Task Assignment**: - Implement a method `assign_tasks(self, tasks)` to accept a list of tasks. Each task should be a tuple, where the first element is a callable function, and the second element is a tuple of arguments for that function. 3. **Worker Function**: - Implement a worker function (`worker(queue, result_queue, lock)`) that processes tasks from the task queue and puts the results into the result queue. - Use locks to synchronize access to resources if necessary. 4. **Execution and Result Handling**: - Start the worker processes and feed tasks to them. - Collect results from the result queue and return them in the same order the tasks were provided. 5. **Cleanup**: - Ensure all processes are properly terminated and all queues are closed after task execution. Input: - A list of tasks, where each task is a tuple `(function, (args))`. Output: - A list of results obtained from processing the tasks. Example: ```python from multiprocessing import Manager from multiprocessing_manager import MultiprocessingManager # your solution def multiply(x, y): return x * y def add(x, y): return x + y if __name__ == \\"__main__\\": tasks = [ (multiply, (2, 3)), (add, (5, 7)), (multiply, (4, 5)), (add, (10, 12)) ] manager = MultiprocessingManager(num_workers=4) results = manager.assign_tasks(tasks) print(results) # Expected output: [6, 12, 20, 22] ``` In this example: - You have a list of tasks which are simple arithmetic operations. - You create an instance of `MultiprocessingManager` with 4 worker processes. - You assign tasks to the manager and retrieve the results. Constraints: - You must use the `multiprocessing` module. - Ensure synchronization to avoid data corruption. - Tasks should be processed in the order they are given, and results should also be returned in the order of tasks.","solution":"import multiprocessing from multiprocessing import Queue, Lock, Process class MultiprocessingManager: def __init__(self, num_workers): Initializes the MultiprocessingManager with the specified number of worker processes. self.num_workers = num_workers self.task_queue = Queue() self.result_queue = Queue() self.lock = Lock() self.processes = [] def worker(self, task_queue, result_queue, lock): Worker function to process tasks from the task queue and put results into the result queue. while True: task = task_queue.get() if task is None: break func, args = task result = func(*args) with lock: result_queue.put(result) def assign_tasks(self, tasks): Accepts a list of tasks and processes them using multiple worker processes. # Starting worker processes for _ in range(self.num_workers): p = Process(target=self.worker, args=(self.task_queue, self.result_queue, self.lock)) p.start() self.processes.append(p) # Putting tasks into the task queue for task in tasks: self.task_queue.put(task) # Putting end signals for each worker process for _ in range(self.num_workers): self.task_queue.put(None) # Collecting results results = [] for _ in range(len(tasks)): result = self.result_queue.get() results.append(result) # Ensuring all worker processes are terminated for p in self.processes: p.join() return results # Example usage if __name__ == \\"__main__\\": def multiply(x, y): return x * y def add(x, y): return x + y tasks = [ (multiply, (2, 3)), (add, (5, 7)), (multiply, (4, 5)), (add, (10, 12)) ] manager = MultiprocessingManager(num_workers=4) results = manager.assign_tasks(tasks) print(results) # Expected output: [6, 12, 20, 22]"},{"question":"**Question: Build a Simple Web Crawler Using Python Modules** **Objective:** Create a simple web crawler that navigates through web pages, retrieves information, handles HTTP responses, and manages cookies. The crawler should be able to parse the page content and follow certain links based on specified criteria. **Tasks:** 1. **Request Handling:** - Implement functions using the `urllib` module to make GET requests to a given URL. - Handle possible HTTP errors gracefully using the appropriate mechanisms in the `urllib.error` module. - Manage cookies using the `http.cookiejar` module to simulate session management. 2. **Content Parsing:** - Use the `urllib.parse` module to extract useful components from URLs. - Implement a function to parse and filter links from the HTML content of the page according to specific patterns. 3. **Crawling Logic:** - Implement a function that takes a starting URL, a depth limit, and a pattern for links to follow. - The function should keep track of visited URLs to avoid cycles. - The crawler should follow links up to the specified depth, collecting titles of visited pages. 4. **IP Address Management:** - Use the `ipaddress` module to validate and handle any IP addresses encountered during the crawling. **Input Format:** - A starting URL (string). - A depth limit (integer). - A URL pattern to follow (string). **Output Format:** - A dictionary where keys are URLs and values are page titles (string). **Function Signature:** ```python def web_crawler(start_url: str, depth_limit: int, url_pattern: str) -> dict: pass ``` **Constraints:** - The starting URL will be a valid URL. - Depth limit will be a non-negative integer. - URL pattern will be a valid regex string for matching URLs. **Example:** ```python result = web_crawler(\\"http://example.com\\", 2, \\"example.com/posts/\\") print(result) ``` This should produce output similar to: ```python { \\"http://example.com\\": \\"Example Domain\\", \\"http://example.com/posts/1\\": \\"Post 1 Title\\", \\"http://example.com/posts/2\\": \\"Post 2 Title\\" } ``` **Performance Requirements:** - The function should handle network delays and possible errors without crashing. - Efficiently manage and limit the number of simultaneous requests to avoid overloading servers. **Hints:** - Use `urllib.request.urlopen` to fetch URLs and handle responses. - Use `http.cookiejar.CookieJar` for managing cookies. - Parse URLs and links using `urllib.parse.urljoin` and `urllib.parse.urlparse`. - Use regular expressions to filter links matching the given pattern. - Use a set to track visited URLs. This question will test students\' ability to integrate multiple modules for a real-world application, handle HTTP responses and cookies, and work with IP addresses and URL parsing.","solution":"import urllib.request import urllib.error import http.cookiejar import urllib.parse import re from bs4 import BeautifulSoup def make_request(url, cookie_jar): try: opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie_jar)) response = opener.open(url) return response.read(), response.geturl() except urllib.error.URLError as e: print(f\\"Failed to open {url}: {e.reason}\\") return None, None def parse_links(html_content, base_url, pattern): soup = BeautifulSoup(html_content, \'html.parser\') links = [] for anchor in soup.find_all(\'a\', href=True): href = anchor.get(\'href\') full_url = urllib.parse.urljoin(base_url, href) if re.match(pattern, full_url): links.append(full_url) return links def get_page_title(html_content): soup = BeautifulSoup(html_content, \'html.parser\') title_tag = soup.title if title_tag: return title_tag.string return \'No Title\' def web_crawler(start_url: str, depth_limit: int, url_pattern: str) -> dict: visited = set() to_visit = [(start_url, 0)] titles = {} cookie_jar = http.cookiejar.CookieJar() while to_visit: url, depth = to_visit.pop(0) if url in visited or depth > depth_limit: continue html_content, real_url = make_request(url, cookie_jar) if not html_content: continue title = get_page_title(html_content) titles[real_url] = title visited.add(real_url) if depth < depth_limit: for link in parse_links(html_content, real_url, url_pattern): if link not in visited: to_visit.append((link, depth + 1)) return titles"},{"question":"**Objective:** Create a Python script that performs a set of system-level operations and logs the results. The script should: 1. Parse command-line arguments to determine a directory to scan and a keyword to search for within file names. 2. Traverse the given directory and its subdirectories to find all files that contain the specified keyword in their names. 3. For each found file, log the file path and its size. If the file is larger than a given threshold (e.g., 1MB), log a warning. 4. The script should log all operational messages to a log file, and for errors, it should output to both the log file and the console. **Requirements:** - Implement using the `os`, `argparse`, and `logging` modules. - Expected input: Two command-line arguments: the directory path and the keyword. - Expected output: Log file named `scan_log.txt` containing file paths and sizes, and warnings for files larger than 1MB. **Constraints:** - Assume the directory path provided exists and is accessible. - File size warning threshold is 1MB. - The log file should be created in the current working directory. **Performance:** - The script should efficiently handle directories with a large number of files (up to tens of thousands). **Function Implementation:** Write a function `scan_directory(directory: str, keyword: str, size_threshold: int = 1048576) -> None` which handles the directory scanning and logging as described. ```python import os import argparse import logging def scan_directory(directory: str, keyword: str, size_threshold: int = 1048576) -> None: # Initialize the logger logging.basicConfig(filename=\'scan_log.txt\', filemode=\'w\', level=logging.INFO, format=\'%(asctime)s - %(levelname)s - %(message)s\') for root, dirs, files in os.walk(directory): for file in files: if keyword in file: file_path = os.path.join(root, file) file_size = os.path.getsize(file_path) message = f\'Found file: {file_path} with size: {file_size} bytes\' logging.info(message) if file_size > size_threshold: warning_message = f\'File {file_path} exceeds size threshold with size: {file_size} bytes\' logging.warning(warning_message) print(warning_message) if __name__ == \'__main__\': parser = argparse.ArgumentParser(description=\\"Scan a directory for files containing a keyword in their names.\\") parser.add_argument(\'directory\', type=str, help=\'Directory to scan\') parser.add_argument(\'keyword\', type=str, help=\'Keyword to search for within file names\') args = parser.parse_args() scan_directory(args.directory, args.keyword) ``` **Explanation:** - **Command-line Parsing:** Uses `argparse` to handle command-line input for the directory and keyword. - **Directory Traversal:** Uses `os.walk` to recursively traverse the specified directory and search for files. - **File Operations:** Checks each file name for the keyword, retrieves the file size, and logs the details. - **Logging:** Utilizes `logging` to record operational messages and warnings into a log file. - **Threshold Check:** Issues warnings for files exceeding the size threshold. Test your script thoroughly to ensure it meets the requirements and handles edge cases effectively.","solution":"import os import argparse import logging def scan_directory(directory: str, keyword: str, size_threshold: int = 1048576) -> None: Scans the given directory and logs file paths and sizes. Args: directory (str): The directory path to scan. keyword (str): The keyword to search for in file names. size_threshold (int): The file size threshold for logging warnings (default is 1MB). # Initialize the logger logging.basicConfig(filename=\'scan_log.txt\', filemode=\'w\', level=logging.INFO, format=\'%(asctime)s - %(levelname)s - %(message)s\') for root, dirs, files in os.walk(directory): for file in files: if keyword in file: file_path = os.path.join(root, file) file_size = os.path.getsize(file_path) message = f\'Found file: {file_path} with size: {file_size} bytes\' logging.info(message) if file_size > size_threshold: warning_message = f\'File {file_path} exceeds size threshold with size: {file_size} bytes\' logging.warning(warning_message) print(warning_message) if __name__ == \'__main__\': parser = argparse.ArgumentParser(description=\\"Scan a directory for files containing a keyword in their names.\\") parser.add_argument(\'directory\', type=str, help=\'Directory to scan\') parser.add_argument(\'keyword\', type=str, help=\'Keyword to search for within file names\') args = parser.parse_args() scan_directory(args.directory, args.keyword)"},{"question":"# Advanced Coding Assessment: Nearest Neighbors in scikit-learn Objective: Demonstrate your understanding of the different nearest neighbor methods in scikit-learn, their implementation, and trade-offs in terms of efficiency and performance. Problem Statement: You are given a dataset with input features `X` and corresponding labels `y`. Your task is to implement several nearest neighbor methods to compare their performance for a classification task. Specifically, you should: 1. Implement k-nearest neighbors (k-NN) classification using `KNeighborsClassifier`. 2. Implement radius-based nearest neighbors (r-NN) classification using `RadiusNeighborsClassifier`. 3. Compare the results and performance of both methods using different neighbor search algorithms (`BallTree`, `KDTree`, and `brute`). Instructions: 1. Load your dataset (you can use any available dataset such as Iris, Wine, etc. from scikit-learn or load your own). 2. Split the dataset into a training set (70%) and a testing set (30%). 3. Implement both k-NN and r-NN classifiers using the following criteria: - Use `KNeighborsClassifier` with `n_neighbors = 3`. - Use `RadiusNeighborsClassifier` with `radius = 1.0`. - For both classifiers, use different algorithms (`BallTree`, `KDTree`, `brute`). 4. Evaluate each implementation using accuracy score and computation time. Expectations: - Your code should be well-organized and commented. - Use appropriate scikit-learn methods and classes. - Provide a summary of your findings, discussing the trade-offs in terms of accuracy and computational efficiency for each algorithm. Constraints: - Assume `X` is a 2D NumPy array of shape `(n_samples, n_features)`. - Assume `y` is a 1D NumPy array of shape `(n_samples,)`. - You must implement and compare at least three different neighbor search algorithms (`BallTree`, `KDTree`, `brute`) for both k-NN and r-NN classifiers. Input & Output Format: - **Input:** Dataset `X` and target labels `y`. - **Output:** Print the accuracy and computation time for all combinations of classifiers and algorithms. Code Template: ```python import numpy as np from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.datasets import load_iris import time # Load dataset (example using Iris, replace with your dataset if needed) data = load_iris() X = data.data y = data.target # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Define parameters n_neighbors = 3 radius = 1.0 algorithms = [\'ball_tree\', \'kd_tree\', \'brute\'] # Function to evaluate k-NN and r-NN classifiers def evaluate_classifiers(): results = {} for algo in algorithms: clf_knn = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=algo) clf_rnn = RadiusNeighborsClassifier(radius=radius, algorithm=algo) # Train and evaluate KNeighborsClassifier start_time = time.time() clf_knn.fit(X_train, y_train) y_pred_knn = clf_knn.predict(X_test) knn_acc = accuracy_score(y_test, y_pred_knn) knn_time = time.time() - start_time # Train and evaluate RadiusNeighborsClassifier start_time = time.time() clf_rnn.fit(X_train, y_train) y_pred_rnn = clf_rnn.predict(X_test) rnn_acc = accuracy_score(y_test, y_pred_rnn) rnn_time = time.time() - start_time # Record results results[algo] = { \'k-NN Accuracy\': knn_acc, \'k-NN Time\': knn_time, \'r-NN Accuracy\': rnn_acc, \'r-NN Time\': rnn_time } return results # Run evaluation results = evaluate_classifiers() # Print results for algo in results: print(f\\"Algorithm: {algo}\\") print(f\\"k-NN Accuracy: {results[algo][\'k-NN Accuracy\']} | Time: {results[algo][\'k-NN Time\']} seconds\\") print(f\\"r-NN Accuracy: {results[algo][\'r-NN Accuracy\']} | Time: {results[algo][\'r-NN Time\']} seconds\\") print() ```","solution":"import numpy as np from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.datasets import load_iris import time # Load dataset (example using Iris, replace with your dataset if needed) data = load_iris() X = data.data y = data.target # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Define parameters n_neighbors = 3 radius = 1.0 algorithms = [\'ball_tree\', \'kd_tree\', \'brute\'] # Function to evaluate k-NN and r-NN classifiers def evaluate_classifiers(X_train, X_test, y_train, y_test, n_neighbors=3, radius=1.0, algorithms=[\'ball_tree\', \'kd_tree\', \'brute\']): results = {} for algo in algorithms: clf_knn = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=algo) clf_rnn = RadiusNeighborsClassifier(radius=radius, algorithm=algo) # Train and evaluate KNeighborsClassifier start_time = time.time() clf_knn.fit(X_train, y_train) y_pred_knn = clf_knn.predict(X_test) knn_acc = accuracy_score(y_test, y_pred_knn) knn_time = time.time() - start_time # Train and evaluate RadiusNeighborsClassifier start_time = time.time() clf_rnn.fit(X_train, y_train) y_pred_rnn = clf_rnn.predict(X_test) rnn_acc = accuracy_score(y_test, y_pred_rnn) rnn_time = time.time() - start_time # Record results results[algo] = { \'k-NN Accuracy\': knn_acc, \'k-NN Time\': knn_time, \'r-NN Accuracy\': rnn_acc, \'r-NN Time\': rnn_time } return results # Run evaluation results = evaluate_classifiers(X_train, X_test, y_train, y_test) # Print results for algo in results: print(f\\"Algorithm: {algo}\\") print(f\\"k-NN Accuracy: {results[algo][\'k-NN Accuracy\']} | Time: {results[algo][\'k-NN Time\']} seconds\\") print(f\\"r-NN Accuracy: {results[algo][\'r-NN Accuracy\']} | Time: {results[algo][\'r-NN Time\']} seconds\\") print()"},{"question":"# Nested Tensors for Variable-Length Data Sequences Problem Statement You have been tasked with implementing a function that processes batches of variable-length sequences using PyTorch\'s nested tensors. Specifically, the function should take a list of 2D tensors of varying lengths (but the same number of columns) and perform a specified operation on each tensor in the nested tensor. The operation is element-wise squaring of each tensor. Implement a function `process_nested_tensors` that: - Takes a list of 2D tensors with varying lengths as input. - Constructs a nested tensor using the `torch.jagged` layout. - Applies an element-wise squaring operation to each tensor in the nested tensor. - Returns the resulting nested tensor. Input - `tensor_list`: A list of 2D PyTorch tensors. Each tensor in the list has the same number of columns but can have a different number of rows. Output - A nested tensor of the same shape as the input, where each element is squared. Constraints - The input list must contain at least one tensor. - Each tensor should have the same number of columns. - The function should utilize the `torch.jagged` layout for nested tensors. Example ```python import torch # Example input tensor_list = [ torch.tensor([[1, 2], [3, 4], [5, 6]]), torch.tensor([[7, 8], [9, 10]]) ] # Expected output (each element squared): # [ # [[ 1, 4], [ 9, 16], [25, 36]], # [[49, 64], [81, 100]] # ] result = process_nested_tensors(tensor_list) print([component for component in result]) # Output should match the expected output nested tensor ``` Implementation The implementation should follow these steps: 1. Validate the input to ensure all tensors have the same number of columns and the list is not empty. 2. Create a nested tensor using the `torch.jagged` layout. 3. Apply element-wise squaring to each tensor in the nested tensor. 4. Return the resulting nested tensor. ```python import torch def process_nested_tensors(tensor_list): # Validate input if not tensor_list or any(tensor.ndim != 2 for tensor in tensor_list): raise ValueError(\\"Input must be a list of 2D tensors\\") num_columns = tensor_list[0].shape[1] for tensor in tensor_list: if tensor.shape[1] != num_columns: raise ValueError(\\"All tensors must have the same number of columns\\") # Create nested tensor nt = torch.nested.nested_tensor(tensor_list, layout=torch.jagged) # Apply element-wise squaring squared_nt = nt * nt return squared_nt # Example usage: tensor_list = [ torch.tensor([[1, 2], [3, 4], [5, 6]]), torch.tensor([[7, 8], [9, 10]]) ] result = process_nested_tensors(tensor_list) print([component for component in result]) # Expected output as described ```","solution":"import torch def process_nested_tensors(tensor_list): # Validate input if not tensor_list or any(tensor.ndim != 2 for tensor in tensor_list): raise ValueError(\\"Input must be a list of 2D tensors\\") num_columns = tensor_list[0].shape[1] for tensor in tensor_list: if tensor.shape[1] != num_columns: raise ValueError(\\"All tensors must have the same number of columns\\") # Create nested tensor nt = torch.nested.nested_tensor(tensor_list) # Apply element-wise squaring squared_nt = nt * nt return squared_nt"},{"question":"**Question: Create a Python program using the `sqlite3` module to manage a database of employees. Your program should include the following functionalities:** 1. **Create a connection to a database named `company.db`.** 2. **Create a table named `employee` with columns `id`, `name`, `department`, and `salary`.** - The `id` column should be the primary key. - The `department` column should store the department name. 3. **Insert data into the `employee` table for at least five employees.** 4. **Write a function `fetch_high_salary_employees(min_salary)` that queries and returns employees with a salary greater than `min_salary`. This function should return the results as a list of dictionaries where each dictionary represents an employee.** 5. **Write a function `update_department(employee_id, new_department)` that updates the department of an employee given their `employee_id`.** 6. **Implement exception handling to manage any database errors that might occur during these operations.** **Input and Output Formats:** - The `fetch_high_salary_employees(min_salary)` function should take an integer `min_salary` as input and return a list of dictionaries representing the employees. - The `update_department(employee_id, new_department)` function should take an integer `employee_id` and a string `new_department` as inputs. **Constraints:** 1. Ensure proper connection cleanup and resource management. 2. Use parameterized queries to avoid SQL injection. 3. Handle the case where the employee does not exist when updating the department. **Performance Requirements:** 1. The program should handle up to 10,000 employee records efficiently. ```python import sqlite3 from typing import List, Dict def create_connection(db_name: str) -> sqlite3.Connection: # Create and return a connection to the sqlite database \'db_name\' pass def create_table(con: sqlite3.Connection) -> None: # Create the employee table pass def insert_employees(con: sqlite3.Connection, employees: List[Dict]) -> None: # Insert multiple employee records into the employee table pass def fetch_high_salary_employees(con: sqlite3.Connection, min_salary: int) -> List[Dict]: # Fetch and return employees with salary greater than min_salary pass def update_department(con: sqlite3.Connection, employee_id: int, new_department: str) -> None: # Update the department for the specified employee_id pass # Add any necessary exception handling and additional helper functions here # Main execution block if __name__ == \'__main__\': # Sample employee data employees = [ {\\"id\\": 1, \\"name\\": \\"Alice\\", \\"department\\": \\"HR\\", \\"salary\\": 50000}, {\\"id\\": 2, \\"name\\": \\"Bob\\", \\"department\\": \\"Engineering\\", \\"salary\\": 80000}, {\\"id\\": 3, \\"name\\": \\"Charlie\\", \\"department\\": \\"Sales\\", \\"salary\\": 60000}, {\\"id\\": 4, \\"name\\": \\"David\\", \\"department\\": \\"Marketing\\", \\"salary\\": 55000}, {\\"id\\": 5, \\"name\\": \\"Eve\\", \\"department\\": \\"Engineering\\", \\"salary\\": 90000} ] con = create_connection(\'company.db\') create_table(con) insert_employees(con, employees) # Fetch and print employees with salary > 70000 high_salary_employees = fetch_high_salary_employees(con, 70000) print(high_salary_employees) # Update department for employee with id 2 and print result update_department(con, 2, \\"Product\\") high_salary_employees = fetch_high_salary_employees(con, 70000) print(high_salary_employees) # Close the connection con.close() ```","solution":"import sqlite3 from typing import List, Dict def create_connection(db_name: str) -> sqlite3.Connection: try: con = sqlite3.connect(db_name) return con except sqlite3.Error as e: print(f\\"Error connecting to database: {e}\\") return None def create_table(con: sqlite3.Connection) -> None: try: cursor = con.cursor() cursor.execute(\'\'\' CREATE TABLE IF NOT EXISTS employee ( id INTEGER PRIMARY KEY, name TEXT NOT NULL, department TEXT NOT NULL, salary REAL NOT NULL ) \'\'\') con.commit() except sqlite3.Error as e: print(f\\"Error creating table: {e}\\") def insert_employees(con: sqlite3.Connection, employees: List[Dict]) -> None: try: cursor = con.cursor() cursor.executemany(\'\'\' INSERT INTO employee (id, name, department, salary) VALUES (:id, :name, :department, :salary) \'\'\', employees) con.commit() except sqlite3.Error as e: print(f\\"Error inserting employees: {e}\\") def fetch_high_salary_employees(con: sqlite3.Connection, min_salary: int) -> List[Dict]: try: cursor = con.cursor() cursor.execute(\'\'\' SELECT id, name, department, salary FROM employee WHERE salary > ? \'\'\', (min_salary,)) rows = cursor.fetchall() return [{\\"id\\": row[0], \\"name\\": row[1], \\"department\\": row[2], \\"salary\\": row[3]} for row in rows] except sqlite3.Error as e: print(f\\"Error fetching employees: {e}\\") return [] def update_department(con: sqlite3.Connection, employee_id: int, new_department: str) -> None: try: cursor = con.cursor() cursor.execute(\'\'\' UPDATE employee SET department = ? WHERE id = ? \'\'\', (new_department, employee_id)) con.commit() if cursor.rowcount == 0: print(f\\"No employee found with id {employee_id}\\") except sqlite3.Error as e: print(f\\"Error updating department: {e}\\") # Main execution block if __name__ == \'__main__\': # Sample employee data employees = [ {\\"id\\": 1, \\"name\\": \\"Alice\\", \\"department\\": \\"HR\\", \\"salary\\": 50000}, {\\"id\\": 2, \\"name\\": \\"Bob\\", \\"department\\": \\"Engineering\\", \\"salary\\": 80000}, {\\"id\\": 3, \\"name\\": \\"Charlie\\", \\"department\\": \\"Sales\\", \\"salary\\": 60000}, {\\"id\\": 4, \\"name\\": \\"David\\", \\"department\\": \\"Marketing\\", \\"salary\\": 55000}, {\\"id\\": 5, \\"name\\": \\"Eve\\", \\"department\\": \\"Engineering\\", \\"salary\\": 90000} ] con = create_connection(\'company.db\') if con: create_table(con) insert_employees(con, employees) # Fetch and print employees with salary > 70000 high_salary_employees = fetch_high_salary_employees(con, 70000) print(high_salary_employees) # Update department for employee with id 2 and print result update_department(con, 2, \\"Product\\") high_salary_employees = fetch_high_salary_employees(con, 70000) print(high_salary_employees) # Close the connection con.close()"},{"question":"You are given a series of text documents, and your task is to build a helper function that can analyze and summarize the differences between two given texts using the `difflib` module. This will help in identifying changes made to a document over different versions. # Function Description Write a function `compare_texts` that takes two strings `text1` and `text2` as input and returns a summary of the differences between these texts. # Input * `text1` (str): The first text document to compare. * `text2` (str): The second text document to compare. # Output * `summary` (str): A string containing a line-by-line summary of the differences between `text1` and `text2`. Each line in the summary should indicate: * Lines that differ between the texts or are present in only one of the texts. * Use a notation where lines from `text1` are prefixed with \\"- \\" and lines from `text2` are prefixed with \\"+ \\". * If lines are the same, they should not be included in the summary. # Constraints * The input strings `text1` and `text2` can be of any length. * You should use the `difflib` module to compute the differences. # Example Usage ```python text1 = This is a sample text. It has multiple lines. It can be compared to another text. text2 = This is a sample text. It contains several lines. It will be compared to another text. print(compare_texts(text1, text2)) ``` # Example Output ``` - It has multiple lines. + It contains several lines. - It can be compared to another text. + It will be compared to another text. ``` In this example, lines that exist in one text but not in the other are marked accordingly. Use the `difflib` module\'s capabilities to identify and mark these differences accurately.","solution":"import difflib def compare_texts(text1, text2): Return a summary of the differences between text1 and text2. Parameters: text1 (str): The first text document to compare. text2 (str): The second text document to compare. Returns: str: A string containing the differences between text1 and text2, with lines prefixed by \'- \' or \'+ \'. # Split the texts into lines lines1 = text1.splitlines() lines2 = text2.splitlines() # Create a Differ object d = difflib.Differ() # Compute the differences diff = list(d.compare(lines1, lines2)) # Filter out lines that are the same result = [line for line in diff if line.startswith(\\"- \\") or line.startswith(\\"+ \\")] # Join the result into a single string return \\"n\\".join(result)"},{"question":"**Python Library Comprehension Assessment** **Objective:** Develop a Python program that processes and analyzes a collection of text data. The program should be capable of reading multiple text files, performing text processing, and generating summary statistics. The tasks will involve working with file I/O, string operations, regular expressions, and data persistence. **Task:** 1. **Reading Files:** - Implement a function `read_files(file_paths: List[str]) -> List[str]` that takes a list of file paths and returns a list of strings, where each string contains the contents of one file. 2. **Text Processing:** - Implement a function `process_texts(texts: List[str]) -> List[str]` to clean and normalize the text. This function should: - Convert all text to lowercase. - Remove punctuation using regular expressions. - Strip leading and trailing whitespace. 3. **Word Frequency Analysis:** - Implement a function `word_frequency(texts: List[str]) -> Dict[str, int]` that computes and returns a dictionary with words as keys and their frequencies as values. 4. **Data Persistence:** - Implement a function `save_to_csv(word_freq: Dict[str, int], output_file: str)` that saves the word frequency dictionary to a CSV file. **Constraints:** - Assume that the input files are small enough to fit into memory. - The program should handle exceptions gracefully, including cases where files might not be accessible or contain non-text data. **Expected Input and Output:** 1. `read_files(file_paths: List[str]) -> List[str]` - Input: A list of file paths (e.g., `[\'file1.txt\', \'file2.txt\']`) - Output: A list of strings, each corresponding to the content of one file. 2. `process_texts(texts: List[str]) -> List[str]` - Input: A list of raw text strings. - Output: A list of cleaned and normalized text strings. 3. `word_frequency(texts: List[str]) -> Dict[str, int]` - Input: A list of cleaned text strings. - Output: A dictionary with words as keys and their frequencies as values. 4. `save_to_csv(word_freq: Dict[str, int], output_file: str)` - Input: A word frequency dictionary and an output file path. - Output: A CSV file written to the specified location with words and their frequencies. **Performance Requirements:** - The operations should be efficient with respect to both time and space. - The program should favor readability and code clarity over extreme optimization. **Additional Notes:** - Use the `os` module for handling file paths. - Use the `re` module for regular expressions. - Use the `csv` module for writing data to a CSV file. ```python from typing import List, Dict import os import re import csv def read_files(file_paths: List[str]) -> List[str]: texts = [] for path in file_paths: try: with open(path, \'r\') as file: texts.append(file.read()) except Exception as e: print(f\\"Error reading file {path}: {e}\\") return texts def process_texts(texts: List[str]) -> List[str]: processed_texts = [] for text in texts: text = text.lower() text = re.sub(r\'[^ws]\', \'\', text) text = text.strip() processed_texts.append(text) return processed_texts def word_frequency(texts: List[str]) -> Dict[str, int]: freq_dict = {} for text in texts: words = text.split() for word in words: if word in freq_dict: freq_dict[word] += 1 else: freq_dict[word] = 1 return freq_dict def save_to_csv(word_freq: Dict[str, int], output_file: str): with open(output_file, \'w\', newline=\'\') as csvfile: writer = csv.writer(csvfile) writer.writerow([\'Word\', \'Frequency\']) for word, freq in word_freq.items(): writer.writerow([word, freq]) # Example usage file_paths = [\'file1.txt\', \'file2.txt\'] texts = read_files(file_paths) cleaned_texts = process_texts(texts) freq_dict = word_frequency(cleaned_texts) save_to_csv(freq_dict, \'word_frequencies.csv\') ``` **Submit your code implementation along with a brief explanation of how it works.**","solution":"from typing import List, Dict import os import re import csv def read_files(file_paths: List[str]) -> List[str]: texts = [] for path in file_paths: try: with open(path, \'r\') as file: texts.append(file.read()) except Exception as e: print(f\\"Error reading file {path}: {e}\\") return texts def process_texts(texts: List[str]) -> List[str]: processed_texts = [] for text in texts: text = text.lower() text = re.sub(r\'[^ws]\', \'\', text) text = text.strip() processed_texts.append(text) return processed_texts def word_frequency(texts: List[str]) -> Dict[str, int]: freq_dict = {} for text in texts: words = text.split() for word in words: if word in freq_dict: freq_dict[word] += 1 else: freq_dict[word] = 1 return freq_dict def save_to_csv(word_freq: Dict[str, int], output_file: str): with open(output_file, \'w\', newline=\'\') as csvfile: writer = csv.writer(csvfile) writer.writerow([\'Word\', \'Frequency\']) for word, freq in word_freq.items(): writer.writerow([word, freq])"},{"question":"Seaborn Style Management and Plotting Using the `seaborn` library, you are required to create a function that: 1. Retrieves and prints the current seaborn style settings. 2. Prints the style settings of a user-defined seaborn style parameter. 3. Creates a bar plot with a style temporarily set by user input and displays it using the `seaborn` library. The function signature should look like this: ```python def style_and_plot(style_name: str, x_values: list, y_values: list) -> None: ``` # Input - `style_name`: A string representing the seaborn style (e.g., \\"darkgrid\\", \\"whitegrid\\"). - `x_values`: A list of numerical values to be used for the x-axis of the bar plot. - `y_values`: A list of numerical values to be used for the y-axis of the bar plot. # Output - The function should print the current seaborn style settings. - The function should print the seaborn style settings for `style_name`. - The function should display a bar plot with the style temporarily set to `style_name`. # Example ```python style_and_plot(\\"whitegrid\\", [1, 2, 3, 4], [4, 3, 2, 1]) ``` Expected output: 1. Current seaborn style settings printed. 2. Seaborn style settings for \\"whitegrid\\" printed. 3. A bar plot displayed with `x_values` in the x-axis and `y_values` in the y-axis using the \\"whitegrid\\" style. # Constraints - You can assume `x_values` and `y_values` are lists of numerical values and are of the same length. **Note**: Make sure to import necessary packages within the function.","solution":"import seaborn as sns import matplotlib.pyplot as plt def style_and_plot(style_name: str, x_values: list, y_values: list) -> None: Retrieves and prints the current seaborn style settings. Prints the style settings of a user-defined seaborn style parameter. Creates a bar plot with a style temporarily set by user input and displays it. # Print the current seaborn style settings current_style = sns.axes_style() print(\\"Current seaborn style settings:\\") print(current_style) # Print the seaborn style settings for `style_name` user_defined_style = sns.axes_style(style_name) print(f\\"nSeaborn style settings for \'{style_name}\':\\") print(user_defined_style) # Set the seaborn style to `style_name` temporarily with sns.axes_style(style_name): # Create the bar plot plt.figure(figsize=(10, 6)) sns.barplot(x=x_values, y=y_values) # Display the bar plot plt.show()"},{"question":"**Problem Statement:** You are given a scenario where you need to perform several operations efficiently on an XPU (e.g., a GPU) using the `torch.xpu` module. Implement a function `process_on_xpu(data, device_id)` that performs the required operations as described below. # Function Signature ```python def process_on_xpu(data: List[int], device_id: int) -> int: pass ``` # Input - `data`: A list of integers. - `device_id`: An integer representing the id of the XPU device to be used. # Output - Returns an integer which is the sum of squares of the input data processed on the specified XPU device. # Instructions 1. **Device Initialization**: Check if the specified device is available and initialize it. Raise an error if the device is not available. 2. **Random Seed Initialization**: Set the random seed to ensure reproducibility. 3. **Memory Management**: Ensure that you monitor the memory usage before and after processing the data, and clean up any allocated memory properly. 4. **Data Processing**: Perform the sum of squares computation on the input data using XPU-enabled PyTorch operations. 5. **Stream Handling**: Use custom streams effectively to perform concurrent operations. # Constraints - You must use `torch.xpu` functionalities as much as possible. - Ensure that memory usage is optimized and there are no memory leaks. - Your solution should handle exceptions (e.g., unavailable device). # Example ```python data = [1, 2, 3, 4] device_id = 0 output = process_on_xpu(data, device_id) print(output) # Expected: 30 ``` # Notes - The above example calculates the sum of squares as (1^2 + 2^2 + 3^2 + 4^2 = 30). - Ensure that the operations are performed on the specified `device_id`. Hint: You may use `manual_seed` for setting the RNG state, `memory_allocated` and `empty_cache` for memory management, and `Stream` for stream handling.","solution":"import torch def process_on_xpu(data, device_id): Perform sum of squares of the input data on a specified XPU device. Parameters: data (List[int]): List of integers. device_id (int): ID of the XPU device. Returns: int: Sum of squares of the input data. # Check if XPU device is available if not torch.xpu.is_available(): raise RuntimeError(f\\"XPU device with id {device_id} is not available.\\") device = torch.device(f\'xpu:{device_id}\') # Set random seed for reproducibility torch.manual_seed(0) # Convert data to tensor and move to specified device input_tensor = torch.tensor(data, device=device) # Calculate sum of squares on the XPU result = torch.sum(input_tensor ** 2).item() # Perform necessary memory management tasks (like empty cache) torch.xpu.empty_cache() return result"},{"question":"You are required to create an enumeration for traffic lights using the `enum` module, along with a function to manage the traffic light sequence in a simulation. TrafficLight Enum 1. Define an enumeration `TrafficLight` with the following colors: - RED - GREEN - YELLOW 2. Ensure that the `TrafficLight` enum uses auto-incremented values starting from 1. TrafficLightSequence Class 3. Create a class `TrafficLightSequence` that initializes with a starting traffic light color and has the following methods: - `next_light()`: This method should return the next traffic light in the sequence. The order should be RED -> GREEN -> YELLOW -> RED. - `current_light()`: This method should return the current traffic light. Usage and Constraints: - Implement the `TrafficLight` enum using the `auto()` function. - Ensure that the `TrafficLightSequence` class properly follows the traffic light sequence. - You may not use any external libraries other than `enum`. Input/Output Specifications: - Function `next_light()` should return the next traffic light as defined by the sequence. - Function `current_light()` should return the current traffic light. Example: ```python from enum import Enum, auto class TrafficLight(Enum): RED = auto() GREEN = auto() YELLOW = auto() class TrafficLightSequence: def __init__(self, initial_light: TrafficLight): self.current = initial_light def next_light(self): if self.current == TrafficLight.RED: self.current = TrafficLight.GREEN elif self.current == TrafficLight.GREEN: self.current = TrafficLight.YELLOW elif self.current == TrafficLight.YELLOW: self.current = TrafficLight.RED return self.current def current_light(self): return self.current # Usage example sequence = TrafficLightSequence(TrafficLight.RED) print(sequence.current_light()) # Output: TrafficLight.RED print(sequence.next_light()) # Output: TrafficLight.GREEN print(sequence.next_light()) # Output: TrafficLight.YELLOW print(sequence.next_light()) # Output: TrafficLight.RED ``` # Constraints: - All methods should handle the traffic light sequence as described. - You can assume the sequence will not start with an invalid value. Implement the `TrafficLight` enum and `TrafficLightSequence` class according to the specifications.","solution":"from enum import Enum, auto class TrafficLight(Enum): RED = auto() GREEN = auto() YELLOW = auto() class TrafficLightSequence: def __init__(self, initial_light: TrafficLight): self.current = initial_light def next_light(self): if self.current == TrafficLight.RED: self.current = TrafficLight.GREEN elif self.current == TrafficLight.GREEN: self.current = TrafficLight.YELLOW elif self.current == TrafficLight.YELLOW: self.current = TrafficLight.RED return self.current def current_light(self): return self.current"},{"question":"# Python Coding Assessment Question Objective: To assess the students\' understanding of the `fractions` module and their ability to use various constructors and methods within the `Fraction` class effectively. Problem Statement: Write a function `process_fractions(data: List[Union[str, Tuple[int, int], float, Decimal]]) -> List[str]` that accepts a list of different types of data representing fractions and returns a list of strings representing those fractions in their simplest form. The input can be: 1. A string representing a fraction (e.g., \'3/4\', \'0.5\'). 2. A tuple of two integers representing the numerator and denominator (e.g., `(3, 4)`). 3. A floating point number (e.g., `0.5`). 4. An instance of `decimal.Decimal` representing a decimal number. Your function should: - Convert each element in the list to a `Fraction` instance. - Return a list of strings where each string is the simplest form `\\"numerator/denominator\\"` of the fraction. Constraints: 1. The denominator should never be zero; if encountered, raise a `ZeroDivisionError`. 2. The input list can contain up to 1000 elements. 3. Conversion should maintain the value\'s accuracy to the greatest extent possible. Example: ```python from decimal import Decimal from typing import List, Union, Tuple def process_fractions(data: List[Union[str, Tuple[int, int], float, Decimal]]) -> List[str]: pass # Implement your solution here # Example Usage: data = [\'3/4\', (1, 2), 0.25, Decimal(\'0.75\')] print(process_fractions(data)) # Output: [\'3/4\', \'1/2\', \'1/4\', \'3/4\'] ``` # Note: - You might need to handle different input types appropriately, ensuring the correct use of the `Fraction` class constructors. - Pay attention to potential conversion issues with floating points. **Good Luck!**","solution":"from fractions import Fraction from typing import List, Union, Tuple from decimal import Decimal def process_fractions(data: List[Union[str, Tuple[int, int], float, Decimal]]) -> List[str]: result = [] for item in data: if isinstance(item, str): frac = Fraction(item) elif isinstance(item, Tuple) and len(item) == 2: if item[1] == 0: raise ZeroDivisionError(\\"Denominator cannot be zero\\") frac = Fraction(item[0], item[1]) elif isinstance(item, float): frac = Fraction(item) elif isinstance(item, Decimal): frac = Fraction(float(item)) else: raise ValueError(\\"Unsupported data type\\") result.append(f\\"{frac.numerator}/{frac.denominator}\\") return result"},{"question":"- Advanced `doctest` Implementation **Objective:** Implement a custom `doctest` runner that enhances some of the capabilities found in the Python Standard Library\'s `doctest` module. # Task Description: You are to write a Python function `custom_doctest(module)`, which takes a Python module as input and performs the following: 1. Extracts all the docstring examples from the module and its classes, methods, and functions. 2. Executes the extracted examples and captures their output. 3. Compares the actual output with the expected output as described in the docstrings. 4. Handles exceptions that may occur during the example executions. 5. Reports the results of the tests, showing both successful and failed examples in a clear format. # Requirements: 1. Use regular expressions or string parsing to extract examples from docstrings. 2. Use the Python `exec` function to execute the code examples dynamically. 3. Implement custom logic for output comparison and exception handling. 4. Provide a summary that reports: - Total number of examples. - Number of successful vs. failed examples. 5. Include functional-level comments/docstrings according to `doctest` convention with examples that `custom_doctest` should be able to parse and validate. # Input: - A Python module object or the module name as a string. # Output: - A printed summary report showing the results of all the docstring examples: ``` Total examples: X Successful examples: Y Failed examples: Z Details of failed examples: - Example from <module name/function/method> at line N: Expected: <expected output> Got: <actual output> ``` # Constraints: - Do not use the built-in `doctest.testmod` function or other high-level helpers provided by `doctest`. - The examples in docstrings must be defined in the standard Python shell format (e.g., using `>>>`). - Assume that all code in the examples can be executed in the global context of the provided module. # Example Use of `custom_doctest`: ```python This example is part of the module-level documentation. >>> add(1, 2) 3 >>> divide(4, 2) 2.0 def add(a, b): Adds two numbers. >>> add(3, 4) 7 >>> add(-1, 1) 0 return a + b def divide(a, b): Divides a by b. >>> divide(9, 3) 3.0 >>> divide(1, 0) Traceback (most recent call last): ... ZeroDivisionError: division by zero return a / b if __name__ == \\"__main__\\": custom_doctest(\\"example\\") # Assuming the script is named example.py ``` # Expected Output: ``` Total examples: 6 Successful examples: 5 Failed examples: 1 Details of failed examples: - Example from example.divide at line 16: Expected: Traceback (most recent call last): ... ZeroDivisionError: division by zero Got: <actual exception traceback> ```","solution":"import re def custom_doctest(module): Custom doctest runner that extracts and runs examples from docstrings. Parameters: module: A Python module name as a string or a module object. Returns: None. Prints a summary report. if isinstance(module, str): module = __import__(module) examples = extract_examples(module) total = len(examples) success_count = 0 failed_examples = [] for example in examples: expected_output = example[\'expected\'] actual_output = execute_example(example[\'code\']) if actual_output == expected_output: success_count += 1 else: failed_examples.append({ \'example\': example, \'actual_output\': actual_output }) failed_count = total - success_count print(f\'Total examples: {total}\') print(f\'Successful examples: {success_count}\') print(f\'Failed examples: {failed_count}\') if failed_count > 0: print(\'nDetails of failed examples:\') for failed in failed_examples: example = failed[\'example\'] print(f\\"- Example from {example[\'name\']} at line {example[\'line\']}:\\") print(f\\" Expected: {example[\'expected\']}\\") print(f\\" Got: {failed[\'actual_output\']}\\") def extract_examples(module): Extracts examples from the docstrings of a module and its members. Parameters: module: A Python module object. Returns: A list of dictionaries with keys \'name\', \'code\', \'expected\', and \'line\'. import inspect examples = [] def parse_docstring(docstring, name, line): if not docstring: return for match in re.finditer(r\'(?ms)^>>> (.+?)(?=^>>> |Z)\', docstring): code = match.group(1).strip() expected = get_expected_output(code) if expected is not None: examples.append({ \'name\': name, \'code\': code, \'expected\': expected, \'line\': line + docstring[:match.start()].count(\'n\') + 1 }) for name, member in inspect.getmembers(module): if inspect.isfunction(member) or inspect.isclass(member): parse_docstring(member.__doc__, name, inspect.getsourcelines(member)[1]) if inspect.isclass(member): for mname, mmember in inspect.getmembers(member): if inspect.isfunction(mmember): parse_docstring(mmember.__doc__, f\\"{name}.{mname}\\", inspect.getsourcelines(mmember)[1]) parse_docstring(module.__doc__, f\\"{module.__name__}\\", 0) return examples def get_expected_output(code): Gets the expected output from a docstring example. Parameters: code: A string containing the example code. Returns: The expected output as a string or None if no output is expected. try: output = eval(code) return repr(output) except: lines = code.split(\'n\') if lines[-1].startswith(\'raise\') or \'Traceback\' in lines[-1]: return \'n\'.join(lines[1:]).strip() return None def execute_example(code): Executes an example and captures its output. Parameters: code: A string containing the example code. Returns: The actual output as a string. try: exec_globals = {} exec(code, exec_globals) return repr(exec_globals.get(\'_\')) except Exception as e: return repr(e) # Example function for testing This module is an example with some functions for custom doctest. >>> add(1, 2) 3 >>> divide(4, 2) 2.0 def add(a, b): Adds two numbers. >>> add(3, 4) 7 >>> add(-1, 1) 0 return a + b def divide(a, b): Divides a by b. >>> divide(9, 3) 3.0 >>> divide(1, 0) Traceback (most recent call last): ... ZeroDivisionError: division by zero return a / b if __name__ == \\"__main__\\": custom_doctest(__name__)"},{"question":"# **Coding Assessment Question** Overview You are required to write a Python function that dynamically fetches certain configuration details about the current Python installation and returns these details in a specific format. This involves understanding and utilizing the `sysconfig` module functions. Task Implement a function named `get_python_configuration_details()` that returns a dictionary with the following keys and corresponding values: - `\\"platform\\"`: The current platform (use `sysconfig.get_platform()`). - `\\"python_version\\"`: The current Python version in \\"MAJOR.MINOR\\" format (use `sysconfig.get_python_version()`). - `\\"default_scheme\\"`: The default installation scheme for the current platform (use `sysconfig.get_default_scheme()`). - `\\"purelib_path\\"`: The installation path for \\"purelib\\" under the default scheme (use `sysconfig.get_path(\\"purelib\\")`). - `\\"LIBDIR\\"`: The value of the configuration variable \'LIBDIR\' (use `sysconfig.get_config_var(\\"LIBDIR\\")`). Constraints - Do not use hardcoded values; your function should work dynamically based on the configuration of the Python environment it runs in. - Ensure the function returns `None` if a requested value or path cannot be found. Function Signature ```python def get_python_configuration_details() -> dict: pass ``` Example ```python import sysconfig def get_python_configuration_details() -> dict: config_details = { \\"platform\\": sysconfig.get_platform(), \\"python_version\\": sysconfig.get_python_version(), \\"default_scheme\\": sysconfig.get_default_scheme(), \\"purelib_path\\": sysconfig.get_path(\\"purelib\\"), \\"LIBDIR\\": sysconfig.get_config_var(\\"LIBDIR\\") } return config_details # Example Usage details = get_python_configuration_details() print(details) ``` Expected Output (the actual output will vary depending on the environment): ```python { \\"platform\\": \\"linux-x86_64\\", \\"python_version\\": \\"3.10\\", \\"default_scheme\\": \\"posix_prefix\\", \\"purelib_path\\": \\"/usr/local/lib/python3.10/site-packages\\", \\"LIBDIR\\": \\"/usr/local/lib\\" } ``` Note Ensure your function is well-documented and includes all necessary imports. The accuracy and correctness of values will be verified based on the environment within which the test is run.","solution":"import sysconfig def get_python_configuration_details() -> dict: Fetch the current Python configuration details and return them as a dictionary. try: platform = sysconfig.get_platform() except Exception: platform = None try: python_version = sysconfig.get_python_version() except Exception: python_version = None try: default_scheme = sysconfig.get_default_scheme() except Exception: default_scheme = None try: purelib_path = sysconfig.get_path(\\"purelib\\") except Exception: purelib_path = None try: libdir = sysconfig.get_config_var(\\"LIBDIR\\") except Exception: libdir = None config_details = { \\"platform\\": platform, \\"python_version\\": python_version, \\"default_scheme\\": default_scheme, \\"purelib_path\\": purelib_path, \\"LIBDIR\\": libdir } return config_details"},{"question":"# Advanced Python Coding Challenge: Custom Pareto Distribution Generator Objective: Your task is to implement a custom random number generator class for the Pareto distribution, named `CustomParetoRandom`, by subclassing the `random.Random` class. You should override the necessary methods to ensure the generator is correctly seeded and maintains its state. The Pareto distribution function is defined as: [ P(x; alpha, x_m) = left( frac{x_m}{x} right)^alpha ] where: - ( x_m ) is the minimum possible value, - ( alpha ) is the shape parameter (must be greater than 0), - ( x geq x_m ). Requirements: 1. **Class Definition**: - Define a class `CustomParetoRandom` that subclasses `random.Random`. 2. **Pareto Distribution**: - Implement a method `pareto_variate(self, alpha, xm)` to generate a random float following a Pareto distribution with given parameters (alpha) and (x_m). 3. **Seeding and State Management**: - Override the `random(self)` method to generate random floats. - Implement the `seed(self, a=None)` method to initialize the random number generator. - Implement methods `getstate(self)` and `setstate(self, state)` to handle the generator\'s state. Implementation Details: - Your class should ensure that the sequence of generated numbers is reproducible when the same seed is provided. - Ensure the implementation of the `pareto_variate(self, alpha, xm)` method conforms to the mathematical definition of the Pareto distribution. - Utilize the core functionality of the `random.Random` class wherever appropriate. Example Usage: ```python # Create an instance of the custom generator generator = CustomParetoRandom() # Seed the generator generator.seed(123) # Generate Pareto distributed random numbers print(generator.pareto_variate(2.5, 1.0)) # Example output: 1.318 print(generator.pareto_variate(2.5, 1.0)) # Example output: 1.107 # Capture and restore state state = generator.getstate() print(generator.pareto_variate(2.5, 1.0)) # Example output: 1.573 generator.setstate(state) print(generator.pareto_variate(2.5, 1.0)) # Should match the previous output: 1.573 ``` Constraints: - The value of (alpha) must be greater than 0. - The value of (x_m) must be greater than 0. Submission: Submit your implementation as a Python script containing the complete `CustomParetoRandom` class definition, ensuring it meets the requirements specified above.","solution":"import random class CustomParetoRandom(random.Random): def pareto_variate(self, alpha, xm): Generate a random float following the Pareto distribution with shape parameter alpha and minimum value xm. if alpha <= 0: raise ValueError(\\"alpha must be greater than 0\\") if xm <= 0: raise ValueError(\\"xm must be greater than 0\\") # Generate a uniformly random number in the range (0, 1) u = self.random() return xm / (u ** (1.0 / alpha)) def random(self): Generate a random float uniformly in the range [0.0, 1.0). return super().random() def seed(self, a=None): Initialize the random number generator. super().seed(a) def getstate(self): Return the internal state of the generator. return super().getstate() def setstate(self, state): Restore the internal state of the generator. super().setstate(state)"},{"question":"You have been tasked with developing a logging utility using the `syslog` module that will log different levels of messages to the system logger. Your utility should also be able to dynamically change the log facility and priority mask. Requirements: 1. Implement a class `SyslogUtility` with the following methods: - `__init__(self, ident=None, logoption=0, facility=syslog.LOG_USER)`: Initializes the utility and calls `syslog.openlog()` with the provided parameters. - `log_message(self, level, message)`: Logs the message with the specified priority level using `syslog.syslog()`. - `change_facility(self, new_facility)`: Changes the logging facility by re-initializing the log with the new facility. - `set_priority_mask(self, mask)`: Sets the priority mask for logging using `syslog.setlogmask()`. - `close(self)`: Closes the log using `syslog.closelog()`. 2. Your solution should: - Ensure logging works correctly by using appropriate methods and default values. - Handle errors gracefully and ensure the log state is maintained correctly when changing facilities or setting priority masks. Input and Output formats: - **Initialization**: - `SyslogUtility(ident=None, logoption=0, facility=syslog.LOG_USER)` - **Logging a message**: - `log_message(level, message)` - Example: `log_message(syslog.LOG_ERR, \'Error message\')` - **Changing the facility**: - `change_facility(new_facility)` - Example: `change_facility(syslog.LOG_MAIL)` - **Setting the priority mask**: - `set_priority_mask(mask)` - Example: `set_priority_mask(syslog.LOG_UPTO(syslog.LOG_ERR))` - **Closing the log**: - `close()` - Example: `close()` Constraints: - You can use any priority level and facility constants defined in the `syslog` module. - Ensure all methods are called in a sequence that maintains the integrity of the log. Example: ```python import syslog class SyslogUtility: def __init__(self, ident=None, logoption=0, facility=syslog.LOG_USER): self.ident = ident self.logoption = logoption self.facility = facility syslog.openlog(ident, logoption, facility) def log_message(self, level, message): syslog.syslog(level, message) def change_facility(self, new_facility): syslog.openlog(self.ident, self.logoption, new_facility) self.facility = new_facility def set_priority_mask(self, mask): syslog.setlogmask(mask) def close(self): syslog.closelog() # Example of usage: logger = SyslogUtility(ident=\'MyApp\', logoption=syslog.LOG_PID, facility=syslog.LOG_MAIL) logger.log_message(syslog.LOG_INFO, \'This is an info message\') logger.change_facility(syslog.LOG_CRON) logger.log_message(syslog.LOG_ERR, \'This is an error message\') logger.set_priority_mask(syslog.LOG_UPTO(syslog.LOG_WARNING)) logger.log_message(syslog.LOG_DEBUG, \'This is a debug message and should not be logged\') logger.close() ``` Evaluation: Your solution will be evaluated based on: - Correct implementation of the logging utility methods. - Proper usage of the `syslog` functions and constants. - Handling of errors and maintaining log state.","solution":"import syslog class SyslogUtility: def __init__(self, ident=None, logoption=0, facility=syslog.LOG_USER): self.ident = ident self.logoption = logoption self.facility = facility syslog.openlog(ident, logoption, facility) def log_message(self, level, message): Logs the message with the specified priority level. syslog.syslog(level, message) def change_facility(self, new_facility): Changes the logging facility and re-initializes the log. syslog.openlog(self.ident, self.logoption, new_facility) self.facility = new_facility def set_priority_mask(self, mask): Sets the priority mask for logging. syslog.setlogmask(mask) def close(self): Closes the log. syslog.closelog()"},{"question":"Objective Demonstrate your understanding of the \\"secrets\\" module in Python by implementing a function that generates a secure random password and a URL containing a security token. Your function should use various methods from the \\"secrets\\" module to ensure cryptographic security. Problem Statement Create a function `create_secure_credentials(username)` that takes a `username` string as input and returns a dictionary containing two key-value pairs: 1. `password`: A 12-character long password that includes at least one lowercase letter, one uppercase letter, one digit, and one special character from the set `!@#%^&*`. 2. `reset_url`: A URL-safe string for password recovery that combines a base URL with a securely generated token. Use the base URL: `\'https://example.com/reset=\'`. Input - `username` (str): The username for which the credentials are being created. Output - `credentials` (dict): A dictionary with the following structure: ```python { \\"password\\": \\"<generated_password>\\", \\"reset_url\\": \\"<generated_url>\\" } ``` Constraints - The generated password must be exactly 12 characters long. - The password must include at least: - One lowercase letter. - One uppercase letter. - One digit. - One special character from the set `!@#%^&*`. - The URL must be secure and contain a token with at least 32 bytes of randomness. Example ```python def create_secure_credentials(username): # Implementation to be added by the student. credentials = create_secure_credentials(\'john_doe\') print(credentials) # Expected output (Example): # { # \\"password\\": \\"A1b!C2d@E3f#\\", # \\"reset_url\\": \\"https://example.com/reset=Drmhze6EPcv0fN_81Bj-nA\\" # } ``` Note You can assume that any standard library needed (e.g., `string` for character sets) is already imported or available without needing to add import statements in your function.","solution":"import secrets import string def create_secure_credentials(username): Generates a secure random password and a URL containing a security token. Args: - username (str): The username for which the credentials are being created. Returns: - dict: A dictionary containing the password and reset URL. # Define character sets lower = string.ascii_lowercase upper = string.ascii_uppercase digits = string.digits special = \'!@#%^&*\' # Ensure the password contains at least one of each required character type password = [ secrets.choice(lower), secrets.choice(upper), secrets.choice(digits), secrets.choice(special) ] # Fill the remaining characters remaining_length = 12 - len(password) all_chars = lower + upper + digits + special password += [secrets.choice(all_chars) for _ in range(remaining_length)] # Shuffle the password list to avoid predictable patterns secrets.SystemRandom().shuffle(password) # Join the list to form the final password string final_password = \'\'.join(password) # Generate a secure token for the reset URL token = secrets.token_urlsafe(32) reset_url = f\'https://example.com/reset={token}\' return { \\"password\\": final_password, \\"reset_url\\": reset_url }"},{"question":"# Python Configuration Using Python C-API You are tasked with writing a Python script that mimics the behavior of configuring and initializing Python as described in the provided documentation. Your script should: 1. **Set up a configuration for an isolated mode** environment using the configuration functions. 2. **Handle a list of command-line arguments**. 3. **Incorporate `PyWideStringList` to manage command-line arguments**. 4. **Simulate status handling** using Python\'s exceptions to align with `PyStatus`. # Detailed Instructions: 1. **Configuration Setup**: - Define a class `PyConfig` with fields similar to the mentioned `PyConfig` structure in the documentation. - Create methods within `PyConfig` class to handle setting of string values, byte string values, and handling wide string lists: - `set_string(config_str, value)` - `set_byte_string(config_str, value)` - `set_wide_string_list(items)` 2. **Command-line Argument Handling**: - Create a method `set_argv(argc, argv)` inside the `PyConfig` class to handle command-line arguments using a mock `PyWideStringList` implementation. 3. **Initialization Simulation**: - Mimic preinitialization and configuration steps, initializing necessary fields of `PyConfig`. - Use these configurations to print or log the settings applied. 4. **Status Handling**: - Define a class `PyStatus` to simulate error, memory exception, and other status types. 5. **Write a Test Case**: - Simulate setting up a Python environment with specific configurations and demonstrate error handling if configurations fail. # Example: To assist you, below is a basic structure you can follow: ```python class PyWideStringList: def __init__(self): self.items = [] self.length = 0 def append(self, item): if isinstance(item, str): self.items.append(item) self.length += 1 else: raise TypeError(\\"Item must be a string\\") class PyStatus: SUCCESS = \\"OK\\" ERROR = \\"ERROR\\" def __init__(self, status=\\"OK\\", err_msg=None): self.status = status self.err_msg = err_msg @staticmethod def ok(): return PyStatus(PyStatus.SUCCESS) @staticmethod def error(msg): return PyStatus(PyStatus.ERROR, msg) def is_exception(self): return self.status == PyStatus.ERROR class PyConfig: def __init__(self): self.argv = PyWideStringList() self.program_name = None self.isolated = 0 def set_string(self, config_str, value): if isinstance(value, str): setattr(self, config_str, value) return PyStatus.ok() return PyStatus.error(\\"Value must be a string\\") def set_byte_string(self, config_str, value): if isinstance(value, bytes): string_value = value.decode(\'utf-8\') return self.set_string(config_str, string_value) return PyStatus.error(\\"Value must be bytes\\") def set_argv(self, argc, argv): try: for arg in argv: self.argv.append(arg) return PyStatus.ok() except Exception as e: return PyStatus.error(str(e)) def main(): config = PyConfig() status = config.set_string(\'program_name\', \'my_program\') if status.is_exception(): print(f\\"Failed to set program name: {status.err_msg}\\") status = config.set_argv(3, [\'arg1\', \'arg2\', \'arg3\']) if status.is_exception(): print(f\\"Failed to set argv: {status.err_msg}\\") else: print(f\\"Initialization successful with argv: {config.argv.items}\\") if __name__ == \\"__main__\\": main() ``` # Expected Output: - Print the configuration settings applied successfully. - Print any errors encountered during the configuration process. # Constraints: - Simulate functionality as close to the documentation as possible. - Implement error handling to cover various setbacks in configuration.","solution":"class PyWideStringList: def __init__(self): self.items = [] self.length = 0 def append(self, item): if isinstance(item, str): self.items.append(item) self.length += 1 else: raise TypeError(\\"Item must be a string\\") class PyStatus: SUCCESS = \\"OK\\" ERROR = \\"ERROR\\" def __init__(self, status=\\"OK\\", err_msg=None): self.status = status self.err_msg = err_msg @staticmethod def ok(): return PyStatus(PyStatus.SUCCESS) @staticmethod def error(msg): return PyStatus(PyStatus.ERROR, msg) def is_exception(self): return self.status == PyStatus.ERROR class PyConfig: def __init__(self): self.argv = PyWideStringList() self.program_name = None self.isolated = 0 def set_string(self, config_str, value): if isinstance(value, str): setattr(self, config_str, value) return PyStatus.ok() return PyStatus.error(\\"Value must be a string\\") def set_byte_string(self, config_str, value): if isinstance(value, bytes): string_value = value.decode(\'utf-8\') return self.set_string(config_str, string_value) return PyStatus.error(\\"Value must be bytes\\") def set_argv(self, argc, argv): try: for arg in argv: self.argv.append(arg) return PyStatus.ok() except Exception as e: return PyStatus.error(str(e)) def main(): config = PyConfig() # Setting program name status = config.set_string(\'program_name\', \'my_program\') if status.is_exception(): print(f\\"Failed to set program name: {status.err_msg}\\") # Setting command-line arguments status = config.set_argv(3, [\'arg1\', \'arg2\', \'arg3\']) if status.is_exception(): print(f\\"Failed to set argv: {status.err_msg}\\") else: print(f\\"Initialization successful with argv: {config.argv.items}\\") if __name__ == \\"__main__\\": main()"},{"question":"Using the `pwd` module, write a function `get_all_user_homes_and_shells()` that retrieves the home directory and shell information for all users on the system and returns it as a dictionary. The keys of the dictionary should be the user names, and the values should be tuples containing the home directory and shell. Function Signature ```python def get_all_user_homes_and_shells() -> dict: pass ``` Input - This function does not take any input arguments. Output - Returns a dictionary with user names as keys and a tuple (home directory, shell interpreter) as values. - Example: `{\'root\': (\'/root\', \'/bin/bash\'), \'user1\': (\'/home/user1\', \'/bin/zsh\'), ...}` Constraints - Only valid users (those that can be retrieved without raising a `KeyError`) should be included in the result. Performance Requirements - The function should handle the case when there are hundreds of users efficiently. **Hint:** Use `pwd.getpwall()` to retrieve all user entries and iterate through them to construct the output dictionary. Example ```python # Example usage of the implemented function result = get_all_user_homes_and_shells() print(result) ``` Note: The exact output dictionary will vary depending on the system\'s user database.","solution":"import pwd def get_all_user_homes_and_shells() -> dict: Retrieves home directory and shell information for all users on the system. Returns: dict: A dictionary with user names as keys and tuples (home directory, shell) as values. users_info = {} for user in pwd.getpwall(): users_info[user.pw_name] = (user.pw_dir, user.pw_shell) return users_info"},{"question":"# Question: Asynchronous Web Scraper using `concurrent.futures` You are required to implement a function that fetches the content from a list of URLs asynchronously using `ThreadPoolExecutor` from the `concurrent.futures` module. The function should handle timeouts and exceptions gracefully, and return the results in an organized manner. Specifications: 1. **Function Signature**: ```python def async_web_scraper(urls: List[str], timeout: int) -> List[Tuple[str, Optional[str], Optional[Exception]]]: ``` 2. **Input**: - `urls`: List of URLs (strings) to fetch. - `timeout`: Timeout in seconds for each URL request. 3. **Output**: - A list of tuples: - The first element is the URL (string). - The second element is the content of the URL (string), or `None` if an exception occurs. - The third element is the exception that occurred (if any), or `None` if no exception occurs. 4. **Constraints**: - Use `ThreadPoolExecutor` for asynchronous execution. - Handle exceptions and timeouts using appropriate handling mechanisms provided by `concurrent.futures`. - Ensure threads are cleaned up properly by utilizing the `with` statement when using `ThreadPoolExecutor`. 5. **Example**: ```python urls = [ \\"http://www.foxnews.com/\\", \\"http://www.cnn.com/\\", \\"http://europe.wsj.com/\\", \\"http://www.bbc.co.uk/\\", \\"http://nonexistent-url.org/\\" ] results = async_web_scraper(urls, 10) for url, content, exc in results: if exc is not None: print(f\\"Error fetching {url}: {exc}\\") else: print(f\\"{url} content of length {len(content)} fetched successfully.\\") ``` 6. **Hints**: - Use the `urllib.request.urlopen` method to fetch the URL inside a helper function. - Use `concurrent.futures.as_completed` to handle the futures as they complete. - Add appropriate exception handling for different scenarios like timeouts, HTTP errors, etc. Implement the function `async_web_scraper` below: ```python from typing import List, Tuple, Optional import concurrent.futures import urllib.request import urllib.error def async_web_scraper(urls: List[str], timeout: int) -> List[Tuple[str, Optional[str], Optional[Exception]]]: def fetch_url(url, timeout): try: with urllib.request.urlopen(url, timeout=timeout) as conn: return url, conn.read().decode(\'utf-8\'), None except Exception as e: return url, None, e results = [] with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor: future_to_url = {executor.submit(fetch_url, url, timeout): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): results.append(future.result()) return results ```","solution":"from typing import List, Tuple, Optional import concurrent.futures import urllib.request import urllib.error def async_web_scraper(urls: List[str], timeout: int) -> List[Tuple[str, Optional[str], Optional[Exception]]]: def fetch_url(url, timeout): try: with urllib.request.urlopen(url, timeout=timeout) as conn: return url, conn.read().decode(\'utf-8\'), None except Exception as e: return url, None, e results = [] with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor: future_to_url = {executor.submit(fetch_url, url, timeout): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): results.append(future.result()) return results"},{"question":"Problem Statement You are provided with a dataset containing scores of various models across multiple tasks. Your task is to visualize this data using seaborn\'s heatmap functionality with specific customizations. # Input Format - A CSV file named `model_scores.csv` with the following columns: `Model`, `Task`, `Score`. # Output Format A heatmap visual representation of the data with the following customizations: 1. Display cell values as ranked scores within columns. 2. Use a colormap of your choosing. 3. Add lines between cells for better readability. 4. Control the color mapping range to be between specific values. # Constraints - The ranking should be computed such that the highest score in a column is ranked `1`, the second-highest `2`, and so on. - You must use Seaborn for generating the heatmap and matplotlib for additional customizations. # Performance Requirements - The solution should handle datasets up to 10,000 rows efficiently. # Example Given the following input CSV (`model_scores.csv`): ``` Model,Task,Score Model1,TaskA,90 Model2,TaskA,85 Model3,TaskA,78 Model1,TaskB,88 Model2,TaskB,90 Model3,TaskB,86 ... ``` Your script should perform the following steps: 1. Load the dataset into a DataFrame. 2. Pivot the DataFrame to have Models as rows, Tasks as columns, and Scores as values. 3. Create a heatmap where each cell displays the rank of the score within its column. 4. Apply a colormap to the heatmap. 5. Add lines between cells. 6. Set the colormap normalization with a minimum value (e.g., 50) and a maximum value (e.g., 100). # Solution Template You can use the following template to write your solution: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load data df = pd.read_csv(\'model_scores.csv\') # Step 2: Pivot the DataFrame data_pivoted = df.pivot(index=\'Model\', columns=\'Task\', values=\'Score\') # Step 3: Generate heatmap ax = sns.heatmap(data_pivoted.rank(axis=\'columns\'), anno=True, cmap=\'cividis\', linewidth=0.5, vmin=50, vmax=100) ax.set(xlabel=\'\', ylabel=\'\') ax.xaxis.tick_top() plt.show() ``` # Evaluation Criteria - Correctness: The solution meets all specified requirements and constraints. - Code Quality: Code is clean, well-organized, and includes comments explaining critical sections. - Efficiency: Solution effectively handles the given size constraints.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def generate_heatmap(csv_file, colormap=\'coolwarm\', vmin=None, vmax=None): Generates a heatmap from the given CSV file with specified customizations. Parameters: - csv_file (str): The path to the CSV file containing model scores data. - colormap (str): The colormap to use for the heatmap. - vmin (float): Minimum value for the colormap normalization. - vmax (float): Maximum value for the colormap normalization. # Step 1: Load data df = pd.read_csv(csv_file) # Step 2: Pivot the DataFrame data_pivoted = df.pivot(index=\'Model\', columns=\'Task\', values=\'Score\') # Rank the scores within each column ranked_data = data_pivoted.rank(ascending=False) # Step 3: Generate heatmap plt.figure(figsize=(10, 8)) ax = sns.heatmap(ranked_data, annot=True, cmap=colormap, linewidths=0.5, vmin=vmin, vmax=vmax, fmt=\\".0f\\") # Set labels ax.set_xlabel(\'Task\', fontsize=14) ax.set_ylabel(\'Model\', fontsize=14) # Display the heatmap plt.title(\'Model Scores Heatmap\', fontsize=16) plt.show()"},{"question":"You are tasked with creating a command-line utility using Python\'s `argparse` module named `fileinfo` that prints information about a file. The utility should accept a variety of command-line arguments to specify which pieces of file information to display. The information the utility can display includes the file size, creation date, modification date, and whether the file is a regular file or a directory. Requirements: 1. **Positional Argument:** - `filename` (str): The path to the file whose information is to be displayed. 2. **Optional Arguments:** - `--size` (or `-s`): Displays the size of the file. - `--created` (or `-c`): Displays the creation date of the file. - `--modified` (or `-m`): Displays the last modification date of the file. - `--type` (or `-t`): Displays whether the file is a regular file or a directory. - `--all` (or `-a`): Displays all the above information. 3. **Mutually Exclusive Group:** - `--verbose` (or `-v`): Enables verbose output. - `--quiet` (or `-q`): Disables all output except errors. 4. **Usage:** - The utility should print an appropriate error message if the file does not exist or cannot be accessed. - If no specific information argument is provided by the user, the utility should display all available information by default. 5. **Output:** - If `--size` is specified, display: `Size: <file_size> bytes` - If `--created` is specified, display: `Created: <creation_date>` - If `--modified` is specified, display: `Modified: <modification_date>` - If `--type` is specified, display: `Type: <file_type>` where `<file_type>` is either \\"File\\" or \\"Directory\\". - If `--all` is specified, display all of the above information. 6. **Constraints:** - Assume the input filename is always valid and the file exists. - The date format should be `YYYY-MM-DD HH:MM:SS`. Function Signature: ```python import argparse import os from datetime import datetime def fileinfo(): parser = argparse.ArgumentParser(description=\\"Display information about a file.\\") ... # Complete the implementation as per the above requirements if __name__ == \\"__main__\\": fileinfo() ``` Example Usage: ```sh python fileinfo.py example.txt --size Size: 1024 bytes python fileinfo.py example.txt --created Created: 2023-01-15 14:22:10 python fileinfo.py example.txt --all Size: 1024 bytes Created: 2023-01-15 14:22:10 Modified: 2023-01-20 10:45:00 Type: File python fileinfo.py example.txt -v --size Size: 1024 bytes ``` **Note:** The program should make use of the `os.stat` and `datetime` modules for retrieving and formatting file information. The function should handle mutually exclusive `--verbose` and `--quiet` arguments correctly.","solution":"import argparse import os from datetime import datetime def fileinfo(): parser = argparse.ArgumentParser(description=\\"Display information about a file.\\") parser.add_argument(\\"filename\\", type=str, help=\\"The path to the file\\") parser.add_argument(\\"-s\\", \\"--size\\", action=\\"store_true\\", help=\\"Displays the size of the file\\") parser.add_argument(\\"-c\\", \\"--created\\", action=\\"store_true\\", help=\\"Displays the creation date of the file\\") parser.add_argument(\\"-m\\", \\"--modified\\", action=\\"store_true\\", help=\\"Displays the last modification date of the file\\") parser.add_argument(\\"-t\\", \\"--type\\", action=\\"store_true\\", help=\\"Displays whether the file is a regular file or a directory\\") parser.add_argument(\\"-a\\", \\"--all\\", action=\\"store_true\\", help=\\"Displays all available information about the file\\") group = parser.add_mutually_exclusive_group() group.add_argument(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", help=\\"Enables verbose output\\") group.add_argument(\\"-q\\", \\"--quiet\\", action=\\"store_true\\", help=\\"Disables all output except errors\\") args = parser.parse_args() if not os.path.exists(args.filename): if not args.quiet: print(f\\"Error: File \'{args.filename}\' does not exist or cannot be accessed.\\") return file_stat = os.stat(args.filename) file_info = { \\"size\\": f\\"Size: {file_stat.st_size} bytes\\", \\"created\\": f\\"Created: {datetime.fromtimestamp(file_stat.st_ctime).strftime(\'%Y-%m-%d %H:%M:%S\')}\\", \\"modified\\": f\\"Modified: {datetime.fromtimestamp(file_stat.st_mtime).strftime(\'%Y-%m-%d %H:%M:%S\')}\\", \\"type\\": f\\"Type: {\'Directory\' if os.path.isdir(args.filename) else \'File\'}\\" } if args.all or (not args.size and not args.created and not args.modified and not args.type): output_keys = [\\"size\\", \\"created\\", \\"modified\\", \\"type\\"] else: output_keys = [] if args.size: output_keys.append(\\"size\\") if args.created: output_keys.append(\\"created\\") if args.modified: output_keys.append(\\"modified\\") if args.type: output_keys.append(\\"type\\") if not args.quiet: for key in output_keys: print(file_info[key]) if __name__ == \\"__main__\\": fileinfo()"},{"question":"# PyTorch Tensor Storage Manipulation In this task, you will demonstrate your understanding of PyTorch tensor storage by implementing a function that manipulates the underlying storage of a tensor. Task: Implement a function `modify_tensor_storage` that takes a PyTorch tensor and an integer value as inputs. The function should: 1. Retrieve the underlying untyped storage of the tensor. 2. Clone this storage and fill it with the given integer value. 3. Create a new tensor using this modified storage, keeping the original tensor\'s shape, stride, and offset. 4. Return the newly created tensor. Function Signature: ```python import torch def modify_tensor_storage(tensor: torch.Tensor, value: int) -> torch.Tensor: pass ``` Input: - `tensor` (torch.Tensor): A PyTorch tensor of any shape and dtype. - `value` (int): An integer value with which to fill the cloned storage of the tensor. Output: - A new tensor (torch.Tensor) with the same shape, stride, and offset as the original tensor, but with its storage filled with the specified integer value. Example: ```python import torch # Example tensor t = torch.tensor([1.0, 2.0, 3.0]) # Modify tensor storage with value 7 new_tensor = modify_tensor_storage(t, 7) print(new_tensor) # Expected output (the exact values may vary due to dtype\'s byte representation): # tensor([7., 7., 7.]) ``` Constraints: - Do not modify the original tensor\'s storage in-place. - Ensure that the storage of the new tensor is correctly filled with the given value. - Handle tensors of any valid PyTorch dtype and shape. Notes: - Directly modifying a tensor\'s storage is not a recommended practice in general applications but can be useful for understanding the internal workings of PyTorch. - Utilize PyTorch\'s methods responsibly to create a robust and efficient implementation.","solution":"import torch def modify_tensor_storage(tensor: torch.Tensor, value: int) -> torch.Tensor: Modifies the underlying storage of the input tensor. Args: - tensor (torch.Tensor): Input tensor. - value (int): Value to fill the storage with. Returns: - torch.Tensor: A new tensor with modified storage. # Retrieve the underlying storage of the tensor storage = tensor.storage() # Clone the storage and fill it with the given integer value new_storage = storage.clone() new_storage.fill_(value) # Create a new tensor using the modified storage, maintaining shape and stride new_tensor = torch.tensor([], dtype=tensor.dtype).set_(new_storage, tensor.storage_offset(), tensor.size(), tensor.stride()) return new_tensor"},{"question":"Functionalization and Gradients in PyTorch As a PyTorch practitioner, you are tasked with implementing a function that computes the gradients of the loss with respect to the model parameters and the Jacobian of the model\'s output with respect to the input using the new `torch.func` APIs. You will: 1. Define a simple neural network model. 2. Implement a function to compute the gradients of the loss with respect to the model parameters. 3. Implement another function to compute the Jacobian of the model\'s output with respect to the input. # Requirements 1. **Model Definition:** Define a simple feedforward neural network with one hidden layer. ```python import torch.nn as nn class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x ``` 2. **Gradient Computation:** Implement the function `compute_gradients` that computes the gradients of the loss with respect to the model parameters using `torch.func.grad`. ```python def compute_gradients(model, inputs, targets): Computes the gradients of the loss with respect to the model parameters. Parameters: model (nn.Module): The neural network model. inputs (torch.Tensor): Input tensor to the model. targets (torch.Tensor): Target output tensor. Returns: dict: Dictionary containing gradients with respect to each parameter. # Your code here pass ``` 3. **Jacobian Computation:** Implement the function `compute_jacobian` that computes the Jacobian of the model\'s output with respect to the input using `torch.func.jacrev`. ```python def compute_jacobian(model, inputs): Computes the Jacobian of the model\'s output with respect to the input. Parameters: model (nn.Module): The neural network model. inputs (torch.Tensor): Input tensor to the model. Returns: torch.Tensor: Jacobian tensor of model output with respect to input. # Your code here pass ``` # Function Specifications `compute_gradients` - **Input:** - `model`: An instance of an `nn.Module` (the neural network). - `inputs`: A `torch.Tensor` representing the inputs to the network. - `targets`: A `torch.Tensor` representing the expected outputs. - **Output:** - Returns a dictionary where keys are parameter names and values are their respective gradients. `compute_jacobian` - **Input:** - `model`: An instance of an `nn.Module` (the neural network). - `inputs`: A `torch.Tensor` representing the inputs to the network. - **Output:** - Returns a `torch.Tensor` representing the Jacobian of the model\'s output with respect to the input. # Constraints - Use the provided `SimpleNN` class for defining the model. - Use the latest `torch.func` APIs for implementing the function transforms. - Do not use deprecated `functorch` APIs in your implementation. # Example Run ```python import torch # Example usage model = SimpleNN(input_size=3, hidden_size=5, output_size=2) inputs = torch.randn(64, 3) targets = torch.randn(64, 2) gradients = compute_gradients(model, inputs, targets) jacobian = compute_jacobian(model, inputs) print(gradients) print(jacobian) ``` Your task is to complete the `compute_gradients` and `compute_jacobian` functions.","solution":"import torch import torch.nn as nn import torch.nn.functional as F from torch.func import functional_call, grad, jacrev class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def compute_gradients(model, inputs, targets): Computes the gradients of the loss with respect to the model parameters. Parameters: model (nn.Module): The neural network model. inputs (torch.Tensor): Input tensor to the model. targets (torch.Tensor): Target output tensor. Returns: dict: Dictionary containing gradients with respect to each parameter. def loss_fn(params, x, y): output = functional_call(model, params, (x,)) loss = F.mse_loss(output, y) return loss params = {name: param for name, param in model.named_parameters()} gradients = grad(loss_fn)(params, inputs, targets) return gradients def compute_jacobian(model, inputs): Computes the Jacobian of the model\'s output with respect to the input. Parameters: model (nn.Module): The neural network model. inputs (torch.Tensor): Input tensor to the model. Returns: torch.Tensor: Jacobian tensor of model output with respect to input. def model_fn(x): return model(x) jac_fn = jacrev(model_fn) jacobian = jac_fn(inputs) return jacobian"},{"question":"Controlling MPS Behavior in PyTorch **Objective**: Demonstrate your understanding of configuring and using Metal Performance Shaders (MPS) related environment variables in a PyTorch script. Problem Statement You are provided with a PyTorch script that performs matrix multiplication using MPS when available. However, you need to implement functionality that adjusts various MPS settings based on the provided configuration dictionary and ensures appropriate behavior during execution. Implement the function `configure_and_run_mps(settings: dict) -> torch.Tensor` that takes a dictionary `settings` as input and performs the following: 1. Configures the MPS environment variables according to the values provided in the `settings` dictionary. 2. Executes a simple matrix multiplication operation using MPS and returns the result. Input - `settings` (dict): A dictionary containing key-value pairs for MPS environment variables. The keys can include: - `\\"PYTORCH_DEBUG_MPS_ALLOCATOR\\"` - `\\"PYTORCH_MPS_LOG_PROFILE_INFO\\"` - `\\"PYTORCH_MPS_TRACE_SIGNPOSTS\\"` - `\\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\\"` - `\\"PYTORCH_MPS_LOW_WATERMARK_RATIO\\"` - `\\"PYTORCH_MPS_FAST_MATH\\"` - `\\"PYTORCH_MPS_PREFER_METAL\\"` - `\\"PYTORCH_ENABLE_MPS_FALLBACK\\"` Output - Returns a `torch.Tensor` resulting from the matrix multiplication operation using MPS. Constraints - Default behavior should be maintained if a particular environment variable is not provided in the `settings` dictionary. - Ensure that the code runs correctly even if MPS is not available on the system. - The input matrices for the multiplication will be fixed: ```python A = torch.tensor([[1.0, 2.0], [3.0, 4.0]], device=\'mps\') B = torch.tensor([[5.0, 6.0], [7.0, 8.0]], device=\'mps\') ``` Example ```python settings = { \\"PYTORCH_DEBUG_MPS_ALLOCATOR\\": \\"1\\", \\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\\": \\"1.5\\" } result = configure_and_run_mps(settings) # Expects the result of A @ B to be tensor([[19.0, 22.0], [43.0, 50.0]], device=\'mps\') ``` Notes - You may need to handle the case where the `mps` device is not available and fall back to CPU. - Remember to set the environment variables within your code appropriately before performing the operations. **Good Luck!**","solution":"import os import torch def configure_and_run_mps(settings: dict) -> torch.Tensor: Configures MPS environment variables and performs a matrix multiplication. # Set the environment variables based on the settings dictionary for key, value in settings.items(): os.environ[key] = value # Ensure that the \'mps\' device is available, otherwise fall back to \'cpu\' device = torch.device(\'mps\') if torch.backends.mps.is_available() else torch.device(\'cpu\') # Create the input matrices A = torch.tensor([[1.0, 2.0], [3.0, 4.0]], device=device) B = torch.tensor([[5.0, 6.0], [7.0, 8.0]], device=device) # Perform the matrix multiplication result = torch.matmul(A, B) return result"},{"question":"# Shared Memory Data Synchronization You are required to implement a function that uses shared memory to perform data synchronization between multiple processes. You will create a shared integer value and a shared list using `multiprocessing.shared_memory`. Your task is to ensure that two separate processes can increment the shared integer and update the shared list concurrently. # Function Signature ```python def shared_memory_sync(n: int): pass ``` # Input - `n` (int): The number of times each process should increment the shared integer value and update the shared list. # Requirements 1. **Shared Integer Value**: Create a shared integer value initialized to zero. 2. **Shared List**: Create a shared list initialized with zeros, of size `n`. 3. **Two Processes**: Implement two separate processes that concurrently: - Increment the shared integer value by one each time. - Update the shared list by inserting their process ID at each index. # Example Here\'s an illustration of the expected functionality using `n = 5`: ```python # Initial state shared_integer = 0 shared_list = [0, 0, 0, 0, 0] # After first process runs n times shared_integer = 5 shared_list = [1, 1, 1, 1, 1] # After second process runs n times shared_integer = 10 shared_list = [2, 2, 2, 2, 2] ``` # Constraints - You should use the `multiprocessing.shared_memory` module. - Ensure proper cleanup of shared memory resources. - Handle potential synchronization issues when two processes access the shared memory concurrently. # Implementation Guide 1. Create the shared integer and shared list using `SharedMemory` and `ShareableList`. 2. Define the worker function for each process to increment the shared integer and update the shared list. 3. Ensure synchronization to avoid race conditions. 4. Spawn two processes that execute the worker function. 5. Wait for both processes to complete and then clean up the shared memory. # Deliverables - Complete implementation of the `shared_memory_sync` function. - An example showing the function usage and demonstrating the correctness. # Notes - Consider using synchronization primitives like `multiprocessing.Lock` to manage concurrent access to shared memory. - Ensure that your implementation correctly utilizes the shared memory capabilities provided by the `multiprocessing.shared_memory` module.","solution":"import multiprocessing from multiprocessing import Process, Lock from multiprocessing.shared_memory import SharedMemory from multiprocessing.managers import SharedMemoryManager import numpy as np def worker(shared_int, shared_list, lock, n, pid): for i in range(n): with lock: shared_int[0] += 1 shared_list[i] = pid def shared_memory_sync(n: int): # Create the shared memory resources with SharedMemoryManager with SharedMemoryManager() as smm: shared_int = smm.ShareableList([0]) # Shared integer value as a list shared_list = smm.ShareableList([0]*n) # Shared list initialized with zeros lock = Lock() # Create two processes p1 = Process(target=worker, args=(shared_int, shared_list, lock, n, 1)) p2 = Process(target=worker, args=(shared_int, shared_list, lock, n, 2)) # Start processes p1.start() p2.start() # Wait for processes to finish p1.join() p2.join() # Verify the result shared_int_value = shared_int[0] shared_list_value = list(shared_list) # Cleanup shared_int.shm.close() shared_int.shm.unlink() shared_list.shm.close() shared_list.shm.unlink() return shared_int_value, shared_list_value"},{"question":"# Question: XML Data Manipulation You are provided with an XML document containing book information. Your task is to write a Python function that performs various operations on this XML data using the `xml.etree.ElementTree` module. The XML document has the following structure: ```xml <?xml version=\\"1.0\\"?> <library> <book genre=\\"fiction\\" ISBN=\\"123456789\\"> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <year>1925</year> </book> <book genre=\\"sci-fi\\" ISBN=\\"987654321\\"> <title>Dune</title> <author>Frank Herbert</author> <year>1965</year> </book> <!-- More book elements can be added here --> </library> ``` Write a function `process_books(xml_data: str) -> str` that performs the following steps: 1. Parse the given XML data. 2. Print the title and author of each book. 3. Add one to each year of publication. 4. Add an attribute `updated=\\"yes\\"` to each year element. 5. Remove books published before the year 1930. 6. Write the modified XML data to a string and return it. # Input - `xml_data`: A string representing the XML document. # Output - A string representing the modified XML document. # Constraints - The function should handle potentially large XML files efficiently. # Example ```python xml_data = <?xml version=\\"1.0\\"?> <library> <book genre=\\"fiction\\" ISBN=\\"123456789\\"> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <year>1925</year> </book> <book genre=\\"sci-fi\\" ISBN=\\"987654321\\"> <title>Dune</title> <author>Frank Herbert</author> <year>1965</year> </book> </library> result = process_books(xml_data) print(result) ``` Expected output: ```xml <?xml version=\'1.0\' encoding=\'us-ascii\'?> <library> <book genre=\\"sci-fi\\" ISBN=\\"987654321\\"> <title>Dune</title> <author>Frank Herbert</author> <year updated=\\"yes\\">1966</year> </book> </library> ``` # Requirements: - Use the `xml.etree.ElementTree` module. - Demonstrate the ability to parse, search, modify, and write XML data. - Ensure the code is efficient and handles larger XML documents appropriately.","solution":"import xml.etree.ElementTree as ET def process_books(xml_data: str) -> str: Parses the given XML data, performs various operations on it, and returns the modified XML as a string. # Parse the XML data root = ET.fromstring(xml_data) # Iterate over each book and print title and author for book in root.findall(\'book\'): title = book.find(\'title\').text author = book.find(\'author\').text print(f\\"Title: {title}, Author: {author}\\") # Increment the year by 1 and add updated=\\"yes\\" attribute year_elem = book.find(\'year\') year = int(year_elem.text) + 1 year_elem.text = str(year) year_elem.set(\'updated\', \'yes\') # Remove books published before 1930 for book in root.findall(\'book\'): year = int(book.find(\'year\').text) if year < 1930: root.remove(book) # Generate the modified XML string return ET.tostring(root, encoding=\'unicode\')"},{"question":"# Seaborn Coding Assessment Question You are tasked with analyzing a dataset using Seaborn\'s `boxenplot` function. Specifically, you will work with the `diamonds` dataset, which can be loaded directly using Seaborn\'s `load_dataset` function. **Dataset Description:** The `diamonds` dataset contains the prices and attributes of approximately 54,000 diamonds. The key columns you will be working with include: - `price`: the price of the diamond. - `clarity`: a categorical measurement of how clear the diamond is. - `cut`: the quality of the cut (Fair, Good, Very Good, Premium, Ideal). - `carat`: the weight of the diamond. **Objective:** Write a Python function `plot_diamond_analysis()` that performs the following tasks: 1. Loads the `diamonds` dataset. 2. Draws a horizontal boxen plot of diamond prices grouped by their clarity with the boxes color-coded by `cut`. 3. Adds a small gap between the boxes. 4. Modifies the outlines of the boxes by setting their color to `0.5` (grayish) and linewidth to `0.8`. 5. Ensures that the plots do not overlap by setting the `dodge` parameter appropriately. 6. Saves the generated plot as a PNG file named `diamond_analysis.png`. **Function Signature:** ```python def plot_diamond_analysis(): pass ``` **Expected Output:** - A PNG file named `diamond_analysis.png` storing the customized Seaborn boxen plot as described above. **Constraints:** - Please ensure that the solution runs efficiently without unnecessary computations. - Use only the Seaborn library and other standard Python libraries (e.g., pandas) for data handling if necessary. **Example Usage:** ```python plot_diamond_analysis() # This should create and save the PNG file in the current working directory. ``` **Note:** Ensure your function includes appropriate comments and documentation to explain the steps taken.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_diamond_analysis(): Loads the diamonds dataset and creates a horizontal boxen plot of diamond prices grouped by their clarity and color-coded by cut. The plot is then saved as a PNG file named \'diamond_analysis.png\'. # Load the diamonds dataset diamonds = sns.load_dataset(\'diamonds\') # Create the boxen plot plt.figure(figsize=(10, 8)) sns.boxenplot( x=\'price\', y=\'clarity\', hue=\'cut\', data=diamonds, dodge=False, # prevents overlapping of points from different levels linewidth=0.8, # sets the width of box outlines palette=\\"Set3\\" # uses a color palette for better distinction between cuts ) # Customize the appearance of the plot plt.legend(title=\'Cut\') plt.title(\'Diamond Prices by Clarity and Cut\') plt.xlabel(\'Price\') plt.ylabel(\'Clarity\') # Save the plot as a PNG file plt.savefig(\'diamond_analysis.png\') plt.close()"},{"question":"**Coding Question: Context Variable Manager** You are tasked with implementing a Python class `ContextVariableManager` that uses the context variables API to manage context variables within a Python application. This class should provide functionalities to create, set, get, and reset context variables in a multi-threaded environment. # Requirements: 1. **Create and Initialize Context Variables**: - Method: `create_context_var(name: str, default_value: Any = None) -> None` - This method should create a new context variable with an optional default value and store it within the class for future access. 2. **Set Context Variable Value**: - Method: `set_context_var(name: str, value: Any) -> None` - This method should set the value of the specified context variable. If the variable does not exist, it should raise a `ValueError`. 3. **Get Context Variable Value**: - Method: `get_context_var(name: str) -> Any` - This method should retrieve the value of the specified context variable. If the variable does not exist, it should raise a `ValueError`. 4. **Reset Context Variable Value**: - Method: `reset_context_var(name: str) -> None` - This method should reset the context variable to its default value. If the variable does not exist, it should raise a `ValueError`. 5. **Manage Context Entry and Exit**: - Method: `enter_context() -> None` - Method: `exit_context() -> None` - These methods should manage the context for the current thread, setting up and tearing down the context as needed. # Input and Output: - The methods should take standard Python types (e.g., `str`, `Any`) as parameters and should either perform their tasks or raise appropriate exceptions when preconditions are not met. - The methods should not return values unless specified. # Performance Requirements: - Ensure that context switching and variable management are efficient and handle multiple threads appropriately. # Example Usage: ```python cv_manager = ContextVariableManager() # Create and initialize context variables cv_manager.create_context_var(\\"user_id\\", 1001) cv_manager.create_context_var(\\"session_token\\") # Set a value for an existing context variable cv_manager.set_context_var(\\"session_token\\", \\"abcd1234\\") # Retrieve the value of a context variable user_id = cv_manager.get_context_var(\\"user_id\\") print(user_id) # Output: 1001 # Reset the value of a context variable cv_manager.reset_context_var(\\"user_id\\") # Manage context entry and exit cv_manager.enter_context() cv_manager.exit_context() ``` *Note*: You are encouraged to use appropriate C API calls as described in the provided documentation to implement these functionalities.","solution":"import contextvars from typing import Any, Dict class ContextVariableManager: def __init__(self): self._context_vars: Dict[str, contextvars.ContextVar] = {} self._defaults: Dict[str, Any] = {} def create_context_var(self, name: str, default_value: Any = None) -> None: if name in self._context_vars: raise ValueError(f\\"Context variable \'{name}\' already exists\\") ctx_var = contextvars.ContextVar(name, default=default_value) self._context_vars[name] = ctx_var self._defaults[name] = default_value def set_context_var(self, name: str, value: Any) -> None: if name not in self._context_vars: raise ValueError(f\\"Context variable \'{name}\' does not exist\\") self._context_vars[name].set(value) def get_context_var(self, name: str) -> Any: if name not in self._context_vars: raise ValueError(f\\"Context variable \'{name}\' does not exist\\") return self._context_vars[name].get() def reset_context_var(self, name: str) -> None: if name not in self._context_vars: raise ValueError(f\\"Context variable \'{name}\' does not exist\\") ctx_var = self._context_vars[name] if name in self._defaults: ctx_var.set(self._defaults[name]) else: ctx_var.set(None) def enter_context(self) -> None: pass # Additional functionality can be added if required def exit_context(self) -> None: pass # Additional functionality can be added if required"},{"question":"Objective Demonstrate your understanding of the `http.cookiejar` module by implementing a Python function that interacts with HTTP cookies, stores them in a file, and enforces a custom cookie policy. Task Write a Python function `manage_cookies(url: str, save_file: str, blocked_domains: list) -> list` that performs the following tasks: 1. **Create an instance of `DefaultCookiePolicy`**: - Enable RFC 2965 handling. - Specify `blocked_domains` to block any cookies from these domains. 2. **Create an instance of `MozillaCookieJar`**: - Use this instance to store cookies in the file specified by `save_file`. 3. **Open the provided `url`**: - Use the `MozillaCookieJar` instance to handle cookies for this URL. 4. **Extract cookies from the response and store them**: - Store the cookies in the `MozillaCookieJar` file. 5. **Return a list of valid (non-expired) cookies**: - Each cookie should be represented as a dictionary with the following keys: `name`, `value`, `domain`, `path`, and `expires`. Input - `url` (str): The URL to open and extract cookies from. - `save_file` (str): The file path where the cookies should be saved. - `blocked_domains` (list): A list of domains that should be blocked from setting or receiving cookies. Output - Returns a list of dictionaries, each representing a valid (non-expired) cookie. Constraints - Use the `http.cookiejar` and `urllib.request` modules to manage cookies and HTTP requests. - Ensure the function handles network errors gracefully. Example ```python url = \\"http://example.com\\" save_file = \\"cookies.txt\\" blocked_domains = [\\"ads.net\\", \\"tracking.com\\"] cookies = manage_cookies(url, save_file, blocked_domains) ``` Function signature: ```python def manage_cookies(url: str, save_file: str, blocked_domains: list) -> list: pass ```","solution":"import urllib.request import http.cookiejar def manage_cookies(url: str, save_file: str, blocked_domains: list) -> list: # Create a cookie policy and block specified domains cookie_policy = http.cookiejar.DefaultCookiePolicy( rfc2965=True, blocked_domains=blocked_domains ) # Create a cookie jar to store cookies cookie_jar = http.cookiejar.MozillaCookieJar(save_file, policy=cookie_policy) # Build an opener with the cookie jar opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie_jar)) try: # Open the URL and extract cookies opener.open(url) # Save cookies to the file cookie_jar.save(ignore_discard=True, ignore_expires=True) except Exception as e: print(f\\"An error occurred: {e}\\") # Extract non-expired cookies valid_cookies = [] for cookie in cookie_jar: if not cookie.is_expired(): valid_cookies.append({ \'name\': cookie.name, \'value\': cookie.value, \'domain\': cookie.domain, \'path\': cookie.path, \'expires\': cookie.expires }) return valid_cookies"},{"question":"You are tasked with building a simple machine learning pipeline using one of the real-world datasets available in scikit-learn. Specifically, you will work with the California housing dataset to predict house prices. # Question Implement a Python function `train_california_housing_model` that performs the following steps: 1. Fetch the California housing dataset. 2. Preprocess the data: - Split the data into training and test sets (80% training, 20% testing). - Normalize the feature values to have a mean of 0 and a standard deviation of 1. 3. Train a linear regression model on the preprocessed training data. 4. Evaluate the model on the test data and return the mean squared error of the predictions. # Function Signature ```python def train_california_housing_model() -> float: pass ``` # Requirements 1. You must use `fetch_california_housing` from `sklearn.datasets` to load the data. 2. Use `train_test_split` from `sklearn.model_selection` to split the data. 3. Use `StandardScaler` from `sklearn.preprocessing` to normalize the feature values. 4. Use `LinearRegression` from `sklearn.linear_model` to train the model. 5. Use `mean_squared_error` from `sklearn.metrics` to evaluate the predictions. # Expected Output The function should return a float, which is the mean squared error of the model on the test data. # Example If your function is implemented correctly, it should output a reasonable mean squared error when trained on the California housing dataset (e.g., something around 1.0). # Constraints - Do not change the random state for reproducibility. - Ensure your code is well-documented and follows PEP 8 guidelines.","solution":"from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error def train_california_housing_model() -> float: Trains a linear regression model on the California housing dataset and returns the mean squared error on test data. # Fetch the California housing dataset housing = fetch_california_housing() X, y = housing.data, housing.target # Split the data into training and test sets (80% training, 20% testing) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Normalize the feature values to have a mean of 0 and a standard deviation of 1 scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train a linear regression model on the preprocessed training data model = LinearRegression() model.fit(X_train, y_train) # Evaluate the model on the test data y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) return mse"},{"question":"# Question **Custom Neural Network Module Implementation with PyTorch** You are required to implement a neural network using PyTorch which involves designing custom modules, composing them, training the network, and switching between training and evaluation modes. # Requirements 1. **Define a Custom Linear Layer (MyLinear)** - This layer will perform an affine transformation using learnable parameters `weight` and `bias`. - Initialize `weight` and `bias` as random tensors with `nn.Parameter`. - Define a `forward` function that computes the affine transformation on the input. 2. **Define a Simple Neural Network (SimpleNet)** - Compose multiple `MyLinear` layers and `ReLU` activation functions. - Define a `forward` function that processes input data through these layers. 3. **Training and Evaluation** - Create a training loop that optimizes the model parameters on a dummy objective using stochastic gradient descent (SGD). - Implement a method to switch between training and evaluation modes, and demonstrate how the model behaves differently in each mode. # Specifications 1. **MyLinear Module Implementation** ```python class MyLinear(nn.Module): def __init__(self, in_features, out_features): ... def forward(self, input): ... ``` 2. **SimpleNet Implementation** ```python class SimpleNet(nn.Module): def __init__(self): ... def forward(self, x): ... ``` 3. **Training and Evaluation Demonstration** ```python def train_network(): ... def switch_modes(model): ... ``` # Input and Output Formats **Input:** - Integer values for the dimensions of layers. - Datasets represented as tensors for training and evaluation. **Output:** - Trained model parameters. - Output tensor from the SimpleNet in both training and evaluation modes. # Constraints - Use ReLU activation for non-linear transformations. - Keep the network architecture simple (e.g., 2-3 layers). # Performance Requirements - Ensure the training loop executes efficiently, particularly focusing on gradient descent updates. - Correctly demonstrate the difference in behavior between training and evaluation modes. # Example Here is a partial code template to get you started: ```python import torch from torch import nn class MyLinear(nn.Module): def __init__(self, in_features, out_features): super(MyLinear, self).__init__() self.weight = nn.Parameter(torch.randn(in_features, out_features)) self.bias = nn.Parameter(torch.randn(out_features)) def forward(self, input): return (input @ self.weight) + self.bias class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.layer1 = MyLinear(4, 3) self.layer2 = MyLinear(3, 1) def forward(self, x): x = self.layer1(x) x = torch.relu(x) x = self.layer2(x) return x def train_network(): # Define model, loss function, and optimizer model = SimpleNet() optimizer = torch.optim.SGD(model.parameters(), lr=0.01) loss_fn = nn.MSELoss() # Dummy training loop for epoch in range(100): optimizer.zero_grad() input = torch.randn(4) target = torch.zeros(1) output = model(input) loss = loss_fn(output, target) loss.backward() optimizer.step() return model def switch_modes(model): model.eval() print(\\"Evaluation Mode:\\", model(torch.randn(4))) model.train() print(\\"Training Mode:\\", model(torch.randn(4))) model = train_network() switch_modes(model) ``` Complete the unimplemented parts to meet the requirements and demonstrate your understanding of custom module implementation and usage in PyTorch.","solution":"import torch from torch import nn class MyLinear(nn.Module): def __init__(self, in_features, out_features): super(MyLinear, self).__init__() self.weight = nn.Parameter(torch.randn(in_features, out_features)) self.bias = nn.Parameter(torch.randn(out_features)) def forward(self, input): return (input @ self.weight) + self.bias class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.layer1 = MyLinear(4, 3) self.layer2 = MyLinear(3, 1) def forward(self, x): x = self.layer1(x) x = torch.relu(x) x = self.layer2(x) return x def train_network(): # Define model, loss function, and optimizer model = SimpleNet() optimizer = torch.optim.SGD(model.parameters(), lr=0.01) loss_fn = nn.MSELoss() # Dummy training loop for epoch in range(100): optimizer.zero_grad() input = torch.randn(4) target = torch.zeros(1) output = model(input) loss = loss_fn(output, target) loss.backward() optimizer.step() return model def switch_modes(model): model.eval() print(\\"Evaluation Mode:\\", model(torch.randn(4))) model.train() print(\\"Training Mode:\\", model(torch.randn(4))) # Train the network model = train_network() # Switch modes and print outputs switch_modes(model)"},{"question":"# Coding Assessment: Advanced File Analysis with `sndhdr` Objective Using the deprecated `sndhdr` module, students are required to analyze a set of audio files and determine specific characteristics and statistics about these files. Description You are given a list of audio file paths. Your task is to write a function `analyze_audio_files(filepaths)` that performs the following steps: 1. For each file in the provided list: - Use the `sndhdr.what()` function to determine the type of the audio file. - If the `sndhdr.what()` method returns `None`, use the `sndhdr.whathdr()` function instead. - If both methods return `None`, print a message stating the file could not be analyzed and skip it. 2. Collect the `namedtuple` results for successfully analyzed files. 3. Compute and return a dictionary where: - The keys are the unique `filetype` values of the analyzed files. - The value for each key is a dictionary containing: - `average_framerate`: The average framerate for that file type. - `average_channels`: The average number of channels for that file type. - `total_files`: The total number of files of that type analyzed. - `total_frames`: The total sum of frames for that file type. - `bits_per_sample_count`: A dictionary where keys are the unique `sampwidth` values found and values are their respective counts. Function Signature ```python def analyze_audio_files(filepaths: list) -> dict: pass ``` Input - `filepaths` (list): A list of strings, each representing the path to an audio file. Output - A dictionary with the structure described above. Constraints - You may assume that all file paths provided in the list are valid and accessible. Example Usage ```python file_list = [\\"path/to/audio1.wav\\", \\"path/to/audio2.aiff\\", \\"path/to/audio3.au\\"] result = analyze_audio_files(file_list) print(result) ``` Notes - Ensure your solution handles scenarios where files cannot be analyzed gracefully. - Your implementation should use the `sndhdr` module to its fullest extent to gather and process information about each audio file.","solution":"import sndhdr from collections import namedtuple from collections import defaultdict def analyze_audio_files(filepaths: list) -> dict: Analyzes audio files using sndhdr module and returns a summary dictionary. audio_info = [] # Step 1: Analyze each file for filepath in filepaths: info = sndhdr.what(filepath) if info is None: info = sndhdr.whathdr(filepath) if info is None: print(f\\"File {filepath} could not be analyzed.\\") else: audio_info.append(info) # Step 2: Collect statistics filetype_stats = defaultdict(lambda: {\'total_files\': 0, \'total_frames\': 0, \'average_framerate\': 0, \'average_channels\': 0, \'bits_per_sample_count\': defaultdict(int)}) for info in audio_info: filetype = info.filetype filetype_stats[filetype][\'total_files\'] += 1 filetype_stats[filetype][\'total_frames\'] += info.nframes filetype_stats[filetype][\'average_framerate\'] += info.framerate filetype_stats[filetype][\'average_channels\'] += info.nchannels filetype_stats[filetype][\'bits_per_sample_count\'][info.sampwidth] += 1 # Calculate averages for filetype, stats in filetype_stats.items(): total_files = stats[\'total_files\'] if total_files > 0: stats[\'average_framerate\'] /= total_files stats[\'average_channels\'] /= total_files return filetype_stats # Define the audio file information namedtuple structure _SoundFileInfo = namedtuple(\'_SoundFileInfo\', \'filetype framerate nchannels sampwidth nframes\')"}]'),P={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:z,isLoading:!1}},computed:{filteredPoems(){const i=this.searchQuery.trim().toLowerCase();return i?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(i)||e.solution&&e.solution.toLowerCase().includes(i)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(i=>setTimeout(i,1e3)),this.visibleCount+=4,this.isLoading=!1}}},q={class:"search-container"},F={class:"card-container"},L={key:0,class:"empty-state"},D=["disabled"],N={key:0},O={key:1};function R(i,e,l,h,s,r){const m=g("PoemCard");return a(),n("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",q,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),_(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=o=>s.searchQuery=o),placeholder:"Search..."},null,512),[[y,s.searchQuery]]),s.searchQuery?(a(),n("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=o=>s.searchQuery="")}," ✕ ")):d("",!0)]),t("div",F,[(a(!0),n(b,null,v(r.displayedPoems,(o,f)=>(a(),w(m,{key:f,poem:o},null,8,["poem"]))),128)),r.displayedPoems.length===0?(a(),n("div",L,' No results found for "'+u(s.searchQuery)+'". ',1)):d("",!0)]),r.hasMorePoems?(a(),n("button",{key:0,class:"load-more-button",disabled:s.isLoading,onClick:e[2]||(e[2]=(...o)=>r.loadMore&&r.loadMore(...o))},[s.isLoading?(a(),n("span",O,"Loading...")):(a(),n("span",N,"See more"))],8,D)):d("",!0)])}const M=p(P,[["render",R],["__scopeId","data-v-41ab6351"]]),U=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"library/58.md","filePath":"library/58.md"}'),j={name:"library/58.md"},B=Object.assign(j,{setup(i){return(e,l)=>(a(),n("div",null,[x(M)]))}});export{U as __pageData,B as default};
