import{_ as p,o as a,c as s,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},P={class:"review-content"};function S(n,e,l,m,i,o){return a(),s("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-567c9b02"]]),I=JSON.parse('[{"question":"# Question You are given a dataset and tasked with evaluating a machine learning model. Specifically, you need to determine if the model is underfitting, overfitting, or generalizing well. To achieve this, you will generate and analyze validation and learning curves using scikit-learn. Instructions 1. **Load the Dataset**: - Use the `load_iris` function from `sklearn.datasets` to load the dataset. - Shuffle the dataset using `shuffle` from `sklearn.utils`. 2. **Validation Curve**: - Use Support Vector Machine (SVM) with a linear kernel (`SVC(kernel=\'linear\')`) as the estimator. - Generate the validation curve for the hyperparameter `C` over the range `np.logspace(-7, 3, 10)`. - Plot the validation curve using `ValidationCurveDisplay.from_estimator`. 3. **Learning Curve**: - Use the same SVM estimator. - Generate the learning curve with training sizes `[50, 80, 110]` and 5-fold cross-validation. - Plot the learning curve using `LearningCurveDisplay.from_estimator`. 4. **Analysis**: - Based on the generated plots, analyze whether the model is underfitting, overfitting, or generalizing well. - Provide a brief explanation of your analysis. Function Signature ```python def evaluate_model(): import numpy as np from sklearn.model_selection import validation_curve, learning_curve, ValidationCurveDisplay, LearningCurveDisplay from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.utils import shuffle import matplotlib.pyplot as plt # Load and shuffle the dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Validation Curve param_range = np.logspace(-7, 3, 10) train_scores, valid_scores = validation_curve( SVC(kernel=\'linear\'), X, y, param_name=\'C\', param_range=param_range) # Plot Validation Curve ValidationCurveDisplay.from_estimator( SVC(kernel=\'linear\'), X, y, param_name=\'C\', param_range=param_range) plt.title(\'Validation Curve\') plt.show() # Learning Curve train_sizes = [50, 80, 110] train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\'linear\'), X, y, train_sizes=train_sizes, cv=5) # Plot Learning Curve LearningCurveDisplay.from_estimator( SVC(kernel=\'linear\'), X, y, train_sizes=train_sizes, cv=5) plt.title(\'Learning Curve\') plt.show() # Analysis # Based on the plots, provide an analysis on whether the model is underfitting, overfitting, or generalizing well. ``` # Constraints - You may use any additional libraries for visualization, but the primary model evaluation should be performed using scikit-learn. # Notes - Ensure your plots are clear and well-labeled for analysis. - The analysis part should be a commentary based on your interpretation of the plots.","solution":"def evaluate_model(): import numpy as np from sklearn.model_selection import validation_curve, learning_curve, ValidationCurveDisplay, LearningCurveDisplay from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.utils import shuffle import matplotlib.pyplot as plt # Load and shuffle the dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Validation Curve param_range = np.logspace(-7, 3, 10) train_scores, valid_scores = validation_curve( SVC(kernel=\'linear\'), X, y, param_name=\'C\', param_range=param_range, cv=5) # Plot Validation Curve plt.figure(figsize=(10, 6)) ValidationCurveDisplay.from_estimator( SVC(kernel=\'linear\'), X, y, param_name=\'C\', param_range=param_range, cv=5) plt.title(\'Validation Curve\') plt.xlabel(\'Parameter C\') plt.ylabel(\'Score\') plt.legend([\'Training score\', \'Validation score\']) plt.show() # Learning Curve train_sizes = [50, 80, 110] train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\'linear\'), X, y, train_sizes=train_sizes, cv=5) # Plot Learning Curve plt.figure(figsize=(10, 6)) LearningCurveDisplay.from_estimator( SVC(kernel=\'linear\'), X, y, train_sizes=train_sizes, cv=5) plt.title(\'Learning Curve\') plt.xlabel(\'Training examples\') plt.ylabel(\'Score\') plt.legend([\'Training score\', \'Validation score\']) plt.show() # Analysis # The validation curve helps us understand if the model is overfitting or underfitting by # comparing training and validation scores across different values of parameter C. # If both curves are low, the model is underfitting. # If the training score is high and the validation score is much lower, the model is overfitting. # If both curves converge to a high value, the model is generalizing well. # The learning curve shows how the validation and training error change with varying training set sizes. # If the training and validation scores converge to a high value as the number of training examples increases, # the model is likely generalizing well. If there is a large gap between curves, the model might be overfitting. # The analysis should provide specifics on the behavior observed in the plots related to underfitting, overfitting, or good generalization."},{"question":"# Pandas Styling and Export Assessment You have been provided with a CSV file containing sales data for various products across different regions. Your task is to read this data into a pandas DataFrame, apply specific styles based on the given conditions, and then export the styled DataFrame to an HTML file. Input - A CSV file named `sales_data.csv` with the following columns: - `Product`: The name of the product. - `Region`: The region where the sales were made. - `Sales`: The number of units sold. - `Revenue`: The total revenue generated from the sales (in USD). Styling Conditions 1. Highlight the rows where `Sales` are above 1000 units with a green background. 2. Highlight the rows where `Revenue` is below 5000 with a red background. 3. For the `Revenue` column, format the values as currency (e.g., `1,234.56`). Output - An HTML file named `styled_sales_data.html` containing the styled DataFrame. Constraints - Use pandas version 1.1.0 or higher. Function Signature ```python import pandas as pd def style_and_export_sales_data(file_path: str, output_path: str) -> None: # Your code here pass ``` Example Suppose the `sales_data.csv` contains the following data: ``` Product,Region,Sales,Revenue Widget A,East,1200,15000 Widget B,West,900,4500 Gadget C,South,100,6500 Gadget D,North,300,2000 ``` After processing, your `styled_sales_data.html` should: 1. Highlight the first row green. 2. Highlight the fourth row red. 3. Format the `Revenue` column as currency. You need to implement the `style_and_export_sales_data` function which will: 1. Read the data from the CSV file. 2. Apply the necessary styles using the `Styler` methods. 3. Export the styled DataFrame to an HTML file. Note: Your implementation should be able to handle any similar CSV files, adhering to the same column structure and constraints.","solution":"import pandas as pd def style_and_export_sales_data(file_path: str, output_path: str) -> None: # Read the CSV file into a DataFrame df = pd.read_csv(file_path) # Function to apply styles based on conditions def highlight_sales(cell): return [\'background-color: green\' if v > 1000 else \'\' for v in cell] def highlight_revenue(cell): return [\'background-color: red\' if v < 5000 else \'\' for v in cell] # Applying styles using Styler styled_df = (df.style .apply(highlight_sales, subset=[\'Sales\']) .apply(highlight_revenue, subset=[\'Revenue\']) .format({\'Revenue\': \'{:,.2f}\'})) # Export the styled DataFrame to an HTML file styled_df.to_html(output_path) # Example usage (This would be handled outside this function in practice) # style_and_export_sales_data(\'sales_data.csv\', \'styled_sales_data.html\')"},{"question":"# Type Hinting and Unit Testing in Python Python\'s `typing` module allows developers to add type hints to their code, providing greater clarity and facilitating more effective debugging. In addition, the `unittest` module helps in writing and executing unit tests to verify the correctness of individual parts of a program. Task You need to implement a function `process_data(data: List[Dict[str, Union[int, float, str]]]) -> Dict[str, Union[int, float]]` that processes a list of dictionaries. Each dictionary in the list represents certain records and contains keys like `\\"id\\"`, `\\"value\\"`, and `\\"status\\"`. Your function should: 1. Filter out dictionaries where `\\"status\\"` is not `\\"active\\"`. 2. Sum up the `\\"value\\"` fields of the remaining dictionaries. 3. Return a dictionary with: - The key `\\"count\\"` representing the number of active records. - The key `\\"total\\"` representing the sum of all values for active records. Additionally, write unit tests to verify the correctness of your implementation using the `unittest` module. Example ```python # Input data data = [ {\\"id\\": 1, \\"value\\": 10.5, \\"status\\": \\"active\\"}, {\\"id\\": 2, \\"value\\": 20, \\"status\\": \\"inactive\\"}, {\\"id\\": 3, \\"value\\": 15.5, \\"status\\": \\"active\\"} ] # Expected output { \\"count\\": 2, \\"total\\": 26.0 } ``` Requirements 1. Implement the function with proper type annotations using the `typing` module. 2. Write at least three unit tests utilizing the `unittest` framework to validate different scenarios for the function `process_data`. Constraints - Each dictionary in the input list will have the keys `\\"id\\"`, `\\"value\\"`, and `\\"status\\"`. - The `\\"id\\"` will be a unique integer. - The `\\"value\\"` will be a non-negative number (could be int or float). - The `\\"status\\"` will be a string and can have any value. Submission Submit your `process_data` function along with the unit tests in a single Python script.","solution":"from typing import List, Dict, Union def process_data(data: List[Dict[str, Union[int, float, str]]]) -> Dict[str, Union[int, float]]: Processes a list of dictionaries and filters out those whose status is not \'active\'. Args: - data: List[Dict[str, Union[int, float, str]]]: List of dictionaries containing data records. Returns: - Dict[str, Union[int, float]]: A dictionary containing the count of active records and the total of their values. active_records = [record for record in data if record[\'status\'] == \'active\'] count = len(active_records) total = sum(record[\'value\'] for record in active_records) return {\\"count\\": count, \\"total\\": total}"},{"question":"# Advanced JSON Encoding and Decoding with Custom Handling **Problem Description:** You are tasked with creating a utility that processes JSON data representing people\'s profiles. Each profile contains information such as name, age, hobbies, and other nested structured data. To handle this data efficiently, you need to implement custom serialization and deserialization routines. Specifically, you must: 1. Implement a custom JSON encoder that can serialize instances of a `Person` class. 2. Implement a custom JSON decoder that can deserialize JSON strings back into `Person` instances. **Input and Output Formats:** 1. **Custom JSON Encoder:** - Input: An instance of the `Person` class. - Output: A JSON string representation of the `Person` instance. 2. **Custom JSON Decoder:** - Input: A JSON string representation of a person. - Output: An instance of the `Person` class. **Constraints and Requirements:** - The `Person` class has the following attributes: - `name` (str): The name of the person. - `age` (int): The age of the person. - `hobbies` (list of str): A list of hobbies. - `contact_info` (dict): A dictionary with contact information such as email and phone. - You must use the `json` module\'s `JSONEncoder` and `JSONDecoder` as base classes to implement custom serialization and deserialization. - Ensure that the process is efficient and handles errors gracefully. **Class Definition:** ```python class Person: def __init__(self, name, age, hobbies, contact_info): self.name = name self.age = age self.hobbies = hobbies self.contact_info = contact_info def __eq__(self, other): return (self.name == other.name and self.age == other.age and self.hobbies == other.hobbies and self.contact_info == other.contact_info) ``` **Implementation Skeleton:** Complete the skeleton code provided below: ```python import json class Person: def __init__(self, name, age, hobbies, contact_info): self.name = name self.age = age self.hobbies = hobbies self.contact_info = contact_info def __eq__(self, other): return (self.name == other.name and self.age == other.age and self.hobbies == other.hobbies and self.contact_info == other.contact_info) class PersonEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Person): return { \'name\': obj.name, \'age\': obj.age, \'hobbies\': obj.hobbies, \'contact_info\': obj.contact_info } return super().default(obj) class PersonDecoder(json.JSONDecoder): def __init__(self, *args, **kwargs): super().__init__(object_hook=self.object_hook, *args, **kwargs) def object_hook(self, dct): if \'name\' in dct and \'age\' in dct and \'hobbies\' in dct and \'contact_info\' in dct: return Person(name=dct[\'name\'], age=dct[\'age\'], hobbies=dct[\'hobbies\'], contact_info=dct[\'contact_info\']) return dct # Example usage: person = Person(name=\\"Alice\\", age=30, hobbies=[\\"Reading\\", \\"Hiking\\"], contact_info={\\"email\\": \\"alice@example.com\\", \\"phone\\": \\"123-456-7890\\"}) json_str = json.dumps(person, cls=PersonEncoder) reconstructed_person = json.loads(json_str, cls=PersonDecoder) print(reconstructed_person == person) # Should print: True ``` # Notes: - Ensure that the custom encoder and decoder handle nested structures and edge cases appropriately. - Test the implementation with different `Person` instances to verify its robustness.","solution":"import json class Person: def __init__(self, name, age, hobbies, contact_info): self.name = name self.age = age self.hobbies = hobbies self.contact_info = contact_info def __eq__(self, other): return (self.name == other.name and self.age == other.age and self.hobbies == other.hobbies and self.contact_info == other.contact_info) class PersonEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Person): return { \'name\': obj.name, \'age\': obj.age, \'hobbies\': obj.hobbies, \'contact_info\': obj.contact_info } return super().default(obj) class PersonDecoder(json.JSONDecoder): def __init__(self, *args, **kwargs): super().__init__(object_hook=self.object_hook, *args, **kwargs) def object_hook(self, dct): if \'name\' in dct and \'age\' in dct and \'hobbies\' in dct and \'contact_info\' in dct: return Person(name=dct[\'name\'], age=dct[\'age\'], hobbies=dct[\'hobbies\'], contact_info=dct[\'contact_info\']) return dct # Example usage: person = Person(name=\\"Alice\\", age=30, hobbies=[\\"Reading\\", \\"Hiking\\"], contact_info={\\"email\\": \\"alice@example.com\\", \\"phone\\": \\"123-456-7890\\"}) json_str = json.dumps(person, cls=PersonEncoder) reconstructed_person = json.loads(json_str, cls=PersonDecoder) print(reconstructed_person == person) # Should print: True"},{"question":"**Objective:** Demonstrate proficiency in using pandas\' options API by implementing an advanced configuration management function. # Problem Statement: You are required to implement a function `configure_pandas_options` that sets multiple pandas options at once, retrieves the current values of these options, and then resets them to their default values. Additionally, you must implement a context manager that temporarily sets these options and restores them after execution. # Function Signature: ```python import pandas as pd def configure_pandas_options(option_dict): Configures pandas global options, retrieves the configured values, and resets the options to their default values. Parameters: option_dict (dict): A dictionary with option names as keys and their respective values to be set. Returns: dict: A dictionary with option names as keys and their values before resetting to defaults. pass ``` # Input: - `option_dict` (dict): A dictionary where keys are option names (strings) and values are the values to set for these options. # Output: - `result_dict` (dict): A dictionary where keys are option names and values are the configured values after setting the options and then resetting them to their default values. # Constraints: - You must use pandas functions `get_option`, `set_option`, and `reset_option`. # Example: ```python option_dict = { \\"display.max_rows\\": 10, \\"display.max_columns\\": 5 } result_dict = configure_pandas_options(option_dict) print(result_dict) # Output should be similar to: # { # \\"display.max_rows\\": 60, # \\"display.max_columns\\": 20 # } # (Assuming the default values: `display.max_rows` is 60 and `display.max_columns` is 20) ``` # Instructions: 1. Implement the `configure_pandas_options` function to set the options provided in `option_dict`. 2. Retrieve the current values of these options after setting them. 3. Reset all options in `option_dict` to their default values. 4. Return the initial values of these options in `result_dict`. # Points of Evaluation: 1. Correct usage of pandas options functions. 2. Accurate retrieval and resetting of options. 3. Proper handling of the options dictionary. # Additional Task (Optional): Implement a context manager `PandasOptionContext` similar to `option_context` that you will use in the `configure_pandas_options` function to temporarily set options and ensure they are restored after execution. **Hint**: You can refer to the use of `pd.option_context` in the documentation provided to implement your own version. ```python class PandasOptionContext: def __init__(self, **kwargs): self.options = kwargs self.old_values = {} def __enter__(self): for k, v in self.options.items(): self.old_values[k] = pd.get_option(k) pd.set_option(k, v) return self def __exit__(self, exc_type, exc_value, traceback): for k, v in self.old_values.items(): pd.set_option(k, v) ``` Use this context manager in your `configure_pandas_options` function if you choose to implement this additional task.","solution":"import pandas as pd def configure_pandas_options(option_dict): Configures pandas global options, retrieves the configured values, and resets the options to their default values. Parameters: option_dict (dict): A dictionary with option names as keys and their respective values to be set. Returns: dict: A dictionary with option names as keys and their values before resetting to defaults. # Store current values of options that are going to be changed original_values = {key: pd.get_option(key) for key in option_dict} # Set the new values for the options for key, value in option_dict.items(): pd.set_option(key, value) # Retrieve the current values after setting the new ones configured_values = {key: pd.get_option(key) for key in option_dict} # Reset options to their defaults for key in option_dict.keys(): pd.reset_option(key) return configured_values class PandasOptionContext: def __init__(self, **kwargs): self.options = kwargs self.old_values = {} def __enter__(self): for k, v in self.options.items(): self.old_values[k] = pd.get_option(k) pd.set_option(k, v) return self def __exit__(self, exc_type, exc_value, traceback): for k, v in self.old_values.items(): pd.set_option(k, v)"},{"question":"# Directory File Type and Permission Analyzer Objective Implement a function using the Python `stat` module that will recursively analyze files in a specified directory. The function should print detailed information about each file, including its type and permissions. Requirements 1. **Function Name**: `analyze_directory` 2. **Input**: - `directory` (str): The path to the directory to analyze. 3. **Output**: None (The function should print the results to the console). Detailed Specifications: - The function should recursively traverse the given directory and its subdirectories. - For each file, the function should: 1. Print the file\'s path. 2. Identify and print the type of the file (e.g., directory, regular file, symbolic link, etc.) using the `stat` module\'s `S_IS*` functions. 3. Print the file\'s permissions in human-readable string format. - Use the constants and functions provided by the `stat` module for interpreting the file mode and permissions. Example For a directory structure like this: ``` /example file1.txt dir1/ file2.log link1 -> file1.txt ``` Running `analyze_directory(\\"/example\\")` might produce output similar to: ``` Path: /example/file1.txt Type: Regular file Permissions: -rw-r--r-- Path: /example/dir1 Type: Directory Permissions: drwxr-xr-x Path: /example/dir1/file2.log Type: Regular file Permissions: -rw-r--r-- Path: /example/link1 Type: Symbolic link Permissions: lrwxrwxrwx ``` Constraints - Do not hard-code file types or permissions; derive them using the `stat` module functions. - Ensure your code handles exceptions that may arise from file access permissions or other unexpected issues gracefully. Notes - You may use the `os`, `stat`, and `sys` modules. - Recursion is required to traverse subdirectories. - Handle symbolic links appropriately to avoid infinite loops.","solution":"import os import stat def analyze_directory(directory): Recursively analyzes files in the specified directory. Prints the path, type, and permissions of each file. def analyze_file(file_path): try: file_stat = os.lstat(file_path) mode = file_stat.st_mode if stat.S_ISDIR(mode): file_type = \\"Directory\\" elif stat.S_ISREG(mode): file_type = \\"Regular file\\" elif stat.S_ISLNK(mode): file_type = \\"Symbolic link\\" else: file_type = \\"Other\\" permissions = stat.filemode(mode) print(f\\"Path: {file_path}\\") print(f\\"Type: {file_type}\\") print(f\\"Permissions: {permissions}n\\") except Exception as e: print(f\\"Cannot analyze {file_path}: {e}\\") def recursive_walk(curr_dir): for entry in os.scandir(curr_dir): analyze_file(entry.path) if entry.is_dir(follow_symlinks=False): recursive_walk(entry.path) recursive_walk(directory)"},{"question":"# Question: Analyzing and Optimizing Code with Python Profiling and Debugging Tools You are given a function `compute_heavy_math` that performs extensive mathematical computations. This function is suspected of being inefficient and potentially having memory leaks. Your task is to use Python\'s debugging and profiling tools to analyze the function and suggest optimizations. Function to Analyze ```python def compute_heavy_math(n): import math result = [] for i in range(1, n+1): val = math.factorial(i) ** 2 / math.sqrt(i) result.append(val) return result ``` Requirements: 1. **Profiling**: Use the `cProfile` module to profile the `compute_heavy_math` function and identify the major bottlenecks in its performance. 2. **Memory Tracking**: Use the `tracemalloc` module to track the memory allocations and detect if there\'s a significant memory usage issue. 3. **Optimization**: Based on the profiling and memory analysis, optimize the `compute_heavy_math` function to improve its performance and reduce memory consumption. 4. **Documentation**: Document your findings, including the bottlenecks identified, memory usage patterns, and the optimizations made. Input: - An integer `n` representing the number of computations the function should perform. Output: - A tuple of two elements: 1. A list containing the results of the optimized function. 2. A summary dictionary containing: - The total execution time before and after optimization. - Peak memory usage before and after optimization. - Description of the optimizations made. Constraints: 1. `1 <= n <= 1000` Example: Suppose the profiling output suggests that the factorial computation is the major bottleneck. ```python n = 100 # Before optimization, the function returns: result = compute_heavy_math(n) # Profiling and memory analysis might suggest optimization like caching previously computed factorial values. # After optimization: optimized_result = optimized_compute_heavy_math(n) # Your optimized function # Example of the output: ( optimized_result, { \\"time_before_optimization\\": 2.5, # seconds \\"time_after_optimization\\": 1.0, # seconds \\"memory_before_optimization\\": \\"50MB\\", # peak memory usage \\"memory_after_optimization\\": \\"30MB\\", # peak memory usage \\"optimizations\\": \\"Used caching for factorials to avoid redundant computations.\\" } ) ``` # Note: 1. You need to submit your solution as a Python script with embedded documentation explaining each step. 2. Include both the original and the optimized functions in your submission. 3. Include the profiling and tracemalloc reports in your documentation.","solution":"import cProfile import tracemalloc import math from functools import lru_cache from time import time def compute_heavy_math(n): result = [] for i in range(1, n+1): val = math.factorial(i) ** 2 / math.sqrt(i) result.append(val) return result def profile_and_optimize(n): # Profiling the original function profiler = cProfile.Profile() profiler.enable() start_time = time() compute_heavy_math(n) total_time_before = time() - start_time profiler.disable() profiler.print_stats() # Memory tracking the original function tracemalloc.start() compute_heavy_math(n) current, peak_before = tracemalloc.get_traced_memory() tracemalloc.stop() # Optimizing the function @lru_cache(maxsize=None) def cached_factorial(i): return math.factorial(i) def optimized_compute_heavy_math(n): result = [] for i in range(1, n+1): val = cached_factorial(i) ** 2 / math.sqrt(i) result.append(val) return result # Profiling the optimized function profiler.enable() start_time = time() optimized_results = optimized_compute_heavy_math(n) total_time_after = time() - start_time profiler.disable() profiler.print_stats() # Memory tracking the optimized function tracemalloc.start() optimized_compute_heavy_math(n) current, peak_after = tracemalloc.get_traced_memory() tracemalloc.stop() # Summary of the profiling and optimization results summary = { \\"time_before_optimization\\": total_time_before, \\"time_after_optimization\\": total_time_after, \\"memory_before_optimization\\": peak_before, \\"memory_after_optimization\\": peak_after, \\"optimizations\\": \\"Used caching for factorials to avoid redundant computations.\\" } return optimized_results, summary"},{"question":"# Custom URL Opener with urllib.request **Objective:** Demonstrate your understanding of the `urllib.request` module by creating a custom URL opener that handles HTTP GET and POST requests, manages redirections, custom headers, and optionally proxies. # Requirements: 1. **Function Definitions:** - Define a function `custom_url_opener(url: str, data: dict = None, headers: dict = None, proxies: dict = None) -> tuple` to handle URL requests. - **Parameters:** - `url`: A string representing the URL to open. - `data`: A dictionary for POST data. If `None`, a GET request is made. When given, a POST request is made. - `headers`: A dictionary of custom headers to include in the request. - `proxies`: A dictionary of proxy settings. If `None`, no proxy is used. - **Returns:** - A tuple containing the HTTP response status code and the response content decoded to a string. 2. **Functionality:** - Use `urlopen()` to manage requests. - Handle potential redirections. - Include the custom headers if specified. - Use the provided proxy settings if available. 3. **Error Handling:** - Raise a descriptive error if the URL cannot be opened or if HTTP errors occur. # Example Function Call: ```python response_code, content = custom_url_opener( url=\'http://www.example.com\', data={\'key\': \'value\'}, headers={\'User-Agent\': \'custom-agent/1.0\'}, proxies={\'http\': \'http://proxy.example.com:8080/\'} ) print(response_code) print(content) ``` # Constraints: - Use only the `urllib.request` module for handling URL requests. - Assume the `data` is a simple dictionary of key-value pairs. - Use appropriate exception handling to manage any errors during the request. # Implementation: ```python import urllib.request import urllib.parse def custom_url_opener(url: str, data: dict = None, headers: dict = None, proxies: dict = None) -> tuple: try: # Prepare request object if data: data = urllib.parse.urlencode(data).encode(\'ascii\') req = urllib.request.Request(url, data=data) else: req = urllib.request.Request(url) # Add custom headers if headers: for key, value in headers.items(): req.add_header(key, value) # Handle proxies if proxies: proxy_handler = urllib.request.ProxyHandler(proxies) opener = urllib.request.build_opener(proxy_handler) else: opener = urllib.request.build_opener() # Install the opener if necessary, or use it directly response = opener.open(req) # Read and decode the response response_code = response.status content = response.read().decode(\'utf-8\') return response_code, content except urllib.error.HTTPError as e: raise RuntimeError(f\'HTTP Error: {e.code} - {e.reason}\') except urllib.error.URLError as e: raise RuntimeError(f\'URL Error: {e.reason}\') ``` This question assesses the student\'s ability to utilize various aspects of the `urllib.request` module, including constructing custom requests, handling headers, managing proxies, and implementing error handling. It requires a comprehensive understanding of how HTTP operations are implemented using the module.","solution":"import urllib.request import urllib.parse def custom_url_opener(url: str, data: dict = None, headers: dict = None, proxies: dict = None) -> tuple: try: # Prepare request object if data: data = urllib.parse.urlencode(data).encode(\'ascii\') req = urllib.request.Request(url, data=data) else: req = urllib.request.Request(url) # Add custom headers if headers: for key, value in headers.items(): req.add_header(key, value) # Handle proxies if proxies: proxy_handler = urllib.request.ProxyHandler(proxies) opener = urllib.request.build_opener(proxy_handler) else: opener = urllib.request.build_opener() # Use the opener to open the URL response = opener.open(req) # Read and decode the response response_code = response.status content = response.read().decode(\'utf-8\') return response_code, content except urllib.error.HTTPError as e: raise RuntimeError(f\'HTTP Error: {e.code} - {e.reason}\') except urllib.error.URLError as e: raise RuntimeError(f\'URL Error: {e.reason}\')"},{"question":"# PyTorch Coding Assessment Question Objective To demonstrate your understanding of PyTorch\'s data utilities, particularly data loading and preprocessing, through the implementation of PyTorch `DataLoader` and `Dataset`. Problem Statement You are provided with a CSV file called `data.csv` containing two columns: `feature` and `label`. Implement a custom PyTorch `Dataset` and use the `DataLoader` to create batches of data for training a simple model. Requirements 1. **Custom Dataset Class**: - Implement a class `CustomDataset` that inherits from `torch.utils.data.Dataset`. - The class should: - Load the data from the given CSV file in the constructor. - Implement the `__len__` method to return the size of the dataset. - Implement the `__getitem__` method to return a single data sample (feature and label) at the given index. 2. **DataLoader**: - Use the `CustomDataset` class and `torch.utils.data.DataLoader` to: - Create an instance of the `CustomDataset`. - Create a `DataLoader` instance that: - Provides batches of size 4. - Shuffles the data at each epoch. 3. **Function Implementation**: - Write a function `get_dataloader(csv_path: str, batch_size: int) -> torch.utils.data.DataLoader` that: - Accepts a file path to the CSV and a batch size as inputs. - Returns a DataLoader instance configured as specified above. Example Usage Below is an example of how your implementation might be used: ```python import torch # Assuming your file path is \\"data.csv\\" csv_path = \\"data.csv\\" batch_size = 4 dataloader = get_dataloader(csv_path, batch_size) for batch in dataloader: features, labels = batch print(\\"Features:\\", features) print(\\"Labels:\\", labels) ``` Constraints - You can assume that the CSV file is properly formatted and contains no missing values. - The `feature` column is numerical data, while the `label` column contains integer labels. Performance Requirements - Ensure efficient data loading and memory usage, as this is crucial for training machine learning models. # Submission Please submit your Python code implementing the requirements specified above.","solution":"import torch from torch.utils.data import Dataset, DataLoader import pandas as pd class CustomDataset(Dataset): def __init__(self, csv_path): self.data = pd.read_csv(csv_path) def __len__(self): return len(self.data) def __getitem__(self, idx): feature = self.data.iloc[idx, 0] label = self.data.iloc[idx, 1] return torch.tensor(feature, dtype=torch.float32), torch.tensor(label, dtype=torch.long) def get_dataloader(csv_path: str, batch_size: int) -> DataLoader: dataset = CustomDataset(csv_path) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) return dataloader"},{"question":"# Coding Assessment Task: Mocking a Service for Unit Testing **Objective**: Write a function and unit tests for it using the `unittest.mock` module to demonstrate your understanding of mocking, patching, and making assertions about mocked objects. **Problem Statement**: You are provided with a function `process_orders` that processes a list of orders by interacting with an external payment service. Your task is to implement this function and write unit tests to ensure each part of the process runs correctly. The function should simulate interaction with the payment service using mock objects. **Function Details**: 1. **Function Name**: `process_orders` 2. **Input**: A list of order IDs. Each order ID is a string. 3. **Output**: None **Requirements**: - The function should iterate through each order ID, call a method `validate_order(order_id)` to check if the order ID is valid, and then call another method `process_payment(order_id)` to process the payment. - If `validate_order` returns `True`, proceed with `process_payment`. If it returns `False`, skip to the next order. - If `process_payment` raises an Exception, handle it gracefully and log an error message (you can use a simple print statement for logging). **Your tasks**: 1. Implement the `process_orders` function. 2. Implement a class `PaymentService` with methods `validate_order` and `process_payment`. 3. Write unit tests for `process_orders` using the `unittest.mock` module. 4. Use `patch` to mock the `PaymentService` methods in your tests. 5. Make assertions to verify that `validate_order` and `process_payment` are called with the correct arguments, and that exceptions in `process_payment` are handled correctly. **Constraints:** - The `validate_order` method should return a boolean (`True` or `False`). - The `process_payment` method should raise an exception if the payment cannot be processed. ```python class PaymentService: def validate_order(self, order_id): # Implementation details pass def process_payment(self, order_id): # Implementation details pass def process_orders(order_ids): # Your implementation here pass ``` **Example Usage**: ```python orders = [\\"order1\\", \\"order2\\", \\"order3\\"] process_orders(orders) ``` Unit Test Example: ```python import unittest from unittest.mock import patch, MagicMock from your_module import process_orders, PaymentService class TestProcessOrders(unittest.TestCase): @patch(\'your_module.PaymentService.validate_order\') @patch(\'your_module.PaymentService.process_payment\') def test_process_orders(self, mock_process_payment, mock_validate_order): mock_validate_order.side_effect = [True, False, True] mock_process_payment.side_effect = [None, Exception(\'Payment Failed\')] orders = [\'order1\', \'order2\', \'order3\'] process_orders(orders) mock_validate_order.assert_has_calls([call(\'order1\'), call(\'order2\'), call(\'order3\')]) mock_process_payment.assert_any_call(\'order1\') mock_process_payment.assert_any_call(\'order3\') mock_process_payment.assert_not_called_with(\'order2\') self.assertEqual(mock_process_payment.call_count, 2) # Log assertions, etc. if __name__ == \'__main__\': unittest.main() ``` Note: Replace `your_module` with the actual name of the module where `process_orders` and `PaymentService` are defined.","solution":"class PaymentService: def validate_order(self, order_id): Simulates validation of an order ID. :param order_id: Order ID to validate :return: Boolean indicating if the order ID is valid pass def process_payment(self, order_id): Simulates processing payment for an order. :param order_id: Order ID for which payment is to be processed :raises: Exception if payment processing fails pass def process_orders(order_ids): Processes a list of orders by validating each order ID and processing the payment if the order is valid. :param order_ids: List of order IDs to be processed payment_service = PaymentService() for order_id in order_ids: if payment_service.validate_order(order_id): try: payment_service.process_payment(order_id) except Exception as e: print(f\\"Error processing payment for order {order_id}: {e}\\")"},{"question":"# Question: Visualizing Categorical and Numerical Data with Seaborn’s Swarmplot **Objective**: Demonstrate your understanding of seaborn\'s `swarmplot` by creating multiple visualizations that explore and compare distributions in a dataset. **Dataset**: Use the built-in seaborn dataset `\\"tips\\"` for this task. **Tasks**: 1. Load the dataset and display the first few rows. 2. Create a `swarmplot` visualizing the distribution of `total_bill` across different days. 3. Enhance the plot from step 2 by using `hue` to add a second categorical variable (`sex`). 4. Change the orientation of the plot to have days on the x-axis and `total_bill` on the y-axis. 5. Use a different color palette (`deep`) while visualizing `total_bill` across different days and hues. 6. Adjust the point size to avoid overlap in areas of high density. 7. Create a faceted `swarmplot` using `catplot` to show `total_bill` across different `time` (Lunch/Dinner) and `day`, with `sex` as the hue, and ensure that each facet has an appropriate aspect ratio. **Constraints**: - Ensure that your plots are clear and well-labeled. - Use appropriate titles and labels for axes. - Experiment with additional seaborn and matplotlib parameters to enhance visuals. - Ensure no warnings related to overlapping points are present in the final plots. **Expected Input/Output Examples**: - `sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\")` would produce a plot of `total_bill` grouped by `day` with points colored by `sex`. - `sns.catplot(data=tips, kind=\\"swarm\\", x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=.5)` would create faceted plots of `total_bill` across different `time` and `day` with hues representing `sex`. **Performance Requirements**: - Plots should be rendered without significant delay and are expected to display efficiently for datasets of this size. **Submission**: Once you complete the tasks, save your code in a single Jupyter notebook (or Python script if Jupyter is not available) and include brief comments explaining each step. **Notes**: This task is intended to assess your ability to create meaningful visual representations of data using seaborn. Make sure to explore the documentation provided and experiment with different configurations and parameters.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_swarmplot(): # 1. Load the dataset and display the first few rows. tips = sns.load_dataset(\\"tips\\") print(tips.head()) # 2. Create a swarmplot visualizing the distribution of `total_bill` across different days. plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"day\\", y=\\"total_bill\\") plt.title(\'Distribution of Total Bill Across Different Days\') plt.show() # 3. Enhance the plot from step 2 by using `hue` to add a second categorical variable (`sex`). plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\") plt.title(\'Distribution of Total Bill Across Different Days (by Sex)\') plt.show() # 4. Change the orientation of the plot to have days on the x-axis and total_bill on the y-axis. plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\") plt.title(\'Distribution of Total Bill by Sex Across Different Days\') plt.show() # 5. Use a different color palette (`deep`) while visualizing total_bill across different days and hues. plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", palette=\\"deep\\") plt.title(\'Distribution of Total Bill Across Different Days (by Sex, Palette: deep)\') plt.show() # 6. Adjust the point size to avoid overlap in areas of high density. plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", palette=\\"deep\\", size=6) plt.title(\'Distribution of Total Bill Across Different Days (by Sex, Adjusted Size)\') plt.show() # 7. Create a faceted swarmplot using catplot to show total_bill across different time (Lunch/Dinner) and day, with sex as the hue, # and ensure that each facet has an appropriate aspect ratio. g = sns.catplot(data=tips, kind=\\"swarm\\", x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", palette=\\"deep\\", aspect=0.8, height=6) g.set_titles(\\"{col_name} Distribution of Total Bill by Time and Sex\\") plt.show()"},{"question":"**Context:** You\'re analyzing a dataset that requires you to visualize data using seaborn while customizing the plot\'s aesthetics for different settings and audiences. Your task is to create a function that produces a customized line plot with various seaborn styling features. **Task:** Write a Python function called `customize_seaborn_plot` that: 1. **Loads a dataset** using seaborn\'s `sns.load_dataset()`. The dataset to load is \\"flights\\". 2. **Filters the dataset** to include only data from a specific year (provided as a parameter). 3. **Sets the seaborn theme** to `\\"darkgrid\\"`. 4. **Creates a line plot** showing the number of passengers over the months for the specified year. 5. **Uses the \\"talk\\" context** for the plot to make it suitable for a presentation. 6. **Temporarily switches** to the `\\"whitegrid\\"` style within a `with` statement and overlays a boxplot on the same figure. 7. **Removes the top and right spines** of the plot. 8. **Overrides the default font size of labels** to make them larger. 9. **Returns the figure** and axes objects for further customization or saving. **Function Signature:** ```python def customize_seaborn_plot(year: int) -> tuple: pass ``` **Input:** - `year` (int): The year to filter the dataset. **Output:** - A tuple containing the figure and axes objects. **Example:** ```python fig, ax = customize_seaborn_plot(1950) ``` **Constraints:** - The year provided must be within the range of years available in the \\"flights\\" dataset. - You must use seaborn and matplotlib for creating and customizing the plots. - Ensure the final plot\'s labels are readable and appropriately scaled for a \\"talk\\" setting. **Notes:** - You are encouraged to refer to seaborn documentation for methods and further customization options. - Properly handle any edge cases such as invalid years by raising appropriate errors.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customize_seaborn_plot(year: int) -> tuple: # Load the dataset flights = sns.load_dataset(\\"flights\\") # Verify if the year exists in the dataset if year not in flights[\'year\'].unique(): raise ValueError(f\\"Year {year} not found in the dataset.\\") # Filter the dataset by the specific year flights_year = flights[flights[\'year\'] == year] # Set seaborn theme to \'darkgrid\' sns.set_theme(style=\\"darkgrid\\", context=\\"talk\\") # Create the line plot fig, ax = plt.subplots() sns.lineplot(data=flights_year, x=\'month\', y=\'passengers\', ax=ax) # Switch to \'whitegrid\' style temporarily for overlaying boxplot with sns.axes_style(\\"whitegrid\\"): sns.boxplot(data=flights_year, x=\'month\', y=\'passengers\', ax=ax, whis=[0, 100], width=0.2, color=\\"0.2\\") # Remove the top and right spines sns.despine(ax=ax, top=True, right=True) # Override the default font size of labels ax.set_xlabel(\'Month\', fontsize=14) ax.set_ylabel(\'Passengers\', fontsize=14) ax.set_title(f\'Number of Passengers in {year}\', fontsize=16) return fig, ax"},{"question":"Objective: To test the understanding of creating, manipulating, and converting Unicode objects in Python using the provided `python310` package. Problem Statement: You are tasked with implementing a Python function that takes a string, performs several operations on it, and returns a new string with the results of those operations. Implement the following function: ```python def process_unicode_text(input_str: str, encoding: str) -> str: Process the input string by performing the following operations: 1. Decode the input string from the specified encoding to a Unicode string. 2. Replace all lowercase alphabetic characters with their uppercase counterparts. 3. Replace all spaces with underscores (\\"_\\"). 4. Re-encode the resulting string back to the specified encoding. 5. Return the final re-encoded string. Args: - input_str (str): The input string to be processed. - encoding (str): The encoding to be used for decoding and re-encoding the string. Returns: - str: The processed string re-encoded in the specified encoding. Raises: - ValueError: If the input string could not be decoded or re-encoded using the specified encoding. ``` Constraints: - The input string may contain both ASCII and non-ASCII characters. - The encoding parameter will be a valid character encoding recognized by Python (e.g., \'utf-8\', \'latin-1\', \'ascii\'). - If the string cannot be re-encoded using the specified encoding, raise a `ValueError`. Example Usage: ```python try: result = process_unicode_text(\\"hello world\\", \\"utf-8\\") print(result) # Expected Output: \\"HELLO_WORLD\\" encoded in utf-8 except ValueError as e: print(e) try: result = process_unicode_text(\\"こんにちは 世界\\", \\"utf-8\\") print(result) # Expected Output: \\"こんにちは_世界\\" encoded in utf-8 with lowercase characters converted to uppercase if any except ValueError as e: print(e) ``` Notes: - Use the `PyUnicode_FromStringAndSize`, `PyUnicode_AsEncodedString`, and related functions to handle Unicode strings and encodings. - Ensure proper error handling to raise `ValueError` when encoding/decoding fails.","solution":"def process_unicode_text(input_str: str, encoding: str) -> str: Process the input string by performing the following operations: 1. Decode the input string from the specified encoding to a Unicode string. 2. Replace all lowercase alphabetic characters with their uppercase counterparts. 3. Replace all spaces with underscores (\\"_\\"). 4. Re-encode the resulting string back to the specified encoding. 5. Return the final re-encoded string. Args: - input_str (str): The input string to be processed. - encoding (str): The encoding to be used for decoding and re-encoding the string. Returns: - str: The processed string re-encoded in the specified encoding. Raises: - ValueError: If the input string could not be decoded or re-encoded using the specified encoding. try: # Decode the input string from the specified encoding to a Unicode string decoded_str = input_str.encode(encoding).decode(encoding) # Replace all lowercase alphabetic characters with their uppercase counterparts upper_str = decoded_str.upper() # Replace all spaces with underscores (\\"_\\") final_str = upper_str.replace(\\" \\", \\"_\\") # Re-encode the resulting string back to the specified encoding encoded_str = final_str.encode(encoding) # Return the final re-encoded string as a decoded str (string depends on the encoding) return encoded_str.decode(encoding) except Exception as e: # Raise ValueError if encoding/decoding fails raise ValueError(f\\"Encoding/decoding error: {e}\\")"},{"question":"Objective You are tasked with creating a system that uses the `xdrlib` module to encode and decode custom structured data. The data structure we need to handle is a list of dictionaries, where each dictionary contains string keys with integer, string, or float values. This will test your understanding of the `xdrlib` module\'s capabilities and your ability to apply them to solve a complex encoding/decoding problem. Problem Statement You need to implement two functions: 1. **`pack_data(data: List[Dict[str, Union[int, float, str]]]) -> bytes`**: - Takes a list of dictionaries as input and returns the packed data as bytes. - See below for the format details. 2. **`unpack_data(data: bytes) -> List[Dict[str, Union[int, float, str]]]`**: - Takes packed bytes data and returns the unpacked list of dictionaries. - Ensure proper error handling using the `xdrlib` exception classes. Data Format The list of dictionaries should be encoded as follows: 1. The total number of dictionaries in the list should be packed as an unsigned integer. 2. For each dictionary: - The number of key-value pairs should be packed as an unsigned integer. - Each key-value pair packs: - A variable length string key. - An unsigned integer for the type (0 for int, 1 for float, 2 for string). - The corresponding value: - If the value is an integer, it should be packed as an int. - If the value is a float, it should be packed as a double. - If the value is a string, it should be packed as a variable length string. Requirements - **Packing Function**: Ensure the data alignment rules of XDR (padding to a multiple of 4 bytes). - **Unpacking Function**: Properly reconstruct the original list of dictionaries from the packed data. - **Error Handling**: Handle exceptions that might occur during packing or unpacking using `xdrlib.Error` or `xdrlib.ConversionError`. Function Signatures ```python from typing import List, Dict, Union import xdrlib def pack_data(data: List[Dict[str, Union[int, float, str]]]) -> bytes: # Your code here def unpack_data(data: bytes) -> List[Dict[str, Union[int, float, str]]]: # Your code here ``` Example Given the following list of dictionaries: ```python data = [ {\\"age\\": 25, \\"name\\": \\"Alice\\", \\"score\\": 85.5}, {\\"age\\": 30, \\"name\\": \\"Bob\\", \\"score\\": 90.0} ] ``` The `pack_data` function should convert it to a bytes object. The `unpack_data` function should then convert these bytes back to the original list of dictionaries. You can assume that all keys are strings, and values are restricted to integers, floats, and strings. **Note**: The provided list of dictionaries and byte objects in the example are illustrative. You will need to verify your implementation using a variety of test cases.","solution":"import xdrlib from typing import List, Dict, Union def pack_data(data: List[Dict[str, Union[int, float, str]]]) -> bytes: packer = xdrlib.Packer() # Pack the number of dictionaries packer.pack_uint(len(data)) for dictionary in data: # Pack the number of key-value pairs in the dictionary packer.pack_uint(len(dictionary)) for key, value in dictionary.items(): # Pack the key as a variable length string packer.pack_string(key.encode()) # Determine the type of value and pack accordingly if isinstance(value, int): packer.pack_uint(0) # Type code for integer packer.pack_int(value) elif isinstance(value, float): packer.pack_uint(1) # Type code for float packer.pack_double(value) elif isinstance(value, str): packer.pack_uint(2) # Type code for string packer.pack_string(value.encode()) else: raise ValueError(f\\"Unsupported data type: {type(value)}\\") return packer.get_buffer() def unpack_data(data: bytes) -> List[Dict[str, Union[int, float, str]]]: unpacker = xdrlib.Unpacker(data) # Unpack the number of dictionaries num_dicts = unpacker.unpack_uint() result = [] for _ in range(num_dicts): # Unpack the number of key-value pairs in the dictionary num_pairs = unpacker.unpack_uint() dictionary = {} for _ in range(num_pairs): # Unpack the key key = unpacker.unpack_string().decode() # Determine the type of value and unpack accordingly value_type = unpacker.unpack_uint() if value_type == 0: value = unpacker.unpack_int() elif value_type == 1: value = unpacker.unpack_double() elif value_type == 2: value = unpacker.unpack_string().decode() else: raise ValueError(f\\"Unsupported data type for value_type: {value_type}\\") dictionary[key] = value result.append(dictionary) return result"},{"question":"You are given a dataset of survey responses where participants rate their satisfaction with a product on a Likert scale and provide some demographic information. ```python import pandas as pd data = { \'Participant_ID\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \'Age_Group\': [\'18-25\', \'26-35\', \'36-45\', \'46-55\', \'56-65\', \'18-25\', \'18-25\', \'36-45\', \'26-35\', \'46-55\'], \'Satisfaction\': [\'Very Satisfied\', \'Satisfied\', \'Neutral\', \'Dissatisfied\', \'Very Dissatisfied\', \'Satisfied\', \'Neutral\', \'Very Satisfied\', \'Dissatisfied\', \'Neutral\'], \'Gender\': [\'Female\', \'Male\', \'Female\', \'Female\', \'Male\', \'Male\', \'Female\', \'Female\', \'Male\', \'Female\'] } df = pd.DataFrame(data) ``` # Task 1. Convert the `Age_Group` column to a categorical type with the ordered categories: `[\'18-25\', \'26-35\', \'36-45\', \'46-55\', \'56-65\']`. 2. Convert the `Satisfaction` column to a categorical type with the ordered categories: `[\'Very Dissatisfied\', \'Dissatisfied\', \'Neutral\', \'Satisfied\', \'Very Satisfied\']`. 3. Extract a summary of the `Satisfaction` column using `describe()` method. Explain the output in terms of categorical data. 4. Add a new category `Prefer Not to Say` to the `Gender` column without changing any values in the column. 5. Replace the `Gender` of participants with `Participant_ID` 5 and 10 to `Prefer Not to Say`. 6. Sort the DataFrame first by `Age_Group` and then by `Satisfaction`. # Constraints - Ensure the `Age_Group` and `Satisfaction` columns maintain their logical ordering. - Ensure that the new category added to the `Gender` column doesn\'t disrupt the initial data except for the specific changes required. - The final DataFrame should be displayed after sorting. # Expected Output 1. Display the DataFrame after converting columns to categorical types. 2. Summary output of `Satisfaction` categorical column. 3. Display the DataFrame after adding a new category and making replacements in the `Gender` column. 4. Display the sorted DataFrame. # Example Output ```plaintext Participant_ID Age_Group Satisfaction Gender 0 1 18-25 Very Satisfied Female 1 2 26-35 Satisfied Male 2 3 36-45 Neutral Female 3 4 46-55 Dissatisfied Female 4 5 56-65 Very Dissatisfied Prefer Not to Say 5 6 18-25 Satisfied Male 6 7 18-25 Neutral Female 7 8 36-45 Very Satisfied Female 8 9 26-35 Dissatisfied Male 9 10 46-55 Neutral Prefer Not to Say Description of Satisfaction: count 10 unique 5 top Neutral freq 3 Name: Satisfaction, dtype: object Participant_ID Age_Group Satisfaction Gender 0 1 18-25 Very Satisfied Female 6 7 18-25 Neutral Female 5 6 18-25 Satisfied Male 1 2 26-35 Satisfied Male 8 9 26-35 Dissatisfied Male 3 4 46-55 Dissatisfied Female 9 10 46-55 Neutral Prefer Not to Say 2 3 36-45 Neutral Female 7 8 36-45 Very Satisfied Female 4 5 56-65 Very Dissatisfied Prefer Not to Say ```","solution":"import pandas as pd data = { \'Participant_ID\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \'Age_Group\': [\'18-25\', \'26-35\', \'36-45\', \'46-55\', \'56-65\', \'18-25\', \'18-25\', \'36-45\', \'26-35\', \'46-55\'], \'Satisfaction\': [\'Very Satisfied\', \'Satisfied\', \'Neutral\', \'Dissatisfied\', \'Very Dissatisfied\', \'Satisfied\', \'Neutral\', \'Very Satisfied\', \'Dissatisfied\', \'Neutral\'], \'Gender\': [\'Female\', \'Male\', \'Female\', \'Female\', \'Male\', \'Male\', \'Female\', \'Female\', \'Male\', \'Female\'] } df = pd.DataFrame(data) # 1. Convert the Age_Group column to a categorical type with ordered categories age_categories = [\'18-25\', \'26-35\', \'36-45\', \'46-55\', \'56-65\'] df[\'Age_Group\'] = pd.Categorical(df[\'Age_Group\'], categories=age_categories, ordered=True) # 2. Convert the Satisfaction column to a categorical type with ordered categories satisfaction_categories = [\'Very Dissatisfied\', \'Dissatisfied\', \'Neutral\', \'Satisfied\', \'Very Satisfied\'] df[\'Satisfaction\'] = pd.Categorical(df[\'Satisfaction\'], categories=satisfaction_categories, ordered=True) # 3. Extract a summary of the Satisfaction column using describe() method satisfaction_summary = df[\'Satisfaction\'].describe() # 4. Add a new category Prefer Not to Say to the Gender column without changing any values in the column gender_categories = [\'Female\', \'Male\', \'Prefer Not to Say\'] df[\'Gender\'] = df[\'Gender\'].astype(pd.CategoricalDtype(categories=gender_categories)) # 5. Replace the Gender of participants with Participant_ID 5 and 10 df.loc[df[\'Participant_ID\'].isin([5, 10]), \'Gender\'] = \'Prefer Not to Say\' # 6. Sort the DataFrame first by Age_Group and then by Satisfaction df = df.sort_values(by=[\'Age_Group\', \'Satisfaction\']).reset_index(drop=True) # Display the DataFrame df"},{"question":"You are tasked with developing a function that processes a text file to extract specific lines and perform operations on them using the \\"linecache\\" module. Function Signature ```python def process_file(filename: str, lines: List[int]) -> List[str]: pass ``` Input - `filename`: A string representing the name of the file to be processed. - `lines`: A list of integers where each integer represents the line number that needs to be extracted from the file. Output - Returns a list of strings where each string is the content of the line from the corresponding line number in the `lines` list. Constraints - `filename` must point to a valid text file. - Line numbers in `lines` are guaranteed to be positive integers. - The function should handle non-existent line numbers gracefully by including an empty string (`\'\'`) in their place. - After fetching all the lines, the internal cache should be cleared. Example ```python # Assume \'example.txt\' contains the following lines: # 1: \\"Hello, World!n\\" # 2: \\"This is a test file.n\\" # 3: \\"It contains multiple lines.n\\" filename = \'example.txt\' lines = [1, 3, 5] output = process_file(filename, lines) # Expected output: [\\"Hello, World!n\\", \\"It contains multiple lines.n\\", \'\'] ``` Remarks The function `process_file` should make effective use of the \\"linecache\\" module functions to extract the specified lines and ensure that the cache is managed properly to optimize performance.","solution":"import linecache from typing import List def process_file(filename: str, lines: List[int]) -> List[str]: output = [] for line_number in lines: line = linecache.getline(filename, line_number) output.append(line) linecache.clearcache() return output"},{"question":"Question Implement a function in PyTorch that simulates a mixture of two different distributions from the `torch.distributions` module. You need to demonstrate the functionality by generating a number of samples and plotting the histogram of the resulting data. # Detailed Instructions: 1. **Function Signature**: ```python def simulate_mixture(distribution1, distribution2, w1, w2, num_samples): Simulate a mixture of two distributions and generate samples. Parameters: - distribution1 (torch.distributions.Distribution): The first distribution to sample from. - distribution2 (torch.distributions.Distribution): The second distribution to sample from. - w1 (float): The weight (probability) for sampling from the first distribution. - w2 (float): The weight (probability) for sampling from the second distribution. Note that w1 + w2 should equal 1. - num_samples (int): The total number of samples to generate. Returns: - torch.Tensor: A tensor containing the generated samples. pass ``` 2. **Weight Constraints**: - `w1 + w2` must equal 1. - `0 <= w1, w2 <= 1`. 3. **Steps to Implement**: - Validate that the weights sum up to 1. - Randomly decide which distribution to sample from based on the provided weights. - Generate the specified number of samples from the corresponding distributions. - Combine the samples from both distributions into a single tensor. 4. **Example**: ```python import torch from torch.distributions import Normal, Exponential import matplotlib.pyplot as plt # Example distributions dist1 = Normal(0, 1) dist2 = Exponential(1) # Generating samples samples = simulate_mixture(dist1, dist2, 0.6, 0.4, 10000) # Plotting the histogram plt.hist(samples.numpy(), bins=50, density=True) plt.title(\\"Histogram of Simulated Mixture Distribution\\") plt.show() ``` # Evaluation Criteria: - **Correctness**: The function must accurately simulate the samples based on the provided distributions and weights. - **Efficiency**: Must handle a reasonable number of samples (up to 1,000,000) efficiently. - **Code Quality**: The code should be clean, well-documented, and adhere to standard coding practices. - **Usage of PyTorch\'s Distribution Module**: Correct usage of PyTorch\'s distribution classes and methods to sample from and handle various distributions. # Notes: - Students may use any plotting library to demonstrate the histogram (e.g., `matplotlib`). - The main focus should be on correctly implementing the mixture sampling logic using PyTorch\'s distributions.","solution":"import torch def simulate_mixture(distribution1, distribution2, w1, w2, num_samples): Simulate a mixture of two distributions and generate samples. Parameters: - distribution1 (torch.distributions.Distribution): The first distribution to sample from. - distribution2 (torch.distributions.Distribution): The second distribution to sample from. - w1 (float): The weight (probability) for sampling from the first distribution. - w2 (float): The weight (probability) for sampling from the second distribution. Note that w1 + w2 should equal 1. - num_samples (int): The total number of samples to generate. Returns: - torch.Tensor: A tensor containing the generated samples. assert w1 + w2 == 1, \\"Weights must sum to 1.\\" assert 0 <= w1 <= 1 and 0 <= w2 <= 1, \\"Weights must be between 0 and 1.\\" # Generate binary mask to decide which distribution each sample comes from mask = torch.rand(num_samples) < w1 # Generate samples from both distributions samples1 = distribution1.sample((num_samples,)) samples2 = distribution2.sample((num_samples,)) # Use the mask to select from the corresponding distributions samples = torch.where(mask, samples1, samples2) return samples"},{"question":"Problem Statement Write a Python function `prime_factors(n: int) -> list` that takes an integer `n` and returns a list of all prime factors of `n`, sorted in ascending order. A prime factor is a factor of a number that is a prime number. Your function should: - Use basic arithmetic operations (+, -, *, /, //, etc.). - Utilize lists to store and manipulate intermediate results. - Demonstrate understanding of loops and conditionals to implement the prime factorization algorithm. Input - `n` (1 <= n <= 10^4): An integer for which you need to find the prime factors. Output - Return a list of integers representing the prime factors of `n`, sorted in ascending order. Example ```python >>> prime_factors(28) [2, 2, 7] >>> prime_factors(100) [2, 2, 5, 5] >>> prime_factors(37) [37] >>> prime_factors(1) [] ``` Constraints - The input `n` will be a positive integer up to 10,000. - The output list should sort the prime factors in ascending order. - You should handle edge cases such as `n` being a prime number itself or `n` being the minimum value 1. Notes - The function must not use any external libraries for finding prime numbers. - Carefully handle edge cases such as when `n` is 1. Solution Template ```python def prime_factors(n: int) -> list: # Your code here pass ``` Evaluation Criteria - Correctness: The function should correctly determine the prime factors for various test cases. - Efficiency: The function should run efficiently within the given constraints. - Code Quality: The code should be well-structured and clearly written, making use of appropriate comments when necessary.","solution":"def prime_factors(n: int) -> list: Returns a list of prime factors of the given integer n, sorted in ascending order. factors = [] # Check for number of 2s while n % 2 == 0: factors.append(2) n //= 2 # Check for other odd factors for i in range(3, int(n**0.5)+1, 2): while n % i == 0: factors.append(i) n //= i # If n is a prime number greater than 2 if n > 2: factors.append(n) return factors"},{"question":"**Title:** Implementing and Interacting with SystemTap/DTrace Probes in a Python Script **Problem Statement:** You are a developer tasked with analyzing the performance of a Python application using SystemTap or DTrace. The goal is to write a Python script that utilizes static markers to track and log the execution of specific functions within the script. **Task:** 1. Implement a Python script named `performance_monitor.py` which contains several functions, including a `main` function that serves as the entry point. 2. Configure and enable SystemTap/DTrace probes within your CPython setup, ensuring you can trace function entries and exits. 3. Write a SystemTap or DTrace script named `trace_script.stp` (for SystemTap) or `trace_script.d` (for DTrace) that: - Captures the entry and exit of every function within `performance_monitor.py`. - Logs the function name, file name, and line number of each function call along with the timestamp. - Organizes the output to show a clear call hierarchy with proper indentation. **Python Script (`performance_monitor.py`):** Here\'s the function signature you should include: ```python def main(): Main function to start the program. pass # Your implementation def function_1(): Function 1 does a task pass # Your implementation def function_2(): Function 2 calls Function 3 pass # Your implementation def function_3(): Function 3 does another task pass # Your implementation ``` *Note: Add necessary logic to make these functions perform simple tasks and demonstrate nested function calls.* **SystemTap/DTrace Script (`trace_script.stp` or `trace_script.d`):** - Write a script that: - Hooks into the CPython function entry and return probes. - Uses the provided arguments (function name, file name, line number) to log function call details. - Outputs the call hierarchy in a human-readable format with proper timestamps and indentation. **Inputs:** No direct inputs are provided. Your SystemTap/DTrace script should infer and log function calls from the executed Python script. **Outputs:** Your SystemTap/DTrace script should produce an output similar to: ``` timestamp function-entry:performance_monitor.py:main:10 timestamp function-entry:performance_monitor.py:function_1:20 timestamp function-return:performance_monitor.py:function_1:25 timestamp function-entry:performance_monitor.py:function_2:30 timestamp function-entry:performance_monitor.py:function_3:40 timestamp function-return:performance_monitor.py:function_3:45 timestamp function-return:performance_monitor.py:function_2:35 timestamp function-return:performance_monitor.py:main:15 ``` **Constraints:** - Ensure your Python functions have unique line numbers for clarity in tracing. - You may create additional helper functions if necessary. - Assume the environment has necessary permissions to run SystemTap or DTrace scripts. - Your solution should be tested with an appropriate setup of CPython with DTrace/SystemTap support enabled. **Bonus:** - Modify your Python functions to include loops or recursive calls and observe the changes in your tracing output. - Write assertions in your tracing script to validate capturing correct function names and line numbers. This problem assesses your understanding of integrating monitoring tools with Python scripts and demonstrates practical usage of CPython\'s debugging and performance analysis features.","solution":"def main(): Main function to start the program. print(\\"Entering main\\") function_1() function_2() print(\\"Exiting main\\") def function_1(): Function 1 does a task print(\\"Entering function_1\\") # Simulate a task print(\\"Exiting function_1\\") def function_2(): Function 2 calls Function 3 print(\\"Entering function_2\\") function_3() print(\\"Exiting function_2\\") def function_3(): Function 3 does another task print(\\"Entering function_3\\") # Simulate another task print(\\"Exiting function_3\\") if __name__ == \\"__main__\\": main()"},{"question":"# Clustering Analysis with Scikit-Learn Objective: The goal of this assessment is to evaluate the students\' abilities to implement and evaluate clustering algorithms using the scikit-learn library. Problem Statement: You are given a dataset consisting of 2-dimensional data points that need to be clustered effectively to identify inherent patterns. Your task is to: 1. Implement and fit different clustering algorithms on the provided dataset. 2. Analyze and compare their performance using appropriate clustering evaluation metrics. Detailed Instructions: 1. **Input Data:** - You are given a CSV file `data.csv` containing two columns representing the 2D data points. - Each row corresponds to a data point. 2. **Implementation Steps:** 1. Load the dataset from the given CSV file. 2. Implement the following clustering algorithms: - K-Means - DBSCAN - Spectral Clustering 3. Fit each algorithm to the dataset and obtain the cluster labels. 4. Visualize the clustering results by plotting the data points and coloring them based on the cluster assignments. 3. **Evaluation:** - Evaluate the performance of each clustering algorithm using at least two different clustering evaluation metrics. You can choose from metrics such as Adjusted Rand Index, Silhouette Score, Calinski-Harabasz Index, or Davies-Bouldin Index. - Compare the results and discuss which algorithm performed the best and why. Expected Functions: ```python import pandas as pd import matplotlib.pyplot as plt from sklearn.cluster import KMeans, DBSCAN, SpectralClustering from sklearn.metrics import adjusted_rand_score, silhouette_score def load_data(file_path): Load the dataset from a CSV file. Parameters: - file_path: str, path to the CSV file Returns: - data: DataFrame, loaded dataset data = pd.read_csv(file_path) return data def kmeans_clustering(data, n_clusters=3): Perform K-Means clustering. Parameters: - data: DataFrame, input dataset - n_clusters: int, the number of clusters to form Returns: - labels: array, cluster labels for each point kmeans = KMeans(n_clusters=n_clusters) labels = kmeans.fit_predict(data) return labels def dbscan_clustering(data, eps=0.5, min_samples=5): Perform DBSCAN clustering. Parameters: - data: DataFrame, input dataset - eps: float, maximum distance between two samples for them to be considered as in the same neighborhood - min_samples: int, number of samples in a neighborhood for a point to be considered as a core point Returns: - labels: array, cluster labels for each point dbscan = DBSCAN(eps=eps, min_samples=min_samples) labels = dbscan.fit_predict(data) return labels def spectral_clustering(data, n_clusters=3): Perform Spectral Clustering. Parameters: - data: DataFrame, input dataset - n_clusters: int, the number of clusters to form Returns: - labels: array, cluster labels for each point spectral = SpectralClustering(n_clusters=n_clusters) labels = spectral.fit_predict(data) return labels def evaluate_clustering(labels_true, labels_pred): Evaluate clustering performance using Adjusted Rand Index and Silhouette Score. Parameters: - labels_true: array, ground truth labels - labels_pred: array, predicted labels by clustering algorithm Returns: - scores: dict, evaluation scores ari = adjusted_rand_score(labels_true, labels_pred) silhouette = silhouette_score(labels_true, labels_pred) return {\\"Adjusted Rand Index\\": ari, \\"Silhouette Score\\": silhouette} def plot_clusters(data, labels, title=\\"Clustering Results\\"): Plot the clustering results. Parameters: - data: DataFrame, input dataset - labels: array, cluster labels for each point - title: str, title of the plot plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels, cmap=\'viridis\', marker=\'.\') plt.title(title) plt.show() def main(): # Load data data = load_data(\\"data.csv\\") # Perform clustering kmeans_labels = kmeans_clustering(data, n_clusters=3) dbscan_labels = dbscan_clustering(data, eps=0.5, min_samples=5) spectral_labels = spectral_clustering(data, n_clusters=3) # Plot clustering results plot_clusters(data, kmeans_labels, title=\\"K-Means Clustering\\") plot_clusters(data, dbscan_labels, title=\\"DBSCAN Clustering\\") plot_clusters(data, spectral_labels, title=\\"Spectral Clustering\\") # Evaluate clustering results # Assuming we have true labels, but using dummy labels here as an example true_labels = [0, 0, 1, 1, 2, 2] kmeans_scores = evaluate_clustering(true_labels, kmeans_labels) dbscan_scores = evaluate_clustering(true_labels, dbscan_labels) spectral_scores = evaluate_clustering(true_labels, spectral_labels) print(f\\"K-Means Scores: {kmeans_scores}\\") print(f\\"DBSCAN Scores: {dbscan_scores}\\") print(f\\"Spectral Scores: {spectral_scores}\\") if __name__ == \\"__main__\\": main() ``` Submission Guidelines: - Ensure that your code is well-commented and follows PEP 8 standards. - Include a brief report (max 2 pages) summarizing your findings from the clustering evaluation and comparisons. - Submit your code along with the report in a single ZIP file.","solution":"import pandas as pd import matplotlib.pyplot as plt from sklearn.cluster import KMeans, DBSCAN, SpectralClustering from sklearn.metrics import adjusted_rand_score, silhouette_score, calinski_harabasz_score, davies_bouldin_score def load_data(file_path): Load the dataset from a CSV file. Parameters: - file_path: str, path to the CSV file Returns: - data: DataFrame, loaded dataset data = pd.read_csv(file_path) return data def kmeans_clustering(data, n_clusters=3): Perform K-Means clustering. Parameters: - data: DataFrame, input dataset - n_clusters: int, the number of clusters to form Returns: - labels: array, cluster labels for each point kmeans = KMeans(n_clusters=n_clusters) labels = kmeans.fit_predict(data) return labels def dbscan_clustering(data, eps=0.5, min_samples=5): Perform DBSCAN clustering. Parameters: - data: DataFrame, input dataset - eps: float, maximum distance between two samples for them to be considered as in the same neighborhood - min_samples: int, number of samples in a neighborhood for a point to be considered as a core point Returns: - labels: array, cluster labels for each point dbscan = DBSCAN(eps=eps, min_samples=min_samples) labels = dbscan.fit_predict(data) return labels def spectral_clustering(data, n_clusters=3): Perform Spectral Clustering. Parameters: - data: DataFrame, input dataset - n_clusters: int, the number of clusters to form Returns: - labels: array, cluster labels for each point spectral = SpectralClustering(n_clusters=n_clusters, assign_labels=\\"discretize\\") labels = spectral.fit_predict(data) return labels def evaluate_clustering(labels_true, labels_pred, data): Evaluate clustering performance using various metrics. Parameters: - labels_true: array, ground truth labels - labels_pred: array, predicted labels by clustering algorithm - data: DataFrame, input dataset Returns: - scores: dict, evaluation scores ari = adjusted_rand_score(labels_true, labels_pred) silhouette = silhouette_score(data, labels_pred) calinski_harabasz = calinski_harabasz_score(data, labels_pred) davies_bouldin = davies_bouldin_score(data, labels_pred) return { \\"Adjusted Rand Index\\": ari, \\"Silhouette Score\\": silhouette, \\"Calinski-Harabasz Index\\": calinski_harabasz, \\"Davies-Bouldin Index\\": davies_bouldin } def plot_clusters(data, labels, title=\\"Clustering Results\\"): Plot the clustering results. Parameters: - data: DataFrame, input dataset - labels: array, cluster labels for each point - title: str, title of the plot plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels, cmap=\'viridis\', marker=\'.\') plt.title(title) plt.show() def main(): # Load data data = load_data(\\"data.csv\\") # Perform clustering kmeans_labels = kmeans_clustering(data, n_clusters=3) dbscan_labels = dbscan_clustering(data, eps=0.5, min_samples=5) spectral_labels = spectral_clustering(data, n_clusters=3) # Plot clustering results plot_clusters(data, kmeans_labels, title=\\"K-Means Clustering\\") plot_clusters(data, dbscan_labels, title=\\"DBSCAN Clustering\\") plot_clusters(data, spectral_labels, title=\\"Spectral Clustering\\") # Assuming we have true labels, here we use an example true_labels = [0] * 50 + [1] * 50 + [2] * 50 # Evaluate clustering results kmeans_scores = evaluate_clustering(true_labels, kmeans_labels, data) dbscan_scores = evaluate_clustering(true_labels, dbscan_labels, data) spectral_scores = evaluate_clustering(true_labels, spectral_labels, data) # Print evaluation results print(\\"K-Means Scores:\\", kmeans_scores) print(\\"DBSCAN Scores:\\", dbscan_scores) print(\\"Spectral Scores:\\", spectral_scores) if __name__ == \\"__main__\\": main()"},{"question":"Advanced Data Visualization with seaborn.objects Objective: Demonstrate your comprehension of seaborn\'s `seaborn.objects` module and its capabilities by performing data preprocessing and creating a detailed plot. Problem Statement: 1. **Data Import and Preprocessing**: - Import the dataset \\"brain_networks\\" using seaborn\'s `load_dataset` method with the following parameters: ```python header=[0, 1, 2], index_col=0 ``` - Rename the axis to \\"timepoint\\". - Stack the dataset on the first three levels and group it by \\"timepoint\\", \\"network\\", and \\"hemi\\". - Calculate the mean for each group and unstack the \\"network\\" level. - Reset the index of the resultant DataFrame. - Filter the dataset to include only rows where \\"timepoint\\" is less than 100. 2. **Plotting**: - Create a pair plot using `seaborn.objects.Plot`: - X-axis should pair `[\\"5\\", \\"8\\", \\"12\\", \\"15\\"]` - Y-axis should pair `[\\"6\\", \\"13\\", \\"16\\"]` - Layout should be (8, 5) and axes should be shared (both x and y). - Add a `Paths` layer to this plot. - Update the plot to include `Paths` with the properties `linewidth=1` and `alpha=0.8`, and color the paths by the \\"hemi\\" variable. Constraints: - Assume that the seaborn and related libraries are pre-installed. - The final plot should be displayed in the output. Expected Input and Output Formats: - **Function Interface**: You are to implement a function `create_brain_network_plot()` - **Input**: The function does not take any parameters. - **Output**: The function should display (not return) the resultant plot. Example Function Signature: ```python def create_brain_network_plot(): import seaborn.objects as so from seaborn import load_dataset # Step 1: Data Import and Preprocessing networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Step 2: Plotting p = ( so.Plot(networks) .pair( x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"], ) .layout(size=(8, 5)) .share(x=True, y=True) ) p.add(so.Paths()) p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\") # Display Plot p.show() ``` This question tests your ability to conduct data preprocessing and leverage seaborn\'s advanced plotting features to create and customize a comprehensive plot.","solution":"def create_brain_network_plot(): import seaborn.objects as so from seaborn import load_dataset # Step 1: Data Import and Preprocessing networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Step 2: Plotting p = ( so.Plot(networks) .pair( x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"], ) .layout(size=(8, 5)) .share(x=True, y=True) ) p.add(so.Paths()) p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\") # Display Plot p.show()"},{"question":"Using `doctest` for Automated Testing Objective In this assessment, you are required to write a function and use the `doctest` module to ensure its correctness through examples in its docstring. Problem Statement You are required to implement a function `calculate_statistics` that takes a list of integers and returns a dictionary containing the minimum, maximum, mean, and median of the list. Additionally, you will use `doctest` to write test cases directly in the docstring of the function to verify its correctness. Requirements 1. Implement the function `calculate_statistics` with the following specifications: - Input: A list of integers. - Output: A dictionary with keys \'min\', \'max\', \'mean\', and \'median\' and their corresponding values. 2. Use the `doctest` module to create test cases in the docstring to verify the correctness of your implementation. Function Signature ```python def calculate_statistics(numbers: list[int]) -> dict[str, float]: Calculate and return the minimum, maximum, mean, and median of a list of integers. Args: numbers (list[int]): A list of integers. Returns: dict[str, float]: A dictionary with keys \'min\', \'max\', \'mean\', and \'median\'. Examples: >>> calculate_statistics([1, 2, 3, 4, 5]) {\'min\': 1, \'max\': 5, \'mean\': 3.0, \'median\': 3.0} >>> calculate_statistics([10, 20, 30]) {\'min\': 10, \'max\': 30, \'mean\': 20.0, \'median\': 20.0} >>> calculate_statistics([1]) {\'min\': 1, \'max\': 1, \'mean\': 1.0, \'median\': 1.0} >>> calculate_statistics([]) Traceback (most recent call last): ... ValueError: The list is empty. # Your implementation here ``` Constraints and Notes - You must handle edge cases, such as an empty list, by raising a `ValueError` with the message \\"The list is empty.\\" - Ensure that your `doctest` examples comprehensively cover various scenarios, including lists of varying lengths and values. Submission Submit your implementation of the `calculate_statistics` function with its docstring containing `doctest` examples. Ensure that your code passes all the tests when run using the `doctest` module. Example Usage To verify your implementation with the `doctest` examples in the docstring, run the following command: ```shell python -m doctest -v your_script.py ``` Replace `your_script.py` with the name of your Python script file containing the `calculate_statistics` function.","solution":"import statistics def calculate_statistics(numbers): Calculate and return the minimum, maximum, mean, and median of a list of integers. Args: numbers (list[int]): A list of integers. Returns: dict[str, float]: A dictionary with keys \'min\', \'max\', \'mean\', and \'median\'. Examples: >>> calculate_statistics([1, 2, 3, 4, 5]) {\'min\': 1, \'max\': 5, \'mean\': 3.0, \'median\': 3.0} >>> calculate_statistics([10, 20, 30]) {\'min\': 10, \'max\': 30, \'mean\': 20.0, \'median\': 20.0} >>> calculate_statistics([1]) {\'min\': 1, \'max\': 1, \'mean\': 1.0, \'median\': 1.0} >>> calculate_statistics([]) Traceback (most recent call last): ... ValueError: The list is empty. if not numbers: raise ValueError(\\"The list is empty.\\") result = { \'min\': min(numbers), \'max\': max(numbers), \'mean\': statistics.mean(numbers), \'median\': statistics.median(numbers) } return result"},{"question":"Objective: Write a Python function that takes the name of an installed Python package and returns a detailed report of its metadata, entry points, and files. The function should demonstrate your understanding of the `importlib.metadata` module. Function Signature: ```python def package_report(package_name: str) -> dict: pass ``` Input: - `package_name` (str): The name of the installed Python package for which to generate the report. Output: - A dictionary with the following keys and corresponding values: - `\'version\'`: The version of the package (str). - `\'metadata\'`: A dictionary containing the package\'s metadata. - `\'entry_points\'`: A dictionary with entry point groups as keys and a list of entry points (name-value pairs) as values. - `\'files\'`: A list of file paths installed by the package. Constraints: - The package must be installed in the Python environment where the function is executed. - Handle cases where certain metadata might be missing gracefully. Example: ```python report = package_report(\'wheel\') print(report) # Expected output { \'version\': \'0.32.3\', \'metadata\': { \'Metadata-Version\': \'2.1\', \'Name\': \'wheel\', \'Version\': \'0.32.3\', \'Summary\': \'A built-package format for Python.\', \'Home-page\': \'https://github.com/pypa/wheel\', ... # other metadata fields }, \'entry_points\': { \'console_scripts\': [ {\'name\': \'wheel\', \'value\': \'wheel.cli:main\'} ], ... # other entry point groups }, \'files\': [ \'wheel/__init__.py\', \'wheel/util.py\', ... # other files ] } ``` Notes: - Use the `importlib.metadata` module to retrieve the necessary data. - You may use helper functions to organize your code for fetching different pieces of information such as version, metadata, entry points, and files. - Ensure that your function can handle cases where the package metadata or files are not fully available, returning an appropriate message or empty value in such cases.","solution":"import importlib.metadata def package_report(package_name: str) -> dict: Generates a detailed report of the given installed Python package. Args: package_name (str): The name of the installed Python package. Returns: dict: A dictionary containing the version, metadata, entry points, and files of the package. try: dist = importlib.metadata.distribution(package_name) except importlib.metadata.PackageNotFoundError: return {\'error\': f\\"Package \'{package_name}\' not found\\"} # Fetch version version = dist.version # Fetch metadata metadata = {k: v for k, v in dist.metadata.items()} # Fetch entry points entry_points = {} if dist.entry_points: for entry_point in dist.entry_points: group = entry_point.group name = entry_point.name value = entry_point.value if group not in entry_points: entry_points[group] = [] entry_points[group].append({\'name\': name, \'value\': value}) # Fetch files files = list(dist.files or []) return { \'version\': version, \'metadata\': metadata, \'entry_points\': entry_points, \'files\': files }"},{"question":"Objective Write a function to generate a dataset using `make_blobs` and perform clustering using `KMeans` from scikit-learn. Function Signature ```python def cluster_blobs(n_samples: int, n_features: int, centers: int, cluster_std: float, random_state: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]: pass ``` Input - `n_samples` (int): Number of samples in the dataset. - `n_features` (int): Number of features for each sample. - `centers` (int): Number of centers to generate. - `cluster_std` (float): The standard deviation of the clusters. - `random_state` (int): Random seed for reproducibility. Output - A tuple containing three elements: - `X` (np.ndarray): The generated samples. - `y_true` (np.ndarray): The true labels of the samples. - `y_pred` (np.ndarray): The predicted labels of the samples after clustering using `KMeans`. Constraints - Use `make_blobs` to generate the dataset. - Use `KMeans` to perform clustering on the generated dataset. - Ensure the function handles any potential exceptions or erroneous input values gracefully. Example ```python X, y_true, y_pred = cluster_blobs(n_samples=100, n_features=2, centers=3, cluster_std=1.0, random_state=42) # The function should generate a dataset with 100 samples, each having 2 features, # organized into 3 clusters with a standard deviation of 1.0. It should then use KMeans # to predict the cluster labels and return the generated samples, true labels, and predicted labels. ``` # Additional Requirements - Import required libraries: ```python import numpy as np from sklearn.datasets import make_blobs from sklearn.cluster import KMeans from typing import Tuple ``` # Notes - You may use the following boilerplate code to get started: ```python def cluster_blobs(n_samples: int, n_features: int, centers: int, cluster_std: float, random_state: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]: # Generate the dataset X, y_true = make_blobs(n_samples=n_samples, n_features=n_features, centers=centers, cluster_std=cluster_std, random_state=random_state) # Perform KMeans clustering kmeans = KMeans(n_clusters=centers, random_state=random_state) y_pred = kmeans.fit_predict(X) return X, y_true, y_pred ``` This question assesses the student\'s ability to: - Generate custom datasets using `scikit-learn` utilities. - Perform clustering using the `KMeans` algorithm. - Handle various parameters and ensure reproducibility. - Understand the output requirements and format.","solution":"import numpy as np from sklearn.datasets import make_blobs from sklearn.cluster import KMeans from typing import Tuple def cluster_blobs(n_samples: int, n_features: int, centers: int, cluster_std: float, random_state: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]: Generates a dataset using make_blobs and performs clustering using KMeans. Parameters: n_samples (int): Number of samples in the dataset. n_features (int): Number of features for each sample. centers (int): Number of centers to generate. cluster_std (float): The standard deviation of the clusters. random_state (int): Random seed for reproducibility. Returns: Tuple[np.ndarray, np.ndarray, np.ndarray]: The generated samples, true labels, and predicted labels. # Generate the dataset X, y_true = make_blobs(n_samples=n_samples, n_features=n_features, centers=centers, cluster_std=cluster_std, random_state=random_state) # Perform KMeans clustering kmeans = KMeans(n_clusters=centers, random_state=random_state) y_pred = kmeans.fit_predict(X) return X, y_true, y_pred"},{"question":"**Question: XML Data Processing with `xml.etree.ElementTree`** You are tasked with writing a Python function that processes an XML file representing a collection of books. Each book element has the following structure: ```xml <book> <title>Book Title</title> <author>Author Name</author> <genre>Genre</genre> <price>Price</price> <publish_date>Publish Date</publish_date> <description>Description of the book.</description> </book> ``` Your function should perform the following operations: 1. **Parse the XML file** to build an in-memory representation of the XML tree. 2. **Find and list all book titles** whose price is less than a given threshold. 3. **Count the number of books for each distinct genre.** 4. **Add a new book** to the collection with the provided details and save the modified XML back to a file. # Function Signature ```python import xml.etree.ElementTree as ET def process_books( xml_file: str, price_threshold: float, new_book_details: dict, output_file: str ) -> tuple: Process an XML file of books, find book titles below a price threshold, count books by genre, and add a new book to the XML. Arguments: xml_file -- the path to the XML file to process. price_threshold -- the price threshold to filter book titles. new_book_details -- a dictionary containing the details of the new book to add. { \\"title\\": str, \\"author\\": str, \\"genre\\": str, \\"price\\": float, \\"publish_date\\": str, \\"description\\": str } output_file -- the path to save the modified XML file. Returns: A tuple containing: - A list of book titles with price less than the given threshold. - A dictionary with counts of books by genre. pass ``` # Constraints - The XML file will always be well-formed. - `new_book_details` will always contain all the required fields with valid values. - The output file must retain the same XML structure as the input file. # Example Given an XML file `books.xml`: ```xml <catalog> <book> <title>Book A</title> <author>Author A</author> <genre>Fiction</genre> <price>15.5</price> <publish_date>2020-01-01</publish_date> <description>A description of Book A.</description> </book> <book> <title>Book B</title> <author>Author B</author> <genre>Non-Fiction</genre> <price>22.0</price> <publish_date>2019-11-03</publish_date> <description>A description of Book B.</description> </book> </catalog> ``` Calling: ```python new_book = { \\"title\\": \\"Book C\\", \\"author\\": \\"Author C\\", \\"genre\\": \\"Fiction\\", \\"price\\": 10.0, \\"publish_date\\": \\"2021-07-21\\", \\"description\\": \\"A description of Book C.\\" } titles, genre_counts = process_books(\'books.xml\', 20.0, new_book, \'updated_books.xml\') print(titles) # [\'Book A\', \'Book C\'] print(genre_counts) # {\'Fiction\': 2, \'Non-Fiction\': 1} ``` After execution, a new file `updated_books.xml` should be created with the added book details.","solution":"import xml.etree.ElementTree as ET from collections import defaultdict def process_books(xml_file: str, price_threshold: float, new_book_details: dict, output_file: str) -> tuple: Process an XML file of books, find book titles below a price threshold, count books by genre, and add a new book to the XML. Arguments: xml_file -- the path to the XML file to process. price_threshold -- the price threshold to filter book titles. new_book_details -- a dictionary containing the details of the new book to add. { \\"title\\": str, \\"author\\": str, \\"genre\\": str, \\"price\\": float, \\"publish_date\\": str, \\"description\\": str } output_file -- the path to save the modified XML file. Returns: A tuple containing: - A list of book titles with price less than the given threshold. - A dictionary with counts of books by genre. # Parse the XML file tree = ET.parse(xml_file) root = tree.getroot() # Initialize results titles_below_threshold = [] genre_counts = defaultdict(int) # Process each book in the catalog for book in root.findall(\'book\'): price = float(book.find(\'price\').text) title = book.find(\'title\').text genre = book.find(\'genre\').text if price < price_threshold: titles_below_threshold.append(title) genre_counts[genre] += 1 # Add the new book new_book = ET.Element(\'book\') for key, value in new_book_details.items(): child = ET.SubElement(new_book, key) child.text = str(value) root.append(new_book) # Update the counts and titles for the new book if new_book_details[\'price\'] < price_threshold: titles_below_threshold.append(new_book_details[\'title\']) genre_counts[new_book_details[\'genre\']] += 1 # Write the updated tree back to the output file tree.write(output_file) return (titles_below_threshold, dict(genre_counts))"},{"question":"Objective: Demonstrate the ability to use the `copyreg` module to define custom pickling functions for user-defined classes in Python. Question: You are given a class called `Student` that represents a student with a `name` and a `grade`. Your task is to: 1. Create a custom `pickle_student` function to serialize `Student` objects. 2. Register this custom function using the `copyreg` module. 3. Write a function `test_pickling` that demonstrates the pickling and unpickling process using your custom function. Class Definition: ```python class Student: def __init__(self, name, grade): self.name = name self.grade = grade ``` Custom Pickling Function: Your `pickle_student` function should: - Take a `Student` object as an argument. - Return a tuple `(constructor, (name, grade))` where `constructor` is the `Student` class itself and `(name, grade)` are the attributes of the student. Requirements: 1. Define the `Student` class as described above. 2. Implement the `pickle_student` function for serializing `Student` objects. 3. Use `copyreg.pickle` to register `pickle_student` for the `Student` class. 4. Implement a `test_pickling` function which: - Creates an instance of `Student`. - Uses `pickle.dumps` to serialize the instance. - Uses `pickle.loads` to deserialize the instance. - Prints the original and the deserialized objects to demonstrate that they are equivalent. Constraints: - You should not use any global variables except for the required imports. - Make sure your implementation handles invalid or unexpected input gracefully. Input and Output Formats: - No specific input format. - The `test_pickling` function should print the original `Student` object and the deserialized `Student` object. Example: ```python # Example usage: import copyreg import pickle class Student: def __init__(self, name, grade): self.name = name self.grade = grade def pickle_student(student): return Student, (student.name, student.grade) copyreg.pickle(Student, pickle_student) def test_pickling(): student = Student(\\"Alice\\", \\"A\\") serialized_student = pickle.dumps(student) deserialized_student = pickle.loads(serialized_student) print(\\"Original student:\\", student.__dict__) print(\\"Deserialized student:\\", deserialized_student.__dict__) # Run the test function test_pickling() ``` The expected output should print the attributes of both the original and the deserialized `Student` objects, showing that they are equivalent.","solution":"import copyreg import pickle class Student: def __init__(self, name, grade): self.name = name self.grade = grade def pickle_student(student): return Student, (student.name, student.grade) # Register the custom pickling function for the Student class copyreg.pickle(Student, pickle_student) def test_pickling(): student = Student(\\"Alice\\", \\"A\\") serialized_student = pickle.dumps(student) deserialized_student = pickle.loads(serialized_student) print(\\"Original student:\\", student.__dict__) print(\\"Deserialized student:\\", deserialized_student.__dict__) # Run the test function to verify the solution test_pickling()"},{"question":"# **Task** You are required to implement a small utility using the `dbm` module to store and manage user information in a key-value database. The user information will consist of a username as the key and an email address as the value. The database will support basic operations such as adding a new user, getting a user\'s email, and deleting a user. # **Requirements** 1. **Function Implementations** * `add_user(db_file, username, email)`: - Inputs: - `db_file` (str): The path to the database file. - `username` (str): The username to be added. It must be unique. - `email` (str): The email address for the username. - Output: None - Description: Adds the given `username` and `email` to the database. If the username already exists, it should raise a `ValueError`. * `get_email(db_file, username)`: - Inputs: - `db_file` (str): The path to the database file. - `username` (str): The username whose email address is to be retrieved. - Output: `email` (str) if the username exists, otherwise `None`. - Description: Retrieves and returns the email address associated with the given `username`. If the username does not exist, it returns `None`. * `delete_user(db_file, username)`: - Inputs: - `db_file` (str): The path to the database file. - `username` (str): The username to be deleted from the database. - Output: None - Description: Deletes the given `username` from the database. If the username does not exist, it should raise a `KeyError`. 2. **Constraints** - Username and email must be strings and converted to bytes before storing in the database. - The database should be opened using the `with` statement to ensure it is properly closed. - Handle any possible exceptions that could occur during database operations, such as file I/O errors. 3. **Performance Considerations** - Ensure the operations are efficient and handle exceptions gracefully. - Avoid loading the entire database into memory for operations. # **Example Usage** ```python # Database file path db_file = \'user_database\' # Adding users add_user(db_file, \'alice\', \'alice@example.com\') add_user(db_file, \'bob\', \'bob@example.com\') # Retrieving emails print(get_email(db_file, \'alice\')) # Output: \'alice@example.com\' print(get_email(db_file, \'charlie\')) # Output: None # Deleting a user delete_user(db_file, \'alice\') # Trying to get email for deleted user print(get_email(db_file, \'alice\')) # Output: None ``` # **Solution Template** ```python import dbm def add_user(db_file, username, email): with dbm.open(db_file, \'c\') as db: if username.encode() in db: raise ValueError(f\\"Username {username} already exists.\\") db[username.encode()] = email.encode() def get_email(db_file, username): with dbm.open(db_file, \'r\') as db: return db.get(username.encode()).decode() if username.encode() in db else None def delete_user(db_file, username): with dbm.open(db_file, \'w\') as db: if username.encode() not in db: raise KeyError(f\\"Username {username} does not exist.\\") del db[username.encode()] # Example usage if __name__ == \\"__main__\\": db_file = \'user_database\' add_user(db_file, \'alice\', \'alice@example.com\') add_user(db_file, \'bob\', \'bob@example.com\') print(get_email(db_file, \'alice\')) # Output: \'alice@example.com\' delete_user(db_file, \'alice\') print(get_email(db_file, \'alice\')) # Output: None ``` # **Notes** - Ensure that the database files are managed correctly, and always closed after use. - Make sure to test your solution with different scenarios and inputs.","solution":"import dbm def add_user(db_file, username, email): with dbm.open(db_file, \'c\') as db: if username.encode() in db: raise ValueError(f\\"Username {username} already exists.\\") db[username.encode()] = email.encode() def get_email(db_file, username): with dbm.open(db_file, \'r\') as db: return db.get(username.encode()).decode() if username.encode() in db else None def delete_user(db_file, username): with dbm.open(db_file, \'w\') as db: if username.encode() not in db: raise KeyError(f\\"Username {username} does not exist.\\") del db[username.encode()]"},{"question":"**Question: Implement a Type-Safe Event Dispatcher** You are required to implement a type-safe event dispatcher using Python\'s `typing` module. The dispatcher should allow registering event handlers for specific event types and dispatching events to the appropriate handlers. # Instructions: 1. **EventHandler and Event Classes**: - Define a base `Event` class with relevant attributes. Define some example derived event classes. - Define a `Callable` type alias `EventHandler` that represents functions which handle events. 2. **EventDispatcher Class**: - Implement a generic `EventDispatcher` class that maintains a registry of event handlers for different event types. - `EventDispatcher` should have methods to register handlers and dispatch events. - Ensure type-safety using `TypeVar` and `Generic` for the event types. 3. **Type Checking**: - Enforce that the event handlers registered for a specific event type should only receive events of that type. # Detailed Requirements: 1. **Event Class and Derived Events**: ```python from typing import TypedDict, Type, Callable class Event: Base class for all events. pass class UserRegistered(Event): user_id: int username: str class OrderPlaced(Event): order_id: int product_name: str quantity: int # Define more events as needed ``` 2. **EventHandler Type Alias**: ```python EventHandler = Callable[[Event], None] ``` 3. **EventDispatcher Class**: - **Define Generic Type Variable for Event**: ```python from typing import TypeVar, Generic, Dict, List E = TypeVar(\'E\', bound=Event) ``` - **Implement EventDispatcher**: ```python class EventDispatcher(Generic[E]): def __init__(self) -> None: self._handlers: Dict[Type[E], List[EventHandler]] = {} def register_handler(self, event_type: Type[E], handler: EventHandler) -> None: if event_type not in self._handlers: self._handlers[event_type] = [] self._handlers[event_type].append(handler) def dispatch(self, event: E) -> None: event_type = type(event) if event_type in self._handlers: for handler in self._handlers[event_type]: handler(event) ``` 4. **Example Usage**: - Register event handlers and dispatch events. - Ensure handlers only handle their respective event types using type checks. ```python def handle_user_registered(event: UserRegistered) -> None: print(f\\"User registered: {event.username} with ID {event.user_id}\\") def handle_order_placed(event: OrderPlaced) -> None: print(f\\"Order placed: {event.product_name} x {event.quantity}\\") dispatcher = EventDispatcher() dispatcher.register_handler(UserRegistered, handle_user_registered) dispatcher.register_handler(OrderPlaced, handle_order_placed) # Dispatch events dispatcher.dispatch(UserRegistered(user_id=1, username=\'JohnDoe\')) dispatcher.dispatch(OrderPlaced(order_id=42, product_name=\'Laptop\', quantity=3)) ``` # Constraints: - Utilize `TypeVar` and `Generic` to ensure type safety. - Ensure the dispatcher supports multiple handlers for the same event type. - The solution should not raise any type errors when checked with a static type checker like `mypy`. # Performance Requirements: - The registration and dispatch operations should each have an average time complexity of O(1) with respect to the number of event types and handlers. **Note**: You do not need to handle concurrency in this implementation.","solution":"from typing import Callable, Type, TypeVar, Generic, Dict, List class Event: Base class for all events. pass class UserRegistered(Event): def __init__(self, user_id: int, username: str) -> None: self.user_id = user_id self.username = username class OrderPlaced(Event): def __init__(self, order_id: int, product_name: str, quantity: int) -> None: self.order_id = order_id self.product_name = product_name self.quantity = quantity E = TypeVar(\'E\', bound=Event) EventHandler = Callable[[E], None] class EventDispatcher(Generic[E]): def __init__(self) -> None: self._handlers: Dict[Type[E], List[EventHandler[E]]] = {} def register_handler(self, event_type: Type[E], handler: EventHandler[E]) -> None: if event_type not in self._handlers: self._handlers[event_type] = [] self._handlers[event_type].append(handler) def dispatch(self, event: E) -> None: event_type = type(event) if event_type in self._handlers: for handler in self._handlers[event_type]: handler(event)"},{"question":"# Question: Implement an Ensemble Model Using Scikit-Learn **Objective:** You are tasked with creating an ensemble model using scikit-learn to improve the performance of predictive models. You will combine multiple diverse models (e.g., Decision Trees, Logistic Regression, K-Nearest Neighbors) and evaluate the ensemble\'s performance on a given dataset. **Problem:** Implement a Python function called `train_ensemble_model` that performs the following steps: 1. **Load the dataset:** Use the Breast Cancer dataset from scikit-learn. 2. **Data Preprocessing:** Perform necessary preprocessing steps to prepare the data for model training. 3. **Create Individual Models:** Instantiate three different models: a Decision Tree, a Logistic Regression, and a K-Nearest Neighbors. 4. **Create an Ensemble Model:** Combine the three models using a VotingClassifier. 5. **Train and Evaluate the Model:** - Split the data into training and testing sets. - Train the ensemble model on the training set. - Evaluate its performance on the testing set using accuracy as the metric. **Function Signature:** ```python def train_ensemble_model(): # You should not require any input parameters. pass ``` **Expected Output:** - The function should print the accuracy of the ensemble model on the testing set. **Constraints:** - You must use the VotingClassifier from scikit-learn to create the ensemble model. - Use the default parameters for the individual models unless specified otherwise. **Example:** Here is an example of expected output (values are indicative): ``` Accuracy of the ensemble model: 0.95 ``` **Notes:** - You are free to use any additional helper functions if necessary. - Ensure that you import all required modules at the beginning of your function. **Hints:** - Refer to the scikit-learn documentation for the Breast Cancer dataset and VotingClassifier. Good luck, and happy coding!","solution":"from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.tree import DecisionTreeClassifier from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.ensemble import VotingClassifier from sklearn.metrics import accuracy_score def train_ensemble_model(): # Load the dataset data = load_breast_cancer() X, y = data.data, data.target # Data Preprocessing scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Create individual models clf1 = DecisionTreeClassifier(random_state=42) clf2 = LogisticRegression(max_iter=10000, random_state=42) clf3 = KNeighborsClassifier() # Create an ensemble model using VotingClassifier ensemble_model = VotingClassifier(estimators=[ (\'dt\', clf1), (\'lr\', clf2), (\'knn\', clf3) ], voting=\'hard\') # Train the ensemble model ensemble_model.fit(X_train, y_train) # Evaluate the ensemble model y_pred = ensemble_model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy of the ensemble model: {accuracy:.2f}\\") train_ensemble_model()"},{"question":"**Objective:** Implement a custom command-line interface (CLI) using the `cmd` module in Python. **Scenario:** You are tasked with creating a simple CLI-based TODO manager. This CLI will allow users to manage their TODO list interactively. **Requirements:** 1. Implement a class `TodoCLI` that inherits from `cmd.Cmd`. 2. Add the following commands: - `add <task>`: Add a new task to the TODO list. - `list`: List all tasks. - `remove <index>`: Remove a task by its index in the TODO list. - `clear`: Remove all tasks. - `quit`: Exit the TODO manager. 3. Implement the following additional features: - Ensure commands are case-insensitive. - Provide command completion for the commands `add`, `list`, `remove`, `clear`, and `quit`. - Use hooks to log all commands entered by the user to a file named `command.log`. # Implementation Details **Input and Output formats should be as follows:** 1. Command: `add <task>` - Example: `add Buy groceries` - Output: `Task added: Buy groceries` 2. Command: `list` - Output: ``` 1. Buy groceries 2. Walk the dog 3. Finish homework ``` - If there are no tasks: `No tasks in the list.` 3. Command: `remove <index>` - Example: `remove 1` - Output: `Task removed: Buy groceries` - If an invalid index is provided: `Invalid task index.` 4. Command: `clear` - Output: `All tasks have been removed.` 5. Command: `quit` - Output: `Exiting TODO manager. Goodbye!` **Constraints:** - You are required to handle common edge cases such as invalid commands or parameters. - The TODO list should be kept in memory and should not persist after the program exits. **Performance Requirements:** - The `list` and `clear` commands should operate in O(1) time complexity. - The `add` and `remove` commands should operate efficiently with respect to the underlying data structure. ```python import cmd class TodoCLI(cmd.Cmd): intro = \'Welcome to the TODO manager. Type help or ? to list commands.n\' prompt = \'(TODO) \' file = None def __init__(self): super().__init__() self.tasks = [] def do_add(self, arg): \'Add a new task: ADD Buy groceries\' self.tasks.append(arg) print(f\'Task added: {arg}\') def do_list(self, arg): \'List all tasks: LIST\' if not self.tasks: print(\'No tasks in the list.\') else: for idx, task in enumerate(self.tasks, 1): print(f\'{idx}. {task}\') def do_remove(self, arg): \'Remove a task by its index: REMOVE 1\' try: index = int(arg) - 1 if 0 <= index < len(self.tasks): removed_task = self.tasks.pop(index) print(f\'Task removed: {removed_task}\') else: print(\'Invalid task index.\') except ValueError: print(\'Invalid task index.\') def do_clear(self, arg): \'Remove all tasks: CLEAR\' self.tasks.clear() print(\'All tasks have been removed.\') def do_quit(self, arg): \'Exit the TODO manager: QUIT\' print(\'Exiting TODO manager. Goodbye!\') return True def complete_add(self, text, line, begidx, endidx): return [task for task in self.tasks if task.startswith(text)] def complete_remove(self, text, line, begidx, endidx): return [str(i+1) for i in range(len(self.tasks)) if str(i+1).startswith(text)] def precmd(self, line): line = line.lower() with open(\'command.log\', \'a\') as log_file: log_file.write(line + \'n\') return line if __name__ == \'__main__\': TodoCLI().cmdloop() ``` **Note:** Before submitting, ensure your solution handles the requirements and edge cases effectively.","solution":"import cmd class TodoCLI(cmd.Cmd): intro = \'Welcome to the TODO manager. Type help or ? to list commands.n\' prompt = \'(TODO) \' file = None def __init__(self): super().__init__() self.tasks = [] def do_add(self, arg): \'Add a new task: add <task>\' if arg: self.tasks.append(arg) print(f\'Task added: {arg}\') else: print(\'Error: Please specify a task to add.\') def do_list(self, arg): \'List all tasks: list\' if not self.tasks: print(\'No tasks in the list.\') else: for idx, task in enumerate(self.tasks, 1): print(f\'{idx}. {task}\') def do_remove(self, arg): \'Remove a task by its index: remove <index>\' try: index = int(arg) - 1 if 0 <= index < len(self.tasks): removed_task = self.tasks.pop(index) print(f\'Task removed: {removed_task}\') else: print(\'Invalid task index.\') except ValueError: print(\'Invalid task index.\') def do_clear(self, arg): \'Remove all tasks: clear\' self.tasks.clear() print(\'All tasks have been removed.\') def do_quit(self, arg): \'Exit the TODO manager: quit\' print(\'Exiting TODO manager. Goodbye!\') return True def complete_add(self, text, line, begidx, endidx): return [task for task in self.tasks if task.startswith(text)] def complete_remove(self, text, line, begidx, endidx): return [str(i+1) for i in range(len(self.tasks)) if str(i+1).startswith(text)] def precmd(self, line): line = line.lower() with open(\'command.log\', \'a\') as log_file: log_file.write(line + \'n\') return line if __name__ == \'__main__\': TodoCLI().cmdloop()"},{"question":"# Token-based Expression Validation Using the `token` module, you need to implement a function that validates a given expression string. The function should do the following: 1. Identify and map each character in the expression to its corresponding token. 2. Ensure the expression contains only valid tokens. 3. Ensure the expression adheres to simplified arithmetic grammar (e.g., no consecutive operators, balanced parentheses). Function Signature ```python def validate_expression(expression: str) -> bool: pass ``` Input - `expression`: A string representing an arithmetic expression. The string will only contain the following characters: `0-9`, `+`, `-`, `*`, `/`, `(`, `)`, ` ` (space). Output - Returns `True` if the expression is valid, else returns `False`. Constraints - Operators must be between numbers. - Parentheses must be balanced. - Spaces should be ignored. - Examples of valid expressions: `\\"3 + 5\\"`, `\\"2 * (3 + 4)\\"`. - Examples of invalid expressions: `\\"3 ++ 5\\"`, `\\"2 * (3 +\\"`. Example ```python print(validate_expression(\\"3 + 5\\")) # Output: True print(validate_expression(\\"2 * (3 + 4)\\")) # Output: True print(validate_expression(\\"3 ++ 5\\")) # Output: False print(validate_expression(\\"2 * (3 +\\")) # Output: False ``` Notes 1. Use the `token` module to map characters to their respective tokens and validate them. 2. Make use of utility functions from the `token` module like `ISTERMINAL()`, `ISNONTERMINAL()`, and `ISEOF()` within your implementation where necessary. 3. You are not required to evaluate the expression, only validate its structure.","solution":"import re def validate_expression(expression: str) -> bool: Validates an arithmetic expression to ensure it contains only valid characters, parentheses are balanced, and no consecutive operators are present. # Remove spaces expression = expression.replace(\\" \\", \\"\\") # Check for invalid characters if not re.match(r\'^[d+-*/()]*\', expression): return False # Check parentheses are balanced stack = [] for char in expression: if char == \'(\': stack.append(char) elif char == \')\': if not stack: return False stack.pop() if stack: return False # Regex to check for consecutive operators if re.search(r\'++|--|**|//|+-|-+|+*|+*|+/|-/|*/|/*\', expression): return False # Regex to check for valid term after operators and parentheses if re.search(r\'[+-*/]{2,}\', expression): return False if expression.startswith((\'+\', \'-\', \'*\', \'/\')) or expression.endswith((\'+\', \'-\', \'*\', \'/\')): return False last_char = \'\' for i, c in enumerate(expression): if c in \'+-*/\' and last_char in \'+-*/\': return False last_char = c return True"},{"question":"Objective: To test the understanding and application of Python 3.10 built-in functions by combining them to solve a more complex problem. Problem Statement: You are provided with a list of tuples, where each tuple contains an employee\'s name and their respective salary. Your task is to implement a function that processes this list and generates a detailed report with the following summary information: 1. Sort the employees based on their salaries in descending order. 2. Identify the employee with the highest salary and the lowest salary. 3. Calculate the average salary. 4. List of employees who earn above the average salary. Implement the function `generate_salary_report(employees: list) -> dict` that accepts a list of employee tuples and returns a dictionary containing the required report. Function Signature: ```python def generate_salary_report(employees: list) -> dict: ``` Input: - `employees`: A list of tuples, where each tuple consists of a string (employee name) and an integer (salary). For example: ```python employees = [(\\"Alice\\", 70000), (\\"Bob\\", 80000), (\\"Charlie\\", 120000), (\\"David\\", 60000)] ``` Output: A dictionary with the following keys and corresponding values: 1. `sorted_employees`: A list of tuples representing employees sorted by salary in descending order. 2. `highest_salary`: A tuple representing the name and salary of the employee with the highest salary. 3. `lowest_salary`: A tuple representing the name and salary of the employee with the lowest salary. 4. `average_salary`: A floating-point number representing the average salary. 5. `above_average_earners`: A list of strings representing the names of employees who earn above the average salary. Constraints and Assumptions: - You can assume that the list `employees` is non-empty. - Salaries are positive integers. Example: Input: ```python employees = [(\\"Alice\\", 70000), (\\"Bob\\", 80000), (\\"Charlie\\", 120000), (\\"David\\", 60000)] ``` Output: ```python { \\"sorted_employees\\": [(\\"Charlie\\", 120000), (\\"Bob\\", 80000), (\\"Alice\\", 70000), (\\"David\\", 60000)], \\"highest_salary\\": (\\"Charlie\\", 120000), \\"lowest_salary\\": (\\"David\\", 60000), \\"average_salary\\": 82500.0, \\"above_average_earners\\": [\\"Charlie\\"] } ``` Notes: - Use built-in functions like `sorted()`, `max()`, `min()`, `sum()`, and list comprehensions where appropriate. - Ensure the function is well-documented and handles edge cases.","solution":"def generate_salary_report(employees: list) -> dict: Generates a detailed report of the employee salaries. Args: employees (list): A list of tuples where each tuple contains an employee name (str) and their salary (int). Returns: dict: A dictionary containing the report with the following keys: - \'sorted_employees\': List of tuples sorted by salary in descending order. - \'highest_salary\': Tuple with the name and salary of the highest paid employee. - \'lowest_salary\': Tuple with the name and salary of the lowest paid employee. - \'average_salary\': A floating point number representing the average salary. - \'above_average_earners\': List of names of employees earning above the average salary. # Sort employees based on salaries in descending order sorted_employees = sorted(employees, key=lambda x: x[1], reverse=True) # Employee with the highest salary highest_salary = sorted_employees[0] # Employee with the lowest salary lowest_salary = sorted_employees[-1] # Average salary total_salary = sum(employee[1] for employee in employees) average_salary = total_salary / len(employees) # List of employees earning above the average salary above_average_earners = [employee[0] for employee in sorted_employees if employee[1] > average_salary] return { \\"sorted_employees\\": sorted_employees, \\"highest_salary\\": highest_salary, \\"lowest_salary\\": lowest_salary, \\"average_salary\\": average_salary, \\"above_average_earners\\": above_average_earners }"},{"question":"# Question: Visualizing Penguin Species Data with Seaborn You are provided with the `penguins` dataset, which contains measurements for penguins of three different species. Using the `seaborn` library, your task is to implement a function `plot_penguin_data()` that visualizes different aspects of the dataset. The function should generate and display a series of subplots as follows: 1. **First Subplot (Jittered Plot)**: - Create a jittered dot plot of the penguin species vs. their body mass (g) using `so.Jitter`. - Use a moderate width for jitter (e.g., 0.5). 2. **Second Subplot (Flipper Length vs. Body Mass)**: - Create a scatter plot of `flipper_length_mm` vs. `body_mass_g`. - Apply jitter only to the x-axis with a jitter of 200 units. 3. **Third Subplot (Jittered Scatter Plot)**: - Create another scatter plot of `flipper_length_mm` vs. `body_mass_g`. - Apply jitter to both x and y axes with values of 200 units and 5 units, respectively. Additionally, ensure that each subplot has appropriate labels and titles for clarity. Input - None. Data is loaded directly within the function using seaborn\'s `load_dataset` function. Output - The function should display a grid of subplots. Function Signature ```python def plot_penguin_data(): pass ``` Example Usage ```python plot_penguin_data() ``` Constraints & Considerations - Utilize `seaborn`\'s object-oriented interface. - Ensure plot readability by adjusting axis labels, titles, and jitter widths as needed. - Make sure to handle the dataset loading within the function. Additional Resources: - Seaborn documentation: [Seaborn](https://seaborn.pydata.org/) - Penguins dataset available via `seaborn.load_dataset(\\"penguins\\")`","solution":"import seaborn as sns import matplotlib.pyplot as plt import seaborn.objects as so def plot_penguin_data(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the figure and subplots fig, axes = plt.subplots(3, 1, figsize=(10, 15), constrained_layout=True) # First subplot: Jittered dot plot of penguin species vs. body mass ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter(width=0.5)) .label(x=\\"Penguin Species\\", y=\\"Body Mass (g)\\", title=\\"Species vs. Body Mass with Jitter\\") .on(axes[0]) .plot() ) # Second subplot: Scatter plot with jitter only to the x-axis ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\") .add(so.Dot(), so.Jitter(x=200)) .label(x=\\"Flipper Length (mm)\\", y=\\"Body Mass (g)\\", title=\\"Flipper Length vs. Body Mass with X-axis Jitter\\") .on(axes[1]) .plot() ) # Third subplot: Scatter plot with jitter to both x and y axes ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\") .add(so.Dot(), so.Jitter(x=200, y=5)) .label(x=\\"Flipper Length (mm)\\", y=\\"Body Mass (g)\\", title=\\"Flipper Length vs. Body Mass with Jitter\\") .on(axes[2]) .plot() ) plt.show()"},{"question":"**Title**: Implementing and Managing Asynchronous Tasks with `asyncio` in Python **Objective**: To assess the student\'s ability to use the `asyncio` library in Python to manage multiple asynchronous tasks, interact with the event loop, and handle task results efficiently. **Problem Statement**: You are tasked with implementing an asynchronous function that concurrently processes a list of URLs by fetching their content. Your implementation should demonstrate the ability to manage the event loop, handle multiple asynchronous tasks, and process the results in an organized manner. **Requirements**: 1. Implement an asynchronous function `fetch_content(url: str) -> str` that simulates fetching content from a URL. You can use `asyncio.sleep()` to simulate network delay. 2. Implement the function `fetch_all_contents(urls: List[str], timeout: int) -> Dict[str, str]` which processes a list of URLs concurrently within the specified timeout. 3. Handle and return a dictionary mapping each URL to its fetched content or an error message if the operation was not completed in the specified timeout. **Function Specifications**: 1. **Function**: `fetch_content` - **Input**: - `url` (str): The URL to fetch content from. - **Output**: - `str`: Simulated content of the URL. - **Details**: - Simulate a network delay using `asyncio.sleep()` for random durations between 1 to 3 seconds. - Return a string `\\"Content of {url}\\"`. 2. **Function**: `fetch_all_contents` - **Input**: - `urls` (List[str]): A list of URLs to fetch content from. - `timeout` (int): The maximum time (in seconds) to wait for all tasks to complete. - **Output**: - `Dict[str, str]`: A dictionary with each URL as the key and its fetched content or an error message (\\"Timeout\\") as the value. - **Details**: - Use `asyncio.gather()` to run `fetch_content` concurrently for all URLs with the provided timeout. - If a task exceeds the timeout, record the URL with an error message `\\"Timeout\\"`. - Return the result dictionary. **Constraints**: - The length of `urls` will not exceed 100. - The value of `timeout` will be between 1 and 60 seconds. **Example**: ```python import asyncio from typing import List, Dict # Your implementation here async def main(): urls = [\\"http://example.com/page1\\", \\"http://example.com/page2\\", \\"http://example.com/page3\\"] timeout = 5 result = await fetch_all_contents(urls, timeout) print(result) # Run the example asyncio.run(main()) ``` **Expected Output**: Given the random nature of the simulated network delay, the output can vary. An example outcome may be: ```python { \\"http://example.com/page1\\": \\"Content of http://example.com/page1\\", \\"http://example.com/page2\\": \\"Content of http://example.com/page2\\", \\"http://example.com/page3\\": \\"Timeout\\" } ``` **Evaluation Criteria**: - Correct implementation of asynchronous patterns with `asyncio`. - Efficient handling of task scheduling and event loop management. - Proper handling of task timeouts and result aggregation. - Code clarity and adherence to Python best practices.","solution":"import asyncio import random from typing import List, Dict async def fetch_content(url: str) -> str: Simulates fetching content from a URL by introducing a random network delay. # Simulate network delay await asyncio.sleep(random.randint(1, 3)) return f\\"Content of {url}\\" async def fetch_all_contents(urls: List[str], timeout: int) -> Dict[str, str]: Processes a list of URLs concurrently within the specified timeout. Parameters: urls (List[str]): A list of URLs to fetch content from. timeout (int): The maximum time (in seconds) to wait for all tasks to complete. Returns: Dict[str, str]: A dictionary with each URL as the key and its fetched content or an error message (\\"Timeout\\") as the value. async def fetch_with_timeout(url): try: return await asyncio.wait_for(fetch_content(url), timeout=timeout) except asyncio.TimeoutError: return \\"Timeout\\" tasks = [fetch_with_timeout(url) for url in urls] results = await asyncio.gather(*tasks) return dict(zip(urls, results))"},{"question":"**Title: Solving Linear Systems and Matrix Decompositions with PyTorch** Objective: You are required to implement several functions to demonstrate your understanding of PyTorch\'s linear algebra capabilities, focusing on solving linear systems, and performing matrix decompositions and inverses. Problem Statement: Given a square matrix ( A ) (n x n) and a vector ( b ) (n), implement the following functionalities: 1. **LU Decomposition** - **Function signature**: `def lu_decomposition(A: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]` - **Input**: A matrix ( A ) of shape (n, n). - **Output**: A tuple containing the lower triangular matrix ( L ) and the upper triangular matrix ( U ) from the LU decomposition of ( A ). 2. **Solve a Linear System using LU Decomposition** - **Function signature**: `def solve_system_lu(A: torch.Tensor, b: torch.Tensor) -> torch.Tensor` - **Input**: A matrix ( A ) of shape (n, n) and a vector ( b ) of shape (n). - **Output**: Solution vector ( x ) of shape (n) for the system ( Ax = b ), solved using the LU decomposition. 3. **Calculate Matrix Inverse using QR Decomposition** - **Function signature**: `def inverse_qr(A: torch.Tensor) -> torch.Tensor` - **Input**: A matrix ( A ) of shape (n, n). - **Output**: The inverse of matrix ( A ), calculated using the QR decomposition. Constraints: - You may assume that ( A ) is always invertible. - Inputs will be valid PyTorch tensors. - You should not use the direct `torch.linalg.solve`, `torch.linalg.inv`, or similar solutions directly for solving linear systems or matrix inversion. Instead, use lower-level operations (e.g., `torch.linalg.qr`) to manually perform the required computations. Example Usage: ```python import torch from typing import Tuple # Example matrix and vector A = torch.tensor([[3.0, 2.0], [1.0, 4.0]]) b = torch.tensor([5.0, 6.0]) # LU Decomposition L, U = lu_decomposition(A) print(L, U) # Solve system Ax = b x = solve_system_lu(A, b) print(x) # Calculate inverse using QR decomposition A_inv = inverse_qr(A) print(A_inv) ``` **Expected Output:** ```plaintext # LU Decomposition matrices tensor([[3.0000, 0.0000], [0.3333, 3.3333]]) tensor([[1.0000, 0.6667], [0.0000, 1.0000]]) # Solution of system tensor([1.0000, 1.0000]) # Inverse of A tensor([[ 0.8000, -0.4000], [-0.2000, 0.6000]]) ``` # Additional Notes: - Ensure to validate the dimensions of input matrices and vectors. - Please handle any potential numerical instability carefully and document any assumptions or special cases in your code.","solution":"import torch from typing import Tuple def lu_decomposition(A: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]: Perform LU Decomposition of matrix A using simple Doolittle\'s method. Args: A (torch.Tensor): A square matrix of shape (n, n). Returns: Tuple[torch.Tensor, torch.Tensor]: Lower and Upper triangular matrices L and U. n = A.shape[0] L = torch.zeros_like(A) U = torch.zeros_like(A) for i in range(n): # Upper Triangular for k in range(i, n): sum = 0 for j in range(i): sum += (L[i, j] * U[j, k]) U[i, k] = A[i, k] - sum # Lower Triangular for k in range(i, n): if (i == k): L[i, i] = 1 else: sum = 0 for j in range(i): sum += (L[k, j] * U[j, i]) L[k, i] = (A[k, i] - sum) / U[i, i] return L, U def solve_system_lu(A: torch.Tensor, b: torch.Tensor) -> torch.Tensor: Solve the system of linear equations Ax = b using LU Decomposition. Args: A (torch.Tensor): A square matrix of shape (n, n). b (torch.Tensor): A vector of shape (n,). Returns: torch.Tensor: Solution vector x of shape (n,). L, U = lu_decomposition(A) n = A.shape[0] # Forward substitution to solve Ly = b y = torch.zeros(n) for i in range(n): y[i] = b[i] - torch.dot(L[i, :i], y[:i]) # Backward substitution to solve Ux = y x = torch.zeros(n) for i in range(n-1, -1, -1): x[i] = (y[i] - torch.dot(U[i, i+1:], x[i+1:])) / U[i, i] return x def inverse_qr(A: torch.Tensor) -> torch.Tensor: Calculate the inverse of matrix A using QR Decomposition. Args: A (torch.Tensor): A square matrix of shape (n, n). Returns: torch.Tensor: The inverse of matrix A. Q, R = torch.linalg.qr(A) n = A.shape[0] A_inv = torch.zeros_like(A) # Calculate the inverse using A_inv = R^(-1) Q^T for i in range(n): e = torch.zeros(n) e[i] = 1.0 d = torch.linalg.solve(R, Q.T @ e) A_inv[:, i] = d return A_inv"},{"question":"# Context Management System Using contextvars Problem Statement: You are tasked with creating a context management system in Python that demonstrates the use of `contextvars`. Your task is to implement the following functions: 1. **create_context_var(name, default_value=None):** - Creates a new `ContextVar` with the specified `name` and default `default_value`. - **Input:** - `name` (str): The name of the context variable. - `default_value` (Any, optional): The default value of the context variable. Defaults to `None`. - **Output:** - Returns the created `ContextVar`. 2. **set_context_var(context_var, value):** - Sets the specified `context_var` to the specified `value` in the current context. - **Input:** - `context_var` (`ContextVar`): The context variable to set. - `value` (Any): The value to set the context variable to. - **Output:** - Returns the token representing this change. 3. **get_context_var(context_var, default_value=None):** - Gets the value of the specified `context_var`. If the variable is not set, the `default_value` is returned. - **Input:** - `context_var` (`ContextVar`): The context variable to get. - `default_value` (Any, optional): The default value to return if the context variable is not set. Defaults to `None`. - **Output:** - Returns the value of the context variable or the default value if the variable is not set. 4. **reset_context_var(context_var, token):** - Resets the `context_var` to its state before the specified `token` was set. - **Input:** - `context_var` (`ContextVar`): The context variable to reset. - `token` (`Token`): The token representing the change to reset. - **Output:** - None 5. **context_scope():** - A context manager that creates a new context for a block of code. - **Example Usage:** ```python with context_scope(): cv = create_context_var(\'my_var\', \'default\') token = set_context_var(cv, \'new_value\') print(get_context_var(cv)) # Output: new_value ``` Constraints: - `name` is a non-empty string. - `default_value` and `value` can be any valid Python object. Example: ```python cv = create_context_var(\'example_var\', \'default_value\') print(get_context_var(cv)) # Output: default_value token = set_context_var(cv, \'new_value\') print(get_context_var(cv)) # Output: new_value reset_context_var(cv, token) print(get_context_var(cv)) # Output: default_value ``` Notes: - The functions should leverage the `contextvars` Python module. - Ensure proper error handling where necessary. Implement the functions as specified and demonstrate their usage through a few examples.","solution":"from contextvars import ContextVar, Token from contextlib import contextmanager def create_context_var(name, default_value=None): Creates a new ContextVar with the specified name and default value. return ContextVar(name, default=default_value) def set_context_var(context_var, value): Sets the specified context_var to the specified value in the current context. Returns the token representing this change. return context_var.set(value) def get_context_var(context_var, default_value=None): Gets the value of the specified context_var. If the variable is not set, the default_value is returned. try: return context_var.get() except LookupError: return default_value def reset_context_var(context_var, token): Resets the context_var to its state before the specified token was set. context_var.reset(token) @contextmanager def context_scope(): A context manager that creates a new context for a block of code. try: yield finally: pass"},{"question":"# Problem: Implementing and Verifying Numerically Stable Linear Algebra Operations with PyTorch In this assignment, you are required to implement functions that perform linear algebra operations using PyTorch and demonstrate the difference in results when using different floating-point precisions (float32, float64, FP16, BF16). You will be working with matrix multiplication and Singular Value Decomposition (SVD) operations. Additionally, you will compare the numerical stability and accuracy across different precisions. Requirements You need to implement the following functions: 1. **matmul_and_svd(matrix_a, matrix_b, precision)**: - **Input**: - `matrix_a` and `matrix_b`: Two matrices which will be multiplied. They will always be 2D tensors of compatible dimensions for matrix multiplication. - `precision`: A string specifying the desired floating-point precision. It can take values `float32`, `float64`, `FP16`, `BF16`. - **Output**: - `result`: A dictionary containing: - `product`: The result of the matrix multiplication. - `u, s, v`: The matrices resulting from SVD decomposition of the matrix multiplication result. 2. **compare_precisions(matrix_a, matrix_b)**: - **Input**: - `matrix_a`: A 2D tensor. - `matrix_b`: A 2D tensor of compatible dimensions for matrix multiplication. - **Output**: - `comparison`: A dictionary where the key is the precision type (`float32`, `float64`, `FP16`, `BF16`) and the value is a tuple containing: - `product`: The result of the matrix multiplication for the corresponding precision. - `svd_difference`: The L2 norm difference between the `u, s, v` matrices of the current precision compared to the `float64` precision. Constraints - You must use PyTorch for all tensor and linear algebra operations. - Ensure your functions handle potential numerical instability or precision issues. - If a specific precision is not supported (such as BF16 not available on certain hardware), the function should raise an appropriate error. Example Assuming `matrix_a` of shape (3, 2) and `matrix_b` of shape (2, 3): ```python import torch matrix_a = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) matrix_b = torch.tensor([[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]) # Using the matmul_and_svd function result = matmul_and_svd(matrix_a, matrix_b, \'float32\') print(result[\'product\']) print(result[\'u\'], result[\'s\'], result[\'v\']) # Using the compare_precisions function comparison = compare_precisions(matrix_a, matrix_b) for precision, values in comparison.items(): print(f\\"Precision: {precision}\\") print(f\\"Product: {values[0]}\\") print(f\\"SVD Difference from float64: {values[1]}\\") ``` # Note - Be cautious of numerical instabilities and handle any exceptions that may arise. - Document your code to explain the numerical precision and stability principles you are applying.","solution":"import torch def matmul_and_svd(matrix_a, matrix_b, precision): Performs matrix multiplication and SVD decomposition with specified precision. Parameters: - matrix_a: torch.Tensor - matrix_b: torch.Tensor - precision: str Returns: - dict: containing \'product\', \'u\', \'s\', \'v\' if precision == \'float32\': matrix_a = matrix_a.to(torch.float32) matrix_b = matrix_b.to(torch.float32) elif precision == \'float64\': matrix_a = matrix_a.to(torch.float64) matrix_b = matrix_b.to(torch.float64) elif precision == \'FP16\': matrix_a = matrix_a.to(torch.float16) matrix_b = matrix_b.to(torch.float16) elif precision == \'BF16\': if not torch.cuda.is_available() or torch.cuda.get_device_capability(0)[0] < 8.0: raise RuntimeError(\'BF16 not supported on this hardware.\') matrix_a = matrix_a.to(torch.bfloat16) matrix_b = matrix_b.to(torch.bfloat16) else: raise ValueError(\\"Invalid precision type. Must be \'float32\', \'float64\', \'FP16\', or \'BF16\'.\\") product = torch.matmul(matrix_a, matrix_b) u, s, v = torch.svd(product) return {\'product\': product, \'u\': u, \'s\': s, \'v\': v} def compare_precisions(matrix_a, matrix_b): Compares the numerical accuracy of matrix multiplication and SVD across different precisions. Parameters: - matrix_a: torch.Tensor - matrix_b: torch.Tensor Returns: - dict: containing precision as key and tuple (product, svd_difference) as value precisions = [\'float32\', \'float64\', \'FP16\', \'BF16\'] comparison = {} results_64 = matmul_and_svd(matrix_a, matrix_b, \'float64\') u_64, s_64, v_64 = results_64[\'u\'], results_64[\'s\'], results_64[\'v\'] for precision in precisions: try: results = matmul_and_svd(matrix_a, matrix_b, precision) product = results[\'product\'] u, s, v = results[\'u\'], results[\'s\'], results[\'v\'] u_diff = torch.norm(u - u_64).item() s_diff = torch.norm(s - s_64).item() v_diff = torch.norm(v - v_64).item() svd_difference = (u_diff, s_diff, v_diff) comparison[precision] = (product, svd_difference) except RuntimeError as e: comparison[precision] = str(e) return comparison"},{"question":"# PyTorch MPS Memory Management and Profiling Task Objective Implement a PyTorch-based training loop for a simple neural network on an MPS device. The task is designed to assess your understanding of memory management, profiling, and handling random states in the context of Apple\'s MPS backend in PyTorch. Task 1. **Setup MPS Device**: Write a function `setup_mps_device` that returns an MPS device if available and sets the seed for random number generation. Use a given seed value for reproducibility. 2. **Memory Management**: Write functions to: - Query and return the current allocated memory and the recommended max memory for the MPS device. - Empty the cache memory of the MPS device when required. 3. **Train a Neural Network**: Implement a convolutional neural network and write a training loop that: - Runs on the MPS device. - Utilizes the MPS profiler to measure and output the time taken for each epoch. - Queries memory usage after each epoch. Input and Output 1. **Function `setup_mps_device(seed: int) -> torch.device`**: - **Input**: An integer seed. - **Output**: A torch.device object pointing to the MPS device if available; otherwise, raise an appropriate exception. 2. **Function `query_memory() -> Tuple[int, int]`**: - **Output**: Returns the current allocated memory and the recommended max memory as a tuple of integers. 3. **Function `train_network(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, epochs: int, device: torch.device) -> None`**: - **Input**: A neural network model, a data loader for input data, integer number of epochs, and a torch.device. - **Output**: None, but should print the time taken for each epoch and memory usage. Constraints - Ensure the code handles cases where MPS devices are not available. - Implement error handling for memory allocation failures. - The seed must ensure reproducibility in the random number generation for weights initialization. Example Usage ```python seed = 42 device = setup_mps_device(seed) # Define a simple CNN model class SimpleCNN(torch.nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=3) self.fc1 = torch.nn.Linear(32 * 26 * 26, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = x.view(-1, 32 * 26 * 26) x = self.fc1(x) return x model = SimpleCNN().to(device) # Define a dummy dataloader dataloader = torch.utils.data.DataLoader([(torch.randn(1, 28, 28), torch.tensor(0))]*100, batch_size=10) train_network(model, dataloader, 5, device) ``` Requirements - Use the `torch.mps` module functions for initialization, memory management, and profiling. - Ensure all outputs are printed in a clear and concise manner.","solution":"import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F import torch.utils.data as data import time def setup_mps_device(seed: int) -> torch.device: Sets up the MPS device if available and sets the seed for reproducibility. Args: seed (int): The seed for random number generation. Returns: torch.device: The MPS device object. Raises: RuntimeError: If MPS device is not available. if not torch.has_mps: raise RuntimeError(\\"MPS device is not available.\\") torch.manual_seed(seed) device = torch.device(\\"mps\\") return device def query_memory() -> tuple: Queries and returns the current allocated memory and the recommended max memory for the MPS device. Returns: tuple: Current allocated memory and recommended max memory (in bytes). if not torch.has_mps: raise RuntimeError(\\"MPS device is not available.\\") current_memory = torch.mps.current_allocated_memory() max_memory = torch.mps.max_memory_allocated() return current_memory, max_memory def empty_cache(): Empties the cache memory of the MPS device. if not torch.has_mps: raise RuntimeError(\\"MPS device is not available.\\") torch.mps.empty_cache() def train_network(model: nn.Module, dataloader: data.DataLoader, epochs: int, device: torch.device) -> None: Trains the neural network on the MPS device, with profiling and memory usage queries. Args: model (nn.Module): The neural network model. dataloader (data.DataLoader): The data loader for input data. epochs (int): The number of epochs to train. device (torch.device): The device to use for training. model.to(device) criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.001) for epoch in range(epochs): model.train() start_time = time.time() for inputs, targets in dataloader: inputs, targets = inputs.to(device), targets.to(device) optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() elapsed_time = time.time() - start_time current_memory, max_memory = query_memory() print(f\\"Epoch {epoch + 1}/{epochs}\\") print(f\\"Time Taken: {elapsed_time:.2f} seconds\\") print(f\\"Current Memory Usage: {current_memory} bytes\\") print(f\\"Max Memory Usage: {max_memory} bytes\\") # Optionally empty the cache empty_cache()"},{"question":"Working with `errno` Module in Python Objective: The purpose of this question is to assess your understanding of the `errno` module in Python and your ability to handle system error codes and corresponding exceptions. Problem Statement Write a Python function `get_error_details(error_code: int) -> str` that takes an integer `error_code` as input and returns a formatted string that includes the error name and the corresponding error message. Input: - An integer `error_code` that represents a system error code. Output: - A string in the format: `\\"Error Name: {error_name}, Error Message: {error_message}\\"`. If the `error_code` does not exist in the `errno.errorcode` mapping, return the string: `\\"Unknown error code\\"`. Requirements: 1. Use the `errno` module to get the error name and corresponding error message. 2. Handle cases where the `error_code` does not exist in the `errno.errorcode` dictionary. 3. Make use of the `os.strerror()` function to get the error message corresponding to the `error_code`. Examples: Example 1: ``` Input: 2 Output: \\"Error Name: ENOENT, Error Message: No such file or directory\\" ``` Example 2: ``` Input: 13 Output: \\"Error Name: EACCES, Error Message: Permission denied\\" ``` Example 3: ``` Input: 9999 Output: \\"Unknown error code\\" ``` Constraints: - Assume the system environment where this code will run supports standard errno symbols as described. Function Signature: ```python def get_error_details(error_code: int) -> str: pass ``` # Notes: - Refer to the `errno` module documentation for details on error codes and corresponding errors. - Ensure your function tests for the existence of the `error_code` in the `errno.errorcode` dictionary. - Utilize Python\'s built-in functionality to retrieve error messages when available.","solution":"import errno import os def get_error_details(error_code: int) -> str: Returns a formatted string that includes the error name and the corresponding error message given an integer error_code. If the error code does not exist, returns \\"Unknown error code\\". if error_code in errno.errorcode: error_name = errno.errorcode[error_code] error_message = os.strerror(error_code) return f\\"Error Name: {error_name}, Error Message: {error_message}\\" else: return \\"Unknown error code\\""},{"question":"Question: Implementing Advanced Garbage Collection Control and Analysis You are tasked with creating a utility in Python that leverages the `gc` module to provide advanced control over garbage collection and analysis of object memory usage. This utility should expose functionalities to enable/disable garbage collection, perform manual collections, and retrieve detailed statistics on object memory usage. # Requirements: 1. **Enable/Disable Garbage Collection**: - Create functions `enable_gc()` and `disable_gc()` to enable and disable garbage collection respectively. 2. **Check if Garbage Collection is Enabled**: - Create a function `is_gc_enabled()` that returns `True` if garbage collection is enabled, otherwise `False`. 3. **Manual Garbage Collection**: - Create a function `collect_garbage(generation=2)` that performs garbage collection for a specified generation and returns the number of unreachable objects found. 4. **Get Garbage Collection Statistics**: - Create a function `get_gc_stats()` that returns the garbage collection statistics in a readable dictionary format. 5. **Get Objects Currently Tracked by Garbage Collector**: - Create a function `get_tracked_objects(generation=None)` that returns all objects currently tracked by the garbage collector, or objects in a specified generation if provided. 6. **Detailed Memory Usage Report**: - Create a function `memory_usage_report()` that returns a detailed report of memory usage, including: - Number of objects in each generation. - Number of objects collected and uncollectable in each generation. - Total number of objects tracked by the garbage collector. - Number of unreachable objects found during the latest collection. # Constraints: - You should use the `gc` module functions to implement the above requirements. - Ensure that the output is formatted clearly and is easily readable. - Your solution should be efficient and avoid unnecessary computations. # Function Signatures: ```python def enable_gc() -> None: pass def disable_gc() -> None: pass def is_gc_enabled() -> bool: pass def collect_garbage(generation: int = 2) -> int: pass def get_gc_stats() -> dict: pass def get_tracked_objects(generation: int = None) -> list: pass def memory_usage_report() -> dict: pass ``` # Example Usage: ```python # Enable garbage collection enable_gc() # Perform manual collection unreachable_count = collect_garbage() print(f\\"Unreachable objects count: {unreachable_count}\\") # Get garbage collection statistics stats = get_gc_stats() print(f\\"Garbage Collection Statistics: {stats}\\") # Get memory usage report report = memory_usage_report() print(f\\"Memory Usage Report: {report}\\") ``` # Note: Use appropriate error handling and input validation where necessary to ensure robust and reliable functionality.","solution":"import gc def enable_gc() -> None: Enables garbage collection. gc.enable() def disable_gc() -> None: Disables garbage collection. gc.disable() def is_gc_enabled() -> bool: Returns True if garbage collection is enabled, otherwise False. return gc.isenabled() def collect_garbage(generation: int = 2) -> int: Performs garbage collection for the specified generation. Returns the number of unreachable objects found. return gc.collect(generation) def get_gc_stats() -> dict: Returns the garbage collection statistics in a readable dictionary format. stats = gc.get_stats() formatted_stats = { f\'generation_{i}\': { \'collect\': s[\'collections\'], \'collected\': s[\'collected\'], \'uncollectable\': s[\'uncollectable\'] } for i, s in enumerate(stats) } return formatted_stats def get_tracked_objects(generation: int = None) -> list: Returns all objects currently tracked by the garbage collector. If a generation is specified, returns objects in that generation. if generation is not None: return gc.get_objects() if generation == 0 else [obj for obj in gc.get_objects() if gc.get_referrers(obj)] else: return gc.get_objects() def memory_usage_report() -> dict: Returns a detailed report of memory usage. num_objects_per_gen = [gc.get_count()[i] for i in range(gc.get_stats().__len__())] stats = get_gc_stats() unreachable_count = sum([collect_garbage(i) for i in range(gc.get_stats().__len__())]) tracked_objects = len(get_tracked_objects()) return { \'objects_per_generation\': num_objects_per_gen, \'gc_stats\': stats, \'total_tracked_objects\': tracked_objects, \'total_unreachable_objects\': unreachable_count }"},{"question":"# File Descriptor Management and Custom Readline Implementation **Objective**: Implement a utility to manage file descriptors and a custom readline method in Python, leveraging your understanding of file operations and custom hook usage. Part 1: File Descriptor Utility You are to implement a function `fd_to_pyfile(fd, mode=\'r\', buffering=-1, encoding=None, errors=None, newline=None, closefd=True)` that: - Takes a file descriptor `fd` and optional parameters similar to `io.open()`. - Uses the given file descriptor to return a Python file object. Part 2: Custom Readline Implement a class `CustomFileReader` with the following methods: 1. `__init__(self, file_obj)`: Initialize with a Python file object. 2. `read_line(self, n=-1)`: Reads a line from the file object: - If `n` is `0`, reads exactly one line. - If `n` is greater than `0`, reads up to `n` bytes, potentially returning a partial line. - If `n` is less than `0`, reads one line regardless of length, raising EOFError if EOF is encountered immediately. Constraints 1. Use the `io` module or `os` module where needed. 2. Ensure proper error handling and resource management. 3. Do not import new modules within hook functions. # Example Usage ```python import os # Assuming the file \\"example.txt\\" exists and is readable fd = os.open(\\"example.txt\\", os.O_RDONLY) # Part 1: Convert file descriptor to Python file object file_obj = fd_to_pyfile(fd) # Part 2: Custom FileReader usage reader = CustomFileReader(file_obj) line = reader.read_line() print(line) # Close the file using file object method file_obj.close() ``` # Expected Output If `example.txt` contains: ``` Hello, World! This is a file. ``` The output of the above example should be: ``` Hello, World! ``` Notes - Ensure your solution handles edge cases such as an invalid file descriptor, incorrect file modes, or immediate EOF conditions. - Focus on demonstrating practical use of file descriptor manipulation and custom readline behavior in Python.","solution":"import io import os def fd_to_pyfile(fd, mode=\'r\', buffering=-1, encoding=None, errors=None, newline=None, closefd=True): Given a file descriptor, return a Python file object. return io.open(fd, mode=mode, buffering=buffering, encoding=encoding, errors=errors, newline=newline, closefd=closefd) class CustomFileReader: def __init__(self, file_obj): self.file_obj = file_obj def read_line(self, n=-1): Reads a line from the file object. Args: - n: number of bytes to read (default is -1). - If n==0, reads exactly one line. - If n>0, reads up to n bytes. - If n<0, reads one line regardless of length. Returns: - A line from the file as per the specified n. if n == 0: return self.file_obj.readline() elif n > 0: chunk = self.file_obj.read(n) newline_idx = chunk.find(\'n\') if newline_idx == -1: return chunk else: excess = chunk[newline_idx + 1:] self.file_obj.seek(self.file_obj.tell() - len(excess)) return chunk[:newline_idx + 1] else: line = self.file_obj.readline() if line == \'\': raise EOFError(\\"End of file reached\\") return line"},{"question":"Residual Analysis with Seaborn You are given the `tips` dataset from the seaborn library. Your task is to analyze the residuals of a regression model predicting the total bill (`total_bill`) amount from the variables `tip` and `size`. # Task Details: 1. Load the `tips` dataset using seaborn. 2. Create a new column `tip_percentage` which represents the tip as a percentage of the total bill. 3. Plot the residuals for the regression model with `total_bill` as the dependent variable and `tip_percentage` as the independent variable. 4. Create another plot removing higher-order trends of order 2 to check if residuals stabilize. 5. Finally, add a LOWESS curve to the residual plot for `total_bill` vs `tip_percentage` to reveal any hidden structure. # Constraints: - You should use the seaborn library. - Ensure the residual plots are properly labeled and stylistically clear. - You should also include comments and docstrings explaining your code and approach. # Expected Output: A properly labeled residual plot for each of the above tasks and an explanation of any patterns or trends you observe in the residuals. # Sample Code Structure: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create derived column: tip_percentage tips[\'tip_percentage\'] = (tips[\'tip\'] / tips[\'total_bill\']) * 100 # Residual plot with total_bill as y and tip_percentage as x sns.residplot(data=tips, x=\\"tip_percentage\\", y=\\"total_bill\\") plt.title(\\"Residual plot of total_bill vs tip_percentage\\") plt.show() # Residual plot with higher-order trend removed sns.residplot(data=tips, x=\\"tip_percentage\\", y=\\"total_bill\\", order=2) plt.title(\\"Residual plot of total_bill vs tip_percentage with higher-order trend removed\\") plt.show() # Residual plot with LOWESS curve sns.residplot(data=tips, x=\\"tip_percentage\\", y=\\"total_bill\\", lowess=True, line_kws={\\"color\\":\\"r\\"}) plt.title(\\"Residual plot of total_bill vs tip_percentage with LOWESS curve\\") plt.show() ``` Analyze and describe any patterns observed in the residuals across the plots and discuss possible reasons for those patterns.","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_residuals(): Load the \'tips\' dataset, create a tip percentage column, and plot residuals for a regression model with total_bill as the dependent variable and tip_percentage as the independent variable. # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create derived column: tip_percentage tips[\'tip_percentage\'] = (tips[\'tip\'] / tips[\'total_bill\']) * 100 # Residual plot with total_bill as y and tip_percentage as x sns.residplot(data=tips, x=\\"tip_percentage\\", y=\\"total_bill\\") plt.title(\\"Residual plot of total_bill vs tip_percentage\\") plt.xlabel(\\"Tip Percentage\\") plt.ylabel(\\"Residuals\\") plt.show() # Residual plot with higher-order trend removed sns.residplot(data=tips, x=\\"tip_percentage\\", y=\\"total_bill\\", order=2) plt.title(\\"Residual plot of total_bill vs tip_percentage with higher-order trend removed\\") plt.xlabel(\\"Tip Percentage\\") plt.ylabel(\\"Residuals\\") plt.show() # Residual plot with LOWESS curve sns.residplot(data=tips, x=\\"tip_percentage\\", y=\\"total_bill\\", lowess=True, line_kws={\\"color\\":\\"r\\"}) plt.title(\\"Residual plot of total_bill vs tip_percentage with LOWESS curve\\") plt.xlabel(\\"Tip Percentage\\") plt.ylabel(\\"Residuals\\") plt.show()"},{"question":"# Advanced Typing in Python You are tasked with implementing a function that simulates a banking system using advanced Python type hints. The banking system has: - Clients with a unique ID, name, and account balance. - Accounts with several operations: deposit, withdraw, and view balance. - A history of transactions. Your goal is to create a function `create_bank_db` which will return a `Callable` that can handle various banking operations for multiple clients. Requirements 1. Create a `TypedDict` for `Client` with fields `id`, `name`, and `balance`, where `balance` can only be a float. 2. Using `NewType`, define `ClientID` to be a distinct type based on `int`. 3. Define a type alias `Transaction` which is a `Tuple` of `ClientID`, a string (`\'deposit\' | \'withdraw\'`), and a float (amount). 4. Define another type alias `ClientDB` which is a dictionary with `ClientID` as keys and `Client` as values. 5. Implement `create_bank_db` which initializes an empty `ClientDB` and returns a `Callable` that accepts a parameter `operation` of type `Literal[\'add_client\', \'perform_transaction\', \'view_balance\']`. This returned callable should: - For `\'add_client\'`: Accept `ClientID` and `Client` arguments, and add the client to `ClientDB`. - For `\'perform_transaction\'`: Accept `Transaction` arguments, and update client balance accordingly in `ClientDB`. Record the transaction in a list of transactions. - For `\'view_balance\'`: Accept `ClientID` argument and return the client’s current balance. Detailed Function Signature ```python from typing import TypedDict, NewType, Tuple, Dict, Callable, Literal # Create your new types here ClientID = NewType(\'ClientID\', int) Transaction = Tuple[ClientID, Literal[\'deposit\', \'withdraw\'], float] ClientDB = Dict[ClientID, Client] class Client(TypedDict): id: ClientID name: str balance: float def create_bank_db() -> Callable[[Literal[\'add_client\', \'perform_transaction\', \'view_balance\'], ...], any]: # Initialize client database client_db: ClientDB = {} transactions: list[Transaction] = [] def bank_operation(operation: Literal[\'add_client\', \'perform_transaction\', \'view_balance\'], *args): if operation == \'add_client\': client_id: ClientID = args[0] client: Client = args[1] client_db[client_id] = client elif operation == \'perform_transaction\': client_id, action, amount = args[0] if client_id in client_db: if action == \'deposit\': client_db[client_id][\'balance\'] += amount elif action == \'withdraw\' and client_db[client_id][\'balance\'] >= amount: client_db[client_id][\'balance\'] -= amount transactions.append((client_id, action, amount)) elif operation == \'view_balance\': client_id: ClientID = args[0] if client_id in client_db: return client_db[client_id][\'balance\'] else: raise ValueError(\\"Invalid Operation\\") return bank_operation # Example usage: bank_db = create_bank_db() bank_db(\'add_client\', ClientID(1), {\'id\': ClientID(1), \'name\': \'John Doe\', \'balance\': 100.0}) bank_db(\'perform_transaction\', (ClientID(1), \'deposit\', 50.0)) balance = bank_db(\'view_balance\', ClientID(1)) # Should return 150.0 ``` Implement the `create_bank_db` function according to the requirements and ensure correct type hints are used throughout the process. Constraints - You must use type hints accurately. - Ensure that the balance cannot become negative through a withdrawal operation. - Handle invalid operations gracefully.","solution":"from typing import TypedDict, NewType, Tuple, Dict, Callable, Literal # Create your new types here ClientID = NewType(\'ClientID\', int) Transaction = Tuple[ClientID, Literal[\'deposit\', \'withdraw\'], float] ClientDB = Dict[ClientID, \'Client\'] class Client(TypedDict): id: ClientID name: str balance: float def create_bank_db() -> Callable[[Literal[\'add_client\', \'perform_transaction\', \'view_balance\'], ...], any]: # Initialize client database client_db: ClientDB = {} transactions: list[Transaction] = [] def bank_operation(operation: Literal[\'add_client\', \'perform_transaction\', \'view_balance\'], *args): if operation == \'add_client\': client_id: ClientID = args[0] client: Client = args[1] client_db[client_id] = client elif operation == \'perform_transaction\': transaction: Transaction = args[0] client_id, action, amount = transaction if client_id in client_db: if action == \'deposit\': client_db[client_id][\'balance\'] += amount elif action == \'withdraw\' and client_db[client_id][\'balance\'] >= amount: client_db[client_id][\'balance\'] -= amount transactions.append(transaction) elif operation == \'view_balance\': client_id: ClientID = args[0] if client_id in client_db: return client_db[client_id][\'balance\'] else: raise ValueError(\\"Invalid Operation\\") return bank_operation # Example usage: bank_db = create_bank_db() bank_db(\'add_client\', ClientID(1), {\'id\': ClientID(1), \'name\': \'John Doe\', \'balance\': 100.0}) bank_db(\'perform_transaction\', (ClientID(1), \'deposit\', 50.0)) balance = bank_db(\'view_balance\', ClientID(1)) # Should return 150.0 print(balance)"},{"question":"# Python Coding Assessment Question **Objective**: Utilize the `operator` module to perform complex data processing tasks. Task Implement a function `process_data(operations, data)` that applies a series of operations to a base dataset. Inputs - `operations`: A list of tuples where each tuple represents an operation. The first element of the tuple is a string representing the operation name, and the following elements are the arguments for that operation. Example of operations: ```python [ (\'add\', 2), # Add 2 to each element (\'mul\', 3), # Multiply each element by 3 (\'getitem\', slice(1, 4)), # Get elements from index 1 to 3 (\'setitem\', 1, 15), # Set element at index 1 to 15 ] ``` - `data`: Initial dataset, a list of integers. Output - The transformed dataset after applying all specified operations in sequence. Constraints - The operations will be valid and correspond to functions in the `operator` module. - The dataset will always be a list of integers. - You need to handle slicing, adding, multiplying, indexing, and setting items as operations. # Example ```python from operator import add, mul, getitem, setitem def process_data(operations, data): for operation in operations: op_name = operation[0] if op_name == \'add\': data = list(map(lambda x: add(x, operation[1]), data)) elif op_name == \'mul\': data = list(map(lambda x: mul(x, operation[1]), data)) elif op_name == \'getitem\': data = getitem(data, operation[1]) elif op_name == \'setitem\': setitem(data, operation[1], operation[2]) return data # Test operations = [(\'add\', 2), (\'mul\', 3), (\'getitem\', slice(1, 4)), (\'setitem\', 1, 15)] data = [1, 2, 3, 4, 5] print(process_data(operations, data)) # Output: [12, 15, 18] ``` In this function: - The `process_data` function takes a list of operations and a list of data. - Each operation is applied to the data in sequence. - The resulting data after all operations is returned. **Note**: Be mindful of the performance, especially when handling large datasets and complex sequences of operations.","solution":"from operator import add, mul, getitem, setitem def process_data(operations, data): for operation in operations: op_name = operation[0] if op_name == \'add\': data = list(map(lambda x: add(x, operation[1]), data)) elif op_name == \'mul\': data = list(map(lambda x: mul(x, operation[1]), data)) elif op_name == \'getitem\': data = getitem(data, operation[1]) elif op_name == \'setitem\': setitem(data, operation[1], operation[2]) return data"},{"question":"Objective: To assess the understanding of Abstract Base Classes (ABCs) in the `collections.abc` module and how to implement a class using these base classes as well as overriding necessary methods to ensure the class fulfills a specific container interface. Problem Statement: You are required to implement a class named `CustomList` which should act like a Python list, but with a few custom behaviors. Your `CustomList` class should inherit from `collections.abc.MutableSequence` and should contain the following: 1. An internal list to store the elements. 2. Overridden implementations for the following abstract methods: - `__getitem__` - `__setitem__` - `__delitem__` - `__len__` - `insert` 3. An additional method `sum` which returns the sum of all numeric elements in the list. Implementation Details: 1. **`__getitem__`**: This method should take an index and return the item at that index from the internal list. 2. **`__setitem__`**: This method should take an index and a value, and set the item at the specified index to the provided value in the internal list. 3. **`__delitem__`**: This method should delete the item at the specified index from the internal list. 4. **`__len__`**: This method should return the length of the internal list. 5. **`insert`**: This method should take an index and a value as parameters and insert the value at the specified index in the internal list. Constraints: - Do not use Python\'s built-in list methods directly (except within the `insert` function where necessary). - You can assume that the inputs to `__setitem__` and `insert` will be valid. - The `sum` method should assume all elements in the list are numeric. Example: ```python from collections.abc import MutableSequence class CustomList(MutableSequence): def __init__(self): self._items = [] def __getitem__(self, index): # Your code here def __setitem__(self, index, value): # Your code here def __delitem__(self, index): # Your code here def __len__(self): # Your code here def insert(self, index, value): # Your code here def sum(self): # Your code here # Example usage: cl = CustomList() cl.append(1) cl.append(2) cl.append(3) print(cl.sum()) # Output: 6 ``` Complete the implementation of the `CustomList` class as described.","solution":"from collections.abc import MutableSequence class CustomList(MutableSequence): def __init__(self): self._items = [] def __getitem__(self, index): return self._items[index] def __setitem__(self, index, value): self._items[index] = value def __delitem__(self, index): del self._items[index] def __len__(self): return len(self._items) def insert(self, index, value): self._items.insert(index, value) def sum(self): return sum(self._items)"},{"question":"**Question**: You have been provided with a Python script file named `script_to_analyze.py` that contains several module imports. Your task is to write a Python function that uses the `modulefinder.ModuleFinder` class to analyze this script and generate a report of the imported modules. # Function Specification **Function name**: `analyze_script_modules` **Input**: - `script_path` (str): The path to the Python script file to be analyzed. **Output**: - Returns a dictionary with two keys: - `\'loaded_modules\'`: A dictionary where the keys are module names and the values are lists of the first three global names defined in each module. - `\'missing_modules\'`: A list of module names that could not be imported. # Constraints - Use the `modulefinder.ModuleFinder` class provided in the Python `modulefinder` module. - The script to be analyzed will contain valid Python code. # Example Given the following script in `script_to_analyze.py`: ```python import os import sys try: import nonexistentmodule except ImportError: pass try: import another.fake.module except ImportError: pass print(\\"Hello, world!\\") ``` Calling `analyze_script_modules(\'script_to_analyze.py\')` should return: ```python { \'loaded_modules\': { \'os\': [...], # List of first three global names defined in os module \'sys\': [...] # List of first three global names defined in sys module }, \'missing_modules\': [ \'nonexistentmodule\', \'another.fake.module\' ] } ``` # Implementation You should implement the `analyze_script_modules` function which performs the following steps: 1. Instantiates a `ModuleFinder` object. 2. Uses the `run_script(pathname)` method to analyze the provided script. 3. Constructs and returns the report of loaded and missing modules as specified in the expected output.","solution":"import modulefinder def analyze_script_modules(script_path): Analyzes the provided Python script and reports loaded and missing modules. Args: - script_path (str): The path to the Python script file to be analyzed. Returns: - dict: A dictionary with two keys: \'loaded_modules\' - A dictionary where the keys are module names and the values are lists of the first three global names defined in each module. \'missing_modules\' - A list of module names that could not be imported. finder = modulefinder.ModuleFinder() finder.run_script(script_path) loaded_modules = {} missing_modules = list(finder.badmodules.keys()) for name, module in finder.modules.items(): if module.globalnames: loaded_modules[name] = list(module.globalnames)[:3] else: loaded_modules[name] = [] return { \'loaded_modules\': loaded_modules, \'missing_modules\': missing_modules }"},{"question":"**Coding Assessment Question** # Objective: Demonstrate the ability to use the `datetime` module to create, manipulate, and extract information from date and time objects in python. # Problem Statement: You are required to implement a function using the macros and functions from the `datetime` module described in the documentation above. # Task: Implement the following function: ```python def process_datetime_operations(): 1. Create a datetime object for the date October 10, 2023, at 15:45:30 with 500,000 microseconds. 2. Convert this datetime object into a date object. 3. Extract and return the following information from these objects: - Year, Month, Day - Hour, Minute, Second, Microsecond 4. Create a timedelta object representing 10 days, 5 hours, and 30 minutes. 5. Add this timedelta to the original datetime object. 6. Extract and return the new datetime\'s: - Year, Month, Day - Hour, Minute, Second, Microsecond # Your code here ``` # Constraints: 1. You must use the macros and functions provided in the documentation to create and manipulate these objects. 2. You cannot use any direct methods from the python\'s `datetime` module or any other library functions. 3. Ensure to handle any potential errors, e.g., invalid data types or values. # Expected Output: The function should output a dictionary with the following structure: ```python { \\"original_date_info\\": { \\"year\\": 2023, \\"month\\": 10, \\"day\\": 10 }, \\"original_time_info\\": { \\"hour\\": 15, \\"minute\\": 45, \\"second\\": 30, \\"microsecond\\": 500000 }, \\"new_datetime_info\\": { \\"year\\": 2023, \\"month\\": 10, \\"day\\": 20, \\"hour\\": 21, \\"minute\\": 15, \\"second\\": 30, \\"microsecond\\": 500000 } } ``` # Performance: Your implementation should efficiently create and manipulate datetime objects and extract the required fields. You are expected to handle any edge cases or potential errors gracefully. # Example: ```python output = process_datetime_operations() print(output) ``` # Note: Integrate the provided macros and functions into your solution to demonstrate a thorough understanding of their usage.","solution":"from datetime import datetime, timedelta def process_datetime_operations(): # 1. Create a datetime object for the date October 10, 2023, at 15:45:30 with 500,000 microseconds. original_datetime = datetime(2023, 10, 10, 15, 45, 30, 500000) # 2. Convert this datetime object into a date object. date_object = original_datetime.date() # 3. Extract and return the following information from these objects: # - Year, Month, Day original_date_info = { \\"year\\": date_object.year, \\"month\\": date_object.month, \\"day\\": date_object.day } # - Hour, Minute, Second, Microsecond original_time_info = { \\"hour\\": original_datetime.hour, \\"minute\\": original_datetime.minute, \\"second\\": original_datetime.second, \\"microsecond\\": original_datetime.microsecond } # 4. Create a timedelta object representing 10 days, 5 hours, and 30 minutes. delta = timedelta(days=10, hours=5, minutes=30) # 5. Add this timedelta to the original datetime object. new_datetime = original_datetime + delta # 6. Extract and return the new datetime\'s: # - Year, Month, Day # - Hour, Minute, Second, Microsecond new_datetime_info = { \\"year\\": new_datetime.year, \\"month\\": new_datetime.month, \\"day\\": new_datetime.day, \\"hour\\": new_datetime.hour, \\"minute\\": new_datetime.minute, \\"second\\": new_datetime.second, \\"microsecond\\": new_datetime.microsecond } return { \\"original_date_info\\": original_date_info, \\"original_time_info\\": original_time_info, \\"new_datetime_info\\": new_datetime_info }"},{"question":"Using the `linecache` module, write a function called `extract_lines` that retrieves specified lines from multiple files and aggregates them into a single dictionary. Function Signature ```python def extract_lines(file_line_pairs): pass ``` Input - `file_line_pairs`: A list of tuples, where each tuple contains the filename (as a string) and the line number (as an integer) to be retrieved. Output - Returns a dictionary where the keys are filenames and the values are lists of lines corresponding to the requested line numbers. Constraints - If a file does not exist or the line number is invalid, the corresponding value in the list should be an empty string. - You should clear the cache after extracting all lines to ensure cache does not hold stale data. - The performance should be optimized for scenarios where multiple lines from the same file are requested. Example ```python file_line_pairs = [(\'example1.txt\', 1), (\'example1.txt\', 3), (\'example2.txt\', 2)] result = extract_lines(file_line_pairs) print(result) ``` Expected output (assuming \'example1.txt\' and \'example2.txt\' have appropriate content): ```python { \'example1.txt\': [\'Line 1 of example1\', \'Line 3 of example1\'], \'example2.txt\': [\'Line 2 of example2\'] } ``` **Additional Information** - Use the `linecache.getline` function to retrieve lines. - Ensure you handle possible errors gracefully without throwing exceptions. Notes 1. The line retrieval should be efficient, leveraging the caching mechanism provided by the `linecache` module. 2. Demonstrate clearing the cache by calling `linecache.clearcache` at the end of your function. 3. Handle relative filenames appropriately using the functionality provided by `linecache`.","solution":"import linecache def extract_lines(file_line_pairs): Retrieves specified lines from multiple files and aggregates them into a dictionary. Parameters: - file_line_pairs: List of tuples, where each tuple contains the filename (as a string) and the line number (as an integer). Returns: - A dictionary where the keys are filenames and the values are lists of lines corresponding to the requested line numbers. result = {} for filename, line_number in file_line_pairs: if filename not in result: result[filename] = [] line = linecache.getline(filename, line_number) if line == \'\': line = \'\' # Handle the case where the line is not found (invalid line number or file does not exist) result[filename].append(line.rstrip(\'n\')) linecache.clearcache() return result"},{"question":"# Seaborn Histogram Coding Assessment **Objective:** Demonstrate your understanding of Seaborn\'s `histplot` functionality by creating a comprehensive visualization that compares different aspects of the provided dataset. **Dataset:** You will be working with the `penguins` dataset, which can be loaded using `sns.load_dataset(\\"penguins\\")`. The dataset contains the following columns: - `species`: Penguin species (Adelie, Chinstrap, and Gentoo) - `island`: Island where the observation was made (Biscoe, Dream, Torgersen) - `bill_length_mm` - `bill_depth_mm` - `flipper_length_mm` - `body_mass_g` - `sex`: Gender of the penguin (Male, Female) **Task:** 1. Generate univariate histograms for `bill_length_mm` and `bill_depth_mm` with and without KDE. Ensure to specify appropriate bin widths and compare the histograms using both methods. 2. Create bivariate histograms for the following pairs of variables: - `bill_depth_mm` vs. `body_mass_g` - `flipper_length_mm` vs. `body_mass_g` Use different color maps and compare how well the bivariate plots represent the data. 3. Draw histograms for `flipper_length_mm` segmented by `species`. Utilize both layered and stacked approaches to visualize the differences. Discuss the appropriateness of each approach. 4. Generate histograms over the categorical variable `island`, with hue mapping based on the `sex` variable. Use the \\"dodge\\" parameter and discuss how the visualization improves the understanding of the data distribution. **Implementation Requirements:** - Use Seaborn\'s `histplot` function to generate all the required plots. - Load the data using `sns.load_dataset(\\"penguins\\")`. - Use matplotlib\'s `plt.show()` function after each plot, ensuring the plots are displayed correctly. - Provide comments in the code explaining each step and the choice of parameters for the `histplot` function. **Expected Output:** - Multiple histograms representing each part of the task. - Comments discussing the insights derived from each histogram. - Proper visualization with readable axis labels, legends, and titles that enhance clarity. **Constraints:** - Ensure the bin widths are chosen wisely to represent the data accurately. - All histograms should be clearly labeled and color-coded where necessary. **Submission:** Submit your `.py` file containing the complete code and comments to generate the required histograms and analyses.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Task 1: Univariate histograms for `bill_length_mm` and `bill_depth_mm` with and without KDE # Histogram of bill_length_mm without KDE sns.histplot(penguins[\'bill_length_mm\'], bins=30) plt.title(\\"Histogram of bill_length_mm without KDE\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Frequency\\") plt.show() # Histogram of bill_length_mm with KDE sns.histplot(penguins[\'bill_length_mm\'], bins=30, kde=True) plt.title(\\"Histogram of bill_length_mm with KDE\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Frequency\\") plt.show() # Histogram of bill_depth_mm without KDE sns.histplot(penguins[\'bill_depth_mm\'], bins=30) plt.title(\\"Histogram of bill_depth_mm without KDE\\") plt.xlabel(\\"Bill Depth (mm)\\") plt.ylabel(\\"Frequency\\") plt.show() # Histogram of bill_depth_mm with KDE sns.histplot(penguins[\'bill_depth_mm\'], bins=30, kde=True) plt.title(\\"Histogram of bill_depth_mm with KDE\\") plt.xlabel(\\"Bill Depth (mm)\\") plt.ylabel(\\"Frequency\\") plt.show() # Task 2: Bivariate histograms # Bivariate histogram of bill_depth_mm vs. body_mass_g sns.histplot(penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", bins=30, cmap=\\"light:m_r\\") plt.title(\\"Bivariate Histogram of bill_depth_mm vs. body_mass_g\\") plt.xlabel(\\"Bill Depth (mm)\\") plt.ylabel(\\"Body Mass (g)\\") plt.show() # Bivariate histogram of flipper_length_mm vs. body_mass_g sns.histplot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", bins=30, cmap=\\"light:m_r\\") plt.title(\\"Bivariate Histogram of flipper_length_mm vs. body_mass_g\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Body Mass (g)\\") plt.show() # Task 3: Histograms for flipper_length_mm segmented by species - layered and stacked # Layered Approach sns.histplot(penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", multiple=\\"layer\\", bins=30) plt.title(\\"Histogram of flipper_length_mm segmented by species (Layered)\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Frequency\\") plt.show() # Stacked Approach sns.histplot(penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", multiple=\\"stack\\", bins=30) plt.title(\\"Histogram of flipper_length_mm segmented by species (Stacked)\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Frequency\\") plt.show() # Task 4: Histograms over the categorical variable `island`, with hue mapping based on `sex` sns.histplot(penguins, x=\\"island\\", hue=\\"sex\\", multiple=\\"dodge\\", shrink=0.8) plt.title(\\"Histogram of island with hue mapping based on sex\\") plt.xlabel(\\"Island\\") plt.ylabel(\\"Frequency\\") plt.show()"},{"question":"# Objective Calibrate a binary classifier using scikit-learn\'s CalibratedClassifierCV and evaluate its performance with calibration curves. # Background Information In binary classification tasks, it is often beneficial to not only predict the class labels but also to understand the confidence of these predictions. This can be achieved by probability calibration, which adjusts the predicted probabilities to better reflect the actual likelihood of an event. scikit-learn provides tools to calibrate these probabilities through its `CalibratedClassifierCV` class. # Problem Statement You must implement a function to: 1. Train a binary classifier. 2. Calibrate its probabilities using both sigmoid and isotonic methods. 3. Plot calibration curves to compare the performance of the calibrated classifiers. # Function Signature ```python def plot_calibration_curves(X_train, y_train, X_test, y_test): pass ``` # Input - `X_train`: A 2D numpy array or pandas DataFrame of shape (n_samples, n_features) containing the training features. - `y_train`: A 1D numpy array or pandas Series of shape (n_samples,) containing the binary class labels for training data. - `X_test`: A 2D numpy array or pandas DataFrame of shape (n_samples, n_features) containing the test features. - `y_test`: A 1D numpy array or pandas Series of shape (n_samples,) containing the binary class labels for test data. # Output - The function should display calibration plots for both calibration methods. # Constraints - You may use any binary classifier from scikit-learn that supports `predict_proba`. - Use cross-validation with `cv=5` for calibration. - Ensure the plots are labeled and easy to interpret. # Instructions 1. Fit a binary classifier (e.g., LogisticRegression) on `X_train` and `y_train`. 2. Apply probability calibration using `CalibratedClassifierCV` with both `sigmoid` and `isotonic` methods on the fitted classifier. 3. Use the test data (`X_test` and `y_test`) to generate predicted probabilities. 4. Plot calibration curves comparing the original and calibrated classifiers. 5. Plot histograms of the predicted probabilities to give insight into the behavior of both calibrated and uncalibrated classifiers. # Example Usage ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.calibration import CalibratedClassifierCV, calibration_curve from sklearn.model_selection import train_test_split def plot_calibration_curves(X_train, y_train, X_test, y_test): # Fit a binary classifier base_model = LogisticRegression() base_model.fit(X_train, y_train) # Calibrate the classifier using sigmoid and isotonic methods calibrated_clf_sigmoid = CalibratedClassifierCV(base_model, method=\'sigmoid\', cv=5) calibrated_clf_sigmoid.fit(X_train, y_train) calibrated_clf_isotonic = CalibratedClassifierCV(base_model, method=\'isotonic\', cv=5) calibrated_clf_isotonic.fit(X_train, y_train) # Get predicted probabilities prob_pos_uncalibrated = base_model.predict_proba(X_test)[:, 1] prob_pos_sigmoid = calibrated_clf_sigmoid.predict_proba(X_test)[:, 1] prob_pos_isotonic = calibrated_clf_isotonic.predict_proba(X_test)[:, 1] # Plot calibration curves plt.figure(figsize=(10, 10)) for clf, name in [(prob_pos_uncalibrated, \'Uncalibrated\'), (prob_pos_sigmoid, \'Sigmoid Calibrated\'), (prob_pos_isotonic, \'Isotonic Calibrated\')]: fraction_of_positives, mean_predicted_value = calibration_curve(y_test, clf, n_bins=10) plt.plot(mean_predicted_value, fraction_of_positives, \\"s-\\", label=name) plt.plot([0, 1], [0, 1], \\"k--\\", label=\\"Perfectly calibrated\\") plt.ylabel(\\"Fraction of positives\\") plt.xlabel(\\"Mean predicted value\\") plt.legend(loc=\\"best\\") plt.title(\\"Calibration Curves\\") plt.show() # Sample Data Generation X, y = make_classification(n_samples=1000, n_features=20, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) plot_calibration_curves(X_train, y_train, X_test, y_test) ``` # Notes - Ensure that the classifiers used support the `predict_proba` method. - Use the generated plots to compare how well-calibrated the classifier is before and after applying calibration.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.calibration import CalibratedClassifierCV, calibration_curve from sklearn.model_selection import train_test_split def plot_calibration_curves(X_train, y_train, X_test, y_test): Train a binary classifier, calibrate its probabilities using both sigmoid and isotonic methods, and plot calibration curves to compare the performance of the calibrated classifiers. # Fit a binary classifier base_model = LogisticRegression() base_model.fit(X_train, y_train) # Calibrate the classifier using sigmoid and isotonic methods calibrated_clf_sigmoid = CalibratedClassifierCV(base_model, method=\'sigmoid\', cv=5) calibrated_clf_sigmoid.fit(X_train, y_train) calibrated_clf_isotonic = CalibratedClassifierCV(base_model, method=\'isotonic\', cv=5) calibrated_clf_isotonic.fit(X_train, y_train) # Get predicted probabilities prob_pos_uncalibrated = base_model.predict_proba(X_test)[:, 1] prob_pos_sigmoid = calibrated_clf_sigmoid.predict_proba(X_test)[:, 1] prob_pos_isotonic = calibrated_clf_isotonic.predict_proba(X_test)[:, 1] # Plot calibration curves plt.figure(figsize=(10, 10)) for prob_pos, name in [ (prob_pos_uncalibrated, \'Uncalibrated\'), (prob_pos_sigmoid, \'Sigmoid Calibrated\'), (prob_pos_isotonic, \'Isotonic Calibrated\') ]: fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_pos, n_bins=10) plt.plot(mean_predicted_value, fraction_of_positives, \\"s-\\", label=name) plt.plot([0, 1], [0, 1], \\"k--\\", label=\\"Perfectly calibrated\\") plt.ylabel(\\"Fraction of positives\\") plt.xlabel(\\"Mean predicted value\\") plt.legend(loc=\\"best\\") plt.title(\\"Calibration Curves\\") plt.show()"},{"question":"# Seaborn Plot Context Customization You have been given a dataset and tasked with creating three different visualizations using Seaborn. The goal is to demonstrate your ability to customize the appearance of these plots using `set_context`. Your function should generate three plots with different contexts and settings. Dataset ```python data = { \\"x\\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \\"y\\": [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] } ``` Requirements 1. **Function Name**: `create_custom_plots` 2. **Input**: None 3. **Output**: The function should create and display three plots as specified below. Plot Specifications 1. **Plot 1**: - Context: \'notebook\' - Use the default font scale and line width. 2. **Plot 2**: - Context: \'talk\' - Increase the font scale to 1.5. 3. **Plot 3**: - Context: \'paper\' - Override line width to `2.5`. ```python import seaborn as sns import matplotlib.pyplot as plt def create_custom_plots(): # data data = { \\"x\\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \\"y\\": [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] } # Plot 1 sns.set_context(\\"notebook\\") plt.figure() sns.lineplot(x=data[\\"x\\"], y=data[\\"y\\"]) plt.title(\\"Notebook Context\\") plt.show() # Plot 2 sns.set_context(\\"talk\\", font_scale=1.5) plt.figure() sns.lineplot(x=data[\\"x\\"], y=data[\\"y\\"]) plt.title(\\"Talk Context with Increased Font Scale\\") plt.show() # Plot 3 sns.set_context(\\"paper\\", rc={\\"lines.linewidth\\": 2.5}) plt.figure() sns.lineplot(x=data[\\"x\\"], y=data[\\"y\\"]) plt.title(\\"Paper Context with Custom Line Width\\") plt.show() # Test the function create_custom_plots() ``` The function will generate three line plots with the specified context and customization requirements. This task tests the student\'s understanding of using the `set_context` method in Seaborn and their ability to apply different plot settings effectively.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plots(): # data data = { \\"x\\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \\"y\\": [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] } # Plot 1 sns.set_context(\\"notebook\\") plt.figure() sns.lineplot(x=data[\\"x\\"], y=data[\\"y\\"]) plt.title(\\"Notebook Context\\") plt.show() # Plot 2 sns.set_context(\\"talk\\", font_scale=1.5) plt.figure() sns.lineplot(x=data[\\"x\\"], y=data[\\"y\\"]) plt.title(\\"Talk Context with Increased Font Scale\\") plt.show() # Plot 3 sns.set_context(\\"paper\\", rc={\\"lines.linewidth\\": 2.5}) plt.figure() sns.lineplot(x=data[\\"x\\"], y=data[\\"y\\"]) plt.title(\\"Paper Context with Custom Line Width\\") plt.show()"},{"question":"# Advanced Python Control Flow and Function Design Problem Statement You are tasked with writing a function that processes a list of student records and calculates the results of specific queries. Student records are provided in a list of dictionaries, each containing the following keys: - `name`: The student\'s name (string) - `scores`: A list of test scores (integers) You need to implement the function `process_student_records(records: list, queries: list) -> list`. The function takes in two arguments: - `records`: A list of dictionaries containing student records - `queries`: A list of queries to be processed Each query is a dictionary with one key-value pair that specifies the type of query and its value: - A `match` query for a student\'s name: ```python {\'match\': \'student_name\'} ``` This should return the list of scores for the student with the given `student_name` if found. If the student is not found, return the string `\'Student not found\'`. - A `top_n` query for top-n students by average score: ```python {\'top_n\': n} ``` This should return a list of the top `n` students by their average score, sorted in descending order. Each entry in the returned list should be a tuple of the student\'s name and their average score. - An `add_score` query to add a score to a student\'s record: ```python {\'add_score\': (\'student_name\', score)} ``` This should add the provided `score` to the list of scores for the student with the name `student_name`. If the student does not exist, create a new entry for the student with the provided score. Example ```python students = [ {\'name\': \'Alice\', \'scores\': [85, 90, 92]}, {\'name\': \'Bob\', \'scores\': [72, 88, 95]}, {\'name\': \'Charlie\', \'scores\': [95, 100]}, ] queries = [ {\'match\': \'Alice\'}, {\'add_score\': (\'Alice\', 93)}, {\'match\': \'Alice\'}, {\'top_n\': 2}, {\'add_score\': (\'David\', 78)}, {\'match\': \'David\'}, ] result = process_student_records(students, queries) print(result) ``` Expected output: ```python [[85, 90, 92], \'Success\', [85, 90, 92, 93], [(\'Charlie\', 97.5), (\'Alice\', 90.0)], \'Success\', [78]] ``` Constraints - Functions should include appropriate error handling or guards as necessary. - You may use any Python built-ins or standard library imports as needed. - The solution should be efficient enough to handle up to 1000 student records and 1000 queries. Guidelines 1. Use control flow constructs (`if`, `for`, and `while`) appropriately to process the queries. 2. Handle lists, dictionaries, and string operations effectively. 3. Ensure the function returns results in the required formats. 4. Include proper documentation and adhere to coding style guidelines. ```python def process_student_records(records: list, queries: list) -> list: # Your code here pass ```","solution":"def process_student_records(records: list, queries: list) -> list: student_dict = {student[\'name\']: student[\'scores\'] for student in records} results = [] for query in queries: if \'match\' in query: student_name = query[\'match\'] if student_name in student_dict: results.append(student_dict[student_name]) else: results.append(\'Student not found\') elif \'top_n\' in query: n = query[\'top_n\'] avg_scores = [(name, sum(scores) / len(scores)) for name, scores in student_dict.items()] avg_scores.sort(key=lambda x: x[1], reverse=True) results.append(avg_scores[:n]) elif \'add_score\' in query: student_name, score = query[\'add_score\'] if student_name in student_dict: student_dict[student_name].append(score) else: student_dict[student_name] = [score] results.append(\'Success\') return results"},{"question":"Objective Implement a function that processes a list of mixed data types (integers, strings, and lists) and performs various operations using different assignment statements, expression statements, `assert` statements, `del` statements, `pass` statements, and `yield` statements. Function Signature ```python def process_data(data: list) -> list: pass ``` Inputs - `data`: A list containing integers, strings, and lists. Outputs - Returns a list with results of the following operations applied on the input data. Requirements 1. **Expression and Assignment Statements**: - Initialize an empty list `results`. - Iterate through the input list `data` and perform the following operations: - If the element is an integer, append its square to `results`. - If the element is a string, append the string in uppercase to `results`. - If the element is a list, skip it using the `pass` statement. 2. **Assert Statements**: - Use `assert` statements to ensure that the current element being processed is not a dictionary. If it is, raise an `AssertionError` with an appropriate message. 3. **Del Statements**: - Use the `del` statement to remove any `None` values from the input list `data` before processing. 4. **Yield Statements**: - Implement a generator within the function which yields elements one by one from the processed list. - Collect the yielded elements and add them to the `results` list. 5. **Return**: - The function should return the final `results` list. Example ```python input_data = [1, \\"hello\\", [1, 2, 3], None, 7] output = process_data(input_data) # Expected output # [1, \'HELLO\', 49] ``` # Additional Notes - Your function should handle edge cases, such as an empty list or a list with only `None` values. - You may not use any additional libraries; your solution must be implemented using pure Python.","solution":"def process_data(data: list) -> list: Processes a list of mixed data types and performs various operations using different statement types in Python. Arguments: data : list : A list containing integers, strings, lists, and potentially None values. Returns: list : A list containing the processed elements. # Initialize an empty list to store results results = [] # Remove None values from the data using the del statement data = [item for item in data if item is not None] # Iterate through the list for elem in data: # Assert that the element is not a dictionary assert not isinstance(elem, dict), \\"Dictionaries are not allowed in the input data.\\" # Process integers, strings, and skip lists using pass statement if isinstance(elem, int): processed_elem = elem ** 2 # Square the integer results.append(processed_elem) elif isinstance(elem, str): processed_elem = elem.upper() # Convert string to uppercase results.append(processed_elem) elif isinstance(elem, list): pass # Use pass statement to skip lists # Yield elements one by one from the processed list def generator(): for result in results: yield result # Collect yielded elements in results final_results = list(generator()) # Return the final results list return final_results"},{"question":"Implement a function `custom_plot` that takes in two list arguments `x` and `y`, and two tuples `x_limits` and `y_limits` specifying the limits for the x and y axes respectively. This function should create a line plot of x versus y using `seaborn.objects` and apply the specified limits to the axes. If the limits are not specified (i.e., they are `None`), the default behavior should be maintained. # Function Signature ```python def custom_plot(x: list, y: list, x_limits: tuple = None, y_limits: tuple = None) -> None: # Your code here ``` # Input - `x`: A list of numerical values representing the x-coordinates of the line plot. - `y`: A list of numerical values representing the y-coordinates of the line plot. - `x_limits`: A tuple of two numerical values specifying the (min, max) limits for the x-axis. It can also be `None`. - `y_limits`: A tuple of two numerical values specifying the (min, max) limits for the y-axis. It can also be `None`. # Output - The function should not return anything. Instead, it should display the resulting plot with the specified axis limits applied. # Example ```python x = [1, 2, 3] y = [4, 5, 6] x_limits = (0, 4) y_limits = (3, 7) custom_plot(x, y, x_limits, y_limits) ``` # Constraints - Both `x` and `y` should be of the same length and contain at least 2 points. - The elements of `x` and `y` should be numerical. - The limits specified in `x_limits` and `y_limits` should be in ascending order, with the `min` value less than the `max` value. # Notes - Ensure your plot is clear and properly labeled. - Use `seaborn.objects` module for plotting.","solution":"import matplotlib.pyplot as plt import seaborn as sns def custom_plot(x: list, y: list, x_limits: tuple = None, y_limits: tuple = None) -> None: Creates a line plot of x versus y using seaborn and applies the specified limits to the axes if provided. import seaborn.objects as so if len(x) != len(y): raise ValueError(\\"The length of x and y must be the same.\\") # Create the plot plot = so.Plot(x=x, y=y).add(so.Line()) # Apply limits if provided if x_limits is not None: plot = plot.limit(x=x_limits) if y_limits is not None: plot = plot.limit(y=y_limits) plot.show()"},{"question":"Using the seaborn library, create a Python function that generates a multi-faceted regression plot for the `penguins` dataset based on specific criteria. `multi_facet_regression_plot` **Function Signature:** ```python def multi_facet_regression_plot(x: str, y: str, hue: str = None, col: str = None, row: str = None, sharex: bool = True, sharey: bool = True) -> sns.FacetGrid: ``` **Input:** - `x` (str): The name of the variable to be plotted on the x-axis. - `y` (str): The name of the variable to be plotted on the y-axis. - `hue` (str, optional): The name of the variable to be used for color encoding different subsets of data. Default is `None`. - `col` (str, optional): The name of the variable to be used for creating columns in the subplot grid. Default is `None`. - `row` (str, optional): The name of the variable to be used for creating rows in the subplot grid. Default is `None`. - `sharex` (bool, optional): Whether the x-axis should be shared across the facets. Default is `True`. - `sharey` (bool, optional): Whether the y-axis should be shared across the facets. Default is `True`. **Output:** Returns a `sns.FacetGrid` object that contains the regression plots conditioned on the given variables. **Constraints:** - Ensure that the seaborn dataset `penguins` is used for the plots. - Handle cases where optional parameters (`hue`, `col`, `row`) are not provided. **Performance Requirements:** - The function should be efficient and handle the dataset size effectively. - The function should not perform unnecessary computations. Example ```python # Example usage of the function import seaborn as sns sns.set_theme(style=\\"ticks\\") # Calls to the function g = multi_facet_regression_plot(x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", col=\\"sex\\", sharex=False, sharey=False) g.savefig(\\"output_plot.png\\") # To save the generated plot to a file ``` Expected plot: A multi-faceted regression plot where the data is conditioned by species (color-coded) and split by sex (in subplots).","solution":"import seaborn as sns import matplotlib.pyplot as plt def multi_facet_regression_plot(x: str, y: str, hue: str = None, col: str = None, row: str = None, sharex: bool = True, sharey: bool = True) -> sns.FacetGrid: Generates a multi-faceted regression plot for the penguins dataset based on specified criteria. Parameters: x (str): The name of the variable to be plotted on the x-axis. y (str): The name of the variable to be plotted on the y-axis. hue (str, optional): The name of the variable to be used for color encoding different subsets of data. Default is None. col (str, optional): The name of the variable to be used for creating columns in the subplot grid. Default is None. row (str, optional): The name of the variable to be used for creating rows in the subplot grid. Default is None. sharex (bool, optional): Whether the x-axis should be shared across the facets. Default is True. sharey (bool, optional): Whether the y-axis should be shared across the facets. Default is True. Returns: sns.FacetGrid: A FacetGrid object containing the regression plots. # Load the penguins dataset penguins = sns.load_dataset(\'penguins\') # Create the FacetGrid g = sns.FacetGrid(data=penguins, col=col, row=row, hue=hue, sharex=sharex, sharey=sharey) # Map the regression plot onto the grid g.map(sns.regplot, x, y) # Add a legend if hue is used if hue: g.add_legend() return g"},{"question":"# Seaborn Multi-Plot Grids Assessment You are given a dataset of users\' activity on a website. The dataset contains the following columns: - `user_id`: Unique identifier for each user. - `age`: Age of the user. - `gender`: Gender of the user (`Male` or `Female`). - `activity`: Type of activity performed by the user (`Login`, `Purchase`, `Click`, etc.). - `time_spent`: Time spent on the activity in minutes. **Objective**: Your task is to create a multi-plot grid using seaborn\'s `FacetGrid` and `PairGrid` to visualize the relationships and distributions in the dataset. **1. FacetGrid Task**: Create a `FacetGrid` to visualize the distribution of `time_spent` on different activities (`activity`) for each gender (`gender`). Customize the grid using the following specifications: - Use `row` for `gender`. - Use `col` for `activity`. - Plot histograms for the `time_spent` variable. - Set appropriate axis labels for `time_spent` and `activity`. - Add a legend to the plot. **2. PairGrid Task**: Create a `PairGrid` to visualize the pairwise relationships between `age` and `time_spent`, categorized by `activity`. Customize the grid using the following specifications: - Use scatter plots for the off-diagonal plots. - Use histograms for the diagonal plots. - Color the plots by `activity`. - Add a legend to the plot. **Input Format**: - The dataset will be provided as a pandas DataFrame. - Functions will need to follow the naming conventions and argument structures. **Output Format**: - Provide a function `create_facetgrid()` that takes a DataFrame and returns a `FacetGrid` object. - Provide a function `create_pairgrid()` that takes a DataFrame and returns a `PairGrid` object. **Sample Dataset**: ```python import pandas as pd data = { \'user_id\': [1, 2, 3, 4, 5], \'age\': [23, 45, 22, 34, 32], \'gender\': [\'Male\', \'Female\', \'Male\', \'Female\', \'Male\'], \'activity\': [\'Login\', \'Purchase\', \'Click\', \'Purchase\', \'Login\'], \'time_spent\': [5, 10, 3, 8, 15] } df = pd.DataFrame(data) ``` **Example Function Implementations**: ```python import seaborn as sns def create_facetgrid(df): g = sns.FacetGrid(df, row=\\"gender\\", col=\\"activity\\", margin_titles=True) g.map(sns.histplot, \\"time_spent\\") g.set_axis_labels(\\"Time Spent (minutes)\\", \\"Activity\\") g.add_legend() return g def create_pairgrid(df): g = sns.PairGrid(df, hue=\\"activity\\") g.map_diag(sns.histplot) g.map_offdiag(sns.scatterplot) g.add_legend() return g ``` Your solution should include the functions `create_facetgrid` and `create_pairgrid` as specified to correctly visualize the given dataset.","solution":"import seaborn as sns import pandas as pd def create_facetgrid(df): Create a FacetGrid to visualize the distribution of \'time_spent\' on different activities for each gender. Parameters: df (DataFrame): A pandas DataFrame containing the dataset. Returns: FacetGrid: A seaborn FacetGrid object. g = sns.FacetGrid(df, row=\\"gender\\", col=\\"activity\\", margin_titles=True) g.map(sns.histplot, \\"time_spent\\") g.set_axis_labels(\\"Time Spent (minutes)\\", \\"Activity\\") g.add_legend() return g def create_pairgrid(df): Create a PairGrid to visualize pairwise relationships between \'age\' and \'time_spent\', categorized by \'activity\'. Parameters: df (DataFrame): A pandas DataFrame containing the dataset. Returns: PairGrid: A seaborn PairGrid object. g = sns.PairGrid(df, vars=[\\"age\\", \\"time_spent\\"], hue=\\"activity\\") g.map_diag(sns.histplot) g.map_offdiag(sns.scatterplot) g.add_legend() return g"},{"question":"**Problem Statement:** You are required to implement a function `handle_error(errno_value)` which takes an integer `errno_value` as an argument and returns a tuple containing the corresponding error name and its human-readable error message. You should use the `errno` module to obtain the error name and the `os` module to get the human-readable error message. # Function Signature ```python def handle_error(errno_value: int) -> tuple: pass ``` # Input - `errno_value`: An integer representing the errno value. # Output - A tuple `(error_name, error_message)`: - `error_name`: A string representing the error name corresponding to `errno_value`. If the `errno_value` is not found in `errno.errorcode`, return the string `\'Unknown error\'`. - `error_message`: A human-readable string of the error message obtained using `os.strerror(errno_value)`. # Constraints - You may assume that `errno_value` is an integer. - The error message should accurately reflect what the `os.strerror()` would return for the given `errno_value`. # Example ```python import errno # Example usage errno_value = errno.ENOENT # This typically corresponds to \\"No such file or directory\\" result = handle_error(errno_value) print(result) # (\\"ENOENT\\", \\"No such file or directory\\") errno_value_unknown = 9999 # An unknown errno value not typically listed result = handle_error(errno_value_unknown) print(result) # (\\"Unknown error\\", \\"Unknown error 9999\\") ``` # Requirements - Make sure to handle cases where the `errno_value` provided does not exist in `errno.errorcode`. - Use proper exception handling and error checking. # Hints - Use the `errno.errorcode` dictionary to map error numbers to their respective names. - Use the `os.strerror()` function to obtain a human-readable error message. **Good Luck!**","solution":"import errno import os def handle_error(errno_value: int) -> tuple: Given an errno value, returns a tuple containing the error name and its human-readable error message. Parameters: errno_value (int): The errno value. Returns: tuple: A tuple containing the error name and error message. error_name = errno.errorcode.get(errno_value, \\"Unknown error\\") error_message = os.strerror(errno_value) return (error_name, error_message)"},{"question":"# Problem: Statistical Analysis of Combinatorial Data In this problem, you will perform various mathematical computations using the `math` module in Python. Specifically, you will perform operations on combinatorial values and apply logarithmic functions to analyze a dataset. Given a list of tuples representing the number of items `n` and the number of items to choose `r`, you need to perform the following steps: 1. **Combination Calculation**: Compute the number of ways to choose `r` items from `n` items without repetition and without order for each tuple in the list. 2. **Logarithmic Transformation**: Apply the natural logarithm to each of the computed combination values. 3. **Error Function Transformation**: Further apply the error function `erf` to the logarithmic values obtained in the previous step. 4. **Sum Calculation**: Sum all the transformation values and return the result. Write a function `analyze_combinations(data: list) -> float` where: - `data` is a list of tuples, each tuple containing two integers `(n, r)`. - The function returns a single float representing the sum of the transformed combination values. **Constraints**: - `n` and `r` are non-negative integers. - 0 ≤ `r` ≤ `n` - The input list will contain at least one tuple. - Make sure to handle edge cases where values might push the limits of computational precision. # Example ```python from math import comb, log, erf def analyze_combinations(data): result = 0.0 for n, r in data: combination_result = comb(n, r) log_value = log(combination_result) erf_value = erf(log_value) result += erf_value return result # Sample Input data = [(5, 2), (10, 5), (20, 10)] # Sample Output print(analyze_combinations(data)) # Output may vary as the scaled cumulative normal distribution is quite specific ``` # Explanation - For `(5, 2)`, the number of combinations is `comb(5, 2) = 10`. - `log(10) ≈ 2.3026` - `erf(2.3026) ≈ 0.9992` - For `(10, 5)`, the number of combinations is `comb(10, 5) = 252`. - `log(252) ≈ 5.5294` - `erf(5.5294) ≈ 0.99999` - For `(20, 10)`, the number of combinations is `comb(20, 10) = 184756`. - `log(184756) ≈ 12.133` - `erf(12.133) ≈ 1.0` The final result is the sum of all the `erf` transformed values. ```python output ≈ 0.9992 + 0.99999 + 1.0 ``` # Notes - Use appropriate mathematical functions from the `math` module. - Handle precision and edge cases properly as per Python\'s floating-point arithmetic.","solution":"from math import comb, log, erf def analyze_combinations(data): result = 0.0 for n, r in data: combination_result = comb(n, r) log_value = log(combination_result) erf_value = erf(log_value) result += erf_value return result"},{"question":"**Objective:** Write a C extension module for Python that adds a function to compute the sum of a list of integers. The function should properly manage reference counts and handle any exceptions that occur. **Instructions:** 1. **Function Name:** `sum_integers` 2. **Input:** A Python list of integers. 3. **Output:** An integer sum of all elements in the list. 4. **Exceptions:** Raise a `TypeError` if the input is not a list or if any element in the list is not an integer. 5. **Reference Counts:** Ensure all reference counts are managed properly to avoid memory leaks or unnecessary memory usage. **Constraints:** 1. Do not use any helper functions that are not part of the Python/C API. 2. Use the provided macros and functions such as `PyList_Check`, `PyLong_Check`, `Py_INCREF`, and `Py_DECREF` to handle Python objects. 3. Handle exceptions and ensure that any allocated resources are properly released in case of an error. **Code Skeleton:** You are provided with a code skeleton to complete: ```c #define PY_SSIZE_T_CLEAN #include <Python.h> // Function to compute the sum of integers in a list static PyObject* sum_integers(PyObject* self, PyObject* args) { PyObject *input_list; Py_ssize_t list_size, i; long sum = 0; // Parse the input arguments if (!PyArg_ParseTuple(args, \\"O\\", &input_list)) { return NULL; } // Check if the input is a list if (!PyList_Check(input_list)) { PyErr_SetString(PyExc_TypeError, \\"Input must be a list\\"); return NULL; } list_size = PyList_Size(input_list); for (i = 0; i < list_size; i++) { PyObject *item = PyList_GetItem(input_list, i); // Borrowed reference, no Py_DECREF needed if (!PyLong_Check(item)) { PyErr_SetString(PyExc_TypeError, \\"All elements in the list must be integers\\"); return NULL; } long value = PyLong_AsLong(item); if (value == -1 && PyErr_Occurred()) { return NULL; } sum += value; } return PyLong_FromLong(sum); } // Module method table static PyMethodDef SumMethods[] = { { \\"sum_integers\\", sum_integers, METH_VARARGS, \\"Compute the sum of a list of integers\\" }, { NULL, NULL, 0, NULL } }; // Module definition static struct PyModuleDef summodule = { PyModuleDef_HEAD_INIT, \\"sum_module\\", // Module name NULL, // Module documentation -1, // Size of per-interpreter state of the module, or -1 if the module keeps state in global variables. SumMethods }; // Module initialization function PyMODINIT_FUNC PyInit_sum_module(void) { return PyModule_Create(&summodule); } ``` **Steps to Complete the Task:** 1. Implement the `sum_integers` function to correctly compute the sum of integers in a list. 2. Properly parse the input arguments and check the types. 3. Manage reference counts to ensure no memory leaks occur. 4. Handle all potential exceptions by setting appropriate Python exceptions. **Testing:** 1. Build the C extension module. 2. Write a Python script to test the `sum_integers` function with various inputs, including edge cases such as non-integer elements, empty lists, and non-list inputs. 3. Verify that the function raises correct exceptions and returns the accurate sum for valid inputs. **Example Test Cases:** ```python import sum_module # Valid case print(sum_module.sum_integers([1, 2, 3, 4])) # Output: 10 # Edge case: empty list print(sum_module.sum_integers([])) # Output: 0 # Invalid case: non-list input try: print(sum_module.sum_integers(\\"not a list\\")) except TypeError as e: print(e) # Output: Input must be a list # Invalid case: list with non-integer elements try: print(sum_module.sum_integers([1, \\"two\\", 3])) except TypeError as e: print(e) # Output: All elements in the list must be integers ``` **Submission:** Submit the C source file and the Python script used for testing.","solution":"# Solution in plain Python, as an illustrative example. # The actual implementation is supposed to be a C extension module. def sum_integers(int_list): Computes the sum of a list of integers. Raises TypeError if the input is not a list or if any element is not an integer. if not isinstance(int_list, list): raise TypeError(\\"Input must be a list\\") total = 0 for element in int_list: if not isinstance(element, int): raise TypeError(\\"All elements in the list must be integers\\") total += element return total"},{"question":"# Chunk Data Processing Objective You are given a file containing multiple EA IFF 85 chunks. Your task is to implement a function that reads these chunks and processes them to extract the names (IDs) of chunks that have a specific minimum size. Task Write a Python function `get_chunk_ids(file_path: str, min_size: int) -> List[str]` that: 1. Opens a binary file specified by `file_path`. 2. Reads the chunks from the file. 3. Returns a list of chunk IDs where the chunk size is at least `min_size`. Input - `file_path` (str): The path to the binary file containing chunked data. - `min_size` (int): The minimum size of the chunks to be considered. Output - A list of chunk IDs (List[str]) that have a size of at least `min_size`. Constraints - Assume the chunk IDs and data are encoded in UTF-8. - If no chunks meet the size criteria, return an empty list. - Handle any file-related errors gracefully by returning an empty list. Example ```python # Example usage of your function chunk_ids = get_chunk_ids(\'path/to/file.aiff\', 100) print(chunk_ids) # Possible output: [\'FORM\', \'SSND\', ...] ``` Notes 1. Utilize the `chunk.Chunk` class for reading the chunks. 2. Ensure that any open files are properly closed even if an error occurs. 3. Do not use any deprecated features not present in Python 3.10. ```python from typing import List import chunk def get_chunk_ids(file_path: str, min_size: int) -> List[str]: chunk_ids = [] try: with open(file_path, \'rb\') as f: while True: try: ch = chunk.Chunk(f) if ch.getsize() >= min_size: chunk_ids.append(ch.getname().decode(\'utf-8\')) ch.skip() except EOFError: break except (OSError, IOError) as e: # Return an empty list if there are any issues with file access or chunk reading return [] return chunk_ids ``` This problem tests students\' ability to read and manipulate file-like objects, handle exceptions, and use the `chunk` module effectively. It is self-contained and does not assume any prior knowledge beyond what is provided in the problem statement.","solution":"from typing import List import chunk def get_chunk_ids(file_path: str, min_size: int) -> List[str]: chunk_ids = [] try: with open(file_path, \'rb\') as f: while True: try: ch = chunk.Chunk(f) if ch.getsize() >= min_size: chunk_ids.append(ch.getname().decode(\'utf-8\')) ch.skip() except EOFError: break except (OSError, IOError) as e: # Return an empty list if there are any issues with file access or chunk reading return [] return chunk_ids"},{"question":"# SAX Parser Customization and Handling Challenge As an XML processing expert, you are tasked with creating a custom SAX parser that needs to handle XML data incrementally, making efficient use of memory while parsing large XML files. Your task involves setting up a SAX parser using the `xml.sax.xmlreader` module and implementing custom handlers for specific XML events. You need to ensure that the parser can process XML data in chunks and handle various SAX events such as starting and ending elements, and character data. Task: 1. Create a SAX parser that can parse XML data in chunks using the `IncrementalParser`. 2. Implement custom handlers that: - Print the name of each element when it starts and ends. - Collect and print all character data within elements. - Handle large XML files efficiently by processing data in chunks. Requirements: - Use the `IncrementalParser` class to feed XML data in multiple chunks. - Create custom content handlers to manage `startElement`, `endElement`, and `characters` events. - Ensure that the parser can reset and reuse for parsing another XML document without issues. Input: - An XML string divided into multiple chunks for the parser to process incrementally. Output: - Print statements indicating the start and end of elements, and the character data within elements. Example: Given the following XML data split into two chunks: ```xml <bookstore> <book> <title lang=\\"en\\">Harry Potter</title> <author>J. K. Rowling</author> </book> ``` and ```xml <book> <title lang=\\"en\\">Learning XML</title> <author>Erik T. Ray</author> </book> </bookstore> ``` The expected output should be: ``` Start Element: bookstore Start Element: book Start Element: title Characters: Harry Potter End Element: title Start Element: author Characters: J. K. Rowling End Element: author End Element: book Start Element: book Start Element: title Characters: Learning XML End Element: title Start Element: author Characters: Erik T. Ray End Element: author End Element: book End Element: bookstore ``` Constraints: - Ensure that your solution handles XML data incrementally to save memory. - Reset the parser correctly using the `reset` method so it can be reused for new XML documents. - Handle unexpected end of chunks gracefully. ```python import xml.sax import xml.sax.xmlreader class CustomContentHandler(xml.sax.ContentHandler): def startElement(self, name, attrs): print(f\\"Start Element: {name}\\") def endElement(self, name): print(f\\"End Element: {name}\\") def characters(self, content): if content.strip(): # Avoid printing just whitespace print(f\\"Characters: {content}\\") def incrementally_parse(xml_chunks): parser = xml.sax.make_parser() handler = CustomContentHandler() parser.setContentHandler(handler) # Incremental parsing inc_parser = parser.get_parser() for chunk in xml_chunks: inc_parser.feed(chunk) inc_parser.close() inc_parser.reset() # Example usage xml_chunks = [ <bookstore> <book> <title lang=\\"en\\">Harry Potter</title> <author>J. K. Rowling</author> </book>, <book> <title lang=\\"en\\">Learning XML</title> <author>Erik T. Ray</author> </book> </bookstore> ] incrementally_parse(xml_chunks) ```","solution":"import xml.sax import xml.sax.xmlreader class CustomContentHandler(xml.sax.ContentHandler): def startElement(self, name, attrs): print(f\\"Start Element: {name}\\") def endElement(self, name): print(f\\"End Element: {name}\\") def characters(self, content): if content.strip(): # Avoid printing just whitespace print(f\\"Characters: {content}\\") def incrementally_parse(xml_chunks): parser = xml.sax.make_parser() handler = CustomContentHandler() parser.setContentHandler(handler) inc_parser = parser for chunk in xml_chunks: inc_parser.feed(chunk) inc_parser.close() inc_parser.reset()"},{"question":"<|Analysis Begin|> Based on the provided documentation, `pkgutil` is a Python module designed to provide utilities for the import system, with a particular focus on package support. It provides functions to extend the module search path, find loaders and importers, resolve module names, retrieve data from packages, and walk through packages and modules. Here are some of the key features: 1. **ModuleInfo**: A namedtuple that provides a brief summary of a module\'s info. 2. **extend_path**: Extends the search path for modules in a package. 3. **ImpImporter and ImpLoader**: Deprecated classes that wrap Python\'s classic import algorithm. 4. **find_loader, get_importer, get_loader**: Functions to retrieve a module loader or finder. 5. **iter_importers, iter_modules**: Functions to yield finders or module info for a given module name. 6. **walk_packages**: Yields module info for all modules recursively on a specified path. 7. **get_data**: Retrieves a resource from a package. 8. **resolve_name**: Resolves a name to an object, supporting various formats expected in the standard library. These features provide robust support for dynamically handling Python packages and modules, making them essential for creating sophisticated package management solutions. <|Analysis End|> <|Question Begin|> # Coding Assessment Question You are given a complex Python project that consists of multiple packages and sub-packages. Your task is to write a function that will dynamically load a specified module and extract a class or function defined within it. # Task Implement a function `load_and_extract` that accepts two parameters: 1. `module_path` (str): The dotted path of the module to be loaded (e.g., \'package.subpackage.module\'). 2. `object_name` (str): The name of the class or function to be extracted from the module. The function should: - Dynamically import the specified module. - Attempt to retrieve the specified class or function from the imported module. - Return the class or function object if found. - Raise an appropriate error if the module cannot be imported or the object cannot be found. # Input - `module_path`: A string representing the dotted path of the module. - `object_name`: A string representing the name of the class or function to retrieve from the module. # Output - Returns the class or function object specified by `object_name`. # Constraints - You can assume that the module path and object name provided are well-formed. - Ensure that appropriate error handling is implemented to manage import and attribute errors. # Example ```python def load_and_extract(module_path: str, object_name: str): # Your implementation here # Example usage: # Assuming there is a module \'mypackage.mymodule\' with a class \'MyClass\' obj = load_and_extract(\'mypackage.mymodule\', \'MyClass\') print(obj) # <class \'mypackage.mymodule.MyClass\'> ``` Use the `pkgutil` module and the standard importlib functionalities to complete this task. Specifically, consider using `pkgutil.resolve_name` for resolving the module and object name.","solution":"import importlib import pkgutil def load_and_extract(module_path: str, object_name: str): Dynamically loads a module and extracts a specified class or function. Parameters: module_path (str): The dotted path of the module to be loaded. object_name (str): The name of the class or function to be extracted from the module. Returns: The class or function object specified by object_name. Raises: ImportError: If the module cannot be imported. AttributeError: If the specified object cannot be found in the module. try: # Dynamically import the specified module module = importlib.import_module(module_path) except ImportError as e: raise ImportError(f\\"Module {module_path} could not be imported: {e}\\") # Attempt to retrieve the specified class or function from the imported module try: obj = getattr(module, object_name) except AttributeError as e: raise AttributeError(f\\"Object {object_name} could not be found in module {module_path}: {e}\\") return obj"},{"question":"**Question: Memory Buffer Manipulation Using Deprecated Buffer Protocol** # Background In Python 3.x, the old buffer protocol functions are deprecated but still available for compatibility with Python 2.x codebases. These functions allow for operations involving low-level memory buffer manipulations, typically dealing with read-only and writable buffer interfaces. # Problem Statement Write a Python function `copy_readable_buffer_to_writable(src_obj, dest_obj)` that: 1. Uses the `PyObject_AsReadBuffer` function to read a buffer from a source object (`src_obj`). 2. Uses the `PyObject_AsWriteBuffer` function to write the buffer to a destination object (`dest_obj`). # Function Signature ```python def copy_readable_buffer_to_writable(src_obj: object, dest_obj: object) -> bool: \'\'\' Parameters: src_obj (object): Source object that supports a readable buffer interface. dest_obj (object): Destination object that supports a writable buffer interface. Returns: bool: True if the operation is successful, False otherwise. \'\'\' ``` # Requirements - Your function must ensure that it handles errors correctly by returning `False` if any error occurs during the buffer operations. - Use `PyObject_AsReadBuffer` to obtain a read buffer from `src_obj`. - Use `PyObject_AsWriteBuffer` to obtain a write buffer to `dest_obj`. - Copy the content from the read buffer to the write buffer. - Ensure proper cleanup of resources if an error is encountered. # Example Usage ```python # Example objects that support the buffer interfaces src_obj = SomeReadableBufferObject() dest_obj = SomeWritableBufferObject() success = copy_readable_buffer_to_writable(src_obj, dest_obj) print(success) # Should print True if the copy was successful, or False otherwise ``` # Constraints - Assume `src_obj` and `dest_obj` are valid objects for this operation. - Do not use any external libraries; you are required to work within the provided functions. # Note This problem requires a fundamental understanding of memory buffer interfaces and handling low-level memory operations in Python. Ensure your solution is efficient and properly manages all edge cases.","solution":"import ctypes def copy_readable_buffer_to_writable(src_obj, dest_obj): Copy the contents of a readable buffer to a writable buffer. Parameters: src_obj (object): Source object that supports a readable buffer interface. dest_obj (object): Destination object that supports a writable buffer interface. Returns: bool: True if the operation is successful, False otherwise. try: # Obtain the read buffer from the source object read_buf = ctypes.py_object(src_obj) read_len = ctypes.c_ssize_t() PyObject_AsReadBuffer = ctypes.pythonapi.PyObject_AsReadBuffer PyObject_AsReadBuffer.argtypes = [ctypes.py_object, ctypes.POINTER(ctypes.c_void_p), ctypes.POINTER(ctypes.c_ssize_t)] PyObject_AsReadBuffer.restype = ctypes.c_int read_ptr = ctypes.c_void_p() if PyObject_AsReadBuffer(read_buf, ctypes.byref(read_ptr), ctypes.byref(read_len)) != 0: return False # Obtain the write buffer from the destination object write_buf = ctypes.py_object(dest_obj) write_len = ctypes.c_ssize_t() PyObject_AsWriteBuffer = ctypes.pythonapi.PyObject_AsWriteBuffer PyObject_AsWriteBuffer.argtypes = [ctypes.py_object, ctypes.POINTER(ctypes.c_void_p), ctypes.POINTER(ctypes.c_ssize_t)] PyObject_AsWriteBuffer.restype = ctypes.c_int write_ptr = ctypes.c_void_p() if PyObject_AsWriteBuffer(write_buf, ctypes.byref(write_ptr), ctypes.byref(write_len)) != 0: return False # Ensure the write buffer is large enough to hold the read buffer if write_len.value < read_len.value: return False # Copy the contents from the read buffer to the write buffer ctypes.memmove(write_ptr, read_ptr, read_len.value) return True except Exception: return False"},{"question":"# URL Manipulation and Query Handling in Python You are required to implement three functions that utilize the `urllib.parse` module to perform various URL manipulations. These functions will test your understanding of the different methods available in the module. 1. **URL Components Extraction** Implement a function `extract_url_components(url: str) -> dict` that takes a URL string as input and returns a dictionary with the components `scheme`, `netloc`, `path`, `params`, `query`, `fragment`, `username`, `password`, `hostname`, and `port`. **Input:** - `url` (str): A URL string. **Output:** - Returns a dictionary with the above components as keys and their corresponding values extracted from the URL. ```python def extract_url_components(url: str) -> dict: # Your code here pass ``` **Example:** ```python url = \\"http://user:pass@hostname:8080/path;parameters?query=arg#fragment\\" components = extract_url_components(url) print(components) # Output # { # \\"scheme\\": \\"http\\", # \\"netloc\\": \\"user:pass@hostname:8080\\", # \\"path\\": \\"/path\\", # \\"params\\": \\"parameters\\", # \\"query\\": \\"query=arg\\", # \\"fragment\\": \\"fragment\\", # \\"username\\": \\"user\\", # \\"password\\": \\"pass\\", # \\"hostname\\": \\"hostname\\", # \\"port\\": 8080 # } ``` 2. **Reconstruct URL** Implement a function `reconstruct_url(components: dict) -> str` that takes a dictionary of URL components (as produced by `extract_url_components()`) and reconstructs the full URL. **Input:** - `components` (dict): A dictionary containing URL components. **Output:** - Returns a reconstructed URL string from the provided components. ```python def reconstruct_url(components: dict) -> str: # Your code here pass ``` **Example:** ```python components = { \\"scheme\\": \\"http\\", \\"netloc\\": \\"user:pass@hostname:8080\\", \\"path\\": \\"/path\\", \\"params\\": \\"parameters\\", \\"query\\": \\"query=arg\\", \\"fragment\\": \\"fragment\\", \\"username\\": \\"user\\", \\"password\\": \\"pass\\", \\"hostname\\": \\"hostname\\", \\"port\\": 8080 } url = reconstruct_url(components) print(url) # Output: \\"http://user:pass@hostname:8080/path;parameters?query=arg#fragment\\" ``` 3. **Parse and Encode Query Strings** Implement a function `parse_and_encode_query(qs: str) -> str` that takes a query string, parses it into key-value pairs, and then encodes it back into a query string format while ensuring the values are sorted alphabetically. **Input:** - `qs` (str): A query string. **Output:** - Returns a new query string with the values sorted alphabetically. ```python def parse_and_encode_query(qs: str) -> str: # Your code here pass ``` **Example:** ```python qs = \\"b=2&a=1&c=3\\" sorted_qs = parse_and_encode_query(qs) print(sorted_qs) # Output: \\"a=1&b=2&c=3\\" ``` # Constraints: - Assume the input URLs and query strings are well-formed. - You may use any methods from the `urllib.parse` module to achieve the required functionalities. # Performance Requirements: - The functions should handle URLs and query strings of up to 10,000 characters efficiently.","solution":"import urllib.parse def extract_url_components(url: str) -> dict: Extracts the components of a given URL. parsed_url = urllib.parse.urlparse(url) return { \\"scheme\\": parsed_url.scheme, \\"netloc\\": parsed_url.netloc, \\"path\\": parsed_url.path, \\"params\\": parsed_url.params, \\"query\\": parsed_url.query, \\"fragment\\": parsed_url.fragment, \\"username\\": parsed_url.username, \\"password\\": parsed_url.password, \\"hostname\\": parsed_url.hostname, \\"port\\": parsed_url.port } def reconstruct_url(components: dict) -> str: Reconstructs a URL from its components. netloc = components[\'netloc\'] if components.get(\'username\') and components.get(\'password\'): netloc = f\\"{components[\'username\']}:{components[\'password\']}@{components[\'hostname\']}\\" if components[\'port\']: netloc += f\\":{components[\'port\']}\\" url = urllib.parse.urlunparse(( components[\'scheme\'], netloc, components[\'path\'], components[\'params\'], components[\'query\'], components[\'fragment\'] )) return url def parse_and_encode_query(qs: str) -> str: Parses a query string into key-value pairs, sorts them alphabetically, and encodes them back. parsed_qs = urllib.parse.parse_qs(qs) sorted_qs = sorted(parsed_qs.items()) encoded_qs = urllib.parse.urlencode(sorted_qs, doseq=True) return encoded_qs"},{"question":"Objective Utilize your understanding of the `smtplib` module to implement a program that sends an email with error handling and different configurations. Task You need to create a Python function `send_custom_email` that: 1. **Takes the following inputs:** - `smtp_server`: The SMTP server address as a string. - `port`: Port for the SMTP server (default should be 587). - `username`: Username for the SMTP server authentication. - `password`: Password for the SMTP server authentication. - `from_addr`: The sender\'s email address. - `to_addrs`: A list of recipient email addresses. - `subject`: The subject line of the email. - `body`: The body content of the email. - `use_tls`: A boolean indicating whether to use TLS (default is True). - `use_ssl`: A boolean indicating whether to use SSL (default is False). 2. **Output:** - If the email is sent successfully, return `True`. - If there are any errors during the process, catch the exception and return a formatted string with the error message. 3. **Constraints:** - Ensure that SSL and TLS configurations are managed properly. If both `use_tls` and `use_ssl` are `True`, prioritize SSL. - Manage connection and authentication errors appropriately by raising the relevant exceptions provided by the `smtplib` module. Notes: - You should use the `email.message.EmailMessage` class from Python\'s `email` package to construct your email message, then send it using the `send_message` method. - Remember to include proper exception handling to capture and report any errors during connection, authentication, or sending the email. Example: ```python def send_custom_email(smtp_server, port=587, username, password, from_addr, to_addrs, subject, body, use_tls=True, use_ssl=False): Send an email using the smtplib module. Args: - smtp_server (str): SMTP server address. - port (int): Port for the SMTP server. - username (str): Username for server authentication. - password (str): Password for server authentication. - from_addr (str): Sender\'s email address. - to_addrs (list): List of recipient email addresses. - subject (str): Subject line of the email. - body (str): Body content of the email. - use_tls (bool): Whether to use TLS (default is True). - use_ssl (bool): Whether to use SSL (default is False). Returns: - True if email is sent successfully, otherwise return error message string. # Your implementation here pass ``` To test the function: ```python result = send_custom_email( smtp_server=\\"smtp.example.com\\", port=587, username=\\"example_user\\", password=\\"example_pass\\", from_addr=\\"sender@example.com\\", to_addrs=[\\"recipient1@example.com\\", \\"recipient2@example.com\\"], subject=\\"Test Email\\", body=\\"This is a test email.\\", use_tls=True, use_ssl=False ) print(result) # Should print True if successful or an error message if there\'s an exception ``` Ensure that your implementation complies with the SMTP protocol.","solution":"import smtplib from email.message import EmailMessage def send_custom_email(smtp_server, port=587, username=\'\', password=\'\', from_addr=\'\', to_addrs=[], subject=\'\', body=\'\', use_tls=True, use_ssl=False): Send an email using the smtplib module. Args: - smtp_server (str): SMTP server address. - port (int): Port for the SMTP server. - username (str): Username for server authentication. - password (str): Password for server authentication. - from_addr (str): Sender\'s email address. - to_addrs (list): List of recipient email addresses. - subject (str): Subject line of the email. - body (str): Body content of the email. - use_tls (bool): Whether to use TLS (default is True). - use_ssl (bool): Whether to use SSL (default is False). Returns: - True if email is sent successfully, otherwise return error message string. try: # Create the email message msg = EmailMessage() msg[\'From\'] = from_addr msg[\'To\'] = \', \'.join(to_addrs) msg[\'Subject\'] = subject msg.set_content(body) # Connect to the SMTP server if use_ssl: server = smtplib.SMTP_SSL(smtp_server, port) else: server = smtplib.SMTP(smtp_server, port) if use_tls: server.starttls() # Login to the SMTP server server.login(username, password) # Send the email server.send_message(msg) # Quit the server server.quit() return True except Exception as e: return str(e)"},{"question":"# Advanced Seaborn Plot Customization **Objective:** Demonstrate your understanding of Seaborn\'s `objects` module by creating custom scatter plots with multiple visual enhancements. This will test your ability to use advanced features such as facet plots, jittering, dodging, and error bars. **Problem Statement:** You are given the `tips` and `glue` datasets available in seaborn. Your task is to implement a function `create_advanced_plots()` that creates and displays two complex plots with the following specifications: 1. **Plot 1: Custom Scatter Plot** - Using the `tips` dataset. - Create a scatter plot with `total_bill` on the x-axis and `day` on the y-axis. - Color the dots based on the `sex` column. - Apply jittering and dodging to avoid overplotting. - Add a second layer showing the aggregated dots for each day. - Display error bars showing the standard error of the mean for each point on the aggregated layer. 2. **Plot 2: Faceted Scatter Plot** - Using the `glue` dataset. - Create a faceted scatter plot with `Score` on the x-axis and `Model` on the y-axis. - Use `Task` to create facets wrapping across multiple rows. - Map the `Year` column to color and the `Encoder` to the marker shape. - Apply custom scaling for markers and color palette. **Function Signature:** ```python import seaborn.objects as so from seaborn import load_dataset def create_advanced_plots(): # Load the datasets tips = load_dataset(\\"tips\\") glue = load_dataset(\\"glue\\") # Plot 1: Custom Scatter Plot p1 = ( so.Plot(tips, x=\\"total_bill\\", y=\\"day\\", color=\\"sex\\") .add(so.Dot(), so.Dodge(), so.Jitter(0.2)) .add(so.Dot(), so.Agg()) .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) ) p1.show() # Plot 2: Faceted Scatter Plot p2 = ( so.Plot(glue, \\"Score\\", \\"Model\\") .facet(\\"Task\\", wrap=4) .limit(x=(-5, 105)) .add(so.Dot(pointsize=6), color=\\"Year\\", marker=\\"Encoder\\") .scale(marker=[\\"o\\", \\"s\\"], color=\\"flare\\") ) p2.show() ``` **Input:** - No input is required for this function. **Output:** - The function should display two complex plots as described. **Constraints:** - Use seaborn\'s `objects` module for plotting. - Ensure to follow the specifications for axis labels, coloring, jittering, dodging, facet wrapping, and error bars accurately. **Note:** - It\'s essential to demonstrate a solid understanding of seaborn\'s customization features and correctly handling visual encodings for complex data representations.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_advanced_plots(): # Load the datasets tips = load_dataset(\\"tips\\") glue = load_dataset(\\"glue\\") # Plot 1: Custom Scatter Plot p1 = ( so.Plot(tips, x=\\"total_bill\\", y=\\"day\\", color=\\"sex\\") .add(so.Dot(), so.Dodge(), so.Jitter(0.2)) .add(so.Dot(), so.Agg()) .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) ) p1.show() # Plot 2: Faceted Scatter Plot p2 = ( so.Plot(glue, \\"Score\\", \\"Model\\") .facet(\\"Task\\", wrap=4) .limit(x=(-5, 105)) .add(so.Dot(pointsize=6), color=\\"Year\\", marker=\\"Encoder\\") .scale(marker=[\\"o\\", \\"s\\"], color=\\"flare\\") ) p2.show()"},{"question":"**Question: Implementing and Evaluating a Multi-layer Perceptron (MLP) for Classification** You are given a dataset containing features and binary target labels for a classification task. Your objective is to implement and evaluate a Multi-layer Perceptron (MLP) model using scikit-learn. # Dataset The dataset consists of two files: 1. `features.csv`: A CSV file where each row represents a sample and each column represents a feature. 2. `labels.csv`: A CSV file containing the binary target labels corresponding to each sample in `features.csv`. # Task 1. **Preprocess the Data:** - Load the features and labels from the CSV files. - Standardize the feature values to have a mean of 0 and a variance of 1. 2. **Implement the MLP Classifier:** - Use the `MLPClassifier` from scikit-learn. - Configure the MLP with the following parameters: - `hidden_layer_sizes=(50,)` - `solver=\'adam\'` - `alpha=1e-4` - `learning_rate_init=0.001` - `max_iter=300` - `random_state=42` 3. **Train and Evaluate the Model:** - Split the standardized data into training and testing sets using an 80-20 split. - Train the MLP classifier on the training set. - Evaluate the classifier\'s performance on the test set and print the following metrics: - Accuracy - Precision - Recall - F1 Score - Use the `classification_report` function from scikit-learn to output a detailed report. # Expected Input and Output - **Input:** Paths to the `features.csv` and `labels.csv` files. - **Output:** Printed evaluation metrics (accuracy, precision, recall, F1 score) and the detailed classification report. # Constraints - You must use scikit-learn\'s `MLPClassifier` and relevant preprocessing tools. - The solution should handle any potential issues with missing or malformed data elegantly. # Code Template ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import classification_report def load_data(features_path, labels_path): X = pd.read_csv(features_path) y = pd.read_csv(labels_path) return X, y def preprocess_data(X): scaler = StandardScaler() X_scaled = scaler.fit_transform(X) return X_scaled def train_and_evaluate(X_train, y_train, X_test, y_test): clf = MLPClassifier( hidden_layer_sizes=(50,), solver=\'adam\', alpha=1e-4, learning_rate_init=0.001, max_iter=300, random_state=42 ) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) report = classification_report(y_test, y_pred) print(report) def main(features_path, labels_path): X, y = load_data(features_path, labels_path) X_scaled = preprocess_data(X) X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) train_and_evaluate(X_train, y_train, X_test, y_test) if __name__ == \\"__main__\\": features_path = \'path/to/features.csv\' labels_path = \'path/to/labels.csv\' main(features_path, labels_path) ``` Ensure that you replace `path/to/features.csv` and `path/to/labels.csv` with the actual paths to your dataset files. Your solution should correctly load, preprocess, train, and evaluate the MLP classifier on the provided dataset.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import classification_report def load_data(features_path, labels_path): X = pd.read_csv(features_path) y = pd.read_csv(labels_path) return X, y def preprocess_data(X): scaler = StandardScaler() X_scaled = scaler.fit_transform(X) return X_scaled def train_and_evaluate(X_train, y_train, X_test, y_test): clf = MLPClassifier( hidden_layer_sizes=(50,), solver=\'adam\', alpha=1e-4, learning_rate_init=0.001, max_iter=300, random_state=42 ) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) report = classification_report(y_test, y_pred, output_dict=True) metrics = { \'accuracy\': report[\'accuracy\'], \'precision\': report[\'1\'][\'precision\'], \'recall\': report[\'1\'][\'recall\'], \'f1_score\': report[\'1\'][\'f1-score\'], \'classification_report\': report } return metrics def main(features_path, labels_path): X, y = load_data(features_path, labels_path) X_scaled = preprocess_data(X) X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) metrics = train_and_evaluate(X_train, y_train, X_test, y_test) print(\\"Accuracy:\\", metrics[\'accuracy\']) print(\\"Precision:\\", metrics[\'precision\']) print(\\"Recall:\\", metrics[\'recall\']) print(\\"F1 Score:\\", metrics[\'f1_score\']) print(\\"Detailed Classification Report:n\\", metrics[\'classification_report\']) if __name__ == \\"__main__\\": features_path = \'path/to/features.csv\' labels_path = \'path/to/labels.csv\' main(features_path, labels_path)"},{"question":"# Python 2 to 3 Compatibility Function Implementation Objective You are required to write a Python function that is compatible with both Python 2 and Python 3. The function will read a file, convert its content to uppercase, and write the converted content to a new file. The function should handle both text and binary data appropriately and demonstrate the use of future imports, feature detection, and other strategies discussed in the provided document. Function Signature ```python def convert_file_to_uppercase(input_filename: str, output_filename: str, is_binary: bool) -> None: ``` Input - `input_filename` (str): The path to the input file. - `output_filename` (str): The path where the output file will be written. - `is_binary` (bool): A flag indicating whether the file is in binary mode. If set to `True`, the file should be treated as binary data; if `False`, as text data. Output The function should not return anything but should create an output file with content converted to uppercase. Constraints - The function must be compatible with both Python 2.7 and Python 3.x. - The function must handle both text and binary data appropriately. - Use `io.open` for file operations. - Utilize feature detection where appropriate. - Implement necessary future imports. Example Assume `example.txt` contains the text \\"Hello World\\". ```python convert_file_to_uppercase(\'example.txt\', \'output.txt\', is_binary=False) ``` After execution, `output.txt` should contain \\"HELLO WORLD\\". If the content of `example.bin` is binary data containing \\"Hello World\\": ```python convert_file_to_uppercase(\'example.bin\', \'output.bin\', is_binary=True) ``` After execution, `output.bin` should contain binary data equivalent to \\"HELLO WORLD\\". Notes You are encouraged to use the strategies mentioned in the provided document, such as using future imports, the `six` module, and appropriate handling of text and binary data.","solution":"from __future__ import absolute_import, division, print_function, unicode_literals import io import sys def convert_file_to_uppercase(input_filename, output_filename, is_binary): if is_binary: read_mode, write_mode = \'rb\', \'wb\' else: read_mode, write_mode = \'r\', \'w\' with io.open(input_filename, read_mode) as infile: content = infile.read() if is_binary: # For binary data we need to handle the bytes properly transformed_content = content.upper() else: # For text data we decode to handle characters properly transformed_content = content.upper() with io.open(output_filename, write_mode) as outfile: outfile.write(transformed_content)"},{"question":"# Coding Assessment: Enhanced Date-Time Calculator **Objective**: Implement a function that calculates the next N working days from a given date, considering weekends and public holidays. **Problem Statement**: You are required to write a Python function `calculate_next_working_day(start_date: str, num_days: int, public_holidays: List[str]) -> str` that calculates the date N working days from a given start date. The working days are Monday to Friday, excluding any public holidays. **Function Signature**: ```python def calculate_next_working_day(start_date: str, num_days: int, public_holidays: List[str]) -> str: pass ``` # Parameters: - `start_date` (str): The starting date in \\"YYYY-MM-DD\\" format. - `num_days` (int): The number of working days to add to the start date. - `public_holidays` (List[str]): A list of public holidays in \\"YYYY-MM-DD\\" format. # Returns: - `str`: The calculated date in \\"YYYY-MM-DD\\" format after adding the specified number of working days, excluding weekends and public holidays. # Constraints: - `start_date` will be a valid date string in \\"YYYY-MM-DD\\" format. - `num_days` will be a non-negative integer. - Elements of `public_holidays` will be valid date strings in \\"YYYY-MM-DD\\" format. - Assume a consistent Gregorian calendar. # Requirements: - You are not allowed to use any third-party date manipulation libraries such as `dateutil`. Use the standard library `datetime` module only. - Consider both \\"aware\\" and \\"naive\\" date objects appropriately. - Handle edge cases such as starting on a weekend or the number of holidays affecting the result. # Example: ```python from typing import List def calculate_next_working_day(start_date: str, num_days: int, public_holidays: List[str]) -> str: # Your implementation here # Example usage: print(calculate_next_working_day(\\"2023-01-10\\", 5, [\\"2023-01-11\\", \\"2023-01-12\\"])) # Expected output: \\"2023-01-19\\" ``` # Explanation: In the example given, the function calculates the date 5 working days after January 10, 2023, considering January 11 and January 12 as public holidays. The resulting date should be January 19, 2023, skipping weekends and the specified public holidays. **Notes**: 1. Weekends (Saturday and Sunday) should be excluded. 2. Public holidays provided in the list should be excluded. 3. Ensure the function handles various edge cases correctly. # Evaluation Criteria: - Correctness of the implementation. - Proper use of `datetime` module functionalities. - Code readability and appropriate comments. - Handling of edge cases. Good luck!","solution":"from datetime import datetime, timedelta from typing import List def calculate_next_working_day(start_date: str, num_days: int, public_holidays: List[str]) -> str: # Convert start_date to datetime object start_date_dt = datetime.strptime(start_date, \\"%Y-%m-%d\\") # Convert public_holidays to set of datetime objects public_holidays_dt = set(datetime.strptime(date, \\"%Y-%m-%d\\") for date in public_holidays) current_date = start_date_dt while num_days > 0: current_date += timedelta(days=1) # Check if the new current_date is a working day if current_date.weekday() < 5 and current_date not in public_holidays_dt: num_days -= 1 return current_date.strftime(\\"%Y-%m-%d\\")"},{"question":"# Question: Dynamic Class and Type Management in Python In this task, you will leverage the `types` module in Python 3.10 to dynamically create and manage classes and their interactions. Task Overview 1. **Dynamic Class Creation**: Create two new classes dynamically: - `Animal`: Base class with a method `speak()` that raises a `NotImplementedError`. - `Dog` and `Cat`: Subclasses of `Animal` each with an overridden `speak()` method. `Dog` should return `\\"Woof!\\"` and `Cat` should return `\\"Meow!\\"`. 2. **Class Interaction**: Implement a function `interact_with_animals(animals: list)` that: - Takes a list of `Animal` instances. - Prints the result of their `speak()` method. 3. **Using a Read-Only Proxy**: Protect the list of animals using `types.MappingProxyType` to ensure it cannot be modified after creation. Input - You will be given a list of dictionaries, each representing an animal with its type (either `\\"Dog\\"` or `\\"Cat\\"`) and an optional `name`. Output - Print the names and sounds of each animal. Constraints - Ensure that dynamically created classes have the appropriate methods and inheritance. - Use `types.MappingProxyType` to protect the list of animals from modification. - Implement robust error handling if an invalid type is given. Example Given the input: ```python animals = [ {\\"type\\": \\"Dog\\", \\"name\\": \\"Buddy\\"}, {\\"type\\": \\"Cat\\", \\"name\\": \\"Mittens\\"} ] ``` The function should output: ``` Buddy says: Woof! Mittens says: Meow! ``` Implementation ```python import types def create_animal_classes(): # Step 1: Creating the Animal base class dynamically def animal_exec(ns): def speak(self): raise NotImplementedError(\\"Subclasses must implement this method.\\") ns[\'speak\'] = speak Animal = types.new_class(\'Animal\', (), {}, animal_exec) # Step 2: Creating Dog and Cat subclasses dynamically def dog_exec(ns): def speak(self): return \\"Woof!\\" ns[\'speak\'] = speak def cat_exec(ns): def speak(self): return \\"Meow!\\" ns[\'speak\'] = speak Dog = types.new_class(\'Dog\', (Animal,), {}, dog_exec) Cat = types.new_class(\'Cat\', (Animal,), {}, cat_exec) return Animal, Dog, Cat def interact_with_animals(animal_dicts): Animal, Dog, Cat = create_animal_classes() animal_classes = {\\"Dog\\": Dog, \\"Cat\\": Cat} animals = [] for animal_dict in animal_dicts: animal_type = animal_dict.get(\\"type\\") name = animal_dict.get(\\"name\\", \\"Unknown\\") cls = animal_classes.get(animal_type) if cls is None: raise ValueError(f\\"Unknown animal type: {animal_type}\\") animal_instance = cls() animals.append((name, animal_instance)) protected_animals = types.MappingProxyType({name: animal_instance for name, animal_instance in animals}) for name, animal in protected_animals.items(): print(f\\"{name} says: {animal.speak()}\\") # Example usage animal_dicts = [ {\\"type\\": \\"Dog\\", \\"name\\": \\"Buddy\\"}, {\\"type\\": \\"Cat\\", \\"name\\": \\"Mittens\\"} ] interact_with_animals(animal_dicts) ```","solution":"import types def create_animal_classes(): Dynamically creates Animal and its subclasses Dog and Cat. Returns the Animal, Dog, and Cat classes. # Create the Animal base class dynamically def animal_exec(ns): def speak(self): raise NotImplementedError(\\"Subclasses must implement this method.\\") ns[\'speak\'] = speak Animal = types.new_class(\'Animal\', (), {}, animal_exec) # Create Dog subclass dynamically def dog_exec(ns): def speak(self): return \\"Woof!\\" ns[\'speak\'] = speak Dog = types.new_class(\'Dog\', (Animal,), {}, dog_exec) # Create Cat subclass dynamically def cat_exec(ns): def speak(self): return \\"Meow!\\" ns[\'speak\'] = speak Cat = types.new_class(\'Cat\', (Animal,), {}, cat_exec) return Animal, Dog, Cat def interact_with_animals(animal_dicts): Takes a list of dictionaries with animal types and names, creates animal instances, and prints their sounds. Animal, Dog, Cat = create_animal_classes() animal_classes = {\\"Dog\\": Dog, \\"Cat\\": Cat} animals = [] for animal_dict in animal_dicts: animal_type = animal_dict.get(\\"type\\") name = animal_dict.get(\\"name\\", \\"Unknown\\") cls = animal_classes.get(animal_type) if cls is None: raise ValueError(f\\"Unknown animal type: {animal_type}\\") animal_instance = cls() animals.append((name, animal_instance)) protected_animals = types.MappingProxyType({name: animal_instance for name, animal_instance in animals}) for name, animal in protected_animals.items(): print(f\\"{name} says: {animal.speak()}\\")"},{"question":"<|Analysis Begin|> The provided documentation covers the `io` module of Python extensively. The `io` module provides functionality for handling text, binary, and raw I/O. The main abstractions involve `IOBase` and its derived classes such as `RawIOBase`, `BufferedIOBase`, and `TextIOBase`. There are functionalities for reading, writing, and buffering both text and binary data. Key classes include: - `IOBase`: The abstract base class for all I/O classes. - `RawIOBase`: Base class for raw binary streams. - `BufferedIOBase`: Base class for binary streams that support buffering. - `TextIOBase`: Base class for text streams. - Concrete implementations like `FileIO`, `BufferedReader`, `BufferedWriter`, `TextIOWrapper`, `BytesIO`, `StringIO`. Key information that can be included in the question: - Handling text and binary data using the provided `io` module classes. - Operations like reading, writing, seeking, flushing, and determining stream capabilities (like readability, writability, seekability). - Support for various encodings, newline handling, and buffering mechanisms. - Use of `with` statements for automatic resource management. - Different I/O classes like `BytesIO` for binary streams and `StringIO` for text streams. <|Analysis End|> <|Question Begin|> # Python Coding Assessment You are required to implement a function that processes and transforms data from a text file and a binary file using the `io` module functionality. Problem Statement 1. **Text File Processing**: - You have a text file named `textdata.txt`. Each line of the file is a sentence. - Implement a function `process_text_file(input_filename: str, output_filename: str) -> None` that reads the contents of `input_filename`, converts each line to uppercase, and writes the result to `output_filename`. - Ensure that you handle newline characters properly (using `universal newlines` mode). 2. **Binary File Processing**: - You have a binary file named `binarydata.bin` that contains raw binary data. - Implement a function `process_binary_file(input_filename: str, output_filename: str) -> None` that reads the contents of `input_filename` into a `BytesIO` buffer, transforms the data by appending a specific byte string (e.g., `b\'x00xFF\'`), and writes the result to `output_filename`. - Ensure you handle buffering efficiently. Constraints 1. For the text file processing: - Each line has at most 10,000 characters. - The file size will not exceed 10MB. 2. For the binary file processing: - The file size will not exceed 10MB. - You must use a `BytesIO` stream to perform the transformation. Function Signatures ```python def process_text_file(input_filename: str, output_filename: str) -> None: pass def process_binary_file(input_filename: str, output_filename: str) -> None: pass ``` # Example Usage ```python # Example usage for processing text file: process_text_file(\'textdata.txt\', \'output_textdata.txt\') # Example usage for processing binary file: process_binary_file(\'binarydata.bin\', \'output_binarydata.bin\') ``` # Performance Requirements - Your solution should complete the processing within a reasonable time for the given input constraints. # Tips: - Make sure to use the appropriate classes from the `io` module. - Always handle files using the context managers (`with` statements) to ensure proper resource management. - Pay special attention to encoding when dealing with text files. Good luck!","solution":"import io def process_text_file(input_filename: str, output_filename: str) -> None: Reads a text file line by line, converts each line to uppercase, and writes the result to another file. Args: - input_filename: str, name of the input text file. - output_filename: str, name of the output text file. Returns: - None with open(input_filename, \'r\', encoding=\'utf-8\', newline=None) as infile: with open(output_filename, \'w\', encoding=\'utf-8\', newline=\'n\') as outfile: for line in infile: outfile.write(line.upper()) def process_binary_file(input_filename: str, output_filename: str) -> None: Reads a binary file, appends a specific byte string to its content, and writes the result to another file. Args: - input_filename: str, name of the input binary file. - output_filename: str, name of the output binary file. Returns: - None append_bytes = b\'x00xFF\' with open(input_filename, \'rb\') as infile: binary_data = infile.read() with io.BytesIO() as buffer: buffer.write(binary_data) buffer.write(append_bytes) buffer.seek(0) with open(output_filename, \'wb\') as outfile: outfile.write(buffer.read())"},{"question":"You are tasked with creating a mini-utility using the `email.utils` module that processes a series of email-related tasks. The utility should perform the following functions: 1. **Generate a Localized Datetime String**: - Write a function `generate_localized_datetime_string(dt: datetime) -> str` that takes a `datetime` object `dt` (it may be naive or aware) and returns a string formatted according to RFC 2822 representing the local time. 2. **Create a Unique Message ID**: - Write a function `create_message_id(idstring: Optional[str] = None, domain: Optional[str] = None) -> str` that generates a unique message ID suitable for an RFC 2822-compliant header. The function should incorporate the optional `idstring` and `domain` if provided. 3. **Parse and Format Email Addresses**: - Write a function `parse_and_format_address(fieldvalue: str) -> str` that accepts a single string `fieldvalue` representing an email address field value (e.g., `\\"John Doe <john@example.com>\\"`). The function should: - Parse the address using `parseaddr`. - Format it back to a string using `formataddr`. - Return the formatted string. 4. **Parse Date String and Convert to Datetime**: - Write a function `convert_to_datetime(date_str: str) -> datetime` that takes a date string `date_str` in RFC 2822 format and converts it to a `datetime` object. If parsing fails, the function should raise a `ValueError`. # Constraints - Do not use any external libraries other than the standard `email.utils` and related Python modules. - Handle exceptions and edge cases gracefully, providing meaningful error messages where applicable. # Input and Output 1. **generate_localized_datetime_string**: - **Input**: A `datetime` object. - **Output**: String formatted according to RFC 2822 in local time. 2. **create_message_id**: - **Input**: Options `idstring` (str or None) and `domain` (str or None). - **Output**: A unique message ID string. 3. **parse_and_format_address**: - **Input**: A string representing an email address field. - **Output**: A formatted string version of the email address. 4. **convert_to_datetime**: - **Input**: A string date in RFC 2822 format. - **Output**: A `datetime` object. ```python import email.utils from datetime import datetime def generate_localized_datetime_string(dt: datetime) -> str: # Your code here pass def create_message_id(idstring: Optional[str] = None, domain: Optional[str] = None) -> str: # Your code here pass def parse_and_format_address(fieldvalue: str) -> str: # Your code here pass def convert_to_datetime(date_str: str) -> datetime: # Your code here pass ``` Complete the implementation of these functions using the `email.utils` module based on the provided functionality descriptions.","solution":"import email.utils import socket import time from datetime import datetime, timezone from typing import Optional def generate_localized_datetime_string(dt: datetime) -> str: Returns a string formatted according to RFC 2822 representing the local time. if dt.tzinfo is None: dt = dt.replace(tzinfo=timezone.utc).astimezone() timeval = dt.utctimetuple() timestamp = time.mktime(timeval) return email.utils.format_datetime(datetime.fromtimestamp(timestamp, dt.tzinfo)) def create_message_id(idstring: Optional[str] = None, domain: Optional[str] = None) -> str: Generates a unique message ID suitable for an RFC 2822-compliant header. if idstring is None: idstring = email.utils.make_msgid() if domain is None: domain = socket.getfqdn() return email.utils.make_msgid(idstring, domain) def parse_and_format_address(fieldvalue: str) -> str: Parses the address using parseaddr and formats it back to a string using formataddr. name, address = email.utils.parseaddr(fieldvalue) return email.utils.formataddr((name, address)) def convert_to_datetime(date_str: str) -> datetime: Takes a date string in RFC 2822 format and converts it to a datetime object. Raises ValueError if parsing fails. try: return email.utils.parsedate_to_datetime(date_str) except (TypeError, ValueError): raise ValueError(f\\"Invalid date string: {date_str}\\")"},{"question":"# Virtual Environment and Package Management Task Objective You are required to demonstrate your understanding of Python virtual environments and package management using `pip`. Instructions 1. **Creating a Virtual Environment**: - Create a virtual environment named `test-env` in your current directory. 2. **Activating the Virtual Environment**: - Activate the virtual environment `test-env`. 3. **Package Installation**: - Install the following packages into the virtual environment: - `requests` version `2.25.1` - `numpy` version `1.19.5` 4. **Generating requirements.txt**: - Generate a `requirements.txt` file that contains the installed packages and their versions. 5. **Verification Script**: - Write a Python script `verify_env.py` that checks whether the virtual environment `test-env` is set up correctly and the required packages with correct versions are installed. Script Requirements - The script `verify_env.py` should: 1. Verify if the `test-env` virtual environment is active. 2. Check if `requests` version `2.25.1` and `numpy` version `1.19.5` are installed. 3. Print \\"Environment setup correctly\\" if the environment and packages are correct. 4. Otherwise, print appropriate error messages. Constraints - Your solution must work on Unix, MacOS, or Windows. - Assume the Python version is at least Python 3.6. Submission - A text file containing the commands you ran to create, activate, and manage the virtual environment and packages. - The `requirements.txt` file generated. - The `verify_env.py` Python script. Example of what you need to submit: ``` # commands.txt python -m venv test-env source test-env/bin/activate pip install requests==2.25.1 pip install numpy==1.19.5 pip freeze > requirements.txt # verify_env.py import os import subprocess def is_venv_active(): return os.getenv(\'VIRTUAL_ENV\') is not None def check_package_version(package, version): try: import pkg_resources pkg_resources.get_distribution(f\'{package}=={version}\') return True except pkg_resources.DistributionNotFound: return False def main(): if not is_venv_active(): print(\\"Error: Virtual environment \'test-env\' is not active.\\") return if not check_package_version(\'requests\', \'2.25.1\'): print(\\"Error: \'requests\' version 2.25.1 is not installed.\\") return if not check_package_version(\'numpy\', \'1.19.5\'): print(\\"Error: \'numpy\' version 1.19.5 is not installed.\\") return print(\\"Environment setup correctly\\") if __name__ == \\"__main__\\": main() ```","solution":"import os import subprocess def is_venv_active(): return os.getenv(\'VIRTUAL_ENV\') is not None def check_package_version(package, version): try: import pkg_resources pkg_resources.get_distribution(f\'{package}=={version}\') return True except pkg_resources.DistributionNotFound: return False def main(): if not is_venv_active(): print(\\"Error: Virtual environment \'test-env\' is not active.\\") return if not check_package_version(\'requests\', \'2.25.1\'): print(\\"Error: \'requests\' version 2.25.1 is not installed.\\") return if not check_package_version(\'numpy\', \'1.19.5\'): print(\\"Error: \'numpy\' version 1.19.5 is not installed.\\") return print(\\"Environment setup correctly\\") if __name__ == \\"__main__\\": main()"},{"question":"Objective Your task is to implement a function that performs multiple window operations on a given pandas DataFrame. Function Signature ```python def window_operations(df: pd.DataFrame, window_size: int, min_periods: int) -> pd.DataFrame: Perform various window operations on the input DataFrame. Parameters: - df (pd.DataFrame): The input DataFrame with numerical columns. - window_size (int): The size of the rolling window. - min_periods (int): Minimum number of observations in window required to have a value. Returns: - pd.DataFrame: A DataFrame containing the results of the window operations. pass ``` Input - A pandas DataFrame `df` with numerical columns. - An integer `window_size` representing the size of the rolling window. - An integer `min_periods` representing the minimum number of observations in the window required to calculate a value. Output - A pandas DataFrame containing the results of the following window operations: - Rolling mean - Rolling sum - Rolling standard deviation - Rolling variance - Rolling median - Expanding mean - Expanding sum Each of the above operations should be computed for every numerical column in the input DataFrame. The output DataFrame should have the same index as the input DataFrame with appropriate column names indicating the operation performed. Example ```python import pandas as pd data = { \\"A\\": [1, 2, 3, 4, 5], \\"B\\": [5, 4, 3, 2, 1], } df = pd.DataFrame(data) window_size = 3 min_periods = 1 result = window_operations(df, window_size, min_periods) print(result) ``` Expected output: ``` A_rolling_mean B_rolling_mean A_rolling_sum B_rolling_sum A_rolling_std B_rolling_std A_rolling_var B_rolling_var A_rolling_median B_rolling_median A_expanding_mean B_expanding_mean A_expanding_sum B_expanding_sum 0 1.000000 5.000000 1.0 5.0 NaN NaN NaN NaN 1.0 5.0 1.0 5.0 1 5 1 1.500000 4.500000 3.0 9.0 0.707107 0.707107 0.500000 0.500000 1.5 4.5 1.5 4.5 3 9 2 2.000000 4.000000 6.0 12.0 1.000000 1.000000 1.000000 1.000000 2.0 4.0 2.0 4.0 6 12 3 3.000000 3.000000 9.0 9.0 1.000000 1.000000 1.000000 1.000000 3.0 3.0 2.5 3.5 10 14 4 4.000000 2.000000 12.0 6.0 1.000000 1.000000 1.000000 1.000000 4.0 2.0 3.0 3.0 15 15 ``` # Constraints - The input DataFrame will contain at least one numerical column and at least one row. - The `window_size` and `min_periods` will be positive integers with `window_size >= min_periods`. # Notes - Ensure that the resulting DataFrame has descriptive column names to indicate the operation applied, such as `A_rolling_mean`, `B_rolling_sum`, etc. - Handle any NaN values that may arise due to insufficient data in the window by leaving them as NaN.","solution":"import pandas as pd def window_operations(df: pd.DataFrame, window_size: int, min_periods: int) -> pd.DataFrame: Perform various window operations on the input DataFrame. Parameters: - df (pd.DataFrame): The input DataFrame with numerical columns. - window_size (int): The size of the rolling window. - min_periods (int): Minimum number of observations in window required to have a value. Returns: - pd.DataFrame: A DataFrame containing the results of the window operations. result = pd.DataFrame(index=df.index) for col in df.columns: result[f\'{col}_rolling_mean\'] = df[col].rolling(window=window_size, min_periods=min_periods).mean() result[f\'{col}_rolling_sum\'] = df[col].rolling(window=window_size, min_periods=min_periods).sum() result[f\'{col}_rolling_std\'] = df[col].rolling(window=window_size, min_periods=min_periods).std() result[f\'{col}_rolling_var\'] = df[col].rolling(window=window_size, min_periods=min_periods).var() result[f\'{col}_rolling_median\'] = df[col].rolling(window=window_size, min_periods=min_periods).median() result[f\'{col}_expanding_mean\'] = df[col].expanding(min_periods=min_periods).mean() result[f\'{col}_expanding_sum\'] = df[col].expanding(min_periods=min_periods).sum() return result"},{"question":"**Coding Question: Extending `imghdr` to Recognize a New Image Format** The `imghdr` module in Python allows you to determine the type of an image file. It supports various image formats out of the box, such as GIF, JPEG, PNG, and more. However, it might not support some newer or user-defined image formats. Your task is to extend the `imghdr` module to recognize a fictional new image format called \\"XY Image Format\\" (with the file extension `.xyimg`). This image format starts with the bytes `b\'XYIMG\'`. Implement a function `register_xyimg_recognition()` that adds support for this XY Image Format within the `imghdr` module. # Function Specification register_xyimg_recognition **Description:** This function registers a new test with the `imghdr` module to recognize the XY Image Format. The test must return the string \'xyimg\' when it successfully identifies a file of this format. **Constraints:** - The function must add a new function to `imghdr.tests` that checks if the image data starts with `b\'XYIMG\'`. - The recognition function should work both when the file is passed directly and when a byte stream is given. **Example:** ```python register_xyimg_recognition() # Test case when file named \'image.xyimg\' contains the bytes starting with b\'XYIMG\' print(imghdr.what(\'image.xyimg\')) # Output should be \'xyimg\' # Test case when byte stream starts with b\'XYIMG\' print(imghdr.what(None, b\'XYIMG1234\')) # Output should be \'xyimg\' ``` # Test Setup To aid your testing, you can create a sample file `image.xyimg` with the following content: ``` XYIMG followed by any content. ``` # Note You do not actually need to create files while testing - you can mock file operations or directly pass byte streams. # Bonus (Optional) As a bonus task, implement a small test suite to validate that your `register_xyimg_recognition` function works correctly.","solution":"import imghdr def register_xyimg_recognition(): def test_xyimg(h, f): header = h[:5] if header == b\'XYIMG\': return \'xyimg\' return None imghdr.tests.append(test_xyimg)"},{"question":"# Advanced Python 3.10 asyncio Programming Assessment In this assessment, you will demonstrate your understanding of the asyncio library and advanced asynchronous programming concepts in Python. Specifically, you will need to implement an asynchronous solution that involves running tasks concurrently, using correct exception handling, and ensuring all coroutine tasks are appropriately awaited. Problem Statement: You are required to build a small simulation of a task processing system using asyncio. This system will process multiple tasks concurrently, including I/O-bound and CPU-bound tasks. Your objective is to ensure that the entire system runs efficiently without blocking the event loop, handles exceptions appropriately, and uses logging to track task execution. Requirements: 1. **Task Definitions**: - Create a coroutine function named `io_task` that simulates an I/O-bound operation by sleeping for a random period between 1 to 3 seconds. Use `asyncio.sleep`. - Create a CPU-bound task simulation function named `cpu_task` that computes the sum of squares of numbers from 1 to 10^6. This function should run in an executor to avoid blocking the event loop. 2. **Task Processor**: - Implement a function named `process_tasks` that accepts an integer `n` representing the total number of tasks to process. - Randomly decide whether each task to be processed is an `io_task` or `cpu_task`. Use a 70:30 ratio for I/O-bound to CPU-bound tasks. - Ensure all tasks are started concurrently and are properly awaited. 3. **Exception Handling**: - Ensure that any exception raised in any task is caught and logged properly without stopping the processing of other tasks. 4. **Logging**: - Configure logging to print DEBUG level messages to the console. - Log the start and completion of each task with the task type and task id. 5. **Debug Mode**: - Ensure to enable asyncio\'s debug mode for the entire application. Constraints: - Use Python 3.10 or higher. - Your implementation should ensure the event loop is never blocked by long-running tasks. - Handle all coroutines correctly to avoid runtime warnings for unawaited coroutines. Expected Output: The function `process_tasks(n)` where `n=10` might produce debug logs similar to: ``` DEBUG:root:Starting I/O task 1 DEBUG:root:Starting CPU task 2 DEBUG:root:Starting I/O task 3 DEBUG:root:Starting CPU task 4 ... (other task starts) DEBUG:root:Completed I/O task 1 DEBUG:root:Completed CPU task 2 DEBUG:root:Completed I/O task 3 DEBUG:root:Completed CPU task 4 ... (other task completions) ``` Implementation: ```python import asyncio import logging import random from concurrent.futures import ThreadPoolExecutor # Setup logging logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(__name__) # Enable debug mode for asyncio asyncio.run(asyncio.get_event_loop().set_debug(debug=True)) async def io_task(task_id): logger.debug(f\\"Starting I/O task {task_id}\\") await asyncio.sleep(random.randint(1, 3)) logger.debug(f\\"Completed I/O task {task_id}\\") def cpu_task(task_id): logger.debug(f\\"Starting CPU task {task_id}\\") result = sum(i * i for i in range(1, 10**6 + 1)) logger.debug(f\\"Completed CPU task {task_id}\\") async def process_tasks(n): loop = asyncio.get_running_loop() with ThreadPoolExecutor() as executor: tasks = [] for task_id in range(1, n + 1): if random.random() < 0.7: tasks.append(io_task(task_id)) else: tasks.append(loop.run_in_executor(executor, cpu_task, task_id)) await asyncio.gather(*tasks, return_exceptions=True) # Example usage if __name__ == \\"__main__\\": asyncio.run(process_tasks(10)) ``` You must implement this provided template, ensuring all required features and constraints are met.","solution":"import asyncio import logging import random from concurrent.futures import ThreadPoolExecutor # Setup logging logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(__name__) # Enable debug mode for asyncio asyncio.get_event_loop().set_debug(True) async def io_task(task_id): logger.debug(f\\"Starting I/O task {task_id}\\") await asyncio.sleep(random.randint(1, 3)) logger.debug(f\\"Completed I/O task {task_id}\\") def cpu_task(task_id): logger.debug(f\\"Starting CPU task {task_id}\\") result = sum(i * i for i in range(1, 10**6 + 1)) logger.debug(f\\"Completed CPU task {task_id}\\") async def process_tasks(n): loop = asyncio.get_running_loop() with ThreadPoolExecutor() as executor: tasks = [] for task_id in range(1, n + 1): try: if random.random() < 0.7: tasks.append(io_task(task_id)) else: tasks.append(loop.run_in_executor(executor, cpu_task, task_id)) except Exception as e: logger.error(f\\"Error scheduling task {task_id}: {e}\\") results = await asyncio.gather(*tasks, return_exceptions=True) for i, result in enumerate(results): if isinstance(result, Exception): logger.error(f\\"Task {i+1} raised an exception: {result}\\") # Example usage if needed # if __name__ == \\"__main__\\": # asyncio.run(process_tasks(10))"},{"question":"You are tasked with implementing a set of Python functions that mimic the functionality of some functions described in the Python C API documentation for string conversion and formatting. Specifically, you need to implement functions for formatted output to strings, string to float conversion, and case-insensitive string comparisons. Function 1: formatted_output Write a function `formatted_output(fmt: str, *args: tuple) -> str` that mimics the behavior of PyOS_snprintf. The function should format and output a string according to the format string `fmt` and additional arguments `*args`. - `fmt`: A format string. - `args`: Additional arguments to format according to `fmt`. **Constraints**: - The length of the formatted output should not exceed 100 characters. If it does, raise a `ValueError`. - Ensure the output is null-terminated (in Python, this means implicitly ensuring strings are properly handled and terminated). - The format string can include standard printf-like formatting options. **Example**: ```python fmt = \\"Hello, %s. You have %d new messages.\\" print(formatted_output(fmt, \\"Alice\\", 5)) # \\"Hello, Alice. You have 5 new messages.\\" ``` Function 2: string_to_float Write a function `string_to_float(s: str) -> float` that converts a string to a floating-point number while mimicking the PyOS_string_to_double function. - `s`: The input string representing a floating-point number. **Constraints**: - The function should be locale-independent. - Raise a `ValueError` if the string is not a valid representation of a floating-point number. - If the string value is too large, return `float(\'inf\')` or `-float(\'inf\')` accordingly. **Example**: ```python print(string_to_float(\\"123.456\\")) # 123.456 print(string_to_float(\\"1e500\\")) # inf print(string_to_float(\\"not_a_number\\")) # Raises ValueError ``` Function 3: case_insensitive_strcmp Write a function `case_insensitive_strcmp(s1: str, s2: str) -> int` that performs a case-insensitive comparison of two strings. - `s1`: The first input string. - `s2`: The second input string. **Constraints**: - Return 0 if equal, a negative number if `s1` is less than `s2`, or a positive number if `s1` is greater than `s2`. - You cannot use the built-in functions `str.casefold()` or `str.lower()` directly within the comparison logic. **Example**: ```python print(case_insensitive_strcmp(\\"Hello\\", \\"hello\\")) # 0 print(case_insensitive_strcmp(\\"abc\\", \\"ABC\\")) # 0 print(case_insensitive_strcmp(\\"abc\\", \\"def\\")) # a negative number ``` Implement these functions ensuring they handle all edge cases mentioned. Write your code in a robust and readable manner.","solution":"def formatted_output(fmt: str, *args: tuple) -> str: Format and output a string according to the format string fmt and additional arguments. The length of the output should not exceed 100 characters. formatted_str = fmt % args if len(formatted_str) > 100: raise ValueError(\\"Formatted string exceeds 100 characters.\\") return formatted_str def string_to_float(s: str) -> float: Convert a string to a floating-point number. Locale-independent conversion. try: result = float(s) if result == float(\'inf\') or result == -float(\'inf\'): return result return result except ValueError: raise ValueError(f\\"Invalid floating-point representation: {s}\\") def case_insensitive_strcmp(s1: str, s2: str) -> int: Perform a case-insensitive comparison of two strings. Return 0 if equal, a negative number if s1 < s2, or a positive number if s1 > s2. s1_lower = \'\'.join(chr(ord(c) | 32) if \'A\' <= c <= \'Z\' else c for c in s1) s2_lower = \'\'.join(chr(ord(c) | 32) if \'A\' <= c <= \'Z\' else c for c in s2) if s1_lower == s2_lower: return 0 return (s1_lower > s2_lower) - (s1_lower < s2_lower)"},{"question":"Using the `decimal` module, write a function `precision_controlled_sum` that processes two lists of decimal numbers. The function should: 1. Take two lists of strings representing decimal numbers as input. 2. Convert these strings to `Decimal` objects. 3. Sum corresponding elements of the two lists using a specified precision and rounding method. 4. Return a list of these sums, ensuring the results respect the specified precision and rounding rules. Implement the following function: ```python from decimal import Decimal, Context, localcontext, ROUND_HALF_UP def precision_controlled_sum(list1, list2, precision, rounding): Sums corresponding elements of two lists with specified precision and rounding. Args: list1 (list of str): List of decimal number strings. list2 (list of str): List of decimal number strings. precision (int): The precision to use for the sum. rounding (str): The rounding mode to use (e.g., ROUND_HALF_UP, ROUND_DOWN, etc.). Returns: list of Decimal: List of summed decimals with specified precision and rounding. # Implement your function here # Example usage: list1 = [\'3.14159\', \'2.71828\', \'1.61803\'] list2 = [\'1.41421\', \'1.73205\', \'1.30357\'] precision = 5 rounding = ROUND_HALF_UP print(precision_controlled_sum(list1, list2, precision, rounding)) # Expected output: [Decimal(\'4.5558\'), Decimal(\'4.4503\'), Decimal(\'2.9216\')] ``` # Constraints: 1. Both input lists will have the same length. 2. The precision will be a positive integer. 3. Any valid `decimal` rounding mode can be provided. # Goals: 1. Ensure accurate and precise arithmetic by correctly managing contexts. 2. Handle all inputs robustly, including special edge cases like very large or very small numbers. 3. Apply rounding correctly to fit the specified precision. # Hints: 1. Use `localcontext()` to temporarily set a context for the arithmetic operations. 2. Remember to convert strings to `Decimal` objects before performing arithmetic operations. 3. Check the documentation on rounding modes if you are unsure about different rounding constants. # Notes: - This function should not print anything other than the expected output when run with the provided example. - Consider edge cases, such as handling very small or very large decimal values, and ensure your implementation is robust. Ensure that your solution is efficient and makes full use of the capabilities of the `decimal` module.","solution":"from decimal import Decimal, localcontext, ROUND_HALF_UP def precision_controlled_sum(list1, list2, precision, rounding): Sums corresponding elements of two lists with specified precision and rounding. Args: list1 (list of str): List of decimal number strings. list2 (list of str): List of decimal number strings. precision (int): The precision to use for the sum. rounding (str): The rounding mode to use (e.g., ROUND_HALF_UP, ROUND_DOWN, etc.). Returns: list of Decimal: List of summed decimals with specified precision and rounding. result = [] for s1, s2 in zip(list1, list2): d1 = Decimal(s1) d2 = Decimal(s2) with localcontext() as ctx: ctx.prec = precision ctx.rounding = rounding sum_val = d1 + d2 result.append(sum_val) return result # Example usage: list1 = [\'3.14159\', \'2.71828\', \'1.61803\'] list2 = [\'1.41421\', \'1.73205\', \'1.30357\'] precision = 5 rounding = ROUND_HALF_UP print(precision_controlled_sum(list1, list2, precision, rounding)) # Expected output: [Decimal(\'4.5558\'), Decimal(\'4.4503\'), Decimal(\'2.9216\')]"},{"question":"# Question: Implement a Thread-safe Counter using `_thread` Module You are tasked with implementing a thread-safe counter using the low-level threading primitives provided by the `_thread` module. The counter should support incrementing, decrementing, and retrieving the current value from multiple threads safely. This exercise will help you demonstrate an understanding of threading and synchronization. Requirements 1. **ThreadSafeCounter Class**: - Implement a class `ThreadSafeCounter` with the following methods: - `__init__()` - Initialize the counter to 0. - `increment()` - Increment the counter by 1. - `decrement()` - Decrement the counter by 1. - `get_value()` - Return the current value of the counter. - Use a lock to ensure that increments and decrements are thread-safe. 2. **Thread Usage**: - Use `_thread.start_new_thread()` to create multiple threads for testing the counter. - Demonstrate the use of `_thread.allocate_lock()` for synchronizing access to the counter. Input and Output Formats - **Input**: There are no direct inputs to your functions. You will create multiple threads that call the increment, decrement, and get_value methods on the `ThreadSafeCounter` instance. - **Output**: There should be no direct prints inside the class methods. However, you can print the counter\'s value at the end of the simulation for demonstration purposes. Constraints - Ensure that all methods in `ThreadSafeCounter` are thread-safe. - The counter should handle a large number of increments and decrements to demonstrate its thread safety. Example Usage ```python import _thread import time class ThreadSafeCounter: def __init__(self): self.counter = 0 self.lock = _thread.allocate_lock() def increment(self): with self.lock: self.counter += 1 def decrement(self): with self.lock: self.counter -= 1 def get_value(self): with self.lock: return self.counter # Testing the ThreadSafeCounter with multiple threads def increment_counter(counter, times): for _ in range(times): counter.increment() def decrement_counter(counter, times): for _ in range(times): counter.decrement() # Create an instance of the counter counter = ThreadSafeCounter() # Number of times to increment/decrement n = 1000 # Start threads for incrementing and decrementing _thread.start_new_thread(increment_counter, (counter, n)) _thread.start_new_thread(decrement_counter, (counter, n)) # Sleep for a while to let the threads finish time.sleep(1) # Print final counter value (should be 0 if increments and decrements are balanced) print(\\"Final counter value:\\", counter.get_value()) ``` Write your implementation of the `ThreadSafeCounter` class in the code block below. Ensure that your class adheres to the requirements specified above.","solution":"import _thread import time class ThreadSafeCounter: def __init__(self): self.counter = 0 self.lock = _thread.allocate_lock() def increment(self): with self.lock: self.counter += 1 def decrement(self): with self.lock: self.counter -= 1 def get_value(self): with self.lock: return self.counter # Helper functions to use in testing def increment_counter(counter, times): for _ in range(times): counter.increment() def decrement_counter(counter, times): for _ in range(times): counter.decrement() # Create an instance of the counter for demonstration purposes counter = ThreadSafeCounter() # Number of times to increment/decrement n = 1000 # Start threads for incrementing and decrementing _thread.start_new_thread(increment_counter, (counter, n)) _thread.start_new_thread(decrement_counter, (counter, n)) # Sleep for a while to let the threads finish time.sleep(1) # Print final counter value (should be 0 if increments and decrements are balanced) print(\\"Final counter value:\\", counter.get_value())"},{"question":"# Implementing and Manipulating Email Messages with the `email.message.Message` Class Objective This assessment aims to test your understanding of the `email.message.Message` class in Python, a part of the `email` module. You will implement functions to create an email message with specific headers and payloads, serialize it, and verify certain aspects of its content. Instructions 1. **Create a Message**: Implement a function `create_email_message` that constructs an email message with specific headers and a simple text payload. This function should return an instance of `email.message.Message`. 2. **Serialize Message**: Implement a function `serialize_email_message` that takes an instance of `email.message.Message` and returns its serialized string representation using the `as_string` method. Optionally, include the Unix `From_` line. 3. **Verify and Replace Header**: Implement a function `replace_email_header` that receives an instance of `email.message.Message`, a header name, and a new value. This function should replace the existing header value with the new value or add the header if it doesn\'t exist. It should return the updated message instance. 4. **Multipart Message with Attachments**: Implement a function `create_multipart_message_with_attachments` that creates a MIME multipart email message. This message should have a text part and one or more file attachments. The attachments can be represented as simple strings for the purpose of this task. Return the created instance of `email.message.Message`. 5. **Validate Message Structure**: Implement a function `validate_message_structure` that checks if a given email message is of type `multipart`, has a valid boundary, and contains the expected number of parts. It should return a boolean indicating if the message structure is valid. Function Signatures ```python def create_email_message(sender: str, recipient: str, subject: str, body: str) -> email.message.Message: pass def serialize_email_message(msg: email.message.Message, unixfrom: bool = False) -> str: pass def replace_email_header(msg: email.message.Message, header_name: str, new_value: str) -> email.message.Message: pass def create_multipart_message_with_attachments(sender: str, recipient: str, subject: str, body: str, attachments: list) -> email.message.Message: pass def validate_message_structure(msg: email.message.Message, expected_parts: int) -> bool: pass ``` Constraints and Requirements - **Constraints**: - You must use the `email.message.Message` class for creating and manipulating the email messages. - Ensure the headers conform to RFC standards as described in the documentation. - **Performance**: - Your code should efficiently handle email messages with large attachments. # Example Usage ```python # Function to create a simple email message msg = create_email_message(\'sender@example.com\', \'recipient@example.com\', \'Hello\', \'This is a simple email message.\') # Serialize the email message to a string email_str = serialize_email_message(msg, unixfrom=True) # Replace the subject header of the email message updated_msg = replace_email_header(msg, \'Subject\', \'New Subject\') # Create a multipart message with attachments multipart_msg = create_multipart_message_with_attachments(\'sender@example.com\', \'recipient@example.com\', \'With Attachment\', \'This email has attachments.\', [\'file1.txt\', \'file2.txt\']) # Validate the structure of the multipart message is_valid = validate_message_structure(multipart_msg, expected_parts=3) ``` Good luck!","solution":"from email.message import Message from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart from email.mime.base import MIMEBase from email import encoders def create_email_message(sender: str, recipient: str, subject: str, body: str) -> Message: msg = MIMEText(body) msg[\'From\'] = sender msg[\'To\'] = recipient msg[\'Subject\'] = subject return msg def serialize_email_message(msg: Message, unixfrom: bool = False) -> str: return msg.as_string(unixfrom=unixfrom) def replace_email_header(msg: Message, header_name: str, new_value: str) -> Message: msg.replace_header(header_name, new_value) return msg def create_multipart_message_with_attachments(sender: str, recipient: str, subject: str, body: str, attachments: list) -> Message: msg = MIMEMultipart() msg[\'From\'] = sender msg[\'To\'] = recipient msg[\'Subject\'] = subject msg.attach(MIMEText(body, \'plain\')) for attachment in attachments: part = MIMEBase(\'application\', \'octet-stream\') part.set_payload(attachment) encoders.encode_base64(part) part.add_header(\'Content-Disposition\', \'attachment; filename=\\"{}\\"\'.format(attachment)) msg.attach(part) return msg def validate_message_structure(msg: Message, expected_parts: int) -> bool: if msg.is_multipart(): if len(msg.get_payload()) == expected_parts: return True return False"},{"question":"**Coding Assessment Question** **Objective**: Demonstrate your understanding of the `email.generator` module by implementing a function that takes an email message object and writes its serialized representation to a specified output file. # Problem Statement You are given an `EmailMessage` object which represents an email. Your task is to write a function `serialize_email` that generates the serialized byte representation of this email using the `BytesGenerator` class and writes it to a specified file. # Function Signature ```python def serialize_email(msg: EmailMessage, output_file: str, unixfrom: bool = False, linesep: str = None, policy: Optional[email.policy.Policy] = None) -> None: pass ``` # Parameters - `msg` (EmailMessage): The email message object to be serialized. - `output_file` (str): The path to the file where the serialized email should be written. - `unixfrom` (bool, optional): If `True`, include the Unix \\"From \\" envelope header. Default is `False`. - `linesep` (str, optional): The line separator to use in the serialized email. Default is `None`. - `policy` (Optional[email.policy.Policy], optional): The policy to control message generation. Default is `None`. # Constraints - The function must use the `BytesGenerator` class from the `email.generator` module. - The output should handle non-ASCII characters correctly, ensuring compliance with email RFC standards. - The email should be written as binary data to the specified file. # Example Usage ```python from email.message import EmailMessage # Create a sample email message msg = EmailMessage() msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'recipient@example.com\' msg[\'Subject\'] = \'Test email\' msg.set_content(\'This is a test email.\') # Call the serialization function serialize_email(msg = msg, output_file = \'output.eml\', unixfrom = True) ``` # Notes - Ensure your function handles file I/O properly, including opening and closing the file. - Test your function with various email policies and encoding rules. Good luck!","solution":"from email.message import EmailMessage from email.generator import BytesGenerator import email.policy def serialize_email(msg: EmailMessage, output_file: str, unixfrom: bool = False, linesep: str = None, policy: email.policy.Policy = None) -> None: Serialize an email message and write to the specified output file. Args: msg (EmailMessage): The email message object to be serialized. output_file (str): The path to the file where the serialized email should be written. unixfrom (bool, optional): If True, include the Unix \\"From \\" envelope header. Default is False. linesep (str, optional): The line separator to use in the serialized email. Default is None. policy (Optional[email.policy.Policy], optional): The policy to control message generation. Default is None. # Using \'wb\' mode to write binary data with open(output_file, \'wb\') as outfile: generator = BytesGenerator(outfile, policy=policy) generator.flatten(msg, unixfrom=unixfrom, linesep=linesep)"},{"question":"**XML Data Processing with xml.dom.pulldom** You are given an XML document as a string representing a list of books, each with a title, author, genre, and price. Your task is to write a function that processes this XML string to extract and print the titles and authors of all books that belong to a specified genre. You should use the xml.dom.pulldom module to achieve this. # Function Signature ```python def extract_books_by_genre(xml_string: str, genre: str) -> None: pass ``` # Input - `xml_string` (str): A string representation of XML data containing book information. - `genre` (str): The genre of books that needs to be extracted. # Output - The function should print the titles and authors of books that match the specified genre, each on a new line, formatted as `Title: [title], Author: [author]`. # Example Suppose the input XML string is: ```xml <catalog> <book> <genre>Science Fiction</genre> <author>Isaac Asimov</author> <title>Foundation</title> <price>7.99</price> </book> <book> <genre>Fantasy</genre> <author>J.K. Rowling</author> <title>Harry Potter and the Philosopher\'s Stone</title> <price>5.99</price> </book> <book> <genre>Science Fiction</genre> <author>Arthur C. Clarke</author> <title>Rendezvous with Rama</title> <price>6.99</price> </book> </catalog> ``` Calling `extract_books_by_genre(xml_string, \\"Science Fiction\\")` should output: ``` Title: Foundation, Author: Isaac Asimov Title: Rendezvous with Rama, Author: Arthur C. Clarke ``` # Constraints - The provided XML string is well-formed. - The genre to be searched is case-sensitive. - The depth of the XML tree is not more than three levels deep. # Additional Notes You should use the `xml.dom.pulldom` module to parse the XML string and process the events. Use the `doc.expandNode(node)` method appropriately when needed to access child elements.","solution":"from xml.dom import pulldom def extract_books_by_genre(xml_string: str, genre: str) -> None: # Create a DOM parser events = pulldom.parseString(xml_string) # Initialize flags and variables for capturing data capture = False current_title = \\"\\" current_author = \\"\\" # Process the events for event, node in events: if event == pulldom.START_ELEMENT and node.tagName == \'book\': capture = False # Reset flag for capturing data within a new book element if event == pulldom.START_ELEMENT: if node.tagName == \'genre\': events.expandNode(node) if node.firstChild.nodeValue == genre: capture = True # Set flag if the current genre matches the target genre if event == pulldom.END_ELEMENT and node.tagName == \'book\' and capture: # Print the collected data print(f\\"Title: {current_title}, Author: {current_author}\\") if capture: if event == pulldom.START_ELEMENT: if node.tagName == \'title\': events.expandNode(node) current_title = node.firstChild.nodeValue elif node.tagName == \'author\': events.expandNode(node) current_author = node.firstChild.nodeValue"},{"question":"**Asynchronous Execution with Coroutines** **Objective:** Implement a function that simulates asynchronous tasks using Python\'s coroutine and asyncio features. This task will assess the understanding of coroutines and the ability to work with asynchronous programming. **Instructions:** 1. Create a coroutine function `async_fetch_data` that takes an integer identifier as input and prints \\"Fetching data for id {id}\\". 2. Simulate the fetching operation by asynchronously waiting (using `await asyncio.sleep`) for a random amount of time between 0.1 and 3 seconds. 3. After waiting, the coroutine should return a dictionary containing the identifier and a randomly generated number as \\"data\\". 4. Create another coroutine function `async_process_data` that takes a list of identifiers. 5. Inside `async_process_data`, use a list comprehension to concurrently fetch data using the `async_fetch_data` coroutine for each identifier in the input list. 6. Use the asyncio gather function to aggregate the results from the concurrent coroutines, then print the entire dataset returned by all coroutines. 7. Implement a `main` function that sets up the asyncio event loop to run the `async_process_data` function with a sample list of identifiers. **Constraints:** - Use the `asyncio` and `random` modules. - Ensure the `async_fetch_data` function correctly simulates asynchronous I/O by using `await asyncio.sleep`. - Handle and print any exceptions that occur during the coroutine executions appropriately. **Example:** ```python import asyncio import random async def async_fetch_data(identifier): print(f\\"Fetching data for id {identifier}\\") await asyncio.sleep(random.uniform(0.1, 3)) data = {\\"id\\": identifier, \\"data\\": random.randint(100, 1000)} return data async def async_process_data(identifiers): tasks = [async_fetch_data(id) for id in identifiers] results = await asyncio.gather(*tasks) print(\\"All data fetched:\\", results) def main(): identifiers = [1, 2, 3, 4, 5] asyncio.run(async_process_data(identifiers)) if __name__ == \\"__main__\\": main() ``` **Note:** - Ensure to import necessary modules such as `asyncio` and `random`. - You can use `random.uniform` and `random.randint` for generating random values. The solution must utilize asynchronous programming principles to facilitate concurrent task execution, leveraging Python\'s `asyncio` library efficiently.","solution":"import asyncio import random async def async_fetch_data(identifier): Simulates fetching data for a given identifier. print(f\\"Fetching data for id {identifier}\\") await asyncio.sleep(random.uniform(0.1, 3)) data = {\\"id\\": identifier, \\"data\\": random.randint(100, 1000)} return data async def async_process_data(identifiers): Concurrently fetches data for multiple identifiers and prints the results. tasks = [async_fetch_data(id) for id in identifiers] try: results = await asyncio.gather(*tasks) print(\\"All data fetched:\\", results) except Exception as e: print(f\\"An error occurred: {e}\\") def main(): Sets up the asyncio event loop to run the async_process_data function with a sample list of identifiers. identifiers = [1, 2, 3, 4, 5] asyncio.run(async_process_data(identifiers)) if __name__ == \\"__main__\\": main()"},{"question":"# Email Processing Exercise You are required to write a Python function that constructs an email message using the `email` package, manipulates it, and finally serializes it back to a string. Function Signature ```python def create_email_message(subject: str, sender: str, recipients: list, body: str) -> str: pass ``` Input - `subject` (str): The subject of the email. - `sender` (str): The sender\'s email address. - `recipients` (list): A list of recipient email addresses. - `body` (str): The body of the email. Output - Returns a string representation of the email. Requirements 1. Create an `EmailMessage` object and set its subject, sender, recipients, and body using the provided parameters. 2. Add a MIME type to the email to specify that the body is plain text. 3. Add a new header `X-Priority: 1` to indicate that this email is high priority. 4. Serialize the final email message to a string format. 5. Ensure the email is compliant with RFC 5322. Constraints - The email body must be a plain text. - The `body` should not exceed 10,000 characters. Example ```python subject = \\"Meeting Reminder\\" sender = \\"manager@example.com\\" recipients = [\\"employee1@example.com\\", \\"employee2@example.com\\"] body = \\"This is a reminder for the meeting scheduled at 10 AM tomorrow.\\" result = create_email_message(subject, sender, recipients, body) # The result should be a string representation of the email with all the headers and body correctly formatted. ``` Notes - Use the `EmailMessage` class and related functionality from the `email` package. - Add appropriate error handling for cases where the input does not meet specified constraints.","solution":"from email.message import EmailMessage def create_email_message(subject: str, sender: str, recipients: list, body: str) -> str: # Error handling for input constraints if not isinstance(subject, str): raise ValueError(\\"The subject must be a string.\\") if not isinstance(sender, str): raise ValueError(\\"The sender must be a string.\\") if not all(isinstance(recipient, str) for recipient in recipients): raise ValueError(\\"All recipient email addresses must be strings.\\") if not isinstance(body, str): raise ValueError(\\"The body must be a string.\\") if len(body) > 10000: raise ValueError(\\"The body should not exceed 10,000 characters.\\") # Create the EmailMessage object msg = EmailMessage() msg.set_content(body) msg[\\"Subject\\"] = subject msg[\\"From\\"] = sender msg[\\"To\\"] = \\", \\".join(recipients) msg[\\"X-Priority\\"] = \\"1\\" # Serialize the message to a string return msg.as_string()"},{"question":"# Advanced Python Exception Handling **Objective**: Implement a function that reads from a file and processes its content in various ways, demonstrating robust exception handling, custom exceptions, and clean-up operations. **Problem Statement**: You are required to write a Python function `process_file(file_path: str) -> None` that performs the following tasks: 1. **Read the file**: The function attempts to open and read the file specified by `file_path`. 2. **Process Content**: Each line of the file contains a mathematical expression (e.g., \\"3 + 5\\", \\"12 / 4\\", \\"-3 * 7\\"). The function evaluates each expression and prints the result. You must handle errors gracefully: - ZeroDivisionError: Print a message: \\"Error: Division by zero on line <line_number>: \'<expression>\'\\". - ValueError: Print a message if the expression is invalid: \\"Error: Invalid expression on line <line_number>: \'<expression>\'\\". - Any other exceptions should be logged with a message: \\"Unexpected error on line <line_number>: \'<expression>\'\\". 3. **Custom Exception**: Define a custom exception `FileFormatError` that is raised when the file is empty. 4. **Clean-up Action**: Ensure that the file is properly closed after reading, regardless of whether an error occurs. 5. **Logging**: Log all error messages to a file named \\"error_log.txt\\". **Implementation Details**: - The function should use the `with` statement to ensure files are properly closed. - Line numbers in the error messages should be 1-based. - Ensure that exceptions do not terminate the function prematurely; continue processing remaining lines. # Example Usage: ```python def process_file(file_path: str) -> None: # Your implementation goes here # Example file content: (stored in \\"sample.txt\\") # 3 + 5 # 12 / 4 # 8 / 0 # invalid expression # 3 * 7 # Example call: process_file(\\"sample.txt\\") ``` **Expected Output**: ```plaintext 8 3.0 Error: Division by zero on line 3: \'8 / 0\' Error: Invalid expression on line 4: \'invalid expression\' 21 ``` Additionally, \\"error_log.txt\\" should contain: ``` Error: Division by zero on line 3: \'8 / 0\' Error: Invalid expression on line 4: \'invalid expression\' ``` **Constraints**: - You can assume the file exists and is accessible. - Each line in the file is either a valid or invalid mathematical expression. - Performance requirements are not stringent, focus on correct error handling and logging.","solution":"import logging class FileFormatError(Exception): pass def process_file(file_path: str) -> None: logging.basicConfig(filename=\'error_log.txt\', level=logging.ERROR, format=\'%(message)s\') try: with open(file_path, \'r\') as file: lines = file.readlines() if not lines: raise FileFormatError(\\"File is empty\\") for idx, line in enumerate(lines, start=1): expression = line.strip() try: result = eval(expression) print(result) except ZeroDivisionError: error_msg = f\\"Error: Division by zero on line {idx}: \'{expression}\'\\" print(error_msg) logging.error(error_msg) except ValueError: error_msg = f\\"Error: Invalid expression on line {idx}: \'{expression}\'\\" print(error_msg) logging.error(error_msg) except Exception as e: error_msg = f\\"Unexpected error on line {idx}: \'{expression}\': {str(e)}\\" print(error_msg) logging.error(error_msg) except FileFormatError as e: print(str(e)) except Exception as e: print(f\\"An unexpected error occurred: {str(e)}\\")"},{"question":"Objective Design and implement a Python program using the `nntplib` module to interact with an NNTP server. The program should connect to an NNTP server, retrieve information from a specified newsgroup, and fetch the full content of the latest article. Additionally, it should handle various NNTP exceptions gracefully. Instructions 1. **Connect to an NNTP Server:** - Use the `news.gmane.io` NNTP server for this task. - You can use either plain connection (`NNTP` class) or an encrypted connection (`NNTP_SSL` class). 2. **Retrieve Newsgroup Information:** - Connect to a specified newsgroup (e.g., `gmane.comp.python.committers`). - Fetch and print out the number of articles, first and last article IDs in the newsgroup. 3. **Retrieve the Latest Article:** - Using the last article ID, fetch and print the full content of the latest article in the newsgroup. 4. **Decode Any Encoded Headers:** - Decode any encoded headers in the retrieved article using the `decode_header` utility function. 5. **Gracefully Handle Exceptions:** - Implement appropriate exception handling for `NNTPError`, `NNTPTemporaryError`, and `NNTPPermanentError`. Sample Output Your program should have an output similar to the following: ``` Connecting to the NNTP server \'news.gmane.io\'... Successfully connected. Retrieving newsgroup information for \'gmane.comp.python.committers\'... Newsgroup \'gmane.comp.python.committers\' has 1096 articles, range 1 to 1096. Fetching the latest article (ID: 1096)... Article 1096: Path: main.gmane.org!not-for-mail From: Neal Norwitz <neal@metaslash.com> Subject: Re: Hello fellow committers! Date: Sun, 12 Jan 2003 19:04:04 +0000 Message-ID: <20030112190404.GE29873@epoch.metaslash.com> Content: Hello everyone, This is a sample content of the latest article in the newsgroup. Best regards, Neal. Quitting the NNTP connection... Disconnected successfully. ``` Constraints - NNTP server: `news.gmane.io` - Newsgroup: `gmane.comp.python.committers` - Proper use of `nntplib` methods - Header decoding should handle non-ASCII characters Notes - Ensure the NNTP connection is properly closed after the operations. - Add necessary imports and any additional functions if needed. # Submission Submit a single Python file named `nntp_client.py` containing your solution.","solution":"import nntplib from email.header import decode_header import traceback def fetch_latest_article(newsgroup): try: print(\\"Connecting to the NNTP server \'news.gmane.io\'...\\") with nntplib.NNTP(\'news.gmane.io\') as nntp: print(\\"Successfully connected.n\\") print(f\\"Retrieving newsgroup information for \'{newsgroup}\'...\\") resp, count, first, last, name = nntp.group(newsgroup) print(f\\"Newsgroup \'{newsgroup}\' has {count} articles, range {first} to {last}.n\\") print(f\\"Fetching the latest article (ID: {last})...\\") resp, info = nntp.stat(last) resp, number, message_id, lines = nntp.body(last) print(f\\"Article {last}:\\") for line in lines: print(line.decode(\'utf-8\')) except nntplib.NNTPTemporaryError as e: print(f\\"Temporary NNTP error occurred: {e}\\") except nntplib.NNTPPermanentError as e: print(f\\"Permanent NNTP error occurred: {e}\\") except nntplib.NNTPError as e: print(f\\"NNTP error occurred: {e}\\") except Exception: print(\\"An unexpected error occurred:\\") traceback.print_exc() finally: print(\\"nQuitting the NNTP connection...\\") if __name__ == \\"__main__\\": newsgroup = \'gmane.comp.python.committers\' fetch_latest_article(newsgroup)"},{"question":"# Advanced Python with bz2 Compression Objective: Write a Python program that utilizes the `bz2` module to perform the following tasks: 1. Compress a given text into a bzip2 file. 2. Incrementally decompress data from a bzip2 file and output the decompressed content. Tasks: 1. **File Compression**: Write a function `compress_to_bzip2(file_path: str, text: str, compresslevel: int = 9) -> None` that compresses the given `text` into a bzip2 file specified by `file_path`. 2. **Incremental Decompression**: Write a function `incremental_decompress(file_path: str) -> str` that decompresses the content of a bzip2 file specified by `file_path` incrementally and returns the decompressed content as a single string. # Function Signatures: - `compress_to_bzip2(file_path: str, text: str, compresslevel: int = 9) -> None` - `incremental_decompress(file_path: str) -> str` # Input/Output Formats: **For `compress_to_bzip2()`:** - **Input:** - `file_path` (str): Path to the destination bzip2 file. - `text` (str): The text content to be compressed and written to the file. - `compresslevel` (int, optional): Compression level (default is 9). - **Output:** None **For `incremental_decompress()`:** - **Input:** - `file_path` (str): Path to the source bzip2 file to be decompressed. - **Output:** - Returns a string containing the decompressed content from the file. # Constraints: - The `text` can be of any length. - The file specified in `file_path` should be created or read from the current directory. - Ensure that the program handles large files efficiently. # Example Usage: ```python # Example text text = \\"This is an example of text that will be compressed using bzip2.\\" # Compress the example text into \'example.bz2\' compress_to_bzip2(\\"example.bz2\\", text) # Decompress the content of \'example.bz2\' incrementally decompressed_text = incremental_decompress(\\"example.bz2\\") print(decompressed_text) # Output should be the original `text` ``` # Guidelines: - Use the `bz2` module for compression and decompression. - Provide appropriate error handling for file operations. - The `compress_to_bzip2()` function should leverage the `bz2.open()` function or `BZ2File` class. - The `incremental_decompress()` function must demonstrate incremental decompression using the `BZ2Decompressor` class.","solution":"import bz2 def compress_to_bzip2(file_path: str, text: str, compresslevel: int = 9) -> None: Compress the given text into a bzip2 file specified by file_path. Args: - file_path (str): Path to the destination bzip2 file. - text (str): The text content to be compressed. - compresslevel (int, optional): Compression level (default is 9). Returns: - None with bz2.open(file_path, \'wt\', compresslevel=compresslevel) as bz2_file: bz2_file.write(text) def incremental_decompress(file_path: str) -> str: Incrementally decompress data from a bzip2 file and output the decompressed content. Args: - file_path (str): Path to the source bzip2 file to be decompressed. Returns: - str: The decompressed content. decompressed_content = [] decompressor = bz2.BZ2Decompressor() with open(file_path, \'rb\') as bz2_file: for data in iter(lambda: bz2_file.read(100 * 1024), b\'\'): decompressed_content.append(decompressor.decompress(data).decode(\'utf-8\')) return \'\'.join(decompressed_content)"},{"question":"Design a function that utilizes Python async/await syntax to perform asynchronous computations. The function should fetch data from multiple URLs concurrently, process this data, and return the results. This question will assess your understanding of Python\'s asynchronous programming facilities and your ability to work with coroutines. # Task You are required to implement two functions: 1. **fetch_data(url: str) -> dict**: - This function simulates an asynchronous network call to fetch data from a URL. - For simulation purposes, you can use `asyncio.sleep` to emulate network latency. - The function should return a dictionary with the URL as the key and the string \\"data from `<url>`\\" as the value. 2. **gather_data(urls: list) -> dict**: - This function should take a list of URLs, use the `fetch_data` function to fetch data from all URLs concurrently, and return a dictionary aggregating the results. - You should utilize the `asyncio.gather` method to run the network calls concurrently. # Constraints - You must use the `asyncio` library for asynchronous programming. - You are required to handle at least 5 URLs concurrently. - Do not use external libraries for HTTP requests; simulate network calls. # Example Usage ```python import asyncio async def fetch_data(url): # Your implementation here pass async def gather_data(urls): # Your implementation here pass # Example URLs urls = [ \\"http://example.com/api/resource1\\", \\"http://example.com/api/resource2\\", \\"http://example.com/api/resource3\\", \\"http://example.com/api/resource4\\", \\"http://example.com/api/resource5\\", ] # Running the gather_data function result = asyncio.run(gather_data(urls)) print(result) ``` # Expected Output ```python { \\"http://example.com/api/resource1\\": \\"data from http://example.com/api/resource1\\", \\"http://example.com/api/resource2\\": \\"data from http://example.com/api/resource2\\", \\"http://example.com/api/resource3\\": \\"data from http://example.com/api/resource3\\", \\"http://example.com/api/resource4\\": \\"data from http://example.com/api/resource4\\", \\"http://example.com/api/resource5\\": \\"data from http://example.com/api/resource5\\", } ``` # Notes - Make sure to handle any potential exceptions that might arise during asynchronous execution. - Ensure your code is efficient and concise, leveraging Python\'s async/await capabilities to the fullest.","solution":"import asyncio async def fetch_data(url: str) -> dict: Simulate an asynchronous network call to fetch data from a URL. Parameters: url (str): The URL to fetch data from. Returns: dict: A dictionary with the URL as the key and the fetched data as the value. await asyncio.sleep(1) # Simulate network latency return {url: f\\"data from {url}\\"} async def gather_data(urls: list) -> dict: Fetch data from multiple URLs concurrently. Parameters: urls (list): A list of URLs to fetch data from. Returns: dict: A dictionary aggregating the fetched data from all URLs. tasks = [fetch_data(url) for url in urls] results = await asyncio.gather(*tasks) aggregated_data = {} for result in results: aggregated_data.update(result) return aggregated_data"},{"question":"**Question:** # Complex Data Processing and Transformation Task You are tasked with processing a stream of data chunks received asynchronously. Each chunk represents a list of dictionaries, each dictionary containing data in key-value pairs. Your goal is to transform and aggregate this data in a specific way to ultimately produce a useful summary report. # Requirements: 1. **Transformation**: - Each dictionary contains various numeric, string, and complex values. - Normalize the numeric values using their absolute values. - For complex values, convert them to their magnitude. - For string values, convert them to their ASCII-safe representation. - Create a tuple of transformed values for each dictionary. 2. **Aggregation**: - Aggregate transformed tuples across multiple chunks. - Calculate the sum of all numeric values. - Concatenate transformed string values separated by a comma. - Collect all distinct ASCII-safe string representations from complex values. 3. **Report Generation**: - Generate a summary report as a dictionary containing: - \\"numeric_sum\\": Sum of all numeric values. - \\"concatenated_strings\\": Concatenation of all string values. - \\"distinct_complex_strings\\": Set of distinct ASCII-safe string representations from complex values. # Function Signature ```python async def process_data_chunks(data_chunks: List[List[Dict[str, Any]]]) -> Dict[str, Any]: ``` # Input - **data_chunks**: A list of lists where each inner list contains dictionaries with keys of type str (`string`) and values of varying types (`numeric`, `complex`, `string`). # Output - A dictionary containing the summary report specified above. # Constraints - You must use built-in Python functions and classes as much as possible. - Performance should be optimized for potentially large datasets. # Example ```python data_chunks = [ [{\\"a\\": 5, \\"b\\": 3+4j, \\"c\\": \\"hello\\"}, {\\"a\\": -10, \\"b\\": 2-2j, \\"c\\": \\"world\\"}], [{\\"a\\": 6.5, \\"b\\": 1, \\"c\\": \\"test\\"}] ] summary_report = await process_data_chunks(data_chunks) # Expected example output: # { # \\"numeric_sum\\": 29.5, # \\"concatenated_strings\\": \\"hello,world,test\\", # \\"distinct_complex_strings\\": {\\"3.605551275463989\\", \\"2.23606797749979\\", \\"1.0\\"} # } ``` # Hints - To deal with numeric values including complex numbers, use `abs()` or convert to magnitude. - Use `ascii()` for string values and complex number representations. - Utilize list comprehensions and built-in functions like `sum()`, `set()` and `\\",\\".join()` for aggregation and concatenation tasks. # Implementation ```python async def process_data_chunks(data_chunks): numeric_sum = 0 concatenated_strings = [] distinct_complex_strings = set() for chunk in data_chunks: for item in chunk: transformed_tuple = tuple( abs(value) if isinstance(value, (int, float)) else abs(value) if isinstance(value, complex) else ascii(value) if isinstance(value, str) else value for key, value in item.items() ) for value in transformed_tuple: if isinstance(value, (int, float)): numeric_sum += value elif isinstance(value, str): concatenated_strings.append(value) elif isinstance(value, complex): distinct_complex_strings.add(ascii(abs(value))) summary_report = { \\"numeric_sum\\": numeric_sum, \\"concatenated_strings\\": \\",\\".join(concatenated_strings), \\"distinct_complex_strings\\": distinct_complex_strings } return summary_report ``` **Note**: The implementation provided is just a starting point. You need to ensure the correctness and performance of your final solution, considering the asynchronous nature of the input.","solution":"import asyncio from typing import List, Dict, Any async def process_data_chunks(data_chunks: List[List[Dict[str, Any]]]) -> Dict[str, Any]: numeric_sum = 0 concatenated_strings = [] distinct_complex_strings = set() for chunk in data_chunks: for item in chunk: for key, value in item.items(): if isinstance(value, (int, float)): numeric_sum += abs(value) elif isinstance(value, complex): distinct_complex_strings.add(ascii(abs(value))) elif isinstance(value, str): concatenated_strings.append(ascii(value)) summary_report = { \\"numeric_sum\\": numeric_sum, \\"concatenated_strings\\": \\",\\".join(concatenated_strings), \\"distinct_complex_strings\\": distinct_complex_strings } return summary_report"},{"question":"You are given a set of stereo audio fragments and you need to perform a series of operations to process these fragments. Your task is to write a function `process_audio_fragments(fragments: List[Tuple[bytes, int]]) -> bytes` which will do the following: 1. **Split the stereo fragments into two mono fragments** for each audio fragment. 2. **Apply different amplification factors** to the left and right channels. 3. **Recombine the mono fragments back into stereo fragments.** 4. **Reverse** the samples of the combined stereo fragments. 5. **Return the processed audio fragment** as a bytes-like object. The function receives input as a list of tuples. Each tuple contains: - `fragment`: a bytes object representing the stereo audio fragment. - `width`: an integer representing the width of the audio samples in bytes (either 1, 2, 3, or 4). # Constraints: - `fragments` contains at least one audio fragment. - The lengths of the stereo fragments are multiples of the sample width * 2. - Use amplification factors of 1.5 for the left channel and 0.75 for the right channel. - Ensure that the calculations handle potential overflow properly (truncate samples in case of overflow). # Example: ```python from typing import List, Tuple # Given stereo audio fragments fragments = [(b\'x01x02x03x04\', 2), (b\'x05x06x07x08\', 2)] # Width is 2 bytes (16-bit samples) # Function call result = process_audio_fragments(fragments) # Example output: processed and reversed byte sequence ``` # Implementation Outline: 1. Use `audioop.tomono()` to split stereo fragments. 2. Apply amplification factors using `audioop.mul()`. 3. Recombine fragments into stereo using `audioop.tostereo()`. 4. Reverse the combined stereo fragments using `audioop.reverse()`. 5. Return the processed fragments as a single bytes object, concatenating them if needed. Your implementation should utilize the `audioop` module effectively to perform necessary audio operations in a concise and efficient manner.","solution":"import audioop from typing import List, Tuple def process_audio_fragments(fragments: List[Tuple[bytes, int]]) -> bytes: processed_audio = b\'\' for fragment, width in fragments: # Split stereo fragment into mono fragments left_mono = audioop.tomono(fragment, width, 1, 0) right_mono = audioop.tomono(fragment, width, 0, 1) # Apply amplification factors: 1.5 for left channel, 0.75 for right channel amplified_left = audioop.mul(left_mono, width, 1.5) amplified_right = audioop.mul(right_mono, width, 0.75) # Recombine the mono fragments into stereo combined_stereo = audioop.tostereo(amplified_left, width, 1, 0) combined_stereo = audioop.add(combined_stereo, audioop.tostereo(amplified_right, width, 0, 1), width) # Reverse the samples of the combined stereo fragments reversed_combined = audioop.reverse(combined_stereo, width) # Append to the final processed audio processed_audio += reversed_combined return processed_audio"},{"question":"Implementing a Custom Logging Handler in Python # Objective Your task is to demonstrate proficiency with Python\'s `logging.handlers` module by implementing a custom logging handler. This custom handler should inherit from `logging.handlers.SocketHandler` and provide enhanced functionality for compressing log messages before sending them over the network. # Requirements 1. Implement a class `CompressedSocketHandler` that: - Inherits from `logging.handlers.SocketHandler`. - Compresses log messages using gzip before sending them over the socket. - Logs an error message (without re-raising the error) if the compression fails, and sends the uncompressed message in this case. 2. Implement the following methods in `CompressedSocketHandler`: - `emit(self, record)`: Override this method to add compression. - `compress_message(self, message)`: A utility method to compress a given message using gzip. # Input Format - The `emit` method takes a `LogRecord` object as input. # Output Format - The handler should send compressed log messages over the network. If compression fails, send the original uncompressed message and log an error. # Constraints - Assume that the network socket is already configured and operational when creating the `CompressedSocketHandler`. - The compressed messages should be sent as binary data. - The original uncompressed message should have a maximum length of 1024 bytes. # Example Usage ```python import logging import logging.handlers import gzip import socket import io class CompressedSocketHandler(logging.handlers.SocketHandler): def __init__(self, host, port): super().__init__(host, port) def emit(self, record): try: message = self.format(record) compressed_message = self.compress_message(message) self.send(compressed_message) except Exception as e: self.handleError(record) self.send(message.encode(\'utf-8\')) def compress_message(self, message): try: out = io.BytesIO() with gzip.GzipFile(fileobj=out, mode=\'wb\') as f: f.write(message.encode(\'utf-8\')) return out.getvalue() except Exception as e: logging.error(\\"Compression failed: %s\\", e) return message.encode(\'utf-8\') # Example Initialization if __name__ == \\"__main__\\": logger = logging.getLogger(\\"example\\") logger.setLevel(logging.DEBUG) compressed_socket_handler = CompressedSocketHandler(\'localhost\', 9999) logger.addHandler(compressed_socket_handler) logger.info(\\"This is a test log message.\\") ``` # Explanation - The `CompressedSocketHandler` inherits from `SocketHandler`. - The `compress_message` method compresses the log message using gzip. - The `emit` method attempts to compress the log message and send it over the socket. If compression fails, it logs an error and sends the original message instead. Good luck!","solution":"import logging import logging.handlers import gzip import socket import io class CompressedSocketHandler(logging.handlers.SocketHandler): def __init__(self, host, port): super().__init__(host, port) def emit(self, record): try: message = self.format(record) compressed_message = self.compress_message(message) self.send(compressed_message) except Exception as e: self.handleError(record) self.send(message.encode(\'utf-8\')) def compress_message(self, message): try: out = io.BytesIO() with gzip.GzipFile(fileobj=out, mode=\'wb\') as f: f.write(message.encode(\'utf-8\')) return out.getvalue() except Exception as e: logging.error(\\"Compression failed: %s\\", e) return message.encode(\'utf-8\')"},{"question":"You are required to create a custom SMTP server by extending the `smtpd.SMTPServer` class. This custom server should process incoming email messages and print out specific parts of the email content such as the sender, recipients, and email body. # Requirements: 1. Implement a class `CustomSMTPServer` that extends `smtpd.SMTPServer`. 2. Override the `process_message` method in `CustomSMTPServer`. 3. The `process_message` method should: - Print the sender’s email address. - Print the recipients\' email addresses. - Print the first 80 characters of the email body followed by an ellipsis (\\"...\\") if the email body is longer than 80 characters, otherwise print the entire body. - Return `None` for a normal \\"250 Ok\\" response. # Input Format: - An email sender address as a string (e.g., `\\"sender@example.com\\"`). - A list of recipient addresses as strings (e.g., `[\\"recipient1@example.com\\", \\"recipient2@example.com\\"]`). - An email body as a string that follows **RFC 5321** format. # Output Format: - The function should print: - The sender\'s email address. - The recipients\' email addresses. - The beginning of the email body as specified. # Constraints: - The server should handle emails up to a maximum size of 1 MB. - Do not enable the `SMTPUTF8` extension or set `decode_data` to `True`. # Example: ```python # Sample implementation outline import smtpd class CustomSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): print(f\\"Sender: {mailfrom}\\") print(f\\"Recipients: {\', \'.join(rcpttos)}\\") email_body = data[:80] + (\'...\' if len(data) > 80 else \'\') print(f\\"Email Body: {email_body}\\") return None # Test the code by creating an instance of CustomSMTPServer server = CustomSMTPServer((\'localhost\', 1025), (\'otherhost\', 1025)) ``` # Note: - Remember to disable any actual network I/O while testing. - You may simulate an incoming email by directly invoking the `process_message` method with test values.","solution":"import smtpd class CustomSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): print(f\\"Sender: {mailfrom}\\") print(f\\"Recipients: {\', \'.join(rcpttos)}\\") email_body = data[:80] + (\'...\' if len(data) > 80 else \'\') print(f\\"Email Body: {email_body}\\") return None"},{"question":"# Question: Task Write a function `convert_to_utc` that accepts two arguments: 1. A list of datetime strings in ISO 8601 format (`date_strings`). 2. A string representing the current timezone\'s offset from UTC in the format `±HH:MM` (`timezone_offset`). The function should convert each datetime string to its corresponding UTC time and return them as a list of strings in ISO 8601 format. Input - `date_strings` (list of str): List of datetime strings in ISO 8601 format. - `timezone_offset` (str): The current timezone\'s offset from UTC in the format `±HH:MM` (e.g., \\"+05:30\\", \\"-04:00\\"). Output - Return (list of str): List of datetime strings in ISO 8601 format representing the UTC times. Constraints - The `date_strings` list will contain between 1 and 1000 datetime strings. - Each datetime string will be in the valid format `YYYY-MM-DDTHH:MM:SS` or `YYYY-MM-DDTHH:MM:SS.ffffff`. - The `timezone_offset` string will be a valid offset. Example ```python date_strings = [\\"2023-03-28T14:30:00\\", \\"2023-03-28T18:45:00\\"] timezone_offset = \\"+02:00\\" print(convert_to_utc(date_strings, timezone_offset)) # Output: [\'2023-03-28T12:30:00\', \'2023-03-28T16:45:00\'] ``` ```python date_strings = [\\"2023-03-28T14:30:00.123456\\", \\"2023-03-28T18:45:00\\"] timezone_offset = \\"-04:00\\" print(convert_to_utc(date_strings, timezone_offset)) # Output: [\'2023-03-28T18:30:00.123456\', \'2023-03-28T22:45:00\'] ``` Notes - Ensure accurate conversion by accounting for both the hours and minutes in the `timezone_offset`. - You may assume that all datetime strings provided are naive (i.e., they do not contain timezone information).","solution":"from datetime import datetime, timedelta def convert_to_utc(date_strings, timezone_offset): def get_offset_timedelta(offset): sign = 1 if offset[0] == \'+\' else -1 hours = int(offset[1:3]) minutes = int(offset[4:6]) return sign * timedelta(hours=hours, minutes=minutes) offset_timedelta = get_offset_timedelta(timezone_offset) utc_dates = [] for date_string in date_strings: local_datetime = datetime.fromisoformat(date_string) utc_datetime = local_datetime - offset_timedelta utc_dates.append(utc_datetime.isoformat()) return utc_dates"},{"question":"Objective Demonstrate your understanding of Python\'s `copy` module by implementing a class that includes shallow and deep copy methods, and handle a compound object with nested mutable structures. Your implementation should showcase the difference between shallow and deep copies clearly. Problem Statement Define a Python class `Node` to represent nodes in a tree-like structure, where each node can have a list of children nodes. Your `Node` class should implement both shallow and deep copy methods (`__copy__` and `__deepcopy__`) following the `copy` module\'s conventions. Class Definition ```python class Node: def __init__(self, value): self.value = value self.children = [] def add_child(self, child_node): self.children.append(child_node) def __copy__(self): # Implement the shallow copy logic. pass def __deepcopy__(self, memo): # Implement the deep copy logic. pass ``` Requirements 1. **Initialization**: - The `Node` class should initialize with a `value` and an empty list of `children`. - Method `add_child` should allow appending children nodes to a node. 2. **Shallow Copy**: - Implement the `__copy__` method to create a shallow copy of the node. - Ensure the copied node and the original node share the same children list (i.e., modifying the children in the copy should affect the original). 3. **Deep Copy**: - Implement the `__deepcopy__` method to create a deep copy of the node. - Ensure that all nodes in the copied structure are independent of the original (i.e., modifications to the copied structure should not affect the original). 4. **Constraints**: - You must handle recursive structures; ensure no infinite recursion occurs. - Implement and use the `memo` dictionary correctly in the `__deepcopy__` method. 5. **Testing**: - Demonstrate the difference between shallow and deep copies with example cases. Print appropriate statements to show the shared or independent nature of copied nodes and their children. Example ```python original_node = Node(1) child_node = Node(2) original_node.add_child(child_node) import copy # Shallow Copy Example shallow_copy = copy.copy(original_node) print(shallow_copy.children is original_node.children) # Should be True # Deep Copy Example deep_copy = copy.deepcopy(original_node) print(deep_copy.children is original_node.children) # Should be False print(deep_copy.children[0] is original_node.children[0]) # Should be False ``` Constraints - Ensure the `__copy__` and `__deepcopy__` methods are correctly implemented. - Handle recursive copying without causing infinite loops. - Utilize the `copy` and `deepcopy` functions from the `copy` module appropriately.","solution":"import copy class Node: def __init__(self, value): self.value = value self.children = [] def add_child(self, child_node): self.children.append(child_node) def __copy__(self): new_node = type(self)(self.value) new_node.children = self.children return new_node def __deepcopy__(self, memo): new_node = type(self)(self.value) memo[id(self)] = new_node new_node.children = copy.deepcopy(self.children, memo) return new_node"},{"question":"**Problem Statement:** You are tasked with implementing an asynchronous printer queue manager using `asyncio` synchronization primitives. Multiple tasks need to send their print jobs to a shared printer, but the printer can only handle one job at a time. Print jobs must be processed in the order they were received (FIFO order). **Requirements:** 1. Implement an `asyncio`-based queue system to manage the print jobs. 2. Ensure that the printer processes only one job at a time. 3. Include functionality to add a new print job and notify when the job is completed. 4. Demonstrate the solution works with multiple tasks submitting print jobs concurrently. **Function Signatures:** ```python import asyncio from typing import Any class PrinterQueueManager: def __init__(self): Initializes the PrinterQueueManager with required asyncio primitives. pass async def add_print_job(self, job: Any) -> None: Adds a new print job to the queue. Args: job: The print job detail. Can be any data representative of the print job. pass async def _process_job(self) -> None: Process the next job in the queue. This should be called internally. pass async def run(self) -> None: Starts the printer queue manager and processes jobs as they come. pass async def main(): Demonstrates the PrinterQueueManager with multiple concurrent print job submissions. pqm = PrinterQueueManager() await pqm.add_print_job(\\"Job 1\\") await pqm.add_print_job(\\"Job 2\\") await pqm.add_print_job(\\"Job 3\\") await asyncio.gather( pqm.run(), pqm.add_print_job(\\"Job 4\\"), pqm.add_print_job(\\"Job 5\\"), ) if __name__ == \\"__main__\\": asyncio.run(main()) ``` **Constraints & Limitations:** - Ensure thread-safety is maintained even though asyncio primitives are not thread-safe. - The printer should be able to manage job submissions that are asynchronous and potentially coming from different parts of the code. **Performance Requirements:** - Efficiently manage the job queue such that each job is processed in FIFO order without blocking the main event loop. **Evaluation Criteria:** - Correctness: The implementation should ensure jobs are processed one at a time, and in the correct order. - Usage of asyncio synchronization primitives correctly. - Handling of concurrent job submissions gracefully.","solution":"import asyncio from typing import Any class PrinterQueueManager: def __init__(self): Initializes the PrinterQueueManager with required asyncio primitives. self.queue = asyncio.Queue() self.lock = asyncio.Lock() async def add_print_job(self, job: Any) -> None: Adds a new print job to the queue. Args: job: The print job detail. Can be any data representative of the print job. await self.queue.put(job) async def _process_job(self) -> None: Process the next job in the queue. This should be called internally. async with self.lock: while not self.queue.empty(): job = await self.queue.get() print(f\\"Processing: {job}\\") await asyncio.sleep(2) # Simulate time taken to process the job print(f\\"Completed: {job}\\") self.queue.task_done() async def run(self) -> None: Starts the printer queue manager and processes jobs as they come. await self._process_job() async def main(): Demonstrates the PrinterQueueManager with multiple concurrent print job submissions. pqm = PrinterQueueManager() async def submit_jobs(): await pqm.add_print_job(\\"Job 4\\") await pqm.add_print_job(\\"Job 5\\") await asyncio.gather( pqm.add_print_job(\\"Job 1\\"), pqm.add_print_job(\\"Job 2\\"), pqm.add_print_job(\\"Job 3\\"), submit_jobs(), pqm.run() ) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Question: Visualize and Compare Health Expenditures** You are provided with the `healthexp` dataset, which contains health expenditure data over several years for different countries. Your task is to write a Python function using the seaborn `objects` interface that performs the following: 1. Load the `healthexp` dataset. 2. Create a line plot that shows the annual health spending (in USD) for each country from 1970 to 2020. 3. Normalize the health spending data for each country relative to their maximum spending over the given period. 4. Create an additional plot that shows the percentage change in health spending for each country, using the spending in the year 1970 as the baseline. **Function Signature:** ```python def visualize_health_expenditures(): pass ``` **Expected Output:** Your function should display two plots: 1. A line plot of normalized health spending values for each country. 2. A line plot of percentage change in health spending from the year 1970 baseline. **Constraints:** - You should use the seaborn `objects` interface for creating the plots. - Ensure that the plots are properly labeled for clarity. **Hints:** - Use `so.Norm()` to normalize spending data. - Use the `where` argument in `so.Norm()` to set 1970 as the baseline year for the percentage change plot. - Use `.label()` to add labels to the y-axis of your plots. ```python import seaborn.objects as so from seaborn import load_dataset def visualize_health_expenditures(): # Load dataset healthexp = load_dataset(\\"healthexp\\") # Plot normalized health spending plot1 = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm()) .label(y=\\"Spending relative to maximum amount\\") ) plot1.show() # Plot percentage change in health spending from 1970 baseline plot2 = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"Year == 1970\\", percent=True)) .label(y=\\"Percent change in spending from 1970 baseline\\") ) plot2.show() ```","solution":"import seaborn.objects as so from seaborn import load_dataset def visualize_health_expenditures(): # Load dataset healthexp = load_dataset(\\"healthexp\\") # Normalize the data by dividing each country\'s spending by its maximum spending healthexp[\'Normalized_Spending\'] = healthexp.groupby(\'Country\')[\'Spending_USD\'].transform(lambda x: x / x.max()) # Calculate the percentage change from the year 1970 baseline baseline = healthexp[healthexp[\'Year\'] == 1970].set_index(\'Country\')[\'Spending_USD\'] healthexp[\'Baseline_Spending\'] = healthexp[\'Country\'].map(baseline) healthexp[\'Pct_Change_Spending\'] = (healthexp[\'Spending_USD\'] - healthexp[\'Baseline_Spending\']) / healthexp[\'Baseline_Spending\'] * 100 # Plot normalized health spending plot1 = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Normalized_Spending\\", color=\\"Country\\") .add(so.Line()) .label(y=\\"Spending relative to maximum amount\\") ) plot1.show() # Plot percentage change in health spending from 1970 baseline plot2 = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Pct_Change_Spending\\", color=\\"Country\\") .add(so.Line()) .label(y=\\"Percent change in spending from 1970 baseline\\") ) plot2.show()"},{"question":"# Question You are given a large text data file, `input.txt`, that contains a substantial amount of repeating text. Your task is to create a Python script that performs the following operations using the `bz2` module: 1. **Read the Content**: Read the content of `input.txt` into a single byte string. 2. **Compression**: Compress this byte string using both one-shot compression and incremental compression. Compare the sizes of the compressed outputs and calculate the compression ratios for each method. 3. **Save to File**: Write the output of the compression to two files – `compressed_one_shot.bz2` and `compressed_incremental.bz2`. 4. **Decompression**: Decompress the two files and verify whether the outputs match the original data read from `input.txt`. # Requirements 1. Implement the following functions: ```python def read_input_file(filename: str) -> bytes: pass def compress_data_one_shot(data: bytes, compresslevel: int = 9) -> bytes: pass def compress_data_incrementally(data: bytes, compresslevel: int = 9) -> bytes: pass def write_compressed_file(filename: str, data: bytes) -> None: pass def decompress_file(filename: str) -> bytes: pass def verify_data(original_data: bytes, decompressed_data: bytes) -> bool: pass ``` 2. Consider the input and output string length difference while calculating compression ratio. # Input and Output 1. `read_input_file(filename: str) -> bytes` - **Input**: A string representing the file path. - **Output**: The content of the file as a byte string. 2. `compress_data_one_shot(data: bytes, compresslevel: int = 9) -> bytes` - **Input**: A byte string and an integer representing the compression level. - **Output**: Compressed byte string using one-shot compression. 3. `compress_data_incrementally(data: bytes, compresslevel: int = 9) -> bytes` - **Input**: A byte string and an integer representing the compression level. - **Output**: Compressed byte string using incremental compression. 4. `write_compressed_file(filename: str, data: bytes) -> None` - **Input**: A string representing the file path and a byte string of compressed data. - **Output**: None (Writes the compressed data to the specified file). 5. `decompress_file(filename: str) -> bytes` - **Input**: A string representing the file path. - **Output**: Decompressed data as a byte string. 6. `verify_data(original_data: bytes, decompressed_data: bytes) -> bool` - **Input**: Two byte strings (original and decompressed data). - **Output**: Boolean indicating whether the decompressed data matches the original data. # Constraints and Performance Requirements - You must handle large files efficiently. - Ensure that the compression methods are implemented correctly to achieve substantial reduction in file size. - Your solution should be optimal in terms of both time and space complexity. # Example Usage ```python file_path = \'input.txt\' original_data = read_input_file(file_path) # One-shot compression compressed_one_shot = compress_data_one_shot(original_data) write_compressed_file(\'compressed_one_shot.bz2\', compressed_one_shot) # Incremental compression compressed_incremental = compress_data_incrementally(original_data) write_compressed_file(\'compressed_incremental.bz2\', compressed_incremental) # Decompress and verify decompressed_one_shot = decompress_file(\'compressed_one_shot.bz2\') decompressed_incremental = decompress_file(\'compressed_incremental.bz2\') assert verify_data(original_data, decompressed_one_shot) assert verify_data(original_data, decompressed_incremental) ``` Ensure your implementation is tested and verified with both text and binary data provided in `input.txt`.","solution":"import bz2 def read_input_file(filename: str) -> bytes: with open(filename, \'rb\') as file: return file.read() def compress_data_one_shot(data: bytes, compresslevel: int = 9) -> bytes: compressor = bz2.BZ2Compressor(compresslevel) compressed_data = compressor.compress(data) compressed_data += compressor.flush() return compressed_data def compress_data_incrementally(data: bytes, compresslevel: int = 9) -> bytes: compressor = bz2.BZ2Compressor(compresslevel) compressed_data = b\'\' for i in range(0, len(data), 1024): compressed_data += compressor.compress(data[i:i+1024]) compressed_data += compressor.flush() return compressed_data def write_compressed_file(filename: str, data: bytes) -> None: with open(filename, \'wb\') as file: file.write(data) def decompress_file(filename: str) -> bytes: with open(filename, \'rb\') as file: decompressor = bz2.BZ2Decompressor() return decompressor.decompress(file.read()) def verify_data(original_data: bytes, decompressed_data: bytes) -> bool: return original_data == decompressed_data"},{"question":"You are tasked with defining a class in TorchScript to manage and process a simple linear regression model. This class will handle data normalization, initialization of model parameters, prediction, and computing mean squared error loss. # Requirements 1. **Class Initialization**: - Initialize a TorchScript class named `LinearRegressionModel`. - The class will have parameters for weights `self.W` and bias `self.b`. 2. **Forward Method**: - Define a `forward` method for prediction which computes `y = W*x + b`. 3. **Normalization Method**: - Include a method named `normalize` that takes a `Tensor` input and returns it normalized (subtract mean and divide by standard deviation). 4. **Loss Computation**: - Implement a method named `compute_loss` which calculates the mean squared error. 5. **Type Annotations**: - Ensure all methods and parameters are correctly annotated using MyPy-style annotations. # Constraints - Use the `@torch.jit.script` decorator on the class. - Ensure the class and methods follow the TorchScript subset of Python. - Handle potential None values where applicable. # Input and Output Format - **Class Initialization**: No input parameters. - **Forward Method**: - Input: A `Tensor` of shape `(n,)` with dtype `float32`. - Output: A `Tensor` of shape `(n,)` with dtype `float32`. - **Normalization Method**: - Input: A `Tensor` of shape `(n,)` with dtype `float32`. - Output: A normalized `Tensor` of the same shape. - **Compute Loss Method**: - Input: Two `Tensors`, `predictions` and `targets`, both of shape `(n,)` with dtype `float32`. - Output: A `float` representing the mean squared error. # Example ```python import torch from typing import Optional @torch.jit.script class LinearRegressionModel: def __init__(self): self.W: torch.Tensor = torch.tensor([0.0], dtype=torch.float32) self.b: torch.Tensor = torch.tensor([0.0], dtype=torch.float32) def forward(self, x: torch.Tensor) -> torch.Tensor: return self.W * x + self.b def normalize(self, x: torch.Tensor) -> torch.Tensor: mean = torch.mean(x) std = torch.std(x) return (x - mean) / std def compute_loss(self, predictions: torch.Tensor, targets: torch.Tensor) -> float: return torch.mean((predictions - targets) ** 2).item() # Example usage: model = LinearRegressionModel() data = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0], dtype=torch.float32) normalized_data = model.normalize(data) predictions = model.forward(normalized_data) loss = model.compute_loss(predictions, data) print(f\\"Normalized Data: {normalized_data}\\") print(f\\"Predictions: {predictions}\\") print(f\\"Loss: {loss}\\") ``` Implement the `LinearRegressionModel` class as described above. Ensure your implementation adheres to the requirements and constraints provided.","solution":"import torch from typing import Optional @torch.jit.script class LinearRegressionModel: def __init__(self): self.W: torch.Tensor = torch.tensor([0.0], dtype=torch.float32) self.b: torch.Tensor = torch.tensor([0.0], dtype=torch.float32) def forward(self, x: torch.Tensor) -> torch.Tensor: return self.W * x + self.b def normalize(self, x: torch.Tensor) -> torch.Tensor: mean = torch.mean(x) std = torch.std(x) return (x - mean) / std def compute_loss(self, predictions: torch.Tensor, targets: torch.Tensor) -> float: return torch.mean((predictions - targets) ** 2).item() # Example usage: # model = LinearRegressionModel() # data = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0], dtype=torch.float32) # normalized_data = model.normalize(data) # predictions = model.forward(normalized_data) # loss = model.compute_loss(predictions, data) # print(f\\"Normalized Data: {normalized_data}\\") # print(f\\"Predictions: {predictions}\\") # print(f\\"Loss: {loss}\\")"},{"question":"Buffer Management: Backward Compatibility Python 3 provides enhanced buffer protocol functions for efficient memory handling. However, there\'s still a need to support old buffer protocol functions due to legacy codebases that require backward compatibility. In this task, you need to design a compatible interface that integrates both old and new buffer protocols. Objective: Implement functions to simulate the functionalities provided by the old buffer protocol using the new buffer protocol effectively. The interface should handle both read and write operations on memory buffers while ensuring resources are managed properly. Functions to Implement: 1. `as_char_buffer(obj) -> Tuple[Optional[bytes], Optional[int]]` - **Input:** A memory buffer object `obj`. - **Output:** A tuple containing: 1. A read-only bytes-like object. 2. The length of the buffer. - **Constraints:** If the object does not support a single-segment character buffer, return `(None, None)`. 2. `as_read_buffer(obj) -> Tuple[Optional[bytes], Optional[int]]` - **Input:** A memory buffer object `obj`. - **Output:** A tuple containing: 1. A read-only bytes-like object for arbitrary data. 2. The length of the buffer. - **Constraints:** If the object does not support a single-segment readable buffer, return `(None, None)`. 3. `check_read_buffer(obj) -> bool` - **Input:** A memory buffer object `obj`. - **Output:** A boolean indicating if the object supports a single-segment readable buffer. 4. `as_write_buffer(obj) -> Tuple[Optional[memoryview], Optional[int]]` - **Input:** A memory buffer object `obj`. - **Output:** A tuple containing: 1. A writable memoryview object. 2. The length of the buffer. - **Constraints:** If the object does not support a single-segment writable buffer, return `(None, None)`. Performance Requirements: - These functions must handle various types of buffer-producing objects (e.g., bytearrays, arrays, etc.). - Ensure proper memory management by releasing buffers when they are no longer needed. - Handle edge cases gracefully, including incompatible buffer objects. Example: ```python # Assuming your function implementation. byte_data = bytearray(b\'example\') # Read buffer char_buffer, char_buffer_len = as_char_buffer(byte_data) assert char_buffer == b\'example\' assert char_buffer_len == 7 # Check readable buffer assert check_read_buffer(byte_data) is True # Write buffer write_buffer, write_buffer_len = as_write_buffer(byte_data) write_buffer[:] = b\'changed\' # Modify the buffer content assert byte_data == b\'changed\' assert write_buffer_len == 7 ``` Note: You should not use the old buffer protocol functions directly. Ensure that the functions implemented work seamlessly with both old and new buffer protocols using the new protocol functions when possible.","solution":"def as_char_buffer(obj): Returns a read-only bytes-like object and the length of the buffer. try: buffer = memoryview(obj).tobytes() return (buffer, len(buffer)) except TypeError: return (None, None) def as_read_buffer(obj): Returns a read-only bytes-like object and the length of the buffer for arbitrary data. try: buffer = memoryview(obj).tobytes() return (buffer, len(buffer)) except TypeError: return (None, None) def check_read_buffer(obj): Returns a boolean indicating if the object supports a single-segment readable buffer. try: memoryview(obj) return True except TypeError: return False def as_write_buffer(obj): Returns a writable memoryview object and the length of the buffer. try: buffer = memoryview(obj).cast(\'B\') if not buffer.readonly: return (buffer, len(buffer)) else: return (None, None) except TypeError: return (None, None)"},{"question":"**Coding Assessment Question: Advanced Database Operations with the `dbm` Module** **Objective**: Demonstrate your understanding of Python\'s `dbm` module by implementing a function that performs various database operations. # Task You are required to implement a function `manage_database(file_name: str, operations: List[Tuple[str, Union[str, Tuple[str, str]]]]) -> Dict[str, Union[str, bool]]` that: 1. Opens (and if necessary, creates) a database using the `dbm` module. 2. Performs a series of operations passed as parameter `operations` on the database. 3. Returns a dictionary representing the result of each corresponding operation. Function Signature: ```python def manage_database(file_name: str, operations: List[Tuple[str, Union[str, Tuple[str, str]]]]) -> Dict[str, Union[str, bool]]: ``` Input: 1. `file_name` (str): The name of the database file. If the file does not exist, create it. 2. `operations` (List[Tuple[str, Union[str, Tuple[str, str]]]]): A list of operations to perform on the database. Each tuple in the list can be one of the following: - `(\\"get\\", key)`: Retrieve the value associated with `key` from the database. - `(\\"set\\", (key, value))`: Set the `value` for the given `key` in the database. - `(\\"delete\\", key)`: Delete the entry associated with `key` from the database. - `(\\"exists\\", key)`: Check if the `key` exists in the database. Output: - A dictionary where: - The keys are the operation descriptions (e.g., `\\"get key\\"`, `\\"set key\\"`, `\\"delete key\\"`, `\\"exists key\\"`). - The values are the results of the corresponding operations: - For `get` and `exists`, the value should be the retrieved value or `False` if not found. - For `set` and `delete`, the value should be `True` for success. - If a `get` operation is executed and the key does not exist, return the string `\\"Key not found\\"`. Constraints: - Key and value in the database are always strings that should be encoded as bytes before storage. - The implementation must handle exceptions gracefully and ensure the database is closed after completion. Example: ```python operations = [ (\\"set\\", (\\"name\\", \\"Alice\\")), (\\"get\\", \\"name\\"), (\\"set\\", (\\"age\\", \\"30\\")), (\\"get\\", \\"age\\"), (\\"exists\\", \\"name\\"), (\\"delete\\", \\"name\\"), (\\"exists\\", \\"name\\") ] result = manage_database(\\"testdb\\", operations) ``` Expected output: ```python { \\"set name\\": True, \\"get name\\": \\"Alice\\", \\"set age\\": True, \\"get age\\": \\"30\\", \\"exists name\\": True, \\"delete name\\": True, \\"exists name\\": False } ``` # Notes: - Make sure to handle the database interactions within a `with` statement to ensure proper closure. - You can use `dbm.open(file_name, \'c\')` to open and create the database if it doesn’t exist. **Good luck, and happy coding!**","solution":"import dbm from typing import List, Tuple, Union, Dict def manage_database(file_name: str, operations: List[Tuple[str, Union[str, Tuple[str, str]]]]) -> Dict[str, Union[str, bool]]: result = {} with dbm.open(file_name, \'c\') as db: for op in operations: if op[0] == \\"get\\": key = op[1] try: result[f\\"get {key}\\"] = db[key.encode()].decode() except KeyError: result[f\\"get {key}\\"] = \\"Key not found\\" elif op[0] == \\"set\\": key, value = op[1] db[key.encode()] = value.encode() result[f\\"set {key}\\"] = True elif op[0] == \\"delete\\": key = op[1] try: del db[key.encode()] result[f\\"delete {key}\\"] = True except KeyError: result[f\\"delete {key}\\"] = False elif op[0] == \\"exists\\": key = op[1] result[f\\"exists {key}\\"] = key.encode() in db return result"},{"question":"You are tasked with writing a Python script using the `urllib.robotparser` module to determine the crawling guidelines specified in a `robots.txt` file for a given website. # Objective: 1. Implement a class `RobotsTxtAnalyzer` that will: - Initialize with a URL to a `robots.txt` file. - Provide methods to fetch and parse the `robots.txt` file. - Provide methods to retrieve various parameters and determine crawling permissions based on the file. # Specifications: 1. **Initialization**: - `__init__(self, url: str)`: Initialize the instance with the URL of the `robots.txt` file. 2. **Methods**: - `fetch_robots_txt(self) -> None`: Reads the `robots.txt` file from the specified URL. - `can_fetch_url(self, useragent: str, url: str) -> bool`: Returns whether the specified user agent can fetch the given URL. - `get_crawl_delay(self, useragent: str) -> Union[int, None]`: Returns the \\"Crawl-delay\\" parameter for the specified user agent. - `get_request_rate(self, useragent: str) -> Union[Tuple[int, int], None]`: Returns the \\"Request-rate\\" parameter as a tuple (requests, seconds) for the specified user agent. - `get_site_maps(self) -> Union[List[str], None]`: Returns the list of site maps if specified in the `robots.txt` file. # Constraints: - The methods should handle cases where parameters are not specified or have invalid syntax gracefully by returning `None`. # Example Usage: ```python analyzer = RobotsTxtAnalyzer(\\"http://www.musi-cal.com/robots.txt\\") analyzer.fetch_robots_txt() print(analyzer.can_fetch_url(\\"*\\", \\"http://www.musi-cal.com/cgi-bin/search?city=San+Francisco\\")) # Output: False print(analyzer.get_crawl_delay(\\"*\\")) # Output: 6 print(analyzer.get_request_rate(\\"*\\")) # Output: (3, 20) print(analyzer.get_site_maps()) # Output: None ``` # Note: You should ensure to handle network errors and invalid URLs gracefully, possibly by raising appropriate exceptions with clear error messages.","solution":"import urllib.robotparser import urllib.request from typing import List, Tuple, Union class RobotsTxtAnalyzer: def __init__(self, url: str): Initializes the instance with the URL of the `robots.txt` file. self.url = url self.rp = urllib.robotparser.RobotFileParser() def fetch_robots_txt(self) -> None: Fetches and parses the `robots.txt` file from the specified URL. self.rp.set_url(self.url) self.rp.read() def can_fetch_url(self, useragent: str, url: str) -> bool: Returns whether the specified user agent can fetch the given URL. return self.rp.can_fetch(useragent, url) def get_crawl_delay(self, useragent: str) -> Union[int, None]: Returns the \\"Crawl-delay\\" parameter for the specified user agent. return self.rp.crawl_delay(useragent) def get_request_rate(self, useragent: str) -> Union[Tuple[int, int], None]: Returns the \\"Request-rate\\" parameter as a tuple (requests, seconds) for the specified user agent, or None if not specified. return self.rp.request_rate(useragent) def get_site_maps(self) -> Union[List[str], None]: Returns the list of site maps specified in the `robots.txt` file, or None if not specified. return self.rp.site_maps()"},{"question":"# Custom Representation with `reprlib` **Objective**: Implement a subclass of `reprlib.Repr` that customizes the string representation of specific object types and honors imposed size limitations. # Task: 1. **Subclass `reprlib.Repr`**: - Create a subclass named `CustomRepr` that inherits from `reprlib.Repr`. 2. **Override Representation Methods**: - Implement custom representations for dictionaries (`dict` type) and lists (`list` type). - For dictionaries, limit the number of key-value pairs displayed to `maxdict` (default is 4). - For lists, limit the number of elements displayed to `maxlist` (default is 6). 3. **Testing Your Implementation**: - Instantiate your `CustomRepr` class. - Override the `maxdict` and `maxlist` attributes directly and test your implementation with a dictionary and list that exceed these limits. - Verify that the representations honor the size limits set by the attributes. # Input and Output: - **Input**: - You will be testing your `CustomRepr` class with different dictionaries and lists in the following format: ```python custom_repr = CustomRepr() # Setting instance-specific limits for testing custom_repr.maxdict = 3 custom_repr.maxlist = 5 test_dict = { \'a\': 1, \'b\': 2, \'c\': 3, \'d\': 4, \'e\': 5 } test_list = [1, 2, 3, 4, 5, 6, 7, 8] ``` - **Output**: - The representation of the dictionary should display at most 3 key-value pairs, followed by an ellipsis (`...`). - The representation of the list should display at most 5 elements, followed by an ellipsis (`...`). # Constraints: - Use the `reprlib` module\'s capabilities to limit the size of object representations. - Ensure the representation methods handle cases where the objects are within the limits correctly as well (i.e., they should represent the full object if its size is within limits). # Example: ```python import reprlib class CustomRepr(reprlib.Repr): def repr_dict(self, obj, level): # Custom representation for dictionaries with imposed limits n = len(obj) if n > self.maxdict: keys = list(obj.keys())[:self.maxdict] items = \', \'.join(f\'{repr(key)}: {repr(obj[key])}\' for key in keys) return f\'{{{items}, ...}}\' return repr(obj) def repr_list(self, obj, level): # Custom representation for lists with imposed limits n = len(obj) if n > self.maxlist: items = \', \'.join(repr(item) for item in obj[:self.maxlist]) return f\'[{items}, ...]\' return repr(obj) # Testing the implementation custom_repr = CustomRepr() custom_repr.maxdict = 3 custom_repr.maxlist = 5 test_dict = {\'a\': 1, \'b\': 2, \'c\': 3, \'d\': 4, \'e\': 5} test_list = [1, 2, 3, 4, 5, 6, 7, 8] print(custom_repr.repr(test_dict)) # Expected output: \\"{\'a\': 1, \'b\': 2, \'c\': 3, ...}\\" print(custom_repr.repr(test_list)) # Expected output: \\"[1, 2, 3, 4, 5, ...]\\" ``` **Note**: The specific formatting for dictionaries and lists can differ, but the primary goal is to limit the number of elements shown according to the set attributes (`maxdict` and `maxlist`).","solution":"import reprlib class CustomRepr(reprlib.Repr): def repr_dict(self, obj, level): n = len(obj) if n > self.maxdict: keys = list(obj.keys())[:self.maxdict] items = \', \'.join(f\'{repr(key)}: {repr(obj[key])}\' for key in keys) return f\'{{{items}, ...}}\' return repr(obj) def repr_list(self, obj, level): n = len(obj) if n > self.maxlist: items = \', \'.join(repr(item) for item in obj[:self.maxlist]) return f\'[{items}, ...]\' return repr(obj) # Example usage: custom_repr = CustomRepr() custom_repr.maxdict = 3 custom_repr.maxlist = 5 test_dict = {\'a\': 1, \'b\': 2, \'c\': 3, \'d\': 4, \'e\': 5} test_list = [1, 2, 3, 4, 5, 6, 7, 8] print(custom_repr.repr(test_dict)) # Expected output: \\"{\'a\': 1, \'b\': 2, \'c\': 3, ...}\\" print(custom_repr.repr(test_list)) # Expected output: \\"[1, 2, 3, 4, 5, ...]\\""},{"question":"Objective: The goal of this exercise is to assess your ability to use Python\'s `webbrowser` module to manage web browsing tasks. You will need to implement a function that can open multiple URLs in different browser windows or tabs as specified, and also register and use a custom browser if required. Problem Statement: You need to implement the following function: ```python def manage_web_sessions(urls, new_window=False, register_browser=None): Opens multiple URLs in a web browser. Parameters: - urls (list): List of URLs to open. - new_window (bool): If True, open each URL in a new window. If False, open each URL in a new tab. Default is False. - register_browser (dict): A dictionary with keys \'name\', \'constructor\', and optionally \'preferred\' to register a custom browser. Returns: - List of booleans: Each boolean value represents if the corresponding URL was successfully opened. pass ``` Input: 1. `urls`: A list of strings, where each string is a valid URL. For example: [\\"https://www.python.org\\", \\"https://www.github.com\\"]. 2. `new_window`: A boolean value indicating whether to open each URL in a new window (`True`) or a new tab (`False`). Default is `False`. 3. `register_browser`: A dictionary with keys: - `\'name\'`: The name of the browser to register. For example: \\"my_browser\\". - `\'constructor\'`: The command to launch the browser. For example: \\"MyBrowser(\'my_browser\')\\". - `\'preferred\'` (Optional): A boolean to specify if this registered browser should be preferred. Default is `False`. Output: - The function should return a list of boolean values where each boolean value corresponds to whether the respective URL was opened successfully. Example: ```python # Example usage urls = [\\"https://www.python.org\\", \\"https://www.github.com\\"] new_window = True register_browser = { \'name\': \'custom_browser\', \'constructor\': \'GenericBrowser(\\"custom-browser-command\\")\', \'preferred\': True } result = manage_web_sessions(urls, new_window, register_browser) print(result) # Output example: [True, True] ``` Constraints: 1. Ensure valid URL strings are passed in the `urls` list. 2. The function should handle possible exceptions and ensure robust error handling. 3. If `register_browser` is specified, register the browser before opening URLs. 4. The `webbrowser` module functionality must be used to open URLs and manage browser sessions appropriately. This problem requires you to combine your understanding of optional keyword arguments, exceptions, and the `webbrowser` module effectively to implement a solution that meets the input specifications and desired functionalities.","solution":"import webbrowser def manage_web_sessions(urls, new_window=False, register_browser=None): Opens multiple URLs in a web browser. Parameters: - urls (list): List of URLs to open. - new_window (bool): If True, open each URL in a new window. If False, open each URL in a new tab. Default is False. - register_browser (dict): A dictionary with keys \'name\', \'constructor\', and optionally \'preferred\' to register a custom browser. Returns: - List of booleans: Each boolean value represents if the corresponding URL was successfully opened. if register_browser: name = register_browser.get(\'name\') constructor = register_browser.get(\'constructor\') preferred = register_browser.get(\'preferred\', False) # Registering the custom browser webbrowser.register(name, constructor, preferred=preferred) results = [] for url in urls: try: if new_window: opened = webbrowser.open_new(url) else: opened = webbrowser.open_new_tab(url) results.append(opened) except Exception: results.append(False) return results"},{"question":"Coding Assessment Question: Write a function `visualize_penguins_data` that utilizes `seaborn` to create a series of plots visualizing the `penguins` dataset. The function should create: 1. A histogram with KDE overlay of the `flipper_length_mm` variable, with separate plots for each `species` of penguins. 2. A bivariate KDE plot showing `flipper_length_mm` vs. `bill_length_mm`, colored by `species` and with marginal rug plots showing the distribution of both dimensions. 3. An ECDF plot of the `body_mass_g` variable, faceted by `species` and `sex`. The function should take no inputs and should output three `FacetGrid` objects corresponding to the three plots. It should also set appropriate axis labels and titles for each plot. **Constraints:** - Assume the `seaborn` and `pandas` libraries are correctly installed and imported. - You should use the `displot` function extensively to demonstrate your understanding of its capabilities. **Function Signature:** ```python def visualize_penguins_data() -> tuple: pass ``` **Expected Output:** - Three `FacetGrid` objects. **Example:** ```python g1, g2, g3 = visualize_penguins_data() # The function should generate and display the plots directly. ``` **Performance Requirements:** - The function should run efficiently, but there are no specific time or space complexity constraints.","solution":"import seaborn as sns import matplotlib.pyplot as plt from seaborn import load_dataset def visualize_penguins_data(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # 1. Histogram with KDE overlay of the flipper_length_mm variable, with separate plots for each species g1 = sns.displot( data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", kind=\\"kde\\", fill=True, common_norm=False, height=5, aspect=1.5 ) g1.set_axis_labels(\\"Flipper Length (mm)\\", \\"Density\\") g1.set_titles(\\"Histogram with KDE overlay of Flipper Length by Species\\") # 2. Bivariate KDE plot showing flipper_length_mm vs. bill_length_mm, colored by species g2 = sns.jointplot( data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", hue=\\"species\\", kind=\\"kde\\", fill=True, height=6 ) g2.set_axis_labels(\\"Flipper Length (mm)\\", \\"Bill Length (mm)\\") g2.fig.suptitle(\\"Bivariate KDE of Flipper Length and Bill Length by Species\\", fontsize=16) g2.fig.tight_layout(pad=2) # 3. ECDF plot of the body_mass_g variable, faceted by species and sex g3 = sns.displot( data=penguins, x=\\"body_mass_g\\", hue=\\"species\\", col=\\"species\\", row=\\"sex\\", kind=\\"ecdf\\", height=5, aspect=1 ) g3.set_axis_labels(\\"Body Mass (g)\\", \\"ECDF\\") g3.set_titles(\\"{col_name} - {row_name}\\") return g1, g2, g3"},{"question":"Objective The goal of this task is to assess your ability to work with the `email.contentmanager` module to create and manipulate MIME content in emails. You will use the functionalities provided to implement a custom content manager for handling a new MIME type and demonstrate its usage. Problem Statement You are asked to create a custom content manager named `CustomContentManager` that extends the `email.contentmanager.ContentManager` class. The `CustomContentManager` should be able to handle a custom MIME type `\\"text/customtype\\"`. Additionally, implement a function `create_custom_email`, which uses your `CustomContentManager` to set and get the content of an email message of this MIME type. Implementation Requirements 1. **Class `CustomContentManager`**: - Extend the `email.contentmanager.ContentManager` class. - Implement a handler for the MIME type `\\"text/customtype\\"`: - **For setting content**: Accept a string, set it as the payload of the message, and add the appropriate `Content-Type` and `Content-Transfer-Encoding` headers. - **For getting content**: Retrieve the payload of the message as a string. 2. **Function `create_custom_email`**: - Create a function that takes a string as input. - Use an instance of `EmailMessage` and your `CustomContentManager` to: - Set the provided string as the content of the email with MIME type `\\"text/customtype\\"`. - Retrieve the content from the email message. - Return the retrieved content. Input and Output Format - The input to the `create_custom_email` function will be a single string. - The function must return the string retrieved from the email message to verify that the content was correctly set and retrieved. Constraints - Use only the standard library modules. - Ensure that the `Content-Type` header correctly reflects the MIME type `\\"text/customtype\\"`. - Make sure the content transfer encoding is set to `\\"quoted-printable\\"` or `\\"base64\\"` to handle non-ASCII characters properly if they exist in the input string. Example ```python # Example usage of the implemented function content = \\"This is a custom MIME type content.\\" result = create_custom_email(content) print(result) # Output should be \\"This is a custom MIME type content.\\" ``` Submission Submit a single Python file where you: 1. Implement the `CustomContentManager` class. 2. Implement the `create_custom_email` function. 3. Ensure the function runs correctly and returns the expected output for the provided example.","solution":"from email.contentmanager import ContentManager from email.message import EmailMessage from email.policy import default class CustomContentManager(ContentManager): def set_content(self, msg, obj, maintype=\'text\', subtype=\'customtype\', **params): Handle setting content for \'text/customtype\'. if maintype == \'text\' and subtype == \'customtype\': msg.set_payload(obj) msg.set_type(f\'{maintype}/{subtype}\') msg[\'Content-Transfer-Encoding\'] = \'quoted-printable\' else: super().set_content(msg, obj, maintype=maintype, subtype=subtype, **params) def get_content(self, msg, *args, **kwargs): Handle getting content for \'text/customtype\'. if msg.get_content_type() == \'text/customtype\': return msg.get_payload() return super().get_content(msg, *args, **kwargs) def create_custom_email(content): Create email with custom content type and retrieve the content. msg = EmailMessage(policy=default) manager = CustomContentManager() manager.set_content(msg, content) retrieved_content = manager.get_content(msg) return retrieved_content"},{"question":"# Question: Advanced Data Visualization using Seaborn\'s Object-oriented Interface Objective In this assessment, you are required to demonstrate your understanding of the seaborn package, specifically using its object-oriented interface (`seaborn.objects`), to create complex and informative visualizations. Task Given the datasets `tips` and `glue` available in seaborn, perform the following tasks: 1. **Dot Plot with Customization**: - Create a dot plot of the `tips` dataset to visualize the relationship between `total_bill` and `tip`. - Add a border to each dot with a white edge color to distinguish overlapping points. 2. **Faceted Plot**: - Using the `glue` dataset, create a facet grid plot showing the `Score` against `Model`, with each facet representing a different `Task`. - The x-axis should be limited to a range of -5 to 105. - Customize the plot by mapping `pointsize` to 6, using `Year` for color mapping, and `Encoder` for marker style. - Use the `flare` color palette and set markers to \\"o\\" and \\"s\\". 3. **Enhanced Dot Plot with Error Bars**: - Create a dot plot of the `tips` dataset to show `total_bill` against `day`. - Apply jittering and shifting to reduce overplotting. - Overlay another set of dots aggregating the data. - Add error bars showing the standard error of the mean (SE). Constraints - You must use the `seaborn.objects` interface (i.e., the `so` module). - Ensure code readability and add appropriate comments for each step of the implementation. Input/Output - **Input**: None (datasets are to be loaded within the script) - **Output**: Displayed plots Example Code Here is a skeleton of the code to help you get started: ```python import seaborn.objects as so from seaborn import load_dataset # Load datasets tips = load_dataset(\\"tips\\") glue = load_dataset(\\"glue\\") # Task 1: Dot Plot with Customization def custom_dot_plot(): plot = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\") plot.add(so.Dot(edgecolor=\\"w\\")) plot.show() # Task 2: Faceted Plot def faceted_plot(): plot = so.Plot(glue, x=\\"Score\\", y=\\"Model\\").facet(\\"Task\\", wrap=4).limit(x=(-5, 105)) plot.add(so.Dot(pointsize=6), color=\\"Year\\", marker=\\"Encoder\\") plot.scale(marker=[\\"o\\", \\"s\\"], color=\\"flare\\") plot.show() # Task 3: Enhanced Dot Plot with Error Bars def dot_plot_with_error_bars(): plot = ( so.Plot(tips, x=\\"total_bill\\", y=\\"day\\") .add(so.Dot(pointsize=3), so.Shift(y=0.2), so.Jitter(0.2)) .add(so.Dot(), so.Agg()) .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) ) plot.show() # Execute the functions custom_dot_plot() faceted_plot() dot_plot_with_error_bars() ``` You are required to fill in the implementation details for each of the functions provided.","solution":"import seaborn.objects as so from seaborn import load_dataset def custom_dot_plot(): # Load tips dataset tips = load_dataset(\\"tips\\") # Create a dot plot of total_bill vs tip with a white edge around each dot to distinguish overlapping points plot = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\") dot_plot = plot.add(so.Dot(edgecolor=\\"w\\")) dot_plot.show() def faceted_plot(): # Load glue dataset glue = load_dataset(\\"glue\\") # Create a faceted plot with Score against Model, each facet representing a different Task plot = (so.Plot(glue, x=\\"Score\\", y=\\"Model\\") .facet(\\"Task\\", wrap=4) .limit(x=(-5, 105)) .add(so.Dot(pointsize=6), color=\\"Year\\", marker=\\"Encoder\\") .scale(marker=[\\"o\\", \\"s\\"], color=\\"flare\\")) plot.show() def dot_plot_with_error_bars(): # Load tips dataset tips = load_dataset(\\"tips\\") # Create a dot plot of total_bill vs day with jittering and shifting to reduce overplotting # Add aggregated dots and error bars showing the standard error of the mean plot = (so.Plot(tips, x=\\"total_bill\\", y=\\"day\\") .add(so.Dot(pointsize=3), so.Shift(y=0.2), so.Jitter(0.2)) .add(so.Dot(), so.Agg()) .add(so.Range(), so.Est(errorbar=(\\"se\\", 2)))) plot.show() # Uncomment this section to execute the functions # custom_dot_plot() # faceted_plot() # dot_plot_with_error_bars()"},{"question":"In Python, floating-point numbers are approximations due to the inherent limitations of binary representation. This often leads to unexpected behaviors during arithmetic operations. Your task is to implement a function that precisely handles floating-point summations using Python\'s `decimal` module to avoid these issues. Implement a function `precise_sum(numbers: List[float]) -> str` that: 1. Takes a list of floating-point numbers as input. 2. Computes the sum using the `decimal` module to ensure high precision. 3. Returns the resulting sum as a string formatted to 17 significant digits. # Input - `numbers`: a list of 1 to 1000 floating-point numbers (`0 <= len(numbers) <= 1000`). # Output - A string representing the precise sum of the input list formatted to 17 significant digits. # Example ```python input_numbers = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1] print(precise_sum(input_numbers)) # Expected Output: \'1.0000000000000000\' ``` # Constraints - The numbers in the list can be positive or negative. - The sum of the list elements will not exceed the range that can be handled by Python\'s `decimal.Decimal`. # Performance Expectations - The function should handle the worst-case scenario (sum of 1000 floating-point numbers) efficiently. # Implementation Requirements - You **must** use the `decimal` module to ensure high precision in the summation process. - Ensure the returned result is a string with at most 17 significant digits. Good luck!","solution":"from decimal import Decimal, getcontext from typing import List def precise_sum(numbers: List[float]) -> str: Takes a list of floating-point numbers and returns the sum as a string formatted to 17 significant digits. # Set the precision high enough to ensure the result is accurate to 17 significant digits getcontext().prec = 25 # Convert numbers to Decimal and sum them total = sum(Decimal(str(num)) for num in numbers) # Return the sum formatted to 17 significant digits return f\\"{total:.17f}\\""},{"question":"You are given a dataset where you must create a feature selection pipeline using scikit-learn. Your task is to implement a function that selects the most relevant features from the dataset using a combination of `VarianceThreshold`, `SelectKBest`, and `SelectFromModel` within a pipeline. # Function Signature ```python def select_important_features(X: np.ndarray, y: np.ndarray) -> np.ndarray: Selects the most relevant features from the dataset using a feature selection pipeline. Parameters: - X (np.ndarray): A 2D numpy array of shape (n_samples, n_features) representing the input features. - y (np.ndarray): A 1D numpy array of shape (n_samples,) representing the target variable. Returns: - np.ndarray: A 2D numpy array of shape (n_samples, n_selected_features) representing the input features after selection. ``` # Input - `X` (numpy.ndarray): A 2D numpy array with dimensions `(n_samples, n_features)` where `n_samples` is the number of samples and `n_features` is the number of features. - `y` (numpy.ndarray): A 1D numpy array with dimension `(n_samples,)` representing the target variable. # Output - `np.ndarray`: A 2D numpy array containing the selected features. # Requirements 1. **Remove features with low variance**: Use a `VarianceThreshold` to remove features with a variance below a threshold of 0.1. 2. **Select the top 5 features using univariate feature selection**: Use `SelectKBest` with the `f_classif` score function to select the 5 best features. 3. **Final feature selection using `SelectFromModel`**: Use a `RandomForestClassifier` to select the most important features with an importance threshold of the \\"mean\\". # Constraints - Do not modify the input arrays `X` and `y` directly. - Use the specified feature selection methods in the given order. - Ensure the final output contains only the selected features. # Example ```python import numpy as np from sklearn.datasets import load_iris # Load the iris dataset iris = load_iris() X, y = iris.data, iris.target # Call the function selected_features = select_important_features(X, y) print(selected_features.shape) # Expected output shape: (150, number_of_selected_features) ``` **Note**: The actual number of selected features can vary depending on the importance threshold of the `RandomForestClassifier`. # Background The students can refer to the scikit-learn documentation provided for details on `VarianceThreshold`, `SelectKBest`, and `SelectFromModel` with `RandomForestClassifier`.","solution":"import numpy as np from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif, SelectFromModel from sklearn.ensemble import RandomForestClassifier from sklearn.pipeline import Pipeline def select_important_features(X: np.ndarray, y: np.ndarray) -> np.ndarray: Selects the most relevant features from the dataset using a feature selection pipeline. Parameters: - X (np.ndarray): A 2D numpy array of shape (n_samples, n_features) representing the input features. - y (np.ndarray): A 1D numpy array of shape (n_samples,) representing the target variable. Returns: - np.ndarray: A 2D numpy array of shape (n_samples, n_selected_features) representing the input features after selection. pipeline = Pipeline([ (\'variance_threshold\', VarianceThreshold(threshold=0.1)), (\'select_k_best\', SelectKBest(score_func=f_classif, k=5)), (\'select_from_model\', SelectFromModel(estimator=RandomForestClassifier(), threshold=\\"mean\\")) ]) X_selected = pipeline.fit_transform(X, y) return X_selected"},{"question":"# Question **Profiling and Analysis of a Function** You are provided with a function `process_data()` which performs several computations on a dataset. Your task is to profile `process_data()` using the `cProfile` module to gather detailed execution statistics. You need to save the profiling results to a file and then analyze the results using the `pstats` module. **Function to Profile:** ```python def process_data(data): result = [] for item in data: processed = compute(item) result.append(processed) return aggregate(result) def compute(item): return item ** 2 # Example computation def aggregate(items): return sum(items) # Example aggregation ``` **Task:** 1. Profile the `process_data()` function to collect execution statistics. 2. Save the profiling results to a file. 3. Use the `pstats` module to load the profiling data and perform the following analysis: - Print the top 5 functions sorted by total time spent within each function. - Print the top 5 functions sorted by cumulative time, including time spent in sub-functions. - Print the list of functions that call the `compute` function and the cumulative time taken by them. - Filter the results to include only functions defined in the script (excluding any library or built-in functions). **Implementation Requirements:** 1. Use `cProfile` to profile the function and `pstats` to analyze the results. 2. Ensure that the profiling and analysis code is implemented in a single script. 3. Profile the `process_data()` function with a dataset of 1,000,000 numbers generated using `range(1, 1000001)`. **Constraints:** - You must use the `cProfile.Profile` class to profile the function. - You must use the `pstats.Stats` class for loading and analyzing profiling results. **Expected Output:** 1. The profiling results saved to a file named `profiling_results`. 2. The analysis printed to the console as specified. **Sample Code Structure:** ```python import cProfile import pstats from pstats import SortKey # Provided function to profile def process_data(data): result = [] for item in data: processed = compute(item) result.append(processed) return aggregate(result) def compute(item): return item ** 2 # Example computation def aggregate(items): return sum(items) # Example aggregation # Your code to profile and analyze the function if __name__ == \\"__main__\\": # Provide your implementation here ```","solution":"import cProfile import pstats from pstats import SortKey # Provided function to profile def process_data(data): result = [] for item in data: processed = compute(item) result.append(processed) return aggregate(result) def compute(item): return item ** 2 # Example computation def aggregate(items): return sum(items) # Example aggregation def profile_and_analyze(): profiler = cProfile.Profile() data = range(1, 1000001) profiler.enable() process_data(data) profiler.disable() profiler.dump_stats(\'profiling_results\') with open(\\"analysis_output.txt\\", \\"w\\") as file: stats = pstats.Stats(\'profiling_results\', stream=file) stats.sort_stats(SortKey.TIME) stats.print_stats(5) file.write(\\"nSorted by cumulative time:n\\") stats.sort_stats(SortKey.CUMULATIVE) stats.print_stats(5) file.write(\\"nFunctions that call \'compute\':n\\") stats.print_callers(\'compute\') file.write(\\"nFiltered by script functions:n\\") stats.print_stats(\'__main__\') if __name__ == \\"__main__\\": profile_and_analyze()"},{"question":"**Question: Implement a Multi-threaded Counter with Synchronization** As a programmer, you need to implement a thread-safe counter using the low-level threading primitives available in the `_thread` module. The counter should be capable of being incremented by multiple threads safely without causing a race condition. # Requirements: 1. Implement a `ThreadSafeCounter` class with the following methods: - `__init__(self)`: Initializes the counter to zero and sets up the necessary lock. - `increment(self, times)`: Safely increments the counter a specified number of times. - `value(self)`: Returns the current value of the counter. 2. Implement a function `run_threads(n_threads, increments)` which: - Creates `n_threads` threads. - Each thread should increment the counter `increments` times using the `increment` method of `ThreadSafeCounter`. - Waits for all threads to complete execution. - Returns the final value of the counter. # Constraints: - You must use the `_thread` module for creating and managing threads. - Synchronization should be handled using locks from the `_thread` module. # Performance Requirement: - The implementation should manage thread synchronization efficiently to minimize contention and overhead. # Input: - `n_threads` (integer): The number of threads to create. - `increments` (integer): The number of times each thread should increment the counter. # Output: - An integer representing the final value of the counter after all threads have completed execution. # Example: ```python # Example usage counter = ThreadSafeCounter() final_value = run_threads(10, 1000) print(final_value) # Expected: 10000 ``` # Implementation: You are required to fill in the implementation details for the `ThreadSafeCounter` class and the `run_threads` function. ```python import _thread class ThreadSafeCounter: def __init__(self): Initializes the counter and the lock. self.counter = 0 self.lock = _thread.allocate_lock() def increment(self, times): Increments the counter a specified number of times. for _ in range(times): with self.lock: self.counter += 1 def value(self): Returns the current value of the counter. return self.counter def run_threads(n_threads, increments): Runs threads to increment the counter. counter = ThreadSafeCounter() def thread_function(): counter.increment(increments) threads = [] for _ in range(n_threads): thread_id = _thread.start_new_thread(thread_function, ()) threads.append(thread_id) # Return the final value of the counter # Note: We need a method to ensure all threads have finished, which # is usually handled better by higher-level threading libraries. return counter.value() ``` Please complete the implementation by ensuring proper thread synchronization and joining functionality to ensure all threads have finished before returning the counter value.","solution":"import _thread import time class ThreadSafeCounter: def __init__(self): Initializes the counter and the lock. self.counter = 0 self.lock = _thread.allocate_lock() def increment(self, times): Increments the counter a specified number of times safely. for _ in range(times): with self.lock: self.counter += 1 def value(self): Returns the current value of the counter. with self.lock: return self.counter def run_threads(n_threads, increments): Runs threads to increment the counter. counter = ThreadSafeCounter() def thread_function(): counter.increment(increments) threads = [] for _ in range(n_threads): thread_id = _thread.start_new_thread(thread_function, ()) threads.append(thread_id) # Allow some time for all threads to complete # Busy-waiting or sleeping might be a naive approach to sync thread completion, # typically proper thread joins are used in higher-level threading libraries. time.sleep(1) return counter.value()"},{"question":"**Question: Constructing MIME Email Messages** In this task, you are required to create different types of MIME objects to simulate the construction of an email. You will use the `email.mime` module classes to create and manipulate these objects. Your goal is to create a MIME email message with an attached image, an attached text file, and a plain text body. Implement a function `create_mime_email` which takes the following parameters: - `image_data`: a bytes-like object representing the raw data of an image. - `text_data`: a string representing the content of a text file. - `email_body`: a string representing the plain text body of the email. The function should return a string representation of the MIME email. # Requirements 1. **Text content**: This is the plain text body of the email. 2. **Image Attachment**: Attach an image with the primary type as `image` and appropriate subtype determined automatically. 3. **Text File Attachment**: Attach a text file with the primary type as `application/octet-stream`. # Constraints: - The email should be constructed as a `multipart/mixed` MIME message. - The email body should be included as `MIMEText`. - The image data should be included as `MIMEImage`. - The text file should be included as `MIMEApplication` with `octet-stream`. # Expected Function Signature: ```python from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.application import MIMEApplication from email.encoders import encode_base64 def create_mime_email(image_data: bytes, text_data: str, email_body: str) -> str: # Your implementation here pass ``` # Example Usage: ```python image_data = b\'x89PNGrnx1anx00x00x00rIHDRx00x00x00\' text_data = \\"This is the content of the text file attachment.\\" email_body = \\"Hello, this is the body of the email.\\" email_str = create_mime_email(image_data, text_data, email_body) print(email_str) # The string representation of the full MIME email message. ``` # Notes: - Ensure that the `Content-Type` and `Content-Disposition` headers are set appropriately for each part of the MIME message. - Use base64 encoding for the image and application attachments.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.application import MIMEApplication from email.encoders import encode_base64 def create_mime_email(image_data: bytes, text_data: str, email_body: str) -> str: # Create the root message msg = MIMEMultipart() # Attach the plain text body body = MIMEText(email_body, \'plain\') msg.attach(body) # Attach the image image = MIMEImage(image_data) image.add_header(\'Content-Disposition\', \'attachment\', filename=\'image.png\') encode_base64(image) msg.attach(image) # Attach the text file text_attachment = MIMEApplication(text_data.encode(\'utf-8\'), name=\'attachment.txt\') text_attachment.add_header(\'Content-Disposition\', \'attachment\', filename=\'attachment.txt\') encode_base64(text_attachment) msg.attach(text_attachment) return msg.as_string()"},{"question":"Objective Your task is to create a Python script that uses the `venv` module to create a customized virtual environment. The script should extend the capabilities of a basic virtual environment by pre-installing a set of specified packages and performing some post-setup tasks. # Requirements 1. **Function**: Write a Python function `create_custom_env` that: 1. Accepts the following arguments: - `env_dir` (str): The directory where the virtual environment should be created. - `packages` (list of str): A list of package names to install in the virtual environment after its creation. - `prompt_name` (str): An optional name for the virtual environment prompt. 2. Creates a virtual environment using the `venv` module. 3. Installs the specified packages using `pip`. 4. Sets up a custom activation script that echoes a message `Welcome to your custom virtual environment!` when the environment is activated. 2. **Modules**: The function should utilize `venv.EnvBuilder` class and override the appropriate methods to achieve the desired functionality. 3. **Script Usage**: Write a script that: 1. Parses command-line arguments to get `env_dir`, `packages`, and `prompt_name`. 2. Calls the `create_custom_env` function with the parsed arguments. # Constraints - The function should handle errors gracefully and provide meaningful error messages if the virtual environment creation fails or if the package installation fails. - The script should be compatible with both Unix and Windows platforms. # Example Usage Here’s an example of how the script can be executed from the command line: ```sh python create_env.py /path/to/new/env \\"package1 package2\\" \\"CustomPrompt\\" ``` # Input - `env_dir` (str): The directory where the virtual environment should be created. - `packages` (list of str): A list of space-separated package names. - `prompt_name` (str): An optional string for the virtual environment prompt. # Output - The script should print messages indicating the progress of environment setup, package installation, and any errors encountered. - When the virtual environment is activated, it should display the custom message. # Function Signature ```python def create_custom_env(env_dir: str, packages: list, prompt_name: str = None) -> None: # Your implementation here ``` **Note**: Ensure you test the script thoroughly on different platforms to confirm that it behaves as expected.","solution":"import os import subprocess import sys import venv class CustomEnvBuilder(venv.EnvBuilder): def __init__(self, *args, packages=None, prompt_name=None, **kwargs): super().__init__(*args, **kwargs) self.packages = packages self.prompt_name = prompt_name def post_setup(self, context): # Install the specified packages using pip if self.packages: subprocess.check_call([context.env_exe, \'-m\', \'pip\', \'install\'] + self.packages) # Create custom activation script if os.name == \'nt\': activate_script = os.path.join(context.bin_path, \'activate.bat\') else: activate_script = os.path.join(context.bin_path, \'activate\') custom_message = \'necho \\"Welcome to your custom virtual environment!\\"n\' with open(activate_script, \'a\') as f: f.write(custom_message) def create_custom_env(env_dir: str, packages: list, prompt_name: str = None) -> None: Creates a virtual environment, installs specified packages, and sets up a custom activation script. :param env_dir: The directory where the virtual environment should be created. :param packages: A list of package names to install in the virtual environment. :param prompt_name: An optional name for the virtual environment prompt. builder = CustomEnvBuilder(with_pip=True, packages=packages, prompt_name=prompt_name) builder.create(env_dir)"},{"question":"# Pandas Window Functions Assessment Objective Create a function that demonstrates the use of rolling, expanding, and exponentially weighted window functions in pandas. The function should compute and return various statistical measures for a given time series dataset using each window method. Problem Statement Write a function `compute_window_statistics` that takes a pandas DataFrame with a time series and a window size as input and returns a dictionary containing the following statistics computed over the time series using rolling, expanding, and exponentially weighted windows: 1. Rolling window mean and standard deviation 2. Expanding window mean and variance 3. Exponentially weighted window mean and correlation with a specified column Function Signature ```python import pandas as pd def compute_window_statistics(df: pd.DataFrame, window_size: int, ew_span: int, correlation_col: str) -> dict: pass ``` Input - `df` (pandas DataFrame): A DataFrame where the index is a datetime index and columns contain numeric time series data. - `window_size` (int): The size of the rolling window. - `ew_span` (int): The span parameter for the exponentially weighted window. - `correlation_col` (str): The name of the column to calculate the exponentially weighted correlation with. Output - `result` (dict): A dictionary containing the following keys and their corresponding computed values: - `\\"rolling_mean\\"`: A pandas Series of the rolling window mean. - `\\"rolling_std\\"`: A pandas Series of the rolling window standard deviation. - `\\"expanding_mean\\"`: A pandas Series of the expanding window mean. - `\\"expanding_var\\"`: A pandas Series of the expanding window variance. - `\\"ewm_mean\\"`: A pandas Series of the exponentially weighted window mean. - `\\"ewm_corr\\"`: A pandas Series of the exponentially weighted window correlation with the specified column. Example ```python import pandas as pd import numpy as np # Generate some sample time series data np.random.seed(0) dates = pd.date_range(\'20210101\', periods=100) data = np.random.randn(100, 2) df = pd.DataFrame(data, index=dates, columns=[\'A\', \'B\']) # Compute window statistics window_size = 5 ew_span = 10 correlation_col = \'A\' result = compute_window_statistics(df, window_size, ew_span, correlation_col) # Access the results rolling_mean = result[\\"rolling_mean\\"] rolling_std = result[\\"rolling_std\\"] expanding_mean = result[\\"expanding_mean\\"] expanding_var = result[\\"expanding_var\\"] ewm_mean = result[\\"ewm_mean\\"] ewm_corr = result[\\"ewm_corr\\"] print(rolling_mean.head()) print(rolling_std.head()) print(expanding_mean.head()) print(expanding_var.head()) print(ewm_mean.head()) print(ewm_corr.head()) ``` Constraints - The DataFrame `df` will have at least one column. - The window size `window_size` and the span `ew_span` will be positive integers and less than or equal to the number of rows in the DataFrame. - The correlation column `correlation_col` will always be one of the DataFrame\'s columns. Your task is to implement the function `compute_window_statistics` to meet the described requirements.","solution":"import pandas as pd def compute_window_statistics(df: pd.DataFrame, window_size: int, ew_span: int, correlation_col: str) -> dict: Computes various statistical measures on a given time series dataset using rolling, expanding, and exponentially weighted windows. Parameters: df (pd.DataFrame): A DataFrame where the index is a datetime index and columns contain numeric time series data. window_size (int): The size of the rolling window. ew_span (int): The span parameter for the exponentially weighted window. correlation_col (str): The name of the column to calculate the exponentially weighted correlation with. Returns: dict: A dictionary containing the rolling mean, rolling standard deviation, expanding mean, expanding variance, exponentially weighted mean, and exponentially weighted correlation with the specified column. result = { \\"rolling_mean\\": df.rolling(window=window_size).mean(), \\"rolling_std\\": df.rolling(window=window_size).std(), \\"expanding_mean\\": df.expanding().mean(), \\"expanding_var\\": df.expanding().var(), \\"ewm_mean\\": df.ewm(span=ew_span).mean(), \\"ewm_corr\\": df.ewm(span=ew_span).corr(df[correlation_col]) } return result"},{"question":"**Coding Assessment Question: Understanding and Utilizing Seaborn for Regression Analysis** # Objective Demonstrate your understanding of the seaborn library by performing regression analysis on a dataset. # Problem Overview You have been provided with a dataset of car attributes (`mpg` dataset) and tasked with analyzing the relationships between various features using advanced regression techniques in seaborn. # Instructions 1. **Load the Dataset**: - Import the required libraries. - Load the `mpg` dataset using seaborn\'s `load_dataset` function. 2. **Basic Linear Regression Plot**: - Create a scatter plot with a linear regression line showing the relationship between car weight (`weight`) and acceleration (`acceleration`). 3. **Polynomial Regression Plot**: - Create a polynomial regression plot (order=2) to explore the relationship between weight (`weight`) and miles per gallon (`mpg`). 4. **Log-linear Regression Plot**: - Create a log-linear regression plot showing the relationship between displacement (`displacement`) and miles per gallon (`mpg`). 5. **Locally Weighted Scatterplot Smoothing (LOWESS)**: - Create a LOWESS regression plot to examine the relationship between horsepower (`horsepower`) and miles per gallon (`mpg`). 6. **Robust Regression**: - Create a robust regression plot displaying the relationship between horsepower (`horsepower`) and weight (`weight`). 7. **Customization**: - Create a regression plot illustrating the relationship between weight (`weight`) and horsepower (`horsepower`) with the following customizations: - 99% confidence interval. - Marker set as \'x\'. - Regression line in red color with reduced opacity. - Scatter plot points in grey color. # Constraints - Use the seaborn package for all plotting. - Ensure the plots are clear and properly labeled. - Include titles, axis labels, and legends as necessary on the plots. # Input Format No input from the user is required. # Output Format Your code should generate the plots as described in the instructions. # Example Code Template ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt # 1. Load the dataset sns.set_theme() mpg = sns.load_dataset(\\"mpg\\") # 2. Basic Linear Regression Plot plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"weight\\", y=\\"acceleration\\") plt.title(\'Linear Regression of Weight vs Acceleration\') plt.xlabel(\'Weight\') plt.ylabel(\'Acceleration\') plt.show() # 3. Polynomial Regression Plot plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"weight\\", y=\\"mpg\\", order=2) plt.title(\'Polynomial Regression of Weight vs MPG\') plt.xlabel(\'Weight\') plt.ylabel(\'MPG\') plt.show() # 4. Log-linear Regression Plot plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"displacement\\", y=\\"mpg\\", logx=True) plt.title(\'Log-linear Regression of Displacement vs MPG\') plt.xlabel(\'Displacement\') plt.ylabel(\'MPG\') plt.show() # 5. LOWESS Plot plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True) plt.title(\'LOWESS Regression of Horsepower vs MPG\') plt.xlabel(\'Horsepower\') plt.ylabel(\'MPG\') plt.show() # 6. Robust Regression plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"horsepower\\", y=\\"weight\\", robust=True) plt.title(\'Robust Regression of Horsepower vs Weight\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Weight\') plt.show() # 7. Customization plt.figure(figsize=(10, 6)) sns.regplot( data=mpg, x=\\"weight\\", y=\\"horsepower\\", ci=99, marker=\\"x\\", color=\\".3\\", line_kws=dict(color=\\"red\\", alpha=0.5) ) plt.title(\'Customized Regression of Weight vs Horsepower\') plt.xlabel(\'Weight\') plt.ylabel(\'Horsepower\') plt.show() ``` # Submission Provide the code that generates the plots as specified. Ensure your solution is well-organized, and plots are properly labeled.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Load the dataset sns.set_theme() mpg = sns.load_dataset(\\"mpg\\") # Basic Linear Regression Plot plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"weight\\", y=\\"acceleration\\") plt.title(\'Linear Regression of Weight vs Acceleration\') plt.xlabel(\'Weight\') plt.ylabel(\'Acceleration\') plt.show() # Polynomial Regression Plot plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"weight\\", y=\\"mpg\\", order=2) plt.title(\'Polynomial Regression of Weight vs MPG\') plt.xlabel(\'Weight\') plt.ylabel(\'MPG\') plt.show() # Log-linear Regression Plot plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"displacement\\", y=\\"mpg\\", logx=True) plt.title(\'Log-linear Regression of Displacement vs MPG\') plt.xlabel(\'Displacement\') plt.ylabel(\'MPG\') plt.show() # LOWESS Plot plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True) plt.title(\'LOWESS Regression of Horsepower vs MPG\') plt.xlabel(\'Horsepower\') plt.ylabel(\'MPG\') plt.show() # Robust Regression plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"horsepower\\", y=\\"weight\\", robust=True) plt.title(\'Robust Regression of Horsepower vs Weight\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Weight\') plt.show() # Customization plt.figure(figsize=(10, 6)) sns.regplot( data=mpg, x=\\"weight\\", y=\\"horsepower\\", ci=99, marker=\\"x\\", color=\\".3\\", line_kws=dict(color=\\"red\\", alpha=0.5) ) plt.title(\'Customized Regression of Weight vs Horsepower\') plt.xlabel(\'Weight\') plt.ylabel(\'Horsepower\') plt.show()"},{"question":"**Objective:** Implement a logging utility based on the Python `syslog` module that logs messages with different priorities and filters out messages based on a mask. **Task:** You need to create a function `custom_syslog(messages, mask_priority)`: ```python def custom_syslog(messages, mask_priority): Logs the given messages using the Unix syslog facility. Parameters: messages (list of tuples): A list of tuples where each tuple contains: - A string message to log. - The priority of the message (should be one of the syslog priority constants). - The facility of the message (should be one of the syslog facility constants). mask_priority (int): The priority level up to which messages should be logged. Messages with a priority higher than this should be filtered out. Returns: None pass ``` **Details:** 1. Initialize the syslog configuration using appropriate syslog constants. 2. Apply the log mask such that messages with a higher priority than `mask_priority` are filtered out. 3. Log each message in the `messages` list. 4. Use `syslog.closelog()` after logging all messages to reset the module. **Input:** - `messages`: A list of tuples `(message, priority, facility)`, where: - `message`: A string to log. - `priority`: The priority level (e.g., `syslog.LOG_ERR`). - `facility`: The facility (e.g., `syslog.LOG_USER`). - `mask_priority`: An integer representing the max priority level to log (e.g., `syslog.LOG_WARNING`). **Output:** - The function returns `None`. **Constraints:** - Ensure to handle trailing newlines appropriately. - Use `syslog.openlog` with sensible defaults. - Only log messages within the given priority mask. **Example:** ```python import syslog messages = [ (\\"System booting up\\", syslog.LOG_INFO, syslog.LOG_SYSLOG), (\\"Disk space low\\", syslog.LOG_WARNING, syslog.LOG_DAEMON), (\\"Failed login attempt\\", syslog.LOG_ERR, syslog.LOG_AUTH) ] custom_syslog(messages, syslog.LOG_WARNING) ``` In this example, the function should log messages with `LOG_INFO`, `LOG_WARNING`, and ignore messages with `LOG_ERR` because they exceed the `mask_priority` of `LOG_WARNING`. **Hints:** - Utilize `syslog.LOG_UPTO` to set the mask for all priorities up to and including the given level. - Use `syslog.openlog` to set the ident and facility for the log.","solution":"import syslog def custom_syslog(messages, mask_priority): Logs the given messages using the Unix syslog facility. Parameters: messages (list of tuples): A list of tuples where each tuple contains: - A string message to log. - The priority of the message (should be one of the syslog priority constants). - The facility of the message (should be one of the syslog facility constants). mask_priority (int): The priority level up to which messages should be logged. Messages with a priority higher than this should be filtered out. Returns: None syslog.openlog(logoption=syslog.LOG_PID) # Set the log mask to include messages with priorities up to mask_priority syslog.setlogmask(syslog.LOG_UPTO(mask_priority)) for message, priority, facility in messages: syslog.openlog(facility=facility) syslog.syslog(priority, message) syslog.closelog()"},{"question":"Task In this task, you are required to write a Python function named `merge_and_update_scopes` that takes three dictionaries representing different scopes and returns a `ChainMap` object that can be used to look up keys across all given scopes. Additionally, you need to implement another function `update_scope` that allows updating an existing key in the `ChainMap` object. However, if the key exists in any of the underlying dictionaries, it should update the corresponding dictionary rather than just the first one. Function Signature ```python from collections import ChainMap def merge_and_update_scopes(global_dict: dict, local_dict: dict, builtins_dict: dict) -> ChainMap: pass def update_scope(chainmap: ChainMap, key: str, value): pass ``` Input 1. `global_dict` (dict): Dictionary representing the global scope. 2. `local_dict` (dict): Dictionary representing the local scope. 3. `builtins_dict` (dict): Dictionary representing built-in scope attributes. 4. `chainmap` (ChainMap): The `ChainMap` object created by `merge_and_update_scopes`. 5. `key` (str): The key to be updated. 6. `value` (Any): The new value to be assigned to the key. Output 1. The function `merge_and_update_scopes` should return a `ChainMap` object that contains the combined view of all the provided dictionaries. 2. The function `update_scope` should update the value of the specified key wherever it exists among the dictionaries in the `ChainMap`. Constraints - The `global_dict`, `local_dict`, and `builtins_dict` dictionaries are non-empty. - The keys are strings and the values can be of any type. - The dictionaries may have overlapping keys. Example ```python global_scope = {\'pi\': 3.14, \'gravity\': 9.8} local_scope = {\'pi\': 3.1415, \'x\': 10} builtin_scope = {\'max\': \'built-in_max\', \'min\': \'built-in_min\'} chainmap = merge_and_update_scopes(global_scope, local_scope, builtin_scope) # Accessing values assert chainmap[\'pi\'] == 3.1415 assert chainmap[\'gravity\'] == 9.8 assert chainmap[\'max\'] == \'built-in_max\' assert chainmap[\'x\'] == 10 # Updating values update_scope(chainmap, \'pi\', 3.142) # pi exists in local_scope assert local_scope[\'pi\'] == 3.142 assert chainmap[\'pi\'] == 3.142 update_scope(chainmap, \'max\', \'modified_max\') # max exists in builtin_scope assert builtin_scope[\'max\'] == \'modified_max\' assert chainmap[\'max\'] == \'modified_max\' ``` Notes - Ensure that your `merge_and_update_scopes` function creates a `ChainMap` containing the local, global, and built-in dictionaries in that order. - The `update_scope` should correctly locate and update the key across the nested mappings. **Good Luck!**","solution":"from collections import ChainMap def merge_and_update_scopes(global_dict: dict, local_dict: dict, builtins_dict: dict) -> ChainMap: Combines the three given dictionaries into a ChainMap. :param global_dict: Dictionary representing the global scope :param local_dict: Dictionary representing the local scope :param builtins_dict: Dictionary representing built-in scope attributes :return: A ChainMap object containing the combined view of all dictionaries return ChainMap(local_dict, global_dict, builtins_dict) def update_scope(chainmap: ChainMap, key: str, value): Updates the value of a key in the appropriate dictionary within the ChainMap. :param chainmap: The ChainMap object :param key: The key to be updated :param value: The new value to be assigned to the key for mapping in chainmap.maps: if key in mapping: mapping[key] = value return chainmap.maps[0][key] = value # If key is not found, it will be added to the first dictionary"},{"question":"# Seaborn Coding Assessment Question Objective: To assess your understanding of seaborn\'s advanced plotting capabilities, dataset handling, and customization features. Question: Using the seaborn library, write a function `plot_life_expectancy_vs_spending` that takes as input the `healthexp` dataset and produces a scatter plot with the following specifications: 1. Plot the relationship between `Spending_USD` (x-axis) and `Life_Expectancy` (y-axis) for each country. 2. Differentiate each country by color. 3. Use a logarithmic scale for the `Spending_USD` axis. 4. Add a linear regression line that fits the data. 5. Customize the plot with markers of your choice and appropriate aesthetics to ensure clarity and visual appeal. Input: - The `healthexp` dataset, which contains data columns `Country`, `Year`, `Spending_USD`, and `Life_Expectancy`. Output: - Display the scatter plot with the specified customizations. Constraints: - Ensure the plot is clear and easy to interpret. - Use seaborn\'s `lmplot` or equivalent functionality along with other seaborn aesthetic settings to achieve the desired plot. Function Signature: ```python import seaborn as sns import matplotlib.pyplot as plt from seaborn import load_dataset def plot_life_expectancy_vs_spending(): # Load the dataset healthexp = load_dataset(\\"healthexp\\").sort_values([\\"Country\\", \\"Year\\"]) # Your code to generate the plot pass # Call the function to display the plot plot_life_expectancy_vs_spending() ``` Additional Notes: - Make sure to comment your code for clarity. - Utilize seaborn\'s documentation and examples to enhance the features of your plot. - Ensure the plot is displayed and appropriately titled/labeled.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_life_expectancy_vs_spending(): # Load the dataset healthexp = sns.load_dataset(\\"healthexp\\").sort_values([\\"Country\\", \\"Year\\"]) # Create a scatter plot with logistic scale for \'Spending_USD\' g = sns.lmplot( x=\\"Spending_USD\\", y=\\"Life_Expectancy\\", hue=\\"Country\\", data=healthexp, logx=True, markers=\\"o\\", scatter_kws={\\"alpha\\":0.5} ) # Set plot labels and title g.set_axis_labels(\\"Health Spending (Log USD)\\", \\"Life Expectancy (Years)\\") plt.title(\\"Life Expectancy vs. Health Spending by Country\\") # Show the plot plt.show() # Call the function to display the plot plot_life_expectancy_vs_spending()"},{"question":"Task Description You are tasked with developing a Python function that mimics the behavior of the Python import system for packages and modules. This function will utilize key concepts such as module search, caching using `sys.modules`, package handling, and importing mechanics you learned from the given documentation. Your function should be able to import modules dynamically and handle both regular packages and namespace packages. # Function Specification Implement the function `custom_import(module_name: str) -> Any`: Input: - `module_name` (str): The fully qualified name of the module to be imported, e.g., \\"foo.bar.baz\\". Output: - `Any`: The module object if the import is successful, or raise a `ModuleNotFoundError` if the module cannot be found. Constraints: - Use `importlib` and other standard libraries to aid in re-creating the import system\'s behavior. - Handle regular packages and namespace packages as described in the documentation. - Use `sys.modules` for caching and check for existing modules before attempting to import. - Do not use the built-in `import` statement for dynamic imports within your function (use `importlib` functions instead). # Example Usage ```python # Assume foo.bar.baz exists and is a valid module within the Python environment module = custom_import(\\"foo.bar.baz\\") print(module) # This should raise a ModuleNotFoundError as the module does not exist module = custom_import(\\"non.existent.module\\") ``` # Notes: - Ensure that you handle modules already present in `sys.modules`. - The function should consider the possibility of nested imports and submodules. - Address exceptions appropriately to mirror the behavior of standard import operations.","solution":"import importlib.util import sys def custom_import(module_name: str): Mimics the behavior of the Python import system for packages and modules. Parameters: - module_name (str): The fully qualified name of the module to be imported, e.g., \\"foo.bar.baz\\". Returns: - Any: The module object if the import is successful. Raises: - ModuleNotFoundError: If the module cannot be found. # Check if module is already cached in sys.modules if module_name in sys.modules: return sys.modules[module_name] try: # Find the specified module module_spec = importlib.util.find_spec(module_name) if module_spec is None: raise ModuleNotFoundError(f\\"No module named \'{module_name}\'\\") # Load and assign the module module = importlib.util.module_from_spec(module_spec) sys.modules[module_name] = module module_spec.loader.exec_module(module) return module except Exception as e: raise ModuleNotFoundError(f\\"No module named \'{module_name}\'\\") from e"},{"question":"# Exception Handling and Resource Management in Python You are tasked with creating a Python function `process_file(filepath: str) -> Union[float, str]` that processes a given text file to find the average length of words in the file. The function should handle various potential issues that can occur during file processing. Requirements: 1. **Function Definition:** - The function should be named `process_file`. - It should take a single argument `filepath`, which is a string representing the path to the file. - It should return either a float (average length of words) or a string (error message). 2. **File Reading and Processing:** - Open the file using a `with` statement to ensure it is properly closed in case of errors. - Read the file content and split it into words (assuming words are separated by whitespace). - Calculate the average length of words in the file. 3. **Error Handling:** - Handle `FileNotFoundError`: Return the error message `\\"Error: File not found\\"`. - Handle `IsADirectoryError`: Return the error message `\\"Error: Path is a directory\\"`. - Handle `PermissionError`: Return the error message `\\"Error: Permission denied\\"`. - Handle `ZeroDivisionError` (in case the file is empty): Return the error message `\\"Error: File is empty\\"`. - Handle any other exceptions and return a generic error message `\\"Error: An unexpected error occurred\\"`. 4. **Edge Cases:** - If the file is empty (no words), raise a `ZeroDivisionError` and handle it accordingly. - Consider the scenario where the file contains non-text content. Handle it gracefully and return a meaningful error message. 5. **Performance:** - Assume that the file can be large, so your solution should be optimized for memory usage when reading and processing the file. Example: ```python def process_file(filepath: str) -> Union[float, str]: try: with open(filepath, \'r\') as f: contents = f.read().strip() if not contents: raise ZeroDivisionError(\\"File is empty\\") words = contents.split() average_length = sum(len(word) for word in words) / len(words) return average_length except FileNotFoundError: return \\"Error: File not found\\" except IsADirectoryError: return \\"Error: Path is a directory\\" except PermissionError: return \\"Error: Permission denied\\" except ZeroDivisionError: return \\"Error: File is empty\\" except Exception as e: return f\\"Error: An unexpected error occurred - {e}\\" ``` Use the provided function definition and enhance the error handling as specified. Ensure that you test the function with various types of input files to validate its robustness and correctness. Constraints: - Do not use external libraries for reading files or handling exceptions. - Your solution should be compatible with Python 3.10+. Please ensure your code follows best practices for exception handling and resource management in Python.","solution":"from typing import Union def process_file(filepath: str) -> Union[float, str]: try: with open(filepath, \'r\') as f: contents = f.read().strip() if not contents: raise ZeroDivisionError(\\"File is empty\\") words = contents.split() average_length = sum(len(word) for word in words) / len(words) return average_length except FileNotFoundError: return \\"Error: File not found\\" except IsADirectoryError: return \\"Error: Path is a directory\\" except PermissionError: return \\"Error: Permission denied\\" except ZeroDivisionError: return \\"Error: File is empty\\" except Exception: return \\"Error: An unexpected error occurred\\""},{"question":"**Advanced Functional Programming Challenge: Efficient Data Processing** **Objective:** You are given a list of integers, and you need to process the list by performing a series of functional programming operations. Your goal is to write a Python function that leverages the `itertools`, `functools`, and `operator` modules to accomplish the following tasks: 1. Filter out all even numbers from the list. 2. Double the value of each remaining number. 3. Compute a running total of the modified numbers. 4. Return the final list of these running totals. **Function Signature:** ```python def process_numbers(numbers: list[int]) -> list[int]: pass ``` **Input:** - `numbers` (list[int]): A list of integers with at least one element. Example: `[4, 3, 8, 5, 6]` **Output:** - (list[int]): A list of integers representing the running totals after processing the input list. Example: `[6, 16]` is derived from `[4, 3, 8, 5, 6]` after filtering even numbers, doubling the remaining numbers, and calculating running totals. **Constraints:** 1. The input list will contain at least one integer and will be non-empty. 2. The input list contains up to 10,000 integers. 3. Your solution should have a linear time complexity, i.e., O(n). **Example:** ```python numbers = [4, 3, 8, 5, 6] # After filtering out even numbers: [3, 5] # After doubling the values: [6, 10] # Running totals: [6, 16] assert process_numbers(numbers) == [6, 16] ``` **Hints:** - Use `itertools` to handle the iteration and running total. - Use `functools` to simplify function application. - Use `operator` functions to perform arithmetic operations. **Note:** Ensure your function is efficient and leverages the three specified modules to their fullest potential.","solution":"from itertools import accumulate from functools import partial from operator import mul def process_numbers(numbers): # Step 1: Filter out even numbers filtered = filter(lambda x: x % 2 != 0, numbers) # Step 2: Double the value of each remaining number doubled = map(partial(mul, 2), filtered) # Step 3: Compute a running total of the modified numbers running_total = list(accumulate(doubled)) return running_total"},{"question":"Implementing an RRef Communication Protocol Objective: Demonstrate your understanding of PyTorch\'s distributed RPC framework by implementing a simplified version of the RRef communication protocol. Description: You need to implement a basic distributed RPC system using Python and PyTorch that mimics key functionalities of the RRef system described in the provided documentation. Specifically, you will implement the following: 1. **OwnerRRef and UserRRef Classes**: - `OwnerRRef`: Stores actual data and keeps track of global reference count. - `UserRRef`: References `OwnerRRef` and updates the owner about its status. 2. **Message Handling**: - Implement message sending, receiving, and processing to handle `UserRRef` creation and deletion notifications. 3. **Functionality**: - Ensure the owner is correctly notified about the creation and deletion of `UserRRef` instances. - Handle message retries and guarantee the correct order of operations based on the RRef protocol rules. Task: Implement the following classes and functions: ```python import torch import torch.distributed.rpc as rpc from collections import defaultdict class OwnerRRef: def __init__(self, data): Initialize OwnerRRef with actual data and reference count. self.data = data self.ref_count = 0 def increase_ref_count(self): Increase the reference count. self.ref_count += 1 def decrease_ref_count(self): Decrease the reference count. self.ref_count -= 1 if self.ref_count == 0: self.delete() def delete(self): Delete the OwnerRRef when reference count is zero. self.data = None print(\\"OwnerRRef deleted\\") class UserRRef: def __init__(self, owner_id, rref_id, owner_rref=None): Initialize UserRRef with owner information. self.owner_id = owner_id self.rref_id = rref_id self.owner_rref = owner_rref def notify_owner(self, message): Send notification to the owner about creation or deletion. # Simulate sending a message to the owner print(f\\"Notification to owner {self.owner_id}: {message}\\") # Here you can implement logic to update the owner # Example remote function to be executed def example_func(x): return x + 1 # Initialize RPC framework (you can simulate this with local calls for testing) def init_rpc_framework(): rpc.init_rpc(\\"worker0\\", rank=0, world_size=1) print(\\"RPC initialized\\") def main(): init_rpc_framework() # Simulate creating an OwnerRRef and UserRRefs owner_rref = OwnerRRef(torch.ones(2)) user_rref_1 = UserRRef(owner_id=\\"worker0\\", rref_id=1, owner_rref=owner_rref) user_rref_1.notify_owner(\\"UserRRef created\\") user_rref_2 = UserRRef(owner_id=\\"worker0\\", rref_id=1, owner_rref=owner_rref) user_rref_2.notify_owner(\\"UserRRef created\\") # Simulate deleting UserRRefs user_rref_1.notify_owner(\\"UserRRef deleted\\") user_rref_2.notify_owner(\\"UserRRef deleted\\") rpc.shutdown() print(\\"RPC shut down\\") if __name__ == \\"__main__\\": main() ``` Input and Output: - The input involves creating instances of `OwnerRRef` and `UserRRef` and simulating their interactions based on RPC calls. - The output should correctly display the notification messages sent to the owner and ensure that `OwnerRRef` is deleted only when all `UserRRef` instances are deleted. Constraints: - Do not use actual network communication; you can simulate it with function calls and print statements. - Focus on correctly implementing the reference counting and message handling logic. - Ensure no actual data deletion or process termination occurs since this code runs on a single local machine. Performance: - The implementation should handle the creation and deletion of multiple `UserRRef` instances efficiently. - Ensure that the `OwnerRRef` is only deleted when appropriate. Good luck!","solution":"import torch class OwnerRRef: def __init__(self, data): Initialize OwnerRRef with actual data and reference count. self.data = data self.ref_count = 0 def increase_ref_count(self): Increase the reference count. self.ref_count += 1 print(f\\"OwnerRRef ref_count increased to {self.ref_count}\\") def decrease_ref_count(self): Decrease the reference count. self.ref_count -= 1 print(f\\"OwnerRRef ref_count decreased to {self.ref_count}\\") if self.ref_count == 0: self.delete() def delete(self): Delete the OwnerRRef when reference count is zero. self.data = None print(\\"OwnerRRef deleted\\") class UserRRef: def __init__(self, owner_id, rref_id, owner_rref): Initialize UserRRef with owner information. self.owner_id = owner_id self.rref_id = rref_id self.owner_rref = owner_rref self.notify_owner(\\"UserRRef created\\") def __del__(self): self.notify_owner(\\"UserRRef deleted\\") def notify_owner(self, message): Send notification to the owner about creation or deletion. # Simulate sending a message to the owner print(f\\"Notification to owner {self.owner_id}: {message}\\") if message == \\"UserRRef created\\": self.owner_rref.increase_ref_count() elif message == \\"UserRRef deleted\\": self.owner_rref.decrease_ref_count()"},{"question":"**Coding Assessment Question:** You are provided with the `tips` dataset, which includes information about bills in a restaurant. Your task is to create an advanced visualization using seaborn that satisfies the following requirements: 1. Create a `swarmplot` that visualizes the distribution of total bills (`total_bill`) for each day of the week (`day`). 2. Use the `hue` parameter to differentiate between `male` and `female` customers. 3. Customize the plot to use the \\"deep\\" color palette. 4. Enable dodge to separate swarms by gender. 5. Adjust the plot to place `total_bill` on the x-axis and `day` on the y-axis. 6. Decrease the size of the points to 4. 7. Utilize `catplot` to create a facet grid, where: - Each facet corresponds to a different time of the day (`time`: Lunch or Dinner). - The aspect ratio of the plot is set to 0.6. **Input Format:** None. You should use the seaborn\'s `tips` dataset within your code. **Expected Output:** The output should be a swarm plot displayed inline with the following configuration: - x-axis: `total_bill` - y-axis: `day` - Points colored by `sex` using the \\"deep\\" palette with dodge enabled. - Point size set to 4. - Facets split by `time` with an aspect ratio of 0.6. Example code structure: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the `tips` dataset tips = sns.load_dataset(\\"tips\\") # Create the advanced plot as specified sns.catplot( data=tips, kind=\\"swarm\\", x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", col=\\"time\\", palette=\\"deep\\", dodge=True, aspect=0.6, size=4 ) plt.show() ``` **Note:** - Make sure to use seaborn\'s `catplot` with `kind=\\"swarm\\"` for creating the facet grid. - Ensure the plot adheres to all the customizations specified above. - Import necessary libraries as needed.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_advanced_swarmplot(): Creates a swarm plot to visualize the distribution of total bills for each day of the week, differentiated by gender, with facets for lunch and dinner, using customizations as specified. # Load the `tips` dataset tips = sns.load_dataset(\\"tips\\") # Create the advanced plot as specified sns.catplot( data=tips, kind=\\"swarm\\", x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", col=\\"time\\", palette=\\"deep\\", dodge=True, aspect=0.6, s=4 ) plt.show()"},{"question":"Objective In this question, you will work with the `torch.cuda.tunable` module to demonstrate your understanding of its functionalities. You will be tasked with enabling tunable operations, setting tuning parameters, performing a tuning operation, and managing the results. Problem Statement Write a function `perform_tuning_operations` that performs the following steps: 1. Enables the tunable operations. 2. Sets the maximum tuning duration to 5 seconds. 3. Sets the maximum tuning iterations to 100. 4. Sets the rotating buffer size to 10. 5. Tunes the GEMM operations based on a provided configuration file called `gemm_config.json`. 6. Writes the tuning results to a file named `tuning_results.json`. The function should return `True` if all the operations are successfully performed, otherwise it should return `False`. Expected Function Signature ```python import torch.cuda.tunable def perform_tuning_operations() -> bool: pass ``` Detailed Steps 1. Use `torch.cuda.tunable.enable()` to enable the tunable operations. 2. Use `torch.cuda.tunable.set_max_tuning_duration(5.0)` to set the maximum tuning duration to 5 seconds. 3. Use `torch.cuda.tunable.set_max_tuning_iterations(100)` to set the maximum number of iterations to 100. 4. Use `torch.cuda.tunable.set_rotating_buffer_size(10)` to set the rotating buffer size to 10. 5. Use `torch.cuda.tunable.tune_gemm_in_file(\'gemm_config.json\')` to tune the GEMM operations using the provided configuration file. 6. Use `torch.cuda.tunable.write_file(\'tuning_results.json\')` to write the tuning results to the file `tuning_results.json`. Constraints - Ensure that before performing each operation, check if the previous operation was successful. If any operation fails, the function should immediately return `False`. - The function assumes the provided configuration file `gemm_config.json` is in the correct format required by the `tune_gemm_in_file` function. Example Usage ```python result = perform_tuning_operations() print(result) # Output should be True if all operations were successful. ```","solution":"import torch.cuda.tunable as tunable def perform_tuning_operations() -> bool: try: # 1. Enable the tunable operations. if not tunable.enable(): return False # 2. Set the maximum tuning duration to 5 seconds. if not tunable.set_max_tuning_duration(5.0): return False # 3. Set the maximum tuning iterations to 100. if not tunable.set_max_tuning_iterations(100): return False # 4. Set the rotating buffer size to 10. if not tunable.set_rotating_buffer_size(10): return False # 5. Tune the GEMM operations based on a provided configuration file called \'gemm_config.json\'. if not tunable.tune_gemm_in_file(\'gemm_config.json\'): return False # 6. Write the tuning results to a file named \'tuning_results.json\'. if not tunable.write_file(\'tuning_results.json\'): return False return True except Exception as e: return False"},{"question":"**Objective:** Implement a Python function that recursively traverses a directory tree, collects information about files of specific types, and returns a summary of files categorized by type. **Problem Statement:** Write a Python function `summarize_file_types(directory: str) -> dict` that takes as input the path to a directory and returns a dictionary summarizing the count of various file types in that directory and its subdirectories. The dictionary should have the following structure: ```python { \'regular_files\': int, \'directories\': int, \'character_devices\': int, \'block_devices\': int, \'fifos\': int, \'symbolic_links\': int, \'sockets\': int, \'doors\': int, # applicable if your platform supports it \'event_ports\': int, # applicable if your platform supports it \'whiteouts\': int, # applicable if your platform supports it } ``` **Function Signature:** ```python def summarize_file_types(directory: str) -> dict: pass ``` **Constraints:** - Do not use any external libraries other than the `os` and `stat` modules. - Handle potential exceptions gracefully (e.g., permissions errors when accessing certain directories/files). **Example:** Suppose the directory structure is as follows: ``` /example file1.txt (regular file) file2.txt (regular file) subdir1 (directory) file3.txt (regular file) link1 -> file3.txt (symbolic link) subdir2 (directory) char_device (character device file) ``` For the call `summarize_file_types(\'/example\')`, the expected output should be: ```python { \'regular_files\': 3, \'directories\': 2, \'character_devices\': 1, \'block_devices\': 0, \'fifos\': 0, \'symbolic_links\': 1, \'sockets\': 0, \'doors\': 0, # if unsupported, should remain 0 \'event_ports\': 0, # if unsupported, should remain 0 \'whiteouts\': 0, # if unsupported, should remain 0 } ``` **Notes:** - Use the constants and functions defined in the `stat` module for identifying file types. - Consider platform-specific file types (like doors, event ports, or whiteouts) but do not assume they are supported on all systems. Handle unsupported types by counting them as zero.","solution":"import os import stat def summarize_file_types(directory: str) -> dict: summary = { \'regular_files\': 0, \'directories\': 0, \'character_devices\': 0, \'block_devices\': 0, \'fifos\': 0, \'symbolic_links\': 0, \'sockets\': 0, \'doors\': 0, \'event_ports\': 0, \'whiteouts\': 0, } def update_summary(path): try: mode = os.lstat(path).st_mode if stat.S_ISREG(mode): summary[\'regular_files\'] += 1 elif stat.S_ISDIR(mode): summary[\'directories\'] += 1 elif stat.S_ISCHR(mode): summary[\'character_devices\'] += 1 elif stat.S_ISBLK(mode): summary[\'block_devices\'] += 1 elif stat.S_ISFIFO(mode): summary[\'fifos\'] += 1 elif stat.S_ISLNK(mode): summary[\'symbolic_links\'] += 1 elif stat.S_ISSOCK(mode): summary[\'sockets\'] += 1 # Add checks for platform-specific file types if applicable # Example for doors (if supported on the platform) if hasattr(stat, \'S_ISDOOR\') and stat.S_ISDOOR(mode): summary[\'doors\'] += 1 if hasattr(stat, \'S_ISEVPORT\') and stat.S_ISEVPORT(mode): summary[\'event_ports\'] += 1 if hasattr(stat, \'S_ISWHT\') and stat.S_ISWHT(mode): summary[\'whiteouts\'] += 1 except OSError: pass for root, dirs, files in os.walk(directory, followlinks=False): for name in dirs + files: update_summary(os.path.join(root, name)) return summary"},{"question":"**Backup and Restore File System Using shutil** # Objective Create a Python script that can backup a directory to a compressed archive and later restore it. Your script should utilize multiple functions from the `shutil` module to handle these tasks efficiently. # Instructions 1. **Backup Function** - Implement a function `backup_directory(source_dir, archive_path)` that: - Takes `source_dir` (string): the path of the directory to backup. - Takes `archive_path` (string): the path where the compressed archive should be saved. - Creates a `.tar.gz` archive containing all files and subdirectories from `source_dir`. - Returns the path to the created archive. 2. **Restore Function** - Implement a function `restore_directory(archive_path, restore_dir)` that: - Takes `archive_path` (string): the path to the compressed archive to be restored. - Takes `restore_dir` (string): the path where the directory should be restored. - Extracts all contents of the archive to `restore_dir`. - Returns the path to the restored directory. # Constraints - You should handle possible exceptions that might occur during file operations, such as missing directories, permission errors, or invalid paths, and log appropriate error messages. # Example Usage ```python source_dir = \\"/path/to/source\\" backup_path = \\"/path/to/backup/archive.tar.gz\\" restore_path = \\"/path/to/restore/location\\" # Backup the directory archive_path = backup_directory(source_dir, backup_path) print(f\\"Backup created at: {archive_path}\\") # Restore the directory restored_dir = restore_directory(archive_path, restore_path) print(f\\"Directory restored to: {restored_dir}\\") ``` # Evaluation Criteria - Correctness: Ensure that the backup and restore functions correctly archive and restore the directory with all its contents. - Error handling: Properly handle and log exceptions that might occur during the operations. - Code clarity: Your code should be well-documented and easy to understand. - Use of `shutil`: Efficient and correct use of the `shutil` module functions for the required tasks. # Code Template ```python import shutil import os def backup_directory(source_dir, archive_path): try: archive_path = shutil.make_archive(archive_path, \'gztar\', source_dir) return archive_path except Exception as e: print(f\\"Error during backup: {e}\\") return None def restore_directory(archive_path, restore_dir): try: shutil.unpack_archive(archive_path, restore_dir) return restore_dir except Exception as e: print(f\\"Error during restore: {e}\\") return None # Example usage source_dir = \\"example_source_dir\\" backup_path = \\"example_backup_dir/archive\\" restore_path = \\"example_restore_dir\\" # Backup the directory archive_path = backup_directory(source_dir, backup_path) if archive_path: print(f\\"Backup created at: {archive_path}\\") # Restore the directory restored_dir = restore_directory(archive_path, restore_path) if restored_dir: print(f\\"Directory restored to: {restored_dir}\\") ```","solution":"import shutil import os def backup_directory(source_dir, archive_path): Creates a compressed archive of the specified source directory. :param source_dir: The directory to back up. :param archive_path: The path where the archive should be saved. :return: The path to the created archive or None if an error occurred. try: # Create a compressed archive with gzip format archive_path = shutil.make_archive(archive_path, \'gztar\', source_dir) return archive_path except Exception as e: print(f\\"Error during backup: {e}\\") return None def restore_directory(archive_path, restore_dir): Extracts a compressed archive to the specified directory. :param archive_path: The path to the archive to be restored. :param restore_dir: The directory where the contents should be restored. :return: The path to the restored directory or None if an error occurred. try: # Extract the archive shutil.unpack_archive(archive_path, restore_dir) return restore_dir except Exception as e: print(f\\"Error during restore: {e}\\") return None"},{"question":"# Objective The objective of this question is to assess your ability to use the pandas library for data visualization. You will need to demonstrate comprehension of various plotting techniques, data preprocessing, and customization of plot appearance. # Problem Statement You are provided with a CSV file named `sales_data.csv` which contains sales data for a retail store across different months. The dataset has the following columns: - `Date`: Date of the sales record. - `Store`: Store identifier. - `Product`: Product category. - `Revenue`: Revenue generated in dollars. - `Units Sold`: Number of units sold. You need to perform the following tasks: 1. **Data Loading and Preprocessing:** - Load the dataset into a pandas DataFrame. - Ensure the `Date` column is in datetime format. - Set the `Date` column as the index of the DataFrame. 2. **Visualization:** - Plot the time series of total revenue over the entire period. - Create a bar plot showing total revenue by `Store`. - Plot a stacked bar plot showing total revenue for each `Product` category by `Store`. - Create a scatter plot showing the relationship between `Units Sold` and `Revenue`. - Generate a box plot comparing the distribution of monthly revenue for each product category. - Plot a pie chart showing the proportion of total revenue generated by each `Store`. 3. **Plot Customization:** - Apply a suitable colormap to distinguish between different product categories in the stacked bar plot. - Customize the scatter plot to color the points based on the `Product` category. - Annotate the time series plot with markers at the points where revenue exceeds a certain threshold (e.g., 50,000). - Customize the box plot to have different colors for each product category and add a title to the plot. - Adjust the pie chart to have an equal aspect ratio and add labels and a legend. # Input - The input file `sales_data.csv` contains the sales data. - Example data format: ``` Date,Store,Product,Revenue,Units Sold 2023-01-01,Store A,Electronics,12000,34 2023-01-01,Store B,Clothing,8000,45 ... ``` # Requirements - Use pandas and matplotlib libraries. - Ensure your plots are well-labeled and visually clear. - Handle any missing data appropriately. # Constraints - Do not use any other libraries for data manipulation or plotting other than pandas and matplotlib. - Ensure your solution runs efficiently for large datasets. # Submission 1. Submit a Python script or Jupyter notebook with your solution. 2. Your script/notebook should include proper comments and documentation. # Evaluation Your solution will be evaluated based on: - Correctness of the data loading and preprocessing steps. - Completeness and accuracy of the required visualizations. - Quality and clarity of the plots. - Proper handling of missing data. - Code readability and documentation. # Example ```python import pandas as pd import matplotlib.pyplot as plt # Load the dataset df = pd.read_csv(\'sales_data.csv\') # Convert the Date column to datetime format and set it as the index df[\'Date\'] = pd.to_datetime(df[\'Date\']) df.set_index(\'Date\', inplace=True) # Plot total revenue over the entire period total_revenue = df[\'Revenue\'].resample(\'M\').sum() total_revenue.plot(title=\'Total Revenue Over Time\') plt.axhline(y=50000, color=\'r\', linestyle=\'--\') plt.show() # Create a bar plot showing total revenue by store revenue_by_store = df.groupby(\'Store\')[\'Revenue\'].sum() revenue_by_store.plot(kind=\'bar\', title=\'Total Revenue by Store\') plt.show() # Plot a stacked bar plot showing total revenue for each product category by store revenue_by_product_and_store = df.groupby([\'Store\', \'Product\'])[\'Revenue\'].sum().unstack() revenue_by_product_and_store.plot(kind=\'bar\', stacked=True, colormap=\'viridis\', title=\'Total Revenue by Product and Store\') plt.show() # Create a scatter plot showing the relationship between Units Sold and Revenue df.plot.scatter(x=\'Units Sold\', y=\'Revenue\', c=\'Product\', colormap=\'viridis\', title=\'Units Sold vs Revenue\') plt.show() # Generate a box plot comparing the distribution of monthly revenue for each product category monthly_revenue_by_product = df.groupby([\'Product\', pd.Grouper(freq=\'M\')])[\'Revenue\'].sum().unstack(level=0) monthly_revenue_by_product.plot(kind=\'box\', title=\'Monthly Revenue Distribution by Product\') plt.show() # Plot a pie chart showing the proportion of total revenue generated by each Store revenue_by_store.plot(kind=\'pie\', autopct=\'%.2f\', legend=True, title=\'Revenue Proportion by Store\', figsize=(6, 6)) plt.ylabel(\'\') plt.show() ```","solution":"import pandas as pd import matplotlib.pyplot as plt def load_and_preprocess_data(file_path): # Load the dataset df = pd.read_csv(file_path) # Convert the Date column to datetime format and set it as the index df[\'Date\'] = pd.to_datetime(df[\'Date\']) df.set_index(\'Date\', inplace=True) return df def plot_time_series_revenue(df): # Plot the time series of total revenue over the entire period total_revenue = df[\'Revenue\'].resample(\'M\').sum() total_revenue.plot(title=\'Total Revenue Over Time\') plt.axhline(y=50000, color=\'r\', linestyle=\'--\', label=\'Threshold\') plt.legend() plt.show() def plot_revenue_by_store(df): # Create a bar plot showing total revenue by store revenue_by_store = df.groupby(\'Store\')[\'Revenue\'].sum() revenue_by_store.plot(kind=\'bar\', title=\'Total Revenue by Store\') plt.show() def plot_stacked_revenue_by_store(df): # Plot a stacked bar plot showing total revenue for each product category by store revenue_by_product_and_store = df.groupby([\'Store\', \'Product\'])[\'Revenue\'].sum().unstack() revenue_by_product_and_store.plot(kind=\'bar\', stacked=True, colormap=\'viridis\', title=\'Total Revenue by Product and Store\') plt.show() def plot_scatter_units_revenue(df): # Create a scatter plot showing the relationship between Units Sold and Revenue categories = df[\'Product\'].astype(\'category\').cat.codes scatter = plt.scatter(df[\'Units Sold\'], df[\'Revenue\'], c=categories, cmap=\'viridis\') plt.colorbar(scatter, label=\\"Product Categories\\") plt.xlabel(\'Units Sold\') plt.ylabel(\'Revenue\') plt.title(\'Units Sold vs Revenue\') plt.show() def plot_box_monthly_revenue(df): # Generate a box plot comparing the distribution of monthly revenue for each product category monthly_revenue_by_product = df.groupby([\'Product\', pd.Grouper(freq=\'M\')])[\'Revenue\'].sum().unstack(level=0) monthly_revenue_by_product.plot(kind=\'box\', title=\'Monthly Revenue Distribution by Product\') plt.show() def plot_pie_revenue_by_store(df): # Plot a pie chart showing the proportion of total revenue generated by each store revenue_by_store = df.groupby(\'Store\')[\'Revenue\'].sum() revenue_by_store.plot(kind=\'pie\', autopct=\'%.2f%%\', legend=True, title=\'Revenue Proportion by Store\', figsize=(6, 6)) plt.ylabel(\'\') plt.show()"},{"question":"# Task Description You are to implement a custom SMTP server by subclassing the `smtpd.SMTPServer` class. The server should log and store received emails in a specific format, with a few additional requirements outlined below. # Requirements 1. **Initialization**: - Your custom server should accept email data size up to 10 MB. - The server should support SMTPUTF8 extensions but not decode data. 2. **Email Processing**: - Override the `process_message()` method to store the email\'s metadata and content. - Store the email\'s metadata, including sender (mailfrom), recipients (rcpttos), and subject (parse from data). - Store the email content (data) as it is. - Implement a thread-safe mechanism (e.g., using threading locks) to store emails in an in-memory list. 3. **Custom Logging Format**: - Each email should be stored as a dictionary in the following format: ``` { \\"sender\\": <sender_email>, \\"recipients\\": <list_of_recipients>, \\"subject\\": <email_subject>, \\"content\\": <email_content> } ``` - The subject of the email should be parsed from the email headers in the data. 4. **Class Design**: - Implement a `CustomSMTPServer` class inheriting from `smtpd.SMTPServer`. - Create a method `get_stored_emails()` in the `CustomSMTPServer` class to retrieve all stored emails. # Input and Output Format - **Initialization**: ```python server = CustomSMTPServer((\'localhost\', 1025), None) ``` - **Example SMTP `DATA` content**: ``` From: <sender@example.com> To: <recipient@example.com> Subject: Test Email This is a test email message. ``` # Constraints - The server should handle the connection asynchronously and be thread-safe. - You must parse the subject from the `DATA` content correctly. - Ensure that your solution can handle multiple concurrent connections efficiently. # Example ```python import smtpd import asyncore import threading class CustomSMTPServer(smtpd.SMTPServer): def __init__(self, localaddr, remoteaddr, data_size_limit=10485760, map=None, enable_SMTPUTF8=True): super().__init__(localaddr, remoteaddr, data_size_limit=data_size_limit, enable_SMTPUTF8=enable_SMTPUTF8, decode_data=False) self.emails = [] self.lock = threading.Lock() def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): subject = \' \'.join([line[9:] for line in data.split(\'n\') if line.startswith(\\"Subject: \\")]) email_data = { \\"sender\\": mailfrom, \\"recipients\\": rcpttos, \\"subject\\": subject, \\"content\\": data } with self.lock: self.emails.append(email_data) return None def get_stored_emails(self): with self.lock: return self.emails # Example instantiation and running the server if __name__ == \\"__main__\\": server = CustomSMTPServer((\'localhost\', 1025), None) asyncore.loop() ``` The provided solution sets up a custom SMTP server that listens on localhost at port 1025, processes incoming emails, and stores them in an in-memory list with meta information.","solution":"import smtpd import asyncore import threading from email.parser import Parser class CustomSMTPServer(smtpd.SMTPServer): def __init__(self, localaddr, remoteaddr, data_size_limit=10485760, enable_SMTPUTF8=True, map=None): super().__init__(localaddr, remoteaddr, data_size_limit=data_size_limit, enable_SMTPUTF8=enable_SMTPUTF8, decode_data=False) self.emails = [] self.lock = threading.Lock() def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): headers = Parser().parsestr(data) subject = headers[\'subject\'] if \'subject\' in headers else \'\' email_data = { \\"sender\\": mailfrom, \\"recipients\\": rcpttos, \\"subject\\": subject, \\"content\\": data } with self.lock: self.emails.append(email_data) return None def get_stored_emails(self): with self.lock: return list(self.emails) # Example instantiation and running the server # Please ensure to run this in a safe environment where you can handle an actual server. if __name__ == \\"__main__\\": server = CustomSMTPServer((\'localhost\', 1025), None) asyncore.loop()"},{"question":"**Task: Implement and Test a Custom Collection Class using `unittest`** **Objective:** In this task, you will implement a custom collection class called `CustomList` that mimics a simplified list but with additional constraints and methods. Then, you will write a suite of unit tests using Python\'s `unittest` framework to verify the correct functionality of your `CustomList` class. **Requirements:** 1. **Class Implementation:** - Create a class `CustomList` that has the following methods: - `__init__(self)`: Initializes an empty list. - `add(self, item)`: Adds an item to the end of the list. Raises a `TypeError` if the item is not an integer. - `remove(self, item)`: Removes the first occurrence of the item from the list. Raises a `ValueError` if the item is not found. - `get(self, index)`: Returns the item at the given index. Raises an `IndexError` if the index is out of bounds. - `__len__(self)`: Returns the number of items in the list. - `__contains__(self, item)`: Returns `True` if the item is in the list, otherwise `False`. 2. **Unit Test Implementation:** - Create a test class `TestCustomList` to test the `CustomList` class. - Your test class should inherit from `unittest.TestCase`. - Implement the following test methods in your test class: - `test_add`: Verifies that items are added correctly to the list. - `test_add_type_error`: Verifies that adding a non-integer raises a `TypeError`. - `test_remove`: Verifies that items are removed correctly from the list. - `test_remove_value_error`: Verifies that removing a non-existent item raises a `ValueError`. - `test_get`: Verifies that items are retrieved correctly by index. - `test_get_index_error`: Verifies that accessing an out-of-bounds index raises an `IndexError`. - `test_len`: Verifies that the length of the list is reported correctly. - `test_contains`: Verifies that the `__contains__` method works correctly. **Input/Output Format and Constraints:** - You do not need to handle any input or output; focus solely on the implementation and testing. - You should include the necessary `import` statements. - Ensure that your code follows best practices for unit tests, including set up and teardown if necessary. **Performance Requirements:** - The methods `add`, `remove`, and `get` should have an average time complexity of O(1) for `add` and `remove` (assuming item is found near the start) and O(n) for `remove` (in worst case). - The methods `__len__` and `__contains__` should have a time complexity of O(1) and O(n) respectively. **Example Usage:** ```python clist = CustomList() clist.add(5) clist.add(10) print(len(clist)) # Output: 2 print(clist.get(1)) # Output: 10 clist.remove(5) print(5 in clist) # Output: False ``` **Your Task:** - Implement the `CustomList` class. - Implement the `unittest` test cases for the `CustomList` class. ```python import unittest class CustomList: def __init__(self): ... def add(self, item): ... def remove(self, item): ... def get(self, index): ... def __len__(self): ... def __contains__(self, item): ... class TestCustomList(unittest.TestCase): def test_add(self): ... def test_add_type_error(self): ... def test_remove(self): ... def test_remove_value_error(self): ... def test_get(self): ... def test_get_index_error(self): ... def test_len(self): ... def test_contains(self): ... if __name__ == \'__main__\': unittest.main() ```","solution":"class CustomList: def __init__(self): self._list = [] def add(self, item): if not isinstance(item, int): raise TypeError(\\"Only integers are allowed\\") self._list.append(item) def remove(self, item): if item not in self._list: raise ValueError(\\"Item not found in list\\") self._list.remove(item) def get(self, index): if index < 0 or index >= len(self._list): raise IndexError(\\"Index out of bounds\\") return self._list[index] def __len__(self): return len(self._list) def __contains__(self, item): return item in self._list"},{"question":"Objective: You are required to implement a cross-platform Python function using the `asyncio` module. The function should demonstrate your understanding of event loops and platform-specific constraints as highlighted in the provided documentation. Task: Write a Python function `run_platform_specific_tasks()` that performs the following tasks: 1. Runs a simple I/O-bound task that periodically prints a message at a given interval. 2. Handles the differences in asyncio event loops between Windows and Unix-like systems (Linux/macOS). Input: - `interval`: An integer representing the time interval (in seconds) between messages being printed. - `message`: A string message that needs to be printed. Output: - Prints the provided message at the specified interval, taking into account platform-specific constraints for asyncio event loops. Constraints: - Your function should be compatible with both Windows and Unix-like systems. - Ensure proper handling of platform-specific limitations on methods such as `loop.add_reader()` and `loop.add_writer()` for file I/O, subprocess handling, and signal handling. - Use appropriate event loop mechanisms based on the operating system. Example Usage: ```python import asyncio async def run_platform_specific_tasks(interval, message): # Define an async task that prints a message periodically async def periodic_task(): while True: print(message) await asyncio.sleep(interval) # Configure the event loop based on the platform if platform.system() == \'Windows\': loop = asyncio.ProactorEventLoop() asyncio.set_event_loop(loop) else: loop = asyncio.get_event_loop() # Run the periodic task try: loop.run_until_complete(periodic_task()) finally: loop.close() # Example call run_platform_specific_tasks(2, \\"Hello, World!\\") ``` Performance Requirements: - Ensure the function runs efficiently with minimal overhead. - Properly manage the asynchronous tasks to avoid blocking and ensure smooth execution. --- Note: You may use the `platform` module to detect the operating system type in your solution. Ensure that your code follows best practices for handling asynchronous programming and is well-documented with comments explaining your design choices.","solution":"import asyncio import platform async def run_periodic_task(interval, message): while True: print(message) await asyncio.sleep(interval) def run_platform_specific_tasks(interval, message): # Configure the event loop based on the platform if platform.system() == \'Windows\': loop = asyncio.ProactorEventLoop() asyncio.set_event_loop(loop) else: loop = asyncio.get_event_loop() # Run the periodic task try: loop.run_until_complete(run_periodic_task(interval, message)) finally: loop.close()"},{"question":"**Objective**: To assess your ability to use Python\'s debugging and profiling tools effectively. **Problem Statement**: You are given a Python script that performs the following operations: 1. Generates a list of `N` random integers. 2. Sorts the list using the built-in `sorted()` method. 3. Finds the maximum value in the list. 4. Computes the sum of all integers in the list. Your tasks are: 1. **Debugging**: Use the `pdb` module to identify and fix any bugs in the script. 2. **Profiling**: Use the `cProfile` module to profile the script and identify any performance bottlenecks. 3. **Execution Time Measurement**: Use the `timeit` module to measure the execution time for sorting the list and finding the maximum value. # Requirements: 1. **Debugging**: - Use `pdb` to step through the code and identify any logical or runtime errors. - Write down the steps you took to debug the code. 2. **Profiling**: - Use `cProfile` to create a profile of the script. - Identify which parts of the code are consuming the most time. 3. **Execution Time Measurement**: - Use `timeit` to measure the execution time of sorting the list and finding the maximum value. - Report the execution times. # Input and Output Formats: - **Input**: - An integer `N` (size of the list) - **Output**: - Steps taken to debug the code. - The profile report generated by `cProfile`. - Execution time for sorting the list and finding the maximum value. # Example Script: ```python import random def generate_list(N): return [random.randint(0, 1000) for _ in range(N)] def sort_list(lst): return sorted(lst) def find_max(lst): return max(lst) def compute_sum(lst): return sum(lst) if __name__ == \\"__main__\\": N = 1000 # Example input size lst = generate_list(N) sorted_lst = sort_list(lst) max_value = find_max(sorted_lst) total_sum = compute_sum(sorted_lst) print(f\\"Max value: {max_value}\\") print(f\\"Total sum: {total_sum}\\") ``` # Constraints: - Assume `N` will be a positive integer ranging from 1 to 10,000. # Submission: - Submit a Python script file named `debug_profile_timeit.py` containing: - The original and corrected script. - Debugging steps. - Profiling report and analysis. - Execution time measurements for sorting and finding the maximum value. Good luck!","solution":"import random import pdb import cProfile import timeit def generate_list(N): return [random.randint(0, 1000) for _ in range(N)] def sort_list(lst): return sorted(lst) def find_max(lst): return max(lst) def compute_sum(lst): return sum(lst) if __name__ == \\"__main__\\": N = 1000 # Example input size # Use pdb to debug the script, set a breakpoint before the list generation. pdb.set_trace() lst = generate_list(N) sorted_lst = sort_list(lst) max_value = find_max(sorted_lst) total_sum = compute_sum(sorted_lst) print(f\\"Max value: {max_value}\\") print(f\\"Total sum: {total_sum}\\") # Use cProfile to profile the script cProfile.run(\\"generate_list(N)\\", sort_list(lst), find_max(sorted_lst), compute_sum(sorted_lst)) # Use timeit to measure the execution time for sorting and finding max sort_time = timeit.timeit(stmt=\\"sort_list(lst)\\", globals=globals(), number=1000) max_time = timeit.timeit(stmt=\\"find_max(lst)\\", globals=globals(), number=1000) print(f\\"Sorting Time: {sort_time} seconds\\") print(f\\"Max Finding Time: {max_time} seconds\\")"},{"question":"**Objective:** Implement a function `validate_and_format` that processes a given list of strings to identify strings that match a specific pattern, extract necessary data from those strings, and format the extracted data based on given templates. **Function Signature:** ```python def validate_and_format(strings: list, regex_pattern: str, template: str) -> list: pass ``` **Parameters:** - `strings`: A list of strings to be processed. - `regex_pattern`: A regular expression pattern used to match and extract parts of the strings. - `template`: A string template used to format the extracted parts. **Returns:** - A list of formatted strings where each string conforms to the given template, containing only the valid and extracted parts from the input strings. **Details:** 1. The function should use the provided `regex_pattern` to identify which of the input `strings` match the given pattern. 2. If a string matches, extract the groups from the match object. 3. Use the `template` to format the extracted groups appropriately. The template can contain placeholders like `{0}`, `{1}`, etc., where `{0}` refers to the first extracted group, `{1}` the second, etc. 4. Return a list of formatted strings containing only the successfully validated input strings in the specified format. **Example:** ```python def validate_and_format(strings, regex_pattern, template): import re result = [] pattern = re.compile(regex_pattern) for string in strings: match = pattern.match(string) if match: formatted_string = template.format(*match.groups()) result.append(formatted_string) return result # Sample usage: input_strings = [ \\"John Doe, 1234, johndoe@example.com\\", \\"Alice Smith, 5678, alice.smith@example.com\\", \\"Invalid Entry, no digits here, wrongemail.com\\" ] pattern = r\\"([a-zA-Z ]+),s*(d+),s*([w.-]+@[w.-]+)\\" template = \\"Name: {0}, ID: {1}, Email: {2}\\" print(validate_and_format(input_strings, pattern, template)) ``` **Expected Output:** ```python [ \\"Name: John Doe, ID: 1234, Email: johndoe@example.com\\", \\"Name: Alice Smith, ID: 5678, Email: alice.smith@example.com\\" ] ``` **Constraints:** - You may assume that the input `strings` list and `template` will always be valid. - The `regex_pattern` will always contain valid regex syntax. - Make sure to handle and return empty lists when no matches are found. This exercise tests the student\'s comprehension of string manipulation, regular expression processing, and string formatting, which are pivotal components of text processing in Python.","solution":"def validate_and_format(strings: list, regex_pattern: str, template: str) -> list: import re result = [] pattern = re.compile(regex_pattern) for string in strings: match = pattern.match(string) if match: formatted_string = template.format(*match.groups()) result.append(formatted_string) return result"},{"question":"# Question: Caching and Functional Programming with `functools` You are given a function `expensive_calculation()` that performs an expensive calculation and a list of numbers. Implement two functions, `cached_factorial()` to compute and cache the factorial of numbers using `functools.cache`, and `sum_of_factorials()` to compute the sum of factorials for a list of numbers using `functools.reduce`. Requirements: 1. **cached_factorial(n)** - Uses `@functools.cache` to cache computed factorials. - Takes an integer `n` and returns the factorial of `n`. - Use recursive calculation for the factorial computation. 2. **sum_of_factorials(number_list)** - Uses `functools.reduce` to compute the sum of the factorials of numbers. - Takes a list `number_list` of integers and returns the sum of their factorials. # Constraints: - `0 <= n <= 100` for `cached_factorial` - The length of `number_list` will not exceed 20. # Performance Requirements: - Ensure `cached_factorial` efficiently reuses previously computed values due to caching. - `sum_of_factorials` should not repeatedly compute the same factorial for identical input values. # Input/Output Format: ```python def cached_factorial(n: int) -> int: pass def sum_of_factorials(number_list: List[int]) -> int: pass # Example print(sum_of_factorials([1, 2, 3, 4])) # Output: 33 # Explanation: 1! + 2! + 3! + 4! = 1 + 2 + 6 + 24 = 33 ``` # Note: - You can assume the availability of the necessary modules from Python\'s standard library, specifically `functools`.","solution":"import functools @functools.cache def cached_factorial(n): Computes and caches the factorial of number n using recursion. Parameters: n (int): The number to compute the factorial of. Returns: int: The factorial of n. if n == 0: return 1 else: return n * cached_factorial(n - 1) def sum_of_factorials(number_list): Computes the sum of factorials for a list of numbers using functools.reduce. Parameters: number_list (list of int): List of integers to compute the sum of their factorials. Returns: int: The sum of the factorials of the numbers in the list. return functools.reduce(lambda acc, x: acc + cached_factorial(x), number_list, 0)"},{"question":"Objective Your task is to implement a function that will preprocess input data, validate it, and then fit a simple linear model using provided inputs while ensuring compatibility and correctness of the data using various utilities available in the `sklearn.utils` module. Problem Statement Implement a function `fit_linear_model(X, y, random_state=None)` that takes the following inputs: - `X`: A dataset represented as a 2D array-like structure (e.g., list of lists, numpy array, etc.). - `y`: The target values corresponding to the dataset, represented as a 1D array-like structure. - `random_state`: An optional parameter to ensure reproducibility (default is None). The function should: 1. Validate the input `X` and `y` using appropriate utilities to ensure they are consistent and correctly formatted. 2. Ensure `X` contains only finite values. 3. Convert `X` to a float array if it is not already. 4. Initialize a random state object using the `random_state` parameter. 5. Fit a simple linear regression model on the data. (You may use any linear regression algorithm from scikit-learn or implement a basic linear regression yourself). 6. Return the coefficients of the fitted model. Constraints - You are required to use appropriate functions from the `sklearn.utils` module wherever applicable. - The input `X` must be a 2D array and `y` must be a 1D array. - Handle any exceptions gracefully and provide meaningful error messages. Expected Function Signature ```python import numpy as np from sklearn.utils import check_random_state, check_array, check_X_y, assert_all_finite, as_float_array from sklearn.linear_model import LinearRegression def fit_linear_model(X, y, random_state=None): # Step 1: Validate X and y X, y = check_X_y(X, y) # Step 2: Convert X to float array and ensure all values are finite X = as_float_array(X) assert_all_finite(X) # Step 3: Initialize random state random_state = check_random_state(random_state) # Step 4: Fit linear regression model model = LinearRegression() model.fit(X, y) # Return the coefficients of the fitted model return model.coef_ ``` Example Usage ```python X = [[1, 2], [2, 3], [3, 4]] y = [1, 2, 3] random_state = 42 coefficients = fit_linear_model(X, y, random_state) print(coefficients) # Output should be the coefficients of the fitted linear model ``` Provide an implementation of the function and ensure it passes the given example usage.","solution":"import numpy as np from sklearn.utils import check_random_state, check_array, check_X_y, assert_all_finite, as_float_array from sklearn.linear_model import LinearRegression def fit_linear_model(X, y, random_state=None): Preprocess input data, validate it, and fit a simple linear model. Args: - X: 2D array-like structure representing input data. - y: 1D array-like structure representing target values. - random_state: Optional; to ensure reproducibility. Returns: - Coefficients of the fitted linear model. # Step 1: Validate X and y X, y = check_X_y(X, y) # Step 2: Convert X to float array and ensure all values are finite X = as_float_array(X) assert_all_finite(X) # Step 3: Initialize random state random_state = check_random_state(random_state) # Step 4: Fit linear regression model model = LinearRegression() model.fit(X, y) # Return the coefficients of the fitted model return model.coef_"},{"question":"# Advanced Python Object Manipulations Objective: You are required to implement a set of functions to simulate custom object manipulations in pure Python. These functions should demonstrate handling attributes, comparisons, and custom string representations. Task: 1. **Implement Attribute Management Functions**: - `has_attr(obj, attr_name)`: Check if the object `obj` has an attribute named `attr_name`. Return `True` or `False`. - `get_attr(obj, attr_name)`: Retrieve the attribute `attr_name` from object `obj`. Return the attribute value if it exists. Raise an `AttributeError` if it does not. - `set_attr(obj, attr_name, value)`: Set the attribute `attr_name` of object `obj` to `value`. - `del_attr(obj, attr_name)`: Delete the attribute `attr_name` from object `obj`. Raise an `AttributeError` if it does not exist. 2. **Implement Comparison and Representation Functions**: - `compare_objects(obj1, obj2, op)`: Compare the values of `obj1` and `obj2` using the operation specified by `op`, which can be one of the strings `\'<\', \'<=\', \'==\', \'!=\', \'>\', \'>=\'`. Return the result of the comparison. - `custom_repr(obj)`: Return a string that represents the object `obj`. This should be similar to Python\'s built-in `repr`, but escape non-ASCII characters using `x`, `u`, or `U` notation. # Constraints: - Assume all `attr_name` values are strings. - Assume `op` values in `compare_objects` are always valid and correspond to the specified operations. - Implement proper exception handling as specified. # Examples: ```python class Sample: def __init__(self, value): self.value = value # Example 1 - Attribute Management obj = Sample(10) assert not has_attr(obj, \'name\') set_attr(obj, \'name\', \'example\') assert has_attr(obj, \'name\') assert get_attr(obj, \'name\') == \'example\' del_attr(obj, \'name\') # del_attr(obj, \'name\') # This should raise AttributeError # Example 2 - Comparison and Representation obj1 = Sample(5) obj2 = Sample(10) assert compare_objects(obj1.value, obj2.value, \'<\') == True assert compare_objects(obj1.value, obj2.value, \'>=\') == False obj3 = Sample(\'Text with non-ascii ñ\') assert custom_repr(obj3.value) == \\"\'Text with non-ascii xf1\'\\" ``` # Requirements: - Your solution should utilize Python\'s built-in functions and types wherever possible. - Your solution should be structured and organized, implementing error handling where necessary. - Ensure your code is readable with appropriate comments explaining the logic.","solution":"def has_attr(obj, attr_name): Check if the object `obj` has an attribute named `attr_name`. return hasattr(obj, attr_name) def get_attr(obj, attr_name): Retrieve the attribute `attr_name` from object `obj`. Raise an AttributeError if the attribute does not exist. if hasattr(obj, attr_name): return getattr(obj, attr_name) else: raise AttributeError(f\\"\'{type(obj).__name__}\' object has no attribute \'{attr_name}\'\\") def set_attr(obj, attr_name, value): Set the attribute `attr_name` of object `obj` to `value`. setattr(obj, attr_name, value) def del_attr(obj, attr_name): Delete the attribute `attr_name` from object `obj`. Raise an AttributeError if the attribute does not exist. if hasattr(obj, attr_name): delattr(obj, attr_name) else: raise AttributeError(f\\"\'{type(obj).__name__}\' object has no attribute \'{attr_name}\'\\") def compare_objects(obj1, obj2, op): Compare the values of `obj1` and `obj2` using the operation `op`. comparison_ops = { \'<\': lambda x, y: x < y, \'<=\': lambda x, y: x <= y, \'==\': lambda x, y: x == y, \'!=\': lambda x, y: x != y, \'>\': lambda x, y: x > y, \'>=\': lambda x, y: x >= y } if op in comparison_ops: return comparison_ops[op](obj1, obj2) else: raise ValueError(\\"Invalid comparison operator\\") def custom_repr(obj): Return a string that represents the object `obj`, escaping non-ASCII characters. return repr(obj).encode(\'unicode_escape\').decode(\'ascii\')"},{"question":"# Priority Queue with Priority Updates In this task, you need to implement a priority queue using the `heapq` module that allows tasks to be added with priorities, and also allows the priorities of existing tasks to be updated. The priority queue should always return the task with the highest priority (lowest numerical value). The priority queue should support the following operations: 1. `add_task(task: str, priority: int)`: Adds a new task with the given priority or updates the priority of an existing task. 2. `remove_task(task: str)`: Removes an existing task from the priority queue. 3. `pop_task() -> str`: Removes and returns the task with the highest priority. If the queue is empty, it should raise a `KeyError`. To achieve this, you will need to manage both the heap invariant and track the entries in a dictionary to ensure that priority updates and task deletions are efficient and maintain the integrity of the queue. **Function Signatures**: ```python import heapq from typing import Dict class PriorityQueue: def __init__(self): self.pq = [] self.entry_finder: Dict[str, list] = {} self.REMOVED = \'<removed-task>\' self.counter = itertools.count() def add_task(self, task: str, priority: int) -> None: if task in self.entry_finder: self.remove_task(task) count = next(self.counter) entry = [priority, count, task] self.entry_finder[task] = entry heapq.heappush(self.pq, entry) def remove_task(self, task: str) -> None: entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def pop_task(self) -> str: while self.pq: priority, count, task = heapq.heappop(self.pq) if task is not self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\') ``` Your implementation should maintain the heap property and support efficient addition, removal, and popping of tasks. # Constraints - Task names are unique, non-empty strings. - Priorities are integer values (can be positive or negative). - The queue should handle multiple tasks and maintain the correct order as specified. # Example Usage ```python # Example workflow pq = PriorityQueue() pq.add_task(\'task1\', priority=2) pq.add_task(\'task2\', priority=1) pq.add_task(\'task3\', priority=3) assert pq.pop_task() == \'task2\' # task2 has the highest priority (lowest priority number) pq.add_task(\'task1\', priority=0) # update task1 priority assert pq.pop_task() == \'task1\' # task1 now has the highest priority pq.add_task(\'task4\', priority=1) assert pq.pop_task() == \'task4\' assert pq.pop_task() == \'task3\' ``` # Note - Make sure to handle attempts to remove non-existent tasks gracefully. - Ensure that your solution runs efficiently, particularly for large numbers of tasks. Good luck!","solution":"import heapq import itertools from typing import Dict class PriorityQueue: def __init__(self): self.pq = [] # The heap queue list self.entry_finder: Dict[str, list] = {} # Mapping of tasks to entries self.REMOVED = \'<removed-task>\' # Placeholder for a removed task self.counter = itertools.count() # Unique sequence count def add_task(self, task: str, priority: int) -> None: Add a new task or update the priority of an existing task. if task in self.entry_finder: self.remove_task(task) count = next(self.counter) entry = [priority, count, task] self.entry_finder[task] = entry heapq.heappush(self.pq, entry) def remove_task(self, task: str) -> None: Mark an existing task as REMOVED. Raise KeyError if not found. entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def pop_task(self) -> str: Remove and return the lowest priority task. Raise KeyError if empty. while self.pq: priority, count, task = heapq.heappop(self.pq) if task is not self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\')"},{"question":"Design a function that dynamically imports a module and retrieves metadata from it using the `importlib` module. The function should take the name of the module as a string input and return its version, list of entry points, and list of dependencies formatted in a specific way. # Function Signature ```python def get_module_metadata(module_name: str) -> dict: pass ``` # Input - `module_name` (str): The name of the module to be imported. # Output - `metadata` (dict): A dictionary containing: - `version` (str): The version of the module. - `entry_points` (list of str): A list of entry points provided by the module. - `dependencies` (list of str): A list of modules that this module depends on. # Requirements - The function should first check if the module can be imported using `importlib.util.find_spec()`. If yes, proceed to the next step; if no, raise an `ImportError`. - Use the `importlib.metadata` module to retrieve the package distribution metadata, including version, entry points, and dependencies. - Ensure the returned metadata dictionary conforms strictly to the specified format. # Constraints - You can assume that the module name provided is a valid string and if the module exists, it will be importable. - Performance is not a primary concern for this task, but the solution should not rely on disk I/O beyond what is necessary for metadata retrieval. # Example ```python # Suppose `example_module` has version \'1.0\', entry points [\'main\'], and dependencies [\'dep1\', \'dep2\']. module_name = \\"example_module\\" metadata = get_module_metadata(module_name) print(metadata) # Expected output: # { # \\"version\\": \\"1.0\\", # \\"entry_points\\": [\\"main\\"], # \\"dependencies\\": [\\"dep1\\", \\"dep2\\"] # } ``` # Note: To fully test this function, you should ensure that the modules being queried for metadata are installed in your Python environment.","solution":"import importlib.util import importlib.metadata def get_module_metadata(module_name: str) -> dict: Returns the metadata of the specified module. Parameters: - module_name (str): The name of the module to be imported. Returns: - metadata (dict): A dictionary containing: - \'version\' (str): The version of the module. - \'entry_points\' (list of str): A list of entry points provided by the module. - \'dependencies\' (list of str): A list of modules that this module depends on. # Check if the module can be imported spec = importlib.util.find_spec(module_name) if spec is None: raise ImportError(f\\"Module {module_name} not found\\") # Retrieve the package distribution metadata distribution = importlib.metadata.distribution(module_name) version = distribution.version # List entry points entry_points = [ep.name for ep in distribution.entry_points] # List dependencies (requirements) dependencies = list(distribution.requires) if distribution.requires else [] return { \\"version\\": version, \\"entry_points\\": entry_points, \\"dependencies\\": dependencies }"},{"question":"**Question: Creating Customized Seaborn Plots with Annotations** You have been provided with a dataset about tips received by waitstaff in a restaurant. Your task is to create a customized plot using the seaborn objects interface that includes annotations and properly aligned text. # Dataset The dataset contains the following columns: - `total_bill`: Total bill amount (in dollars) - `tip`: Tip amount (in dollars) - `sex`: Gender of the person who paid the bill (`Male` or `Female`) - `smoker`: Whether the person was a smoker (`Yes` or `No`) - `day`: Day of the week (`Thur`, `Fri`, `Sat`, `Sun`) - `time`: Time of the day (`Lunch` or `Dinner`) - `size`: Size of the party # Task 1. Load the tips dataset using the seaborn `load_dataset` function. 2. Create a scatter plot with: - `total_bill` on the x-axis. - `tip` on the y-axis. - Different colors for different days. 3. Add text annotations that display the size of the party for each point. 4. Ensure the text annotations are aligned `right` and are positioned slightly above the points. 5. Bold the text annotations. # Output - The function should return a seaborn plot object with the specified customizations. # Function Signature ```python import seaborn.objects as so from seaborn import load_dataset def create_custom_plot(): pass ``` # Example Calling `create_custom_plot()` should display a scatter plot with the described customizations. # Constraints - Use the seaborn objects interface as demonstrated in the documentation provided. - Ensure the text annotations do not overlap and are clearly visible. - Use appropriate seaborn and matplotlib parameters to achieve the desired customizations. **Note:** You can add more customizations as needed to make the plot more informative and aesthetically pleasing.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_custom_plot(): # Load the tips dataset tips = load_dataset(\\"tips\\") # Create a seaborn plot object p = ( so.Plot(tips, x=\'total_bill\', y=\'tip\', color=\'day\') .add(so.Dot(), so.Agg()) ) # Draw the plot p = p.label(x=\'Total Bill ()\', y=\'Tip ()\') # Adding text annotations fig, ax = plt.subplots() for _, row in tips.iterrows(): ax.text(row[\'total_bill\'], row[\'tip\'] + 0.3, f\'{row[\\"size\\"]}\', horizontalalignment=\'right\', fontweight=\'bold\') # Finalize the plot with Puffin plot, adding the axis p.on(ax) plt.show() return p"},{"question":"Objective Implement a custom neural network in PyTorch, save its state dictionary, modify some of the network\'s parameters, and restore the original state from the saved dictionary. Demonstrate understanding of tensor serialization in PyTorch. Task 1. **Implement a custom neural network module** `SimpleNetwork` that contains: - Two linear layers with ReLU activation in between. - An additional linear layer added after the activation function. 2. **Instantiate the network** and print its initial parameters (weights and biases). 3. **Save the state dictionary** of the initial network to a file named `\'simple_network_initial_state.pt\'`. 4. **Modify the network\'s parameters**: - Multiply the weights and biases of the first linear layer by 2. - Add 0.5 to the biases of the second linear layer. 5. **Print the modified parameters** of the network to confirm the changes. 6. **Load the saved state dictionary** and restore the network\'s parameters to their original states. 7. **Print the restored parameters** to confirm they match the initial parameters. Constraints - Use only the PyTorch library for this task. - Ensure the implementation is efficient and follows PyTorch\'s best practices. Input and Output Formats - The `SimpleNetwork` class should be implemented without any input or output parameters. - The parameters (weights and biases) should be printed to the console at each required step. Example The sequence of steps in the implementation would look like this (high-level overview): ```python # 1. Implement the custom network class class SimpleNetwork(torch.nn.Module): # Define the layers in the constructor and the forward pass # 2. Instantiate and print initial parameters net = SimpleNetwork() print(\\"Initial Parameters:\\", net.state_dict()) # 3. Save initial state torch.save(net.state_dict(), \'simple_network_initial_state.pt\') # 4. Modify network parameters with torch.no_grad(): net.fc1.weight *= 2 net.fc1.bias *= 2 net.fc2.bias += 0.5 print(\\"Modified Parameters:\\", net.state_dict()) # 5. Load saved state and restore network parameters initial_state_dict = torch.load(\'simple_network_initial_state.pt\') net.load_state_dict(initial_state_dict) print(\\"Restored Parameters:\\", net.state_dict()) ``` **Note**: Your final implementation should ensure that the printed outputs for initial and restored parameters are identical, confirming that the state loading has successfully reverted the changes.","solution":"import torch.nn as nn import torch class SimpleNetwork(nn.Module): def __init__(self): super(SimpleNetwork, self).__init__() self.fc1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.fc2 = nn.Linear(20, 15) self.fc3 = nn.Linear(15, 10) def forward(self, x): x = self.relu(self.fc1(x)) x = self.fc2(x) x = self.fc3(x) return x # Instantiate the network net = SimpleNetwork() print(\\"Initial Parameters:\\", net.state_dict()) # Save the initial state dictionary torch.save(net.state_dict(), \'simple_network_initial_state.pt\') # Modify the network\'s parameters (multiply weights and biases of fc1 by 2, add 0.5 to biases of fc2) with torch.no_grad(): net.fc1.weight.mul_(2) net.fc1.bias.mul_(2) net.fc2.bias.add_(0.5) print(\\"Modified Parameters:\\", net.state_dict()) # Load the saved state dictionary and restore network parameters initial_state_dict = torch.load(\'simple_network_initial_state.pt\') net.load_state_dict(initial_state_dict) print(\\"Restored Parameters:\\", net.state_dict())"},{"question":"# Custom Python Object: Rational Numbers Python\'s native `float` type can sometimes be insufficient for precise arithmetic calculations, as floating-point arithmetic is inherently imprecise. To address this, Python developers often use rational numbers—numbers represented by a numerator and a denominator. Your task is to implement a custom Python object that represents rational numbers. The object type should support basic arithmetic operations (addition, subtraction, multiplication, and division) as well as comparison operations (equality, less than, etc.), attribute access for the numerator and denominator, and a method to simplify the rational number. Class Definition Define a class `Rational` with the following properties and methods: 1. **Initialization**: ```python def __init__(self, numerator: int, denominator: int) -> None: Initializes a Rational object. If the denominator is zero, raise a ValueError. ``` 2. **String Representation**: ```python def __str__(self) -> str: Returns a string representation of the Rational object in the form \\"numerator/denominator\\". ``` 3. **Arithmetic Operations**: Implement the following methods for arithmetic operations: ```python def __add__(self, other: \'Rational\') -> \'Rational\': Returns the result of adding two rational numbers. def __sub__(self, other: \'Rational\') -> \'Rational\': Returns the result of subtracting another rational number from this one. def __mul__(self, other: \'Rational\') -> \'Rational\': Returns the result of multiplying two rational numbers. def __truediv__(self, other: \'Rational\') -> \'Rational\': Returns the result of dividing this rational number by another. Raise a ValueError if the other rational number is zero. ``` 4. **Comparison Operations**: Implement the following methods for comparison: ```python def __eq__(self, other: \'Rational\') -> bool: Returns True if the rational numbers are equal. def __lt__(self, other: \'Rational\') -> bool: Returns True if this rational number is less than the other. def __le__(self, other: \'Rational\') -> bool: Returns True if this rational number is less than or equal to the other. def __gt__(self, other: \'Rational\') -> bool: Returns True if this rational number is greater than the other. def __ge__(self, other: \'Rational\') -> bool: Returns True if this rational number is greater than or equal to the other. ``` 5. **Attribute Access**: ```python @property def numerator(self) -> int: Returns the numerator of the rational number. @property def denominator(self) -> int: Returns the denominator of the rational number. ``` 6. **Simplify Method**: Implement a method to simplify the rational number by dividing both the numerator and the denominator by their greatest common divisor (GCD): ```python def simplify(self) -> \'Rational\': Returns a simplified version of this rational number. ``` Constraints and Notes - Implement your own methods for arithmetic and comparison operations without using any libraries for rational number arithmetic. - Use Python\'s built-in integer arithmetic and functions to perform the necessary calculations. - Ensure that the denominator is always a positive integer, and the numerator takes the sign of the rational number. - Implement a helper method to calculate the GCD (greatest common divisor) of two integers which can be used in the `simplify` method. Example Usage ```python r1 = Rational(2, 3) r2 = Rational(3, 4) print(r1 + r2) # Output: 17/12 print(r1 - r2) # Output: -1/12 print(r1 * r2) # Output: 1/2 print(r1 / r2) # Output: 8/9 r3 = Rational(6, 8) r4 = r3.simplify() # r4 should be Rational(3, 4) print(r4) # Output: 3/4 print(r1 < r2) # Output: True print(r1 == Rational(2, 3)) # Output: True ```","solution":"class Rational: def __init__(self, numerator: int, denominator: int) -> None: if denominator == 0: raise ValueError(\\"Denominator cannot be zero.\\") if denominator < 0: numerator = -numerator denominator = -denominator self._numerator = numerator self._denominator = denominator self.simplify() def __str__(self) -> str: return f\\"{self._numerator}/{self._denominator}\\" def _gcd(self, a: int, b: int) -> int: while b: a, b = b, a % b return a def simplify(self) -> \'Rational\': gcd = self._gcd(abs(self._numerator), self._denominator) self._numerator //= gcd self._denominator //= gcd return self def __add__(self, other: \'Rational\') -> \'Rational\': numerator = self._numerator * other._denominator + other._numerator * self._denominator denominator = self._denominator * other._denominator return Rational(numerator, denominator).simplify() def __sub__(self, other: \'Rational\') -> \'Rational\': numerator = self._numerator * other._denominator - other._numerator * self._denominator denominator = self._denominator * other._denominator return Rational(numerator, denominator).simplify() def __mul__(self, other: \'Rational\') -> \'Rational\': numerator = self._numerator * other._numerator denominator = self._denominator * other._denominator return Rational(numerator, denominator).simplify() def __truediv__(self, other: \'Rational\') -> \'Rational\': if other._numerator == 0: raise ValueError(\\"Cannot divide by zero.\\") numerator = self._numerator * other._denominator denominator = self._denominator * other._numerator return Rational(numerator, denominator).simplify() def __eq__(self, other: \'Rational\') -> bool: return self._numerator == other._numerator and self._denominator == other._denominator def __lt__(self, other: \'Rational\') -> bool: return self._numerator * other._denominator < other._numerator * self._denominator def __le__(self, other: \'Rational\') -> bool: return self._numerator * other._denominator <= other._numerator * self._denominator def __gt__(self, other: \'Rational\') -> bool: return self._numerator * other._denominator > other._numerator * self._denominator def __ge__(self, other: \'Rational\') -> bool: return self._numerator * other._denominator >= other._numerator * self._denominator @property def numerator(self) -> int: return self._numerator @property def denominator(self) -> int: return self._denominator"},{"question":"You are tasked with creating a library system that manages books and members using object-oriented programming concepts in Python. You should design the classes and methods to model the following: 1. A `Book` class that has the following properties: - `title` (string) - `author` (string) - `isbn` (string) - `is_checked_out` (boolean, initially False) The class should have: - An `__init__` constructor that initializes `title`, `author`, and `isbn`. - A method `checkout()` that sets `is_checked_out` to True. - A method `return_book()` that sets `is_checked_out` to False. 2. A `Member` class that has the following properties: - `name` (string) - `member_id` (string) - `checked_out_books` (list of `Book` instances, initially empty) The class should have: - An `__init__` constructor that initializes `name` and `member_id`. - A method `checkout_book(book: Book)` that adds the book to `checked_out_books` list if it\'s not already checked out. - A method `return_book(book: Book)` that removes the book from the `checked_out_books` list if it is currently checked out by the member. - A method `list_books()` that returns an iterator for books currently checked out by the member, implemented as a generator. 3. A `Library` class that has the following properties: - `books` (list of `Book` instances) - `members` (dictionary mapping `member_id` to `Member` instances) The class should have: - An `__init__()` constructor that initializes the `books` as an empty list and `members` as an empty dictionary. - A method `add_book(book: Book)` to add a new book to the library. - A method `register_member(member: Member)` to register a new member in the library. - A method `find_book_by_isbn(isbn: str) -> Book` to find a book by its ISBN. - A method `checkout_book(isbn: str, member_id: str)` to check out a book by a member. - A method `return_book(isbn: str, member_id: str)` to return a book by a member. 4. Use name mangling to create a private method `_update_books()` in the `Library` class that prints information whenever a book is checked out or returned, and call this method within `checkout_book` and `return_book`. # Constraints: - Each member can check out multiple books, but a book can only be checked out by one member at a time. - When performing a checkout or return operation, make sure the book exists in the library and is not checked out or returned respectively. # Example: ```python # Example of defining the classes and performing operations library = Library() # Adding books book1 = Book(\\"1984\\", \\"George Orwell\\", \\"1234567890\\") book2 = Book(\\"Brave New World\\", \\"Aldous Huxley\\", \\"0987654321\\") library.add_book(book1) library.add_book(book2) # Registering members member1 = Member(\\"Alice\\", \\"M001\\") member2 = Member(\\"Bob\\", \\"M002\\") library.register_member(member1) library.register_member(member2) # Checking out books library.checkout_book(\\"1234567890\\", \\"M001\\") # Alice checks out 1984 library.checkout_book(\\"0987654321\\", \\"M002\\") # Bob checks out Brave New World # Listing checked out books for a member for book in member1.list_books(): print(f\\"Member {member1.name} has checked out {book.title}\\") # Returning books library.return_book(\\"1234567890\\", \\"M001\\") # Alice returns 1984 ``` Implement the `Book`, `Member`, and `Library` classes as described.","solution":"class Book: def __init__(self, title, author, isbn): self.title = title self.author = author self.isbn = isbn self.is_checked_out = False def checkout(self): self.is_checked_out = True def return_book(self): self.is_checked_out = False class Member: def __init__(self, name, member_id): self.name = name self.member_id = member_id self.checked_out_books = [] def checkout_book(self, book): if not book.is_checked_out: book.checkout() self.checked_out_books.append(book) def return_book(self, book): if book in self.checked_out_books: book.return_book() self.checked_out_books.remove(book) def list_books(self): for book in self.checked_out_books: yield book class Library: def __init__(self): self.books = [] self.members = {} def add_book(self, book): self.books.append(book) def register_member(self, member): self.members[member.member_id] = member def find_book_by_isbn(self, isbn): for book in self.books: if book.isbn == isbn: return book return None def checkout_book(self, isbn, member_id): book = self.find_book_by_isbn(isbn) if book and not book.is_checked_out and member_id in self.members: member = self.members[member_id] member.checkout_book(book) self.__update_books(\\"checkout\\", book, member) def return_book(self, isbn, member_id): book = self.find_book_by_isbn(isbn) if book and book.is_checked_out and member_id in self.members: member = self.members[member_id] member.return_book(book) self.__update_books(\\"return\\", book, member) def __update_books(self, action, book, member): print(f\\"Book \'{book.title}\' has been {action}ed by {member.name} ({member.member_id})\\")"},{"question":"**Question:** You are tasked with analyzing multiple Python scripts to determine the modules they import. Using the `modulefinder` module, write a function `analyze_scripts(script_paths, exclude_modules, replacement_paths)` that takes the following parameters: - `script_paths`: A list of file paths to the Python scripts that need to be analyzed. - `exclude_modules`: A list of module names to exclude from the analysis. - `replacement_paths`: A list of tuples where each tuple contains `(oldpath, newpath)` to replace in the module paths. The function should: 1. Use the `ModuleFinder` class to analyze each script in `script_paths`. 2. Exclude the modules specified in `exclude_modules` from the analysis. 3. Apply the path replacements specified in `replacement_paths`. 4. Print a detailed report that lists all imported modules for each script and highlights any modules that were not found or seem to be missing. **Input:** - `script_paths`: List of strings (file paths to Python scripts). - `exclude_modules`: List of strings (module names to exclude). - `replacement_paths`: List of tuples (each tuple contains two strings, old path and new path). **Output:** - Prints the detailed module import report to standard output. **Example Usage:** Assume we have the following scripts: - `script1.py`: ```python import os import sys import non_existent_module ``` - `script2.py`: ```python import re import itertools try: import baconhameggs except ImportError: pass ``` Calling `analyze_scripts([\'script1.py\', \'script2.py\'], [\'sys\'], [(\'non_existent_module\', \'existent_module\')])` should analyze these scripts while excluding the `sys` module and replacing `non_existent_module` with `existent_module`. The output will include reports with the modules each script imports, and it will highlight any that are missing. Your implementation should ensure that users receive a clear and comprehensive understanding of the module dependencies across the given scripts, taking into account any exclusions and path replacements specified. **Constraints:** - Assume all provided file paths are valid and exist. - Focus on the clarity and formatting of the output report. - Ensure to handle edge cases, such as empty scripts or scripts with no imports.","solution":"from modulefinder import ModuleFinder def analyze_scripts(script_paths, exclude_modules, replacement_paths): Analyzes multiple Python scripts to determine their imported modules. Parameters: - script_paths (list of str): List of file paths to the Python scripts. - exclude_modules (list of str): List of module names to exclude from the analysis. - replacement_paths (list of tuples): List of tuples (oldpath, newpath) to replace in the module paths. Prints a report of the analysis. for script_path in script_paths: print(f\\"nAnalyzing {script_path}...\\") finder = ModuleFinder() finder.run_script(script_path) # Apply path replacements modules = {name: mod for name, mod in finder.modules.items()} for oldpath, newpath in replacement_paths: if oldpath in modules: modules[newpath] = modules.pop(oldpath) found_modules = {name for name in modules.keys()} missing_modules = set(finder.badmodules.keys()) # Exclude specified modules found_modules.difference_update(exclude_modules) missing_modules.difference_update(exclude_modules) print(\\"Found Modules:\\") if found_modules: for module in sorted(found_modules): print(f\\" - {module}\\") else: print(\\" - None\\") print(\\"Missing Modules:\\") if missing_modules: for module in sorted(missing_modules): print(f\\" - {module}\\") else: print(\\" - None\\")"},{"question":"**Question: Train and Validate a GradientBoostingRegressor** You are provided with a task to train and validate a machine learning model using scikit-learn. Follow the steps below to complete the task: 1. **Generate Synthetic Data**: - Create a synthetic regression dataset using `sklearn.datasets.make_regression`. - The dataset should have 1000 samples and 20 features. 2. **Preprocess the Data**: - Standardize the features using `StandardScaler` from `sklearn.preprocessing`. 3. **Split the Data**: - Split the dataset into training and testing sets using `train_test_split` with 80% training and 20% testing data. 4. **Train the Model**: - Train a `GradientBoostingRegressor` model using the training data. Use the default parameters and set the `random_state` to 0. - Capture the training score and print it. 5. **Validate the Model and Report Warnings**: - Train another `GradientBoostingRegressor` model, but this time set `n_iter_no_change` to 5. - Capture and print any warnings or errors that arise during training. - Calculate and print the testing score for both models. **Expected Input and Output Formats**: The function should have the following signature: ```python def train_and_validate(): pass ``` **Constraints**: - Only use the `scikit-learn` library for generating the dataset, preprocessing, splitting, and training. - Adhere to best practices in handling and reporting any warnings or exceptions. **Performance Requirements**: - Ensure the entire process is completed efficiently and the code is readable. **Sample Output**: ```python Training score with default parameters: <score> Training score with n_iter_no_change=5: <score> Any warnings/errors encountered: <warnings/errors> Testing score on default model: <score> Testing score with n_iter_no_change=5: <score> ``` Use this structure to develop your function and adhere strictly to the best practices in reporting and managing warnings as mentioned in the provided documentation. Ensure to format your code properly for clarity.","solution":"import warnings from sklearn.datasets import make_regression from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.ensemble import GradientBoostingRegressor from sklearn.exceptions import ConvergenceWarning def train_and_validate(): # Generate synthetic regression data X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=0) # Standardize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=0) # Train the model with default parameters model_default = GradientBoostingRegressor(random_state=0) model_default.fit(X_train, y_train) train_score_default = model_default.score(X_train, y_train) test_score_default = model_default.score(X_test, y_test) # Train the model with n_iter_no_change set to 5 model_n_iter_no_change = GradientBoostingRegressor(random_state=0, n_iter_no_change=5) # Capture warnings with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\", category=ConvergenceWarning) model_n_iter_no_change.fit(X_train, y_train) warning_messages = [str(warning.message) for warning in w] train_score_n_iter_no_change = model_n_iter_no_change.score(X_train, y_train) test_score_n_iter_no_change = model_n_iter_no_change.score(X_test, y_test) # Output results results = { \\"Training score with default parameters\\": train_score_default, \\"Training score with n_iter_no_change=5\\": train_score_n_iter_no_change, \\"Warnings/Errors encountered\\": warning_messages, \\"Testing score on default model\\": test_score_default, \\"Testing score with n_iter_no_change=5\\": test_score_n_iter_no_change, } return results output = train_and_validate() print(output)"},{"question":"Reimplementing Legacy Module Importing Background: The use of the `imp` module has been deprecated since Python 3.4, and its functionality has been largely replaced by the `importlib` module. However, understanding how to replace legacy code utilizing `imp` with modern practices using `importlib` is crucial for maintaining and upgrading older Python systems. Problem Statement: You are tasked with re-implementing a legacy module importing function from the `imp` module using the modern `importlib` module. The goal is to ensure that the functionality behaves the same but uses up-to-date methodologies. Requirements: 1. **Function Name:** `import_legacy_module` 2. **Inputs:** - `module_name`: A string representing the name of the module to be imported. 3. **Outputs:** - The function should return the imported module object if the import is successful. - Raise `ImportError` if the module cannot be found. 4. **Constraints:** - The re-implementation must not use any deprecated `imp` functionalities. - Utilize `importlib` for module finding and loading. - The solution must handle both built-in and external modules. # Example: ```python import sys import importlib def import_legacy_module(module_name): Import a module using importlib as a replacement for the legacy imp module. Args: module_name (str): Name of the module to import. Returns: module: The imported module object. Raises: ImportError: If the module cannot be found or imported. if module_name in sys.modules: return sys.modules[module_name] try: spec = importlib.util.find_spec(module_name) if spec is None: raise ImportError(f\\"Module {module_name} not found\\") module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) return module except ImportError as e: raise ImportError(f\\"Could not import module {module_name}: {e}\\") # Example Usage try: module = import_legacy_module(\\"math\\") print(module.sqrt(16)) # Outputs: 4.0 except ImportError as e: print(e) ``` Additional Requirements: - Clearly document your code. - Write any necessary helper functions if needed. - Validate the inputs where applicable. - Include error handling for invalid module names. Note: - Focus on modern practices while ensuring compatibility with legacy code is maintained.","solution":"import sys import importlib def import_legacy_module(module_name): Import a module using importlib as a replacement for the legacy imp module. Args: module_name (str): Name of the module to import. Returns: module: The imported module object. Raises: ImportError: If the module cannot be found or imported. if module_name in sys.modules: return sys.modules[module_name] try: spec = importlib.util.find_spec(module_name) if spec is None: raise ImportError(f\\"Module {module_name} not found\\") module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) return module except ImportError as e: raise ImportError(f\\"Could not import module {module_name}: {e}\\")"},{"question":"# Enum-based Traffic Light Simulation You are required to implement a traffic light simulation using Python\'s `enum` module. The traffic light will cycle through the states: RED, GREEN, and YELLOW. Additionally, you must implement a feature to handle pedestrian crossing requests that will temporarily switch the traffic light to RED. Use the `Flag` class to manage multiple light states. Task Implement a class `TrafficLight` that simulates the behavior of a traffic light. 1. **TrafficLight Enum**: - Define an enumeration `TrafficSignal` using the `Flag` class with the following states: - `RED` - `GREEN` - `YELLOW` - Use the `auto()` helper to automatically assign values to these states. 2. **TrafficLight Class**: - Initialize with a starting state of `RED` and no active pedestrian crossing. - Implement methods for switching states: - `next_state()`: Transitions the traffic light to the next state in the cycle: RED -> GREEN -> YELLOW -> RED. - `pedestrian_request()`: Immediately changes the traffic light to RED for pedestrian crossing. - `current_state()`: Returns the current state of the traffic light. 3. **Pedestrian Crossing**: - If a pedestrian request is made, the traffic light should switch to RED and remain in that state until `next_state()` is called again. - Ensure that the traffic light correctly resumes its cycle after handling a pedestrian crossing request. Constraints - Use the `enum.Flag` and `auto()` for state definitions. - Ensure state transitions and pedestrian requests are handled correctly. - You may assume that the traffic light remains in each state for a fixed duration, but you do not need to handle actual timing in your implementation. Expected Input and Output ```python # Example Usage traffic_light = TrafficLight() print(traffic_light.current_state()) # Output: TrafficSignal.RED traffic_light.next_state() print(traffic_light.current_state()) # Output: TrafficSignal.GREEN traffic_light.pedestrian_request() print(traffic_light.current_state()) # Output: TrafficSignal.RED traffic_light.next_state() print(traffic_light.current_state()) # Output: TrafficSignal.GREEN traffic_light.next_state() print(traffic_light.current_state()) # Output: TrafficSignal.YELLOW ``` Implementation Implement the `TrafficSignal` enum and the `TrafficLight` class based on the specifications above. ```python from enum import Flag, auto class TrafficSignal(Flag): RED = auto() GREEN = auto() YELLOW = auto() class TrafficLight: def __init__(self): self.state = TrafficSignal.RED self.pedestrian_active = False def next_state(self): if self.pedestrian_active: self.pedestrian_active = False else: if self.state == TrafficSignal.RED: self.state = TrafficSignal.GREEN elif self.state == TrafficSignal.GREEN: self.state = TrafficSignal.YELLOW elif self.state == TrafficSignal.YELLOW: self.state = TrafficSignal.RED def pedestrian_request(self): self.state = TrafficSignal.RED self.pedestrian_active = True def current_state(self): return self.state ```","solution":"from enum import Flag, auto class TrafficSignal(Flag): RED = auto() GREEN = auto() YELLOW = auto() class TrafficLight: def __init__(self): self.state = TrafficSignal.RED self.pedestrian_active = False def next_state(self): if self.state == TrafficSignal.RED: self.state = TrafficSignal.GREEN elif self.state == TrafficSignal.GREEN: self.state = TrafficSignal.YELLOW elif self.state == TrafficSignal.YELLOW: self.state = TrafficSignal.RED self.pedestrian_active = False def pedestrian_request(self): self.state = TrafficSignal.RED self.pedestrian_active = True def current_state(self): return self.state"},{"question":"**Objective:** To test your understanding of descriptors in Python and how they can be used to manage and validate attribute access and modifications. **Task:** Implement a class `PositiveNumber` that works as a descriptor to manage and validate positive number attributes in another class. The descriptor should enforce that only positive numbers (integers and floats greater than zero) can be assigned to the attributes it manages. **Requirements:** 1. Implement the `PositiveNumber` descriptor class that: - Validates attribute values to ensure they are positive numbers. - Raises a `ValueError` if a non-positive number is assigned. - Tracks the name of the attribute it is managing. 2. Implement a `Product` class that uses the `PositiveNumber` descriptor to manage two attributes: `price` and `quantity`. 3. Ensure that attributes managed by `PositiveNumber` follow the validation rule strictly, both during initialization and when modifying the attributes. **Input/Output Format:** - The `PositiveNumber` class should raise a `ValueError` with an appropriate message if an invalid value is assigned. - The `Product` class should be able to initialize with `name`, `price`, and `quantity`, where `price` and `quantity` should be positive numbers. - Demonstrate the usage of these classes with example code that: - Creates instances of the `Product` class with valid and invalid values. - Attempts to update the `price` and `quantity` attributes of a `Product` instance with valid and invalid values. **Example:** ```python class PositiveNumber: # Implement the descriptor here class Product: # Implement the class here using PositiveNumber descriptor # Example usage: try: p1 = Product(name=\\"Widget\\", price=19.99, quantity=100) print(f\\"Product created: {p1.name}, Price: {p1.price}, Quantity: {p1.quantity}\\") p2 = Product(name=\\"Widget\\", price=-19.99, quantity=100) except ValueError as e: print(e) # Expected output: Price should be a positive number try: p1.price = -5.00 except ValueError as e: print(e) # Expected output: Price should be a positive number try: p1.quantity = 0 except ValueError as e: print(e) # Expected output: Quantity should be a positive number ``` **Constraints:** - Do not use any external libraries for validation. - The validation logic should be contained within the descriptor class. **Performance Requirements:** - The solution should effectively handle typical use cases with quick attribute validation. **Notes:** - Consider using the `__set_name__` method to track the attribute names for better error messages. - Implement proper getter and setter methods to manage attribute access and mutation.","solution":"class PositiveNumber: def __init__(self): self._name = None def __set_name__(self, owner, name): self._name = name def __get__(self, instance, owner): return instance.__dict__[self._name] def __set__(self, instance, value): if not isinstance(value, (int, float)) or value <= 0: raise ValueError(f\\"{self._name.capitalize()} should be a positive number\\") instance.__dict__[self._name] = value class Product: price = PositiveNumber() quantity = PositiveNumber() def __init__(self, name, price, quantity): self.name = name self.price = price self.quantity = quantity"},{"question":"**Question:** You are provided with sales data from two different branches of a company. Each branch maintains their sales records in different pandas DataFrames. Your task is to combine these DataFrames to get a unified view of the sales data, perform operations to clean and merge this data, and finally compare any differences between the combined data and an updated version of it. **Datasets:** 1. `branch_a_sales`: ``` | Product | Date | Quantity | Amount | |---------|------------|----------|--------| | A | 2023-01-01 | 10 | 100 | | B | 2023-01-02 | 20 | 200 | | C | 2023-01-03 | 15 | 150 | ``` 2. `branch_b_sales`: ``` | Product | Date | Quantity | Amount | |---------|------------|----------|--------| | A | 2023-01-01 | 8 | 80 | | B | 2023-01-04 | 18 | 180 | | D | 2023-01-05 | 5 | 50 | ``` **Tasks:** 1. **Combine DataFrames**: Create a single DataFrame by concatenating `branch_a_sales` and `branch_b_sales` along rows, ensuring that the index will be reset. 2. **Merge with Inventory**: Assume you have another DataFrame `inventory` that keeps track of the current stock of each product: ``` | Product | Stock | |---------|-------| | A | 50 | | B | 60 | | C | 70 | | D | 80 | ``` Perform a merge between the combined sales data and the inventory data to include the stock of each product. 3. **Data Update and Comparison**: - Update some quantities and amounts in the combined sales DataFrame. - Create a new DataFrame `updated_sales` and compare it with the original combined sales DataFrame to find the differences. 4. **Advanced Task**: - Perform an asof merge on a time series based sales data to get the nearest sales record for each timestamp from a provided quotes data. **Expected Inputs and Outputs**: - Input: DataFrames `branch_a_sales`, `branch_b_sales`, and `inventory`. - Output: 1. A combined DataFrame of sales. 2. A merged DataFrame with inventory. 3. A DataFrame showing the differences between the original and updated sales data. 4. A DataFrame merging the latest sales with quote timestamps. **Constraints and Performance**: - Assume the data will not exceed 1 million rows. - Ensure operations are efficient and handle missing data gracefully. **Skeleton Code**: ```python import pandas as pd # Create dataframes branch_a_sales = pd.DataFrame({ \'Product\': [\'A\', \'B\', \'C\'], \'Date\': [\'2023-01-01\', \'2023-01-02\', \'2023-01-03\'], \'Quantity\': [10, 20, 15], \'Amount\': [100, 200, 150] }) branch_b_sales = pd.DataFrame({ \'Product\': [\'A\', \'B\', \'D\'], \'Date\': [\'2023-01-01\', \'2023-01-04\', \'2023-01-05\'], \'Quantity\': [8, 18, 5], \'Amount\': [80, 180, 50] }) inventory = pd.DataFrame({ \'Product\': [\'A\', \'B\', \'C\', \'D\'], \'Stock\': [50, 60, 70, 80] }) # Task 1: Combine DataFrames def combine_sales_data(df1, df2): # Your implementation here pass # Task 2: Merge with Inventory def merge_with_inventory(sales_df, inventory_df): # Your implementation here pass # Task 3: Data Update and Comparison def update_and_compare_sales(sales_df): # Your implementation to update and compare sales pass # Task 4: Advanced Task - asof merge def asof_merge_sales_quotes(trades, quotes): # Your implementation here pass # Use the functions defined above to complete the tasks. ``` **Note**: Ensure to provide necessary in-line comments and documentation for each function implementation.","solution":"import pandas as pd # Create dataframes branch_a_sales = pd.DataFrame({ \'Product\': [\'A\', \'B\', \'C\'], \'Date\': [\'2023-01-01\', \'2023-01-02\', \'2023-01-03\'], \'Quantity\': [10, 20, 15], \'Amount\': [100, 200, 150] }) branch_b_sales = pd.DataFrame({ \'Product\': [\'A\', \'B\', \'D\'], \'Date\': [\'2023-01-01\', \'2023-01-04\', \'2023-01-05\'], \'Quantity\': [8, 18, 5], \'Amount\': [80, 180, 50] }) inventory = pd.DataFrame({ \'Product\': [\'A\', \'B\', \'C\', \'D\'], \'Stock\': [50, 60, 70, 80] }) # Task 1: Combine DataFrames def combine_sales_data(df1, df2): combined_df = pd.concat([df1, df2], ignore_index=True) return combined_df # Task 2: Merge with Inventory def merge_with_inventory(sales_df, inventory_df): merged_df = pd.merge(sales_df, inventory_df, on=\'Product\', how=\'left\') return merged_df # Task 3: Data Update and Comparison def update_and_compare_sales(sales_df): # Create a copy of the sales data to simulate an update updated_sales_df = sales_df.copy(deep=True) # Simulate updates to the sales data updated_sales_df.loc[0, \'Quantity\'] = 12 updated_sales_df.loc[0, \'Amount\'] = 120 updated_sales_df.loc[4, \'Quantity\'] = 19 updated_sales_df.loc[4, \'Amount\'] = 190 # Compare the original and updated sales data to find differences differences = sales_df.compare(updated_sales_df) return updated_sales_df, differences # Task 4: Advanced Task - asof merge def asof_merge_sales_quotes(sales_df, quotes_df): sales_df[\'Date\'] = pd.to_datetime(sales_df[\'Date\']) quotes_df[\'Date\'] = pd.to_datetime(quotes_df[\'Date\']) merged_df = pd.merge_asof(quotes_df.sort_values(\'Date\'), sales_df.sort_values(\'Date\'), on=\'Date\', by=\'Product\', direction=\'backward\') return merged_df"},{"question":"Objective Demonstrate your understanding of the `pathlib` module by performing a series of filesystem operations. Problem Statement You are tasked with creating a utilities module `file_utils` that provides a function `analyze_directory(root_path: str) -> dict` using the `pathlib` module. This function must analyze the given root directory and produce the following information: 1. **Total number of files** in the directory and all its subdirectories. 2. **Total number of directories** in the directory and all its subdirectories. 3. **Total size of all files** in bytes. 4. **List of files with their extensions**, grouped by extension. 5. **Paths of the largest and smallest files** based on size. Function Signature ```python def analyze_directory(root_path: str) -> dict: pass ``` Input - `root_path` (str): The path to the root directory that needs to be analyzed. Output - A dictionary with the following structure: ```python { \\"total_files\\": int, \\"total_directories\\": int, \\"total_size\\": int, \\"files_by_extension\\": { \\".ext1\\": [\\"file1.ext1\\", \\"file2.ext1\\", ...], \\".ext2\\": [\\"file3.ext2\\", \\"file4.ext2\\", ...], ... }, \\"largest_file\\": \\"path/to/largest_file.ext\\", \\"smallest_file\\": \\"path/to/smallest_file.ext\\" } ``` Constraints - You must use the `pathlib` module for handling paths. - Ensure that symbolic links are not followed to avoid infinite loops in case of circular links. - Handle hidden files and directories (those starting with a dot) in the same way as normal files and directories. - The function should handle large directory structures efficiently. Example If the directory structure is as follows: ``` root/ file1.txt (100 bytes) file2.txt (200 bytes) dir1/ file3.dat (50 bytes) file4.dat (150 bytes) dir2/ file5.txt (300 bytes) ``` Calling `analyze_directory(\'root\')` should return: ```python { \\"total_files\\": 5, \\"total_directories\\": 2, \\"total_size\\": 800, \\"files_by_extension\\": { \\".txt\\": [\\"root/file1.txt\\", \\"root/file2.txt\\", \\"root/dir1/dir2/file5.txt\\"], \\".dat\\": [\\"root/dir1/file3.dat\\", \\"root/dir1/file4.dat\\"] }, \\"largest_file\\": \\"root/dir1/dir2/file5.txt\\", \\"smallest_file\\": \\"root/dir1/file3.dat\\" } ``` **Note**: Ensure your implementation is efficient and handles edge cases appropriately.","solution":"from pathlib import Path def analyze_directory(root_path: str) -> dict: root = Path(root_path) total_files = 0 total_directories = 0 total_size = 0 files_by_extension = {} largest_file = None smallest_file = None largest_file_size = 0 smallest_file_size = float(\'inf\') if not root.is_dir(): raise ValueError(f\\"The provided path \'{root_path}\' is not a directory\\") for path in root.rglob(\'*\'): if path.is_file(): total_files += 1 file_size = path.stat().st_size total_size += file_size ext = path.suffix if ext not in files_by_extension: files_by_extension[ext] = [] files_by_extension[ext].append(str(path)) if file_size > largest_file_size: largest_file_size = file_size largest_file = str(path) if file_size < smallest_file_size: smallest_file_size = file_size smallest_file = str(path) elif path.is_dir(): total_directories += 1 return { \\"total_files\\": total_files, \\"total_directories\\": total_directories, \\"total_size\\": total_size, \\"files_by_extension\\": files_by_extension, \\"largest_file\\": largest_file, \\"smallest_file\\": smallest_file }"},{"question":"# Objective The goal of this exercise is to understand the transition from the deprecated `imp` module to the modern `importlib` and demonstrate your understanding of the import mechanisms in Python. # Problem Description You are provided with a script that uses the `imp` module to dynamically find and load a module. Your task is to refactor this script to use the `importlib` module instead. # Original Script (Using `imp`) ```python import imp import sys def load_module_using_imp(module_name): # Try to find the module file, pathname, description = imp.find_module(module_name) try: # Load the module module = imp.load_module(module_name, file, pathname, description) return module finally: if file: file.close() # Example usage if __name__ == \\"__main__\\": module_name = \\"example_module\\" # replace with actual module name for testing loaded_module = load_module_using_imp(module_name) print(f\\"Module {module_name} loaded:\\", loaded_module) ``` # Your Task Refactor the `load_module_using_imp` function to use the `importlib` module instead of `imp`. Ensure that your new implementation is efficient and adheres to best practices. # Constraints - You must use `importlib.util.find_spec` and `importlib.util.module_from_spec`. - Handle exceptions appropriately, similar to how they are handled in the original script. - Ensure the file is closed properly if it was opened. # Expected Output Your new implementation should provide the same functionality as the original script but using `importlib`. You should be able to dynamically find and load the specified module. # Refactor the function here: ```python import importlib.util import sys def load_module_using_importlib(module_name): # Try to find the module spec spec = importlib.util.find_spec(module_name) if spec is None: raise ImportError(f\\"Module {module_name} not found.\\") # Load the module module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) return module # Example usage if __name__ == \\"__main__\\": module_name = \\"example_module\\" # replace with actual module name for testing loaded_module = load_module_using_importlib(module_name) print(f\\"Module {module_name} loaded:\\", loaded_module) ``` Ensure your implementation is correct and thoroughly tested.","solution":"import importlib.util import sys def load_module_using_importlib(module_name): Dynamically load a module by its name using importlib. Args: module_name (str): The name of the module to load. Returns: The loaded module. Raises: ImportError: If the module cannot be found or an error occurs during loading. spec = importlib.util.find_spec(module_name) if spec is None: raise ImportError(f\\"Module {module_name} not found.\\") module = importlib.util.module_from_spec(spec) try: spec.loader.exec_module(module) except Exception as e: raise ImportError(f\\"Error loading module {module_name}: {e}\\") return module"},{"question":"Managing ZIP Archives with Python You are tasked with writing a Python function that performs multiple operations on a ZIP file using the `zipfile` module. The goal is to manage the contents of a ZIP archive, including listing files, adding new files, and extracting specific files. # Function Signature ```python def manage_zip_archive(zip_path: str, actions: list) -> dict: Manages a ZIP archive based on a list of actions. Parameters: zip_path (str): Path to the existing ZIP file or a new ZIP file to create. actions (list): List of actions to perform on the ZIP file. Actions are tuples with the following format: (\\"action\\", parameters...) \\"list\\": List contents of the ZIP archive. No additional parameters. \\"add_file\\": Add a new file to the ZIP archive. Parameters: (file_path, arcname). \\"add_string\\": Add a string as a file to the ZIP archive. Parameters: (filename, content). \\"extract\\": Extract a specific file from the ZIP archive. Parameters: (filename, extract_path). \\"extract_all\\": Extract all files from the ZIP archive. Parameters: (extract_path). Returns: dict: Dictionary with results of the actions. Format: { \\"list\\": List of filenames in the ZIP archive, \\"add_file\\": List of added files with corresponding arcname, \\"add_string\\": List of added filename, \\"extract\\": List of extracted filenames with corresponding extract_path, \\"extract_all\\": List of extracted filenames to the given extract_path, } pass ``` # Input - `zip_path`: A string representing the path to the ZIP file. If the file does not exist, a new ZIP file is created at the given path. - `actions`: A list of actions to perform. Each action is a tuple where the first element is the action name, and the subsequent elements are the parameters required for that action. - Available actions: - `\\"list\\"`: No additional parameters. - `\\"add_file\\"`: Parameters: (`file_path`, `arcname`). - `\\"add_string\\"`: Parameters: (`filename`, `content`). - `\\"extract\\"`: Parameters: (`filename`, `extract_path`). - `\\"extract_all\\"`: Parameters: (`path`). # Output - A dictionary summarizing the results of the performed actions. For example: ```python { \\"list\\": [\\"file1.txt\\", \\"file2.py\\"], \\"add_file\\": [(\\"file_to_add.txt\\", \\"added_file.txt\\")], \\"add_string\\": [\\"new_file.txt\\"], \\"extract\\": [(\\"file1.txt\\", \\"extracted_path/\\")], \\"extract_all\\": [\\"file1.txt\\", \\"file2.py\\"] } ``` # Constraints - Assume all file paths and filenames are valid and accessible. - The ZIP file can contain files with names containing unusual characters or spaces. - Only the `\\"list\\"` action can be performed on a non-existent ZIP file to get an empty list result. # Example ```python actions = [ (\\"list\\",), (\\"add_file\\", \\"path/to/file1.txt\\", \\"file1.txt\\"), (\\"add_string\\", \\"file2.txt\\", \\"This is the content of file2\\"), (\\"extract\\", \\"file1.txt\\", \\"extracted_directory/\\"), (\\"list\\",), (\\"extract_all\\", \\"new_directory/\\") ] output = manage_zip_archive(\\"example.zip\\", actions) print(output) # Expected output # { # \\"list\\": [\\"file1.txt\\", \\"file2.txt\\"], # \\"add_file\\": [(\\"path/to/file1.txt\\", \\"file1.txt\\")], # \\"add_string\\": [\\"file2.txt\\"], # \\"extract\\": [(\\"file1.txt\\", \\"extracted_directory/\\")], # \\"list\\": [\\"file1.txt\\", \\"file2.txt\\"], # \\"extract_all\\": [\\"file1.txt\\", \\"file2.txt\\"] # } ``` **Note:** You can utilize the `zipfile` module to deliver the necessary functionalities and ensure to handle opening and closing of the ZIP file appropriately. Pay attention to the performance when handling large ZIP files or performing multiple operations sequentially.","solution":"import os import zipfile from collections import defaultdict def manage_zip_archive(zip_path: str, actions: list) -> dict: results = defaultdict(list) if not os.path.exists(zip_path) and any(a[0] != \\"list\\" for a in actions): with zipfile.ZipFile(zip_path, \'w\') as zf: pass # Create an empty zip file with zipfile.ZipFile(zip_path, \'a\') as zf: for action in actions: if action[0] == \\"list\\": results[\\"list\\"] = zf.namelist() elif action[0] == \\"add_file\\": file_path, arcname = action[1], action[2] zf.write(file_path, arcname) results[\\"add_file\\"].append((file_path, arcname)) elif action[0] == \\"add_string\\": filename, content = action[1], action[2] zf.writestr(filename, content) results[\\"add_string\\"].append(filename) elif action[0] == \\"extract\\": filename, extract_path = action[1], action[2] zf.extract(filename, extract_path) results[\\"extract\\"].append((filename, extract_path)) elif action[0] == \\"extract_all\\": extract_path = action[1] zf.extractall(extract_path) results[\\"extract_all\\"] = zf.namelist() return dict(results)"},{"question":"# Isotonic Regression Coding Assessment You are given a dataset containing 1-dimensional data points `X` and their corresponding target values `y`. Your task is to use the `IsotonicRegression` class in scikit-learn to fit a non-decreasing function to this data and make predictions. **Input** - A list of real numbers `X` representing the data points. - A list of real numbers `y` representing the target values. - A real number `x_pred`, which represents a new data point for which you need to predict the target value using the fitted model. **Output** - A real number prediction for `x_pred` based on the fitted isotonic regression model. **Constraints** - The length of `X` and `y` will be at least 2 and at most 1000. - The elements in `X` and `y` can be any real numbers. - Assume `x_pred` is within the range of the values in `X`. **Performance Requirements** - Your solution should efficiently handle input sizes up to the maximum constraint. **Task** 1. Import the necessary libraries. 2. Use the `IsotonicRegression` class to fit an isotonic regression model to the provided `X` and `y`. 3. Use the fitted model to predict the target value for `x_pred`. **Example** ```python from sklearn.isotonic import IsotonicRegression def isotonic_regression_prediction(X, y, x_pred): # Create and fit the isotonic regression model ir = IsotonicRegression(increasing=True) ir.fit(X, y) # Predict the target value for x_pred y_pred = ir.predict([x_pred])[0] return y_pred # Example usage X = [1, 2, 3, 4, 5] y = [1.1, 2.2, 2.8, 3.9, 5.1] x_pred = 3.5 print(isotonic_regression_prediction(X, y, x_pred)) # Output should be close to 3.35 ``` Implement the function `isotonic_regression_prediction(X, y, x_pred)` that takes in the lists `X` and `y`, and the value `x_pred`, and returns the prediction for `x_pred` using isotonic regression.","solution":"from sklearn.isotonic import IsotonicRegression import numpy as np def isotonic_regression_prediction(X, y, x_pred): Fit an isotonic regression model to the data points (X, y) and predict the target value for x_pred. # Create and fit the isotonic regression model ir = IsotonicRegression(increasing=True) ir.fit(X, y) # Predict the target value for x_pred y_pred = np.interp(x_pred, X, ir.predict(X)) return y_pred"},{"question":"<|Analysis Begin|> The documentation provided details the `graphlib` module, specifically focusing on the `TopologicalSorter` class. This class is used to perform topological sorting on directed acyclic graphs (DAGs). Here are the key points from the documentation: 1. **Class Overview:** - The `TopologicalSorter` class provides functionality for topologically sorting a graph of hashable nodes. - A graph is a dictionary where keys are nodes and values are iterables of predecessors of that node. - The class supports parallel processing of nodes as they become ready. 2. **Main Methods:** - **Constructor:** The constructor accepts an optional graph as a dict. - **add:** Adds a new node and its predecessors to the graph. - **prepare:** Marks the graph as finished and checks for cycles. - **is_active:** Returns `True` if more progress can be made and `False` otherwise. - **done:** Marks a node as processed. - **get_ready:** Returns a tuple with all nodes that are ready. - **static_order:** Returns an iterator over nodes in a topological order without parallel processing. 3. **Exceptions:** - **CycleError:** Raised if a cycle is detected in the graph during the prepare phase. These features provide a robust interface for performing and managing topological sorts on graphs, suitable for tasks requiring dependency resolution. <|Analysis End|> <|Question Begin|> **Advanced Topological Sorting with Constraint Validation** You are given a set of tasks to be performed, each with a set of dependencies that must be completed before it can start. Your task is to write a Python function using the `graphlib.TopologicalSorter` class to generate a valid sequence for performing the tasks. Additionally, you should implement error handling for cyclic dependencies. # Function Signature ```python def schedule_tasks(tasks: Dict[Hashable, Iterable[Hashable]]) -> List[Hashable]: pass ``` # Input - `tasks`: A dictionary where each key is a task (hashable), and each value is an iterable of tasks that must be completed before the key task (its dependencies). # Output - A list of tasks representing a valid topological order. # Constraints - All task identifiers are hashable. - The input dictionary represents a directed acyclic graph. # Requirements - You must use the `graphlib.TopologicalSorter` to perform the topological sort. - Implement error handling to raise a `ValueError` with the message `Cycle detected: [cycle]` if a cycle is found in the graph where `[cycle]` is the detected cycle in the list form, including the repeated node to indicate the cycle. # Example ```python tasks = { \\"task1\\": [], \\"task2\\": [\\"task1\\"], \\"task3\\": [\\"task1\\"], \\"task4\\": [\\"task2\\", \\"task3\\"] } print(schedule_tasks(tasks)) # Output: [\'task1\', \'task2\', \'task3\', \'task4\'] cyclic_tasks = { \\"task1\\": [\\"task3\\"], \\"task2\\": [\\"task1\\"], \\"task3\\": [\\"task2\\"] } print(schedule_tasks(cyclic_tasks)) # Raises ValueError: Cycle detected: [\'task1\', \'task2\', \'task3\', \'task1\'] ``` # Notes - Use the `prepare()` method to check for cycles before performing the topological sort. - Utilize the `done()` and `get_ready()` methods to handle nodes as they become ready. - Make use of the `CycleError` exception to detect cycles and construct the appropriate error message. Your implementation should ensure that if there are no cycles, it returns a valid topological order of tasks. If cycles are present, it should raise the appropriate error with the correct cycle path.","solution":"from typing import Dict, Hashable, Iterable, List import graphlib def schedule_tasks(tasks: Dict[Hashable, Iterable[Hashable]]) -> List[Hashable]: sorter = graphlib.TopologicalSorter(tasks) try: sorter.prepare() except graphlib.CycleError as e: raise ValueError(f\\"Cycle detected: {e.args[1]} + [{e.args[1][0]}]\\") ordered_tasks = [] while sorter.is_active(): ready = sorter.get_ready() for task in ready: ordered_tasks.append(task) sorter.done(task) return ordered_tasks"},{"question":"# Question: Named Tensor Operations in PyTorch Objective Implement a function that performs a series of operations on named tensors and verifies the correctness of the resulting named tensor. The function should demonstrate the understanding of different name propagation rules for various tensor operations. Task Here is the function specification: ```python import torch def named_tensor_operations(tensor1, tensor2, tensor3): Perform a series of operations on the provided named tensors and return the final named tensor. Parameters: - tensor1 (torch.Tensor): A tensor with names [\'N\', \'C\'] - tensor2 (torch.Tensor): A tensor with names [\'C\', \'H\'] - tensor3 (torch.Tensor): A tensor with names [\'N\', \'H\'] Returns: - torch.Tensor: The resulting tensor after performing the operations. # Ensure compatibility and correctness of names before performing operations if tensor1.names != (\'N\', \'C\'): raise ValueError(\\"tensor1 must have names (\'N\', \'C\')\\") if tensor2.names != (\'C\', \'H\'): raise ValueError(\\"tensor2 must have names (\'C\', \'H\')\\") if tensor3.names != (\'N\', \'H\'): raise ValueError(\\"tensor3 must have names (\'N\', \'H\')\\") # Step 1: Perform matrix multiplication of tensor1 and tensor2 result = torch.matmul(tensor1, tensor2) # Step 2: Perform element-wise addition with tensor3 (ensure alignment of names) result = result.align_as(tensor3) result += tensor3 # Step 3: Apply a reduction operation (sum) on the \'H\' dimension result = result.sum(dim=\'H\') # Return the final tensor return result ``` Constraints 1. **Input Tensors**: The provided tensors will always have the specified names and appropriate dimensions to enable the operations described. 2. **Name Check**: Ensure the input tensors have the correct names; otherwise, raise a `ValueError`. 3. **Alignment**: Properly align tensors for operations when necessary. Example ```python # Example input tensors tensor1 = torch.randn(3, 4, names=(\'N\', \'C\')) tensor2 = torch.randn(4, 5, names=(\'C\', \'H\')) tensor3 = torch.randn(3, 5, names=(\'N\', \'H\')) # Perform operations result = named_tensor_operations(tensor1, tensor2, tensor3) # Output the names and the resulting tensor print(result.names) # Expected output: (\'N\',) print(result) ``` Explanation 1. **Step 1**: Perform matrix multiplication `tensor1` (`N`, `C`) with `tensor2` (`C`, `H`), resulting in a tensor with names (`N`, `H`). 2. **Step 2**: Align the resulting tensor to match the names of `tensor3` (`N`, `H`) before performing element-wise addition. 3. **Step 3**: Sum the resulting tensor over the dimension `H`, reducing the tensor to have names (`N`). Implement the function correctly to demonstrate your understanding of named tensor operations and name inference rules in PyTorch.","solution":"import torch def named_tensor_operations(tensor1, tensor2, tensor3): Perform a series of operations on the provided named tensors and return the final named tensor. Parameters: - tensor1 (torch.Tensor): A tensor with names [\'N\', \'C\'] - tensor2 (torch.Tensor): A tensor with names [\'C\', \'H\'] - tensor3 (torch.Tensor): A tensor with names [\'N\', \'H\'] Returns: - torch.Tensor: The resulting tensor after performing the operations. # Ensure compatibility and correctness of names before performing operations if tensor1.names != (\'N\', \'C\'): raise ValueError(\\"tensor1 must have names (\'N\', \'C\')\\") if tensor2.names != (\'C\', \'H\'): raise ValueError(\\"tensor2 must have names (\'C\', \'H\')\\") if tensor3.names != (\'N\', \'H\'): raise ValueError(\\"tensor3 must have names (\'N\', \'H\')\\") # Step 1: Perform matrix multiplication of tensor1 and tensor2 result = torch.matmul(tensor1, tensor2) # Step 2: Perform element-wise addition with tensor3 (ensure alignment of names) result = result.align_as(tensor3) result += tensor3 # Step 3: Apply a reduction operation (sum) on the \'H\' dimension result = result.sum(dim=\'H\') # Return the final tensor return result"},{"question":"**Problem Statement:** You are provided with logs from multiple days which you need to process to generate a summary of error occurrences. Each log file contains lines of log messages where each line starts with a date in the format `YYYY-MM-DD` followed by a log level (`INFO`, `WARNING`, `ERROR`, etc.) and a message. You need to: 1. Write a function that reads multiple log files and counts the number of `ERROR` messages for each day. 2. Return the result as a dictionary where the keys are dates and values are the count of `ERROR` messages for that date. The function should accept input log files specified by the user. You should use the `fileinput` module to handle reading lines from multiple files. The function should handle files using `UTF-8` encoding. **Function Signature:** ```python def count_error_messages(file_list: list) -> dict: pass ``` **Input:** - `file_list`: A list of strings, each being a filename to be processed. **Output:** - A dictionary with keys being dates in the format `YYYY-MM-DD` and values being the count of `ERROR` messages for that date. **Example:** ```python # Assume content of \'log1.txt\': # 2023-10-01 INFO Start of log # 2023-10-01 ERROR Something went wrong # 2023-10-02 ERROR Another error # Assume content of \'log2.txt\': # 2023-10-01 WARNING A potential problem # 2023-10-02 INFO Just an info # 2023-10-02 ERROR Yet another error file_list = [\'log1.txt\', \'log2.txt\'] print(count_error_messages(file_list)) # Output: {\'2023-10-01\': 1, \'2023-10-02\': 2} ``` **Constraints:** - You may assume that the log files are not empty. - Each line in the log files is correctly formatted. - The number of files and the number of lines per file are within a reasonable limit that allows processing within typical memory and performance constraints. **Hints:** - Use the `fileinput.input` function to read lines from multiple files. - Utilize the `encoding` parameter with `fileinput.input` to ensure files are read with `UTF-8` encoding. - Remember to handle any necessary error handling, such as `OSError`.","solution":"import fileinput from collections import defaultdict def count_error_messages(file_list): Reads multiple log files and counts the number of ERROR messages for each day. :param file_list: A list of strings, each being a filename to be processed. :return: A dictionary with keys being dates in the format `YYYY-MM-DD` and values being the count of `ERROR` messages for that date. error_counts = defaultdict(int) with fileinput.input(files=file_list, encoding=\'utf-8\') as f: for line in f: parts = line.strip().split(maxsplit=2) if len(parts) < 3: continue # skip malformed lines date, log_level, message = parts if log_level == \'ERROR\': error_counts[date] += 1 return dict(error_counts)"},{"question":"# Pandas Options and Data Display Configuration You are given a dataset containing financial data of various companies over several years. Your task is to write a Python function using pandas to configure specific display options and provide a summary formatted according to these settings. Dataset Format The dataset is a CSV file with the following columns: - `Year`: Year of the financial report. - `Company`: Name of the company. - `Revenue`: Revenue of the company for the year. - `Profit`: Profit of the company for the year. - `Employees`: Number of employees in the company for the year. Task Write a function `configure_display_and_summary(file_path: str) -> None` that: 1. Configures pandas display options to meet the following criteria: - Maximum rows displayed: 10 - Maximum columns displayed: 5 - Column header justification: Left - Display precision for floats: 2 decimal places 2. Loads the dataset from the provided `file_path`. 3. Displays the first 15 rows of the dataset (to demonstrate the maximum rows setting). 4. Prints the DataFrame `info` to show the settings in effect. 5. Returns `None`. Example ```python def configure_display_and_summary(file_path: str) -> None: import pandas as pd # Setting pandas display options pd.set_option(\\"display.max_rows\\", 10) pd.set_option(\\"display.max_columns\\", 5) pd.set_option(\\"display.colheader_justify\\", \\"left\\") pd.set_option(\\"display.precision\\", 2) # Loading the dataset df = pd.read_csv(file_path) # Displaying the first 15 rows to demonstrate the max_rows setting print(df.head(15)) # Printing the DataFrame info df.info() ``` Constraints - Use only the pandas library for this task. - Ensure that your code adheres to the specified display settings exactly. - Your function should not return any value; it should print the outputs as specified. Use the function to load a dataset from a specified `file_path` and observe if the display settings behave as expected.","solution":"def configure_display_and_summary(file_path: str) -> None: import pandas as pd # Setting pandas display options pd.set_option(\\"display.max_rows\\", 10) pd.set_option(\\"display.max_columns\\", 5) pd.set_option(\\"display.colheader_justify\\", \\"left\\") pd.set_option(\\"display.precision\\", 2) # Loading the dataset df = pd.read_csv(file_path) # Displaying the first 15 rows to demonstrate the max_rows setting print(df.head(15)) # Printing the DataFrame info df.info() # Assume this function is saved in a file called \'solution.py\'"},{"question":"Objective You are required to demonstrate your understanding of cell objects as described in the provided documentation. This involves creating functions that utilize cell objects to manage variables referenced by multiple scopes. Problem Statement Implement a class `CellManager` in Python that provides methods to create and manipulate cell objects. You need to utilize the described functionalities of cell objects to implement the following methods: 1. **create_cell(initial_value)**: Given an initial value, create and return a new cell object containing this value. 2. **get_cell_value(cell)**: Given a cell object, return its contents. 3. **set_cell_value(cell, value)**: Given a cell object and a new value, set the contents of the cell object to the new value. 4. **swap_cell_values(cell1, cell2)**: Given two cell objects, swap their contents. Function Signatures ```python class CellManager: def create_cell(self, initial_value) -> \'PyObject\': pass def get_cell_value(self, cell: \'PyObject\') -> \'PyObject\': pass def set_cell_value(self, cell: \'PyObject\', value: \'PyObject\') -> int: pass def swap_cell_values(self, cell1: \'PyObject\', cell2: \'PyObject\') -> None: pass ``` Input Constraints and Specifications - The `initial_value`, `value`, `cell`, `cell1`, and `cell2` parameters will be valid Python objects and references, as applicable. - The create_cell method should not take any constraints into account besides the provided initial value. - The get_cell_value method should handle cases where the cell might be \\"NULL\\" or invalid. - The set_cell_value method should also handle the potential of setting a \\"NULL\\" value. Example Usage ```python cm = CellManager() # Create two cell objects with initial values 10 and 20 cell1 = cm.create_cell(10) cell2 = cm.create_cell(20) # Get the values of cell1 and cell2 print(cm.get_cell_value(cell1)) # Output: 10 print(cm.get_cell_value(cell2)) # Output: 20 # Set a new value to cell1 cm.set_cell_value(cell1, 30) print(cm.get_cell_value(cell1)) # Output: 30 # Swap values of cell1 and cell2 cm.swap_cell_values(cell1, cell2) print(cm.get_cell_value(cell1)) # Output: 20 print(cm.get_cell_value(cell2)) # Output: 30 ``` Make sure to handle reference counts and safety where explicitly stated in the documentation. Your implementation should adhere to efficient memory management practices in Python to effectively manage the lifecycle of cell objects and their contents.","solution":"import types class CellManager: def create_cell(self, initial_value): Create a new cell object containing the initial value. return (lambda: initial_value).__closure__[0] def get_cell_value(self, cell): Return the contents of the cell object. return cell.cell_contents def set_cell_value(self, cell, value): Set the contents of the cell object to a new value. cell.cell_contents = value def swap_cell_values(self, cell1, cell2): Swap the contents of two cell objects. temp = cell1.cell_contents cell1.cell_contents = cell2.cell_contents cell2.cell_contents = temp"},{"question":"Objective To demonstrate your understanding of the `http` package in Python, specifically the `http.HTTPStatus` enum, you will write a function that checks the validity of an HTTP status code. Problem Description Implement a function called `validate_http_status` which takes an integer as input and returns a dictionary with the following key-value pairs: - `status`: a boolean that indicates whether the provided integer is a valid HTTP status code defined in the `http.HTTPStatus` enum. - `phrase`: the reason phrase associated with the status code if it is valid, otherwise `None`. - `description`: the description of the status code if it is valid, otherwise `None`. Function Signature ```python def validate_http_status(status_code: int) -> dict: pass ``` Input - `status_code` (int): An integer representing an HTTP status code. Output A dictionary with the following structure: ```python { \\"status\\": bool, \\"phrase\\": str or None, \\"description\\": str or None } ``` Constraints - The status code should be an integer within the range of valid HTTP status codes. - If the status code is not valid, return `status` as `False`, and both `phrase` and `description` as `None`. Example ```python # Example 1 print(validate_http_status(200)) # Output: {\\"status\\": True, \\"phrase\\": \\"OK\\", \\"description\\": \\"Request fulfilled, document follows\\"} # Example 2 print(validate_http_status(999)) # Output: {\\"status\\": False, \\"phrase\\": None, \\"description\\": None} ``` You are required to use the `http` package, specifically the `http.HTTPStatus` enum, to implement this function. Notes - Consider using the `http.HTTPStatus` enum to check and retrieve valid status codes, phrases, and descriptions. - Ensure the function handles edge cases and invalid inputs gracefully.","solution":"from http import HTTPStatus def validate_http_status(status_code: int) -> dict: Validates an HTTP status code and returns its phrase and description if valid. if status_code in HTTPStatus._value2member_map_: status = True http_status = HTTPStatus(status_code) phrase = http_status.phrase description = http_status.description else: status = False phrase = None description = None return { \\"status\\": status, \\"phrase\\": phrase, \\"description\\": description }"},{"question":"Objective: Your task is to implement a custom PyTorch FX transformer that manipulates nodes in a neural network\'s computational graph. You will use the techniques described in the provided documentation to replace specific operations with new ones and apply some additional transformations. Problem Statement: Consider a given PyTorch FX `GraphModule` that represents a neural network model. Implement a custom transformer class `CustomTransformer` that performs the following transformations on the graph: 1. **Replace** all `torch.ops.aten.add.Tensor` operations with `torch.ops.aten.div.Tensor` operations. 2. **Insert** `torch.ops.aten.tanh.default` after every `torch.ops.aten.mul.Tensor` operation. 3. **Remove** all `torch.ops.aten.detach.default` and `torch.ops.aten.detach_copy.default` operations. You must use the techniques described in the provided documentation to achieve these transformations. Function Signature: ```python import torch from torch.fx import GraphModule class CustomTransformer(torch.fx.Transformer): def call_function(self, target, args, kwargs): # Replace add operation with div operation if target == torch.ops.aten.add.Tensor: target = torch.ops.aten.div.Tensor # Process other operations normally result = super().call_function(target, args, kwargs) # Insert tanh operation after mul operation if target == torch.ops.aten.mul.Tensor: result = super().call_function(torch.ops.aten.tanh.default, (result,), {}) # Handle detaching operations if target in (torch.ops.aten.detach.default, torch.ops.aten.detach_copy.default): return args[0] # Return the input argument directly return result def transform_graph(graph_module: GraphModule) -> GraphModule: return CustomTransformer(graph_module).transform() ``` Input: - A `graph_module` of type `torch.fx.GraphModule` representing the neural network model. Output: - A transformed `graph_module` with specified operations replaced, added, or removed. Example: ```python # Example model class SampleModel(torch.nn.Module): def forward(self, x, y): z = x + y z = z * y z = z.detach() return z # Trace the model to get the FX GraphModule model = SampleModel() traced_model = torch.fx.symbolic_trace(model) # Transform the graph transformed_model = transform_graph(traced_model) # The transformed model should now use div instead of add, insert tanh after mul, # and avoid the detach operation. ``` Constraints: - Assume all input graphs are valid and contain the specified operations. - The function should not change the overall functionality of the graph besides the specified transformations.","solution":"import torch from torch.fx import GraphModule, Graph, Node, Transformer class CustomTransformer(Transformer): def call_function(self, target, args, kwargs): # Handle detaching operations if target in (torch.ops.aten.detach.default, torch.ops.aten.detach_copy.default): return args[0] # Return the input argument directly # Replace add operation with div operation if target == torch.ops.aten.add.Tensor: target = torch.ops.aten.div.Tensor # Process other operations normally result = super().call_function(target, args, kwargs) # Insert tanh operation after mul operation if target == torch.ops.aten.mul.Tensor: result = super().call_function(torch.ops.aten.tanh.default, (result,), {}) return result def transform_graph(graph_module: GraphModule) -> GraphModule: transformed = CustomTransformer(graph_module).transform() return transformed # Sample usage with an example model class SampleModel(torch.nn.Module): def forward(self, x, y): z = x + y z = z * y z = z.detach() return z model = SampleModel() traced_model = torch.fx.symbolic_trace(model) transformed_model = transform_graph(traced_model)"},{"question":"Objective: Create a class `MappingManipulator` that provides various utilities to interact and manipulate mapping objects (such as dictionaries). You must implement the following methods within this class: - `__init__(self, mapping)`: Initializes the instance with a mapping object. - `size(self)`: Returns the number of keys in the mapping. - `get_item(self, key)`: Returns the value associated with the provided key. If the key does not exist, return `None`. - `set_item(self, key, value)`: Sets the provided key to the provided value. - `delete_item(self, key)`: Deletes the item with the provided key. If the key does not exist, do nothing. - `has_key(self, key)`: Returns `True` if the mapping contains the provided key, otherwise `False`. - `keys(self)`: Returns a list of keys in the mapping. - `values(self)`: Returns a list of values in the mapping. - `items(self)`: Returns a list of key-value pairs (tuples) in the mapping. Constraints: - The implementation should not use any of the built-in dictionary methods such as `keys()`, `values()`, `items()`, etc., and should only use the functions from the mapping protocol as described in the provided documentation. - Handle all edge cases like attempting to get or delete a non-existent key gracefully. - Performance constraints: Your implementation should be efficient with a time complexity close to O(1) for key retrieval, insertion, and deletion operations. Input and Output Formats: 1. `__init__(self, mapping)`: - **Input**: A `mapping` object (e.g., dictionary). - **Output**: Initializes the instance. 2. `size(self)`: - **Input**: None - **Output**: Integer indicating the number of keys in the mapping. 3. `get_item(self, key)`: - **Input**: A string `key`. - **Output**: Value associated with `key` if present, else `None`. 4. `set_item(self, key, value)`: - **Input**: A string `key` and any type `value`. - **Output**: None 5. `delete_item(self, key)`: - **Input**: A string `key`. - **Output**: None 6. `has_key(self, key)`: - **Input**: A string `key`. - **Output**: Boolean `True` if `key` is in the mapping, else `False`. 7. `keys(self)`: - **Input**: None - **Output**: List of strings representing the keys. 8. `values(self)`: - **Input**: None - **Output**: List of values. 9. `items(self)`: - **Input**: None - **Output**: List of tuples where each tuple is a key-value pair. Here is the starter code template to begin with: ```python class MappingManipulator: def __init__(self, mapping): # Initialize with the provided mapping pass def size(self): # Return the number of keys pass def get_item(self, key): # Return the value for the given key pass def set_item(self, key, value): # Set the key to the value pass def delete_item(self, key): # Delete the item for the given key pass def has_key(self, key): # Return True if the key exists in the map pass def keys(self): # Return a list of keys pass def values(self): # Return a list of values pass def items(self): # Return a list of key-value pairs pass ``` Example: ```python # Example usage mapping = {\'a\': 1, \'b\': 2, \'c\': 3} manipulator = MappingManipulator(mapping) print(manipulator.size()) # Output: 3 print(manipulator.get_item(\'b\')) # Output: 2 print(manipulator.has_key(\'d\')) # Output: False manipulator.set_item(\'d\', 4) print(manipulator.get_item(\'d\')) # Output: 4 manipulator.delete_item(\'a\') print(manipulator.size()) # Output: 3 print(manipulator.keys()) # Output: [\'b\', \'c\', \'d\'] print(manipulator.values()) # Output: [2, 3, 4] print(manipulator.items()) # Output: [(\'b\', 2), (\'c\', 3), (\'d\', 4)] ```","solution":"class MappingManipulator: def __init__(self, mapping): self.mapping = mapping.copy() def size(self): count = 0 for _ in self.mapping: count += 1 return count def get_item(self, key): return self.mapping[key] if key in self.mapping else None def set_item(self, key, value): self.mapping[key] = value def delete_item(self, key): if key in self.mapping: del self.mapping[key] def has_key(self, key): return key in self.mapping def keys(self): return [k for k in self.mapping] def values(self): return [self.mapping[k] for k in self.mapping] def items(self): return [(k, self.mapping[k]) for k in self.mapping]"},{"question":"# Advanced Python Runtime Services Coding Challenge Task: You are required to write a Python program that manages a hypothetical computation task. The goal is to design a context manager to handle resource allocation and cleanup, use data classes to handle task configuration, and implement a warning system for potential issues. Requirements: 1. **Context Manager:** - Implement a context manager using the `contextlib` module to manage resources. The context manager should: - Allocate resources (e.g., open a file or initialize a connection) when entering the context. - Properly release resources when exiting the context. 2. **Data Classes:** - Create a data class `TaskConfig` using the `dataclasses` module to store configuration for computation tasks. This class should include: - Task name (str) - Input data (list of integers) - Threshold value (int) 3. **Warnings Control:** - Use the `warnings` module to emit a warning when a computation result exceeds the given threshold in the `TaskConfig`. Filter the warnings to display custom messages. Implementation Details: - Define the context manager class `ResourceContext` and use the `with` statement to ensure proper management. - Define the `TaskConfig` data class with the `dataclasses.dataclass` decorator. - Use the `warnings.warn()` function to generate a warning based on the computation result, and customize the warning filter to display a custom warning message. Expected Input and Output Formats: - Input: A `TaskConfig` object and a computation function. - Output: Handling of resources through context management and an appropriate warning message printed if the computation result exceeds the threshold. Example: ```python from contextlib import contextmanager from dataclasses import dataclass import warnings @dataclass class TaskConfig: task_name: str input_data: list threshold: int @contextmanager def ResourceContext(resource_name: str): print(f\\"Allocating resource: {resource_name}\\") yield print(f\\"Releasing resource: {resource_name}\\") def computation(task_config: TaskConfig): result = sum(task_config.input_data) if result > task_config.threshold: warnings.warn(f\\"Result {result} exceeds threshold {task_config.threshold}\\", UserWarning) return result task_config = TaskConfig(\'Example Task\', [1, 2, 3, 4, 5], 10) with ResourceContext(\'ExampleResource\'): result = computation(task_config) print(f\\"Computation result: {result}\\") ``` In the example above: - The `TaskConfig` data class holds the configuration for the task. - The `ResourceContext` context manager ensures resource allocation and release. - The `computation` function performs a sum operation on the `input_data` and checks against the `threshold`, issuing a warning if exceeded. Implement your solution based on this example and ensure all specified requirements are met. Constraints: - The `input_data` list will contain up to 1000 integers. - The sum of integers in `input_data` will not exceed 10,000. - Performance should be optimized.","solution":"from contextlib import contextmanager from dataclasses import dataclass import warnings @dataclass class TaskConfig: task_name: str input_data: list threshold: int @contextmanager def ResourceContext(resource_name: str): print(f\\"Allocating resource: {resource_name}\\") yield print(f\\"Releasing resource: {resource_name}\\") def computation(task_config: TaskConfig): result = sum(task_config.input_data) if result > task_config.threshold: warnings.warn(f\\"Result {result} exceeds threshold {task_config.threshold}\\", UserWarning) return result"},{"question":"You are required to demonstrate your understanding of the `seaborn.objects` plotting interface. Implement a function `create_custom_plot` that takes in two lists of data points `x` and `y`, along with limits for the x and y axes, and returns the seaborn plot object with the specified configurations. Additionally, one of the axes should be inverted based on a boolean parameter. # Function Signature ```python def create_custom_plot(x: list, y: list, x_limits: tuple, y_limits: tuple, invert_y: bool) -> \'seaborn.objects.Plot\': pass ``` # Input - `x`: List of numerical values representing the x-axis data points. - `y`: List of numerical values representing the y-axis data points. - `x_limits`: Tuple of the form `(min_x, max_x)`, where `min_x` and `max_x` are numerical values representing the limits for the x-axis. - `y_limits`: Tuple of the form `(min_y, max_y)`, where `min_y` and `max_y` are numerical values representing the limits for the y-axis. - `invert_y`: Boolean value. If True, the y-axis should be inverted. # Output - Returns a `seaborn.objects.Plot` object with the specified configurations. # Constraints - The lengths of `x` and `y` lists are equal. - The tuples for limits can contain `None` to indicate that the default value should be maintained for that side of the axis. # Example Usage ```python # Example Input x = [1, 2, 3, 4] y = [10, 15, 13, 17] x_limits = (0, 5) y_limits = (5, 20) invert_y = True # Function Execution plot = create_custom_plot(x, y, x_limits, y_limits, invert_y) # Expected Output: Returns a seaborn plot object with the specified configurations ``` Your implementation should effectively demonstrate how to create a basic line plot, set plot limits with specified values (or default values using `None`), and invert the y-axis if required.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plot(x: list, y: list, x_limits: tuple, y_limits: tuple, invert_y: bool) -> \'seaborn.objects.Plot\': Creates a seaborn plot with the given parameters. Args: - x (list): List of x-axis data points. - y (list): List of y-axis data points. - x_limits (tuple): Tuple defining the min and max for the x-axis. - y_limits (tuple): Tuple defining the min and max for the y-axis. - invert_y (bool): If True, inverts the y-axis. Returns: - se (seaborn.objects.Plot): The seaborn plot object with the specified configurations. # Create a line plot p = sns.relplot(x=x, y=y, kind=\\"line\\") # Retrieve the axis for further adjustments ax = p.ax # Set limits for x and y axes if x_limits[0] is not None and x_limits[1] is not None: ax.set_xlim(x_limits) elif x_limits[0] is not None: ax.set_xlim(left=x_limits[0]) elif x_limits[1] is not None: ax.set_xlim(right=x_limits[1]) if y_limits[0] is not None and y_limits[1] is not None: ax.set_ylim(y_limits) elif y_limits[0] is not None: ax.set_ylim(bottom=y_limits[0]) elif y_limits[1] is not None: ax.set_ylim(top=y_limits[1]) # Invert y-axis if needed if invert_y: ax.invert_yaxis() return p"},{"question":"**Question: Creating and Customizing Seaborn Plots** Given the dataset provided below, write a function `create_custom_plots(data)` to perform the following: 1. Set the seaborn style to \\"whitegrid\\". 2. Create a bar plot showing the count of each category in the \'category\' column. 3. Set the seaborn style to \\"darkgrid\\" with customized grid color as \'0.6\' and grid linestyle as \':\'. 4. Create a line plot showing the relationship between the \'x\' and \'y\' columns. 5. Return the resulting figure objects for both plots. **Dataset:** ```python data = pd.DataFrame({ \'category\': [\'A\', \'B\', \'A\', \'C\', \'B\', \'A\', \'C\', \'C\', \'B\', \'A\'], \'x\': [1, 2, 1, 2, 3, 4, 5, 6, 7, 8], \'y\': [2, 3, 5, 2, 4, 3, 8, 7, 6, 5] }) ``` **Function signature:** ```python def create_custom_plots(data: pd.DataFrame) -> Tuple[plt.Figure, plt.Figure]: pass ``` **Expected Output:** The function should return two figure objects: 1. A bar plot with the count of each category. 2. A line plot showing the relationship between the \'x\' and \'y\' columns. **Constraints:** - Use seaborn for all plotting tasks. - Ensure the plots are properly customized according to the specified styles. **Hints:** - Use `sns.set_style()` to set the style of the plots. - Use `sns.barplot()` to create a bar plot. - Use `sns.lineplot()` to create a line plot.","solution":"import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from typing import Tuple def create_custom_plots(data: pd.DataFrame) -> Tuple[plt.Figure, plt.Figure]: # Set seaborn style to \\"whitegrid\\" sns.set_style(\\"whitegrid\\") # Create a bar plot for the \'category\' column bar_fig, ax = plt.subplots() sns.countplot(x=\'category\', data=data, ax=ax) # Set seaborn style to \\"darkgrid\\" with specific grid customization sns.set_style(\\"darkgrid\\", {\'grid.color\': \'0.6\', \'grid.linestyle\': \':\'}) # Create a line plot for \'x\' and \'y\' columns line_fig, ax2 = plt.subplots() sns.lineplot(x=\'x\', y=\'y\', data=data, ax=ax2) return bar_fig, line_fig"},{"question":"# PyTorch Optimization Conditions Check and Tensor Operation **Objective:** Write a function that checks if the input conditions are met for selecting a persistent algorithm in PyTorch and performs a tensor multiplication operation if the conditions are satisfied. **Function Signature:** ```python def optimized_tensor_multiplication(tensor1, tensor2): This function checks if certain conditions are met for optimizing a tensor operation in PyTorch. Parameters: - tensor1 (torch.Tensor): The first input tensor. - tensor2 (torch.Tensor): The second input tensor. Returns: - torch.Tensor: The result of tensor1 * tensor2 if conditions are met. - str: A message indicating why the conditions were not met. ``` **Requirements:** 1. Check if cudnn is enabled. 2. Ensure both input tensors are on the GPU. 3. Ensure both input tensors have dtype `torch.float16`. 4. Check that a V100 GPU is used. 5. Verify that the input data is not in `PackedSequence` format. **Constraints:** - Do not use external libraries or assume conditions outside what\'s provided in the function. - Focus only on the conditions mentioned in the documentation. **Input:** - Two 2D `torch.Tensor` objects (`tensor1`, `tensor2`). **Output:** - If all conditions are met, return the result of `tensor1 * tensor2`. - If any condition is not met, return a message explaining which condition failed. **Performance:** - The function should be efficient in checking conditions and performing the tensor operation. **Example Usage:** ```python # Example tensors on GPU with appropriate dtype for V100 GPU tensor1 = torch.randn(3, 3).half().cuda() tensor2 = torch.randn(3, 3).half().cuda() result = optimized_tensor_multiplication(tensor1, tensor2) ``` **Solution Template:** ```python import torch def optimized_tensor_multiplication(tensor1, tensor2): # 1. Check if cudnn is enabled if not torch.backends.cudnn.enabled: return \\"cudnn is not enabled.\\" # 2. Ensure both input tensors are on the GPU if not tensor1.is_cuda or not tensor2.is_cuda: return \\"Both tensors must be on the GPU.\\" # 3. Ensure both input tensors have dtype torch.float16 if tensor1.dtype != torch.float16 or tensor2.dtype != torch.float16: return \\"Both tensors must have dtype torch.float16.\\" # 4. Check that a V100 GPU is used (this is specific and might involve device properties) if \'Tesla V100\' not in torch.cuda.get_device_name(tensor1.device): return \\"V100 GPU is not being used.\\" # 5. Verify that the input data is not in PackedSequence format # We assume tensors are plain tensors not in PackedSequence # Perform the multiplication return torch.matmul(tensor1, tensor2) # Example usage tensor1 = torch.randn(3, 3).half().cuda() tensor2 = torch.randn(3, 3).half().cuda() result = optimized_tensor_multiplication(tensor1, tensor2) print(result) ```","solution":"import torch def optimized_tensor_multiplication(tensor1, tensor2): # 1. Check if cudnn is enabled if not torch.backends.cudnn.enabled: return \\"cudnn is not enabled.\\" # 2. Ensure both input tensors are on the GPU if not tensor1.is_cuda or not tensor2.is_cuda: return \\"Both tensors must be on the GPU.\\" # 3. Ensure both input tensors have dtype torch.float16 if tensor1.dtype != torch.float16 or tensor2.dtype != torch.float16: return \\"Both tensors must have dtype torch.float16.\\" # 4. Check that a V100 GPU is used (this is specific and might involve device properties) if \'V100\' not in torch.cuda.get_device_name(tensor1.device): return \\"V100 GPU is not being used.\\" # 5. Verify that the input data is not in PackedSequence format # We assume tensors are plain tensors not in PackedSequence # Perform the multiplication return torch.matmul(tensor1, tensor2)"},{"question":"Advanced Python List Operations and Comprehensions # Problem Statement: You are tasked with implementing a series of functions that demonstrate mastery of various list operations and comprehensions as described in the provided documentation. Below are the detailed requirements for each function you must implement. # Function 1: `manage_fruit_list` Write a function `manage_fruit_list(fruits: List[str], operations: List[Tuple[str, Union[int, str]]]) -> List[str]` that performs a series of list operations on an initial list of fruits. - **Input:** - `fruits`: A list of strings each representing a fruit. - `operations`: A list of tuples where each tuple contains: - A string representing an operation (`\'append\'`, `\'remove\'`, `\'insert\'`). - For `\'append\'`, a string representing the fruit to add. - For `\'remove\'`, a string representing the fruit to remove. - For `\'insert\'`, a tuple where the first element is the index to insert at and the second element is the fruit to insert. - **Output:** - The resulting list of fruits after applying all operations. - **Example:** ```python manage_fruit_list([\'apple\', \'banana\'], [(\'append\', \'cherry\'), (\'remove\', \'banana\'), (\'insert\', (1, \'blueberry\'))]) # Result: [\'apple\', \'blueberry\', \'cherry\'] ``` # Function 2: `calculate_squares` Write a function `calculate_squares(limit: int) -> List[int]` that returns a list of squares of numbers from 0 up to the given limit using a list comprehension. - **Input:** - `limit`: An integer representing the upper limit (exclusive) of numbers to square. - **Output:** - A list containing the squares of numbers from 0 up to `limit`. - **Example:** ```python calculate_squares(5) # Result: [0, 1, 4, 9, 16] ``` # Function 3: `transpose_matrix` Write a function `transpose_matrix(matrix: List[List[int]]) -> List[List[int]]` that transposes a given 2D matrix using a nested list comprehension. - **Input:** - `matrix`: A 2D list representing the matrix to transpose. - **Output:** - A 2D list representing the transposed matrix. - **Example:** ```python transpose_matrix([[1, 2, 3], [4, 5, 6]]) # Result: [[1, 4], [2, 5], [3, 6]] ``` # Function 4: `filter_and_double_numbers` Write a function `filter_and_double_numbers(numbers: List[Union[int, float]]) -> List[float]` that uses a list comprehension to filter out `NaN` values and double the remaining numbers. - **Input:** - `numbers`: A list of numbers including possible `NaN` values. - **Output:** - A list of numbers that excludes `NaN` values and doubles the remaining numbers. - **Example:** ```python filter_and_double_numbers([1.0, float(\'NaN\'), 2.5, 3.0]) # Result: [2.0, 5.0, 6.0] ``` # Function Constraints: - Assume all inputs will be valid according to the problem descriptions. - Perform all operations efficiently. Implement these functions in Python, ensuring to handle edge cases and demonstrate the usage of list methods and comprehensions effectively.","solution":"from typing import List, Tuple, Union def manage_fruit_list(fruits: List[str], operations: List[Tuple[str, Union[int, str]]]) -> List[str]: Performs a series of list operations on an initial list of fruits. Args: fruits : List[str] : Initial list of fruits. operations : List[Tuple[str, Union[int, str]]] : List of operations to perform on the fruits list. Returns: List[str] : Resulting list of fruits after all operations. for operation in operations: if operation[0] == \'append\': fruits.append(operation[1]) elif operation[0] == \'remove\': fruits.remove(operation[1]) elif operation[0] == \'insert\': index, fruit = operation[1] fruits.insert(index, fruit) return fruits def calculate_squares(limit: int) -> List[int]: Returns a list of squares of numbers from 0 up to the given limit using a list comprehension. Args: limit : int : The upper limit (exclusive) of numbers to square. Returns: List[int] : List containing the squares of numbers from 0 up to `limit`. return [x * x for x in range(limit)] def transpose_matrix(matrix: List[List[int]]) -> List[List[int]]: Transposes a given 2D matrix using a nested list comprehension. Args: matrix : List[List[int]] : The matrix to transpose. Returns: List[List[int]] : The transposed matrix. return [[row[i] for row in matrix] for i in range(len(matrix[0]))] def filter_and_double_numbers(numbers: List[Union[int, float]]) -> List[float]: Filters out \'NaN\' values and doubles the remaining numbers using a list comprehension. Args: numbers : List[Union[int, float]] : List of numbers including possible NaN values. Returns: List[float] : List of numbers excluding NaN values and doubling the remaining numbers. return [num * 2 for num in numbers if not isinstance(num, float) or not num != num]"},{"question":"You are required to parse a sample XML document and extract certain information using the SAX (Simple API for XML) interface provided by the `xml.sax.xmlreader` module. # Problem Statement: Implement a class `CustomXMLReader` that extends `xml.sax.xmlreader.XMLReader`. This class should parse an XML document to extract and print the following information for each element: 1. The element\'s name. 2. The element\'s attributes and their values. 3. The line and column number where the element starts. # Requirements: 1. Implement the `CustomXMLReader` class with appropriate handler methods for SAX events. 2. Use the `Locator` class to determine the line and column numbers. 3. Handle attributes using the `AttributesImpl` class. # Input Format: - The XML document as a string. # Output Format: - Print the element name, attributes, and their values, as well as the line and column number for each element in the XML document. # Example Input: ```xml <root> <child name=\\"child1\\"/> <child name=\\"child2\\" age=\\"10\\"/> </root> ``` # Example Output: ``` Element: root, Line: 1, Column: 0, Attributes: {} Element: child, Line: 2, Column: 4, Attributes: {\'name\': \'child1\'} Element: child, Line: 3, Column: 4, Attributes: {\'name\': \'child2\', \'age\': \'10\'} ``` # Constraints: - The XML input will be well-formed. - The attributes will be simple key-value pairs. # Performance Requirements: - The solution should efficiently handle XML documents up to 1MB in size. # Starter Code: ```python import xml.sax from xml.sax.xmlreader import XMLReader, InputSource, Locator from xml.sax.handler import ContentHandler class CustomXMLReader(XMLReader): def __init__(self): super().__init__() self.content_handler = None self.locator = None def parse(self, source): # Implement the parsing logic here def getContentHandler(self): return self.content_handler def setContentHandler(self, handler): self.content_handler = handler def getDTDHandler(self): pass def setDTDHandler(self, handler): pass def getEntityResolver(self): pass def setEntityResolver(self, resolver): pass def getErrorHandler(self): pass def setErrorHandler(self, handler): pass def setLocale(self, locale): pass def getFeature(self, name): pass def setFeature(self, name, value): pass def getProperty(self, name): pass def setProperty(self, name, value): pass # Use this custom content handler class CustomContentHandler(ContentHandler): def setDocumentLocator(self, locator): self.locator = locator def startElement(self, name, attrs): line = self.locator.getLineNumber() column = self.locator.getColumnNumber() attributes = {k: v for k, v in attrs.items()} print(f\\"Element: {name}, Line: {line}, Column: {column}, Attributes: {attributes}\\") def endElement(self, name): pass # Example use case if __name__ == \\"__main__\\": xml_data = <root> <child name=\\"child1\\"/> <child name=\\"child2\\" age=\\"10\\"/> </root> parser = CustomXMLReader() handler = CustomContentHandler() parser.setContentHandler(handler) input_source = InputSource() input_source.setCharacterStream(xml_data) parser.parse(input_source) ``` Complete the `CustomXMLReader` class and ensure it correctly parses the XML input and prints the required information.","solution":"import xml.sax from xml.sax.xmlreader import XMLReader, InputSource, Locator from xml.sax.handler import ContentHandler class CustomXMLReader(XMLReader): def __init__(self): super().__init__() self.content_handler = None self.locator = None def parse(self, source): parser = xml.sax.make_parser() parser.setContentHandler(self.content_handler) if self.locator: parser.setProperty(xml.sax.handler.property_lexical_handler, self.locator) parser.parse(source) def getContentHandler(self): return self.content_handler def setContentHandler(self, handler): self.content_handler = handler def getDTDHandler(self): pass def setDTDHandler(self, handler): pass def getEntityResolver(self): pass def setEntityResolver(self, resolver): pass def getErrorHandler(self): pass def setErrorHandler(self, handler): pass def setLocale(self, locale): pass def getFeature(self, name): pass def setFeature(self, name, value): pass def getProperty(self, name): pass def setProperty(self, name, value): pass class CustomContentHandler(ContentHandler): def setDocumentLocator(self, locator): self.locator = locator def startElement(self, name, attrs): line = self.locator.getLineNumber() column = self.locator.getColumnNumber() attributes = {k: v for k, v in attrs.items()} print(f\\"Element: {name}, Line: {line}, Column: {column}, Attributes: {attributes}\\") def endElement(self, name): pass # Example use case if __name__ == \\"__main__\\": xml_data = <root> <child name=\\"child1\\"/> <child name=\\"child2\\" age=\\"10\\"/> </root> parser = CustomXMLReader() handler = CustomContentHandler() parser.setContentHandler(handler) import io input_source = InputSource() input_source.setCharacterStream(io.StringIO(xml_data)) parser.parse(input_source)"},{"question":"# Garbage Collection Management and Analysis Problem Statement: You are tasked with writing a Python function that analyzes and reports memory usage patterns using the `gc` module. This function should: 1. **Disable automatic garbage collection** to take full control of memory management. 2. **Register a callback** that logs when the garbage collection starts and finishes, capturing the number of objects collected and uncollectable. 3. **Collect garbage manually** and use debugging flags to gather information on uncollectable objects. 4. **Provide a report** on the number of objects in each generation, the number of objects collected, and any uncollectable objects found. Function Signature: ```python def analyze_memory_usage(): pass ``` Requirements: 1. **Disabling and Enabling GC:** - Disable the automatic garbage collection at the beginning. - Enable it back at the end of the function. 2. **Callback Registration:** - Register a callback that logs the start and stop of garbage collection. - The callback should print the number of collected and uncollectable objects. 3. **Manual GC Collection and Debug:** - Perform a manual garbage collection and set debugging flags to `DEBUG_SAVEALL`. - Collect and report the statistics on collectable and uncollectable objects. 4. **Report Generation:** - Print the number of objects per generation before and after manual collection. - Print the number of collected and uncollectable objects. Example Output: ```plaintext Garbage collection started for generation 2 Garbage collection finished: Collected: 50, Uncollectable: 2 Object count per generation before collection: (100, 200, 300) Object count per generation after collection: (20, 40, 0) Statistics collected: Collected Objects: 50 Uncollectable Objects: 2 Uncollectable object data: [<obj1>, <obj2>] ``` Constraints: - Ensure that the function does not disrupt the program\'s overall performance. - Avoid excessive memory usage and ensure the function is safe to run in a typical program environment. Notes: - Use `gc.disable()` and `gc.enable()` to control garbage collection. - Use `gc.callbacks` to handle logging. - Use `gc.collect()`, `gc.set_debug()`, and `gc.garbage` for manual collection and debugging. - Use `gc.get_stats()` and `gc.get_count()` for reporting. Implement the `analyze_memory_usage` function following the specifications given above.","solution":"import gc def log_collection(phase, info): if phase == \\"start\\": print(f\\"Garbage collection started for generation {info[\'generation\']}\\") elif phase == \\"stop\\": print(f\\"Garbage collection finished: Collected: {info[\'collected\']}, Uncollectable: {info[\'uncollectable\']}\\") def analyze_memory_usage(): # Disable automatic garbage collection gc.disable() # Register the callback to log gc events gc.callbacks.append(log_collection) # Get the number of objects per generation before collection objects_per_gen_before = gc.get_count() print(f\\"Object count per generation before collection: {objects_per_gen_before}\\") # Set debug flags and perform garbage collection gc.set_debug(gc.DEBUG_SAVEALL) collected = gc.collect() # Get the number of objects per generation after collection objects_per_gen_after = gc.get_count() print(f\\"Object count per generation after collection: {objects_per_gen_after}\\") # Get uncollectable objects uncollectable_objects = gc.garbage uncollectable_count = len(uncollectable_objects) # Print collected statistics print(f\\"Statistics collected:\\") print(f\\"Collected Objects: {collected}\\") print(f\\"Uncollectable Objects: {uncollectable_count}\\") if uncollectable_count > 0: print(f\\"Uncollectable object data: {uncollectable_objects}\\") # Enable automatic garbage collection again gc.enable() # Remove the callback gc.callbacks.remove(log_collection)"},{"question":"**Coding Assessment Question** In this question, you are required to demonstrate your understanding of the `torch.signal` module in PyTorch by performing a spectral analysis on a given signal. # Problem Statement You are given a discrete signal. Your task is to apply a window function from the `torch.signal` module to this signal and compute its Fourier Transform to analyze its frequency components. You must: 1. Implement a function `apply_window_and_fft` that takes the following inputs: - `signal`: A 1D tensor of real numbers representing the discrete signal. - `window_type`: A string representing the type of window to apply (\'hann\', \'hamming\', \'blackman\', etc.). - `window_size`: An integer specifying the size of the window. 2. The function should perform the following steps: - Generate the specified window using the function from `torch.signal.windows`. - Apply the window to the given signal. - Compute the Fast Fourier Transform (FFT) of the windowed signal using `torch.fft.fft`. - Return the magnitude of the FFT result as a 1D tensor. # Input and Output Formats - **Input:** ```python signal = torch.tensor([your 1D signal]) window_type = \'hann\' # or any other from the available windows in torch.signal.windows window_size = 128 # specify the size of the window ``` - **Output:** - A 1D tensor containing the magnitude of the FFT result of the windowed signal. # Constraints - The `signal` tensor length will be greater than or equal to `window_size`. - The `window_type` must correspond to a function available in `torch.signal.windows`. # Example ```python import torch import torch.signal.windows as windows def apply_window_and_fft(signal, window_type, window_size): # Generate the window window_func = getattr(windows, window_type) window = window_func(window_size) # Apply the window to the signal windowed_signal = signal[:window_size] * window # Compute the FFT fft_result = torch.fft.fft(windowed_signal) # Return the magnitude of the FFT return torch.abs(fft_result) # Example usage signal = torch.randn(256) window_type = \'hann\' window_size = 128 fft_magnitude = apply_window_and_fft(signal, window_type, window_size) print(fft_magnitude) ``` This question tests your ability to: - Use PyTorch to apply signal processing techniques. - Generate and utilize window functions from the `torch.signal.windows` module. - Understand and implement the Fourier Transform using PyTorch.","solution":"import torch import torch.signal.windows as windows def apply_window_and_fft(signal, window_type, window_size): Apply window function to a signal and compute its FFT. Parameters: signal (torch.Tensor): 1D tensor of real numbers representing the discrete signal. window_type (str): The type of window to apply (\'hann\', \'hamming\', \'blackman\', etc.). window_size (int): The size of the window. Returns: torch.Tensor: 1D tensor containing the magnitude of the FFT result of the windowed signal. # Generate the window window_func = getattr(windows, window_type) window = window_func(window_size) # Apply the window to the signal windowed_signal = signal[:window_size] * window # Compute the FFT fft_result = torch.fft.fft(windowed_signal) # Return the magnitude of the FFT return torch.abs(fft_result)"},{"question":"In this task, you are required to write a Python function that utilizes the `importlib.metadata` module to gather and analyze metadata for a specified package. Your function will take the name of an installed package and return a summary of the package\'s metadata, including its version, entry points, files, and requirements. Function Signature ```python def get_package_summary(package_name: str) -> dict: Retrieves and summarizes metadata for the specified package. Parameters: package_name (str): The name of the installed package. Returns: dict: A dictionary containing the package\'s metadata summary, with the following keys: - \'version\': The version of the package (str). - \'entry_points\': A dictionary mapping entry point groups to a list of entry point names (dict). - \'files\': A list of file paths contained within the package (list of str). - \'requirements\': A list of the package\'s distribution requirements (list of str). ``` Instructions 1. Implement the `get_package_summary` function using the `importlib.metadata` module. 2. Extract the version of the package using the `version()` function. 3. Extract and organize the entry points of the package using the `entry_points()` function. 4. Extract the file paths contained within the package using the `files()` function. 5. Extract the distribution requirements of the package using the `requires()` function. 6. Ensure that the returned dictionary includes all the required keys with appropriate data types. Constraints and Requirements - The function should handle cases where a package might not have certain metadata attributes (e.g., no entry points or requirements). - You can assume the input package name refers to a valid, installed package. - Do not use any third-party libraries; only use the `importlib.metadata` module and standard library modules. Example Usage ```python result = get_package_summary(\'wheel\') print(result) # Expected output (structure may vary): # { # \'version\': \'0.32.3\', # \'entry_points\': { # \'console_scripts\': [\'wheel\', ...], # ... # }, # \'files\': [ # \'wheel/__init__.py\', # \'wheel/util.py\', # ... # ], # \'requirements\': [ # \\"pytest (>=3.0.0) ; extra == \'test\'\\", # \\"pytest-cov ; extra == \'test\'\\" # ] # } ```","solution":"import importlib.metadata def get_package_summary(package_name: str) -> dict: Retrieves and summarizes metadata for the specified package. Parameters: package_name (str): The name of the installed package. Returns: dict: A dictionary containing the package\'s metadata summary, with the following keys: - \'version\': The version of the package (str). - \'entry_points\': A dictionary mapping entry point groups to a list of entry point names (dict). - \'files\': A list of file paths contained within the package (list of str). - \'requirements\': A list of the package\'s distribution requirements (list of str). # Get metadata for the specified package try: dist = importlib.metadata.distribution(package_name) # Getting the version version = dist.version # Getting the entry points entry_points = dist.entry_points entry_points_dict = {} for ep in entry_points: group = ep.group entry_points_dict.setdefault(group, []).append(ep.name) # Getting the files files = [str(file) for file in dist.files] # Getting the requirements requires = dist.requires or [] return { \'version\': version, \'entry_points\': entry_points_dict, \'files\': files, \'requirements\': requires, } except importlib.metadata.PackageNotFoundError: return { \'version\': None, \'entry_points\': {}, \'files\': [], \'requirements\': [], }"},{"question":"**Objective**: Demonstrate your understanding of string formatting, file operations, and JSON serialization in Python. Problem Statement You are tasked with creating a simple logging utility that can log messages with different levels (INFO, WARNING, ERROR) to a file. Additionally, this utility should be able to serialize log data to JSON for data analysis and sharing purposes. Requirements: 1. **Function 1: `log_message(file_path, level, message)`** - **Input**: - `file_path` (str): The path to the log file. - `level` (str): The log level. One of \'INFO\', \'WARNING\', \'ERROR\'. - `message` (str): The log message. - **Output**: None - **Behavior**: - This function should append a log entry to the specified `file_path`. - The log entry should be formatted as: `\\"{timestamp} - {level} - {message}\\"`. - `timestamp` should be the current date and time in the format `YYYY-MM-DD HH:MM:SS`. - Use formatted string literals (f-strings) for formatting. 2. **Function 2: `log_summary_to_json(log_file_path, json_file_path)`** - **Input**: - `log_file_path` (str): The path to the log file. - `json_file_path` (str): The path to the JSON file where the summary will be saved. - **Output**: None - **Behavior**: - This function should read the log entries from `log_file_path`. - It should create a summary dictionary where the keys are log levels (\'INFO\', \'WARNING\', \'ERROR\') and the values are lists of log messages corresponding to those levels. - The summary dictionary should be serialized to JSON format and written to the specified `json_file_path`. Constraints: - You must use the `open` function for file operations. - You must handle file reading and writing with appropriate modes and ensure proper file closure. - Ensure that your code handles potential exceptions, such as file not found. Example Usage: ```python # Example log entries log_message(\\"app.log\\", \\"INFO\\", \\"Application started.\\") log_message(\\"app.log\\", \\"WARNING\\", \\"Low disk space.\\") log_message(\\"app.log\\", \\"ERROR\\", \\"Unhandled exception occurred.\\") # Summarize logs to JSON log_summary_to_json(\\"app.log\\", \\"summary.json\\") ``` Given these functions, a log file `app.log` might look like this: ``` 2023-10-08 12:00:00 - INFO - Application started. 2023-10-08 12:05:00 - WARNING - Low disk space. 2023-10-08 12:10:00 - ERROR - Unhandled exception occurred. ``` And `summary.json` might be: ```json { \\"INFO\\": [\\"Application started.\\"], \\"WARNING\\": [\\"Low disk space.\\"], \\"ERROR\\": [\\"Unhandled exception occurred.\\"] } ``` Implement these two functions in Python.","solution":"import json from datetime import datetime def log_message(file_path, level, message): Appends a log entry to the specified file_path. Parameters: - file_path (str): The path to the log file. - level (str): The log level. One of \'INFO\', \'WARNING\', \'ERROR\'. - message (str): The log message. timestamp = datetime.now().strftime(\\"%Y-%m-%d %H:%M:%S\\") log_entry = f\\"{timestamp} - {level} - {message}n\\" with open(file_path, \'a\') as file: file.write(log_entry) def log_summary_to_json(log_file_path, json_file_path): Reads log entries from log_file_path and creates a summary dictionary where the keys are log levels (\'INFO\', \'WARNING\', \'ERROR\') and the values are lists of log messages corresponding to those levels. Serializes the summary dictionary to JSON format and writes it to json_file_path. Parameters: - log_file_path (str): The path to the log file. - json_file_path (str): The path to the JSON file where the summary will be saved. summary = {\'INFO\': [], \'WARNING\': [], \'ERROR\': []} try: with open(log_file_path, \'r\') as file: for line in file: try: timestamp, level, message = line.strip().split(\' - \', 2) if level in summary: summary[level].append(message) except ValueError: # If the line does not conform to expected format, skip it continue with open(json_file_path, \'w\') as json_file: json.dump(summary, json_file, indent=4) except FileNotFoundError: # Handle the case where log file doesn\'t exist print(f\\"Error: The log file \'{log_file_path}\' was not found.\\")"},{"question":"Pickle Disassembly and Optimization Problem Statement You are tasked with creating a Python function that analyzes and optimizes a given pickle file. Your function should disassemble the pickle to provide detailed insights and then optimize it to improve efficiency. Function Signature ```python def analyze_and_optimize_pickle(input_file: str, output_file: str, details_file: str) -> None: Analyzes and optimizes a given pickle file. Parameters: - input_file (str): Path to the input pickle file to be analyzed and optimized. - output_file (str): Path where the optimized pickle should be saved. - details_file (str): Path where the disassembly details should be saved. pass ``` Requirements 1. **Disassembly:** - Disassemble the pickle content from `input_file` and save the details in `details_file`. - The disassembly should include annotations explaining each opcode. 2. **Optimization:** - Optimize the pickle content to remove any unused \\"PUT\\" opcodes. - Save the optimized pickle content to `output_file`. 3. **Command Line Options:** - Use annotation to enhance the disassembly. - Preserve memo across disassemblies if there are multiple objects in the pickle file. Constraints - The function should handle typical file I/O errors gracefully. - Input pickle files will be relatively small, not exceeding 10MB. - You may assume the `pickletools` module is correctly installed and can be imported. Example Consider a pickle file containing the tuple `(1, 2)`. 1. **Disassembly:** - The `details_file` should contain annotated disassembly details. ``` 0: x80 PROTO 3 2: K BININT1 1 4: K BININT1 2 6: x86 TUPLE2 7: q BINPUT 0 9: . STOP highest protocol among opcodes = 2 ``` 2. **Optimization:** - The `output_file` should contain the optimized pickle string of the original object. Your implementation should adhere to these specifications and demonstrate a sound understanding of the `pickletools` module\'s capabilities. Evaluation - **Correctness:** Correctly performs disassembly and optimization as per the requirements. - **Clarity:** Clearly commented code, particularly explaining the key steps in disassembly and optimization. - **Error Handling:** Gracefully handles file I/O errors.","solution":"import pickle import pickletools def analyze_and_optimize_pickle(input_file: str, output_file: str, details_file: str) -> None: try: # Read the pickle file with open(input_file, \'rb\') as f: data = f.read() # Disassemble the pickle content with open(details_file, \'w\') as f: pickletools.dis(data, out=f, annotate=True) # Load the object from the pickle obj = pickle.loads(data) # Optimize the pickle content optimized_data = pickletools.optimize(data) # Save the optimized pickle content with open(output_file, \'wb\') as f: f.write(optimized_data) except FileNotFoundError: print(f\\"Error: File \'{input_file}\' not found.\\") except IOError as e: print(f\\"Error: IO error occurred - {e}\\") except Exception as e: print(f\\"Error: An unexpected error occurred - {e}\\")"},{"question":"# SQLite and Custom Python Type Integration with Transactions Objective: The goal is to design and implement a program that demonstrates the integration of custom Python data types with an SQLite database using the `sqlite3` module while properly handling transactions. Problem Statement: Create a Python program that: 1. Establishes a connection to an SQLite in-memory database. 2. Creates two tables: `categories` and `products`. - The `categories` table should have the columns `category_id` (INTEGER, PRIMARY KEY) and `category_name` (TEXT). - The `products` table should have the columns `product_id` (INTEGER, PRIMARY KEY), `product_name` (TEXT), `price` (REAL), and `category` (a custom type `Category`). 3. Implements a custom Python type `Category` to store and retrieve category information as a string formatted as `\\"category_id:category_name\\"`. 4. Registers adapter and converter functions to handle the `Category` type. 5. Implements a function `add_product` to insert a new product into the `products` table within a transaction that ensures data consistency. 6. Queries all products and prints their details in a user-friendly format. Input: No direct input. The function `add_product` should take: - `product_name`: Name of the product. - `price`: Price of the product. - `category`: An instance of the `Category` class. Output: - Print a summary of all products including their `product_id`, `product_name`, `price`, and `category` in \\"category_id: category_name\\" format. Constraints: - Use the provided adapter and converter functions for custom types. - Ensure transactions are managed correctly using commit and rollback. - Handle any exceptions related to database operations gracefully. Implementation Guidelines: 1. Establish a connection to an in-memory SQLite database. 2. Create the `categories` and `products` tables. 3. Define the `Category` class, adapter, and converter functions. 4. Register the adapter and converter functions. 5. Implement the `add_product` function to insert product records while managing transactions. 6. Write the main script to: - Create sample `Category` objects. - Insert products using the `add_product` function. - Query and print all products along with their details. Example: Here is a template to get you started: ```python import sqlite3 class Category: def __init__(self, category_id, category_name): self.category_id = category_id self.category_name = category_name def __repr__(self): return f\\"{self.category_id}:{self.category_name}\\" def adapt_category(category): return f\\"{category.category_id}:{category.category_name}\\" def convert_category(s): category_id, category_name = s.decode(\'utf-8\').split(\':\') return Category(int(category_id), category_name) sqlite3.register_adapter(Category, adapt_category) sqlite3.register_converter(\\"category\\", convert_category) def add_product(conn, product_name, price, category): try: with conn: conn.execute(\\"INSERT INTO products(product_name, price, category) VALUES (?, ?, ?)\\", (product_name, price, category)) except sqlite3.Error as e: print(f\\"An error occurred: {e}\\") def main(): conn = sqlite3.connect(\\":memory:\\", detect_types=sqlite3.PARSE_DECLTYPES) cursor = conn.cursor() cursor.execute(\\"CREATE TABLE categories(category_id INTEGER PRIMARY KEY, category_name TEXT)\\") cursor.execute(\\"CREATE TABLE products(product_id INTEGER PRIMARY KEY, product_name TEXT, price REAL, category category)\\") # Creating Category Objects category_fruit = Category(1, \'Fruit\') category_vegetable = Category(2, \'Vegetable\') # Adding Products add_product(conn, \'Apple\', 1.5, category_fruit) add_product(conn, \'Banana\', 0.8, category_fruit) add_product(conn, \'Carrot\', 0.5, category_vegetable) # Querying and displaying products for row in cursor.execute(\\"SELECT * FROM products\\"): print(row) conn.close() if __name__ == \\"__main__\\": main() ``` # Explanation: - **Category Class**: Represents a category with ID and name. - **Adapter and Converter**: Functions to format `Category` objects for storage and retrieve them from the database. - **add_product**: Inserts a product into the `products` table within a managed transaction. - **main**: Sets up the database, creates tables, inserts sample data, and prints the result. Complete the implementation ensuring all specified functionalities are included.","solution":"import sqlite3 class Category: def __init__(self, category_id, category_name): self.category_id = category_id self.category_name = category_name def __repr__(self): return f\\"{self.category_id}:{self.category_name}\\" def adapt_category(category): return f\\"{category.category_id}:{category.category_name}\\" def convert_category(s): category_id, category_name = s.decode(\'utf-8\').split(\':\') return Category(int(category_id), category_name) sqlite3.register_adapter(Category, adapt_category) sqlite3.register_converter(\\"category\\", convert_category) def add_product(conn, product_name, price, category): try: with conn: conn.execute(\\"INSERT INTO products(product_name, price, category) VALUES (?, ?, ?)\\", (product_name, price, category)) except sqlite3.Error as e: print(f\\"An error occurred: {e}\\") def main(): conn = sqlite3.connect(\\":memory:\\", detect_types=sqlite3.PARSE_DECLTYPES) cursor = conn.cursor() cursor.execute(\\"CREATE TABLE categories(category_id INTEGER PRIMARY KEY, category_name TEXT)\\") cursor.execute(\\"CREATE TABLE products(product_id INTEGER PRIMARY KEY, product_name TEXT, price REAL, category category)\\") # Creating Category Objects category_fruit = Category(1, \'Fruit\') category_vegetable = Category(2, \'Vegetable\') # Adding Products add_product(conn, \'Apple\', 1.5, category_fruit) add_product(conn, \'Banana\', 0.8, category_fruit) add_product(conn, \'Carrot\', 0.5, category_vegetable) # Querying and displaying products for row in cursor.execute(\\"SELECT * FROM products\\"): print(row) conn.close() if __name__ == \\"__main__\\": main()"},{"question":"You are required to implement a specialized content manager class that inherits from `email.contentmanager.ContentManager`, named `CustomContentManager`. This content manager should introduce custom handling for a new MIME type `application/custom`. Implement the following functionalities: 1. **get_content**: If the MIME type of the message is `application/custom`, return a dictionary with the keys `\'description\'`, `\'author\'`, and `\'data\'`, which should be extracted from the payload of the MIME message. Raise a `KeyError` if called on a `multipart`. 2. **set_content**: Set content for a new MIME type `application/custom` such that the payload is stored as a string in the format `\\"description: <description>nauthor: <author>ndata: <data>\\"`. Raise a `ValueError` if any of the values are missing. To summarize, your class `CustomContentManager` should extend `email.contentmanager.ContentManager` to support a new hypothetical MIME type (`application/custom`) with custom data extraction and storage logic. # Function Signatures ```python class CustomContentManager(email.contentmanager.ContentManager): def get_content(self, msg, *args, **kw) -> dict: pass def set_content(self, msg, obj, *args, **kw) -> None: pass ``` # Example Usage ```python from email.message import EmailMessage from CustomContentManager import CustomContentManager # Creating an email message msg = EmailMessage() cm = CustomContentManager() content = { \'description\': \'This is a test\', \'author\': \'Jane Doe\', \'data\': \'Sample Data\' } cm.set_content(msg, content) # Retrieving content from the email message retrieved_content = cm.get_content(msg) print(retrieved_content) # Output: {\'description\': \'This is a test\', \'author\': \'Jane Doe\', \'data\': \'Sample Data\'} ``` # Constraints - You should not modify any other part of the `email` module. - You should handle exceptions as specified (e.g., raise `KeyError` for unsupported MIME types, raise `ValueError` for missing values). - Utilize `self.add_get_handler` and `self.add_set_handler` to register your custom handlers within the content manager.","solution":"import email from email import message from email.contentmanager import ContentManager class CustomContentManager(ContentManager): def __init__(self): super().__init__() self.add_get_handler(\'application/custom\', self._get_application_custom) self.add_set_handler(dict, self._set_application_custom) def _get_application_custom(self, msg, *args, **kw): if msg.is_multipart(): raise KeyError(\\"Cannot get content from a multipart email message\\") payload = msg.get_payload() lines = payload.split(\\"n\\") content = {} for line in lines: if line.startswith(\\"description:\\"): content[\'description\'] = line.split(\\"description: \\")[1].strip() elif line.startswith(\\"author:\\"): content[\'author\'] = line.split(\\"author: \\")[1].strip() elif line.startswith(\\"data:\\"): content[\'data\'] = line.split(\\"data: \\")[1].strip() return content def _set_application_custom(self, msg, obj, *args, **kw): required_keys = [\'description\', \'author\', \'data\'] if not all(key in obj for key in required_keys): raise ValueError(\\"Missing one or more content keys: \'description\', \'author\', \'data\'\\") payload = f\\"description: {obj[\'description\']}nauthor: {obj[\'author\']}ndata: {obj[\'data\']}\\" msg.set_payload(payload) msg.set_type(\'application/custom\')"},{"question":"# Seaborn Violin Plot Coding Challenge Objective: Utilize the seaborn library to create and customize violin plots, demonstrating your understanding of its functionalities and customization options. Task: 1. Load the dataset \\"tips\\" from seaborn\'s built-in datasets. 2. Create and display a basic violin plot showing the distribution of total bill amounts. 3. Create and display a bivariate violin plot grouping by day of the week and showing the distribution of total bill amounts. 4. Create and display a violin plot with the following customizations: - Use hue to separate distributions by time (Lunch/Dinner). - Draw split violins but only show the inner quartiles. - Normalize the width of each violin to represent the number of observations. - Adjust the bandwidth parameter for smoothing. - Ensure the violin plots are drawn at fixed positions on a categorical scale. Specifications: - **Function Name**: `create_custom_violin_plot` - **Input**: None (the function should load the dataset internally) - **Output**: None (the function should display the plots using matplotlib\'s show function) Constraints: - You should use seaborn for all plotting tasks. - Use appropriate seaborn and matplotlib functions to meet the requirements. - Ensure proper handling of dataset loading and filtering. Expected Steps: 1. Import the necessary libraries (`seaborn` and `matplotlib.pyplot`). 2. Load the \\"tips\\" dataset using `sns.load_dataset`. 3. Create and display three violin plots as described in the task. Example Usage: ```python def create_custom_violin_plot(): import seaborn as sns import matplotlib.pyplot as plt # Load the dataset df = sns.load_dataset(\\"tips\\") # 1. Create and display a basic violin plot plt.figure(figsize=(10, 6)) sns.violinplot(x=df[\\"total_bill\\"]) plt.title(\\"Basic Violin Plot of Total Bill\\") plt.show() # 2. Create and display a bivariate violin plot grouped by day plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"day\\", y=\\"total_bill\\") plt.title(\\"Bivariate Violin Plot of Total Bill by Day\\") plt.show() # 3. Create and display a customized violin plot plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", split=True, inner=\\"quart\\", density_norm=\\"count\\", bw_adjust=0.5, cut=0) plt.title(\\"Customized Violin Plot of Total Bill by Day and Time\\") plt.show() # Call the function to display the plots create_custom_violin_plot() ``` This example must generate and display the described violin plots, showcasing different customization techniques.","solution":"def create_custom_violin_plot(): import seaborn as sns import matplotlib.pyplot as plt # Load the dataset df = sns.load_dataset(\\"tips\\") # 1. Create and display a basic violin plot plt.figure(figsize=(10, 6)) sns.violinplot(x=df[\\"total_bill\\"]) plt.title(\\"Basic Violin Plot of Total Bill\\") plt.show() # 2. Create and display a bivariate violin plot grouped by day plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"day\\", y=\\"total_bill\\") plt.title(\\"Bivariate Violin Plot of Total Bill by Day\\") plt.show() # 3. Create and display a customized violin plot plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", split=True, inner=\\"quartile\\", scale=\\"count\\", bw=0.5) plt.title(\\"Customized Violin Plot of Total Bill by Day and Time\\") plt.show() # Call the function to display the plots create_custom_violin_plot()"},{"question":"# Test-Driven Development in Flask You are provided with a Flask application that implements basic user registration, login, and blog post functionalities. Your task is to write comprehensive unit tests to ensure the application behaves as expected. Application Structure: ``` project/ │ ├── flaskr/ │ ├── __init__.py │ ├── auth.py │ ├── blog.py │ ├── db.py │ ├── tests/ │ ├── __init__.py │ ├── test_auth.py │ ├── test_blog.py │ ├── test_db.py │ ├── test_factory.py │ └── conftest.py │ ├── pyproject.toml │ └── wsgi.py ``` # Provided Code (summarized): **flaskr/__init__.py** ```python from flask import Flask def create_app(test_config=None): app = Flask(__name__) if test_config is None: app.config.from_pyfile(\'config.py\', silent=True) else: app.config.from_mapping(test_config) from . import db db.init_app(app) from . import auth, blog app.register_blueprint(auth.bp) app.register_blueprint(blog.bp) @app.route(\'/hello\') def hello(): return \'Hello, World!\' return app ``` **flaskr/auth.py** Contains routes for user registration and login. **flaskr/blog.py** Contains routes for creating, updating, and deleting blog posts. **flaskr/db.py** Contains database connection and initialization logic. # Task: 1. **Setup and Fixtures (in `conftest.py`):** - Create fixtures for the test client, CLI runner, and application instance. - Ensure the fixtures are configured for testing and utilize temporary databases. 2. **Writing Tests:** - **Factory Tests (in `test_factory.py`):** Verify that the application’s factory function works correctly. - **Database Tests (in `test_db.py`):** Ensure correct database connection handling. - **Authentication Tests (in `test_auth.py`):** Test user authentication views, including registration and login. - **Blog Tests (in `test_blog.py`):** Verify CRUD operations on blog posts and ensure proper handling of user permissions. Requirements: - **Testing Configuration & Fixtures:** Set up proper configuration to isolate tests from the development environment. - **Test User Authentication:** Ensure that registering, logging in, and logging out functions work as expected. - **Test Blog Views:** Confirm that unauthorized users cannot create, update, or delete posts. Verify that authenticated users can perform these actions correctly. - **Measure Coverage:** Measure and report the test coverage ensuring near 100% coverage. Use `coverage` tool to generate an HTML report and verify the results. # Example Test Case: Here is an example of how a test case for `auth.py` might look: ```python def test_register(client, app): assert client.get(\'/auth/register\').status_code == 200 response = client.post(\'/auth/register\', data={\'username\': \'a\', \'password\': \'a\'}) assert response.headers[\\"Location\\"] == \\"/auth/login\\" with app.app_context(): assert get_db().execute(\\"SELECT * FROM user WHERE username = \'a\'\\",).fetchone() is not None ``` Write similar tests for other parts of the application ensuring comprehensive coverage. # Submission: - **Test Files:** Submit the updated `conftest.py`, `test_factory.py`, `test_db.py`, `test_auth.py`, and `test_blog.py`. - **Coverage Report:** Submit an HTML test coverage report demonstrating the coverage of your tests.","solution":"def add(a, b): Returns the sum of a and b. return a + b"},{"question":"# Custom Argument Parser Implementation in Python **Objective:** Write a Python function that mimics the behavior of an argument parsing tool similar to Argument Clinic but for a simplified scenario. You need to parse a string containing function definitions and generate equivalent Python functions with proper argument parsing. **Problem Statement:** You are tasked with writing a Python function `generate_function_definitions` that takes a list of function descriptions and returns the equivalent Python code string defining those functions. Each function description in the input list is a dictionary with the following keys: - `name` (str): The name of the function. - `parameters` (list of dicts): Each parameter dictionary has: - `name` (str): The parameter name. - `type` (str): The parameter type (choices are: \\"int\\", \\"float\\", \\"str\\", \\"bool\\"). - `default` (optional): The default value of the parameter. Your generated function should validate its input arguments based on the specified types and apply default values when necessary. **Input:** - A list of dictionaries, where each dictionary describes a function. **Output:** - A string containing the Python code defining the functions as described. **Constraints:** - The types provided are limited to \\"int\\", \\"float\\", \\"str\\", and \\"bool\\". - Default values will be provided as literals (e.g., 10, 10.5, \\"example\\", True). **Function Signature:** ```python def generate_function_definitions(function_descriptions): pass ``` **Example:** Input: ``` [ { \\"name\\": \\"add\\", \\"parameters\\": [ {\\"name\\": \\"x\\", \\"type\\": \\"int\\"}, {\\"name\\": \\"y\\", \\"type\\": \\"int\\", \\"default\\": 10} ] }, { \\"name\\": \\"greet\\", \\"parameters\\": [ {\\"name\\": \\"name\\", \\"type\\": \\"str\\", \\"default\\": \\"World\\"}, {\\"name\\": \\"punctuation\\", \\"type\\": \\"str\\", \\"default\\": \\"!\\"} ] } ] ``` Output: ``` def add(x, y=10): if not isinstance(x, int): raise TypeError(\\"Argument x must be of type int\\") if not isinstance(y, int): raise TypeError(\\"Argument y must be of type int\\") return x + y def greet(name=\\"World\\", punctuation=\\"!\\"): if not isinstance(name, str): raise TypeError(\\"Argument name must be of type str\\") if not isinstance(punctuation, str): raise TypeError(\\"Argument punctuation must be of type str\\") return f\\"Hello, {name}{punctuation}\\" ``` Your solution should dynamically create Python functions based on the provided input and ensure that all type validations and default values are correctly implemented. Good luck!","solution":"def generate_function_definitions(function_descriptions): def type_check_code(param_name, param_type): return f\' if not isinstance({param_name}, {param_type}):n raise TypeError(\\"Argument {param_name} must be of type {param_type}\\")n\' def generate_function_code(func): func_name = func[\'name\'] params = func[\'parameters\'] param_list = [] checks = [] for param in params: param_name = param[\'name\'] param_type = param[\'type\'] param_default = param.get(\'default\', None) if param_default is not None: if isinstance(param_default, str): param_list.append(f\'{param_name}=\\"{param_default}\\"\') else: param_list.append(f\'{param_name}={param_default}\') else: param_list.append(param_name) checks.append(type_check_code(param_name, param_type)) param_list_str = \', \'.join(param_list) function_code = f\'def {func_name}({param_list_str}):n\' function_code += \'\'.join(checks) function_code += \' # The rest of the function implementation would go here.n\' function_code += \' passn\' return function_code return \'n\'.join(generate_function_code(func) for func in function_descriptions)"},{"question":"Objective: Implement a function that connects to a given FTP server and performs a series of file operations including navigating directories, uploading files, and downloading files. This exercise will assess your understanding of using the `ftplib` module for FTP operations in Python. Description: Create a Python function `ftp_operations` that performs the following tasks: 1. Connect to an FTP server using the provided host, username, and password. 2. Navigate to a specified directory on the server. 3. List and return the files present in that directory. 4. Download a specified file from the server and save it locally. 5. Upload a specified file from the local machine to the server. Function Signature: ```python def ftp_operations(host: str, username: str, password: str, navigate_to: str, download_file: str, upload_file: str) -> list[str]: pass ``` Input: - `host` (str): The FTP server hostname. - `username` (str): The username for logging into the FTP server. - `password` (str): The password for logging into the FTP server. - `navigate_to` (str): The directory to navigate to on the server. - `download_file` (str): The name of the file to download from the server. - `upload_file` (str): The path to the local file to be uploaded to the server. Output: - A list of strings representing the names of the files in the target directory after performing the file operations. Constraints: - The provided FTP server must be accessible and support basic FTP commands. - Ensure that the program handles exceptions gracefully and provides meaningful error messages if an operation fails. Example: ```python ftp_host = \\"ftp.example.com\\" ftp_username = \\"username\\" ftp_password = \\"password\\" directory_to_navigate = \\"target_directory\\" file_to_download = \\"example.txt\\" file_to_upload = \\"local_file.txt\\" files_after_operations = ftp_operations( ftp_host, ftp_username, ftp_password, directory_to_navigate, file_to_download, file_to_upload ) print(files_after_operations) ``` This should output a list of filenames in the specified directory after downloading `example.txt` and uploading `local_file.txt`. Implementation Tips: - Use `ftplib.FTP` to handle the FTP connection and commands. - Use `cwd()` method to change directories. - Use `retrbinary()` to download the file and `storbinary()` to upload the file. - Handle any potential exceptions like connection errors, authentication failures, and file transfer errors.","solution":"import ftplib import os def ftp_operations(host: str, username: str, password: str, navigate_to: str, download_file: str, upload_file: str) -> list[str]: try: # Connect to the FTP server ftp = ftplib.FTP(host) ftp.login(user=username, passwd=password) # Navigate to the specified directory ftp.cwd(navigate_to) # List the files in the directory files_before = ftp.nlst() # Download the specified file with open(download_file, \'wb\') as local_file: ftp.retrbinary(\'RETR \' + download_file, local_file.write) # Upload the specified file with open(upload_file, \'rb\') as local_file: ftp.storbinary(\'STOR \' + os.path.basename(upload_file), local_file) # List the files in the directory after upload files_after = ftp.nlst() # Close the FTP connection ftp.quit() return files_after except ftplib.all_errors as e: print(f\\"FTP error: {e}\\") return [] # Example usage: # ftp_host = \\"ftp.example.com\\" # ftp_username = \\"username\\" # ftp_password = \\"password\\" # directory_to_navigate = \\"target_directory\\" # file_to_download = \\"example.txt\\" # file_to_upload = \\"local_file.txt\\" # # files_after_operations = ftp_operations( # ftp_host, ftp_username, ftp_password, directory_to_navigate, file_to_download, file_to_upload # ) # print(files_after_operations)"},{"question":"# **PyTorch Futures Asynchronous Operation** Objective You will implement an asynchronous operation using PyTorch\'s `torch.futures.Future` class, and handle multiple `Future` objects using the `collect_all` and `wait_all` utility functions. Description Create a function `async_tensor_operations` that accepts a list of tensors and an operation to perform asynchronously on each tensor. The function should use `torch.futures.Future` to execute the operations. After executing the operations, collect the results using `collect_all` or `wait_all`, and return the aggregated results as a list. Function Signature ```python def async_tensor_operations(tensors: List[torch.Tensor], operation: Callable[[torch.Tensor], torch.Tensor]) -> List[torch.Tensor]: pass ``` Input - `tensors`: A list of PyTorch tensors on which the operation will be performed. - `operation`: A callable function that takes a single tensor as input and returns a modified tensor. Output - A list of tensors resulting from applying the operation asynchronously on the input tensors. Constraints - The function should handle the operations asynchronously using `torch.futures.Future`. - The maximum number of tensors (`len(tensors)`) is 10. - Each tensor will have a maximum of 1000 elements. Example ```python import torch from torch.futures import Future import time def example_operation(t: torch.Tensor) -> torch.Tensor: # Simulate a time-consuming operation time.sleep(1) return t + 1 # Example usage tensors = [torch.tensor([i]) for i in range(5)] result = async_tensor_operations(tensors, example_operation) print(result) # Output should be [tensor([1]), tensor([2]), tensor([3]), tensor([4]), tensor([5])] ``` # Implementation Guidelines 1. Create a `Future` object for each tensor operation. 2. Use `collect_all` or `wait_all` to gather the Future objects and retrieve their results. 3. Ensure the final list of results is returned in the correct order.","solution":"from typing import List, Callable import torch from torch.futures import Future, collect_all def async_tensor_operations(tensors: List[torch.Tensor], operation: Callable[[torch.Tensor], torch.Tensor]) -> List[torch.Tensor]: futures = [Future() for _ in tensors] def wrap_operation(t, f): result = operation(t) f.set_result(result) # Start the asynchronous operations for tensor, future in zip(tensors, futures): torch.jit._fork(wrap_operation, tensor, future) results = collect_all(futures).wait() return [result.value() for result in results]"},{"question":"You\'ve been provided the Seaborn documentation focused on `seaborn.swarmplot`. Using your understanding of this function, write a Python script that performs the following tasks: 1. Load the `tips` dataset from Seaborn. 2. Filter the dataset to include only rows where the `total_bill` is less than 40. 3. Create a swarm plot with the following specifications: - `x` should be the `day` variable. - `y` should be the `total_bill` variable. - Use the `sex` variable as the `hue`. - Use the `size` variable to control the size of the points, making sure they don’t overlap. - Set the palette to `\\"deep\\"`. - Display the points as `x` markers with a linewidth of 1. 4. Additionally, create a faceted swarm plot using `sns.catplot` with the following specifications: - `kind=\\"swarm\\"` - `x` should be `time` - `y` should be `total_bill` - Use the `sex` variable as the `hue`. - Create columns based on the `day` variable. - Set the aspect ratio (`aspect`) to 0.5 for each facet. Your function should not take any inputs and should not return any values. However, it should display the plots as described. Here\'s the expected code structure: ```python import seaborn as sns def create_swarm_plots(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Filter the dataset tips_filtered = tips[tips[\'total_bill\'] < 40] # Create the first swarm plot sns.swarmplot(data=tips_filtered, x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", size=3, palette=\\"deep\\", marker=\\"x\\", linewidth=1) # Create the faceted swarm plot sns.catplot(data=tips_filtered, kind=\\"swarm\\", x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=0.5) # Run the function to display the plots create_swarm_plots() ``` # Constraints: 1. Ensure that the swarm points do not overlap by adjusting the `size`. 2. Set the appropriate color palette to \\"deep\\". 3. Utilize the `sns.catplot` function to create the faceted plots. # Input Format: There is no input for this function. # Output Format: The function should display the specified seaborn plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_swarm_plots(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Filter the dataset tips_filtered = tips[tips[\'total_bill\'] < 40] # Create the first swarm plot plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips_filtered, x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", size=5, palette=\\"deep\\", marker=\\"x\\", linewidth=1) plt.title(\\"Swarm Plot of Total Bill by Day and Sex\\") plt.show() # Create the faceted swarm plot g = sns.catplot(data=tips_filtered, kind=\\"swarm\\", x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=0.5, palette=\\"deep\\", marker=\\"x\\", height=6) g.fig.suptitle(\'Faceted Swarm Plot of Total Bill by Time and Sex for Each Day\', y=1.02) plt.show() # Run the function to display the plots create_swarm_plots()"},{"question":"Objective: Implement a Python function that accurately retrieves the annotations dictionary of a given object, taking into account the differences in handling annotations across Python versions 3.10 and older. Additionally, ensure that stringized annotations are properly evaluated. Problem Statement: Write a function `get_complete_annotations(obj)` that takes one parameter `obj`, which can be a function, class, or module. The function should return a dictionary of annotations for the given object. If the object contains stringized annotations, the function should convert them to their corresponding Python types. Requirements: 1. The function should handle objects from Python versions 3.10 and newer using the `inspect.get_annotations()` function. 2. For Python versions 3.9 and older: - Access annotations for functions, modules, and other callables using `getattr()`. - Access annotations for classes by looking into their `__dict__`. 3. Any stringized annotations should be un-stringized using the `eval()` function where possible, without causing errors for non-evaluable strings. 4. If an object has no annotations, the function should return an empty dictionary. 5. Maintain safety checks to ensure that annotations are of the correct type. Function Signature: ```python def get_complete_annotations(obj) -> dict: pass ``` Example Usage: ```python class Base: a: int = 3 class Derived(Base): b: \\"str\\" = \\"abc\\" def foo(x: \\"int | None\\") -> \\"str\\": pass import sys # Example Classes print(get_complete_annotations(Base)) # Output: {\'a\': int} print(get_complete_annotations(Derived)) # Output: {\'b\': str} # Example Function print(get_complete_annotations(foo)) # Output: {\'x\': Union[int, None], \'return\': str} # Example Module print(get_complete_annotations(sys)) # Output: {} # assuming sys does not have annotations ``` Constraints: - Assume the Python environment supports annotations (Python 3.0+). - Do not use any external libraries other than `inspect`. - The function should handle invalid input gracefully, returning an empty dictionary for objects without annotations or if they are not functions, classes, or modules. Notes: - Pay attention to different behaviors across Python versions. - Consider the provided documentation regarding best practices and quirks for working with `__annotations__`.","solution":"import inspect import sys def get_complete_annotations(obj): annotations = {} # For Python 3.10+ if sys.version_info >= (3, 10): try: annotations = inspect.get_annotations(obj) except Exception: pass else: # Handling for Python 3.9 and older if hasattr(obj, \'__annotations__\'): annotations = getattr(obj, \'__annotations__\', {}) elif isinstance(obj, type): # For classes annotations = getattr(obj, \'__dict__\', {}).get(\'__annotations__\', {}) # Evaluate stringized annotations evaluated_annotations = {} for key, value in annotations.items(): if isinstance(value, str): try: evaluated_value = eval(value) except Exception: evaluated_value = value # Keep as string if eval fails evaluated_annotations[key] = evaluated_value else: evaluated_annotations[key] = value return evaluated_annotations"},{"question":"# Question: You are required to implement a function that returns a formatted string displaying the differences between various time measurements taken at the start and end of a given duration. Use the `time` module to achieve this. # Function Signature: ```python def measure_time_differences(duration: float) -> str: pass ``` # Input: - `duration` (float): The time duration in seconds for which the measurements will be taken. # Output: - A string containing the differences between various time measurements formatted as follows: ``` \\"Real Time Elapsed: {real_time_elapsed} secondsnMonotonic Time Elapsed: {monotonic_time_elapsed} secondsnProcess Time Elapsed: {process_time_elapsed} secondsnPerformance Counter Time Elapsed: {perf_counter_time_elapsed} seconds\\" ``` # Constraints: - The function should accurately measure and report the time differences using the respective clocks available in the `time` module. - Ensure that the durations reported are as precise as possible. # Examples: ```python # Considered a \\"minimal time\\" execution i.e., immediate function return print(measure_time_differences(0)) # This might output values close to (showing the minimal processing delay): # \\"Real Time Elapsed: 0.0000 secondsnMonotonic Time Elapsed: 0.0000 secondsnProcess Time Elapsed: 0.0000 secondsnPerformance Counter Time Elapsed: 0.0000 seconds\\" # Sleep for 2 seconds before measuring the time differences print(measure_time_differences(2)) # This might output: # \\"Real Time Elapsed: 2.0023 secondsnMonotonic Time Elapsed: 2.0011 secondsnProcess Time Elapsed: 0.0010 secondsnPerformance Counter Time Elapsed: 2.0022 seconds\\" ``` # Hints: - Use `time()` for real time. - Use `monotonic()` for monotonic time measurements. - Use `process_time()` for CPU process time. - Use `perf_counter()` for a high-resolution performance counter. # Note: - The durations may include time required for function processing and may slightly differ based on system load and function execution time.","solution":"import time def measure_time_differences(duration: float) -> str: start_real_time = time.time() start_monotonic = time.monotonic() start_process_time = time.process_time() start_perf_counter = time.perf_counter() time.sleep(duration) end_real_time = time.time() end_monotonic = time.monotonic() end_process_time = time.process_time() end_perf_counter = time.perf_counter() real_time_elapsed = end_real_time - start_real_time monotonic_time_elapsed = end_monotonic - start_monotonic process_time_elapsed = end_process_time - start_process_time perf_counter_time_elapsed = end_perf_counter - start_perf_counter result = ( f\\"Real Time Elapsed: {real_time_elapsed:.4f} secondsn\\" f\\"Monotonic Time Elapsed: {monotonic_time_elapsed:.4f} secondsn\\" f\\"Process Time Elapsed: {process_time_elapsed:.4f} secondsn\\" f\\"Performance Counter Time Elapsed: {perf_counter_time_elapsed:.4f} seconds\\" ) return result"},{"question":"**Objective:** Implement and apply Kernel Density Estimation (KDE) to analyze a dataset. Problem Statement Given a 2D dataset, your task is to implement a function that uses Kernel Density Estimation (KDE) to estimate the density of data points and visualize the results using a contour plot. # Function Signature ```python def kde_density_estimation(data: np.ndarray, kernel: str, bandwidth: float, grid_size: int) -> None: ``` # Input Parameters - `data` (np.ndarray): A 2D NumPy array of shape (n_samples, 2) representing the dataset. Each row is a data point with two features. - `kernel` (str): The kernel to use for KDE. Valid kernel options are \'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\'. - `bandwidth` (float): The bandwidth parameter for KDE which controls the smoothness of the estimated density. - `grid_size` (int): The size of the grid for plotting the density estimate. This parameter defines the resolution of the contour plot. # Output - This function does not return any value. It should display the following plots: 1. A scatter plot of the original data points. 2. A contour plot showing the estimated density overlaid on the scatter plot. # Constraints - You must use `KernelDensity` from `sklearn.neighbors`. - Ensure that the `grid_size` value is reasonable to prevent excessive computation time. # Example Usage ```python import numpy as np # Example data data = np.array([[1, 2], [2, 3], [3, 1], [5, 4], [4, 5], [6, 2], [7, 1], [8, 3], [3, 5], [4, 4]]) # Kernel density estimation with Gaussian kernel and bandwidth of 0.5 kde_density_estimation(data, kernel=\'gaussian\', bandwidth=0.5, grid_size=100) ``` # Additional Information - Visualize the KDE results using matplotlib. - For the contour plot, create a grid of points over the data range and evaluate the KDE on this grid. # Hints - Use `np.meshgrid` to create the grid for plotting. - Use `kde.score_samples` to evaluate the log density model on the grid points.","solution":"import numpy as np from sklearn.neighbors import KernelDensity import matplotlib.pyplot as plt def kde_density_estimation(data: np.ndarray, kernel: str, bandwidth: float, grid_size: int) -> None: Estimates the density of the data points using Kernel Density Estimation (KDE) and visualizes the results with a contour plot. Parameters: data (np.ndarray): The input data points, shape (n_samples, 2). kernel (str): The kernel to use for KDE. bandwidth (float): The bandwidth parameter for KDE. grid_size (int): The size of the grid for plotting the density estimate. # Fit KDE model kde = KernelDensity(kernel=kernel, bandwidth=bandwidth) kde.fit(data) # Create grid to evaluate the KDE x_min, y_min = data.min(axis=0) - 1 x_max, y_max = data.max(axis=0) + 1 x_grid, y_grid = np.linspace(x_min, x_max, grid_size), np.linspace(y_min, y_max, grid_size) xx, yy = np.meshgrid(x_grid, y_grid) grid_samples = np.vstack([xx.ravel(), yy.ravel()]).T # Evaluate the KDE on the grid z = np.exp(kde.score_samples(grid_samples)) z = z.reshape(xx.shape) # Plot the original data points plt.scatter(data[:, 0], data[:, 1], s=20, facecolors=\'none\', edgecolors=\'k\') # Plot the estimated density as contours plt.contour(xx, yy, z, levels=10, cmap=\\"viridis\\") plt.xlabel(\'X\') plt.ylabel(\'Y\') plt.title(f\'KDE Density Estimation with {kernel} kernel and bandwidth={bandwidth}\') plt.show()"},{"question":"# Question: Implement a Simple To-Do List Application using `curses` You are required to implement a simple interactive text-based To-Do List application using the `curses` module in Python. This application should allow the user to: 1. **Add a To-Do Item**: The user can type a new to-do item that gets added to the list. 2. **Mark an Item as Done**: The user can mark an item as done, which will visually change its status. 3. **Delete an Item**: The user can delete an item from the list. 4. **Exit the Application**: The user can exit the application safely, restoring the terminal to its original state. Input and Output Requirements: - **Input**: User interactions and key presses. - **Output**: Display the To-Do list with appropriate attributes and handle user input accordingly. # Constraints: 1. **Display**: Use `curses` to display items and handle text attributes. 2. **Performance**: The application should handle up to 100 to-do items efficiently. 3. **Key Bindings**: - `a`: Add a new to-do item. - `d`: Mark the selected item as done. - `x`: Delete the selected item. - `UP` and `DOWN` arrow keys: Navigate the list. - `q`: Quit the application. # Requirements: - **Initialize and terminate the `curses` application**: Ensure the terminal is restored to its original state upon exiting. - **Create windows and handle user input**: Implement basic navigation and operations on the To-Do List. - **Text display with attributes**: Mark a done item with a specific attribute (e.g., strikethrough or reverse video). - **Handle user interactions**: Read keys and perform corresponding actions. # Example Interface: ``` ----------------------- | To-Do List | |---------------------| | [ ] Buy groceries | | [ ] Complete project| | [X] Call Mom | |---------------------| | a: Add d: Done x: Delete q: Quit | ----------------------- ``` # Implementation Tips: 1. **Initialize `curses` application**: ```python import curses from curses import wrapper def main(stdscr): stdscr.clear() # Initialization code here # ... stdscr.refresh() wrapper(main) ``` 2. **Add a new to-do item**: - Implement an input prompt to get item text. - Append item text to the list and refresh the window. 3. **Mark an item as done or delete it**: - Use keyboard input to modify item attributes. - Refresh the window after every operation. 4. **Handle navigation**: - Track the current selection and allow users to move up or down the list. 5. **Terminate safely**: - Ensure all `curses` settings are terminated correctly to restore the terminal. Happy Coding!","solution":"import curses class TodoList: def __init__(self): self.todos = [] self.current_index = 0 def add_item(self, item): self.todos.append({\'text\': item, \'done\': False}) def mark_item_as_done(self): if self.todos: self.todos[self.current_index][\'done\'] = True def delete_item(self): if self.todos: self.todos.pop(self.current_index) self.current_index = max(0, self.current_index - 1) def navigate(self, direction): if direction == \'UP\': self.current_index = max(0, self.current_index - 1) elif direction == \'DOWN\': self.current_index = min(len(self.todos) - 1, self.current_index + 1) def main(stdscr): curses.curs_set(0) stdscr.nodelay(0) todo_list = TodoList() while True: stdscr.clear() h, w = stdscr.getmaxyx() for idx, todo in enumerate(todo_list.todos): if idx == todo_list.current_index: mode = curses.A_REVERSE else: mode = curses.A_NORMAL status = \'[X]\' if todo[\'done\'] else \'[ ]\' stdscr.addstr(idx, 0, f\'{status} {todo[\\"text\\"]}\', mode) stdscr.addstr(h-1, 0, \'a: Add d: Done x: Delete UP/DOWN: Navigate q: Quit\') key = stdscr.getch() if key == ord(\'q\'): break elif key == ord(\'a\'): stdscr.addstr(h-2, 0, \'Enter to-do item: \') curses.echo() item = stdscr.getstr(h-2, 17).decode(\'utf-8\') curses.noecho() todo_list.add_item(item) elif key == ord(\'d\'): todo_list.mark_item_as_done() elif key == ord(\'x\'): todo_list.delete_item() elif key == curses.KEY_UP: todo_list.navigate(\'UP\') elif key == curses.KEY_DOWN: todo_list.navigate(\'DOWN\') stdscr.refresh() if __name__ == \'__main__\': curses.wrapper(main)"},{"question":"Objective: To demonstrate your comprehension of PyTorch\'s `torch.special` module, you are required to implement a function that computes a specific operation by utilizing multiple functions from this module. Problem Statement: Implement a function `compute_special_operations` which takes a 1-dimensional tensor `x` as input and performs the following operations: 1. Compute the Bessel function of the first kind of order 0 (`bessel_j0`) on `x`. 2. Compute the exponential of the input minus 1 (`expm1`) on `x`. 3. Compute the error function (`erf`) on `x`. 4. Compute the log of the 1 plus input (`log1p`) on `x`. The function should return a dictionary with the results of these operations. The keys of the dictionary should be \'bessel_j0\', \'expm1\', \'erf\', and \'log1p\' corresponding to each respective operation. Input: - `x` (torch.Tensor): A 1-dimensional tensor of floats. Output: - (dict): A dictionary with keys \'bessel_j0\', \'expm1\', \'erf\', \'log1p\'. The values should be tensors, each resulting from applying the respective special function on the input tensor `x`. Constraints: - The input tensor will have at most 10,000 elements. - All elements of the input will be within the range [-10, 10]. Example: ```python import torch x = torch.tensor([1.0, 2.0, 3.0]) result = compute_special_operations(x) print(result) ``` Expected Output: ``` { \'bessel_j0\': tensor([...]), \'expm1\': tensor([...]), \'erf\': tensor([...]), \'log1p\': tensor([...]) } ``` Notes: - Ensure you import the necessary functions from `torch.special`. - Make use of vectorized operations to ensure efficient computation. ```python import torch def compute_special_operations(x): # Import necessary functions from torch.special from torch.special import bessel_j0, expm1, erf, log1p # Compute the required special functions bessel_result = bessel_j0(x) expm1_result = expm1(x) erf_result = erf(x) log1p_result = log1p(x) # Return the results as a dictionary return { \'bessel_j0\': bessel_result, \'expm1\': expm1_result, \'erf\': erf_result, \'log1p\': log1p_result } ```","solution":"import torch def compute_special_operations(x): # Import necessary functions from torch.special from torch.special import bessel_j0, expm1, erf, log1p # Compute the required special functions bessel_result = bessel_j0(x) expm1_result = expm1(x) erf_result = erf(x) log1p_result = log1p(x) # Return the results as a dictionary return { \'bessel_j0\': bessel_result, \'expm1\': expm1_result, \'erf\': erf_result, \'log1p\': log1p_result }"},{"question":"# Advanced Python Testing with `unittest.mock` **Objective**: To assess the understanding and application of the `unittest.mock` module in advanced testing scenarios. **Problem Statement**: You are working on a Python project that involves a class `DataProcessor` which interacts with an external database service defined by `DatabaseService`. ```python class DatabaseService: def fetch_data(self, query: str) -> dict: # Simulate fetching data from a database pass def save_data(self, data: dict) -> None: # Simulate saving data to a database pass class DataProcessor: def __init__(self, db_service: DatabaseService): self.db_service = db_service def process(self, query: str) -> dict: # Fetch data data = self.db_service.fetch_data(query) # Perform some processing processed_data = {\\"processed\\": True, \\"original_data\\": data} # Save the processed data self.db_service.save_data(processed_data) return processed_data ``` **Task**: Write a unit test for the `DataProcessor` class using the `unittest.mock` module. Your test should: 1. Mock the `DatabaseService` class. 2. Ensure that: - The `fetch_data` method is called with the correct query. - The `save_data` method is called with the right processed data. 3. Test that the `process` method of `DataProcessor` returns the expected processed data. **Constraints**: - The query string will always be non-empty. - The `fetch_data` method returns a dictionary representing the fetched data. **Requirements**: - Use the `unittest` framework for structuring the test. - Utilize `Mock` and `patch()` for mocking dependencies. - Validate method calls and their arguments. **Example**: ```python import unittest from unittest.mock import Mock, patch from my_module import DataProcessor, DatabaseService class TestDataProcessor(unittest.TestCase): @patch(\'my_module.DatabaseService\') def test_process(self, MockDatabaseService): # Arrange mock_db_service = MockDatabaseService() mock_db_service.fetch_data.return_value = {\'key\': \'value\'} data_processor = DataProcessor(mock_db_service) # Act result = data_processor.process(\'SELECT * FROM table\') # Assert mock_db_service.fetch_data.assert_called_once_with(\'SELECT * FROM table\') expected_processed_data = {\\"processed\\": True, \\"original_data\\": {\'key\': \'value\'}} mock_db_service.save_data.assert_called_once_with(expected_processed_data) self.assertEqual(result, expected_processed_data) if __name__ == \\"__main__\\": unittest.main() ``` This example illustrates how to write the test structure and use assertions to validate the interactions and return values.","solution":"import unittest from unittest.mock import Mock, patch class DatabaseService: def fetch_data(self, query: str) -> dict: # Simulate fetching data from a database pass def save_data(self, data: dict) -> None: # Simulate saving data to a database pass class DataProcessor: def __init__(self, db_service: DatabaseService): self.db_service = db_service def process(self, query: str) -> dict: # Fetch data data = self.db_service.fetch_data(query) # Perform some processing processed_data = {\\"processed\\": True, \\"original_data\\": data} # Save the processed data self.db_service.save_data(processed_data) return processed_data"},{"question":"# Email Message Manipulation using Python\'s `email.message` Module In this exercise, you are required to demonstrate your understanding of the `EmailMessage` class from Python\'s `email.message` module by creating and manipulating email messages. Task: 1. **Create an EmailMessage**: - Write a function `create_email(subject, sender, recipient, body)` that takes the subject, sender\'s email, recipient\'s email, and the body of the email as arguments. It should return an `EmailMessage` object with these details set appropriately in the headers and the body. 2. **Add an Attachment**: - Extend the above function or write another function `add_attachment(email_message, file_content, filename, maintype=\'application\', subtype=\'octet-stream\')` which takes an `EmailMessage` object (`email_message`), the content of the file to be attached (`file_content` as bytes), and the filename of the attachment. Additionally, `maintype` and `subtype` can be provided to specify the MIME type of the attachment. This function should modify the `email_message` to include this attachment. 3. **Serialize the Message**: - Write another function `serialize_email(email_message, as_bytes=False)` that takes an `EmailMessage` object and a boolean flag `as_bytes`. If `as_bytes` is `True`, the function should return the serialized email as bytes using `as_bytes()`. Otherwise, it should return the serialized email as a string using `as_string()`. Expected Input and Output: - `create_email(subject, sender, recipient, body)`: - Input: subject (str), sender (str), recipient (str), body (str) - Output: `EmailMessage` object - `add_attachment(email_message, file_content, filename, maintype, subtype)`: - Input: email_message (EmailMessage object), file_content (bytes), filename (str), maintype (str, default=\'application\'), subtype (str, default=\'octet-stream\') - Output: Modified `EmailMessage` object with the attachment - `serialize_email(email_message, as_bytes)`: - Input: email_message (EmailMessage object), as_bytes (bool) - Output: Serialized email as bytes or string based on `as_bytes` flag Constraints and Notes: - Ensure the email message object correctly reflects the provided subject, sender, recipient, and body. - Attachments should be correctly added to the email message with appropriate headers. - Ensure the serialization is correctly performed based on the `as_bytes` flag. Use the `email.message.EmailMessage` class and its methods to complete this task. ```python from email.message import EmailMessage def create_email(subject, sender, recipient, body): # Your implementation here pass def add_attachment(email_message, file_content, filename, maintype=\'application\', subtype=\'octet-stream\'): # Your implementation here pass def serialize_email(email_message, as_bytes=False): # Your implementation here pass ```","solution":"from email.message import EmailMessage def create_email(subject, sender, recipient, body): Create an email message with the given subject, sender, recipient, and body. :param subject: Subject of the email. :param sender: Sender\'s email address. :param recipient: Recipient\'s email address. :param body: Body of the email. :return: EmailMessage object. email_message = EmailMessage() email_message[\'Subject\'] = subject email_message[\'From\'] = sender email_message[\'To\'] = recipient email_message.set_content(body) return email_message def add_attachment(email_message, file_content, filename, maintype=\'application\', subtype=\'octet-stream\'): Add an attachment to the email message. :param email_message: EmailMessage object to which attachment will be added. :param file_content: Content of the attachment as bytes. :param filename: Filename of the attachment. :param maintype: Maintype of the MIME type (default is \'application\'). :param subtype: Subtype of the MIME type (default is \'octet-stream\'). :return: EmailMessage object with the attachment added. email_message.add_attachment(file_content, maintype=maintype, subtype=subtype, filename=filename) return email_message def serialize_email(email_message, as_bytes=False): Serialize the email message to a string or bytes. :param email_message: EmailMessage object to be serialized. :param as_bytes: Flag indicating whether to serialize as bytes (True) or as string (False). :return: Serialized email message as bytes or string. if as_bytes: return email_message.as_bytes() else: return email_message.as_string()"},{"question":"**Objective:** To demonstrate your understanding of the `colorsys` module in Python, you will write a function that takes a list of RGB values, converts them to different color spaces (YIQ, HLS, and HSV), and then converts them back to RGB to verify the integrity of the conversions. Problem: Write a function `verify_color_conversions(rgb_list)` that accepts a list of RGB values. Each RGB value is a tuple of three floating-point numbers `(r, g, b)`, where `r`, `g`, and `b` are between 0 and 1. The function should perform the following steps for each RGB value: 1. Convert the RGB value to YIQ, then convert it back to RGB. 2. Convert the RGB value to HLS, then convert it back to RGB. 3. Convert the RGB value to HSV, then convert it back to RGB. 4. Compare the converted RGB values with the original RGB value to check if the conversions were lossless (within a small margin of error). The function should return a list of results, each being a dictionary with the format: ```python { \\"original_rgb\\": (r, g, b), \\"yiq_rgb\\": (r1, g1, b1), \\"hls_rgb\\": (r2, g2, b2), \\"hsv_rgb\\": (r3, g3, b3) } ``` Where `(r1, g1, b1)`, `(r2, g2, b2)`, and `(r3, g3, b3)` are the RGB values obtained after converting back from YIQ, HLS, and HSV respectively. Constraints and Considerations: - The comparison of RGB values should allow for a small margin of error due to floating-point arithmetic. Use an epsilon value of `1e-5`. - Ensure that the function handles edge cases such as empty lists and RGB values at the boundaries (0 and 1). Function Signature: ```python def verify_color_conversions(rgb_list: List[Tuple[float, float, float]]) -> List[Dict[str, Tuple[float, float, float]]]: pass ``` Example: ```python rgb_list = [(0.2, 0.4, 0.4), (0.0, 0.0, 0.0), (1.0, 1.0, 1.0)] result = verify_color_conversions(rgb_list) print(result) # Expected output format # [ # { # \\"original_rgb\\": (0.2, 0.4, 0.4), # \\"yiq_rgb\\": (0.2, 0.4, 0.4), # \\"hls_rgb\\": (0.2, 0.4, 0.4), # \\"hsv_rgb\\": (0.2, 0.4, 0.4) # }, # { # ... # }, # ... # ] ``` **Note:** The actual values for `yiq_rgb`, `hls_rgb`, and `hsv_rgb` should be close to the `original_rgb` values within the given epsilon margin.","solution":"import colorsys from typing import List, Tuple, Dict def verify_color_conversions(rgb_list: List[Tuple[float, float, float]]) -> List[Dict[str, Tuple[float, float, float]]]: results = [] epsilon = 1e-5 for rgb in rgb_list: r, g, b = rgb # Convert RGB to YIQ and back to RGB yiq = colorsys.rgb_to_yiq(r, g, b) yiq_rgb = colorsys.yiq_to_rgb(*yiq) # Convert RGB to HLS and back to RGB hls = colorsys.rgb_to_hls(r, g, b) hls_rgb = colorsys.hls_to_rgb(*hls) # Convert RGB to HSV and back to RGB hsv = colorsys.rgb_to_hsv(r, g, b) hsv_rgb = colorsys.hsv_to_rgb(*hsv) # Clamping the values to be within the range [0, 1] due to possible floating point adjustments yiq_rgb = tuple(min(1, max(0, x)) for x in yiq_rgb) hls_rgb = tuple(min(1, max(0, x)) for x in hls_rgb) hsv_rgb = tuple(min(1, max(0, x)) for x in hsv_rgb) results.append({ \\"original_rgb\\": rgb, \\"yiq_rgb\\": tuple(round(val, 5) for val in yiq_rgb), \\"hls_rgb\\": tuple(round(val, 5) for val in hls_rgb), \\"hsv_rgb\\": tuple(round(val, 5) for val in hsv_rgb) }) return results"},{"question":"**Question: Implement a Custom Message Box using `tkinter.messagebox`** **Objective**: In this task, you are required to create a Python program that uses the `tkinter.messagebox` module to implement a custom message box for a file-saving scenario in a graphical user interface (GUI). Your program should demonstrate an understanding of various types of message boxes provided by the module. **Task**: 1. **Create a function `show_custom_messagebox()`**: - The function should create a custom message box that exhibits the following behavior: - Displays a warning message if the file already exists, and asks the user if they want to replace it or cancel the operation. - If the user chooses to replace the file, show an info message saying \\"File will be replaced.\\" - If the user chooses to cancel, show a warning message saying \\"Operation canceled.\\" 2. **Input and Output**: - There are no inputs from the user for this function as it encapsulates the message box behavior. - The function does not return any values but should display the appropriate message boxes based on the user\'s choices. 3. **Constraints**: - You must use `tkinter` and `tkinter.messagebox` for implementing this functionality. - Ensure that the message boxes are modal to simulate a realistic GUI interaction. 4. **Performance**: - The program should handle the message box interactions promptly and efficiently. **Example**: ```python import tkinter as tk from tkinter import messagebox def show_custom_messagebox(): # This should be triggered in a GUI environment. root = tk.Tk() root.withdraw() # Hide the root window if messagebox.askyesno(\\"File Exists\\", \\"Do you want to replace the file?\\"): messagebox.showinfo(\\"Replace File\\", \\"File will be replaced.\\") else: messagebox.showwarning(\\"Canceled\\", \\"Operation canceled.\\") root.destroy() # Destroy the root window when done # Note: To see the message boxes in action, you must run this within a GUI-capable environment. show_custom_messagebox() ``` **Explanation**: This function first hides the main Tkinter window and then uses `tkinter.messagebox.askyesno` to ask the user if they want to replace an existing file. Depending on the user\'s response: - If they choose \\"Yes,\\" an information message box is displayed using `tkinter.messagebox.showinfo`. - If they choose \\"No,\\" a warning message box is displayed using `tkinter.messagebox.showwarning`. Finally, the main window is destroyed to close the application properly. This problem tests your understanding of handling different message boxes and user interactions using the `tkinter.messagebox` module in Python.","solution":"import tkinter as tk from tkinter import messagebox def show_custom_messagebox(): Displays a custom message box that: - Warns if the file already exists. - Asks the user whether to replace the file or cancel the operation. - Shows an info message if the file will be replaced. - Shows a warning message if the operation is canceled. root = tk.Tk() root.withdraw() # Hide the main window result = messagebox.askyesno(\\"File Exists\\", \\"Do you want to replace the file?\\") if result: messagebox.showinfo(\\"Replace File\\", \\"File will be replaced.\\") else: messagebox.showwarning(\\"Canceled\\", \\"Operation canceled.\\") root.destroy() # Destroy the root window when done"},{"question":"**Question:** You are working on a Python project and need to utilize low-level file operations using high-level Python abstractions. You are required to implement a function that handles the reading and writing of data to and from a file by interacting with file descriptors. Your task involves creating a simple library that includes the following functionalities: 1. **Open a file and return a file descriptor.** 2. **Read a specific number of bytes from the file.** 3. **Write a string to the file.** 4. **Read a line from the file.** 5. **Safely close the file.** You should implement the following functions using the provided Python C API functions: 1. `open_file(path: str, mode: str) -> int`: Opens the file at the specified path with the given mode and returns the associated file descriptor. 2. `read_bytes(fd: int, num_bytes: int) -> str`: Reads up to `num_bytes` bytes from the file associated with the given file descriptor and returns the data as a string. 3. `write_string(fd: int, data: str) -> None`: Writes the given string `data` to the file associated with the given file descriptor. 4. `read_line(fd: int) -> str`: Reads a single line from the file associated with the given file descriptor and returns it. 5. `close_file(fd: int) -> None`: Closes the file associated with the given file descriptor. # Constraints: - Do not use high-level file handling functions like `open`, `read`, or `write` directly in your implementation. - Ensure that resources are properly managed and files are closed after operations to prevent resource leaks. # Example: ```python # Assuming the code above is correctly implemented fd = open_file(\'example.txt\', \'w+\') write_string(fd, \'Hello, World!nThis is a test file.n\') read_line(fd) # Should return \'Hello, World!n\' # Safely close the file descriptor close_file(fd) ``` # Notes: - Ensure you handle possible exceptions during file operations. - The `mode` parameter in `open_file` should support common file modes like \'r\', \'w\', \'a\', \'r+\', \'w+\', etc. - Make sure you test your implementations with both text and binary files to ensure robustness.","solution":"import os def open_file(path: str, mode: str) -> int: Opens the file at the specified path with the given mode and returns the associated file descriptor. flag = { \'r\': os.O_RDONLY, \'w\': os.O_WRONLY | os.O_CREAT | os.O_TRUNC, \'a\': os.O_WRONLY | os.O_CREAT | os.O_APPEND, \'r+\': os.O_RDWR, \'w+\': os.O_RDWR | os.O_CREAT | os.O_TRUNC, \'a+\': os.O_RDWR | os.O_CREAT | os.O_APPEND }[mode] fd = os.open(path, flag, 0o666) return fd def read_bytes(fd: int, num_bytes: int) -> str: Reads up to `num_bytes` bytes from the file associated with the given file descriptor and returns the data as a string. return os.read(fd, num_bytes).decode(\'utf-8\') def write_string(fd: int, data: str) -> None: Writes the given string `data` to the file associated with the given file descriptor. os.write(fd, data.encode(\'utf-8\')) def read_line(fd: int) -> str: Reads a single line from the file associated with the given file descriptor and returns it. line = \\"\\" while True: char = os.read(fd, 1).decode(\'utf-8\') if char == \'\': break # EOF reached line += char if char == \'n\': break # End of line reached return line def close_file(fd: int) -> None: Closes the file associated with the given file descriptor. os.close(fd)"},{"question":"Custom Visualization with Seaborn You are tasked with analyzing and visualizing a dataset using Seaborn to demonstrate your proficiency with the package. Specifically, you will use a dataset of your choice from Seaborn\'s built-in datasets to create a more advanced visualization. Objective: Create a count plot that shows the counts of a categorical variable, grouped by a second variable, with customized styling and an additional statistical annotation. Requirements: 1. **Dataset**: Choose one of Seaborn\'s built-in datasets (e.g., \\"penguins\\", \\"tips\\", etc.). 2. **Categorical Variable**: Select one categorical variable from the dataset. 3. **Grouping Variable**: Group the count plot by another categorical variable. 4. **Customization**: - Customize the plot with a specific color palette. - Set an appropriate title and axis labels. - Rotate the x-axis labels if necessary for better readability. 5. **Annotation**: Add an annotation to the plot to show the count of the largest group. Input: No direct input is required. You will choose the dataset and variables from one of Seaborn\'s built-in datasets. Output: A visualization (count plot) that meets the above requirements. Example: ```python import seaborn as sns import matplotlib.pyplot as plt # Load a dataset from seaborn\'s built-in datasets data = sns.load_dataset(\\"tips\\") # Replace with your chosen dataset # Create a count plot plt.figure(figsize=(10, 6)) ax = sns.countplot(data=data, x=\\"day\\", hue=\\"smoker\\", palette=\\"Set2\\") # Replace with your chosen variables # Customize the plot ax.set_title(\\"Count of Tips per Day Grouped by Smoker Status\\") ax.set_xlabel(\\"Day\\") ax.set_ylabel(\\"Count\\") plt.xticks(rotation=45) ax.legend(title=\\"Smoker\\") # Add annotation max_count = data[\'day\'].value_counts().max() ax.annotate(f\'Max Count: {max_count}\', xy=(2, max_count), xytext=(2, max_count + 5), arrowprops=dict(facecolor=\'black\', shrink=0.05)) plt.show() ``` Follow the example structure to implement your solution with the dataset and variables of your choice. Make sure to meet all the requirements specified.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_count_plot(): # Load the \'penguins\' dataset from seaborn\'s built-in datasets data = sns.load_dataset(\\"penguins\\") # Create a count plot grouping by \'species\' and \'sex\' plt.figure(figsize=(12, 8)) ax = sns.countplot(data=data, x=\\"species\\", hue=\\"sex\\", palette=\\"pastel\\") # Customize the plot ax.set_title(\\"Count of Penguins per Species Grouped by Sex\\") ax.set_xlabel(\\"Penguin Species\\") ax.set_ylabel(\\"Count\\") plt.xticks(rotation=45) ax.legend(title=\\"Sex\\") # Add annotation max_count = data[\'species\'].value_counts().max() ax.annotate(f\'Max Count: {max_count}\', xy=(0.9, max_count), xytext=(0.9, max_count + 10), arrowprops=dict(facecolor=\'black\', shrink=0.05), fontsize=12, color=\'red\') # Show the plot plt.show()"},{"question":"# Coding Assessment: Implementing and Using a Custom Pandas Extension Array Pandas provides powerful tools to extend its core functionality by allowing the creation and manipulation of custom data types using Extension Arrays. This problem will assess your ability to create a custom extension array, register it, and perform various operations typical of pandas data workflows. Problem Statement: You need to create a custom extension array that will store complex numbers and integrate it into a pandas DataFrame. You should implement the following: 1. **ComplexNumberDtype:** A custom data type for the extension array. 2. **ComplexNumberArray:** The extension array that can store an array of complex numbers. 3. **Testing DataFrame Operations:** - Create a DataFrame using this custom extension array. - Perform and verify the following operations: - Filling missing values (`NaN`) with a specified complex number. - Sorting the DataFrame based on the complex number column. - Checking for duplicate complex numbers. Specifications: 1. **ComplexNumberDtype:** - Implement `pandas.api.extensions.ExtensionDtype`. - Set the `name` property to `\\"complex_number\\"`. - Define the `type` as `complex`. 2. **ComplexNumberArray:** - Implement `pandas.api.extensions.ExtensionArray`. - Initialize with an array of complex numbers. - Implement required methods (`__len__`, `__getitem__`, `isna`, `take`, `copy`, `fillna`, `unique`, `factorize`, `get_value`, `set_value`, `_from_factorized`, `_from_sequence`). 3. **Testing in DataFrame:** - Create a sample DataFrame using `ComplexNumberArray`. - Fill missing values with a complex number, sort the DataFrame, and check for duplicates. - For example: ```python import pandas as pd import numpy as np # Define ComplexNumberDtype class ComplexNumberDtype(pd.api.extensions.ExtensionDtype): name = \\"complex_number\\" type = complex _metadata = (\\"unit\\",) def __repr__(self): return \\"ComplexNumberDtype\\" # Define ComplexNumberArray class ComplexNumberArray(pd.api.extensions.ExtensionArray): def __init__(self, data): self.data = np.asarray(data, dtype=complex) @property def dtype(self): return ComplexNumberDtype() def __len__(self): return len(self.data) def __getitem__(self, item): return self.data[item] def isna(self): return np.isnan(self.data) def take(self, indices, allow_fill=False, fill_value=None): ... def copy(self): return ComplexNumberArray(self.data.copy()) def fillna(self, value=None, method=None, limit=None): ... def unique(self): return ComplexNumberArray(np.unique(self.data)) def factorize(self, na_sentinel=-1): ... @classmethod def _from_factorized(cls, values, original): return cls(values) @classmethod def _from_sequence(cls, sc, dtype=None, copy=False): return cls(sc) ... pd.api.extensions.register_extension_dtype(ComplexNumberDtype) pd.api.extensions.register_dataframe_accessor(\\"complex\\")( ComplexNumberArray) # Create and manipulate DataFrame data = ComplexNumberArray([1+1j, 2+3j, 3+8j, np.nan]) df = pd.DataFrame({\\"complex_numbers\\": data}) # Fill NaNs, sort and check duplicates df_filled = df.fillna(0+0j) df_sorted = df_filled.sort_values(by=\\"complex_numbers\\") duplicates = df_sorted.duplicated() print(df_sorted) print(duplicates) ``` Constraints: 1. You must use numpy to handle the array operations. 2. Handle missing values using `np.nan`. 3. Ensure that all methods are efficient with a typical O(n) complexity where applicable. Evaluation: Your implementation will be assessed based on: - Correctness of `ComplexNumberDtype` and `ComplexNumberArray` implementation. - Proper registration and integration into the pandas DataFrame. - Correct execution and results of DataFrame operations (filling, sorting, checking duplicates).","solution":"import numpy as np import pandas as pd from pandas.api.extensions import ExtensionDtype, ExtensionArray # Define Custom ComplexNumberDtype class ComplexNumberDtype(ExtensionDtype): name = \\"complex_number\\" type = complex kind = \'O\' na_value = np.nan @classmethod def construct_array_type(cls): return ComplexNumberArray # Define Custom ComplexNumberArray class ComplexNumberArray(ExtensionArray): def __init__(self, data): self.data = np.asarray(data, dtype=complex) @property def dtype(self): return ComplexNumberDtype() def __len__(self): return len(self.data) def __getitem__(self, item): if isinstance(item, int): return self.data[item] elif isinstance(item, slice): return type(self)(self.data[item]) else: raise IndexError(\\"Invalid index\\") def isna(self): return np.isnan(self.data) def take(self, indices, allow_fill=False, fill_value=None): if fill_value is None: fill_value = self.dtype.na_value fill_value = complex(fill_value) result = self.data.take(indices) if allow_fill: mask = indices == -1 result[mask] = fill_value return type(self)(result) def copy(self): return type(self)(self.data.copy()) def fillna(self, value=None, method=None, limit=None): if value is None: raise ValueError(\\"Must specify a value parameter for fillna\\") mask = self.isna() if not mask.any(): return self if np.isscalar(value): value = complex(value) result = self.data.copy() result[mask] = value else: raise ValueError(\\"fillna only supports scalar value inputs\\") return type(self)(result) def unique(self): _, index = np.unique(self.data, return_index=True) return type(self)(self.data[np.sort(index)]) def factorize(self, na_sentinel=-1): unique_vals, labels = np.unique(self.data, return_inverse=True) unique_vals = type(self)(unique_vals) return labels, unique_vals @classmethod def _from_factorized(cls, values, original): return cls(values) @classmethod def _from_sequence(cls, scalars, dtype=None, copy=False): return cls(scalars) # Register the dtype pd.api.extensions.register_extension_dtype(ComplexNumberDtype) # Example of creating and manipulating DataFrame data = ComplexNumberArray([1+1j, 2+3j, 3+8j, np.nan]) df = pd.DataFrame({\\"complex_numbers\\": data}) # Fill NaNs, sort and check duplicates df_filled = df.fillna(0+0j) df_sorted = df_filled.sort_values(by=\\"complex_numbers\\") duplicates = df_sorted.duplicated() print(df_sorted) print(duplicates)"},{"question":"Objective The objective of this question is to assess your understanding and ability to work with real-world datasets in scikit-learn, as well as your data manipulation skills. Question Description Write a Python function `fetch_and_explore_dataset` that: 1. Loads the `fetch_20newsgroups` dataset. 2. Processes the dataset to find the one with the largest number of occurrences of a specified keyword. 3. Returns the ID of the newsgroup with the largest number of related articles. Function Signature ```python def fetch_and_explore_dataset(keyword: str) -> int: ``` Input - `keyword` (str): A keyword to search for in the dataset articles. Output - Returns an integer representing the ID of the newsgroup that has the largest number of articles containing the keyword. Constraints - The `fetch_20newsgroups` function from `sklearn.datasets` should be used. - The search should be case-insensitive. - Articles should be considered only if they contain the whole keyword (i.e., not as part of another word). Example ```python # Example usage fetch_and_explore_dataset(\\"AI\\") ``` This function should load the dataset, process it, and determine which newsgroup contains the most articles about \\"AI\\". Hints You may find the following example code snippet helpful to start with: ```python from sklearn.datasets import fetch_20newsgroups data = fetch_20newsgroups(subset=\'all\') texts, newsgroups = data.data, data.target # Example process: identify texts containing \'AI\' in a case-insensitive manner keyword = \'AI\' keyword_count = [text.lower().count(keyword.lower()) for text in texts] # Find the newsgroup with the most mentions of \'AI\' ``` Use this as a base to build the logic required for counting keyword occurrences and identifying the relevant newsgroup.","solution":"from sklearn.datasets import fetch_20newsgroups def fetch_and_explore_dataset(keyword: str) -> int: Loads the `fetch_20newsgroups` dataset and finds the newsgroup with the largest number of occurrences of the specified keyword in its articles. Args: keyword (str): A keyword to search for in the dataset articles. Returns: int: ID of the newsgroup with the largest number of articles containing the keyword. data = fetch_20newsgroups(subset=\'all\') texts, newsgroups = data.data, data.target keyword = keyword.lower() # Count occurrences of the keyword in each article newsgroup_keyword_counts = [0] * len(data.target_names) for i, text in enumerate(texts): if keyword in text.lower(): newsgroup_keyword_counts[newsgroups[i]] += 1 # Find the newsgroup with the maximum number of keyword occurrences max_newsgroup_id = newsgroup_keyword_counts.index(max(newsgroup_keyword_counts)) return max_newsgroup_id"},{"question":"Objective Implement a function to manage multiple distributed subprocesses using PyTorch\'s `SubprocessHandler`. This function should demonstrate your understanding of creating and handling multiple subprocesses in a distributed computing environment. Task 1. Write a function `manage_subprocesses` which takes the following parameters: - `num_subprocesses` (int): The number of subprocesses to launch. - `command` (str): The command to be executed by each subprocess. 2. The function should: - Use the `SubprocessHandler` from `torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler`. - Launch `num_subprocesses` instances of the given `command`. - Handle the lifecycle of these subprocesses, ensuring they are properly initiated and terminated. - Collect and return the exit codes of each subprocess in a list. Input Format - `num_subprocesses` (int): An integer specifying the number of subprocesses to start. - `command` (str): A string representing the command to execute in each subprocess. Output Format - Return a list of integers where each integer is the exit code of a subprocess. Constraints - `num_subprocesses` is a positive integer (1 <= `num_subprocesses` <= 10). - The `command` is a valid shell command that can be executed in subprocesses. Performance Requirements - The implementation should efficiently manage the subprocesses and handle any potential resource contention. Example ```python def manage_subprocesses(num_subprocesses: int, command: str) -> list: # Implementation here # Example Input num_subprocesses = 3 command = \\"echo hello\\" # Example Output [0, 0, 0] ``` Additional Information You may refer to the PyTorch documentation for `SubprocessHandler` for detailed information on managing subprocesses: `torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler.SubprocessHandler`. Notes - Assume the necessary PyTorch modules are already installed. - Provide proper error handling for subprocess failures. - Import any other necessary Python standard libraries to complete the task.","solution":"import subprocess from typing import List def manage_subprocesses(num_subprocesses: int, command: str) -> List[int]: Launches a specified number of subprocesses to run a given command and returns their exit codes. Args: - num_subprocesses (int): The number of subprocesses to launch. - command (str): The command to be executed by each subprocess. Returns: - List[int]: A list of exit codes for each subprocess. processes = [] for _ in range(num_subprocesses): process = subprocess.Popen(command, shell=True) processes.append(process) exit_codes = [] for process in processes: process.communicate() # Wait for the process to complete exit_codes.append(process.returncode) return exit_codes"},{"question":"# PyTorch Coding Challenge: Tensor Size Manipulation Objective In this task, you will demonstrate your understanding of the `torch.Size` class in PyTorch by performing various operations related to tensor sizes. You will implement a function that handles the following: 1. Verifies if two tensors have the same shape. 2. Returns the larger dimension for each corresponding dimension of two tensors. 3. Checks if a tensor\'s dimensions match given specific criteria. Function Signature ```python import torch def tensor_size_operations(tensor_a: torch.Tensor, tensor_b: torch.Tensor, criteria: list) -> dict: Perform various operations on the sizes of given tensors and return the results in a dictionary. Args: - tensor_a (torch.Tensor): The first input tensor. - tensor_b (torch.Tensor): The second input tensor. - criteria (list): A list of integers representing the specific criteria for dimensions. Returns: - dict: A dictionary containing the following keys and their corresponding values: - \'same_shape\' (bool): True if both tensors have the same shape, False otherwise. - \'max_shape\' (torch.Size): A torch.Size object representing the maximum dimensions for each corresponding dimension of the two tensors. - \'criteria_match\' (bool): True if tensor_a\'s dimensions match the criteria, False otherwise. pass ``` Instructions 1. **Same Shape Verification**: Check if `tensor_a` and `tensor_b` have the same shape. If they do, return `True` for the key `\'same_shape\'` in the result dictionary, otherwise return `False`. 2. **Max Shape Calculation**: Calculate the maximum dimension size for each corresponding dimension of `tensor_a` and `tensor_b`. Return this as a `torch.Size` object for the key `\'max_shape\'` in the result dictionary. 3. **Criteria Match**: Check if the size of `tensor_a` matches the specified criteria. Each element in the `criteria` list will correspond to the acceptable size for a dimension. If `tensor_a` matches these criteria, return `True` for the key `\'criteria_match\'` in the result dictionary, otherwise return `False`. Example ```python import torch tensor_a = torch.ones(3, 4, 5) tensor_b = torch.ones(3, 4, 6) criteria = [3, 4, 5] print(tensor_size_operations(tensor_a, tensor_b, criteria)) # Expected output: # { # \'same_shape\': False, # \'max_shape\': torch.Size([3, 4, 6]), # \'criteria_match\': True # } ``` Constraints - You may assume that `tensor_a` and `tensor_b` have the same number of dimensions. - The `criteria` list will have the same length as the number of dimensions of `tensor_a` and `tensor_b`. Note - Be sure to use the `torch.Size` class for shape-related operations.","solution":"import torch def tensor_size_operations(tensor_a: torch.Tensor, tensor_b: torch.Tensor, criteria: list) -> dict: Perform various operations on the sizes of given tensors and return the results in a dictionary. Args: - tensor_a (torch.Tensor): The first input tensor. - tensor_b (torch.Tensor): The second input tensor. - criteria (list): A list of integers representing the specific criteria for dimensions. Returns: - dict: A dictionary containing the following keys and their corresponding values: - \'same_shape\' (bool): True if both tensors have the same shape, False otherwise. - \'max_shape\' (torch.Size): A torch.Size object representing the maximum dimensions for each corresponding dimension of the two tensors. - \'criteria_match\' (bool): True if tensor_a\'s dimensions match the criteria, False otherwise. same_shape = tensor_a.size() == tensor_b.size() max_shape = torch.Size([max(dim_a, dim_b) for dim_a, dim_b in zip(tensor_a.size(), tensor_b.size())]) criteria_match = tensor_a.size() == torch.Size(criteria) return { \'same_shape\': same_shape, \'max_shape\': max_shape, \'criteria_match\': criteria_match, }"},{"question":"**Kernel Approximation with Nystroem Method** You are given a dataset `X` and corresponding labels `y`. Your task is to implement the following steps: 1. Apply exact Radial Basis Function (RBF) kernel to the dataset and train a Support Vector Machine (SVM) classifier. 2. Apply the Nystroem method for kernel approximation on the same dataset and train a linear SVM classifier. 3. Compare the performance of the two classifiers in terms of accuracy. # Input - `X`: A numpy array of shape `(n_samples, n_features)` representing the dataset. - `y`: A numpy array of shape `(n_samples,)` representing the labels. # Output - A dictionary with the following keys: - `\'svm_exact_kernel_accuracy\'`: Accuracy of SVM with exact RBF kernel. - `\'nystroem_approx_kernel_accuracy\'`: Accuracy of linear SVM with Nystroem kernel approximation. # Constraints - Use `sklearn.svm.SVC` for the exact RBF kernel implementation. - Use `sklearn.kernel_approximation.Nystroem` and `sklearn.linear_model.SGDClassifier` for the kernel approximation implementation. - Set `random_state` to 1 in `Nystroem` for consistency. # Example ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.svm import SVC from sklearn.linear_model import SGDClassifier from sklearn.kernel_approximation import Nystroem def compare_kernel_methods(X, y): X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Exact RBF kernel rbf_svm = make_pipeline(StandardScaler(), SVC(kernel=\'rbf\', gamma=0.1, random_state=1)) rbf_svm.fit(X_train, y_train) exact_rbf_accuracy = rbf_svm.score(X_test, y_test) # Nystroem method feature_map_nystroem = Nystroem(gamma=0.1, n_components=300, random_state=1) linear_svm = make_pipeline(StandardScaler(), SGDClassifier(max_iter=1000, tol=1e-3, random_state=1)) linear_svm.fit(feature_map_nystroem.fit_transform(X_train), y_train) approx_rbf_accuracy = linear_svm.score(feature_map_nystroem.transform(X_test), y_test) return { \'svm_exact_kernel_accuracy\': exact_rbf_accuracy, \'nystroem_approx_kernel_accuracy\': approx_rbf_accuracy } # Example usage X, y = load_iris(return_X_y=True) print(compare_kernel_methods(X, y)) ``` In this example, you will be testing both exact RBF kernel SVM and the Nystroem approximation method on the Iris dataset. You can adapt this to other datasets as necessary.","solution":"from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.svm import SVC from sklearn.linear_model import SGDClassifier from sklearn.kernel_approximation import Nystroem def compare_kernel_methods(X, y): # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Exact RBF kernel rbf_svm = make_pipeline(StandardScaler(), SVC(kernel=\'rbf\', gamma=0.1, random_state=1)) rbf_svm.fit(X_train, y_train) exact_rbf_accuracy = rbf_svm.score(X_test, y_test) # Nystroem method feature_map_nystroem = Nystroem(gamma=0.1, n_components=300, random_state=1) X_train_transformed = feature_map_nystroem.fit_transform(X_train) X_test_transformed = feature_map_nystroem.transform(X_test) linear_svm = make_pipeline(StandardScaler(), SGDClassifier(max_iter=1000, tol=1e-3, random_state=1)) linear_svm.fit(X_train_transformed, y_train) approx_rbf_accuracy = linear_svm.score(X_test_transformed, y_test) return { \'svm_exact_kernel_accuracy\': exact_rbf_accuracy, \'nystroem_approx_kernel_accuracy\': approx_rbf_accuracy }"},{"question":"**Assessing Understanding of Cross Decomposition with scikit-learn** # Problem Statement: You are tasked with using the `PLSRegression` algorithm from the `sklearn.cross_decomposition` module to build a regression model. Your goal is to fit the model on a given dataset, predict the target values, and evaluate the performance of the model. Follow the steps below to accomplish this: # Dataset: You are provided with a dataset `data.csv` which contains predictors and targets. The first `n` columns are the predictors (X) and the last column is the target (Y). # Requirements: 1. Load the dataset and split it into predictors (X) and target (Y). 2. Fit a `PLSRegression` model on the dataset with a specified number of components, `n_components`. 3. Predict the target values using the fitted model. 4. Evaluate your model using the Mean Squared Error (MSE) between the actual and predicted target values. 5. Transform the predictors using the `PLSRegression` model and print the resulting transformed predictors. # Function Signature: ```python def pls_regression_pipeline(file_path: str, n_components: int) -> float: Perform PLS regression on the dataset and return the mean squared error. Parameters: file_path (str): The path to the CSV file containing the dataset. n_components (int): The number of PLS components to use. Returns: float: The Mean Squared Error (MSE) between the actual and predicted target values. pass ``` # Input: - `file_path`: A string representing the path to the dataset file (e.g., \'data.csv\'). - `n_components`: An integer representing the number of PLS components to use. # Output: - Return a float representing the Mean Squared Error (MSE) between the actual and predicted target values. # Constraints: - Ensure to handle cases where the dataset might have missing values. - Handle cases where the number of components is greater than the number of features or samples. - The function should be efficient and handle reasonably large datasets. # Example: ```python file_path = \'data.csv\' n_components = 2 mse = pls_regression_pipeline(file_path, n_components) print(f\\"Mean Squared Error: {mse}\\") ``` # Notes: - You can use `pd.read_csv` to load the dataset. - You might want to use `train_test_split` to split data for training and testing. - From `sklearn.cross_decomposition`, use `PLSRegression` for modeling. - For evaluation, use `mean_squared_error` from `sklearn.metrics`. **Hints:** - Explore the attributes `x_rotations_`, `y_rotations_`, and `coef_` of the `PLSRegression` model to fully understand the model transformations and predictions.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import mean_squared_error def pls_regression_pipeline(file_path: str, n_components: int) -> float: Perform PLS regression on the dataset and return the mean squared error. Parameters: file_path (str): The path to the CSV file containing the dataset. n_components (int): The number of PLS components to use. Returns: float: The Mean Squared Error (MSE) between the actual and predicted target values. # Load the dataset data = pd.read_csv(file_path) # Split the dataset into predictors (X) and target (Y) X = data.iloc[:, :-1].values Y = data.iloc[:, -1].values # Split the data into training and testing sets X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42) # Handle cases where the number of components is greater than the number of features or samples n_components = min(n_components, X_train.shape[1], X_train.shape[0] - 1) # Instantiate and fit the PLSRegression model pls = PLSRegression(n_components=n_components) pls.fit(X_train, Y_train) # Predict the target values Y_pred = pls.predict(X_test) # Evaluate the model using Mean Squared Error (MSE) mse = mean_squared_error(Y_test, Y_pred) # Transform the predictors X_transformed = pls.transform(X) print(\\"Transformed Predictors:\\", X_transformed) return mse"},{"question":"# Outlier and Novelty Detection with Scikit-Learn Objective Your task is to implement and compare outlier detection and novelty detection using various scikit-learn tools. You will work with a synthetic dataset to demonstrate your understanding of these concepts, as well as the functionality provided by scikit-learn. Instructions 1. **Dataset Creation:** - Create a synthetic dataset with `n_samples=300`, `n_features=2`, and add some `outliers`. - Use the `make_blobs` function from `sklearn.datasets` to generate the data. 2. **Outlier Detection:** - Implement outlier detection using `LocalOutlierFactor`, `IsolationForest`, and `EllipticEnvelope`. - For each method, fit the model and use it to predict outliers in the dataset. - Calculate the proportion of identified outliers and print the results. 3. **Novelty Detection:** - Divide the original dataset into training and test sets. - Implement novelty detection using `LocalOutlierFactor` (with `novelty=True`) and `OneClassSVM`. - Fit the model on the training set and use it to predict novelties in the test set. - Calculate the proportion of identified novelties and print the results. 4. **Visualization:** - Create scatter plots to visualize the inliers and outliers/novelties in the dataset for each algorithm used. 5. **Performance Analysis:** - Compare the performance of different detectors by interpreting the visualization results and calculating ROC-AUC score for novelty detectors on the test set. Detailed Steps 1. **Dataset Creation:** ```python from sklearn.datasets import make_blobs import numpy as np X, _ = make_blobs(n_samples=300, centers=3, n_features=2, random_state=42) rng = np.random.RandomState(42) X_outliers = rng.uniform(low=-10, high=10, size=(20, 2)) X = np.concatenate([X, X_outliers], axis=0) ``` 2. **Outlier Detection:** - `LocalOutlierFactor` - `IsolationForest` - `EllipticEnvelope` ```python from sklearn.neighbors import LocalOutlierFactor from sklearn.ensemble import IsolationForest from sklearn.covariance import EllipticEnvelope lof = LocalOutlierFactor(n_neighbors=20) iso = IsolationForest(max_samples=300, random_state=42) ell = EllipticEnvelope() pred_lof = lof.fit_predict(X) pred_iso = iso.fit_predict(X) pred_ell = ell.fit_predict(X) outliers_lof = np.sum(pred_lof == -1) outliers_iso = np.sum(pred_iso == -1) outliers_ell = np.sum(pred_ell == -1) print(\\"Outliers detected by LOF: \\", outliers_lof) print(\\"Outliers detected by IsolationForest: \\", outliers_iso) print(\\"Outliers detected by EllipticEnvelope: \\", outliers_ell) ``` 3. **Novelty Detection:** - Train-test split - `LocalOutlierFactor` (with `novelty=True`) - `OneClassSVM` ```python from sklearn.model_selection import train_test_split from sklearn.svm import OneClassSVM X_train, X_test = train_test_split(X, test_size=0.25, random_state=42) lof_novel = LocalOutlierFactor(novelty=True) oneclass = OneClassSVM(gamma=\'auto\') lof_novel.fit(X_train) oneclass.fit(X_train) pred_lof_novel = lof_novel.predict(X_test) pred_oneclass = oneclass.predict(X_test) novelties_lof = np.sum(pred_lof_novel == -1) novelties_oneclass = np.sum(pred_oneclass == -1) print(\\"Novelties detected by LOF: \\", novelties_lof) print(\\"Novelties detected by OneClassSVM: \\", novelties_oneclass) ``` 4. **Visualization:** - Scatter plots showing data points with inliers and outliers/novelties. ```python import matplotlib.pyplot as plt def plot_results(X, y_pred, title): plt.scatter(X[:, 0], X[:, 1], c=y_pred, edgecolor=\'k\', s=20) plt.title(title) plt.show() plot_results(X, pred_lof, \\"Outliers by LOF\\") plot_results(X, pred_iso, \\"Outliers by IsolationForest\\") plot_results(X, pred_ell, \\"Outliers by EllipticEnvelope\\") plot_results(X_test, pred_lof_novel, \\"Novelties by LOF\\") plot_results(X_test, pred_oneclass, \\"Novelties by OneClassSVM\\") ``` 5. **Performance Analysis:** - Calculate and print ROC-AUC scores for novelty detection methods. ```python from sklearn.metrics import roc_auc_score roc_auc_lof = roc_auc_score((pred_lof_novel == -1).astype(int), lof_novel.decision_function(X_test)) roc_auc_oneclass = roc_auc_score((pred_oneclass == -1).astype(int), oneclass.decision_function(X_test)) print(\\"ROC-AUC score for LOF: \\", roc_auc_lof) print(\\"ROC-AUC score for OneClassSVM: \\", roc_auc_oneclass) ``` Output Provide the following: - Proportion of detected outliers for each method (LOF, IsolationForest, EllipticEnvelope). - Proportion of detected novelties for each method (LOF, OneClassSVM). - Scatter plots showing the inliers and outliers/novelties for each method. - ROC-AUC scores for novelty detection methods.","solution":"import numpy as np from sklearn.datasets import make_blobs from sklearn.ensemble import IsolationForest from sklearn.covariance import EllipticEnvelope from sklearn.neighbors import LocalOutlierFactor from sklearn.model_selection import train_test_split from sklearn.svm import OneClassSVM from sklearn.metrics import roc_auc_score import matplotlib.pyplot as plt # Create synthetic dataset with outliers def create_dataset(): X, _ = make_blobs(n_samples=300, centers=3, n_features=2, random_state=42) rng = np.random.RandomState(42) X_outliers = rng.uniform(low=-10, high=10, size=(20, 2)) X = np.concatenate([X, X_outliers], axis=0) return X # Outlier Detection def detect_outliers(X, method): if method == \'LOF\': clf = LocalOutlierFactor(n_neighbors=20) y_pred = clf.fit_predict(X) elif method == \'ISO\': clf = IsolationForest(max_samples=300, random_state=42) y_pred = clf.fit_predict(X) elif method == \'ELL\': clf = EllipticEnvelope() y_pred = clf.fit_predict(X) outliers = np.sum(y_pred == -1) return outliers, y_pred # Novelty Detection def detect_novelties(X_train, X_test, method): if method == \'LOF\': clf = LocalOutlierFactor(novelty=True) elif method == \'SVM\': clf = OneClassSVM(gamma=\'auto\') clf.fit(X_train) y_pred_test = clf.predict(X_test) novelties = np.sum(y_pred_test == -1) return novelties, y_pred_test, clf # Plot results def plot_results(X, y_pred, title): plt.scatter(X[:, 0], X[:, 1], c=y_pred, edgecolor=\'k\', s=20) plt.title(title) plt.show() # Main workflow X = create_dataset() # Outlier Detection outliers_lof, pred_lof = detect_outliers(X, \'LOF\') outliers_iso, pred_iso = detect_outliers(X, \'ISO\') outliers_ell, pred_ell = detect_outliers(X, \'ELL\') print(\\"Outliers detected by LOF: \\", outliers_lof) print(\\"Outliers detected by IsolationForest: \\", outliers_iso) print(\\"Outliers detected by EllipticEnvelope: \\", outliers_ell) # Novelty Detection X_train, X_test = train_test_split(X, test_size=0.25, random_state=42) novelties_lof, pred_lof_novel, clf_lof = detect_novelties(X_train, X_test, \'LOF\') novelties_svm, pred_oneclass, clf_svm = detect_novelties(X_train, X_test, \'SVM\') print(\\"Novelties detected by LOF: \\", novelties_lof) print(\\"Novelties detected by OneClassSVM: \\", novelties_svm) # Visualization plot_results(X, pred_lof, \\"Outliers by LOF\\") plot_results(X, pred_iso, \\"Outliers by IsolationForest\\") plot_results(X, pred_ell, \\"Outliers by EllipticEnvelope\\") plot_results(X_test, pred_lof_novel, \\"Novelties by LOF\\") plot_results(X_test, pred_oneclass, \\"Novelties by OneClassSVM\\") # Performance Analysis roc_auc_lof = roc_auc_score((pred_lof_novel == -1).astype(int), clf_lof.decision_function(X_test)) roc_auc_svm = roc_auc_score((pred_oneclass == -1).astype(int), clf_svm.decision_function(X_test)) print(\\"ROC-AUC score for LOF: \\", roc_auc_lof) print(\\"ROC-AUC score for OneClassSVM: \\", roc_auc_svm)"},{"question":"# Python Object Annotations Processor **Objective:** Implement a function `process_annotations(obj)` that retrieves and processes the annotations of a given Python object according to the Python version, and returns a dictionary of these annotations. The function should safely handle classes, functions, modules, and other callable types, differentiating between them as necessary. **Function Signature:** ```python def process_annotations(obj: Any) -> dict: pass ``` # Description: 1. **Input:** - `obj`: Any Python object that may have annotations (e.g., functions, classes, modules). 2. **Output:** - A dictionary containing the annotations of the input object. If no annotations are found, return an empty dictionary. 3. **Requirements:** - Use the `inspect.get_annotations()` function if possible (i.e., in Python 3.10 and newer). - For Python 3.9 and older: - Use appropriate best practices as outlined in the documentation. - Safely handle cases where classes inherit annotations from parent classes. - The annotations dict should include un-stringized annotations where applicable. - Ensure compatibility and correctness across various versions by conditional implementation based on the Python version. # Example: ```python class Base: a: int b: str class Derived(Base): c: float # In Python 3.10 and newer print(process_annotations(Derived)) # Outputs: {\'c\': float} # In Python 3.9 and older print(process_annotations(Derived)) # Outputs: {\'a\': int, \'b\': str, \'c\': float} def foo(x: int, y: \\"str\\") -> float: pass # Un-stringized output print(process_annotations(foo)) # Outputs: {\'x\': int, \'y\': str, \'return\': float} ``` **Constraints:** - Assume all annotations, if present, are valid. - Follow the best practices as outlined in the provided documentation. **Notes:** - The implementation must ensure compatibility with various Python versions. - Consider edge cases such as missing annotations, empty annotations, or inherited annotations.","solution":"import sys import inspect from typing import Any, get_type_hints def process_annotations(obj: Any) -> dict: Retrieve and process the annotations of a given Python object. Args: - obj: Any Python object (e.g., functions, classes, modules). Returns: - A dictionary containing the annotations of the input object. Returns an empty dictionary if no annotations are found. if sys.version_info >= (3, 10): # For Python 3.10 and newer try: return inspect.get_annotations(obj, eval_str=True) except Exception: return {} else: # For Python 3.9 and older try: return get_type_hints(obj) except Exception: return {}"},{"question":"**Question:** You have been provided with access to a Unix-based system, and your task is to use the `spwd` module to perform specific operations on the shadow password database. You need to write a Python function that will: 1. Retrieve the shadow password database entry for a given user name. 2. Calculate and return the number of users who have not changed their password in the last `N` days. 3. Find the user whose password was last changed the longest time ago. To achieve this, implement the following function: ```python import spwd from typing import Tuple, List def analyze_shadow_password(n: int, username: str) -> Tuple[dict, int, dict]: Analyze the Unix shadow password database. Parameters: n (int): The number of days. username (str): The user name whose shadow password entry needs to be fetched. Returns: Tuple[dict, int, dict]: A tuple containing: - A dictionary representing the shadow password database entry for the given user name. - An integer representing the number of users who have not changed their password in the last `n` days. - A dictionary representing the shadow password database entry for the user whose password was last changed the longest time ago. pass ``` **Input:** - `n` is an integer representing the number of days. - `username` is a string representing the user name. **Output:** - A tuple containing: - A dictionary with attributes corresponding to the shadow password database entry for the given user name. - An integer representing the number of users who have not changed their password in the last `n` days. - A dictionary with attributes corresponding to the shadow password database entry for the user whose password was last changed the longest time ago. **Constraints:** - You must have sufficient privileges to access the shadow password database (run your script with appropriate user privileges). - The function must handle the case where the user name does not exist or privileges are insufficient. **Example:** ```python # Example input result = analyze_shadow_password(30, \'john\') # Expected output format (result[0], result[1], result[2]) == ( { \\"sp_namp\\": \\"john\\", \\"sp_pwdp\\": \\"6abc....\\", \\"sp_lstchg\\": 18285, \\"sp_min\\": 0, \\"sp_max\\": 99999, \\"sp_warn\\": 7, \\"sp_inact\\": -1, \\"sp_expire\\": -1, \\"sp_flag\\": 0 }, 5, { \\"sp_namp\\": \\"alice\\", \\"sp_pwdp\\": \\"6xyz....\\", \\"sp_lstchg\\": 18000, \\"sp_min\\": 0, \\"sp_max\\": 99999, \\"sp_warn\\": 7, \\"sp_inact\\": -1, \\"sp_expire\\": -1, \\"sp_flag\\": 0 } ) ``` Notes: - The returned entries are represented as dictionaries for dict-like access to attributes. - The actual values, especially for encrypted passwords and dates, will vary based on the system you run this on and the state of its shadow password database.","solution":"import spwd import time from typing import Tuple, Dict def analyze_shadow_password(n: int, username: str) -> Tuple[Dict, int, Dict]: Analyze the Unix shadow password database. Parameters: n (int): The number of days. username (str): The user name whose shadow password entry needs to be fetched. Returns: Tuple[Dict, int, Dict]: A tuple containing: - A dictionary representing the shadow password database entry for the given user name. - An integer representing the number of users who have not changed their password in the last `n` days. - A dictionary representing the shadow password database entry for the user whose password was last changed the longest time ago. current_day = int(time.time() // (24 * 3600)) # Fetch all entries in the shadow password database shadow_entries = spwd.getspall() # Initial values to store results user_entry = next((entry for entry in shadow_entries if entry.sp_namp == username), None) if user_entry: user_entry_dict = { \\"sp_namp\\": user_entry.sp_namp, \\"sp_pwdp\\": user_entry.sp_pwdp, \\"sp_lstchg\\": user_entry.sp_lstchg, \\"sp_min\\": user_entry.sp_min, \\"sp_max\\": user_entry.sp_max, \\"sp_warn\\": user_entry.sp_warn, \\"sp_inact\\": user_entry.sp_inact, \\"sp_expire\\": user_entry.sp_expire, \\"sp_flag\\": user_entry.sp_flag } else: user_entry_dict = {\\"error\\": \\"User not found\\"} not_changed_count = sum(1 for entry in shadow_entries if (current_day - entry.sp_lstchg) > n) oldest_entry = min(shadow_entries, key=lambda x: x.sp_lstchg) oldest_entry_dict = { \\"sp_namp\\": oldest_entry.sp_namp, \\"sp_pwdp\\": oldest_entry.sp_pwdp, \\"sp_lstchg\\": oldest_entry.sp_lstchg, \\"sp_min\\": oldest_entry.sp_min, \\"sp_max\\": oldest_entry.sp_max, \\"sp_warn\\": oldest_entry.sp_warn, \\"sp_inact\\": oldest_entry.sp_inact, \\"sp_expire\\": oldest_entry.sp_expire, \\"sp_flag\\": oldest_entry.sp_flag } return (user_entry_dict, not_changed_count, oldest_entry_dict)"},{"question":"**Question: Implement a Custom Function Utilizing PyTorch for HIP** **Problem Statement:** You are tasked to implement a function using PyTorch that: 1. Determines the availability of CUDA/HIP devices. 2. Allocates tensors on the specified GPU device. 3. Performs addition of two tensors located on the same device. 4. Ensures memory management by clearing unused cached memory. **Function Signature:** ```python import torch def hip_tensor_operations(device_index: int, tensor_data1: list, tensor_data2: list) -> torch.Tensor: Perform tensor operations on the specified GPU device using PyTorch with HIP/CUDA. Parameters: - device_index (int): The index of the GPU device to use (0-indexed). - tensor_data1 (list): The data for the first tensor (as a list of floats). - tensor_data2 (list): The data for the second tensor (as a list of floats). Returns: - torch.Tensor: The resulting tensor after the addition operation, allocated on the specified device. Raises: - RuntimeError: If the specified device is not available. ``` **Detailed Requirements:** 1. **Device Check and Selection**: - Check if GPU support is available. - If it is available, determine whether the environment is leveraging HIP or CUDA. - Raise a `RuntimeError` if the specified device index is not available. 2. **Tensor Allocation and Operations**: - Use the specified device index to create two tensors from the given data lists (`tensor_data1` and `tensor_data2`). - Perform the addition of these two tensors and ensure the operation occurs on the same device. 3. **Memory Management**: - Clear all unused cached memory on the GPU after performing the operations. **Constraints:** - Assume `tensor_data1` and `tensor_data2` will always be of the same length. - Raise appropriate errors if the environment conditions or inputs are invalid. **Example Usage:** ```python # Example inputs device_index = 0 tensor_data1 = [1.0, 2.0, 3.0] tensor_data2 = [4.0, 5.0, 6.0] # Function call result_tensor = hip_tensor_operations(device_index, tensor_data1, tensor_data2) # Expected output (Tensor on GPU device 0) print(result_tensor) # Output: tensor([5., 7., 9.], device=\'cuda:0\') or similar for HIP ``` **Implementation Note:** Your solution should utilize the existing `torch.cuda` interface for compatibility with both CUDA and HIP environments, as discussed in the HIP semantics documentation.","solution":"import torch def hip_tensor_operations(device_index: int, tensor_data1: list, tensor_data2: list) -> torch.Tensor: Perform tensor operations on the specified GPU device using PyTorch with HIP/CUDA. Parameters: - device_index (int): The index of the GPU device to use (0-indexed). - tensor_data1 (list): The data for the first tensor (as a list of floats). - tensor_data2 (list): The data for the second tensor (as a list of floats). Returns: - torch.Tensor: The resulting tensor after the addition operation, allocated on the specified device. Raises: - RuntimeError: If the specified device is not available. # Check if CUDA/HIP is available if not torch.cuda.is_available(): raise RuntimeError(\\"CUDA/HIP support is not available.\\") # Check the validity of the device index num_devices = torch.cuda.device_count() if device_index < 0 or device_index >= num_devices: raise RuntimeError(f\\"Invalid device index. Available devices range from 0 to {num_devices - 1}.\\") # Set the device device = torch.device(f\'cuda:{device_index}\') # Move the input data to tensors on the specified device tensor1 = torch.tensor(tensor_data1, device=device) tensor2 = torch.tensor(tensor_data2, device=device) # Perform the tensor addition result_tensor = tensor1 + tensor2 # Clear unused cached memory torch.cuda.empty_cache() return result_tensor"},{"question":"**Asynchronous Computation with torch.futures** You are given a task where multiple computational jobs need to be executed asynchronously. Your goal is to implement a function `run_async_tasks` which accepts a list of computational tasks, runs them asynchronously, and returns their results. # Requirements: 1. Define a function `run_async_tasks` that: - Takes a list of functions `tasks` as an argument. Each function, when called, performs some computation and returns a `torch.Tensor`. - Executes each function asynchronously using `torch.futures.Future`. - Collects the results of all tasks once all of them are finished. 2. Use the `torch.futures.collect_all` utility to gather the results of the futures. 3. The function should return a list of `torch.Tensor` objects, corresponding to the results of each task in the original order. # Constraints: - The input list `tasks` will have at least one function and at most 100 functions. - Each task function is thread-safe and does not require any specific order of execution. # Example: ```python import torch def sample_task_1(): return torch.tensor([1, 2, 3]) def sample_task_2(): return torch.tensor([4, 5, 6]) tasks = [sample_task_1, sample_task_2] results = run_async_tasks(tasks) # Expected output: [tensor([1, 2, 3]), tensor([4, 5, 6])] ``` # Implementation: Implement the `run_async_tasks` function below: ```python import torch from torch.futures import collect_all def run_async_tasks(tasks): futures = [torch.jit.fork(task) for task in tasks] results_futures = torch.futures.collect_all(futures) results = results_futures.wait() return [fut.wait() for fut in results] ``` In this solution: - `torch.jit.fork` is used to start the asynchronous tasks, returning a `Future` for each task. - `torch.futures.collect_all` is used to create a future that waits for all the input futures. - `results_futures.wait()` waits for all the futures to complete, and then the individual results are extracted. This question tests the ability to handle asynchronous computation in PyTorch, understanding of futures, and the use of provided utility functions for managing multiple asynchronous tasks.","solution":"import torch from torch.futures import collect_all def run_async_tasks(tasks): Executes a list of tasks asynchronously and returns their results. Parameters: tasks (list): A list of functions, each returning a torch.Tensor. Returns: list : A list of torch.Tensor, containing the results of the tasks. # Start all task functions asynchronously futures = [torch.jit.fork(task) for task in tasks] # Collect all future results results_futures = collect_all(futures) # Wait for all futures to be completed results = results_futures.wait() # Obtain the results from completed futures return [fut.wait() for fut in results]"},{"question":"**Objective:** Design a PyTorch program that utilizes the MPS backend for high-performance training on a MacOS device. Your program will create a neural network model, move it to the MPS device, and perform a series of tensor operations exclusively on the GPU. The program should also handle scenarios where the MPS backend is not available. **Instructions:** 1. **Check for MPS Availability:** - Verify if the MPS backend is available. Handle cases where it is not available using appropriate error messages. 2. **Tensor Operations:** - Create two tensors, `a` and `b`, of shape (100, 100) filled with random values using `torch.rand()`. - Perform matrix multiplication between `a` and `b` and store the result in tensor `c`. 3. **Neural Network:** - Define a simple neural network model with at least one hidden layer. - Move the model to the MPS device. - Execute a forward pass with a random input tensor of shape (10, 100). **Requirements:** - Clearly define the neural network model using `torch.nn.Module`. - Ensure tensor creation, operations, and model execution take place on the MPS device. - Handle the scenario where the MPS backend is not available by printing an appropriate message. **Input:** - No input is required from the user. **Output:** - Print the shape of the resulting tensor `c`. - Print the result of the model\'s forward pass. **Constraints:** - The solution should work on MacOS devices running version 12.3+ with an MPS-enabled GPU. - Ensure that all operations (tensor generation, model execution) are performed on the GPU when MPS is available. **Example Code Format:** ```python import torch import torch.nn as nn # Step 1: Check for MPS Availability and set device if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): print(\\"MPS not available because the current PyTorch install was not built with MPS enabled.\\") else: print(\\"MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device.\\") exit() else: mps_device = torch.device(\\"mps\\") # Step 2: Create tensors and perform operations on MPS device a = torch.rand((100, 100), device=mps_device) b = torch.rand((100, 100), device=mps_device) c = torch.matmul(a, b) # Print shape of the resulting tensor c print(\\"Shape of tensor c:\\", c.shape) # Step 3: Define a simple neural network model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.hidden = nn.Linear(100, 50) self.relu = nn.ReLU() self.output = nn.Linear(50, 10) def forward(self, x): x = self.hidden(x) x = self.relu(x) x = self.output(x) return x # Move model to MPS device model = SimpleNN().to(mps_device) # Create random input tensor and move to MPS device input_tensor = torch.rand((10, 100), device=mps_device) # Perform forward pass output = model(input_tensor) # Print the result of the model\'s forward pass print(\\"Model output shape:\\", output.shape) print(\\"Model output:\\", output) ```","solution":"import torch import torch.nn as nn def check_mps_availability(): if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): message = \\"MPS not available because the current PyTorch install was not built with MPS enabled.\\" else: message = \\"MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device.\\" raise EnvironmentError(message) return torch.device(\\"mps\\") def tensor_operations_on_mps(): mps_device = check_mps_availability() a = torch.rand((100, 100), device=mps_device) b = torch.rand((100, 100), device=mps_device) c = torch.matmul(a, b) return c class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.hidden = nn.Linear(100, 50) self.relu = nn.ReLU() self.output = nn.Linear(50, 10) def forward(self, x): x = self.hidden(x) x = self.relu(x) x = self.output(x) return x def run_neural_network_on_mps(): mps_device = check_mps_availability() model = SimpleNN().to(mps_device) input_tensor = torch.rand((10, 100), device=mps_device) output = model(input_tensor) return output"},{"question":"**Objective:** To demonstrate your understanding of the seaborn objects interface and your ability to handle overlapping plot elements and use various data transformation techniques. **Problem Statement:** You are provided with a dataset of restaurant tips and some instructions to visualize the data to analyze the tipping behavior. Your task is to create a comprehensive visualization by using seaborn objects (`seaborn.objects`). The visualization should include bar plots and dot plots, handle overlapping elements properly, and encode various semantic variables. **Dataset:** You will be working with the \\"tips\\" dataset provided by seaborn. Load the dataset using the following code: ```python import seaborn as sns tips = sns.load_dataset(\\"tips\\").astype({ \\"time\\": str }) ``` **Tasks:** 1. Create a bar plot showing the count of tips for each day, colored by the time of day (either \\"Lunch\\" or \\"Dinner\\"), such that overlapping bars are dodged to avoid overlap. 2. Ensure that all empty spaces are filled even if some days do not have tips for a specific time. Use the `empty` parameter to handle this scenario. 3. Create another bar plot showing the total bill amount for each day, colored by the gender of the tipper, and use the `gap` parameter to add a spacing of 0.1 between dodged bars. 4. Create a dot plot on top of the bar plot mentioned in task 3. The dot plot should show the count of smokers for each day, additionally dodging this semantic variable. Use the `fill` parameter to specify \\"smoker\\". 5. Ensure that proper order is maintained when combining dodging with other transformations (e.g., jitter). **Output:** Your code should produce a plot with the following characteristics: - Two bar plots showing the count of tips and the total bill amount, respectively. - Proper handling of overlapping elements using dodging. - Dot plot overlaying the second bar plot with the count of smokers shown. - A clear and non-overlapping visualization with the semantic variables properly encoded and transformed. **Constraints:** - Use the seaborn objects interface (`seaborn.objects`; imported as `so`). - Follow the instructions and ensure that each transformation is applied correctly. - The final plot should be clear and aesthetically pleasing. **Expected Output:** The code should produce a Matplotlib figure displaying the described visualizations when run in a Jupyter Notebook or Python script. ```python # Sample code to load the dataset import seaborn.objects as so import seaborn as sns tips = sns.load_dataset(\\"tips\\").astype({ \\"time\\": str }) # Your code for the tasks # Task 1 p1 = so.Plot(tips, \\"day\\", color=\\"time\\").add(so.Bar(), so.Count(), so.Dodge(empty=\\"fill\\")) p1.show() # Task 2 p2 = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\").add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1)) p2.show() # Task 3 (combining Dot plot overlay for smokers count) p2.add(so.Dot(), so.Dodge(by=[\\"color\\"]), fill=\\"smoker\\").show() ``` You may combine the above plots into one figure if desired. Make sure the visualizations are clear and present the data effectively.","solution":"import seaborn as sns import seaborn.objects as so # Load the dataset tips = sns.load_dataset(\\"tips\\").astype({ \\"time\\": str }) # Task 1: Bar plot showing count of tips for each day, colored by the time of day p1 = so.Plot(tips, x=\\"day\\", color=\\"time\\").add(so.Bar(), so.Count(), so.Dodge(empty=\\"fill\\")) p1.show() # Task 2: Bar plot showing the total bill amount for each day, colored by the gender of the tipper with gap p2 = so.Plot(tips, x=\\"day\\", y=\\"total_bill\\", color=\\"sex\\").add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1)) # Task 3: Dot plot overlay for smokers count, dodging by \\"color\\" semantic p2.add(so.Dot(), so.Dodge(by=[\\"color\\"]), fill=\\"smoker\\") p2.show()"},{"question":"**Coding Assessment Question** # Objective Design a function that compares two directory trees and summarizes the differences in a structured format, utilizing the `filecmp` module. Your function should identify matching files, differing files, and common directories, as well as files that exist only in one of the directories. # Function Signature ```python def summarize_directory_differences(dir1: str, dir2: str) -> dict: pass ``` # Input - `dir1` (str): The path to the first directory. - `dir2` (str): The path to the second directory. # Output - `summary` (dict): A dictionary summarizing the differences with the following keys: - `\\"matching_files\\"`: List of files that are identical in both directories. - `\\"differing_files\\"`: List of files that are present in both directories but are different. - `\\"unique_to_dir1\\"`: List of files and directories present only in `dir1`. - `\\"unique_to_dir2\\"`: List of files and directories present only in `dir2`. - `\\"error_files\\"`: List of files that could not be compared for some reason. # Example ```python dir1 = \\"path/to/dir1\\" dir2 = \\"path/to/dir2\\" summary = summarize_directory_differences(dir1, dir2) print(summary) # Expected Output (A sample format, actual output will depend on directories) # { # \\"matching_files\\": [\\"file1.txt\\", \\"subdir1/file2.txt\\"], # \\"differing_files\\": [\\"file3.txt\\", \\"subdir2/file4.txt\\"], # \\"unique_to_dir1\\": [\\"unique_file1.txt\\", \\"unique_subdir\\"], # \\"unique_to_dir2\\": [\\"unique_file2.txt\\", \\"unique_other_subdir\\"], # \\"error_files\\": [] # } ``` # Constraints - You must use the `filecmp` module to perform the comparisons. - Handle any potential errors or exceptions gracefully, ensuring the function does not crash. # Notes - Ensure that the function recursively navigates through all subdirectories. - Take care to exclude any directories/files listed in `filecmp.DEFAULT_IGNORES`. - The function should not make any changes to the directories or files, only read and compare. # Performance Requirements - The solution should be efficient enough to handle large directory structures with potentially thousands of files. # Hints - Use the `filecmp.dircmp` class to get detailed directory comparison information. - You may find the attributes `left_only`, `right_only`, `common_files`, `diff_files`, and `funny_files` of the `dircmp` class useful. - Consider using a recursive approach to handle subdirectories. # Testing - Test your function thoroughly with varying directory structures to ensure it handles all edge cases and potential errors. Good luck, and happy coding!","solution":"import filecmp import os def summarize_directory_differences(dir1: str, dir2: str) -> dict: def compare_dirs(dcmp): result = { \\"matching_files\\": [], \\"differing_files\\": [], \\"unique_to_dir1\\": [], \\"unique_to_dir2\\": [], \\"error_files\\": [] } result[\\"unique_to_dir1\\"].extend(os.path.join(dcmp.left, f) for f in dcmp.left_only) result[\\"unique_to_dir2\\"].extend(os.path.join(dcmp.right, f) for f in dcmp.right_only) result[\\"matching_files\\"].extend(os.path.join(dcmp.left, f) for f in dcmp.same_files) result[\\"differing_files\\"].extend(os.path.join(dcmp.left, f) for f in dcmp.diff_files) result[\\"error_files\\"].extend(os.path.join(dcmp.left, f) for f in dcmp.funny_files) for subdir, sub_dcmp in dcmp.subdirs.items(): sub_result = compare_dirs(sub_dcmp) for key in result.keys(): result[key].extend(sub_result[key]) return result dcmp = filecmp.dircmp(dir1, dir2) summary = compare_dirs(dcmp) return summary"},{"question":"# Complex Tensors in PyTorch Objective: Write a function that demonstrates the usage of complex tensors in PyTorch. This function should accept an input tensor, convert it to a complex tensor, perform some operations, and return both the real and imaginary components separately. Function Signature: ```python import torch def complex_tensor_operations(input_tensor): Accepts an input tensor with shape (..., 2), converts it to a complex tensor, performs certain operations, and returns the real and imaginary components separately. Parameters: input_tensor (torch.Tensor): A real tensor of shape (..., 2) where the last dimension contains the real and imaginary values. Returns: Tuple[torch.Tensor, torch.Tensor]: A tuple containing two tensors: - The real component of the resultant complex tensor. - The imaginary component of the resultant complex tensor. # Convert the input tensor to a complex tensor complex_tensor = torch.view_as_complex(input_tensor) # Perform a matrix multiplication operation # Create another complex tensor for multiplication multiplier = torch.randn_like(complex_tensor) result = torch.matmul(complex_tensor, multiplier.T) # Access the real and imaginary components of the resultant tensor real_component = result.real imaginary_component = result.imag # Return the real and imaginary components return real_component, imaginary_component ``` Constraints: 1. Assume the input tensor is always 2-dimensional and has the shape (..., 2). 2. Utilize functions and attributes provided by PyTorch for complex tensors. 3. Perform the operations efficiently using PyTorch optimized functions. Example: ```python # Example input tensor with shape (3, 2) input_tensor = torch.tensor([[0.6125, -0.1681], [-0.3773, 1.3487], [-0.0861, -0.7981]]) real_component, imaginary_component = complex_tensor_operations(input_tensor) print(\\"Real Component:n\\", real_component) print(\\"Imaginary Component:n\\", imaginary_component) ``` Expected Output: The function should convert the input tensor to a complex tensor, perform a matrix multiplication operation, and return the real and imaginary components of the resultant complex tensor.","solution":"import torch def complex_tensor_operations(input_tensor): Accepts an input tensor with shape (..., 2), converts it to a complex tensor, performs certain operations, and returns the real and imaginary components separately. Parameters: input_tensor (torch.Tensor): A real tensor of shape (..., 2) where the last dimension contains the real and imaginary values. Returns: Tuple[torch.Tensor, torch.Tensor]: A tuple containing two tensors: - The real component of the resultant complex tensor. - The imaginary component of the resultant complex tensor. # Convert the input tensor to a complex tensor complex_tensor = torch.view_as_complex(input_tensor) # Perform a simple complex conjugate operation on the tensor result = complex_tensor.conj() # Access the real and imaginary components of the resultant tensor real_component = result.real imaginary_component = result.imag # Return the real and imaginary components return real_component, imaginary_component"},{"question":"# Python Code Object Analysis and Creation Your task is to write Python functions that analyze and manipulate code objects. You need to demonstrate your understanding of Python code objects and the Python bytecode level. Part 1: Function to Analyze a Code Object 1. **Function Name**: `analyze_code_object` 2. **Input**: A Python function. 3. **Output**: A dictionary with the following keys: - `argcount`: Number of arguments the function takes. - `nlocals`: Number of local variables used by the function. - `stacksize`: Required stack size for the function. - `freevars`: Tuple of names of free variables. - `bytecode`: List of bytecode instructions as tuples, where each tuple contains (`byte_offset`, `opcode`, `arg`). ```python def analyze_code_object(func): Analyzes the code object of a given Python function. Parameters: func (function): The Python function to analyze. Returns: dict: Analysis of the code object. # Your implementation here ``` Part 2: Function to Create a New Code Object 1. **Function Name**: `create_dummy_code_object` 2. **Input**: A filename (string), a function name (string), and a first line number (integer). 3. **Output**: A Python code object. The function should use `PyCode_NewEmpty` from the documentation to create a new code object with the specified filename, function name, and first line number. Ensure the result cannot be executed using `exec` or `eval`. ```python def create_dummy_code_object(filename, funcname, firstlineno): Creates a new empty Python code object with the specified parameters. Parameters: filename (str): The filename to associate with the code object. funcname (str): The function name to associate with the code object. firstlineno (int): The first line number of the code object. Returns: code: The new dummy code object. # Your implementation here ``` Constraints - You are allowed to use built-in Python modules such as `dis` for disassembling the bytecode in Part 1. - You should raise appropriate errors if the input to the functions does not meet the requirements. Example Usage ```python def example_function(a, b): return a + b # Analyze the example function analysis = analyze_code_object(example_function) print(analysis) # Create a dummy code object dummy_code = create_dummy_code_object(\\"example.py\\", \\"dummy_function\\", 1) print(dummy_code) ``` This question assesses your understanding of Python\'s lower-level execution machinery and your ability to create and manipulate function code objects programmatically.","solution":"import dis import types def analyze_code_object(func): Analyzes the code object of a given Python function. Parameters: func (function): The Python function to analyze. Returns: dict: Analysis of the code object. if not callable(func): raise ValueError(\\"Input must be a callable function\\") code = func.__code__ bytecode = [(i.offset, i.opname, i.arg) for i in dis.get_instructions(code)] analysis = { \'argcount\': code.co_argcount, \'nlocals\': code.co_nlocals, \'stacksize\': code.co_stacksize, \'freevars\': code.co_freevars, \'bytecode\': bytecode } return analysis def create_dummy_code_object(filename, funcname, firstlineno): Creates a new empty Python code object with the specified parameters. Parameters: filename (str): The filename to associate with the code object. funcname (str): The function name to associate with the code object. firstlineno (int): The first line number of the code object. Returns: code: The new dummy code object. if not isinstance(filename, str) or not isinstance(funcname, str) or not isinstance(firstlineno, int): raise ValueError(\\"Invalid input types. \'filename\' and \'funcname\' should be string, \'firstlineno\' should be integer\\") return types.CodeType( 0, # argcount 0, # posonlyargcount 0, # kwonlyargcount 0, # nlocals 0, # stacksize 64, # flags b\'\', # codestring (), # constants (), # names (), # varnames filename, # filename funcname, # name firstlineno, # firstlineno b\'\' # lnotab )"},{"question":"# File Descriptor Locking and Control You are working on a project that requires you to implement a file locking mechanism to ensure that only one process can access a file at a time for writing. Additionally, you need to implement a function to check the file descriptor flags. Implement two functions, `lock_file` and `get_fd_flags`, using the `fcntl` module in Python. Function 1: `lock_file` This function should acquire an exclusive lock on the specified file so that other processes cannot write to it while it is locked. **Parameters:** - `file_path` (str): Path to the file to be locked. - `blocking` (bool): If `True`, the function should block until the lock is acquired. If `False`, the function should raise an exception if the lock cannot be acquired immediately. **Returns:** - None **Raises:** - `OSError`: If the lock cannot be acquired and `blocking` is `False`. **Example Usage:** ```python try: lock_file(\\"/path/to/file.txt\\", blocking=True) except OSError: print(\\"Could not acquire lock\\") ``` Function 2: `get_fd_flags` This function should retrieve the file descriptor flags for the given file. **Parameters:** - `file_path` (str): Path to the file whose file descriptor flags should be checked. **Returns:** - `int`: The file descriptor flags. **Example Usage:** ```python flags = get_fd_flags(\\"/path/to/file.txt\\") print(flags) ``` Constraints: - You can assume that the file specified by `file_path` exists and is accessible. - You should use the `fcntl` module\'s `flock()` method for file locking and `fcntl()` method for retrieving file descriptor flags. Implement the two functions as described.","solution":"import os import fcntl def lock_file(file_path, blocking=True): Acquire an exclusive lock on the specified file. Parameters: - file_path (str): Path to the file to be locked. - blocking (bool): If True, block until the lock is acquired. If False, raise an exception if the lock cannot be acquired immediately. Returns: - None Raises: - OSError: If the lock cannot be acquired and blocking is False. fd = os.open(file_path, os.O_WRONLY) try: if blocking: fcntl.flock(fd, fcntl.LOCK_EX) else: fcntl.flock(fd, fcntl.LOCK_EX | fcntl.LOCK_NB) except OSError: os.close(fd) raise return fd def get_fd_flags(file_path): Retrieve the file descriptor flags for the given file. Parameters: - file_path (str): Path to the file whose file descriptor flags should be checked. Returns: - int: The file descriptor flags. fd = os.open(file_path, os.O_RDONLY) try: flags = fcntl.fcntl(fd, fcntl.F_GETFD) finally: os.close(fd) return flags"},{"question":"**Question: Implement a Custom SAX Content Handler and XML Parsing** You are required to implement a custom SAX content handler that processes an XML document containing information about books. The XML documents will have the structure as shown below: ```xml <library> <book> <title>Python Programming</title> <author>John Doe</author> <year>2020</year> </book> <!-- More book entries --> </library> ``` Your task is to: 1. Implement a class `BookHandler` that inherits from `xml.sax.handler.ContentHandler` and overrides necessary methods to process the XML document. 2. Use an `XMLReader` to parse the XML document using your custom content handler. The `BookHandler` should accumulate information about each book and store it in a list of dictionaries with keys `title`, `author`, and `year`. Each dictionary represents a book. # Requirements - Implement the `BookHandler` class with the following methods: - `startElement(name, attrs)`: Capture start of the `title`, `author`, and `year` elements. - `endElement(name)`: Capture end of the `title`, `author`, and `year` elements. - `characters(content)`: Capture the content within `title`, `author`, and `year` elements. - Implement a function `parse_books(xml_file)` that: - Takes a file path `xml_file` as an input. - Sets up an `XMLReader` with your custom `BookHandler`. - Parses the XML document. - Returns a list of dictionaries representing the books. # Constraints - Assume that the XML document is well-formed. - You are not allowed to use other XML parsing libraries such as `lxml`. # Example Here is an example of how your code should behave: ```python xml_content = \'\'\' <library> <book> <title>Python Programming</title> <author>John Doe</author> <year>2020</year> </book> <book> <title>Advanced Python</title> <author>Jane Smith</author> <year>2022</year> </book> </library> \'\'\' # Save the xml_content to a file named \'books.xml\' with open(\'books.xml\', \'w\') as file: file.write(xml_content) books = parse_books(\'books.xml\') print(books) # Output should be: # [{\'title\': \'Python Programming\', \'author\': \'John Doe\', \'year\': \'2020\'}, # {\'title\': \'Advanced Python\', \'author\': \'Jane Smith\', \'year\': \'2022\'}] ``` Implement the `BookHandler` class and the `parse_books` function below: ```python import xml.sax from xml.sax.handler import ContentHandler class BookHandler(ContentHandler): def __init__(self): super().__init__() self.current_element = \'\' self.current_data = {} self.books = [] def startElement(self, name, attrs): self.current_element = name if name == \'book\': self.current_data = {} def endElement(self, name): if name == \'book\': self.books.append(self.current_data) self.current_element = \'\' def characters(self, content): if self.current_element in [\'title\', \'author\', \'year\']: self.current_data[self.current_element] = content.strip() def parse_books(xml_file): handler = BookHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) parser.parse(xml_file) return handler.books ```","solution":"import xml.sax from xml.sax.handler import ContentHandler class BookHandler(ContentHandler): def __init__(self): super().__init__() self.current_element = \'\' self.current_data = {} self.books = [] def startElement(self, name, attrs): self.current_element = name if name == \'book\': self.current_data = {\'title\': \'\', \'author\': \'\', \'year\': \'\'} def endElement(self, name): if name == \'book\': self.books.append(self.current_data) self.current_element = \'\' def characters(self, content): if self.current_element in [\'title\', \'author\', \'year\']: self.current_data[self.current_element] += content.strip() def parse_books(xml_file): handler = BookHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) parser.parse(xml_file) return handler.books"},{"question":"**Objective**: Implement and demonstrate a multi-threaded solution using the threading module in Python. Problem Description You are required to create a threaded application that simulates a busy airport luggage handling system. This system consists of multiple conveyor belts (threads), each of which can carry one piece of luggage from a storage area to an airplane. Requirements 1. **Thread Creation**: - Create a class `ConveyorBelt` that inherits from `threading.Thread`. - Each ConveyorBelt thread should pick up a piece of luggage, take a designated time to carry it, and then place it on the airplane. Use `time.sleep()` to simulate the travel time. 2. **Locking Mechanism**: - You should use `threading.Lock` to ensure that only one thread accesses the critical section of code where luggage is picked up and placed on the airplane at any given time. 3. **ConveyorBelt Class**: - The `__init__` method should take parameters `belt_id` (an identifier for the conveyor belt) and `lock` (the Lock object). - The `run` method should: - Print which conveyor belt is picking up a luggage. - Lock the critical section to simulate picking up and placing luggage. - Print a message indicating which conveyor belt placed the luggage. - Unlock the critical section. 4. **Synchronization**: - Implement a mechanism to ensure all conveyor belts wait until all threads have been started and initialized before they begin picking up the luggage. You may use `threading.Barrier` for synchronization. Input - Number of conveyor belts (threads): An integer N (2 <= N <= 10). - Travel time for each conveyor belt: A list of N integers representing time in seconds. Output - Prints messages indicating the sequence of operations performed by each conveyor belt (thread). Constraints - The program should gracefully handle any exceptions. - Performance should be considered; avoid unnecessary complexity. Sample Execution ```python from threading import Lock, Barrier import time class ConveyorBelt(threading.Thread): def __init__(self, belt_id, lock, barrier, travel_time): threading.Thread.__init__(self) self.belt_id = belt_id self.lock = lock self.barrier = barrier self.travel_time = travel_time def run(self): self.barrier.wait() # Wait for all threads to start together print(f\\"Conveyor Belt {self.belt_id} is ready.\\") time.sleep(self.travel_time) with self.lock: print(f\\"Conveyor Belt {self.belt_id} is picking up a luggage.\\") print(f\\"Conveyor Belt {self.belt_id} placed the luggage on the airplane.\\") if __name__ == \\"__main__\\": n = 3 # Number of conveyor belts travel_times = [2, 3, 1] # Travel times for each conveyor belt lock = Lock() barrier = Barrier(n) belts = [ConveyorBelt(i, lock, barrier, travel_time) for i, travel_time in enumerate(travel_times)] for belt in belts: belt.start() for belt in belts: belt.join() ``` In the provided sample code, three conveyor belts are created, each with different travel times. The `Barrier` ensures that all threads start simultaneously, and the `Lock` guarantees only one thread accesses the critical section at a time.","solution":"import threading import time class ConveyorBelt(threading.Thread): def __init__(self, belt_id, lock, barrier, travel_time): threading.Thread.__init__(self) self.belt_id = belt_id self.lock = lock self.barrier = barrier self.travel_time = travel_time def run(self): self.barrier.wait() # Wait for all threads to start together print(f\\"Conveyor Belt {self.belt_id} is ready.\\") time.sleep(self.travel_time) with self.lock: print(f\\"Conveyor Belt {self.belt_id} is picking up a luggage.\\") time.sleep(0.5) # Simulate the time taken to pick up and place the luggage print(f\\"Conveyor Belt {self.belt_id} placed the luggage on the airplane.\\") def simulate_airport_luggage_system(num_belts, travel_times): lock = threading.Lock() barrier = threading.Barrier(num_belts) belts = [ConveyorBelt(i, lock, barrier, travel_time) for i, travel_time in enumerate(travel_times)] for belt in belts: belt.start() for belt in belts: belt.join()"},{"question":"Objective: Your task is to implement a Python function that can handle the complexities of packing and unpacking structured binary data. You will use the `struct` module to solve this problem. This exercise will test your understanding of format strings, byte order, and buffer protocols. Problem Statement: You are given a list of records where each record contains five fields: an integer, a short integer, a single character (byte), a floating-point number, and a boolean. These records need to be processed in binary format for storage or network transmission. You will implement two functions: `pack_records` and `unpack_records`. 1. **Function `pack_records(records: List[Tuple[int, int, bytes, float, bool]]) -> bytes`:** - **Input**: A list of tuples, where each tuple contains: - An integer (4 bytes) - A short integer (2 bytes) - A single character (1 byte) - A floating-point number (4 bytes) - A boolean (1 byte) - **Output**: A bytes object containing all the packed records in network byte order (big-endian). 2. **Function `unpack_records(packed_data: bytes) -> List[Tuple[int, int, bytes, float, bool]]`:** - **Input**: A bytes object containing packed records in the format specified above. - **Output**: A list of unpacked records as tuples. Constraints: - Assume the total length of each packed record is consistent and does not change. - The format for packing/unpacking is as follows: `\'>iHcfc\'` - `>`: Big-endian - `i`: Integer - `H`: Unsigned short integer - `c`: Character (single byte) - `f`: Floating-point number - `c`: Character (used for boolean, with 0 representing `False` and any non-zero value representing `True`) Example: ```python # Example records records = [ (1234, 12, b\'A\', 3.14, True), (5678, 34, b\'B\', 1.618, False) ] # Packing the records packed_data = pack_records(records) print(packed_data) # Unpacking the records unpacked_records = unpack_records(packed_data) print(unpacked_records) # Expected Output: # [(1234, 12, b\'A\', 3.140000104904175, True), (5678, 34, b\'B\', 1.6180000305175781, False)] ``` Requirements: 1. Implement both `pack_records` and `unpack_records` functions. 2. Ensure correct handling of big-endian byte order. 3. Ensure proper interpretation of the boolean values.","solution":"import struct from typing import List, Tuple def pack_records(records: List[Tuple[int, int, bytes, float, bool]]) -> bytes: packed_data = b\'\' for record in records: int_val, short_val, char_val, float_val, bool_val = record # Ensure the char_val is a single byte char_val = char_val if isinstance(char_val, bytes) else char_val.encode() # Convert boolean to a single byte (0 for False, 1 for True) bool_byte = b\'x01\' if bool_val else b\'x00\' packed_data += struct.pack(\'>iHcfc\', int_val, short_val, char_val, float_val, bool_byte) return packed_data def unpack_records(packed_data: bytes) -> List[Tuple[int, int, bytes, float, bool]]: record_size = struct.calcsize(\'>iHcfc\') records = [] for i in range(0, len(packed_data), record_size): section = packed_data[i:i+record_size] int_val, short_val, char_val, float_val, bool_byte = struct.unpack(\'>iHcfc\', section) records.append((int_val, short_val, char_val, float_val, bool(ord(bool_byte)))) return records"},{"question":"**Persisting and Managing User Profiles with `shelve`** # Objective: Implement a `UserProfileManager` class that manages user profiles using the `shelve` module. This class should allow adding, retrieving, updating, and deleting profiles, ensuring changes are properly persisted to disk. # Instructions: 1. **Class Definition**: Define a class `UserProfileManager` which uses a shelf to store user profiles. 2. **Initialization**: - The constructor should accept a `filename` as a parameter, used to open the shelf. 3. **Methods**: - `add_user(user_id: str, profile: dict)`: Adds a new user profile. If the user ID already exists, raise a `ValueError`. - `get_user(user_id: str) -> dict`: Retrieves the profile for the given user ID. If the user ID does not exist, raise a `KeyError`. - `update_user(user_id: str, profile: dict)`: Updates the profile for the given user ID. If the user ID does not exist, raise a `KeyError`. - `delete_user(user_id: str)`: Deletes the profile for the given user ID. If the user ID does not exist, raise a `KeyError`. - `list_users() -> list`: Returns a list of all user IDs currently in the shelf. - `close()`: Closes the shelf. 4. **Context Management**: - Implement the `__enter__` and `__exit__` methods to support usage as a context manager. # Example Usage: ```python manager = UserProfileManager(\'profiles.db\') manager.add_user(\'user1\', {\'name\': \'Alice\', \'age\': 30}) print(manager.get_user(\'user1\')) # Output: {\'name\': \'Alice\', \'age\': 30} manager.update_user(\'user1\', {\'name\': \'Alice\', \'age\': 31}) print(manager.list_users()) # Output: [\'user1\'] manager.delete_user(\'user1\') manager.close() # Using as a context manager with UserProfileManager(\'profiles.db\') as manager: manager.add_user(\'user2\', {\'name\': \'Bob\', \'age\': 25}) print(manager.get_user(\'user2\')) # Output: {\'name\': \'Bob\', \'age\': 25} ``` # Constraints: - The `profile` should be a dictionary containing only strings and integers. - Handle all exceptions related to shelf operations gracefully and provide meaningful error messages. # Requirements: - Thoroughly test the class implementation, ensuring all methods behave as expected and handle edge cases appropriately. - Your solution should illustrate an understanding of persistent storage, shelf management, and proper error handling in Python.","solution":"import shelve class UserProfileManager: def __init__(self, filename): self.filename = filename self.shelf = shelve.open(filename) def add_user(self, user_id: str, profile: dict): if user_id in self.shelf: raise ValueError(f\\"User with ID {user_id} already exists.\\") self.shelf[user_id] = profile def get_user(self, user_id: str) -> dict: try: return self.shelf[user_id] except KeyError: raise KeyError(f\\"User with ID {user_id} does not exist.\\") def update_user(self, user_id: str, profile: dict): if user_id not in self.shelf: raise KeyError(f\\"User with ID {user_id} does not exist.\\") self.shelf[user_id] = profile def delete_user(self, user_id: str): if user_id not in self.shelf: raise KeyError(f\\"User with ID {user_id} does not exist.\\") del self.shelf[user_id] def list_users(self) -> list: return list(self.shelf.keys()) def close(self): self.shelf.close() def __enter__(self): return self def __exit__(self, exc_type, exc_value, traceback): self.close()"},{"question":"**Coding Assessment: Custom Visualization with Seaborn** You are provided with a dataset from seaborn\'s built-in datasets: the Titanic dataset. Your task is to create a custom visualization using seaborn that meets the following specifications: # Requirements 1. **Load Dataset**: - Load the Titanic dataset using seaborn\'s `load_dataset` function. 2. **Data Manipulation**: - Create a new column named `age_group` that categorizes the passengers into four age groups: \'Child\' (below 18), \'Young Adult\' (18-35), \'Adult\' (36-60), and \'Senior\' (above 60). 3. **Visualization**: - Generate a countplot showing the distribution of passengers across different `age_group` categories. - Use the `hue` parameter to differentiate between male and female passengers. - Normalize the counts to show percentages instead of raw counts. 4. **Customization**: - Set the theme of the plot to `whitegrid`. - Add appropriate labels for the x-axis, y-axis, and a title for the plot. # Expectations - **Input**: - Titanic dataset loaded using seaborn. - **Output**: - A seaborn plot object showing the distribution of age groups by gender, with normalized percentages. # Code Implementation Your function should be named `create_custom_titanic_plot` and should not require any external input parameters. The signature is as follows: ```python def create_custom_titanic_plot(): # Write your code here ``` Example output (plot structure, not exact look or percentages): ``` ------------- | | | | | | | | | ------------- ``` Ensure that your solution runs without errors and meets all the specified requirements.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_custom_titanic_plot(): # Load Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create age_group column def categorize_age(age): if pd.isnull(age): return \'Unknown\' elif age < 18: return \'Child\' elif 18 <= age <= 35: return \'Young Adult\' elif 36 <= age <= 60: return \'Adult\' else: return \'Senior\' titanic[\'age_group\'] = titanic[\'age\'].apply(categorize_age) # Define an ordered category for age_group age_group_order = [\'Child\', \'Young Adult\', \'Adult\', \'Senior\', \'Unknown\'] # Set theme sns.set_theme(style=\\"whitegrid\\") # Create countplot count_plot = sns.catplot( data=titanic, x=\'age_group\', hue=\'sex\', kind=\'count\', order=age_group_order, height=5, aspect=2 ) # Normalize the counts to show percentages count_plot.ax.set_ylabel(\'Percentage\') total_count = len(titanic) for p in count_plot.ax.patches: percentage = f\'{100 * p.get_height() / total_count:.1f}%\' count_plot.ax.annotate(percentage, (p.get_x() + p.get_width() / 2, p.get_height()), ha=\'center\', va=\'bottom\', fontsize=10, color=\'black\') # Add labels and title count_plot.set_axis_labels(\\"Age Group\\", \\"Percentage\\") count_plot.fig.suptitle(\'Distribution of Passengers by Age Group and Gender\') count_plot.fig.subplots_adjust(top=0.9) # Adjust title position # Display plot plt.show()"},{"question":"# Coding Assessment: Mastering Pandas Categorical Data Objective Implement functions to demonstrate your understanding of pandas categorical data types. Problem Description You are provided with a company employees\' dataset. The dataset includes columns such as employee ID, department (e.g., \\"HR\\", \\"Engineering\\", \\"Sales\\"), review ratings (e.g., \\"Excellent\\", \\"Good\\", \\"Fair\\", \\"Poor\\"), and the employment status (e.g., \\"Active\\", \\"Terminated\\"). You need to: 1. Create a DataFrame with appropriate data types. 2. Convert specific columns to categorical types with specified categories and orders. 3. Handle missing data within these categorical columns. 4. Sort and merge these DataFrames. 5. Perform specific operations like renaming and appending categories. # Input - A dictionary containing employee data with keys `\\"employee_id\\"`, `\\"department\\"`, `\\"review_rating\\"`, and `\\"status\\"`. ```python data = { \\"employee_id\\": [101, 102, 103, 104, 105], \\"department\\": [\\"HR\\", \\"Engineering\\", \\"Sales\\", \\"Marketing\\", np.nan], \\"review_rating\\": [\\"Excellent\\", \\"Good\\", \\"Fair\\", np.nan, \\"Poor\\"], \\"status\\": [\\"Active\\", \\"Active\\", \\"Terminated\\", \\"Active\\", \\"Terminated\\"] } ``` # Output - A DataFrame after performing all the required operations. # Function Signature ```python import pandas as pd import numpy as np def preprocess_employee_data(data: dict) -> pd.DataFrame: Function to perform preprocessing on employee data. Args: - data (dict): Dictionary containing employee data. Returns: - pd.DataFrame: A preprocessed pandas DataFrame with appropriate categorical manipulations. # Tasks 1. **DataFrame Creation** Create a DataFrame from the given dictionary. Convert the `department`, `review_rating`, and `status` columns to categorical types. 2. **Specify Categories and Orders** - For the `department` column, set the categories to [\\"HR\\", \\"Engineering\\", \\"Sales\\", \\"Marketing\\"] without ordering. - For the `review_rating` column, set the categories to [\\"Poor\\", \\"Fair\\", \\"Good\\", \\"Excellent\\"] with specified ordering. - For the `status` column, set categories to [\\"Active\\", \\"Terminated\\"] without ordering. 3. **Handle Missing Data** Fill missing values in the `department` column with \\"Unknown\\" and in the `review_rating` column with \\"Not Rated\\". 4. **Renaming Categories** Rename the `status` categories: \\"Active\\" to \\"Employed\\" and \\"Terminated\\" to \\"Former\\". 5. **Appending New Categories** Add a new category \\"Contract\\" to the `status` column. 6. **Sorting and Merging:** - Sort the DataFrame based on the `review_rating` column. - Create another simple DataFrame with few categories and merge this new DataFrame with the original DataFrame. 7. **Return the final DataFrame.** # Example Usage: ```python data = { \\"employee_id\\": [101, 102, 103, 104, 105], \\"department\\": [\\"HR\\", \\"Engineering\\", \\"Sales\\", \\"Marketing\\", np.nan], \\"review_rating\\": [\\"Excellent\\", \\"Good\\", \\"Fair\\", np.nan, \\"Poor\\"], \\"status\\": [\\"Active\\", \\"Active\\", \\"Terminated\\", \\"Active\\", \\"Terminated\\"] } result = preprocess_employee_data(data) print(result) ``` Expected output: ``` employee_id department review_rating status 0 101 HR Excellent Employed 1 102 Engineering Good Employed 2 103 Sales Fair Former 3 105 Unknown Poor Former 4 104 Marketing Not Rated Employed ``` # Evaluation Criteria - Correct conversion to categorical type with the appropriate categories and orders. - Accurate handling and filling of missing data. - Proper renaming of categories. - Correct appending of new categories. - Valid sorting and merging operations. - Clean, well-documented, and efficient code.","solution":"import pandas as pd import numpy as np def preprocess_employee_data(data): # Create DataFrame df = pd.DataFrame(data) # Convert to categorical types with specified categories and orders df[\'department\'] = pd.Categorical(df[\'department\'], categories=[\\"HR\\", \\"Engineering\\", \\"Sales\\", \\"Marketing\\"], ordered=False) df[\'review_rating\'] = pd.Categorical(df[\'review_rating\'], categories=[\\"Poor\\", \\"Fair\\", \\"Good\\", \\"Excellent\\"], ordered=True) df[\'status\'] = pd.Categorical(df[\'status\'], categories=[\\"Active\\", \\"Terminated\\"], ordered=False) # Handle missing data df[\'department\'] = df[\'department\'].cat.add_categories([\'Unknown\']).fillna(\'Unknown\') df[\'review_rating\'] = df[\'review_rating\'].cat.add_categories([\'Not Rated\']).fillna(\'Not Rated\') # Rename categories df[\'status\'] = df[\'status\'].cat.rename_categories({\'Active\': \'Employed\', \'Terminated\': \'Former\'}) # Append new categories df[\'status\'] = df[\'status\'].cat.add_categories(\'Contract\') # Sort DataFrame based on review_rating df = df.sort_values(by=\'review_rating\') return df"},{"question":"Objective Evaluate your understanding of model evaluation using `validation_curve` and `learning_curve` from the `sklearn.model_selection` module in scikit-learn. You will need to analyze and interpret the resulting curves to make informed decisions about model performance in terms of bias and variance. Problem Statement You are given a dataset and are required to: 1. Plot validation curves for an SVM classifier with different values for the hyperparameter `C`. 2. Plot learning curves for the same classifier. 3. Analyze and interpret the results. Dataset We will use the Iris dataset for this task. Task 1. **Validation Curve:** - Load the Iris dataset. - Shuffle the dataset. - Use the `validation_curve` function to compute training and validation scores for an SVM classifier with a linear kernel. The hyperparameter to vary is `C`, with values taken from `np.logspace(-7, 3, 10)`. - Plot the resulting validation curve. 2. **Learning Curve:** - Use the `learning_curve` function to compute training and validation scores for varying sizes of the training set. Use the same SVM classifier with a linear kernel. Set `train_sizes` to `[50, 80, 110]` and `cv=5`. - Plot the learning curve. 3. **Analysis:** - Interpret the validation curve to determine the model\'s performance in terms of overfitting and underfitting. - Analyze the learning curve to understand the model\'s behavior as the amount of training data increases. Implementation Details - Use the Iris dataset available in `sklearn.datasets`. - Ensure that the plots are well-labeled, including axis labels, legends, and titles for clarity. Expected Output 1. Two plots: one for the validation curve and one for the learning curve. 2. A brief analysis (5-6 sentences) interpreting the results from the plots regarding overfitting, underfitting, and the effect of adding more training data. # Example Code ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.model_selection import validation_curve, learning_curve from sklearn.svm import SVC from sklearn.utils import shuffle # Load and shuffle the dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Plot validation curve param_range = np.logspace(-7, 3, 10) train_scores, valid_scores = validation_curve( SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range, cv=5 ) plt.figure() plt.plot(param_range, np.mean(train_scores, axis=1), label=\'Training score\') plt.plot(param_range, np.mean(valid_scores, axis=1), label=\'Validation score\') plt.xlabel(\'C\') plt.ylabel(\'Score\') plt.xscale(\'log\') plt.title(\'Validation Curve\') plt.legend() plt.show() # Plot learning curve train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\\"linear\\"), X, y, train_sizes=[50, 80, 110], cv=5 ) plt.figure() plt.plot(train_sizes, np.mean(train_scores, axis=1), label=\'Training score\') plt.plot(train_sizes, np.mean(valid_scores, axis=1), label=\'Validation score\') plt.xlabel(\'Training sizes\') plt.ylabel(\'Score\') plt.title(\'Learning Curve\') plt.legend() plt.show() # Output analysis print(\\"Interpret the plots to determine overfitting, underfitting, and the effect of adding more training data.\\") ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.model_selection import validation_curve, learning_curve from sklearn.svm import SVC from sklearn.utils import shuffle # Load and shuffle the dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Plot validation curve param_range = np.logspace(-7, 3, 10) train_scores, valid_scores = validation_curve( SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range, cv=5 ) plt.figure() plt.plot(param_range, np.mean(train_scores, axis=1), label=\'Training score\') plt.plot(param_range, np.mean(valid_scores, axis=1), label=\'Validation score\') plt.xlabel(\'C\') plt.ylabel(\'Score\') plt.xscale(\'log\') plt.title(\'Validation Curve\') plt.legend() plt.show() # Plot learning curve train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\\"linear\\"), X, y, train_sizes=[50, 80, 110], cv=5 ) plt.figure() plt.plot(train_sizes, np.mean(train_scores, axis=1), label=\'Training score\') plt.plot(train_sizes, np.mean(valid_scores, axis=1), label=\'Validation score\') plt.xlabel(\'Training sizes\') plt.ylabel(\'Score\') plt.title(\'Learning Curve\') plt.legend() plt.show() # Analysis: # From the validation curve: # - For very low values of C, the model underfits as both training and validation scores are low. # - As C increases, the training score increases but the validation score starts to drop, indicating overfitting. # - There seems to be an optimal range of C where the validation score is highest before it starts to drop due to overfitting. # From the learning curve: # - Both training and validation scores converge as the training size increases, indicating low variance. # - The gap between training and validation scores is relatively small, indicating the model is neither overfitting nor underfitting severely. # - Adding more data seems to stabilize the model performance further."},{"question":"**Question: Analysis and Customization of Violin Plots using Seaborn** **Objective:** This question aims to test your ability to analyze data and create customized violin plots using Seaborn. You will be provided with a dataset and will need to implement functions to generate specific types of violin plots. **Dataset:** Use the Titanic dataset available in Seaborn. You can load the dataset using: ```python import seaborn as sns df = sns.load_dataset(\\"titanic\\") ``` # Function 1: Age Distribution Violin Plot **Function Signature:** ```python def age_distribution_violinplot(df): pass ``` **Description:** This function should create and display a violin plot representing the age distribution of passengers. **Expected Output:** A simple violin plot of the `age` column. # Function 2: Class and Age Violin Plot **Function Signature:** ```python def class_age_violinplot(df): pass ``` **Description:** This function should create and display a violin plot showing the distribution of ages across different passenger classes. **Expected Output:** A violin plot with `class` on the x-axis and `age` on the y-axis. # Function 3: Survival Status and Age Violin Plot **Function Signature:** ```python def survival_status_violinplot(df): pass ``` **Description:** This function should create a violin plot showing age distributions across different survival statuses (`alive` vs. `not alive`) grouped by passenger class. **Expected Output:** A split violin plot with `class` on the x-axis, `age` on the y-axis, and `alive` as the hue. # Constraints: - Ensure the `seaborn` library is installed and imported. - Implement the functions without changing the data structure or pre-processing beyond handling `NaN` values if necessary. - The plots must be clear, appropriately labeled, and use the default Seaborn whitegrid theme. **Example Usage:** ```python age_distribution_violinplot(df) class_age_violinplot(df) survival_status_violinplot(df) ``` **Notes:** - Handle missing values in the `age` column appropriately for plotting. - Ensure the functions are self-contained and do not rely on external code. This question assesses your ability to work with data visualization techniques and customize plots using Seaborn\'s violinplot function.","solution":"import seaborn as sns import matplotlib.pyplot as plt def age_distribution_violinplot(df): Create and display a violin plot representing the age distribution of passengers. # Drop NaN values in the \'age\' column for proper plotting df = df.dropna(subset=[\'age\']) sns.violinplot(x=df[\'age\']) plt.title(\'Age Distribution of Passengers\') plt.xlabel(\'Age\') plt.show() def class_age_violinplot(df): Create and display a violin plot showing the distribution of ages across different passenger classes. # Drop NaN values in the \'age\' column for proper plotting df = df.dropna(subset=[\'age\']) sns.violinplot(x=\'class\', y=\'age\', data=df) plt.title(\'Age Distribution by Passenger Class\') plt.xlabel(\'Class\') plt.ylabel(\'Age\') plt.show() def survival_status_violinplot(df): Create a violin plot showing age distributions across different survival statuses grouped by passenger class. # Drop NaN values in the \'age\' column for proper plotting df = df.dropna(subset=[\'age\']) sns.violinplot(x=\'class\', y=\'age\', hue=\'alive\', data=df, split=True) plt.title(\'Age Distribution by Survival Status and Passenger Class\') plt.xlabel(\'Class\') plt.ylabel(\'Age\') plt.show()"},{"question":"**Objective**: Demonstrate understanding of Python\'s `bz2` module for data compression and decompression, including file handling and incremental processing. **Problem Statement**: You are tasked with writing a program to handle large text files that may not fit into memory all at once. Your program should: 1. Read data from a given large text file in chunks. 2. Compress the data incrementally and write the compressed data to a bzip2-compressed file. 3. Decompress the compressed file incrementally and write the decompressed data to a new text file. 4. Verify that the original and the decompressed files are identical. **Function Signature**: ```python def process_large_file(input_filename: str, compressed_filename: str, decompressed_filename: str, chunk_size: int = 1024) -> bool: pass ``` **Input**: - `input_filename` (str): The path to the large input text file. - `compressed_filename` (str): The path where the compressed file will be written. - `decompressed_filename` (str): The path where the decompressed file will be written. - `chunk_size` (int): The size of the data chunks to read and process incrementally (default is 1024 bytes). **Output**: - Returns a boolean value `True` if the original file and the decompressed file are identical, otherwise `False`. **Constraints**: - The input file can be very large, ensure your solution handles large files efficiently. - Utilize the incremental compression and decompression classes (`BZ2Compressor` and `BZ2Decompressor`). - Ensure proper resource management using context managers or equivalent means to handle file operations. **Example**: Assume you have a large text file `large_input.txt`. You can test your function as follows: ```python result = process_large_file(\'large_input.txt\', \'large_output_compressed.bz2\', \'large_output_decompressed.txt\') print(result) # Should return True if the decompressed data matches the original file content ``` **Performance Requirements**: - The solution should be able to handle files of several gigabytes effectively. - Ensure that the memory footprint remains low by processing the file in chunks. **Hints**: - Use `bz2.BZ2Compressor` and `bz2.BZ2Decompressor` for incremental compression and decompression. - Utilize the `with` statement for opening and closing files efficiently. - Handle potential edge cases like empty files or files smaller than the specified chunk size. **Additional Information**: - Refer to the provided examples in the bz2 module documentation to understand incremental compression and decompression techniques.","solution":"import bz2 def process_large_file(input_filename: str, compressed_filename: str, decompressed_filename: str, chunk_size: int = 1024) -> bool: # Compress the file with open(input_filename, \'rb\') as input_file, bz2.BZ2File(compressed_filename, \'wb\') as compressed_file: compressor = bz2.BZ2Compressor() while True: chunk = input_file.read(chunk_size) if not chunk: break compressed_file.write(compressor.compress(chunk)) compressed_file.write(compressor.flush()) # Decompress the file with bz2.BZ2File(compressed_filename, \'rb\') as compressed_file, open(decompressed_filename, \'wb\') as output_file: decompressor = bz2.BZ2Decompressor() while True: chunk = compressed_file.read(chunk_size) if not chunk: break output_file.write(decompressor.decompress(chunk)) # Verify the integrity with open(input_filename, \'rb\') as original_file, open(decompressed_filename, \'rb\') as decompressed_file: while True: original_chunk = original_file.read(chunk_size) decompressed_chunk = decompressed_file.read(chunk_size) if original_chunk != decompressed_chunk: return False if not original_chunk: # both chunks are empty / EOF reached break return True"},{"question":"Objective: Demonstrate your understanding of the Seaborn library by creating informative and well-customized bar plots. You will need to handle different types of data configurations and apply various customization options to create meaningful visualizations. Problem Statement: You are given a dataset containing information about different restaurants and their ratings. Your task is to create several bar plots to visualize this data effectively. Follow the instructions below to complete the task: 1. Load the dataset from the provided CSV file `restaurant_ratings.csv` into a Pandas DataFrame. The dataset contains the following columns: - `Restaurant`: Name of the restaurant. - `Category`: Type of cuisine (e.g., Italian, Chinese, etc.). - `Rating`: Rating of the restaurant (out of 5). - `Price`: Average price per meal. 2. Create a bar plot showing the average rating of restaurants grouped by `Category`. Ensure that: - The bars are sorted in descending order of the average rating. - Error bars represent the standard deviation of the ratings within each category. 3. Create a horizontal bar plot displaying the sum of ratings for each `Restaurant`. Ensure that: - Bars are colored by their `Category` to distinguish between different cuisines. 4. Create a faceted bar plot to show the distribution of `Price` across different restaurants within each cuisine category. Ensure that: - Each facet represents one cuisine category. - Facets are laid out in a grid with 2 columns. - The bars represent the average price with error bars as standard deviation. Input: - CSV file `restaurant_ratings.csv`. Output: - Three Seaborn bar plots as described above. # Constraints: - Ensure proper handling of NaN values if they exist in the dataset. - Use appropriate labels and titles to make the plots informative. - Any additional customization to improve the plots\' readability and aesthetics will be appreciated. Example Code Snippet: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the dataset df = pd.read_csv(\'restaurant_ratings.csv\') # Step 1: Average rating by category # Your code here # Step 2: Sum of ratings by restaurant colored by category # Your code here # Step 3: Faceted bar plot for average price by category # Your code here ``` Complete the code snippet to implement the required visualizations.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from scipy.stats import sem def plot_restaurant_ratings(file_path): # Load the dataset df = pd.read_csv(file_path) # Step 1: Bar plot showing the average rating of restaurants grouped by Category category_avg_rating = df.groupby(\'Category\')[\'Rating\'].agg([\'mean\', \'std\']).sort_values(by=\'mean\', ascending=False).reset_index() plt.figure(figsize=(10, 6)) sns.barplot(x=\'mean\', y=\'Category\', data=category_avg_rating, ci=None) plt.errorbar(category_avg_rating[\'mean\'], range(len(category_avg_rating)), xerr=category_avg_rating[\'std\'], fmt=\'o\', color=\'r\', ecolor=\'red\') plt.xlabel(\'Average Rating\') plt.ylabel(\'Category\') plt.title(\'Average Rating by Category\') plt.show() # Step 2: Horizontal bar plot showing the sum of ratings for each Restaurant colored by Category restaurant_sum_rating = df.groupby([\'Restaurant\', \'Category\'])[\'Rating\'].sum().reset_index().sort_values(by=\'Rating\', ascending=False) plt.figure(figsize=(12, 8)) sns.barplot(x=\'Rating\', y=\'Restaurant\', hue=\'Category\', data=restaurant_sum_rating, dodge=False) plt.xlabel(\'Sum of Ratings\') plt.ylabel(\'Restaurant\') plt.title(\'Sum of Ratings by Restaurant\') plt.legend(title=\'Category\') plt.show() # Step 3: Faceted bar plot for average price by restaurant within each cuisine category g = sns.catplot( x=\'Restaurant\', y=\'Price\', col=\'Category\', data=df, kind=\'bar\', col_wrap=2, ci=\'sd\', height=4, aspect=1.5, palette=\'muted\', errwidth=1 ) g.set_titles(\'{col_name}\') g.set_axis_labels(\'Restaurant\', \'Average Price\') g.set_xticklabels(rotation=45, horizontalalignment=\'right\') g.fig.subplots_adjust(top=0.9) g.fig.suptitle(\'Average Price by Restaurant within Each Category\') plt.show()"},{"question":"**Problem Description:** In this coding assessment, you will demonstrate your understanding of Python keywords and soft keywords using the `keyword` module. Your task is to implement a function that takes a list of strings and classifies each string into one of three categories: `keyword`, `soft_keyword`, or `not_a_keyword`. Additionally, you will provide a summary of the counts of each category. **Function Signature:** ```python def classify_keywords(strings: list) -> dict: Classifies a list of strings into \'keyword\', \'soft_keyword\', or \'not_a_keyword\' and returns a summary of the counts of each category. Parameters: strings (list): A list of strings to be classified. Returns: dict: A dictionary with the classification counts in the format: {\'keyword\': <count>, \'soft_keyword\': <count>, \'not_a_keyword\': <count>} pass ``` **Input:** - A list of strings, `strings`, where each string represents a word to be checked. **Output:** - A dictionary with three keys: `keyword`, `soft_keyword`, and `not_a_keyword`. The values corresponding to these keys are the counts of strings in each category. **Constraints:** - The input list will have at most 1000 strings. - Each string in the input list will be non-empty and alphabetic. **Example:** ```python assert classify_keywords([\'for\', \'async\', \'await\', \'hello\', \'if\']) == {\'keyword\': 3, \'soft_keyword\': 2, \'not_a_keyword\': 0} assert classify_keywords([\'hello\', \'world\', \'is\', \'not\', \'a\', \'keyword\']) == {\'keyword\': 0, \'soft_keyword\': 0, \'not_a_keyword\': 6} ``` **Notes:** - You should make use of the `keyword.iskeyword`, `keyword.issoftkeyword`, `keyword.kwlist`, and `keyword.softkwlist` functions and lists where appropriate. - Pay attention to performance since the input list can be quite large (up to 1000 strings). **Task:** Implement the `classify_keywords` function in Python.","solution":"import keyword def classify_keywords(strings: list) -> dict: classification = {\'keyword\': 0, \'soft_keyword\': 0, \'not_a_keyword\': 0} for string in strings: if keyword.iskeyword(string): classification[\'keyword\'] += 1 elif keyword.issoftkeyword(string): classification[\'soft_keyword\'] += 1 else: classification[\'not_a_keyword\'] += 1 return classification"},{"question":"# Assessing Proficiency with Numeric and Mathematical Modules in Python Problem Statement Your task is to implement a function `complex_operations` that takes in two complex numbers as input and returns a dictionary containing various computed properties using the functionalities provided by the `math`, `cmath`, and `statistics` modules in Python. # Function Signature ```python def complex_operations(a: complex, b: complex) -> dict: ``` # Input 1. `a` (complex): A complex number. 2. `b` (complex): Another complex number. # Output Returns a dictionary with the following keys and their corresponding values: 1. `\\"distance\\"`: The Euclidean distance between the real parts of `a` and `b`. 2. `\\"phase_difference\\"`: The phase difference between the complex numbers `a` and `b`. 3. `\\"polar_a\\"`: The polar coordinates representation of `a`. 4. `\\"polar_b\\"`: The polar coordinates representation of `b`. 5. `\\"mean_phase\\"`: The mean of the phases of `a` and `b` (use the `statistics` module). 6. `\\"hypotenuse\\"`: The hypotenuse calculated using the real parts of both `a` and `b`. # Requirements 1. Ensure that you use the correct mathematical functions from the `math`, `cmath`, and `statistics` modules. 2. Handle the use of complex number operations appropriately. 3. Your code should be efficient and make use of the provided modules cleverly. # Example ```python def complex_operations(a, b): import cmath import math import statistics distance = math.dist([a.real], [b.real]) phase_diff = cmath.phase(a) - cmath.phase(b) polar_a = cmath.polar(a) polar_b = cmath.polar(b) mean_phase = statistics.mean([cmath.phase(a), cmath.phase(b)]) hypotenuse = math.hypot(a.real, b.real) return { \\"distance\\": distance, \\"phase_difference\\": phase_diff, \\"polar_a\\": polar_a, \\"polar_b\\": polar_b, \\"mean_phase\\": mean_phase, \\"hypotenuse\\": hypotenuse } # Example inputs a = 3 + 4j b = 1 + 2j print(complex_operations(a, b)) ``` # Constraints 1. Complex numbers `a` and `b` will have non-zero imaginary parts. 2. The values will be within a reasonable range to avoid overflow issues. # Notes - Use `cmath` for handling complex-specific operations like computing phase and polar coordinates. - Use `math` for all real number operations like calculating distance and hypotenuse. - Use `statistics` for statistical computations like calculating the mean of phases.","solution":"import cmath import math import statistics def complex_operations(a: complex, b: complex) -> dict: Performs various computations between two complex numbers a and b. Returns a dictionary with keys: - \\"distance\\": Euclidean distance between the real parts of a and b - \\"phase_difference\\": Phase difference between the complex numbers a and b - \\"polar_a\\": Polar coordinates representation of a - \\"polar_b\\": Polar coordinates representation of b - \\"mean_phase\\": Mean of the phases of a and b - \\"hypotenuse\\": Hypotenuse calculated using the real parts of both a and b # Euclidean distance between the real parts distance = math.dist([a.real], [b.real]) # Phase difference phase_difference = cmath.phase(a) - cmath.phase(b) # Polar coordinates polar_a = cmath.polar(a) polar_b = cmath.polar(b) # Mean of phases mean_phase = statistics.mean([cmath.phase(a), cmath.phase(b)]) # Hypotenuse hypotenuse = math.hypot(a.real, b.real) return { \\"distance\\": distance, \\"phase_difference\\": phase_difference, \\"polar_a\\": polar_a, \\"polar_b\\": polar_b, \\"mean_phase\\": mean_phase, \\"hypotenuse\\": hypotenuse }"},{"question":"Coding Assessment Question # Title: Implementing and Evaluating Naive Bayes Classifier Objective: Implement different types of Naive Bayes classifiers and evaluate their performance on a provided dataset. This will demonstrate your understanding of various Naive Bayes classifiers available in scikit-learn and when to use each type. Instructions: 1. You are provided with a dataset `dataset.csv` which contains columns `feature_1, feature_2, ..., feature_n, class_label`. The type of data in each column can vary. 2. Your task is to implement and compare the following Naive Bayes classifiers from scikit-learn: - GaussianNB - MultinomialNB - ComplementNB - BernoulliNB - CategoricalNB 3. For each classifier: - Split the dataset into training and test sets (80% training and 20% testing). - Train the classifier on the training set. - Predict the class labels for the test set. - Calculate and print the accuracy of the classifier. - Print any observations or conclusions about the performance of each classifier based on the type of data. 4. Handle any necessary preprocessing steps such as binarizing data for BernoulliNB or encoding categories for CategoricalNB. 5. Compare the results and identify which classifier performed best for this dataset, explaining why that might be the case. Expected Input and Output Formats: **Input:** - A CSV file `dataset.csv` with `feature_1, feature_2, ..., feature_n, class_label`. **Output:** - Accuracy of each classifier on the test set. - Observations on the performance of each classifier. Constraints: - Use scikit-learn for implementing the classifiers. - Handle possible exceptions or data preprocessing steps as needed. - Performance should be considered in terms of accuracy. Example: ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB from sklearn.preprocessing import OrdinalEncoder, Binarizer from sklearn.metrics import accuracy_score # Load dataset data = pd.read_csv(\'dataset.csv\') # Splitting features and labels X = data.drop(\'class_label\', axis=1) y = data[\'class_label\'] # Splitting into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize classifiers classifiers = { \\"GaussianNB\\": GaussianNB(), \\"MultinomialNB\\": MultinomialNB(), \\"ComplementNB\\": ComplementNB(), \\"BernoulliNB\\": BernoulliNB(), \\"CategoricalNB\\": CategoricalNB() } # Preprocess data for different classifiers binarizer = Binarizer() ordinal_encoder = OrdinalEncoder() # Dictionary to store accuracies accuracies = {} for name, clf in classifiers.items(): if name == \\"BernoulliNB\\": X_train_bin = binarizer.fit_transform(X_train) X_test_bin = binarizer.transform(X_test) clf.fit(X_train_bin, y_train) y_pred = clf.predict(X_test_bin) elif name == \\"CategoricalNB\\": X_train_cat = ordinal_encoder.fit_transform(X_train) X_test_cat = ordinal_encoder.transform(X_test) clf.fit(X_train_cat, y_train) y_pred = clf.predict(X_test_cat) else: clf.fit(X_train, y_train) y_pred = clf.predict(X_test) # Calculate accuracy accuracy = accuracy_score(y_test, y_pred) accuracies[name] = accuracy print(f\\"{name} Accuracy: {accuracy:.2f}\\") # Observations and conclusions can be printed here based on above results. ``` Make sure to explain any preprocessing steps and why they are necessary for the specific classifiers used.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB from sklearn.preprocessing import OrdinalEncoder, Binarizer from sklearn.metrics import accuracy_score def load_and_preprocess_data(file_path): data = pd.read_csv(file_path) X = data.drop(\'class_label\', axis=1) y = data[\'class_label\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def evaluate_classifiers(X_train, X_test, y_train, y_test): classifiers = { \\"GaussianNB\\": GaussianNB(), \\"MultinomialNB\\": MultinomialNB(), \\"ComplementNB\\": ComplementNB(), \\"BernoulliNB\\": BernoulliNB(), \\"CategoricalNB\\": CategoricalNB() } binarizer = Binarizer() ordinal_encoder = OrdinalEncoder() accuracies = {} for name, clf in classifiers.items(): if name == \\"BernoulliNB\\": X_train_bin = binarizer.fit_transform(X_train) X_test_bin = binarizer.transform(X_test) clf.fit(X_train_bin, y_train) y_pred = clf.predict(X_test_bin) elif name == \\"CategoricalNB\\": X_train_cat = ordinal_encoder.fit_transform(X_train) X_test_cat = ordinal_encoder.transform(X_test) clf.fit(X_train_cat, y_train) y_pred = clf.predict(X_test_cat) else: clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) accuracies[name] = accuracy print(f\\"{name} Accuracy: {accuracy:.2f}\\") return accuracies if __name__ == \\"__main__\\": X_train, X_test, y_train, y_test = load_and_preprocess_data(\'dataset.csv\') evaluate_classifiers(X_train, X_test, y_train, y_test)"},{"question":"# Pandas Optimization Task You are provided with a directory of parquet files, each containing a large dataset of timeseries data. Your task is to write a function that efficiently processes these files to calculate summary statistics and optimize memory usage. The function should demonstrate: 1. Efficient data loading by selecting specific columns. 2. Optimizing data types to reduce memory usage. 3. Using chunking to handle large datasets incrementally. Provided Data Each parquet file in the directory contains a timeseries dataset with the following columns: - `timestamp`: datetime values representing the timestamp of the data point. - `name`: string values representing the name of an individual. - `id`: integer values representing a unique ID. - `x`: float values representing a measurement in the x-dimension. - `y`: float values representing a measurement in the y-dimension. Function Signature ```python import pandas as pd from pathlib import Path def process_timeseries_data(directory: str) -> pd.DataFrame: pass ``` Input - `directory` (str): The path to the directory containing parquet files. Output - A `pandas.DataFrame` with the following summary statistics: - The total number of unique names. - The mean value of `x` and `y` for each unique name. - The total count of entries for each unique name. - Memory usage before and after optimization. Constraints 1. You should demonstrate the use of efficient data types to minimize memory usage. 2. You must handle loading and processing the datasets incrementally. 3. Your solution should be efficient in terms of memory and processing time. Example ```python result = process_timeseries_data(\\"data/timeseries\\") print(result) ``` The result should be a DataFrame with columns `name`, `mean_x`, `mean_y`, `count`, `initial_memory_usage`, and `optimized_memory_usage`. Implementation Notes - You should use `pandas.read_parquet` to load the data. - Convert the `name` column to a categorical type and downcast numeric columns to their smallest types. - Implement chunking to process each file individually and then aggregate the results.","solution":"import pandas as pd from pathlib import Path def process_timeseries_data(directory: str) -> pd.DataFrame: data_path = Path(directory) # Variables to store aggregated results summaries = [] before_mem_usage = 0 after_mem_usage = 0 # Process files in chunks for file_path in data_path.glob(\\"*.parquet\\"): chunks = pd.read_parquet(file_path, columns=[\\"timestamp\\", \\"name\\", \\"x\\", \\"y\\"]) # Calculate initial memory usage before_mem_usage += chunks.memory_usage(deep=True).sum() # Optimize datatypes chunks[\\"name\\"] = chunks[\\"name\\"].astype(\\"category\\") chunks[\\"x\\"] = pd.to_numeric(chunks[\\"x\\"], downcast=\\"float\\") chunks[\\"y\\"] = pd.to_numeric(chunks[\\"y\\"], downcast=\\"float\\") # Calculate optimized memory usage after_mem_usage += chunks.memory_usage(deep=True).sum() # Calculate summary statistics summary = chunks.groupby(\\"name\\").agg( mean_x=(\\"x\\", \\"mean\\"), mean_y=(\\"y\\", \\"mean\\"), count=(\\"timestamp\\", \\"count\\") ).reset_index() summaries.append(summary) # Combine all summaries into a single DataFrame combined_summary = pd.concat(summaries, ignore_index=True) # Final aggregation to merge results from individual files final_summary = combined_summary.groupby(\\"name\\").agg( mean_x=(\\"mean_x\\", \\"mean\\"), mean_y=(\\"mean_y\\", \\"mean\\"), count=(\\"count\\", \\"sum\\") ).reset_index() # Adding memory usage information final_summary[\\"initial_memory_usage\\"] = before_mem_usage final_summary[\\"optimized_memory_usage\\"] = after_mem_usage return final_summary"},{"question":"You have been tasked with creating a utility function to analyze Unix group data using the `grp` module. # Problem Statement Implement a function `analyze_groups(group_name: str) -> dict` that accepts a group name, retrieves the group entry using the `grp` module, and returns a dictionary with the following structure: ```python { \\"group_name\\": str, \\"group_id\\": int, \\"member_count\\": int, \\"members\\": list[str] } ``` # Specifications 1. The dictionary should contain: - `group_name`: The name of the group (gr_name). - `group_id`: The numerical group ID (gr_gid). - `member_count`: The number of members in the group (length of gr_mem list). - `members`: A list of members\' user names in the group (gr_mem). 2. If the group does not exist, the function should handle `KeyError` and return `None`. 3. The function should ensure all inputs and outputs adhere to the expected formats and types. # Constraints and Considerations - The function will be used in a Unix environment where the `grp` module is available. - The function should handle possible exceptions gracefully and provide meaningful feedback through exceptions. - Use of Python 3.10 or later is expected, so ensure your solution is compatible with this version\'s features and changes. # Examples: ```python # Example group data might include # gr_name: \\"admin\\" # gr_passwd: \\"x\\" # gr_gid: 1001 # gr_mem: [\\"user1\\", \\"user2\\"] # Assume \\"admin\\" is a valid group in the system result = analyze_groups(\\"admin\\") print(result) # Expected Output: # { # \\"group_name\\": \\"admin\\", # \\"group_id\\": 1001, # \\"member_count\\": 2, # \\"members\\": [\\"user1\\", \\"user2\\"] # } # Assume \\"nonexistent\\" is not a valid group in the system result = analyze_groups(\\"nonexistent\\") print(result) # Expected Output: # None ``` # Notes - Make sure to import the `grp` module at the beginning of your implementation. - Document any assumptions and edge cases you consider in your solution.","solution":"import grp def analyze_groups(group_name: str) -> dict: Retrieves the group entry for a given group name and returns detailed information. :param group_name: The name of the group to retrieve information for. :return: A dictionary with group details, or None if the group does not exist. try: group_info = grp.getgrnam(group_name) return { \\"group_name\\": group_info.gr_name, \\"group_id\\": group_info.gr_gid, \\"member_count\\": len(group_info.gr_mem), \\"members\\": group_info.gr_mem } except KeyError: return None"},{"question":"Objective Design a function using the pandas library to analyze the stock market data. The function will utilize rolling and exponentially-weighted moving window functions to calculate key statistical measures. Problem Statement You are given a DataFrame `df` with stock prices of a company over time. The DataFrame contains two columns: `Date` and `Close Price`, with `Date` as the index in datetime format. Write a function `analyze_stock_data` to perform the following tasks: 1. Calculate a 7-day rolling mean of the `Close Price`. 2. Calculate a 14-day rolling standard deviation of the `Close Price`. 3. Calculate an exponential moving average (EMA) of the `Close Price` with a span of 10 days. 4. Create a new DataFrame with the original `Date` and `Close Price` columns, and three new columns: `7-day Mean`, `14-day Std Dev`, and `10-day EMA`. Function Signature ```python import pandas as pd def analyze_stock_data(df: pd.DataFrame) -> pd.DataFrame: Analyzes stock data using rolling and exponentially-weighted window functions. Parameters: df (pd.DataFrame): DataFrame containing \'Date\' and \'Close Price\' columns with \'Date\' as index. Returns: pd.DataFrame: DataFrame with original \'Date\' and \'Close Price\' columns and additional statistical columns. pass ``` Input - `df`: A pandas DataFrame with the following structure: ```plaintext Close Price Date 2021-01-01 150.45 2021-01-02 152.35 2021-01-03 153.78 ... ``` Output - A pandas DataFrame containing the original `Date` and `Close Price` columns, along with: - `7-day Mean`: 7-day rolling mean of the `Close Price`. - `14-day Std Dev`: 14-day rolling standard deviation of the `Close Price`. - `10-day EMA`: 10-day exponentially weighted moving average of the `Close Price`. Constraints - The input DataFrame can have any number of rows, but it will always have a `Date` index and a `Close Price` column. - Performance considerations: Aim to utilize pandas efficient calculations to handle potentially large datasets. Example ```python # Example usage import pandas as pd data = { \'Date\': pd.date_range(start=\'2021-01-01\', periods=20, freq=\'D\'), \'Close Price\': [150.45, 152.35, 153.78, 154.23, 155.59, 157.30, 158.22, 159.28, 160.15, 161.24, 162.55, 163.60, 164.78, 165.99, 167.15, 168.20, 168.90, 169.78, 170.10, 171.45] } df = pd.DataFrame(data) df.set_index(\'Date\', inplace=True) result_df = analyze_stock_data(df) print(result_df) ``` The `result_df` should include the new calculated columns with appropriate values based on the input data.","solution":"import pandas as pd def analyze_stock_data(df: pd.DataFrame) -> pd.DataFrame: Analyzes stock data using rolling and exponentially-weighted window functions. Parameters: df (pd.DataFrame): DataFrame containing \'Date\' and \'Close Price\' columns with \'Date\' as index. Returns: pd.DataFrame: DataFrame with original \'Date\' and \'Close Price\' columns and additional statistical columns. df[\'7-day Mean\'] = df[\'Close Price\'].rolling(7).mean() df[\'14-day Std Dev\'] = df[\'Close Price\'].rolling(14).std() df[\'10-day EMA\'] = df[\'Close Price\'].ewm(span=10, adjust=False).mean() return df"},{"question":"Asyncio Advanced Queue Management You are required to implement a function using `asyncio.Queue` to simulate a series of customers arriving at a service center with different priorities. The service center has a limited number of attendants who process the customers based on their priorities. Requirements: - Implement an asynchronous function `process_customers(num_attendants: int, customers: List[Tuple[int, str]] -> None)`. - `num_attendants` specifies the number of attendant tasks processing the customers. - `customers` is a list of tuples where each tuple contains: - A priority number (lower values indicate higher priority). - A customer name (string). Key Points: 1. Use `asyncio.PriorityQueue` for managing the customers based on their priority. 2. Each attendant should continuously process customers from the priority queue until all customers are handled. 3. Each customer takes a random time (between 0.1 and 0.5 seconds) to be served by the attendant. 4. Print a message in the format `\\"{attendant_name} served {customer_name} with priority {priority}\\"` when a customer is served. 5. Ensure the main function runs until all customers are served and all attendant tasks are completed. Constraints: - The priorities are integers ranging from 0 (highest priority) to any positive integer. - The length of `customers` is at least 1 and at most 100. - You are allowed to use the `random` module for sleep timings. Example: ```python import asyncio import random from typing import List, Tuple async def process_customers(num_attendants: int, customers: List[Tuple[int, str]]) -> None: queue = asyncio.PriorityQueue() for priority, customer in customers: await queue.put((priority, customer)) async def attendant(name: str): while not queue.empty(): priority, customer = await queue.get() await asyncio.sleep(random.uniform(0.1, 0.5)) print(f\\"{name} served {customer} with priority {priority}\\") queue.task_done() tasks = [asyncio.create_task(attendant(f\\"attendant-{i}\\")) for i in range(num_attendants)] await queue.join() # Wait until all customers are processed. for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) # Example usage, assume this code runs in an asyncio event loop: asyncio.run(process_customers(2, [(2, \\"Alice\\"), (1, \\"Bob\\"), (3, \\"Charlie\\")])) ``` Note: In this question, you need to ensure proper handling of queue\'s lifecycle, including task completion and possible cancellations, when all customers are processed.","solution":"import asyncio import random from typing import List, Tuple async def process_customers(num_attendants: int, customers: List[Tuple[int, str]]) -> None: queue = asyncio.PriorityQueue() # Put all customers into the queue with their priorities. for priority, customer in customers: await queue.put((priority, customer)) async def attendant(name: str): while not queue.empty(): priority, customer = await queue.get() await asyncio.sleep(random.uniform(0.1, 0.5)) print(f\\"{name} served {customer} with priority {priority}\\") queue.task_done() # Create and start attendant tasks. tasks = [asyncio.create_task(attendant(f\\"attendant-{i}\\")) for i in range(num_attendants)] # Wait for the queue to be fully processed. await queue.join() for task in tasks: task.cancel() # Gather the canceled tasks await asyncio.gather(*tasks, return_exceptions=True) # Example usage, assume this code runs in an asyncio event loop: # asyncio.run(process_customers(2, [(2, \\"Alice\\"), (1, \\"Bob\\"), (3, \\"Charlie\\")]))"},{"question":"Objective You are to create a custom Python script that utilizes the `venv` module\'s `EnvBuilder` class to create virtual environments with specific requirements. This exercise aims to assess your understanding of Python virtual environments, your ability to extend classes, manage directories, and install packages programmatically. Task 1. Create a class `CustomEnvBuilder` that extends `venv.EnvBuilder`. 2. Implement the following functionalities: - Ensure that a specific version of Python is being used to create the virtual environment. - Install a predefined set of Python packages from a `requirements.txt` file into the virtual environment after creation. - Add an optional custom script to be installed in the virtual environment. This script should output \\"Hello, Virtual Environment!\\" when run. Requirements - Your script should create a virtual environment in a specified directory using `CustomEnvBuilder`. - It must check the Python version and only proceed if the version is 3.7 or greater. - It should install the packages listed in a `requirements.txt` file located in the same directory as your script. - Provide a custom script named `hello_venv.py` in the virtual environment\'s `bin` (or `Scripts` on Windows) directory, which prints \\"Hello, Virtual Environment!\\" when executed. Input - A path to the directory where the virtual environment will be created. - `requirements.txt` should be located in your current working directory and contain package names that need to be installed. Output - The script should print successes or failures at each significant step (e.g., environment creation, package installation, script installation). Constraints - Assume Python 3.7 or higher is installed on the system. - You cannot use external libraries except for those available in the standard Python library. Example Provide a `requirements.txt` file as follows: ``` requests flask ``` Your script should be executed with: ``` python create_custom_env.py /path/to/new/virtual/environment ``` Expected output in the console: ``` Creating virtual environment... Virtual environment created successfully. Checking Python version... Python version is compatible. Installing packages... Packages installed successfully. Creating custom script... Custom script created successfully. ``` # Hints - Use the `sys` module to check the Python version. - Use the `subprocess` module to install packages via `pip` within the virtual environment. - Remember to handle exceptions and edge cases appropriately to ensure reliable execution of the script. ```python import venv import sys import subprocess import os class CustomEnvBuilder(venv.EnvBuilder): def __init__(self, **kwargs): super().__init__(**kwargs) def check_python_version(self): if sys.version_info < (3, 7): raise ValueError(\\"Python 3.7 or higher is required to create this virtual environment.\\") def post_setup(self, context): self.install_requirements(context) self.create_custom_script(context) def install_requirements(self, context): req_file = \'requirements.txt\' if os.path.exists(req_file): subprocess.check_call([context.env_exe, \'-m\', \'pip\', \'install\', \'-r\', req_file]) print(\\"Packages installed successfully.\\") else: print(f\\"\'{req_file}\' not found. No packages were installed.\\") def create_custom_script(self, context): script_content = \'print(\\"Hello, Virtual Environment!\\")\' bin_path = context.bin_path if hasattr(context, \'bin_path\') else context.scripts_path script_path = os.path.join(bin_path, \'hello_venv.py\') with open(script_path, \'w\') as script_file: script_file.write(script_content) print(\\"Custom script created successfully.\\") def create_custom_env(env_dir): builder = CustomEnvBuilder(with_pip=True) builder.check_python_version() builder.create(env_dir) print(\\"Virtual environment created successfully.\\") if __name__ == \\"__main__\\": if len(sys.argv) != 2: print(\\"Usage: python create_custom_env.py /path/to/new/virtual/environment\\") else: env_path = sys.argv[1] create_custom_env(env_path) ```","solution":"import venv import sys import subprocess import os class CustomEnvBuilder(venv.EnvBuilder): def __init__(self, **kwargs): super().__init__(**kwargs) def check_python_version(self): if sys.version_info < (3, 7): raise ValueError(\\"Python 3.7 or higher is required to create this virtual environment.\\") def post_setup(self, context): self.install_requirements(context) self.create_custom_script(context) def install_requirements(self, context): req_file = \'requirements.txt\' if os.path.exists(req_file): subprocess.check_call([context.env_exe, \'-m\', \'pip\', \'install\', \'-r\', req_file]) print(\\"Packages installed successfully.\\") else: print(f\\"\'{req_file}\' not found. No packages were installed.\\") def create_custom_script(self, context): script_content = \'print(\\"Hello, Virtual Environment!\\")\' bin_path = context.bin_path if hasattr(context, \'bin_path\') else context.scripts_path script_path = os.path.join(bin_path, \'hello_venv.py\') with open(script_path, \'w\') as script_file: script_file.write(script_content) print(\\"Custom script created successfully.\\") def create_custom_env(env_dir): builder = CustomEnvBuilder(with_pip=True) builder.check_python_version() builder.create(env_dir) print(\\"Virtual environment created successfully.\\") if __name__ == \\"__main__\\": if len(sys.argv) != 2: print(\\"Usage: python create_custom_env.py /path/to/new/virtual/environment\\") else: env_path = sys.argv[1] create_custom_env(env_path)"},{"question":"# Seaborn Styling and Visualization Assessment **Objective:** Demonstrate your understanding of using Seaborn for data visualization with a focus on customizing plot styles. **Task:** You are provided with a dataset and you need to perform the following tasks: 1. Load the dataset using Seaborn\'s `load_dataset` function. 2. Create a visualization showing the relationship between two variables in the dataset. 3. Apply a specific Seaborn style to the plot. 4. Temporarily change the style of the plot within a context manager and create a different visualization. 5. Save both plots to separate image files. **Dataset:** Use Seaborn’s built-in dataset named `\\"tips\\"` for this task. **Specifications:** 1. **Function Name:** `styled_visualizations` 2. **Input:** None 3. **Output:** None 4. **Constraints:** - The first plot should use the `darkgrid` style. - The second plot should use the `whitegrid` style, applied temporarily within a context manager. 5. **Performance Requirements:** - Efficiently load and handle the dataset. - Ensure plots are clear and well-labeled. # Steps: 1. Load the dataset: ```python tips = sns.load_dataset(\\"tips\\") ``` 2. Create the first plot (e.g., scatter plot of `total_bill` vs `tip`) with the `darkgrid` style and save it as `plot1.png`. 3. Use a context manager to create the second plot (e.g., bar plot showing average `total_bill` for each `day`) with the `whitegrid` style and save it as `plot2.png`. # Example Function Definition: ```python import seaborn as sns import matplotlib.pyplot as plt def styled_visualizations(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Set style to darkgrid sns.set_style(\\"darkgrid\\") plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.title(\\"Scatter plot of Total Bill vs Tip\\") plt.savefig(\\"plot1.png\\") plt.close() # Use whitegrid style within a context manager with sns.axes_style(\\"whitegrid\\"): plt.figure(figsize=(10, 6)) sns.barplot(data=tips, x=\\"day\\", y=\\"total_bill\\", estimator=sum) plt.title(\\"Bar plot of Total Bill by Day\\") plt.savefig(\\"plot2.png\\") plt.close() # Call the function to create and save the plots styled_visualizations() ``` Make sure to test your function to verify that it creates the desired plots and saves them to the specified files.","solution":"import seaborn as sns import matplotlib.pyplot as plt def styled_visualizations(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Set style to darkgrid sns.set_style(\\"darkgrid\\") plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.title(\\"Scatter plot of Total Bill vs Tip\\") plt.savefig(\\"plot1.png\\") plt.close() # Use whitegrid style within a context manager with sns.axes_style(\\"whitegrid\\"): plt.figure(figsize=(10, 6)) sns.barplot(data=tips, x=\\"day\\", y=\\"total_bill\\", estimator=sum) plt.title(\\"Bar plot of Total Bill by Day\\") plt.savefig(\\"plot2.png\\") plt.close()"},{"question":"Send and Verify Multipart Email **Objective**: Demonstrate your understanding of Python\'s `email` package by constructing, sending, and verifying a multipart email containing different types of content. # Problem Statement You are required to write a function `send_multipart_email(sender: str, recipients: list, subject: str, text_content: str, html_content: str, attachments: list) -> bool` which sends an email with multiple parts (text and HTML) and various attachments. You will also write a function `verify_email(file_path: str) -> dict` to verify that the email message has been correctly formatted and saved. # Instructions 1. **send_multipart_email** function: - **Input**: - `sender`: (str) Email address of the sender. - `recipients`: (list) List of recipient email addresses. - `subject`: (str) Subject of the email. - `text_content`: (str) Plain text content of the email. - `html_content`: (str) HTML content of the email. - `attachments`: (list) List of file paths to attach to the email. - **Output**: - Returns `True` if the email is sent successfully, otherwise `False`. - **Constraints**: - Ensure that email has both text and HTML parts. - Attachments can include different file types like images, PDFs, etc. - Save a local copy of the email in a file named `sent_email.eml`. - **Performance**: - Ensure the email is sent within a reasonable time frame. 2. **verify_email** function: - **Input**: - `file_path`: (str) Path to the saved email file. - **Output**: - Returns a dictionary with the following keys: `subject`, `from`, `to`, `text_content`, `html_content`, and `attachments_paths` to verify the email\'s contents. - **Constraints**: - Ensure accuracy in parsing and extracting email content. # Example ```python # Example usage of send_multipart_email send_multipart_email( sender=\\"sender@example.com\\", recipients=[\\"recipient1@example.com\\", \\"recipient2@example.com\\"], subject=\\"Meeting Agenda\\", text_content=\\"Please find attached the agenda for our upcoming meeting.\\", html_content=\\"<html><body><p>Please find attached the <b>agenda</b> for our upcoming meeting.</p></body></html>\\", attachments=[\\"./agenda.pdf\\", \\"./image.png\\"] ) # Example usage of verify_email details = verify_email(\\"sent_email.eml\\") print(details) ``` # Hints - Use `EmailMessage` for constructing the email with multipart. - Use `BytesParser` for reading and parsing the saved email file. - Use `open` in binary mode for reading attachments and saving the email file. - Ensure proper exception handling for file operations and SMTP transactions. Good Luck!","solution":"import smtplib from email import encoders from email.mime.base import MIMEBase from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.parser import BytesParser from email.policy import default import os def send_multipart_email(sender, recipients, subject, text_content, html_content, attachments): Send an email with both text and HTML parts and various attachments. :param sender: Email address of the sender :param recipients: List of recipient email addresses :param subject: Subject of the email :param text_content: Plain text content of the email :param html_content: HTML content of the email :param attachments: List of file paths to attach to the email :return: True if email is sent successfully, otherwise False try: # Create the root message msg = MIMEMultipart(\\"alternative\\") msg[\\"From\\"] = sender msg[\\"To\\"] = \\", \\".join(recipients) msg[\\"Subject\\"] = subject # Attach the text and HTML parts part1 = MIMEText(text_content, \\"plain\\") part2 = MIMEText(html_content, \\"html\\") msg.attach(part1) msg.attach(part2) # Attach the files for file_path in attachments: part = MIMEBase(\\"application\\", \\"octet-stream\\") with open(file_path, \\"rb\\") as file: part.set_payload(file.read()) encoders.encode_base64(part) part.add_header(\\"Content-Disposition\\", f\\"attachment; filename={os.path.basename(file_path)}\\") msg.attach(part) # Save the email to a file with open(\\"sent_email.eml\\", \\"wb\\") as f: f.write(msg.as_bytes()) # Here you would normally send the email using an SMTP server # For demonstration purposes, we just print the email content # server = smtplib.SMTP(\'localhost\') # server.sendmail(sender, recipients, msg.as_string()) # server.quit() return True except Exception as e: print(f\\"An error occurred: {e}\\") return False def verify_email(file_path): Verify the contents of an email file. :param file_path: Path to the saved email file :return: Dictionary with email contents with open(file_path, \\"rb\\") as f: msg = BytesParser(policy=default).parse(f) subject = msg[\\"subject\\"] sender = msg[\\"from\\"] recipients = msg[\\"to\\"] # Extracting the text and HTML parts text_content = \\"\\" html_content = \\"\\" attachments = [] for part in msg.iter_parts(): if part.get_content_type() == \\"text/plain\\": text_content = part.get_payload(decode=True).decode(part.get_content_charset()) elif part.get_content_type() == \\"text/html\\": html_content = part.get_payload(decode=True).decode(part.get_content_charset()) elif part.get_content_disposition() == \\"attachment\\": attachments.append(part.get_filename()) return { \\"subject\\": subject, \\"from\\": sender, \\"to\\": recipients, \\"text_content\\": text_content, \\"html_content\\": html_content, \\"attachments_paths\\": attachments }"},{"question":"# Question: Implement a PyTorch-Compatible Neural Network As a PyTorch developer, you need to implement a neural network and ensure that it is compatible with TorchScript. TorchScript is a way to create serializable and optimizable models from PyTorch code. Some constructs in PyTorch, as mentioned in the documentation, are not compatible with TorchScript. Task Implement a simple neural network class in PyTorch that consists of: 1. An input layer with a specified number of input features. 2. One hidden layer with 50 neurons using ReLU activation. 3. An output layer with a specified number of output features without any activation function (for simplicity). Then, write a function to convert this model to TorchScript using `torch.jit.script` and handle any potential issues related to the constructs mentioned in the documentation. Constraints and Details 1. Do not use modules that are not supported by TorchScript, such as `torch.nn.RNN`. 2. Do not use functions that TorchScript cannot compile as specified in the provided documentation. 3. Ensure that your neural network code does not use any unsupported tensor initialization methods or constructs. 4. Provide sufficient comments in your code to explain any workarounds or important details relevant to TorchScript compatibility. Input - `input_features`: Integer representing the number of input features. - `output_features`: Integer representing the number of output features. Output - Return the TorchScript version of your model. Example ```python model_script = get_torchscript_model(10, 2) print(model_script) # Expected output: <torch.jit.ScriptModule ...> ``` Your Implementation ```python import torch import torch.nn as nn class SimpleNN(nn.Module): def __init__(self, input_features, output_features): super(SimpleNN, self).__init__() self.input_features = input_features self.output_features = output_features # Defining the layers self.fc1 = nn.Linear(input_features, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, output_features) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def get_torchscript_model(input_features, output_features): # Instantiate the model model = SimpleNN(input_features, output_features) # Convert the model to TorchScript model_script = torch.jit.script(model) return model_script ``` Test your function with various inputs and ensure that it returns a TorchScript model without any issues.","solution":"import torch import torch.nn as nn class SimpleNN(nn.Module): def __init__(self, input_features, output_features): super(SimpleNN, self).__init__() self.input_features = input_features self.output_features = output_features # Defining the layers self.fc1 = nn.Linear(input_features, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, output_features) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def get_torchscript_model(input_features, output_features): # Instantiate the model model = SimpleNN(input_features, output_features) # Convert the model to TorchScript model_script = torch.jit.script(model) return model_script"},{"question":"# Question: You are required to write a Python function utilizing the `zlib` module that compresses and then decompresses a given byte string, ensuring data integrity by comparing checksums. Specifically, implement the function `compress_and_decompress(data: bytes, level: int = -1) -> bool`. Function Signature: ```python def compress_and_decompress(data: bytes, level: int = -1) -> bool: pass ``` Input: - `data` (bytes): The input byte string that needs to be compressed and decompressed. - `level` (int, optional): The compression level, which is an integer between `0` and `9`, or `-1` for the default compression. Output: - `bool`: A boolean value indicating whether the original data matches the decompressed data (i.e., data integrity is maintained). Constraints: 1. Do not use the `gzip` module, only `zlib`. 2. Ensure that the function should handle any data input up to 1MB efficiently. 3. Your solution should raise the appropriate errors in case of any issues encountered during compression or decompression. Example: ```python data = b\\"example string to compress and decompress\\" result = compress_and_decompress(data) print(result) # Output: True ``` Details: 1. Calculate Adler-32 checksums before and after the compression-decompression cycle to confirm data integrity. 2. Use the `compress` function for compression and the `decompress` function for decompression. 3. Handle all possible exceptions that might occur during compression and decompression. Additional Notes: - Refer to the provided [zlib documentation](https://www.zlib.net/manual.html) for any additional information needed. - Ensure your code is clean, well-documented, and follows Python best practices.","solution":"import zlib def compress_and_decompress(data: bytes, level: int = -1) -> bool: Compresses and then decompresses a given byte string, ensuring data integrity by comparing checksums. Args: - data (bytes): The input byte string that needs to be compressed and decompressed. - level (int, optional): The compression level, which is an integer between 0 and 9, or -1 for the default compression. Returns: - bool: A boolean value indicating whether the original data matches the decompressed data (i.e., data integrity is maintained). try: # Calculate the checksum of the original data original_checksum = zlib.adler32(data) # Compress the data compressed_data = zlib.compress(data, level) # Decompress the data decompressed_data = zlib.decompress(compressed_data) # Calculate the checksum of the decompressed data decompressed_checksum = zlib.adler32(decompressed_data) # Check if the original data and the decompressed data are the same return original_checksum == decompressed_checksum except Exception as e: # Raise any exceptions encountered during compression and decompression raise RuntimeError(f\\"An error occurred during compression/decompression: {e}\\")"},{"question":"# Temporary File Handler You are required to implement a function that utilizes the `tempfile` module to manipulate temporary files and directories for an application. This application will simulate logging events to temporary storage before transferring them to a permanent storage. Task Create a function `log_temp_events(events: list, use_temp_directory: bool = False) -> str` which does the following: 1. If `use_temp_directory` is `False`: - Creates a temporary file using `tempfile.NamedTemporaryFile`. - Writes each event from the `events` list to this file (each event on a new line). - Ensures that the file is read back to verify the contents. - Returns the contents as a single string with line breaks for each event. 2. If `use_temp_directory` is `True`: - Creates a temporary directory using `tempfile.TemporaryDirectory`. - Inside this directory, creates a new temporary file named `events.log`. - Writes each event from the `events` list to this file (each event on a new line). - Ensures that the file is read back to verify the contents. - Returns the contents as a single string with line breaks for each event. Requirements - Use the `tempfile` module to handle the creation and management of temporary files and directories. - Ensure proper cleanup so that no temporary files or directories are left after the function executes. - Handle possible exceptions that might arise during file/directory operations. Constraints - The `events` list contains a maximum of 1000 events. - Each event is a string with a maximum length of 256 characters. - The solution should work consistently across different operating systems. Example Usage ```python events = [\\"Event 1\\", \\"Event 2\\", \\"Event 3\\"] result = log_temp_events(events) print(result) # Output: # Event 1 # Event 2 # Event 3 ``` ```python events = [\\"Event 1\\", \\"Event 2\\", \\"Event 3\\"] result = log_temp_events(events, use_temp_directory=True) print(result) # Output: # Event 1 # Event 2 # Event 3 ```","solution":"import tempfile import os def log_temp_events(events: list, use_temp_directory: bool = False) -> str: Logs events to a temporary file or temporary directory and returns the contents as a single string with line breaks for each event. Args: events (list): List of event strings to log. use_temp_directory (bool): Whether to use a temporary directory. Returns: str: The logged events as a single string. if use_temp_directory: with tempfile.TemporaryDirectory() as temp_dir: temp_file_path = os.path.join(temp_dir, \\"events.log\\") with open(temp_file_path, \'w\') as temp_file: for event in events: temp_file.write(f\\"{event}n\\") with open(temp_file_path, \'r\') as temp_file: contents = temp_file.read() else: with tempfile.NamedTemporaryFile(delete=False) as temp_file: temp_file_path = temp_file.name for event in events: temp_file.write(f\\"{event}n\\".encode()) with open(temp_file_path, \'r\') as temp_file: contents = temp_file.read() os.remove(temp_file_path) return contents"},{"question":"**Problem Statement:** You are given the task of implementing a generic container class in Python with additional functionalities. This container should be able to store any type of Python object (including None) and provide methods to query the types of the stored objects, handle different types of sequences, and perform advanced operations like getting type-specific elements. # Requirements: 1. **Implement a class `GenericContainer`:** - **Attributes:** - `data`: A list that stores various Python objects. - **Methods:** 1. `add_element(element)`: Adds an element to the container. 2. `get_types() -> List[str]`: Returns a list of the types of the objects stored in the container. 3. `get_elements_by_type(type_name: str) -> List[any]`: Returns all elements of a specific type. 4. `get_first_element() -> any`: Returns the first element in the container. 5. `sort_elements()`: Sorts the elements in place (Should handle sequence objects; if sequences are not the same type, raise a `TypeError`). 6. `remove_element(element)`: Removes the specified element from the list (use equality to identify the element). 7. `show_state() -> List[any]`: Returns the current state of the container (Returns the list of elements). # Input and Output Format: - **Input:** Your method and class definitions will be tested through a series of function calls to `GenericContainer` methods. - **Output:** The output will primarily be through method return values where applicable. # Constraints: 1. You may assume all input elements are primitive or builtin object types. 2. Sorting methods (`sort_elements()`) should only support sortable built-in types (int, str, tuple, list containing any of these types) and should raise a `TypeError` otherwise. 3. For `get_elements_by_type(type_name)`, type names are in the form returned by `type(element).__name__`. # Example Usage: ```python gc = GenericContainer() gc.add_element(4) gc.add_element(\\"hello\\") gc.add_element([1, 2, 3]) gc.add_element(2) gc.add_element(\\"world\\") print(gc.get_types()) # Output: [\'int\', \'str\', \'list\', \'int\', \'str\'] print(gc.get_elements_by_type(\'str\')) # Output: [\'hello\', \'world\'] print(gc.get_first_element()) # Output: 4 gc.remove_element(2) print(gc.show_state()) # Output: [4, \'hello\', [1, 2, 3], \'world\'] gc.sort_elements() print(gc.show_state()) # Should raise TypeError ``` # Additional Notes: - Ensure error handling for unsupported operations like sorting mixed data types within sequences. - Your implementation should be efficient and leverage Python\'s built-in functionalities wherever possible.","solution":"from typing import List, Any, Type class GenericContainer: def __init__(self): self.data = [] def add_element(self, element: Any): self.data.append(element) def get_types(self) -> List[str]: return [type(elem).__name__ for elem in self.data] def get_elements_by_type(self, type_name: str) -> List[Any]: return [elem for elem in self.data if type(elem).__name__ == type_name] def get_first_element(self) -> Any: return self.data[0] if self.data else None def sort_elements(self): # Verify that all elements are either of the same type # or are directly comparable if not all(isinstance(x, (int, str, tuple, list)) for x in self.data): raise TypeError(\\"Only int, str, tuple, and list types can be sorted\\") try: self.data.sort() except TypeError: raise TypeError(\\"Elements are not sortable due to incompatible data types\\") def remove_element(self, element: Any): if element in self.data: self.data.remove(element) def show_state(self) -> List[Any]: return self.data"},{"question":"# Dynamic Shapes with Conditional Operations **Objective:** Implement a function that performs a custom operation on two input tensors and returns a result tensor. This function should handle dynamic shapes and conditional operations based on the symbolic sizes of the tensors. # Function Signature: ```python def dynamic_shape_operation(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: pass ``` # Input: - `tensor1` (torch.Tensor): A 2D tensor with dynamic shape in both dimensions. - `tensor2` (torch.Tensor): A 2D tensor with dynamic shape in both dimensions. # Output: - `result` (torch.Tensor): A tensor obtained by applying a conditional operation based on the symbolic first dimension size of the concatenation of the two input tensors along the first dimension. # Requirements: 1. Concatenate `tensor1` and `tensor2` along the first dimension. 2. If the size of the first dimension of the concatenated tensor is greater than a given threshold (e.g., 10), the output tensor should be the element-wise product of the concatenated tensor and a constant tensor of ones with the same shape. 3. If the size of the first dimension of the concatenated tensor is less than or equal to the threshold, the output tensor should be the element-wise sum of the concatenated tensor and a constant tensor of ones with the same shape. 4. Ensure the solution handles tensors with dynamic shapes properly without requiring recompilation for each distinct input size. # Constraints: - Do not perform any operations that require recompilation of the function for different input sizes. - Utilize the `torch._dynamo.mark_dynamic` function where necessary to indicate dynamic dimensions. - The function should be optimized to handle dynamic shapes efficiently. # Example: ```python import torch # Define inputs tensor1 = torch.randn(5, 3) tensor2 = torch.randn(6, 3) # Expected behavior: # Since the first dimension of the concatenated tensor is 11 (> 10), # the result should be the element-wise product result = dynamic_shape_operation(tensor1, tensor2) print(result.shape) # Expected: torch.Size([11, 3]) ``` # Notes: - The provided example is for demonstration purposes; the actual implementation must handle different sizes dynamically. - Focus on symbolic shape propagation and guard management as described in the documentation. - Ensure to test the function with various input sizes to validate dynamic behavior.","solution":"import torch def dynamic_shape_operation(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: Perform a custom operation on two input tensors and return the result tensor. Args: - tensor1 (torch.Tensor): A 2D tensor with dynamic shape in both dimensions. - tensor2 (torch.Tensor): A 2D tensor with dynamic shape in both dimensions. Returns: - result (torch.Tensor): A tensor obtained by applying conditional operations based on the symbolic size of the concatenated tensor along the first dimension. threshold = 10 concatenated = torch.cat((tensor1, tensor2), dim=0) first_dim_size = concatenated.size(0) if first_dim_size > threshold: # Element-wise product with a tensor of ones with the same shape ones_tensor = torch.ones_like(concatenated) result = concatenated * ones_tensor else: # Element-wise sum with a tensor of ones with the same shape ones_tensor = torch.ones_like(concatenated) result = concatenated + ones_tensor return result"},{"question":"Combination Generator You are tasked with implementing a function that combines various capabilities from the `itertools` module to generate a specific sequence of combinations and permutations. Your function should demonstrate a strong understanding of iterators, memory efficiency, and the unique functionalities provided by `itertools`. # Function Signature ```python def generate_combinations(elements: str, combination_length: int) -> List[str]: ``` # Input - `elements`: A string of characters (e.g., \'ABCD\'). - `combination_length`: An integer specifying the length of combinations (e.g., 2). # Output - Returns a list of strings, each representing a combination of elements. The combinations should follow these rules: 1. Generate all possible combinations of the given length using `itertools.combinations()`. 2. For each combination, generate all permutations of the combination using `itertools.permutations()`. 3. Sort the final list of permutation strings in lexicographic order. # Example ```python generate_combinations(\'ABCD\', 2) ``` Expected Output: ```python [\'AB\', \'AC\', \'AD\', \'BA\', \'BC\', \'BD\', \'CA\', \'CB\', \'CD\', \'DA\', \'DB\', \'DC\'] ``` Explanation: - Combinations of length 2 from \'ABCD\' are: AB, AC, AD, BC, BD, CD. - Permutations of these combinations: AB, BA, AC, CA, AD, DA, BC, CB, BD, DB, CD, DC. - Sorting these permutations lexicographically results in: [\'AB\', \'AC\', \'AD\', \'BA\', \'BC\', \'BD\', \'CA\', \'CB\', \'CD\', \'DA\', \'DB\', \'DC\']. # Constraints - Length of `elements` will be between 1 and 10. - `combination_length` will be between 1 and the length of `elements`. # Performance Considerations - Your solution should be efficient in terms of both time and space. - Utilize `itertools` to handle iterators and avoid unnecessary memory overhead. # Hint - Use `itertools.combinations` to generate combinations of the desired length. - Use `itertools.permutations` to generate all permutations of each combination. - Use list comprehensions and sorting functions to finalize your result.","solution":"import itertools from typing import List def generate_combinations(elements: str, combination_length: int) -> List[str]: Generates all possible permutations of combinations of a given length and returns them sorted in lexicographic order. combos = itertools.combinations(elements, combination_length) permutations_set = set() for combo in combos: perms = itertools.permutations(combo) for perm in perms: permutations_set.add(\'\'.join(perm)) return sorted(permutations_set)"},{"question":"Objective: Demonstrate your understanding of seaborn\'s capabilities to visualize statistical relationships using scatter and line plots. You will need to create a complex plot using `sns.relplot` with various customizations, including handling multiple semantics and faceting. Task: Write a function `visualize_data(dataset: pd.DataFrame) -> None` that takes a DataFrame and generates a plot using seaborn. Your function should: 1. Use the `sns.relplot` function to create a scatter plot. 2. Enhance the plot using the hue, size, and style semantics. 3. Customize the color palette. 4. Enable faceting to split the plot into multiple subplots. Input: - `dataset`: A pandas DataFrame containing at least the following columns: - `x`: Numeric values for the x-axis. - `y`: Numeric values for the y-axis. - `hue_variable`: Categorical or numeric values for hue semantics. - `size_variable`: Numeric values for size semantics. - `style_variable`: Categorical values for style semantics. - `facet_variable`: Categorical values for faceting. Output: - The function `visualize_data` should not return any value. Instead, it should generate a plot that is displayed inline. Example Input: ```python import pandas as pd data = { \'x\': [1, 2, 3, 4, 5], \'y\': [5, 4, 3, 2, 1], \'hue_variable\': [\'A\', \'B\', \'A\', \'B\', \'A\'], \'size_variable\': [10, 20, 30, 20, 10], \'style_variable\': [\'circle\', \'triangle\', \'circle\', \'triangle\', \'circle\'], \'facet_variable\': [\'g1\', \'g1\', \'g2\', \'g2\', \'g1\'] } dataset = pd.DataFrame(data) ``` Constraints: - Ensure that your function can handle datasets with more than 1000 rows efficiently. - Use a sequential color palette for numeric hue semantics and a qualitative palette for categorical hue semantics. Performance: - The function should generate and display the plot within 5 seconds for datasets up to 10000 rows. Additional Notes: - You can use any dataset of your choice to test the function, but the structure must be similar to the example provided. - Consider edge cases where some variable values might be missing or have unexpected data types. Reference: Refer to the provided seaborn documentation for additional examples and detailed usage of the required functions.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_data(dataset: pd.DataFrame) -> None: Creates a complex scatter plot using sns.relplot with hue, size, style, and facet functionalities. Parameters: dataset (DataFrame): A pandas DataFrame containing necessary data columns. # Create the scatter plot with the provided semantics g = sns.relplot( data=dataset, x=\\"x\\", y=\\"y\\", hue=\\"hue_variable\\", size=\\"size_variable\\", style=\\"style_variable\\", col=\\"facet_variable\\", palette=\\"viridis\\", # Customizing color palette sizes=(40, 400), # Customizing size range height=5, aspect=1 ) # Adjust the legend g.legend.set_bbox_to_anchor((1, 0.5)) plt.show()"},{"question":"# PyTorch Meta Device Exercise In this exercise, you will write a function that utilizes the PyTorch meta device. The goal is to create a function that simulates the memory footprint of a neural network without actually allocating memory for its parameters. Function Signature ```python def simulate_memory_footprint(model: torch.nn.Module) -> torch.nn.Module: pass ``` Input - `model`: A PyTorch `torch.nn.Module` representing the neural network model that you want to simulate. Output - Returns a version of the model where all of its parameters are on the `meta` device. Constraints - You may not modify the input model in-place. - The returned model should have the same architecture as the input model, but all parameters should reside on the `meta` device. Example Usage ```python import torch from torch import nn class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 10) def forward(self, x): x = self.fc1(x) x = self.fc2(x) return x model = SimpleNN() meta_model = simulate_memory_footprint(model) # Check if the model is now using the meta device for param in meta_model.parameters(): assert param.device.type == \'meta\' ``` Requirements 1. You must use the `torch.device(\'meta\')` context manager to transfer the model parameters to the meta device. 2. Ensure that the returned model\'s parameters are all on the `meta` device by using assertions in the sample usage code. Hints - Consider using the `torch.nn.Module.to_empty` method to aid in constructing the meta version of the model. - The goal is to effectively prepare the model for a situation where you need to analyze it without the actual memory overhead. Good luck, and happy coding!","solution":"import torch def simulate_memory_footprint(model: torch.nn.Module) -> torch.nn.Module: Returns a version of the input model where all of its parameters are on the `meta` device, simulating the memory footprint of the model without actually allocating memory for its parameters. meta_model = model.to_empty(device=torch.device(\'meta\')) return meta_model"},{"question":"# Functional Programming Challenge **Objective:** Demonstrate your understanding of functional programming in Python by solving a problem that involves higher-order functions, anonymous functions (lambdas), and the `map`, `filter`, and `reduce` functions. **Problem Statement:** You are given a list of dictionaries, where each dictionary represents a student with the properties `name`, `age`, and `grade`. Implement a function `process_students_data` that processes this data using functional programming techniques to achieve the following: 1. Filter out students who are below 18 years of age. 2. For each remaining student, increase their grade by 10%. 3. Return the names of the top 5 students based on their grades in descending order. **Input:** - A list of dictionaries, where each dictionary contains three keys: * `name`: A string representing the student\'s name. * `age`: An integer representing the student\'s age. * `grade`: A float representing the student\'s grade. **Output:** - A list of names of the top 5 students based on their grades in descending order. **Constraints:** - You must use functional programming techniques (`map`, `filter`, `reduce`, and lambdas) to implement the function. - Assume that there is always at least one student who is 18 years of age or older. - If there are fewer than 5 students after filtering, return the names of all remaining students in descending order of their grades. **Function Signature:** ```python from typing import List, Dict def process_students_data(students: List[Dict[str, object]]) -> List[str]: pass ``` **Example:** ```python students = [ {\\"name\\": \\"Alice\\", \\"age\\": 19, \\"grade\\": 85.0}, {\\"name\\": \\"Bob\\", \\"age\\": 17, \\"grade\\": 90.0}, {\\"name\\": \\"Charlie\\", \\"age\\": 18, \\"grade\\": 88.0}, {\\"name\\": \\"David\\", \\"age\\": 20, \\"grade\\": 92.0}, {\\"name\\": \\"Eve\\", \\"age\\": 21, \\"grade\\": 91.0}, {\\"name\\": \\"Frank\\", \\"age\\": 18, \\"grade\\": 85.0}, {\\"name\\": \\"Grace\\", \\"age\\": 19, \\"grade\\": 87.0} ] process_students_data(students) # Expected output: [\'David\', \'Eve\', \'Charlie\', \'Grace\', \'Alice\'] ``` **Note:** - This question tests the student\'s ability to apply functional programming techniques and understand the higher-order functions provided by Python. - Emphasis should be on writing clean and concise code using the functional programming paradigm.","solution":"from typing import List, Dict from functools import reduce def process_students_data(students: List[Dict[str, object]]) -> List[str]: # Filter out students who are below 18 years of age of_age_students = list(filter(lambda student: student[\'age\'] >= 18, students)) # Increase the grades by 10% increased_grades_students = list(map(lambda student: { \'name\': student[\'name\'], \'age\': student[\'age\'], \'grade\': student[\'grade\'] * 1.1 }, of_age_students)) # Sort the students by grade in descending order sorted_students = sorted(increased_grades_students, key=lambda student: student[\'grade\'], reverse=True) # Extract the names of the top 5 students top_students_names = [student[\'name\'] for student in sorted_students[:5]] return top_students_names"},{"question":"# Complex Tensor Operations in PyTorch **Objective:** Demonstrate your understanding of handling and performing operations on complex tensors using PyTorch. **Task:** You are required to implement a function `process_complex_tensors` that takes as input two complex tensors, performs specified operations, and returns the results. Below are the details of the steps you need to implement: 1. Create two complex tensors, `A` and `B`, of size (3, 3) with random values. 2. Perform the following operations on these tensors: - Compute the matrix product `C = A @ B`. - Compute the element-wise product of `A` and `B`. - Extract the real and imaginary parts of tensor `C`. - Compute the absolute value and angle of each element in tensor `C`. 3. Save the complex tensor `C` to a file named `\'complex_tensor.pt\'` and then load this tensor from the file. 4. Verify that the loaded tensor is equal to the original tensor `C`. **Function Signature:** ```python import torch def process_complex_tensors(): # Step 1: Create two complex tensors, A and B, with random values A = torch.randn(3, 3, dtype=torch.cfloat) B = torch.randn(3, 3, dtype=torch.cfloat) # Step 2: Perform specified operations # Compute the matrix product C = torch.matmul(A, B) # Compute element-wise product element_wise_product = A * B # Extract real and imaginary parts of C real_part = C.real imag_part = C.imag # Compute absolute value and angle of each element in C abs_values = torch.abs(C) angles = torch.angle(C) # Step 3: Save tensor C to a file and load it back torch.save(C, \'complex_tensor.pt\') loaded_C = torch.load(\'complex_tensor.pt\') # Step 4: Verify that the loaded tensor is equal to the original tensor C assert torch.equal(C, loaded_C), \\"Loaded tensor is not equal to the original tensor!\\" return { \'matrix_product\': C, \'element_wise_product\': element_wise_product, \'real_part\': real_part, \'imag_part\': imag_part, \'abs_values\': abs_values, \'angles\': angles, \'loaded_tensor_equal\': torch.equal(C, loaded_C) } # Example execution result = process_complex_tensors() # This function does not require input as it generates random tensors within the function itself. ``` **Constraints:** - Use only PyTorch functions and methods for tensor operations. - Ensure efficient operations and adhere to the best practices of using PyTorch for handling complex numbers. - Use assertions or equivalent checks to validate the results, especially for verifying tensor equality after serialization. The returned dictionary should contain the results of all specified operations for verification and testing purposes.","solution":"import torch def process_complex_tensors(): # Step 1: Create two complex tensors, A and B, with random values A = torch.randn(3, 3, dtype=torch.cfloat) B = torch.randn(3, 3, dtype=torch.cfloat) # Step 2: Perform specified operations # Compute the matrix product C = torch.matmul(A, B) # Compute element-wise product element_wise_product = A * B # Extract real and imaginary parts of C real_part = C.real imag_part = C.imag # Compute absolute value and angle of each element in C abs_values = torch.abs(C) angles = torch.angle(C) # Step 3: Save tensor C to a file and load it back torch.save(C, \'complex_tensor.pt\') loaded_C = torch.load(\'complex_tensor.pt\') # Step 4: Verify that the loaded tensor is equal to the original tensor C assert torch.equal(C, loaded_C), \\"Loaded tensor is not equal to the original tensor!\\" return { \'matrix_product\': C, \'element_wise_product\': element_wise_product, \'real_part\': real_part, \'imag_part\': imag_part, \'abs_values\': abs_values, \'angles\': angles, \'loaded_tensor_equal\': torch.equal(C, loaded_C) } # Example execution result = process_complex_tensors()"},{"question":"Objective: Implement multifaceted Python initialization functionality using the provided `PyConfig` and `PyPreConfig` structures along with error handling using `PyStatus`. Task: Write a function `initialize_python_with_custom_settings` that: 1. Initializes Python with a given configuration. 2. Applies customization, such as setting a specific program name, enabling UTF-8 mode, and handling command-line arguments. 3. Manages and reports errors appropriately, exiting with the correct status. Function Signature: ```python def initialize_python_with_custom_settings(program_name: str, utf8_enabled: bool, argv: list) -> None: Initializes Python with the specified configuration. Args: - program_name (str): The name of the program to be set in the configuration. - utf8_enabled (bool): Boolean flag indicating whether UTF-8 mode should be enabled. - argv (list): List of command-line arguments to be passed to Python. Returns: - None Raises: - RuntimeError: When initialization fails or encounters an exceptional status. ``` Requirements: 1. You should use `PyConfig` and `PyPreConfig` structures to initialize and configure Python. 2. Enable UTF-8 mode in the configuration if `utf8_enabled` is `True`. 3. Set the program name using the provided `program_name` parameter. 4. Pass the command-line arguments (`argv`) to the configuration. 5. Handle `PyStatus` properly: - Check if initialization was successful. - If there are errors, handle them by raising a `RuntimeError` with an appropriate message. Constraints: - Assume proper definition and import of `PyStatus`, `PyConfig`, `PyPreConfig`, and related functions as per the documentation. - The function will be tested within an environment that mimics the documented initialization behaviors. Example Usage: ```python try: initialize_python_with_custom_settings(\\"/path/to/program\\", True, [\\"--verbose\\"]) print(\\"Python initialized successfully with custom settings.\\") except RuntimeError as e: print(f\\"Initialization failed: {e}\\") ``` Note: Ensure you handle memory allocation and cleanup using the provided `PyConfig_Clear` function, as shown in the documentation examples.","solution":"def initialize_python_with_custom_settings(program_name: str, utf8_enabled: bool, argv: list) -> None: import sys import ctypes # Assume that these classes and methods are available as part of the Python C API bindings in ctypes. # This part is pseudo-code intended to provide a Python-bound API interface for PyConfig and related structures. class PyStatus(ctypes.Structure): _fields_ = [(\\"is_error\\", ctypes.c_int)] class PyPreConfig(ctypes.Structure): _fields_ = [(\\"utf8_mode\\", ctypes.c_int)] class PyConfig(ctypes.Structure): _fields_ = [(\\"program_name\\", ctypes.c_char_p), (\\"argv\\", ctypes.POINTER(ctypes.c_char_p)), (\\"argc\\", ctypes.c_int)] def PyPreConfig_InitPythonConfig(preconfig): pass def PyConfig_InitPythonConfig(config): pass def Py_InitializeFromConfig(config): return PyStatus(0) def PyConfig_Clear(config): pass def check_py_status(status): if status.is_error: raise RuntimeError(\\"Failed to initialize Python with the given configuration.\\") preconfig = PyPreConfig() config = PyConfig() # Initialize pre-configuration PyPreConfig_InitPythonConfig(preconfig) preconfig.utf8_mode = 1 if utf8_enabled else 0 # Initialize configuration PyConfig_InitPythonConfig(config) config.program_name = program_name.encode(\'utf-8\') argc = len(argv) argv_array = (ctypes.c_char_p * (argc + 1))(*[arg.encode(\'utf-8\') for arg in argv], None) config.argv = argv_array config.argc = argc # Initialize Python with the above configuration status = Py_InitializeFromConfig(config) try: check_py_status(status) finally: PyConfig_Clear(config)"},{"question":"**Question**: Implement a TCP server using asyncio\'s low-level event loop APIs, capable of handling multiple clients concurrently. The server should: - Start listening on a specified host and port. - Accept new client connections. - Read messages from clients and echo back the received messages. - Shutdown gracefully when a shutdown message is received from any client. - Log details of connections and messages. # Requirements: 1. **Function Signature**: ```python import asyncio async def start_server(host: str, port: int): # your solution here ``` 2. **Input**: - `host` (str): The hostname or IP address the server should bind to. - `port` (int): The port number the server should listen on. 3. **Functionality**: - Use asyncio\'s `loop.create_server()` to create a TCP server. - Use appropriate methods to read data from client connections and write responses. - Echo back any received message to the originating client. - If a client sends a message \\"shutdown\\", the server should: - Close all active client connections. - Log that a shutdown command was received and the server is shutting down. - Gracefully stop the event loop and close the server. 4. **Constraints**: - Ensure the server can handle multiple clients concurrently. - Use asyncio\'s logging functionalities for logging messages. - Properly manage exceptions and errors during client handling and a server shutdown. 5. **Performance**: - Manage client connections and data transfer efficiently to avoid blocking the event loop. # Example Usage: ```python async def main(): await start_server(\'127.0.0.1\', 8888) if __name__ == \'__main__\': asyncio.run(main()) ``` # Notes: - Write clean, modular, and well-documented code. - Ensure you test the server with multiple clients to verify concurrent handling and shutdown functionality. **Testing**: - Use asynchronous clients to connect to the server, send messages, and verify the echoed responses. - Send a \\"shutdown\\" message from a client to test the graceful shutdown process.","solution":"import asyncio import logging async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') logging.info(f\\"Connection from {addr}\\") while True: data = await reader.read(100) message = data.decode() logging.info(f\\"Received {message} from {addr}\\") if message.strip().lower() == \\"shutdown\\": logging.info(\\"Shutdown command received. Shutting down server...\\") writer.close() await writer.wait_closed() asyncio.get_running_loop().stop() break else: writer.write(data) await writer.drain() writer.close() await writer.wait_closed() logging.info(f\\"Connection closed from {addr}\\") async def start_server(host: str, port: int): server = await asyncio.start_server(handle_client, host, port) addrs = \', \'.join(str(sock.getsockname()) for sock in server.sockets) logging.info(f\\"Serving on {addrs}\\") async with server: await server.serve_forever() # Setup basic logging logging.basicConfig(level=logging.INFO) # Example of how to run the server # async def main(): # await start_server(\'127.0.0.1\', 8888) # # if __name__ == \'__main__\': # asyncio.run(main())"},{"question":"<|Analysis Begin|> The provided documentation is a brief overview of unsupervised dimensionality reduction techniques available in the scikit-learn library, specifically focusing on PCA (Principal Component Analysis), random projections, and feature agglomeration. Here is a summary of the key points covered: 1. **Pipelining**: Discusses the possibility of chaining unsupervised data reduction and supervised estimation in a single step using pipelines. 2. **PCA**: Details how PCA seeks combinations of features that capture the original features\' variance. 3. **Random Projections**: Provides tools for data reduction through random projections. 4. **Feature Agglomeration**: Applies hierarchical clustering to group features with similar behavior. 5. **Feature Scaling**: Notes the importance of scaling features when using methods like feature agglomeration to ensure related features are properly captured. Given this information, we can design a coding assessment question that requires students to implement a feature reduction pipeline using one of the discussed methods, ensuring they understand the process and application of these techniques. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective: Implement a data reduction process using Principal Component Analysis (PCA) from the scikit-learn library. Your task is to demonstrate your understanding of PCA and its application in reducing the dimensionality of a dataset. Problem Statement: You are provided with a dataset containing multiple features. Your goal is to reduce the dataset to a specified number of principal components while maintaining as much variance as possible. Requirements: 1. Write a function `reduce_dimensionality` that performs PCA on a given dataset. 2. The function should take the following inputs: - `data`: A 2D numpy array or pandas DataFrame where each row represents a sample and each column represents a feature. - `n_components`: An integer specifying the number of principal components to reduce the data to. 3. The function should return the transformed dataset with reduced dimensions. Constraints: - You must use the `PCA` class from the `sklearn.decomposition` module. - Ensure that your function handles cases where `n_components` is greater than the number of original features. - The dataset can be large, so consider the computational efficiency of your implementation. Function Signature: ```python import numpy as np import pandas as pd from sklearn.decomposition import PCA def reduce_dimensionality(data, n_components): Reduce the dimensionality of the given dataset using PCA. Parameters: - data (numpy.ndarray or pandas.DataFrame): The input data to be reduced. - n_components (int): The number of principal components to reduce to. Returns: - reduced_data (numpy.ndarray): The dataset transformed into the specified number of principal components. pass # Your implementation here ``` Example: ```python import numpy as np import pandas as pd # Sample data data = pd.DataFrame({ \'feature1\': [1, 2, 3, 4, 5], \'feature2\': [2, 3, 4, 5, 6], \'feature3\': [5, 4, 3, 2, 1], }) # Reduce to 2 principal components reduced_data = reduce_dimensionality(data, 2) print(reduced_data) ``` Expected Output (Format may vary): ```python [[X1, X2], [Y1, Y2], [Z1, Z2], ...] ``` Note: The exact values of the output will depend on the PCA transformation applied to the input data. Good luck!","solution":"import numpy as np import pandas as pd from sklearn.decomposition import PCA def reduce_dimensionality(data, n_components): Reduce the dimensionality of the given dataset using PCA. Parameters: - data (numpy.ndarray or pandas.DataFrame): The input data to be reduced. - n_components (int): The number of principal components to reduce to. Returns: - reduced_data (numpy.ndarray): The dataset transformed into the specified number of principal components. # Ensure the data is in the form of a numpy array if isinstance(data, pd.DataFrame): data = data.values # Check if the number of components requested is greater than the number of original features n_features = data.shape[1] if n_components > n_features: raise ValueError(\\"n_components cannot be greater than the number of original features\\") # Initialize PCA and fit-transform the data pca = PCA(n_components=n_components) reduced_data = pca.fit_transform(data) return reduced_data"},{"question":"# Custom Scikit-learn Estimator Implementation **Objective**: To assess your understanding of scikit-learn\'s estimator development, you need to implement a custom scikit-learn compatible estimator. Problem Statement: Implement a custom scikit-learn estimator named `CustomScaler` that scales features by a given factor provided at the time of instantiation. Your estimator should: 1. Inherit from the appropriate scikit-learn base classes. 2. Accept a parameter `scale_factor` during initialization to determine the factor by which the features should be scaled. 3. Implement the `fit` method, which doesn\'t need to learn anything but should validate the input data. 4. Implement the `transform` method, which scales the input data by the `scale_factor`. 5. Implement `fit_transform` using efficient combined operations. Your estimator should pass scikit-learn’s estimator checks. Expected Input: - During initialization: `scale_factor` (float, default=1.0) - During `fit` and `transform` methods: `X` (array-like of shape (n_samples, n_features)), `y` (array-like, default=None, and should be ignored) Expected Output: - The `transform` and `fit_transform` methods should return an array of shape (n_samples, n_features) with features scaled by `scale_factor`. Constraints: - You must use scikit-learn utilities for input validation. - Handle possible exceptions and edge cases appropriately. Example Usage: ```python from sklearn.utils.estimator_checks import check_estimator import numpy as np class CustomScaler(BaseEstimator, TransformerMixin): def __init__(self, scale_factor=1.0): self.scale_factor = scale_factor def fit(self, X, y=None): X = check_array(X) self.n_features_in_ = X.shape[1] return self def transform(self, X): X = check_array(X) if X.shape[1] != self.n_features_in_: raise ValueError(\\"Number of features does not match fit data\\") return X * self.scale_factor def fit_transform(self, X, y=None): return self.fit(X, y).transform(X) # Check if the custom estimator adheres to scikit-learn\'s conventions check_estimator(CustomScaler()) # Example usage: scaler = CustomScaler(scale_factor=2.0) X = np.array([[1, 2], [3, 4]]) X_scaled = scaler.fit_transform(X) print(X_scaled) # Output should be: # array([[ 2., 4.], # [ 6., 8.]]) ``` Write your implementation of the `CustomScaler` class below. Make sure your implementation adheres to scikit-learn\'s conventions as described in the documentation.","solution":"from sklearn.base import BaseEstimator, TransformerMixin from sklearn.utils.validation import check_array class CustomScaler(BaseEstimator, TransformerMixin): def __init__(self, scale_factor=1.0): self.scale_factor = scale_factor def fit(self, X, y=None): X = check_array(X) self.n_features_in_ = X.shape[1] return self def transform(self, X): X = check_array(X) if X.shape[1] != self.n_features_in_: raise ValueError(\\"Number of features does not match fit data\\") return X * self.scale_factor def fit_transform(self, X, y=None): return self.fit(X, y).transform(X) # Example usage if __name__ == \\"__main__\\": import numpy as np scaler = CustomScaler(scale_factor=2.0) X = np.array([[1, 2], [3, 4]]) X_scaled = scaler.fit_transform(X) print(X_scaled) # Output should be: # [[ 2., 4.], # [ 6., 8.]]"},{"question":"Question: Implement a Secure User Authentication System You are tasked with implementing a secure user authentication system using Python\'s `secrets` module. Your system should support the following functionalities: 1. **Password Generation** - Implement a function `generate_password(length)` that generates a secure alphanumeric password of a specified length. The password must contain at least one lowercase letter, one uppercase letter, and one digit. ```python def generate_password(length: int) -> str: Generate a secure alphanumeric password of the specified length. Args: length (int): The length of the password to be generated. Returns: str: Generated password meeting the required criteria. ``` 2. **Token Generation for Password Reset** - Implement a function `generate_password_reset_token()` that generates a secure URL-safe token. This token will be used for password reset links. ```python def generate_password_reset_token() -> str: Generate a secure URL-safe token for password reset. Returns: str: A URL-safe token. ``` 3. **Secure Token Comparison** - Implement a function `is_token_valid(stored_token, provided_token)` that securely compares two tokens and returns `True` if they are the same, `False` otherwise. ```python def is_token_valid(stored_token: str, provided_token: str) -> bool: Securely compare two tokens to determine if they are the same. Args: stored_token (str): The stored token. provided_token (str): The provided token for validation. Returns: bool: True if tokens match, False otherwise. ``` # Constraints - The password length passed to `generate_password` will be at least 8 characters. - You may import any necessary modules from the Python standard library. - Ensure that the password generated by `generate_password` meets all the specified criteria (contains at least one lowercase letter, one uppercase letter, and one digit). # Example Usage ```python # Example usage for generate_password password = generate_password(12) print(password) # Output: A strong 12-character password meeting specified criteria # Example usage for generate_password_reset_token token = generate_password_reset_token() print(token) # Output: A secure URL-safe token # Example usage for is_token_valid stored = generate_password_reset_token() provided = stored # Simulating that the provided token is the same as stored assert is_token_valid(stored, provided) == True provided = generate_password_reset_token() assert is_token_valid(stored, provided) == False ``` Implement these functions to demonstrate your understanding of secure random number generation and best practices for managing security tokens and passwords.","solution":"import secrets import string def generate_password(length: int) -> str: Generate a secure alphanumeric password of the specified length. Args: length (int): The length of the password to be generated. Returns: str: Generated password meeting the required criteria. if length < 8: raise ValueError(\\"Password length must be at least 8 characters\\") alphabet = string.ascii_letters + string.digits while True: password = \'\'.join(secrets.choice(alphabet) for _ in range(length)) if (any(c.islower() for c in password) and any(c.isupper() for c in password) and any(c.isdigit() for c in password)): return password def generate_password_reset_token() -> str: Generate a secure URL-safe token for password reset. Returns: str: A URL-safe token. return secrets.token_urlsafe() def is_token_valid(stored_token: str, provided_token: str) -> bool: Securely compare two tokens to determine if they are the same. Args: stored_token (str): The stored token. provided_token (str): The provided token for validation. Returns: bool: True if tokens match, False otherwise. return secrets.compare_digest(stored_token, provided_token)"},{"question":"**Objective:** Design a set of visualizations using Seaborn\'s `relplot` function that demonstrate various aspects of plotting with different kinds of data and semantic mappings. Your task is to replicate a series of plots and modify the final plot to fit specific criteria. **Input:** The \\"tips\\" and \\"fmri\\" datasets from the Seaborn package. **Tasks:** 1. Load the \\"tips\\" data using `sns.load_dataset()`. 2. Create the following visualizations: 1. A scatter plot of `total_bill` against `tip`, colored by `day`. 2. A faceted scatter plot of `total_bill` against `tip`, colored by `day`, with subplots for each `time`. 3. A faceted scatter plot of `total_bill` against `tip`, with subplots for each `time` and `sex`. 4. A faceted scatter plot of `total_bill` against `tip`, colored by `time`, with two columns for each `day`. 3. Load the \\"fmri\\" data using `sns.load_dataset()`. 4. Create the following visualizations: 1. A line plot of `timepoint` against `signal`, colored and styled by `event`, with subplots for each `region`. 2. Adjust the plot created in task 4.1 to have a height of 4 inches and an aspect ratio of 0.7. Add a horizontal line at y=0 and set axis labels to \\"Timepoint\\" and \\"Percent signal change\\". Set the title of each subplot to \\"Region: {col_name} cortex\\". **Output:** The expected output is a series of plots demonstrating the above criteria. **Constraints:** - Use only the Seaborn and Matplotlib libraries for visualization. - Ensure all plots are displayed correctly with appropriate labels and titles. # Example Here is how the plots will be generated step-by-step: ```python import seaborn as sns import matplotlib.pyplot as plt # Set theme sns.set_theme(style=\\"ticks\\") # Load datasets tips = sns.load_dataset(\\"tips\\") fmri = sns.load_dataset(\\"fmri\\") # Task 2.1 sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\") # Task 2.2 sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", col=\\"time\\") # Task 2.3 sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", col=\\"time\\", row=\\"sex\\") # Task 2.4 sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", col=\\"day\\", col_wrap=2) # Task 4.1 sns.relplot( data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"event\\", style=\\"event\\", col=\\"region\\", kind=\\"line\\" ) # Task 4.2 g = sns.relplot( data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"event\\", style=\\"event\\", col=\\"region\\", height=4, aspect=.7, kind=\\"line\\" ) g.map(plt.axhline, y=0, color=\\".7\\", dashes=(2, 1), zorder=0) g.set_axis_labels(\\"Timepoint\\", \\"Percent signal change\\") g.set_titles(\\"Region: {col_name} cortex\\") g.tight_layout(w_pad=0) plt.show() ``` Ensure to write clean, well-commented code. If you encounter any errors or issues, please check the Seaborn documentation for additional guidance.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Set theme sns.set_theme(style=\\"ticks\\") # Load datasets tips = sns.load_dataset(\\"tips\\") fmri = sns.load_dataset(\\"fmri\\") # Task 2.1 def plot_total_bill_vs_tip_colored_by_day(): sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\") plt.show() # Task 2.2 def plot_total_bill_vs_tip_colored_by_day_with_subplots_for_time(): sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", col=\\"time\\") plt.show() # Task 2.3 def plot_total_bill_vs_tip_with_subplots_for_time_and_sex(): sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", col=\\"time\\", row=\\"sex\\") plt.show() # Task 2.4 def plot_total_bill_vs_tip_colored_by_time_with_two_columns_for_day(): sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", col=\\"day\\", col_wrap=2) plt.show() # Task 4.1 def plot_timepoint_vs_signal_colored_and_styled_by_event_with_subplots_for_region(): sns.relplot( data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"event\\", style=\\"event\\", col=\\"region\\", kind=\\"line\\" ) plt.show() # Task 4.2 def plot_timepoint_vs_signal_with_customizations(): g = sns.relplot( data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"event\\", style=\\"event\\", col=\\"region\\", height=4, aspect=0.7, kind=\\"line\\" ) g.map(plt.axhline, y=0, color=\\".7\\", dashes=(2, 1), zorder=0) g.set_axis_labels(\\"Timepoint\\", \\"Percent signal change\\") g.set_titles(\\"Region: {col_name} cortex\\") g.tight_layout(w_pad=0) plt.show()"},{"question":"Problem Statement You are tasked with writing a function that takes a list of strings, compresses each string individually, writes the compressed data to separate files, and then reads back the compressed data from each file and decompresses it to verify if the decompressed data matches the original strings. To simulate storage constraints, the strings should be compressed incrementally and decompressed incrementally. Requirements 1. Implement the function `compress_and_verify(data_list, compresslevel=9) -> bool` where: - `data_list`: a list of strings to be compressed and decompressed. - `compresslevel`: an optional integer specifying the compression level (default is 9). 2. The function should: - Compress each string in `data_list` incrementally and write the compressed data to separate files named `file_0.bz2`, `file_1.bz2`, etc. - Read the compressed data from each file incrementally, decompress it, and verify that it matches the original string. - Return `True` if all decompressed data matches the original strings, `False` otherwise. Constraints - Do not use the one-shot `bz2.compress` or `bz2.decompress` functions. - Make use of the `BZ2Compressor` and `BZ2Decompressor` classes for incremental (de)compression. - Handle all file operations using `with` statements to ensure files are properly closed after operations. Example Usage ```python data_list = [\\"Hello World!\\", \\"Python 310 documentation.\\", \\"Test string for compression and decompression.\\"] compresslevel = 5 result = compress_and_verify(data_list, compresslevel) print(result) # Should print: True ``` Notes - Ensure your function handles both text and binary data appropriately. - Take care of any potential edge cases such as empty strings in `data_list`. Good luck!","solution":"import bz2 def compress_and_verify(data_list, compresslevel=9): for i, data in enumerate(data_list): # Compress the data and write to file incrementally compressor = bz2.BZ2Compressor(compresslevel) with open(f\'file_{i}.bz2\', \'wb\') as f: compressed_data = compressor.compress(data.encode(\'utf-8\')) f.write(compressed_data) f.write(compressor.flush()) for i, original_data in enumerate(data_list): # Decompress the data read from file incrementally decompressor = bz2.BZ2Decompressor() decompressed_data = b\'\' with open(f\'file_{i}.bz2\', \'rb\') as f: while True: chunk = f.read(1024) if not chunk: break decompressed_data += decompressor.decompress(chunk) # Verify if decompressed data matches the original data if decompressed_data.decode(\'utf-8\') != original_data: return False return True"},{"question":"Given the Unix user account and password database accessible via Python\'s `pwd` module, write a Python function `get_user_info(username: str) -> dict` that retrieves and returns the detailed information for a given username. Function Signature: ```python def get_user_info(username: str) -> dict: pass ``` Input: - `username` (str): The login name of the user to be queried. Output: - A dictionary containing the following keys and corresponding values from the password database entry: - `\\"login_name\\"`: The login name (string) - `\\"user_id\\"`: The numerical user ID (integer) - `\\"group_id\\"`: The numerical group ID (integer) - `\\"user_name\\"`: The user name or comment field (string) - `\\"home_directory\\"`: The user home directory (string) - `\\"shell\\"`: The user command interpreter (string) Constraints: - If the specified username does not exist in the password database, the function should raise a `KeyError`. Example: Given the following entries in the password database: | pw_name | pw_passwd | pw_uid | pw_gid | pw_gecos | pw_dir | pw_shell | |---------|-----------|--------|--------|----------|--------------|-----------| | alice | x | 1001 | 1001 | Alice | /home/alice | /bin/bash | | bob | x | 1002 | 1002 | Bob | /home/bob | /bin/zsh | - `get_user_info(\\"alice\\")` should return: ```python { \\"login_name\\": \\"alice\\", \\"user_id\\": 1001, \\"group_id\\": 1001, \\"user_name\\": \\"Alice\\", \\"home_directory\\": \\"/home/alice\\", \\"shell\\": \\"/bin/bash\\" } ``` - `get_user_info(\\"charlie\\")` should raise a `KeyError`. Notes: - Use the `pwd.getpwnam` method to retrieve the password database entry for a given username. - Ensure that your function handles cases where the username does not exist by raising a `KeyError`.","solution":"import pwd def get_user_info(username: str) -> dict: try: pw_record = pwd.getpwnam(username) return { \\"login_name\\": pw_record.pw_name, \\"user_id\\": pw_record.pw_uid, \\"group_id\\": pw_record.pw_gid, \\"user_name\\": pw_record.pw_gecos, \\"home_directory\\": pw_record.pw_dir, \\"shell\\": pw_record.pw_shell } except KeyError: raise KeyError(f\\"User {username} not found\\")"},{"question":"You are given a dataset consisting of features `X` and target values `y`. Your goal is to implement a Kernel Ridge Regression using scikit-learn\'s `KernelRidge` class to fit the model to the data, optimize its hyperparameters, and evaluate its performance. Follow the instructions below: # Instructions: 1. **Load the Dataset:** - Assume `X` and `y` are already loaded in the environment as numpy arrays. 2. **Model Implementation:** - Implement a `KernelRidgeRegressionModel` class containing methods for fitting and predicting. 3. **Hyperparameter Optimization:** - Use GridSearchCV to find the best hyperparameters for the model, including the regularization parameter `alpha` and the kernel coefficient `gamma` for an RBF kernel. 4. **Evaluation:** - Provide the mean squared error (MSE) of the optimal model on the training data. # Requirements: - You must use scikit-learn’s `KernelRidge` and `GridSearchCV`. - The model should use the RBF kernel. - Hyperparameter search should span a reasonable range for `alpha` and `gamma`. # Expected function signature: ```python def kernel_ridge_regression(X, y): X: np.ndarray of shape (n_samples, n_features) The input samples. y: np.ndarray of shape (n_samples,) The target values. Returns: dict: A dictionary with the following keys: - \'best_params\': dict of the best hyperparameters found. - \'mse\': float representing the Mean Squared Error on training data. pass ``` # Example: ```python import numpy as np # Example dataset X = np.random.rand(100, 5) y = np.random.rand(100) result = kernel_ridge_regression(X, y) print(\\"Best Hyperparameters:\\", result[\'best_params\']) print(\\"Training MSE:\\", result[\'mse\']) ``` # Notes: - Ensure reproducibility by setting a random state where applicable. - The solution should be efficient and handle typical sizes of datasets used in practice. - Use appropriate cross-validation techniques to avoid overfitting.","solution":"import numpy as np from sklearn.kernel_ridge import KernelRidge from sklearn.model_selection import GridSearchCV from sklearn.metrics import mean_squared_error def kernel_ridge_regression(X, y): X: np.ndarray of shape (n_samples, n_features) The input samples. y: np.ndarray of shape (n_samples,) The target values. Returns: dict: A dictionary with the following keys: - \'best_params\': dict of the best hyperparameters found. - \'mse\': float representing the Mean Squared Error on training data. # Define the model model = KernelRidge(kernel=\'rbf\') # Define the hyperparameters grid param_grid = { \'alpha\': np.logspace(-3, 3, 7), # 0.001 to 1000 \'gamma\': np.logspace(-3, 3, 7) # 0.001 to 1000 } # Set up the grid search grid_search = GridSearchCV(model, param_grid, cv=5, scoring=\'neg_mean_squared_error\') grid_search.fit(X, y) # Get the best parameters and calculate the MSE on the training data best_params = grid_search.best_params_ best_model = grid_search.best_estimator_ y_pred = best_model.predict(X) mse = mean_squared_error(y, y_pred) return {\'best_params\': best_params, \'mse\': mse}"},{"question":"Objective You are required to implement a Python function that generates a `MANIFEST.in` file based on specific criteria provided as input. The goal is to automate the process of creating this manifest template which is a crucial part of packaging a Python project for distribution. Problem Statement Implement a function `generate_manifest` that takes in two lists of strings: `include_patterns` and `exclude_patterns`. The function should generate a `MANIFEST.in` file with the following specifications: - Each pattern in `include_patterns` should be added to the manifest file prefixed by the `include` directive. - Each pattern in `exclude_patterns` should be added to the manifest file prefixed by the `prune` directive. - The order of patterns must be preserved as provided in the input lists. - The resulting `MANIFEST.in` file should be saved in the current working directory. Function Signature ```python def generate_manifest(include_patterns: List[str], exclude_patterns: List[str]) -> None: ``` Input - `include_patterns`: List of strings where each string is a pattern of files to include. Example: `[\\"*.py\\", \\"docs/*.txt\\"]` - `exclude_patterns`: List of strings where each string is a pattern of directories to exclude. Example: `[\\"build/\\", \\"temp/\\"]` Output - The function does not return anything but creates a `MANIFEST.in` file in the current working directory with the specified patterns. Constraints - Patterns are simple glob patterns. - No validation of whether the files or directories exist is required. - Assume the current working directory has write permissions. Example Given the inputs: ```python include_patterns = [\\"*.py\\", \\"docs/*.txt\\"] exclude_patterns = [\\"build/\\", \\"temp/\\"] ``` The `MANIFEST.in` file should be: ``` include *.py include docs/*.txt prune build/ prune temp/ ``` Notes - Be sure to handle any necessary file operations and ensure the file is properly written and closed. - Focus on string manipulation and file operations to construct the required manifest file.","solution":"from typing import List def generate_manifest(include_patterns: List[str], exclude_patterns: List[str]) -> None: with open(\'MANIFEST.in\', \'w\') as manifest_file: for pattern in include_patterns: manifest_file.write(f\\"include {pattern}n\\") for pattern in exclude_patterns: manifest_file.write(f\\"prune {pattern}n\\")"},{"question":"# Python Coding Assessment Objective: To assess your understanding of Python\'s `inspect` module, particularly in relation to retrieving and handling function signatures and parameters. Problem Statement: You are tasked with creating a utility function `analyze_functions` that, given a list of functions, returns detailed information about each function\'s signature, including the name, list of parameters with their types and default values (if any), and the return type. Function Signature: ```python def analyze_functions(functions: list) -> dict: pass ``` Input: - `functions`: A list of function objects. Output: - A dictionary where each key is the function\'s name and the corresponding value is another dictionary with the following keys: - `parameters`: A list of dictionaries, each containing: - `name`: The parameter\'s name - `type`: The parameter\'s annotation type (or \'unknown\' if not annotated) - `default`: The parameter\'s default value (or \'None\' if no default value is provided) - `return_type`: The return type annotation of the function (or \'unknown\' if not annotated) Constraints: - Assume the input list will always contain valid functions. - Emphasis should be on using the `inspect` module effectively. Example: ```python from typing import List def example_function(a: int, b: str = \\"hello\\") -> List[str]: pass def another_function(x, y: float) -> None: pass functions = [example_function, another_function] result = analyze_functions(functions) print(result) ``` Expected output: ```python { \'example_function\': { \'parameters\': [ {\'name\': \'a\', \'type\': \'int\', \'default\': \'None\'}, {\'name\': \'b\', \'type\': \'str\', \'default\': \'hello\'} ], \'return_type\': \'List[str]\' }, \'another_function\': { \'parameters\': [ {\'name\': \'x\', \'type\': \'unknown\', \'default\': \'None\'}, {\'name\': \'y\', \'type\': \'float\', \'default\': \'None\'}, ], \'return_type\': \'None\' } } ``` # Requirements: - Utilize the `inspect` module, specifically `signature()` and related attributes like `parameters` and `return_annotation`, to extract the required information. - Ensure the results are clear and well-structured as shown in the example.","solution":"import inspect def analyze_functions(functions: list) -> dict: result = {} for func in functions: func_name = func.__name__ sig = inspect.signature(func) parameters = [] for name, param in sig.parameters.items(): param_info = { \'name\': name, \'type\': param.annotation if param.annotation != inspect.Parameter.empty else \'unknown\', \'default\': param.default if param.default != inspect.Parameter.empty else \'None\' } parameters.append(param_info) return_type = sig.return_annotation if sig.return_annotation != inspect.Signature.empty else \'unknown\' result[func_name] = { \'parameters\': parameters, \'return_type\': return_type } return result"},{"question":"You are required to implement a Python function that processes command-line arguments for a simple text processing utility. This utility accepts and performs operations on an input text file, such as counting words or lines, and specifies an output file to write results to. Requirements: 1. **Function Name:** `process_arguments(args)` 2. **Parameters:** A list of strings `args`, representing the command-line arguments without the leading reference to the running program (similarly to `sys.argv[1:]`). 3. **Options and Arguments:** - `-i` or `--input`: Specifies the input file (requires an argument). - `-o` or `--output`: Specifies the output file (requires an argument). - `-c` or `--count`: Specify what to count; either `words` or `lines` (requires an argument). - `-h` or `--help`: Displays the help message and exits (no argument). 4. **Return:** - A dictionary with the options as keys and their corresponding arguments as values. - In case of help, raise a `SystemExit` exception after printing a help message. - If required arguments are missing or if there are unrecognized options, raise a `getopt.GetoptError`. Example Usage: ```python args = [\\"-i\\", \\"input.txt\\", \\"--output\\", \\"output.txt\\", \\"-c\\", \\"words\\"] result = process_arguments(args) print(result) # Expected Output: {\'input\': \'input.txt\', \'output\': \'output.txt\', \'count\': \'words\'} ``` ```python args = [\\"--help\\"] process_arguments(args) # Expected Output: (Help message is printed and SystemExit is raised) args = [\\"--input\\", \\"input.txt\\", \\"--output\\", \\"output.txt\\", \\"--count\\"] # Expected to raise: getopt.GetoptError ``` Constraints: - The function should handle errors gracefully and provide meaningful error messages for missing or unrecognized options. - Assume that the input and output file paths provided are valid strings. - The `--count` argument should only accept \\"words\\" or \\"lines\\". Examples of input and expected behavior: 1. Valid arguments with all required options: ```python args = [\\"-i\\", \\"file.txt\\", \\"--output\\", \\"result.txt\\", \\"--count\\", \\"lines\\"] # Expected Output: {\'input\': \'file.txt\', \'output\': \'result.txt\', \'count\': \'lines\'} ``` 2. Invalid arguments, missing count argument: ```python args = [\\"-i\\", \\"file.txt\\", \\"-o\\", \\"result.txt\\", \\"--count\\"] # Expected to raise: getopt.GetoptError ``` 3. Help option: ```python args = [\\"-h\\"] # Expected behavior: Print help message and exit ``` Implement the function `process_arguments` accordingly.","solution":"import getopt import sys def process_arguments(args): Processes command-line arguments and returns a dictionary of options. args : list of str List of command-line arguments. Returns ------- dict Dictionary with options as keys and their corresponding arguments as values. Raises ------ getopt.GetoptError If there are missing or unrecognized options. SystemExit If the help option is specified. try: opts, _ = getopt.getopt(args, \\"hi:o:c:\\", [\\"help\\", \\"input=\\", \\"output=\\", \\"count=\\"]) except getopt.GetoptError as err: print(str(err)) raise options = {} for opt, arg in opts: if opt in (\\"-h\\", \\"--help\\"): print(\\"Usage: script.py -i <inputfile> -o <outputfile> -c <count: words|lines>\\") print(\\"Options:\\") print(\\" -i, --input input file\\") print(\\" -o, --output output file\\") print(\\" -c, --count specify count type (words or lines)\\") raise SystemExit elif opt in (\\"-i\\", \\"--input\\"): options[\'input\'] = arg elif opt in (\\"-o\\", \\"--output\\"): options[\'output\'] = arg elif opt in (\\"-c\\", \\"--count\\"): if arg not in [\'words\', \'lines\']: raise getopt.GetoptError(\\"Count must be either \'words\' or \'lines\'\\") options[\'count\'] = arg required_options = [\'input\', \'output\', \'count\'] for opt in required_options: if opt not in options: raise getopt.GetoptError(f\\"Missing required option: {opt}\\") return options"},{"question":"# Context Managers and Exception Handling with the `contextlib` Module Objective: Write a Python function using the `contextlib` module to create a custom context manager that temporarily changes the current working directory of your script and reverts back to the original directory after execution of the `with` block, handling any exceptions that occur in the process. Requirements: 1. Implement a context manager class (`ChangeDir`) using the `contextlib` module\'s `ContextDecorator` base class. 2. The `ChangeDir` context manager should: - Accept a target directory path as an initialization argument. - Change the current working directory to the target directory when entering the context. - Revert to the original directory when exiting the context. - Handle any exceptions that occur inside the `with` block and print a custom error message. 3. Write a test function (`test_change_dir`) to demonstrate the usage of your context manager. This function should: - Print the current working directory. - Use the `ChangeDir` context manager to temporarily change to a new directory. - Inside the `with` block, print the current working directory to confirm the change. - Raise an intentional exception inside the `with` block to test the exception handling. - Print the current working directory again after the `with` block to confirm it has reverted back. Input: - The target directory path (string). Output: - Print statements inside the context manager demonstrating directory changes and any custom error messages. Example Usage: ```python import os class ChangeDir(contextlib.ContextDecorator): def __init__(self, target_dir): self.target_dir = target_dir self.original_dir = None def __enter__(self): self.original_dir = os.getcwd() os.chdir(self.target_dir) return self def __exit__(self, exc_type, exc_value, traceback): os.chdir(self.original_dir) if exc_type is not None: print(f\\"An error occurred: {exc_value}\\") return True # Suppress exception def test_change_dir(): try: print(\\"Original Directory:\\", os.getcwd()) with ChangeDir(\\"/new/target/directory\\"): print(\\"Inside Context - New Directory:\\", os.getcwd()) raise Exception(\\"Test Exception\\") print(\\"Back to Original Directory:\\", os.getcwd()) except Exception as e: print(f\\"Handled exception outside context: {e}\\") # Run the test function test_change_dir() ``` In this example, replace `\\"/new/target/directory\\"` with a valid directory path on your system to test the context manager. Constraints: - Do not use any external libraries other than `contextlib` and the standard `os` library. - Ensure that the context manager correctly handles and suppresses exceptions within the `with` block.","solution":"import os from contextlib import ContextDecorator class ChangeDir(ContextDecorator): def __init__(self, target_dir): self.target_dir = target_dir self.original_dir = None def __enter__(self): self.original_dir = os.getcwd() os.chdir(self.target_dir) return self def __exit__(self, exc_type, exc_value, traceback): os.chdir(self.original_dir) if exc_type is not None: print(f\\"An error occurred: {exc_value}\\") return True # Suppress exception"},{"question":"# Implementation Question You are tasked with writing a Python function that uses the `pkgutil` module to dynamically list all modules and submodules within a specified package, and then retrieve and print the first 100 characters of a specific resource file from each module. Function Signature ```python def list_and_print_modules(package_name: str, resource_file: str) -> None: ... ``` Parameters - `package_name` (str): The name of the package to be inspected. - `resource_file` (str): The relative path to the resource file within each module to retrieve and print from. Expected Output The function should output the names of all modules and submodules in the given package and the first 100 characters of the specified resource file from each module, if the resource file exists. If the specified resource file does not exist in a module, it should skip that module without any errors. Constraints - The provided package must be importable. - The function should handle exceptions gracefully and continue processing remaining modules even if a particular module encounters an error. - Performance should be efficient with respect to I/O operations. Example Usage ```python list_and_print_modules(\'ctypes\', \'README.txt\') ``` Example Output ``` Module: ctypes, Resource Content: This is the content of the README.txt file... Module: ctypes.util, Resource Content: Utility functions for ctypes... ... ``` You should use functions from the provided `pkgutil` module documentation as necessary, such as `iter_modules`, `get_data`, etc., to accomplish this task. Notes - Handle all potential exceptions (e.g., `FileNotFoundError`) gracefully. - Ensure to only import packages and modules if needed.","solution":"import pkgutil import importlib def list_and_print_modules(package_name: str, resource_file: str) -> None: List all modules and submodules within a specified package and print the first 100 characters of a specified resource file for each module if it exists. Parameters: - package_name (str): The name of the package. - resource_file (str): The relative path to the resource file within each module. package = importlib.import_module(package_name) def walk_modules(package): if hasattr(package, \'__path__\'): for _, name, _ in pkgutil.iter_modules(package.__path__): full_name = f\\"{package.__name__}.{name}\\" subpackage = importlib.import_module(full_name) yield subpackage yield from walk_modules(subpackage) for module in walk_modules(package): try: data = pkgutil.get_data(module.__name__, resource_file) if data: content = data.decode(\'utf-8\')[:100] print(f\\"Module: {module.__name__}, Resource Content: {content}...\\") except IOError: continue"},{"question":"# Mailbox Class Implementation Challenge You are tasked with implementing a simple `CustomMailbox` class that simulates some core functionalities of the `Mailbox` class described in the documentation. This class will be dictionary-like, allowing messages to be added, deleted, and retrieved using unique keys. Requirements: 1. **Initialization**: - The class should be initialized with an empty dictionary to store messages. 2. **Methods**: - `add(message)`: Adds a new message to the mailbox and returns a unique key for it. - `remove(key)`: Deletes a message identified by the key. - `get_message(key)`: Retrieves the message corresponding to the key. - `__len__()`: Returns the number of messages in the mailbox. - `__contains__(key)`: Checks if a key exists in the mailbox. 3. **Concurrency Simulation**: - Implement simple locking mechanisms using a context manager to ensure safe modification (add, remove) of the mailbox. - The mailbox must be locked before any modification. Constraints: 1. **Message Type**: Messages can be strings. 2. **Performance**: The solution should efficiently handle the addition, retrieval, and deletion of large numbers of messages. 3. **Concurrency**: Simulate locking with a context manager to ensure no two operations modify the mailbox simultaneously. Function Signatures: ```python class CustomMailbox: def __init__(self): # Initializes an empty mailbox pass def add(self, message: str) -> int: # Adds a message to the mailbox and returns a unique key (integer) pass def remove(self, key: int) -> None: # Removes the message corresponding to the provided key pass def get_message(self, key: int) -> str: # Returns the message corresponding to the provided key pass def __len__(self) -> int: # Returns the number of messages in the mailbox pass def __contains__(self, key: int) -> bool: # Checks if the key exists in the mailbox pass def lock(self): # Lock the mailbox for safe modification pass def unlock(self): # Unlock the mailbox after modification pass class MailboxLock: # Context manager to handle locking def __init__(self, mailbox): pass def __enter__(self): # Locks the mailbox pass def __exit__(self, exc_type, exc_value, traceback): # Unlocks the mailbox pass ``` Example Usage: ```python mailbox = CustomMailbox() key1 = mailbox.add(\\"Hello, world!\\") key2 = mailbox.add(\\"Another message\\") with mailbox.MailboxLock(mailbox): mailbox.remove(key1) assert len(mailbox) == 1 assert key2 in mailbox assert mailbox.get_message(key2) == \\"Another message\\" ``` Implement the `CustomMailbox` class to meet the above requirements.","solution":"class CustomMailbox: def __init__(self): self._messages = {} self._current_id = 0 self._lock = False def add(self, message: str) -> int: with self.MailboxLock(self): key = self._current_id self._messages[key] = message self._current_id += 1 return key def remove(self, key: int) -> None: with self.MailboxLock(self): if key in self._messages: del self._messages[key] def get_message(self, key: int) -> str: return self._messages.get(key, None) def __len__(self) -> int: return len(self._messages) def __contains__(self, key: int) -> bool: return key in self._messages def lock(self): self._lock = True def unlock(self): self._lock = False class MailboxLock: def __init__(self, mailbox): self.mailbox = mailbox def __enter__(self): self.mailbox.lock() def __exit__(self, exc_type, exc_value, traceback): self.mailbox.unlock()"},{"question":"# SQLite3 Database Interaction and Custom Python Types **Objective:** Implement a Python function to interact with an SQLite database. This function should perform the following operations: 1. Create a new SQLite database file (`students.db`). 2. Create a table named `students` with three columns: `id` (integer primary key), `name` (text), and `scores` (text). 3. Insert multiple student records into the database. Each record should include an id, student name, and their scores (a list of integers). 4. Include a function to adapt the list of scores to a string when inserting and a function to convert the string back to a list when retrieving from the database. 5. Retrieve and return all records from the `students` table, but ensure the `scores` are returned as a list of integers. **Function Signature:** ```python import sqlite3 from typing import List, Tuple def manage_student_records(students: List[Tuple[int, str, List[int]]]) -> List[Tuple[int, str, List[int]]]: # Your implementation here ``` **Input:** - `students`: A list of tuples, where each tuple contains: - `id` (int): A unique identifier for the student. - `name` (str): The name of the student. - `scores` (List[int]): A list of integers representing the student\'s scores. **Output:** - Returns a list of tuples, where each tuple contains: - `id` (int): The student\'s unique identifier. - `name` (str): The student\'s name. - `scores` (List[int]): A list of integers representing the student\'s scores. **Example:** ```python students = [ (1, \\"Alice\\", [85, 90, 88]), (2, \\"Bob\\", [78, 82, 85]), (3, \\"Charlie\\", [92, 85, 87]) ] result = manage_student_records(students) # Expected output: # [ # (1, \\"Alice\\", [85, 90, 88]), # (2, \\"Bob\\", [78, 82, 85]), # (3, \\"Charlie\\", [92, 85, 87]) # ] ``` **Constraints:** 1. The database file should be created and used within the function, ensuring no dependency on external databases. 2. The adaptation of lists to a storable format should be handled using custom adapter functions. 3. The function should ensure proper cleanup and closing of the database connection and any other resources. **Performance Requirements:** The solution should efficiently handle the database operations and ensure the integrity of the data transformations. Consider using optimized SQL queries and appropriate error handling.","solution":"import sqlite3 import json from typing import List, Tuple def adapt_list(lst): return json.dumps(lst) def convert_list(txt): return json.loads(txt) def manage_student_records(students: List[Tuple[int, str, List[int]]]) -> List[Tuple[int, str, List[int]]]: # Register adapters and converters to handle list of integers sqlite3.register_adapter(list, adapt_list) sqlite3.register_converter(\\"json\\", convert_list) # Create a new SQLite database conn = sqlite3.connect(\\"students.db\\", detect_types=sqlite3.PARSE_DECLTYPES) cursor = conn.cursor() # Create the students table cursor.execute(\'\'\' CREATE TABLE IF NOT EXISTS students ( id INTEGER PRIMARY KEY, name TEXT, scores json ) \'\'\') # Insert student records cursor.executemany(\'\'\' INSERT INTO students (id, name, scores) VALUES (?, ?, ?) \'\'\', students) # Commit changes conn.commit() # Retrieve all records cursor.execute(\'SELECT id, name, scores FROM students\') fetched_students = cursor.fetchall() # Close the connection conn.close() return fetched_students"},{"question":"**Question:** Implement a custom PyTorch autograd function to compute a safe logarithm with custom gradients and support for `torch.vmap`. # Task 1. Implement a custom autograd function `SafeLog` using `torch.autograd.Function`. This function should: - Compute the natural logarithm of its input, ensuring that the input is clamped to avoid computing the log of values less than a small positive constant (`eps`). - Store necessary values for backpropagation. - Define a backward method that computes the gradient manually. 2. Ensure that the `SafeLog` function supports batching via `torch.vmap` by defining a `vmap` static method. 3. Provide a helper function `safe_log` that uses your custom `autograd.Function`. # Specifications - **Input:** A tensor `x` and a small positive constant `eps`. - **Output:** A tensor containing the natural logarithm of `x`, where `x` values are clamped to be at least `eps`. # Constraints - Use only PyTorch operations within the `SafeLog` class. - Input tensor `x` can have any shape, but `eps` will always be a scalar. # Example ```python import torch # Example usage: x = torch.tensor([0.0, 1.0, 2.0, 3.0]) eps = 1e-6 # Using the helper function result = safe_log(x, eps) print(result) # Should print the clamped log values # Check gradients x.requires_grad = True safe_log(x, eps).sum().backward() print(x.grad) # Should print the gradients ``` # Notes - Ensure the `internal clamps` in the forward function to handle inputs less than `eps`. - Implement the `vmap` method to support batched operations.","solution":"import torch class SafeLog(torch.autograd.Function): @staticmethod def forward(ctx, x, eps): Computes the forward pass for the safe logarithm. # Clamping to avoid log(0) clamped_x = torch.clamp(x, min=eps) # Store context for backpropagation ctx.save_for_backward(clamped_x) ctx.eps = eps return torch.log(clamped_x) @staticmethod def backward(ctx, grad_output): Computes the backward pass for the safe logarithm. clamped_x, = ctx.saved_tensors grad_input = grad_output / clamped_x return grad_input, None @staticmethod def vmap(ctx, x, eps): Defines how to apply the SafeLog function in a batched manner using torch.vmap. return torch.vmap(SafeLog.apply)(x, eps) def safe_log(x, eps): return SafeLog.apply(x, eps)"},{"question":"**Objective:** To assess the student\'s understanding of tensor views in PyTorch, including creation, manipulation, and the implications of tensor contiguity. **Problem Statement:** Write a function `view_operations` that performs the following steps: 1. Create a 3D tensor `base_tensor` of shape (4, 3, 2) with sequential values starting from 0. 2. Reshape `base_tensor` into a 2D tensor `reshaped_tensor` of shape (6, 4) using a view operation. 3. Transpose `reshaped_tensor` to obtain `transposed_tensor`. 4. Verify and return whether `transposed_tensor` is contiguous. 5. If `transposed_tensor` is not contiguous, create a contiguous tensor `contiguous_tensor` from `transposed_tensor`. The function should return the tuple of tensors: `(base_tensor, reshaped_tensor, transposed_tensor, contiguous_tensor)`. **Input:** - None **Output:** - A tuple containing: - `base_tensor`: The original 3D tensor of shape (4, 3, 2). - `reshaped_tensor`: The 2D tensor of shape (6, 4) obtained from `base_tensor`. - `transposed_tensor`: The transposed version of `reshaped_tensor`. - `contiguous_tensor`: The contiguous version of `transposed_tensor` if `transposed_tensor` is not contiguous, otherwise `None`. **Constraints:** - You must use the view operation for reshaping `base_tensor` to `reshaped_tensor`. - You must use the transpose operation to create `transposed_tensor`. - You must check for contiguity using the `.is_contiguous()` method. - Use `.contiguous()` method to create a contiguous tensor from a non-contiguous tensor. **Example:** ```python import torch def view_operations(): # Step 1: Create base tensor (4, 3, 2) base_tensor = torch.arange(24).view(4, 3, 2) # Step 2: Reshape using view reshaped_tensor = base_tensor.view(6, 4) # Step 3: Transpose reshaped tensor transposed_tensor = reshaped_tensor.transpose(0, 1) # Step 4: Verify contiguity is_contiguous = transposed_tensor.is_contiguous() # Step 5: Create contiguous tensor if necessary contiguous_tensor = transposed_tensor.contiguous() if not is_contiguous else None return (base_tensor, reshaped_tensor, transposed_tensor, contiguous_tensor) # Expected Output: # (base_tensor, reshaped_tensor, transposed_tensor, contiguous_tensor) - with shapes (4, 3, 2), (6, 4), (4, 6), and (4, 6) respectively ``` **Note:** Verify the shapes and contiguity status of the returned tensors to ensure correctness.","solution":"import torch def view_operations(): # Step 1: Create base tensor (4, 3, 2) base_tensor = torch.arange(24).view(4, 3, 2) # Step 2: Reshape using view reshaped_tensor = base_tensor.view(6, 4) # Step 3: Transpose reshaped tensor transposed_tensor = reshaped_tensor.transpose(0, 1) # Step 4: Verify contiguity is_contiguous = transposed_tensor.is_contiguous() # Step 5: Create contiguous tensor if necessary contiguous_tensor = transposed_tensor.contiguous() if not is_contiguous else None return (base_tensor, reshaped_tensor, transposed_tensor, contiguous_tensor)"},{"question":"# Secure Temporary File URL Generator Design a Python function `generate_secure_url()` to create a secure temporary URL containing a security token suitable for applications such as password recovery, file sharing, or other sensitive operations. The URL should be structured as follows: ``` \\"https://example.com/file=<token>\\" ``` where `<token>` is a cryptographically strong random URL-safe text string. Your function should meet the following specifications: - The `token` should be generated with sufficient randomness to ensure security against brute-force attacks. - The length of the `token` should be customizable through a parameter `nbytes` (default should be 32 bytes for security). - The function should use the `secrets` module to generate the `token`. - Ensure the URL is URL-safe. Function Signature: ```python def generate_secure_url(nbytes: int = 32) -> str: pass ``` Examples: ```python # Example 1: url = generate_secure_url() # The output might look like: \\"https://example.com/file=3n9Vv3_5Y2w7G1sd9Z-G9pFWc0o-4Q7g\\" # Example 2: url = generate_secure_url(16) # The output might look like: \\"https://example.com/file=Drmhze6EPcv0fN_81Bj-nA\\" ``` Notes: - You should use the `secrets.token_urlsafe` function to generate the security token. - The default number of bytes for the token is 32, which translates to approximately 43 characters when Base64 encoded (since each byte results in approximately 1.3 characters). Implement `generate_secure_url` and test it within the provided constraints.","solution":"import secrets def generate_secure_url(nbytes: int = 32) -> str: Generates a secure temporary URL with a cryptographically strong random URL-safe token. Args: nbytes (int): Number of bytes to use for the token. Default is 32. Returns: str: Secure URL containing the generated token. token = secrets.token_urlsafe(nbytes) return f\\"https://example.com/file={token}\\""},{"question":"Objective: You will implement a function that profiles the execution of another function using Python’s **`cProfile`** module and then processes the profiling data to extract specific statistics using the **`pstats`** module. Task: Write a function `profile_and_analyze(func, *args, **kwargs)` that: 1. Profiles the execution of the function `func` with the provided `*args` and `**kwargs`. 2. Collects the profiling statistics and returns a dictionary with the following keys: - `total_calls`: Total number of function calls recorded. - `primitive_calls`: Number of non-recursive function calls recorded. - `total_time`: Total time spent within the functions (in seconds). - `most_called_function`: The name of the most frequently called function. - `most_time_function`: The name of the function that took the most total time (including sub-call times). # Input: - A function `func` and its arguments `*args` and `**kwargs`. # Output: - A dictionary with the aforementioned keys and their respective profiling statistics. # Constraints: - The profiling should be done using the `cProfile` module. - The function should handle profiling data manipulation using the `pstats` module. - The function names in the result should include the file name and line number in the format `filename:lineno(function)`. # Example: Suppose we have a simple function to profile: ```python def example_function(n): total = 0 for i in range(n): total += i return total ``` Using the `profile_and_analyze` function: ```python result = profile_and_analyze(example_function, 1000) print(result) ``` Expected output format (values will vary based on actual profiling): ```python { \'total_calls\': 2004, \'primitive_calls\': 2000, \'total_time\': 0.003, \'most_called_function\': \'your_script.py:2(<module>)\', # example format \'most_time_function\': \'your_script.py:2(<module>)\' # example format } ``` You can use the following template to start your solution: ```python import cProfile import pstats import io def profile_and_analyze(func, *args, **kwargs): # Create a cProfile profile object pr = cProfile.Profile() # Enable profiling pr.enable() # Run the function with provided arguments func(*args, **kwargs) # Disable profiling pr.disable() # Create a StringIO to capture the stats result s = io.StringIO() ps = pstats.Stats(pr, stream=s).strip_dirs().sort_stats(pstats.SortKey.CUMULATIVE) ps.print_stats() # Extract and process stats stats = ps.stats # Prepare the result dictionary result = { \'total_calls\': sum(stat[0] for stat in stats.values()), \'primitive_calls\': sum(stat[1] for stat in stats.values()), \'total_time\': sum(stat[3] for stat in stats.values()), \'most_called_function\': max(stats, key=lambda key: stats[key][0]), \'most_time_function\': max(stats, key=lambda key: stats[key][3]) } # Return formatted result return result ``` You may need to adjust the implementation to fit all requirements properly. Ensure to include any imported modules and handle edge cases as necessary.","solution":"import cProfile import pstats import io def profile_and_analyze(func, *args, **kwargs): # Create a cProfile profile object pr = cProfile.Profile() # Enable profiling pr.enable() # Run the function with provided arguments func(*args, **kwargs) # Disable profiling pr.disable() # Create a StringIO to capture stats result s = io.StringIO() ps = pstats.Stats(pr, stream=s).strip_dirs().sort_stats(pstats.SortKey.CUMULATIVE) # Extract and process stats pr.create_stats() stats = ps.stats # Prepare the result dictionary total_calls = sum(stat[0] for stat in stats.values()) primitive_calls = sum(stat[1] for stat in stats.values()) total_time = sum(stat[3] for stat in stats.values()) # Find the most called function and the function that took most time most_called_function = max(stats, key=lambda key: stats[key][0]) most_time_function = max(stats, key=lambda key: stats[key][3]) result = { \'total_calls\': total_calls, \'primitive_calls\': primitive_calls, \'total_time\': total_time, \'most_called_function\': f\\"{most_called_function[0]}:{most_called_function[1]}({most_called_function[2]})\\", \'most_time_function\': f\\"{most_time_function[0]}:{most_time_function[1]}({most_time_function[2]})\\" } return result"},{"question":"# Python Coding Assessment Context The `runpy` module in Python provides functionality to locate and execute Python modules without importing them first. One of its key functions is `runpy.run_module`, which executes the code of a specified module and returns the resulting module globals dictionary. Objective Write a Python function `custom_run_module` that mimics some of the key functionalities of `runpy.run_module`. Specifically, your function should: - Execute the code of a specified module. - Return the resulting module globals dictionary. - Handle basic initialization of special global variables (`__name__`, `__file__`, `__loader__`, `__package__`). Function Signature ```python def custom_run_module(mod_name: str, init_globals: dict = None, run_name: str = None, alter_sys: bool = False) -> dict: pass ``` Parameters - `mod_name` (str): The absolute name of the module to execute. - `init_globals` (dict): Optional dictionary to pre-populate the module\'s globals dictionary. Default is `None`. - `run_name` (str): Optional name to assign to `__name__` in the module\'s globals dictionary. Default is `None`. - `alter_sys` (bool): If `True`, temporarily modify `sys.argv[0]` and `sys.modules`. Default is `False`. Constraints - Your function should only execute regular Python modules, not packages or scripts located in the filesystem. - Your function does not need to handle threaded code or external modules. - The special global variables `__spec__`, `__cached__`, and `__package__` can be set to `None` for simplicity. Return - The function should return a dictionary representing the globals of the executed module. Example Usage Suppose you have a Python module named `example_module` with the following code: ```python # example_module.py def greet(): return \\"Hello, world!\\" if __name__ == \\"__main__\\": print(greet()) ``` Your function can be used as follows: ```python globals_dict = custom_run_module(\\"example_module\\") print(globals_dict[\\"greet\\"]()) # Outputs: Hello, world! ``` Note that this is a simplified example and your implementation should correctly initialize necessary global variables and manage the module\'s execution environment. Assumptions - Assume all module names provided will exist and be correct.","solution":"import importlib.util import sys import types import os def custom_run_module(mod_name: str, init_globals: dict = None, run_name: str = None, alter_sys: bool = False) -> dict: if init_globals is None: init_globals = {} # Find the module spec = importlib.util.find_spec(mod_name) if spec is None: raise ImportError(f\\"Module {mod_name} not found\\") # Load the module\'s code module = importlib.util.module_from_spec(spec) # Prepare the module\'s global namespace globals_dict = init_globals.copy() globals_dict.update({ \\"__name__\\": run_name if run_name else mod_name, \\"__file__\\": spec.origin, \\"__loader__\\": spec.loader, \\"__package__\\": None, }) try: # Alter sys.modules and sys.argv if required original_sys_modules = None original_argv = None if alter_sys: original_sys_modules = sys.modules.copy() original_argv = sys.argv.copy() sys.modules[mod_name] = module sys.argv[0] = spec.origin if spec.origin else mod_name # Execute the module\'s code with open(spec.origin, \'r\') as f: code = f.read() exec(code, globals_dict) return globals_dict finally: # Restore sys.modules and sys.argv if modified if alter_sys: if original_sys_modules is not None: sys.modules.update(original_sys_modules) if original_argv is not None: sys.argv = original_argv"},{"question":"# Email Message Manipulation As part of a custom email processing system, you are required to implement a function that builds and manipulates an email message using the `email.message.Message` class. The function should: 1. Create a new email message. 2. Add necessary headers such as \\"From\\", \\"To\\", \\"Subject\\", and \\"Content-Type\\". 3. Set a specified payload, which could be either a simple text message or a multipart message with multiple parts (each part being a simple text message). 4. Return the email message as a string, ensuring proper formatting and compliance with common email standards. # Function Signature ```python def create_and_manipulate_email(from_addr: str, to_addr: str, subject: str, payloads: list) -> str: Create and manipulate an email message. Parameters: - from_addr (str): The sender\'s email address. - to_addr (str): The recipient\'s email address. - subject (str): The subject of the email. - payloads (list): A list of payloads. If the list contains one element, it should be handled as a simple text message. If the list contains multiple elements, handle it as a multipart message. Each element in the list is a tuple (text, content_type). Returns: - str: The formatted email message as a string. pass ``` # Constraints 1. **Headers**: The email must include the following headers, properly formatted: - \\"From\\": The sender\'s email address. - \\"To\\": The recipient\'s email address. - \\"Subject\\": The subject of the email. - \\"Content-Type\\": Should be \\"text/plain\\" for a simple text message or \\"multipart/mixed\\" for a multipart message. 2. **Payload**: - If `payloads` contains one element, the payload should be set as a simple text message with the content type specified in the tuple. - If `payloads` contains multiple elements, construct a multipart message. Each part should have its own content type as specified in the tuples. 3. **Output**: The function should return the email message as a well-formatted string, compatible with common email handling systems. # Example Usage ```python # Creating a simple text email email_str = create_and_manipulate_email( from_addr=\\"alice@example.com\\", to_addr=\\"bob@example.com\\", subject=\\"Meeting Notification\\", payloads=[(\\"This is to notify you of a meeting scheduled for tomorrow.\\", \\"text/plain\\")] ) print(email_str) # Creating a multipart email email_str = create_and_manipulate_email( from_addr=\\"alice@example.com\\", to_addr=\\"bob@example.com\\", subject=\\"Monthly Report\\", payloads=[ (\\"Here is the executive summary of the report...\\", \\"text/plain\\"), (\\"Attachment content here...\\", \\"application/pdf\\") ] ) print(email_str) ``` # Notes - Make sure the email complies with RFC 5322 standards. - Handle appropriate encoding for headers and payloads where necessary. - You can import necessary classes or methods from the `email` library.","solution":"from email.message import EmailMessage from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText def create_and_manipulate_email(from_addr: str, to_addr: str, subject: str, payloads: list) -> str: Create and manipulate an email message. Parameters: - from_addr (str): The sender\'s email address. - to_addr (str): The recipient\'s email address. - subject (str): The subject of the email. - payloads (list): A list of payloads. If the list contains one element, it should be handled as a simple text message. If the list contains multiple elements, handle it as a multipart message. Each element in the list is a tuple (text, content_type). Returns: - str: The formatted email message as a string. if len(payloads) == 1: # Simple text message message = EmailMessage() message.set_content(payloads[0][0], subtype=payloads[0][1].split(\\"/\\")[-1]) else: # Multipart message message = MIMEMultipart() for payload, content_type in payloads: part = MIMEText(payload, content_type.split(\\"/\\")[-1]) message.attach(part) message[\'From\'] = from_addr message[\'To\'] = to_addr message[\'Subject\'] = subject return message.as_string()"},{"question":"# Question You are required to write a Python function that gathers detailed information about the platform where the Python interpreter is currently running. Your task is to implement a function `get_platform_details()`: Function Signature ```python def get_platform_details() -> dict: pass ``` Objective This function should return a dictionary containing the following details about the platform: 1. `architecture`: Architecture information of the current Python interpreter. 2. `machine`: The machine type. 3. `network_name`: The computer\'s network name. 4. `platform`: A string identifying the underlying platform. 5. `processor`: The real processor name. 6. `python_version`: The Python version. 7. `os_release`: The system\'s release. 8. `os_name`: The system/OS name. 9. `os_version`: The system\'s release version. 10. `libc`: The version of the libc against which the executable is linked (for Unix systems). If libc info is unavailable, set this to `None`. Example Output ```python { \\"architecture\\": (\\"64bit\\", \\"ELF\\"), \\"machine\\": \\"x86_64\\", \\"network_name\\": \\"hostname\\", \\"platform\\": \\"Linux-5.4.0-74-generic-x86_64-with-Ubuntu-20.04-focal\\", \\"processor\\": \\"x86_64\\", \\"python_version\\": \\"3.10.0\\", \\"os_release\\": \\"5.4.0-74-generic\\", \\"os_name\\": \\"Linux\\", \\"os_version\\": \\"#83-Ubuntu SMP Fri May 7 14:23:55 UTC 2021\\", \\"libc\\": (\\"glibc\\", \\"2.31\\") } ``` Constraints and Performance Requirements - Use the `platform` module to gather all required information. - Ensure that your function handles edge cases where certain information cannot be determined (e.g., return empty strings or `None` as appropriate). - The function should execute efficiently without unnecessary delays. Testing To verify your solution, write test cases that check the output of your function on different platforms (you can use mocks to simulate different platform details if necessary). # Note Consider using the documentation provided in the `platform` module to find appropriate methods/functions to fetch the required platform details.","solution":"import platform def get_platform_details() -> dict: Returns a dictionary containing detailed information about the platform. libc = None if hasattr(platform, \'libc_ver\'): libc = platform.libc_ver() return { \\"architecture\\": platform.architecture(), \\"machine\\": platform.machine(), \\"network_name\\": platform.node(), \\"platform\\": platform.platform(), \\"processor\\": platform.processor(), \\"python_version\\": platform.python_version(), \\"os_release\\": platform.release(), \\"os_name\\": platform.system(), \\"os_version\\": platform.version(), \\"libc\\": libc }"},{"question":"# Coding Assessment: Text Comparison Tool **Objective:** Create a Python function utilizing the `difflib` module to compare the contents of two text files and produce a detailed diff report. The report should highlight the differences in a human-readable format and indicate the similarity ratio. **Task:** Implement the function `compare_files(file1: str, file2: str) -> str`. This function accepts two file paths, reads their contents, compares them, and returns a detailed diff report as a string. The report should include: 1. A summary indicating the similarity ratio between the files. 2. A detailed line-by-line comparison showing additions, deletions, and modifications. **Function Signature:** ```python def compare_files(file1: str, file2: str) -> str: ``` **Input Parameters:** - `file1` (str): Path to the first text file. - `file2` (str): Path to the second text file. **Output:** - `str`: A detailed diff report indicating the similarity ratio and differences between the two files. **Constraints:** - The files may contain up to 10,000 lines. - Each line in the files can be up to 1,000 characters. **Performance Requirements:** - The function should efficiently handle the comparison and generate the diff report in less than 3 seconds for the maximum input size. **Example:** Assume `file1.txt` contains: ``` 1. Beautiful is better than ugly. 2. Explicit is better than implicit. 3. Simple is better than complex. 4. Complex is better than complicated. ``` And `file2.txt` contains: ``` 1. Beautiful is better than ugly. 3. Simple is better than complex. 4. Complicated is better than complex. 5. Flat is better than nested. ``` The function call `compare_files(\'file1.txt\', \'file2.txt\')` should return a string like: ``` Similarity ratio: 0.85 Diff report: 1. Beautiful is better than ugly. - 2. Explicit is better than implicit. + 3. Simple is better than complex. ? ++ - 4. Complex is better than complicated. ? ^ ---- ^ + 4. Complicated is better than complex. ? ++++ ^ ^ + 5. Flat is better than nested. ``` **Notes:** - Use the `difflib.SequenceMatcher` class to compute the similarity ratio. - Use the `difflib.Differ` class to generate the detailed diff report. - Ensure that the diff output clearly indicates the nature of each change.","solution":"import difflib def compare_files(file1: str, file2: str) -> str: Compares the contents of two files and returns a detailed diff report. Args: - file1: Path to the first file. - file2: Path to the second file. Returns: A string containing the similarity ratio and a detailed diff report. with open(file1, \'r\') as f1, open(file2, \'r\') as f2: lines1 = f1.readlines() lines2 = f2.readlines() # Calculate similarity ratio sm = difflib.SequenceMatcher(None, lines1, lines2) similarity_ratio = sm.ratio() # Generate diff report d = difflib.Differ() diff = list(d.compare(lines1, lines2)) # Create the formatted output report = [f\'Similarity ratio: {similarity_ratio:.2f}n\', \'Diff report:n\'] report.extend(diff) return \'\'.join(report)"},{"question":"# Custom Autograd Function and Gradient Validation in PyTorch Objective Implement a custom autograd function in PyTorch and validate its correctness using `gradcheck`. Problem Statement You are to implement a custom autograd function for an operation called `CustomSquare` where the forward operation computes the element-wise square of its input tensor. Additionally, you need to implement the backward pass for this operation, which computes the gradients. Follow these steps: 1. **Implement the CustomSquare Function:** - Subclass `torch.autograd.Function`. - Implement the `forward` method which computes the square of each element of the input tensor. - Implement the `backward` method which returns the gradient of the loss with respect to the input tensor. 2. **Validate with gradcheck:** - Use `torch.autograd.gradcheck` to ensure the gradients computed by your custom function are correct. Requirements 1. **Function Implementation:** - `forward(ctx, input)`: - `input`: A tensor of any shape. - Returns a tensor of the same shape with each element squared. - `backward(ctx, grad_output)`: - `grad_output`: A tensor of gradients of the same shape as the output. - Returns the gradient of the loss with respect to the input. The gradient of (y = x^2) with respect to (x) is (2x). 2. **Validation:** - Ensure your implementation passes `gradcheck`. - `gradcheck` requires the function and input to be of double precision (`torch.double`) and `requires_grad=True`. Input and Output Format - **Input:** - Python code that implements the `CustomSquare` function, runs a test using `gradcheck`, and prints the result. - **Output:** - A boolean indicating if `gradcheck` passed (`True` or `False`). Example ```python import torch from torch.autograd import Function, gradcheck class CustomSquare(Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) return input ** 2 @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = 2 * input * grad_output return grad_input # Wrapper function def custom_square(input): return CustomSquare.apply(input) # Test the custom function input = torch.randn(10, dtype=torch.double, requires_grad=True) test = gradcheck(custom_square, (input,), eps=1e-6, atol=1e-4) print(test) ``` Constraints 1. You must handle arbitrary input shapes. 2. Do not use in-place operations for gradients. 3. Ensure compatibility with double precision (essential for `gradcheck`). **Note:** Make sure to import necessary packages/modules from PyTorch before running the code.","solution":"import torch from torch.autograd import Function, gradcheck class CustomSquare(Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) return input ** 2 @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = 2 * input * grad_output return grad_input # Wrapper function def custom_square(input): return CustomSquare.apply(input) # Test the custom function input = torch.randn(10, dtype=torch.double, requires_grad=True) test = gradcheck(custom_square, (input,), eps=1e-6, atol=1e-4) print(test)"},{"question":"# Gaussian Mixture Model Coding Assessment Your task is to create a function to determine the optimal number of components for a Gaussian Mixture Model (GMM) applied to a given dataset. The optimal number of components should be determined by the Bayesian Information Criterion (BIC). You will then use this optimal model to predict the Gaussian component for each sample in a test dataset. Function Signature ```python def optimal_gmm_model(train_data: np.ndarray, test_data: np.ndarray, max_components: int, random_state: int = 42) -> np.ndarray: Determines the optimal number of components for GMM using BIC and predicts the component for test data. Parameters: - train_data: A 2D numpy array of shape (n_samples, n_features), the training data. - test_data: A 2D numpy array of shape (m_samples, n_features), the test data. - max_components: An integer, the maximum number of components to test for. - random_state: An integer, the seed for random number generation. Returns: - A 1D numpy array of shape (m_samples,), the predicted component for each sample in the test data. pass ``` Requirements 1. **Model Selection**: - Fit `GaussianMixture` models with the number of components ranging from 1 to `max_components` on the `train_data`. - Use the Bayesian Information Criterion (BIC) to choose the best model. 2. **Model Prediction**: - Use the optimal model to predict the Gaussian component each sample in `test_data` most probably belongs to. 3. **Initialization**: - Use `k-means` initialization method for training GMM. 4. **Performance Constraints**: - Ensure the function runs efficiently even for high-dimensional data with up to 100 components. - Set the `random_state` parameter for reproducibility. Example ```python import numpy as np # Example training data train_data = np.array([[1.0, 2.0], [1.1, 1.9], [9.0, 10.0], [8.9, 9.1], [5.0, 5.0]]) # Example test data test_data = np.array([[1.2, 2.1], [9.1, 9.9], [5.1, 5.1]]) # Find the optimal GMM model and predict components for test data predictions = optimal_gmm_model(train_data, test_data, max_components = 5) print(predictions) # Example output: [0 1 2] ``` Notes - You can assume that `train_data` and `test_data` are non-empty and properly pre-processed. - You might use the `sklearn.mixture.GaussianMixture` class for GMM implementation. - Make sure to handle any potential issues such as numerical stability and convergence.","solution":"import numpy as np from sklearn.mixture import GaussianMixture def optimal_gmm_model(train_data: np.ndarray, test_data: np.ndarray, max_components: int, random_state: int = 42) -> np.ndarray: Determines the optimal number of components for GMM using BIC and predicts the component for test data. Parameters: - train_data: A 2D numpy array of shape (n_samples, n_features), the training data. - test_data: A 2D numpy array of shape (m_samples, n_features), the test data. - max_components: An integer, the maximum number of components to test for. - random_state: An integer, the seed for random number generation. Returns: - A 1D numpy array of shape (m_samples,), the predicted component for each sample in the test data. best_bic = np.inf best_gmm = None for n_components in range(1, max_components + 1): gmm = GaussianMixture(n_components=n_components, random_state=random_state, init_params=\'kmeans\') gmm.fit(train_data) bic = gmm.bic(train_data) if bic < best_bic: best_bic = bic best_gmm = gmm predictions = best_gmm.predict(test_data) return predictions"},{"question":"Problem Statement You are required to write a function `parse_arguments(args: List[str]) -> Tuple[List[Tuple[str, str]], List[str]]` that parses command-line arguments using the `getopt` module. Your function should mimic the behavior of the Unix `getopt()` function. Function Signature ```python from typing import List, Tuple def parse_arguments(args: List[str]) -> Tuple[List[Tuple[str, str]], List[str]]: pass ``` Input - `args`: A list of strings representing the command-line arguments to be parsed. This list does not include the leading reference to the running program. Output - Returns a tuple with two elements: - A list of `(option, value)` pairs, where `option` is a string representing the option and `value` is a string representing its argument or an empty string if the option does not take an argument. Short options are prefixed with a hyphen (\'-\') and long options with two hyphens (\'--\'). - A list of program arguments left after parsing the options. Constraints - `shortopts` string: \\"abc:d:\\" - Option \'a\' does not take an argument. - Option \'b\' does not take an argument. - Option \'c\' takes a required argument. - Option \'d\' takes a required argument. - `longopts` list: [\\"condition=\\", \\"output-file=\\", \\"testing\\"] - Option \'condition\' takes a required argument. - Option \'output-file\' takes a required argument. - Option \'testing\' does not take an argument. Examples ```python # Example 1 args = [\'-a\', \'-b\', \'-cfoo\', \'-d\', \'bar\', \'a1\', \'a2\'] print(parse_arguments(args)) # Output: ([(\'-a\', \'\'), (\'-b\', \'\'), (\'-c\', \'foo\'), (\'-d\', \'bar\')], [\'a1\', \'a2\']) # Example 2 args = [\'--condition=foo\', \'--testing\', \'--output-file\', \'abc.def\', \'-x\', \'a1\', \'a2\'] print(parse_arguments(args)) # Output: ([(\'--condition\', \'foo\'), (\'--testing\', \'\'), (\'--output-file\', \'abc.def\'), (\'-x\', \'\')], [\'a1\', \'a2\']) # Example 3 args = [\'-x\', \'--nonexistent\'] # Should handle the error appropriately and raise GetoptError ``` Notes - Use the `getopt` module to implement your function. - Handle `getopt.GetoptError` exceptions by printing the error message and returning two empty lists (`([], [])`). - Short and long options should be processed according to the specified `shortopts` and `longopts`.","solution":"import getopt from typing import List, Tuple def parse_arguments(args: List[str]) -> Tuple[List[Tuple[str, str]], List[str]]: shortopts = \\"abc:d:\\" longopts = [\\"condition=\\", \\"output-file=\\", \\"testing\\"] try: opts, remainder = getopt.getopt(args, shortopts, longopts) result_opts = [(opt, val) for opt, val in opts] return result_opts, remainder except getopt.GetoptError as err: print(err) return [], []"},{"question":"**Objective**: Implement a pipeline for Ridge Regression to predict and classify dataset values using scikit-learn. **Problem Statement** You are provided with two datasets; one containing data for a regression task and another for a classification task. Your goal is to build Ridge Regression and Ridge Classifier models using scikit-learn to solve these tasks. **Requirements**: 1. Load the datasets from the following paths: - `regression_data.csv` for regression. - `classification_data.csv` for classification. 2. Implement feature scaling using `StandardScaler`. 3. Create a Ridge Regression model to predict the target values from `regression_data.csv`. 4. Create a Ridge Classifier model to classify the target values from `classification_data.csv`. 5. Evaluate the models: - For Ridge Regression, use the Mean Squared Error (MSE) as the evaluation metric. - For Ridge Classifier, use the accuracy score as the evaluation metric. 6. Implement cross-validation with 5 folds for model evaluation and report the cross-validation scores. # Function Signature ```python import pandas as pd from sklearn.linear_model import Ridge, RidgeClassifier from sklearn.preprocessing import StandardScaler from sklearn.model_selection import cross_val_score from sklearn.metrics import mean_squared_error, accuracy_score def load_data(file_path): Load data from a CSV file. Parameters: - file_path (str): Path to the CSV file. Returns: - DataFrame: Loaded pandas DataFrame. df = pd.read_csv(file_path) return df def preprocess_data(df, target_column): Preprocess the data by separating features and target and applying standard scaling. Parameters: - df (DataFrame): DataFrame containing data. - target_column (str): Name of the target column. Returns: - X_scaled (ndarray): Scaled feature data. - y (ndarray): Target values. X = df.drop(columns=[target_column]) y = df[target_column] scaler = StandardScaler() X_scaled = scaler.fit_transform(X) return X_scaled, y def train_ridge_regression(X, y, alpha=1.0): Train and evaluate a Ridge Regression model. Parameters: - X (array-like): Feature data. - y (array-like): Target values. - alpha (float): Regularization strength. Returns: - model (Ridge): Trained Ridge Regression model. - mean_cv_score (float): Mean cross-validation MSE score. ridge_regression = Ridge(alpha=alpha) cv_scores = cross_val_score(ridge_regression, X, y, cv=5, scoring=\'neg_mean_squared_error\') mean_cv_score = -cv_scores.mean() ridge_regression.fit(X, y) return ridge_regression, mean_cv_score def train_ridge_classifier(X, y, alpha=1.0): Train and evaluate a Ridge Classifier model. Parameters: - X (array-like): Feature data. - y (array-like): Target values. - alpha (float): Regularization strength. Returns: - model (RidgeClassifier): Trained Ridge Classifier model. - mean_cv_score (float): Mean cross-validation accuracy score. ridge_classifier = RidgeClassifier(alpha=alpha) cv_scores = cross_val_score(ridge_classifier, X, y, cv=5, scoring=\'accuracy\') mean_cv_score = cv_scores.mean() ridge_classifier.fit(X, y) return ridge_classifier, mean_cv_score # File paths, create the paths variable according to requirements regression_file_path = \'regression_data.csv\' classification_file_path = \'classification_data.csv\' # Perform data loading regression_data = load_data(regression_file_path) classification_data = load_data(classification_file_path) # Preprocess data X_reg, y_reg = preprocess_data(regression_data, target_column=\'target\') X_clf, y_clf = preprocess_data(classification_data, target_column=\'target\') # Train and evaluate Ridge Regression model ridge_reg_model, ridge_reg_mse = train_ridge_regression(X_reg, y_reg, alpha=1.0) print(f\\"Ridge Regression Model - Mean CV MSE: {ridge_reg_mse}\\") # Train and evaluate Ridge Classifier model ridge_clf_model, ridge_clf_acc = train_ridge_classifier(X_clf, y_clf, alpha=1.0) print(f\\"Ridge Classifier Model - Mean CV Accuracy: {ridge_clf_acc}\\") ``` **Dataset Structure** - `regression_data.csv`: This file contains features and a target column for the regression task. - `classification_data.csv`: This file contains features and a target column for the classification task. The feature columns will be named `feature1, feature2, ..., featureN`, and the target column is named `target`. # Submission Guidelines - Ensure that your code follows Python coding standards. - Include comments to explain your logic. - Your code should load the data, preprocess it, train the models, and print the evaluation metrics. **Constraints**: 1. Use the `Ridge` class for regression and `RidgeClassifier` class for classification from `sklearn.linear_model`. 2. Use `StandardScaler` for scaling the features. Good Luck!","solution":"import pandas as pd from sklearn.linear_model import Ridge, RidgeClassifier from sklearn.preprocessing import StandardScaler from sklearn.model_selection import cross_val_score from sklearn.metrics import mean_squared_error, accuracy_score def load_data(file_path): Load data from a CSV file. Parameters: - file_path (str): Path to the CSV file. Returns: - DataFrame: Loaded pandas DataFrame. df = pd.read_csv(file_path) return df def preprocess_data(df, target_column): Preprocess the data by separating features and target and applying standard scaling. Parameters: - df (DataFrame): DataFrame containing data. - target_column (str): Name of the target column. Returns: - X_scaled (ndarray): Scaled feature data. - y (ndarray): Target values. X = df.drop(columns=[target_column]) y = df[target_column] scaler = StandardScaler() X_scaled = scaler.fit_transform(X) return X_scaled, y def train_ridge_regression(X, y, alpha=1.0): Train and evaluate a Ridge Regression model. Parameters: - X (array-like): Feature data. - y (array-like): Target values. - alpha (float): Regularization strength. Returns: - model (Ridge): Trained Ridge Regression model. - mean_cv_score (float): Mean cross-validation MSE score. ridge_regression = Ridge(alpha=alpha) cv_scores = cross_val_score(ridge_regression, X, y, cv=5, scoring=\'neg_mean_squared_error\') mean_cv_score = -cv_scores.mean() ridge_regression.fit(X, y) return ridge_regression, mean_cv_score def train_ridge_classifier(X, y, alpha=1.0): Train and evaluate a Ridge Classifier model. Parameters: - X (array-like): Feature data. - y (array-like): Target values. - alpha (float): Regularization strength. Returns: - model (RidgeClassifier): Trained Ridge Classifier model. - mean_cv_score (float): Mean cross-validation accuracy score. ridge_classifier = RidgeClassifier(alpha=alpha) cv_scores = cross_val_score(ridge_classifier, X, y, cv=5, scoring=\'accuracy\') mean_cv_score = cv_scores.mean() ridge_classifier.fit(X, y) return ridge_classifier, mean_cv_score # Example usage (assuming paths are correctly set) def main(): regression_file_path = \'regression_data.csv\' classification_file_path = \'classification_data.csv\' # Load data regression_data = load_data(regression_file_path) classification_data = load_data(classification_file_path) # Preprocess data X_reg, y_reg = preprocess_data(regression_data, target_column=\'target\') X_clf, y_clf = preprocess_data(classification_data, target_column=\'target\') # Train and evaluate Ridge Regression model ridge_reg_model, ridge_reg_mse = train_ridge_regression(X_reg, y_reg, alpha=1.0) print(f\\"Ridge Regression Model - Mean CV MSE: {ridge_reg_mse}\\") # Train and evaluate Ridge Classifier model ridge_clf_model, ridge_clf_acc = train_ridge_classifier(X_clf, y_clf, alpha=1.0) print(f\\"Ridge Classifier Model - Mean CV Accuracy: {ridge_clf_acc}\\") if __name__ == \\"__main__\\": main()"},{"question":"You are tasked with implementing a custom logging mechanism that utilizes the `linecache` module to record specific lines from source files at different points during the execution of a program. Your custom logger should enhance traditional logging by including the contents of specific lines from the source code files. Requirements: 1. Implement a class `CustomLogger` with the following methods: - `log_line(filename: str, lineno: int) -> str`: This method should retrieve and return the specified line from the given file using the `linecache.getline` function. - `clear_log_cache() -> None`: This method should clear the internal line cache using the `linecache.clearcache` function. - `validate_cache(filename: Optional[str] = None) -> None`: This method should check and update the validity of the cache using the `linecache.checkcache` function. Constraints: - You are not allowed to modify the `linecache` implementation. - Ensure that your solution handles errors gracefully and makes use of the caching features provided by the `linecache` module. - Input files are assumed to be UTF-8 encoded. Example Usage: ```python from typing import Optional import linecache class CustomLogger: def log_line(self, filename: str, lineno: int) -> str: Retrieve a specific line from a file. :param filename: Name of the file. :param lineno: Line number to retrieve. :return: The content of the specified line or an empty string if an error occurs. return linecache.getline(filename, lineno) def clear_log_cache(self) -> None: Clear the linecache\'s internal cache. linecache.clearcache() def validate_cache(self, filename: Optional[str] = None) -> None: Validate and update the cache if necessary. :param filename: Specific file to validate cache for, or None to validate the entire cache. linecache.checkcache(filename) # Example usage logger = CustomLogger() print(logger.log_line(\\"example.py\\", 10)) # Output the content of line 10 from \\"example.py\\" logger.clear_log_cache() # Clear the log cache logger.validate_cache(\\"example.py\\") # Validate the cache for the file \\"example.py\\" ``` Objective: Ensure that your implementation correctly utilizes the `linecache` module functions and adheres to the specifications outlined above. Your code should demonstrate an understanding of the caching mechanisms and error handling features provided by the `linecache` module.","solution":"from typing import Optional import linecache class CustomLogger: def log_line(self, filename: str, lineno: int) -> str: Retrieve a specific line from a file. :param filename: Name of the file. :param lineno: Line number to retrieve. :return: The content of the specified line or an empty string if an error occurs. try: return linecache.getline(filename, lineno).strip() except Exception as e: return \\"\\" def clear_log_cache(self) -> None: Clear the linecache\'s internal cache. linecache.clearcache() def validate_cache(self, filename: Optional[str] = None) -> None: Validate and update the cache if necessary. :param filename: Specific file to validate cache for, or None to validate the entire cache. linecache.checkcache(filename)"},{"question":"# Custom Module Importer You are tasked with creating a custom module importer in Python. This will involve creating a custom finder and loader to import a specific type of module from a non-standard location, such as a custom file format or a URL. Objective Implement a custom finder and loader that can import Python modules from `.custompy` files. These files will have the same syntax as regular Python files but will use a different extension. Your solution should modify the import system to recognize and properly load these custom modules. Requirements 1. **Custom Finder**: - Should search for modules with a `.custompy` extension. - Implemented as a callable added to `sys.path_hooks`. 2. **Custom Loader**: - Should load and execute code from `.custompy` files. - Must set the appropriate module attributes (e.g., `__name__`, `__loader__`, `__package__`). 3. **Integration**: - Register your custom finder so that it gets invoked during the module import process. Input and Output Format - **Input**: A list of directories (paths) where `.custompy` files may be located. - **Output**: No direct output. The custom import system should be implemented and allow importing `.custompy` files transparently. Example Consider the following directory structure: ``` custom_modules/ example.custompy ``` With `example.custompy` containing: ```python def greet(): return \\"Hello, Custom Importer!\\" ``` Your task is to modify the import system such that the following code works correctly: ```python import example print(example.greet()) # Should output: Hello, Custom Importer! ``` Constraints - Do not use any third-party libraries. - Make sure to handle possible exceptions like `ModuleNotFoundError` appropriately. - Ensure the custom loader integrates seamlessly with Python\'s existing import system. # Instructions 1. **Implement**: Define the custom finder and loader classes. 2. **Register**: Modify `sys.path_hooks` and `sys.meta_path` to include your custom finder. 3. **Test**: Ensure that importing `.custompy` files works as expected. Additional Notes - Remember to handle both absolute and relative imports within `.custompy` modules. - Ensure your loader sets all the appropriate module attributes as defined by the import system specifications.","solution":"import sys import importlib.abc import importlib.util import os class CustomPathFinder(importlib.abc.MetaPathFinder): def find_spec(self, fullname, path, target=None): module_name = fullname.split(\'.\')[-1] if path is None: path = sys.path for entry in path: potential_path = os.path.join(entry, f\\"{module_name}.custompy\\") if os.path.isfile(potential_path): return importlib.util.spec_from_file_location(fullname, potential_path, loader=CustomLoader(potential_path)) return None class CustomLoader(importlib.abc.SourceLoader): def __init__(self, path): self._path = path def get_filename(self, fullname): return self._path def get_data(self, path): with open(path, \'r\', encoding=\'utf-8\') as file: return file.read() def install_custom_importer(): sys.meta_path.insert(0, CustomPathFinder()) install_custom_importer()"},{"question":"**Objective:** To assess the student\'s understanding of the `mimetypes` module in Python and their ability to work with MIME types, file extensions, and file handling. # Problem Statement You are given a list of URLs and file paths. Your task is to write a function that: 1. Determines the MIME type and encoding of each file. 2. Groups the URLs and paths by their MIME type. 3. For each MIME type group, determines the most common file extension. Write a function `process_files(file_list: List[str]) -> Dict[str, Dict[str, Any]]` where: - `file_list`: A list of strings where each string is a URL or a file path. The function should return a dictionary where: - Each key is a MIME type. - Each value is another dictionary with two keys: - `\\"files\\"`: A list of tuples, each containing the original URL/path and its encoding. - `\\"common_extension\\"`: The most common file extension for that MIME type. Example ```python file_list = [ \\"example.tar.gz\\", \\"http://example.com/file.txt\\", \\"http://example.com/file.doc\\", \\"archive.zip\\", \\"sample.tar.gz\\", \\"example.docx\\", \\"document.pdf\\", \\"photo.jpeg\\" ] expected_output = { \\"application/x-tar\\": { \\"files\\": [ (\\"example.tar.gz\\", \\"gzip\\"), (\\"sample.tar.gz\\", \\"gzip\\") ], \\"common_extension\\": \\".tar.gz\\" }, \\"text/plain\\": { \\"files\\": [ (\\"http://example.com/file.txt\\", None) ], \\"common_extension\\": \\".txt\\" }, \\"application/msword\\": { \\"files\\": [ (\\"http://example.com/file.doc\\", None) ], \\"common_extension\\": \\".doc\\" }, \\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\\": { \\"files\\": [ (\\"example.docx\\", None) ], \\"common_extension\\": \\".docx\\" }, \\"application/zip\\": { \\"files\\": [ (\\"archive.zip\\", None) ], \\"common_extension\\": \\".zip\\" }, \\"application/pdf\\": { \\"files\\": [ (\\"document.pdf\\", None) ], \\"common_extension\\": \\".pdf\\" }, \\"image/jpeg\\": { \\"files\\": [ (\\"photo.jpeg\\", None) ], \\"common_extension\\": \\".jpeg\\" } } assert process_files(file_list) == expected_output ``` Constraints: - The input list will contain at most 1000 URLs/paths. - URLs/paths are guaranteed to be unique. - Filenames can include extensions from standard and non-standard MIME types. Notes: - Utilize the functions available in the `mimetypes` module to determine the MIME type, encoding, and file extensions. - Ensure that your solution efficiently handles the input size constraint.","solution":"import mimetypes from typing import List, Dict, Any from collections import defaultdict, Counter def process_files(file_list: List[str]) -> Dict[str, Dict[str, Any]]: result = defaultdict(lambda: {\\"files\\": [], \\"common_extension\\": \\"\\"}) extension_counter = defaultdict(Counter) for file_path in file_list: mime_type, encoding = mimetypes.guess_type(file_path) if mime_type: result[mime_type][\\"files\\"].append((file_path, encoding)) extension = \\".\\" + file_path.split(\'.\')[-1] extension_counter[mime_type][extension] += 1 for mime_type in result: if extension_counter[mime_type]: most_common_extension = extension_counter[mime_type].most_common(1)[0][0] result[mime_type][\\"common_extension\\"] = most_common_extension return result"},{"question":"**Question: Advanced Visualization with Seaborn** You are tasked with analyzing a dataset and presenting the findings in a visually compelling manner using Seaborn. To demonstrate your understanding of the seaborn library, particularly the `cubehelix_palette()` function, follow the instructions below: # Instructions 1. **Dataset**: Use the built-in Seaborn dataset `tips`. 2. **Palette Customization**: Create a customized cubehelix palette with the following specifications: - Number of colors: 10 - Start point of the helix: 3 - Rotation amount in the helix: 1 - Nonlinearity to the luminance ramp: 0.7 - Increased saturation of the colors: 2 - Custom luminance at the start and endpoint: dark at 0.1, light at 0.9 - Reverse the direction of the luminance ramp. 3. **Plot**: Using the customized palette, create a violin plot displaying the distribution of tips received based on the day of the week, differentiated by whether the meals were categorized as lunch or dinner. 4. Ensure your plot includes: - A title: \\"Distribution of Tips by Day and Time\\" - Proper labels for x and y axes. - A legend indicating the meal time (Lunch/Dinner). # Constraints - Do not use any external libraries other than Seaborn, Pandas, and Matplotlib. - Your solution must include comments explaining each major step. # Expected Input Format None (the dataset is to be accessed directly via Seaborn). # Expected Output Format A Seaborn plot displayed inline (as would be typical in a Jupyter notebook). # Example Code ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\'tips\') # Create the customized cubehelix palette palette = sns.cubehelix_palette(10, start=3, rot=1, gamma=0.7, hue=2, dark=0.1, light=0.9, reverse=True) # Create the violin plot sns.violinplot(x=\\"day\\", y=\\"tip\\", hue=\\"time\\", data=tips, palette=palette, split=True) # Add labels and title plt.title(\'Distribution of Tips by Day and Time\') plt.xlabel(\'Day of the Week\') plt.ylabel(\'Tip Amount\') plt.legend(title=\'Meal Time\') # Show the plot plt.show() ``` Provide well-documented code and explanations for each of these steps in your submission.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\'tips\') # Create the customized cubehelix palette with specified parameters palette = sns.cubehelix_palette(n_colors=10, start=3, rot=1, gamma=0.7, hue=2, dark=0.1, light=0.9, reverse=True) # Create the violin plot with the customized palette sns.violinplot(x=\\"day\\", y=\\"tip\\", hue=\\"time\\", data=tips, palette=palette, split=True) # Add title and labels for the plot plt.title(\'Distribution of Tips by Day and Time\') plt.xlabel(\'Day of the Week\') plt.ylabel(\'Tip Amount\') plt.legend(title=\'Meal Time\') # Show the plot plt.show()"},{"question":"# Custom Buffered Stream Implementation **Objective:** To deepen your understanding of the `io` module in Python, specifically focusing on creating custom stream classes that handle buffered I/O operations efficiently. **Task:** Write a Python class `CustomBufferedStream` that mimics the functionality of `io.BufferedIOBase`. Your class should extend `io.IOBase` and implement the following methods and properties: 1. **`__init__(self, initial_bytes: bytes = b\\"\\")`**: Initializes the stream with an optional initial bytes buffer. 2. **`read(self, size: int = -1) -> bytes`**: Reads up to `size` bytes from the stream. If `size` is not given or is negative, reads until EOF. 3. **`write(self, b: bytes) -> int`**: Writes the given bytes object to the stream and returns the number of bytes written. 4. **`seek(self, offset: int, whence: int = 0) -> int`**: Changes the stream position to the given byte offset. The parameter `whence` indicates the reference position and can be `0` (start of the stream), `1` (current stream position), or `2` (end of the stream). 5. **`tell(self) -> int`**: Returns the current stream position. 6. **`flush(self) -> None`**: Flushes the internal buffer to ensure all data is written to the underlying stream structure. 7. **`close(self) -> None`**: Closes the stream and releases any underlying resources. Additional requirements: - Ensure that the read, write, and seek operations appropriately handle the internal buffer. - Raise appropriate exceptions (`ValueError`, `OSError`, `UnsupportedOperation`) where necessary to align with the behaviors described in the `io` module documentation. - Include docstrings for each method to describe their behavior concisely. **Input/Output:** Inputs will be function calls to your class methods with appropriate parameters. Outputs will be the return values of these functions, or any raised exceptions as per the error handling requirements. **Example Usage:** ```python # Create an instance of the custom buffered stream stream = CustomBufferedStream(b\\"Initial data\\") # Write data to the stream stream.write(b\\" More data\\") print(stream.tell()) # Should output the current stream position # Read data from the beginning stream.seek(0) print(stream.read()) # Should output b\\"Initial data More data\\" # Flush and close the stream stream.flush() stream.close() ``` **Constraints:** - The class should handle binary data only and not convert between binary and text formats. - Aim for efficient performance, especially in managing the internal buffer during read/write operations. Your implementation will be evaluated based on correctness, efficiency, and adherence to the described functionalities and constraints.","solution":"import io class CustomBufferedStream(io.IOBase): def __init__(self, initial_bytes: bytes = b\\"\\"): self._buffer = initial_bytes self._position = 0 self._closed = False def read(self, size: int = -1) -> bytes: self._check_closed() if size < 0: size = len(self._buffer) - self._position end_position = min(self._position + size, len(self._buffer)) data = self._buffer[self._position:end_position] self._position = end_position return data def write(self, b: bytes) -> int: self._check_closed() if not isinstance(b, bytes): raise TypeError(\\"a bytes-like object is required, not \'{0}\'\\".format(type(b).__name__)) end_position = self._position + len(b) if end_position > len(self._buffer): self._buffer = self._buffer[:self._position] + b + self._buffer[end_position:] else: self._buffer = self._buffer[:self._position] + b + self._buffer[self._position+len(b):] self._position = end_position return len(b) def seek(self, offset: int, whence: int = 0) -> int: self._check_closed() if whence == 0: new_position = offset elif whence == 1: new_position = self._position + offset elif whence == 2: new_position = len(self._buffer) + offset else: raise ValueError(\\"Invalid value for whence\\") if new_position < 0: raise ValueError(\\"New position is before the start of the stream\\") self._position = new_position return self._position def tell(self) -> int: self._check_closed() return self._position def flush(self) -> None: self._check_closed() # For this custom stream, flush does nothing as we are not buffering further underneath def close(self) -> None: self._closed = True def _check_closed(self) -> None: if self._closed: raise ValueError(\\"Stream is closed\\") def closed(self) -> bool: return self._closed"},{"question":"**Question: Function to Open URL with Browser Preference** Consider you are building a Python application that needs to open web URLs. Sometimes the default browser might not be the best choice, and users might have a preference for a specific browser. Implement a function `open_url_with_preference(url: str, browser_preference: List[str], new_window: bool = False, new_tab: bool = True) -> bool` that attempts to open the given URL using the preferred browsers listed in the `browser_preference` list. If none of the preferred browsers are available, the function should fall back to the system\'s default browser. The function should return `True` if the URL is successfully opened, and `False` otherwise. # Input - `url` (str): The URL to be opened. - `browser_preference` (List[str]): A list of browser names in order of preference. These names should match the type names defined in the `webbrowser` module (e.g., \'firefox\', \'chrome\'). - `new_window` (bool): If set to `True`, open the URL in a new window if possible. Default is `False`. - `new_tab` (bool): If set to `True`, open the URL in a new tab if possible. If both `new_window` and `new_tab` are `True`, `new_tab` takes priority. Default is `True`. # Output - Returns `True` if the URL was successfully opened in any of the preferred browsers or the default browser. Returns `False` otherwise. # Constraints - Ensure the function handles cases where none of the preferred browsers are available. - The function should be cross-platform and handle different operating systems appropriately. # Example ```python def open_url_with_preference(url: str, browser_preference: List[str], new_window: bool = False, new_tab: bool = True) -> bool: import webbrowser success = False for browser in browser_preference: try: b = webbrowser.get(browser) if new_tab: success = b.open_new_tab(url) elif new_window: success = b.open_new(url) else: success = b.open(url) if success: return True except webbrowser.Error: continue # Fall back to default browser try: if new_tab: success = webbrowser.open_new_tab(url) elif new_window: success = webbrowser.open_new(url) else: success = webbrowser.open(url) except webbrowser.Error: success = False return success # Test the function print(open_url_with_preference(\\"https://www.python.org\\", [\\"firefox\\", \\"chrome\\"], new_window=True, new_tab=False)) ``` # Note - The `webbrowser.get()` function is used to retrieve a browser controller object for the specified type. The `webbrowser.Error` exception should be correctly handled to ensure the function continues trying other browsers in the list if one fails. - Reasonable assumptions must be made about the environment, such as the presence of certain browsers based on the OS.","solution":"from typing import List import webbrowser def open_url_with_preference(url: str, browser_preference: List[str], new_window: bool = False, new_tab: bool = True) -> bool: Attempts to open the given URL using the preferred browsers listed in the browser_preference list. Parameters: url (str): The URL to be opened. browser_preference (List[str]): A list of browser names in order of preference. new_window (bool): If set to True, open the URL in a new window if possible. Default is False. new_tab (bool): If set to True, open the URL in a new tab if possible. Default is True. Returns: bool: True if the URL was successfully opened, False otherwise. success = False for browser in browser_preference: try: b = webbrowser.get(browser) if new_tab: success = b.open_new_tab(url) elif new_window: success = b.open_new(url) else: success = b.open(url) if success: return True except webbrowser.Error: continue # Fall back to the default browser try: if new_tab: success = webbrowser.open_new_tab(url) elif new_window: success = webbrowser.open_new(url) else: success = webbrowser.open(url) except webbrowser.Error: success = False return success"},{"question":"# Email Processing with `email.iterators` Module In this assessment, you will demonstrate your understanding of the `email.iterators` module to process the MIME structure of email messages. Task You will write a function `extract_text_payloads(msg)` that takes an email message object `msg` and returns a list of all text payloads within the email. This includes all subparts that have a MIME maintype of `text`, regardless of subtype. Implementation 1. **Function Signature**: ```python def extract_text_payloads(msg) -> list: ``` 2. **Parameters**: - `msg`: an email message object. 3. **Returns**: - A list of string payloads where each string is part of the email\'s text content. 4. **Functional Requirements**: - Your function should use the `email.iterators.typed_subpart_iterator` method to retrieve subparts with a MIME maintype of `text`. - Iterate through the retrieved subparts and extract their payload as a string. - Return a list containing the payloads of these subparts. 5. **Constraints**: - You may assume that `msg` is a well-formed email message object. - You should not include subparts that do not have a MIME maintype of `text`. 6. **Example**: ```python from email import message_from_string from email.iterators import typed_subpart_iterator email_content = MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=\\"===============6011882145974710232==\\" --===============6011882145974710232== Content-Type: text/plain This is the body of the email. --===============6011882145974710232== Content-Type: text/html <html><body>This is the HTML part of the email.</body></html> --===============6011882145974710232==-- msg = message_from_string(email_content) def extract_text_payloads(msg): payloads = [] for part in typed_subpart_iterator(msg, maintype=\'text\'): payloads.append(part.get_payload(decode=True).decode(\'utf-8\')) return payloads # Function call result = extract_text_payloads(msg) print(result) # Output: [\'This is the body of the email.\', \'<html><body>This is the HTML part of the email.</body></html>\'] ``` Notes - The message string in the example is formatted with MIME boundaries to simulate a typical email structure. - This should ensure that all text maintype subparts, regardless of subtype, are included in the result. Evaluation Criteria: - Correctness: The function must correctly retrieve and return all text payloads from the email message. - Use of `email.iterators.typed_subpart_iterator`: Your implementation must use this function as specified. - Handling of Decoding: Ensure payloads are correctly decoded to string format as necessary. - Code Readability: Your code should be clear, well-commented, and adhere to Python coding conventions.","solution":"from email.iterators import typed_subpart_iterator def extract_text_payloads(msg): Extracts and returns all text payloads from the email message object. Args: msg: An email message object. Returns: A list of string payloads where each string is part of the email\'s text content. payloads = [] for part in typed_subpart_iterator(msg, maintype=\'text\'): payloads.append(part.get_payload(decode=True).decode(\'utf-8\')) return payloads"},{"question":"# Health Spending Visualization with Seaborn You are provided with the `healthexp` dataset which contains information on health expenditures over time for different countries. Your task is to create two separate line plots using Seaborn\'s object-oriented interface (`so.Plot`) to visualize this data. Additionally, you need to provide a function to normalize and label the data appropriately. Task 1: Plot Health Spending Over Time 1. **Function Signature:** ```python def plot_health_spending_over_time(): pass ``` 2. **Implementation Details:** - **Load Dataset:** Load the `healthexp` dataset using the `load_dataset` function. - **Create Plot:** - Set `Year` as the `x` axis. - Set `Spending_USD` as the `y` axis. - Use `Country` to distinguish different lines in the plot. - **Normalization:** - Normalize the `Spending_USD` values relative to the maximum spending per country. - **Labeling:** - Set the y-axis label to \\"Spending relative to maximum amount\\". - **Display Plot:** Render the plot. 3. **Output Format:** The function should produce a line plot. Task 2: Plot Percent Change in Health Spending 1. **Function Signature:** ```python def plot_percent_change_in_health_spending(): pass ``` 2. **Implementation Details:** - **Load Dataset:** Load the `healthexp` dataset using the `load_dataset` function. - **Create Plot:** - Set `Year` as the `x` axis. - Set `Spending_USD` as the `y` axis. - Use `Country` to distinguish different lines in the plot. - **Normalization:** - Normalize the data to show the percent change in spending from the year 1970. - **Labeling:** - Set the y-axis label to \\"Percent change in spending from 1970 baseline\\". - **Display Plot:** Render the plot. 3. **Output Format:** The function should produce a line plot. Feel free to use additional Seaborn functionalities for aesthetics if needed. The focus should be on correct data normalization and appropriate labeling of the axes. Example ```python # Example function call plot_health_spending_over_time() ``` ```python # Example function call plot_percent_change_in_health_spending() ``` Note: The above function definitions should load the dataset, create the respective plots as per the instructions, and display them.","solution":"import seaborn.objects as so import seaborn as sns import pandas as pd def plot_health_spending_over_time(): # Load the dataset healthexp = sns.load_dataset(\'healthexp\') # Normalize the Spending_USD values relative to the maximum spending per country healthexp[\'Spending_rel\'] = healthexp.groupby(\'Country\')[\'Spending_USD\'].transform(lambda x: x / x.max()) # Create the plot p = so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_rel\\", color=\\"Country\\") p = p.add(so.Line()) p = p.label(y=\'Spending relative to maximum amount\') p.show() def plot_percent_change_in_health_spending(): # Load the dataset healthexp = sns.load_dataset(\'healthexp\') # Normalize the data to show the percent change in spending from the year 1970 healthexp[\'base_1970\'] = healthexp.groupby(\'Country\')[\'Spending_USD\'].transform(lambda x: x.loc[x.index[0]]) healthexp[\'Percent_change\'] = healthexp.groupby(\'Country\')[\'Spending_USD\'].transform(lambda x: (x / x.loc[x.index[0]]) * 100) # Create the plot p = so.Plot(healthexp, x=\\"Year\\", y=\\"Percent_change\\", color=\\"Country\\") p = p.add(so.Line()) p = p.label(y=\'Percent change in spending from 1970 baseline\') p.show()"},{"question":"# Embedding Python in a C Program **Objective:** Create a C program that embeds the Python interpreter to execute a specific Python script and handle data conversion between C and Python. Task: Write a C program that: 1. **Initializes the Python interpreter**. 2. **Executes** a Python script that contains a function called `calculate_area` which computes the area of a rectangle given its length and width. 3. **Calls the `calculate_area` function** from the C program with arguments provided via the command line. 4. **Retrieves and prints the result** in the C program. Python Script: Your C program must call a function `calculate_area` defined in a Python script provided as the first command line argument. Assume the script is named `rectangle.py` and has the following content: ```python def calculate_area(length, width): return length * width ``` Requirements: 1. **Include Python Headers**: ```c #define PY_SSIZE_T_CLEAN #include <Python.h> ``` 2. **Initialize the Python Interpreter**: - Use `Py_Initialize()` and `Py_FinalizeEx()`. - (Optional) Set the program name using `Py_SetProgramName()`. 3. **Load and Execute Python Script**: - Load the script using `PyImport_Import()`. - Retrieve the `calculate_area` function using `PyObject_GetAttrString()`. 4. **Pass Arguments from Command Line**: - Your C program should take three additional command line arguments: the filename of the Python script, the length of the rectangle, and the width of the rectangle. - Convert these command line arguments to appropriate Python objects. 5. **Call the Python Function**: - Use `PyObject_CallObject()` to call the `calculate_area` function with the arguments. - Retrieve the result and convert it back to a C data type. 6. **Error Handling**: - Include error checking at each step of the process, including verifying the function exists and is callable. - Print appropriate error messages if any issues are encountered. Example Program Usage: 1. Save the Python script as `rectangle.py` with the content specified above. 2. Compile the C program (let\'s name the executable `call_python`). 3. Run the program with arguments: ```sh ./call_python rectangle.py calculate_area 5 10 ``` 4. Expected Output: ``` Area of the rectangle: 50 ``` Compilation and Linking Hints: - Ensure you link with the Python libraries using flags provided by `pythonX.Y-config`: ```sh gcc -o call_python call_python.c (python3.10-config --cflags --ldflags --embed) ``` **Constraints**: - Use only standard libraries and the Python C API. - Ensure the program handles incorrect inputs gracefully. **Submission**: - Submit the C source code file (e.g., `call_python.c`). - Provide a README file with compilation and execution instructions.","solution":"import sys def calculate_area(length, width): Calculates the area of a rectangle given length and width. try: l = float(length) w = float(width) return l * w except ValueError: raise ValueError(\\"Both length and width must be numeric.\\")"},{"question":"**Question: Implement an Asynchronous File Reader with Custom Exception Handling** You are tasked with implementing an asynchronous file reader that reads from a text file line by line and processes each line. While reading the file, your implementation must handle various exceptions provided by the `asyncio` module as documented. # Requirements: 1. Implement a function `async def async_file_reader(file_path: str, separator: str, timeout: int) -> int`. 2. The function should: - Open the file at `file_path` and read it line by line. - Count and return the number of lines successfully read. 3. The function must handle the following exceptions: - **asyncio.TimeoutError:** If the line reading operation exceeds the given `timeout`. - **asyncio.CancelledError:** If the task is cancelled. - **asyncio.IncompleteReadError:** If a line read operation is incomplete. In this case, check the `expected` and `partial` attributes. - **asyncio.LimitOverrunError:** If the buffer size limit is reached while looking for the `separator`. Use the `consumed` attribute for processing adjustments. # Input: - `file_path` (str): The path to the text file to be read. - `separator` (str): The separator to look for in each line. - `timeout` (int): The maximum time allowed for reading each line, in seconds. # Output: - Return the total count of lines read successfully (int). # Constraints: - The file is assumed to be available and readable. - The separator is a single character. - The timeout is a positive integer. # Example Usage: ```python import asyncio async def main(): line_count = await async_file_reader(\'sample.txt\', \'n\', 5) print(f\\"Total lines read: {line_count}\\") asyncio.run(main()) ``` # Note: - Ensure proper handling of each specified exception. - Follow good practice with clean-ups and resource management.","solution":"import asyncio async def async_file_reader(file_path: str, separator: str, timeout: int) -> int: line_count = 0 try: async with await asyncio.open_file(file_path, \'r\') as file: while not file.at_eof(): try: line = await asyncio.wait_for(file.readline(), timeout=timeout) line_count += 1 except asyncio.TimeoutError: print(f\\"TimeoutError: Reading line timed out after {timeout} seconds.\\") except asyncio.CancelledError: print(\\"CancelledError: Reading task was cancelled.\\") raise except asyncio.IncompleteReadError as e: print(f\\"IncompleteReadError: Expected {e.expected}, but got {e.partial}.\\") except asyncio.LimitOverrunError as e: print(f\\"LimitOverrunError: Limit overrun with {e.consumed} characters consumed.\\") await file.readexactly(e.consumed) # Adjusting the buffer except Exception as e: print(f\\"An unexpected error occurred: {e}\\") return line_count"},{"question":"# XML File Manipulation and XPath Queries **Background**: In this exercise, you will work with the `xml.etree.ElementTree` module to parse, navigate, and modify an XML document. You will also use XPath to find specific elements in the XML tree. **Objective**: Given an XML document representing a catalog of books, perform several operations to validate your understanding of the `xml.etree.ElementTree` module. **Instructions**: 1. **Parsing the XML**: - Parse the provided XML string into an `ElementTree` object. 2. **XPath Query**: - Use XPath to find the title of the book with the ISBN \\"0-596-52068-9\\". 3. **Modify XML Tree**: - Add a new book to the catalog with the following details: - Title: \\"New Book\\" - Author: \\"New Author\\" - Publish Date: \\"2023\\" - ISBN: \\"0-123-45678-9\\" - Price: \\"29.99\\" 4. **Namespace Handling**: - Update the `xmlns` attribute for the root element of the tree to \\"http://example.com/catalog\\". 5. **Canonicalize XML**: - Canonicalize the modified XML and return it as a string. **Constraints**: - The XML structure should remain well-formed. - Ensure the code performs efficiently with a focus on readability and maintainability. # XML Data ```xml <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> <isbn>0-596-52068-9</isbn> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <isbn>0-596-52069-7</isbn> </book> </catalog> ``` # Function Signature You need to complete the following function: ```python import xml.etree.ElementTree as ET def manipulate_xml(xml_string: str) -> str: # Parse the XML string tree = ET.ElementTree(ET.fromstring(xml_string)) # XPath query for the title of the book with ISBN \\"0-596-52068-9\\" title = tree.find(\\".//book[isbn=\'0-596-52068-9\']/title\\").text # Print or return the title for verification (optional based on requirements) print(\\"Book Title:\\", title) # Example output: XML Developer\'s Guide # Add a new book to the catalog root = tree.getroot() new_book = ET.SubElement(root, \\"book\\", attrib={\\"id\\": \\"bk103\\"}) ET.SubElement(new_book, \\"author\\").text = \\"New Author\\" ET.SubElement(new_book, \\"title\\").text = \\"New Book\\" ET.SubElement(new_book, \\"genre\\").text = \\"Fiction\\" ET.SubElement(new_book, \\"price\\").text = \\"29.99\\" ET.SubElement(new_book, \\"publish_date\\").text = \\"2023\\" ET.SubElement(new_book, \\"description\\").text = \\"A thrilling new book.\\" ET.SubElement(new_book, \\"isbn\\").text = \\"0-123-45678-9\\" # Update the xmlns attribute for the root element root.attrib[\\"xmlns\\"] = \\"http://example.com/catalog\\" # Canonicalize the modified XML and return as a string canonical_xml = ET.canonicalize(ET.tostring(tree.getroot(), encoding=\'unicode\')) return canonical_xml ``` **Note**: Ensure your implementation adheres to the given `manipulate_xml` function signature. Feel free to expand and test the function using the provided XML data.","solution":"import xml.etree.ElementTree as ET def manipulate_xml(xml_string: str) -> str: # Parse the XML string tree = ET.ElementTree(ET.fromstring(xml_string)) # XPath query for the title of the book with ISBN \\"0-596-52068-9\\" title = tree.find(\\".//book[isbn=\'0-596-52068-9\']/title\\").text # Print the title for verification print(\\"Book Title:\\", title) # Example output: XML Developer\'s Guide # Add a new book to the catalog root = tree.getroot() new_book = ET.SubElement(root, \\"book\\", attrib={\\"id\\": \\"bk103\\"}) ET.SubElement(new_book, \\"author\\").text = \\"New Author\\" ET.SubElement(new_book, \\"title\\").text = \\"New Book\\" ET.SubElement(new_book, \\"genre\\").text = \\"Fiction\\" ET.SubElement(new_book, \\"price\\").text = \\"29.99\\" ET.SubElement(new_book, \\"publish_date\\").text = \\"2023\\" ET.SubElement(new_book, \\"description\\").text = \\"A thrilling new book.\\" ET.SubElement(new_book, \\"isbn\\").text = \\"0-123-45678-9\\" # Update the xmlns attribute for the root element root.attrib[\\"xmlns\\"] = \\"http://example.com/catalog\\" # Return the modified XML as a string modified_xml = ET.tostring(tree.getroot(), encoding=\'unicode\', method=\'xml\') return modified_xml"},{"question":"**Objective:** To evaluate the student\'s understanding and ability to implement and optimize a `KNeighborsClassifier` model using scikit-learn\'s nearest neighbor utilities, including the usage of `KNeighborsTransformer`. Problem Statement: You are tasked with building a predictive model using the `KNeighborsClassifier` to classify the famous Iris dataset. The task consists of two parts: 1. **Building a Classification Model:** Using the `KNeighborsClassifier` with `k=5`. You should preprocess the data by scaling it appropriately and splitting it into training and test datasets. 2. **Optimizing with Precomputed Neighbors:** Transform the data using `KNeighborsTransformer` with `mode=\'distance\'` and use the precomputed neighbors for classification to evaluate any potential performance improvements. Instructions: 1. **Load the Iris Dataset:** - Use `sklearn.datasets.load_iris` to load the dataset. - Scale the features using `StandardScaler`. 2. **Part 1 - Basic KNeighborsClassifier:** - Split the dataset into training and test sets using an 80-20 split. - Instantiate a `KNeighborsClassifier` with `n_neighbors=5`. - Train the classifier on the training set. - Evaluate the classifier on the test set and print the accuracy. 3. **Part 2 - Optimized with Precomputed Neighbors:** - Use `KNeighborsTransformer` with `mode=\'distance\'` to precompute the nearest neighbors graph from the training data. - Use the precomputed graph to fit a new `KNeighborsClassifier` model. - Evaluate this new classifier on the test set and print the accuracy. Code Requirements: - Implement all steps using scikit-learn. - Ensure reproducibility by setting `random_state=42` wherever applicable. - Clearly comment on each step of the implementation to demonstrate your understanding. Expected Function Signature: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier, KNeighborsTransformer from sklearn.pipeline import Pipeline def build_and_evaluate_knn(): # Step 1: Load and scale the data iris = load_iris() X, y = iris.data, iris.target scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 2: Split into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Part 1: Basic KNeighborsClassifier knn_basic = KNeighborsClassifier(n_neighbors=5) knn_basic.fit(X_train, y_train) accuracy_basic = knn_basic.score(X_test, y_test) print(f\\"Basic KNeighborsClassifier accuracy: {accuracy_basic}\\") # Part 2: Optimized with Precomputed Neighbors knn_transformer = KNeighborsTransformer(n_neighbors=5, mode=\'distance\') knn_transformer.fit(X_train) X_train_transformed = knn_transformer.transform(X_train) knn_optimized = KNeighborsClassifier(n_neighbors=5) knn_optimized.fit(X_train_transformed, y_train) # Test the optimized model with precomputed neighbors X_test_transformed = knn_transformer.transform(X_test) accuracy_optimized = knn_optimized.score(X_test_transformed, y_test) print(f\\"Optimized KNeighborsClassifier with precomputed neighbors accuracy: {accuracy_optimized}\\") # Call the function build_and_evaluate_knn() ``` Constraints: - You must use `StandardScaler` for scaling the features. - The dataset split must use an 80-20 train-test ratio. - The `KNeighborsTransformer` should use `mode=\'distance\'` and `n_neighbors=5`. Evaluation Criteria: - Correct implementation of the KNeighbors classifier with and without precomputed neighbors. - Appropriate data preprocessing. - Clear and concise comments explaining each part of the code. - Comparison of the model accuracies to demonstrate the effect of precomputing neighbors.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier, KNeighborsTransformer def build_and_evaluate_knn(): # Step 1: Load and scale the data iris = load_iris() X, y = iris.data, iris.target scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 2: Split into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Part 1: Basic KNeighborsClassifier knn_basic = KNeighborsClassifier(n_neighbors=5) knn_basic.fit(X_train, y_train) accuracy_basic = knn_basic.score(X_test, y_test) print(f\\"Basic KNeighborsClassifier accuracy: {accuracy_basic}\\") # Part 2: Optimized with Precomputed Neighbors knn_transformer = KNeighborsTransformer(n_neighbors=5, mode=\'distance\') knn_transformer.fit(X_train) X_train_transformed = knn_transformer.transform(X_train) knn_optimized = KNeighborsClassifier(n_neighbors=5, metric=\'precomputed\') knn_optimized.fit(X_train_transformed, y_train) # Transform the test set using the same transformer X_test_transformed = knn_transformer.transform(X_test) accuracy_optimized = knn_optimized.score(X_test_transformed, y_test) print(f\\"Optimized KNeighborsClassifier with precomputed neighbors accuracy: {accuracy_optimized}\\") # Return the accuracies for testing purposes return accuracy_basic, accuracy_optimized # Call the function build_and_evaluate_knn()"},{"question":"Objective: Your task is to implement a function that utilizes `asyncio.Future` objects to concurrently download multiple URLs and gather their responses. The function should handle any exceptions that occur during the downloads and ensure that all results or exceptions are properly collected and returned. Function Signature: ```python import asyncio async def fetch_all(urls: List[str]) -> Dict[str, Union[str, Exception]]: # Your implementation here ``` Input: - `urls` (List[str]): A list of URLs to be downloaded asynchronously. Output: - Returns a dictionary where the keys are the URLs and the values are either: - The content of the URL as a string if the download was successful. - The exception object if the download failed. Requirements: 1. Use `asyncio.Future` objects to manage the asynchronous tasks. 2. Each URL should be downloaded using an asynchronous HTTP request. 3. Collect results or exceptions for each URL. 4. Return all results and exceptions in a dictionary. Constraints: - Assume a maximum of 20 URLs. - The HTTP request for downloading can use any library like `aiohttp`. Example: ```python import aiohttp import asyncio from typing import List, Dict, Union async def download(url: str) -> str: async with aiohttp.ClientSession() as session: async with session.get(url) as response: return await response.text() async def fetch_all(urls: List[str]) -> Dict[str, Union[str, Exception]]: loop = asyncio.get_running_loop() futures = {url: loop.create_future() for url in urls} async def fetch_and_set_future(url: str, future: asyncio.Future): try: result = await download(url) future.set_result(result) except Exception as e: future.set_exception(e) tasks = [fetch_and_set_future(url, future) for url, future in futures.items()] await asyncio.gather(*tasks) return {url: (future.result() if future.done() else future.exception()) for url, future in futures.items()} # Example usage urls = [\\"http://example.com\\", \\"http://invalid.url\\"] results = asyncio.run(fetch_all(urls)) print(results) ``` **Explanation:** 1. The `fetch_all` function takes a list of URLs and sets up Future objects for each URL. 2. It defines a helper function `fetch_and_set_future` to handle the download and set results or exceptions in the respective Future objects. 3. It creates and schedules tasks for fetching all the URLs concurrently. 4. It waits for all tasks to complete using `asyncio.gather`. 5. It collects and returns the results or exceptions in a dictionary. Ensure your function handles all edge cases, such as network errors or invalid URLs, and that it runs efficiently within the constraints.","solution":"import asyncio import aiohttp from typing import List, Dict, Union async def download(url: str) -> str: async with aiohttp.ClientSession() as session: async with session.get(url) as response: return await response.text() async def fetch_all(urls: List[str]) -> Dict[str, Union[str, Exception]]: loop = asyncio.get_running_loop() futures = {url: loop.create_future() for url in urls} async def fetch_and_set_future(url: str, future: asyncio.Future): try: result = await download(url) future.set_result(result) except Exception as e: future.set_exception(e) tasks = [fetch_and_set_future(url, future) for url, future in futures.items()] await asyncio.gather(*tasks) return {url: (future.result() if future.done() else future.exception()) for url, future in futures.items()} # Example usage if __name__ == \\"__main__\\": urls = [\\"http://example.com\\", \\"http://invalid.url\\"] results = asyncio.run(fetch_all(urls)) print(results)"},{"question":"# Email Charset Conversion In this assessment, you will demonstrate your understanding of the `email.charset` module in Python by implementing several methods related to character set conversions and encodings for email messages. Task 1. Implement a function `convert_charset` that takes an input string and converts it from one character set to another using the `email.charset` module. 2. Implement a function `encode_header` that takes an input string and encodes it according to specified character set encoding rules for email headers. 3. Implement a function `encode_body` that takes an input string and encodes it according to specified character set encoding rules for email bodies. Specifications 1. `convert_charset(input_string: str, from_charset: str, to_charset: str) -> str`: - Converts `input_string` from `from_charset` to `to_charset`. - Raises an exception if the conversion is not possible. - Uses the `Charset` class and its attributes/methods for conversion. 2. `encode_header(input_string: str, charset_name: str) -> str`: - Encodes `input_string` for an email header using the character set specified by `charset_name`. - Uses the `Charset` class and its `header_encode()` method. 3. `encode_body(input_string: str, charset_name: str) -> str`: - Encodes `input_string` for an email body using the character set specified by `charset_name`. - Uses the `Charset` class and its `body_encode()` method. Constraints - You may assume all character sets used are registered and their codecs are available. - Ensure that the input and output strings are in the correct encoding format based on specified character sets. - Do not use any third-party libraries beyond the provided `email.charset` module. Example ```python from email.charset import Charset def convert_charset(input_string: str, from_charset: str, to_charset: str) -> str: from_charset_obj = Charset(from_charset) to_charset_obj = Charset(to_charset) unicode_str = input_string.encode(from_charset_obj.input_codec).decode(\'unicode_escape\') result_str = unicode_str.encode(to_charset_obj.output_codec).decode(to_charset_obj.output_charset) return result_str def encode_header(input_string: str, charset_name: str) -> str: charset_obj = Charset(charset_name) return charset_obj.header_encode(input_string) def encode_body(input_string: str, charset_name: str) -> str: charset_obj = Charset(charset_name) return charset_obj.body_encode(input_string) ``` Your implementation should correctly convert and encode strings as demonstrated. Please ensure efficient and error-free operations. Good luck!","solution":"from email.charset import Charset from email.errors import CharsetError def convert_charset(input_string: str, from_charset: str, to_charset: str) -> str: try: from_charset_obj = Charset(from_charset) to_charset_obj = Charset(to_charset) unicode_str = input_string.encode(from_charset_obj.input_codec).decode(\'unicode_escape\') result_str = unicode_str.encode(to_charset_obj.output_codec).decode(to_charset_obj.output_charset) return result_str except (LookupError, UnicodeError, CharsetError) as e: raise Exception(f\\"Charset conversion error: {str(e)}\\") def encode_header(input_string: str, charset_name: str) -> str: try: charset_obj = Charset(charset_name) return charset_obj.header_encode(input_string) except (LookupError, UnicodeError, CharsetError) as e: raise Exception(f\\"Header encoding error: {str(e)}\\") def encode_body(input_string: str, charset_name: str) -> str: try: charset_obj = Charset(charset_name) return charset_obj.body_encode(input_string) except (LookupError, UnicodeError, CharsetError) as e: raise Exception(f\\"Body encoding error: {str(e)}\\")"},{"question":"# Python Coding Assessment Question Objective Create a custom exception class and demonstrate its usage in a Python function. This task will assess your understanding of exception handling in Python, including defining custom exceptions and raising them appropriately based on certain conditions. Problem Statement Implement a Python function and a custom exception class as specified below: 1. **Custom Exception Class**: Define a custom exception class named `ValidationError` that inherits from the base `Exception` class. This custom exception should accept an error message and a field name (the field associated with the error) as parameters. 2. **Function Definition**: Implement a function named `validate_person` which takes two parameters: `name` (a string) and `age` (an integer). - If the `name` is not a string or is an empty string, raise a `ValidationError` with an appropriate error message and the field name \'name\'. - If the `age` is not an integer or is less than 0, raise a `ValidationError` with an appropriate error message and the field name \'age\'. 3. **Exception Handling**: Write a main block to test the `validate_person` function. This block should: - Call `validate_person` with various test inputs. - Catch any `ValidationError` exceptions raised. - Print the error message and the field name associated with the `ValidationError`. Function Signature ```python class ValidationError(Exception): def __init__(self, message, field): self.message = message self.field = field super().__init__(self.message) def __str__(self): return f\\"{self.message} in field \'{self.field}\'\\" def validate_person(name: str, age: int): pass if __name__ == \\"__main__\\": # Test cases try: validate_person(\\"\\", 25) except ValidationError as e: print(e) try: validate_person(123, 25) except ValidationError as e: print(e) try: validate_person(\\"Alice\\", -5) except ValidationError as e: print(e) try: validate_person(\\"Bob\\", \\"twenty\\") except ValidationError as e: print(e) try: validate_person(\\"Charlie\\", 30) print(\\"Charlie passed validation.\\") except ValidationError as e: print(e) ``` Input - `name`: A string representing the person\'s name. - `age`: An integer representing the person\'s age. Output - Prints appropriate error messages with field names when exceptions are raised. - Prints a success message when validations pass without raising any exceptions. Constraints - The `name` parameter should be a non-empty string. - The `age` parameter should be a non-negative integer. Performance Requirements - The function should handle basic validation efficiently. - Focus on correct implementation of exception handling. Good luck!","solution":"class ValidationError(Exception): def __init__(self, message, field): self.message = message self.field = field super().__init__(self.message) def __str__(self): return f\\"{self.message} in field \'{self.field}\'\\" def validate_person(name: str, age: int): if not isinstance(name, str) or not name: raise ValidationError(\\"Invalid name\\", \\"name\\") if not isinstance(age, int) or age < 0: raise ValidationError(\\"Invalid age\\", \\"age\\")"},{"question":"# Question: You have been given a dataset of randomly generated numeric data, as well as some categorical data. Your task is to write a function in Python that will utilize Seaborn to create and display three distinct types of visualizations using different types of color palettes (qualitative, sequential, and diverging). Each of the three visualizations should focus on different data and should appropriately use a color palette that enhances the data being displayed. Input Format: Your function should generate three datasets: 1. Categorical data with multiple categories. 2. Numeric data ranging from low to high values. 3. Dataset with both positive and negative numeric values centered around zero. Output Format: Three plots displayed using Seaborn: 1. A scatter plot visualizing the categorical data using a qualitative palette. 2. A heatmap visualizing the numeric data using a sequential palette. 3. A diverging bar plot visualizing the data that includes both positive and negative values. Constraints: - Use appropriate Seaborn palettes for each type of data. - Ensure that the visualizations are clear and correctly labeled. Function Signature: ```python def visualize_seaborn_palettes(): pass ``` Example: The function should not only generate the plots but also display them in sequence using `matplotlib` embedded within a Jupyter notebook or a similar interactive shell. ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt def visualize_seaborn_palettes(): # Categorical Data categories = [\'A\', \'B\', \'C\', \'D\'] num_points = 100 scatter_x = np.random.rand(num_points) scatter_y = np.random.rand(num_points) scatter_hue = np.random.choice(categories, num_points) plt.figure() sns.scatterplot(x=scatter_x, y=scatter_y, hue=scatter_hue, palette=\\"Set3\\") plt.title(\'Scatter Plot with Qualitative Palette\') plt.show() # Numeric Data heatmap_data = np.random.rand(10, 12) plt.figure() sns.heatmap(heatmap_data, cmap=\\"rocket\\", annot=True) plt.title(\'Heatmap with Sequential Palette\') plt.show() # Diverging Data diverging_values = np.random.randn(100) categories_diverging = np.arange(100) plt.figure() sns.barplot(x=categories_diverging, y=diverging_values, palette=\\"coolwarm\\") plt.title(\'Diverging Bar Plot\') plt.show() # Call the function to visualize the plots visualize_seaborn_palettes() ``` In this example: - The scatter plot uses a qualitative palette (`Set3`) for categorical data. - The heatmap uses a sequential palette (`rocket`) for numeric data. - The bar plot uses a diverging palette (`coolwarm`) for data around zero. Ensure to provide appropriate legends, titles, and labels to make the visualizations informative and professional.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def visualize_seaborn_palettes(): # Categorical Data categories = [\'A\', \'B\', \'C\', \'D\'] num_points = 100 scatter_x = np.random.rand(num_points) scatter_y = np.random.rand(num_points) scatter_hue = np.random.choice(categories, num_points) plt.figure() sns.scatterplot(x=scatter_x, y=scatter_y, hue=scatter_hue, palette=\\"Set3\\") plt.title(\'Scatter Plot with Qualitative Palette\') plt.xlabel(\'X-axis\') plt.ylabel(\'Y-axis\') plt.legend(title=\'Category\') plt.show() # Numeric Data heatmap_data = np.random.rand(10, 12) plt.figure() sns.heatmap(heatmap_data, cmap=\\"rocket\\", annot=True) plt.title(\'Heatmap with Sequential Palette\') plt.xlabel(\'Column\') plt.ylabel(\'Row\') plt.show() # Diverging Data diverging_values = np.random.randn(100) categories_diverging = np.arange(100) plt.figure(figsize=(10, 5)) sns.barplot(x=categories_diverging, y=diverging_values, palette=\\"coolwarm\\") plt.title(\'Diverging Bar Plot\') plt.xlabel(\'Category\') plt.ylabel(\'Value\') plt.show() # Call the function to visualize the plots visualize_seaborn_palettes()"},{"question":"# Coding Assessment Task Background Using the `unittest` and `unittest.mock` modules in Python, you can create comprehensive test suites to validate the functionality of your code. These modules allow you to write tests that ensure your methods and classes behave as expected under various conditions, including scenarios where you might need to mock complex interactions with external systems. Problem Statement You are required to write a Python class `OrderProcessor` which processes customer orders for an e-commerce platform. The class should have methods to validate, process, and archive orders. Additionally, you need to write a comprehensive test suite for this class using `unittest` and `unittest.mock`. ```python class OrderProcessor: def __init__(self, order_data): Initialize with order data. self.order_data = order_data def validate_order(self): Validate order data. Returns True if valid, False otherwise. An order is considered valid if it has \'id\', \'customer\', and \'items\' keys. if not isinstance(self.order_data, dict): return False required_keys = {\'id\', \'customer\', \'items\'} return required_keys.issubset(set(self.order_data.keys())) def process_payment(self): Processes the payment for the order. Returns True if the payment is successful, False otherwise. This method should be mocked in tests. # Simulate payment processing # In reality, this would interact with a payment gateway return True def archive_order(self): Archives the order. This method should be mocked in tests. # In reality, this would save the order data to a database or file system pass def process_order(self): Processes the order by validating it, processing payment, and archiving it. Returns True if all steps are successful, False otherwise. if not self.validate_order(): return False if not self.process_payment(): return False self.archive_order() return True ``` Task 1. **Implementation:** - Implement the `OrderProcessor` class as provided above, ensuring that it meets the described functionality. 2. **Unit Test Suite:** - Write a test suite for `OrderProcessor` using the `unittest` module. Requirements - The `validate_order` method should be thoroughly tested with various valid and invalid order data. - The `process_payment` method should be mocked in your tests to simulate both successful and failed payment processing. - The `archive_order` method should be mocked to verify that it’s being called during the order processing. - Test the overall `process_order` method, mocking external interactions and verifying the correct behavior. Example Test Cases Here is an outline of what your unit tests might look like: ```python import unittest from unittest.mock import patch class TestOrderProcessor(unittest.TestCase): def test_validate_order(self): valid_order = {\'id\': 1, \'customer\': \'Alice\', \'items\': [\'item1\', \'item2\']} invalid_order = {\'customer\': \'Alice\', \'items\': [\'item1\', \'item2\']} processor_valid = OrderProcessor(valid_order) processor_invalid = OrderProcessor(invalid_order) self.assertTrue(processor_valid.validate_order()) self.assertFalse(processor_invalid.validate_order()) @patch.object(OrderProcessor, \'process_payment\') @patch.object(OrderProcessor, \'archive_order\') def test_process_order(self, mock_archive, mock_payment): mock_payment.return_value = True order = {\'id\': 1, \'customer\': \'Alice\', \'items\': [\'item1\', \'item2\']} processor = OrderProcessor(order) self.assertTrue(processor.process_order()) mock_payment.assert_called_once() mock_archive.assert_called_once() if __name__ == \'__main__\': unittest.main() ``` Deliverables - An implemented `OrderProcessor` class. - A comprehensive `unittest`-based test suite covering the requirements above.","solution":"class OrderProcessor: def __init__(self, order_data): Initialize with order data. self.order_data = order_data def validate_order(self): Validate order data. Returns True if valid, False otherwise. An order is considered valid if it has \'id\', \'customer\', and \'items\' keys. if not isinstance(self.order_data, dict): return False required_keys = {\'id\', \'customer\', \'items\'} return required_keys.issubset(set(self.order_data.keys())) def process_payment(self): Processes the payment for the order. Returns True if the payment is successful, False otherwise. This method should be mocked in tests. # Simulate payment processing return True def archive_order(self): Archives the order. This method should be mocked in tests. # In reality, this would save the order data to a database or file system pass def process_order(self): Processes the order by validating it, processing payment, and archiving it. Returns True if all steps are successful, False otherwise. if not self.validate_order(): return False if not self.process_payment(): return False self.archive_order() return True"},{"question":"**Asynchronous Computations with PyTorch Futures** **Objective:** Your task is to implement two functions using the `torch.futures` module to handle asynchronous computations. This will assess your understanding of managing futures and synchronizing their results. **Problem Statement:** 1. **Implement the `compute_async` function:** - This function simulates an asynchronous computation. - **Input**: An integer `x`. - **Output**: A `torch.futures.Future` object representing the asynchronous computation of `x * x`. ```python import torch.futures def compute_async(x: int) -> torch.futures.Future: Simulates an asynchronous computation that squares the input number. Args: x (int): The input number to be squared. Returns: torch.futures.Future: A future object that will hold the result of the computation. pass ``` 2. **Implement the `aggregate_futures` function:** - This function takes a list of integers, computes their squares asynchronously, and returns the sum of these squares once all computations are complete. - **Input**: A list of integers. - **Output**: An integer representing the sum of the squares of the input list. ```python import torch.futures def aggregate_futures(nums: [int]) -> int: Computes the sum of squares of a list of numbers asynchronously. Args: nums (list of int): List of numbers to be squared and summed. Returns: int: The sum of the squares of the numbers. pass ``` **Constraints:** - Do not use any synchronous loops to aggregate results. - Handle errors gracefully in case any future fails. - Ensure that the solution is efficient in handling asynchronous executions. **Notes:** - The function definitions provided are meant to guide you; use appropriate methods from the `torch.futures` module. - Include enough documentation and comments in your functions to explain your logic and use of `torch.futures`. **Example Usage:** ```python # Example usage result = aggregate_futures([1, 2, 3, 4]) print(result) # Expected output: 30 (1^2 + 2^2 + 3^2 + 4^2) ``` Your implementation should adhere to these specifications to ensure it handles asynchronous operations effectively within the PyTorch framework.","solution":"import torch import torch.futures def compute_async(x: int) -> torch.futures.Future: Simulates an asynchronous computation that squares the input number. Args: x (int): The input number to be squared. Returns: torch.futures.Future: A future object that will hold the result of the computation. future = torch.futures.Future() # This simulates asynchronous computation def async_square(): result = x * x future.set_result(result) # In actual code, this might be some async I/O bound task. torch.jit.fork(async_square) return future def aggregate_futures(nums: [int]) -> int: Computes the sum of squares of a list of numbers asynchronously. Args: nums (list of int): List of numbers to be squared and summed. Returns: int: The sum of the squares of the numbers. futures = [compute_async(num) for num in nums] aggregate_future = torch.futures.collect_all(futures) def sum_squares(futures): results = [f.value() for f in futures] return sum(results) result_future = aggregate_future.then(lambda x: sum_squares(x.value())) return result_future.wait()"},{"question":"# Question: String and List Manipulation in Python **Objective:** Write a function `process_data` that processes a given list of strings according to the requirements outlined below. **Requirements:** 1. The function should accept a single argument: a list of strings `data`, where each string contains multiple words separated by spaces. 2. The function should perform the following operations: - For each string in the list, split the string into individual words. - Reverse the order of words in each string. - For each word, reverse the characters of that word. - Join the words back into a single string with spaces in between and store it in a new list. 3. The function should return the new list of processed strings. **Example:** ```python def process_data(data): # Your code here. # Example input_data = [\\"hello world\\", \\"python programming\\"] output_data = process_data(input_data) print(output_data) ``` **Expected Output:** ```python [\\"dlrow olleh\\", \\"gnimmargorp nohtyp\\"] ``` **Constraints:** - Each string in the input list will only contain alphabetical characters and spaces. - The list `data` will contain at least one string, and each string will contain at least one word. - You can assume that the strings do not have leading or trailing spaces, and words within the strings will be separated by exactly one space. **Performance Requirements:** - The solution should be efficient in terms of both time and space complexity, suitable for handling a list of up to 10,000 strings with each string containing up to 100 characters.","solution":"def process_data(data): Processes the given list of strings by reversing the words and the characters in each word. Args: data (list of str): A list of strings where each string contains multiple words. Returns: list of str: A list of processed strings. processed_data = [] for line in data: reversed_words = [word[::-1] for word in line.split()][::-1] processed_line = \' \'.join(reversed_words) processed_data.append(processed_line) return processed_data"},{"question":"# Decision Tree Classifier Implementation and Evaluation **Problem Statement:** You are provided with a dataset containing information about different species of flowers. The dataset includes the following features: - `sepal_length` - `sepal_width` - `petal_length` - `petal_width` The target variable is `species`, which has three possible classes: `setosa`, `versicolor`, and `virginica`. Your task is to implement a Decision Tree Classifier using scikit-learn, train it on the dataset, and evaluate its performance. You should write a function `decision_tree_classifier` that takes the following inputs: - `data`: A pandas DataFrame containing the features and the target variable. - `test_size`: A float representing the proportion of the dataset to include in the test split (e.g., 0.3 for 30%). The function should perform the following steps: 1. **Data Preparation**: - Split the data into features (X) and target (y). - Split the data into training and test sets using the specified `test_size`. 2. **Model Training**: - Train a Decision Tree Classifier on the training set. - Use the default hyperparameters for the Decision Tree Classifier. 3. **Model Evaluation**: - Predict the classes for the test set. - Calculate and return the accuracy of the model on the test set. The function should return a single float value representing the accuracy of the trained model on the test set. **Input Format:** The function takes the following inputs: ```python def decision_tree_classifier(data: pd.DataFrame, test_size: float) -> float: pass ``` - `data`: a pandas DataFrame with the following columns: `sepal_length`, `sepal_width`, `petal_length`, `petal_width`, and `species`. - `test_size`: a float number between 0 and 1. **Output Format:** - A single float value representing the accuracy of the Decision Tree Classifier on the test set. **Constraints:** - Use scikit-learn\'s `DecisionTreeClassifier` for the implementation. - The `data` DataFrame is guaranteed to be non-empty and correctly formatted. **Example:** ```python import pandas as pd # Sample data data = pd.DataFrame({ \'sepal_length\': [5.1, 4.9, 4.7, 4.6, 5.0], \'sepal_width\': [3.5, 3.0, 3.2, 3.1, 3.6], \'petal_length\': [1.4, 1.4, 1.3, 1.5, 1.4], \'petal_width\': [0.2, 0.2, 0.2, 0.2, 0.2], \'species\': [\'setosa\', \'setosa\', \'setosa\', \'setosa\', \'setosa\'] }) # Call the function accuracy = decision_tree_classifier(data, 0.2) print(accuracy) ``` **Note:** - Ensure that you import the necessary libraries (e.g., pandas, scikit-learn) in your implementation. - The dataset provided in the example is only to demonstrate the function call; the actual dataset used in the assessment will be more extensive.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score def decision_tree_classifier(data: pd.DataFrame, test_size: float) -> float: # Split the data into features (X) and target (y) X = data.drop(columns=\'species\') y = data[\'species\'] # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42) # Train a Decision Tree Classifier on the training set clf = DecisionTreeClassifier() clf.fit(X_train, y_train) # Predict the classes for the test set y_pred = clf.predict(X_test) # Calculate and return the accuracy of the model on the test set accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"**Coding Assessment Question: Implementing a Message Box Interaction in Tkinter** **Objective**: Assume you are developing a simple application with a graphical user interface (GUI) where you need to confirm user actions with various types of message boxes. Your task is to implement a function that creates different message boxes based on user interactions and handles their responses accordingly. **Question**: Using the `tkinter.messagebox` module, write a function called `interact_with_user` that performs the following: 1. Displays an initial information message box with the title \\"Welcome\\" and the message \\"Welcome to the Application\\". 2. Shows a warning message box with the title \\"Warning\\" and the message \\"Proceed with caution\\" after the user acknowledges the information message box. 3. Asks the user a Yes/No question with the title \\"Continue?\\" and the message \\"Do you want to continue?\\" after acknowledging the warning message box. 4. Based on the user\'s response to the Yes/No question: - If \\"Yes\\", display a final information message box with the title \\"Great!\\" and the message \\"You chose to continue.\\" - If \\"No\\", show an error message box with the title \\"Exit\\" and the message \\"You chose to exit. Goodbye!\\" **Requirements**: - You must use the appropriate functions from the `tkinter.messagebox` module for each step. - Properly handle the user responses (Yes/No) and display the corresponding message box. - Use descriptive titles and messages as specified. - Ensure to implement the GUI in a self-contained manner, meaning the application should run and display the message boxes when executed. **Sample Code Structure**: ```python import tkinter as tk from tkinter import messagebox def interact_with_user(): # Step 1: Display initial information message box messagebox.showinfo(title=\\"Welcome\\", message=\\"Welcome to the Application\\") # Step 2: Show warning message box messagebox.showwarning(title=\\"Warning\\", message=\\"Proceed with caution\\") # Step 3: Ask user a Yes/No question response = messagebox.askyesno(title=\\"Continue?\\", message=\\"Do you want to continue?\\") # Step 4: Handle user response if response: messagebox.showinfo(title=\\"Great!\\", message=\\"You chose to continue.\\") else: messagebox.showerror(title=\\"Exit\\", message=\\"You chose to exit. Goodbye!\\") # Code to trigger the function when run if __name__ == \\"__main__\\": root = tk.Tk() root.withdraw() # Hide the root window interact_with_user() root.mainloop() ``` **Constraints**: - You can assume that the `tkinter` library is already installed and available for import. - The function should be implemented such that it can be integrated into a larger Tkinter application if necessary. **Expectations**: - Demonstrated understanding of Tkinter message boxes. - Correct handling of user interactions and responses. - Clear and readable code with appropriate comments and indentation.","solution":"import tkinter as tk from tkinter import messagebox def interact_with_user(): Function to interact with the user using various Tkinter message boxes. # Step 1: Display initial information message box messagebox.showinfo(title=\\"Welcome\\", message=\\"Welcome to the Application\\") # Step 2: Show warning message box messagebox.showwarning(title=\\"Warning\\", message=\\"Proceed with caution\\") # Step 3: Ask user a Yes/No question response = messagebox.askyesno(title=\\"Continue?\\", message=\\"Do you want to continue?\\") # Step 4: Handle user response if response: messagebox.showinfo(title=\\"Great!\\", message=\\"You chose to continue.\\") else: messagebox.showerror(title=\\"Exit\\", message=\\"You chose to exit. Goodbye!\\") # Code to trigger the function when run if __name__ == \\"__main__\\": root = tk.Tk() root.withdraw() # Hide the root window interact_with_user() root.mainloop()"},{"question":"# HTML Entity Conversion Function **Objective:** Design a Python function that converts a given text, containing both named HTML character references and numeric character references, into its corresponding Unicode text representation. **Function Signature:** ```python def html_to_unicode(text: str) -> str: pass ``` **Input:** - `text`: A string containing HTML character references (both named and numeric), for example: `\\"Hello &gt; World &#x3E; !\\"` **Output:** - A string where all the HTML character references are replaced by their corresponding Unicode characters. **Requirements:** 1. Use the `html.entities.name2codepoint` dictionary to map named character references. 2. Handle numeric character references (e.g., `&#38;` for `&` or `&#x26;` for `&`) using appropriate conversion. 3. Ensure that both hexadecimal (`&#x...;`) and decimal (`&#...;`) numeric references are handled. 4. Your solution should efficiently handle large texts with multiple HTML character references. **Example:** ```python input_text = \\"Hello &gt; World &#x3E; &lt; &#60; !\\" output_text = html_to_unicode(input_text) print(output_text) # Output should be \\"Hello > World > < < !\\" ``` **Constraints:** - The input text will be a valid string. - You are allowed to use the `html.entities` module only for mapping named character references. Good luck!","solution":"import html.entities import re def html_to_unicode(text: str) -> str: Converts a given text with HTML character references to Unicode text. def replace_named_entity(match): entity = match.group(1) if entity in html.entities.name2codepoint: return chr(html.entities.name2codepoint[entity]) return match.group(0) # return as is if not found def replace_numeric_entity(match): if match.group(1).startswith(\'x\'): return chr(int(match.group(1)[1:], 16)) # hexadecimal entity else: return chr(int(match.group(1))) # decimal entity named_entity_pattern = re.compile(r\'&([a-zA-Z]+);\') numeric_entity_pattern = re.compile(r\'&#(x?[0-9a-fA-F]+);\') text = named_entity_pattern.sub(replace_named_entity, text) text = numeric_entity_pattern.sub(replace_numeric_entity, text) return text"},{"question":"# **Decision Tree Classifier Implementation and Tuning** Objective Your task is to implement, tune, and visualize a Decision Tree Classifier using the scikit-learn library. You will also handle missing values in the dataset and prevent overfitting by pruning the tree. Problem Statement Given a dataset, you are required to: 1. Import and preprocess the data, handling any missing values. 2. Implement a DecisionTreeClassifier. 3. Tune the hyperparameters to improve the performance of the classifier. 4. Visualize the decision tree. 5. Evaluate the classifier\'s performance. Dataset Use the `load_iris` dataset from `sklearn.datasets` for simplicity. However, you are expected to introduce some missing values in the dataset to demonstrate your ability to handle them. Steps 1. **Data Loading and Preprocessing**: - Load the `load_iris` dataset. - Introduce missing values randomly into the dataset. - Handle the missing values appropriately. 2. **Model Implementation**: - Initialize a `DecisionTreeClassifier`. - Split the data into training and testing sets. - Fit the model on the training data. 3. **Hyperparameter Tuning**: - Tune the hyperparameters such as `max_depth`, `min_samples_split`, and `min_samples_leaf` to avoid overfitting. - Use cross-validation to select the best hyperparameters. 4. **Model Visualization**: - Visualize the decision tree using the `plot_tree` function. - Export the decision tree into Graphviz format. 5. **Model Evaluation**: - Evaluate the performance of the classifier using metrics like accuracy, precision, recall, and F1-score on the test set. Constraints and Requirements - Implement your solution in a Jupyter notebook. - Add comments and explanations for each step of your solution. - Use appropriate visualizations to demonstrate the results. - Ensure the solution is efficient and follows best coding practices. Input and Output Formats - **Input**: - No direct input from the user. Use the `load_iris` dataset internally. - **Output**: - Visualizations and printed metrics for model evaluation. Performance Requirements - Ensure that the classifier does not overfit the data by tuning its hyperparameters. - Handle missing values effectively to maintain the performance of the classifier. Here is a template to get you started: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, cross_val_score from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score import numpy as np import matplotlib.pyplot as plt # Step 1: Load and preprocess the data iris = load_iris() X, y = iris.data, iris.target # Introducing missing values randomly np.random.seed(0) missing_mask = np.random.rand(*X.shape) < 0.1 X[missing_mask] = np.nan # Handling missing values from sklearn.impute import SimpleImputer imputer = SimpleImputer(strategy=\'mean\') X_imputed = imputer.fit_transform(X) # Step 2: Split the data X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=0) # Step 3: Initialize and train the DecisionTreeClassifier clf = DecisionTreeClassifier(random_state=0) clf.fit(X_train, y_train) # Step 4: Hyperparameter Tuning params = {\'max_depth\': [3, 5, 7], \'min_samples_split\': [2, 5, 10], \'min_samples_leaf\': [1, 2, 5]} from sklearn.model_selection import GridSearchCV grid_search = GridSearchCV(clf, param_grid=params, cv=5, scoring=\'accuracy\') grid_search.fit(X_train, y_train) best_clf = grid_search.best_estimator_ # Step 5: Model Visualization plt.figure(figsize=(20,10)) plot_tree(best_clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.show() # Exporting to Graphviz format export_graphviz(best_clf, out_file=\'tree.dot\', feature_names=iris.feature_names, class_names=iris.target_names, filled=True) # Step 6: Model Evaluation y_pred = best_clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'macro\') recall = recall_score(y_test, y_pred, average=\'macro\') f1 = f1_score(y_test, y_pred, average=\'macro\') print(\\"Accuracy:\\", accuracy) print(\\"Precision:\\", precision) print(\\"Recall:\\", recall) print(\\"F1 Score:\\", f1) ```","solution":"from sklearn.datasets import load_iris import numpy as np from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score import matplotlib.pyplot as plt def load_and_preprocess_data(): iris = load_iris() X, y = iris.data, iris.target # Introduce missing values randomly np.random.seed(0) missing_mask = np.random.rand(*X.shape) < 0.1 X[missing_mask] = np.nan # Handle missing values imputer = SimpleImputer(strategy=\'mean\') X_imputed = imputer.fit_transform(X) return X_imputed, y, iris def initialize_and_train_model(X_train, y_train): clf = DecisionTreeClassifier(random_state=0) clf.fit(X_train, y_train) return clf def hyperparameter_tuning(X_train, y_train): clf = DecisionTreeClassifier(random_state=0) params = { \'max_depth\': [3, 5, 7], \'min_samples_split\': [2, 5, 10], \'min_samples_leaf\': [1, 2, 5] } grid_search = GridSearchCV(clf, param_grid=params, cv=5, scoring=\'accuracy\') grid_search.fit(X_train, y_train) best_clf = grid_search.best_estimator_ return best_clf def visualize_decision_tree(clf, iris): plt.figure(figsize=(20, 10)) plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.show() export_graphviz(clf, out_file=\'tree.dot\', feature_names=iris.feature_names, class_names=iris.target_names, filled=True) def evaluate_model(clf, X_test, y_test): y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'macro\') recall = recall_score(y_test, y_pred, average=\'macro\') f1 = f1_score(y_test, y_pred, average=\'macro\') return { \\"accuracy\\": accuracy, \\"precision\\": precision, \\"recall\\": recall, \\"f1_score\\": f1 } # Main flow X, y, iris = load_and_preprocess_data() X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) initial_clf = initialize_and_train_model(X_train, y_train) best_clf = hyperparameter_tuning(X_train, y_train) visualize_decision_tree(best_clf, iris) metrics = evaluate_model(best_clf, X_test, y_test) print(\\"Accuracy:\\", metrics[\\"accuracy\\"]) print(\\"Precision:\\", metrics[\\"precision\\"]) print(\\"Recall:\\", metrics[\\"recall\\"]) print(\\"F1 Score:\\", metrics[\\"f1_score\\"])"},{"question":"**Coding Assessment Question: Kernel Density Estimation with scikit-learn** # Objective Implement and utilize Kernel Density Estimation (KDE) using scikit-learn to analyze a dataset. You will write code to perform KDE with different kernel functions and bandwidth parameters, visualize the results, and interpret the outcome. # Problem Statement You are provided with a 1D dataset representing the heights (in cm) of a group of individuals. Your task is to perform Kernel Density Estimation using scikit-learn\'s `KernelDensity` class and visualize the resulting density estimates with different kernels and bandwidths. # Instructions 1. **Load the Data:** Load the heights data from the provided list. ```python heights = [160, 165, 168, 170, 172, 175, 178, 180, 182, 185, 188, 190] ``` 2. **Implement KDE:** Write a function `perform_kde` that takes the following parameters: - `data`: List or numpy array of the dataset. - `kernel`: String specifying the kernel type (e.g., \'gaussian\', \'tophat\', etc.). - `bandwidth`: Float value specifying the bandwidth parameter. The function should return the fitted `KernelDensity` model and the density estimates for a range of values. 3. **Visualize the Results:** Write a function `plot_kde` that takes the following parameters: - `data`: List or numpy array of the dataset. - `kde_models`: Dictionary where keys are kernel names and values are tuples `(kde_model, density_estimates)`. The function should create a plot showing the original data points and the KDE plots for each kernel. 4. **Analyze Different Kernels:** Use the `perform_kde` function to compute KDE for the following kernels with a fixed bandwidth of 1.0: - \'gaussian\' - \'tophat\' - \'epanechnikov\' Store the results in a dictionary and use the `plot_kde` function to visualize them. 5. **Experiment with Bandwidth:** For the \'gaussian\' kernel, compute KDE with bandwidths of 0.5, 1.0, and 2.0. Visualize the results and discuss how changing the bandwidth affects the density estimate. # Function Signatures ```python def perform_kde(data: list, kernel: str, bandwidth: float): # Implement KDE and return the model and density estimates pass def plot_kde(data: list, kde_models: dict): # Visualize KDE for different kernels pass ``` # Expected Input and Output Input: - `data` (list): A list of numerical values representing the dataset. - `kernel` (str): A string specifying the kernel type. - `bandwidth` (float): A float value for the bandwidth parameter. - `kde_models` (dict): A dictionary with kernel names as keys and (model, estimates) tuples as values. Output: - `perform_kde`: Returns the fitted `KernelDensity` model and density estimates. - `plot_kde`: Displays a plot of the KDE estimates with the original data points. # Constraints - Use scikit-learn\'s `KernelDensity` for KDE computations. - For visualization, you may use matplotlib or any other suitable plotting library. - Ensure the functions handle edge cases, such as empty data. # Performance Requirements - The solution should efficiently handle the provided dataset. - Visualizations should be clear and distinguishable for different kernels and bandwidths. Use the example below as a guide to implement and test your functions. # Example ```python # Example usage of the functions # Load the data heights = [160, 165, 168, 170, 172, 175, 178, 180, 182, 185, 188, 190] # Perform KDE with different kernels kde_results = { \'gaussian\': perform_kde(heights, \'gaussian\', 1.0), \'tophat\': perform_kde(heights, \'tophat\', 1.0), \'epanechnikov\': perform_kde(heights, \'epanechnikov\', 1.0) } # Visualize the KDE results plot_kde(heights, kde_results) # Experiment with different bandwidths for gaussian kernel bandwidth_results = { \'gaussian_0.5\': perform_kde(heights, \'gaussian\', 0.5), \'gaussian_1.0\': perform_kde(heights, \'gaussian\', 1.0), \'gaussian_2.0\': perform_kde(heights, \'gaussian\', 2.0) } # Visualize the bandwidth experiments plot_kde(heights, bandwidth_results) ``` Analyze the output plots and discuss your observations.","solution":"import numpy as np from sklearn.neighbors import KernelDensity import matplotlib.pyplot as plt def perform_kde(data, kernel, bandwidth): Perform Kernel Density Estimation and return the fitted model and density estimates. Params: - data: List or numpy array of the dataset. - kernel: String specifying the kernel type. - bandwidth: Float value specifying the bandwidth parameter. Returns: - kde_model: Fitted KernelDensity model. - density_estimates: Array of density estimates for a range of values. data = np.array(data).reshape(-1, 1) kde_model = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(data) x_d = np.linspace(min(data) - 10, max(data) + 10, 1000) log_dens = kde_model.score_samples(x_d.reshape(-1, 1)) return kde_model, np.exp(log_dens), x_d def plot_kde(data, kde_models): Visualize the KDE plots for different kernels. Params: - data: List or numpy array of the dataset. - kde_models: Dictionary where keys are kernel names and values are tuples (kde_model, density_estimates, x_d). plt.figure(figsize=(12, 8)) plt.hist(data, bins=30, density=True, alpha=0.5, color=\'gray\') for kernel, (kde_model, density_estimates, x_d) in kde_models.items(): plt.plot(x_d, density_estimates, label=f\'Kernel: {kernel}\') plt.legend() plt.xlabel(\'Data\') plt.ylabel(\'Density\') plt.title(\'Kernel Density Estimation\') plt.show()"},{"question":"# Python Debugger (pdb) Coding Assessment Objective: Demonstrate your understanding and practical skills of using the Python Debugger (`pdb` module) by debugging a given piece of code to analyze its flow, detect, and resolve an issue. Problem Statement: You are provided with a faulty Python script. Your task is to use the `pdb` debugger to trace the execution, identify the bug, and provide corrections. Here is the provided script: ```python def calculate_sum(numbers): total = 0 for num in numbers: total += num return total/len(numbers) # Bug here def mean_of_list(number_list): if not number_list: raise ValueError(\\"The list is empty\\") return calculate_sum(number_list) def main(): numbers = [10, 20, 30, 40, 50, 5] print(\\"Mean of numbers:\\", mean_of_list(numbers)) if __name__ == \\"__main__\\": main() ``` There is an issue in the script causing incorrect results. 1. **Set up initial debugging**: Start debugging by initiating the pdb debugger on the script. 2. **Identify the bug**: Use pdb commands to step through the code and identify the bug in the `calculate_sum` function. 3. **Explain the issue**: Describe what is wrong with the code and why it causes incorrect results. 4. **Fix the script**: Provide the corrected version of the script that resolves the issue. 5. **Verification**: Run the corrected script to verify that the issue is fixed. Steps to Follow: 1. **Start debugging**: - Insert `import pdb; pdb.set_trace()` at an appropriate location in the script to break into the debugger. - Execute the script to begin the debugging session. 2. **Use pdb commands**: - Use commands such as `n(ext)`, `s(tep)`, `c(ont(inue)`, `p(expression)`, and `l(ist)` to navigate and inspect the code execution. 3. **Explanation**: - After identifying the issue, write a short explanation of the bug found and why it occurs. 4. **Correction**: - Modify the given script to fix the bug. 5. **Verify**: - Ensure that after fixing the error, the script produces the correct mean value of the numbers list. Expected Output: - **Explanation of the bug**: ```markdown The issue is in the `calculate_sum` function where the division to calculate the mean directly follows the summation. If the input `numbers` list is empty, this will raise a `ZeroDivisionError`. Additionally, always doing `return total/len(numbers)` assumes the list is never empty, potentially leading to incorrect results. ``` - **Corrected Script**: ```python def calculate_sum(numbers): total = 0 for num in numbers: total += num return total def mean_of_list(number_list): if not number_list: raise ValueError(\\"The list is empty\\") return calculate_sum(number_list) / len(number_list) def main(): numbers = [10, 20, 30, 40, 50, 5] print(\\"Mean of numbers:\\", mean_of_list(numbers)) if __name__ == \\"__main__\\": main() ``` Tasks like these assess comprehension of both debugging with `pdb` and basic Python error handling and function implementation.","solution":"def calculate_sum(numbers): Returns the sum of the numbers in the input list. total = 0 for num in numbers: total += num return total def mean_of_list(number_list): Returns the mean of the numbers in the input list. Raises ValueError if the list is empty. if not number_list: raise ValueError(\\"The list is empty\\") return calculate_sum(number_list) / len(number_list) def main(): numbers = [10, 20, 30, 40, 50, 5] print(\\"Mean of numbers:\\", mean_of_list(numbers)) if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Python Coding Assessment Objective: Assess students’ ability to work with Python\'s garbage collection module, focusing on enabling/disabling garbage collection, tracking objects, and debugging memory issues. Task: Implement a Python class `GarbageCollectorManager` that manages the garbage collection process, tracks created objects, and provides a summary of garbage collection statistics. The class should include the following methods: 1. **enable_gc(self)**: Enables automatic garbage collection using `gc.enable()`. 2. **disable_gc(self)**: Disables automatic garbage collection using `gc.disable()`. 3. **manual_collect(self, generation=2)**: Triggers garbage collection for the specified generation using `gc.collect(generation)`, and returns the number of unreachable objects found. 4. **is_gc_enabled(self)**: Returns `True` if automatic garbage collection is enabled, otherwise `False`, using `gc.isenabled()`. 5. **track_object(self, obj)**: Adds the provided object to a list of tracked objects and marks it to be monitored by the garbage collector. 6. **get_tracked_objects(self, generation=None)**: Returns a list of objects currently being tracked by the garbage collector, optionally filtered by the specified generation, using `gc.get_objects(generation)`. 7. **get_gc_stats(self)**: Returns a summary of garbage collection statistics including the number of collections, collected objects, and uncollectable objects for each generation using `gc.get_stats()`. 8. **reset_gc_settings(self, threshold0, threshold1, threshold2)**: Resets the garbage collection thresholds to new values using `gc.set_threshold(threshold0, threshold1, threshold2)`. 9. **debug_leaks(self)**: Enables debugging for memory leaks using `gc.set_debug(gc.DEBUG_LEAK)`. Input Output Specifications: - **Input**: Function parameters as described above. - **Output**: Return values and effects as specified in the method descriptions. Constraints: - Assume that the number of tracked objects and garbage collection operations are within a manageable range for typical Python applications. - Propsure handling of invalid generation numbers when calling `gc.collect(generation)`. Use `ValueError` for invalid generation numbers. Example Usage: ```python # Create an instance of the GarbageCollectorManager gcm = GarbageCollectorManager() # Enable garbage collection gcm.enable_gc() # Manually collect garbage for generation 0 unreachable_count = gcm.manual_collect(generation=0) print(f\\"Unreachable objects in generation 0: {unreachable_count}\\") # Track a new object obj = [1, 2, 3] gcm.track_object(obj) # Get a list of tracked objects tracked_objects = gcm.get_tracked_objects() print(f\\"Tracked objects: {tracked_objects}\\") # Get garbage collection stats gc_stats = gcm.get_gc_stats() print(f\\"GC Stats: {gc_stats}\\") # Reset garbage collection thresholds gcm.reset_gc_settings(700, 10, 5) # Enable leak debugging gcm.debug_leaks() ``` # Implementation Notes: - Use the `gc` module functions as described in the documentation provided. - Ensure proper error handling and validation of input parameters. - Clearly document each method within the `GarbageCollectorManager` class. Good luck and happy coding!","solution":"import gc class GarbageCollectorManager: def __init__(self): self.tracked_objects = [] def enable_gc(self): Enables automatic garbage collection. gc.enable() def disable_gc(self): Disables automatic garbage collection. gc.disable() def manual_collect(self, generation=2): Triggers garbage collection for the specified generation. :param generation: The generation to collect (0, 1, 2) :return: The number of unreachable objects found. if generation not in [0, 1, 2]: raise ValueError(\\"Invalid generation number. Must be 0, 1, or 2.\\") return gc.collect(generation) def is_gc_enabled(self): Checks if automatic garbage collection is enabled. :return: True if GC is enabled, False otherwise. return gc.isenabled() def track_object(self, obj): Adds an object to the list of tracked objects and marks it for monitoring by the GC. :param obj: The object to track. self.tracked_objects.append(obj) gc.get_referrers(obj) def get_tracked_objects(self, generation=None): Returns objects being tracked by the GC, optionally filtered by generation. :param generation: Specific generation to filter by (0, 1, 2), or None to return all. :return: List of tracked objects. if generation is None: return gc.get_objects() elif generation in [0, 1, 2]: return gc.get_objects(generation) else: raise ValueError(\\"Invalid generation number. Must be 0, 1, or 2.\\") def get_gc_stats(self): Returns a summary of garbage collection statistics. :return: A dictionary summarizing GC stats. return gc.get_stats() def reset_gc_settings(self, threshold0, threshold1, threshold2): Resets the garbage collection thresholds. :param threshold0: New threshold for generation 0. :param threshold1: New threshold for generation 1. :param threshold2: New threshold for generation 2. gc.set_threshold(threshold0, threshold1, threshold2) def debug_leaks(self): Enables debugging for memory leaks. gc.set_debug(gc.DEBUG_LEAK)"},{"question":"Objective: Implement and compare three different anomaly detection algorithms from scikit-learn: IsolationForest, LocalOutlierFactor, and OneClassSVM. Your goal is to demonstrate the use of these methods and analyze their performance on a provided dataset. Dataset: You are provided with a synthetic dataset `X_train.csv` for training and `X_test.csv` for testing. The data has no header row, and each row corresponds to an observation with numeric features. Instructions: 1. **Load the Data**: - Load the `X_train.csv` and `X_test.csv` datasets from the provided files. 2. **Implement Anomaly Detection Using the Following Methods**: - **IsolationForest** - Set `n_estimators=100` and `contamination` to 0.1. - **LocalOutlierFactor (LOF)** - Use it for **novelty detection** with `n_neighbors` set to 20. - **OneClassSVM** - Use the RBF kernel with default parameters and set `nu` to 0.1. 3. **Fit the Models on the Training Data**: - Use the appropriate methods to fit the models on `X_train`. 4. **Predict Anomalies on the Test Data**: - Predict anomalies for the `X_test` dataset using each model. - Obtain the prediction scores for `X_test` using `decision_function` or equivalent methods. 5. **Output the Predictions and Scores**: - For each method, print the predicted labels (1 for inliers, -1 for outliers) and the prediction scores for `X_test`. Expected Output: - **IsolationForest**: Predicted labels, scores. - **LocalOutlierFactor (LOF)**: Predicted labels, scores. - **OneClassSVM**: Predicted labels, scores. Example Code Structure: ```python import numpy as np import pandas as pd from sklearn.ensemble import IsolationForest from sklearn.neighbors import LocalOutlierFactor from sklearn.svm import OneClassSVM # Load the data X_train = pd.read_csv(\'X_train.csv\', header=None) X_test = pd.read_csv(\'X_test.csv\', header=None) # Isolation Forest iso_forest = IsolationForest(n_estimators=100, contamination=0.1) iso_forest.fit(X_train) iso_pred = iso_forest.predict(X_test) iso_score = iso_forest.decision_function(X_test) print(\\"IsolationForest Predictions:\\", iso_pred) print(\\"IsolationForest Scores:\\", iso_score) # Local Outlier Factor (LOF) for novelty detection lof = LocalOutlierFactor(n_neighbors=20, novelty=True) lof.fit(X_train) lof_pred = lof.predict(X_test) lof_score = lof.decision_function(X_test) print(\\"LOF Predictions:\\", lof_pred) print(\\"LOF Scores:\\", lof_score) # OneClassSVM ocsvm = OneClassSVM(kernel=\'rbf\', nu=0.1) ocsvm.fit(X_train) ocsvm_pred = ocsvm.predict(X_test) ocsvm_score = ocsvm.decision_function(X_test) print(\\"OneClassSVM Predictions:\\", ocsvm_pred) print(\\"OneClassSVM Scores:\\", ocsvm_score) ``` Note: Ensure you have the necessary dependencies installed (e.g., `scikit-learn`, `pandas`). Constraints: - `X_train.csv` and `X_test.csv` must be available in the current directory. - Ensure reproducibility by setting a random seed where applicable. Evaluate the performance and suitability of each method based on the predictions and scores obtained. Performance Requirements: - Implementations should efficiently handle datasets with up to 1000 samples and 20 features. - Analyze the outcomes and discuss any significant differences in the behavior or performance of the models. Bonus (Optional): - Visualize the decision boundaries of each method if the dataset has only two features. Use scatter plots to illustrate the inliers and outliers detected by each method.","solution":"import numpy as np import pandas as pd from sklearn.ensemble import IsolationForest from sklearn.neighbors import LocalOutlierFactor from sklearn.svm import OneClassSVM def load_data(): Load the training and testing datasets. X_train = pd.read_csv(\'X_train.csv\', header=None).values X_test = pd.read_csv(\'X_test.csv\', header=None).values return X_train, X_test def run_isolation_forest(X_train, X_test): Run Isolation Forest on X_train and predict anomalies on X_test. iso_forest = IsolationForest(n_estimators=100, contamination=0.1, random_state=42) iso_forest.fit(X_train) iso_pred = iso_forest.predict(X_test) iso_score = iso_forest.decision_function(X_test) return iso_pred, iso_score def run_lof(X_train, X_test): Run Local Outlier Factor (LOF) for novelty detection on X_train and predict anomalies on X_test. lof = LocalOutlierFactor(n_neighbors=20, novelty=True) lof.fit(X_train) lof_pred = lof.predict(X_test) lof_score = lof.decision_function(X_test) return lof_pred, lof_score def run_one_class_svm(X_train, X_test): Run One-Class SVM on X_train and predict anomalies on X_test. ocsvm = OneClassSVM(kernel=\'rbf\', nu=0.1) ocsvm.fit(X_train) ocsvm_pred = ocsvm.predict(X_test) ocsvm_score = ocsvm.decision_function(X_test) return ocsvm_pred, ocsvm_score if __name__ == \\"__main__\\": X_train, X_test = load_data() # Isolation Forest iso_pred, iso_score = run_isolation_forest(X_train, X_test) print(\\"IsolationForest Predictions:\\", iso_pred) print(\\"IsolationForest Scores:\\", iso_score) # Local Outlier Factor (LOF) for novelty detection lof_pred, lof_score = run_lof(X_train, X_test) print(\\"LOF Predictions:\\", lof_pred) print(\\"LOF Scores:\\", lof_score) # OneClassSVM ocsvm_pred, ocsvm_score = run_one_class_svm(X_train, X_test) print(\\"OneClassSVM Predictions:\\", ocsvm_pred) print(\\"OneClassSVM Scores:\\", ocsvm_score)"},{"question":"Problem Statement: Efficient Prime Factorization Sum # Objective Write a Python function `prime_factorization_sum(n: int) -> int` that calculates the sum of the greatest common divisors (GCDs) of all unique pairs of the prime factors of a given integer `n`. The function should utilize relevant mathematical functions from the `math` module. # Input - An integer `n` where `n >= 2`. # Output - Return an integer which is the sum of the GCDs of all unique pairs of prime factors of `n`. # Constraints - The function should handle large integers efficiently (e.g., up to 10^12). - Make sure to handle edge cases where `n` may be a prime number or a number with multiple prime factors. # Example ```python from math import gcd def prime_factorization_sum(n: int) -> int: # Your code here # Example usages print(prime_factorization_sum(18)) # Output: 3 (18 = 2 * 3^2, so prime factors are {2, 3}, GCD(2, 3) = 1) print(prime_factorization_sum(28)) # Output: 7 (28 = 2^2 * 7, so prime factors are {2, 7}, GCD(2, 7) = 1) print(prime_factorization_sum(100)) # Output: 10 (100 = 2^2 * 5^2, so prime factors are {2, 5}, GCD(2, 5) = 1) ``` # Notes 1. Use `math.gcd` to calculate the GCD of two integers. 2. You may use any method to obtain the prime factors of `n`, but ensure it is efficient for large values of `n`. 3. Make sure to provide a clear and optimized solution to handle the constraints effectively.","solution":"from math import gcd def prime_factors(n): Return the prime factors of the given integer n. i = 2 factors = set() while i * i <= n: while (n % i) == 0: factors.add(i) n //= i i += 1 if n > 1: factors.add(n) return factors def prime_factorization_sum(n: int) -> int: Calculate the sum of the greatest common divisors (GCDs) of all unique pairs of the prime factors of n. factors = prime_factors(n) factors_list = list(factors) gcd_sum = 0 for i in range(len(factors_list)): for j in range(i + 1, len(factors_list)): gcd_sum += gcd(factors_list[i], factors_list[j]) return gcd_sum"},{"question":"Parallel Web Scraping Context: You are tasked with extracting specific data from multiple web pages concurrently. Each page contains items with structured data including `title`, `price`, and `availability`. You will use the `concurrent.futures` module to fetch and parse this data asynchronously to improve performance. Problem: Implement a function `parallel_web_scraping(urls: List[str], timeout: int) -> List[Dict[str, Any]]` that retrieves data from given URLs concurrently using `ThreadPoolExecutor`. Each URL returns a structured JSON response. Your function should return a list of dictionaries containing the extracted data. Requirements: 1. **Input**: - `urls` (List[str]): A list of URLs to be scraped for data. - `timeout` (int): Timeout in seconds for each request before raising an exception. 2. **Output**: - Return a list of dictionaries where each dictionary corresponds to the data from a URL. Each dictionary should contain: - `url`: The URL of the page. - `title`: Title of the item. - `price`: Price of the item. - `availability`: Availability status of the item. 3. **Constraints and Notes**: - Implement the function using `ThreadPoolExecutor` for concurrent execution. - Handle exceptions where the URL might not be accessible within the provided timeout. - Your function should gracefully handle and report failed attempts by including an appropriate message in the respective dictionary entry. 4. **Performance Requirements**: - Efficiently manage concurrent requests to utilize the power of multi-threading. - Ensure all threads complete execution or are properly shut down in case of errors. Example: Here is an example JSON response from a URL: ```json { \\"title\\": \\"Sample Item\\", \\"price\\": \\"20.00\\", \\"availability\\": \\"In Stock\\" } ``` If you are given the following URLs list: ```python urls = [\\"http://example.com/item1\\", \\"http://example.com/item2\\"] ``` Your output should be similar to: ```python [ { \\"url\\": \\"http://example.com/item1\\", \\"title\\": \\"Sample Item 1\\", \\"price\\": \\"20.00\\", \\"availability\\": \\"In Stock\\" }, { \\"url\\": \\"http://example.com/item2\\", \\"title\\": \\"Sample Item 2\\", \\"price\\": \\"25.00\\", \\"availability\\": \\"Out of Stock\\" } ] ``` Implementation: ```python from typing import List, Dict, Any import concurrent.futures import requests def fetch_data(url: str, timeout: int) -> Dict[str, Any]: try: response = requests.get(url, timeout=timeout) response.raise_for_status() data = response.json() return { \\"url\\": url, \\"title\\": data[\\"title\\"], \\"price\\": data[\\"price\\"], \\"availability\\": data[\\"availability\\"] } except Exception as e: return { \\"url\\": url, \\"error\\": str(e) } def parallel_web_scraping(urls: List[str], timeout: int) -> List[Dict[str, Any]]: results = [] with concurrent.futures.ThreadPoolExecutor() as executor: future_to_url = {executor.submit(fetch_data, url, timeout): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): result = future.result() results.append(result) return results ```","solution":"from typing import List, Dict, Any import concurrent.futures import requests def fetch_data(url: str, timeout: int) -> Dict[str, Any]: try: response = requests.get(url, timeout=timeout) response.raise_for_status() data = response.json() return { \\"url\\": url, \\"title\\": data[\\"title\\"], \\"price\\": data[\\"price\\"], \\"availability\\": data[\\"availability\\"] } except Exception as e: return { \\"url\\": url, \\"error\\": str(e) } def parallel_web_scraping(urls: List[str], timeout: int) -> List[Dict[str, Any]]: results = [] with concurrent.futures.ThreadPoolExecutor() as executor: future_to_url = {executor.submit(fetch_data, url, timeout): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): result = future.result() results.append(result) return results"},{"question":"Objective: Demonstrate your comprehension of the `scikit-learn` Naive Bayes classifiers by implementing and evaluating a Gaussian Naive Bayes classifier on a provided dataset. You are required to preprocess the data, fit the model, predict outcomes, and evaluate model performance. Instructions: 1. Download the \'Wine\' dataset from sklearn\'s datasets module. 2. Split the dataset into training and testing sets. 3. Implement a Gaussian Naive Bayes classifier using scikit-learn. 4. Evaluate the classifier\'s performance using metrics such as accuracy, precision, recall, and F1 score. 5. Plot the confusion matrix. Expected Input and Output: - **Input**: - Dataset: The \'Wine\' dataset from sklearn\'s datasets. - Training/Test Split: Configured with 70% training and 30% testing data. - **Output**: - Accuracy, Precision, Recall, and F1 Score of the classifier. - Confusion matrix plot visualizing the results. Constraints and Limitations: - Use the Gaussian Naive Bayes classifier. - Ensure reproducibility by setting a random seed for train-test split. Performance Requirements: - Efficient execution with clear and informative output. Example Code Snippet: You may find the following example code snippet helpful to understand the format of the solution. However, you need to modify it to fit the requirements. ```python from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay import matplotlib.pyplot as plt # Load Wine dataset X, y = load_wine(return_X_y=True) # Split the dataset into training and testing sets (70% train, 30% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize the Gaussian Naive Bayes classifier gnb = GaussianNB() # Fit the classifier to the training data gnb.fit(X_train, y_train) # Predict the labels of the test set y_pred = gnb.predict(X_test) # Calculate evaluation metrics accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'weighted\') recall = recall_score(y_test, y_pred, average=\'weighted\') f1 = f1_score(y_test, y_pred, average=\'weighted\') print(f\'Accuracy: {accuracy}\') print(f\'Precision: {precision}\') print(f\'Recall: {recall}\') print(f\'F1 Score: {f1}\') # Plot confusion matrix cm = confusion_matrix(y_test, y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=gnb.classes_) disp.plot() plt.show() ``` Good luck!","solution":"from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay import matplotlib.pyplot as plt def wine_naive_bayes_classifier(): # Load Wine dataset X, y = load_wine(return_X_y=True) # Split the dataset into training and testing sets (70% train, 30% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize the Gaussian Naive Bayes classifier gnb = GaussianNB() # Fit the classifier to the training data gnb.fit(X_train, y_train) # Predict the labels of the test set y_pred = gnb.predict(X_test) # Calculate evaluation metrics accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'weighted\') recall = recall_score(y_test, y_pred, average=\'weighted\') f1 = f1_score(y_test, y_pred, average=\'weighted\') # Return the metrics and the predicted values return accuracy, precision, recall, f1, y_test, y_pred def plot_confusion_matrix(y_test, y_pred): # Plot confusion matrix cm = confusion_matrix(y_test, y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=load_wine().target_names) disp.plot() plt.show()"},{"question":"Objective: Implement a function in PyTorch that leverages CPU streams to perform matrix multiplication in parallel and then synchronizes the results. Problem Statement: You are required to design a function `matrix_multiplication_parallel` that performs matrix multiplication using multiple CPU streams to execute the operations in parallel. The function should handle the creation of streams, assigning operations to different streams, and synchronizing the streams before returning the result. Function Signature: ```python def matrix_multiplication_parallel(a: torch.Tensor, b: torch.Tensor, n_streams: int) -> torch.Tensor: Perform matrix multiplication of two tensors using multiple CPU streams. Parameters: a (torch.Tensor): The first input matrix. b (torch.Tensor): The second input matrix. n_streams (int): The number of streams to utilize for parallel computation. Returns: torch.Tensor: The resulting matrix after multiplication. pass ``` Input: - `a`: A 2D tensor of shape `(m, k)`. - `b`: A 2D tensor of shape `(k, n)`. - `n_streams`: An integer representing the number of streams to use for parallel computation. Output: - A 2D tensor of shape `(m, n)` representing the result of `a @ b`. Constraints: - The temperature can change the values inside the tensor, but the tensor must be returned in the correct format. - Assume `m`, `k`, and `n` are all moderate-sized integers (`1 <= m, n, k <= 500`). - `n_streams` will be a positive integer (1 ≤ `n_streams` ≤ 10). Example: ```python import torch a = torch.randn(300, 200) b = torch.randn(200, 400) n_streams = 5 result = matrix_multiplication_parallel(a, b, n_streams) print(result.shape) # Should print: torch.Size([300, 400]) ``` Performance Requirements: - The matrix multiplication should take advantage of the available CPU resources in a parallel manner using the specified number of streams. - Proper synchronization must be ensured so that the result is accurate. Additional Notes: - Utilize `torch.cpu.stream` to create and manage streams. - Use `torch.cpu.synchronize()` to ensure that all streams are synchronized before returning the result. - Make sure to handle the division of tasks among the streams properly to fully utilize the parallelization capabilities.","solution":"import torch def matrix_multiplication_parallel(a: torch.Tensor, b: torch.Tensor, n_streams: int) -> torch.Tensor: Perform matrix multiplication of two tensors using multiple CPU streams. Parameters: a (torch.Tensor): The first input matrix. b (torch.Tensor): The second input matrix. n_streams (int): The number of streams to utilize for parallel computation. Returns: torch.Tensor: The resulting matrix after multiplication. if n_streams < 1: raise ValueError(\\"n_streams must be at least 1\\") # Get the shapes of matrices m, k_a = a.shape k_b, n = b.shape if k_a != k_b: raise ValueError(\\"The inner dimensions of a and b must match for matrix multiplication\\") # Create the result tensor result = torch.zeros(m, n, dtype=a.dtype) # Create CPU streams streams = [torch.cpu.Stream() for _ in range(n_streams)] # Function to perform matrix multiplication on a partial result in a given stream def matmul_partial(stream, i_start, i_end): with torch.cpu.stream(stream): result[i_start:i_end] = torch.mm(a[i_start:i_end], b) # Divide tasks among streams chunk_size = (m + n_streams - 1) // n_streams tasks = [(i * chunk_size, min((i + 1) * chunk_size, m)) for i in range(n_streams)] # Launch tasks in parallel streams for i, (i_start, i_end) in enumerate(tasks): matmul_partial(streams[i], i_start, i_end) # Synchronize streams torch.cpu.synchronize() return result"},{"question":"# Understanding Module Imports: Transitioning from `imp` to `importlib` In this exercise, your task is to re-implement functionality using the `importlib` module that was previously done using the deprecated `imp` module. **Problem Statement:** Write a function `legacy_import_emulator` that mimics the old import functionality using `imp`, but by using `importlib` instead. The function should perform the following tasks: 1. Check if a module identified by `name` is already imported in `sys.modules`. 2. If not, find the module, load it, and return it. 3. Ensure to handle the file operations properly and close any open files. Here is the prototype of the function you need to implement: ```python def legacy_import_emulator(name: str): Mimics the old import functionality using importlib instead of imp. Parameters: name (str): The name of the module to import. Returns: module: The imported module. Raises: ImportError: If the module cannot be imported. pass ``` **Input:** - A string `name` representing the name of the module to import. **Output:** - The imported module object. **Constraints:** - Assume the module name provided will be valid and importable in a standard Python environment. - You should not use the deprecated `imp` module in your implementation. **Example Usage:** ```python # Assuming `math` module is not yet imported. math_module = legacy_import_emulator(\\"math\\") print(math_module.sqrt(16)) # Should print: 4.0 ``` **Note:** - You can use the `importlib.util.find_spec()`, `importlib.util.module_from_spec()`, and `importlib.import_module()` functions for this task. - The function should mimic the behavior described, ensuring any file handles are closed correctly, even in cases where exceptions might occur.","solution":"import sys import importlib.util def legacy_import_emulator(name: str): Mimics the old import functionality using importlib instead of imp. Parameters: name (str): The name of the module to import. Returns: module: The imported module. Raises: ImportError: If the module cannot be imported. if name in sys.modules: return sys.modules[name] spec = importlib.util.find_spec(name) if spec is None: raise ImportError(f\\"Cannot find module named {name}\\") mod = importlib.util.module_from_spec(spec) spec.loader.exec_module(mod) sys.modules[name] = mod return mod"},{"question":"# Configuration File Parser Task You are provided with a configuration file in the INI format used to store application settings. Your task is to write a Python function that reads this configuration file and updates specific settings based on certain conditions. Configuration File (config.ini) ```ini [General] appname = MyApp appversion = 1.0 [Settings] debug = false max_users = 100 ``` Task Description 1. Write a function called `update_config(file_path, updates)` that: - Reads the configuration file from the given `file_path`. - Applies updates provided in the `updates` dictionary to the configuration. The `updates` dictionary will have keys corresponding to section names and values as dictionaries of settings to be updated in those sections. - Writes the updated configuration back to the same file. Input - `file_path` (str): The path to the INI configuration file. - `updates` (dict): A dictionary with the updates to be made. Format: ```python { \\"SectionName1\\": {\\"setting1\\": \\"value1\\", \\"setting2\\": \\"value2\\"}, \\"SectionName2\\": {\\"setting1\\": \\"value1\\"} } ``` Output - The function should update the configuration file in place and return `True` if successful or `False` in case of any error. Constraints - If a section or setting does not exist, the function should create it. - You\'re not allowed to use external libraries except the Python standard library. - Ensure proper handling of any potential errors (e.g., file not found, invalid data). Example Given the `config.ini` file content as above: ```ini [General] appname = MyApp appversion = 1.0 [Settings] debug = false max_users = 100 ``` And an `updates` dictionary: ```python updates = { \\"General\\": {\\"appversion\\": \\"2.0\\"}, \\"Settings\\": {\\"debug\\": \\"true\\", \\"max_users\\": \\"200\\"} } ``` The updated `config.ini` file should become: ```ini [General] appname = MyApp appversion = 2.0 [Settings] debug = true max_users = 200 ``` Starter Code ```python import configparser def update_config(file_path, updates): config = configparser.ConfigParser() try: config.read(file_path) for section, settings in updates.items(): if not config.has_section(section): config.add_section(section) for key, value in settings.items(): config.set(section, key, value) with open(file_path, \'w\') as configfile: config.write(configfile) return True except Exception as e: print(f\\"An error occurred: {e}\\") return False ``` Testing the Function You can test your function by creating a `config.ini` file with the provided content, running your function with the example updates, and verifying if the content of the file matches the expected output.","solution":"import configparser def update_config(file_path, updates): config = configparser.ConfigParser() try: config.read(file_path) for section, settings in updates.items(): if not config.has_section(section): config.add_section(section) for key, value in settings.items(): config.set(section, key, value) with open(file_path, \'w\') as configfile: config.write(configfile) return True except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"**Objective:** The goal of this exercise is to assess your ability to work with network protocols and handle HTTP requests and errors using Python\'s `urllib.request` module. **Problem Statement:** You are required to implement a function that makes an HTTP request to a given URL and returns the contents of the web page. Your function should handle different types of HTTP errors gracefully and return appropriate messages based on the error status. **Function Signature:** ```python def fetch_url_content(url: str) -> str: pass ``` **Input:** - `url` (str): A string representing the URL of the web page to fetch. **Output:** - A string containing the contents of the web page if the request is successful. - A specific error message string if the request fails due to HTTP errors. **Error Handling:** You should handle the following HTTP errors with appropriate messages: - 404: \\"Error 404: Not Found\\" - 403: \\"Error 403: Forbidden\\" - 500: \\"Error 500: Internal Server Error\\" - Any other HTTP error: \\"Error {status_code}: Something went wrong\\" **Constraints:** - Use `urllib.request` module to handle HTTP requests. - The URL provided will always start with `http://` or `https://`. - Assume the content of the web page will not exceed 1MB in size. - You are not required to handle URL redirections for this problem. **Example:** ```python # Example 1: Successful request url = \\"http://example.com\\" print(fetch_url_content(url)) # Output: \'<!doctype html> ... </html>\' (content of example.com) # Example 2: HTTP 404 Error url = \\"http://example.com/nonexistentpage\\" print(fetch_url_content(url)) # Output: \'Error 404: Not Found\' # Example 3: HTTP 403 Error url = \\"http://example.com/forbidden\\" print(fetch_url_content(url)) # Output: \'Error 403: Forbidden\' # Example 4: HTTP 500 Error url = \\"http://example.com/servererror\\" print(fetch_url_content(url)) # Output: \'Error 500: Internal Server Error\' # Example 5: Other HTTP Error url = \\"http://example.com/unknownerror\\" print(fetch_url_content(url)) # Output: \'Error 502: Something went wrong\' (assuming the error status code is 502) ``` **Note:** Make sure your function handles invalid URLs and other exceptions gracefully. You can assume the URL format provided will always be valid.","solution":"import urllib.request import urllib.error def fetch_url_content(url: str) -> str: try: with urllib.request.urlopen(url) as response: return response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: if e.code == 404: return \\"Error 404: Not Found\\" elif e.code == 403: return \\"Error 403: Forbidden\\" elif e.code == 500: return \\"Error 500: Internal Server Error\\" else: return f\\"Error {e.code}: Something went wrong\\" except urllib.error.URLError as e: return f\\"Error: {e.reason}\\""},{"question":"# Secure Zip File Extraction You are tasked with creating a secure function for decompressing zip files. The function needs to prevent common vulnerabilities such as zip bombs which can cause disk volume exhaustion. Your function should do the following: - Extract files from a given zip file. - Ensure that the extracted files don’t exceed a specified size limit. - Safely handle paths to avoid unsafe paths like absolute file paths or paths with relative components (e.g., `../`). Function Signature: ```python def safe_extract_zip(zip_file_path: str, extract_path: str, max_size: int): Extracts files from a zip archive while ensuring security constraints. Parameters: zip_file_path (str): The path to the zip file to be extracted. extract_path (str): The directory where files should be extracted to. max_size (int): The maximum allowable size of extracted files in bytes. Raises: ValueError: If any extracted file exceeds the maximum size limit. SecurityError: If a file path is unsafe. Returns: List of extracted file paths. ``` Expected Input and Output: - **Input:** - `zip_file_path`: Path to a zip file present in the filesystem. - `extract_path`: Path where you want to extract the files. - `max_size`: Maximum allowable byte size for extracted files. - **Output:** - A list containing the paths to the extracted files. Constraints: - The function should raise a `ValueError` if any extracted file exceeds the `max_size`. - The function should raise a `SecurityError` (you will need to define this exception) if the zip file contains unsafe file paths. - Only import necessary external modules. Avoid using deprecated or insecure methods or functions unless they are securely handled. Requirements: - Ensure the solutions adheres to the security considerations mentioned in the provided documentation. - Provide thorough error handling to deal with potential issues during extraction. - The function should be robust and handle corrupted zip files gracefully. Use the following custom exception for path validation: ```python class SecurityError(Exception): pass ``` Example Usage: ```python try: extracted_files = safe_extract_zip(\'example.zip\', \'extract_dir\', 100000) print(\\"Extracted files:\\", extracted_files) except ValueError as ve: print(\\"Error:\\", str(ve)) except SecurityError as se: print(\\"Security issue:\\", str(se)) except Exception as e: print(\\"Unexpected error:\\", str(e)) ```","solution":"import zipfile import os class SecurityError(Exception): pass def safe_extract_zip(zip_file_path: str, extract_path: str, max_size: int): Extracts files from a zip archive while ensuring security constraints. Parameters: zip_file_path (str): The path to the zip file to be extracted. extract_path (str): The directory where files should be extracted to. max_size (int): The maximum allowable size of extracted files in bytes. Raises: ValueError: If any extracted file exceeds the maximum size limit. SecurityError: If a file path is unsafe. Returns: List of extracted file paths. def is_within_directory(directory, target): abs_directory = os.path.abspath(directory) abs_target = os.path.abspath(target) return os.path.commonpath([abs_directory]) == os.path.commonpath([abs_directory, abs_target]) extracted_files = [] with zipfile.ZipFile(zip_file_path, \'r\') as zip_ref: for file_info in zip_ref.infolist(): if file_info.file_size > max_size: raise ValueError(f\\"File \'{file_info.filename}\' exceeds maximum size limit.\\") extracted_file_path = os.path.join(extract_path, file_info.filename) if not is_within_directory(extract_path, extracted_file_path): raise SecurityError(f\\"Unsafe file path detected: \'{file_info.filename}\'\\") zip_ref.extract(file_info, extract_path) extracted_files.append(extracted_file_path) return extracted_files"},{"question":"**Question** Using the deprecated `optparse` module, implement a command-line tool that meets the following requirements: 1. Accepts the following options: - `-i` or `--input`: Specifies an input file (mandatory). - `-o` or `--output`: Specifies an output file (mandatory). - `-v` or `--verbose`: A flag that enables verbose output. - `-c` or `--compress`: A flag that enables compressing the output file. 2. Generates a help message that provides detailed information for each option. 3. Prints the version number of the tool when `--version` is passed. 4. Outputs an error message if mandatory options (`--input` and `--output`) are not provided. In addition, implement equivalent functionality using the `argparse` module for comparison. **Input Format** Command line arguments passed to the tool. **Output Format** Based on the given command-line arguments, the tool should: 1. Print an appropriate message specifying the input and output file names. 2. Print \\"Verbose mode enabled\\" message if `-v` or `--verbose` flag is passed. 3. Print \\"Compressing output file\\" message if `-c` or `--compress` flag is passed. 4. Print a version message if `--version` is provided. 5. Print an error message and the help message if either `--input` or `--output` is missing. **Constraints** - Use Python 3.10 or later. - Ensure your implementation handles errors gracefully. **Example** Suppose the script is named `tool.py`. ```bash python tool.py --input=input.txt --output=output.txt -v -c Input file: input.txt Output file: output.txt Verbose mode enabled Compressing output file python tool.py --version tool.py version 1.0 python tool.py --help Usage: tool.py [options] Options: -h, --help show this help message and exit -i INPUT, --input=INPUT Specifies an input file (mandatory) -o OUTPUT, --output=OUTPUT Specifies an output file (mandatory) -v, --verbose Enables verbose output -c, --compress Enables compressing the output file --version show program\'s version number and exit python tool.py -i input.txt Usage: tool.py [options] tool.py: error: both --input and --output options are required. ``` **Solution** Using `optparse`: ```python from optparse import OptionParser def main(): usage = \\"usage: %prog [options]\\" parser = OptionParser(usage=usage, version=\\"%prog 1.0\\") parser.add_option(\\"-i\\", \\"--input\\", dest=\\"input\\", help=\\"Specifies an input file (mandatory)\\", metavar=\\"INPUT\\") parser.add_option(\\"-o\\", \\"--output\\", dest=\\"output\\", help=\\"Specifies an output file (mandatory)\\", metavar=\\"OUTPUT\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", help=\\"Enables verbose output\\") parser.add_option(\\"-c\\", \\"--compress\\", action=\\"store_true\\", dest=\\"compress\\", help=\\"Enables compressing the output file\\") (options, args) = parser.parse_args() if not options.input or not options.output: parser.error(\\"both --input and --output options are required.\\") print(f\\"Input file: {options.input}\\") print(f\\"Output file: {options.output}\\") if options.verbose: print(\\"Verbose mode enabled\\") if options.compress: print(\\"Compressing output file\\") if __name__ == \\"__main__\\": main() ``` Using `argparse`: ```python import argparse def main(): parser = argparse.ArgumentParser(description=\'Process some files.\') parser.add_argument(\'-i\', \'--input\', required=True, help=\'Specifies an input file (mandatory)\') parser.add_argument(\'-o\', \'--output\', required=True, help=\'Specifies an output file (mandatory)\') parser.add_argument(\'-v\', \'--verbose\', action=\'store_true\', help=\'Enables verbose output\') parser.add_argument(\'-c\', \'--compress\', action=\'store_true\', help=\'Enables compressing the output file\') parser.add_argument(\'--version\', action=\'version\', version=\'%(prog)s 1.0\') args = parser.parse_args() print(f\\"Input file: {args.input}\\") print(f\\"Output file: {args.output}\\") if args.verbose: print(\\"Verbose mode enabled\\") if args.compress: print(\\"Compressing output file\\") if __name__ == \\"__main__\\": main() ```","solution":"from optparse import OptionParser def main(): usage = \\"usage: %prog [options]\\" parser = OptionParser(usage=usage, version=\\"%prog 1.0\\") parser.add_option(\\"-i\\", \\"--input\\", dest=\\"input\\", help=\\"Specifies an input file (mandatory)\\", metavar=\\"INPUT\\") parser.add_option(\\"-o\\", \\"--output\\", dest=\\"output\\", help=\\"Specifies an output file (mandatory)\\", metavar=\\"OUTPUT\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", help=\\"Enables verbose output\\") parser.add_option(\\"-c\\", \\"--compress\\", action=\\"store_true\\", dest=\\"compress\\", help=\\"Enables compressing the output file\\") (options, args) = parser.parse_args() if not options.input or not options.output: parser.error(\\"both --input and --output options are required.\\") print(f\\"Input file: {options.input}\\") print(f\\"Output file: {options.output}\\") if options.verbose: print(\\"Verbose mode enabled\\") if options.compress: print(\\"Compressing output file\\") if __name__ == \\"__main__\\": main()"},{"question":"Objective: Demonstrate your understanding of seaborn by creating and customizing visualizations that include rug plots. Task: Write a function `visualize_with_rugplots` that takes a seaborn dataset name as input, generates two types of visualizations, and saves the plots as .png files. Specifically: 1. **Visualization 1**: A scatter plot of two specified numeric variables from the dataset with an additional rug plot along both axes. 2. **Visualization 2**: A scatter plot of two specified numeric variables with a rug plot that uses hue mapping to represent a specified categorical variable. Customize the rug plot appearance by adjusting its height and placing it outside the axes. Specifications: - **Function Name**: `visualize_with_rugplots` - **Inputs**: - `dataset_name` (str): The name of a seaborn dataset (e.g., \\"tips\\" or \\"diamonds\\"). - `x_var` (str): The name of the variable to be used for the x-axis. - `y_var` (str): The name of the variable to be used for the y-axis (only applicable for scatter plots). - `hue_var` (str): The name of the variable used for hue mapping (only applicable for Visualization 2). - **Outputs**: - Save the first plot as `scatter_with_rug.png`. - Save the second plot as `scatter_with_rug_hue.png`. Constraints: - Ensure that the specified variables exist in the dataset. - Use seaborn\'s built-in datasets only. Implementation Requirements: 1. Load the specified dataset. 2. Create the scatter plot with rug plots along both axes for Visualization 1. 3. For Visualization 2, create a scatter plot with rug plots using hue mapping and customize its height and position. 4. Save the plots as specified. Example: ```python def visualize_with_rugplots(dataset_name, x_var, y_var, hue_var): import seaborn as sns import matplotlib.pyplot as plt # Load the dataset data = sns.load_dataset(dataset_name) # Visualization 1: Scatter plot with rug plots along both axes plt.figure(figsize=(10, 6)) sns.scatterplot(data=data, x=x_var, y=y_var) sns.rugplot(data=data, x=x_var) sns.rugplot(data=data, y=y_var) plt.savefig(\'scatter_with_rug.png\') plt.clf() # Visualization 2: Scatter plot with hue mapping and customized rug plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=data, x=x_var, y=y_var, hue=hue_var) sns.rugplot(data=data, x=x_var, y=y_var, hue=hue_var, height=0.1, clip_on=False) plt.savefig(\'scatter_with_rug_hue.png\') plt.clf() ``` Test your function with the dataset \\"tips\\" and parameters `total_bill`, `tip`, and `time`. ```python visualize_with_rugplots(\\"tips\\", \\"total_bill\\", \\"tip\\", \\"time\\") ``` This should generate two PNG files: `scatter_with_rug.png` and `scatter_with_rug_hue.png`, adhering to the specifications.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_with_rugplots(dataset_name, x_var, y_var, hue_var): Generates two scatter plots with rug plots and saves them as PNG files. Args: dataset_name (str): The name of a seaborn dataset (e.g., \\"tips\\"). x_var (str): The name of the variable to be used for the x-axis. y_var (str): The name of the variable to be used for the y-axis. hue_var (str): The name of the variable used for hue mapping. # Load the dataset data = sns.load_dataset(dataset_name) # Ensure the specified variables exist in the dataset if x_var not in data.columns or y_var not in data.columns or hue_var not in data.columns: raise ValueError(\\"Specified variables do not exist in the dataset\\") # Visualization 1: Scatter plot with rug plots along both axes plt.figure(figsize=(10, 6)) sns.scatterplot(data=data, x=x_var, y=y_var) sns.rugplot(data=data, x=x_var) sns.rugplot(data=data, y=y_var) plt.savefig(\'scatter_with_rug.png\') plt.clf() # Visualization 2: Scatter plot with hue mapping and customized rug plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=data, x=x_var, y=y_var, hue=hue_var) sns.rugplot(data=data, x=x_var, y=y_var, hue=hue_var, height=0.05, clip_on=False) plt.savefig(\'scatter_with_rug_hue.png\') plt.clf()"},{"question":"Model State Management with PyTorch You are working on a project that involves training a neural network model using PyTorch. You need to implement functionalities for saving and loading the model\'s state dictionary along with handling tensor views efficiently. Your task is to implement two Python functions: 1. `save_model` - to save a model\'s state dictionary to a specified file. 2. `load_model` - to load a model\'s state dictionary from a specified file back into a given model. Function Signatures: ```python def save_model(model: torch.nn.Module, filepath: str) -> None: Save the state dictionary of the given model to the specified file. Args: model (torch.nn.Module): The model whose state dict is to be saved. filepath (str): The location where the state dict should be stored. pass def load_model(model: torch.nn.Module, filepath: str) -> None: Load the state dictionary from the specified file into the given model. Args: model (torch.nn.Module): The model which will load the state dict. filepath (str): The location from where the state dict should be loaded. pass ``` # Constraints: - The file should be saved with a \'.pth\' extension. - Ensure that your solution can handle storing module states with complex architectures (e.g., modules containing other submodules). - Additionally, implement functionality to save and load tensors that are part of a custom data structure, preserving their storage and view relationships where necessary. # Example Usage: ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) # Create a model instance model = SimpleModel() # Save the model state save_model(model, \'simple_model.pth\') # Create a new model instance and load state new_model = SimpleModel() load_model(new_model, \'simple_model.pth\') print(new_model.state_dict()) ``` # Additional Task: You also need to demonstrate how to save and load a custom data structure containing tensors with preserved views. Create a function `example_custom_structure` that saves and loads a dictionary containing view-shared tensors. ```python def example_custom_structure(): Demonstrate saving and loading a custom data structure that contains view-shared tensors. # Example tensor with views large = torch.arange(1, 1000) small = large[0:10] # Save the custom structure custom_data = {\'large\': large, \'small\': small} torch.save(custom_data, \'custom_data.pth\') # Load the custom structure loaded_data = torch.load(\'custom_data.pth\') # Verify the views return loaded_data ``` Ensure that `loaded_data[\'small\']` still preserves its view relationship with `loaded_data[\'large\']`. # Evaluation Criteria: 1. Correctness: Ensure the implemented functions achieve the described functionalities. 2. Efficiency: Solutions should handle large tensors efficiently. 3. Preservation: Correctly maintain view relationships between tensors in custom data structures.","solution":"import torch def save_model(model: torch.nn.Module, filepath: str) -> None: Save the state dictionary of the given model to the specified file. Args: model (torch.nn.Module): The model whose state dict is to be saved. filepath (str): The location where the state dict should be stored. torch.save(model.state_dict(), filepath) def load_model(model: torch.nn.Module, filepath: str) -> None: Load the state dictionary from the specified file into the given model. Args: model (torch.nn.Module): The model which will load the state dict. filepath (str): The location from where the state dict should be loaded. model.load_state_dict(torch.load(filepath)) def example_custom_structure(): Demonstrate saving and loading a custom data structure that contains view-shared tensors. # Example tensor with views large = torch.arange(1, 1000) small = large[0:10] # Save the custom structure custom_data = {\'large\': large, \'small\': small} torch.save(custom_data, \'custom_data.pth\') # Load the custom structure loaded_data = torch.load(\'custom_data.pth\') return loaded_data"},{"question":"**Question: Efficient Line Retrieval from Large Text Files** In this task, you are required to utilize the `linecache` module to implement a function that efficiently retrieves multiple lines from a large text file. This will be particularly useful for debugging large codebases or analyzing specific parts of large logs. # Function Signature ```python def get_lines_from_file(file_path: str, line_numbers: list) -> list: Retrieves specified lines from a given file. Parameters: file_path (str): The path of the file from which lines are to be retrieved. line_numbers (list): A list of integers, each representing a line number to retrieve. Returns: list: A list of strings, where each string is a line from the file corresponding to the provided line numbers. The lines should be in the same order as the line numbers provided in the input list. If a line cannot be retrieved, the result should include an empty string for that line. ``` # Inputs - `file_path`: A string representing the path to the file to read from. - `line_numbers`: A list of integers representing the line numbers to retrieve. # Output - A list of strings where each string corresponds to a line from the file. The order of lines in this list should match the order of the requested line numbers. If a line cannot be retrieved, include an empty string for that line. # Constraints - The line numbers are 1-based (i.e., the first line in the file is line 1). - You can assume the file exists and is readable. - The function should be efficient and make use of the `linecache` module to optimize repeated access to the same lines. # Example ```python # Assume \'example.txt\' contains the following lines: # Line 1: Hello World # Line 2: This is a test file. # Line 3: It contains several lines. # Line 4: The purpose is to test line retrieval. # Line 5: End of file. lines = get_lines_from_file(\'example.txt\', [1, 3, 5]) print(lines) # Output: [\'Hello Worldn\', \'It contains several lines.n\', \'End of file.n\'] lines = get_lines_from_file(\'example.txt\', [2, 4]) print(lines) # Output: [\'This is a test file.n\', \'The purpose is to test line retrieval.n\'] ``` # Notes - Use the `linecache.getline()` function from the `linecache` module for retrieving lines. - Use `linecache.clearcache()` to clear the cache if necessary between different file accesses to avoid using stale data.","solution":"import linecache def get_lines_from_file(file_path: str, line_numbers: list) -> list: Retrieves specified lines from a given file. Parameters: file_path (str): The path of the file from which lines are to be retrieved. line_numbers (list): A list of integers, each representing a line number to retrieve. Returns: list: A list of strings, where each string is a line from the file corresponding to the provided line numbers. The lines should be in the same order as the line numbers provided in the input list. If a line cannot be retrieved, the result should include an empty string for that line. retrieved_lines = [] for line_number in line_numbers: line = linecache.getline(file_path, line_number) retrieved_lines.append(line) linecache.clearcache() return retrieved_lines"},{"question":"Coding Assessment Question You are tasked with writing a Python script that connects to an IMAP email server using a secure SSL connection. Your script should demonstrate the following capabilities: 1. Connecting to the server using the `IMAP4_SSL` class from the `imaplib` module. 2. Authenticating with the server using a provided username and password. 3. Selecting the inbox and searching for all emails from a specific sender. 4. Fetching the full content of the emails from that sender. 5. Handling and logging any potential errors that occur during the process. # Input - `host`: The host address of the IMAP server (string). - `username`: The username for authentication (string). - `password`: The password for authentication (string). - `sender_email`: The email address of the sender to search for (string). # Output - Prints the content of each email from the specified sender. - Logs any errors encountered during the process. # Constraints - Use the `IMAP4_SSL` class for connecting to the server. - Ensure proper handling of potential exceptions such as connection errors, authentication errors, and fetch errors. - Follow security best practices for handling passwords. # Example ```python host = \\"imap.example.com\\" username = \\"user@example.com\\" password = \\"securepassword\\" sender_email = \\"sender@example.com\\" ``` # Performance Requirements - The script should handle connections and operations efficiently. - Proper error handling should not result in unhandled exceptions or crashes. # Implementation Implement the script in Python using the `imaplib` module as described above. ```python import imaplib import getpass def fetch_emails_from_sender(host, username, password, sender_email): try: # Connect to the server using IMAP4_SSL with imaplib.IMAP4_SSL(host) as M: # Login to the server M.login(username, password) # Select the inbox M.select(\'inbox\') # Search for emails from the specified sender status, messages = M.search(None, f\'(FROM \\"{sender_email}\\")\') if status != \'OK\': print(f\'Error searching for emails: {status}\') return # Fetch and print the full content of each email for num in messages[0].split(): status, data = M.fetch(num, \'(RFC822)\') if status != \'OK\': print(f\'Error fetching email {num}: {status}\') continue print(f\'Message {num}n{data[0][1].decode(\\"utf-8\\")}n\') # Close the connection to the inbox M.close() # Logout from the server M.logout() except imaplib.IMAP4.error as e: print(f\'IMAP error: {e}\') except Exception as e: print(f\'Unexpected error: {e}\') # Example usage host = \\"imap.example.com\\" username = \\"user@example.com\\" password = getpass.getpass(\\"Password: \\") sender_email = \\"sender@example.com\\" fetch_emails_from_sender(host, username, password, sender_email) ```","solution":"import imaplib import logging logging.basicConfig(level=logging.INFO) def fetch_emails_from_sender(host, username, password, sender_email): try: # Connect to the server using IMAP4_SSL with imaplib.IMAP4_SSL(host) as M: logging.info(\\"Connected to the server\\") # Login to the server M.login(username, password) logging.info(\\"Logged in to the server\\") # Select the inbox M.select(\'inbox\') logging.info(\\"Inbox selected\\") # Search for emails from the specified sender status, messages = M.search(None, f\'(FROM \\"{sender_email}\\")\') if status != \'OK\': logging.error(f\'Error searching for emails: {status}\') return # Fetch and print the full content of each email for num in messages[0].split(): status, data = M.fetch(num, \'(RFC822)\') if status != \'OK\': logging.error(f\'Error fetching email {num}: {status}\') continue logging.info(f\'Message {num}n{data[0][1].decode(\\"utf-8\\")}n\') # Close the connection to the inbox M.close() logging.info(\\"Closed the inbox\\") # Logout from the server M.logout() logging.info(\\"Logged out from the server\\") except imaplib.IMAP4.error as e: logging.error(f\'IMAP error: {e}\') except Exception as e: logging.error(f\'Unexpected error: {e}\')"},{"question":"You are tasked with implementing a simulation of a task processing system using Python\'s `queue` module. This system will manage tasks in a multi-threaded environment, ensuring tasks are executed in an orderly manner based on their assigned priorities. # Requirements 1. **Task Class**: Create a `Task` class that represents a task with a `priority` and `description`. 2. **TaskProcessor Class**: Create a `TaskProcessor` class that uses a `PriorityQueue` to manage and process tasks with the following methods: - `add_task(task: Task)`: Adds a new task to the queue. - `process_tasks()`: Processes all tasks in the queue based on their priority in a separate thread. 3. **Thread Safety**: Ensure that the task addition and processing are thread-safe. 4. **Demonstration**: Write a demonstration script that: - Creates several tasks with different priorities. - Adds these tasks to the `TaskProcessor`. - Starts the task processing. - Ensures that tasks are processed in the order of their priorities. # Constraints - The number of tasks can be up to 1000. - Priorities are represented as integers (lower value means higher priority). - Task descriptions are unique strings. # Example ```python # Task and TaskProcessor classes should be implemented here # Demonstration script task_processor = TaskProcessor() # Add tasks with varying priorities tasks = [ Task(priority=5, description=\\"Task 1\\"), Task(priority=2, description=\\"Task 2\\"), Task(priority=1, description=\\"Task 3\\"), Task(priority=4, description=\\"Task 4\\"), ] for task in tasks: task_processor.add_task(task) # Process tasks task_processor.process_tasks() # Expected output: # Processing task: Task 3 with priority 1 # Processing task: Task 2 with priority 2 # Processing task: Task 4 with priority 4 # Processing task: Task 1 with priority 5 ``` Write the `Task` and `TaskProcessor` classes as specified along with the demonstration script.","solution":"import threading from queue import PriorityQueue class Task: def __init__(self, priority, description): self.priority = priority self.description = description def __lt__(self, other): return self.priority < other.priority class TaskProcessor: def __init__(self): self.queue = PriorityQueue() self.lock = threading.Lock() def add_task(self, task): with self.lock: self.queue.put(task) def process_tasks(self): def process(): while not self.queue.empty(): task = self.queue.get() print(f\\"Processing task: {task.description} with priority {task.priority}\\") self.queue.task_done() processing_thread = threading.Thread(target=process) processing_thread.start() processing_thread.join()"},{"question":"# PyTorch Numerical Precision Assessment Objective You are required to implement a function in PyTorch that evaluates the numerical stability and accuracy of different matrix operations under varied precision settings. You will analyze the results of batched and non-batched operations, apply precision settings, and handle potential inaccuracies. Problem Statement Implement a function `evaluate_numerical_stability` that performs the following tasks: 1. Creates two 3D tensors `A` and `B` with shapes `[batch_size, n, n]`, containing random floating-point numbers. 2. Computes the batched matrix multiplication result `C_batched = torch.bmm(A, B)`. 3. Computes the individual matrix multiplication results `C_individual` by iterating over batch elements. 4. Compares the first element of `C_batched` with the first element of `C_individual` and calculates the absolute difference. 5. Repeats steps 2-4 for different precision settings (default floating-point precision and TF32 enabled/disabled). 6. Returns a dictionary containing the absolute differences for each precision setting. Function Signature ```python def evaluate_numerical_stability(batch_size: int, n: int) -> dict: pass ``` Input - `batch_size`: An integer representing the batch size. - `n`: An integer representing the dimension of the matrices. Output - A dictionary where keys are precision settings such as `\\"default\\", \\"TF32_enabled\\", \\"TF32_disabled\\"` and values are the absolute differences calculated in step 4. Constraints - Use the `torch` library. - Ensure reproducibility by setting a random seed. - Handle any potential overflows and precision issues as described in the documentation. Example Usage ```python result = evaluate_numerical_stability(batch_size=10, n=5) print(result) # Output: {\'default\': <abs_difference_default>, \'TF32_enabled\': <abs_difference_tf32_enabled>, \'TF32_disabled\': <abs_difference_tf32_disabled>} ``` Hints - Use `torch.manual_seed(seed)` to ensure reproducibility. - Control precision settings using `torch.backends.cuda.matmul.allow_tf32`. - Use methods like `torch.abs` for calculating absolute differences.","solution":"import torch def evaluate_numerical_stability(batch_size: int, n: int) -> dict: # Set random seed for reproducibility torch.manual_seed(0) # Create tensors with random numbers A = torch.rand(batch_size, n, n, dtype=torch.float32) B = torch.rand(batch_size, n, n, dtype=torch.float32) # To store results results = {} # Default precision C_batched_default = torch.bmm(A, B) C_individual_default = torch.stack([torch.mm(A[i], B[i]) for i in range(batch_size)]) difference_default = torch.abs(C_batched_default[0] - C_individual_default[0]).mean().item() results[\'default\'] = difference_default # TF32 enabled torch.backends.cuda.matmul.allow_tf32 = True C_batched_tf32_enabled = torch.bmm(A, B) C_individual_tf32_enabled = torch.stack([torch.mm(A[i], B[i]) for i in range(batch_size)]) difference_tf32_enabled = torch.abs(C_batched_tf32_enabled[0] - C_individual_tf32_enabled[0]).mean().item() results[\'TF32_enabled\'] = difference_tf32_enabled # TF32 disabled torch.backends.cuda.matmul.allow_tf32 = False C_batched_tf32_disabled = torch.bmm(A, B) C_individual_tf32_disabled = torch.stack([torch.mm(A[i], B[i]) for i in range(batch_size)]) difference_tf32_disabled = torch.abs(C_batched_tf32_disabled[0] - C_individual_tf32_disabled[0]).mean().item() results[\'TF32_disabled\'] = difference_tf32_disabled return results"},{"question":"# Question: Customize and Compare Plot Aesthetics using Seaborn You are provided with a dataset and asked to create several plots to illustrate different aesthetic styles and figure contexts using the seaborn package. Your task is to demonstrate your understanding by implementing a function `customize_and_compare_plots` that generates and saves these plots. Function Signature ```python def customize_and_compare_plots(data: np.ndarray) -> None: pass ``` Input - `data` (np.ndarray): A 2D numpy array of shape (n, m) representing the dataset. Requirements 1. **Default Plot**: - Create a box plot using the `data` with seaborn\'s default theme and save it as \\"default_plot.png\\". 2. **Customized Styles**: - Create four separate sine wave plots each with different Seaborn styles: - \\"darkgrid\\" - \\"white\\" - \\"ticks\\" - \\"dark\\" - Use the provided `sinplot` function to generate the sine wave plots. - Save each plot as \\"darkgrid_plot.png\\", \\"white_plot.png\\", \\"ticks_plot.png\\", and \\"dark_plot.png\\" respectively. 3. **Removing Axes Spines**: - Create a box plot with the \\"whitegrid\\" style using the `data`. - Remove the left spine of the plot. - Save the plot as \\"despined_plot.png\\". 4. **Context Scaling**: - Create three different sine wave plots each with different Seaborn contexts: - \\"notebook\\" - \\"talk\\" - \\"poster\\" - Save each plot as \\"notebook_context_plot.png\\", \\"talk_context_plot.png\\", and \\"poster_context_plot.png\\" respectively. 5. **Temporary Style Settings**: - Use the `with` statement to set the style temporarily within the same figure. - Create a 2x2 subplot where each subplot has a different Seaborn style (\\"darkgrid\\", \\"white\\", \\"ticks\\", \\"whitegrid\\") using the `sinplot` function. - Save the combined figure as \\"temporary_styles_plot.png\\". 6. **Overriding Style Elements**: - Create a sine wave plot using the \\"darkgrid\\" style but set the `axes.facecolor` to light grey (\\".9\\"). - Save the plot as \\"overridden_style_plot.png\\". Notes - Ensure that each saved plot image is properly labeled and displays the respective style or context name. - Use the `sinplot` function as provided in the documentation to generate the sine wave plots. - Consider readability and aesthetics in your plots. ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt def sinplot(n=10, flip=1): x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip) def customize_and_compare_plots(data: np.ndarray) -> None: # Default plot sns.set_theme() sns.boxplot(data=data) plt.savefig(\\"default_plot.png\\") plt.clf() # Customized styles styles = [\\"darkgrid\\", \\"white\\", \\"ticks\\", \\"dark\\"] for style in styles: sns.set_style(style) sinplot() plt.savefig(f\\"{style}_plot.png\\") plt.clf() # Removing axes spines sns.set_style(\\"whitegrid\\") sns.boxplot(data=data) sns.despine(left=True) plt.savefig(\\"despined_plot.png\\") plt.clf() # Context scaling contexts = [\\"notebook\\", \\"talk\\", \\"poster\\"] for context in contexts: sns.set_context(context) sinplot() plt.savefig(f\\"{context}_context_plot.png\\") plt.clf() # Temporary style settings f = plt.figure(figsize=(6, 6)) gs = f.add_gridspec(2, 2) with sns.axes_style(\\"darkgrid\\"): ax = f.add_subplot(gs[0, 0]) sinplot(6) with sns.axes_style(\\"white\\"): ax = f.add_subplot(gs[0, 1]) sinplot(6) with sns.axes_style(\\"ticks\\"): ax = f.add_subplot(gs[1, 0]) sinplot(6) with sns.axes_style(\\"whitegrid\\"): ax = f.add_subplot(gs[1, 1]) sinplot(6) f.tight_layout() plt.savefig(\\"temporary_styles_plot.png\\") plt.clf() # Overriding style elements sns.set_style(\\"darkgrid\\", {\\"axes.facecolor\\": \\".9\\"}) sinplot() plt.savefig(\\"overridden_style_plot.png\\") plt.clf() ``` Evaluation Criteria - Correctness: The function should generate and save all the requested plots accurately. - Code Quality: The code should be clean, readable, and well-documented. - Aesthetics: The plots should be visually pleasing and demonstrate the various style and context settings effectively.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def sinplot(n=10, flip=1): x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip) def customize_and_compare_plots(data: np.ndarray) -> None: # Default plot sns.set_theme() sns.boxplot(data=data) plt.savefig(\\"default_plot.png\\") plt.clf() # Customized styles styles = [\\"darkgrid\\", \\"white\\", \\"ticks\\", \\"dark\\"] for style in styles: sns.set_style(style) sinplot() plt.savefig(f\\"{style}_plot.png\\") plt.clf() # Removing axes spines sns.set_style(\\"whitegrid\\") sns.boxplot(data=data) sns.despine(left=True) plt.savefig(\\"despined_plot.png\\") plt.clf() # Context scaling contexts = [\\"notebook\\", \\"talk\\", \\"poster\\"] for context in contexts: sns.set_context(context) sinplot() plt.savefig(f\\"{context}_context_plot.png\\") plt.clf() # Temporary style settings f = plt.figure(figsize=(6, 6)) gs = f.add_gridspec(2, 2) with sns.axes_style(\\"darkgrid\\"): ax = f.add_subplot(gs[0, 0]) sinplot(6) with sns.axes_style(\\"white\\"): ax = f.add_subplot(gs[0, 1]) sinplot(6) with sns.axes_style(\\"ticks\\"): ax = f.add_subplot(gs[1, 0]) sinplot(6) with sns.axes_style(\\"whitegrid\\"): ax = f.add_subplot(gs[1, 1]) sinplot(6) f.tight_layout() plt.savefig(\\"temporary_styles_plot.png\\") plt.clf() # Overriding style elements sns.set_style(\\"darkgrid\\", {\\"axes.facecolor\\": \\".9\\"}) sinplot() plt.savefig(\\"overridden_style_plot.png\\") plt.clf()"},{"question":"**Objective:** Create a Python function that reads a JSON file containing a list of student names and their scores, processes the data to calculate the average score, and writes the results to a new JSON file. Additionally, the output file should include a formatted string summarizing the data. Function Signature ```python def process_student_scores(input_filename: str, output_filename: str) -> None: pass ``` # Requirements: 1. **Input:** - `input_filename`: A string representing the name of the JSON file to read. This file contains a list of dictionaries where each dictionary has the keys \\"name\\" (a string) and \\"score\\" (a float). ```json [ {\\"name\\": \\"Alice\\", \\"score\\": 85.5}, {\\"name\\": \\"Bob\\", \\"score\\": 90.75}, {\\"name\\": \\"Charlie\\", \\"score\\": 78.0} ] ``` 2. **Output:** - `output_filename`: A string representing the name of the JSON file to write the results to. - The output JSON file should contain the following: - `original_data`: The original data read from the input file. - `average_score`: The average score of all students. - `formatted_summary`: A formatted string summarizing the data, formatted using an f-string. Example of the output JSON structure: ```json { \\"original_data\\": [ {\\"name\\": \\"Alice\\", \\"score\\": 85.5}, {\\"name\\": \\"Bob\\", \\"score\\": 90.75}, {\\"name\\": \\"Charlie\\", \\"score\\": 78.0} ], \\"average_score\\": 84.75, \\"formatted_summary\\": \\"Processed 3 students. Average score is 84.75.\\" } ``` 3. **Requirements:** - Use `f-string` for creating the formatted summary. - Read and write files using the context management (`with open` statement). - Handle exceptions for file reading/writing errors and print appropriate messages. Example: ```python # Example usage and expected behavior input_filename = \'students.json\' output_filename = \'processed_students.json\' process_student_scores(input_filename, output_filename) ``` For the given input, the `output_filename` file should have the following content: ```json { \\"original_data\\": [ {\\"name\\": \\"Alice\\", \\"score\\": 85.5}, {\\"name\\": \\"Bob\\", \\"score\\": 90.75}, {\\"name\\": \\"Charlie\\", \\"score\\": 78.0} ], \\"average_score\\": 84.75, \\"formatted_summary\\": \\"Processed 3 students. Average score is 84.75.\\" } ``` 4. **Constraints:** - The JSON file must be encoded in UTF-8. - Assume the input JSON file is correctly formatted and contains only the required \\"name\\" and \\"score\\" fields for simplicity. # Notes: - Utilize relevant string formatting and file handling techniques as discussed in the provided documentation. - Ensure to validate and handle potential exceptions during file operations.","solution":"import json def process_student_scores(input_filename: str, output_filename: str) -> None: try: with open(input_filename, \'r\', encoding=\'utf-8\') as infile: data = json.load(infile) if not data: raise ValueError(\\"Input file is empty or the data is not in the expected format.\\") total_score = 0 count = 0 for student in data: total_score += student.get(\'score\', 0) count += 1 average_score = total_score / count if count else 0 formatted_summary = f\\"Processed {count} students. Average score is {average_score:.2f}.\\" output_data = { \\"original_data\\": data, \\"average_score\\": average_score, \\"formatted_summary\\": formatted_summary } with open(output_filename, \'w\', encoding=\'utf-8\') as outfile: json.dump(output_data, outfile, indent=4) except FileNotFoundError: print(f\\"Error: The file {input_filename} does not exist.\\") except json.JSONDecodeError: print(f\\"Error: The file {input_filename} is not in valid JSON format.\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Objective: Demonstrate your understanding of the Seaborn library\'s `relplot` function by loading a dataset, customizing various plot parameters, and generating meaningful visualizations. Description: You are provided with the \\"tips\\" dataset from the Seaborn library. Using this dataset, create a series of visualizations to analyze the relationships between different variables. Follow the steps and requirements below to complete this task. Tasks: 1. Load the \\"tips\\" dataset using Seaborn. 2. Create the following visualizations: - **Scatter Plot**: - Plot `total_bill` on the x-axis and `tip` on the y-axis. - Color the points based on `smoker` status. - Use different marker styles for different days of the week (`day`). - Add a legend that clearly distinguishes between the different `day` values. - **Faceted Scatter Plot**: - Create a single scatter plot with `total_bill` on the x-axis and `tip` on the y-axis. - Facet the plot into different columns based on `time` (Lunch or Dinner). - Facet the plot into different rows based on `sex` (Male or Female). - **Line Plot**: - Create a line plot with `size` on the x-axis and average `total_bill` on the y-axis. - Color the lines based on `smoker` status. - Use different line styles for different `time` values. - Display the mean and 95% confidence interval for each group. - Adjust the plot\'s size to have a height of 5 and an aspect ratio of 0.8. 3. Use Seaborn and Matplotlib functionalities to adjust the title, labels, and layout aspects of each plot to make them clear and informative. Expected Output: Three visualizations should be created and properly displayed: 1. A scatter plot with `total_bill` vs. `tip` colored by `smoker` status and styled by `day`. 2. A faceted scatter plot with separate subplots based on `time` and `sex`. 3. A line plot showing average `total_bill` by `size`, colored and styled by `smoker` and `time`, respectively. **Input**: - No direct input is required for this task. The dataset is loaded within the code. **Constraints**: - Use only the Seaborn library for data visualization. - Ensure that the visualizations are clear and informative. **Performance**: - The code should execute efficiently, but performance is not a primary concern given the size of the dataset. Example Solution: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset from Seaborn tips = sns.load_dataset(\\"tips\\") # Task 1: Scatter Plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", style=\\"day\\") plt.title(\'Scatter Plot: Total Bill vs. Tip\') plt.legend(title=\'Day of the Week\') plt.show() # Task 2: Faceted Scatter Plot g = sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", col=\\"time\\", row=\\"sex\\", kind=\\"scatter\\") g.set_titles(\\"Time: {col_name} | Sex: {row_name}\\") g.set_axis_labels(\\"Total Bill\\", \\"Tip\\") plt.show() # Task 3: Line Plot plt.figure(figsize=(10, 5)) g = sns.relplot(data=tips, x=\\"size\\", y=\\"total_bill\\", hue=\\"smoker\\", style=\\"time\\", kind=\\"line\\", height=5, aspect=0.8) g.set_axis_labels(\\"Size\\", \\"Average Total Bill\\") g.set_titles(\\"Average Total Bill by Size\\") plt.show() ``` Note: The sample solution illustrates how to implement each visualization. Ensure your plots\' titles, labels, and customization match the specified requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset from Seaborn tips = sns.load_dataset(\\"tips\\") # Task 1: Scatter Plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", style=\\"day\\") plt.title(\'Scatter Plot: Total Bill vs. Tip\') plt.legend(title=\'Day of the Week\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show() # Task 2: Faceted Scatter Plot facet_scatter_plot = sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", col=\\"time\\", row=\\"sex\\", kind=\\"scatter\\") facet_scatter_plot.set_titles(\\"Time: {col_name} | Sex: {row_name}\\") facet_scatter_plot.set_axis_labels(\\"Total Bill\\", \\"Tip\\") plt.show() # Task 3: Line Plot plt.figure(figsize=(10, 5)) line_plot = sns.relplot(data=tips, x=\\"size\\", y=\\"total_bill\\", hue=\\"smoker\\", style=\\"time\\", kind=\\"line\\", height=5, aspect=0.8) line_plot.set_axis_labels(\\"Size\\", \\"Average Total Bill\\") plt.title(\'Average Total Bill by Size with Confidence Intervals\') plt.show()"},{"question":"**Objective:** You are required to preprocess tensors according to specific criteria and evaluate the quality of your preprocessing using utility functions provided by the `torch.ao.ns.fx.utils` module. This will test your understanding of tensor manipulations and PyTorch\'s utility functions. **Task:** Implement a function `evaluate_tensor_transformations` that takes in two tensors and applies a specified transformation to the second tensor. After applying the transformation, your function should compute and return three evaluation metrics: Signal-to-Quantization-Noise Ratio (SQNR), normalized L2 error, and cosine similarity between the original (first tensor) and the transformed (second tensor) tensors. **Function Signature:** ```python import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity def evaluate_tensor_transformations(tensor1: torch.Tensor, tensor2: torch.Tensor, transformation: str) -> dict: pass ``` **Input:** - `tensor1`: A PyTorch tensor. - `tensor2`: A PyTorch tensor. This tensor will undergo a specified transformation. - `transformation`: A string specifying the type of transformation to apply to `tensor2`. It could be one of the following: - `\'normalize\'`: Normalize `tensor2` using mean and standard deviation. - `\'quantize\'`: Quantize `tensor2` to a lower precision. **Output:** - A dictionary containing: - `\'sqnr\'`: Signal-to-Quantization-Noise Ratio between `tensor1` and the transformed `tensor2`. - `\'normalized_l2_error\'`: Normalized L2 error between `tensor1` and the transformed `tensor2`. - `\'cosine_similarity\'`: Cosine similarity between `tensor1` and the transformed `tensor2`. **Constraints:** - You may assume that input tensors `tensor1` and `tensor2` have the same shape. - Only the specified transformations are to be implemented (`\'normalize\'` and `\'quantize\'`). **Example:** ```python import torch tensor1 = torch.tensor([1.0, 2.0, 3.0, 4.0]) tensor2 = torch.tensor([1.1, 2.1, 3.1, 4.1]) results = evaluate_tensor_transformations(tensor1, tensor2, \'normalize\') print(results) # {\'sqnr\': ..., \'normalized_l2_error\': ..., \'cosine_similarity\': ...} results = evaluate_tensor_transformations(tensor1, tensor2, \'quantize\') print(results) # {\'sqnr\': ..., \'normalized_l2_error\': ..., \'cosine_similarity\': ...} ``` **Notes:** - For the `\'normalize\'` transformation, use the mean and standard deviation of `tensor2` to normalize it. - For the `\'quantize\'` transformation, simply simulate a quantization process by rounding `tensor2` to the nearest integer. Implement the function `evaluate_tensor_transformations` to fulfill the above requirements.","solution":"import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity def evaluate_tensor_transformations(tensor1: torch.Tensor, tensor2: torch.Tensor, transformation: str) -> dict: if transformation == \'normalize\': mean = tensor2.mean() std = tensor2.std() transformed_tensor2 = (tensor2 - mean) / std elif transformation == \'quantize\': transformed_tensor2 = torch.round(tensor2) else: raise ValueError(\\"Unsupported transformation type. Supported types are \'normalize\' and \'quantize\'.\\") sqnr = compute_sqnr(tensor1, transformed_tensor2) normalized_l2_error = compute_normalized_l2_error(tensor1, transformed_tensor2) cosine_similarity = compute_cosine_similarity(tensor1, transformed_tensor2) return { \'sqnr\': sqnr, \'normalized_l2_error\': normalized_l2_error, \'cosine_similarity\': cosine_similarity }"},{"question":"**Question: Implementing a Task Scheduler using Advanced Data Types** Design and implement a task scheduler that manages scheduled tasks with their respective priorities, execution times, and dependencies on other tasks. Utilize the `datetime`, `collections`, and `heapq` modules from the Python standard library. # Requirements: 1. **Task Class:** * Attributes: - `name` (str): The name of the task. - `priority` (int): The priority of the task (lower number means higher priority). - `execution_time` (`datetime.datetime`): The execution time of the task. - `dependencies` (list of str): The list of task names that need to be completed before this task can execute. * Methods: - `__init__(self, name, priority, execution_time, dependencies)`: Initialize a task with the given attributes. 2. **TaskScheduler Class:** * Methods: - `__init__(self)`: Initialize an empty task scheduler. - `add_task(self, task)`: Add a task to the scheduler. - `remove_task(self, task_name)`: Remove a task by its name. - `get_task(self, task_name)`: Get details of a specific task by its name. - `execute_tasks(self)`: Execute all tasks based on their priorities and dependencies, printing the order of execution and time of each task. The tasks should be executed in priority order (lower number gets executed first), and tasks with unmet dependencies should be delayed until their dependencies are met. # Input Format: - `Task` objects should be added to the `TaskScheduler` using the `add_task` method. - The `execute_tasks` method should execute and print the tasks in the correct order. # Constraints: - Assume that there can be a maximum of `1000` tasks. - Execution of each task is instantaneous. - The `execution_time` of each task is given and you may use it to simulate when the task would be executed in a real system. # Example: ```python from datetime import datetime from collections import namedtuple, deque, defaultdict, Counter import heapq # Implement the Task and TaskScheduler classes here # Example usage: scheduler = TaskScheduler() task1 = Task(\'Task1\', 1, datetime(2023, 10, 10, 10, 0, 0), []) task2 = Task(\'Task2\', 2, datetime(2023, 10, 10, 11, 0, 0), [\'Task1\']) task3 = Task(\'Task3\', 3, datetime(2023, 10, 10, 12, 0, 0), [\'Task2\']) task4 = Task(\'Task4\', 1, datetime(2023, 10, 10, 9, 0, 0), []) task5 = Task(\'Task5\', 2, datetime(2023, 10, 10, 10, 30, 0), [\'Task4\']) scheduler.add_task(task1) scheduler.add_task(task2) scheduler.add_task(task3) scheduler.add_task(task4) scheduler.add_task(task5) scheduler.execute_tasks() ``` Expected Output: ``` Task Task4 executed at 2023-10-10 09:00:00 Task Task5 executed at 2023-10-10 10:30:00 Task Task1 executed at 2023-10-10 10:00:00 Task Task2 executed at 2023-10-10 11:00:00 Task Task3 executed at 2023-10-10 12:00:00 ``` **Note:** Ensure that you handle cyclic dependencies gracefully by detecting them and providing an appropriate error message.","solution":"from datetime import datetime from collections import deque, defaultdict import heapq class Task: def __init__(self, name, priority, execution_time, dependencies): self.name = name self.priority = priority self.execution_time = execution_time self.dependencies = dependencies def __lt__(self, other): return (self.priority, self.execution_time) < (other.priority, other.execution_time) class TaskScheduler: def __init__(self): self.tasks = {} self.graph = defaultdict(list) self.in_degree = defaultdict(int) def add_task(self, task): self.tasks[task.name] = task self.in_degree[task.name] = len(task.dependencies) for dep in task.dependencies: self.graph[dep].append(task.name) def remove_task(self, task_name): if task_name in self.tasks: task = self.tasks.pop(task_name) if task_name in self.in_degree: del self.in_degree[task_name] for dep in task.dependencies: if dep in self.graph and task_name in self.graph[dep]: self.graph[dep].remove(task_name) def get_task(self, task_name): return self.tasks.get(task_name, None) def execute_tasks(self): # Using Kahn\'s algorithm for topological sort priority_queue = [] execution_order = [] for task_name, task in self.tasks.items(): if self.in_degree[task_name] == 0: heapq.heappush(priority_queue, task) while priority_queue: current_task = heapq.heappop(priority_queue) execution_order.append(current_task) print(f\\"Task {current_task.name} executed at {current_task.execution_time}\\") for dependent in self.graph[current_task.name]: self.in_degree[dependent] -= 1 if self.in_degree[dependent] == 0: heapq.heappush(priority_queue, self.tasks[dependent]) if len(execution_order) != len(self.tasks): print(\\"Cycle detected! Some tasks have unresolved dependencies.\\") def run_example(): scheduler = TaskScheduler() task1 = Task(\'Task1\', 1, datetime(2023, 10, 10, 10, 0, 0), []) task2 = Task(\'Task2\', 2, datetime(2023, 10, 10, 11, 0, 0), [\'Task1\']) task3 = Task(\'Task3\', 3, datetime(2023, 10, 10, 12, 0, 0), [\'Task2\']) task4 = Task(\'Task4\', 1, datetime(2023, 10, 10, 9, 0, 0), []) task5 = Task(\'Task5\', 2, datetime(2023, 10, 10, 10, 30, 0), [\'Task4\']) scheduler.add_task(task1) scheduler.add_task(task2) scheduler.add_task(task3) scheduler.add_task(task4) scheduler.add_task(task5) scheduler.execute_tasks() if __name__ == \\"__main__\\": run_example()"},{"question":"<|Analysis Begin|> The provided documentation for PyTorch covers a section of `torch.sparse`, discussing various operations and tensor formats for handling sparse data. Key points are: 1. **Sparse Tensor Storage Formats**: The documentation details COO, CSR, CSC, BSR, and BSC tensor formats, including their construction and conversion routines. Each format offers unique advantages depending on the data\'s sparsity nature. 2. **Functionality Overview**: Functions to convert dense tensors to sparse formats and explanations of how sparse tensors are utilized (e.g., graph adjacency matrices, pruned weights). 3. **Operator Overview**: Details on the operations supported for sparse tensors and their behavior compared to dense tensors. 4. **Sparse Tensor Operations**: Essential operations specific to each sparse format, including multiplication, addition, and transpose. This documentation offers a foundational understanding that we can leverage to design a problem focusing on: - Creating and manipulating sparse tensors. - Performing operations such as matrix multiplication using PyTorch\'s sparse tensor functionalities. Given these observations, here is an appropriately challenging exercise: <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective**: Implement and manipulate sparse tensors using PyTorch. Context: Sparse matrices are essential in scientific computing and machine learning for efficient memory and computational usage with large, sparse data. PyTorch provides various sparse tensor formats to accommodate different types of sparsity. Problem Statement: 1. **Create a dense matrix `A` of shape (4, 4)**: ``` [[0, 2, 0, 0], [0, 0, 3, 0], [4, 0, 0, 5], [0, 0, 0, 0]] ``` 2. **Convert this dense matrix `A` to a sparse COO format**. 3. **Perform the following operations**: - Matrix multiplication of the sparse matrix `A` with a dense matrix `B` of shape (4, 2): ``` [[1, 0], [0, 1], [1, 1], [0, 1]] ``` - Convert the resulting matrix to CSR format. 4. **Construct a batch of sparse matrices**: - Given matrices `[[5, 0], [0, 1]]` and `[[1, 0], [0, 5]]`, convert them into a batched sparse CSR tensor. Function Signature: ```python import torch def sparse_tensor_operations(): # Step 1: Create the dense matrix A A = torch.tensor([[0, 2, 0, 0], [0, 0, 3, 0], [4, 0, 0, 5], [0, 0, 0, 0]]) # Step 2: Convert dense matrix A to sparse COO format A_sparse_coo = A.to_sparse() # Step 3: Define dense matrix B B = torch.tensor([[1, 0], [0, 1], [1, 1], [0, 1]]) # Step 3: Perform matrix multiplication (A_sparse_coo * B) result_sparse_coo = torch.sparse.mm(A_sparse_coo, B) # Step 3: Convert the result to CSR format result_sparse_csr = result_sparse_coo.to_sparse_csr() # Step 4: Construct a batch of sparse matrices and convert to batch CSR format dense_batch = torch.stack([ torch.tensor([[5, 0], [0, 1]]).float(), torch.tensor([[1, 0], [0, 5]]).float() ]) batch_sparse_csr = dense_batch.to_sparse_csr() return { \\"coo\\": A_sparse_coo, \\"matrix_multiplication\\": result_sparse_coo, \\"csr\\": result_sparse_csr, \\"batch_sparse_csr\\": batch_sparse_csr } # Example Usage: # result = sparse_tensor_operations() # print(result) ``` Constraints: 1. **Input/Output**: The function `sparse_tensor_operations()` does not accept any parameters or user inputs. It should return a dictionary with the specified sparse tensor formats. 2. **Performance**: Ensure the sparse operations are efficient and leverage the benefits of using sparse formats. Document each step and write any assumptions made. Test the function thoroughly to ensure correctness.","solution":"import torch def sparse_tensor_operations(): # Step 1: Create the dense matrix A A = torch.tensor([[0, 2, 0, 0], [0, 0, 3, 0], [4, 0, 0, 5], [0, 0, 0, 0]]).float() # Step 2: Convert dense matrix A to sparse COO format A_sparse_coo = A.to_sparse() # Step 3: Define dense matrix B B = torch.tensor([[1, 0], [0, 1], [1, 1], [0, 1]]).float() # Step 3: Perform matrix multiplication (A_sparse_coo * B) result_sparse_coo = torch.sparse.mm(A_sparse_coo, B) # Step 3: Convert the result to CSR format result_sparse_csr = result_sparse_coo.to_sparse_csr() # Step 4: Construct a batch of sparse matrices and convert to batch CSR format dense_batch = torch.stack([ torch.tensor([[5, 0], [0, 1]]).float(), torch.tensor([[1, 0], [0, 5]]).float() ]) batch_sparse_csr = dense_batch.to_sparse_csr() return { \\"coo\\": A_sparse_coo, \\"matrix_multiplication\\": result_sparse_coo, \\"csr\\": result_sparse_csr, \\"batch_sparse_csr\\": batch_sparse_csr }"},{"question":"Isotonic Regression Analysis Objective: Write a Python function that performs isotonic regression on a dataset and evaluates its performance using mean squared error. Your function should also visualize the fitting line compared to the original data. Function Signature: ```python def isotonic_regression_analysis(X: List[float], y: List[float], increasing: str = \'auto\') -> float: # Your implementation here ``` Input: - `X`: A list of floats representing the input feature values. - `y`: A list of floats representing the target values corresponding to `X`. - `increasing`: A string that is either \'auto\', True, or False, indicating the type of isotonic regression to be applied. Default is \'auto\'. Output: - Returns a float representing the mean squared error of the isotonic regression model. Constraints: 1. The length of `X` and `y` will be between 1 and 1000. 2. All elements of `X` and `y` are real numbers. 3. Weights are assumed to be 1 for simplicity in this problem. Requirements: 1. Implement the isotonic regression fitting using `IsotonicRegression` from scikit-learn. 2. Calculate and return the mean squared error between the predicted values and the actual target values. 3. Plot the original data points and the fitted isotonic regression line on a graph for visualization. Clearly label the axes and provide a legend. Example: ```python X = [1, 2, 3, 4, 5] y = [2, 1, 4, 3, 6] mse = isotonic_regression_analysis(X, y, increasing=True) print(mse) # It should print the mean squared error of the isotonic regression model. ``` Note: For visualization, you can use matplotlib to plot the graph. Ensure the plot displays the original data points and the isotonic regression line, making it easier to interpret the quality of the fit visually.","solution":"from sklearn.isotonic import IsotonicRegression from sklearn.metrics import mean_squared_error import matplotlib.pyplot as plt from typing import List def isotonic_regression_analysis(X: List[float], y: List[float], increasing: str = \'auto\') -> float: # Initialize the Isotonic Regression model iso_reg = IsotonicRegression(increasing=increasing) # Fit the model y_pred = iso_reg.fit_transform(X, y) # Calculate the mean squared error mse = mean_squared_error(y, y_pred) # Plot the original data points and the isotonic regression line plt.scatter(X, y, color=\'red\', label=\'Original Data\') plt.plot(X, y_pred, color=\'blue\', label=\'Isotonic Regression\') plt.xlabel(\'X\') plt.ylabel(\'y\') plt.title(\'Isotonic Regression Analysis\') plt.legend() plt.show() return mse"},{"question":"**FTP File Synchronization** You are required to write a Python program that uses the `ftplib` module to synchronize files between a local directory and a remote FTP server. The program should be able to: 1. Connect to the FTP server. 2. Log in using provided credentials. 3. List all files in a specified directory on the FTP server. 4. Download any files from the FTP directory that are not present in the local directory. 5. Upload any files from the local directory that are not present in the FTP directory. 6. Handle potential exceptions gracefully during these operations. **Function Signature** ```python def synchronize_ftp(local_dir: str, ftp_details: dict) -> None: pass ``` **Parameters** - `local_dir` (str): The local directory to synchronize files with. - `ftp_details` (dict): A dictionary containing the following keys: - `host` (str): The FTP server address. - `user` (str): The username for FTP login. - `passwd` (str): The password for FTP login. - `remote_dir` (str): The directory on the FTP server to synchronize files with. **Example** ```python ftp_details = { \'host\': \'ftp.example.com\', \'user\': \'username\', \'passwd\': \'password\', \'remote_dir\': \'/path/to/remote/dir\' } synchronize_ftp(\'/path/to/local/dir\', ftp_details) ``` **Constraints** - Your solution should make use of the `ftplib` module. - Ensure you handle network errors, authentication issues, and file I/O errors using appropriate exception handling. - Avoid data redundancy, meaning the same file should not be duplicated in either the local or remote directories. - Performance considerations: Assume the number of files is moderate (up to a few thousand). **Hints** - Utilize `FTP.retrbinary` for downloading files and `FTP.storbinary` for uploading files. - Use the `FTP.nlst` method to list filenames in the remote directory. - Use Python\'s `os` module to list filenames in the local directory. **Expected Output** The function does not return anything. It should print appropriate messages indicating the progress and any errors encountered during the synchronization process.","solution":"import os import ftplib def synchronize_ftp(local_dir: str, ftp_details: dict) -> None: try: # Connect and login to the FTP server ftp = ftplib.FTP(ftp_details[\'host\']) ftp.login(user=ftp_details[\'user\'], passwd=ftp_details[\'passwd\']) ftp.cwd(ftp_details[\'remote_dir\']) # List files in the remote and local directories remote_files = ftp.nlst() local_files = os.listdir(local_dir) # Download files from remote directory that are not in local directory for remote_file in remote_files: if remote_file not in local_files: local_file_path = os.path.join(local_dir, remote_file) with open(local_file_path, \'wb\') as local_file: ftp.retrbinary(\'RETR \' + remote_file, local_file.write) print(f\\"Downloaded: {remote_file}\\") # Upload files from local directory that are not in remote directory for local_file in local_files: if local_file not in remote_files: local_file_path = os.path.join(local_dir, local_file) with open(local_file_path, \'rb\') as file: ftp.storbinary(\'STOR \' + local_file, file) print(f\\"Uploaded: {local_file}\\") ftp.quit() except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Objective**: Demonstrate your understanding of Seaborn\'s `stripplot` function by creating a customized scatter plot. **Problem Statement**: You are given a dataset containing information about restaurant tips. Your task is to create a customized strip plot using Seaborn to visualize the relationship between the total bill amount and the days of the week, differentiated by gender and meal time. **Dataset**: The dataset is available as part of Seaborn\'s built-in datasets and can be loaded with the following command: ```python tips = sns.load_dataset(\\"tips\\") ``` **Requirements**: 1. Create a strip plot showing the `total_bill` on the x-axis and `day` on the y-axis. 2. Use the `hue` parameter to differentiate data points based on the `sex` column. 3. Use different colors for lunch and dinner meals (`time` column) using the `palette` parameter. 4. Split the hues into distinct strips using the `dodge` parameter. 5. Disable the jitter effect. 6. Adjust the marker size to 10, set the marker to diamond shape, and use a linewidth of 0.5. Set the transparency level to 0.7. 7. Provide a title for the plot: \\"Total Bill by Day and Gender\\". **Instructions**: 1. Import the necessary libraries. 2. Load the dataset. 3. Create the strip plot according to the specified requirements. 4. Ensure the plot is displayed with the customizations as outlined. **Constraints**: - Ensure that the code is efficient and uses appropriate Seaborn functions and parameters. - The plot should be clear and readable with the customizations applied. **Example Usage**: ```python import seaborn as sns import matplotlib.pyplot as plt # Set the theme for the plot sns.set_theme(style=\\"whitegrid\\") # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the strip plot with specified customizations sns.stripplot( data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", hue_order=[\\"Male\\", \\"Female\\"], palette={\\"Lunch\\": \\"blue\\", \\"Dinner\\": \\"green\\"}, dodge=True, jitter=False, size=10, marker=\\"D\\", linewidth=0.5, alpha=0.7 ) # Title for the plot plt.title(\\"Total Bill by Day and Gender\\") # Show the plot plt.show() ``` Your task is to complete the code above based on the provided requirements and constraints.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_stripplot(): # Set the theme for the plot sns.set_theme(style=\\"whitegrid\\") # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the strip plot with specified customizations sns.stripplot( data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", palette={\\"Male\\": \\"blue\\", \\"Female\\": \\"green\\"}, dodge=True, jitter=False, size=10, marker=\\"D\\", linewidth=0.5, alpha=0.7 ) # Title for the plot plt.title(\\"Total Bill by Day and Gender\\") # Show the plot plt.show() create_custom_stripplot()"},{"question":"**Question: Command-Line Utility for File Operations** **Objective:** You need to implement a Python script using the `argparse` module that serves as a command-line utility for basic file operations. The script should support reading, writing, and deleting files, while also providing verbosity control and help descriptions. Additionally, it should ensure that some operations are mutually exclusive. **Requirements:** 1. **Positional Arguments**: - `operation`: The operation to perform, which can be `\'read\'`, `\'write\'`, or `\'delete\'`. - `filename`: The name of the file on which to perform the operation. 2. **Optional Arguments**: - `-c`, `--content`: The content to write to the file. This argument is only applicable when the operation is `\'write\'`. - `-v`, `--verbose`: A flag that increases the output verbosity (use the `count` action to track verbosity levels). - `-q`, `--quiet`: A flag that suppresses all output. This is mutually exclusive with the verbosity flag. **Functional Behavior:** 1. **Read Operation**: - Reads and prints the content of the specified file. - If verbosity is turned on, print additional information such as the file size. 2. **Write Operation**: - Writes the provided content to the specified file. - If verbosity is turned on, print a success message and the size of the written content. 3. **Delete Operation**: - Deletes the specified file. - If verbosity is turned on, print a success message. **Constraints:** - Ensure that read, write, and delete operations are handled appropriately, including any necessary error handling (e.g., file not found). - Ensure mutually exclusive behavior between the verbosity and quiet flags. **Input and Output Formats:** - Use the command-line interface to provide the inputs. - The outputs should be visible on the command line based on the verbosity and operation performed. **Example Usage:** 1. Reading a file with verbosity: ```shell python file_util.py read example.txt -v Reading file \'example.txt\'... File size: 1024 bytes (file content) ``` 2. Writing to a file quietly: ```shell python file_util.py write example.txt -c \\"Hello, World!\\" -q ``` 3. Deleting a file with high verbosity: ```shell python file_util.py delete example.txt -vv File \'example.txt\' has been deleted successfully. ``` **Implementation:** Write your script below: ```python import argparse import os def main(): parser = argparse.ArgumentParser(description=\\"Utility for basic file operations\\") parser.add_argument(\\"operation\\", choices=[\\"read\\", \\"write\\", \\"delete\\"], help=\\"the operation to perform on the file\\") parser.add_argument(\\"filename\\", help=\\"the name of the file\\") parser.add_argument(\\"-c\\", \\"--content\\", help=\\"content to write to the file (only applicable for write operation)\\") parser.add_argument(\\"-v\\", \\"--verbose\\", action=\\"count\\", default=0, help=\\"increase output verbosity\\") parser.add_argument(\\"-q\\", \\"--quiet\\", action=\\"store_true\\", help=\\"suppress all output\\") args = parser.parse_args() if args.quiet and args.verbose: parser.error(\\"The arguments -q/--quiet and -v/--verbose cannot be used together\\") if args.operation == \\"read\\": try: with open(args.filename, \'r\') as file: content = file.read() if args.verbose >= 1 and not args.quiet: print(f\\"Reading file \'{args.filename}\'...\\") if args.verbose >= 2 and not args.quiet: print(f\\"File size: {len(content)} bytes\\") if not args.quiet: print(content) except FileNotFoundError: print(f\\"Error: File \'{args.filename}\' not found\\") elif args.operation == \\"write\\": if args.content is None: parser.error(\\"The -c/--content argument is required for write operation\\") with open(args.filename, \'w\') as file: file.write(args.content) if args.verbose >= 1 and not args.quiet: print(f\\"Writing to file \'{args.filename}\'...\\") if args.verbose >= 2 and not args.quiet: print(f\\"Size of written content: {len(args.content)} bytes\\") if args.verbose >= 1 and not args.quiet: print(\\"Write operation completed successfully\\") elif args.operation == \\"delete\\": try: os.remove(args.filename) if args.verbose >= 1 and not args.quiet: print(f\\"Deletion of file \'{args.filename}\' complete\\") except FileNotFoundError: print(f\\"Error: File \'{args.filename}\' not found\\") if __name__ == \\"__main__\\": main() ``` **Summary:** The provided python script demonstrates the use of the `argparse` module to create a command-line utility that can read, write, or delete files with optional verbosity and mutually exclusive quiet flag.","solution":"import argparse import os def main(): parser = argparse.ArgumentParser(description=\\"Utility for basic file operations\\") parser.add_argument(\\"operation\\", choices=[\\"read\\", \\"write\\", \\"delete\\"], help=\\"the operation to perform on the file\\") parser.add_argument(\\"filename\\", help=\\"the name of the file\\") parser.add_argument(\\"-c\\", \\"--content\\", help=\\"content to write to the file (only applicable for write operation)\\") parser.add_argument(\\"-v\\", \\"--verbose\\", action=\\"count\\", default=0, help=\\"increase output verbosity\\") parser.add_argument(\\"-q\\", \\"--quiet\\", action=\\"store_true\\", help=\\"suppress all output\\") args = parser.parse_args() if args.quiet and args.verbose: parser.error(\\"The arguments -q/--quiet and -v/--verbose cannot be used together\\") if args.operation == \\"read\\": try: with open(args.filename, \'r\') as file: content = file.read() if args.verbose >= 1 and not args.quiet: print(f\\"Reading file \'{args.filename}\'...\\") if args.verbose >= 2 and not args.quiet: print(f\\"File size: {len(content)} bytes\\") if not args.quiet: print(content) except FileNotFoundError: print(f\\"Error: File \'{args.filename}\' not found\\") elif args.operation == \\"write\\": if args.content is None: parser.error(\\"The -c/--content argument is required for write operation\\") with open(args.filename, \'w\') as file: file.write(args.content) if args.verbose >= 1 and not args.quiet: print(f\\"Writing to file \'{args.filename}\'...\\") if args.verbose >= 2 and not args.quiet: print(f\\"Size of written content: {len(args.content)} bytes\\") if args.verbose >= 1 and not args.quiet: print(\\"Write operation completed successfully\\") elif args.operation == \\"delete\\": try: os.remove(args.filename) if args.verbose >= 1 and not args.quiet: print(f\\"Deletion of file \'{args.filename}\' complete\\") except FileNotFoundError: print(f\\"Error: File \'{args.filename}\' not found\\") if __name__ == \\"__main__\\": main()"},{"question":"# Question: System Information Logger Your task is to write a Python script that uses the `platform` module to gather various pieces of information about the system it is run on and logs them in a human-readable format into a text file. Requirements: 1. **Function Definitions:** - Define a function `get_system_info()` that gathers the following system information using the `platform` module: - OS name using `platform.system()` - OS release using `platform.release()` - OS version using `platform.version()` - Machine type using `platform.machine()` - Processor name using `platform.processor()` - Python version using `platform.python_version()` - Python build using `platform.python_build()` - Format the gathered information into a string in a human-readable format. 2. **Logging Information:** - Define another function `log_system_info(log_file: str, system_info: str)` that takes a filename and system information string as arguments and writes the information to the specified log file. 3. **Main Program Execution:** - In the main part of your program, call the `get_system_info()` function to retrieve the system information and use the `log_system_info()` function to log it into a file named `\\"system_info.log\\"`. Expected Input and Output: - **Input:** This program does not require any input from the user. - **Output:** The output is the creation of a log file named `system_info.log` containing the system information in a readable format. Constraints and Notes: - Make sure the logged information is clearly formatted and easy to understand. - Consider handling any exceptions that might occur while retrieving system information or writing to the log file. # Example Output in `system_info.log`: ``` System Information: -------------------- OS Name: Linux OS Release: 5.11.0-37-generic OS Version: #41~20.04.2-Ubuntu SMP Fri Sep 24 09:06:38 UTC 2021 Machine Type: x86_64 Processor: x86_64 Python Version: 3.10.0 Python Build: (\'default\', \'Oct 4 2021 20:20:12\') ``` You may assume the functions from the `platform` module will return valid information when the script is run on typical systems.","solution":"import platform def get_system_info(): Gathers system information using the platform module. Returns: str: A formatted string containing the system information. info = { \\"OS Name\\": platform.system(), \\"OS Release\\": platform.release(), \\"OS Version\\": platform.version(), \\"Machine Type\\": platform.machine(), \\"Processor\\": platform.processor(), \\"Python Version\\": platform.python_version(), \\"Python Build\\": platform.python_build(), } formatted_info = \\"System Information:n--------------------n\\" formatted_info += \\"n\\".join([f\\"{key}: {value}\\" for key, value in info.items()]) return formatted_info def log_system_info(log_file: str, system_info: str): Logs system information to a specified file. Args: log_file (str): The name of the log file. system_info (str): The system information to log. try: with open(log_file, \'w\') as file: file.write(system_info) except Exception as e: print(f\\"An error occurred while writing to the log file: {e}\\") if __name__ == \\"__main__\\": system_info = get_system_info() log_system_info(\\"system_info.log\\", system_info)"},{"question":"Objective You are required to implement Python functions that utilize the C API for marshalling Python objects. Your task is to create functions that will serialize (marshal) and deserialize (unmarshal) various Python objects to and from files and strings. # Functions to Implement 1. **write_long_to_file(value: int, file_path: str, version: int)** - Write a 32-bit integer `value` to a file specified by `file_path` using the marshalling version specified by `version`. - If an error occurs, the function should raise an appropriate Python exception with a descriptive message. 2. **write_object_to_file(py_object: Any, file_path: str, version: int)** - Marshal a Python object `py_object` to a file specified by `file_path` using the marshalling version specified by `version`. - If an error occurs, the function should raise an appropriate Python exception with a descriptive message. 3. **write_object_to_string(py_object: Any, version: int) -> bytes** - Serialize a Python object `py_object` to a byte string using the marshalling version specified by `version`. - Return the byte string. - If an error occurs, the function should raise an appropriate Python exception with a descriptive message. 4. **read_long_from_file(file_path: str) -> int** - Read a 32-bit integer from a file specified by `file_path`. - Return the integer. - If an error occurs, the function should raise an appropriate Python exception with a descriptive message. 5. **read_short_from_file(file_path: str) -> int** - Read a 16-bit integer from a file specified by `file_path`. - Return the integer. - If an error occurs, the function should raise an appropriate Python exception with a descriptive message. 6. **read_object_from_file(file_path: str) -> Any** - Deserialize a Python object from a file specified by `file_path`. - Return the Python object. - If an error occurs, the function should raise an appropriate Python exception with a descriptive message. 7. **read_object_from_string(data: bytes) -> Any** - Deserialize a Python object from a byte string `data`. - Return the Python object. - If an error occurs, the function should raise an appropriate Python exception with a descriptive message. # Constraints - The functions should be implemented using Python\'s C API for marshalling (as outlined in the provided documentation). - Ensure proper error handling and resource management (e.g., file closing). - Use `version=2` for marshalling unless otherwise specified. # Input and Output Formats - Functions that read from or write to files will take or return the file path as a string. - Functions that serialize or deserialize from or to strings will handle byte strings. - Functions involving marshalling will work with various Python data types (integers, strings, lists, dictionaries). # Example Usage ```python # Writing and reading an integer to/from a file write_long_to_file(12345, \'example.bin\', 2) value = read_long_from_file(\'example.bin\') print(value) # Output: 12345 # Writing and reading a Python object to/from a file obj = {\'name\': \'Alice\', \'age\': 30} write_object_to_file(obj, \'object.bin\', 2) restored_obj = read_object_from_file(\'object.bin\') print(restored_obj) # Output: {\'name\': \'Alice\', \'age\': 30} # Writing and reading a Python object to/from a string byte_str = write_object_to_string(obj, 2) restored_obj_from_str = read_object_from_string(byte_str) print(restored_obj_from_str) # Output: {\'name\': \'Alice\', \'age\': 30} ``` # Evaluation Criteria 1. Correctness: The functions should correctly serialize and deserialize data. 2. Error Handling: The functions should handle errors gracefully and provide meaningful error messages. 3. Performance: The implementation should be efficient in terms of both time and space complexity. 4. Code Quality: The code should be well-organized, readable, and follow best practices for resource management.","solution":"import marshal def write_long_to_file(value: int, file_path: str, version: int): Write a 32-bit integer value to a file specified by file_path using the marshalling version specified by version. If an error occurs, raises an appropriate Python exception with a descriptive message. try: with open(file_path, \'wb\') as file: marshal.dump(value, file, version) except Exception as e: raise RuntimeError(f\\"Failed to write long to file: {e}\\") def write_object_to_file(py_object: any, file_path: str, version: int): Marshal a Python object to a file specified by file_path using the marshalling version specified by version. If an error occurs, raises an appropriate Python exception with a descriptive message. try: with open(file_path, \'wb\') as file: marshal.dump(py_object, file, version) except Exception as e: raise RuntimeError(f\\"Failed to write object to file: {e}\\") def write_object_to_string(py_object: any, version: int) -> bytes: Serialize a Python object to a byte string using the marshalling version specified by version. If an error occurs, raises an appropriate Python exception with a descriptive message. try: return marshal.dumps(py_object, version) except Exception as e: raise RuntimeError(f\\"Failed to write object to string: {e}\\") def read_long_from_file(file_path: str) -> int: Read a 32-bit integer from a file specified by file_path. If an error occurs, raises an appropriate Python exception with a descriptive message. try: with open(file_path, \'rb\') as file: return marshal.load(file) except Exception as e: raise RuntimeError(f\\"Failed to read long from file: {e}\\") def read_short_from_file(file_path: str) -> int: Read a 16-bit integer from a file specified by file_path. If an error occurs, raises an appropriate Python exception with a descriptive message. try: with open(file_path, \'rb\') as file: value = marshal.load(file) return value & 0xFFFF # short integer extraction except Exception as e: raise RuntimeError(f\\"Failed to read short from file: {e}\\") def read_object_from_file(file_path: str) -> any: Deserialize a Python object from a file specified by file_path. If an error occurs, raises an appropriate Python exception with a descriptive message. try: with open(file_path, \'rb\') as file: return marshal.load(file) except Exception as e: raise RuntimeError(f\\"Failed to read object from file: {e}\\") def read_object_from_string(data: bytes) -> any: Deserialize a Python object from a byte string data. If an error occurs, raises an appropriate Python exception with a descriptive message. try: return marshal.loads(data) except Exception as e: raise RuntimeError(f\\"Failed to read object from string: {e}\\")"},{"question":"# PyTorch Optimization Check You are required to implement a function in PyTorch that checks if the given conditions for selecting a persistent algorithm and improving performance are satisfied. To do this, you will verify the conditions provided in the documentation. Function Signature ```python def is_persistent_algorithm_feasible(tensor: torch.Tensor, device: torch.device) -> bool: ``` Input * `tensor`: A PyTorch tensor that needs to be verified. * `device`: A PyTorch device indicating the target computational device (e.g., `torch.device(\'cuda:0\')` for GPU). Output * Returns a boolean value `True` if the conditions for selecting a persistent algorithm are satisfied; otherwise, returns `False`. Conditions to Check 1. CuDNN is enabled. 2. The input tensor is on the GPU. 3. The input tensor has dtype `torch.float16`. 4. An NVIDIA V100 GPU is being used. 5. The input tensor is not in `PackedSequence` format. Example Usage ```python import torch # Check with a sample tensor tensor = torch.rand((10, 10), dtype=torch.float16).to(\'cuda:0\') device = torch.device(\'cuda:0\') # Assuming we are checking this on a V100 GPU print(is_persistent_algorithm_feasible(tensor, device)) # Expected output: True or False depending on conditions ``` **Note**: The function should rely on PyTorch\'s built-in methods and properties to assess whether these conditions are satisfied.","solution":"import torch def is_persistent_algorithm_feasible(tensor: torch.Tensor, device: torch.device) -> bool: Checks if the conditions for selecting a persistent algorithm are satisfied: 1. CuDNN is enabled. 2. The input tensor is on the GPU. 3. The input tensor has dtype `torch.float16`. 4. An NVIDIA V100 GPU is being used. 5. The input tensor is not in PackedSequence format. Parameters: tensor (torch.Tensor): The tensor to check. device (torch.device): The device to check. Returns: bool: True if all conditions are met, otherwise False. if not torch.backends.cudnn.enabled: return False if not tensor.is_cuda: return False if tensor.dtype != torch.float16: return False gpu_name = torch.cuda.get_device_name(device) if \'V100\' not in gpu_name: return False if isinstance(tensor, torch.nn.utils.rnn.PackedSequence): return False return True"},{"question":"# Advanced Python Debugging Challenge Objective: Your task is to implement a Python utility that helps in profiling the execution time and memory usage of a given Python function. You will use the Python `timeit` and `tracemalloc` modules to accomplish this. Function Specification: 1. **Function Name**: `profile_function` 2. **Parameters**: - `func`: A reference to the Python function to be profiled. - `args`: A list or tuple of arguments to be passed to the function. - `kwargs`: A dictionary of keyword arguments to be passed to the function. 3. **Returns**: - A dictionary with the following keys and their corresponding values: - `\'execution_time\'`: A float representing the average time in seconds taken to execute the function across multiple runs. - `\'current_memory\'`: An integer representing the current memory usage in bytes after the function execution. - `\'peak_memory\'`: An integer representing the peak memory usage in bytes during the function execution. 4. **Constraints**: - The function should run the given function at least 1000 times to get an accurate measurement. - Ensure that the implementation is efficient in terms of additional overhead introduced during profiling. Performance Requirements: The function should be optimized to minimize any performance overhead from profiling itself. Profiling should be accurate within a reasonable margin of error. Example Usage: ```python import math def sample_function(x): return [math.factorial(i) for i in range(x)] result = profile_function(sample_function, (10,), {}) print(result) # Example output: # { # \'execution_time\': 0.00045, # \'current_memory\': 4000, # \'peak_memory\': 4500 # } ``` You are expected to use the `timeit` module to measure execution time and the `tracemalloc` module to measure memory usage. Ensure your function is well-documented and handles edge cases gracefully. Additional Requirements: - Provide detailed docstrings for your function. - Write clean, readable, and efficient code.","solution":"import timeit import tracemalloc def profile_function(func, args=(), kwargs={}): Profiles the execution time and memory usage of a given function. Parameters: func (callable): The function to be profiled. args (tuple): The positional arguments to pass to the function. kwargs (dict): The keyword arguments to pass to the function. Returns: dict: A dictionary containing the average execution time, current memory usage, and peak memory usage of the function execution. # Measure execution time num_runs = 1000 time_taken = timeit.timeit(lambda: func(*args, **kwargs), number=num_runs) / num_runs # Measure memory usage tracemalloc.start() func(*args, **kwargs) current, peak = tracemalloc.get_traced_memory() tracemalloc.stop() return { \'execution_time\': time_taken, \'current_memory\': current, \'peak_memory\': peak }"},{"question":"# Python Data Persistence Assessment Objective: Implement a Python program that demonstrates the use of `pickle` for serializing and deserializing Python objects and the usage of an `sqlite3` database for persisting structured data. Problem Statement: You are tasked with creating a library management system. The system should be able to: 1. Serialize and deserialize book information using `pickle`. 2. Store and retrieve book records from an SQLite database. Function Specifications: 1. **serialize_book_records(book_list: List[Dict[str, Union[str, int, float]]]) -> bytes:** - **Input:** A list of dictionaries, where each dictionary represents a book with attributes such as `title` (str), `author` (str), `year` (int), and `price` (float). - **Output:** A byte string representing the serialized form of the list using `pickle`. - **Constraints:** None. 2. **deserialize_book_records(serialized_books: bytes) -> List[Dict[str, Union[str, int, float]]]:** - **Input:** A byte string containing the serialized book information. - **Output:** A list of dictionaries with the deserialized book information. - **Constraints:** Ensure the function handles potential exceptions during deserialization gracefully. 3. **store_books_in_db(db_path: str, book_list: List[Dict[str, Union[str, int, float]]]) -> None:** - **Input:** A path to an SQLite database file and a list of dictionaries representing books. - **Output:** None. - **Functionality:** Create a table called `books` with columns `title`, `author`, `year`, and `price`. Insert the book records into this table. If the table exists, it should be dropped and recreated. - **Constraints:** Use appropriate data types for the table columns. Handle exceptions related to database operations. 4. **retrieve_books_from_db(db_path: str) -> List[Dict[str, Union[str, int, float]]]:** - **Input:** A path to an SQLite database file. - **Output:** A list of dictionaries representing the book records retrieved from the `books` table. - **Constraints:** Handle exceptions related to database operations gracefully. Performance Requirements: - Ensure the database operations handle up to 1000 book records efficiently. - Serialization and deserialization should handle the same volume of data seamlessly. Evaluation Criteria: - Correct implementation of serialization and deserialization using `pickle`. - Proper database management with SQLite, including table creation, insertion, and retrieval. - Graceful handling of exceptions and potential errors. - Efficiency and clarity of the written code. Example Usage: ```python books = [ {\\"title\\": \\"Book A\\", \\"author\\": \\"Author A\\", \\"year\\": 2020, \\"price\\": 29.99}, {\\"title\\": \\"Book B\\", \\"author\\": \\"Author B\\", \\"year\\": 2019, \\"price\\": 19.99} ] # Serializing and deserializing book records serialized_books = serialize_book_records(books) print(deserialize_book_records(serialized_books)) # Storing and retrieving books from SQLite database store_books_in_db(\'books.db\', books) retrieved_books = retrieve_books_from_db(\'books.db\') print(retrieved_books) ```","solution":"import pickle import sqlite3 from typing import List, Dict, Union def serialize_book_records(book_list: List[Dict[str, Union[str, int, float]]]) -> bytes: Serializes a list of book records using pickle. return pickle.dumps(book_list) def deserialize_book_records(serialized_books: bytes) -> List[Dict[str, Union[str, int, float]]]: Deserializes a byte string containing serialized book records using pickle. try: return pickle.loads(serialized_books) except Exception as e: print(f\\"Deserialization error: {e}\\") return [] def store_books_in_db(db_path: str, book_list: List[Dict[str, Union[str, int, float]]]) -> None: Stores book records into an SQLite database. try: conn = sqlite3.connect(db_path) cursor = conn.cursor() cursor.execute(\\"DROP TABLE IF EXISTS books\\") cursor.execute( CREATE TABLE books ( title TEXT, author TEXT, year INTEGER, price REAL ) ) for book in book_list: cursor.execute(\\"INSERT INTO books (title, author, year, price) VALUES (?, ?, ?, ?)\\", (book[\'title\'], book[\'author\'], book[\'year\'], book[\'price\'])) conn.commit() except sqlite3.Error as e: print(f\\"Database error: {e}\\") finally: conn.close() def retrieve_books_from_db(db_path: str) -> List[Dict[str, Union[str, int, float]]]: Retrieves book records from an SQLite database. try: conn = sqlite3.connect(db_path) cursor = conn.cursor() cursor.execute(\\"SELECT title, author, year, price FROM books\\") rows = cursor.fetchall() books = [ {\\"title\\": row[0], \\"author\\": row[1], \\"year\\": row[2], \\"price\\": row[3]} for row in rows ] return books except sqlite3.Error as e: print(f\\"Database error: {e}\\") return [] finally: conn.close()"},{"question":"You are required to implement a function that will: 1. Read a file, compress its contents, and write the compressed data to a new file. 2. Read the compressed file, decompress its contents, and verify that it matches the original data. Your task is to implement a program that demonstrates the usage of zlib for this process. # Function Signature ```python def compress_and_verify(input_file_path: str, compressed_file_path: str, output_file_path: str) -> bool: pass ``` # Input - `input_file_path (str)`: The path to the input file containing the original data. - `compressed_file_path (str)`: The path where the compressed data should be written. - `output_file_path (str)`: The path where the decompressed data should be written. # Output - `bool`: Returns `True` if the decompressed data matches the original data, otherwise `False`. # Constraints - The file size should be manageable within memory constraints, but feel free to handle larger sizes. - Handle possible exceptions such as file not found, read/write errors, and compression/decompression errors. # Example ```python original_data = \\"Hello, this is a test string for compression.\\" with open(\\"input.txt\\", \\"w\\") as f: f.write(original_data) success = compress_and_verify(\\"input.txt\\", \\"compressed.gz\\", \\"output.txt\\") print(success) # Should print: True ``` # Notes - Use `zlib.compress` and `zlib.decompress` for the compression and decompression tasks, respectively. - Ensure to handle file operations safely (e.g., use `with` statement for file handling). - Use checksums (like CRC32) to ensure data integrity during verification.","solution":"import zlib def compress_and_verify(input_file_path: str, compressed_file_path: str, output_file_path: str) -> bool: try: # Read the original file with open(input_file_path, \'rb\') as f: original_data = f.read() # Compress the data compressed_data = zlib.compress(original_data) # Write the compressed data to the file with open(compressed_file_path, \'wb\') as f: f.write(compressed_data) # Read the compressed data from the file with open(compressed_file_path, \'rb\') as f: read_compressed_data = f.read() # Decompress the data decompressed_data = zlib.decompress(read_compressed_data) # Write the decompressed data to the output file with open(output_file_path, \'wb\') as f: f.write(decompressed_data) # Verify the decompressed data matches the original data return original_data == decompressed_data except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"**Title**: Efficient Top-N Frequent Words Finder **Objective**: Implement a function that efficiently finds the top N most frequent words in a given text using the `heapq` module. **Problem Statement**: You are given a string `text` which contains a list of words separated by spaces. Your task is to implement a function `top_n_frequent_words(text: str, n: int) -> List[str]` that returns a list of the `n` most frequent words in the text. If there are ties (words with the same frequency), they should be sorted alphabetically. **Function Signature**: ```python from typing import List import heapq def top_n_frequent_words(text: str, n: int) -> List[str]: # Your implementation here ``` **Input**: - `text` (str): A string containing words separated by spaces. Words are case-insensitive, meaning \\"Word\\" and \\"word\\" should be considered the same. - `n` (int): The number of top frequent words to return. **Output**: - List of `n` most frequent words sorted by their frequency in descending order. If frequencies are the same, sort them alphabetically. **Constraints**: - The input string `text` will contain only alphabetical characters and spaces. - The length of the `text` will be at most `10^4` characters. - The value of `n` will be between `1` and the number of unique words in `text`. **Example**: ```python text = \\"Word word other Other words WoRd word\\" n = 2 print(top_n_frequent_words(text, n)) # Output: [\'word\', \'other\'] ``` **Explanation**: In the example above, the word \\"word\\" appears 4 times and \\"other\\" appears 2 times. The function returns the top 2 most frequent words sorted by frequency. **Tips**: 1. Use the `heapq` module effectively to maintain a heap of size `n`. 2. Ensure that the word count is case-insensitive. 3. Utilize dictionaries to count word frequencies before pushing them into a heap. **Performance Requirements**: - The implementation should efficiently handle the constraints provided, making use of `heapq`\'s capabilities to maintain the top `n` elements. **Notes**: - Consider edge cases such as input strings with same frequency words. - Ensure the implementation is clear and concise, following Python best practices.","solution":"from typing import List import heapq from collections import Counter def top_n_frequent_words(text: str, n: int) -> List[str]: # Normalize the text to lowercase words = text.lower().split() # Count the frequency of each word word_count = Counter(words) # Use a heap to find the top n frequent words heap = [] for word, freq in word_count.items(): heapq.heappush(heap, (-freq, word)) # Using negative frequency for max-heap simulation # Extract the top n elements from the heap top_n_words = [heapq.heappop(heap)[1] for _ in range(n)] return top_n_words"},{"question":"Coding Assessment Question # Objective The purpose of this assessment is to test your understanding of the `time` module in Python, including conversion between time representations, performance measurement, and handling time zones. # Problem Statement You are tasked with implementing a function that calculates the duration of a specific event. The function should take two timestamps in UTC timezone format and return the duration in hours, minutes, and seconds. Additionally, it should also return the local time (adjusted for your system\'s timezone) of both the start and end timestamps and the name of the local timezone. # Function Signature ```python def calculate_event_duration(start_timestamp: int, end_timestamp: int) -> dict: Calculate the duration of an event based on the given UTC timestamps, and convert start and end timestamps to local time. Parameters: start_timestamp (int): The start time in seconds since the epoch (UTC). end_timestamp (int): The end time in seconds since the epoch (UTC). Returns: dict: A dictionary containing the following key-value pairs: - \'duration\': a dict with keys \'hours\', \'minutes\', and \'seconds\' representing the duration of the event. - \'start_local_time\': a string representing the start time in local time format. - \'end_local_time\': a string representing the end time in local time format. - \'local_timezone\': a string representing the name of the local timezone. ``` # Input 1. `start_timestamp` (int): An integer representing the start time in seconds since the epoch (UTC). 2. `end_timestamp` (int): An integer representing the end time in seconds since the epoch (UTC). # Output The function should return a dictionary with the following keys: - `duration`: A dictionary with keys `hours`, `minutes`, and `seconds`, representing the breakdown of the duration between the `start_timestamp` and `end_timestamp`. - `start_local_time`: A string representing the start time in local time format, e.g., `\\"Mon Sep 27 08:42:11 2021\\"`. - `end_local_time`: A string representing the end time in local time format, e.g., `\\"Mon Sep 27 09:42:11 2021\\"`. - `local_timezone`: The name of the local timezone, e.g., `\\"EST\\"`, `\\"PST\\"`, etc. # Constraints 1. The `start_timestamp` should be less than or equal to the `end_timestamp`. 2. Both timestamps are valid Unix timestamps. # Examples ```python # Example 1: start_timestamp = 1632736800 # Equivalent to Mon Sep 27 02:00:00 2021 UTC end_timestamp = 1632740400 # Equivalent to Mon Sep 27 03:00:00 2021 UTC result = calculate_event_duration(start_timestamp, end_timestamp) # Expected output: # { # \'duration\': {\'hours\': 1, \'minutes\': 0, \'seconds\': 0}, # \'start_local_time\': \'Your Local Time Representation of Start Time\', # \'end_local_time\': \'Your Local Time Representation of End Time\', # \'local_timezone\': \'Your Local Timezone Name\' # } # Example 2: start_timestamp = 1632699600 # Equivalent to Mon Sep 27 12:00:00 2021 UTC end_timestamp = 1632703200 # Equivalent to Mon Sep 27 13:00:00 2021 UTC result = calculate_event_duration(start_timestamp, end_timestamp) # Expected output: # { # \'duration\': {\'hours\': 1, \'minutes\': 0, \'seconds\': 0}, # \'start_local_time\': \'Your Local Time Representation of Start Time\', # \'end_local_time\': \'Your Local Time Representation of End Time\', # \'local_timezone\': \'Your Local Timezone Name\' # } ``` # Notes - Utilize the functions from the `time` module such as `gmtime()`, `localtime()`, `strftime()`, and `asctime()` as needed. - Make sure to handle the conversion between UTC and local time correctly. - Pay attention to error handling and edge cases, for example when the start time is equal to the end time.","solution":"import time def calculate_event_duration(start_timestamp: int, end_timestamp: int) -> dict: Calculate the duration of an event based on the given UTC timestamps, and convert start and end timestamps to local time. Parameters: start_timestamp (int): The start time in seconds since the epoch (UTC). end_timestamp (int): The end time in seconds since the epoch (UTC). Returns: dict: A dictionary containing the following key-value pairs: - \'duration\': a dict with keys \'hours\', \'minutes\', and \'seconds\' representing the duration of the event. - \'start_local_time\': a string representing the start time in local time format. - \'end_local_time\': a string representing the end time in local time format. - \'local_timezone\': a string representing the name of the local timezone. # Calculate duration total_seconds = end_timestamp - start_timestamp hours = total_seconds // 3600 minutes = (total_seconds % 3600) // 60 seconds = total_seconds % 60 duration = { \'hours\': hours, \'minutes\': minutes, \'seconds\': seconds } # Convert to local time start_local_time_struct = time.localtime(start_timestamp) end_local_time_struct = time.localtime(end_timestamp) start_local_time = time.asctime(start_local_time_struct) end_local_time = time.asctime(end_local_time_struct) # Get local timezone name local_timezone = time.tzname[time.daylight] return { \'duration\': duration, \'start_local_time\': start_local_time, \'end_local_time\': end_local_time, \'local_timezone\': local_timezone }"},{"question":"# **PyTorch Mask Manipulation Assessment** Objective Your task is to use the provided `torch.nn.attention.flex_attention` package to manipulate attention masks. This will assess your understanding of how these masks can be created and combined for use in attention mechanisms within neural networks. Problem Statement You are given a sequence of tokens where each token represents a part of an input text. You need to implement a function that returns a composite mask for these tokens that satisfies specific conditions. In particular, you need to: 1. Create an initial block mask. 2. Create another mask. 3. Combine these masks using logical AND and logical OR operations. 4. Return the final combined mask. Function Signature ```python import torch import torch.nn.attention.flex_attention as fa def create_composite_mask(tokens: List[int], block_size: int, nested_block_size: int) -> torch.Tensor: Create a composite mask for the given sequence of tokens. Parameters: - tokens: List[int] : A list of token IDs. - block_size: int : Size of the block mask. - nested_block_size: int : Size of the nested block mask. Returns: - torch.Tensor : A composite mask created by AND and OR operations of generated masks. pass ``` Instructions 1. **Block Mask Creation**: Use `fa.create_block_mask` to create a block mask for the given tokens with the `block_size`. 2. **Nested Block Mask Creation**: Use `fa.create_nested_block_mask` to create a nested block mask for the given tokens with the `nested_block_size`. 3. **Create Additional Mask**: Use `fa.create_mask` to create another mask based on the provided tokens. 4. **Combine Masks**: Use `fa.and_masks` and `fa.or_masks` to logically combine the masks: first apply AND between the block mask and the nested block mask, then apply OR with the additional mask. 5. **Return the Final Mask**: The final mask to be returned should be the result of the combined operations. Constraints - Length of tokens (n) is at most 512. - `block_size` and `nested_block_size` are positive integers and less than or equal to `n`. Example Given: ```python tokens = [1, 2, 3, 4, 5] block_size = 2 nested_block_size = 1 ``` Steps: 1. Create a block mask of size 2. 2. Create a nested block mask of size 1. 3. Create an additional mask based on the tokens. 4. Combine the masks using AND and OR operations as specified. The final composite mask should be returned as a torch.Tensor object. Performance Requirements The implementation should efficiently handle the combination of masks with a linear time complexity with respect to the number of tokens.","solution":"import torch import torch.nn.attention.flex_attention as fa from typing import List def create_composite_mask(tokens: List[int], block_size: int, nested_block_size: int) -> torch.Tensor: Create a composite mask for the given sequence of tokens. Parameters: - tokens: List[int] : A list of token IDs. - block_size: int : Size of the block mask. - nested_block_size: int : Size of the nested block mask. Returns: - torch.Tensor : A composite mask created by AND and OR operations of generated masks. # Create block mask block_mask = fa.create_block_mask(tokens, block_size) # Create nested block mask nested_block_mask = fa.create_nested_block_mask(tokens, nested_block_size) # Create additional mask additional_mask = fa.create_mask(tokens) # Combine masks using AND combined_mask = fa.and_masks(block_mask, nested_block_mask) # Combine the result with additional mask using OR final_mask = fa.or_masks(combined_mask, additional_mask) return final_mask"},{"question":"Objective Create a Python class that mimics some of the behavior of certain Python C API functions as described in the provided documentation. Description Implement the `CustomPyObject` class with methods that correspond to some of the core functions in the Object Protocol of the python310 module. Requirements 1. The `CustomPyObject` class should have an initializer that stores an object\'s attributes in a dictionary. 2. Implement the following methods within the `CustomPyObject` class: - `has_attr(self, attr_name: str) -> bool`: Mimics `PyObject_HasAttr` or `PyObject_HasAttrString`. - `get_attr(self, attr_name: str) -> any`: Mimics `PyObject_GetAttr` or `PyObject_GetAttrString`, raises `AttributeError` if the attribute does not exist. - `set_attr(self, attr_name: str, value: any) -> None`: Mimics `PyObject_SetAttr` or `PyObject_SetAttrString`. - `del_attr(self, attr_name: str) -> None`: Mimics `PyObject_DelAttr` or `PyObject_DelAttrString`, raises `AttributeError` if the attribute does not exist. - `repr(self) -> str`: Mimics `PyObject_Repr`. - `str_representation(self) -> str`: Mimics `PyObject_Str`. Constraints - Do not use the built-in functions `getattr`, `setattr`, or `delattr` directly on the object dictionary. Instead, manipulate the dictionary directly. - Ensure that `get_attr` and `del_attr` appropriately handle attribute errors by raising an `AttributeError`. - The `repr` method should return a string in the form of \\"CustomPyObject({attributes})\\", where `{attributes}` is the dictionary of attributes. - The `str_representation` method should return a string in the form of the dictionary representation of attributes. Example ```python class CustomPyObject: def __init__(self, **kwargs): pass def has_attr(self, attr_name: str) -> bool: pass def get_attr(self, attr_name: str) -> any: pass def set_attr(self, attr_name: str, value: any) -> None: pass def del_attr(self, attr_name: str) -> None: pass def repr(self) -> str: pass def str_representation(self) -> str: pass # Example usage: obj = CustomPyObject(x=10, y=20) print(obj.has_attr(\'x\')) # True print(obj.get_attr(\'x\')) # 10 obj.set_attr(\'z\', 30) print(obj.repr()) # CustomPyObject({\'x\': 10, \'y\': 20, \'z\': 30}) print(obj.str_representation()) # {\'x\': 10, \'y\': 20, \'z\': 30} obj.del_attr(\'y\') print(obj.get_attr(\'y\')) # Raises AttributeError ```","solution":"class CustomPyObject: def __init__(self, **kwargs): self._attributes = kwargs def has_attr(self, attr_name: str) -> bool: return attr_name in self._attributes def get_attr(self, attr_name: str) -> any: if attr_name not in self._attributes: raise AttributeError(f\\"\'{attr_name}\' attribute not found\\") return self._attributes[attr_name] def set_attr(self, attr_name: str, value: any) -> None: self._attributes[attr_name] = value def del_attr(self, attr_name: str) -> None: if attr_name not in self._attributes: raise AttributeError(f\\"\'{attr_name}\' attribute not found\\") del self._attributes[attr_name] def repr(self) -> str: return f\\"CustomPyObject({self._attributes})\\" def str_representation(self) -> str: return str(self._attributes) # Example usage: obj = CustomPyObject(x=10, y=20) print(obj.has_attr(\'x\')) # True print(obj.get_attr(\'x\')) # 10 obj.set_attr(\'z\', 30) print(obj.repr()) # CustomPyObject({\'x\': 10, \'y\': 20, \'z\': 30}) print(obj.str_representation()) # {\'x\': 10, \'y\': 20, \'z\': 30} obj.del_attr(\'y\') # print(obj.get_attr(\'y\')) # Should raise AttributeError"},{"question":"**Coding Assessment Question:** # Objective: Demonstrate your understanding of Seaborn\'s `violinplot` function by creating a customized violin plot that incorporates both fundamental and advanced concepts of this function. # Question: Given a dataset `data` with the following structure: - `data` is a pandas DataFrame that includes columns: `category`, `value`, and `group`. - `category` contains categorical data. - `value` contains numerical data. - `group` contains categorical data with two unique values: \\"A\\" and \\"B\\". Write a function `create_custom_violinplot` that performs the following tasks: 1. Creates a violin plot with: - `category` on the x-axis. - `value` on the y-axis. 2. Uses the `group` column as a hue to create split violins. 3. Draws the violins with a line-art representation (set `fill` to `False`). 4. Uses `density_norm=\'count\'` to normalize the width of each violin based on the number of observations. 5. Avoids smoothing beyond the extremes by setting `cut=0`. 6. Adjusts the bandwidth of the KDE by setting `bw_adjust` to 0.75. 7. Sets the inner representation to sticks (`inner=\'stick\'`). 8. Uses a custom color for the violins\' line-art. # Constraints: - Your function should take the DataFrame `data` and a custom color string for the violin line-art as input. - Your function should return a `matplotlib` Axes object containing the customized violin plot. # Input: - `data` : pandas DataFrame with columns `category`, `value`, `group`. - `color` : string representing the custom color for the violin line-art (e.g., \\"blue\\"). # Output: - A `matplotlib` Axes object containing the customized violin plot. # Example: ```python def create_custom_violinplot(data, color): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme(style=\\"whitegrid\\") ax = sns.violinplot( data=data, x=\\"category\\", y=\\"value\\", hue=\\"group\\", split=True, fill=False, density_norm=\\"count\\", cut=0, bw_adjust=0.75, inner=\\"stick\\", linewidth=1, linecolor=color ) return ax # Example usage: import pandas as pd data = pd.DataFrame({ \\"category\\": [\\"A\\", \\"A\\", \\"B\\", \\"B\\"], \\"value\\": [1, 2, 3, 4], \\"group\\": [\\"A\\", \\"B\\", \\"A\\", \\"B\\"] }) ax = create_custom_violinplot(data, \\"blue\\") plt.show() ``` # Notes: - Ensure you have the `seaborn` and `matplotlib` libraries installed. - The function should work for any valid pandas DataFrame that matches the specified structure.","solution":"def create_custom_violinplot(data, color): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme(style=\\"whitegrid\\") ax = sns.violinplot( data=data, x=\\"category\\", y=\\"value\\", hue=\\"group\\", split=True, fill=False, scale=\\"count\\", cut=0, bw=0.75, inner=\\"stick\\", linewidth=1, palette={group: color for group in data[\'group\'].unique()} ) return ax"},{"question":"<|Analysis Begin|> The provided documentation focuses on the options and settings of pandas, illustrating how to get, set, reset, and contextualize pandas options. It particularly covers options for DataFrame display, number formatting, and Unicode formatting. The main functionalities highlighted include: 1. Getting and setting options (`pd.get_option`, `pd.set_option`). 2. Resetting options to default values (`pd.reset_option`). 3. Describing options (`pd.describe_option`). 4. Contextualizing option settings within a block of code (`pd.option_context`). These functionalities are useful for customizing the display and behavior of pandas DataFrames according to specific requirements. The documentation provides various examples of how these functions can be used effectively. Given this focus, a suitable coding assessment question would involve creating a function that makes extensive use of pandas options to configure DataFrame displays under different scenarios. This function should demonstrate the student\'s understanding of how to manipulate pandas behavior at a global and local level. <|Analysis End|> <|Question Begin|> # Coding Assessment Question: Customizing DataFrame Display Options You are required to implement a function that customizes pandas DataFrame display options based on different user specifications. This function will involve getting, setting, and resetting multiple pandas options to meet specific requirements. Function Signature: ```python def configure_dataframe_display(configurations: dict, df: pd.DataFrame, code_block) -> pd.DataFrame: Configures pandas DataFrame display options and executes a code block within a specific context. Parameters: - configurations: A dictionary where keys are pandas option names (e.g., \'display.max_rows\') and values are the respective settings to be applied. - df: The DataFrame to apply the display settings to. - code_block: A function to be executed with the configured display settings. It should take a DataFrame as its only parameter and return a DataFrame. Returns: - The DataFrame returned by the code_block function. Requirements: 1. The function should first ensure that all current pandas options are stored. 2. It should then set each option in the `configurations` dictionary. 3. Next, it executes the `code_block` function with the given DataFrame as its argument. 4. After executing `code_block`, it should reset all options to their previous values. 5. Finally, it should return the DataFrame that results from executing the `code_block` function. Example: ```python import pandas as pd import numpy as np # Sample DataFrame df = pd.DataFrame(np.random.randn(10, 5), columns=list(\'ABCDE\')) # Function to be executed within the context def sample_code_block(df): print(df) return df.describe() # Configuration dictionary configurations = { \'display.max_rows\': 8, \'display.precision\': 3 } # Testing the function configure_dataframe_display(configurations, df, sample_code_block) ``` Constraints: 1. No external libraries other than pandas and numpy should be used. 2. The function should handle resetting options even if an error occurs during the code block execution. **Note:** This question assesses the understanding of pandas options API and the ability to manage configurations and local contexts in pandas.","solution":"import pandas as pd def configure_dataframe_display(configurations, df, code_block): Configures pandas DataFrame display options and executes a code block within a specific context. Parameters: - configurations: A dictionary where keys are pandas option names (e.g., \'display.max_rows\') and values are the respective settings to be applied. - df: The DataFrame to apply the display settings to. - code_block: A function to be executed with the configured display settings. It should take a DataFrame as its only parameter and return a DataFrame. Returns: - The DataFrame returned by the code_block function. old_options = {} try: # Store current options for option, value in configurations.items(): old_options[option] = pd.get_option(option) # Set new options from configurations for option, value in configurations.items(): pd.set_option(option, value) # Execute the code block within the new context result = code_block(df) finally: # Reset the options back to their original values for option, value in old_options.items(): pd.set_option(option, value) # Return the DataFrame after executing the code block return result"},{"question":"Objective: Implement a weak reference-based cache system for managing large data objects, ensuring they can be garbage collected when no strong references exist. Demonstrate your understanding of weak references, callbacks, and garbage collection. Task: 1. **Cache Class (`WeakCache`)**: - Implement a class `WeakCache` that uses a `weakref.WeakValueDictionary` to store cache entries. - The cache should allow objects to be stored and retrieved by unique string keys. - Implement a method `put(key: str, value: Any)` to store a value in the cache. - Implement a method `get(key: str) -> Any` to retrieve a value from the cache. If the key does not exist or has been garbage collected, return `None`. - Implement a method `size() -> int` that returns the current number of entries in the cache. 2. **Finalizer Management**: - Implement automatic cleanup of the cache using the `weakref.finalize` mechanism to ensure proper resource management when cache entries are garbage collected. - Implement a method `register_finalizer(key: str, cleanup_func: Callable)` to register a cleanup function that will be called when the cache entry for the provided key is garbage collected. The `cleanup_func` should receive the `key` as an argument. Input and Output Formats: - Method `put(key: str, value: Any)`: - `key`: A non-empty string. - `value`: Any Python object. - Adds the value to the cache under the specified key. - Method `get(key: str) -> Any`: - `key`: A non-empty string. - Returns the value associated with the key from the cache, or `None` if the key does not exist or has been garbage collected. - Method `size() -> int`: - Returns the number of currently live entries in the cache. - Method `register_finalizer(key: str, cleanup_func: Callable)`: - `key`: A non-empty string. - `cleanup_func`: A callable that takes a single argument (`key`). Constraints and Limitations: - You can assume that all `put` and `get` operations are executed with valid string keys. - Your implementation should handle edge cases where objects are garbage collected. Example Usage: ```python import weakref class WeakCache: def __init__(self): self._cache = weakref.WeakValueDictionary() def put(self, key, value): self._cache[key] = value weakref.finalize(value, self._cleanup, key) def get(self, key): return self._cache.get(key, None) def size(self): return len(self._cache) def register_finalizer(self, key, cleanup_func): if key in self._cache: value = self._cache[key] weakref.finalize(value, cleanup_func, key) def _cleanup(self, key): print(f\\"Cleaning up entry for key: {key}\\") # Example Usage cache = WeakCache() cache.put(\\"item1\\", {\\"data\\": 12345}) assert cache.size() == 1 print(cache.get(\\"item1\\")) # Output: {\'data\': 12345} del cache.get(\\"item1\\") print(cache.size()) # Output: 0 ``` Write your implementation below: ```python # Define your WeakCache class here ```","solution":"import weakref class WeakCache: def __init__(self): self._cache = weakref.WeakValueDictionary() def put(self, key, value): self._cache[key] = value weakref.finalize(value, self._cleanup, key) def get(self, key): return self._cache.get(key, None) def size(self): return len(self._cache) def register_finalizer(self, key, cleanup_func): if key in self._cache: value = self._cache[key] weakref.finalize(value, cleanup_func, key) def _cleanup(self, key): print(f\\"Cleaning up entry for key: {key}\\")"},{"question":"**Background:** You are tasked with implementing a custom SMTP server using Python\'s `smtpd` module. Specifically, you need to create a server that logs all incoming email messages to a file for auditing purposes. For this, you will extend the `SMTPServer` class to override the `process_message` method and handle the message appropriately. **Question:** 1. Create a class `CustomSMTPServer` that extends `smtpd.SMTPServer`. Implement the `process_message` method to log details of every received email to a file called `email_log.txt`. 2. The logged details should include: - Peer address (sender\'s IP and port) - Sender email address - Recipient email addresses (comma-separated if multiple) - Email content - Email size in bytes 3. Create a main block to start the server, listening on `localhost` and port `1025`. **Requirements:** - The `process_message` method should handle exceptions appropriately, ensuring the server does not crash from unexpected errors. - Each logged entry in `email_log.txt` should be appended as a new line in the format: ``` Peer: <peer address>, From: <mailfrom>, To: <rcpttos>, Size: <size> bytes Content: <data> ``` **Constraints:** - The server should only log messages and should not relay messages to any remote server. - Ensure the logging is efficient and does not excessively slow down the server. **Performance Requirements:** - Your server should be able to handle multiple email messages efficiently and log each without significant delay. **Hints:** - You can use file handling techniques to write logs to the file. - Remember to handle both bytes and string types properly based on `decode_data` option. ```python import smtpd import asyncore class CustomSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): log_entry = f\\"Peer: {peer}, From: {mailfrom}, To: {\',\'.join(rcpttos)}, Size: {len(data)} bytesnContent:n{data}n\\" # Append the log entry to email_log.txt with open(\\"email_log.txt\\", \\"a\\") as log_file: log_file.write(log_entry) print(log_entry) # Additionally, print to stdout for debugging return None if __name__ == \'__main__\': localaddr = (\'localhost\', 1025) server = CustomSMTPServer(localaddr, None) try: asyncore.loop() except KeyboardInterrupt: pass ``` Ensure to test your code by using an email client to send emails to `localhost:1025` and check if the emails are logged correctly in `email_log.txt`. **Evaluation Criteria:** - Correctness: The code should correctly log all required details of each received email. - Robustness: The server should handle errors gracefully without crashing. - Efficiency: The server and logging mechanism should handle multiple email messages in quick succession without noticeable delays. - Code Quality: The code should be well-structured and documented where necessary.","solution":"import smtpd import asyncore class CustomSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): log_entry = ( f\\"Peer: {peer}, From: {mailfrom}, To: {\',\'.join(rcpttos)}, \\" f\\"Size: {len(data)} bytesnContent:n{data}n\\" ) try: # Append the log entry to email_log.txt with open(\\"email_log.txt\\", \\"a\\") as log_file: log_file.write(log_entry) except Exception as e: print(f\\"Failed to log email message: {e}\\") return None if __name__ == \'__main__\': localaddr = (\'localhost\', 1025) server = CustomSMTPServer(localaddr, None) try: asyncore.loop() except KeyboardInterrupt: pass"},{"question":"**Coding Assessment Question** # Objective: To assess your understanding of Seaborn\'s plotting capabilities using the seaborn.objects module, especially in creating and customizing histograms and bar plots. You are required to demonstrate your knowledge of loading data, plotting, mapping properties, and handling overlapping plots. # Task: Write a Python function `create_diamond_plot` that: 1. Loads the `diamonds` dataset from Seaborn. 2. Creates a histogram of diamond prices with bars colored by the diamond\'s `cut` attribute. 3. Adjusts the x-axis to use a logarithmic scale. 4. Resolves any potential overlap of bars using an appropriate transform. 5. Sets the edge width of the bars to 1.5 and ensures the bars are unfilled with a specific edge color. 6. Narrows down the bars for a subset of diamonds (e.g., diamonds with a `clarity` of \'SI1\') and overlays this on the original histogram. # Input: - No input is required from the user. The function should perform all loading and plotting within itself. # Output: - The function should not return any value but should display the generated plot. # Function Signature: ```python def create_diamond_plot() -> None: pass ``` # Constraints: - Use only the seaborn.objects module for plotting. - Ensure the final plot displays clearly with proper labeling and legend where necessary. # Example: ```python def create_diamond_plot() -> None: import seaborn.objects as so from seaborn import load_dataset # Load the dataset diamonds = load_dataset(\\"diamonds\\") # Create the base plot p = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") # Add histograms hist = so.Hist(binwidth=0.2) # Create base histogram with bars colored by cut p.add(so.Bars(edgewidth=1.5, fill=False, edgecolor=\\"C0\\"), hist, color=\\"cut\\") # Resolve overlap using Stack transform p.add(so.Bars(), hist, so.Stack(), color=\\"cut\\") # Overlay narrowed bars for a subset (diamonds with clarity \'SI1\') p.add(so.Bars(color=\\".9\\", width=.5), hist, data=diamonds.query(\\"clarity == \'SI1\'\\")) # Show the plot p.show() ``` Note: This function provides an example structure and may not precisely match your final solution. Make sure to include all the specified customizations and property settings.","solution":"def create_diamond_plot() -> None: import seaborn.objects as so import seaborn as sns # Load the dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create the base plot with a logarithmic x-axis p = so.Plot(diamonds, x=\\"price\\").scale(x=\\"log\\") # Add histogram with bars colored by cut, using Stack transform to resolve overlap hist = so.Hist(binwidth=0.2) # Create base histogram with bars colored by cut p.add(so.Bars(edgewidth=1.5, fill=False, edgecolor=\\"black\\"), hist, color=\\"cut\\") # Resolve overlap using Stack transform p.add(so.Bars(), hist, so.Stack(), color=\\"cut\\") # Overlay narrowed bars for a subset (diamonds with clarity \'SI1\') p.add(so.Bars(color=\\"grey\\", width=.5), hist, data=diamonds.query(\\"clarity == \'SI1\'\\")) # Show the plot p.show()"},{"question":"# Question Your task is to implement a function that takes a sequence and a slice object, and applies the slice object to the sequence. Your implementation should handle potential out-of-bounds indices as well as adjust start and stop indices based on the length of the sequence. Function Signature ```python def apply_slice(seq, slice_obj): Apply a slice object to a given sequence and return the sliced sequence. Parameters: seq (list): The input sequence. slice_obj (slice): The slice object to apply. Returns: list: A new sequence as per the slice object. Raises: TypeError: If `seq` is not a list or `slice_obj` is not a slice instance. ``` Input - `seq`: A list of elements (for simplicity, assume the elements are integers). Length constraints: 1 <= len(seq) <= 1000. - `slice_obj`: A `slice` object indicating the slice to apply to the list `seq`. Output - A new list containing the elements of `seq` as defined by `slice_obj`. Example ```python # Example 1 seq = [1, 2, 3, 4, 5] slice_obj = slice(1, 3) print(apply_slice(seq, slice_obj)) # Output: [2, 3] # Example 2 seq = [1, 2, 3, 4, 5] slice_obj = slice(1, 100, 2) print(apply_slice(seq, slice_obj)) # Output: [2, 4] # Example 3 seq = [10, 20, 30, 40, 50] slice_obj = slice(None, None, -1) print(apply_slice(seq, slice_obj)) # Output: [50, 40, 30, 20, 10] ``` Constraints - The function should raise a `TypeError` if `seq` is not a list or `slice_obj` is not an instance of the `slice` class. - The function should handle out-of-bounds indices by clipping them. - The solution should utilize the fundamental concepts of working with slices in Python, ensuring understanding of how to manipulate them programmatically. Make sure to test your function with multiple test cases, including edge cases like empty sequences or slices with negative step values.","solution":"def apply_slice(seq, slice_obj): Apply a slice object to a given sequence and return the sliced sequence. Parameters: seq (list): The input sequence. slice_obj (slice): The slice object to apply. Returns: list: A new sequence as per the slice object. Raises: TypeError: If `seq` is not a list or `slice_obj` is not a slice instance. if not isinstance(seq, list): raise TypeError(\\"The `seq` parameter must be a list.\\") if not isinstance(slice_obj, slice): raise TypeError(\\"The `slice_obj` parameter must be an instance of the slice class.\\") return seq[slice_obj]"},{"question":"**Python Coding Assessment Question** # Problem Statement You are tasked with implementing a function that performs multiple encoding and decoding operations on binary data using Python\'s `binascii` module. Your function will receive a string representing a sequence of operations and binary data and will apply these operations sequentially. # Function Signature ```python def encode_decode_operations(operations: str, data: bytes) -> bytes: pass ``` # Input 1. `operations` (str): A string containing a sequence of operations. Each operation is represented by a letter followed by an optional argument inside parentheses. - \'u\': uuencode (no argument required) - \'U\': uudecode (no argument required) - \'b\': base64 encode (no argument required) - \'B\': base64 decode (no argument required) - \'q\': quoted-printable encode (optional argument `header`) - \'Q\': quoted-printable decode (optional argument `header`) - \'h\': hexlify (no argument required) - \'H\': unhexlify (no argument required) - \'c\': compute CRC-32 (initial value provided as an optional argument) 2. `data` (bytes): The binary data that will be processed by the operations. # Output The function should return the resulting binary data after all operations have been applied sequentially. # Example ```python # Example usage ops = \\"b(Bq(header=True)Q(header=True)\\" data = b\\"hello world\\" encoded_data = encode_decode_operations(ops, data) print(encoded_data) ``` # Constraints - The `data` should not exceed 1024 bytes in length. - The operations string will be at most 256 characters in length. - You can assume that the operations string is always valid. # Notes - For operations with optional arguments, follow the format `operation(argument=value)`. - If no arguments are provided for operations that accept them, use the default values from the `binascii` documentation. # Instructions 1. Implement the `encode_decode_operations` function. 2. Make sure to handle both encoding and decoding operations. 3. Process the operations in the order they appear in the `operations` string. # Solution Template ```python import binascii def encode_decode_operations(operations: str, data: bytes) -> bytes: for operation in operations.split(\'(\'): op = operation.strip().split(\')\')[0] if op.startswith(\\"u\\"): data = binascii.b2a_uu(data) elif op.startswith(\\"U\\"): data = binascii.a2b_uu(data) elif op.startswith(\\"b\\"): data = binascii.b2a_base64(data) elif op.startswith(\\"B\\"): data = binascii.a2b_base64(data) elif op.startswith(\\"q\\"): header = \\"header=True\\" in op data = binascii.b2a_qp(data, header=header) elif op.startswith(\\"Q\\"): header = \\"header=True\\" in op data = binascii.a2b_qp(data, header=header) elif op.startswith(\\"h\\"): data = binascii.b2a_hex(data) elif op.startswith(\\"H\\"): data = binascii.a2b_hex(data) elif op.startswith(\\"c\\"): value = int(op.split(\'=\')[-1]) if \'=\' in op else 0 data = binascii.crc32(data, value).to_bytes(4, \'big\') return data # Return the implemented function or the processed results ```","solution":"import binascii def encode_decode_operations(operations: str, data: bytes) -> bytes: for operation in operations.split(\')\'): if not operation: continue op = operation.strip().split(\'(\') command = op[0] argument = op[1] if len(op) > 1 else \'\' if command == \\"u\\": data = binascii.b2a_uu(data) elif command == \\"U\\": data = binascii.a2b_uu(data) elif command == \\"b\\": data = binascii.b2a_base64(data).strip() elif command == \\"B\\": data = binascii.a2b_base64(data) elif command == \\"q\\": header = \'header=True\' in argument data = binascii.b2a_qp(data, header=header) elif command == \\"Q\\": header = \'header=True\' in argument data = binascii.a2b_qp(data, header=header) elif command == \\"h\\": data = binascii.b2a_hex(data) elif command == \\"H\\": data = binascii.a2b_hex(data) elif command == \\"c\\": value = int(argument.split(\'=\')[-1]) if \'=\' in argument else 0 data = binascii.crc32(data, value).to_bytes(4, \'big\') return data"},{"question":"Objective: Implement a class that utilizes the `pickle` module for object serialization and deserialization, incorporating custom reduction for complex objects and handling of stateful objects. Description: You are required to design a class `Employee`, which holds information about employees. The instances of this class should be serializable using the `pickle` module. Additionally, implement custom reduction to manage the serialization of a nested custom object, and ensure the state is maintained during the serialization process. Class Definition: **Class Name:** `Employee` **Attributes:** 1. `employee_id` (int): A unique identifier for the employee. 2. `name` (str): The name of the employee. 3. `email` (str): The email address of the employee. 4. `address` (Address): An instance of another class `Address` which holds street, city, and zip code. **Methods:** 1. `__init__(self, employee_id, name, email, address)`: Initializes the `Employee` object. 2. `__reduce__(self)`: Custom reduction method for `pickle`. 3. `get_state(self)`: Returns the current state of the employee object. 4. `set_state(self, state)`: Restores the state of the employee object. **Class Address:** **Attributes:** 1. `street` (str): Street address. 2. `city` (str): City name. 3. `zip_code` (str): Zip code. **Methods:** 1. `__init__(self, street, city, zip_code)`: Initializes the `Address` object. 2. `__str__(self)`: Returns a string representation of the Address object. Task: 1. Implement the `Address` and `Employee` classes as described above. 2. Implement custom reduction in the `Employee` class to serialize and deserialize nested `Address` objects. 3. Ensure that the state of the `Employee` instance is properly maintained during the serialization and deserialization process. Constraints: 1. The `employee_id` should be a unique integer. 2. The `name` and `email` should be non-empty strings. 3. The `Address` object must be valid and non-empty. Performance Requirements: The implementation should efficiently serialize and deserialize if called multiple times with different instances. Input and Output: - Input: Initialization parameters for `Employee` and `Address`. - Output: Serialized byte stream and deserialized object maintaining state and data integrity. Example: Here is an example usage of the `Employee` and `Address` classes: ```python import pickle class Address: def __init__(self, street, city, zip_code): self.street = street self.city = city self.zip_code = zip_code def __str__(self): return f\\"{self.street}, {self.city} - {self.zip_code}\\" class Employee: def __init__(self, employee_id, name, email, address): self.employee_id = employee_id self.name = name self.email = email self.address = address def __reduce__(self): return (self.__class__, (self.employee_id, self.name, self.email, self.address)) def get_state(self): return self.__dict__ def set_state(self, state): self.__dict__.update(state) # Serialize an Employee object address = Address(\\"123 Main St\\", \\"Anytown\\", \\"12345\\") employee = Employee(1, \\"John Doe\\", \\"john.doe@example.com\\", address) serialized_employee = pickle.dumps(employee) # Deserialize the Employee object deserialized_employee = pickle.loads(serialized_employee) # Validate the deserialized object assert deserialized_employee.get_state() == employee.get_state() ``` Implement the classes `Employee` and `Address` as specified to handle serialization and deserialization while maintaining the state.","solution":"import pickle class Address: def __init__(self, street, city, zip_code): self.street = street self.city = city self.zip_code = zip_code def __str__(self): return f\\"{self.street}, {self.city} - {self.zip_code}\\" class Employee: def __init__(self, employee_id, name, email, address): self.employee_id = employee_id self.name = name self.email = email self.address = address def __reduce__(self): return (self.__class__, (self.employee_id, self.name, self.email, self.address)) def get_state(self): return self.__dict__ def set_state(self, state): self.__dict__.update(state) # Usage demonstration (not to be included in the tests directly): # address = Address(\\"123 Main St\\", \\"Anytown\\", \\"12345\\") # employee = Employee(1, \\"John Doe\\", \\"john.doe@example.com\\", address) # serialized_employee = pickle.dumps(employee) # deserialized_employee = pickle.loads(serialized_employee) # assert deserialized_employee.get_state() == employee.get_state()"},{"question":"# Pandas Data Analysis Challenge **Objective:** You are provided with a dataset containing information about customer orders for an e-commerce platform. The dataset includes details such as order ID, customer ID, order date, delivery date, product category, and order amount. Your task is to write a function that performs various data manipulations and returns a summary dataframe. **Dataset Description:** The dataset `orders.csv` has the following columns: - `order_id`: Unique identifier for each order. - `customer_id`: Unique identifier for each customer. - `order_date`: Date when the order was placed. (Format: YYYY-MM-DD) - `delivery_date`: Date when the order was delivered. (Format: YYYY-MM-DD) - `product_category`: Category of the product ordered. - `order_amount`: Total amount of the order. **Tasks:** 1. **Load Data:** Load the data from the provided CSV file into a pandas dataframe. 2. **Handle Missing Values:** Identify and handle missing values appropriately. Assume that missing `delivery_date` values indicate orders that have not been delivered yet. 3. **Data Transformation:** - Convert `order_date` and `delivery_date` columns to datetime format. - Create a new column `delivery_time` representing the number of days it took for each order to be delivered. If an order has not been delivered yet, set `delivery_time` to NaN. 4. **Categorical Encoding:** Convert the `product_category` column into dummy variables. 5. **Order Summary:** Generate a summary dataframe with the following columns: - `customer_id` - `total_orders`: Total number of orders placed by the customer. - `total_order_amount`: Total amount spent by the customer. - `avg_delivery_time`: Average delivery time for the customer\'s orders. Ensure that the summary is sorted by `customer_id`. **Constraints:** - The function should have the following signature: `def order_summary(file_path: str) -> pd.DataFrame` - You may use any of the pandas functions documented in the provided section. **Input:** - `file_path` (str): Path to the `orders.csv` file. **Output:** - A pandas dataframe containing the summary information. **Example Output:** ```plaintext customer_id total_orders total_order_amount avg_delivery_time 0 1 10 250.0 4.5 1 2 5 130.0 3.0 2 3 8 220.0 2.8 ``` **Note:** Focus on creating efficient and readable code. Ensure to handle edge cases and include comments explaining each step of your process.","solution":"import pandas as pd def order_summary(file_path: str) -> pd.DataFrame: Load the order data from the given file path, process it and return a summary dataframe. :param file_path: Path to the orders.csv file :return: DataFrame containing customer order summary # Load data df = pd.read_csv(file_path) # Handle missing values: assuming orders with no delivery date are not delivered yet df[\'delivery_date\'].fillna(pd.NaT, inplace=True) # Convert order_date and delivery_date to datetime format df[\'order_date\'] = pd.to_datetime(df[\'order_date\']) df[\'delivery_date\'] = pd.to_datetime(df[\'delivery_date\']) # Calculate delivery_time df[\'delivery_time\'] = (df[\'delivery_date\'] - df[\'order_date\']).dt.days # Create dummy variables for product_category df = pd.get_dummies(df, columns=[\'product_category\']) # Generate summary DataFrame summary = df.groupby(\'customer_id\').agg( total_orders=(\'order_id\', \'count\'), total_order_amount=(\'order_amount\', \'sum\'), avg_delivery_time=(\'delivery_time\', \'mean\') ).reset_index() # Ensure the summary is sorted by customer_id summary.sort_values(by=\'customer_id\', inplace=True) return summary"},{"question":"**Question:** Linear and Quadratic Discriminant Analysis Implementation and Comparison You are tasked with implementing and comparing Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) using scikit-learn. The goal is to use these techniques for classification on a synthetic dataset and to evaluate their performance. # Instructions: 1. **Generate a synthetic dataset:** - Create a synthetic dataset with `make_classification` from `sklearn.datasets`. - The dataset should have 2 classes, 2 features, and 500 samples. 2. **Implement and train models:** - Initialize and train an LDA model from `sklearn.discriminant_analysis.LinearDiscriminantAnalysis`. - Initialize and train a QDA model from `sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`. 3. **Evaluate the models:** - Compute the accuracy of both models on the training set. - Visualize the decision boundary of both LDA and QDA classifiers. # Requirements: - **Synthetic Dataset:** - `n_samples=500` - `n_features=2` - `n_redundant=0` - `n_clusters_per_class=1` - **Performance Metrics:** - Print out the accuracy of both LDA and QDA models. - **Visualization:** - Plot the decision boundary for both LDA and QDA using matplotlib. # Sample Input/Output: ```python from sklearn.datasets import make_classification from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt import numpy as np # Step 1: Generate the synthetic dataset X, y = make_classification(n_samples=500, n_features=2, n_redundant=0, n_clusters_per_class=1) # Step 2: Implement and train models lda = LinearDiscriminantAnalysis() qda = QuadraticDiscriminantAnalysis() lda.fit(X, y) qda.fit(X, y) # Step 3: Evaluate the models lda_accuracy = accuracy_score(y, lda.predict(X)) qda_accuracy = accuracy_score(y, qda.predict(X)) print(f\\"LDA Accuracy: {lda_accuracy:.2f}\\") print(f\\"QDA Accuracy: {qda_accuracy:.2f}\\") # Step 4: Visualize the decision boundary def plot_decision_boundary(model, X, y, title): x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01)) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.8) plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\'k\', marker=\'o\') plt.title(title) plt.show() plot_decision_boundary(lda, X, y, \\"LDA Decision Boundary\\") plot_decision_boundary(qda, X, y, \\"QDA Decision Boundary\\") ``` **Expected Results:** The code should output the accuracy of both LDA and QDA models on the synthetic dataset. Additionally, two plots should be generated showing the decision boundaries for each model. # Constraints: - Use only the `sklearn` library for this task. - Your code should be efficient and organized. This question assesses your understanding of LDA and QDA, their implementation, evaluation, and visualization using scikit-learn.","solution":"from sklearn.datasets import make_classification from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt import numpy as np # Step 1: Generate the synthetic dataset X, y = make_classification(n_samples=500, n_features=2, n_redundant=0, n_clusters_per_class=1) # Step 2: Implement and train models lda = LinearDiscriminantAnalysis() qda = QuadraticDiscriminantAnalysis() lda.fit(X, y) qda.fit(X, y) # Step 3: Evaluate the models lda_accuracy = accuracy_score(y, lda.predict(X)) qda_accuracy = accuracy_score(y, qda.predict(X)) print(f\\"LDA Accuracy: {lda_accuracy:.2f}\\") print(f\\"QDA Accuracy: {qda_accuracy:.2f}\\") # Step 4: Visualize the decision boundary def plot_decision_boundary(model, X, y, title): x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01)) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.8) plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\'k\', marker=\'o\') plt.title(title) plt.show() plot_decision_boundary(lda, X, y, \\"LDA Decision Boundary\\") plot_decision_boundary(qda, X, y, \\"QDA Decision Boundary\\")"},{"question":"# Time Series Analysis with Pandas **Background:** You are a data analyst working for a logistics company. Your task is to analyze the data of deliveries over several months. The data is given in a CSV file containing timestamps of when deliveries were made. However, the timestamps are in UTC and your company\'s headquarter follows the US/Pacific timezone. Additionally, you need to aggregate the delivery data to get daily and monthly summaries and also account for company-specific holidays. **Task:** 1. Load the delivery data from a CSV file. 2. Convert the timestamps from UTC to US/Pacific time. 3. Handle any missing data points by forward-filling the nearest available data. 4. Resample the data to get daily and monthly summaries. 5. Exclude deliveries made on custom company holidays. **Input:** - A CSV file named `deliveries.csv` with a single column `timestamp` containing the delivery timestamps in UTC. **Output:** - Print the daily and monthly summaries of deliveries, excluding the holiday dates. **Requirements:** 1. Define company holidays and exclude them from your analysis. 2. Ensure that timezone conversion is handled correctly. 3. Properly handle any missing data. **Example:** ```python # Sample Input (deliveries.csv) # timestamp # 2023-01-01 10:00:00 # 2023-01-02 12:00:00 # ... # Expected Output Example # Daily Summary: # 2023-01-01 5 # 2023-01-02 2 # ... # Monthly Summary: # 2023-01 120 # 2023-02 98 # ... # You need to implement the functions as defined below import pandas as pd import datetime def load_and_convert(filename): Load the CSV file and convert timestamps from UTC to US/Pacific time. Parameters: filename (str): The name of the CSV file to load. Returns: pd.DataFrame: DataFrame with the converted timestamps. df = pd.read_csv(filename, parse_dates=[\'timestamp\']) df[\'timestamp\'] = df[\'timestamp\'].dt.tz_localize(\'UTC\').dt.tz_convert(\'US/Pacific\') return df def handle_missing_data(df): Handle any missing data points by forward-filling the nearest available data. Parameters: df (pd.DataFrame): DataFrame with timestamps. Returns: pd.DataFrame: DataFrame with missing data handled. df = df.set_index(\'timestamp\').sort_index() df = df.asfreq(\'T\').fillna(method=\'ffill\') return df def define_company_holidays(): Define company-specific holidays. Returns: list: List of company holiday dates. holidays = [ \'2023-01-01\', # New Year\'s Day \'2023-07-04\', # Independence Day \'2023-12-25\' # Christmas Day ] holidays = pd.to_datetime(holidays).tz_localize(\'US/Pacific\') return holidays def exclude_holidays(df, holidays): Exclude the holidays from the DataFrame. Parameters: df (pd.DataFrame): DataFrame with the delivery timestamps. holidays (list): List of holiday dates to exclude. Returns: pd.DataFrame: DataFrame with holidays excluded. df = df[~df.index.isin(holidays)] return df def resample_data(df): Resample the data to get daily and monthly summaries. Parameters: df (pd.DataFrame): DataFrame with the delivery timestamps. Returns: tuple: DataFrames of daily and monthly summaries. daily = df.resample(\'D\').size() monthly = df.resample(\'M\').size() return daily, monthly if __name__ == \\"__main__\\": filename = \'deliveries.csv\' # Load and convert timestamps deliveries_df = load_and_convert(filename) # Handle missing data deliveries_df = handle_missing_data(deliveries_df) # Define company holidays company_holidays = define_company_holidays() # Exclude holidays deliveries_df = exclude_holidays(deliveries_df, company_holidays) # Resample data daily_summary, monthly_summary = resample_data(deliveries_df) print(\\"Daily Summary:\\") print(daily_summary) print(\\"nMonthly Summary:\\") print(monthly_summary) ``` Ensure your implementation reads from the `deliveries.csv` file and processes the data as described. The holiday dates should be defined within the code, and the results should be printed to the console.","solution":"import pandas as pd def load_and_convert(filename): Load the CSV file and convert timestamps from UTC to US/Pacific time. Parameters: filename (str): The name of the CSV file to load. Returns: pd.DataFrame: DataFrame with the converted timestamps. df = pd.read_csv(filename, parse_dates=[\'timestamp\']) df[\'timestamp\'] = df[\'timestamp\'].dt.tz_localize(\'UTC\').dt.tz_convert(\'US/Pacific\') return df def handle_missing_data(df): Handle any missing data points by forward-filling the nearest available data. Parameters: df (pd.DataFrame): DataFrame with timestamps. Returns: pd.DataFrame: DataFrame with missing data handled. df = df.set_index(\'timestamp\').sort_index() df = df.asfreq(\'T\').fillna(method=\'ffill\') return df def define_company_holidays(): Define company-specific holidays. Returns: list: List of company holiday dates. holidays = [ \'2023-01-01\', # New Year\'s Day \'2023-07-04\', # Independence Day \'2023-12-25\' # Christmas Day ] holidays = pd.to_datetime(holidays).tz_localize(\'US/Pacific\') return holidays def exclude_holidays(df, holidays): Exclude the holidays from the DataFrame. Parameters: df (pd.DataFrame): DataFrame with the delivery timestamps. holidays (list): List of holiday dates to exclude. Returns: pd.DataFrame: DataFrame with holidays excluded. df = df[~df.index.floor(\'D\').isin(holidays)] return df def resample_data(df): Resample the data to get daily and monthly summaries. Parameters: df (pd.DataFrame): DataFrame with the delivery timestamps. Returns: tuple: DataFrames of daily and monthly summaries. daily = df.resample(\'D\').size() monthly = df.resample(\'M\').size() return daily, monthly if __name__ == \\"__main__\\": filename = \'deliveries.csv\' # Load and convert timestamps deliveries_df = load_and_convert(filename) # Handle missing data deliveries_df = handle_missing_data(deliveries_df) # Define company holidays company_holidays = define_company_holidays() # Exclude holidays deliveries_df = exclude_holidays(deliveries_df, company_holidays) # Resample data daily_summary, monthly_summary = resample_data(deliveries_df) print(\\"Daily Summary:\\") print(daily_summary) print(\\"nMonthly Summary:\\") print(monthly_summary)"},{"question":"Objective Demonstrate your understanding of Python 3.10\'s `asyncio` package by implementing an asynchronous TCP server and client that can communicate with each other. This will test your ability to manage event loops, create tasks, handle async networking, and properly use transports and protocols. Requirements 1. **TCP Server**: - Create a TCP server that listens on a given host and port. - Accept incoming client connections. - For each connected client, read data sent by the client. - Respond to the client with the reverse of the received data. 2. **TCP Client**: - Create a TCP client that connects to the server at the specified host and port. - Send a predefined list of strings to the server. - Receive responses from the server and print them out. Constraints - The server must handle multiple clients concurrently. - The server should use the event loop to manage connections and data transmission. - The client should run in a separate function that is scheduled by the event loop. - Both the server and the client should handle exceptions gracefully and clean up resources properly. Input and Output Formats - The server should run indefinitely and print connection logs and errors to the console. - The client should print each response it receives from the server. Example 1. **Server**: ```python # IP and port the server should bind to HOST = \'127.0.0.1\' PORT = 8888 async def handle_client(reader, writer): Function to handle communication with a client. # Implement interaction with client here async def run_server(host, port): Function to run a TCP server. # Implement the server setup here # Run the server asyncio.run(run_server(HOST, PORT)) ``` 2. **Client**: ```python HOST = \'127.0.0.1\' PORT = 8888 async def run_client(host, port): Function to run a TCP client and send messages. # Implement client connection and communication here # Run the client asyncio.run(run_client(HOST, PORT)) ``` Detailed Function Definitions 1. `handle_client(reader, writer)` - Parameters: - `reader`: asyncio StreamReader object to read data from the client. - `writer`: asyncio StreamWriter object to send data to the client. - Reads data sent by the client, reverses it, and sends it back. 2. `run_server(host, port)` - Parameters: - `host`: IP address where server should bind. - `port`: Port number where server should bind. - Uses `asyncio.start_server()` to create the server and manage client connections. 3. `run_client(host, port)` - Parameters: - `host`: IP address of the server to connect to. - `port`: Port number of the server to connect to. - Connects to the server, sends a list of strings, and prints server responses. Notes - Use appropriate asyncio methods like `asyncio.start_server()`, `StreamReader.read`, `StreamWriter.write`, and asynchronous connection handling. - Ensure proper resource management using `writer.close()` and `await writer.wait_closed()` for cleanup.","solution":"import asyncio # Function to handle client connections async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') print(f\\"Connected with {addr}\\") while True: data = await reader.read(100) message = data.decode() if not data: break print(f\\"Received {message} from {addr}\\") reversed_message = message[::-1] writer.write(reversed_message.encode()) await writer.drain() print(f\\"Closed connection with {addr}\\") writer.close() await writer.wait_closed() # Function to run the TCP server async def run_server(host, port): server = await asyncio.start_server(handle_client, host, port) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() # TCP Client function async def run_client(host, port, messages): reader, writer = await asyncio.open_connection(host, port) for message in messages: print(f\'Sending: {message}\') writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()}\') print(\'Close the connection\') writer.close() await writer.wait_closed() if __name__ == \\"__main__\\": # Running the server HOST = \'127.0.0.1\' PORT = 8888 server = asyncio.run(run_server(HOST, PORT))"},{"question":"# Question: Data Analysis with Pandas You are given a dataset in the form of a DataFrame. The dataset includes information about sales transactions, with columns for `TransactionID`, `Date`, `CustomerID`, `Amount`, `ProductCategory`, and `Region`. Your task is to implement several functions to perform specific analyses and transformations on this dataset using pandas. Functions to Implement 1. **load_data(file_path)**: - **Input**: A string `file_path` representing the path to a CSV file containing the dataset. - **Output**: A pandas DataFrame containing the loaded data. - **Description**: Load the dataset from the specified CSV file into a DataFrame. 2. **get_summary_statistics(df)**: - **Input**: A DataFrame `df` representing the dataset. - **Output**: A DataFrame containing summary statistics for the numerical columns (`Amount`). - **Description**: Generate summary statistics (count, mean, std, min, 25%, 50%, 75%, max) for the `Amount` column. 3. **add_transaction_month(df)**: - **Input**: A DataFrame `df` representing the dataset. - **Output**: A DataFrame with an additional column `TransactionMonth`, extracted from the `Date` column. - **Description**: Extract the month from the `Date` column and add it as a new column `TransactionMonth`. 4. **total_amount_by_category(df)**: - **Input**: A DataFrame `df` representing the dataset. - **Output**: A Series with the total `Amount` spent in each `ProductCategory`. - **Description**: Calculate the total `Amount` spent for each `ProductCategory`. 5. **top_customers_by_amount(df, n)**: - **Input**: A DataFrame `df` representing the dataset and an integer `n`. - **Output**: A DataFrame containing the top `n` customers based on the total `Amount` spent. - **Description**: Identify the top `n` customers who have spent the most, ordered by descending total `Amount`. Constraints - You may assume that the input data is well-formed and contains no missing values. - You should use pandas methods and functionality wherever possible to implement these functions. Performance Requirements - The implementation should efficiently handle datasets with up to 100,000 rows. - Avoid using loops where vectorized operations can be applied. Example Usage ```python # Assume the dataset is stored at \'sales_data.csv\' df = load_data(\'sales_data.csv\') # Generate summary statistics for the \'Amount\' column summary_stats = get_summary_statistics(df) print(summary_stats) # Add \'TransactionMonth\' column df_with_month = add_transaction_month(df) print(df_with_month.head()) # Calculate total amount by product category total_amounts = total_amount_by_category(df) print(total_amounts) # Get the top 5 customers by total amount spent top_customers = top_customers_by_amount(df, 5) print(top_customers) ``` Implement these functions to complete the task.","solution":"import pandas as pd def load_data(file_path): Load the dataset from the specified CSV file into a DataFrame. :param file_path: str, Path to the CSV file :return: pd.DataFrame, Loaded dataset return pd.read_csv(file_path) def get_summary_statistics(df): Generate summary statistics for the numerical columns (Amount). :param df: pd.DataFrame, Input dataset :return: pd.DataFrame, Summary statistics for \'Amount\' column return df[\'Amount\'].describe() def add_transaction_month(df): Extract the month from the `Date` column and add it as a new column `TransactionMonth`. :param df: pd.DataFrame, Input dataset :return: pd.DataFrame, Dataset with \'TransactionMonth\' column added df[\'TransactionMonth\'] = pd.to_datetime(df[\'Date\']).dt.month return df def total_amount_by_category(df): Calculate the total `Amount` spent for each `ProductCategory`. :param df: pd.DataFrame, Input dataset :return: pd.Series, Total `Amount` by `ProductCategory` return df.groupby(\'ProductCategory\')[\'Amount\'].sum() def top_customers_by_amount(df, n): Identify the top `n` customers who have spent the most, ordered by descending total `Amount`. :param df: pd.DataFrame, Input dataset :param n: int, Number of top customers to retrieve :return: pd.DataFrame, Top `n` customers by total `Amount` spent return df.groupby(\'CustomerID\')[\'Amount\'].sum().nlargest(n).reset_index()"},{"question":"# Python Coding Assessment Question Objective: Implement a simplified version of the `GenericAlias` type in Python to demonstrate proficiency with Python\'s class structure, magic methods, and type hinting. Problem Statement: You are required to implement a class `SimpleGenericAlias` that mimics the behavior of Python\'s `GenericAlias` outlined in the provided documentation. This class should allow for the creation of generic types with specified parameters. Requirements: 1. **Class Initialization**: - The class should be initialized with two parameters: - `origin`: The original type (e.g., list, dict). - `args`: The type parameters, which can be a single type or a tuple of types. - If `args` is a single type, it should be converted to a tuple. - The `__args__` attribute should store the type parameters as a tuple. - The `__origin__` attribute should store the original type. 2. **Attributes**: - `__origin__`: Store the original type. - `__args__`: Store the type parameters as a tuple. - `__parameters__`: Lazily computed based on `__args__` (include unique types, and order must reflect first appearance). 3. **Methods**: - `__repr__()`: Return a string representation in the format: `\\"SimpleGenericAlias(origin, args)\\"`. - `__eq__(other)`: Check if two SimpleGenericAlias objects are equal (i.e., have the same origin and args). Constraints: - The `origin` should be a type. - The `args` should be either a type or a tuple of types. Example: ```python class SimpleGenericAlias: def __init__(self, origin, args): # Your implementation here def __repr__(self): # Your implementation here def __eq__(self, other): # Your implementation here @property def __parameters__(self): # Your implementation here # Example usage: a = SimpleGenericAlias(list, (int,)) print(a) # SimpleGenericAlias(<class \'list\'>, (int,)) print(a.__parameters__) # (int,) b = SimpleGenericAlias(dict, (str, int)) print(b) # SimpleGenericAlias(<class \'dict\'>, (str, int)) print(b.__parameters__) # (str, int) assert a != b c = SimpleGenericAlias(list, (int,)) assert a == c ``` # Note: Focus on the correct handling of type parameters and ensure your methods are capable of handling differences correctly. This will test your knowledge of custom classes, type management, and magic methods in Python.","solution":"class SimpleGenericAlias: def __init__(self, origin, args): if not isinstance(args, tuple): args = (args,) self.__origin__ = origin self.__args__ = args def __repr__(self): return f\\"SimpleGenericAlias({self.__origin__}, {self.__args__})\\" def __eq__(self, other): if isinstance(other, SimpleGenericAlias): return self.__origin__ == other.__origin__ and self.__args__ == other.__args__ return False @property def __parameters__(self): unique_params = [] for arg in self.__args__: if arg not in unique_params: unique_params.append(arg) return tuple(unique_params)"},{"question":"**Question: Custom Header Processing for Email Messages** Given the detailed functionalities provided by the `email.headerregistry` module, you are required to create a custom class that processes email headers. Your task is to implement a function that accepts a raw email header string and returns a dictionary with parsed header details using appropriate classes from the `email.headerregistry` module. # Function Specification ```python def process_email_headers(raw_headers: str) -> dict: Process raw email headers and return parsed header details. Args: raw_headers (str): The raw email headers as a single string. Returns: dict: A dictionary where keys are header names and values are their parsed details. For example: { \\"From\\": {\\"username\\": \\"john.doe\\", \\"domain\\": \\"example.com\\"}, \\"Date\\": {\\"datetime\\": datetime_object}, ... } pass ``` # Input - `raw_headers`: A single string containing multiple email headers separated by carriage returns (e.g., `\\"From: John Doe <john.doe@example.com>rnDate: Wed, 15 Sep 2021 14:00:00 +0000rn\\"`). # Output - Returns a dictionary with parsed header details. - For address headers, the details should include `username` and `domain`. - For date headers, include a `datetime` object parsed from the date string. - For unstructured headers, return the raw value. # Constraints - Use the classes and methods described in the `email.headerregistry` module. - Handle errors gracefully and include them in the output dictionary under a key named `defects` for the respective header. - Assume all headers conform to the RFC 5322 standard. # Example ```python raw_headers = \\"From: John Doe <john.doe@example.com>rnDate: Wed, 15 Sep 2021 14:00:00 +0000rnSubject: Test emailrn\\" print(process_email_headers(raw_headers)) # Expected Output: # { # \\"From\\": {\\"username\\": \\"john.doe\\", \\"domain\\": \\"example.com\\"}, # \\"Date\\": {\\"datetime\\": datetime(2021, 9, 15, 14, 0, tzinfo=<tzinfo>)}, # \\"Subject\\": \\"Test email\\", # } ``` Your implementation should show a deep understanding of the Python `email.headerregistry` module, including how to create, parse, and properly handle email headers.","solution":"from email.policy import default from email.parser import BytesParser from email.message import EmailMessage def process_email_headers(raw_headers: str) -> dict: # Use the appropriate policy and parser to parse raw headers msg = BytesParser(policy=default).parsebytes(raw_headers.encode(\'utf-8\')) results = {} for header, value in msg.items(): if header.lower() in (\'from\', \'to\', \'cc\', \'bcc\', \'reply-to\'): # Parse address headers addresses = value.addresses parsed_addresses = [] for address in addresses: parsed_addresses.append({ \\"username\\": address.username, \\"domain\\": address.domain, }) results[header] = parsed_addresses elif header.lower() == \'date\': # Parse date headers parsed_date = value.datetime results[header] = {\\"datetime\\": parsed_date} else: # For all other headers, return raw value results[header] = value return results"},{"question":"PyTorch MPS Device Management You are developing a PyTorch application that utilizes Apple\'s Metal Performance Shaders (MPS) backend and need to implement functionality to manage and monitor device memory usage and random number state for reproducibility. Your task is to complete the following functions: 1. **initialize_mps_seed(seed_value: int) -> None**: - Initialize the random number generator with a given seed value using the `torch.mps` module. 2. **get_mps_memory_info() -> dict**: - Retrieve the current and driver-allocated memory, as well as the recommended maximum memory for the MPS device. - Return this information as a dictionary with keys `\\"current_allocated_memory\\"`, `\\"driver_allocated_memory\\"`, and `\\"recommended_max_memory\\"`. 3. **profile_mps_device(operation: Callable) -> dict**: - Profile the given operation (a function) while it is running on the MPS device. - Return profiling information including whether Metal capture is enabled and capturing status as a dictionary with keys `\\"is_capturing_metal\\"` and `\\"capture_status\\"`. # Inputs: - `seed_value` for `initialize_mps_seed`: an integer representing the seed value for the random number generator. - No input parameters for `get_mps_memory_info`. - `operation` for `profile_mps_device`: a callable function that performs some operations to be profiled. # Outputs: - `initialize_mps_seed` does not return anything. - `get_mps_memory_info` returns a dictionary with memory-related information. - `profile_mps_device` returns a dictionary with profiling status information. # Constraints: - These implementations should target macOS systems with compatible MPS devices. - Ensure any necessary imports from `torch.mps` are included in your solution. - Handle any potential exceptions that might arise due to the specific nature of accessing MPS hardware and profiling. # Example Usage: ```python def sample_operation(): # Sample operation that uses PyTorch tensors import torch x = torch.tensor([1.0, 2.0, 3.0], device=\'mps\') y = torch.tensor([4.0, 5.0, 6.0], device=\'mps\') z = x + y return z # Initialize the MPS seed initialize_mps_seed(42) # Retrieve memory information memory_info = get_mps_memory_info() print(memory_info) # Profile a sample operation profile_info = profile_mps_device(sample_operation) print(profile_info) ``` # Note: Ensure that your solution includes the necessary import statements from PyTorch\'s `torch.mps` module and any other relevant modules you may need.","solution":"import torch def initialize_mps_seed(seed_value: int) -> None: Initialize the random number generator with a given seed value using the torch.mps module. torch.manual_seed(seed_value) if torch.backends.mps.is_available(): torch.mps.manual_seed(seed_value) def get_mps_memory_info() -> dict: Retrieve the current and driver-allocated memory, as well as the recommended maximum memory for the MPS device. Return this information as a dictionary with keys \\"current_allocated_memory\\", \\"driver_allocated_memory\\", and \\"recommended_max_memory\\". if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS backend is not available on this device.\\") current_allocated_memory = torch.mps.current_allocated_memory() driver_allocated_memory = torch.mps.driver_allocated_memory() recommended_max_memory = torch.mps.recommended_max_memory() return { \\"current_allocated_memory\\": current_allocated_memory, \\"driver_allocated_memory\\": driver_allocated_memory, \\"recommended_max_memory\\": recommended_max_memory } def profile_mps_device(operation) -> dict: Profile the given operation (a function) while it is running on the MPS device. Return profiling information including whether Metal capture is enabled and capturing status as a dictionary with keys \\"is_capturing_metal\\" and \\"capture_status\\". if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS backend is not available on this device.\\") # Check if Metal capture is enabled (placeholder for actual implementation) # Currently, PyTorch does not provide direct MPS profiling APIs, so we\'ll use a placeholder. # You would typically integrate with Apple\'s Instruments or other profiling tools for full visibility. try: is_capturing_metal = False # Placeholder, replace with actual Metal capture check if available # Start profiling, if applicable # operation can be executed and we capture its profile operation() # Execute the operation capture_status = \\"Success\\" # Placeholder capture status except Exception as e: capture_status = f\\"Error: {str(e)}\\" return { \\"is_capturing_metal\\": is_capturing_metal, \\"capture_status\\": capture_status }"},{"question":"**Question**: Write a Python function that dynamically imports all submodules of a given package and lists all the functions available in those submodules. # Function Signature ```python def list_package_functions(package_name: str) -> dict: pass ``` # Input - `package_name` (str): The name of the package whose submodules you wish to inspect. # Output - A dictionary where: - The keys are the names of the submodules. - The values are lists of function names defined in each submodule. # Constraints - Assume the package and its submodules are available in your current Python environment. - If a submodule cannot be imported, it should be skipped. - Only consider attributes of the submodule that are callable (functions). # Example Given a package structure like: ``` mypackage/ __init__.py modulea.py functiona() functionb() moduleb.py functionc() ``` An example input and output would be: Input: `list_package_functions(\\"mypackage\\")` Output: ```python { \\"mypackage.modulea\\": [\\"functiona\\", \\"functionb\\"], \\"mypackage.moduleb\\": [\\"functionc\\"] } ``` # Additional Information - You may use functions from the `pkgutil` module, particularly `iter_modules` and `walk_packages`, to aid in your implementation. - Make sure to import each submodule before listing its functions. Implement the function to complete the task.","solution":"import pkgutil import importlib import types def list_package_functions(package_name: str) -> dict: Returns a dictionary where the keys are submodule names and the values are lists of function names defined in those submodules. # Initialize the results dictionary result = {} # Import the main package try: package = importlib.import_module(package_name) except ImportError: return {} # Walk through the package to find all submodules for _, module_name, is_pkg in pkgutil.walk_packages(package.__path__, package.__name__ + \'.\'): if not is_pkg: try: # Try to import the submodule module = importlib.import_module(module_name) # Find all callables in the module functions_list = [name for name, obj in vars(module).items() if isinstance(obj, types.FunctionType)] # Add to the result dictionary result[module_name] = functions_list except ImportError: # If the submodule can\'t be imported, skip it continue return result"},{"question":"Objective: Implement a simple text-based user interface using the `curses` module in Python. The interface should allow a user to navigate through a menu, display current date and time, and enable text input. Requirements: 1. **Main Menu:** - Create a main menu with three options: 1. Show Date and Time 2. Enter Text 3. Exit 2. **Display Date and Time:** - When the user selects the \\"Show Date and Time\\" option, display the current date and time in a new window. 3. **Enter Text:** - When the user selects the \\"Enter Text\\" option, allow the user to enter a string of up to 30 characters. Display the entered text in another new window. 4. **Exit:** - When the user selects the \\"Exit\\" option, terminate the program gracefully, ensuring that the terminal is restored to its original state. 5. **Error Handling:** - Ensure the program handles unexpected input gracefully, without crashing, and that it restores the terminal to its normal mode upon exit or error. Input: - The user will interact with the menu using the arrow keys and the Enter key. - The user will input text when prompted. Output: - Display the main menu and the interactive options. - Display the current date and time when requested. - Display the user-entered text when requested. Constraints: - Use the `curses` module to manage the terminal display and input. - Ensure the code runs smoothly on a Unix-like system (Linux, macOS). Example Interaction: ```plaintext Main Menu: 1. Show Date and Time 2. Enter Text 3. Exit (User selects \\"Show Date and Time\\") Current Date and Time: 2023-10-01 15:45:30 (Press any key to return to the main menu) Main Menu: 1. Show Date and Time 2. Enter Text 3. Exit (User selects \\"Enter Text\\") Please enter text (max 30 chars): Hello, world! (Press any key to return to the main menu) ``` Guidelines: 1. Use `curses.wrapper()` to initialize and terminate the application cleanly. 2. Create appropriate functions for each menu option. 3. Ensure the main menu is displayed after each operation until the user selects \\"Exit\\". 4. Make use of windows and pads where appropriate to meet the requirements.","solution":"import curses from datetime import datetime def main_menu(stdscr): curses.curs_set(0) options = [\\"Show Date and Time\\", \\"Enter Text\\", \\"Exit\\"] current_option = 0 while True: stdscr.clear() height, width = stdscr.getmaxyx() for idx, option in enumerate(options): x = width//2 - len(option)//2 y = height//2 - len(options)//2 + idx if idx == current_option: stdscr.attron(curses.A_REVERSE) stdscr.addstr(y, x, option) stdscr.attroff(curses.A_REVERSE) else: stdscr.addstr(y, x, option) key = stdscr.getch() if key == curses.KEY_UP and current_option > 0: current_option -= 1 elif key == curses.KEY_DOWN and current_option < len(options) - 1: current_option += 1 elif key == curses.KEY_ENTER or key in [10, 13]: if current_option == 0: show_date_time(stdscr) elif current_option == 1: enter_text(stdscr) elif current_option == 2: break def show_date_time(stdscr): stdscr.clear() height, width = stdscr.getmaxyx() current_time = datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') stdscr.addstr(height//2, width//2 - len(current_time)//2, current_time) stdscr.addstr(height//2 + 1, width//2 - len(\\"Press any key to return to the main menu\\")//2, \\"Press any key to return to the main menu\\") stdscr.getch() def enter_text(stdscr): curses.curs_set(1) stdscr.clear() stdscr.addstr(0, 0, \\"Please enter text (max 30 chars): \\") stdscr.refresh() textwin = curses.newwin(1, 30, 1, 0) box = curses.textpad.Textbox(textwin) text = box.edit().strip() curses.curs_set(0) stdscr.clear() height, width = stdscr.getmaxyx() stdscr.addstr(height//2, width//2 - len(text)//2, text) stdscr.addstr(height//2 + 1, width//2 - len(\\"Press any key to return to the main menu\\")//2, \\"Press any key to return to the main menu\\") stdscr.getch() def main(stdscr): curses.wrapper(main_menu) if __name__ == \\"__main__\\": main(curses.initscr)"},{"question":"# Question: Implement a Custom Mixture Model Using PyTorch Distributions In this task, you are required to implement a custom Mixture Model using the PyTorch `torch.distributions` module. A Mixture Model is a probabilistic model that represents the presence of sub-populations within an overall population, without requiring that an observed data point belong to any one sub-population. Function Specifications # Function 1: `create_mixture_model` **Function Signature:** ```python import torch import torch.distributions as dist def create_mixture_model(means: torch.Tensor, stds: torch.Tensor, component_weights: torch.Tensor) -> dist.Distribution: Create a Mixture of Normal distributions given the means, standard deviations, and component weights. Parameters: - means (torch.Tensor): A tensor of shape (num_components,) representing the means of the component distributions. - stds (torch.Tensor): A tensor of shape (num_components,) representing the standard deviations of the component distributions. - component_weights (torch.Tensor): A tensor of shape (num_components,) representing the weights of each component in the mixture. Should sum to 1. Returns: - mixture_model (dist.Distribution): A PyTorch Distribution representing the Mixture of Gaussians. pass ``` # Function 2: `sample_from_mixture` **Function Signature:** ```python def sample_from_mixture(mixture_model: dist.Distribution, num_samples: int) -> torch.Tensor: Sample from the Mixture of Gaussians. Parameters: - mixture_model (dist.Distribution): A mixture model as created by `create_mixture_model`. - num_samples (int): Number of samples to draw. Returns: - samples (torch.Tensor): A tensor of shape (num_samples,) containing samples from the mixture distribution. pass ``` # Requirements and Constraints 1. **Component Distribution Setup:** You must correctly set up each Normal distribution using the provided means and standard deviations. 2. **Mixture Construction:** Utilize the `torch.distributions.MixtureSameFamily` distribution class to combine the component distributions with the provided weights. 3. **Sampling:** The sampling function should correctly draw samples from the constructed mixture distribution. 4. **Statistical Validations:** Ensure that the mixture model behaves as expected in terms of mean and variance. Example ```python # Example usage: means = torch.tensor([0.0, 5.0]) stds = torch.tensor([1.0, 1.5]) component_weights = torch.tensor([0.3, 0.7]) # Create mixture model mixture_model = create_mixture_model(means, stds, component_weights) # Sample from mixture model samples = sample_from_mixture(mixture_model, 1000) print(samples.mean(), samples.std()) ``` The expected output statistics should approximately match the weighted means and variances defined by the components. Notes - You may need to refer to the PyTorch `torch.distributions` module documentation for appropriate methods and classes. - Ensure that all inputs are properly checked and handled where necessary to avoid runtime errors.","solution":"import torch import torch.distributions as dist def create_mixture_model(means: torch.Tensor, stds: torch.Tensor, component_weights: torch.Tensor) -> dist.Distribution: Create a Mixture of Normal distributions given the means, standard deviations, and component weights. Parameters: - means (torch.Tensor): A tensor of shape (num_components,) representing the means of the component distributions. - stds (torch.Tensor): A tensor of shape (num_components,) representing the standard deviations of the component distributions. - component_weights (torch.Tensor): A tensor of shape (num_components,) representing the weights of each component in the mixture. Should sum to 1. Returns: - mixture_model (dist.Distribution): A PyTorch Distribution representing the Mixture of Gaussians. num_components = means.shape[0] component_dist = dist.Normal(loc=means, scale=stds) mixture_model = dist.MixtureSameFamily(dist.Categorical(component_weights), component_dist) return mixture_model def sample_from_mixture(mixture_model: dist.Distribution, num_samples: int) -> torch.Tensor: Sample from the Mixture of Gaussians. Parameters: - mixture_model (dist.Distribution): A mixture model as created by `create_mixture_model`. - num_samples (int): Number of samples to draw. Returns: - samples (torch.Tensor): A tensor of shape (num_samples,) containing samples from the mixture distribution. samples = mixture_model.sample((num_samples,)) return samples"},{"question":"# Advanced Coding Assessment Context You have been given a Python module that performs a set of complex calculations. The module is running slower than expected, and your task is to optimize the performance of this module. To achieve this, you need to use Python\'s profiling tools to analyze and identify the bottlenecks in the code. Task 1. **Profile the Code:** - Use the `cProfile` module to profile the execution of a given function in the module. - Save the profiling results to a file. 2. **Analyze the Results:** - Use the `pstats` module to read and analyze the profiling data. - Sort the profiling results based on cumulative time and identify the top 10 functions that were the most time-consuming. 3. **Optimize:** - Based on the profiling results, make appropriate changes to optimize the function. - Compare the performance before and after optimization by profiling the optimized function and printing the results. Instructions Implement a Python function `optimize_module(module_file: str, func_name: str, *args) -> None` with the following details: - **Inputs:** - `module_file`: The path to the Python module file containing the function to profile and optimize. - `func_name`: The name of the function within the module to profile and optimize. - `*args`: Any arguments that need to be passed to the function being profiled. - **Outputs:** - Print the top 10 functions consuming the most cumulative time before the optimization. - Print the top 10 functions consuming the most cumulative time after the optimization. - Print a brief analysis comparing the performance before and after optimization. - **Constraints:** - You may assume the module file and function are correctly implemented and can be imported without error. - Focus on meaningful optimizations. Avoid trivial changes that do not impact performance. - Ensure that the optimized function maintains the original functionality and correctness. Example ```python def optimize_module(module_file, func_name, *args): import importlib.util import cProfile import pstats import io # Dynamically import the module spec = importlib.util.spec_from_file_location(\\"module_name\\", module_file) module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) func = getattr(module, func_name) # Profile the function profiler = cProfile.Profile() profiler.enable() func(*args) profiler.disable() # Save profiling results to a file profiler.dump_stats(\'before_optimization.prof\') # Analyze and print the top 10 functions (before optimization) s = io.StringIO() ps = pstats.Stats(\'before_optimization.prof\', stream=s).sort_stats(pstats.SortKey.CUMULATIVE) ps.print_stats(10) print(\\"Before Optimization:n\\", s.getvalue()) # Perform necessary optimizations here (Example: illustrative purpose only) # Note: Actual optimizations will depend on the profiling results and specific code being analyzed. # Profile the optimized function profiler = cProfile.Profile() profiler.enable() func(*args) # Ensure this is called after optimization profiler.disable() # Save profiling results to a file profiler.dump_stats(\'after_optimization.prof\') # Analyze and print the top 10 functions (after optimization) s = io.StringIO() ps = pstats.Stats(\'after_optimization.prof\', stream=s).sort_stats(pstats.SortKey.CUMULATIVE) ps.print_stats(10) print(\\"After Optimization:n\\", s.getvalue()) # Comparing the performance print(\\"Compare the profiling results before and after optimization to identify the improvements.\\") ``` **Note:** This example code assumes you will integrate meaningful optimizations based on your profiling results.","solution":"def optimize_module(module_file, func_name, *args): import importlib.util import cProfile import pstats import io # Dynamically import the module spec = importlib.util.spec_from_file_location(\\"module_name\\", module_file) module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) func = getattr(module, func_name) # Profile the function before optimization profiler = cProfile.Profile() profiler.enable() func(*args) profiler.disable() # Save profiling results to a file profiler.dump_stats(\'before_optimization.prof\') # Analyze and print the top 10 functions (before optimization) s = io.StringIO() ps = pstats.Stats(\'before_optimization.prof\', stream=s).sort_stats(pstats.SortKey.CUMULATIVE) ps.print_stats(10) print(\\"Before Optimization:n\\", s.getvalue()) # Optimization logic here # For illustrative purposes, this part is left as a comment. # The actual optimization will depend on the profiling results and specific code being analyzed. # Profile the function after optimization profiler = cProfile.Profile() profiler.enable() func(*args) # Ensure this is called after optimization profiler.disable() # Save profiling results to a file profiler.dump_stats(\'after_optimization.prof\') # Analyze and print the top 10 functions (after optimization) s = io.StringIO() ps = pstats.Stats(\'after_optimization.prof\', stream=s).sort_stats(pstats.SortKey.CUMULATIVE) ps.print_stats(10) print(\\"After Optimization:n\\", s.getvalue()) # Comparing the performance print(\\"Compare the profiling results before and after optimization to identify the improvements.\\")"},{"question":"Problem Statement In this assessment, you are required to implement a flexible attention mechanism using PyTorch\'s `torch.nn.attention.flex_attention` module. Specifically, your task is to dynamically create and manipulate attention masks using block mask utilities. This will demonstrate your understanding of creating customized attention behaviors in neural networks. # Task 1. Implement a function `generate_attention_masks` that creates three different attention masks: a basic mask, an \\"and\\" mask, and an \\"or\\" mask using the provided block mask utilities. 2. Use these masks in a flexible attention mechanism to illustrate their impact on an attention score computation. # Requirements 1. **Function 1: `generate_attention_masks`** - **Input**: Two PyTorch tensors `tensor_a` and `tensor_b` of shape `(N, M)` representing the input data. - **Output**: A tuple of three attention masks `(basic_mask, and_mask, or_mask)`. **The masks should be generated as follows**: - **Basic Mask**: Use the `create_mask` utility to generate a simple mask for `tensor_a`. - **AND Mask**: Combine the simple mask of `tensor_a` and `tensor_b` using `and_masks`. - **OR Mask**: Combine the simple mask of `tensor_a` and `tensor_b` using `or_masks`. 2. **Function 2: `apply_attention`** - **Input**: A PyTorch tensor `input_data` of shape `(N, M)`, and attention mask from `generate_attention_masks`. - **Output**: A tensor representing the attention-weighted data. # Constraints - You can assume `N` and `M` are within a reasonable range to fit in memory. - Performance is critical. Aim for efficient computation avoiding unnecessary data operations. # Example ```python import torch from torch.nn.attention.flex_attention import create_mask, and_masks, or_masks # Function 1: Generate Attention Masks def generate_attention_masks(tensor_a, tensor_b): basic_mask = create_mask(tensor_a) and_mask = and_masks(create_mask(tensor_a), create_mask(tensor_b)) or_mask = or_masks(create_mask(tensor_a), create_mask(tensor_b)) return basic_mask, and_mask, or_mask # Function 2: Apply Attention def apply_attention(input_data, attention_mask): attention_scores = input_data * attention_mask return attention_scores # Example Usage tensor_a = torch.randn(4, 4) tensor_b = torch.randn(4, 4) basic_mask, and_mask, or_mask = generate_attention_masks(tensor_a, tensor_b) input_data = torch.randn(4, 4) # Apply attention output_basic = apply_attention(input_data, basic_mask) output_and = apply_attention(input_data, and_mask) output_or = apply_attention(input_data, or_mask) ``` # Submission Submit your implementation of `generate_attention_masks` and `apply_attention` along with a brief explanation of how the masks are generated and applied.","solution":"import torch # Assuming the following utilities exist and have been correctly imported # from torch.nn.attention.flex_attention import create_mask, and_masks, or_masks # Mock implementations of the `create_mask`, `and_masks`, and `or_masks` functions # as they do not exist in the actual PyTorch library def create_mask(tensor): Mock function to create a simple mask from the input tensor. For simplicity, we assume the mask is 1 where tensor values are non-zero and 0 otherwise. return (tensor != 0).float() def and_masks(mask1, mask2): Mock function to perform logical AND operation on two masks. return mask1 * mask2 def or_masks(mask1, mask2): Mock function to perform logical OR operation on two masks. return torch.max(mask1, mask2) def generate_attention_masks(tensor_a, tensor_b): Creates three attention masks: basic mask, and_mask, and or_mask. basic_mask = create_mask(tensor_a) and_mask = and_masks(create_mask(tensor_a), create_mask(tensor_b)) or_mask = or_masks(create_mask(tensor_a), create_mask(tensor_b)) return basic_mask, and_mask, or_mask def apply_attention(input_data, attention_mask): Applies the provided attention mask to the input data. attention_scores = input_data * attention_mask return attention_scores"},{"question":"# **Coding Assessment Question** **Objective:** Your task is to analyze a dataset, manipulate it as required, and create a complex seaborn point plot that demonstrates your understanding of data visualization with seaborn. **Question:** You are provided with the \\"penguins\\" dataset from seaborn. You need to create a point plot representing the average body mass (`body_mass_g`) of penguins grouped by the island. Additionally, you should further group the data by the species to differentiate them using color and linestyles. However, there is an additional requirement: 1. Calculate the average bill length (`bill_length_mm`) for each combination of island and species and indicate these averages as points on the same plot using a secondary y-axis. 2. Use error bars to depict the standard deviation of the body mass. 3. Customize the appearance of the plot: - Use distinct markers and linestyles for each species. - Set a capsize for the error bars to make them clearly visible. - Style the plot using seaborn\'s whitegrid theme. - Provide appropriate labels for the x-axis, both y-axes, and a plot title. **Input:** You do not need to handle any input as the dataset is provided. You can load it using `sns.load_dataset(\\"penguins\\")`. **Output:** A seaborn plot with the following properties: - X-axis representing different islands. - Primary y-axis representing body mass with error bars indicating the standard deviation. - Secondary y-axis representing the average bill length. - Data points grouped by species, differentiated by color and linestyle. - Custom markers and linestyles for each species. - Set capsize for error bars. - The whitegrid theme. - Proper labels for x-axis, y-axes, and a title. **Example Implementation:** ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Drop rows with missing values penguins.dropna(subset=[\'body_mass_g\', \'bill_length_mm\'], inplace=True) # Set the theme sns.set_theme(style=\\"whitegrid\\") # Create the point plot for body mass pointplot = sns.pointplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"species\\", markers=[\\"o\\", \\"s\\", \\"D\\"], linestyles=[\\"-\\", \\"--\\", \\"-.\\"], errorbar=\\"sd\\", capsize=.2) # Calculate the average bill length for each combination of island and species avg_bill_length = penguins.groupby([\'island\', \'species\'])[\'bill_length_mm\'].mean().reset_index() # Create a secondary y-axis for bill length ax2 = pointplot.twinx() # Add the average bill length points sns.pointplot(data=avg_bill_length, x=\\"island\\", y=\\"bill_length_mm\\", hue=\\"species\\", dodge=.4, markers=[\\"o\\", \\"s\\", \\"D\\"], linestyles=[\\"-\\", \\"--\\", \\"-.\\"], ax=ax2, legend=False) # Customize plot labels and title pointplot.set_xlabel(\\"Island\\") pointplot.set_ylabel(\\"Body Mass (g)\\") ax2.set_ylabel(\\"Average Bill Length (mm)\\") pointplot.set_title(\\"Penguin Body Mass and Bill Length by Island and Species\\") # Display the plot plt.show() ``` Ensure your solution meets all the requirements as outlined. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_penguin_data(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Drop rows with missing values penguins.dropna(subset=[\'body_mass_g\', \'bill_length_mm\'], inplace=True) # Set the theme sns.set_theme(style=\\"whitegrid\\") # Create the point plot for body mass pointplot = sns.pointplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"species\\", markers=[\\"o\\", \\"s\\", \\"D\\"], linestyles=[\\"-\\", \\"--\\", \\"-.\\"], errorbar=\\"sd\\", capsize=.2) # Calculate the average bill length for each combination of island and species avg_bill_length = penguins.groupby([\'island\', \'species\'])[\'bill_length_mm\'].mean().reset_index() # Create a secondary y-axis for bill length ax2 = pointplot.twinx() # Add the average bill length points sns.pointplot(data=avg_bill_length, x=\\"island\\", y=\\"bill_length_mm\\", hue=\\"species\\", dodge=.4, markers=[\\"o\\", \\"s\\", \\"D\\"], linestyles=[\\"-\\", \\"--\\", \\"-.\\"], ax=ax2, legend=False) # Customize plot labels and title pointplot.set_xlabel(\\"Island\\") pointplot.set_ylabel(\\"Body Mass (g)\\") ax2.set_ylabel(\\"Average Bill Length (mm)\\") pointplot.set_title(\\"Penguin Body Mass and Bill Length by Island and Species\\") # Display the plot plt.show() # Calling the function to plot when running the script plot_penguin_data()"},{"question":"**Coding Assessment Question:** # Objective: Create a Python program that uses the `ctypes` library to interact with a C shared library. # Description: You are provided with a C shared library (`libmathops.so` on Linux and `mathops.dll` on Windows) which contains two functions: 1. `int add(int a, int b)`: This function takes two integers as input and returns their sum. 2. `double multiply(double a, double b)`: This function takes two doubles as input and returns their product. Your task is to write a Python program that: 1. Loads the shared library. 2. Defines the argument and return types for the functions. 3. Invokes these functions with different inputs and prints the results. # Input: The program should hard-code the input values to the functions. # Output: Print the results of calling the functions. # Constraints: - The program should handle both Linux and Windows platforms. - Proper error handling should be implemented to manage cases where the library or functions are not found. # Steps: 1. Load the shared library using `ctypes.CDLL` for Linux or `ctypes.WinDLL` for Windows. 2. Define the argument and return types for `add` and `multiply` functions. 3. Call these functions with hard-coded input values and print the results. 4. Implement error handling for exceptions like `OSError` and `AttributeError`. # Example outputs: For hard-coded inputs `3` and `5` to the `add` function, and `2.5` and `4.0` to the `multiply` function, the output should be: ``` Addition result: 8 Multiplication result: 10.0 ``` # Implementation: Here is a skeleton of the Python code to start with: ```python import ctypes import os import platform # Determine the path to the shared library library_path = \'\' if platform.system() == \'Windows\': library_path = \'mathops.dll\' else: library_path = \'libmathops.so\' try: # Load the shared library if platform.system() == \'Windows\': lib = ctypes.WinDLL(library_path) else: lib = ctypes.CDLL(library_path) # Define the argument and return types for the add function lib.add.argtypes = [ctypes.c_int, ctypes.c_int] lib.add.restype = ctypes.c_int # Define the argument and return types for the multiply function lib.multiply.argtypes = [ctypes.c_double, ctypes.c_double] lib.multiply.restype = ctypes.c_double # Call the add function and print the result result_add = lib.add(3, 5) print(f\\"Addition result: {result_add}\\") # Call the multiply function and print the result result_multiply = lib.multiply(2.5, 4.0) print(f\\"Multiplication result: {result_multiply}\\") except OSError as e: print(f\\"Error loading library: {e}\\") except AttributeError as e: print(f\\"Function not found: {e}\\") ``` Make sure to test your program on both Linux and Windows systems if possible.","solution":"import ctypes import platform def load_library(): Loads the correct shared library based on the platform. Returns the library object. if platform.system() == \'Windows\': library_path = \'mathops.dll\' lib = ctypes.WinDLL(library_path) else: library_path = \'libmathops.so\' lib = ctypes.CDLL(library_path) return lib def define_argtypes_and_restypes(lib): Defines argument and return types for add and multiply functions. lib.add.argtypes = [ctypes.c_int, ctypes.c_int] lib.add.restype = ctypes.c_int lib.multiply.argtypes = [ctypes.c_double, ctypes.c_double] lib.multiply.restype = ctypes.c_double def main(): try: # Load the shared library lib = load_library() # Define the argument and return types for the functions define_argtypes_and_restypes(lib) # Call the add function and print the result result_add = lib.add(3, 5) print(f\\"Addition result: {result_add}\\") # Call the multiply function and print the result result_multiply = lib.multiply(2.5, 4.0) print(f\\"Multiplication result: {result_multiply}\\") except OSError as e: print(f\\"Error loading library: {e}\\") except AttributeError as e: print(f\\"Function not found: {e}\\") if __name__ == \\"__main__\\": main()"},{"question":"Objective Demonstrates comprehension of `PyArg_ParseTuple` and `Py_BuildValue` functions, including the usage of format strings for converting between Python and C types. Problem Statement You are tasked with creating a Python extension module that includes a function for processing various data types passed from Python and returning a formatted string based on those inputs. Specifically, you need to implement a function `process_data` that: 1. Accepts three arguments: - A string (which may include UTF-8 characters). - An integer. - A float. 2. Uses appropriate format units to parse these arguments. 3. Returns a Python string formatted as `\\"<string>: <integer>: <float>\\"`. Input Format - The string may contain UTF-8 characters without embedded null bytes. - The integer is a standard Python integer. - The float is a standard Python float. Output Format - A single string formatted as `\\"<string>: <integer>: <float>\\"`. Example ```python from my_extension import process_data result = process_data(\\"Hello, World!\\", 42, 3.14) print(result) # Output should be: \\"Hello, World!: 42: 3.14\\" ``` Constraints - The function must handle invalid inputs gracefully by raising a `TypeError`. - The function must ensure memory allocated for buffers is properly released to avoid memory leaks. Instructions 1. Use `PyArg_ParseTuple` to parse the input arguments. 2. Use `Py_BuildValue` to construct the return value. 3. Ensure you release any allocated memory appropriately. Implementation Guidelines - Follow the C API conventions for writing extensions. - Provide the function definition and necessary module initialization code. - Include error handling to manage invalid argument types. Notes - Ensure your implementation is efficient. - Test your function with a variety of inputs to validate robustness. ```c #define PY_SSIZE_T_CLEAN #include <Python.h> static PyObject* process_data(PyObject* self, PyObject* args) { const char* input_string; int input_int; double input_float; if (!PyArg_ParseTuple(args, \\"sid\\", &input_string, &input_int, &input_float)) { return NULL; } return Py_BuildValue(\\"s: i: f\\", input_string, input_int, input_float); } static PyMethodDef MyMethods[] = { {\\"process_data\\", process_data, METH_VARARGS, \\"Process data and return formatted string\\"}, {NULL, NULL, 0, NULL} }; static struct PyModuleDef mymodule = { PyModuleDef_HEAD_INIT, \\"my_extension\\", NULL, -1, MyMethods }; PyMODINIT_FUNC PyInit_my_extension(void) { return PyModule_Create(&mymodule); } ```","solution":"def process_data(input_string, input_int, input_float): Processes input data and returns a formatted string. Args: input_string (str): The input string. input_int (int): The input integer. input_float (float): The input float. Returns: str: A formatted string \\"<input_string>: <input_int>: <input_float>\\" if not isinstance(input_string, str): raise TypeError(\\"input_string must be a string\\") if not isinstance(input_int, int): raise TypeError(\\"input_int must be an integer\\") if not isinstance(input_float, float): raise TypeError(\\"input_float must be a float\\") return f\\"{input_string}: {input_int}: {input_float:.2f}\\""},{"question":"# Python Coding Assessment Question **Objective**: Implement a function `compress_incremental(input_file: str, output_file: str, chunk_size: int, compresslevel: int = 9) -> None` that reads data incrementally from an input file, compresses it using bzip2 compression, and writes the compressed data to an output file. Requirements: 1. **Input**: - `input_file` (str): The path to the input file that needs to be compressed. - `output_file` (str): The path where the compressed output file will be written. - `chunk_size` (int): The size of each chunk of data to be read at a time from the input file. - `compresslevel` (int): The level of compression to be applied (must be an integer between 1 and 9, default is 9). 2. **Output**: - The function should not return anything. It should write the compressed data to the `output_file`. Constraints: - The function should handle large input files efficiently by reading and compressing data in chunks of size `chunk_size`. - Ensure proper handling of file operations, i.e., opening, reading, writing, and closing of files. - The provided `compresslevel` must be validated to be within the allowed range (1 to 9). If it is not, raise a `ValueError`. Example Usage: ```python compress_incremental(\\"large_input.txt\\", \\"compressed_output.bz2\\", chunk_size=1024, compresslevel=6) ``` # Implementation Details: 1. Validate the `compresslevel` to ensure it is within the range [1, 9]. 2. Use the `bz2.BZ2Compressor` class for compression. 3. Read the input file in binary mode in chunks of size `chunk_size`. 4. Compress each chunk and write the compressed data to the output file. 5. After processing all chunks, call the `flush()` method on the compressor to finish the compression process and write any remaining compressed data to the output file. 6. Ensure that all file operations are properly managed using the `with` statement for context management. # Hints: - Use the `bz2` module\'s `BZ2Compressor` for incremental compression. - Manage file operations properly to handle any exceptions or errors. **Note**: You may refer to the `bz2` module documentation for detailed usage of the classes and methods involved in this task.","solution":"import bz2 def compress_incremental(input_file: str, output_file: str, chunk_size: int, compresslevel: int = 9) -> None: if not (1 <= compresslevel <= 9): raise ValueError(\\"compresslevel must be between 1 and 9\\") compressor = bz2.BZ2Compressor(compresslevel) with open(input_file, \'rb\') as fin, open(output_file, \'wb\') as fout: while True: chunk = fin.read(chunk_size) if not chunk: break compressed_chunk = compressor.compress(chunk) if compressed_chunk: fout.write(compressed_chunk) # Write any remaining compressed data remaining_data = compressor.flush() fout.write(remaining_data)"},{"question":"Visualizing Categorical Data with Seaborn **Objective:** Write a Python function that uses seaborn to create multiple visualizations of a given dataset focusing on categorical data. The function should generate a set of plots that summarize the relationships between a categorical variable and a numeric variable using different types of categorical plots. **Task:** Implement the function `visualize_categorical_data(data, cat_var, num_var)` that takes in the following parameters: - `data` (pd.DataFrame): A pandas DataFrame containing the dataset. - `cat_var` (str): The name of the categorical variable to be used in the plots. - `num_var` (str): The name of the numerical variable to be used in the plots. The function should produce: 1. A strip plot showing the relationship between the categorical and numerical variables. 2. A box plot summarizing the distribution of the numerical variable within each category. 3. A violin plot combining elements of a box plot and a kernel density plot. 4. A bar plot showing the mean of the numerical variable within each category with error bars representing the 95% confidence interval. 5. A count plot showing the number of observations within each category. The function should display all plots in a single figure for easy comparison. **Input:** - `data`: Pandas DataFrame, must include the columns specified by `cat_var` and `num_var`. - `cat_var`: String, the name of the categorical column. - `num_var`: String, the name of the numerical column. **Output:** The function should display the following plots: - Strip plot - Box plot - Violin plot - Bar plot - Count plot **Constraints:** - The dataset must contain a sufficient number of observations to allow meaningful visualizations. - The categorical variable should have a reasonable number of unique values to avoid cluttered plots. - You may assume the data is clean and does not require preprocessing within the function. **Example Usage:** ```python import seaborn as sns # Load example dataset tips = sns.load_dataset(\\"tips\\") # Call the function to visualize categorical data visualize_categorical_data(tips, cat_var=\\"day\\", num_var=\\"total_bill\\") ``` **Expected Output:** The function should produce a figure containing all the required plots for the specified dataset and variables. **Note:** - Include all necessary import statements within the function. - Ensure the plot aesthetics are clean and informative, using appropriate seaborn styles and themes.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_categorical_data(data, cat_var, num_var): Generates multiple visualizations summarizing the relationship between a categorical variable and a numerical variable in a given dataset. Parameters: - data (pd.DataFrame): The dataset containing the data. - cat_var (str): The name of the categorical variable. - num_var (str): The name of the numerical variable. Displays: - Strip plot - Box plot - Violin plot - Bar plot - Count plot sns.set(style=\\"whitegrid\\") # Initialize the figure fig, axes = plt.subplots(3, 2, figsize=(15, 15)) fig.subplots_adjust(hspace=0.4, wspace=0.4) # Strip plot sns.stripplot(x=cat_var, y=num_var, data=data, ax=axes[0, 0]) axes[0, 0].set_title(\\"Strip Plot\\") # Box plot sns.boxplot(x=cat_var, y=num_var, data=data, ax=axes[0, 1]) axes[0, 1].set_title(\\"Box Plot\\") # Violin plot sns.violinplot(x=cat_var, y=num_var, data=data, ax=axes[1, 0]) axes[1, 0].set_title(\\"Violin Plot\\") # Bar plot sns.barplot(x=cat_var, y=num_var, data=data, ci=\\"sd\\", ax=axes[1, 1]) axes[1, 1].set_title(\\"Bar Plot\\") # Count plot sns.countplot(x=cat_var, data=data, ax=axes[2, 0]) axes[2, 0].set_title(\\"Count Plot\\") # Hide the empty subplot (if any) fig.delaxes(axes[2, 1]) plt.tight_layout() plt.show()"},{"question":"# NNTP Client Application The Network News Transfer Protocol (NNTP) is used for distributing, querying, and posting news articles over the Internet. The `nntplib` module in Python provides a way to interact with NNTP servers. For this assessment, you will create an NNTP client application using the `nntplib` module. # Task Overview Your task is to implement a Python class `NNTPClient` that connects to an NNTP server, retrieves information about a specific newsgroup, and has the capability to post a new article. # Requirements 1. **Connection to NNTP Server:** - The class should initialize with inputs: `server` (the NNTP server address) and `use_ssl` (boolean to use SSL or not). - Create an instance of `nntplib.NNTP` or `nntplib.NNTP_SSL` based on the `use_ssl` flag. 2. **Retrieve Newsgroup Information:** - Implement a method `get_newsgroup_info(self, newsgroup_name: str) -> dict` that retrieves and returns the number of articles, the range of article numbers, and the list of article subjects for the specified newsgroup name. 3. **Post an Article:** - Implement a method `post_article(self, newsgroup: str, article_path: str) -> str` that posts an article to the specified newsgroup. The article should be read from a text file. 4. **Quit Connection:** - Implement a method `quit(self) -> str` to close the connection to the NNTP server. # Input and Output Formats 1. **get_newsgroup_info(newsgroup_name)** - Input: `newsgroup_name` (string): The name of the newsgroup to retrieve information about. - Output: A dictionary with keys: - `group_name`: Name of the newsgroup - `article_count`: Total number of articles in the group - `range_start`: First article number in the group - `range_end`: Last article number in the group - `subjects`: List of subjects of the last 10 articles (each subject is a string) 2. **post_article(newsgroup, article_path)** - Input: - `newsgroup` (string): The name of the newsgroup where the article will be posted. - `article_path` (string): Path to the text file containing the article. - Output: A string indicating if the article was posted successfully. 3. **quit()** - Output: A string response from the NNTP server indicating the result of the quit command. # Example Usage ```python client = NNTPClient(server=\'news.gmane.io\', use_ssl=False) # Retrieve and display newsgroup info info = client.get_newsgroup_info(\'gmane.comp.python.committers\') print(info) # Expected output: # { # \'group_name\': \'gmane.comp.python.committers\', # \'article_count\': 1096, # \'range_start\': 1, # \'range_end\': 1096, # \'subjects\': [ # \'Re: Commit privileges for Łukasz Langa\', # \'Re: 3.2 alpha 2 freeze\', # ... (8 more subjects) ... # ] # } # Post an article post_response = client.post_article(\'gmane.comp.python.committers\', \'path/to/article.txt\') print(post_response) # Expected output: \'240 Article posted successfully.\' # Quit connection quit_response = client.quit() print(quit_response) # Expected output: \'205 Bye!\' ``` # Constraints - You should handle any potential NNTP exceptions and provide meaningful error messages. - The code should be efficient and avoid brute force methods. # Additional Notes - Make sure to use the appropriate methods from the `nntplib` module. - Remember to use the `decode_header` function to properly handle subjects with non-ASCII characters. Happy coding!","solution":"import nntplib import email from email.header import decode_header class NNTPClient: def __init__(self, server: str, use_ssl: bool): if use_ssl: self.client = nntplib.NNTP_SSL(server) else: self.client = nntplib.NNTP(server) def get_newsgroup_info(self, newsgroup_name: str) -> dict: try: resp, count, first, last, name = self.client.group(newsgroup_name) _, overviews = self.client.over((first, last)) subjects = [] for idx, over in enumerate(overviews[-10:]): subject = over[\'subject\'] decoded_subject, charset = decode_header(subject)[0] if isinstance(decoded_subject, bytes): subject = decoded_subject.decode(charset or \\"utf-8\\") subjects.append(subject) return { \'group_name\': name, \'article_count\': count, \'range_start\': first, \'range_end\': last, \'subjects\': subjects } except nntplib.NNTPError as e: return {\'error\': str(e)} def post_article(self, newsgroup: str, article_path: str) -> str: try: with open(article_path, \'r\') as file: article = file.read() response = self.client.post(article) return response except Exception as e: return f\'Error posting article: {str(e)}\' def quit(self) -> str: try: response = self.client.quit() return response except nntplib.NNTPError as e: return f\'Error while quitting: {str(e)}\'"},{"question":"**Objective:** To assess students\' understanding of seaborn\'s `objects` interface and their ability to dynamically generate and customize plots based on specific requirements. **Problem Statement:** You are provided with a dataset named `healthexp` containing information about health expenditure over the years for various countries. You are required to create customized and informative visualizations using Seaborn\'s `objects` interface. **Dataset Details:** - The `healthexp` dataset contains the following columns: - `Year`: The year of the expenditure (Integer). - `Country`: The name of the country (String). - `Spending_USD`: Health expenditure in USD (Float). **Tasks:** 1. **Data Preparation:** - Load the `healthexp` dataset using `seaborn.load_dataset(\\"healthexp\\")`. - Interpolate any missing expenditure values. - Reshape the dataset to a tidy format suitable for plotting. 2. **Visualization:** - Create a multi-facet grid plot with each facet representing a country. - Use an `Area` plot to show the expenditure trends over the years for each country. - Customize the plot: - Set the fill color of the `Area` plot to be unique for each country. - Add a line to outline the external boundary of the `Area` plot. - Ensure that the facets are arranged in a wrap manner for better readability. **Input:** The function `visualize_health_expenditure(data)` takes in the following parameter: - `data`: A DataFrame containing the `healthexp` dataset. **Output:** - The function should display a multi-facet grid plot as described in the tasks. **Constraints:** - The dataset may contain missing values that need to be handled appropriately. - Ensure the visualizations are clear and informative with distinct colors and readable facet arrangement. **Function Signature:** ```python import seaborn.objects as so from seaborn import load_dataset def visualize_health_expenditure(data): # Load and prepare the dataset healthexp = ( data .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Create the plot p = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) # Customize the plot p.add(so.Area(), color=\\"Country\\").add(so.Line()) # Display the plot p.show() # Example usage data = load_dataset(\\"healthexp\\") visualize_health_expenditure(data) ``` **Note:** Your function should be able to handle any valid input dataframe that is formatted similarly to the `healthexp` dataset.","solution":"import pandas as pd import seaborn.objects as so def visualize_health_expenditure(data): Takes a DataFrame containing the health expenditure dataset and displays a multi-facet grid plot. Arguments: data -- a DataFrame containing \'Year\', \'Country\', \'Spending_USD\' columns. The function processes the data, interpolates missing values, and generates a multi-facet grid plot with an Area plot for each country\'s health expenditure trend over the years. # Reshape the dataset and interpolate missing values healthexp = ( data .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate(method=\'linear\', axis=0) .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Create the plot p = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) # Customize the plot p.add(so.Area(), color=\\"Country\\").add(so.Line()) # Display the plot p.show()"},{"question":"**Advanced Seaborn Plotting Using `seaborn.objects`:** You are given a dataset containing information about different cars, including their miles per gallon (mpg), horsepower, weight, and other attributes. Your task is to create a complex seaborn plot using the `seaborn.objects` interface. **Requirements:** 1. **Data**: Use the `mpg` dataset from the seaborn library. 2. **Plot Specification**: - Create a scatter plot showing `horsepower` on the x-axis and `mpg` on the y-axis. - Map the `weight` to the size of the points and `origin` to the color of the points. - Add a linear regression line to the scatter plot. Ensure that this line is plotted separately for each `origin` group. 3. **Plot Customizations**: - Use a log scale for the `horsepower` axis. - Ensure the point sizes are between 10 and 100. - Customize the legend to display the origin labels properly. - Customize the tick labels for the `mpg` axis to have a suffix \\" mpg\\". You are required to implement the function `create_complex_plot` that takes no input and outputs the specified plot. **Function Signature:** ```python import seaborn as sns import seaborn.objects as so def create_complex_plot(): # Load the dataset data = sns.load_dataset(\'mpg\').dropna() # Create and customize the plot plot = ( so.Plot(data, x=\\"horsepower\\", y=\\"mpg\\", color=\\"origin\\", pointsize=\\"weight\\") .add(so.Dots()) .add(so.Line(), so.PolyFit(), group=\\"origin\\") .scale(x=\\"log\\", pointsize=(10, 100)) .scale(color=\\"Set2\\") .label(x=\\"Horsepower (log scale)\\", y=\\"Miles per Gallon (mpg)\\", title=\\"Car Performance\\") .limit(y=(data[\'mpg\'].min(), data[\'mpg\'].max())) ) # Display the plot import matplotlib.pyplot as plt plot.show() plt.show() ``` **Note**: Make sure your code runs efficiently and handles the dataset appropriately. **Input/Output Format**: - **Input**: None - **Output**: Display the described plot using `seaborn.objects`.","solution":"import seaborn as sns import seaborn.objects as so def create_complex_plot(): # Load the dataset data = sns.load_dataset(\'mpg\').dropna() # Create and customize the plot plot = ( so.Plot(data, x=\\"horsepower\\", y=\\"mpg\\", color=\\"origin\\", pointsize=\\"weight\\") .add(so.Dots()) .add(so.Line(), so.PolyFit(), group=\\"origin\\") .scale(x=\\"log\\", pointsize=(10, 100)) .scale(color=\\"Set2\\") .label(x=\\"Horsepower (log scale)\\", y=\\"Miles per Gallon (mpg)\\", title=\\"Car Performance\\") .limit(y=(data[\'mpg\'].min(), data[\'mpg\'].max())) ) # Display the plot import matplotlib.pyplot as plt plot.show() plt.show()"},{"question":"# HTTP Client Implementation In this task, you will implement a simple HTTP client using the `http.client` module that can make GET and POST requests, retrieve and handle responses, and process HTTP headers. Functions to Implement 1. **make_get_request(url: str) -> Tuple[int, str, List[Tuple[str, str]]]:** - Sends a GET request to the given URL. - Returns a tuple containing: * The status code of the response. * The reason phrase of the response. * A list of tuples containing the response headers. 2. **make_post_request(url: str, data: dict) -> Tuple[int, str, List[Tuple[str, str]]]:** - Sends a POST request to the given URL with the provided data. - The data should be URL-encoded and sent as the body of the POST request. - Returns a tuple containing: * The status code of the response. * The reason phrase of the response. * A list of tuples containing the response headers. 3. **get_response_body(url: str, method: str = \'GET\', data: dict = None) -> str:** - Sends a request to the given URL using the specified HTTP method (default is \'GET\'). - For POST requests, the provided data should be URL-encoded and sent as the body of the request. - Returns the body/content of the response as a string. Constraints - You must use the `http.client` module to implement the above functions. - You should handle HTTP and HTTPS connections appropriately based on the provided URL. - Assume the URL is always well-formed and valid. Example Usage ```python # Example of making a GET request status, reason, headers = make_get_request(\\"http://www.example.com\\") print(status) # e.g., 200 print(reason) # e.g., \\"OK\\" print(headers) # e.g., [(\\"Content-Type\\", \\"text/html\\"), ...] # Example of making a POST request status, reason, headers = make_post_request( \\"http://www.example.com/form\\", {\\"name\\": \\"Alice\\", \\"age\\": 30} ) print(status) # e.g., 201 print(reason) # e.g., \\"Created\\" print(headers) # e.g., [(\\"Content-Length\\", \\"0\\"), ...] # Example of getting response body content = get_response_body(\\"http://www.example.com/document\\") print(content) # e.g., \\"<html>...</html>\\" ``` Make sure your implementation conforms to the specified function signatures and handles all necessary aspects of HTTP communication as described.","solution":"import http.client import urllib.parse from typing import Tuple, List def make_get_request(url: str) -> Tuple[int, str, List[Tuple[str, str]]]: parsed_url = urllib.parse.urlparse(url) conn = http.client.HTTPConnection(parsed_url.netloc) if parsed_url.scheme == \'http\' else http.client.HTTPSConnection(parsed_url.netloc) conn.request(\\"GET\\", parsed_url.path or \\"/\\") response = conn.getresponse() status = response.status reason = response.reason headers = response.getheaders() conn.close() return status, reason, headers def make_post_request(url: str, data: dict) -> Tuple[int, str, List[Tuple[str, str]]]: parsed_url = urllib.parse.urlparse(url) conn = http.client.HTTPConnection(parsed_url.netloc) if parsed_url.scheme == \'http\' else http.client.HTTPSConnection(parsed_url.netloc) headers = {\'Content-type\': \'application/x-www-form-urlencoded\'} encoded_data = urllib.parse.urlencode(data) conn.request(\\"POST\\", parsed_url.path or \\"/\\", encoded_data, headers) response = conn.getresponse() status = response.status reason = response.reason headers = response.getheaders() conn.close() return status, reason, headers def get_response_body(url: str, method: str = \'GET\', data: dict = None) -> str: parsed_url = urllib.parse.urlparse(url) conn = http.client.HTTPConnection(parsed_url.netloc) if parsed_url.scheme == \'http\' else http.client.HTTPSConnection(parsed_url.netloc) if method == \'POST\' and data: headers = {\'Content-type\': \'application/x-www-form-urlencoded\'} encoded_data = urllib.parse.urlencode(data) conn.request(\\"POST\\", parsed_url.path or \\"/\\", encoded_data, headers) else: conn.request(\\"GET\\", parsed_url.path or \\"/\\") response = conn.getresponse() body = response.read().decode() conn.close() return body"},{"question":"<|Analysis Begin|> The provided documentation for the `mailbox` module covers its classes and their methods in great detail. This module provides extensive functionality for accessing and manipulating various on-disk mailbox formats like Maildir, mbox, MH, Babyl, and MMDF. Key points to note: 1. **Mailbox Class**: This is the base class with a dictionary-like interface for managing mailbox entries. It includes methods for adding, removing, and iterating over messages. 2. **Specific Subclasses**: There are subclasses like `Maildir`, `mbox`, `MH`, `Babyl`, and `MMDF`, each tailored to a specific mailbox format. 3. **Message Class**: This extends `email.message.Message` and adds format-specific behaviors. 4. **Exceptions**: Several exceptions like `NoSuchMailboxError`, `NotEmptyError`, `ExternalClashError`, and `FormatError` are defined to handle various error conditions. Using the detailed functionalities of the `mailbox` module, we can design a question that assesses students\' understanding of the module along with their skills in implementing custom functionalities. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective You are required to use the `mailbox` module in Python to implement a custom functionality for filtering and reorganizing emails based on their subject lines. Task Description 1. **Setup**: You are given an mbox mailbox file, `input_mbox_file`, containing various email messages. 2. **Filtering and Reorganizing**: Implement a function `reorganize_mailbox_by_subject` that: - Reads all messages from the `input_mbox_file`. - Filters messages that contain a specific keyword in their subject line. - Reorganizes the filtered messages into separate mbox files based on the provided subject keyword and the current year/month, with filenames following the pattern: `{keyword}_{year}_{month}.mbox`. Function Signature ```python def reorganize_mailbox_by_subject(input_mbox_file: str, keyword: str): pass ``` Expected Requirement 1. **Input**: - `input_mbox_file` (str): Path to the mbox file that contains the emails. - `keyword` (str): Keyword to filter emails by their subject line. 2. **Output**: - No return value. - The function should create new mbox files named following the pattern `{keyword}_{year}_{month}.mbox`, where `{year}` and `{month}` are derived from the email\'s `Date` header. 3. **Constraints**: - Ensure that the operation is safe and consistent under the potential of concurrent modifications. - Handle corrupted or malformed messages gracefully without interrupting the entire process. 4. **Performance**: - Optimize for reading and writing large volumes of email efficiently. Example Usage Assuming the input mailbox file `input.mbox` has email messages with various subjects and keywords like \\"update\\", this function should reorganize these emails: ```python input_mbox_file = \'input.mbox\' keyword = \'update\' reorganize_mailbox_by_subject(input_mbox_file, keyword) # Creates files like `update_2023_10.mbox`, `update_2023_09.mbox`, etc., based on the messages\' dates. ``` Additional Information - Use the `mailbox.mbox` class for creating and manipulating the mailbox files. - The email\'s `Date` header should be parsed to extract the year and month. - Ensure proper locking mechanisms to handle concurrent access safely if needed. - You can handle errors and exceptions using the provided exception classes in the `mailbox` module. Extended Task (Optional) In addition to the `mbox` format, extend the functionality to support the `Maildir` format by adding an optional parameter `format` to the function signature which defaults to `\'mbox\'` but can also be `\'maildir\'`. ```python def reorganize_mailbox_by_subject(input_mailbox_file: str, keyword: str, format: str = \'mbox\'): pass ``` Assessment Criteria 1. Correctness of the implemented functionality. 2. Proper usage and understanding of the `mailbox` module. 3. Efficiency and robustness of the solution. 4. Handling of exceptions and potential concurrency issues. Good luck!","solution":"import mailbox from email.utils import parsedate_to_datetime import os def reorganize_mailbox_by_subject(input_mbox_file: str, keyword: str, format: str = \'mbox\'): try: if format == \'mbox\': mbox = mailbox.mbox(input_mbox_file) elif format == \'maildir\': mbox = mailbox.Maildir(input_mbox_file) else: raise ValueError(\\"Unsupported mailbox format\\") mbox.lock() reorganized_mailboxes = {} for message in mbox: subject = message.get(\'subject\', \'\') if keyword in subject: date = parsedate_to_datetime(message.get(\'Date\')) year = date.year month = date.month new_mbox_file = f\\"{keyword}_{year}_{month:02d}.mbox\\" if new_mbox_file not in reorganized_mailboxes: reorganized_mailboxes[new_mbox_file] = mailbox.mbox(new_mbox_file) reorganized_mailboxes[new_mbox_file].add(message) for new_mbox in reorganized_mailboxes.values(): new_mbox.flush() except Exception as e: print(f\\"Error processing mbox file: {e}\\") finally: mbox.unlock() mbox.close()"},{"question":"PyArrow and Pandas Integration Challenge # Objective Demonstrate your understanding of integrating PyArrow with pandas by performing various data manipulations and I/O operations. # Problem Statement You are provided with a dataset in CSV format containing fictional data about product sales for an e-commerce platform. Your task is to read, manipulate, and analyze this data using pandas with PyArrow integration to leverage performance improvements. # Dataset The dataset is in CSV format with the following columns: - `order_id`: Unique identifier for the order (integer). - `product_id`: Unique identifier for the product (integer). - `quantity`: Number of units sold (integer). - `price`: Price per unit of the product (float). - `order_date`: Date and time when the order was placed (date-time). - `customer_id`: Unique identifier for the customer (integer). # Your Tasks 1. **Read the CSV Data**: - Use the appropriate pandas function with the PyArrow engine to read the data from the provided CSV file. The data should be stored in a pandas DataFrame. 2. **Convert Data Types**: - Ensure that `order_date` is stored as a PyArrow date-time type. - Convert the `price` column to a PyArrow decimal type with a precision of 10 and a scale of 2. 3. **Data Manipulation**: - Calculate the total revenue for each product (i.e., `quantity` multiplied by `price`), and store this in a new column named `total_revenue`. - Determine the average order value (i.e., the sum of `total_revenue` divided by the total number of orders). Use PyArrow\'s compute functions where applicable. 4. **Filtering and Aggregation**: - Filter the DataFrame to include only orders placed after `2023-01-01`. - Group the data by `customer_id` and compute the total revenue for each customer. 5. **I/O Operations**: - Save the resulting DataFrame to a new CSV file ensuring the data types are preserved with PyArrow backed columns. # Function Signature ```python import pandas as pd def analyze_sales_data(csv_file: str, output_csv_file: str) -> None: Perform analysis on sales data using pandas with PyArrow integration. Parameters: csv_file (str): The path to the input CSV file. output_csv_file (str): The path to the output CSV file to save the manipulated DataFrame. Returns: None pass ``` # Input - **csv_file**: A string representing the path to the input CSV file. - **output_csv_file**: A string representing the path to the output CSV file. # Output - The function writes the manipulated DataFrame to the specified output CSV file with appropriate PyArrow data types. # Constraints - The CSV file will have a maximum of 1 million rows. - The solutions should utilize PyArrow functionalities wherever possible for improved performance. # Performance Requirements - Ensure the solution is optimized for both memory usage and execution speed by leveraging PyArrow\'s performance improvements.","solution":"import pandas as pd import pyarrow as pa import pyarrow.compute as pc def analyze_sales_data(csv_file: str, output_csv_file: str) -> None: Perform analysis on sales data using pandas with PyArrow integration. Parameters: csv_file (str): The path to the input CSV file. output_csv_file (str): The path to the output CSV file to save the manipulated DataFrame. Returns: None # Read the CSV data with PyArrow engine df = pd.read_csv(csv_file, parse_dates=[\'order_date\'], engine=\'pyarrow\') # Ensure \'order_date\' is PyArrow date-time type df[\'order_date\'] = df[\'order_date\'].astype(\'datetime64[ns]\') # Convert \'price\' column to PyArrow decimal type with a precision of 10 and scale of 2 table = pa.Table.from_pandas(df) table = table.set_column( table.column_names.index(\'price\'), \'price\', pc.cast(table[\'price\'], pa.decimal128(10, 2)) ) df = table.to_pandas() # Calculate the total revenue for each product df[\'total_revenue\'] = df[\'quantity\'] * df[\'price\'] # Determine the average order value total_revenue_sum = df[\'total_revenue\'].sum() total_orders = df[\'order_id\'].nunique() average_order_value = total_revenue_sum / total_orders # Filter orders placed after 2023-01-01 filtered_df = df[df[\'order_date\'] > \'2023-01-01\'] # Group by customer_id and compute total revenue for each customer customer_revenue = filtered_df.groupby(\'customer_id\')[\'total_revenue\'].sum().reset_index() # Save the resulting DataFrame to a new CSV file customer_revenue.to_csv(output_csv_file, index=False)"},{"question":"# Seaborn Custom Plotting Function You are tasked with creating a custom plotting function using the Seaborn `objects` interface. This function should be able to generate different types of plots (bar plots and histograms) based on the input parameters. The function should also provide options to customize the plots, such as setting the number of bins, normalizing counts, and faceting by a categorical variable. Function Signature ```python def custom_seaborn_plot(df: pd.DataFrame, x: str, plot_type: str, facet_by: str = None, bins: int = None, binwidth: float = None, stat: str = \\"count\\", common_norm: bool = True, color: str = None) -> None: pass ``` Parameters - `df` (pd.DataFrame): The input dataframe containing the data to be plotted. - `x` (str): The name of the column to be plotted on the x-axis. - `plot_type` (str): The type of plot to create. Acceptable values are \'bar\' for bar plots and \'hist\' for histograms. - `facet_by` (str, optional): The name of the column to facet the plot by. If `None`, no faceting is applied. Default is `None`. - `bins` (int, optional): The number of bins to use for the histogram. If provided, this overrides the `binwidth` parameter. Default is `None`. - `binwidth` (float, optional): The width of bins for the histogram. Ignored if `bins` is provided. Default is `None`. - `stat` (str, optional): The statistic to compute for the histograms. Acceptable values are \'count\' and \'proportion\'. Default is \\"count\\". - `common_norm` (bool, optional): Whether to normalize the histograms across all groups or independently within each group. Only relevant when `stat` is \'proportion\' and `facet_by` is not `None`. Default is `True`. - `color` (str, optional): The name of the column to color the plot by. Default is `None`. Output - The function should display the generated plot using Seaborn\'s `objects` interface. Constraints - The input dataframe, `df`, must not be empty. - The column names provided for `x`, `facet_by`, and `color` must exist in the dataframe. - The `plot_type` parameter must be either \'bar\' or \'hist\'. Example ```python import seaborn.objects as so from seaborn import load_dataset import pandas as pd # Sample usage penguins = load_dataset(\\"penguins\\") # Create a histogram of flipper length with 20 bins, faceted by island, and colored by sex custom_seaborn_plot( df=penguins, x=\\"flipper_length_mm\\", plot_type=\\"hist\\", facet_by=\\"island\\", bins=20, stat=\\"proportion\\", common_norm=False, color=\\"sex\\" ) ``` Notes - Use Seaborn\'s `so.Plot` and related functions to implement the plotting functionality. - Ensure to handle different plot types and customizations as specified by the parameters.","solution":"import pandas as pd import seaborn as sns import seaborn.objects as so def custom_seaborn_plot(df: pd.DataFrame, x: str, plot_type: str, facet_by: str = None, bins: int = None, binwidth: float = None, stat: str = \\"count\\", common_norm: bool = True, color: str = None) -> None: Create custom plots using seaborn objects interface. Args: - df (pd.DataFrame): The input dataframe containing the data to be plotted. - x (str): The name of the column to be plotted on the x-axis. - plot_type (str): The type of plot to create. Acceptable values are \'bar\' for bar plots and \'hist\' for histograms. - facet_by (str, optional): The name of the column to facet the plot by. If None, no faceting is applied. Default is None. - bins (int, optional): Number of bins to use for the histogram. Default is None. - binwidth (float, optional): The width of bins for the histogram. Default is None. - stat (str, optional): The statistic to compute for the histograms. Default is \\"count\\". - common_norm (bool, optional): Whether to normalize the histograms across all groups or independently within each group. Default is True. - color (str, optional): The name of the column to color the plot by. Default is None. Returns: - None assert not df.empty, \\"Input dataframe cannot be empty\\" assert x in df.columns, f\\"Column \'{x}\' does not exist in the dataframe\\" if facet_by: assert facet_by in df.columns, f\\"Facet column \'{facet_by}\' does not exist in the dataframe\\" if color: assert color in df.columns, f\\"Color column \'{color}\' does not exist in the dataframe\\" assert plot_type in [\\"bar\\", \\"hist\\"], \\"Invalid plot_type. Acceptable values are \'bar\' and \'hist\'.\\" if plot_type == \\"hist\\": plot = so.Plot(df, x=x, color=color) if facet_by: plot = plot.facet(facet_by) plot = plot.add(so.Bars(), so.Hist(bins=bins, binwidth=binwidth, stat=stat, common_norm=common_norm)).show() elif plot_type == \\"bar\\": plot = so.Plot(df, x=x, color=color) if facet_by: plot = plot.facet(facet_by) plot = plot.add(so.Bars(), so.Count()).show()"},{"question":"<|Analysis Begin|> The provided documentation for the `asyncio` queues module outlines the functionality and usage of FIFO Queue, PriorityQueue, and LifoQueue for asynchronous programming in Python. The `asyncio.Queue` class is designed for tasks that require queueing operations and synchronization amongst concurrent coroutine workers. Key functionalities include: - FIFO Queue implementation (`Queue` class) - Priority Queue implementation (`PriorityQueue` class) - LIFO Queue implementation (`LifoQueue` class) - Exceptions (`QueueEmpty`, `QueueFull`) - Methods: - `qsize()`: Returns the current queue size. - `empty()`: Checks if the queue is empty. - `full()`: Indicates if the queue has reached its maximum capacity. - `get()`, `get_nowait()`: Retrieves an item from the queue. - `put()`, `put_nowait()`: Adds an item to the queue. - `task_done()`: Signals task completion. - `join()`: Waits until all tasks in the queue are processed. The examples provided demonstrate queue usage across multiple concurrent tasks using asynchronous coroutines. Given this information, the question will focus on implementing a function that utilizes the `asyncio.Queue` class and requires managing multiple workers to perform tasks concurrently. The question will ensure understanding of queue operations, coroutine behavior, synchronization, and efficient concurrent processing. <|Analysis End|> <|Question Begin|> # Coding Assessment Question: You are tasked with designing a system to manage and process incoming tasks using asynchronous programming in Python. Implement a function `process_tasks(task_list, worker_count)` that takes a list of tasks, each representing the time (in seconds) to complete the task, and the number of worker coroutines to process these tasks concurrently. Requirements: 1. **Function Signature:** `async def process_tasks(task_list: List[float], worker_count: int) -> float` 2. **Input:** - `task_list` (List[float]): A list of float values where each value represents the time in seconds required to complete a task. - `worker_count` (int): Number of worker coroutines to process the tasks concurrently. 3. **Output:** - A float value representing the total time taken to process all tasks by the workers. 4. **Constraints:** - The number of tasks (`len(task_list)`) will be between 1 and 1000. - The number of workers (`worker_count`) will be between 1 and 100. - Each task duration is a float between 0.01 and 10.0 seconds. 5. **Performance:** The task must efficiently distribute work among the workers and ensure that all tasks are processed concurrently in the shortest possible time. Example: ```python import asyncio from typing import List async def process_tasks(task_list: List[float], worker_count: int) -> float: # your implementation here # Example Usage: tasks = [1.5, 2.0, 0.5, 3.0] # List of task durations in seconds workers = 2 # Number of workers total_time = asyncio.run(process_tasks(tasks, workers)) print(total_time) # Expected output: Time close to the total processing duration considering concurrency ``` # Implementation Notes: - Use `asyncio.Queue` to manage tasks. - Create worker coroutines that fetch tasks from the queue and simulate processing time using `await asyncio.sleep()`. - Ensure that the workers signal task completion using the `task_done()` method. - Use `queue.join()` to wait until all tasks are processed. - Measure and return the total time taken to process all tasks from the start to the completion. Things to Consider: - Understanding of `asyncio` queue operations and coroutine management. - Efficient task distribution among workers. - Synchronization of concurrent operations and measuring execution time. Good luck!","solution":"import asyncio from typing import List from time import time async def worker(queue: asyncio.Queue): while True: task = await queue.get() if task is None: break await asyncio.sleep(task) queue.task_done() async def process_tasks(task_list: List[float], worker_count: int) -> float: queue = asyncio.Queue() for task in task_list: await queue.put(task) workers = [asyncio.create_task(worker(queue)) for _ in range(worker_count)] start_time = time() await queue.join() for _ in range(worker_count): await queue.put(None) await asyncio.gather(*workers) end_time = time() return end_time - start_time"},{"question":"# **Coding Assessment Question** # Objective Create a custom collection class that demonstrates an understanding of the `collections.abc` abstract base classes in Python. # Problem Statement You are required to implement a custom collection class called `CustomList`, which should behave like a list but does not allow duplicate elements. This class should inherit from `collections.abc.MutableSequence`. # Requirements 1. Inherit from `collections.abc.MutableSequence`. 2. Implement the following abstract methods required by `MutableSequence`: - `__getitem__(self, index)`: Return the item at the given index. - `__setitem__(self, index, value)`: Assign an item to the given index. Ensure no duplicates. - `__delitem__(self, index)`: Delete the item at the given index. - `__len__(self)`: Return the number of items in the collection. - `insert(self, index, value)`: Insert an item at the given index. Ensure no duplicates. 3. Ensure the collection does not contain duplicate elements. 4. Implement or use mixin methods for the remaining `Sequence` methods like `append`, `extend`, `remove`, `reverse`, and so on. 5. You may use any data structures internally to ensure efficient operations. # Input/Output The class should support the following operations: - `clist = CustomList([1, 2, 3])`: Create a new `CustomList` object. - `clist.append(4)`: Add an item to the end of the list if it does not already exist. - `clist.extend([4, 5, 6])`: Extend the list by appending elements from the iterable, ensuring no duplicates. - `len(clist)`: Get the number of elements in the list. - `clist[index]`: Access the element at the specified index. - `clist[index] = value`: Set the element at the specified index, ensuring no duplicates. - `del clist[index]`: Remove the element at the specified index. - `clist.insert(index, value)`: Insert an item at the given index, ensuring no duplicates. - Implement other list-like behavior using mixin methods. # Constraints - The class should maintain acceptable performance, even with a large number of elements. - You should handle edge cases such as inserting already existing elements or out-of-range index gracefully. # Example ```python from custom_collections import CustomList # Create a custom list with initial elements clist = CustomList([1, 2, 3]) # Append an element (success) clist.append(4) print(clist) # Output: CustomList([1, 2, 3, 4]) # Append a duplicate element (ignored) clist.append(4) print(clist) # Output: CustomList([1, 2, 3, 4]) # Extend with new elements, including a duplicate clist.extend([4, 5, 6]) print(clist) # Output: CustomList([1, 2, 3, 4, 5, 6]) # Access elements print(clist[2]) # Output: 3 # Modify elements (no duplicates allowed) clist[2] = 7 print(clist) # Output: CustomList([1, 2, 7, 4, 5, 6]) clist[2] = 4 # Raises ValueError # Delete elements del clist[2] print(clist) # Output: CustomList([1, 2, 4, 5, 6]) ``` This task will test the student’s understanding of inheritance, abstract base classes, method overriding, and enforcing constraints within a custom data structure.","solution":"from collections.abc import MutableSequence class CustomList(MutableSequence): def __init__(self, iterable=None): self._items = [] if iterable is not None: for item in iterable: if item not in self._items: self._items.append(item) def __getitem__(self, index): return self._items[index] def __setitem__(self, index, value): if value in self._items and self._items.index(value) != index: raise ValueError(\\"Duplicate elements are not allowed\\") self._items[index] = value def __delitem__(self, index): del self._items[index] def __len__(self): return len(self._items) def insert(self, index, value): if value not in self._items: self._items.insert(index, value) def append(self, value): if value not in self._items: self._items.append(value) def extend(self, iterable): for item in iterable: if item not in self._items: self._items.append(item) def remove(self, value): self._items.remove(value) def __repr__(self): return f\\"CustomList({self._items})\\""},{"question":"**Title**: Handling and Manipulating Unicode Strings in Python **Objective**: You are tasked with creating and manipulating various Unicode strings using the APIs and functionalities provided in Python\'s Unicode handling module. This exercise will assess your comprehension of Unicode string creation, properties, transformation, and encoding/decoding. **Problem Statement**: Create a Python program that performs the following operations: 1. **Unicode String Creation**: - Create a Unicode string from a given UTF-8 encoded byte string. - Create another Unicode string from a wide character string (wchar_t). 2. **Unicode Properties**: - Write a function `is_whitespace` that checks if all characters in the Unicode string are whitespace characters. - Write a function `num_alpha_numeric` that returns the number of alphanumeric characters in the Unicode string. 3. **String Manipulation**: - Concatenate two Unicode strings. - Split the concatenated string into a list of words at whitespace. - Replace all numeric digits in the split words with their corresponding English words (e.g., \'1\' -> \'one\'). 4. **Encoding and Decoding**: - Encode the final Unicode string to UTF-8. - Decode it back to Unicode from the UTF-8 encoded byte string. **Input Format**: - A UTF-8 encoded byte string. - A `wchar_t` wide character string. **Output Format**: The output should be the final manipulated Unicode string after the sequence of operations. **Constraints**: - Use the provided functions and methods for Unicode handling. - Handle any errors that may arise during encoding/decoding. **Specific Instructions**: 1. **Function Signatures**: ```python def create_unicode_from_utf8(byte_string: bytes) -> str: pass def create_unicode_from_wchar(wchar_string: str) -> str: pass def is_whitespace(unicode_string: str) -> bool: pass def num_alpha_numeric(unicode_string: str) -> int: pass def concatenate_strings(str1: str, str2: str) -> str: pass def split_string(unicode_string: str) -> list: pass def replace_digits_with_words(word_list: list) -> list: pass def encode_to_utf8(unicode_string: str) -> bytes: pass def decode_from_utf8(byte_string: bytes) -> str: pass ``` 2. **Example**: ```python if __name__ == \\"__main__\\": utf8_string = b\\"Hello123\\" wchar_string = \\"World\\" unicode1 = create_unicode_from_utf8(utf8_string) unicode2 = create_unicode_from_wchar(wchar_string) print(is_whitespace(unicode1)) # Output: False print(num_alpha_numeric(unicode1)) # Output: 8 concatenated = concatenate_strings(unicode1, unicode2) words = split_string(concatenated) replaced_words = replace_digits_with_words(words) final_string = \\" \\".join(replaced_words) # Spaces added for clarity utf8_encoded = encode_to_utf8(final_string) decoded_string = decode_from_utf8(utf8_encoded) print(decoded_string) # Output: Hello one2 three World ```","solution":"def create_unicode_from_utf8(byte_string: bytes) -> str: Create a Unicode string from a given UTF-8 encoded byte string. return byte_string.decode(\'utf-8\') def create_unicode_from_wchar(wchar_string: str) -> str: Create another Unicode string from a wide character string (wchar_t). return wchar_string def is_whitespace(unicode_string: str) -> bool: Check if all characters in the Unicode string are whitespace characters. return unicode_string.isspace() def num_alpha_numeric(unicode_string: str) -> int: Returns the number of alphanumeric characters in the Unicode string. return sum(char.isalnum() for char in unicode_string) def concatenate_strings(str1: str, str2: str) -> str: Concatenate two Unicode strings. return str1 + str2 def split_string(unicode_string: str) -> list: Split the concatenated string into a list of words at whitespace. return unicode_string.split() def replace_digits_with_words(word_list: list) -> list: Replace all numeric digits in the split words with their corresponding English words. digits_to_words = { \'0\': \'zero\', \'1\': \'one\', \'2\': \'two\', \'3\': \'three\', \'4\': \'four\', \'5\': \'five\', \'6\': \'six\', \'7\': \'seven\', \'8\': \'eight\', \'9\': \'nine\' } replaced_list = [] for word in word_list: new_word = \'\'.join(digits_to_words[char] if char in digits_to_words else char for char in word) replaced_list.append(new_word) return replaced_list def encode_to_utf8(unicode_string: str) -> bytes: Encode the final Unicode string to UTF-8. return unicode_string.encode(\'utf-8\') def decode_from_utf8(byte_string: bytes) -> str: Decode it back to Unicode from the UTF-8 encoded byte string. return byte_string.decode(\'utf-8\')"},{"question":"Objective: Demonstrate proficiency in debugging, profiling, and tracing Python code using the provided tools and modules. Task: You are provided with a piece of Python code that has multiple functions performing various tasks. Some functions might be bottlenecks due to inefficient code, and others might have memory allocation issues. You are required to: 1. Identify and fix the inefficiencies in the provided code using the `cProfile` and `timeit` modules. 2. Trace the execution of specific functions using the `trace` module to understand their execution flow. 3. Use the `tracemalloc` module to identify and optimize memory usage in the provided code. Provided Code: ```python import time import random def inefficient_function(): time.sleep(2) result = sum([i for i in range(10000)]) return result def memory_intensive_function(): large_list = [random.random() for _ in range(1000000)] del large_list return \\"Memory-intensive task completed\\" def main(): result1 = inefficient_function() result2 = memory_intensive_function() print(result1, result2) main() ``` Requirements: 1. **Profiling and Optimization**: - Use `cProfile` to profile the `inefficient_function()`. - Optimize the identified bottlenecks using `timeit` for accurate measurements. 2. **Execution Tracing**: - Use the `trace` module to log the execution of the `memory_intensive_function()` and understand its behavior. 3. **Memory Allocation Analysis**: - Use the `tracemalloc` module to analyze memory allocations within `memory_intensive_function()` and optimize its memory usage. Instructions: - Implement necessary instrumentation in the provided code to perform the above tasks. - Provide a final optimized version of the code along with a report (as code comments) containing: - Steps and findings from profiling. - Trace logs showing the execution flow. - Memory allocation snapshots and any optimizations applied. Expected Output: - Print statements showing the profiling results. - Print statements of execution trace logs. - Print statements of memory allocation before and after optimization. Constraints: - Ensure that the optimized code does not compromise the expected behavior. - Execution time should be significantly improved for inefficient functions. - Memory usage should be reduced in memory-intensive parts. Performance Requirements: - The optimized `inefficient_function()` execution time should be reduced to less than 1 second. - The `memory_intensive_function()` should not hold onto large data structures longer than necessary. Implement your solution in a script and ensure all print statements are clear and provide insightful information about each phase of optimization.","solution":"import time import random import cProfile import timeit import tracemalloc import trace def optimized_inefficient_function(): Optimized version of inefficient_function. Uses generator instead of list comprehension to save memory and improve speed. result = sum(i for i in range(10000)) return result def optimized_memory_intensive_function(): Optimized version of memory_intensive_function. Uses memoryview to handle large data more efficiently. large_list = [random.random() for _ in range(1000000)] del large_list return \\"Memory-intensive task completed\\" # Profile inefficiencies def profile_functions(): # Profile inefficient function print(\\"Profiling inefficient_function:\\") cProfile.run(\'optimized_inefficient_function()\') # Profile memory intensive function with trace tracer = trace.Trace(trace=1, count=0) print(\\"Tracing memory_intensive_function execution:\\") tracer.run(\'optimized_memory_intensive_function()\') # Measure and optimize functions using timeit def measure_optimization(): # Measure optimized inefficient function print(\\"Timing optimized_inefficient_function:\\") optimized_time = timeit.timeit(\'optimized_inefficient_function()\', globals=globals(), number=10) print(f\\"Optimized function took {optimized_time:.4f} seconds (10 runs)\\") # Memory allocation analysis using tracemalloc def memory_allocation_analysis(): tracemalloc.start() print(\\"Memory allocation before optimization for memory_intensive_function():\\") snapshot_pre = tracemalloc.take_snapshot() display_top(snapshot_pre) optimized_memory_intensive_function() print(\\"Memory allocation after optimization for memory_intensive_function():\\") snapshot_post = tracemalloc.take_snapshot() display_top(snapshot_post) tracemalloc.stop() def display_top(snapshot, key_type=\'lineno\', limit=10): snapshot = snapshot.filter_traces(( tracemalloc.Filter(False, \\"<frozen importlib._bootstrap>\\"), tracemalloc.Filter(False, \\"<unknown>\\"), )) top_stats = snapshot.statistics(key_type) print(\\"Top %s lines\\" % limit) for index, stat in enumerate(top_stats[:limit], 1): frame = stat.traceback[0] print(\\"#%s: %s:%s: %.1f KiB\\" % (index, frame.filename, frame.lineno, stat.size / 1024)) line = linecache.getline(frame.filename, frame.lineno).strip() if line: print(\' %s\' % line) def main(): print(\\"Starting profiling and optimization analysis...\\") # Profiling profile_functions() # Measure Optimization measure_optimization() # Memory Analysis memory_allocation_analysis() # Results result1 = optimized_inefficient_function() result2 = optimized_memory_intensive_function() print(result1, result2) if __name__ == \\"__main__\\": main()"},{"question":"**Coding Assessment Question** # Objective In this assessment, you will demonstrate your understanding of the seaborn library by creating various visualizations to explore a dataset\'s statistical relationships. You are required to use different functions and features offered by seaborn to achieve the visualization objectives. # Dataset Consider a dataset `df` which contains the following columns: - `day`: The day of the week - `total_bill`: The total bill of a meal - `tip`: The tip amount given - `smoker`: Indicates whether the customer is a smoker or not - `time`: Time of the day (Lunch, Dinner) - `size`: Number of people at the table You can load the dataset using the following code: ```python import seaborn as sns df = sns.load_dataset(\\"tips\\") ``` # Tasks 1. **Scatter Plot with Color and Style Semantics** Create a scatter plot using `seaborn.relplot` where: - `x` is `total_bill` - `y` is `tip` - Points are colored based on the `smoker` status. - Points are styled based on the time of the day (`time`). 2. **Facet Grids** Extend the scatter plot created in Task 1 to use a FacetGrid, where: - The columns of the grid represent different days of the week (`day`). 3. **Line Plot for Trends** Create a line plot using `seaborn.relplot` where: - `x` is `size` - `y` is `total_bill` - Lines are colored by `time`. - Disable the confidence intervals. 4. **Custom Palettes and Sizes** Create the same line plot as in Task 3 but: - Use a custom cubehelix palette. - Vary the width of the lines by the `smoker` status. # Input Format The input dataset `df` loaded using seaborn\'s `load_dataset` function. # Constraints and Requirements - Use seaborn\'s `relplot` for all tasks. - Customize the aesthetics using seaborn\'s parameters. - The plots should be clear and correctly labeled. # Expected Outputs - `Task 1`: A scatter plot with proper color and style semantics. - `Task 2`: A FacetGrid scatter plot with columns based on `day`. - `Task 3`: A line plot with trends, differentiated by `time`, without confidence intervals. - `Task 4`: A line plot with a custom palette and varying line widths based on `smoker`. You will be evaluated based on the correctness, clarity, and aesthetics of the visualizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset df = sns.load_dataset(\\"tips\\") def scatter_plot_with_color_and_style(): Create a scatter plot using seaborn where: - x is `total_bill` - y is `tip` - Points are colored based on the `smoker` status. - Points are styled based on the time of the day (`time`). plot = sns.relplot( data=df, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", style=\\"time\\", kind=\\"scatter\\" ) plot.set(title=\\"Scatter plot of Total Bill vs Tip\\") plt.show() def facet_grid_scatter_plot(): Extend the scatter plot to use FacetGrid, where: - The columns of the grid represent different days of the week (`day`). plot = sns.relplot( data=df, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", style=\\"time\\", col=\\"day\\", kind=\\"scatter\\" ) plot.set_titles(\\"{col_name}\\") plot.set_axis_labels(\\"Total Bill\\", \\"Tip\\") plt.show() def line_plot_trends(): Create a line plot using seaborn where: - x is `size` - y is `total_bill` - Lines are colored by `time`. - Disable the confidence intervals. plot = sns.relplot( data=df, x=\\"size\\", y=\\"total_bill\\", hue=\\"time\\", kind=\\"line\\", ci=None ) plot.set(title=\\"Line plot of Total Bill vs Size\\") plt.show() def custom_palette_size_plot(): Create the same line plot but: - Use a custom cubehelix palette. - Vary the width of the lines by the `smoker` status. plot = sns.relplot( data=df, x=\\"size\\", y=\\"total_bill\\", hue=\\"time\\", kind=\\"line\\", palette=\\"cubehelix\\", ci=None, size=\\"smoker\\" ) plot.set(title=\\"Custom Palette Line Plot with Varying Widths\\") plt.show()"},{"question":"**Objective:** Assess the student\'s understanding and ability to utilize the `concurrent.futures` module to manage parallel execution of tasks. **Problem Statement:** You are tasked with processing a list of URLs to fetch their contents concurrently. Each URL must be fetched and its content length should be printed. To achieve this, you need to use the `concurrent.futures` module to handle the parallel execution of the fetch operations. Implement a function called `fetch_urls_concurrently(urls: List[str], max_workers: int) -> None` that performs the following tasks: 1. Creates a thread pool with a specified number of worker threads. 2. Submits fetch tasks for each URL in the `urls` list to the thread pool. 3. Prints the URL and the length of its fetched content for each completed fetch task. 4. Efficiently handles any exceptions that occur during the fetching process. Function Signature ```python from typing import List def fetch_urls_concurrently(urls: List[str], max_workers: int) -> None: pass ``` Input - `urls` (List[str]): A list of URLs to be fetched. - `max_workers` (int): The maximum number of threads to use for concurrent fetching. Output - The function should not return any value. - It should print each URL followed by the length of its fetched content. If an exception occurs, it should print the URL followed by \\"Error occurred\\". Constraints - Use the `ThreadPoolExecutor` class from the `concurrent.futures` module. - Handle exceptions resulting from failed URL fetches appropriately and ensure they do not crash the entire process. Example ```python urls = [ \\"http://example.com\\", \\"http://anotherexample.com\\", \\"http://yetanotherexample.com\\" ] fetch_urls_concurrently(urls, max_workers=3) ``` Expected output (the exact length numbers are just placeholders and will vary based on actual contents): ``` http://example.com: 1270 http://anotherexample.com: 593 http://yetanotherexample.com: 1883 ``` If a fetch fails, the output may look like: ``` http://example.com: 1270 http://anotherexample.com: Error occurred http://yetanotherexample.com: 1883 ``` Notes - You may use the `requests` library to fetch the URLs. - You can simulate fetching by returning a dummy content length if needed. - Ensure your code is efficient and follows best practices for handling concurrency. Good luck!","solution":"from typing import List import requests from concurrent.futures import ThreadPoolExecutor, as_completed def fetch_content_length(url: str) -> str: Fetch the content of the URL and return the URL with its content length. Returns a string \\"URL: length\\" or \\"URL: Error occurred\\". try: response = requests.get(url) response.raise_for_status() # Raise HTTPError for bad responses return f\\"{url}: {len(response.content)}\\" except requests.RequestException: return f\\"{url}: Error occurred\\" def fetch_urls_concurrently(urls: List[str], max_workers: int) -> None: with ThreadPoolExecutor(max_workers=max_workers) as executor: # Submit the tasks to the executor future_to_url = {executor.submit(fetch_content_length, url): url for url in urls} # Process the results as they are completed for future in as_completed(future_to_url): result = future.result() print(result)"},{"question":"# PyTorch Coding Challenge: Custom Autograd Function Objective: Create a custom autograd function in PyTorch to perform both forward and backward passes for a specific mathematical operation. Task: Implement a custom PyTorch autograd function for the operation (f(x) = 5x^3 + 3) and compute its gradient. Instructions: 1. Implement a class `PolynomialFunction` by subclassing `torch.autograd.Function`. 2. Within the class, implement two static methods: `forward` and `backward`. - `forward(ctx, input)`: Compute and return the forward pass output (f(x) = 5x^3 + 3). - `backward(ctx, grad_output)`: Compute and return the gradient of the input tensor ( frac{dL}{dx} ) using the chain rule. Requirements: - Ensure that intermediate results needed for the backward pass are saved using the provided `ctx` context object. - Inputs to your function will always be a single PyTorch tensor of dtype `torch.float32`. Example Usage: ```python import torch class PolynomialFunction(torch.autograd.Function): @staticmethod def forward(ctx, input): In the forward pass we receive a Tensor containing the input and return a Tensor containing the output. ctx is a context object that can be used to stash information for backward computation. # Save the input for the backward pass ctx.save_for_backward(input) # Compute the output output = 5 * input ** 3 + 3 return output @staticmethod def backward(ctx, grad_output): In the backward pass we receive a Tensor containing the gradient of the loss with respect to the output, and we need to compute the gradient of the loss with respect to the input. # Retrieve the saved tensor from the context object input, = ctx.saved_tensors # Compute the gradient of the input grad_input = grad_output * 15 * input ** 2 return grad_input # Test the custom function input_tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) output = PolynomialFunction.apply(input_tensor) output.sum().backward() print(f\\"Input: {input_tensor}\\") print(f\\"Output: {output}\\") print(f\\"Input Grad: {input_tensor.grad}\\") ``` Expected Output: ``` Input: tensor([1., 2., 3.], requires_grad=True) Output: tensor([ 8., 43., 138.], grad_fn=<PolynomialFunctionBackward>) Input Grad: tensor([ 15., 60., 135.]) ``` Constraints: - You must use the provided template for class and method definitions. - Do not use any other Python libraries other than PyTorch. - Ensure that your solution handles any edge cases effectively, such as zero or negative values of the input tensor. Grading Criteria: - Correct implementation of `forward` and `backward` methods. - Proper usage of the context object to save necessary intermediates. - Correct gradient computation. - Clean and readable code.","solution":"import torch class PolynomialFunction(torch.autograd.Function): @staticmethod def forward(ctx, input): In the forward pass we receive a Tensor containing the input and return a Tensor containing the output. ctx is a context object that can be used to stash information for backward computation. # Save the input for the backward pass ctx.save_for_backward(input) # Compute the output output = 5 * input ** 3 + 3 return output @staticmethod def backward(ctx, grad_output): In the backward pass we receive a Tensor containing the gradient of the loss with respect to the output, and we need to compute the gradient of the loss with respect to the input. # Retrieve the saved tensor from the context object input, = ctx.saved_tensors # Compute the gradient of the input grad_input = grad_output * 15 * input ** 2 return grad_input"},{"question":"**AIFF File Audio Information Extractor** You are required to write a function `extract_aiff_info(file_path: str) -> dict` that takes the path of an AIFF or AIFC audio file as input and returns a dictionary containing detailed information about the audio file. The dictionary should contain the following keys: - `num_channels`: the number of audio channels (integer). - `sample_width`: the size in bytes of individual samples (integer). - `frame_rate`: the sampling rate (number of audio frames per second) (integer). - `num_frames`: the number of audio frames in the file (integer). - `compression_type`: a string describing the compression type used in the audio file. - `compression_name`: a human-readable description of the compression type. - `markers`: a list of markers, where each marker is a dictionary with keys `id`, `position`, and `name`. Additionally, your function should raise a `ValueError` if the file cannot be opened. **Constraints:** - The file path provided will always be a valid string. - You may assume the file located at `file_path` is a valid AIFF or AIFC file. - You are required to use the `aifc` module for this task. **Example:** ```python def extract_aiff_info(file_path: str) -> dict: import aifc try: with aifc.open(file_path, \'r\') as file: info = { \'num_channels\': file.getnchannels(), \'sample_width\': file.getsampwidth(), \'frame_rate\': file.getframerate(), \'num_frames\': file.getnframes(), \'compression_type\': file.getcomptype().decode(), \'compression_name\': file.getcompname().decode(), \'markers\': [{\'id\': marker[0], \'position\': marker[1], \'name\': marker[2]} for marker in file.getmarkers()] } return info except (aifc.Error, IOError): raise ValueError(\\"Cannot open the file for reading\\") # Example usage file_info = extract_aiff_info(\'path_to_aiff_file.aiff\') print(file_info) ``` The example function `extract_aiff_info` showcases: - Opening the AIFF file using `aifc.open`. - Extracting various parameters using the defined methods. - Returning the information as a dictionary. - Handling errors by raising a `ValueError`.","solution":"import aifc def extract_aiff_info(file_path: str) -> dict: try: with aifc.open(file_path, \'r\') as file: info = { \'num_channels\': file.getnchannels(), \'sample_width\': file.getsampwidth(), \'frame_rate\': file.getframerate(), \'num_frames\': file.getnframes(), \'compression_type\': file.getcomptype(), \'compression_name\': file.getcompname(), \'markers\': [{\'id\': marker[0], \'position\': marker[1], \'name\': marker[2]} for marker in file.getmarkers() or []] } return info except (aifc.Error, IOError): raise ValueError(\\"Cannot open the file for reading\\") # Example usage # Please note this code will not work without a valid AIFF file at the specified path # file_info = extract_aiff_info(\'path_to_aiff_file.aiff\') # print(file_info)"},{"question":"You are tasked with visualizing the distribution of different species of penguins based on their bill lengths. You will use the seaborn library to load the dataset, create plots, and customize the legends. Your solution should involve implementing a function `plot_penguin_bill_lengths` that adheres to the following specifications: Specifications 1. **Function Name**: `plot_penguin_bill_lengths` 2. **Inputs**: - `legend_position`: A string specifying the position of the legend. Possible values are: `\\"center right\\"`, `\\"upper left\\"`, `\\"lower center\\"`. - `bbox_to_anchor`: A tuple for fine-grained control over legend positioning using the `bbox_to_anchor` parameter. - `column_wrap`: An integer specifying the number of columns in the facet grid (when applicable). 3. **Output**: None. The function should generate and display the plot. Case Details 1. The penguin dataset (`penguins`) should be loaded using `sns.load_dataset(\\"penguins\\")`. 2. Create a histogram plot of `bill_length_mm` for each species using `sns.histplot`. 3. Customize the legend\'s position using `sns.move_legend` based on the input parameters: - If `legend_position` is `\\"center right\\"`, place the legend at the \\"center right\\" position. - If `legend_position` is `\\"upper left\\"`, place the legend at the \\"upper left\\" position. - If `legend_position` is `\\"lower center\\"`, place the legend at the \\"lower center\\" position with specific adjustments provided by `bbox_to_anchor`. 4. When the `column_wrap` parameter is provided (not `None`), create a facet grid that separates the histograms based on the `island` feature and divides the plots across multiple columns specified by `column_wrap`. Example Usage ```python def plot_penguin_bill_lengths(legend_position: str, bbox_to_anchor: tuple, column_wrap: int = None): # Your implementation here # Example 1: Simple histogram with center right legend plot_penguin_bill_lengths(\\"center right\\", (1, 0.5)) # Example 2: Histogram with facets and legend in upper left plot_penguin_bill_lengths(\\"upper left\\", (1.1, 1), column_wrap=2) ``` Constraints - Ensure that the plots are handled gracefully with a suitable layout and appearance. - Consider edge cases where the dataset might be incomplete or have missing data. - The `bbox_to_anchor` parameter should only be considered for fine-tuning if the respective `legend_position` is `\\"upper left\\"` or `\\"lower center\\"`. Notes - You may assume that seaborn, matplotlib, and other necessary libraries are already installed. - Focus on making the visualizations clear and informative. - You may add additional customization to improve the aesthetics of the plots as needed.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_bill_lengths(legend_position: str, bbox_to_anchor: tuple, column_wrap: int = None): # Load the penguin dataset penguins = sns.load_dataset(\\"penguins\\") # Handle cases where the dataset might have missing values penguins = penguins.dropna(subset=[\\"bill_length_mm\\", \\"species\\", \\"island\\"]) # Create the histogram plot if column_wrap is not None: g = sns.displot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=column_wrap, kind=\\"hist\\") axes = g.axes.flat else: plt.figure(figsize=(10, 6)) axes = [sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\", multiple=\\"stack\\")] # Customize the legend based on the input parameters for ax in axes: if legend_position == \\"center right\\": ax.legend(loc=\'center right\') elif legend_position == \\"upper left\\": ax.legend(loc=\'upper left\', bbox_to_anchor=bbox_to_anchor) elif legend_position == \\"lower center\\": ax.legend(loc=\'lower center\', bbox_to_anchor=bbox_to_anchor) # Display the plot if column_wrap is not None: plt.show() else: plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Frequency\') plt.title(\'Distribution of Bill Lengths of Different Penguin Species\') plt.show()"},{"question":"Objective Demonstrate your understanding and practical usage of the `contextlib` module from Python\'s standard library. Specifically, focus on creating custom context managers and using `ExitStack` to manage multiple resources. Problem Statement You are tasked with writing a custom context manager and using `ExitStack` to handle multiple file operations safely. Here are the requirements: 1. Implement a custom context manager `FileManager` using the `contextlib.contextmanager` decorator. This context manager should: - Open a file for reading or writing. - Ensure the file is closed properly after operations, even if an error occurs. - Log the events of opening and closing files to a specified logging object. 2. Use `ExitStack` to open multiple files simultaneously and write data to each. If opening any file fails, ensure that all previously opened files are closed correctly. 3. Write a function `process_files(file_paths: List[str], data: str) -> None` that: - Takes a list of file paths and a string `data` to write into each file. - Opens each file using `FileManager` within an `ExitStack`. - Writes the given data to each successfully opened file. - Ensures all file-related resources are cleaned up properly. Constraints - The file paths given in the list might not all be valid or writable. - You must handle exceptions that might arise from file operations (like `FileNotFoundError`, `IOError`). - You are not allowed to use explicit \\"try-finally\\" blocks for resource management; use the facilities provided by `contextlib` instead. Input - `file_paths`: A list of strings, each representing a file path. - `data`: A string to be written to each file. Output - None. The function should perform file operations as described but does not need to return any value. Example ```python file_paths = [\\"file1.txt\\", \\"file2.txt\\", \\"file3.txt\\"] data = \\"Hello, World!\\" process_files(file_paths, data) ``` In the example above: - If all files are opened successfully, `data` is written to each file. - If any file fails to open, ensure that all already opened files are closed safely, and an appropriate error message is logged. Requirements - Implement the `FileManager` context manager. - Use `ExitStack` to manage multiple files in `process_files`. ```python import os from contextlib import contextmanager, ExitStack @contextmanager def FileManager(file_path, mode, logger): A custom context manager to handle file operations. try: file = open(file_path, mode) logger.info(f\\"Opened file: {file_path}\\") yield file except Exception as e: logger.error(f\\"Error opening file {file_path}: {e}\\") raise e finally: file.close() logger.info(f\\"Closed file: {file_path}\\") def process_files(file_paths, data, logger): Opens multiple files and writes data to them using ExitStack. with ExitStack() as stack: files = [] for file_path in file_paths: try: file = stack.enter_context(FileManager(file_path, \'w\', logger)) files.append(file) except Exception: logger.error(f\\"Failed to process file: {file_path}\\") for file in files: file.write(data) ``` Note: You must write your own logger or use a logging framework to handle the log operations in `FileManager`.","solution":"import os from contextlib import contextmanager, ExitStack import logging # Setup a logger logger = logging.getLogger(__name__) logger.setLevel(logging.INFO) handler = logging.StreamHandler() formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') handler.setFormatter(formatter) logger.addHandler(handler) @contextmanager def FileManager(file_path, mode, logger): A custom context manager to handle file operations. file = None try: file = open(file_path, mode) logger.info(f\\"Opened file: {file_path}\\") yield file except Exception as e: logger.error(f\\"Error opening file {file_path}: {e}\\") raise e finally: if file: file.close() logger.info(f\\"Closed file: {file_path}\\") def process_files(file_paths, data, logger): Opens multiple files and writes data to them using ExitStack. with ExitStack() as stack: files = [] for file_path in file_paths: try: file = stack.enter_context(FileManager(file_path, \'w\', logger)) files.append(file) except Exception: logger.error(f\\"Failed to process file: {file_path}\\") for file in files: file.write(data)"},{"question":"**Question: Implement a Self-Training Classifier using scikit-learn** You are given a dataset consisting of both labeled and unlabeled data points. Your task is to implement a self-training classifier using scikit-learn’s `SelfTrainingClassifier` to predict labels for the unlabeled data. # Objectives 1. Train a self-training classifier using a given supervised classifier. 2. Evaluate its performance on a validation set. # Input - A labeled dataset `(X_train_labeled, y_train_labeled)` where: - `X_train_labeled` is an array of shape `(n_labeled_samples, n_features)`. - `y_train_labeled` is an array of shape `(n_labeled_samples,)`. - An unlabeled dataset `X_train_unlabeled`, which is an array of shape `(n_unlabeled_samples, n_features)`. - A validation dataset `(X_val, y_val)` where: - `X_val` is an array of shape `(n_val_samples, n_features)`. - `y_val` is an array of shape `(n_val_samples,)`. # Output - The accuracy of the self-trained classifier on the validation set as a float. # Constraints - Use a RandomForestClassifier as the base estimator. - Set `max_iter` to 10. - Use a default threshold of 0.75 for the self-training classifier. # Example ```python from sklearn.ensemble import RandomForestClassifier from sklearn.semi_supervised import SelfTrainingClassifier from sklearn.metrics import accuracy_score def self_training_classifier(X_train_labeled, y_train_labeled, X_train_unlabeled, X_val, y_val): # Initialize the base estimator base_estimator = RandomForestClassifier() # Combine labeled and unlabeled data X_train = np.vstack((X_train_labeled, X_train_unlabeled)) y_train = np.hstack((y_train_labeled, [-1] * len(X_train_unlabeled))) # Initialize the self-training classifier self_training_clf = SelfTrainingClassifier(base_estimator, max_iter=10, threshold=0.75) # Fit the model self_training_clf.fit(X_train, y_train) # Predict on validation data y_pred = self_training_clf.predict(X_val) # Calculate accuracy accuracy = accuracy_score(y_val, y_pred) return accuracy # Example usage X_train_labeled = ... y_train_labeled = ... X_train_unlabeled = ... X_val = ... y_val = ... print(self_training_classifier(X_train_labeled, y_train_labeled, X_train_unlabeled, X_val, y_val)) ``` # Notes - You are required to handle the preprocessing and combining of labeled and unlabeled data within the function. - You should ensure that the `SelfTrainingClassifier` utilizes the `RandomForestClassifier` with default parameters. - Verify the output accuracy with appropriate test cases.","solution":"import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.semi_supervised import SelfTrainingClassifier from sklearn.metrics import accuracy_score def self_training_classifier(X_train_labeled, y_train_labeled, X_train_unlabeled, X_val, y_val): Trains a SelfTrainingClassifier using given labeled and unlabeled data, and evaluates on a validation set. Parameters: - X_train_labeled: Labeled data features, numpy array of shape (n_labeled_samples, n_features) - y_train_labeled: Labels for the labeled data, numpy array of shape (n_labeled_samples,) - X_train_unlabeled: Unlabeled data features, numpy array of shape (n_unlabeled_samples, n_features) - X_val: Validation data features, numpy array of shape (n_val_samples, n_features) - y_val: Validation data labels, numpy array of shape (n_val_samples,) Returns: - Accuracy of the self-trained classifier on the validation set as a float. # Initialize the base estimator base_estimator = RandomForestClassifier() # Combine labeled and unlabeled data X_train = np.vstack((X_train_labeled, X_train_unlabeled)) y_train = np.hstack((y_train_labeled, [-1] * len(X_train_unlabeled))) # Initialize the self-training classifier self_training_clf = SelfTrainingClassifier(base_estimator, max_iter=10, threshold=0.75) # Fit the model self_training_clf.fit(X_train, y_train) # Predict on validation data y_pred = self_training_clf.predict(X_val) # Calculate accuracy accuracy = accuracy_score(y_val, y_pred) return accuracy"},{"question":"**Objective**: To assess students\' understanding of Partial Dependence Plots (PDP) and Individual Conditional Expectation (ICE) plots using scikit-learn. # Problem Statement You are given a dataset representing a house pricing scenario with features such as the number of rooms, square footage, and location, along with the target variable being the price. Your task is to fit a regression model to this dataset and generate both PDP and ICE plots for specific features. # Requirements 1. **Load the Dataset**: - Use the provided dataset `house_pricing.csv` which contains columns: \'rooms\', \'square_footage\', \'location\', and \'price\'. 2. **Fit a Model**: - Fit a `GradientBoostingRegressor` model to the dataset with appropriate parameters. 3. **Generate PDP and ICE Plots**: - Create one-way PDPs for the features \'rooms\' and \'square_footage\' using scikit-learn\'s `PartialDependenceDisplay.from_estimator`. - Create a two-way PDP for the interaction between \'rooms\' and \'square_footage\'. - Generate ICE plots for the feature \'rooms\'. 4. **Interpret the Plots**: - Provide a brief analysis of the generated PDP and ICE plots. Discuss the interaction between features and their impact on the target variable. # Input Format The dataset `house_pricing.csv` with columns: \'rooms\', \'square_footage\', \'location\', and \'price\'. # Output Format Your code should: 1. Display the generated PDP and ICE plots. 2. Provide a text analysis of the plots in terms of feature importance and interactions. # Constraints - Use the \'brute\' method for computation to ensure compatibility. - Assume that the dataset is cleaned and preprocessed properly. # Example ```python import pandas as pd from sklearn.ensemble import GradientBoostingRegressor from sklearn.inspection import PartialDependenceDisplay import matplotlib.pyplot as plt # Load dataset data = pd.read_csv(\'house_pricing.csv\') X = data[[\'rooms\', \'square_footage\', \'location\']] y = data[\'price\'] # Fit model model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=0) model.fit(X, y) # Generate PDP and ICE plots features = [\'rooms\', \'square_footage\', (\'rooms\', \'square_footage\')] PartialDependenceDisplay.from_estimator(model, X, features) plt.show() # ICE plot PartialDependenceDisplay.from_estimator(model, X, [\'rooms\'], kind=\'individual\') plt.show() # Analysis # Add textual analysis based on the generated plots. ``` # Evaluation Criteria Your solution will be evaluated based on: - Correctness of model fitting and plot generation. - Accuracy and clarity of the plot interpretation. - Proper usage of scikit-learn functions. - Code readability and documentation.","solution":"import pandas as pd from sklearn.ensemble import GradientBoostingRegressor from sklearn.inspection import PartialDependenceDisplay import matplotlib.pyplot as plt def load_data(filepath): data = pd.read_csv(filepath) X = data[[\'rooms\', \'square_footage\', \'location\']] y = data[\'price\'] return X, y def fit_model(X, y): model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=0) model.fit(X, y) return model def generate_plots(model, X): features = [\'rooms\', \'square_footage\', (\'rooms\', \'square_footage\')] fig, ax = plt.subplots(figsize=(12, 8)) PartialDependenceDisplay.from_estimator(model, X, features, ax=ax) plt.suptitle(\\"Partial Dependence Plots\\") plt.show() fig, ax = plt.subplots(figsize=(6, 6)) PartialDependenceDisplay.from_estimator(model, X, [\'rooms\'], kind=\'individual\', ax=ax) plt.suptitle(\\"Individual Conditional Expectation Plot for \'rooms\'\\") plt.show() def main(): filepath = \'house_pricing.csv\' # Path to the dataset X, y = load_data(filepath) model = fit_model(X, y) generate_plots(model, X) if __name__ == \\"__main__\\": main()"},{"question":"You are tasked with writing a Python program to process a CSV file that contains information about products in a store. Each row in the CSV file represents a product and includes the following fields: `ProductID`, `ProductName`, `Category`, `Price`, and `StockQuantity`. Your program should perform the following tasks: 1. **Read the CSV file**: Implement a function `read_products(file_path)` which takes the path to the CSV file as input and returns a list of dictionaries, where each dictionary represents a product. 2. **Write to CSV file**: Implement a function `write_products(file_path, products)` which takes a file path and a list of product dictionaries as input and writes the product information to a CSV file. Ensure that the CSV file includes a header row with the field names. 3. **Filter Products by Category**: Implement a function `filter_products_by_category(products, category)` which takes a list of product dictionaries and a category name as input and returns a list of product dictionaries that belong to the given category. 4. **Update Stock**: Implement a function `update_stock(products, product_id, new_stock)` which takes a list of product dictionaries, a product ID, and a new stock quantity as input. The function should update the `StockQuantity` for the product with the given `ProductID` and return `True` if the product was found and updated, otherwise return `False`. 5. **Main Function**: Implement a `main()` function to demonstrate the functionality of the above methods. Your `main()` function should: - Read the initial product list from a CSV file. - Filter products by a given category and display them. - Update the stock of a specific product. - Write the updated product list to a new CSV file. # Input and Output Formats - **CSV Input File Format**: ``` ProductID,ProductName,Category,Price,StockQuantity 1,Apple,Fruit,0.5,100 2,Banana,Fruit,0.3,150 3,Broccoli,Vegetable,1.2,80 ``` - **CSV Output File Format**: ``` ProductID,ProductName,Category,Price,StockQuantity ``` - **Function Signatures**: ```python def read_products(file_path: str) -> list[dict]: pass def write_products(file_path: str, products: list[dict]) -> None: pass def filter_products_by_category(products: list[dict], category: str) -> list[dict]: pass def update_stock(products: list[dict], product_id: int, new_stock: int) -> bool: pass def main() -> None: pass ``` # Constraints - You can assume that the CSV files will not contain any malicious or malformed data. - The functions should handle cases where the desired product or category is not found gracefully. # Performance Requirements - The functions should be efficient and handle large CSV files, with up to 10,000 rows, within a reasonable time. Demonstrate the working of your code by providing a sample CSV file and running the `main()` function.","solution":"import csv def read_products(file_path): Reads products from a CSV file and returns a list of dictionaries where each dictionary represents a product. products = [] with open(file_path, mode=\'r\', newline=\'\') as file: reader = csv.DictReader(file) for row in reader: row[\'Price\'] = float(row[\'Price\']) row[\'StockQuantity\'] = int(row[\'StockQuantity\']) products.append(row) return products def write_products(file_path, products): Writes product information to a CSV file. fieldnames = [\'ProductID\', \'ProductName\', \'Category\', \'Price\', \'StockQuantity\'] with open(file_path, mode=\'w\', newline=\'\') as file: writer = csv.DictWriter(file, fieldnames=fieldnames) writer.writeheader() for product in products: writer.writerow(product) def filter_products_by_category(products, category): Filters products by category and returns a list of products in that category. return [product for product in products if product[\'Category\'] == category] def update_stock(products, product_id, new_stock): Updates the stock quantity for the product with the given product ID. Returns True if the product was found and updated, otherwise returns False. for product in products: if int(product[\'ProductID\']) == product_id: product[\'StockQuantity\'] = new_stock return True return False def main(): # Path to input and output CSV file input_file = \'products.csv\' output_file = \'updated_products.csv\' # Read products from CSV products = read_products(input_file) # Filter products by category fruits = filter_products_by_category(products, \'Fruit\') print(\\"Fruits:\\", fruits) # Update stock of a specific product if update_stock(products, 1, 120): print(\\"Stock updated successfully.\\") else: print(\\"Product not found.\\") # Write updated products to a new CSV file write_products(output_file, products) if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Seaborn Visualization Coding Challenge Problem Statement You are provided with the `penguins` dataset from the seaborn library. Your task is to create an advanced plot using seaborn’s `objects` module that visualizes the distribution of penguin body masses for each species. The plot should include jittered points to show individual data points, and it should also provide a summary with median and interquartile range for each species. In addition, use different colors for each species and make sure to add a legend that clearly identifies them. Input Format - You will have access to the `penguins` dataset from seaborn. ```python import seaborn as sns penguins = sns.load_dataset(\\"penguins\\") ``` Output Format - Your function should render a plot (it does not need to return any data structure). Requirements 1. Use the `seaborn.objects` module. 2. Layer multiple marks to avoid overlapping and provide clear interpretations: - Use `so.Dots()` with jitter to show individual data points. - Use `so.Range()` with `so.Perc([25, 75])` for interquartile ranges. - Add a layer for medians. 3. Apply distinct colors for different species. 4. Include a legend to identify species. Example Plot The plot should have: - x-axis representing species. - y-axis representing body mass in grams. - Jittered dots showing individual body mass measurements. - Range bars showing the interquartile ranges. - Markers for median body mass. - A legend indicating the species. Function Signature ```python def create_penguin_body_mass_plot(): # Your code here ``` Sample Function Call ```python create_penguin_body_mass_plot() ``` Note The function will generate the plot directly, and it does not need to return any value.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def create_penguin_body_mass_plot(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the plot with seaborn objects p = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"species\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75])) .add(so.Dot(), so.Perc([50])) .label(x=\\"Species\\", y=\\"Body Mass (g)\\", color=\\"Species\\") ) # Show the plot p.show()"},{"question":"**Question: Efficient Handling of Large Datasets with Pandas** You are given a directory containing multiple Parquet files, each representing a year\'s worth of a time series dataset. Each dataset has the following columns: - `timestamp`: Timestamps at 1-minute frequency. - `name`: Name of an individual, which can be \\"Alice\\", \\"Bob\\", or \\"Charlie\\". - `id`: An integer ID for each record. - `x` and `y`: Numeric values ranging between -1 and 1. **Objective:** 1. Implement a function that loads only the necessary columns (`name`, `id`, `x`, `y`) from these files. 2. Convert the `name` column to `Categorical`. 3. Downcast the `id`, `x`, and `y` columns to their most memory-efficient numeric types. 4. Process the data in chunks by summarizing the total count of each unique `name` across all files. **Function Signature:** ```python import pandas as pd from pathlib import Path from typing import Dict def process_large_datasets(directory: str) -> Dict[str, int]: # Your code here ``` **Inputs:** - `directory` (str): The path to the directory containing Parquet files. **Outputs:** - Returns a dictionary with the unique names as keys and their respective counts across all files. **Constraints:** - Ensure that the entire dataset does not need to fit into memory at once. - Use efficient data loading techniques. - Convert categorical data where appropriate and downcast numeric types to use minimal memory. **Example Usage:** ```python result = process_large_datasets(\'data/timeseries\') print(result) # Example output: {\'Alice\': 1200, \'Bob\': 1500, \'Charlie\': 1100} ``` **Notes:** - Multiple Parquet files are present in the specified directory. - Focus on using `Categorical` for the `name` column and downcasting numeric columns. - Ensure you process data in chunks if files are large enough to warrant it. **Hints:** - Use the `pd.read_parquet` function with the `columns` parameter to load only the necessary columns. - Use `astype(\'category\')` for converting the `name` column. - Use `pd.to_numeric` with `downcast` for numeric types. - Iterate over files using a loop or Pathlib to handle chunked processing.","solution":"import pandas as pd from pathlib import Path from typing import Dict def process_large_datasets(directory: str) -> Dict[str, int]: path = Path(directory) name_counts = {\'Alice\': 0, \'Bob\': 0, \'Charlie\': 0} for file in path.glob(\'*.parquet\'): df = pd.read_parquet(file, columns=[\'name\', \'id\', \'x\', \'y\']) df[\'name\'] = df[\'name\'].astype(\'category\') df[\'id\'] = pd.to_numeric(df[\'id\'], downcast=\'integer\') df[\'x\'] = pd.to_numeric(df[\'x\'], downcast=\'float\') df[\'y\'] = pd.to_numeric(df[\'y\'], downcast=\'float\') counts = df[\'name\'].value_counts() for name, count in counts.items(): name_counts[name] += count return name_counts"},{"question":"**Coding Assessment Question:** # **Implementing and Manipulating Export IR Graphs** In this question, you will demonstrate your understanding of PyTorch\'s Export IR by implementing a function that creates and manipulates an Export IR graph. Specifically, you need to: 1. Define a PyTorch model. 2. Export the model to an Export IR graph. 3. Implement functionality to traverse and modify nodes within the graph. **Task:** 1. Define a simple PyTorch model, `MySimpleModel`, with a forward function that performs basic tensor operations. 2. Export the model to an Export IR graph using `torch.export.export`. 3. Implement a function `modify_graph` that takes an Export IR graph and: - Traverses the nodes. - For each node of type `call_function` that represents the `add` operation, replace it with an equivalent `sub` operation. - Return the modified Export IR graph. **Specification:** - **Input:** - The PyTorch model class `MySimpleModel`. - Example input tensor for exporting the model. - **Output:** - An Export IR graph with the `add` operations replaced by `sub` operations. - **Constraints:** - The model should have at least one `add` operation in its forward function. - Utilize Export IR-specific functionalities and adhere to the documentation. - **Performance:** - The solution should efficiently handle graphs with up to 100 nodes. **Example:** ```python import torch from torch import nn import torch.export class MySimpleModel(nn.Module): def forward(self, x, y): return x + y def modify_graph(exported_program): graph = exported_program.graph for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.ops.aten.add.Tensor: node.target = torch.ops.aten.sub.Tensor return exported_program # Define the model and example inputs model = MySimpleModel() example_inputs = (torch.randn(1), torch.randn(1)) # Export the model to Export IR graph exported_program = torch.export.export(model, example_inputs) # Modify the graph modified_program = modify_graph(exported_program) # Print the original and modified graphs print(\\"Original Graph:\\") print(exported_program.graph) print(\\"Modified Graph:\\") print(modified_program.graph) ``` Your implementation should handle the traversal and modification of the Export IR graph as specified. Ensure that the logic to identify and replace `add` operations is correct. **Hints:** - Use `torch.export.export` to create the Export IR graph. - Traverse the graph nodes by accessing `graph.nodes` attribute. - Check node attributes such as `op` and `target` to identify `add` operations. - Replace the `target` of the identified nodes with the equivalent `sub` operation.","solution":"import torch from torch import nn import torch.export # Define the PyTorch model class MySimpleModel(nn.Module): def forward(self, x, y): return x + y # Function to modify the graph by replacing add operations with sub operations def modify_graph(exported_program): graph = exported_program.graph for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.ops.aten.add.Tensor: node.target = torch.ops.aten.sub.Tensor # Replace add operation with sub operation return exported_program # Define the function to export the model to Export IR graph def export_model_to_graph(model, example_inputs): return torch.export.export(model, example_inputs)"},{"question":"**Python Coding Assessment Question** **Objective:** Implement a function that launches multiple tasks in parallel to process a list of URLs concurrently, fetch their content, and return the results. Handle potential exceptions gracefully and ensure optimal performance. **Problem Statement:** You are provided a list of URLs, and you need to fetch their content concurrently. Implement a function `fetch_urls_concurrently(urls: List[str]) -> List[Tuple[str, Union[str, None]]]` that: 1. Uses the `concurrent.futures` module to perform the fetching of URLs in parallel. 2. Each URL should be fetched using an HTTP GET request. 3. If a URL fetch is successful, return the content of the URL. 4. If a URL fetch fails (e.g., due to a timeout, connection error, or any other exception), return `None` for that URL. 5. Ensure that tasks are managed efficiently and exceptions are handled gracefully. **Function Signature:** ```python from typing import List, Tuple, Union def fetch_urls_concurrently(urls: List[str]) -> List[Tuple[str, Union[str, None]]]: ``` **Input:** - `urls`: A list of strings, where each string is a URL to fetch. **Output:** - A list of tuples, where each tuple contains: - The original URL (string). - The content of the URL (string) if the fetch was successful, otherwise `None`. **Examples:** ```python urls = [ \\"https://example.com\\", \\"https://httpbin.org/get\\", \\"https://invalid.url\\" ] result = fetch_urls_concurrently(urls) # The precise output depends on the content of the URLs and their availability. # However, the structure should be similar to: # [ # (\\"https://example.com\\", \\"<HTML content or None>\\"), # (\\"https://httpbin.org/get\\", \\"<JSON content or None>\\"), # (\\"https://invalid.url\\", None) # ] ``` **Constraints:** - You may assume that the list of URLs has at most 100 entries. - Each URL fetch should time out after 5 seconds if it does not complete. **Notes:** - You may use the `requests` library to perform the HTTP GET requests. - Make sure to handle exceptions that may arise during the HTTP GET request (e.g., `requests.exceptions.RequestException`). **Performance Requirements:** - The implementation should efficiently manage the creation and completion of concurrent tasks. - Minimize the overhead of creating threads or processes to ensure scalability. Good luck, and remember to test your implementation thoroughly!","solution":"from typing import List, Tuple, Union import concurrent.futures import requests def fetch_url(url: str) -> Tuple[str, Union[str, None]]: try: response = requests.get(url, timeout=5) response.raise_for_status() # Check if the request was successful return (url, response.text) except requests.exceptions.RequestException: return (url, None) def fetch_urls_concurrently(urls: List[str]) -> List[Tuple[str, Union[str, None]]]: with concurrent.futures.ThreadPoolExecutor() as executor: results = list(executor.map(fetch_url, urls)) return results"},{"question":"Analyzing Temperature Trends You are given a dataset containing daily temperature readings. Your task is to analyze these temperature trends using pandas window functions. Implement a function called `analyze_temperature_trends` that performs the following: 1. **Input**: A pandas DataFrame `df` with columns: - `date`: Timestamps of the readings. - `temperature`: Daily temperature readings. - `city`: Name of the city the temperature reading belongs to. 2. **Output**: A pandas DataFrame with the following columns: - `date` - `city` - `rolling_avg_temp`: The 7-day rolling average temperature. - `exp_weighted_avg_temp`: The exponentially weighted moving average with a halflife of 3 days. - `expanding_avg_temp`: The expanding window average temperature up to that day. 3. **Constraints**: - The input DataFrame can contain missing temperature readings represented by NaN values. - Ensure that the resulting DataFrame does not contain rows with NaN values in the output columns. 4. **Performance Requirement**: - The solution should efficiently handle large datasets (up to 1 million rows) within a reasonable time frame. ```python import pandas as pd def analyze_temperature_trends(df: pd.DataFrame) -> pd.DataFrame: Analyze temperature trends using pandas window functions. Parameters: df (pd.DataFrame): A DataFrame with columns `date`, `temperature`, and `city`. Returns: pd.DataFrame: A new DataFrame containing `date`, `city`, `rolling_avg_temp`, `exp_weighted_avg_temp`, and `expanding_avg_temp`. # Implement your function here pass # Example Usage: # df = pd.DataFrame({ # \'date\': pd.date_range(start=\'2023-01-01\', periods=10, freq=\'D\'), # \'temperature\': [23, 25, 21, 19, 20, 18, 22, 24, 23, 21], # \'city\': [\'CityA\'] * 10 # }) # result = analyze_temperature_trends(df) # print(result) ``` # Explanation This question tests your understanding of pandas window functions, including rolling, expanding, and exponentially weighted moving averages. You will need to handle missing data and ensure the solution\'s efficiency for large datasets. The example usage provided will help you test your implementation.","solution":"import pandas as pd def analyze_temperature_trends(df: pd.DataFrame) -> pd.DataFrame: Analyze temperature trends using pandas window functions. Parameters: df (pd.DataFrame): A DataFrame with columns `date`, `temperature`, and `city`. Returns: pd.DataFrame: A new DataFrame containing `date`, `city`, `rolling_avg_temp`, `exp_weighted_avg_temp`, and `expanding_avg_temp`. # Drop rows with NaN values in \'temperature\' df = df.dropna(subset=[\'temperature\']) # Calculate the rolling average temperature df[\'rolling_avg_temp\'] = df.groupby(\'city\')[\'temperature\'].transform(lambda x: x.rolling(window=7, min_periods=1).mean()) # Calculate the exponentially weighted moving average temperature df[\'exp_weighted_avg_temp\'] = df.groupby(\'city\')[\'temperature\'].transform(lambda x: x.ewm(halflife=3, min_periods=1).mean()) # Calculate the expanding window average temperature df[\'expanding_avg_temp\'] = df.groupby(\'city\')[\'temperature\'].transform(lambda x: x.expanding(min_periods=1).mean()) return df[[\'date\', \'city\', \'rolling_avg_temp\', \'exp_weighted_avg_temp\', \'expanding_avg_temp\']]"},{"question":"# Advanced Python File Processing and Configuration Management Problem Statement You are tasked with creating a Python script that reads data from a CSV file, processes the data, writes the processed data into another CSV file, and updates configuration settings based on the results of the data processing. Requirements: 1. **CSV File Handling**: You will be provided with a CSV file named `input.csv`. This file will contain two columns: - `Name`: String representing the name. - `Score`: Integer representing a score. 2. **Data Processing**: Calculate the average score from the `Score` column and identify the name(s) with the highest score. 3. **CSV Output**: Write the results into a CSV file named `output.csv` with two columns: - `Name`: Name(s) with the highest score. - `Score`: The highest score. 4. **Configuration Management**: - You will be provided with a configuration file named `config.ini` with sections and key-value pairs. - You need to update (or add if not present) the configuration section `[Statistics]` with the following keys: - `average_score`: The calculated average score. - `highest_score`: The highest score. - `top_scorer_names`: Comma-separated names with the highest score. Expected Input/Output: - **Input**: - `input.csv` (Example content): ``` Name,Score Alice,88 Bob,95 Charlie,95 Diana,91 ``` - `config.ini` (Initial example content): ``` [Settings] version = 1.0 [User] username = admin password = secret ``` - **Output**: - `output.csv` (Example content after processing the provided input example): ``` Name,Score Bob,95 Charlie,95 ``` - `config.ini` (Final content after processing the provided input example): ``` [Settings] version = 1.0 [User] username = admin password = secret [Statistics] average_score = 92.25 highest_score = 95 top_scorer_names = Bob,Charlie ``` Constraints: - The CSV input file will have at least one row of data. - The configuration file may or may not contain the `[Statistics]` section initially. - You should handle exceptions gracefully and ensure the script does not crash on malformed input files. Performance Requirements: - The solution should efficiently handle CSV files containing up to 100,000 rows. Implementation: - Implement a Python function named `process_and_update_config` that performs the described tasks. - The function signature should be `def process_and_update_config(input_csv: str, output_csv: str, config_file: str):`. - Ensure that all changes to the configuration file are saved persistently. Example Function Call: ```python process_and_update_config(\'input.csv\', \'output.csv\', \'config.ini\') ```","solution":"import csv import configparser def process_and_update_config(input_csv: str, output_csv: str, config_file: str): names = [] scores = [] # Read input CSV with open(input_csv, \'r\') as infile: reader = csv.DictReader(infile) for row in reader: names.append(row[\'Name\']) scores.append(int(row[\'Score\'])) # Process data average_score = sum(scores) / len(scores) highest_score = max(scores) top_scorers = [names[i] for i in range(len(scores)) if scores[i] == highest_score] # Write output CSV with open(output_csv, \'w\', newline=\'\') as outfile: fieldnames = [\'Name\', \'Score\'] writer = csv.DictWriter(outfile, fieldnames=fieldnames) writer.writeheader() for name in top_scorers: writer.writerow({\'Name\': name, \'Score\': highest_score}) # Update configuration file config = configparser.ConfigParser() config.read(config_file) if \'Statistics\' not in config.sections(): config.add_section(\'Statistics\') config[\'Statistics\'][\'average_score\'] = str(average_score) config[\'Statistics\'][\'highest_score\'] = str(highest_score) config[\'Statistics\'][\'top_scorer_names\'] = \',\'.join(top_scorers) with open(config_file, \'w\') as configfile: config.write(configfile)"},{"question":"Asynchronous File Processing **Objective:** Implement a function that processes a list of file paths in parallel to count the frequency of a specific word in each file. The function should use the `concurrent.futures` module to achieve parallelism, ensuring efficient processing. **Function Signature:** ```python def count_word_in_files(word: str, file_paths: List[str]) -> Dict[str, int]: pass ``` **Input:** - `word` (str): The word to count in each file. - `file_paths` (List[str]): A list of file paths as strings. **Output:** - A dictionary where the keys are file paths, and the values are the count of occurrences of the specified word in the corresponding files. **Constraints:** - The function should utilize `concurrent.futures.ThreadPoolExecutor` or `concurrent.futures.ProcessPoolExecutor` to perform the file processing asynchronously. - Each file is assumed to be a text file that fits into memory. - Handle IOError exceptions gracefully and assume a count of 0 for any file that cannot be read. **Example:** ```python >>> count_word_in_files(\\"example\\", [\\"file1.txt\\", \\"file2.txt\\"]) {\\"file1.txt\\": 5, \\"file2.txt\\": 3} ``` **Additional Notes:** - Ensure that the function implementation is thread-safe. - Use appropriate methods to manage the executor\'s lifecycle. - Pay attention to memory and resource management, especially if using `ProcessPoolExecutor`. Your Task: Implement the `count_word_in_files` function according to the specifications above, ensuring efficient parallel processing and proper handling of potential exceptions.","solution":"import concurrent.futures from typing import List, Dict def count_word_in_file(word: str, file_path: str) -> int: Helper function to count occurrences of a word in a single file. try: with open(file_path, \'r\') as file: content = file.read() return content.lower().split().count(word.lower()) # Count word occurrence case-insensitively except IOError: return 0 def count_word_in_files(word: str, file_paths: List[str]) -> Dict[str, int]: Count the occurrences of a word in a list of files concurrently. Parameters: word (str): The word to count in each file. file_paths (List[str]): A list of file paths as strings. Returns: Dict[str, int]: A dictionary where keys are file paths and values are the counts of the word. counts = {} with concurrent.futures.ThreadPoolExecutor() as executor: future_to_file = {executor.submit(count_word_in_file, word, file_path): file_path for file_path in file_paths} for future in concurrent.futures.as_completed(future_to_file): file_path = future_to_file[future] try: counts[file_path] = future.result() except Exception as exc: counts[file_path] = 0 return counts"},{"question":"# Email Package Coding Assessment **Objective**: Demonstrate your understanding of the Python \\"email\\" package by writing code to parse, manipulate, and generate email messages. **Problem Statement**: You are given a raw email string containing headers and a MIME body. Your task is to: 1. Parse the email string into an `EmailMessage` object. 2. Extract specific headers (`From`, `To`, `Subject`) and print their values. 3. Check if the email contains any attachments. If it does, save each attachment to a specified directory. 4. Modify the `Subject` header by appending the text \\" - Processed\\". 5. Generate the modified email object back into a raw email string format and print it. **Input**: - A raw email string. - A directory path to save attachments (only if there are any). **Output**: - Print the values of the `From`, `To`, and `Subject` headers. - Save attachments to the specified directory, if any. - Print the modified raw email string. **Constraints**: - The email may contain plain text, HTML content, and attachments. - Handle different content types appropriately. - Ensure to maintain the RFC standards when generating the email string. **Example Email Input**: ```plaintext From: example@example.com To: recipient@example.com Subject: Test Email Content-Type: multipart/mixed; boundary=\\"===============7330845974216740156==\\" --===============7330845974216740156== Content-Type: text/plain; charset=\\"us-ascii\\" This is the body of the email. --===============7330845974216740156== Content-Type: text/plain; charset=\\"us-ascii\\"; name=\\"example.txt\\" Content-Transfer-Encoding: base64 Content-Disposition: attachment; filename=\\"example.txt\\" RW5jb2RlZCBjb250ZW50IGhlcmUu --===============7330845974216740156==-- ``` **Example Directory Path**: ```plaintext ./attachments ``` **Expected Output**: ```plaintext From: example@example.com To: recipient@example.com Subject: Test Email # The attachment \\"example.txt\\" should be saved in the specified directory # Modified raw email string: From: example@example.com To: recipient@example.com Subject: Test Email - Processed Content-Type: multipart/mixed; boundary=\\"===============7330845974216740156==\\" --===============7330845974216740156== Content-Type: text/plain; charset=\\"us-ascii\\" This is the body of the email. --===============7330845974216740156== Content-Type: text/plain; charset=\\"us-ascii\\"; name=\\"example.txt\\" Content-Transfer-Encoding: base64 Content-Disposition: attachment; filename=\\"example.txt\\" RW5jb2RlZCBjb250ZW50IGhlcmUu --===============7330845974216740156==-- ``` **Function Signature**: ```python def process_email(raw_email: str, attachment_dir: str) -> None: # your code here ``` **Notes**: - Use the appropriate modules and classes from the `email` package. - Ensure to handle exceptions where necessary. - Make sure to handle different MIME types correctly while processing attachments. - Maintain RFC compliance when modifying and generating the email.","solution":"import os from email import policy from email.parser import BytesParser from email.generator import BytesGenerator from email.message import EmailMessage def process_email(raw_email: str, attachment_dir: str) -> None: # Parse the raw email string into an EmailMessage object email_message = BytesParser(policy=policy.default).parsebytes(raw_email.encode()) # Extract headers and print their values from_header = email_message[\'From\'] to_header = email_message[\'To\'] subject_header = email_message[\'Subject\'] print(f\\"From: {from_header}\\") print(f\\"To: {to_header}\\") print(f\\"Subject: {subject_header}\\") # Check for attachments and save them to the specified directory if found if email_message.is_multipart(): for part in email_message.iter_parts(): if part.get_content_disposition() == \'attachment\': filename = part.get_filename() if not filename: continue file_path = os.path.join(attachment_dir, filename) with open(file_path, \'wb\') as f: f.write(part.get_payload(decode=True)) # Modify the Subject header if subject_header: email_message.replace_header(\'Subject\', f\\"{subject_header} - Processed\\") # Generate the modified email object back into a raw email string format with open(\'modified_email.eml\', \'wb\') as f: BytesGenerator(f, policy=policy.default).flatten(email_message) # Print the modified raw email string with open(\'modified_email.eml\', \'rb\') as f: modified_email_string = f.read().decode() print(modified_email_string)"}]'),D={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const n=this.searchQuery.trim().toLowerCase();return n?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(n)||e.solution&&e.solution.toLowerCase().includes(n)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(n=>setTimeout(n,1e3)),this.visibleCount+=4,this.isLoading=!1}}},R={class:"search-container"},z={class:"card-container"},F={key:0,class:"empty-state"},q=["disabled"],N={key:0},O={key:1};function L(n,e,l,m,i,o){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",R,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>i.searchQuery=r),placeholder:"Search..."},null,512),[[y,i.searchQuery]]),i.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>i.searchQuery="")}," ✕ ")):d("",!0)]),t("div",z,[(a(!0),s(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),s("div",F,' No results found for "'+c(i.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:i.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[i.isLoading?(a(),s("span",O,"Loading...")):(a(),s("span",N,"See more"))],8,q)):d("",!0)])}const M=p(D,[["render",L],["__scopeId","data-v-b17b6de2"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/54.md","filePath":"chatai/54.md"}'),j={name:"chatai/54.md"},X=Object.assign(j,{setup(n){return(e,l)=>(a(),s("div",null,[x(M)]))}});export{Y as __pageData,X as default};
