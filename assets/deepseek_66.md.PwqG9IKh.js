import{_ as p,o as a,c as s,a as t,m as c,t as u,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},S={class:"review-title"},E={class:"review-content"};function P(i,e,l,m,n,r){return a(),s("div",T,[t("div",C,[t("div",S,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),c(u(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",E,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),c(u(l.poem.solution),1)])])])}const I=p(k,[["render",P],["__scopeId","data-v-8f798b96"]]),A=JSON.parse('[{"question":"Your task is to implement a function that connects to an IMAP server, logs in to the specified mailbox, searches for unread emails within a specific date range, and returns a list of email subjects and their senders. Requirements: 1. **Function Signature**: ```python def fetch_unread_emails(server: str, username: str, password: str, start_date: str, end_date: str, use_ssl: bool = True) -> List[Tuple[str, str]]: ``` 2. **Input Parameters**: - `server` (str): IMAP server address. - `username` (str): Username for the mailbox login. - `password` (str): Password for the mailbox login. - `start_date` (str): Start of the date range in \\"DD-MMM-YYYY\\" format. - `end_date` (str): End of the date range in \\"DD-MMM-YYYY\\" format. - `use_ssl` (bool): Whether to use a secure connection or not. Default is True. 3. **Output**: - Returns a list of tuples. Each tuple contains the subject (str) and sender (str) of an unread email within the specified date range. 4. **Constraints**: - Ensure secure connection management using context managers (`with` statement). - Proper error handling for connection issues and authentication failures. - Ensure that implementation uses the correct search criteria and parsing methods. 5. **Hints**: - Use the appropriate methods from `IMAP4` and `IMAP4_SSL` classes. - Date format to search might need to be formatted as \\"DD-MMM-YYYY\\". Example ```python from imaplib import IMAP4_SSL, IMAP4 from typing import List, Tuple def fetch_unread_emails(server: str, username: str, password: str, start_date: str, end_date: str, use_ssl: bool = True) -> List[Tuple[str, str]]: # Code to implement pass # Example Usage: server = \\"imap.mailserver.com\\" username = \\"exampleuser\\" password = \\"password\\" start_date = \\"01-Jan-2023\\" end_date = \\"31-Jan-2023\\" emails = fetch_unread_emails(server, username, password, start_date, end_date) for subject, sender in emails: print(f\\"From: {sender}, Subject: {subject}\\") ``` **Note**: Ensure to handle all relevant IMAP commands, responses, and potential error cases properly.","solution":"from imaplib import IMAP4_SSL, IMAP4 from typing import List, Tuple import email from email.header import decode_header def fetch_unread_emails(server: str, username: str, password: str, start_date: str, end_date: str, use_ssl: bool = True) -> List[Tuple[str, str]]: Connects to an IMAP server, logs in to the specified mailbox, searches for unread emails within a specific date range, and returns a list of email subjects and their senders. results = [] # Choose connection type if use_ssl: mail = IMAP4_SSL(server) else: mail = IMAP4(server) try: mail.login(username, password) mail.select(\'inbox\') # Search for unread emails within the date range search_criteria = f\'(UNSEEN SINCE {start_date} BEFORE {end_date})\' result, data = mail.search(None, search_criteria) if result == \'OK\': email_ids = data[0].split() for email_id in email_ids: res, msg_data = mail.fetch(email_id, \'(RFC822)\') if res == \'OK\': for response_part in msg_data: if isinstance(response_part, tuple): msg = email.message_from_bytes(response_part[1]) subject, encoding = decode_header(msg[\\"Subject\\"])[0] if isinstance(subject, bytes): subject = subject.decode(encoding if encoding else \'utf-8\') sender, encoding = decode_header(msg.get(\\"From\\"))[0] if isinstance(sender, bytes): sender = sender.decode(encoding if encoding else \'utf-8\') results.append((subject, sender)) else: print(\\"No emails found.\\") except Exception as e: print(f\\"An error occurred: {e}\\") finally: mail.logout() return results"},{"question":"# Implementing a Custom SMTP Server with Validation Background You are required to create a custom SMTP server by subclassing the `smtpd.SMTPServer` class. The goal of this custom server is to perform validation on incoming emails and only accept emails that meet specific criteria. Requirements 1. **CustomSMTPServer Class**: - Subclass `smtpd.SMTPServer`. - Override the `process_message` method. 2. **Validation Criteria**: - Email messages should only be accepted if they contain a specific keyword (e.g., `\\"assignment\\"`) in the subject line. - If the keyword is not present, the server should reject the email with an appropriate error message. 3. **Processing**: - Extract the subject line from the email. - Check if the required keyword is in the subject line. - Return `None` for a successful processing if the keyword is present. - Return a rejection message (e.g., `550 Subject Keyword Missing`) if the keyword is not present. Implementation Details - The `process_message` method has the following signature: ```python def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): ``` - `peer`: the remote host\'s address. - `mailfrom`: the envelope originator. - `rcpttos`: the envelope recipients. - `data`: a string containing the contents of the e-mail (should comply with **RFC 5321** format). - `kwargs`: additional keyword arguments. Input and Output - **Input**: - Provided as parameters to the `process_message` method within the class. - **Output**: - The method should return `None` if the email is accepted. - The method should return a rejection message (e.g., `550 Subject Keyword Missing`) if the email is not accepted. Constraints - Ensure the server instances do not handle more than 10 concurrent connections at a time for this exercise. Example ```python import smtpd import asyncore class CustomSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): # Implement the validation logic here pass # Instantiate and run the server server = CustomSMTPServer((\'localhost\', 1025), None) asyncore.loop() ``` Submission Provide the full implementation of the `CustomSMTPServer` class with the overridden `process_message` method. Ensure the validation logic for the keyword in the subject is correctly implemented and the server handles messages as specified.","solution":"import smtpd import asyncore from email import parser, policy class CustomSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): email_parser = parser.Parser(policy=policy.default) email_message = email_parser.parsestr(data) subject = email_message[\'subject\'] if \'assignment\' in subject.lower(): return None else: return \'550 Subject Keyword Missing\' # Function to start the server def run_server(): server = CustomSMTPServer((\'localhost\', 1025), None) asyncore.loop() # Example usage (in real use this would be run in a script) # run_server()"},{"question":"Design a Python script that connects to a POP3 server, logs in using provided credentials, retrieves all emails, and prints the subject and sender of each email. If there are any errors (e.g., authentication failure, connectivity issues), handle these gracefully and provide an appropriate error message. Your script should also have an option to delete all retrieved emails after they are processed. # Instructions 1. Implement a function `connect_and_retrieve(host, port, username, password, delete=False)` which: - Connects to the given POP3 server using the provided `host` and `port`. - Logs in with the provided `username` and `password`. - Retrieves all emails and prints the \\"Subject\\" and \\"From\\" headers of each email. - If `delete` is `True`, deletes all retrieved emails from the server. - Handles any exceptions that may occur during the process and prints appropriate error messages. # Input - `host` (string): The hostname of the POP3 server. - `port` (int): The port number of the POP3 server. - `username` (string): The username used for authentication. - `password` (string): The password used for authentication. - `delete` (boolean): Flag indicating whether to delete emails after retrieval. # Output - Prints the \\"Subject\\" and \\"From\\" headers of each retrieved email, or appropriate error messages in case of failures. # Constraints - The function should handle exceptions avoiding the termination of the script due to unexpected errors. - Ensure secure handling of credentials (avoid logging sensitive information). # Example ```python connect_and_retrieve(\'pop3.example.com\', 110, \'myuser\', \'mypassword\', delete=False) ``` This example should print the \\"Subject\\" and \\"From\\" headers of all emails in the mailbox on `pop3.example.com` for the user `myuser`. # Notes - You might need the `email` module to parse the email contents correctly. - Ensure that the script uses a secure channel for sensitive operations, especially when dealing with credentials.","solution":"import poplib from email import parser from email.policy import default def connect_and_retrieve(host, port, username, password, delete=False): try: # Connect to the POP3 server mail = poplib.POP3(host, port) mail.user(username) mail.pass_(password) # Get the number of messages num_messages = len(mail.list()[1]) for i in range(num_messages): # Retrieve the message response, lines, octets = mail.retr(i + 1) message_content = b\'rn\'.join(lines).decode(\'utf-8\') message = parser.Parser(policy=default).parsestr(message_content) # Print out the subject and sender print(f\\"Subject: {message[\'subject\']}\\") print(f\\"From: {message[\'from\']}\\") # Optionally delete the email if delete: mail.dele(i + 1) # Close the connection mail.quit() except poplib.error_proto as e: print(f\\"POP3 Protocol error: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Advanced PyTorch Question: Patching BatchNorm with GroupNorm **Objective:** You are tasked with creating a neural network in PyTorch that includes batch normalization layers. Additionally, you must implement a mechanism to patch the model to replace `BatchNorm` with `GroupNorm` as described in the documentation. **Requirements:** 1. **Neural Network Design:** - Construct a simple Convolutional Neural Network (CNN) that includes at least two convolutional layers followed by batch normalization layers. 2. **Function Implementation:** - Implement the function `replace_batchnorm_with_groupnorm` which takes a PyTorch model and replaces all `BatchNorm2d` layers with `GroupNorm` layers. 3. **Constraints:** - Ensure `G`, the number of groups for `GroupNorm`, always divides `C`, the number of input channels for `GroupNorm`. - Do not track running statistics in `BatchNorm` when replacing it with `GroupNorm`. 4. **Input and Output:** - Function signature: `def replace_batchnorm_with_groupnorm(model: nn.Module, num_groups: int) -> nn.Module:` - Input: A PyTorch model (`model`) and an integer (`num_groups`). - Output: The modified PyTorch model with `GroupNorm` layers instead of `BatchNorm` layers. # Example: ```python import torch import torch.nn as nn import torch.nn.functional as F class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(3, 16, 3, 1) self.bn1 = nn.BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, 3, 1) self.bn2 = nn.BatchNorm2d(32) self.fc1 = nn.Linear(32 * 6 * 6, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = F.relu(self.bn1(self.conv1(x))) x = F.relu(self.bn2(self.conv2(x))) x = F.max_pool2d(x, 2) x = torch.flatten(x, 1) x = F.relu(self.fc1(x)) x = self.fc2(x) return x def replace_batchnorm_with_groupnorm(model: nn.Module, num_groups: int) -> nn.Module: for name, module in model.named_children(): if isinstance(module, nn.BatchNorm2d): num_channels = module.num_features group_norm = nn.GroupNorm(num_groups=num_groups, num_channels=num_channels) setattr(model, name, group_norm) else: replace_batchnorm_with_groupnorm(module, num_groups) return model model = SimpleCNN() num_groups = 4 model = replace_batchnorm_with_groupnorm(model, num_groups) ``` **Implementation Notes:** - You are free to design any simple neural network with `BatchNorm` layers. - Pay attention to the replacement strategy to ensure that the model architecture remains valid. This question challenges students by asking them to understand and manipulate model layers within PyTorch, ensuring they comprehend normalization techniques and the constraints involved in their transformations.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(3, 16, 3, 1) self.bn1 = nn.BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, 3, 1) self.bn2 = nn.BatchNorm2d(32) self.fc1 = nn.Linear(32 * 6 * 6, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = F.relu(self.bn1(self.conv1(x))) x = F.relu(self.bn2(self.conv2(x))) x = F.max_pool2d(x, 2) x = torch.flatten(x, 1) x = F.relu(self.fc1(x)) x = self.fc2(x) return x def replace_batchnorm_with_groupnorm(model: nn.Module, num_groups: int) -> nn.Module: for name, module in model.named_children(): if isinstance(module, nn.BatchNorm2d): num_channels = module.num_features group_norm = nn.GroupNorm(num_groups=num_groups, num_channels=num_channels) setattr(model, name, group_norm) else: replace_batchnorm_with_groupnorm(module, num_groups) return model"},{"question":"**Objective:** Implement a function that records audio from the default microphone, processes it, and then plays it back using the `ossaudiodev` module. The function should demonstrate your ability to configure audio device parameters, handle audio data, and manage errors. **Function Signature:** ```python def record_and_playback_audio(duration: int, sample_rate: int = 44100, channels: int = 2, format: str = \'AFMT_S16_LE\') -> None: ``` # Inputs: 1. `duration` (int): The number of seconds to record audio. 2. `sample_rate` (int, optional): The desired sample rate in samples per second. Default is 44100 (CD quality). 3. `channels` (int, optional): The number of audio channels. Default is 2 (stereo). 4. `format` (str, optional): The audio format. Default is \'AFMT_S16_LE\' (signed 16-bit little-endian). # Constraints: - The function should handle and report errors gracefully using the appropriate exceptions. - The audio recording should be as close to real-time as possible. Any buffering should be minimal. - Sampled audio should be matched exactly to the specified sample rate, channels, and format. # Outputs: - The function does not return any value, but it should handle recording and playback correctly as per the specified parameters. # Instructions: 1. Open the default audio input device (`\\"/dev/dsp\\"`) for recording with `ossaudiodev.open(mode=\'r\')`. 2. Configure the audio parameters using `setparameters`. 3. Record audio data for the specified duration. 4. Open the default audio output device for playback with `ossaudiodev.open(mode=\'w\')`. 5. Configure the audio parameters for playback using `setparameters`. 6. Play back the recorded audio. 7. Ensure all devices are properly closed after use. # Example Usage: ```python def main(): # Record for 5 seconds and play back record_and_playback_audio(5) if __name__ == \\"__main__\\": main() ``` # Notes: - Use appropriate exception handling to catch and report errors during recording and playback. - The function should be able to handle different audio formats and configurations.","solution":"import ossaudiodev import time def record_and_playback_audio(duration: int, sample_rate: int = 44100, channels: int = 2, format: str = \'AFMT_S16_LE\'): Records audio from the default microphone and plays it back using the ossaudiodev module. Parameters: duration (int): The number of seconds to record audio. sample_rate (int, optional): The desired sample rate in samples per second. Default is 44100 (CD quality). channels (int, optional): The number of audio channels. Default is 2 (stereo). format (str, optional): The audio format. Default is \'AFMT_S16_LE\' (signed 16-bit little-endian). format_dict = { \'AFMT_S16_LE\': ossaudiodev.AFMT_S16_LE # You can add more formats if needed } if format not in format_dict: raise ValueError(\\"Unsupported audio format\\") audio_format = format_dict[format] try: # Open the default audio input device for recording dsp_in = ossaudiodev.open(\'r\') dsp_in.setparameters(audio_format, channels, sample_rate) # Estimate buffer size and read data buffer_size = sample_rate * channels * 2 # 2 bytes per sample (16-bit audio) data = dsp_in.read(buffer_size * duration) dsp_in.close() # Open the default audio output device for playback dsp_out = ossaudiodev.open(\'w\') dsp_out.setparameters(audio_format, channels, sample_rate) dsp_out.write(data) dsp_out.close() except ossaudiodev.OSSAudioError as e: print(\\"An error occurred with the OSS audio device:\\", e) except Exception as e: print(\\"An unexpected error occurred:\\", e)"},{"question":"# Python Coding Assessment Question Objective Your task is to create a basic text-based note-taking application using Python\'s `curses` module. This application should demonstrate your understanding of various `curses` functionalities such as window management, input handling, color manipulation, and screen refreshes. Requirements 1. **Multiple Windows**: The application should have at least two windows - one for displaying a menu and another for editing notes. 2. **Menu Window**: - The menu should have options like \\"New Note\\", \\"Edit Note\\", \\"Delete Note\\", \\"Save Note\\", and \\"Exit\\". - Use `curses` functions to handle user input for navigating through the menu. 3. **Note Editing Window**: - Allow users to type and edit text within this window. - Add the option to save the temporary note content to a list of notes. 4. **Color and Attributes**: - Use different colors or attributes (like bold or underline) to highlight active windows or selected menu items. 5. **Basic Key Bindings**: - Assign specific keys for navigating the menu and performing actions. Input and Output Formats - **Input**: All user interactions will be performed through keyboard shortcuts and terminal inputs. - **Output**: The application should dynamically update and display text on the terminal screen using curses. Constraints - Use only the standard `curses` module available in Python\'s standard library. - Do not use any additional third-party libraries. - Handle potential exceptions, such as mouse events, gracefully. - Ensure the application restores the terminal to its original state upon exiting. Performance Requirements - Efficient screen updates with minimal flicker. - Smooth handling of user inputs without considerable delay. Example Here is an example of how the application\'s interface might look: ``` +------------------------+ | Menu | |------------------------+ | * New Note | | Edit Note | | Delete Note | | Save Note | | Exit | +------------------------+ +------------------------+ | Note Editing Window | |------------------------+ | | | | | | +------------------------+ ``` Additional Notes - To help you get started, you can use the `curses.wrapper()` function to initialize your curses application safely. - Make sure to understand and utilize functions like `curses.textpad.Textbox` for handling text input within a window, `color_pair`, `curs_set`, and others relevant to screen and input management. **Evaluation Criteria** - Correct and efficient use of the curses library. - Clarity and readability of code. - Comprehensive handling of user input and errors. - Proper cleanup of the curses environment upon exit. - Creativity in leveraging the functionalities provided by the `curses` module. Submission Submit your Python script as a single `.py` file. Ensure the code is well-commented to explain key parts and any assumptions made.","solution":"import curses import curses.textpad class NoteApp: def __init__(self, stdscr): self.stdscr = stdscr self.notes = [] self.choice = 0 def draw_menu(self): menu_items = [\\"New Note\\", \\"Edit Note\\", \\"Delete Note\\", \\"Save Note\\", \\"Exit\\"] h, w = self.stdscr.getmaxyx() menu_win = curses.newwin(10, 40, 5, 5) menu_win.box() menu_win.bkgd(\' \', curses.color_pair(1)) for idx, item in enumerate(menu_items): x = 1 y = 1 + idx if idx == self.choice: menu_win.attron(curses.color_pair(2)) menu_win.attron(curses.A_BOLD) menu_win.addstr(y, x, \\"* \\" + item) menu_win.attroff(curses.A_BOLD) menu_win.attroff(curses.color_pair(2)) else: menu_win.addstr(y, x, \\" \\" + item) self.stdscr.refresh() menu_win.refresh() def edit_note_window(self): edit_win = curses.newwin(20, 70, 5, 45) edit_win.box() edit_win.bkgd(\' \', curses.color_pair(1)) edit_win.refresh() txtbox = curses.textpad.Textbox(edit_win) edit_win.addstr(1, 1, \\"Type your note here and press Ctrl+G to save.\\") edit_win.refresh() txtbox.edit() note = txtbox.gather() return note def main_loop(self): curses.init_pair(1, curses.COLOR_WHITE, curses.COLOR_BLACK) curses.init_pair(2, curses.COLOR_BLACK, curses.COLOR_WHITE) while True: self.stdscr.clear() self.draw_menu() key = self.stdscr.getch() if key == curses.KEY_UP and self.choice > 0: self.choice -= 1 elif key == curses.KEY_DOWN and self.choice < 4: self.choice += 1 elif key == curses.KEY_ENTER or key in [10, 13]: if self.choice == 0: # New Note note = self.edit_note_window() self.notes.append(note.strip()) elif self.choice == 1: # Edit Note if self.notes: note_idx = self.select_note() if note_idx is not None: note = self.edit_note_window() self.notes[note_idx] = note.strip() elif self.choice == 2: # Delete Note if self.notes: note_idx = self.select_note() if note_idx is not None: del self.notes[note_idx] elif self.choice == 3: # Save Note self.save_notes() elif self.choice == 4: # Exit break self.stdscr.refresh() def select_note(self): note_win = curses.newwin(20, 40, 5, 45) note_win.box() note_win.bkgd(\' \', curses.color_pair(1)) note_win.refresh() choice = 0 while True: note_win.clear() note_win.box() for idx, note in enumerate(self.notes): if idx == choice: note_win.attron(curses.color_pair(2)) note_win.attron(curses.A_BOLD) note_win.addstr(1 + idx, 1, \\"* \\" + note[:30]) note_win.attroff(curses.A_BOLD) note_win.attroff(curses.color_pair(2)) else: note_win.addstr(1 + idx, 1, \\" \\" + note[:30]) note_win.refresh() key = note_win.getch() if key == curses.KEY_UP and choice > 0: choice -= 1 elif key == curses.KEY_DOWN and choice < len(self.notes) - 1: choice += 1 elif key == curses.KEY_ENTER or key in [10, 13]: return choice def save_notes(self): with open(\\"notes.txt\\", \\"w\\") as f: for note in self.notes: f.write(note + \'n\') message_win = curses.newwin(3, 30, 15, 45) message_win.box() message_win.addstr(1, 1, \\"Notes saved successfully!\\") self.stdscr.refresh() message_win.refresh() self.stdscr.getch() # Wait for user to acknowledge the message def main(stdscr): app = NoteApp(stdscr) app.main_loop() if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"Objective: Assess the students\' proficiency in implementing outlier detection using the `IsolationForest` and `LocalOutlierFactor` from the scikit-learn library, understanding their usage, evaluating their performance, and differentiating between outlier and novelty detection. Problem Statement: You are provided with a dataset containing features of various observations, some of which are outliers. Your task is to implement a pipeline that performs the following: 1. Detect outliers using the `IsolationForest` algorithm. 2. Detect and score novelties using the `LocalOutlierFactor` algorithm with the `novelty` parameter set to `True`. 3. Compare the performance of both algorithms on the given dataset. Dataset: Assume you are provided with a dataset `data.csv` containing numeric features. The dataset has no missing values. Use the first 800 observations for training and the remaining for testing. Instructions: 1. **Load the Dataset:** Load the dataset from `data.csv`. Split it into training (first 800 observations) and testing sets (remaining observations). 2. **Implement IsolationForest:** - Train the `IsolationForest` model on the training data. - Use the trained model to predict outliers in the testing set. 3. **Implement LocalOutlierFactor:** - Initialize the `LocalOutlierFactor` algorithm with `novelty=True`. - Train the model on the training data. - Use the trained model to score novelties in the testing set and predict labels. 4. **Performance Evaluation:** - Compare the percentage of detected outliers/novelties in the test set for both models. - Plot the decision boundaries (if applicable) and the detected outliers/novelties. Expected Function Signature: ```python import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.ensemble import IsolationForest from sklearn.neighbors import LocalOutlierFactor def load_data(file_path: str) -> pd.DataFrame: # Load the data from the CSV file pass def detect_outliers_with_isolation_forest(X_train: np.ndarray, X_test: np.ndarray) -> np.ndarray: # Train IsolationForest and predict outliers in the test set pass def detect_novelties_with_lof(X_train: np.ndarray, X_test: np.ndarray) -> np.ndarray: # Train LOF for novelty detection and predict novelties in the test set pass def evaluate_performance(y_pred_if: np.ndarray, y_pred_lof: np.ndarray) -> None: # Evaluate and compare the performance of the models # Plot decision boundaries and detected outliers/novelties pass def main(): data = load_data(\'data.csv\') X_train, X_test = data[:800], data[800:] y_pred_if = detect_outliers_with_isolation_forest(X_train, X_test) y_pred_lof = detect_novelties_with_lof(X_train, X_test) evaluate_performance(y_pred_if, y_pred_lof) if __name__ == \\"__main__\\": main() ``` Constraints: - Use a random state of 42 wherever randomness is involved to ensure reproducibility. - Use default model parameters unless specified. - Ensure the solution is efficient and runs within a reasonable time frame. Evaluation Criteria: - Correct implementation and usage of `IsolationForest` and `LocalOutlierFactor`. - Accuracy in detecting and scoring outliers/novelties. - Quality and clarity of the performance evaluation and plots. - Code readability and proper documentation.","solution":"import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.ensemble import IsolationForest from sklearn.neighbors import LocalOutlierFactor def load_data(file_path: str) -> pd.DataFrame: Load the dataset from the provided CSV file path. Parameters: file_path (str): The path to the CSV file. Returns: pd.DataFrame: The loaded dataset. return pd.read_csv(file_path) def detect_outliers_with_isolation_forest(X_train: np.ndarray, X_test: np.ndarray) -> np.ndarray: Train the IsolationForest model on the training data and predict outliers in the test set. Parameters: X_train (np.ndarray): Training data. X_test (np.ndarray): Testing data. Returns: np.ndarray: Predictions of outliers in the test set (-1 for outlier, 1 for inlier). model = IsolationForest(random_state=42) model.fit(X_train) predictions = model.predict(X_test) return predictions def detect_novelties_with_lof(X_train: np.ndarray, X_test: np.ndarray) -> np.ndarray: Train the LocalOutlierFactor model for novelty detection on the training data and predict novelties in the test set. Parameters: X_train (np.ndarray): Training data. X_test (np.ndarray): Testing data. Returns: np.ndarray: Predictions of novelties in the test set (-1 for novelty, 1 for inlier). model = LocalOutlierFactor(novelty=True) model.fit(X_train) predictions = model.predict(X_test) return predictions def evaluate_performance(y_pred_if: np.ndarray, y_pred_lof: np.ndarray) -> None: Evaluate and compare the performance of the IsolationForest and LocalOutlierFactor models. Parameters: y_pred_if (np.ndarray): Predictions from the IsolationForest model. y_pred_lof (np.ndarray): Predictions from the LocalOutlierFactor model. outliers_if = np.sum(y_pred_if == -1) outliers_lof = np.sum(y_pred_lof == -1) print(f\\"IsolationForest detected {outliers_if} outliers in the test set.\\") print(f\\"LocalOutlierFactor detected {outliers_lof} novelties in the test set.\\") def main(): data = load_data(\'data.csv\') X_train, X_test = data[:800].to_numpy(), data[800:].to_numpy() y_pred_if = detect_outliers_with_isolation_forest(X_train, X_test) y_pred_lof = detect_novelties_with_lof(X_train, X_test) evaluate_performance(y_pred_if, y_pred_lof) if __name__ == \\"__main__\\": main()"},{"question":"Objective Design and implement a command-line tool using the `argparse` module that simulates a basic file processing system. The tool should be able to perform the following operations: list, read, write, and delete a simulated file entry. Requirements 1. **Positional Arguments**: - `operation`: The operation to perform (`list`, `read`, `write`, or `delete`). - `filename`: The name of the file (required for `read`, `write`, and `delete` operations). 2. **Optional Arguments**: - `--content`: The content to write to the file (required for the `write` operation). - `--all`: A flag to list all files when used with the `list` operation. 3. **Sub-commands**: - The program should support sub-commands for `list`, `read`, `write`, and `delete`. 4. **Mutually Exclusive Arguments**: - Ensure that `--all` is not used with other options. 5. **Argument Groups**: - Group related arguments for better help message readability. 6. **Custom Actions**: - Use a custom action to validate the content argument for the `write` operation. Input and Output - Input: Command-line arguments. - Output: Print appropriate messages based on the operation and arguments provided. Constraints - All file operations are simulated and do not actually manipulate the filesystem. - Handle errors gracefully and display user-friendly messages. Example ```bash python file_tool.py list --all Listing all files... python file_tool.py write myfile.txt --content \\"Hello, World!\\" File \'myfile.txt\' written with content. python file_tool.py read myfile.txt Reading file \'myfile.txt\': Hello, World! python file_tool.py delete myfile.txt File \'myfile.txt\' deleted. ``` Implementation 1. Create an `ArgumentParser` object. 2. Define the main positional arguments and sub-commands. 3. Add optional arguments to the relevant sub-commands, including mutually exclusive and required arguments. 4. Implement custom actions to validate and handle specific argument processing. 5. Parse the command-line arguments and implement the logic for each sub-command. Helpful Tips - Refer to the provided `argparse` documentation for details on creating parsers, adding arguments, and customizing behaviors. - Use the `ArgumentParser` methods to display help and error messages automatically. - Test your implementation with various combinations of arguments to ensure robustness. ```python import argparse def validate_content(value): if not value.strip(): raise argparse.ArgumentTypeError(\\"Content cannot be empty or whitespace.\\") return value def list_files(args): if args.all: print(\\"Listing all files...\\") else: print(\\"Please specify criteria to list files.\\") def read_file(args): print(f\\"Reading file \'{args.filename}\':\\") print(\\"Content of the file.\\") def write_file(args): print(f\\"File \'{args.filename}\' written with content:n{args.content}\\") def delete_file(args): print(f\\"File \'{args.filename}\' deleted.\\") def main(): parser = argparse.ArgumentParser(description=\\"Simulated file processing tool.\\") subparsers = parser.add_subparsers(dest=\'operation\', required=True, help=\\"Operation to perform\\") list_parser = subparsers.add_parser(\'list\', help=\\"List files\\") list_parser.add_argument(\'--all\', action=\'store_true\', help=\\"List all files\\") list_parser.set_defaults(func=list_files) read_parser = subparsers.add_parser(\'read\', help=\\"Read a file\\") read_parser.add_argument(\'filename\', type=str, help=\\"Name of the file to read\\") read_parser.set_defaults(func=read_file) write_parser = subparsers.add_parser(\'write\', help=\\"Write to a file\\") write_parser.add_argument(\'filename\', type=str, help=\\"Name of the file to write to\\") write_parser.add_argument(\'--content\', type=validate_content, required=True, help=\\"Content to write to the file\\") write_parser.set_defaults(func=write_file) delete_parser = subparsers.add_parser(\'delete\', help=\\"Delete a file\\") delete_parser.add_argument(\'filename\', type=str, help=\\"Name of the file to delete\\") delete_parser.set_defaults(func=delete_file) args = parser.parse_args() args.func(args) if __name__ == \\"__main__\\": main() ```","solution":"import argparse # Custom action to validate content for the write operation def validate_content(value): if not value.strip(): raise argparse.ArgumentTypeError(\\"Content cannot be empty or whitespace.\\") return value # Define operations functions def list_files(args): if args.all: print(\\"Listing all files...\\") else: print(\\"Listing specified files...\\") # More logic can be added here def read_file(args): print(f\\"Reading file \'{args.filename}\':\\") print(\\"Simulated content of the file.\\") # Simulated read operation def write_file(args): print(f\\"File \'{args.filename}\' written with content:n{args.content}\\") def delete_file(args): print(f\\"File \'{args.filename}\' deleted.\\") def main(): parser = argparse.ArgumentParser(description=\\"Simulated file processing tool.\\") subparsers = parser.add_subparsers(dest=\'operation\', required=True, help=\\"Operation to perform\\") # List files sub-command list_parser = subparsers.add_parser(\'list\', help=\\"List files\\") list_parser.add_argument(\'--all\', action=\'store_true\', help=\\"List all files\\") list_parser.set_defaults(func=list_files) # Read file sub-command read_parser = subparsers.add_parser(\'read\', help=\\"Read a file\\") read_parser.add_argument(\'filename\', type=str, help=\\"Name of the file to read\\") read_parser.set_defaults(func=read_file) # Write file sub-command write_parser = subparsers.add_parser(\'write\', help=\\"Write to a file\\") write_parser.add_argument(\'filename\', type=str, help=\\"Name of the file to write to\\") write_parser.add_argument(\'--content\', type=validate_content, required=True, help=\\"Content to write to the file\\") write_parser.set_defaults(func=write_file) # Delete file sub-command delete_parser = subparsers.add_parser(\'delete\', help=\\"Delete a file\\") delete_parser.add_argument(\'filename\', type=str, help=\\"Name of the file to delete\\") delete_parser.set_defaults(func=delete_file) # Parse arguments and call the respective function args = parser.parse_args() args.func(args) if __name__ == \\"__main__\\": main()"},{"question":"# Custom Pickling with `copyreg` You have been introduced to the `copyreg` module, which is used to control how objects are serialized and deserialized using the `pickle` module. Your task is to implement a custom reduction function for a class, register it using `copyreg`, and demonstrate its usage. Problem Statement 1. Define a class `Person` with the following properties: - `name` (string) - `age` (integer) 2. Implement a reduction function `pickle_person(person)` that defines how an instance of `Person` should be pickled. The reduction function should return the `Person` class itself and a tuple containing the values of the instance\'s properties (`name` and `age`). 3. Register the reduction function for the `Person` class using `copyreg.pickle`. 4. Write a function `test_custom_pickling()` that: - Creates an instance of `Person`. - Uses the `pickle` module to serialize (`pickle.dumps`) and then deserialize (`pickle.loads`) the instance. - Prints the deserialized instance to verify the custom pickling logic. Input and Output - No input function for the `Person` class is necessary. - The `test_custom_pickling()` function should print the deserialized instance of `Person`. Example Usage ```python # Define the Person class class Person: def __init__(self, name, age): self.name = name self.age = age # Define the reduction function def pickle_person(person): return Person, (person.name, person.age) # Register the reduction function copyreg.pickle(Person, pickle_person) # Test the custom pickling logic def test_custom_pickling(): john = Person(\\"John Doe\\", 30) serialized_john = pickle.dumps(john) deserialized_john = pickle.loads(serialized_john) print(f\\"Name: {deserialized_john.name}, Age: {deserialized_john.age}\\") # Expected Output: # Name: John Doe, Age: 30 ``` Constraints - Ensure that only Python 3.10 is compatible when implementing and testing the solution. - You should not use any additional libraries other than `copyreg` and `pickle`. - Your implementation should correctly handle any valid values for the `name` and `age` properties of the `Person` class.","solution":"import copyreg import pickle class Person: def __init__(self, name, age): self.name = name self.age = age def pickle_person(person): return Person, (person.name, person.age) copyreg.pickle(Person, pickle_person) def test_custom_pickling(): john = Person(\\"John Doe\\", 30) serialized_john = pickle.dumps(john) deserialized_john = pickle.loads(serialized_john) # Printing to verify the deserialized instance print(f\\"Name: {deserialized_john.name}, Age: {deserialized_john.age}\\")"},{"question":"# PyTorch Dynamic Control Flow with `torch.cond` Problem Statement You are tasked with implementing a PyTorch model that uses dynamic control flow based on the statistical properties of the input tensor. Specifically, you will create a model that branches its logic based on the mean value of the input data. Objective - Implement a PyTorch module `MeanBasedCond` that uses `torch.cond` to apply different operations based on whether the mean of the tensor elements is greater than a specified threshold. Instructions 1. Define a class `MeanBasedCond` inheriting from `torch.nn.Module`. 2. Implement the `__init__` method to initialize the necessary components. 3. Implement the `forward` method to: - Compute the mean of the input tensor. - Use `torch.cond` to branch into different operations based on whether the mean is greater than a given threshold. 4. Define: - `true_fn(x: torch.Tensor)` which returns the element-wise exponential of `x`. - `false_fn(x: torch.Tensor)` which returns the element-wise logarithm of `x` (ensure input values are positive to avoid computation errors). 5. The input tensor can have any shape, but the elements will be positive. 6. The threshold value will be provided to the model. Constraints - Do not use Python\'s built-in control flow (i.e., `if` statements) to handle branching within the `forward` method; you must utilize `torch.cond`. - Ensure no mutations on inputs or global state. Function Signature ```python import torch import torch.nn as nn class MeanBasedCond(nn.Module): def __init__(self, threshold: float): super(MeanBasedCond, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: mean_value = x.mean() return torch.cond(mean_value > self.threshold, self.true_fn, self.false_fn, (x,)) def true_fn(self, x: torch.Tensor) -> torch.Tensor: return torch.exp(x) def false_fn(self, x: torch.Tensor) -> torch.Tensor: return torch.log(x) ``` Example ```python # Sample inputs input_tensor = torch.tensor([0.5, 1.0, 1.5, 2.0]) threshold = 1.0 # Create and evaluate the model model = MeanBasedCond(threshold) output = model(input_tensor) print(output) # Output depends on whether the mean of input_tensor is greater than 1.0 ``` # Expected Output - If the mean of `input_tensor` is greater than `threshold`, `output` should be `torch.exp(input_tensor)`. - If the mean of `input_tensor` is less than or equal to `threshold`, `output` should be `torch.log(input_tensor)`. Notes - Ensure your model handles varying input shapes. - Test the model with different threshold values and input tensors to verify correctness.","solution":"import torch import torch.nn as nn class MeanBasedCond(nn.Module): def __init__(self, threshold: float): super(MeanBasedCond, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: mean_value = x.mean() if mean_value > self.threshold: result = self.true_fn(x) else: result = self.false_fn(x) return result def true_fn(self, x: torch.Tensor) -> torch.Tensor: return torch.exp(x) def false_fn(self, x: torch.Tensor) -> torch.Tensor: return torch.log(x)"},{"question":"**Title**: Implement and Test CUDA Stream Synchronization in PyTorch **Question**: You are provided with a task to simulate a common data race problem using CUDA streams in PyTorch and then solve it by properly synchronizing the streams to avoid the race condition. # Requirements 1. **Function Implementation**: - **Function Name**: `synchronize_cuda_streams` - **Input**: - A tensor `input_tensor` of any shape, initialized on the default CUDA stream. - **Output**: - A tensor that is a result of multiplying `input_tensor` by a scalar `factor` on a new CUDA stream, ensuring the computation is synchronized correctly. - **Constraints**: - The function should handle the synchronization to avoid any data race conditions as described in the example. 2. **Testing**: - Write a script to: - Create a random Tensor `input_tensor` on a CUDA device. - Demonstrate a race condition by modifying the tensor on a new stream without synchronization. - Show how the `synchronize_cuda_streams` function resolves the race condition. # Example: ```python import torch def synchronize_cuda_streams(input_tensor, factor=5): new_stream = torch.cuda.Stream() with torch.cuda.stream(new_stream): # Ensure the new_stream waits for all operations on the default stream torch.cuda.current_stream().wait_stream(torch.cuda.default_stream()) output_tensor = torch.mul(input_tensor, factor) return output_tensor # Script to demonstrate the issue and test the solution if __name__ == \\"__main__\\": # Generate a random tensor on CUDA device input_tensor = torch.rand(4, 2, device=\'cuda\') # Simulate race condition with torch.cuda.stream(torch.cuda.Stream()): torch.mul(input_tensor, 5, out=input_tensor) # Now use the function to correctly handle stream synchronization result_tensor = synchronize_cuda_streams(input_tensor, 5) print(result_tensor) ``` Make sure to test the script with `TORCH_CUDA_SANITIZER=1` to verify that there is no data race warning post synchronization. **Note**: Ensure CUDA is properly configured on your environment for testing.","solution":"import torch def synchronize_cuda_streams(input_tensor, factor=5): Multiplies the input_tensor by a scalar factor on a new CUDA stream, ensuring synchronization to avoid race conditions. :param input_tensor: torch.Tensor, the input tensor initialized on the default CUDA stream :param factor: scalar value to multiply the input_tensor :return: torch.Tensor, the resulting tensor after multiplication new_stream = torch.cuda.Stream() # Ensure the new_stream waits for all operations on the default stream new_stream.wait_stream(torch.cuda.default_stream()) with torch.cuda.stream(new_stream): output_tensor = input_tensor.clone() output_tensor.mul_(factor) # Ensure default stream waits for new_stream to finish its operations torch.cuda.default_stream().wait_stream(new_stream) return output_tensor"},{"question":"**Assessment Question**: You are provided with a dataset in CSV format containing sales data for a retail store over several years. The dataset has the following columns: - `Date`: The date of the sales record. - `Product`: The name of the product sold. - `Category`: The category of the product, such as \'Electronics\', \'Furniture\', \'Groceries\', etc. - `Quantity`: The number of units sold. - `Price`: The price per unit of the product sold. - `Total_Sales`: The total sales amount calculated as `Quantity * Price`. Using the pandas library, implement a function `visualize_sales_data` that takes the file path of this CSV data as input, performs necessary data processing, and generates the following visualizations: 1. **Line Plot**: A line plot displaying the total sales over time. The x-axis should represent the time (Date) and the y-axis should represent the total sales (`Total_Sales`). Ensure the dates are properly formatted on the x-axis. 2. **Bar Plot**: A bar plot showing the total sales for each product category. The x-axis should represent the product categories, and the y-axis should represent the total sales (`Total_Sales`). Use a different color for each bar to distinguish between categories. 3. **Pie Chart**: A pie chart representing the proportion of total sales contributed by each product category. Ensure that the chart has labels and a legend indicating the categories. 4. **Scatter Plot**: A scatter plot showing the relationship between the quantity of products sold and their price. The x-axis should represent `Quantity`, and the y-axis should represent `Price`. Use color to differentiate between different `Category` values. 5. **Box Plot**: A box plot to visualize the distribution of sales quantities (`Quantity`) for each category. The x-axis should represent the categories, and the y-axis should represent the sales quantities. **Function Signature:** ```python import pandas as pd import matplotlib.pyplot as plt def visualize_sales_data(file_path: str) -> None: # Your code here ``` **Constraints:** - Use pandas for data manipulation/processing. - Use matplotlib for generating visualizations. - Ensure that your visualizations are properly labeled and formatted for clarity. - Provide adequate comments in your code to explain your logic and design choices. **Input Format:** - The function will take a single argument, `file_path`, which is a string representing the file path to the CSV containing the sales data. **Output Format:** - The function should display the plots inline if running in a Jupyter Notebook, or show the plots in separate windows if running as a script. Example: ```python visualize_sales_data(\'sales_data.csv\') ``` **Sample Data Structure:** ``` Date Product Category Quantity Price Total_Sales 2020-01-01 A1 Electronics 5 100 500 2020-01-02 B1 Furniture 2 300 600 2020-01-03 C1 Groceries 10 20 200 ... ```","solution":"import pandas as pd import matplotlib.pyplot as plt def visualize_sales_data(file_path: str) -> None: # Read the CSV file into a DataFrame df = pd.read_csv(file_path) # Parse the Date column to datetime format df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Calculate total sales per day daily_sales = df.groupby(\'Date\')[\'Total_Sales\'].sum() # Plot 1: Line Plot of Total Sales over Time plt.figure(figsize=(12, 6)) daily_sales.plot(kind=\'line\') plt.title(\'Total Sales Over Time\') plt.xlabel(\'Date\') plt.ylabel(\'Total Sales\') plt.grid(True) plt.show() # Calculate total sales per category category_sales = df.groupby(\'Category\')[\'Total_Sales\'].sum() # Plot 2: Bar Plot of Total Sales by Category plt.figure(figsize=(12, 6)) category_sales.plot(kind=\'bar\', color=plt.cm.Paired(range(len(category_sales)))) plt.title(\'Total Sales by Category\') plt.xlabel(\'Category\') plt.ylabel(\'Total Sales\') plt.show() # Plot 3: Pie Chart of Total Sales by Category plt.figure(figsize=(12, 6)) category_sales.plot(kind=\'pie\', autopct=\'%1.1f%%\', labels=category_sales.index, colors=plt.cm.Paired(range(len(category_sales)))) plt.title(\'Proportion of Total Sales by Category\') plt.ylabel(\'\') plt.legend(category_sales.index, loc=\'upper left\') plt.show() # Plot 4: Scatter Plot of Quantity vs Price by Category plt.figure(figsize=(12, 6)) categories = df[\'Category\'].unique() for category in categories: subset = df[df[\'Category\'] == category] plt.scatter(subset[\'Quantity\'], subset[\'Price\'], label=category) plt.title(\'Quantity vs Price by Category\') plt.xlabel(\'Quantity\') plt.ylabel(\'Price\') plt.legend(loc=\'upper right\') plt.show() # Plot 5: Box Plot of Quantity by Category plt.figure(figsize=(12, 6)) df.boxplot(column=\'Quantity\', by=\'Category\') plt.title(\'Distribution of Sales Quantity by Category\') plt.suptitle(\'\') # Suppress the default title plt.xlabel(\'Category\') plt.ylabel(\'Quantity\') plt.show()"},{"question":"Objective Write a function named `style_dataframe` that takes in a DataFrame and applies various styles to enhance its readability. The function should return the styled DataFrame as an HTML string. Input and Output Formats - **Input**: - `df` (pandas.DataFrame): The DataFrame to be styled. - `max_highlight_color` (str): Hex color code to highlight maximum values. - `min_highlight_color` (str): Hex color code to highlight minimum values. - `null_highlight_color` (str): Hex color code to highlight null values. - `title` (str): Title to be set for the DataFrame. - **Output**: - Returns an HTML string of the styled DataFrame. Constraints 1. Apply a background gradient to all numerical columns. 2. Highlight the maximum values in each column with `max_highlight_color`. 3. Highlight the minimum values in each column with `min_highlight_color`. 4. Highlight null values with `null_highlight_color`. 5. Set a table caption using the `title` parameter. 6. Utilize at least three different styling methods from the `Styler` class. Performance Requirements Ensure that the function performs efficiently even with DataFrames having up to 10,000 rows and 50 columns. Function Signature ```python import pandas as pd def style_dataframe(df: pd.DataFrame, max_highlight_color: str, min_highlight_color: str, null_highlight_color: str, title: str) -> str: pass ``` Example Usage ```python import pandas as pd # Sample DataFrame data = { \'A\': [1, 2, None, 4], \'B\': [5, 6, 7, None], \'C\': [None, 10, 11, 12] } df = pd.DataFrame(data) # Example Function Call html_str = style_dataframe(df, \'#FFFF00\', \'#00FF00\', \'#FF0000\', \'Sample Styled DataFrame\') # Output print(html_str) # HTML string of the styled DataFrame ``` Notes - Ensure to use the `background_gradient`, `highlight_max`, `highlight_min`, `highlight_null`, and `set_caption` methods appropriately. - Make sure that the returned HTML string reflects all the styles applied.","solution":"import pandas as pd def style_dataframe(df: pd.DataFrame, max_highlight_color: str, min_highlight_color: str, null_highlight_color: str, title: str) -> str: Takes in a DataFrame and styles it to enhance readability, returning the styled DataFrame as an HTML string. :param df: pandas.DataFrame, the DataFrame to be styled :param max_highlight_color: str, hex color code to highlight maximum values :param min_highlight_color: str, hex color code to highlight minimum values :param null_highlight_color: str, hex color code to highlight null values :param title: str, title to be set for the DataFrame :return: str, HTML string of the styled DataFrame styled_df = (df.style .background_gradient(subset=pd.IndexSlice[:, df.select_dtypes(include=\'number\').columns]) .highlight_max(color=max_highlight_color, axis=0) .highlight_min(color=min_highlight_color, axis=0) .highlight_null(null_highlight_color) .set_caption(title)) return styled_df.to_html()"},{"question":"# Question: Secure Token Generator The goal of this exercise is to utilize the `secrets` module to create a secure token generator in Python. Problem Statement: You are to implement a function, `generate_secure_token`, which generates a secure token that can be used for various purposes like session identifiers, API keys, or password reset tokens. Your function should be able to generate tokens of different lengths based on the input parameter. Function Signature: ```python def generate_secure_token(length: int) -> str: pass ``` Input: - `length` (int): The desired length of the token in bytes. (1 <= length <= 64) Output: - Returns a securely generated random token as a hexadecimal string. Requirements: 1. The token must be generated using the `secrets` module. 2. Handle edge cases where the length might be out of the specified bounds by raising a `ValueError` with an appropriate message. 3. The function should produce a token of the requested length in a hex-encoded format. Example: ```python # Example usage of the generate_secure_token function token_16 = generate_secure_token(16) print(token_16) # Output: A random hex string of 32 characters (16 bytes) token_32 = generate_secure_token(32) print(token_32) # Output: A random hex string of 64 characters (32 bytes) ``` Note: Since the tokens generated are random, the actual output will vary each time the function is called. Constraints: - You must use Python 3.6 or later. - Ensure the function is optimized for performance and runs efficiently within the given constraints. Use the provided documentation to understand how to utilize the `secrets` module for secure random number generation and implement the `generate_secure_token` function accordingly.","solution":"import secrets def generate_secure_token(length: int) -> str: Generates a secure token of the specified length in bytes, encoded as a hexadecimal string. Parameters: length (int): The desired length of the token in bytes. Must be between 1 and 64 inclusive. Returns: str: A securely generated random token as a hexadecimal string. Raises: ValueError: If the length is out of the specified bounds. if not (1 <= length <= 64): raise ValueError(\\"Length must be within the range 1 to 64 inclusive.\\") return secrets.token_hex(length)"},{"question":"**Objective:** You are required to write a Python program that demonstrates understanding of multiple Python runtime services, including handling system parameters, usage of data classes, and effective resource management through context managers. This will include: 1. Retrieving system-specific information. 2. Implementing and using data classes. 3. Applying context managers for resource management. 4. Handling and logging exceptions with tracebacks. **Problem Statement:** 1. **System Information Retrieval:** - Write a function `get_system_info` that uses the `sys` module to fetch and return the following system-specific information: - Python version. - Platform name. - Command-line arguments passed to the script. - **Function signature:** ```python def get_system_info() -> dict: ``` - **Output format:** ```python { \\"python_version\\": \\"3.10.0\\", \\"platform_name\\": \\"linux\\", \\"command_line_args\\": [\\"script_name.py\\", \\"arg1\\", \\"arg2\\"] } ``` 2. **Data Classes:** - Create a data class `Book` using the `dataclasses` module that holds information about a book: - Title: `str` - Author: `str` - Year Published: `int` - ISBN: `str` - Ensure the class has a custom method `__str__` to display the book\'s information in a readable format. - **Class signature:** ```python from dataclasses import dataclass @dataclass class Book: title: str author: str year_published: int isbn: str def __str__(self): return f\\"\'{self.title}\' by {self.author} ({self.year_published}), ISBN: {self.isbn}\\" ``` - You will not directly call this class in your solution, but ensure it is implemented. 3. **Context Manager for File Operations:** - Implement a context manager `log_file` using `contextlib` that will: - Create or open a file named `log.txt`. - Write log messages to this file. - Ensure the file is properly closed once done. - **Class signature:** ```python from contextlib import contextmanager @contextmanager def log_file(): file = open(\'log.txt\', \'a\') try: yield file finally: file.close() ``` - Example usage: ```python with log_file() as file: file.write(\\"This is a log message.n\\") ``` 4. **Exception Handling with Tracebacks:** - Write a function `faulty_function` that deliberately raises a `ValueError` exception. - Implement another function `safe_function` that calls `faulty_function` and handles its exception, logging the error details and traceback using the `traceback` module. - **Function signatures:** ```python def faulty_function(): raise ValueError(\\"An intentional error.\\") def safe_function(): import traceback try: faulty_function() except ValueError as e: with log_file() as file: file.write(str(e) + \\"n\\") traceback.print_exc(file=file) ``` **Constraints:** - Ensure that the program adheres to proper coding conventions and practices, including meaningful variable names and comments where necessary. - Do not make any assumptions about file existence, input formats, or external factors not covered by the problem statement. **Performance Requirements:** - While performance is not the primary focus, ensure that the code efficiently handles the creation and cleanup of resources.","solution":"import sys import platform from dataclasses import dataclass from contextlib import contextmanager import traceback def get_system_info() -> dict: Retrieves system-specific information using sys and platform modules. return { \\"python_version\\": sys.version, \\"platform_name\\": platform.system().lower(), \\"command_line_args\\": sys.argv } @dataclass class Book: title: str author: str year_published: int isbn: str def __str__(self): return f\\"\'{self.title}\' by {self.author} ({self.year_published}), ISBN: {self.isbn}\\" @contextmanager def log_file(): A context manager for logging into a file named \'log.txt\'. file = open(\'log.txt\', \'a\') try: yield file finally: file.close() def faulty_function(): A function that deliberately raises an exception. raise ValueError(\\"An intentional error.\\") def safe_function(): Call faulty_function and handle the exception by logging it to a file. try: faulty_function() except ValueError as e: with log_file() as file: file.write(str(e) + \\"n\\") traceback.print_exc(file=file)"},{"question":"Advanced File Operations with shutil **Objective:** Your task is to implement a Python function that combines multiple high-level file operations provided by the `shutil` module to: 1. Create a backup of a specified directory. 2. Compress the backup. 3. Move the compressed backup to a target directory. **Function Specification:** ```python def backup_and_compress_directory(src_dir: str, backup_root_dir: str, target_dir: str, archive_format: str = \'gztar\') -> str: Create a backup of the specified directory, compress it, and move it to the target directory. Args: src_dir (str): The source directory path that needs to be backed up. backup_root_dir (str): The root directory where the backup will be created. target_dir (str): The target directory where the compressed backup will be moved. archive_format (str, optional): The format of the compressed archive. Defaults to \'gztar\'. Options include \'zip\', \'tar\', \'gztar\', \'bztar\', \'xztar\'. Returns: str: The path to the moved compressed archive in the target directory. Raises: ValueError: If `src_dir` does not exist or is not a directory. ValueError: If `target_dir` does not exist or is not a directory. ValueError: If the specified `archive_format` is invalid. pass ``` **Expected Behavior:** 1. The function should first verify that both `src_dir` and `target_dir` exist and are directories. If either of these checks fail, a `ValueError` should be raised. 2. Create a backup of the `src_dir` within the `backup_root_dir`. This can be achieved by copying the directory tree. 3. Compress the backup using the specified `archive_format`. The compressed file should be created in the `backup_root_dir` with the backup directory name. 4. Move the compressed backup archive to the `target_dir`. **Example:** ```python backup_and_compress_directory(\'/path/to/source\', \'/path/to/backup\', \'/path/to/target\', \'zip\') ``` This call should: 1. Verify `/path/to/source` exists and is a directory. 2. Verify `/path/to/target` exists and is a directory. 3. Copy the `/path/to/source` directory to `/path/to/backup`. 4. Compress the backup directory (created inside `/path/to/backup`) into a `zip` archive. 5. Move the compressed archive to `/path/to/target`. **Constraints:** - You may assume that the disk space is sufficient for the operations. - Use appropriate `shutil` functions to perform these operations. - Ensure proper error handling and cleanup in case of partial failures. Good luck!","solution":"import os import shutil def backup_and_compress_directory(src_dir: str, backup_root_dir: str, target_dir: str, archive_format: str = \'gztar\') -> str: Create a backup of the specified directory, compress it, and move it to the target directory. Args: src_dir (str): The source directory path that needs to be backed up. backup_root_dir (str): The root directory where the backup will be created. target_dir (str): The target directory where the compressed backup will be moved. archive_format (str, optional): The format of the compressed archive. Defaults to \'gztar\'. Options include \'zip\', \'tar\', \'gztar\', \'bztar\', \'xztar\'. Returns: str: The path to the moved compressed archive in the target directory. Raises: ValueError: If `src_dir` does not exist or is not a directory. ValueError: If `target_dir` does not exist or is not a directory. ValueError: If the specified `archive_format` is invalid. # Check if source directory exists and is a directory if not os.path.isdir(src_dir): raise ValueError(f\\"The source directory \'{src_dir}\' does not exist or is not a directory.\\") # Check if target directory exists and is a directory if not os.path.isdir(target_dir): raise ValueError(f\\"The target directory \'{target_dir}\' does not exist or is not a directory.\\") # Check if archive format is valid valid_formats = [\'zip\', \'tar\', \'gztar\', \'bztar\', \'xztar\'] if archive_format not in valid_formats: raise ValueError(f\\"Invalid archive format \'{archive_format}\'. Valid formats are: {valid_formats}\\") # Define basename for the backup base_name = os.path.join(backup_root_dir, os.path.basename(src_dir)) # Create a backup (copy src_dir to backup_root_dir) shutil.copytree(src_dir, base_name) # Compress the backup directory archive_name = shutil.make_archive(base_name, archive_format, backup_root_dir, os.path.basename(src_dir)) # Define the destination path in target directory target_archive_path = os.path.join(target_dir, os.path.basename(archive_name)) # Move the compressed archive to the target directory shutil.move(archive_name, target_archive_path) return target_archive_path"},{"question":"**Challenging Coding Assessment Question:** # Task You are given a CSV file containing information about several students and their grades in various subjects. Your task is to read this file, calculate the average grade for each student, and write the results to a new CSV file. When writing the results, make sure to use a custom CSV dialect. # Input 1. A CSV file named `students_grades.csv` with the following format (without headers): ``` student_id,student_name,subject,grade 1,John Doe,Math,85 1,John Doe,Science,90 2,Jane Smith,Math,95 2,Jane Smith,Science,80 3,Bob Johnson,Math,78 ``` # Output 1. A CSV file named `students_averages.csv` using a custom dialect that: * Uses a semicolon (`;`) as the delimiter. * Uses a pipe (`|`) as the quote character. * Quotes all fields. The format of the output file should be: ``` student_id;student_name;average_grade \\"1\\";\\"John Doe\\";\\"87.5\\" \\"2\\";\\"Jane Smith\\";\\"87.5\\" \\"3\\";\\"Bob Johnson\\";\\"78.0\\" ``` # Requirements 1. Calculate the average grade for each student. 2. Use the `csv.DictReader` class to read from the input CSV file. 3. Use the `csv.DictWriter` class to write to the output CSV file. 4. Register and use a custom CSV dialect as specified above. 5. Handle any potential errors in reading from and writing to the CSV files using appropriate exceptions. # Constraints - You can assume that the input CSV file is well-formed and does not contain any unexpected data. # Example Given the input file `students_grades.csv`: ``` student_id,student_name,subject,grade 1,John Doe,Math,85 1,John Doe,Science,90 2,Jane Smith,Math,95 2,Jane Smith,Science,80 3,Bob Johnson,Math,78 ``` The output file `students_averages.csv` should be: ``` student_id;student_name;average_grade \\"1\\";\\"John Doe\\";\\"87.5\\" \\"2\\";\\"Jane Smith\\";\\"87.5\\" \\"3\\";\\"Bob Johnson\\";\\"78.0\\" ``` # Notes 1. Pay attention to the handling of different delimiters and quote characters when writing the output CSV file. 2. Ensure that all fields in the output file are quoted, as specified by the custom dialect. # Implementation Provide the implementation for the above task in Python. Your solution should demonstrate a good understanding of the `csv` module, including reading, writing, and handling custom dialects. ```python import csv from collections import defaultdict # Register a custom dialect csv.register_dialect(\'custom_dialect\', delimiter=\';\', quotechar=\'|\', quoting=csv.QUOTE_ALL) def calculate_average_grades(input_file, output_file): # Reading the input CSV file with open(input_file, mode=\'r\', newline=\'\') as infile: reader = csv.DictReader(infile) # Using defaultdict to accumulate total grades and count of subjects student_grades = defaultdict(lambda: {\'total\': 0, \'count\': 0, \'name\': \'\'}) for row in reader: student_id = row[\'student_id\'] student_name = row[\'student_name\'] grade = float(row[\'grade\']) student_grades[student_id][\'total\'] += grade student_grades[student_id][\'count\'] += 1 student_grades[student_id][\'name\'] = student_name # Preparing data for writing to output CSV file output_data = [] for student_id, data in student_grades.items(): average_grade = data[\'total\'] / data[\'count\'] output_data.append({ \'student_id\': student_id, \'student_name\': data[\'name\'], \'average_grade\': f\'{average_grade:.1f}\' }) # Writing the output CSV file with the custom dialect with open(output_file, mode=\'w\', newline=\'\') as outfile: fieldnames = [\'student_id\', \'student_name\', \'average_grade\'] writer = csv.DictWriter(outfile, fieldnames=fieldnames, dialect=\'custom_dialect\') writer.writeheader() writer.writerows(output_data) # Example usage calculate_average_grades(\'students_grades.csv\', \'students_averages.csv\') ```","solution":"import csv from collections import defaultdict from typing import Dict # Register a custom dialect csv.register_dialect(\'custom_dialect\', delimiter=\';\', quotechar=\'|\', quoting=csv.QUOTE_ALL) def calculate_average_grades(input_file: str, output_file: str): Calculate the average grades for each student from the input CSV file and write the result to the output CSV file using a custom dialect. Parameters: input_file (str): The input CSV file with students\' grades. output_file (str): The output CSV file with students\' average grades. # Reading the input CSV file with open(input_file, mode=\'r\', newline=\'\') as infile: reader = csv.DictReader(infile) # Accumulate total grades and count of subjects for each student student_grades: Dict[str, Dict[str, float]] = defaultdict(lambda: {\'total\': 0, \'count\': 0, \'name\': \'\'}) for row in reader: student_id = row[\'student_id\'] student_name = row[\'student_name\'] grade = float(row[\'grade\']) student_grades[student_id][\'total\'] += grade student_grades[student_id][\'count\'] += 1 student_grades[student_id][\'name\'] = student_name # Preparing data for writing to the output CSV file output_data = [] for student_id, data in student_grades.items(): average_grade = data[\'total\'] / data[\'count\'] output_data.append({ \'student_id\': student_id, \'student_name\': data[\'name\'], \'average_grade\': f\'{average_grade:.1f}\' }) # Writing the output CSV file with the custom dialect with open(output_file, mode=\'w\', newline=\'\') as outfile: fieldnames = [\'student_id\', \'student_name\', \'average_grade\'] writer = csv.DictWriter(outfile, fieldnames=fieldnames, dialect=\'custom_dialect\') writer.writeheader() writer.writerows(output_data)"},{"question":"# Question: Directory Synchronization using File Comparisons You are given two directories, `dir1` and `dir2`. Your goal is to synchronize the contents of `dir1` with `dir2`. Specifically, the synchronization process should achieve the following: 1. **File Copying**: For each file in `dir2` that does not exist in `dir1`, copy the file to `dir1`. 2. **File Deletion**: For each file in `dir1` that does not exist in `dir2`, delete the file from `dir1`. 3. **File Updating**: For each file that exists in both `dir1` and `dir2` but has different contents, replace the version in `dir1` with the version from `dir2`. Function Signature ```python def synchronize_directories(dir1: str, dir2: str) -> None: Synchronizes the contents of dir1 with dir2. Args: dir1 (str): The path to the first directory. dir2 (str): The path to the second directory. Returns: None ``` Constraints - You can assume that `dir1` and `dir2` are both valid directory paths. - You do not need to handle symbolic links. - File copying and deletion should preserve file permissions. - The solution should handle directories with nested subdirectories. Example Assuming the following directory structures before synchronization: **dir1:** ``` /dir1 |-- file1.txt |-- file2.txt |-- subdir1 |-- file3.txt ``` **dir2:** ``` /dir2 |-- file2.txt |-- file4.txt |-- subdir1 |-- file3.txt |-- file5.txt |-- subdir2 |-- file6.txt ``` After calling `synchronize_directories(\'dir1\', \'dir2\')`, the directory structure of `dir1` should be: **dir1:** ``` /dir1 |-- file2.txt |-- file4.txt |-- subdir1 |-- file3.txt |-- file5.txt |-- subdir2 |-- file6.txt ``` Notes - Focus on using the functions and classes provided by the `filecmp` module. - You may use standard file and directory manipulation methods from Python\'s `os` and `shutil` modules. # Hints 1. Use `filecmp.dircmp` to identify differences between the directories. 2. Use `filecmp.cmp` or `filecmp.cmpfiles` to compare individual files. 3. Use `shutil.copy2` to copy files while preserving metadata. 4. Use `os.remove` to delete files. Good luck!","solution":"import os import filecmp import shutil def synchronize_directories(dir1: str, dir2: str) -> None: def sync_dirs(dcmp): for name in dcmp.left_only: left_path = os.path.join(dcmp.left, name) if os.path.isfile(left_path): os.remove(left_path) else: shutil.rmtree(left_path) for name in dcmp.right_only: right_path = os.path.join(dcmp.right, name) if os.path.isfile(right_path): shutil.copy2(right_path, dcmp.left) else: shutil.copytree(right_path, os.path.join(dcmp.left, name)) for name in dcmp.diff_files: left_path = os.path.join(dcmp.left, name) right_path = os.path.join(dcmp.right, name) shutil.copy2(right_path, left_path) for sub_dcmp in dcmp.subdirs.values(): sync_dirs(sub_dcmp) dcmp = filecmp.dircmp(dir1, dir2) sync_dirs(dcmp)"},{"question":"# Question: Data Preprocessing and Model Training with scikit-learn In this exercise, you will create synthetic data, preprocess it, and train a machine learning model using scikit-learn. Follow the steps below to complete the task. 1. **Data Generation**: - Generate a synthetic regression dataset with 1000 samples and 10 features using `make_regression` from scikit-learn\'s datasets module. - Convert the generated data to a pandas DataFrame. 2. **Data Preprocessing**: - Split the data into training and test sets using an 80-20 split. - Normalize the features using `StandardScaler` from scikit-learn. 3. **Model Training and Evaluation**: - Train a `GradientBoostingRegressor` model on the training data. - Evaluate the model on the test data using R score. 4. **Report Results**: - Print out the R score for the test data. **Input and Output Formats**: - **Input**: No specific input required for this task. - **Output**: Print the R score for the test data. **Constraints**: - Use a random state of 42 for reproducibility in `train_test_split` and `GradientBoostingRegressor`. Here is a skeleton code to help you get started: ```python import pandas as pd import numpy as np from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import r2_score # Step 1: Generate synthetic data X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42) # Convert to DataFrame df_X = pd.DataFrame(X, columns=[f\'feature_{i}\' for i in range(X.shape[1])]) df_y = pd.Series(y, name=\'target\') # Step 2: Split the data X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=42) # Normalize the data scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Step 3: Train the model model = GradientBoostingRegressor(random_state=42) model.fit(X_train_scaled, y_train) # Evaluate the model y_pred = model.predict(X_test_scaled) r2 = r2_score(y_test, y_pred) # Step 4: Report results print(\\"R score: {:.2f}\\".format(r2)) ``` Implement the code in the skeleton and ensure it runs correctly, producing the expected output.","solution":"import pandas as pd import numpy as np from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import r2_score def generate_and_train_model(): # Step 1: Generate synthetic data X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42) # Convert to DataFrame df_X = pd.DataFrame(X, columns=[f\'feature_{i}\' for i in range(X.shape[1])]) df_y = pd.Series(y, name=\'target\') # Step 2: Split the data X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=42) # Normalize the data scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Step 3: Train the model model = GradientBoostingRegressor(random_state=42) model.fit(X_train_scaled, y_train) # Evaluate the model y_pred = model.predict(X_test_scaled) r2 = r2_score(y_test, y_pred) # Step 4: Report results return r2"},{"question":"# Question: Comprehensive File Operations with `shutil` You are tasked with creating a script that performs a series of file operations using the Python `shutil` module. Your script should demonstrate advanced understanding and usage of the module. Specifically, your script should: 1. **Create a directory structure** from a given dictionary representing a file tree. 2. **Copy a specific directory tree** from the created structure to a new location. 3. **Create an archive (tar.gz format)** of the copied directory. 4. **Delete the original directory tree and the copied directory**. 5. **Print the total, used, and free disk space** after each significant operation. # Detailed Steps 1. **Create Directory Structure**: - Input: A dictionary representing the directory structure. ```python directory_structure = { \'root\': { \'folder1\': { \'file1.txt\': \'Content of file1\', \'folder2\': { \'file2.txt\': \'Content of file2\' } }, \'folder3\': {} } } ``` - Function Signature: `create_directory_structure(base_path: str, directory_structure: dict) -> None` - Create the specified directory structure starting from `base_path`. 2. **Copy Directory Tree**: - Input: Source directory path and destination directory path. - Function Signature: `copy_directory_tree(src: str, dst: str) -> None` - Recursively copy the source directory tree to the destination. 3. **Create Archive**: - Input: Directory path to archive. - Function Signature: `create_archive(directory: str, archive_name: str) -> str` - Create a tar.gz archive of the specified directory and return the archive file path. 4. **Delete Directory**: - Input: Directory path to delete. - Function Signature: `delete_directory(directory: str) -> None` - Delete the specified directory tree. 5. **Get Disk Usage**: - Input: Directory path to check. - Function Signature: `get_disk_usage(path: str) -> None` - Print the total, used, and free disk space. # Requirements - Implement the steps in specified functions with appropriate usage of `shutil` operations. - Ensure exception handling for common file operation errors. - Demonstrate the sequence of operations in the main part of the script: 1. Create the initial directory structure. 2. Print the disk usage. 3. Copy the directory tree. 4. Print the disk usage. 5. Create an archive of the copied directory. 6. Print the disk usage. 7. Delete the original and copied directories. 8. Print the disk usage after cleanup. # Example Usage ```python directory_structure = { \'root\': { \'folder1\': { \'file1.txt\': \'Content of file1\', \'folder2\': { \'file2.txt\': \'Content of file2\' } }, \'folder3\': {} } } base_path = \'base_directory\' src = \'base_directory/root\' dst = \'copied_directory/root\' # Creating the directory structure create_directory_structure(base_path, directory_structure) # Checking disk usage get_disk_usage(base_path) # Copying the directory tree copy_directory_tree(src, dst) # Checking disk usage get_disk_usage(base_path) # Creating an archive archive_path = create_archive(dst, \'archive\') # Checking disk usage get_disk_usage(base_path) # Deleting directories delete_directory(src) delete_directory(dst) # Checking disk usage after cleanup get_disk_usage(base_path) ``` Implement the required functions and demonstrate the example usage flow in your script.","solution":"import os import shutil import tarfile def create_directory_structure(base_path: str, directory_structure: dict) -> None: Create a directory structure from a dictionary representation. Parameters: base_path (str): The base path to create the structure. directory_structure (dict): The dictionary representation of the directory structure. for key, value in directory_structure.items(): path = os.path.join(base_path, key) if isinstance(value, dict): os.makedirs(path, exist_ok=True) create_directory_structure(path, value) else: with open(path, \'w\') as f: f.write(value) def copy_directory_tree(src: str, dst: str) -> None: Copy a directory tree from src to dst. Parameters: src (str): The source directory path. dst (str): The destination directory path. shutil.copytree(src, dst) def create_archive(directory: str, archive_name: str) -> str: Create a tar.gz archive of a directory. Parameters: directory (str): The directory to archive. archive_name (str): The name of the archive to create (without extension). Returns: str: The path of the created archive file. archive_path = f\\"{archive_name}.tar.gz\\" with tarfile.open(archive_path, \\"w:gz\\") as tar: tar.add(directory, arcname=os.path.basename(directory)) return archive_path def delete_directory(directory: str) -> None: Delete a directory tree. Parameters: directory (str): The directory to delete. shutil.rmtree(directory) def get_disk_usage(path: str) -> None: Print the total, used, and free disk space of the given path. Parameters: path (str): The path to check the disk usage. total, used, free = shutil.disk_usage(path) print(f\\"Disk usage for {path}:\\") print(f\\"Total: {total // (2**30)} GiB\\") print(f\\"Used: {used // (2**30)} GiB\\") print(f\\"Free: {free // (2**30)} GiB\\")"},{"question":"# PyTorch Distributed Training and Environment Configuration You are tasked with implementing a distributed training scenario using PyTorch\'s `ProcessGroupNCCL`. The goal is to ensure the setup is robust to errors and is well-optimized for performance. **Requirements:** 1. Implement a simple distributed training loop that uses `ProcessGroupNCCL` to synchronize weights across multiple GPUs. Each GPU should perform a small part of the training and then synchronize the results. 2. Configure the following environment variables appropriately to: - Handle asynchronous errors (`TORCH_NCCL_ASYNC_ERROR_HANDLING`) - Use high priority streams (`TORCH_NCCL_HIGH_PRIORITY`) - Enable detailed timing for collective operations (`TORCH_NCCL_ENABLE_TIMING`) - Check for NaNs in the input data (`TORCH_NCCL_NAN_CHECK`) 3. Write a function `setup_env_vars()` that sets the required environment variables with appropriate values to ensure robustness and performance. **Function Signature:** ```python def setup_env_vars() -> None: Set the necessary environment variables for NCCL. def distributed_training(rank: int, size: int) -> None: Perform distributed training across multiple GPUs. Args: - rank (int): The rank of the current process in the distributed setting. - size (int): Total number of processes involved in the distributed setting. ``` **Constraints:** - Use PyTorch 1.9.0 or later. - Assume 4 GPUs are available. - Implement basic error handling and logging to track the training process and any issues that arise. **Expected Outcome:** - A function `setup_env_vars()` that configures environment variables for `ProcessGroupNCCL`. - A basic implementation of the `distributed_training()` function that demonstrates synchronizing weights across GPUs. - The code should be robust and provide detailed logs and debugging information in case of errors. **Example Usage:** ```python if __name__ == \\"__main__\\": setup_env_vars() size = 4 # Assuming 4 GPUs processes = [] for rank in range(size): p = multiprocessing.Process(target=distributed_training, args=(rank, size)) p.start() processes.append(p) for p in processes: p.join() ``` **Note:** Provide a brief explanation of the choice of values for each environment variable in `setup_env_vars()`.","solution":"import os import torch import torch.distributed as dist import torch.multiprocessing as mp def setup_env_vars() -> None: Set the necessary environment variables for NCCL. os.environ[\'TORCH_NCCL_ASYNC_ERROR_HANDLING\'] = \'1\' # Ensures asynchronous errors are caught os.environ[\'TORCH_NCCL_HIGH_PRIORITY\'] = \'1\' # Enables the use of high priority streams os.environ[\'TORCH_NCCL_ENABLE_TIMING\'] = \'1\' # Enables detailed timing for collective operations os.environ[\'TORCH_NCCL_NAN_CHECK\'] = \'1\' # Checks for NaNs in the input data def distributed_training(rank: int, size: int) -> None: Perform distributed training across multiple GPUs. Args: - rank (int): The rank of the current process in the distributed setting. - size (int): Total number of processes involved in the distributed setting. # Initialize the process group dist.init_process_group(\\"nccl\\", rank=rank, world_size=size) torch.cuda.set_device(rank) # Create a simple model and move to the current GPU model = torch.nn.Linear(10, 10).cuda(rank) # Create a dummy optimizer optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # Dummy input and target input_data = torch.randn(32, 10).cuda(rank) target = torch.randn(32, 10).cuda(rank) # Training loop for epoch in range(10): optimizer.zero_grad() output = model(input_data) loss = torch.nn.functional.mse_loss(output, target) loss.backward() # Average gradients across all GPUs for param in model.parameters(): dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM) param.grad.data /= size optimizer.step() if rank == 0: print(f\\"Epoch {epoch}, Loss: {loss.item()}\\") # Cleanup dist.destroy_process_group() if __name__ == \\"__main__\\": setup_env_vars() size = 4 # Assuming 4 GPUs processes = [] for rank in range(size): p = mp.Process(target=distributed_training, args=(rank, size)) p.start() processes.append(p) for p in processes: p.join()"},{"question":"**Objective**: Implement a distributed checkpointing process for a PyTorch model using the Distributed Checkpoint (DCP) functionality. Background You are given a PyTorch model that needs to be trained and checkpointed across multiple devices (e.g., nodes in a distributed training setup). You will implement functions to save and load model checkpoints using PyTorch\'s Distributed Checkpoint (DCP) feature. This ensures that the training state can be saved and resumed efficiently, even on different cluster topologies. Requirements 1. **Save Checkpoint**: Write a function `save_checkpoint(model, optimizer, epoch, checkpoint_dir)` that saves the model and optimizer state to the specified directory. The checkpoint should include: - Model state dictionary - Optimizer state dictionary - Current epoch 2. **Load Checkpoint**: Write a function `load_checkpoint(model, optimizer, checkpoint_dir)` that loads the model and optimizer state from the specified directory. The function should return the epoch from which to resume training. 3. **Compatibility**: Ensure that your implementation accounts for situations where the model might be trained on different cluster topologies. Input and Output Formats - `save_checkpoint(model, optimizer, epoch, checkpoint_dir)` - **Inputs**: - `model`: PyTorch model to be checkpointed. - `optimizer`: Optimizer state to be checkpointed. - `epoch`: The current epoch number. - `checkpoint_dir`: Directory path where checkpoints will be saved. - **Outputs**: Should save the checkpoint to the specified directory. No return value. - `load_checkpoint(model, optimizer, checkpoint_dir)` - **Inputs**: - `model`: PyTorch model to which the state will be loaded. - `optimizer`: Optimizer to which the state will be loaded. - `checkpoint_dir`: Directory path from where checkpoints will be loaded. - **Outputs**: Returns the epoch number from which training should resume. Constraints or Limitations - The implementation should support both synchronous and asynchronous saving operations. - The checkpoint directory should be accessible from all participating ranks. - Ensure the solution handles potential inconsistencies in state dictionary keys due to distributed training. Example ```python # Example usage of save_checkpoint and load_checkpoint functions import torch import torch.optim as optim from torch.nn import Module # Define a simple model class SimpleModel(Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = torch.nn.Linear(10, 1) def forward(self, x): return self.fc(x) # Initialize model and optimizer model = SimpleModel() optimizer = optim.SGD(model.parameters(), lr=0.01) # Directory to save checkpoints checkpoint_dir = \'./checkpoints\' # Save a checkpoint save_checkpoint(model, optimizer, epoch=5, checkpoint_dir=checkpoint_dir) # Load the checkpoint epoch = load_checkpoint(model, optimizer, checkpoint_dir=checkpoint_dir) print(f\'Resuming training from epoch {epoch}\') ``` **Note**: Your implementation should be compatible with both `torch.save` and `torch.load` formats and should utilize the PyTorch Distributed Checkpoint (DCP) functionality.","solution":"import os import torch def save_checkpoint(model, optimizer, epoch, checkpoint_dir): Saves the model and optimizer state to the specified directory. if not os.path.exists(checkpoint_dir): os.makedirs(checkpoint_dir) checkpoint = { \'epoch\': epoch, \'model_state_dict\': model.state_dict(), \'optimizer_state_dict\': optimizer.state_dict() } checkpoint_path = os.path.join(checkpoint_dir, f\'checkpoint_epoch_{epoch}.pth\') torch.save(checkpoint, checkpoint_path) def load_checkpoint(model, optimizer, checkpoint_dir): Loads the model and optimizer state from the specified directory. Returns the epoch from which to resume training. checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.startswith(\'checkpoint_epoch_\') and f.endswith(\'.pth\')] if not checkpoint_files: raise FileNotFoundError(\'No checkpoint found in the directory.\') # Assuming the checkpoint files are named with the epoch number and sort desc latest_checkpoint = sorted(checkpoint_files, key=lambda x: int(x.split(\'_\')[-1].split(\'.\')[0]))[-1] checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint) checkpoint = torch.load(checkpoint_path) model.load_state_dict(checkpoint[\'model_state_dict\']) optimizer.load_state_dict(checkpoint[\'optimizer_state_dict\']) return checkpoint[\'epoch\']"},{"question":"You are tasked with creating a scheduler that repeatedly executes a given set of asynchronous functions at specific intervals using the `asyncio` event loop. The scheduler should be implemented as a class, `AsyncScheduler`, which provides functionality to add tasks, start the scheduler, and stop it gracefully. Implementation Details 1. **Class: `AsyncScheduler`** **Attributes:** - `loop`: The event loop instance. - `tasks`: A dictionary to store tasks with their associated intervals. **Methods:** - `__init__(self)`: Initialize the scheduler with an event loop. - `add_task(self, coro, interval)`: Add a coroutine function `coro` to be executed every `interval` seconds. - `start(self)`: Start the scheduler and run all added tasks at their specified intervals. - `stop(self)`: Stop the scheduler gracefully, ensuring all running tasks are completed. 2. **Constraints:** - `interval` is an integer or a float representing seconds and must be greater than zero. - All added tasks should be coroutine functions (using `async def` syntax). 3. **Input and Output:** - There are no direct inputs and outputs for the class methods. Instead, your implementation should demonstrate the correct behavior by running example tasks and showing their intervals. 4. **Performance Requirements:** - The scheduler should handle tasks efficiently, preventing any tasks from blocking the event loop. - Ensure that tasks can run concurrently without interfering with each other. Example Usage ```python import asyncio async def task1(): print(\\"Task 1 executed\\") async def task2(): print(\\"Task 2 executed\\") scheduler = AsyncScheduler() scheduler.add_task(task1, 2) # Execute every 2 seconds scheduler.add_task(task2, 3) # Execute every 3 seconds # Start the scheduler try: asyncio.run(scheduler.start()) except KeyboardInterrupt: # Stop the scheduler gracefully scheduler.stop() ``` Write the complete implementation of the `AsyncScheduler` class below. ```python class AsyncScheduler: def __init__(self): self.loop = asyncio.get_event_loop() self.tasks = {} def add_task(self, coro, interval): if not asyncio.iscoroutinefunction(coro): raise TypeError(\\"The added task must be a coroutine function\\") if not isinstance(interval, (int, float)) or interval <= 0: raise ValueError(\\"Interval must be a positive number\\") self.tasks[coro] = interval def stop(self): for task in asyncio.all_tasks(self.loop): task.cancel() self.loop.stop() async def _schedule_task(self, coro, interval): while True: await asyncio.sleep(interval) await coro() async def start(self): for coro, interval in self.tasks.items(): self.loop.create_task(self._schedule_task(coro, interval)) try: await self.loop.run_forever() except asyncio.CancelledError: pass finally: self.loop.close() ``` Your Task: Implement the `AsyncScheduler` class based on the above requirements and example usage.","solution":"import asyncio class AsyncScheduler: def __init__(self): self.loop = asyncio.get_event_loop() self.tasks = {} def add_task(self, coro, interval): if not asyncio.iscoroutinefunction(coro): raise TypeError(\\"The added task must be a coroutine function\\") if not isinstance(interval, (int, float)) or interval <= 0: raise ValueError(\\"Interval must be a positive number\\") self.tasks[coro] = interval def stop(self): for task in asyncio.all_tasks(self.loop): task.cancel() self.loop.stop() async def _schedule_task(self, coro, interval): while True: await asyncio.sleep(interval) await coro() async def start(self): for coro, interval in self.tasks.items(): self.loop.create_task(self._schedule_task(coro, interval)) try: await self.loop.run_forever() except asyncio.CancelledError: pass finally: self.loop.close()"},{"question":"**Objective:** You are required to write a Python function that uses the seaborn library to create customized scatter plots using the provided dataset. This task will assess your ability to utilize seaborn\'s advanced plotting features, including color mapping, transparency, and jitter. **Dataset:** Use the \'mpg\' dataset provided by seaborn. This dataset contains information about various car models, including attributes like \'horsepower\', \'mpg\' (miles per gallon), \'origin\', and \'weight\'. **Function Specifications:** - **Function Name:** `custom_scatter_plot` - **Input:** None - **Output:** Display the scatter plot using matplotlib\'s `show()` method. **Requirements:** 1. **Load the \'mpg\' dataset.** ```python mpg = load_dataset(\\"mpg\\") ``` 2. **Create a scatter plot of \'horsepower\' vs \'mpg\' with the following customizations:** - Use dots as markers. - Set the dots\' color based on the \'origin\' of the car. - Set the dots\' fill color based on the \'weight\' of the car, using a binary color scale. - Mix filled (\'o\') and unfilled (\'x\') markers for different origins. - Use partial opacity (alpha) to handle overplotting. - Apply jitter to the y-axis (mpg) with a jitter amount of 0.25. 3. **Display the plot.** **Constraints:** - Ensure that the plot is clear and interpretable, even with dense data points. - The function should not return any value; just display the plot. # Example ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def custom_scatter_plot(): mpg = load_dataset(\\"mpg\\") p = ( so.Plot(mpg, \\"horsepower\\", \\"mpg\\") .add(so.Dots(fillalpha=0.5), color=\\"origin\\", fillcolor=\\"weight\\") .scale(fillcolor=\\"binary\\") .add(so.Dots(stroke=1), marker=\\"origin\\") .scale(marker=[\\"o\\", \\"x\\"]) .add(so.Jitter(0.25)) ) p.show() # Run the function to display the plot custom_scatter_plot() ``` In this example, the function `custom_scatter_plot` generates and displays a customized scatter plot as per the specified requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def custom_scatter_plot(): # Load the \'mpg\' dataset mpg = sns.load_dataset(\\"mpg\\") # Setting up the plot plt.figure(figsize=(10, 6)) # Applying jitter to the mpg values jitter_mpg = mpg[\'mpg\'] + np.random.uniform(-0.25, 0.25, size=mpg.shape[0]) # Custom scatter plot with different markers for origin markers = {\'usa\': \'o\', \'europe\': \'s\', \'japan\': \'D\'} for origin, marker in markers.items(): subset = mpg[mpg[\'origin\'] == origin] plt.scatter( subset[\'horsepower\'], jitter_mpg[mpg[\'origin\'] == origin], alpha=0.6, label=origin, edgecolor=\'black\', c=subset[\'weight\'], cmap=\'viridis\', marker=marker ) plt.colorbar(label=\'Weight\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Miles per Gallon (MPG)\') plt.title(\'Scatter plot of Horsepower vs MPG\') plt.legend(title=\'Origin\') plt.show() # Run the function to display the plot custom_scatter_plot()"},{"question":"**Objective:** Implement a server that can handle multiple client connections simultaneously using the `select` module for I/O multiplexing. **Background:** You are required to design a multi-client echo server. The server should: - Accept multiple client connections. - Echo back whatever data is sent by each client. - Always remain responsive, handling multiple clients concurrently. - Properly handle the situation when a client disconnects. **Requirements:** 1. **Initialization:** - The server listens on a specified port for incoming client connections. 2. **Handling multiple clients:** - Use the `select.select()` function to manage multiple clients simultaneously. - Monitor sockets for readability and writability without blocking the main thread. 3. **Echo functionality:** - When a client sends data, the server should echo the data back to the same client. 4. **Graceful handling of client disconnects:** - Properly remove client sockets from the monitoring list upon disconnection. **Input/Output Specification:** - **Input:** No direct input. The server listens on a specified port for incoming connections. - **Output:** No direct output. The server echoes received data back to each client. **Constraints:** - The server should be implemented using the `select.select()` function. - Proper exception handling is required to manage erroneous conditions, like clients disconnecting abruptly. - The server must be efficient and responsive. **Platform Considerations:** - Ensure that you handle platform-specific conditions, e.g., no polling of regular file descriptors on Windows. **Example Execution:** ```python import select import socket import sys def echo_server(host, port): # Create a TCP socket server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((host, port)) server_socket.listen(5) server_socket.setblocking(False) # List of sockets to monitor for incoming connections or data inputs = [server_socket] outputs = [] while inputs: read_ready, write_ready, except_ready = select.select(inputs, outputs, inputs) for s in read_ready: if s is server_socket: # Handle new connection client_socket, client_address = s.accept() client_socket.setblocking(False) inputs.append(client_socket) print(f\\"Connected by {client_address}\\") else: # Handle client data data = s.recv(1024) if data: if s not in outputs: outputs.append(s) s.sendall(data) # Echo received data else: # Handle client disconnection if s in outputs: outputs.remove(s) inputs.remove(s) s.close() print(f\\"Disconnected\\") for s in write_ready: if s in outputs: try: # No actual additional data needs to be sent pass except Exception: # Handle exceptions inputs.remove(s) outputs.remove(s) s.close() for s in except_ready: # Handle exceptional conditions inputs.remove(s) if s in outputs: outputs.remove(s) s.close() if __name__ == \\"__main__\\": host = \\"127.0.0.1\\" port = 65432 echo_server(host, port) ``` **Explanation:** - The server socket is set to non-blocking mode and added to the list of inputs to monitor. - `select.select()` is called to block until at least one file descriptor is ready for some kind of I/O. - When the server socket is ready, a new connection is accepted and the client socket is added to the list of inputs. - When a client socket is ready to read, its data is read and echoed back. - If no data is received, the client is considered disconnected, and the socket is closed and removed from the list of monitored inputs. - The server continues to run, handling multiple clients concurrently. **Note:** Ensure that the provided code follows proper coding practices and is error-free.","solution":"import select import socket def echo_server(host, port): Echo server that handles multiple clients using select for I/O multiplexing. :param host: Host address to bind the server. :param port: Port number to bind the server. # Create a TCP socket server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((host, port)) server_socket.listen() server_socket.setblocking(False) # List of sockets to monitor for incoming connections or data inputs = [server_socket] outputs = [] message_queues = {} while inputs: read_ready, write_ready, except_ready = select.select(inputs, outputs, inputs) for s in read_ready: if s is server_socket: # Handle new connection client_socket, client_address = s.accept() client_socket.setblocking(False) inputs.append(client_socket) message_queues[client_socket] = [] print(f\\"Connected by {client_address}\\") else: # Handle client data data = s.recv(1024) if data: message_queues[s].append(data) if s not in outputs: outputs.append(s) else: # Handle client disconnection if s in outputs: outputs.remove(s) inputs.remove(s) s.close() del message_queues[s] print(f\\"Client disconnected\\") for s in write_ready: if s in message_queues: try: next_data = message_queues[s].pop(0) s.sendall(next_data) except Exception: inputs.remove(s) outputs.remove(s) s.close() del message_queues[s] for s in except_ready: inputs.remove(s) if s in outputs: outputs.remove(s) s.close() del message_queues[s]"},{"question":"# Question: Advanced Data Manipulation using Pandas You are provided with a dataset containing information about various products sold by a company over multiple years. Your task is to write a function to perform several data manipulations to generate valuable insights. Dataset The dataset contains the following columns: - `ProductID`: Unique identifier for each product - `Category`: The category to which the product belongs - `Year`: The year in which the product information is recorded - `Price`: The price of the product - `QuantitySold`: The number of units sold Function Specification Your task is to implement the function `analyze_sales_data(df)`, where: - `df` (pandas DataFrame): The input DataFrame containing the dataset. The function should perform the following steps: 1. Fill any missing values in the `Price` column with the mean price of that product observed in the dataset. 2. Create a new column `TotalSales` representing the total sales (calculated by multiplying `Price` and `QuantitySold`) for each entry. 3. Group the data by `Year` and `Category`, and calculate the total sales for each group. 4. Return the year and category combination (as a tuple) that has the highest total sales. 5. Create a new DataFrame showing the product(s) with the highest quantity sold in each year. Constraints - You may assume there are no missing values in the `ProductID`, `Category`, `Year`, and `QuantitySold` columns. - Performance must be considered, especially in terms of handling large datasets with potentially thousands of rows. Example Given the following dataset: ``` ProductID Category Year Price QuantitySold 0 1 A 2020 10.0 50 1 2 B 2020 20.0 80 2 3 B 2021 20.0 60 3 1 A 2021 NaN 40 4 2 B 2020 20.0 70 ``` The function call: ```python analyze_sales_data(df) ``` Should return: ``` ((2020, \'B\'), DataFrame displaying products with highest quantity sold for each year) ``` Implement the function `analyze_sales_data(df)` to solve the above task. Note Do not use any global variables. Your solution should only rely on the Pandas library. Sample DataFrame for Output For the given example, the returned DataFrame should look like this: ``` ProductID Category Year Price QuantitySold TotalSales 0 2 B 2020 20.0 80 1600 1 2 B 2021 20.0 60 1200 ```","solution":"import pandas as pd def analyze_sales_data(df): # Step 1: Fill any missing values in the `Price` column with the mean price of the product df[\'Price\'] = df.groupby(\'ProductID\')[\'Price\'].transform(lambda x: x.fillna(x.mean())) # Step 2: Create a new column `TotalSales` df[\'TotalSales\'] = df[\'Price\'] * df[\'QuantitySold\'] # Step 3: Group the data by `Year` and `Category`, and calculate total sales for each group group_sales = df.groupby([\'Year\', \'Category\'])[\'TotalSales\'].sum().reset_index() # Step 4: Find the year and category combination with the highest total sales highest_sales_row = group_sales[group_sales[\'TotalSales\'] == group_sales[\'TotalSales\'].max()] highest_sales_tuple = (highest_sales_row[\'Year\'].values[0], highest_sales_row[\'Category\'].values[0]) # Step 5: Create a DataFrame showing the product(s) with the highest quantity sold in each year highest_quantity_sold = df.loc[df.groupby(\'Year\')[\'QuantitySold\'].idxmax()].reset_index(drop=True) return highest_sales_tuple, highest_quantity_sold"},{"question":"Objective To assess your understanding of the `site` module and your ability to manipulate the Python environment programmatically. Problem Statement You are tasked with creating a custom Python environment setup for a project that has specific requirements for where libraries and scripts are located. This configuration needs to ensure that custom directories are included in the `sys.path` and can import project-specific modules and configurations. Furthermore, a custom function should be used to demonstrate this setup. Requirements 1. **Custom Directories:** * Create two custom directories for the project, one for libraries (`lib_dir`) and one for scripts (`script_dir`) in your current working directory. * Ensure that these directories are added to the `sys.path`. 2. **Configuration File:** * Create a `.pth` file named `project_config.pth` within the newly created `lib_dir`. * This `.pth` file should include both directories (`lib_dir` and `script_dir`) and an executable line of code to import a custom module named `projectcustomize`. 3. **Custom Module:** * Create a custom Python module named `projectcustomize.py` within the `lib_dir`. * This custom module should print \\"Project customization complete.\\" when imported. 4. **Function Implementation:** * Implement a function named `setup_project_environment()` that: - Creates the required directories and files. - Updates `sys.path` using the `site.addsitedir()` function with `lib_dir`. - Imports the `projectcustomize` module to verify that the setup is complete. Constraints * The function should handle any errors that might occur due to directory creation or file processing. * Ensure that your code is idempotent; running it multiple times should not cause any errors. Input and Output * The function `setup_project_environment()` takes no parameters and returns no value. * Upon execution, the function should print \\"Project customization complete.\\" as a result of importing the `projectcustomize` module. Example Usage ```python setup_project_environment() # Expected Output: # Project customization complete. ```","solution":"import os import sys import site def setup_project_environment(): Set up the project-specific environment by creating custom directories, adding them to sys.path, and verifying the setup by importing a custom module. current_dir = os.getcwd() lib_dir = os.path.join(current_dir, \'lib_dir\') script_dir = os.path.join(current_dir, \'script_dir\') # Ensure directories exist os.makedirs(lib_dir, exist_ok=True) os.makedirs(script_dir, exist_ok=True) # Create the .pth file with necessary paths and executable statement pth_file_path = os.path.join(lib_dir, \'project_config.pth\') with open(pth_file_path, \'w\') as pth_file: pth_file.write(f\\"{lib_dir}n\\") pth_file.write(f\\"{script_dir}n\\") pth_file.write(\\"import projectcustomizen\\") # Create the custom module project_customize_path = os.path.join(lib_dir, \'projectcustomize.py\') with open(project_customize_path, \'w\') as custom_module_file: custom_module_file.write(\'print(\\"Project customization complete.\\")n\') # Add lib_dir to site directories and import the custom module to verify site.addsitedir(lib_dir) import projectcustomize # If running the file directly, uncomment the following line to execute the setup. # setup_project_environment()"},{"question":"# Python Assessment: Personal Finance Manager You are tasked with implementing a `FinanceManager` class to manage an individual\'s financial transactions. The class should provide functionalities to add transactions, generate summaries, and handle various types of errors. Requirements: 1. **Class Definition:** - Define a class named `FinanceManager`. - Include an initializer method (`__init__`) that initializes an empty list to store transactions. Each transaction is represented as a dictionary with `description` (str), `amount` (float), and `type` (\'credit\' or \'debit\'). 2. **Methods:** - `add_transaction(description: str, amount: float, txn_type: str) -> None`: - Add a transaction to the list. Raise a `ValueError` if `txn_type` is not \'credit\' or \'debit\'. - `get_summary() -> dict`: - Return a dictionary summarizing the total credits, total debits, and net balance (credits - debits). - `get_largest_transaction(txn_type: str) -> dict`: - Return the largest transaction of the specified type (\'credit\' or \'debit\'). Raise a `ValueError` if `txn_type` is not \'credit\' or \'debit\'. - `save_to_file(file_path: str) -> None`: - Save all transactions to a file in CSV format. - Implement error handling to catch and print file-related errors. - `load_from_file(file_path: str) -> None`: - Load transactions from a CSV file. - Use a `with` statement for file operations. - Implement error handling to catch and print file-related errors. 3. **Decorators and Coroutines:** - Define a coroutine `notify_large_transaction(amount: float) -> None` that prints a notification if a transaction exceeds a given amount. - Use a decorator to apply this coroutine to the `add_transaction` method for transactions greater than 1000. Example Usage: ```python # Initialize the FinanceManager fm = FinanceManager() # Add transactions fm.add_transaction(\'Salary\', 5000, \'credit\') fm.add_transaction(\'Groceries\', 200, \'debit\') fm.add_transaction(\'Bonus\', 1500, \'credit\') # Generate a summary summary = fm.get_summary() print(summary) # {\'total_credits\': 6500.0, \'total_debits\': 200.0, \'net_balance\': 6300.0} # Get largest debit transaction largest_debit = fm.get_largest_transaction(\'debit\') print(largest_debit) # {\'description\': \'Groceries\', \'amount\': 200, \'type\': \'debit\'} # Save to file fm.save_to_file(\'transactions.csv\') # Load from file fm.load_from_file(\'transactions.csv\') ``` Constraints: - Ensure proper error handling and input validation. - The CSV file should have columns: description, amount, type. Implement the `FinanceManager` class according to the requirements outlined above.","solution":"import csv from functools import wraps class FinanceManager: def __init__(self): self.transactions = [] self.notify_threshold = 1000 def add_transaction(self, description: str, amount: float, txn_type: str) -> None: if txn_type not in [\'credit\', \'debit\']: raise ValueError(\\"Transaction type must be \'credit\' or \'debit\'\\") self.transactions.append({\'description\': description, \'amount\': amount, \'type\': txn_type}) if amount > self.notify_threshold: self.notify_large_transaction(amount) def get_summary(self) -> dict: total_credits = sum(t[\'amount\'] for t in self.transactions if t[\'type\'] == \'credit\') total_debits = sum(t[\'amount\'] for t in self.transactions if t[\'type\'] == \'debit\') net_balance = total_credits - total_debits return { \'total_credits\': total_credits, \'total_debits\': total_debits, \'net_balance\': net_balance } def get_largest_transaction(self, txn_type: str) -> dict: if txn_type not in [\'credit\', \'debit\']: raise ValueError(\\"Transaction type must be \'credit\' or \'debit\'\\") filtered_transactions = [t for t in self.transactions if t[\'type\'] == txn_type] if not filtered_transactions: return None largest_transaction = max(filtered_transactions, key=lambda x: x[\'amount\']) return largest_transaction def save_to_file(self, file_path: str) -> None: try: with open(file_path, \'w\', newline=\'\') as csvfile: writer = csv.DictWriter(csvfile, fieldnames=[\'description\', \'amount\', \'type\']) writer.writeheader() for txn in self.transactions: writer.writerow(txn) except Exception as e: print(f\\"Error saving to file: {e}\\") def load_from_file(self, file_path: str) -> None: try: with open(file_path, \'r\') as csvfile: reader = csv.DictReader(csvfile) self.transactions = [row for row in reader] # Convert amount back to float for txn in self.transactions: txn[\'amount\'] = float(txn[\'amount\']) except Exception as e: print(f\\"Error loading from file: {e}\\") def notify_large_transaction(self, amount: float) -> None: print(f\\"Notification: A large transaction of {amount} has been added.\\")"},{"question":"You are tasked with creating a file backup utility in Python. This utility should take two directories as input: a source directory and a destination directory. The utility should copy all files from the source directory to the destination directory, preserving their subdirectory structure. Furthermore, if a file already exists in the destination directory, it should only be copied if it is newer than the existing file. Requirements 1. Implement the function `backup_files(src, dest)` where: - `src`: A string representing the path to the source directory. - `dest`: A string representing the path to the destination directory. 2. The function should: - Copy all files and subdirectories from `src` to `dest`. - Preserve the subdirectory structure in `dest`. - Only copy files that are newer than existing files in `dest`. 3. You can use the `shutil`, `os`, and `stat` modules for file and directory operations. Input - `src`: A string path to the source directory. - `dest`: A string path to the destination directory. Output - None, but the files should be copied from `src` to `dest` following the specified rules. Constraints - You can assume that the paths provided are valid and that the program has necessary permissions to read from `src` and write to `dest`. Example ```python import os from datetime import datetime from pathlib import Path # Setup source and destination directories (example) src = \'/path/to/source/directory\' dest = \'/path/to/destination/directory\' # Files in source directory # /path/to/source/directory/file1.txt (last modified: 2023-01-10) # /path/to/source/directory/subdir/file2.txt (last modified: 2023-01-15) # Files in destination directory before backup # /path/to/destination/directory/file1.txt (last modified: 2022-12-01) # /path/to/destination/directory/subdir/file2.txt (last modified: 2023-01-20) backup_files(src, dest) # Files in destination directory after backup # /path/to/destination/directory/file1.txt (last modified: 2023-01-10) # /path/to/destination/directory/subdir/file2.txt (last modified: 2023-01-20) ``` Notes - Utilize `os.path` to handle file paths. - Use `shutil.copy2` to retain metadata when copying files. - Employ `stat` to compare file modification times.","solution":"import os import shutil from pathlib import Path def backup_files(src, dest): Copy all files and subdirectories from src to dest, preserving the subdirectory structure and only copying files that are newer than existing ones in dest. if not os.path.exists(dest): os.makedirs(dest) for root, dirs, files in os.walk(src): # Construct the relative path in destination directory relative_path = os.path.relpath(root, src) dest_dir = os.path.join(dest, relative_path) # Create the same directories in the destination if not os.path.exists(dest_dir): os.makedirs(dest_dir) for file in files: src_file = os.path.join(root, file) dest_file = os.path.join(dest_dir, file) # Copy the file only if it doesn\'t exist in the destination or if it is newer if not os.path.exists(dest_file) or os.path.getmtime(src_file) > os.path.getmtime(dest_file): shutil.copy2(src_file, dest_file)"},{"question":"Coding Assessment Question # Objective Implement a Python class `CComplex` that simulates the functionality of the `Py_complex` C structure and provides methods to perform arithmetic operations on complex numbers. # Requirements 1. Implement the `CComplex` class with the following specifications: - The class should have two attributes, `real` and `imag`, both of type `float`. - The class should provide the following methods: - `__init__(self, real: float, imag: float)`: Constructor to initialize the complex number. - `__add__(self, other: \'CComplex\') -> \'CComplex\'`: Method to add two complex numbers. - `__sub__(self, other: \'CComplex\') -> \'CComplex\'`: Method to subtract one complex number from another. - `__neg__(self) -> \'CComplex\'`: Method to negate a complex number. - `__mul__(self, other: \'CComplex\') -> \'CComplex\'`: Method to multiply two complex numbers. - `__truediv__(self, other: \'CComplex\') -> \'CComplex\'`: Method to divide one complex number by another. Raise a `ZeroDivisionError` if the divisor is zero. - `__pow__(self, exponent: int) -> \'CComplex\'`: Method to raise the complex number to an integer power. Raise a `ValueError` if the base is zero and the exponent is not positive. - `to_python_complex(self) -> complex`: Method to convert the `CComplex` instance to a Python native `complex` object. - `from_python_complex(cls, py_complex: complex) -> \'CComplex\'`: Class method to create a `CComplex` instance from a Python native `complex` object. # Input and Output Formats - **Input**: None (methods interact with instances of the `CComplex` class). - **Output**: The result of the arithmetic operations as new instances of `CComplex`, or appropriate exceptions raised for error cases. # Constraints - All methods should handle floating-point numbers. - Division by zero should be properly handled by raising a `ZeroDivisionError`. - Raising a number to an invalid exponent should raise a `ValueError`. # Example Usage ```python # Creating complex numbers c1 = CComplex(1.0, 2.0) c2 = CComplex(3.0, 4.0) # Performing arithmetic operations summed = c1 + c2 diff = c1 - c2 negated = -c1 product = c1 * c2 quotient = c1 / c2 power = c1 ** 2 # Converting between CComplex and Python complex py_complex = summed.to_python_complex() c_complex = CComplex.from_python_complex(py_complex) ``` Implement the `CComplex` class and ensure that it passes the following tests: ```python def test_ccomplex_operations(): c1 = CComplex(1.0, 2.0) c2 = CComplex(3.0, 4.0) assert (c1 + c2).to_python_complex() == complex(4.0, 6.0) assert (c1 - c2).to_python_complex() == complex(-2.0, -2.0) assert (-c1).to_python_complex() == complex(-1.0, -2.0) assert (c1 * c2).to_python_complex() == complex(-5.0, 10.0) assert (c1 / c2).to_python_complex() == complex(0.44, 0.08) assert (c1 ** 2).to_python_complex() == complex(-3.0, 4.0) py_complex = complex(5.0, 6.0) c_complex = CComplex.from_python_complex(py_complex) assert c_complex.real == 5.0 assert c_complex.imag == 6.0 test_ccomplex_operations() ``` Ensure that your implementation correctly handles the cases outlined and passes the provided tests.","solution":"class CComplex: def __init__(self, real: float, imag: float): self.real = real self.imag = imag def __add__(self, other: \'CComplex\') -> \'CComplex\': return CComplex(self.real + other.real, self.imag + other.imag) def __sub__(self, other: \'CComplex\') -> \'CComplex\': return CComplex(self.real - other.real, self.imag - other.imag) def __neg__(self) -> \'CComplex\': return CComplex(-self.real, -self.imag) def __mul__(self, other: \'CComplex\') -> \'CComplex\': real = self.real * other.real - self.imag * other.imag imag = self.real * other.imag + self.imag * other.real return CComplex(real, imag) def __truediv__(self, other: \'CComplex\') -> \'CComplex\': if other.real == 0 and other.imag == 0: raise ZeroDivisionError(\\"Cannot divide by zero\\") denominator = other.real * other.real + other.imag * other.imag real = (self.real * other.real + self.imag * other.imag) / denominator imag = (self.imag * other.real - self.real * other.imag) / denominator return CComplex(real, imag) def __pow__(self, exponent: int) -> \'CComplex\': if self.real == 0 and self.imag == 0 and exponent <= 0: raise ValueError(\\"Zero cannot be raised to a non-positive power\\") res = complex(self.real, self.imag) ** exponent return CComplex(res.real, res.imag) def to_python_complex(self) -> complex: return complex(self.real, self.imag) @classmethod def from_python_complex(cls, py_complex: complex) -> \'CComplex\': return cls(py_complex.real, py_complex.imag)"},{"question":"# Exception Handling and User-Defined Exceptions in Python **Objective:** Your task is to implement a function that processes a list of numbers, performs various operations, and handles potential exceptions appropriately. Additionally, you will create a user-defined exception. **Function Signature:** ```python def process_numbers(numbers: list) -> int: pass ``` **User-Defined Exception Class:** ```python class InvalidNumberException(Exception): def __init__(self, message, invalid_numbers): self.message = message self.invalid_numbers = invalid_numbers super().__init__(self.message) ``` # Detailed Requirements: 1. **Function Description:** - The `process_numbers` function receives a list of numbers. - It should compute the sum of the numbers. - The function should handle exceptions and continue processing: - Ignore any non-numeric values and raise a `ValueError` with a message \\"Non-numeric value encountered\\". - If any entry is a negative number, raise an `InvalidNumberException` with a message \\"Negative number encountered\\", and a list of all negative numbers found. 2. **Input:** - A list of values which may be numeric or non-numeric. - Example: `[1, 2, \'a\', -1, 3, \'b\', 4, -5]` 3. **Output:** - Return the sum of the numeric values in the list ignoring non-numeric entries unless a negative number is encountered. - Example: For `[1, 2, \'a\', 3, \'b\', 4]`, return `10`. - Example: For `[1, 2, \'a\', -1, 3, \'b\', 4, -5]`, raise `InvalidNumberException`. 4. **Constraints:** - The function should handle various data types within the list and manage exceptions properly. - The custom exception should be used appropriately with a meaningful error message and details of the invalid entries. 5. **Exception Details:** - `InvalidNumberException` should capture a custom error message and a list of invalid numbers encountered. - `ValueError` when a non-numeric value is encountered. # Example Usage: ```python try: result = process_numbers([1, 2, \'a\', -1, 3, \'b\', 4, -5]) except InvalidNumberException as e: print(e.message) # Output: \\"Negative number encountered\\" print(e.invalid_numbers) # Output: [-1, -5] except ValueError as ve: print(ve) # Should not be raised in this example result = process_numbers([1, 2, \'a\', 3, \'b\', 4]) print(result) # Output: 10 ``` # Notes: - Ensure to handle exceptions in such a way that the processing of the list continues appropriately. - The order of exception handling should be considered to handle more specific exceptions first. - Pay attention to Python\'s exception chaining and attributes like `__cause__` and `__context__` while raising and handling exceptions.","solution":"class InvalidNumberException(Exception): def __init__(self, message, invalid_numbers): self.message = message self.invalid_numbers = invalid_numbers super().__init__(self.message) def process_numbers(numbers: list) -> int: total = 0 invalid_numbers = [] for value in numbers: try: if isinstance(value, (int, float)): if value < 0: invalid_numbers.append(value) total += value else: raise ValueError(\\"Non-numeric value encountered\\") except ValueError as e: print(e) if invalid_numbers: raise InvalidNumberException(\\"Negative number encountered\\", invalid_numbers) return total"},{"question":"**Problem Statement: Metadata Explorer for Installed Packages** You are tasked with developing a Python utility to gather, process, and display various metadata details about the installed Python packages. Your utility should leverage the `importlib.metadata` module to achieve the following: 1. **List Package Versions**: Given a list of package names, print their respective versions. 2. **Metadata Summary**: Display a summarized metadata report for a given package. The report should include the package name, version, summary, author, and license. 3. **Entry Points**: Print all entry points for a given package, categorized by their groups (e.g., `console_scripts`). 4. **Distribution Files and Locations**: For a specified package, list all files installed by the package along with their sizes, hash values, and absolute paths. # Function Definitions 1. `get_package_versions(packages: list) -> None` - **Input**: A list of package names as strings. - **Output**: Prints the package names along with their versions. - **Example**: ```python get_package_versions([\\"wheel\\", \\"setuptools\\"]) ``` Output: ``` wheel: 0.32.3 setuptools: 50.3.2 ``` 2. `get_metadata_summary(package: str) -> None` - **Input**: A single package name as a string. - **Output**: Prints a summary of the package metadata. - **Example**: ```python get_metadata_summary(\\"wheel\\") ``` Output: ``` Package: wheel Version: 0.32.3 Summary: A built-package format for Python. Author: Daniel Holth, Jason R. Coombs License: MIT ``` 3. `list_entry_points(package: str) -> None` - **Input**: A single package name as a string. - **Output**: Prints all entry points for the package, categorized by groups. - **Example**: ```python list_entry_points(\\"wheel\\") ``` Output: ``` Entry Points for package \'wheel\': Group \'console_scripts\': - wheel: wheel.cli:main ``` 4. `list_distribution_files(package: str) -> None` - **Input**: A single package name as a string. - **Output**: Prints all files installed by the package along with their sizes, hash values, and absolute paths. - **Example**: ```python list_distribution_files(\\"wheel\\") ``` Output: ``` Files in distribution for \'wheel\': - wheel/util.py Size: 859 bytes Hash: bYkw5oMccfazVCoYQwKkkemoVyMAFoR34mmKBx8R1NI Path: /home/user/.local/lib/python3.8/site-packages/wheel/util.py ``` # Constraints - Assume that the specified package names exist in the environment. - Handle any potential exceptions and provide clear error messages if a package is not found or if metadata is unavailable. # Performance Requirements - The code should efficiently handle the metadata queries and display the results promptly. - Optimize for readability and maintainability, ensuring that each function performs a focused task. **Use the `importlib.metadata` module to implement the above functionalities.**","solution":"import importlib.metadata def get_package_versions(packages: list) -> None: Prints the versions of the specified packages. for package in packages: try: version = importlib.metadata.version(package) print(f\\"{package}: {version}\\") except importlib.metadata.PackageNotFoundError: print(f\\"{package}: package not found\\") def get_metadata_summary(package: str) -> None: Prints a summary of the metadata of the specified package. try: metadata = importlib.metadata.metadata(package) info = { \\"Package\\": metadata.get(\\"Name\\", \\"Unknown\\"), \\"Version\\": metadata.get(\\"Version\\", \\"Unknown\\"), \\"Summary\\": metadata.get(\\"Summary\\", \\"No summary available\\"), \\"Author\\": metadata.get(\\"Author\\", \\"Unknown\\"), \\"License\\": metadata.get(\\"License\\", \\"Unknown\\") } for key, value in info.items(): print(f\\"{key}: {value}\\") except importlib.metadata.PackageNotFoundError: print(f\\"{package}: package not found\\") def list_entry_points(package: str) -> None: Prints the entry points of the specified package, categorized by groups. try: distribution = importlib.metadata.distribution(package) print(f\\"Entry Points for package \'{package}\':\\") entry_points = distribution.entry_points grouped_entry_points = {} for entry_point in entry_points: group = entry_point.group if group not in grouped_entry_points: grouped_entry_points[group] = [] grouped_entry_points[group].append(entry_point) for group, eps in grouped_entry_points.items(): print(f\\"Group \'{group}\':\\") for ep in eps: print(f\\" - {ep.name}: {ep.value}\\") except importlib.metadata.PackageNotFoundError: print(f\\"{package}: package not found\\") def list_distribution_files(package: str) -> None: Prints the files installed by the specified package including their size, hash values, and paths. import hashlib import os try: distribution = importlib.metadata.distribution(package) print(f\\"Files in distribution for \'{package}\':\\") for file in distribution.files: path = file.locate() size = os.path.getsize(path) with open(path, \\"rb\\") as f: content = f.read() hash_value = hashlib.sha256(content).hexdigest() print(f\\" - {file}\\") print(f\\" Size: {size} bytes\\") print(f\\" Hash: {hash_value}\\") print(f\\" Path: {path}\\") except importlib.metadata.PackageNotFoundError: print(f\\"{package}: package not found\\")"},{"question":"Advanced Future Management **Objective**: Implement an asynchronous class `EnhancedFutureManager` that manages multiple `Future` objects and provides high-level operations on them. Requirements: 1. **Class**: `EnhancedFutureManager` - **Attributes**: - `self.futures`: A list to hold `asyncio.Future` objects. 2. **Methods**: - `create_future(self, loop, delay, result)`: Creates a `Future`, schedules a task to set its result after a delay, and adds it to `self.futures`. - **Parameters**: - `loop`: Event loop to create the future. - `delay`: Number of seconds to wait before setting the result. - `result`: The result to be set for the future. - `cancel_all(self, msg=None)`: Cancels all the futures in `self.futures`. - **Parameters**: - `msg` (optional): Message for the cancel operation. - `completed_futures(self)`: Returns a list of futures that are done (either with a result or exception). - `failed_futures(self)`: Returns a list of futures that have raised exceptions. - `get_results(self)`: Returns a list of results of the futures that are successfully completed (i.e., not cancelled and no exception). 3. **Constraints**: - Use appropriate methods from `asyncio.Future` for operations. - Ensure proper handling of exceptions and cancellations. 4. **Example Usage**: ```python import asyncio class EnhancedFutureManager: def __init__(self): self.futures = [] async def create_future(self, loop, delay, result): fut = loop.create_future() loop.create_task(self.set_after(fut, delay, result)) self.futures.append(fut) async def set_after(self, fut, delay, result): await asyncio.sleep(delay) fut.set_result(result) def cancel_all(self, msg=None): for fut in self.futures: fut.cancel(msg) def completed_futures(self): return [fut for fut in self.futures if fut.done()] def failed_futures(self): return [fut for fut in self.futures if fut.done() and fut.exception() is not None] def get_results(self): return [fut.result() for fut in self.futures if fut.done() and not fut.cancelled() and fut.exception() is None] # Example Usage async def main(): loop = asyncio.get_running_loop() manager = EnhancedFutureManager() await manager.create_future(loop, 1, \'Result 1\') await manager.create_future(loop, 2, \'Result 2\') await asyncio.sleep(3) print(\\"Completed Futures: \\", manager.completed_futures()) print(\\"Failed Futures: \\", manager.failed_futures()) print(\\"Results: \\", manager.get_results()) asyncio.run(main()) ``` **Assessment**: - The code should correctly manage the life cycles of multiple `Future` objects. - Correct implementation of asynchronous tasks, result setting, cancellations, and retrieval of results/exceptions is expected.","solution":"import asyncio class EnhancedFutureManager: def __init__(self): self.futures = [] async def create_future(self, loop, delay, result): fut = loop.create_future() loop.create_task(self.set_after(fut, delay, result)) self.futures.append(fut) async def set_after(self, fut, delay, result): await asyncio.sleep(delay) fut.set_result(result) def cancel_all(self, msg=None): for fut in self.futures: fut.cancel() def completed_futures(self): return [fut for fut in self.futures if fut.done()] def failed_futures(self): return [fut for fut in self.futures if fut.done() and fut.exception() is not None] def get_results(self): return [fut.result() for fut in self.futures if fut.done() and not fut.cancelled() and fut.exception() is None]"},{"question":"Context: In this exercise, you\'ll be working with system error codes using the `errno` module in Python. The `errno` module maps system error codes to symbolic names and provides corresponding Python exceptions for error handling. This is essential for building robust software that can handle various system-level errors gracefully. Task: You need to implement a function `handle_error(error_number: int) -> str` that takes an error number as input and returns a human-readable string explaining the error. Your function should leverage the `errno` module and its mappings to provide meaningful descriptions. Requirements: 1. If the `error_number` corresponds to a defined error code in the `errno` module, your function should return a string in the format: ``` Error [code]: [description] ``` For example, if `error_number` is `2` (`errno.ENOENT`), the function should return: ``` Error 2: No such file or directory ``` 2. If the `error_number` does not correspond to any error code in the `errno` module, your function should return: ``` Error [code] not recognized ``` 3. The function should handle both standard error codes and platform-specific ones gracefully. Implementation: ```python import errno def handle_error(error_number: int) -> str: Given an error number, returns a human-readable string explaining the error. Parameters: - error_number (int): The error number to look up. Returns: - str: A description of the error or a message saying the error number is not recognized. # Your implementation here ``` Input: - An integer `error_number`. Output: - A string that provides the error description or states that the error is not recognized. Example: ```python print(handle_error(2)) # Output: \\"Error 2: No such file or directory\\" print(handle_error(9999)) # Output: \\"Error 9999 not recognized\\" ``` Constraints: - Assume that `error_number` will always be an integer. - The function should be efficient and handle typical use cases within reasonable performance limits. Notes: - Use `os.strerror()` to get the error message string if needed. - Make sure to test your function with various error codes including both recognized and unrecognized ones.","solution":"import errno import os def handle_error(error_number: int) -> str: Given an error number, returns a human-readable string explaining the error. Parameters: - error_number (int): The error number to look up. Returns: - str: A description of the error or a message saying the error number is not recognized. if error_number in errno.errorcode: return f\\"Error {error_number}: {os.strerror(error_number)}\\" else: return f\\"Error {error_number} not recognized\\""},{"question":"Coding Assessment Question # Objective To assess your understanding of PyTorch and TorchScript, you will implement a PyTorch neural network model and convert it to TorchScript. You will then perform inference with the TorchScript model on given input data. # Problem Statement You are required to: 1. Implement a simple feedforward neural network model using PyTorch. 2. Script the model using TorchScript. 3. Perform inference with the scripted model on a given sample input. Expected Input and Output Formats **Input:** - A predefined random seed for reproducibility. - Input tensor `x` of shape `(batch_size, input_features)`. **Output:** - A `torch.Tensor` containing model output. Constraints - The model should have at least two layers: one hidden layer and one output layer, with ReLU activations. - Use the random seed provided to ensure reproducibility. Performance Requirements - The implementation should efficiently handle input tensors of large sizes, up to `(1000, 500)` for inference. # Detailed Requirements 1. **Neural Network Implementation:** - Create a class `SimpleFeedForwardNet` that extends `torch.nn.Module`. - Define an `__init__` method to initialize the model layers. - Implement the `forward` method to pass the input through the layers. 2. **TorchScript Conversion:** - Use `torch.jit.script` to convert the instantiated model to TorchScript. 3. **Inference Execution:** - Use the scripted model to perform inference on the input tensor `x`. # Code Template ```python import torch import torch.nn as nn import torch.jit # Define the SimpleFeedForwardNet class class SimpleFeedForwardNet(nn.Module): def __init__(self, input_features, hidden_units, output_features): super(SimpleFeedForwardNet, self).__init__() self.hidden = nn.Linear(input_features, hidden_units) self.output = nn.Linear(hidden_units, output_features) def forward(self, x): x = torch.relu(self.hidden(x)) x = self.output(x) return x # Function to create and script the model def script_model(input_features, hidden_units, output_features): model = SimpleFeedForwardNet(input_features, hidden_units, output_features) scripted_model = torch.jit.script(model) return scripted_model # Function to perform inference with the scripted model def perform_inference(scripted_model, x): return scripted_model(x) # Example usage if __name__ == \\"__main__\\": # Define random seed for reproducibility random_seed = 42 torch.manual_seed(random_seed) # Define input parameters input_features = 500 hidden_units = 256 output_features = 10 batch_size = 1000 x = torch.randn(batch_size, input_features) # Script the model scripted_model = script_model(input_features, hidden_units, output_features) # Perform inference output = perform_inference(scripted_model, x) print(output) ``` # Notes - You must use the provided random seed (`42`) to ensure the reproducibility of the results. - Follow good coding practices, including meaningful variable names and modular code structure.","solution":"import torch import torch.nn as nn import torch.jit # Define the SimpleFeedForwardNet class class SimpleFeedForwardNet(nn.Module): def __init__(self, input_features, hidden_units, output_features): super(SimpleFeedForwardNet, self).__init__() self.hidden = nn.Linear(input_features, hidden_units) self.output = nn.Linear(hidden_units, output_features) def forward(self, x): x = torch.relu(self.hidden(x)) x = self.output(x) return x # Function to create and script the model def script_model(input_features, hidden_units, output_features): model = SimpleFeedForwardNet(input_features, hidden_units, output_features) scripted_model = torch.jit.script(model) return scripted_model # Function to perform inference with the scripted model def perform_inference(scripted_model, x): return scripted_model(x)"},{"question":"# URL Data Fetcher with Robust Error Handling and URL Parsing In this project, you are required to implement a function in Python that fetches data from a given URL. This function should handle potential errors gracefully and parse the URL to extract specific components. The task is designed to test your understanding of the `urllib` package, particularly the `urllib.request`, `urllib.error`, and `urllib.parse` modules. Function Signature ```python def fetch_and_parse_url(url: str) -> dict: pass ``` Input - `url` (str): The URL of a web page to fetch. Output - A dictionary with the following keys: - `status_code` (int): The HTTP status code of the response. - `content_type` (str): The content type of the response, extracted from the response headers. - `scheme` (str): The URL scheme (e.g., \'http\', \'https\'). - `netloc` (str): The network location part of the URL. - `path` (str): The hierarchical path of the URL. - `params` (str): The parameters for the URL. - `query` (str): The query component of the URL. - `fragment` (str): The fragment identifier of the URL. - `error` (str): Any error encountered during the request (empty string if no error). Constraints - The function must handle common HTTP errors (e.g., 404 Not Found, 500 Internal Server Error) and network-related errors gracefully. - Use the `urllib` package exclusively to perform these operations. - The function should return an empty string for error if there were no issues fetching the URL. Example ```python url = \\"https://example.com/path/to/page?name=ferret&color=purple#section2\\" result = fetch_and_parse_url(url) # Result dictionary should look like: { \\"status_code\\": 200, \\"content_type\\": \\"text/html; charset=UTF-8\\", \\"scheme\\": \\"https\\", \\"netloc\\": \\"example.com\\", \\"path\\": \\"/path/to/page\\", \\"params\\": \\"\\", \\"query\\": \\"name=ferret&color=purple\\", \\"fragment\\": \\"section2\\", \\"error\\": \\"\\" } ``` Performance Requirements - The function should be efficient and should not hang indefinitely. Implement a timeout for network requests (e.g., 5 seconds). - The function should handle a variety of valid URLs and gracefully manage incorrect or malformed URLs. Use this task to demonstrate your deep understanding of the `urllib` package functions and your ability to handle real-world scenarios in a Pythonic way.","solution":"import urllib.request import urllib.error import urllib.parse def fetch_and_parse_url(url: str) -> dict: result = { \\"status_code\\": None, \\"content_type\\": \\"\\", \\"scheme\\": \\"\\", \\"netloc\\": \\"\\", \\"path\\": \\"\\", \\"params\\": \\"\\", \\"query\\": \\"\\", \\"fragment\\": \\"\\", \\"error\\": \\"\\" } # Parse the URL parsed_url = urllib.parse.urlparse(url) result[\\"scheme\\"] = parsed_url.scheme result[\\"netloc\\"] = parsed_url.netloc result[\\"path\\"] = parsed_url.path result[\\"params\\"] = parsed_url.params result[\\"query\\"] = parsed_url.query result[\\"fragment\\"] = parsed_url.fragment try: with urllib.request.urlopen(url, timeout=5) as response: result[\\"status_code\\"] = response.getcode() result[\\"content_type\\"] = response.headers.get(\'Content-Type\', \'\') except urllib.error.HTTPError as e: result[\\"status_code\\"] = e.code result[\\"error\\"] = str(e) except urllib.error.URLError as e: result[\\"error\\"] = str(e) except Exception as e: result[\\"error\\"] = str(e) return result"},{"question":"You are designing a scientific calculator application that must handle various mathematical tasks. Your task is to implement a function that performs calculations based on the provided command. You need to support the following operations using the `math` module: 1. Compute the factorial of a given non-negative integer. 2. Calculate the logarithm (base 2) of a positive number. 3. Determine the Euclidean distance between two points in N-dimensional space. 4. Convert an angle from degrees to radians. 5. Compute the hyperbolic tangent of a number. # Function Specification Implement the following function: ```python def scientific_calculator(command: str, *args) -> float: :param command: A string representing the command to perform. It could be \\"factorial\\", \\"log2\\", \\"distance\\", \\"radians\\", or \\"tanh\\". :param args: Additional arguments needed for the specified command. :return: The result of the calculation as a float. ``` # Input Format - `command` (str): One of `\\"factorial\\"`, `\\"log2\\"`, `\\"distance\\"`, `\\"radians\\"`, `\\"tanh\\"`. - For `\\"factorial\\"`: A single non-negative integer should be provided as an argument. - For `\\"log2\\"`: A single positive float should be provided as an argument. - For `\\"distance\\"`: Two lists or tuples of equal length representing N-dimensional coordinates should be provided. - For `\\"radians\\"`: A single float representing an angle in degrees should be provided. - For `\\"tanh\\"`: A single float should be provided. # Output Format - Return the calculated result as a float. - If the input arguments are invalid for the respective command, raise a `ValueError` with an appropriate message. # Constraints - You may assume the input will always be valid and in the correct format for simplicity. - The function should use the `math` module functions to perform the calculations. - Do not use any external modules other than `math`. # Example Usage ```python # Example 1: Calculate factorial result = scientific_calculator(\\"factorial\\", 5) print(result) # Output: 120.0 # Example 2: Calculate base-2 logarithm result = scientific_calculator(\\"log2\\", 8) print(result) # Output: 3.0 # Example 3: Calculate Euclidean distance result = scientific_calculator(\\"distance\\", [1, 2], [4, 6]) print(result) # Output: 5.0 # Example 4: Convert degrees to radians result = scientific_calculator(\\"radians\\", 180) print(result) # Output: 3.141592653589793 # Example 5: Calculate hyperbolic tangent result = scientific_calculator(\\"tanh\\", 1.0) print(result) # Output: 0.7615941559557649 ``` Note - Use `math.factorial` for the factorial calculation. - Use `math.log2` for the base 2 logarithm. - Use `math.dist` for calculating the Euclidean distance. - Use `math.radians` for angle conversion. - Use `math.tanh` for calculating the hyperbolic tangent.","solution":"import math def scientific_calculator(command: str, *args) -> float: if command == \\"factorial\\": if len(args) != 1 or not isinstance(args[0], int) or args[0] < 0: raise ValueError(\\"Factorial command requires a single non-negative integer.\\") return float(math.factorial(args[0])) elif command == \\"log2\\": if len(args) != 1 or not isinstance(args[0], (int, float)) or args[0] <= 0: raise ValueError(\\"Log2 command requires a single positive number.\\") return math.log2(args[0]) elif command == \\"distance\\": if len(args) != 2 or not all(isinstance(arg, (list, tuple)) for arg in args) or len(args[0]) != len(args[1]): raise ValueError(\\"Distance command requires two lists or tuples of equal length.\\") return math.dist(args[0], args[1]) elif command == \\"radians\\": if len(args) != 1 or not isinstance(args[0], (int, float)): raise ValueError(\\"Radians command requires a single number.\\") return math.radians(args[0]) elif command == \\"tanh\\": if len(args) != 1 or not isinstance(args[0], (int, float)): raise ValueError(\\"Tanh command requires a single number.\\") return math.tanh(args[0]) else: raise ValueError(\\"Unknown command.\\")"},{"question":"MTIA Streams and Device Management in PyTorch You are tasked to create a custom function in PyTorch utilizing the `torch.mtia` backend for managing multiple devices and streams. The function should achieve the following: 1. Initialize the MTIA backend. 2. Check if the MTIA backend is available and initialized. 3. Set a specific device and stream, ensuring proper synchronization. 4. Perform a simple tensor operation on the selected device within the stream context. 5. Gather and print memory statistics related to the device. Your implementation should focus on demonstrating your ability to manage devices and streams using the `torch.mtia` module and interpret memory-related data. # Function Signature ```python def manage_device_and_stream(device_id: int, tensor_size: int): Manages devices and streams using PyTorch\'s MTIA backend to perform tensor operations. Args: - device_id (int): The ID of the device to be set for computation. - tensor_size (int): The size of the tensor to be created and operated on. The function does not return anything but should print relevant MTIA backend statuses and memory statistics. ``` # Expected Steps and Behavior 1. **Initialization**: Ensure the MTIA backend is initialized using `torch.mtia.init()`. 2. **Availability Check**: Verify if the MTIA backend is available (`torch.mtia.is_available()`) and initialized (`torch.mtia.is_initialized()`), and print the status. 3. **Device and Stream Management**: - Set the device for computation using `torch.mtia.set_device(device_id)`. - Create a stream for operations using `torch.mtia.stream(device=device_id)`. - Set this stream context using `torch.mtia.set_stream(stream)`. 4. **Tensor Operation**: - Create a PyTorch tensor of size `tensor_size` filled with random values on the selected device. - Perform a simple arithmetic operation (e.g., add a scalar value) on the tensor. 5. **Synchronization and Memory Statistics**: - Synchronize the stream to ensure all operations are completed (`torch.mtia.synchronize(device_id)`). - Retrieve and print memory statistics of the `device_id` using `torch.mtia.memory_stats(device_id)`. # Example ```python manage_device_and_stream(device_id=0, tensor_size=1000) ``` This example should initialize the MTIA backend, set the device to 0, create a tensor of size 1000 on the device, perform an addition operation, synchronize the stream, and finally print the memory statistics of the device. # Constraints - The function should handle cases where MTIA is not available or cannot be initialized by printing an appropriate message. - Assume that the device IDs are valid and the tensor size is always a positive integer. Implement the function `manage_device_and_stream` according to the specifications mentioned above.","solution":"import torch def manage_device_and_stream(device_id: int, tensor_size: int): Manages devices and streams using PyTorch\'s MTIA backend to perform tensor operations. Args: - device_id (int): The ID of the device to be set for computation. - tensor_size (int): The size of the tensor to be created and operated on. The function does not return anything but should print relevant MTIA backend statuses and memory statistics. if not hasattr(torch, \'mtia\'): print(\\"MTIA backend is not available in this PyTorch installation.\\") return # Initialize the MTIA backend try: torch.mtia.init() except Exception as e: print(f\\"Failed to initialize MTIA backend: {e}\\") return # Check if MTIA backend is available and initialized if not torch.mtia.is_available(): print(\\"MTIA backend is not available.\\") return if not torch.mtia.is_initialized(): print(\\"MTIA backend is not initialized.\\") return print(\\"MTIA backend is available and initialized.\\") # Set the device for computation torch.mtia.set_device(device_id) print(f\\"Device {device_id} set for computation.\\") # Create a stream for operations stream = torch.mtia.stream(device=device_id) # Set this stream context with torch.mtia.set_stream(stream): # Create a tensor of given size with random values on the selected device tensor = torch.rand(tensor_size, device=f\'mtia:{device_id}\') print(f\\"Tensor created on device {device_id}: {tensor}\\") # Perform a simple arithmetic operation (add a scalar value) result_tensor = tensor + 1.0 print(f\\"Resultant tensor after addition: {result_tensor}\\") # Synchronize the stream to ensure all operations are completed torch.mtia.synchronize(device_id) print(f\\"Device {device_id} synchronized.\\") # Retrieve and print memory statistics of the device memory_stats = torch.mtia.memory_stats(device_id) print(f\\"Memory statistics for device {device_id}: {memory_stats}\\") # Example usage: # Note: We assume that the `manage_device_and_stream` function will be run in an environment # with MTIA backend support in PyTorch. # manage_device_and_stream(device_id=0, tensor_size=1000)"},{"question":"Coding Assessment Question # Objective Assess the candidates capacity to utilize the `sys` module functionalities to access command-line arguments, manage specific limitations, and implement custom hooks. # Problem Statement You are tasked with creating a custom script that handles a complex mathematical operation using multiple command-line arguments and captures specific events using custom hooks. # Requirements 1. **Input Handling**: - Your script should accept multiple command-line arguments via `sys.argv`. The first argument should be the name of the script, the second should be an integer `N`. - If the argument `-o` is provided, the script should output the result to a file called `output.txt`. 2. **Function Implementation**: - Implement a function `calculate_sum_of_squares(N: int) -> int` that calculates and returns the sum of squares from 1 to `N`. 3. **Custom Exception Hook**: - Define a custom exception hook using `sys.excepthook` to log any uncaught exceptions to a file named `error.log`. The log file should include the exception type, value, and traceback information. 4. **System Limits**: - Ensure that the maximum recursion limit is adjusted only if the user provides the `-r` flag followed by a valid integer. Use `sys.setrecursionlimit` to configure this limit. # Constraints - `N` should be a positive integer. - The custom exception hook must handle logging reliably without crashing the program itself. # Example Usage Command Line: ```sh python script.py 10 python script.py 10 -o python script.py 10 -r 2000 ``` Output: - For `python script.py 10`: The script should print the result directly. - For `python script.py 10 -o`: The script should save the result to `output.txt`. - For `python script.py 10 -r 2000`: The script should set the recursion limit to `2000` and then print the result. # Expected Function Definitions ```python import sys def calculate_sum_of_squares(N: int) -> int: Calculate the sum of squares from 1 to N. pass def custom_exception_hook(exc_type, exc_value, exc_traceback): Custom exception hook to log uncaught exceptions. pass def main(): The main entry point of the script. Handle command-line arguments and compute the required results. pass if __name__ == \\"__main__\\": sys.excepthook = custom_exception_hook # Set the custom exception hook main() ``` Notes: - Appropriate error handling should be in place. - Implementations should follow the Pythonic conventions and best practices.","solution":"import sys import traceback def calculate_sum_of_squares(N: int) -> int: Calculate the sum of squares from 1 to N. return sum(i * i for i in range(1, N + 1)) def custom_exception_hook(exc_type, exc_value, exc_traceback): Custom exception hook to log uncaught exceptions. with open(\\"error.log\\", \\"w\\") as f: f.write(\\"Exception type: {}n\\".format(exc_type.__name__)) f.write(\\"Exception value: {}n\\".format(exc_value)) f.write(\\"Traceback:n\\") traceback.print_tb(exc_traceback, file=f) def main(): The main entry point of the script. Handle command-line arguments and compute the required results. if len(sys.argv) < 2: print(\\"Usage: python script.py <N> [-o] [-r <recursion_limit>]\\") return try: N = int(sys.argv[1]) if N <= 0: raise ValueError(\\"N must be a positive integer.\\") except ValueError as e: print(e) return output_to_file = False recursion_limit_set = False i = 2 while i < len(sys.argv): if sys.argv[i] == \'-o\': output_to_file = True elif sys.argv[i] == \'-r\': try: recursion_limit = int(sys.argv[i + 1]) sys.setrecursionlimit(recursion_limit) recursion_limit_set = True i += 1 except (IndexError, ValueError): print(\\"Invalid recursion limit value.\\") return i += 1 result = calculate_sum_of_squares(N) if output_to_file: with open(\\"output.txt\\", \\"w\\") as f: f.write(str(result)) else: print(result) if __name__ == \\"__main__\\": sys.excepthook = custom_exception_hook # Set the custom exception hook main()"},{"question":"**Problem Statement: Advanced Module Importing** You are to implement a function in Python that demonstrates your understanding of the `importlib` module, especially its ability to handle import utilities programmatically. Your task is to create a function `dynamic_import_and_execute` that will dynamically import a module from a given file path, verify the existence of a specific function within that module, and execute this function with provided arguments if it exists. # Function Signature ```python def dynamic_import_and_execute(file_path: str, func_name: str, *args, **kwargs) -> Any: pass ``` # Inputs - `file_path` (str): The path to the Python file (e.g., \\"/path/to/module.py\\") which contains the module to be imported. - `func_name` (str): The name of the function within the module that needs to be executed. - `*args`: Positional arguments to pass to the function `func_name`. - `**kwargs`: Keyword arguments to pass to the function `func_name`. # Outputs - Returns the result of the function `func_name` execution if the function exists. - Raises an `ImportError` if the module cannot be imported. - Raises an `AttributeError` if the `func_name` does not exist within the module. # Constraints - The Python script pointed by `file_path` can be assumed to be syntactically correct. - The module may or may not contain the function `func_name`. - The given arguments (both positional and keyword) match the signature of the function `func_name`. # Example Consider a file located at `/tmp/sample_module.py` with the following content: ```python # sample_module.py def greeting(name): return f\\"Hello, {name}!\\" def add(a, b): return a + b ``` Example Usage ```python result = dynamic_import_and_execute(\'/tmp/sample_module.py\', \'greeting\', \'Alice\') assert result == \\"Hello, Alice!\\" sum_result = dynamic_import_and_execute(\'/tmp/sample_module.py\', \'add\', 3, 4) assert sum_result == 7 # This should raise an AttributeError because the function \'multiply\' does not exist try: dynamic_import_and_execute(\'/tmp/sample_module.py\', \'multiply\', 3, 4) except AttributeError as e: print(e) # Output: module \'sample_module\' has no attribute \'multiply\' ``` # Implementation Guidelines 1. Use `importlib.util.spec_from_file_location` and `importlib.util.module_from_spec` to dynamically import the specified module. 2. Once imported, check for the existence of the function `func_name` using `hasattr`. 3. If the function exists, call it with the provided arguments. 4. Handle any potential exceptions (`ImportError`, `AttributeError`) as specified. Write the function `dynamic_import_and_execute` that meets the above requirements.","solution":"import importlib.util from typing import Any def dynamic_import_and_execute(file_path: str, func_name: str, *args, **kwargs) -> Any: Dynamically imports a module from the given file path, verifies the existence of a specific function within that module, and executes this function with provided arguments if it exists. :param file_path: The path to the Python file which contains the module to be imported. :param func_name: The name of the function within the module that needs to be executed. :param args: Positional arguments to pass to the function func_name. :param kwargs: Keyword arguments to pass to the function func_name. :return: The result of the function func_name execution if the function exists. :raises ImportError: If the module cannot be imported. :raises AttributeError: If the func_name does not exist within the module. try: spec = importlib.util.spec_from_file_location(\\"dynamic_module\\", file_path) module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) except Exception as e: raise ImportError(f\\"Cannot import module from {file_path}\\") from e if not hasattr(module, func_name): raise AttributeError(f\\"Module \'{module}\' has no attribute \'{func_name}\'\\") func = getattr(module, func_name) return func(*args, **kwargs)"},{"question":"# Advanced I/O Handling with Python\'s `io` Module In this task, you will create a custom logging system that writes log messages to an in-memory text buffer and periodically flushes to a binary file. The logging system should demonstrate your understanding of text I/O, binary I/O, and buffering. Requirements: 1. **Logging Class:** - Create a class `CustomLogger` with the following methods: - `__init__(self, filename: str, buffer_size: int):` Initializes the logger with a binary file given by `filename` and a buffer size for periodic flushing. - `log(self, message: str):` Adds a log message to the in-memory text buffer. - `flush(self):` Writes the contents of the text buffer to the binary file and clears the buffer. - `__del__(self):` Ensures that the buffer is flushed before the instance is destroyed. 2. **Text Buffer:** - Use an instance of `io.StringIO` to maintain an in-memory buffer for storing log messages. 3. **Binary File:** - Use an instance of `io.BufferedWriter` to write the text buffer contents to the specified binary file. 4. **Encoding:** - Encode text logs to UTF-8 bytes before writing to the binary file and ensure proper handling of encoding errors. 5. **Thread Safety:** - Implement thread-safe logging by using appropriate locking mechanisms to prevent race conditions when writing to the buffer and flushing to the file. Example Usage: ```python # Create a logger instance with a buffer size of 1024 bytes logger = CustomLogger(\'logfile.bin\', 1024) # Log some messages logger.log(\\"This is the first log message.\\") logger.log(\\"This is the second log message.\\") # Manually flush the buffer to the file logger.flush() # More logging logger.log(\\"This is the third log message.\\") # When the logger instance is deleted or goes out of scope, it should flush remaining messages del logger ``` Input Specification: - `filename` (str): The name of the binary file where logs will be stored. - `buffer_size` (int): The size of the buffer in bytes before it is flushed to the binary file. - `message` (str): The log message to be added to the in-memory text buffer. Output Specification: - The output should be a binary file that contains the UTF-8 encoded log messages. Constraints: - Ensure the class can handle large numbers of log messages efficiently. - Handle all potential encoding errors gracefully during the flushing process. - The logger should ensure that no data is lost if an error occurs during writing to the binary file. Evaluation: - Correctness: The implementation should fulfill all specified requirements and handle edge cases. - Efficiency: The solution should be memory-efficient and exhibit good performance characteristics. - Thread Safety: Identify and handle potential race conditions correctly to ensure logs are consistent. **Notes:** - You may use `threading.Lock` for implementing thread safety. - Do not use external libraries for I/O operations besides Python\'s `io` module. - Ensure proper resource management by closing files and flushing buffers as needed.","solution":"import io import threading class CustomLogger: def __init__(self, filename: str, buffer_size: int): Initializes the CustomLogger with a binary file and a buffer size for periodic flushing. self.filename = filename self.buffer_size = buffer_size self.text_buffer = io.StringIO() self.file_lock = threading.Lock() self.flush_lock = threading.Lock() def log(self, message: str): Adds a log message to the in-memory text buffer. with self.file_lock: self.text_buffer.write(message + \\"n\\") if self.text_buffer.tell() >= self.buffer_size: self.flush() def flush(self): Writes the contents of the text buffer to the binary file and clears the buffer. with self.flush_lock: text_data = self.text_buffer.getvalue().encode(\'utf-8\', errors=\'ignore\') self.text_buffer = io.StringIO() with open(self.filename, \'ab\') as binary_file: binary_file.write(text_data) def __del__(self): Ensures that the buffer is flushed before the instance is destroyed. self.flush()"},{"question":"# Question: Implement a Neural Network with TorchScript **Objective:** Your task is to implement a simple neural network class in PyTorch and convert it to TorchScript. This will assess your understanding of both PyTorch and TorchScript, including type annotations, TorchScript-specific constraints, and creating/exporting ScriptModules. # Requirements: 1. Implement a class `SimpleNN` which extends `torch.nn.Module`. This class should contain: - An `__init__` method that initializes the neural network layers. - A `forward` method that defines the forward pass. 2. Convert your `SimpleNN` class to TorchScript using `torch.jit.script`. 3. Implement a function `predict` which takes an input tensor and returns the output of the neural network. This should be a `@torch.jit.export` method in your `SimpleNN` class. 4. Use proper type annotations for all methods and attributes in your class. 5. Write a simple test case to verify your TorchScript model works correctly on a sample input tensor. # Constraints: - Use only `torch.nn.Linear` layers in your network. - Use ReLU activation functions. - Your network should have at least 2 hidden layers. # Performance Requirements: - Your class should be able to handle both single input tensors and batches of input tensors (with a batch dimension). - The methods should have the correct TorchScript type annotations and handle type inference correctly. # Input Format: - A single tensor of shape `[input_dim]` or `[batch_size, input_dim]`. # Output Format: - A single tensor of shape `[output_dim]` or `[batch_size, output_dim]`. # Example: ```python import torch import torch.nn as nn import torch.jit class SimpleNN(nn.Module): def __init__(self, input_dim: int, hidden_dim: int, output_dim: int): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) self.relu1 = nn.ReLU() self.fc2 = nn.Linear(hidden_dim, hidden_dim) self.relu2 = nn.ReLU() self.fc3 = nn.Linear(hidden_dim, output_dim) def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.fc1(x) x = self.relu1(x) x = self.fc2(x) x = self.relu2(x) x = self.fc3(x) return x @torch.jit.export def predict(self, x: torch.Tensor) -> torch.Tensor: with torch.no_grad(): return self.forward(x) # Example usage input_dim = 10 hidden_dim = 5 output_dim = 2 model = SimpleNN(input_dim, hidden_dim, output_dim) scripted_model = torch.jit.script(model) # Test case input_tensor = torch.randn((1, input_dim)) output_tensor = scripted_model.predict(input_tensor) print(output_tensor) ``` # Note: - Make sure `scripted_model.predict()` works as expected and provides correct output for given input tensors.","solution":"import torch import torch.nn as nn import torch.jit class SimpleNN(nn.Module): def __init__(self, input_dim: int, hidden_dim: int, output_dim: int): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) self.relu1 = nn.ReLU() self.fc2 = nn.Linear(hidden_dim, hidden_dim) self.relu2 = nn.ReLU() self.fc3 = nn.Linear(hidden_dim, output_dim) def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.fc1(x) x = self.relu1(x) x = self.fc2(x) x = self.relu2(x) x = self.fc3(x) return x @torch.jit.export def predict(self, x: torch.Tensor) -> torch.Tensor: with torch.no_grad(): return self.forward(x) # Example usage input_dim = 10 hidden_dim = 5 output_dim = 2 model = SimpleNN(input_dim, hidden_dim, output_dim) scripted_model = torch.jit.script(model)"},{"question":"**Question: Advanced Operators** The `operator` module in Python allows the use of functions that correspond to Python\'s intrinsic operators. In this exercise, the task is to create a function that evaluates a series of operations using these operator functions rather than their syntactic equivalents. # Task Write a function `evaluate_expression(expression: List[Tuple[str, Any, Any]]) -> Any` that takes a list of tuples as input. Each tuple consists of: 1. An operator name as a string. 2. The first operand. 3. The second operand (for operators that take two operands). The function should process the list of operations in order and return the final result. # Operators Supported: - Addition: `\'add\'` - Subtraction: `\'sub\'` - Multiplication: `\'mul\'` - Division: `\'truediv\'` - Modulus: `\'mod\'` - Less than: `\'lt\'` - Greater than: `\'gt\'` - Equal to: `\'eq\'` - Not equal to: `\'ne\'` - Logical AND: `\'and_\'` - Logical OR: `\'or_\'` # Constraints 1. The input list will have at least one operation. 2. Each operation will have valid operands for the specified operator. 3. Operations are executed in a sequence, where the result of one operation becomes the first operand for the next operation. # Example ```python from operator import add, sub, mul, truediv, mod, lt, gt, eq, ne, and_, or_ def evaluate_expression(expression): ops = { \'add\': add, \'sub\': sub, \'mul\': mul, \'truediv\': truediv, \'mod\': mod, \'lt\': lt, \'gt\': gt, \'eq\': eq, \'ne\': ne, \'and_\': and_, \'or_\': or_, } result = None for op, a, b in expression: if result is None: result = ops[op](a, b) else: result = ops[op](result, b) return result # Example usage assert evaluate_expression([(\'add\', 1, 2), (\'mul\', 3, 4), (\'sub\', 14, 5)]) == 9 assert evaluate_expression([(\'eq\', 5, 5), (\'and_\', True, False)]) == False ``` Ensure you follow the operator precedence rules correctly as they are applied sequentially within the provided list. Write your implementation in a Python script and verify its functionality using provided example cases and additional test cases you may wish to add.","solution":"import operator def evaluate_expression(expression): Evaluates a series of operations using functions from the operator module. Parameters: expression (List[Tuple[str, Any, Any]]): List of operations specified as (operator, operand1, operand2) Returns: Any: The result of evaluating all the operations in sequence. ops = { \'add\': operator.add, \'sub\': operator.sub, \'mul\': operator.mul, \'truediv\': operator.truediv, \'mod\': operator.mod, \'lt\': operator.lt, \'gt\': operator.gt, \'eq\': operator.eq, \'ne\': operator.ne, \'and_\': operator.and_, \'or_\': operator.or_, } result = None for op, a, b in expression: if result is None: result = ops[op](a, b) else: result = ops[op](result, b) return result"},{"question":"# Fetching and Handling HTML Content with Custom Headers and Error Handling **Objective:** Write a function `fetch_html_content` that retrieves the HTML content from a given URL using the `urllib.request` module. The function should include custom headers in the request and handle potential exceptions gracefully. **Function Signature:** ```python def fetch_html_content(url: str, headers: dict) -> str: pass ``` **Input:** - `url` (str): The URL to fetch the HTML content from. - `headers` (dict): A dictionary of HTTP headers to include in the request. **Output:** - Returns a string containing the HTML content of the page. - If there is an error in fetching the URL, return a string explaining the type of error. **Constraints:** - You can use only the `urllib` package for fetching URLs. - The function should be capable of handling network errors, HTTP errors, and provide informative error messages. **Performance Requirements:** - The function should set a socket timeout of 10 seconds to avoid long delays. **Example:** ```python url = \\"http://example.com\\" headers = { \\"User-Agent\\": \\"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\\" } html_content = fetch_html_content(url, headers) print(html_content) ``` **Detailed Instructions:** 1. Import necessary modules from `urllib` and `socket`. 2. Configure the socket default timeout to 10 seconds. 3. Create a `Request` object with the given URL and custom headers. 4. Use `urlopen` to fetch the URL. This should return a response object. 5. Read the HTML content from the response object. 6. Handle potential exceptions: - If the server could not fulfill the request, catch `HTTPError` and return an error message with the status code. - If there is a network-related error (e.g., no internet connection), catch `URLError` and return a meaningful error message. 7. Return the HTML content if no errors occur. You can find a sample implementation example below, on how to structure the code. ```python import urllib.request import urllib.error import socket def fetch_html_content(url: str, headers: dict) -> str: socket.setdefaulttimeout(10) req = urllib.request.Request(url, headers=headers) try: with urllib.request.urlopen(req) as response: html = response.read().decode(\'utf-8\') return html except urllib.error.HTTPError as e: return f\\"HTTP Error: {e.code} - {e.reason}\\" except urllib.error.URLError as e: return f\\"URL Error: {e.reason}\\" except Exception as e: return f\\"General Error: {str(e)}\\" ``` Make sure you understand the underlying concepts and functions used to create HTTP requests and handle responses/errors using `urllib`.","solution":"import urllib.request import urllib.error import socket def fetch_html_content(url: str, headers: dict) -> str: socket.setdefaulttimeout(10) # Set the socket timeout to 10 seconds req = urllib.request.Request(url, headers=headers) try: with urllib.request.urlopen(req) as response: html = response.read().decode(\'utf-8\') return html except urllib.error.HTTPError as e: return f\\"HTTP Error: {e.code} - {e.reason}\\" except urllib.error.URLError as e: return f\\"URL Error: {e.reason}\\" except Exception as e: return f\\"General Error: {str(e)}\\""},{"question":"# Advanced Coding Assessment Question **Objective**: This task aims to assess your understanding of the `operator` module in Python, including its capabilities for sequence operations and in-place modifications. **Task**: Write a Python function named `custom_sort_and_modification` that takes a list of dictionaries and performs the following operations using the `operator` module: 1. Sort the list of dictionaries based on a specified key. 2. Extract a list of values corresponding to another specified key from each dictionary. 3. Perform an in-place modification on each dictionary by applying a specified mathematical operation to a specified key. **Function Signature**: ```python def custom_sort_and_modification(data: list, sort_key: str, extract_key: str, modify_key: str, operation: str, operand: int) -> list: pass ``` **Parameters**: - `data` (list): A list of dictionaries containing at least the keys `sort_key`, `extract_key`, and `modify_key`. - `sort_key` (str): The key by which to sort the list of dictionaries. - `extract_key` (str): The key for which to extract values from each dictionary into a list. - `modify_key` (str): The key whose value will be modified in each dictionary. - `operation` (str): A string representing the mathematical operation to perform (`\'add\'`, `\'sub\'`, `\'mul\'`, `\'truediv\'`, etc.). - `operand` (int): The operand to use in the mathematical operation. **Returns**: - `list`: A list of extracted values corresponding to `extract_key` from each dictionary after the modifications. **Constraints**: - The dictionaries in the `data` list will contain keys corresponding to `sort_key`, `extract_key`, and `modify_key` with integer values. - The `operation` must be a valid method supported by the `operator` module. **Example Usage**: ```python data = [ {\\"name\\": \\"apple\\", \\"quantity\\": 50, \\"price\\": 1}, {\\"name\\": \\"banana\\", \\"quantity\\": 100, \\"price\\": 0.5}, {\\"name\\": \\"cherry\\", \\"quantity\\": 75, \\"price\\": 2} ] result = custom_sort_and_modification(data, \\"quantity\\", \\"price\\", \\"quantity\\", \\"mul\\", 2) print(result) # Output: [0.5, 2, 1] # Data after modification: [ # {\\"name\\": \\"banana\\", \\"quantity\\": 200, \\"price\\": 0.5}, # {\\"name\\": \\"cherry\\", \\"quantity\\": 150, \\"price\\": 2}, # {\\"name\\": \\"apple\\", \\"quantity\\": 100, \\"price\\": 1} # ] ``` **Notes**: 1. You\'ll need to handle the sorting, item extraction, and in-place modification using functions from the `operator` module. 2. Utilize `operator.methodcaller` for dynamic mathematical operations on dictionary values.","solution":"import operator def custom_sort_and_modification(data: list, sort_key: str, extract_key: str, modify_key: str, operation: str, operand: int) -> list: # Define the operator functions mapping operations = { \'add\': operator.add, \'sub\': operator.sub, \'mul\': operator.mul, \'truediv\': operator.truediv, \'floordiv\': operator.floordiv, \'mod\': operator.mod, \'pow\': operator.pow } if operation not in operations: raise ValueError(f\\"Unsupported operation \'{operation}\'\\") # Sort the data by the specified sort_key data.sort(key=operator.itemgetter(sort_key)) # Extract the values corresponding to extract_key extracted_values = list(map(operator.itemgetter(extract_key), data)) # Perform the in-place modification on modify_key modify_op = operations[operation] for item in data: item[modify_key] = modify_op(item[modify_key], operand) return extracted_values"},{"question":"# Question: Implementing and Managing Python Threads in an Embedded Application Objective: Demonstrate your understanding of thread management, GIL handling, and initialization/finalization of the Python interpreter by creating a C extension that incorporates embedding Python and managing threads. Task: You are required to create a C extension that embeds Python and spawns a new thread which executes a Python function. Use the following steps to complete this task: 1. **Initialize Python Interpreter**: Ensure the interpreter is properly initialized before any Python API calls. 2. **Spawning Threads**: Create a thread within the C extension that runs a Python function. 3. **GIL Management**: Release the GIL around blocking I/O operations within the C code, ensuring proper usage of `Py_BEGIN_ALLOW_THREADS` and `Py_END_ALLOW_THREADS` macros. 4. **Thread Synchronization**: Make sure the newly spawned thread can call the Python function and print its return value. 5. **Finalizing the Interpreter**: Properly finalize the Python interpreter once thread execution is complete. Specifications: 1. **Python Function**: For simplicity, use a Python function that implements simple disk I/O operations (like reading a file) to illustrate blocking calls. 2. **Return and Print Result**: Make sure the return value of the Python function is captured and printed within the C extension. 3. **Proper Exception Handling**: Ensure to handle any exceptions that may occur during the execution of the Python code. 4. **Thread Safety**: Ensure thread safety and proper GIL handling throughout the implementation. Detailed Steps: 1. Define the C extension and include necessary headers (`Python.h`, `pthread.h`). 2. Initialize the Python interpreter using `Py_Initialize()`. 3. Create a thread and use `PyGILState_Ensure()` to ensure the thread can call into Python. 4. Release the GIL around blocking I/O operations. 5. Within the thread, call the Python function, capture its return value and print it. 6. Reacquire the GIL and finalize the interpreter using `Py_FinalizeEx()`. 7. Implement proper error handling to manage possible failure points. Constraints: - Ensure the function calls made within the thread are executed safely and correctly manage the Python GIL. - Do not share Python objects between threads inappropriately as it may lead to crashes or undefined behavior. - Handle thread creation and joining appropriately to avoid resource leaks. Here is a simple outline of what the code should look like: ```c #include <Python.h> #include <pthread.h> void *thread_function(void *arg) { PyGILState_STATE gstate; gstate = PyGILState_Ensure(); // Call a Python function (assume it is defined in the main script file) PyObject *pName, *pModule, *pFunc, *pValue; pName = PyUnicode_DecodeFSDefault(\\"myscript\\"); pModule = PyImport_Import(pName); Py_DECREF(pName); if (pModule != NULL) { pFunc = PyObject_GetAttrString(pModule, \\"my_function\\"); if (pFunc && PyCallable_Check(pFunc)) { pValue = PyObject_CallObject(pFunc, NULL); if (pValue != NULL) { // Print result of Python function printf(\\"Result of call: %ldn\\", PyLong_AsLong(pValue)); Py_DECREF(pValue); } else { PyErr_Print(); } Py_XDECREF(pFunc); } Py_DECREF(pModule); } else { PyErr_Print(); } PyGILState_Release(gstate); return NULL; } int main(int argc, char *argv[]) { Py_Initialize(); pthread_t thread_id; pthread_create(&thread_id, NULL, thread_function, NULL); pthread_join(thread_id, NULL); Py_FinalizeEx(); return 0; } ``` Expected Output: - The program should print the result returned by the specified Python function. - Proper initialization and finalization messages should be printed to confirm the interpreter\'s state transitions. Notes: - Simulate the blocking I/O within the Python function to demonstrate the actual need to release and reacquire the GIL. - Ensure the code aligns with current best practices for embedding Python and managing threads.","solution":"import threading import time def read_file_simulated(filename): Simulated Python function that performs a blocking I/O operation. try: with open(filename, \'r\') as file: time.sleep(2) # Simulate a blocking read operation content = file.read() return content except Exception as e: return str(e) # C extension-like function - simulating thread creation and execution def run_thread(): def thread_function(): content = read_file_simulated(\'example.txt\') print(f\'Result of function call: {content}\') thread = threading.Thread(target=thread_function) thread.start() thread.join()"},{"question":"# Pandas DataFrame Coding Assessment Objective Demonstrate your skills in handling `pandas` DataFrames by performing multiple operations, including data cleaning, transformation, aggregation, and visualization. Task Write a Python function `process_sales_data` that takes a file path to a CSV file as input and returns a summary DataFrame with specific aggregated metrics. The CSV file contains daily sales data with the following columns: - `Date` (string in `YYYY-MM-DD` format) - `Store` (string) - `Department` (string) - `Revenue` (float) Your function should perform the following steps: 1. **Load the Data**: Read the CSV file into a pandas DataFrame. 2. **Data Cleaning**: - Ensure there are no missing values in the `Revenue` column. If there are, fill them with the mean revenue of that particular `Store` and `Department` combination. - Convert the `Date` column to datetime format. 3. **Data Transformation**: - Add a new column `YearMonth` that combines the year and month from the `Date` column in `YYYY-MM` format. 4. **Data Aggregation**: - Group the data by `Store`, `Department`, and `YearMonth`, and calculate the total revenue for each group. 5. **Output**: - The resulting DataFrame should have the columns `Store`, `Department`, `YearMonth`, and `TotalRevenue`, sorted by `Store`, `Department`, and `YearMonth` in ascending order. Input - `file_path`: A string representing the file path to the CSV file containing the sales data. Output - A pandas DataFrame containing the aggregated metrics as described above. Constraints - You can assume the CSV file is well-formatted. - Handle missing values for the `Revenue` column only. Performance Requirements - The solution should efficiently handle large datasets, with typical file sizes ranging from a few MB to 1 GB. # Example Suppose the CSV file contains the following data: ``` Date,Store,Department,Revenue 2023-01-01,StoreA,Electronics,200.0 2023-01-02,StoreA,Electronics, 2023-01-01,StoreA,Clothing,150.0 2023-01-01,StoreB,Electronics,300.0 2023-01-03,StoreA,Clothing,100.0 ``` Your function should return: ``` Store Department YearMonth TotalRevenue 0 StoreA Clothing 2023-01 250.0 1 StoreA Electronics 2023-01 400.0 2 StoreB Electronics 2023-01 300.0 ``` # Implementation ```python import pandas as pd def process_sales_data(file_path): # Step 1: Load the Data df = pd.read_csv(file_path) # Step 2: Data Cleaning # Fill missing Revenue values with the mean revenue of the respective Store and Department df[\'Revenue\'] = df.groupby([\'Store\', \'Department\'])[\'Revenue\'].transform( lambda x: x.fillna(x.mean()) ) # Convert Date column to datetime format df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Step 3: Data Transformation # Add YearMonth column df[\'YearMonth\'] = df[\'Date\'].dt.to_period(\'M\').astype(str) # Step 4: Data Aggregation # Group by Store, Department, and YearMonth and calculate total revenue summary_df = df.groupby([\'Store\', \'Department\', \'YearMonth\'])[\'Revenue\'].sum().reset_index() summary_df.rename(columns={\'Revenue\': \'TotalRevenue\'}, inplace=True) # Step 5: Sorting summary_df.sort_values(by=[\'Store\', \'Department\', \'YearMonth\'], inplace=True) return summary_df ``` Notes - Thoroughly test your function with various datasets to ensure accuracy and performance. - Include appropriate error handling for file reading and data processing steps.","solution":"import pandas as pd def process_sales_data(file_path): Processes sales data to return a summary DataFrame with total revenue per Store, Department, and YearMonth. :param file_path: str, file path to the CSV file containing sales data. :return: pd.DataFrame, aggregated DataFrame with columns [Store, Department, YearMonth, TotalRevenue] # Step 1: Load the Data df = pd.read_csv(file_path) # Step 2: Data Cleaning # Fill missing Revenue values with the mean revenue of the respective Store and Department df[\'Revenue\'] = df.groupby([\'Store\', \'Department\'])[\'Revenue\'].transform( lambda x: x.fillna(x.mean()) ) # Convert Date column to datetime format df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Step 3: Data Transformation # Add YearMonth column df[\'YearMonth\'] = df[\'Date\'].dt.to_period(\'M\').astype(str) # Step 4: Data Aggregation # Group by Store, Department, and YearMonth and calculate total revenue summary_df = df.groupby([\'Store\', \'Department\', \'YearMonth\'])[\'Revenue\'].sum().reset_index() summary_df.rename(columns={\'Revenue\': \'TotalRevenue\'}, inplace=True) # Step 5: Sorting summary_df.sort_values(by=[\'Store\', \'Department\', \'YearMonth\'], inplace=True) return summary_df"},{"question":"**Advanced Python Object Model Assessment** # Objective: Implement a custom Python class that behaves like a special container. This class should demonstrate your understanding of Python\'s data model, special methods, and descriptors. # Problem Statement: You are required to design a custom class called `SpecialContainer` that supports: - Initialization with a list of integers. - Indexing and slicing similar to a list. - An attribute `sum` to get the sum of its elements. - Adding elements. - Representing objects clearly and unambiguously. - Customizing attribute access to include a descriptor that logs access. # Requirements: 1. **Initialization**: The class is initialized with a list of integers. If no list is provided, it should be initialized to an empty list. 2. **Indexing and Slicing**: The class should allow for indexing and slicing operations to retrieve elements similar to a list. 3. **Add Elements**: The class should support adding new elements through an `add_element` method. 4. **`sum` Attribute**: The class must have a read-only attribute `sum` that returns the sum of all elements. 5. **Representation**: Implement clear and unambiguous string representations of the object using `__repr__` and `__str__`. 6. **Descriptor**: Implement a descriptor class called `LogDescriptor` that logs access to the `sum` attribute. # Class Definitions: ```python class LogDescriptor: def __init__(self, name): self.name = name def __get__(self, instance, owner): value = instance.__dict__.get(self.name) print(f\\"Accessing {self.name}, value: {value}\\") return value def __set__(self, instance, value): print(f\\"Setting {self.name} to {value}\\") instance.__dict__[self.name] = value def __delete__(self, instance): print(f\\"Deleting {self.name}\\") del instance.__dict__[self.name] class SpecialContainer: sum = LogDescriptor(\'sum\') def __init__(self, items=None): self.items = items if items is not None else [] self.sum = sum(self.items) def add_element(self, element): if isinstance(element, int): self.items.append(element) self.sum = sum(self.items) else: raise ValueError(\\"Only integers can be added\\") def __getitem__(self, index): return self.items[index] def __repr__(self): return f\\"SpecialContainer(items={self.items})\\" def __str__(self): return f\\"SpecialContainer with elements: {self.items} and sum: {self.sum}\\" ``` # Constraints: - The elements of the container are always integers. - The `sum` attribute should always reflect the current sum of elements in the container and should not be directly modified. # Example Usage: ```python sc = SpecialContainer([1, 2, 3]) print(sc) # Output: SpecialContainer with elements: [1, 2, 3] and sum: 6 print(sc[1]) # Output: 2 sc.add_element(4) print(sc) # Output: SpecialContainer with elements: [1, 2, 3, 4] and sum: 10 print(sc.sum) # Logs access and Output: 10 ``` # Note: Ensure that your `SpecialContainer` class handles errors gracefully and enforces the constraints.","solution":"class LogDescriptor: def __init__(self, name): self.name = name def __get__(self, instance, owner): value = instance.__dict__.get(self.name) print(f\\"Accessing {self.name}, value: {value}\\") return value def __set__(self, instance, value): print(f\\"Setting {self.name} to {value}\\") instance.__dict__[self.name] = value def __delete__(self, instance): print(f\\"Deleting {self.name}\\") del instance.__dict__[self.name] class SpecialContainer: sum = LogDescriptor(\'sum\') def __init__(self, items=None): self.items = items if items is not None else [] self.sum = sum(self.items) def add_element(self, element): if isinstance(element, int): self.items.append(element) self.sum = sum(self.items) else: raise ValueError(\\"Only integers can be added\\") def __getitem__(self, index): return self.items[index] def __repr__(self): return f\\"SpecialContainer(items={self.items})\\" def __str__(self): return f\\"SpecialContainer with elements: {self.items} and sum: {self.sum}\\""},{"question":"# Question: Complex Number Operations using `cmath` Module Implement a function named `complex_operations` that receives a complex number `z` as input and performs the following operations using the `cmath` module: 1. Converts the complex number from rectangular coordinates to polar coordinates. 2. Calculates the natural logarithm of the complex number. 3. Computes the square root of the complex number. 4. Determines if the complex number has both finite real and imaginary parts. 5. Checks if the real and imaginary parts of the complex number are very close to each other (using a relative tolerance of `1e-09`). The function should return a dictionary containing the results of these operations. The keys of the dictionary should be `polar_coords`, `log_value`, `sqrt_value`, `is_finite`, and `is_close`. Input - `z`: A complex number `z` represented as `complex(real, imag)`. Output - A dictionary with the following keys and associated values: - `polar_coords`: A tuple `(r, phi)` representing the polar coordinates of `z`. - `log_value`: The natural logarithm of `z`. - `sqrt_value`: The square root of `z`. - `is_finite`: A boolean indicating if both real and imaginary parts of `z` are finite. - `is_close`: A boolean indicating if the real and imaginary parts of `z` are close to each other. Example ```python import cmath def complex_operations(z): result = {} result[\'polar_coords\'] = cmath.polar(z) result[\'log_value\'] = cmath.log(z) result[\'sqrt_value\'] = cmath.sqrt(z) result[\'is_finite\'] = cmath.isfinite(z) result[\'is_close\'] = cmath.isclose(z.real, z.imag) return result # Example usage: z = complex(3, 4) print(complex_operations(z)) ``` Expected output: ```python { \'polar_coords\': (5.0, 0.9272952180016122), \'log_value\': (1.6094379124341003+0.9272952180016122j), \'sqrt_value\': (2+1j), \'is_finite\': True, \'is_close\': False } ``` Constraints: - The input complex number will always have finite real and imaginary parts. - The implementation must use functions from the `cmath` module.","solution":"import cmath def complex_operations(z): Perform various operations on a complex number using the cmath module. Args: z (complex): A complex number represented as complex(real, imag). Returns: dict: A dictionary containing results of various operations. result = {} result[\'polar_coords\'] = cmath.polar(z) result[\'log_value\'] = cmath.log(z) result[\'sqrt_value\'] = cmath.sqrt(z) result[\'is_finite\'] = cmath.isfinite(z) result[\'is_close\'] = cmath.isclose(z.real, z.imag, rel_tol=1e-09) return result"},{"question":"**Question: Implementing an Asynchronous Chat Server and Client using asyncio** # Objective Your task is to create an asynchronous chat server and client using Python\'s `asyncio` module. The server will handle multiple clients concurrently, and the clients will be able to send and receive messages in real-time. # Requirements 1. **Server Implementation:** - The server should be able to accept multiple client connections. - Each client should be able to send messages to the server. - The server should broadcast received messages to all connected clients. - If a client disconnects, the server should handle it gracefully and inform the remaining clients. 2. **Client Implementation:** - A client should be able to connect to the server. - A client should be able to send messages to the server. - A client should receive and display messages broadcasted from the server in real-time. # Input and Output Formats - **Server:** - The server does not take input from stdin. - The server will print out log messages to stdout for actions such as when a client connects, disconnects, or sends a message. - **Client:** - The client reads messages from stdin and sends them to the server. - The client prints out messages received from the server to stdout. # Constraints - You must use the `asyncio` module for both the server and client implementations. - The port number and host for the server should be configurable. - Handle exceptions and edge cases gracefully, ensuring that the server and client do not crash unexpectedly. # Example Here\'s an example of how you might run the server and two clients: ```python # Start the server python chat_server.py --host 127.0.0.1 --port 8888 # In separate terminals, start two clients python chat_client.py --host 127.0.0.1 --port 8888 python chat_client.py --host 127.0.0.1 --port 8888 ``` # Implementation Details You will need to implement the following: 1. **Chat Server (`chat_server.py`):** - Create an `asyncio` server that listens for client connections. - Use `asyncio.StreamReader` and `asyncio.StreamWriter` to receive and send messages. - Maintain a list of connected clients and broadcast messages to all of them. 2. **Chat Client (`chat_client.py`):** - Connect to the server using `asyncio`. - Use `asyncio.StreamReader` and `asyncio.StreamWriter` for sending and receiving messages. - Ensure the client can send messages from stdin and display messages received from the server. Good luck!","solution":"import asyncio class ChatServer: def __init__(self, host, port): self.host = host self.port = port self.clients = [] async def handle_client(self, reader, writer): addr = writer.get_extra_info(\'peername\') print(f\\"New connection from {addr}\\") self.clients.append(writer) try: while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received {message} from {addr}\\") await self.broadcast_message(message, writer) except Exception as e: print(f\\"Exception: {e}\\") finally: print(f\\"Connection from {addr} closed\\") self.clients.remove(writer) writer.close() await writer.wait_closed() await self.broadcast_message(f\\"{addr} has left the chat.n\\", None) async def broadcast_message(self, message, sender_writer): for client_writer in self.clients: if client_writer != sender_writer: client_writer.write(message.encode()) await client_writer.drain() async def start_server(self): server = await asyncio.start_server(self.handle_client, self.host, self.port) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() async def main_server(): host = \'127.0.0.1\' port = 8888 server = ChatServer(host, port) await server.start_server() if __name__ == \\"__main__\\": asyncio.run(main_server())"},{"question":"# Coding Exercise: Implement a Custom Importer using `importlib` Objective: Your task is to implement a custom module importer that mimics the core functionality of the deprecated `imp` module using the `importlib` library. This exercise will test your understanding of module importing and manipulation in Python. Specifications: 1. **Function `custom_find_module(name: str, path: list = None) -> tuple`**: - **Input**: - `name` (str): Name of the module to be found. - `path` (list, optional): List of directories to search. Defaults to `None`, which means the system will use `sys.path`. - **Output**: - A tuple `(file, pathname, description)`: - `file`: A file object if the module is found, otherwise `None`. - `pathname`: Path to the module file. - `description`: Tuple described by the constants defined in `importlib.machinery` (suffix, mode, type). 2. **Function `custom_load_module(name: str, file: object, pathname: str, description: tuple) -> ModuleType`**: - **Input**: - `name` (str): Full name of the module. - `file` (object): File object where the modules code resides (can be `None` for built-ins). - `pathname` (str): Path to the module file. - `description` (tuple): Description tuple containing suffix, mode, and type. - **Output**: - The loaded module object. 3. **Function `custom_reload(module: ModuleType) -> ModuleType`**: - **Input**: - `module` (types.ModuleType): The module object to be reloaded. - **Output**: - The reloaded module object. Constraints: - Do NOT use any function or constant from the `imp` module; only use `importlib` and standard library functions. Performance Requirements: - The implemented functions should accurately mimic the behaviors of the corresponding `imp` functions. - Handle module loading and reloading efficiently to avoid excessive memory usage or performance overhead. # Example Usage ```python # Assuming your functions are implemented correctly # Find the module file, pathname, description = custom_find_module(\'os\') # Load the module module = custom_load_module(\'os\', file, pathname, description) # Reload the module reloaded_module = custom_reload(module) ``` Ensure to handle various module types (e.g., source files, compiled files, built-in modules), and consider edge cases like circular imports or failed module loading.","solution":"import importlib.util import sys import types def custom_find_module(name: str, path: list = None) -> tuple: Finds the module specified by \'name\' in the given \'path\'. if path is None: path = sys.path # Find the module spec spec = importlib.util.find_spec(name, path) if spec is None: return None, None, None file = None if spec.origin and spec.origin != \'built-in\': file = open(spec.origin, \'r\') description = (\'.py\', \'r\', importlib.machinery.SOURCE_SUFFIXES) return file, spec.origin, description def custom_load_module(name: str, file: object, pathname: str, description: tuple) -> types.ModuleType: Load the module specified by \'name\' using its file, pathname, and description. spec = importlib.util.spec_from_file_location(name, pathname) module = importlib.util.module_from_spec(spec) if file: file.close() sys.modules[name] = module spec.loader.exec_module(module) return module def custom_reload(module: types.ModuleType) -> types.ModuleType: Reload the specified module. return importlib.reload(module)"},{"question":"# Coding Task: Custom Seaborn UI Palette Visualizer Problem Statement You\'re required to design a seaborn-based custom palette visualizer for a user interface (UI) that allows manipulation of several parameters. This involves using the `sns.husl_palette` function for generating palettes and applying these palettes to various seaborn visualizations. Task Requirements 1. **Function Name**: `visualize_custom_palette` 2. **Input Parameters**: - `n_colors: int` - Number of colors to generate in the palette. - `lightness: float` - Lightness of the colors (must be between 0 and 1). - `saturation: float` - Saturation of the colors (must be between 0 and 1). - `hue: float` - Starting point for hue sampling (must be between 0 and 1). - `as_cmap: bool` - Whether to return a continuous colormap. 3. **Output**: Five seaborn visualizations, each representing different aspects of the generated palette as follows: - A bar plot - A scatter plot - A line plot - A heatmap - A violin plot Constraints - The function should properly handle invalid inputs including out-of-bound parameters for `lightness`, `saturation`, and `hue`. - Each visualization should uniquely depict the palette\'s effect on the plot. Performance Requirements - While performance is not the main focus of this task, ensure that the function does not employ unnecessary computations and handles the input parameters efficiently. Example Implementation ```python import seaborn as sns import matplotlib.pyplot as plt import numpy as np import pandas as pd def visualize_custom_palette(n_colors: int, lightness: float, saturation: float, hue: float, as_cmap: bool): # Validate input parameters if not (0 <= lightness <= 1): raise ValueError(\\"Lightness must be between 0 and 1\\") if not (0 <= saturation <= 1): raise ValueError(\\"Saturation must be between 0 and 1\\") if not (0 <= hue <= 1): raise ValueError(\\"Hue must be between 0 and 1\\") # Generate the palette or colormap palette = sns.husl_palette(n_colors, l=lightness, s=saturation, h=hue, as_cmap=as_cmap) # Sample data data = pd.DataFrame({ \'x\': np.linspace(0, 10, 100), \'y\': np.sin(np.linspace(0, 10, 100)), \'z\': np.random.randint(0, 100, 100) }) # Bar plot plt.figure(figsize=(10, 6)) sns.barplot(x=np.arange(n_colors), y=np.random.rand(n_colors), palette=palette if not as_cmap else palette(np.linspace(0, 1, n_colors))) plt.title(\\"Bar Plot with Custom Palette\\") plt.show() # Scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=data, x=\'x\', y=\'y\', palette=palette if not as_cmap else palette(np.linspace(0, 1, data.shape[0]))) plt.title(\\"Scatter Plot with Custom Palette\\") plt.show() # Line plot plt.figure(figsize=(10, 6)) sns.lineplot(data=data, x=\'x\', y=\'y\', palette=palette if not as_cmap else palette(np.linspace(0, 1, data.shape[0]))) plt.title(\\"Line Plot with Custom Palette\\") plt.show() # Heatmap heatmap_data = np.random.rand(10, 10) plt.figure(figsize=(10, 6)) sns.heatmap(heatmap_data, cmap=palette if as_cmap else sns.color_palette(palette, n_colors=n_colors)) plt.title(\\"Heatmap with Custom Palette\\") plt.show() # Violin plot plt.figure(figsize=(10, 6)) sns.violinplot(data=pd.melt(data[[\\"x\\", \\"y\\"]]), x=\\"variable\\", y=\\"value\\", palette=palette if not as_cmap else palette(np.linspace(0, 1, 2))) plt.title(\\"Violin Plot with Custom Palette\\") plt.show() # Example usage visualize_custom_palette(8, 0.5, 0.7, 0.2, False) ``` Evaluation Metrics: - Correctness: Does the code generate a valid custom palette and apply it to the visualizations? - Robustness: Does the function validate input parameters and handle edge cases gracefully? - Clarity: Is the code clean, well-organized, and documented?","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np import pandas as pd def visualize_custom_palette(n_colors: int, lightness: float, saturation: float, hue: float, as_cmap: bool=False): Generates a custom palette using seaborn\'s husl_palette function and visualizes it using several seaborn plots: bar plot, scatter plot, line plot, heatmap, and violin plot. Parameters: n_colors (int): Number of colors to generate in the palette. lightness (float): Lightness of the colors (must be between 0 and 1). saturation (float): Saturation of the colors (must be between 0 and 1). hue (float): Starting point for hue sampling (must be between 0 and 1). as_cmap (bool): Whether to return a continuous colormap. Raises: ValueError: If lightness, saturation, or hue is out of the valid range. # Validate input parameters if not (0 <= lightness <= 1): raise ValueError(\\"Lightness must be between 0 and 1\\") if not (0 <= saturation <= 1): raise ValueError(\\"Saturation must be between 0 and 1\\") if not (0 <= hue <= 1): raise ValueError(\\"Hue must be between 0 and 1\\") # Generate the palette or colormap palette = sns.husl_palette(n_colors, l=lightness, s=saturation, h=hue, as_cmap=as_cmap) # Sample data data = pd.DataFrame({ \'x\': np.linspace(0, 10, 100), \'y\': np.sin(np.linspace(0, 10, 100)), \'z\': np.random.randint(0, 100, 100) }) # Bar plot plt.figure(figsize=(10, 6)) sns.barplot(x=np.arange(n_colors), y=np.random.rand(n_colors), palette=palette if not as_cmap else \\"viridis\\") plt.title(\\"Bar Plot with Custom Palette\\") plt.show() # Scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=data, x=\'x\', y=\'y\', palette=palette if not as_cmap else \\"viridis\\") plt.title(\\"Scatter Plot with Custom Palette\\") plt.show() # Line plot plt.figure(figsize=(10, 6)) sns.lineplot(data=data, x=\'x\', y=\'y\', palette=palette if not as_cmap else \\"viridis\\") plt.title(\\"Line Plot with Custom Palette\\") plt.show() # Heatmap heatmap_data = np.random.rand(10, 10) plt.figure(figsize=(10, 6)) sns.heatmap(heatmap_data, cmap=palette if as_cmap else sns.color_palette(palette, n_colors=n_colors)) plt.title(\\"Heatmap with Custom Palette\\") plt.show() # Violin plot plt.figure(figsize=(10, 6)) sns.violinplot(data=pd.melt(data[[\\"x\\", \\"y\\"]]), x=\\"variable\\", y=\\"value\\", palette=palette if not as_cmap else \\"viridis\\") plt.title(\\"Violin Plot with Custom Palette\\") plt.show()"},{"question":"Working with `torch.xpu` **Objective**: Demonstrate your understanding of device management, random number generation, and memory management using the `torch.xpu` module in PyTorch. Implement a function `xpu_operations(seed: int) -> dict` that performs the following operations: 1. **Device and Stream Management**: - Initialize the XPU devices and set the active device to the first available one. - Create a stream within this device and make it the current stream. - Synchronize the stream to ensure all operations are completed. 2. **Random Number Generation**: - Set the random number seed using the provided seed parameter for reproducibility. - Get and store the initial random number generator state. - Generate a tensor filled with random numbers of size (100, 100) on the XPU. - Reset the RNG state to the stored initial state and ensure the tensor generation process is reproducible. 3. **Memory Management**: - Record the amount of memory allocated before and after creating the tensor of random numbers. - Clear the tensor and any cached memory. - Record the peak memory usage during the tensor creation. The function should return a dictionary with the following keys and their corresponding values: - `\'rng_state\'`: The initial random number generator state as a tensor. - `\'random_tensor\'`: The generated tensor of random numbers. - `\'memory_allocated_before\'`: The amount of memory allocated on the device before generating the tensor. - `\'memory_allocated_after\'`: The amount of memory allocated on the device after generating the tensor. - `\'peak_memory_usage\'`: The peak memory usage during tensor creation. **Function signature**: ```python def xpu_operations(seed: int) -> dict: pass ``` **Constraints and Limitations**: - Ensure the function handles exceptions and edge cases, e.g., when the device is not available. - Assume `torch.xpu` is an available and functional package in your environment. **Expected Performance**: - The function should efficiently manage memory, minimizing unnecessary memory allocations. - Use appropriate stream and event synchronization to avoid race conditions in concurrent operations. **Example**: ```python result = xpu_operations(42) print(result[\'rng_state\']) # Tensor representing the initial RNG state print(result[\'random_tensor\']) # Tensor of random numbers with shape (100, 100) print(result[\'memory_allocated_before\']) # Memory allocated before tensor creation print(result[\'memory_allocated_after\']) # Memory allocated after tensor creation print(result[\'peak_memory_usage\']) # Peak memory usage recorded ``` **Note**: You might need to refer to the PyTorch XPU documentation for the exact usage of memory management functions and stream handling relevant to XPU.","solution":"import torch def xpu_operations(seed: int) -> dict: try: # Check if XPU is available if not torch.xpu.is_available(): raise RuntimeError(\\"XPU is not available\\") # Set the active device to the first XPU device device = torch.xpu.current_device() # Create a stream and make it the current stream stream = torch.xpu.Stream(device=device) stream.synchronize() # Set random seed and get initial RNG state torch.manual_seed(seed) initial_rng_state = torch.random.get_rng_state() # Measure memory allocated before creating the tensor memory_allocated_before = torch.xpu.memory_allocated(device) # Generate a tensor on the XPU with torch.cuda.stream(stream): random_tensor = torch.rand((100, 100), device=device) stream.synchronize() # Measure memory allocated after creating the tensor memory_allocated_after = torch.xpu.memory_allocated(device) peak_memory_usage = torch.xpu.max_memory_allocated(device) # Clear tensor and cached memory del random_tensor torch.xpu.empty_cache() return { \'rng_state\': initial_rng_state, \'random_tensor\': random_tensor, \'memory_allocated_before\': memory_allocated_before, \'memory_allocated_after\': memory_allocated_after, \'peak_memory_usage\': peak_memory_usage } except Exception as e: print(f\\"Error: {e}\\")"},{"question":"Coding Assessment Question # Objective: In this task, you are required to create a function using PyTorch FX\'s experimental symbolic shapes module. The function will analyze and optimize a model by identifying and applying constraints to its dynamic dimensions. # Problem Statement: You are given a PyTorch model which includes layers with dynamic dimensions. Your task is to write a function `optimize_model_with_constraints` that: 1. Takes a PyTorch model as input. 2. Identifies dynamic dimensions and applies strict minimum-maximum constraints to dimensions that are flagged as dynamic. 3. Returns a new model with applied constraints on dynamic dimensions. # Input: - `model` (torch.nn.Module): A PyTorch model with dynamic dimensions. # Output: - (torch.nn.Module): A new PyTorch model with strict min-max constraints applied to dynamic dimensions. # Constraints: - You must use the `ShapeEnv` and `StrictMinMaxConstraint` classes from `torch.fx.experimental.symbolic_shapes`. # Example: ```python import torch import torch.nn as nn from torch.fx.experimental.symbolic_shapes import ShapeEnv, StrictMinMaxConstraint class SimpleDynamicModel(nn.Module): def __init__(self): super(SimpleDynamicModel, self).__init__() self.linear = nn.Linear(10, 20) def forward(self, x): return self.linear(x) # Sample Model model = SimpleDynamicModel() def optimize_model_with_constraints(model): # Create a ShapeEnv shape_env = ShapeEnv() # Traverse the model to detect dynamic dimensions # Apply StrictMinMaxConstraints to dynamic dimensions # Return the new optimized model with constraints applied pass # Implement your function and show the transformation optimized_model = optimize_model_with_constraints(model) ``` # Notes: - Ensure your function identifies and handles dynamic dimensions correctly. - Use the available classes and methods from the `torch.fx.experimental.symbolic_shapes` module appropriately. - The optimization should modify the model such that any dynamic dimension has strict minimum and maximum constraints. # Performance Requirements: - The function should handle models with multiple dynamic dimensions efficiently. - The constraints should be applied in such a way that it does not negatively impact the forward pass performance of the model.","solution":"import torch import torch.nn as nn from torch.fx.experimental.symbolic_shapes import ShapeEnv, StrictMinMaxConstraint class SimpleDynamicModel(nn.Module): def __init__(self): super(SimpleDynamicModel, self).__init__() self.linear = nn.Linear(10, 20) def forward(self, x): return self.linear(x) def optimize_model_with_constraints(model): # Create a ShapeEnv shape_env = ShapeEnv() # Traverse the model to detect dynamic dimensions for name, param in model.named_parameters(): for dim, size in enumerate(param.shape): if size == -1: # Assuming -1 denotes a dynamic dimension # Apply StrictMinMaxConstraints to dynamic dimensions constraint = StrictMinMaxConstraint(min=1, max=1000) # Example constraint range shape_env.set_shape(param.size(dim), constraint) # Return the new optimized model with constraints applied return model # Sample usage model = SimpleDynamicModel() optimized_model = optimize_model_with_constraints(model)"},{"question":"# Multilingual Support Implementation **Objective:** Implement a function that provides multilingual support using the `gettext` module in Python. **Problem Statement:** You are tasked with writing a function `translate_message(language_code: str, message: str) -> str` that translates a given message into the specified language. The `gettext` module should be used to handle the translations. **Requirements:** 1. Your function should support three languages: English (`en`), French (`fr`), and Spanish (`es`). 2. You are provided with the following translations for a sample set of messages: - For the message \\"Hello, world!\\": - French: \\"Bonjour, le monde!\\" - Spanish: \\"Hola, mundo!\\" - For the message \\"Goodbye!\\": - French: \\"Au revoir!\\" - Spanish: \\"Adis!\\" 3. The default language should be English if an unsupported language code is provided. **Function Signature:** ```python def translate_message(language_code: str, message: str) -> str: pass ``` **Input:** - `language_code` (str): The language code for translation. Supported values are \\"en\\", \\"fr\\", \\"es\\". - `message` (str): The message that needs to be translated. Supported messages are \\"Hello, world!\\" and \\"Goodbye!\\". **Output:** - Returns the translated message as a string. **Constraints:** - Use the `gettext` module for managing translations. - Use appropriate error handling to handle unsupported language codes. **Examples:** ```python print(translate_message(\'fr\', \'Hello, world!\')) # Output: \'Bonjour, le monde!\' print(translate_message(\'es\', \'Goodbye!\')) # Output: \'Adis!\' print(translate_message(\'de\', \'Hello, world!\')) # Output: \'Hello, world!\' (default to English as \'de\' is not supported) ``` **Note:** You may hardcode the translations for the provided messages within your solution. **Provided Code:** ```python import gettext def translate_message(language_code: str, message: str) -> str: # Hardcoded translations for simplicity translations = { \'en\': { \\"Hello, world!\\": \\"Hello, world!\\", \\"Goodbye!\\": \\"Goodbye!\\" }, \'fr\': { \\"Hello, world!\\": \\"Bonjour, le monde!\\", \\"Goodbye!\\": \\"Au revoir!\\" }, \'es\': { \\"Hello, world!\\": \\"Hola, mundo!\\", \\"Goodbye!\\": \\"Adis!\\" } } # Choose the appropriate language translation dictionary lang_translations = translations.get(language_code, translations[\'en\']) # Translate the message translated_message = lang_translations.get(message, message) return translated_message ``` **Instructions:** Fill in the `translate_message` function to complete the assignment as per the requirements. Make sure to use the `gettext` module to handle translations where applicable.","solution":"import gettext def translate_message(language_code: str, message: str) -> str: # Define translations translations = { \'en\': { \\"Hello, world!\\": \\"Hello, world!\\", \\"Goodbye!\\": \\"Goodbye!\\" }, \'fr\': { \\"Hello, world!\\": \\"Bonjour, le monde!\\", \\"Goodbye!\\": \\"Au revoir!\\" }, \'es\': { \\"Hello, world!\\": \\"Hola, mundo!\\", \\"Goodbye!\\": \\"Adis!\\" } } # Choose the appropriate language translation dictionary lang_translations = translations.get(language_code, translations[\'en\']) # Translate the message translated_message = lang_translations.get(message, message) return translated_message"},{"question":"**Coding Assessment Question:** You are provided with a dataset `data` which contains two columns: `time` (representing time in hours) and `temperature` (representing temperature in degrees Celsius). Your task is to utilize seaborn to perform the following actions and plot the data with specific axis limits. # Requirements: 1. **Initial Plot**: - Create a line plot of the data using `time` on the x-axis and `temperature` on the y-axis. - The plot should use markers at each data point. 2. **Customized Limits**: - Set the x-axis limits to range from 0 to 24 (representing a full day). - Set the y-axis limits to range from -10 to 40 degrees Celsius. 3. **Axis Inversion**: - Create a new plot where the y-axis is inverted (i.e., 40 degrees at the bottom and -10 at the top). # Input Format: - A pandas DataFrame `data` with the following structure: ```plaintext | time | temperature | |------|-------------| | 1 | 15.5 | | 2 | 16.0 | | ... | ... | ``` # Output: - Two seaborn line plots fulfilling the requirements above. ```python import pandas as pd import seaborn.objects as so def generate_plots(data: pd.DataFrame): # Plot 1: Initial plot with customized limits p1 = so.Plot(x=data[\'time\'], y=data[\'temperature\']).add(so.Line(marker=\\"o\\")) p1 = p1.limit(x=(0, 24), y=(-10, 40)) # Plot 2: Plot with inverted y-axis p2 = so.Plot(x=data[\'time\'], y=data[\'temperature\']).add(so.Line(marker=\\"o\\")) p2 = p2.limit(x=(0, 24), y=(40, -10)) return p1, p2 # Example usage: # data = pd.DataFrame({ # \\"time\\": [1, 2, 3, 4, 5], # \\"temperature\\": [15.5, 16.0, 17.2, 18.3, 19.0] # }) # p1, p2 = generate_plots(data) # p1.show() # p2.show() ``` **Constraints:** - Ensure that `seaborn.objects` is used for plotting. - Handle any potential missing or null values in the dataset.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def generate_plots(data: pd.DataFrame): Generate two seaborn line plots: 1. A line plot of time vs temperature with markers, x-axis limit (0, 24), y-axis limit (-10, 40). 2. A line plot of time vs temperature with markers, x-axis limit (0, 24), y-axis inverted with limit (40, -10). Parameters: data (pd.DataFrame): DataFrame containing \'time\' and \'temperature\' columns. Returns: fig1 (plt.Figure): Figure of the first plot. fig2 (plt.Figure): Figure of the second plot with inverted y-axis. fig1, ax1 = plt.subplots() sns.lineplot(data=data, x=\'time\', y=\'temperature\', marker=\'o\', ax=ax1) ax1.set_xlim(0, 24) ax1.set_ylim(-10, 40) fig2, ax2 = plt.subplots() sns.lineplot(data=data, x=\'time\', y=\'temperature\', marker=\'o\', ax=ax2) ax2.set_xlim(0, 24) ax2.set_ylim(40, -10) return fig1, fig2"},{"question":"**Question:** You are given a dataset containing information about different species of flowers (`iris` dataset) which includes the following columns: - `sepal_length` - `sepal_width` - `petal_length` - `petal_width` - `species` Using the Seaborn library, you are required to create a comprehensive visualization encompassing the following features: 1. **Scatter Plot**: Plot `sepal_length` vs. `sepal_width`. 2. **Rug Plot**: Add a rug plot along both axes for `sepal_length` and `sepal_width`. 3. **Hue Mapping**: Use `species` to distinguish different species with colors. 4. **Customization**: - Adjust the height of the rug plots to `0.05`. - Place the rug plots outside the axes boundaries (negative height). 5. **Additional Elements**: - Overlay a KDE plot (Kernel Density Estimate) for the `sepal_length` data. - Ensure the KDE plot uses alpha blending for better visibility over the scatter plot. **Input Format**: - There are no input arguments for the function. Your task is to load the `iris` dataset from Seaborn\'s built-in datasets. **Output Format**: - The output should be a single comprehensive Seaborn plot as specified. **Constraints**: - Constraints are inherent to the dataset and typical plotting limitations, which need not be explicitly mentioned here. **Performance**: - The performance requirements are standard given the typical size of the `iris` dataset. **Example**: Your function should create a plot similar to the illustration below (actual appearance may vary based on default styles and color mappings). ```python import seaborn as sns def plot_iris_data(): # Load iris dataset iris = sns.load_dataset(\\"iris\\") # Set theme sns.set_theme() # Create scatter plot scatter = sns.scatterplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", hue=\\"species\\") # Add rug plots rug = sns.rugplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", hue=\\"species\\", height=-0.05, clip_on=False) # Add KDE plot kde = sns.kdeplot(data=iris, x=\\"sepal_length\\", alpha=0.5) # Show the plot scatter.figure.show() # Call the function to generate the plot plot_iris_data() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_iris_data(): # Load the iris dataset iris = sns.load_dataset(\\"iris\\") # Set Seaborn theme sns.set_theme() # Create a scatter plot with hue mapping to species scatter = sns.scatterplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", hue=\\"species\\") # Add rug plots along both axes with height adjustment and placed outside boundaries sns.rugplot(data=iris, x=\\"sepal_length\\", height=-0.05, clip_on=False, palette=scatter.get_legend().get_title().get_text()) sns.rugplot(data=iris, y=\\"sepal_width\\", height=-0.05, clip_on=False, palette=scatter.get_legend().get_title().get_text()) # Overlay a KDE plot for the sepal_length with alpha blending for better visibility sns.kdeplot(data=iris, x=\\"sepal_length\\", alpha=0.5) # Show the plot plt.show()"},{"question":"Objective: You are required to demonstrate your understanding of pandas Copy-on-Write (CoW) mechanism by performing a data manipulation task. The task will test your ability to work with DataFrame and Series without causing unintended side effects. Problem Statement: 1. Create a DataFrame `df` with the following data: ``` foo bar 1 4 2 5 3 6 ``` 2. Extract the `foo` column into a Series named `foo_series`. 3. Modify the first element of `foo_series` to 100. Verify if `df` is affected by this change or not and explain the behavior. 4. Create a new DataFrame `df2` which is a reset index version of `df` (`reset_index(drop=True)`). 5. Modify the first element of `df2` with the following values: - `foo`: 150 - `bar`: 400 6. Verify if `df` is affected by this modification of `df2`. If it is, provide a rationale. If not, explain why based on CoW principles. 7. Finally, update the original DataFrame `df` where `bar` is greater than 5 to set `foo` values to 200 using the correct method complying with CoW rules. Input: No external input is required. Output: Your code should print: 1. The original DataFrame `df` after each step to show whether it has been modified. 2. The modified `foo_series`. 3. The modified DataFrame `df2`. 4. Explanations where necessary. Constraints: - Use pandas 3.0 or later. - Adhere to Copy-on-Write principles to ensure no unintended data modification. Example: Heres an example of what your function might print out: ```python # Step 3: Modify foo_series df after modifying foo_series: foo bar 0 1 4 1 2 5 2 3 6 # Step 5: Modify df2 df after modifying df2: foo bar 0 1 4 1 2 5 2 3 6 # Final Step: Update df Final df after setting foo to 200 where bar > 5: foo bar 0 1 4 1 2 5 2 200 6 ``` Guidelines: - Clearly comment on each step of your solution. - Explain the observations after each relevant step to demonstrate your understanding of CoW. - Ensure that your solution is both clear and adheres to the specified constraints.","solution":"import pandas as pd def demonstrate_CoW(): # Step 1: Create the DataFrame df = pd.DataFrame({\'foo\': [1, 2, 3], \'bar\': [4, 5, 6]}) print(\\"Original DataFrame \'df\':\\") print(df) print() # Step 2: Extract \'foo\' column into a Series foo_series = df[\'foo\'] # Step 3: Modify the first element of foo_series foo_series.iloc[0] = 100 print(\\"Modified \'foo_series\':\\") print(foo_series) print() print(\\"DataFrame \'df\' after modifying \'foo_series\' directly:\\") print(df) print(\\"Observation: \'df\' will be affected by the modification as we modified \'foo_series\'\\") print() # Step 4: Create a new DataFrame df2 with reset index version of df df2 = df.reset_index(drop=True) # Step 5: Modify the first element of df2 df2.at[0, \'foo\'] = 150 df2.at[0, \'bar\'] = 400 print(\\"Modified DataFrame \'df2\':\\") print(df2) print() print(\\"DataFrame \'df\' after modifying \'df2\':\\") print(df) print(\\"Observation: \'df\' will not be affected by modifications in \'df2\' because df2 is a separate copy.\\") print() # Final Step: Update df where \'bar\' > 5 to set \'foo\' values to 200 df.loc[df[\'bar\'] > 5, \'foo\'] = 200 print(\\"Final DataFrame \'df\' after setting \'foo\' to 200 where \'bar\' > 5:\\") print(df) demonstrate_CoW()"},{"question":"You are required to demonstrate your understanding of scikit-learn\'s dataset generation functions. Specifically, you will generate synthetic data for both classification and regression problems and then implement a basic machine learning pipeline to process this data. Part 1: Generate Synthetic Datasets 1. **Classification Dataset:** Using `make_classification`, generate a classification dataset with the following properties: - `n_samples=1000` - `n_features=20` - `n_informative=5` - `n_redundant=2` - `n_classes=3` - `n_clusters_per_class=1` - `random_state=42` This dataset will be used to train a classification model. 2. **Regression Dataset:** Using `make_regression`, generate a regression dataset with the following properties: - `n_samples=1000` - `n_features=10` - `n_informative=7` - `noise=0.1` - `random_state=42` This dataset will be used to train a regression model. Part 2: Build Machine Learning Pipelines 3. **Classification Model:** - Split the synthetic classification dataset into training and testing sets (80% train, 20% test). - Standardize the features using `StandardScaler`. - Train a Logistic Regression classifier with default parameters on the training set. - Evaluate the classifier\'s accuracy on the testing set. 4. **Regression Model:** - Split the synthetic regression dataset into training and testing sets (80% train, 20% test). - Standardize the features using `StandardScaler`. - Train a Linear Regression model with default parameters on the training set. - Evaluate the model\'s mean squared error (MSE) on the testing set. Expected Functions to Implement ```python from sklearn.datasets import make_classification, make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression, LinearRegression from sklearn.metrics import accuracy_score, mean_squared_error def generate_classification_data(): # Generate classification data X, y = make_classification(n_samples=1000, n_features=20, n_informative=5, n_redundant=2, n_classes=3, n_clusters_per_class=1, random_state=42) return X, y def generate_regression_data(): # Generate regression data X, y = make_regression(n_samples=1000, n_features=10, n_informative=7, noise=0.1, random_state=42) return X, y def classification_pipeline(X, y): # Split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train and evaluate classifier classifier = LogisticRegression() classifier.fit(X_train, y_train) y_pred = classifier.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy def regression_pipeline(X, y): # Split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train and evaluate regression model regression_model = LinearRegression() regression_model.fit(X_train, y_train) y_pred = regression_model.predict(X_test) mse = mean_squared_error(y_test, y_pred) return mse ``` Constraints and Performance Requirements: - Ensure that the code is optimized for performance and readability. - Use appropriate random states to ensure reproducibility. - The final functions should be well-documented with clear explanations of each step. Input and Output - The `generate_classification_data` and `generate_regression_data` functions will not take any input parameters and will return the generated dataset features and targets. - The `classification_pipeline` and `regression_pipeline` functions will take the generated datasets as inputs and return the accuracy and MSE, respectively.","solution":"from sklearn.datasets import make_classification, make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression, LinearRegression from sklearn.metrics import accuracy_score, mean_squared_error def generate_classification_data(): Generate synthetic classification dataset with specified properties. Returns: X (numpy.ndarray): Features of generated dataset. y (numpy.ndarray): Labels of generated dataset. X, y = make_classification(n_samples=1000, n_features=20, n_informative=5, n_redundant=2, n_classes=3, n_clusters_per_class=1, random_state=42) return X, y def generate_regression_data(): Generate synthetic regression dataset with specified properties. Returns: X (numpy.ndarray): Features of generated dataset. y (numpy.ndarray): Targets of generated dataset. X, y = make_regression(n_samples=1000, n_features=10, n_informative=7, noise=0.1, random_state=42) return X, y def classification_pipeline(X, y): Build and run a classification pipeline. Args: X (numpy.ndarray): Features of the classification dataset. y (numpy.ndarray): Labels of the classification dataset. Returns: float: Accuracy of the classifier on the test set. # Split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train classifier classifier = LogisticRegression(max_iter=1000) classifier.fit(X_train, y_train) # Predict and evaluate y_pred = classifier.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy def regression_pipeline(X, y): Build and run a regression pipeline. Args: X (numpy.ndarray): Features of the regression dataset. y (numpy.ndarray): Targets of the regression dataset. Returns: float: Mean Squared Error (MSE) of the regression model on the test set. # Split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train regression model regression_model = LinearRegression() regression_model.fit(X_train, y_train) # Predict and evaluate y_pred = regression_model.predict(X_test) mse = mean_squared_error(y_test, y_pred) return mse"},{"question":"Objective Implement a SAX-based XML parser using the `xml.sax.handler` module that processes a given XML content and extracts specific information. Problem Statement You are given an XML document containing book information. Your task is to implement an XML parser using the SAX API to extract and print the title and author of each book. Additionally, handle any parsing errors gracefully and provide meaningful error messages. Requirements 1. **ContentHandler Implementation**: Create a subclass of `xml.sax.handler.ContentHandler` to handle the relevant XML elements. 2. **ErrorHandler Implementation**: Create a subclass of `xml.sax.handler.ErrorHandler` to handle errors during parsing. 3. **Main Function**: Implement the main function to parse the provided XML content using your custom handlers. Input - An XML string containing a list of books. Each book has a `title` and `author` element. - Example XML content: ```xml <library> <book> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> </book> <book> <title>1984</title> <author>George Orwell</author> </book> <!-- More books can be added here --> </library> ``` Output - Print the title and author of each book in the following format: ``` Title: The Great Gatsby, Author: F. Scott Fitzgerald Title: 1984, Author: George Orwell ``` Constraints - Assume the XML content is well-formed. - Handle any unexpected errors during parsing by printing an error message. Example **Input:** ```xml <library> <book> <title>Brave New World</title> <author>Aldous Huxley</author> </book> <book> <title>To Kill a Mockingbird</title> <author>Harper Lee</author> </book> </library> ``` **Output:** ``` Title: Brave New World, Author: Aldous Huxley Title: To Kill a Mockingbird, Author: Harper Lee ``` Instructions 1. Implement the `BookHandler` class inheriting from `xml.sax.handler.ContentHandler`. 2. Implement the `BookErrorHandler` class inheriting from `xml.sax.handler.ErrorHandler`. 3. Write a function `parse_books(xml_content: str)` that sets up the SAX parser, registers handlers, and parses the XML content. 4. Ensure your solution is well-documented and follows best coding practices. **Note:** Use the `xml.sax` module which is part of the standard library to parse the XML content.","solution":"import xml.sax class BookHandler(xml.sax.handler.ContentHandler): def __init__(self): self.current_element = \\"\\" self.title = \\"\\" self.author = \\"\\" self.is_title = False self.is_author = False def startElement(self, name, attrs): self.current_element = name if name == \\"title\\": self.is_title = True elif name == \\"author\\": self.is_author = True def endElement(self, name): if name == \\"book\\": print(f\\"Title: {self.title}, Author: {self.author}\\") self.title = \\"\\" self.author = \\"\\" self.current_element = \\"\\" self.is_title = False self.is_author = False def characters(self, content): if self.is_title: self.title += content.strip() elif self.is_author: self.author += content.strip() class BookErrorHandler(xml.sax.handler.ErrorHandler): def error(self, exception): print(f\\"Error: {str(exception)}\\") def fatalError(self, exception): print(f\\"Fatal Error: {str(exception)}\\") def warning(self, exception): print(f\\"Warning: {str(exception)}\\") def parse_books(xml_content): parser = xml.sax.make_parser() handler = BookHandler() error_handler = BookErrorHandler() parser.setContentHandler(handler) parser.setErrorHandler(error_handler) xml.sax.parseString(xml_content, handler)"},{"question":"# Question: Label Transformation Using Scikit-Learn In this exercise, you are required to implement a function using the scikit-learn library to transform a given set of labels into binarized and encoded formats. You will use `LabelBinarizer`, `MultiLabelBinarizer`, and `LabelEncoder` for this task. Function Signature ```python def transform_labels(labels: List[Any], transformation_type: str) -> Tuple[np.ndarray, Dict[str, Any]]: Transforms the given labels using the specified transformation type. Parameters: labels (List[Any]): A list of labels that need to be transformed. It can contain either: - Multiclass labels: e.g. [1, 2, 2, 6] - Multilabel data: e.g. [[2, 3, 4], [2], [0, 1, 3]] - Non-numerical labels: e.g. [\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"] transformation_type (str): The type of transformation to apply. It can be \\"label_binarizer\\", \\"multi_label_binarizer\\", or \\"label_encoder\\". Returns: Tuple[np.ndarray, Dict[str, Any]]: A tuple containing the transformed labels and a dictionary with additional information such as class mappings or inverse transformation capability. return transformed_labels, info_dict ``` Requirements 1. Implement the transformation based on the `transformation_type` parameter: - **label_binarizer**: Use `LabelBinarizer` to transform multiclass labels. - **multi_label_binarizer**: Use `MultiLabelBinarizer` to transform multilabel data. - **label_encoder**: Use `LabelEncoder` to encode and then decode labels. 2. Return the transformed labels and a dictionary with the following information based on the `transformation_type`: - **label_binarizer**: Classes used for binarization. - **multi_label_binarizer**: Classes used for binarization. - **label_encoder**: Classes and the reverse transformation for the given labels. 3. Handle potential errors such as incorrect label types or invalid transformation types gracefully with appropriate error messages. Constraints - Use only the scikit-learn library for transforming the labels. - Ensure the function works efficiently with large datasets. Example ```python labels1 = [1, 2, 2, 6] transformation_type1 = \\"label_binarizer\\" transformed_labels1, info1 = transform_labels(labels1, transformation_type1) # transformed_labels1: array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1]]) # info1: {\'classes_\': array([1, 2, 6])} labels2 = [[2, 3, 4], [2], [0, 1, 3]] transformation_type2 = \\"multi_label_binarizer\\" transformed_labels2, info2 = transform_labels(labels2, transformation_type2) # transformed_labels2: array([[0, 0, 1, 1, 1], [0, 0, 1, 0, 0], [1, 1, 0, 1, 0]]) # info2: {\'classes_\': array([0, 1, 2, 3, 4])} labels3 = [\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"] transformation_type3 = \\"label_encoder\\" transformed_labels3, info3 = transform_labels(labels3, transformation_type3) # transformed_labels3: array([1, 1, 2, 0]) # info3: {\'classes_\': array([\'amsterdam\', \'paris\', \'tokyo\'], dtype=\'<U9\'), \'inverse_transform_\': array([\'paris\', \'paris\', \'tokyo\', \'amsterdam\'])} ```","solution":"from typing import List, Any, Tuple, Dict import numpy as np from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder def transform_labels(labels: List[Any], transformation_type: str) -> Tuple[np.ndarray, Dict[str, Any]]: if transformation_type == \\"label_binarizer\\": lb = LabelBinarizer() transformed_labels = lb.fit_transform(labels) info_dict = {\'classes_\': lb.classes_} elif transformation_type == \\"multi_label_binarizer\\": mlb = MultiLabelBinarizer() transformed_labels = mlb.fit_transform(labels) info_dict = {\'classes_\': mlb.classes_} elif transformation_type == \\"label_encoder\\": le = LabelEncoder() transformed_labels = le.fit_transform(labels) inverse_transformed_labels = le.inverse_transform(transformed_labels) info_dict = {\'classes_\': le.classes_, \'inverse_transform_\': inverse_transformed_labels} else: raise ValueError(\\"Invalid transformation type. Must be \'label_binarizer\', \'multi_label_binarizer\', or \'label_encoder\'.\\") return transformed_labels, info_dict"},{"question":"**Objective:** Implement a function to create and send a MIME email containing text, an image, and a PDF attachment using the `email.mime` module. **Problem Statement:** Write a function `create_and_send_mime_email(sender: str, recipient: str, subject: str, text: str, image_path: str, pdf_path: str) -> None` that performs the following steps: 1. Creates a MIME multipart email. 2. Adds a text part to the email. 3. Adds an image part to the email (ensure it is correctly encoded). 4. Adds a PDF attachment to the email (ensure it is correctly encoded). 5. Sets the basic headers for the email (From, To, Subject). 6. Sends the email using an SMTP server. **Requirements:** - The function should take the following inputs: - `sender`: Email address of the sender. - `recipient`: Email address of the recipient. - `subject`: Subject of the email. - `text`: Text body of the email. - `image_path`: File path to the image to be attached. - `pdf_path`: File path to the PDF document to be attached. - Ensure you use appropriate MIME types for the text, image, and PDF parts. - Use base64 encoding for the image and PDF attachments. - You can use any SMTP server for sending the email (you can mock this part if necessary). **Input and Output Format:** - The function input consists of the specified parameters. - The function does not return any value but sends an email. **Constraints:** - The file paths provided (`image_path` and `pdf_path`) will always be valid. - The provided SMTP server details will be correct for sending the email. **Example Usage:** ```python create_and_send_mime_email( sender=\\"example.sender@example.com\\", recipient=\\"example.recipient@example.com\\", subject=\\"Test Email\\", text=\\"This is a test email with an image and PDF attachment.\\", image_path=\\"/path/to/image.jpg\\", pdf_path=\\"/path/to/document.pdf\\" ) ``` This should create and send an email with the specified contents and attachments.","solution":"import smtplib from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.base import MIMEBase from email import encoders from email.utils import formataddr from email.header import Header def create_and_send_mime_email(sender: str, recipient: str, subject: str, text: str, image_path: str, pdf_path: str) -> None: # Create the MIME multipart message msg = MIMEMultipart() msg[\'From\'] = formataddr((str(Header(\'Sender Name\', \'utf-8\')), sender)) msg[\'To\'] = recipient msg[\'Subject\'] = subject # Add the text part msg.attach(MIMEText(text, \'plain\')) # Add the image part with open(image_path, \'rb\') as img_file: img = MIMEImage(img_file.read()) img.add_header(\'Content-Disposition\', \'attachment\', filename=\'image.jpg\') msg.attach(img) # Add the PDF part with open(pdf_path, \'rb\') as pdf_file: attachment = MIMEBase(\'application\', \'octet-stream\') attachment.set_payload(pdf_file.read()) encoders.encode_base64(attachment) attachment.add_header(\'Content-Disposition\', \'attachment\', filename=\'document.pdf\') msg.attach(attachment) # Send the email smtp_server = \'smtp.example.com\' # Replace with your SMTP server details smtp_port = 587 # Replace with your SMTP server port smtp_user = \'your_username\' # Replace with your SMTP server username smtp_password = \'your_password\' # Replace with your SMTP server password with smtplib.SMTP(smtp_server, smtp_port) as server: server.starttls() server.login(smtp_user, smtp_password) server.sendmail(sender, recipient, msg.as_string())"},{"question":"# Custom Scikit-Learn Estimator Implementation **Objective:** Implement a custom scikit-learn-compatible classifier, ensuring to satisfy the required API and check for proper functionality using scikit-learn utilities. **Question:** You need to implement a custom classifier `NearestCentroidClassifier` that classifies data points based on the nearest centroid for each class. Follow these guidelines: 1. Implement a new class `NearestCentroidClassifier` which should inherit from `BaseEstimator` and `ClassifierMixin`. 2. Your classifier should: - Calculate the centroid (mean) of each class during the `fit` method. - Classify new data points to the class of the nearest centroid during the `predict` method. 3. Ensure your class implements the `fit` and `predict` methods properly. 4. Your class should also include a `get_params` and `set_params` method, which are inherited from `BaseEstimator`. 5. Handle input data validation using `validate_data` from `sklearn.utils.validation`. 6. Ensure your class handles random states appropriately, if needed, using `check_random_state`. # Implementation Details - **Input:** - `fit` method should take: - `X`: array-like of shape (n_samples, n_features), Input data - `y`: array-like of shape (n_samples,), Target values - `predict` method should take: - `X`: array-like of shape (n_samples, n_features), Input data to classify - **Output:** - `fit` method should return the instance itself (`self`). - `predict` method should return an array of shape (n_samples,) with predicted class labels. Example Usage ```python import numpy as np from sklearn.utils.estimator_checks import check_estimator class NearestCentroidClassifier(BaseEstimator, ClassifierMixin): def __init__(self): pass def fit(self, X, y): X, y = validate_data(self, X, y) self.classes_, y_indices = np.unique(y, return_inverse=True) self.centroids_ = [X[y_indices == i].mean(axis=0) for i in range(len(self.classes_))] return self def predict(self, X): check_is_fitted(self) X = validate_data(self, X, reset=False) distances = np.linalg.norm(X[:, np.newaxis] - self.centroids_, axis=2) return self.classes_[np.argmin(distances, axis=1)] # You can test your estimator using: check_estimator(NearestCentroidClassifier()) ``` **Notes:** - Ensure your implementation adheres to scikit-learn standards. - Ensure the code passed `check_estimator` to be considered compatible with scikit-learn. **Constraints:** - You may assume that the input arrays are non-empty and correctly formatted. - Ensure reusability and maintainability with clear and concise code. Before you submit, make sure to test your custom classifier to ensure it meets the requirements and behaves as expected.","solution":"import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.utils.validation import check_is_fitted, check_random_state, validate_data class NearestCentroidClassifier(BaseEstimator, ClassifierMixin): def __init__(self): pass def fit(self, X, y): # Validate input data X, y = validate_data(self, X, y) # Calculate centroids for each class self.classes_, y_indices = np.unique(y, return_inverse=True) self.centroids_ = np.array([X[y_indices == i].mean(axis=0) for i in range(len(self.classes_))]) return self def predict(self, X): # Ensure the estimator is already fitted check_is_fitted(self) # Validate input data X = validate_data(self, X, reset=False) # Calculate distances from each point to the centroids distances = np.linalg.norm(X[:, np.newaxis] - self.centroids_, axis=2) # Assign class of the nearest centroid return self.classes_[np.argmin(distances, axis=1)]"},{"question":"Objective You are required to demonstrate your understanding of the `fileinput` module by writing a program that processes multiple files, handles compressed files, and reads lines with specific encodings. Problem Description Write a Python script that: 1. Reads from multiple input files, some of which may be compressed (with `.gz` or `.bz2` extensions). 2. Counts the total number of lines across all provided files. 3. Determines the cumulative line number of each line being processed and appends this number to each line. 4. Handles files with different specified encodings. 5. Writes the processed lines to new output files with the same name as the input files, but with an added `_processed` suffix before the file extension. If an input file is `data.txt`, the output file should be `data_processed.txt`. Function Signature ```python def process_files(file_list: list, encodings: dict) -> None: # Your code here ``` Input - `file_list`: A list of strings representing the file paths to be read. The files can be regular text files or compressed files with extensions `.gz` or `.bz2`. - `encodings`: A dictionary where keys are file paths and values are the encoding to be used for reading the corresponding file. If a file path is not in this dictionary, assume UTF-8 encoding. Output - The function should not return anything. Instead, it should write the processed lines to new files as described. Example Suppose the following files are given: - `data1.txt` (contains 2 lines) - `data2.gz` (contains 3 lines, gzipped) - `data3.bz2` (contains 4 lines, bzipped) and the `encodings` dictionary is: ```python { \'data1.txt\': \'utf-8\', \'data2.gz\': \'latin-1\', \'data3.bz2\': \'utf-16\' } ``` Your function will process these files and produce: - `data1_processed.txt` with the processed contents of `data1.txt` - `data2_processed.txt` with the processed contents of `data2.gz` - `data3_processed.txt` with the processed contents of `data3.bz2` Each line in the output files will be prefixed with its cumulative line number. Constraints - You may assume that the file paths provided in `file_list` and their extensions are valid. - The files may be large, so your solution should be efficient in terms of both time and space complexity. Notes - Make sure to use the `fileinput` module to manage file reading operations. - Handle errors gracefully, providing informative messages in case of issues, such as file not found or encoding errors.","solution":"import fileinput import gzip import bz2 def process_files(file_list: list, encodings: dict) -> None: line_number = 0 for file in file_list: encoding = encodings.get(file, \'utf-8\') if file.endswith(\'.gz\'): open_func = gzip.open output_file = file[:-3] + \'_processed.txt\' elif file.endswith(\'.bz2\'): open_func = bz2.open output_file = file[:-4] + \'_processed.txt\' else: open_func = open output_file = file[:-4] + \'_processed.txt\' with open_func(file, \'rt\', encoding=encoding) as input_file, open(output_file, \'w\', encoding=encoding) as output_file: for line in input_file: line_number += 1 output_file.write(f\\"{line_number}: {line}\\")"},{"question":"Coding Assessment Question You are tasked with implementing and analyzing a neural network model using the new `torch.func` API in PyTorch 2.0. The specific focus will be on using function transforms to compute gradients and jacobians of the model parameters. # Objective 1. Implement a neural network model for a simple regression task. 2. Compute the gradients of the model parameters with respect to the loss using `torch.func.grad`. 3. Compute the jacobians of the model parameters with respect to the inputs using `torch.func.jacrev`. # Instructions 1. Define a simple neural network model (`torch.nn.Module`) consisting of an input layer, one hidden layer, and an output layer. 2. Create synthetic data for the regression task (e.g., inputs and targets). 3. Implement a function `compute_loss` that computes the mean squared error (MSE) loss given the model parameters, inputs, and targets. 4. Use `torch.func.functional_call` to perform the forward pass with the provided parameters. 5. Compute the gradients of the model parameters using `torch.func.grad`. 6. Compute the jacobians of the model parameters with respect to the inputs using `torch.func.jacrev`. # Expected Input and Output Formats - **Input**: - `inputs`: Tensor of shape (batch_size, input_features) - `targets`: Tensor of shape (batch_size, output_features) - `params`: Dictionary containing the model parameters - **Output**: - `gradients`: Dictionary containing gradients of the loss with respect to each model parameter - `jacobians`: Dictionary containing jacobians of the outputs with respect to each model parameter # Performance Requirements - The implementation should handle a batch size of 64 and models with input and output feature sizes of up to 1000. - Ensure that the memory consumption is kept optimized by managing the parameter copies as suggested in the documentation. # Constraints - Do not use the old `functorch` API; only use the `torch.func` API. - Ensure that your code is efficient and follows best practices for using PyTorch. # Example ```python import torch from torch.func import grad, jacrev, functional_call # Define model class SimpleNN(torch.nn.Module): def __init__(self, input_features, hidden_features, output_features): super(SimpleNN, self).__init__() self.fc1 = torch.nn.Linear(input_features, hidden_features) self.fc2 = torch.nn.Linear(hidden_features, output_features) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Create synthetic data batch_size = 64 input_features = 10 hidden_features = 20 output_features = 1 inputs = torch.randn(batch_size, input_features) targets = torch.randn(batch_size, output_features) # Initialize model and parameters model = SimpleNN(input_features, hidden_features, output_features) params = dict(model.named_parameters()) # Define loss function def compute_loss(params, inputs, targets): prediction = functional_call(model, params, (inputs,)) return torch.nn.functional.mse_loss(prediction, targets) # Compute gradients grads = grad(compute_loss)(params, inputs, targets) # Compute jacobians jacobians = jacrev(functional_call, argnums=1)(model, params, (inputs,)) print(\\"Gradients:\\", grads) print(\\"Jacobians:\\", jacobians) ```","solution":"import torch from torch.func import grad, jacrev, functional_call # Define model class SimpleNN(torch.nn.Module): def __init__(self, input_features, hidden_features, output_features): super(SimpleNN, self).__init__() self.fc1 = torch.nn.Linear(input_features, hidden_features) self.fc2 = torch.nn.Linear(hidden_features, output_features) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Define loss function def compute_loss(params, inputs, targets): prediction = functional_call(model, params, (inputs,)) return torch.nn.functional.mse_loss(prediction, targets) # Compute gradients def compute_gradients(params, inputs, targets): return grad(compute_loss)(params, inputs, targets) # Compute jacobians def compute_jacobians(model, params, inputs): return jacrev(functional_call, argnums=1)(model, params, (inputs,)) # Create synthetic data and initialize model batch_size = 64 input_features = 10 hidden_features = 20 output_features = 1 inputs = torch.randn(batch_size, input_features) targets = torch.randn(batch_size, output_features) model = SimpleNN(input_features, hidden_features, output_features) params = dict(model.named_parameters())"},{"question":"# Custom Pickling and Unpickling for a Complex Class Objective Your task is to implement custom pickling and unpickling for a class that simulates a small database for storing and retrieving user profiles. Each profile has sensitive information like passwords that should not be stored in plain text when pickling. Class Definition: 1. **Profile Class**: - Attributes: - `username` (string) - `email` (string) - `password` (string, in plaintext) - Methods: - `__init__(self, username, email, password)` - `__repr__(self)` 2. **UserDatabase Class**: - Manages a collection of `Profile` instances. - Attributes: - `profiles` (dictionary where keys are usernames and values are `Profile` instances) - Methods: - `add_user(self, username, email, password)` - Adds a `Profile` to `profiles`. - `get_user(self, username)` - Retrieves a `Profile` by username. - `__getstate__(self)` - Custom method for pickling. - `__setstate__(self, state)` - Custom method for unpickling. - `save_to_file(self, filename)` - Pickles the `UserDatabase` instance to a file. - `load_from_file(cls, filename)` - Unpickles a `UserDatabase` instance from a file. # Constraints: - For security, `password` should be pickled in an encoded format, not plain text. - Implement encoding for passwords during pickling and decoding during unpickling. - Ensure the class works efficiently with the `pickle` module using the highest protocol available. # Input/Output: - **Input**: None directly. Your class methods take in the necessary data. - **Output**: None directly. Your methods should return appropriate objects or save/load from files. # Example: ```python import pickle import base64 class Profile: def __init__(self, username, email, password): self.username = username self.email = email self.password = password def __repr__(self): return f\'Profile(username={self.username}, email={self.email}, password={self.password})\' class UserDatabase: def __init__(self): self.profiles = {} def add_user(self, username, email, password): self.profiles[username] = Profile(username, email, password) def get_user(self, username): return self.profiles.get(username) def __getstate__(self): state = self.__dict__.copy() for username, profile in state[\'profiles\'].items(): profile.password = base64.b64encode(profile.password.encode()).decode() return state def __setstate__(self, state): self.__dict__.update(state) for username, profile in self.profiles.items(): profile.password = base64.b64decode(profile.password.encode()).decode() def save_to_file(self, filename): with open(filename, \'wb\') as f: pickle.dump(self, f, pickle.HIGHEST_PROTOCOL) @classmethod def load_from_file(cls, filename): with open(filename, \'rb\') as f: return pickle.load(f) # Example Usage: db = UserDatabase() db.add_user(\'john_doe\', \'john@example.com\', \'password123\') db.save_to_file(\'userdb.pkl\') loaded_db = UserDatabase.load_from_file(\'userdb.pkl\') print(loaded_db.get_user(\'john_doe\')) ``` Requirements: 1. Implement the `Profile` and `UserDatabase` classes as described. 2. Ensure the `UserDatabase` class methods handle the encoding and decoding of passwords correctly during pickling and unpickling. 3. Demonstrate usage with an example similar to the one provided above.","solution":"import pickle import base64 class Profile: def __init__(self, username, email, password): self.username = username self.email = email self.password = password def __repr__(self): return f\'Profile(username={self.username}, email={self.email})\' class UserDatabase: def __init__(self): self.profiles = {} def add_user(self, username, email, password): self.profiles[username] = Profile(username, email, password) def get_user(self, username): return self.profiles.get(username) def __getstate__(self): state = self.__dict__.copy() for username, profile in state[\'profiles\'].items(): profile.password = base64.b64encode(profile.password.encode()).decode() return state def __setstate__(self, state): self.__dict__.update(state) for username, profile in self.profiles.items(): profile.password = base64.b64decode(profile.password.encode()).decode() def save_to_file(self, filename): with open(filename, \'wb\') as f: pickle.dump(self, f, pickle.HIGHEST_PROTOCOL) @classmethod def load_from_file(cls, filename): with open(filename, \'rb\') as f: return pickle.load(f) # Example Usage: db = UserDatabase() db.add_user(\'john_doe\', \'john@example.com\', \'password123\') db.save_to_file(\'userdb.pkl\') loaded_db = UserDatabase.load_from_file(\'userdb.pkl\') print(loaded_db.get_user(\'john_doe\'))"},{"question":"**Problem Statement:** You are given a dataset containing information about users and their activities. The dataset is represented in a DataFrame and includes the following columns: 1. `user_id`: Nullable integer type, representing the unique identifier of each user. This column may contain missing values. 2. `activity_count`: Nullable integer type, representing the number of activities performed by each user. This column may contain missing values. 3. `activity_date`: String type, representing the date when the activities were recorded. You need to implement the following functions: 1. **clean_user_data(df: pd.DataFrame) -> pd.DataFrame:** This function should clean the dataset by: - Filling missing `user_id` values with the maximum existing `user_id` plus unique incremental values for each missing entry. - Filling missing `activity_count` values with zero. 2. **aggregate_activities(df: pd.DataFrame) -> pd.DataFrame:** This function should aggregate the total `activity_count` for each `user_id`. The returned DataFrame should have two columns: - `user_id`: The user identifier. - `total_activity_count`: The sum of `activity_count` for each user. 3. **get_active_users(df: pd.DataFrame, min_activity_threshold: int) -> pd.DataFrame:** This function should filter and return a DataFrame of users whose total activity count is greater than or equal to the specified `min_activity_threshold`. The returned DataFrame should have the same columns as the input DataFrame. # Input Format - A pandas DataFrame `df` with three columns: `user_id`, `activity_count`, and `activity_date`. - An integer `min_activity_threshold` for the `get_active_users` function. # Output Format 1. Function `clean_user_data` should return a cleaned pandas DataFrame with no missing values in `user_id` and `activity_count`. 2. Function `aggregate_activities` should return a pandas DataFrame with two columns: `user_id` and `total_activity_count`. 3. Function `get_active_users` should return a filtered pandas DataFrame with the same columns as the input DataFrame. # Constraints - You must use pandas for all DataFrame manipulations. - The `activity_date` column should not be modified by any function. ```python import pandas as pd def clean_user_data(df: pd.DataFrame) -> pd.DataFrame: # Your implementation here pass def aggregate_activities(df: pd.DataFrame) -> pd.DataFrame: # Your implementation here pass def get_active_users(df: pd.DataFrame, min_activity_threshold: int) -> pd.DataFrame: # Your implementation here pass # Example usage df = pd.DataFrame({ \'user_id\': pd.array([1, 2, None, 4, None], dtype=\\"Int64\\"), \'activity_count\': pd.array([10, None, 5, 7, None], dtype=\\"Int64\\"), \'activity_date\': [\'2021-01-01\', \'2021-01-02\', \'2021-01-03\', \'2021-01-04\', \'2021-01-05\'] }) cleaned_df = clean_user_data(df) aggregated_df = aggregate_activities(cleaned_df) active_users_df = get_active_users(aggregated_df, 10) print(cleaned_df) print(aggregated_df) print(active_users_df) ``` # Notes: - The function definitions provided above should be implemented using standard pandas operations, paying close attention to the dtype handling as described in the documentation. - Make sure to handle missing values appropriately using `pandas.NA`. - Ensure that all functions are thoroughly tested with different scenarios, including varying amounts of missing data, to validate the solution\'s robustness.","solution":"import pandas as pd def clean_user_data(df: pd.DataFrame) -> pd.DataFrame: # Fill missing user_id values with incremental unique values starting from max user_id + 1 if df[\'user_id\'].isna().any(): max_user_id = df[\'user_id\'].max() if not pd.isna(df[\'user_id\'].max()) else 0 missing_user_indices = df[df[\'user_id\'].isna()].index new_user_ids = range(max_user_id + 1, max_user_id + 1 + len(missing_user_indices)) df.loc[missing_user_indices, \'user_id\'] = new_user_ids # Fill missing activity_count values with zero df[\'activity_count\'] = df[\'activity_count\'].fillna(0) return df def aggregate_activities(df: pd.DataFrame) -> pd.DataFrame: return df.groupby(\'user_id\', as_index=False)[\'activity_count\'].sum().rename(columns={\'activity_count\': \'total_activity_count\'}) def get_active_users(df: pd.DataFrame, min_activity_threshold: int) -> pd.DataFrame: return df[df[\'total_activity_count\'] >= min_activity_threshold]"},{"question":"**Seaborn Coding Challenge** # Problem Statement You are given a dataset representing the monthly sales of a product in two different regions, Region A and Region B, over several years. Your task is to: 1. Load the dataset and convert it from wide-form to long-form. 2. Use Seaborn to visualize the data by creating a line plot showing the monthly sales trend over the years for each region. 3. Customize the plot such that: - The x-axis represents the years. - The y-axis represents the sales. - The hue represents the region (Region A or Region B). # Input Format A CSV file named `sales_data.csv` with the following structure in wide-form: ``` Year, Jan_A, Jan_B, Feb_A, Feb_B, ..., Dec_A, Dec_B 2019, 200, 250, 220, 260, ..., 210, 270 2020, 210, 260, 230, 270, ..., 220, 280 ... ``` # Output Format A line plot, created using Seaborn, that meets the aforementioned specifications. # Constraints - The dataset contains data for at least 2 years. - The sales numbers are positive integers. # Performance Requirements - The plot should be generated efficiently, considering the transformations needed. # Function Signature ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_sales_data(file_path: str) -> None: # Load the dataset sales_data = pd.read_csv(file_path) # Convert the dataset from wide-form to long-form sales_long = sales_data.melt(id_vars=[\'Year\'], var_name=\'Month_Region\', value_name=\'Sales\') # Split Month_Region into separate columns sales_long[[\'Month\', \'Region\']] = sales_long[\'Month_Region\'].str.split(\'_\', expand=True) # Create a line plot using Seaborn sns.lineplot(data=sales_long, x=\'Year\', y=\'Sales\', hue=\'Region\', style=\'Month\') # Customize the plot plt.title(\'Monthly Sales Trend Over Years for Each Region\') plt.xlabel(\'Year\') plt.ylabel(\'Sales\') plt.legend(title=\'Region\') plt.show() # Example usage: # visualize_sales_data(\'sales_data.csv\') ``` # Explanation The code provided loads the data from a CSV file, transforms it to a long-form dataset fit for visualization, and then plots it using Seaborn. The transformation and plotting steps ensure that the data is correctly handled, showing trends properly.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_sales_data(file_path: str) -> None: Reads sales data from a CSV file, transforms it from wide-form to long-form, and creates a line plot using Seaborn to visualize the monthly sales trends over years for different regions. Parameters: file_path (str): The path to the CSV file containing the sales data. # Load the dataset sales_data = pd.read_csv(file_path) # Convert the dataset from wide-form to long-form sales_long = sales_data.melt(id_vars=[\'Year\'], var_name=\'Month_Region\', value_name=\'Sales\') # Split Month_Region into separate columns sales_long[[\'Month\', \'Region\']] = sales_long[\'Month_Region\'].str.split(\'_\', expand=True) # Create a line plot using Seaborn plt.figure(figsize=(12, 6)) sns.lineplot(data=sales_long, x=\'Year\', y=\'Sales\', hue=\'Region\', style=\'Month\') # Customize the plot plt.title(\'Monthly Sales Trend Over Years for Each Region\') plt.xlabel(\'Year\') plt.ylabel(\'Sales\') plt.legend(title=\'Region\') plt.xticks(rotation=45) plt.tight_layout() plt.show() # Example usage: # visualize_sales_data(\'sales_data.csv\')"},{"question":"Nullable Boolean Operations with Pandas Objective: To evaluate your understanding and ability to work with pandas\' Nullable Boolean data types, including indexing, logical operations, and performance optimization with columns containing NA values. Problem Statement: You are given a dataframe that contains information about movies, including columns for title, rating, and whether they are part of a franchise. Some of the franchise indicators are missing (NA). You are required to implement a function that processes this dataframe to meet the following requirements: 1. **Input**: - `df`: A pandas DataFrame with the following columns: - `title` (string): Name of the movie. - `rating` (float): IMDB rating of the movie. - `franchise` (Nullable Boolean): Whether the movie is part of a franchise (True, False, or NA). 2. **Output**: - A processed pandas DataFrame with the same columns but modified as follows: - The `franchise` column should retain key values (True/False), and NA values should be filled with False. - An additional column called `family_friendly` should be added with the following criteria: - True if the rating is greater than 7 and the movie is part of a franchise. - True if the rating is greater than 8 and the movie is NOT part of a franchise. - False otherwise. - Ensure the dtype of the new column `family_friendly` is `boolean`. 3. **Constraints**: - You cannot use loops; utilize vectorized operations within pandas. - Handle nullable boolean logic as described in Kleene logic. - Ensure that the dtype of relevant columns is set appropriately to optimize performance. Implementation Details: ```python import pandas as pd import numpy as np def process_movie_dataframe(df: pd.DataFrame) -> pd.DataFrame: Processes the given dataframe as per the requirements. Parameters: df (pd.DataFrame): Input dataframe containing movie details with \'title\', \'rating\' and \'franchise\' columns. Returns: pd.DataFrame: Processed dataframe with modified \'franchise\' and new \'family_friendly\' column. # Ensure the \'franchise\' column has dtype \'boolean\' and fill NA with False df[\'franchise\'] = df[\'franchise\'].astype(\'boolean\').fillna(False) # Create the \'family_friendly\' column with specified logic df[\'family_friendly\'] = (df[\'rating\'] > 7) & df[\'franchise\'] | (df[\'rating\'] > 8) & ~df[\'franchise\'] # Ensure the dtype of \'family_friendly\' is \'boolean\' df[\'family_friendly\'] = df[\'family_friendly\'].astype(\'boolean\') return df ``` # Example: Given the following dataframe: ```python data = { \'title\': [\'Movie A\', \'Movie B\', \'Movie C\', \'Movie D\'], \'rating\': [6.9, 8.2, 7.5, 9.0], \'franchise\': [pd.NA, True, False, pd.NA] } df = pd.DataFrame(data) ``` Calling `process_movie_dataframe(df)` should return: ``` title rating franchise family_friendly 0 Movie A 6.9 False False 1 Movie B 8.2 True True 2 Movie C 7.5 False False 3 Movie D 9.0 False True ``` Notes: - Pay special attention to ensure correct handling of NA values using the `fillna` method and setting the correct dtypes. - Use vectorized operations to avoid loops for better performance.","solution":"import pandas as pd import numpy as np def process_movie_dataframe(df: pd.DataFrame) -> pd.DataFrame: Processes the given dataframe as per the requirements. Parameters: df (pd.DataFrame): Input dataframe containing movie details with \'title\', \'rating\' and \'franchise\' columns. Returns: pd.DataFrame: Processed dataframe with modified \'franchise\' and new \'family_friendly\' column. # Ensure the \'franchise\' column has dtype \'boolean\' and fill NA with False df[\'franchise\'] = df[\'franchise\'].astype(\'boolean\').fillna(False) # Create the \'family_friendly\' column with specified logic df[\'family_friendly\'] = (df[\'rating\'] > 7) & df[\'franchise\'] | (df[\'rating\'] > 8) & ~df[\'franchise\'] # Ensure the dtype of \'family_friendly\' is \'boolean\' df[\'family_friendly\'] = df[\'family_friendly\'].astype(\'boolean\') return df"},{"question":"# Custom Autograd Function Implementation **Objective:** Implement a custom autograd function in PyTorch that demonstrates the concepts of saving intermediate results during the forward pass and using them in the backward pass. # Description: You are tasked to implement a custom autograd function `CustomSin` which computes the sine of the input tensor element-wise. Additionally, in the backward pass, it should compute the gradient accurately using the saved values from the forward pass. # Requirements: 1. Implement the forward pass to compute the sine of the input. 2. Save the cosine of the input for the backward computation. 3. Implement the backward pass that uses the saved cosine values to compute the gradient. # Function Signature: ```python class CustomSin(torch.autograd.Function): @staticmethod def forward(ctx, input): # Save for backward # Return the sine of the input @staticmethod def backward(ctx, grad_output): # Retrieve saved tensors # Compute the gradient of the input ``` # Input: - A 1D tensor of any shape with floating point numbers, requiring gradient (`requires_grad=True`). # Output: - Returns a tensor with the sine of each element of the input tensor in the forward pass. - In the backward pass, it should return the gradient with respect to the input tensor. # Example: ```python import torch # Input tensor x = torch.tensor([0.0, torch.pi / 2, torch.pi, 3 * torch.pi / 2], requires_grad=True) # Apply CustomSin function output = CustomSin.apply(x) # Compute the sum of output to get scalar output output_sum = output.sum() # Perform backward pass output_sum.backward() # Expected output for the forward pass: sin(x) print(\\"Sin(x):\\", output) # tensor([0.0, 1.0, 0.0, -1.0], grad_fn=<CustomSinBackward>) # Expected gradients for the backward pass: cos(x) print(\\"Gradients:\\", x.grad) # tensor([1.0, 0.0, -1.0, 0.0]) ``` # Constraints: - Do not use PyTorch\'s built-in `torch.sin` or `torch.cos` in the backward pass computation. - Use `save_for_backward` and `saved_tensors` appropriately. # Evaluation Criteria: - Correctness of the forward pass. - Correctness of the backward pass with accurate gradient values. - Proper use of `save_for_backward` and retrieval in `backward`. - Code should be efficient and adhere to PyTorch\'s best practices.","solution":"import torch class CustomSin(torch.autograd.Function): @staticmethod def forward(ctx, input): sine = input.sin() # Calculate sine of the input cosine = input.cos() # Calculate cosine of the input ctx.save_for_backward(cosine) # Save cosine for the backward pass return sine @staticmethod def backward(ctx, grad_output): cosine, = ctx.saved_tensors # Retrieve saved tensors grad_input = grad_output * cosine # Chain rule: grad_output * derivative of sine return grad_input"},{"question":"# Shared Memory and Process Synchronization in Python Objective Write a Python script to demonstrate the use of shared memory for managing data across multiple processes. You will implement a scenario where two processes update shared memory concurrently, and a third process reads the updates, ensuring data consistency and synchronization. Task 1. Create a shared memory block and initialize it with a NumPy array of random integers. 2. Initialize two processes that: - Process 1: Doubles the values of all even-indexed elements in the shared array. - Process 2: Triples the values of all odd-indexed elements in the shared array. 3. After both processes complete their updates, a third process should read and print the final state of the shared array. 4. Properly close and unlink the shared memory block after all processes are done. Requirements 1. Use the `multiprocessing.shared_memory.SharedMemory` class to create and manage the shared memory block. 2. Use the `multiprocessing.Process` class to create and start the processes. 3. Utilize NumPy for array manipulations. 4. Ensure that access to shared memory is properly synchronized to prevent race conditions. Input and Output Formats - **Input**: None - **Output**: Print statements showing the initial, intermediate, and final states of the shared memory array. Constraints - The shared array size should be at least 10 integers. - The elements in the array should be in the range of 1 to 100. - Ensure proper error handling for shared memory operations. Example Here is an indicative structure of your script: ```python import numpy as np import multiprocessing from multiprocessing import shared_memory def double_even_indices(shm_name, n): existing_shm = shared_memory.SharedMemory(name=shm_name) np_array = np.ndarray((n,), dtype=np.int64, buffer=existing_shm.buf) np_array[::2] *= 2 existing_shm.close() def triple_odd_indices(shm_name, n): existing_shm = shared_memory.SharedMemory(name=shm_name) np_array = np.ndarray((n,), dtype=np.int64, buffer=existing_shm.buf) np_array[1::2] *= 3 existing_shm.close() def print_final_state(shm_name, n): existing_shm = shared_memory.SharedMemory(name=shm_name) np_array = np.ndarray((n,), dtype=np.int64, buffer=existing_shm.buf) print(\\"Final array state:\\", np_array) existing_shm.close() if __name__ == \\"__main__\\": n = 10 original_array = np.random.randint(1, 101, size=n) shm = shared_memory.SharedMemory(create=True, size=original_array.nbytes) np_array = np.ndarray(original_array.shape, dtype=original_array.dtype, buffer=shm.buf) np_array[:] = original_array[:] print(\\"Initial array state:\\", np_array) p1 = multiprocessing.Process(target=double_even_indices, args=(shm.name, n)) p2 = multiprocessing.Process(target=triple_odd_indices, args=(shm.name, n)) p1.start() p2.start() p1.join() p2.join() print_final_state(shm.name, n) shm.close() shm.unlink() ``` Make sure your script includes proper synchronization and cleanup steps as described in the requirements.","solution":"import numpy as np import multiprocessing from multiprocessing import shared_memory def double_even_indices(shm_name, n): existing_shm = shared_memory.SharedMemory(name=shm_name) np_array = np.ndarray((n,), dtype=np.int64, buffer=existing_shm.buf) np_array[::2] *= 2 existing_shm.close() def triple_odd_indices(shm_name, n): existing_shm = shared_memory.SharedMemory(name=shm_name) np_array = np.ndarray((n,), dtype=np.int64, buffer=existing_shm.buf) np_array[1::2] *= 3 existing_shm.close() def print_final_state(shm_name, n): existing_shm = shared_memory.SharedMemory(name=shm_name) np_array = np.ndarray((n,), dtype=np.int64, buffer=existing_shm.buf) print(\\"Final array state:\\", np_array) existing_shm.close() if __name__ == \\"__main__\\": n = 10 original_array = np.random.randint(1, 101, size=n) shm = shared_memory.SharedMemory(create=True, size=original_array.nbytes) np_array = np.ndarray(original_array.shape, dtype=original_array.dtype, buffer=shm.buf) np_array[:] = original_array[:] print(\\"Initial array state:\\", np_array) p1 = multiprocessing.Process(target=double_even_indices, args=(shm.name, n)) p2 = multiprocessing.Process(target=triple_odd_indices, args=(shm.name, n)) p1.start() p2.start() p1.join() p2.join() print_final_state(shm.name, n) shm.close() shm.unlink()"},{"question":"**Coding Assessment Question** # Objective: Implement functions that demonstrate the ability to create, manipulate, and interact with tuples using Python\'s tuple-related capabilities. # Question: You are provided with a set of functions that need to be implemented. These functions will handle tuples and struct sequence objects as described in the documentation. The task is to implement the following functions: 1. **create_tuple(elements: list) -> tuple:** - **Input:** A list of Python objects. - **Output:** A tuple containing the elements from the list. - **Constraints:** The input list will have at most 1000 elements. - **Description:** Use the provided `PyTuple_New` and `PyTuple_SetItem` functionality to create and return the tuple. 2. **get_tuple_size(t: tuple) -> int:** - **Input:** A Python tuple. - **Output:** The size of the tuple. - **Description:** Use the `PyTuple_Size` function to determine and return the size of the tuple. 3. **access_tuple_elements(t: tuple, index: int) -> object:** - **Input:** A Python tuple and an integer index. - **Output:** The object at the specified index in the tuple. - **Constraints:** 0 <= index < len(t). - **Description:** Use `PyTuple_GetItem` to get the element at the specified index and return it. 4. **resize_tuple(t: tuple, new_size: int) -> tuple:** - **Input:** A tuple and an integer specifying the new size. - **Output:** A new tuple resized to the new size. - **Constraints:** The new size must be non-negative and at most 1000. - **Description:** Uses `_PyTuple_Resize` to resize the tuple to the new length. Return the new tuple. 5. **create_struct_sequence(name: str, field_names: list) -> type:** - **Input:** A string for the name of the struct sequence and a list of strings for field names. - **Output:** A new struct sequence type. - **Constraints:** The field names list will have at most 10 elements. - **Description:** Create and return a struct sequence type using the `PyStructSequence_NewType` function. Code Template: ```python def create_tuple(elements: list) -> tuple: # Your implementation here pass def get_tuple_size(t: tuple) -> int: # Your implementation here pass def access_tuple_elements(t: tuple, index: int) -> object: # Your implementation here pass def resize_tuple(t: tuple, new_size: int) -> tuple: # Your implementation here pass def create_struct_sequence(name: str, field_names: list) -> type: # Your implementation here pass ``` # Note: - Ensure you handle all edge cases and constraints specified. - Do not use any library functions except those specified (like `PyTuple_New`, `PyTuple_SetItem`, `PyTuple_Size`, `PyTuple_GetItem`, `PyTuple_Resize`, `PyStructSequence_NewType`). - You are required to import necessary items from the Python/C API when handling tuple and struct sequence operations. Good Luck!","solution":"def create_tuple(elements: list) -> tuple: Creates a tuple from the given list of elements. return tuple(elements) def get_tuple_size(t: tuple) -> int: Returns the size of the tuple. return len(t) def access_tuple_elements(t: tuple, index: int) -> object: Accesses and returns the element at the given index of the tuple. return t[index] def resize_tuple(t: tuple, new_size: int) -> tuple: Resizes the tuple to the new size. For the purpose of this function, resizing will truncate or pad with None. if new_size < len(t): return t[:new_size] else: return t + (None,) * (new_size - len(t)) def create_struct_sequence(name: str, field_names: list) -> type: Creates a new struct sequence type with the given name and field names. from collections import namedtuple return namedtuple(name, field_names)"},{"question":"**Objective:** Implement a Python program using the \\"readline\\" module to demonstrate proficiency in handling input history and implementing custom auto-completion for a set of commands. **Problem Statement:** Write a Python program that: 1. Loads command history from a file named `.custom_history` when the program starts, if the file exists. 2. Saves command history to the file named `.custom_history` when the program exits. 3. Implements a custom auto-completion feature for a predefined set of commands, which include `\\"start\\"`, `\\"stop\\"`, `\\"status\\"`, and `\\"exit\\"`. The auto-completion should trigger when the user presses the `Tab` key. 4. Provides a loop that continuously reads input commands, processes them (simply print the command entered as feedback), and breaks the loop when the `exit` command is entered. **Function Specifications:** - Function `load_history(histfile: str) -> None`: - Loads the history from the specified file. - If the file doesn\'t exist, handle the `FileNotFoundError` without raising it. - Function `save_history(histfile: str) -> None`: - Writes the current session\'s history to the specified file. - Function `complete(text: str, state: int) -> Optional[str]`: - Implements the custom auto-completion logic for the predefined commands. **Input Constraints:** - No input parameters are provided to the program on execution. **Output Requirements:** - Print each command entered by the user. - Properly handle history loading and saving, and implement custom auto-completion. **Performance Requirements:** - The program should efficiently handle the operations described and be responsive to user input. **Example Usage:** ```python # Expected output when user enters some commands python custom_readline.py > star[TAB] (start is auto-completed) > start start > stop stop > status status > exit exit ``` # Implementation: Your task is to complete the following code template to meet the requirements: ```python import readline import atexit import os HISTORY_FILE = os.path.expanduser(\\"~/.custom_history\\") COMMANDS = [\\"start\\", \\"stop\\", \\"status\\", \\"exit\\"] def load_history(histfile): try: readline.read_history_file(histfile) except FileNotFoundError: pass def save_history(histfile): readline.write_history_file(histfile) def complete(text, state): options = [cmd for cmd in COMMANDS if cmd.startswith(text)] if state < len(options): return options[state] else: return None def main(): readline.parse_and_bind(\\"tab: complete\\") readline.set_completer(complete) load_history(HISTORY_FILE) atexit.register(save_history, HISTORY_FILE) while True: try: line = input(\\"> \\") print(line) if line == \\"exit\\": break except EOFError: break if __name__ == \\"__main__\\": main() ``` **Submission:** Submit your completed Python script that meets the above requirements.","solution":"import readline import atexit import os HISTORY_FILE = os.path.expanduser(\\"~/.custom_history\\") COMMANDS = [\\"start\\", \\"stop\\", \\"status\\", \\"exit\\"] def load_history(histfile): try: readline.read_history_file(histfile) except FileNotFoundError: pass def save_history(histfile): readline.write_history_file(histfile) def complete(text, state): options = [cmd for cmd in COMMANDS if cmd.startswith(text)] if state < len(options): return options[state] else: return None def main(): readline.parse_and_bind(\\"tab: complete\\") readline.set_completer(complete) load_history(HISTORY_FILE) atexit.register(save_history, HISTORY_FILE) while True: try: line = input(\\"> \\") print(line) if line == \\"exit\\": break except EOFError: break if __name__ == \\"__main__\\": main()"},{"question":"# Decision Tree Coding Assessment Objective Implement and evaluate decision tree models for both classification and regression tasks using the `scikit-learn` library. Problem Statement You are provided with two datasets: 1. **Iris Dataset**: A classic dataset in machine learning used for the classification task. It contains four features of iris flowers and a target variable indicating their species. 2. **Synthetic Dataset**: A generated dataset for a regression task. Your tasks are: 1. **Classification Task**: - Load the Iris dataset. - Train a `DecisionTreeClassifier` model on the dataset. - Evaluate the model using accuracy score and visualize the tree. 2. **Regression Task**: - Generate a synthetic dataset for regression using `make_regression`. - Train a `DecisionTreeRegressor` model on the dataset. - Evaluate the model using mean squared error and visualize the tree. Input 1. For the classification task, use the Iris dataset. 2. For the regression task, use the `make_regression` function to generate the dataset. Output 1. **Classification Task**: - Accuracy score of the trained model on the test set. - Visualization of the trained decision tree. 2. **Regression Task**: - Mean squared error of the trained model on the test set. - Visualization of the trained decision tree. Constraints - The depth of the decision tree should be controlled to avoid overfitting (use a maximum depth of 3 for both tasks). - Data should be split into training and testing sets with an 80/20 split. Steps 1. **Classification Task**: - Load the Iris dataset using `load_iris`. - Split the dataset into training and testing sets. - Train a `DecisionTreeClassifier` with a maximum depth of 3. - Evaluate the model by computing the accuracy score on the test set. - Visualize the decision tree using `plot_tree`. ```python from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier, plot_tree from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt # Load the dataset data = load_iris() X, y = data.data, data.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the decision tree classifier clf = DecisionTreeClassifier(max_depth=3, random_state=42) clf.fit(X_train, y_train) # Evaluate the model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy: {accuracy}\\") # Visualize the decision tree plt.figure(figsize=(12, 8)) plot_tree(clf, feature_names=data.feature_names, class_names=data.target_names, filled=True) plt.show() ``` 2. **Regression Task**: - Generate a synthetic dataset using `make_regression`. - Split the dataset into training and testing sets. - Train a `DecisionTreeRegressor` with a maximum depth of 3. - Evaluate the model by computing the mean squared error on the test set. - Visualize the decision tree using `plot_tree`. ```python from sklearn.datasets import make_regression from sklearn.tree import DecisionTreeRegressor, plot_tree from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error import matplotlib.pyplot as plt # Generate synthetic dataset X, y = make_regression(n_samples=100, n_features=4, noise=0.1, random_state=42) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the decision tree regressor reg = DecisionTreeRegressor(max_depth=3, random_state=42) reg.fit(X_train, y_train) # Evaluate the model y_pred = reg.predict(X_test) mse = mean_squared_error(y_test, y_pred) print(f\\"Mean Squared Error: {mse}\\") # Visualize the decision tree plt.figure(figsize=(12, 8)) plot_tree(reg, filled=True) plt.show() ``` Submission - Submit your code in a Jupyter notebook or `.py` file. - Include a markdown cell or comments explaining each step of your implementation. - Ensure your visualizations are clear and properly labeled.","solution":"from sklearn.datasets import load_iris, make_regression from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, mean_squared_error import matplotlib.pyplot as plt def classification_task(): # Load the Iris dataset data = load_iris() X, y = data.data, data.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the decision tree classifier clf = DecisionTreeClassifier(max_depth=3, random_state=42) clf.fit(X_train, y_train) # Evaluate the model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy: {accuracy}\\") # Visualize the decision tree plt.figure(figsize=(12, 8)) plot_tree(clf, feature_names=data.feature_names, class_names=data.target_names, filled=True) plt.show() def regression_task(): # Generate synthetic dataset X, y = make_regression(n_samples=100, n_features=4, noise=0.1, random_state=42) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the decision tree regressor reg = DecisionTreeRegressor(max_depth=3, random_state=42) reg.fit(X_train, y_train) # Evaluate the model y_pred = reg.predict(X_test) mse = mean_squared_error(y_test, y_pred) print(f\\"Mean Squared Error: {mse}\\") # Visualize the decision tree plt.figure(figsize=(12, 8)) plot_tree(reg, filled=True) plt.show()"},{"question":"# Secure User Authentication System You are tasked with creating a secure user authentication system using Python. The system should be able to: 1. Prompt a user to enter their username and password. 2. Verify if the provided username exists in the system. 3. If the username exists, verify if the provided password matches the stored password for that username. 4. If both the username and password are correct, print a welcome message. Otherwise, print an error message. Your implementation should use the `getpass` module for secure password input and should include a predefined dictionary of users and passwords to simulate a simple user database. Expected Input and Output - **Input**: The user will be prompted to input their username and password. - **Output**: A welcome message if authenticated, otherwise an error message. Constraints - The usernames and passwords are case-sensitive. - Use the `getpass.getpass` function to securely prompt for the password. - The user database should be hardcoded as a dictionary within your code, for example: ```python users = { \'user1\': \'password123\', \'user2\': \'securepassword\', \'admin\': \'admin1234\' } ``` - Ensure that the password is not echoed to the screen when the user is typing it. Performance Requirements - The system should handle the input securely without compromising the visible safety of the password input process. - It should operate efficiently, checking for the existence of the user and the validity of the password in constant time. Function Signature Your implementation should include a function with the following signature: ```python def authenticate_user(users: dict): pass ``` Example usage: ```python def authenticate_user(users: dict): import getpass username = input(\\"Enter your username: \\") password = getpass.getpass(\\"Enter your password: \\") if username in users and users[username] == password: print(f\\"Welcome, {username}!\\") else: print(\\"Error: Invalid username or password.\\") # Example User Database users = { \'user1\': \'password123\', \'user2\': \'securepassword\', \'admin\': \'admin1234\' } authenticate_user(users) ```","solution":"def authenticate_user(users: dict): import getpass username = input(\\"Enter your username: \\") password = getpass.getpass(\\"Enter your password: \\") if username in users and users[username] == password: print(f\\"Welcome, {username}!\\") else: print(\\"Error: Invalid username or password.\\") # Example User Database users = { \'user1\': \'password123\', \'user2\': \'securepassword\', \'admin\': \'admin1234\' }"},{"question":"# PyTorch Coding Assessment Objective: Design a PyTorch-based function to train a simple linear regression model using gradient descent and serialize the model. This task will test your understanding of tensor operations, gradient computation, and serialization in PyTorch. Problem Statement: You are required to implement a function `train_and_save_linear_model` that performs the following tasks: 1. **Generate a synthetic dataset**: - Create a dataset of `N` data points with `X` (independent variable) and `Y` (dependent variable), where `Y = 3 * X + 4` with some added noise. - Use `torch.randn` to generate the noise. 2. **Define a simple linear regression model**: - Initialize the parameters `weights` (`w`) and `bias` (`b`) using `torch.randn`. 3. **Train the model using gradient descent**: - Use mean squared error (MSE) as the loss function. - Train for a specified number of epochs `num_epochs` and learning rate `lr`. - Update the weights and bias using gradient descent. 4. **Serialize the trained model parameters**: - Save the final weights and bias to a file named `linear_model.pth`. Function Signature: ```python def train_and_save_linear_model(N: int, num_epochs: int, lr: float) -> None: pass ``` Input: - `N` (int): Number of data points (e.g., 100) - `num_epochs` (int): Number of epochs for training (e.g., 1000) - `lr` (float): Learning rate for gradient descent (e.g., 0.01) Output: - The function should save the final model parameters (weights and bias) to a file named `linear_model.pth` Requirements: 1. Use `torch.no_grad()` to update the model parameters. 2. Ensure gradients are zeroed before each backward pass. 3. Serialize the model parameters using `torch.save`. Example Usage: ```python train_and_save_linear_model(100, 1000, 0.01) ``` Evaluation Criteria: - Correctness: The implementation should generate the correct model parameters. - Efficiency: The code should run efficiently for the given constraints. - Clarity: The code should be well-organized and properly commented.","solution":"import torch def train_and_save_linear_model(N: int, num_epochs: int, lr: float) -> None: # Generate a synthetic dataset X = torch.randn(N, 1) noise = torch.randn(N, 1) Y = 3 * X + 4 + noise # Initialize weights and bias w = torch.randn(1, requires_grad=True) b = torch.randn(1, requires_grad=True) # Training loop for epoch in range(num_epochs): # Forward pass: compute predicted Y Y_pred = X * w + b # Compute and print the loss loss = torch.mean((Y_pred - Y) ** 2) # Zero gradients, perform a backward pass, and update the weights loss.backward() with torch.no_grad(): w -= lr * w.grad b -= lr * b.grad # Manually zero the gradients after updating weights w.grad.zero_() b.grad.zero_() # Save the final model parameters torch.save({\'weights\': w.item(), \'bias\': b.item()}, \'linear_model.pth\') # Example usage train_and_save_linear_model(100, 1000, 0.01)"},{"question":"<|Analysis Begin|> The provided documentation snippet talks about the `torch.utils.deterministic` module, specifically focusing on the `fill_uninitialized_memory` attribute. This boolean attribute, when set to `True`, ensures that uninitialized memory is filled with known values (NaN for floating point and complex, maximum values for integers) when deterministic algorithms are used via `torch.use_deterministic_algorithms(True)`. It mentions that this can be detrimental to performance but is useful for ensuring that no uninitialized memory is used. To design a coding question, we can leverage the concept of deterministic algorithms and the consequences of working with uninitialized memory. Students can be asked to create a function that works with tensors and demonstrate the effects of toggling `fill_uninitialized_memory`. They can also be tasked to illustrate deterministic vs. non-deterministic operations. <|Analysis End|> <|Question Begin|> You are required to implement a function that demonstrates the difference between deterministic and non-deterministic behaviors in PyTorch, especially focusing on the handling of uninitialized memory. # Task Implement a function `compare_deterministic_behaviors()` that performs the following steps: 1. Initializes two uninitialized tensors using `torch.empty` under two different settings: a. When `torch.use_deterministic_algorithms(True)` is set with `fill_uninitialized_memory` enabled. b. When `torch.use_deterministic_algorithms(False)` is set. 2. Returns these two tensors for comparison. # Expected Function Signature ```python def compare_deterministic_behaviors(tensor_size: Tuple[int, ...]) -> Tuple[torch.Tensor, torch.Tensor]: ``` # Input - `tensor_size (Tuple[int, ...])`: A tuple representing the size of the tensor to create. # Output - `A Tuple of two torch.Tensors`: * The first tensor should be created with deterministic algorithms enabled and the `fill_uninitialized_memory` setting. * The second tensor should be created without deterministic algorithms. # Example ```python compare_deterministic_behaviors((2, 3)) ``` This may output something similar to: ```python (tensor([[nan, nan, nan], [nan, nan, nan]]), tensor([[ 0.0000e+00, 1.4013e-45, 0.0000e+00], [ 1.4013e-45, 0.0000e+00, 1.4013e-45]])) ``` # Constraints - The function should ensure that uninitialized memory under deterministic algorithms are filled with known values. - The function should not perform any operations on the tensors other than initializing them. - Use the PyTorch library (torch) for tensor operations. # Performance - Ensure the function runs efficiently for larger tensor sizes. # Hints - Use `torch.empty(...)` to create uninitialized tensors. - Utilize `torch.use_deterministic_algorithms(True/False)` to toggle deterministic behavior. - Set `torch.utils.deterministic.fill_uninitialized_memory` appropriately to demonstrate its effect. # Important Ensure your implementation adheres to the above constraints and correctly demonstrates the impact of deterministic algorithms on uninitialized memory in PyTorch.","solution":"import torch from typing import Tuple def compare_deterministic_behaviors(tensor_size: Tuple[int, ...]) -> Tuple[torch.Tensor, torch.Tensor]: Compares the behaviors of uninitialized memory with deterministic and non-deterministic settings. Args: tensor_size (Tuple[int, ...]): Size of the tensors to initialize. Returns: Tuple[torch.Tensor, torch.Tensor]: A tuple of two tensors, one with deterministic algorithms and filled uninitialized memory, and one without deterministic algorithms. # Enabling deterministic algorithms and fill uninitialized memory torch.use_deterministic_algorithms(True) torch.utils.deterministic.fill_uninitialized_memory = True det_tensor = torch.empty(tensor_size) # Disabling deterministic algorithms torch.use_deterministic_algorithms(False) non_det_tensor = torch.empty(tensor_size) return det_tensor, non_det_tensor"},{"question":"# ByteArray Manipulations Objective Implement a Python function that performs several manipulations on bytearrays using the provided C API and macro functions. The function should accept two bytearrays and return a new bytearray that is the concatenation of specified slices from each input bytearray. Function Signature ```python def manipulate_bytearrays(a: bytearray, b: bytearray, start_a: int, end_a: int, start_b: int, end_b: int) -> bytearray: Manipulates two input bytearrays by concatenating specified slices. Parameters: - a: A bytearray object. - b: A bytearray object. - start_a: Start index for slicing bytearray \'a\'. - end_a: End index for slicing bytearray \'a\'. - start_b: Start index for slicing bytearray \'b\'. - end_b: End index for slicing bytearray \'b\'. Returns: A new bytearray object that is the concatenation of slices \'a[start_a:end_a]\' and \'b[start_b:end_b]\'. pass ``` Input Constraints - `a` and `b` are non-empty bytearrays. - `start_a`, `end_a`, `start_b`, and `end_b` are valid indexes within the respective bytearrays. - `start_a <= end_a` and `start_b <= end_b`. Output - A bytearray resulting from the concatenation of the specified slices from bytearrays `a` and `b`. Implementation Requirements - Use `PyByteArray_FromObject`, `PyByteArray_FromStringAndSize`, `PyByteArray_Concat`, `PyByteArray_Size`, `PyByteArray_AsString`, and `PyByteArray_Resize` where applicable. - Utilize the provided macros (`PyByteArray_AS_STRING` and `PyByteArray_GET_SIZE`) as part of your implementation. Example ```python a = bytearray(b\\"hello\\") b = bytearray(b\\"world\\") # Slicing \'hello\' from index 1 to 4 gives \'ell\' # Slicing \'world\' from index 0 to 3 gives \'wor\' # Concatenating results in \'ellwor\' result = manipulate_bytearrays(a, b, 1, 4, 0, 3) print(result) # Output: bytearray(b\'ellwor\') ``` This problem requires students to understand and apply API functions and macros related to Python bytearrays, thereby demonstrating a comprehensive understanding of the bytearray manipulation capabilities in Python.","solution":"def manipulate_bytearrays(a: bytearray, b: bytearray, start_a: int, end_a: int, start_b: int, end_b: int) -> bytearray: Manages two input bytearrays by concatenating specific slices. Parameters: - a: A bytearray object. - b: A bytearray object. - start_a: Start index for slicing bytearray \'a\'. - end_a: End index for slicing bytearray \'a\'. - start_b: Start index for slicing bytearray \'b\'. - end_b: End index for slicing bytearray \'b\'. Returns: A new bytearray object that is the concatenation of slices \'a[start_a:end_a]\' and \'b[start_b:end_b]\'. slice_a = a[start_a:end_a] slice_b = b[start_b:end_b] return slice_a + slice_b"},{"question":"**Question: Automating File Operations with Python Standard Library** You are tasked with writing a Python script that automates the following operations: 1. **Create a new directory**: Use the `os` module to create a new directory named \\"example_dir\\" in the current working directory. 2. **Generate a File with Random Numbers**: - In this new directory, create a file named \\"random_numbers.txt\\". - Use the `random` module to generate 1000 random integers between 1 and 100. - Write these numbers to the file, each number on a new line. 3. **Read and Process the File**: - Read the numbers from \\"random_numbers.txt\\". - Use the `statistics` module to compute and display the mean, median, and variance of these numbers. 4. **Compress the File**: - Use the `zipfile` module to compress \\"random_numbers.txt\\" into \\"random_numbers.zip\\". - Ensure the original \\"random_numbers.txt\\" is deleted after compression. 5. **Log the Process**: - Use the `datetime` module to log timestamps for each major step (directory creation, file creation, data processing, and file compression). - Save these logs in a file named \\"process.log\\" within \\"example_dir\\". **Input/Output formats:** - There is no specific input for this script, it will act upon execution. - The outputs are the creation of files (`random_numbers.txt`, `random_numbers.zip`, `process.log`) and console-printed statistical results (mean, median, variance). **Constraints:** - You must use the modules as specified to complete each task. - Ensure your script handles potential errors gracefully, such as directory already existing or inability to write to files. **Performance requirements**: - The script should complete within a reasonable time frame for the task size specified, given the operations on 1000 integers are not computationally intense. Your implementation should be efficient and should follow good coding practices. Ensure to comment your code where necessary. **Implementation Tip:** - Make use of functions to modularize your script. - Ensure that you demonstrate an understanding of file handling, random number generation, data processing with statistics, and file compression. Good luck!","solution":"import os import random import statistics import zipfile import datetime def create_directory(directory_name): if not os.path.exists(directory_name): os.makedirs(directory_name) log_event(f\\"Created directory: {directory_name}\\") else: log_event(f\\"Directory already exists: {directory_name}\\") def generate_random_numbers_file(file_path): numbers = [random.randint(1, 100) for _ in range(1000)] with open(file_path, \'w\') as file: for number in numbers: file.write(f\\"{number}n\\") log_event(f\\"Generated random numbers and saved to: {file_path}\\") return numbers def read_numbers_from_file(file_path): with open(file_path, \'r\') as file: numbers = [int(line.strip()) for line in file] return numbers def compute_statistics(numbers): mean = statistics.mean(numbers) median = statistics.median(numbers) variance = statistics.variance(numbers) print(f\\"Mean: {mean}, Median: {median}, Variance: {variance}\\") log_event(\\"Computed statistics: mean, median, and variance\\") return mean, median, variance def compress_file(file_path, zip_path): with zipfile.ZipFile(zip_path, \'w\') as zipf: zipf.write(file_path, os.path.basename(file_path)) os.remove(file_path) log_event(f\\"Compressed {file_path} to {zip_path} and deleted the original file\\") def log_event(message): log_message = f\\"{datetime.datetime.now()} - {message}n\\" with open(\\"example_dir/process.log\\", \'a\') as log_file: log_file.write(log_message) def main(): directory_name = \\"example_dir\\" create_directory(directory_name) file_path = os.path.join(directory_name, \\"random_numbers.txt\\") zip_path = os.path.join(directory_name, \\"random_numbers.zip\\") numbers = generate_random_numbers_file(file_path) read_numbers = read_numbers_from_file(file_path) compute_statistics(read_numbers) compress_file(file_path, zip_path) if __name__ == \\"__main__\\": main()"},{"question":"Objective: Implement a Python function that attempts to download a file from a given URL. The function should handle the various exceptions defined in the `urllib.error` module, providing meaningful error messages or retrying the download based on the type of error encountered. Requirements: 1. The function should take in a URL and a local file path as input. 2. If the download is successful, the file should be saved to the specified local path. 3. The function should handle the following exceptions: - `URLError`: Print a generic error message. - `HTTPError`: Print the HTTP status code and reason. - `ContentTooShortError`: Retry the download up to 3 times before giving up and printing an error message. 4. The function should return `True` if the download is successful and `False` otherwise. Function Signature: ```python def download_file(url: str, local_path: str) -> bool: pass ``` Input: - `url` (str): The URL of the file to download. - `local_path` (str): The local file path where the downloaded file should be saved. Output: - Returns `True` if the download is successful, `False` otherwise. Example Usage: ```python success = download_file(\\"http://example.com/file.txt\\", \\"local_file.txt\\") print(success) # Output: True or False ``` Constraints: - Use only the `urllib.request` module for handling HTTP requests. - The function should handle timeouts and network issues gracefully. Example Implementation: ```python import urllib.request import urllib.error def download_file(url: str, local_path: str) -> bool: retries = 3 while retries > 0: try: with urllib.request.urlopen(url) as response, open(local_path, \'wb\') as out_file: data = response.read() out_file.write(data) return True except urllib.error.HTTPError as e: print(f\\"HTTPError: {e.code} - {e.reason}\\") return False except urllib.error.URLError as e: print(f\\"URLError: {e.reason}\\") return False except urllib.error.ContentTooShortError as e: print(\\"ContentTooShortError: The download was incomplete. Retrying...\\") retries -= 1 print(\\"Failed to download the file after multiple attempts.\\") return False ```","solution":"import urllib.request import urllib.error def download_file(url: str, local_path: str) -> bool: retries = 3 while retries > 0: try: with urllib.request.urlopen(url) as response, open(local_path, \'wb\') as out_file: data = response.read() out_file.write(data) return True except urllib.error.HTTPError as e: print(f\\"HTTPError: {e.code} - {e.reason}\\") return False except urllib.error.URLError as e: print(f\\"URLError: {e.reason}\\") return False except urllib.error.ContentTooShortError as e: print(\\"ContentTooShortError: The download was incomplete. Retrying...\\") retries -= 1 print(\\"Failed to download the file after multiple attempts.\\") return False"},{"question":"# Color Palette Visualizations with Seaborn **Objective:** In this coding assessment, you will demonstrate your understanding of Seaborn\'s color palettes and their applications in data visualization. **Task:** You are given a dataset that contains information about cars, including their fuel efficiency, horsepower, weight, and origin (USA, Europe, Japan). Your task is to create visual representations of this data using Seaborn\'s color palettes. **Dataset:** The dataset is structured as follows: ```python import pandas as pd data = { \'mpg\': [18, 15, 36, 24, 17, 14, 23, 29, 32, 20], \'horsepower\': [130, 165, 80, 150, 140, 198, 95, 68, 155, 103], \'weight\': [3504, 3693, 2234, 3175, 3449, 4332, 2372, 2140, 2665, 2639], \'origin\': [\'USA\', \'USA\', \'Japan\', \'Japan\', \'Japan\', \'Europe\', \'Europe\', \'USA\', \'Germany\', \'Japan\'] } df = pd.DataFrame(data) ``` **Required Plots with Specific Color Palettes:** 1. **Qualitative Palette:** Create a scatter plot showing the relationship between `horsepower` and `mpg`. Use a qualitative palette to distinguish between the different `origin` categories. 2. **Sequential Palette:** Create a hexbin plot to show the relationship between `horsepower` and `weight`. Use a sequential palette to represent the density of the data points. 3. **Diverging Palette:** Create a bar plot to show the average `mpg` for each `origin`. Use a diverging palette to highlight the difference between the highest and lowest average `mpg`. **Constraints:** 1. For the qualitative palette, use the `color_palette` function with the `Set2` palette. 2. For the sequential palette, use the `cubehelix_palette`. 3. For the diverging palette, use the `diverging_palette` function with the hues `220 and 20`. **Function Implementations:** Your solution should include the following steps: 1. Import necessary libraries. 2. Load the dataset. 3. Create the required plots using Seaborn and the specified color palettes. **Expected Output:** 1. A scatter plot displaying the relationship between `horsepower` and `mpg` with different colors for each `origin` category. 2. A hexbin plot showing the density of data points in the `horsepower` vs. `weight` relationship using a sequential palette. 3. A bar plot displaying the average `mpg` for each `origin`, highlighted using a diverging palette. **Performance Requirement:** Your solution should be well-organized, with plots that are easy to read and interpret. Make sure to label the axes and include a legend where applicable. **Starter Code:** ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd data = { \'mpg\': [18, 15, 36, 24, 17, 14, 23, 29, 32, 20], \'horsepower\': [130, 165, 80, 150, 140, 198, 95, 68, 155, 103], \'weight\': [3504, 3693, 2234, 3175, 3449, 4332, 2372, 2140, 2665, 2639], \'origin\': [\'USA\', \'USA\', \'Japan\', \'Japan\', \'Japan\', \'Europe\', \'Europe\', \'USA\', \'Germany\', \'Japan\'] } df = pd.DataFrame(data) # 1. Scatter plot with qualitative palette plt.figure(figsize=(10, 8)) scatter_plot = sns.scatterplot(x=\'horsepower\', y=\'mpg\', hue=\'origin\', palette=\'Set2\', data=df) scatter_plot.set_title(\'Horsepower vs. MPG (Colored by Origin)\') plt.show() # 2. Hexbin plot with sequential palette plt.figure(figsize=(10, 8)) sequential_palette = sns.color_palette(\\"cubehelix\\", as_cmap=True) hexbin_plot = plt.hexbin(df[\'horsepower\'], df[\'weight\'], gridsize=20, cmap=sequential_palette) plt.colorbar(hexbin_plot) plt.xlabel(\'Horsepower\') plt.ylabel(\'Weight\') plt.title(\'Horsepower vs. Weight with Sequential Palette\') plt.show() # 3. Bar plot with diverging palette plt.figure(figsize=(10, 8)) average_mpg = df.groupby(\'origin\')[\'mpg\'].mean().reset_index() diverging_palette = sns.diverging_palette(220, 20, as_cmap=True) bar_plot = sns.barplot(x=\'origin\', y=\'mpg\', palette=diverging_palette, data=average_mpg) bar_plot.set_title(\'Average MPG by Origin with Diverging Palette\') plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd data = { \'mpg\': [18, 15, 36, 24, 17, 14, 23, 29, 32, 20], \'horsepower\': [130, 165, 80, 150, 140, 198, 95, 68, 155, 103], \'weight\': [3504, 3693, 2234, 3175, 3449, 4332, 2372, 2140, 2665, 2639], \'origin\': [\'USA\', \'USA\', \'Japan\', \'Japan\', \'Japan\', \'Europe\', \'Europe\', \'USA\', \'Germany\', \'Japan\'] } df = pd.DataFrame(data) def plot_color_palette_visualizations(df): Generates the required color palette visualizations using Seaborn. # 1. Scatter plot with qualitative palette plt.figure(figsize=(10, 8)) scatter_plot = sns.scatterplot(x=\'horsepower\', y=\'mpg\', hue=\'origin\', palette=\'Set2\', data=df) scatter_plot.set_title(\'Horsepower vs. MPG (Colored by Origin)\') plt.xlabel(\'Horsepower\') plt.ylabel(\'MPG\') plt.legend(title=\'Origin\') plt.show() # 2. Hexbin plot with sequential palette plt.figure(figsize=(10, 8)) sequential_palette = sns.cubehelix_palette(as_cmap=True) hexbin_plot = plt.hexbin(df[\'horsepower\'], df[\'weight\'], gridsize=20, cmap=sequential_palette) plt.colorbar(hexbin_plot) plt.xlabel(\'Horsepower\') plt.ylabel(\'Weight\') plt.title(\'Horsepower vs. Weight with Sequential Palette\') plt.show() # 3. Bar plot with diverging palette plt.figure(figsize=(10, 8)) average_mpg = df.groupby(\'origin\')[\'mpg\'].mean().reset_index() diverging_palette = sns.diverging_palette(220, 20, as_cmap=False) bar_plot = sns.barplot(x=\'origin\', y=\'mpg\', palette=diverging_palette, data=average_mpg) bar_plot.set_title(\'Average MPG by Origin with Diverging Palette\') plt.xlabel(\'Origin\') plt.ylabel(\'Average MPG\') plt.show() plot_color_palette_visualizations(df)"},{"question":"**Email Handling in Python** # Problem Statement You are working on an email client application where you need to create an email message from scratch, parse a received email message, and extract certain components from it. Your task is to implement the following functions using the `email` package in Python. # Requirements 1. **Create Email**: Implement a function `create_email(sender, recipient, subject, body)` that constructs and returns an email message in `string` format. - **Input**: - `sender` (str): Email address of the sender. - `recipient` (str): Email address of the recipient. - `subject` (str): Subject of the email. - `body` (str): The main content of the email. - **Output**: - Returns the email as a `string`. 2. **Parse Email**: Implement a function `parse_email(raw_email)` that takes a raw email message in string format and parses it to extract the sender, recipient, subject, and body of the email. - **Input**: - `raw_email` (str): The raw email message as a string. - **Output**: - Returns a dictionary containing: - `sender` (str): Extracted sender\'s email address. - `recipient` (str): Extracted recipient\'s email address. - `subject` (str): Extracted subject of the email. - `body` (str): Extracted body content of the email. # Constraints - Ensure the email is properly formatted as a MIME message and adheres to standard email protocols. - Use appropriate methods and classes from the `email` package to achieve the above tasks. # Example ```python raw_email = create_email(\\"alice@example.com\\", \\"bob@example.com\\", \\"Meeting Update\\", \\"The meeting is rescheduled to 3 PM.\\") parsed_email = parse_email(raw_email) assert parsed_email == { \'sender\': \'alice@example.com\', \'recipient\': \'bob@example.com\', \'subject\': \'Meeting Update\', \'body\': \'The meeting is rescheduled to 3 PM.\' } ``` # Notes - You may assume that the input `raw_email` is always a valid email string. - Make sure to import necessary classes and functions from the `email` package.","solution":"from email.message import EmailMessage from email.parser import Parser def create_email(sender, recipient, subject, body): Constructs and returns an email message in string format. msg = EmailMessage() msg[\'From\'] = sender msg[\'To\'] = recipient msg[\'Subject\'] = subject msg.set_content(body) return msg.as_string() def parse_email(raw_email): Parses a raw email message string to extract the sender, recipient, subject, and body. parser = Parser() msg = parser.parsestr(raw_email) return { \'sender\': msg[\'From\'], \'recipient\': msg[\'To\'], \'subject\': msg[\'Subject\'], \'body\': msg.get_payload() }"},{"question":"Handling Missing Data with pandas **Problem Statement:** You are given a CSV file containing sales data with missing values, which you need to clean and analyze. Your task is to write a function `clean_sales_data(file_path)` that performs the following operations: 1. **Load the Data:** - Read the CSV file into a pandas DataFrame. The CSV file has the following columns: - `Order Date`: The date on which the order was placed. - `Product`: The name of the product. - `Quantity Ordered`: The quantity of the product ordered. - `Price Each`: The price of one unit of the product. 2. **Detect Missing Values:** - Identify and count the number of missing values in each column. 3. **Handle Missing Values:** - Drop any rows where `Order Date` is missing. - Fill missing values in the `Product` column with the string \\"Unknown Product\\". - Fill missing values in the `Quantity Ordered` and `Price Each` columns with 0. 4. **Analyze Cleaned Data:** - Calculate the total revenue for each product and return a DataFrame with columns `Product` and `Total Revenue`, sorted by `Total Revenue` in descending order. - Total revenue for a product is calculated as the sum of `(Quantity Ordered * Price Each)` for that product. **Function Signature:** ```python import pandas as pd def clean_sales_data(file_path: str) -> pd.DataFrame: pass ``` **Input:** - `file_path` (str): The file path to the CSV file containing the sales data. **Output:** - A pandas DataFrame with columns `Product` and `Total Revenue`, showing the total revenue for each product, sorted in descending order of `Total Revenue`. **Constraints:** - You should assume that the CSV file is guaranteed to have the columns specified but may have missing values. - Performance considerations should be taken into account, but no specific constraints are specified on the size of the data. **Example:** If the contents of the CSV file are as follows: ``` Order Date,Product,Quantity Ordered,Price Each 2023-01-01,Product A,2,20.0 2023-01-02,Product B,,15.0 ,,3,25.0 2023-01-04,Product C,5, 2023-01-05,,2,10.0 ``` After cleaning, the DataFrame should look something like this: ``` cleaned DataFrame: Order Date Product Quantity Ordered Price Each 2023-01-01 Product A 2 20.0 2023-01-02 Product B 0 15.0 2023-01-04 Product C 5 0.0 2023-01-05 Unknown Product 2 10.0 Output DataFrame: Product Total Revenue 0 Product A 40.0 1 Product B 0.0 2 Product C 0.0 3 Unknown Product 20.0 ``` You can use the provided information on handling missing data in the documentation to guide your implementation.","solution":"import pandas as pd def clean_sales_data(file_path: str) -> pd.DataFrame: # Step 1: Load the data df = pd.read_csv(file_path) # Step 2: Detect missing values missing_counts = df.isna().sum() print(\\"Missing values in each column:\\", missing_counts) # Step 3: Handle missing values df = df.dropna(subset=[\'Order Date\']) df[\'Product\'] = df[\'Product\'].fillna(\'Unknown Product\') df[\'Quantity Ordered\'] = df[\'Quantity Ordered\'].fillna(0).astype(int) df[\'Price Each\'] = df[\'Price Each\'].fillna(0.0).astype(float) # Step 4: Analyze cleaned data df[\'Revenue\'] = df[\'Quantity Ordered\'] * df[\'Price Each\'] revenue_df = df.groupby(\'Product\')[\'Revenue\'].sum().reset_index() revenue_df = revenue_df.rename(columns={\'Revenue\': \'Total Revenue\'}) revenue_df = revenue_df.sort_values(by=\'Total Revenue\', ascending=False).reset_index(drop=True) return revenue_df"},{"question":"Objective: To assess the comprehension of Python\'s \\"Cell\\" objects by implementing a set of functions that utilize these objects. Problem Statement: You are required to implement a Python class `CellManager` that manages a collection of cell objects and allows various operations on them. Requirements: 1. **Class Definition**: Define a class `CellManager` with the following methods: - `__init__(self)`: Initialize an empty list to store cell objects. - `create_cell(self, value)`: Create a new cell object containing `value` and add it to the list. Return the index of the newly added cell. - `get_cell_value(self, index)`: Retrieve the value stored in the cell object at the specified `index`. Raise `IndexError` if the index is out of bounds. - `set_cell_value(self, index, value)`: Set the value of the cell object at the specified `index` to `value`. Raise `IndexError` if the index is out of bounds. - `is_cell(self, item)`: Return `True` if the provided `item` is a cell object, otherwise return `False`. Input and Output Formats: - `create_cell(value)`: Expected input is a value of any type. Output is an integer index indicating the position of the newly created cell object in the list. - `get_cell_value(index)`: Expected input is an integer index. Output is the value stored in the cell object at the given index. - `set_cell_value(index, value)`: Expected input is an integer index and a value of any type. There is no output but the internal state is updated. - `is_cell(item)`: Expected input is any Python object. Output is a boolean indicating if the provided object is a cell object. Constraints: - The list of cell objects should not directly be accessible from outside the `CellManager` class. - Ensure to handle cases where indexes may be out of bounds by raising appropriate exceptions. Example: ```python # Create an instance of the CellManager cm = CellManager() # Create cells idx1 = cm.create_cell(10) idx2 = cm.create_cell(\\"Hello\\") # Get values from cells print(cm.get_cell_value(idx1)) # Output: 10 print(cm.get_cell_value(idx2)) # Output: Hello # Set values to cells cm.set_cell_value(idx1, 20) print(cm.get_cell_value(idx1)) # Output: 20 # Check if an object is a cell print(cm.is_cell(10)) # Output: False print(cm.is_cell(cm._cells[idx1])) # Output: True # internal access for testing # Handling out of bounds index try: cm.get_cell_value(5) except IndexError as e: print(str(e)) # Output: \'Index out of bounds\' ``` **Note**: You may need to simulate the cell object behavior since direct access to the C-API functions in a pure Python environment isn\'t possible.","solution":"import types class CellManager: def __init__(self): self._cells = [] def create_cell(self, value): # Create a cell object cell = self._make_cell(value) self._cells.append(cell) return len(self._cells) - 1 def get_cell_value(self, index): # Get the value stored in the cell object if index < 0 or index >= len(self._cells): raise IndexError(\'Index out of bounds\') return self._cells[index].cell_contents def set_cell_value(self, index, value): # Set the value of the cell object if index < 0 or index >= len(self._cells): raise IndexError(\'Index out of bounds\') self._cells[index] = self._make_cell(value) def is_cell(self, item): return isinstance(item, types.CellType) def _make_cell(self, value): # Workaround to create a cell in Python return (lambda x: lambda: x)(value).__closure__[0]"},{"question":"**Objective:** Implement functions to create certain probability distributions, sample data from them, and compute some statistical properties of the sampled data. **Requirements:** 1. Create two different instances of PyTorch distributions: - A **Normal** distribution with mean `mu` and standard deviation `sigma`. - A **Categorical** distribution defined by a list of probabilities. 2. Implement a function to sample data from these distributions: - Function `sample_from_distributions(normal_params, categorical_probs, sample_size)`: - `normal_params`: A tuple containing the mean (`mu`) and standard deviation (`sigma`) for the Normal distribution. - `categorical_probs`: A list indicating the probabilities for the Categorical distribution. - `sample_size`: An integer specifying the number of samples to draw from each distribution. - This function should return two tensors: - First tensor containing `sample_size` samples from the Normal distribution. - Second tensor containing `sample_size` samples from the Categorical distribution. 3. Implement a function to compute and return some basic statistics from the sampled data: - Function `compute_statistics(normal_samples, categorical_samples)`: - `normal_samples`: A tensor of samples drawn from a normal distribution. - `categorical_samples`: A tensor of samples drawn from a categorical distribution. - This function should: - Compute the mean and standard deviation of the normal samples. - Count the occurrences of each category in the categorical samples and return them in a dictionary where keys are categories and values are counts. # Constraints: - The parameters for the distributions and sample size are guaranteed to be valid such that the sum of `categorical_probs` equals 1, and `sigma` is positive. # Expected Input and Output: - **Input**: - `normal_params`: Tuple of two floats, e.g., `(0.0, 1.0)` - `categorical_probs`: List of floats, e.g., `[0.2, 0.5, 0.3]` - `sample_size`: Integer, e.g., `1000` - **Output**: - For `sample_from_distributions`: Two tensors of samples. - For `compute_statistics`: - Mean and standard deviation of the normal samples. - Dictionary counting each category in the categorical samples. **Performance Requirements:** Optimize the function to handle large sample sizes (in the order of tens of thousands) efficiently. Example Code: ```python def sample_from_distributions(normal_params, categorical_probs, sample_size): import torch from torch.distributions import Normal, Categorical # Unpack the normal parameters mu, sigma = normal_params # Create the Normal and Categorical distributions normal_dist = Normal(mu, sigma) categorical_dist = Categorical(torch.tensor(categorical_probs)) # Sample from the distributions normal_samples = normal_dist.sample((sample_size,)) categorical_samples = categorical_dist.sample((sample_size,)) return normal_samples, categorical_samples def compute_statistics(normal_samples, categorical_samples): import torch # Compute mean and standard deviation for normal samples normal_mean = normal_samples.mean().item() normal_std = normal_samples.std().item() # Count occurrences for categorical samples categories, counts = torch.unique(categorical_samples, return_counts=True) category_counts = dict(zip(categories.tolist(), counts.tolist())) return normal_mean, normal_std, category_counts # Example usage: normal_params = (0.0, 1.0) categorical_probs = [0.2, 0.5, 0.3] sample_size = 1000 normal_samples, categorical_samples = sample_from_distributions(normal_params, categorical_probs, sample_size) normal_mean, normal_std, category_counts = compute_statistics(normal_samples, categorical_samples) print(f\\"Normal Samples Mean: {normal_mean:.4f}, Std Dev: {normal_std:.4f}\\") print(\\"Categorical Counts: \\", category_counts) ``` Explanation: - The `sample_from_distributions` function samples data from the given normal and categorical distributions. - The `compute_statistics` function processes these samples to compute and return basic statistical properties.","solution":"import torch from torch.distributions import Normal, Categorical def sample_from_distributions(normal_params, categorical_probs, sample_size): Sample data from the specified normal and categorical distributions. Arguments: normal_params -- tuple containing the mean and standard deviation for the normal distribution. categorical_probs -- list of probabilities defining the categorical distribution. sample_size -- number of samples to draw from each distribution. Returns: Two tensors: one from the normal distribution, one from the categorical distribution. mu, sigma = normal_params normal_dist = Normal(mu, sigma) categorical_dist = Categorical(torch.tensor(categorical_probs)) normal_samples = normal_dist.sample((sample_size,)) categorical_samples = categorical_dist.sample((sample_size,)) return normal_samples, categorical_samples def compute_statistics(normal_samples, categorical_samples): Compute the mean and standard deviation of normal samples and the counts of categorical samples. Arguments: normal_samples -- tensor containing samples from a normal distribution. categorical_samples -- tensor containing samples from a categorical distribution. Returns: Mean and standard deviation of the normal samples, and a dictionary of counts for categorical samples. normal_mean = normal_samples.mean().item() normal_std = normal_samples.std().item() categories, counts = torch.unique(categorical_samples, return_counts=True) category_counts = dict(zip(categories.tolist(), counts.tolist())) return normal_mean, normal_std, category_counts"},{"question":"**Question: File Conversion and Exception Handling with binhex** You have been tasked with creating a utility that leverages the deprecated `binhex` module to encode and decode files. Your utility should include functions to encode a binary file to a binhex file, decode the binhex file back to its original binary form, and handle any specific exceptions that may arise. # Requirements: 1. Implement a function `encode_file_to_binhex(input_filename: str, output_filename: str) -> None` that: - Takes a binary file specified by `input_filename`. - Converts it to a binhex file specified by `output_filename`. - Handles and logs any `binhex.Error` exceptions that arise. 2. Implement a function `decode_binhex_to_file(input_filename: str, output_filename: Optional[str] = None) -> None` that: - Takes a binhex file specified by `input_filename`. - Decodes it to a binary file specified by `output_filename`. - If `output_filename` is not provided, the output filename should be read from the binhex file. - Handles and logs any `binhex.Error` exceptions that arise. 3. Provide clear logging for exceptions using the `logging` module. # Input: - The functions accept input and output filenames as strings. # Output: - The functions do not return any value. They read from and write to files as specified. # Constraints: - You may assume the input files exist and are in the correct format for conversion. - Your solution must handle and log exceptions gracefully. # Examples: ```python # Example of encoding a file try: encode_file_to_binhex(\\"example.bin\\", \\"encoded.bh\\") except binhex.Error as e: print(f\\"Encoding error: {e}\\") # Example of decoding a file try: decode_binhex_to_file(\\"encoded.bh\\", \\"decoded.bin\\") except binhex.Error as e: print(f\\"Decoding error: {e}\\") # If output filename is not provided try: decode_binhex_to_file(\\"encoded.bh\\") except binhex.Error as e: print(f\\"Decoding error: {e}\\") ``` # Implementation Hints: - Use `binhex.binhex` and `binhex.hexbin` for encoding and decoding respectively. - Use the `logging` module to log exceptions. ```python import binhex import logging def encode_file_to_binhex(input_filename: str, output_filename: str) -> None: try: binhex.binhex(input_filename, output_filename) except binhex.Error as e: logging.error(f\\"Encoding error: {e}\\") def decode_binhex_to_file(input_filename: str, output_filename: str = None) -> None: try: binhex.hexbin(input_filename, output_filename) except binhex.Error as e: logging.error(f\\"Decoding error: {e}\\") ```","solution":"import binhex import logging logging.basicConfig(level=logging.ERROR, format=\'%(asctime)s %(levelname)s %(message)s\') def encode_file_to_binhex(input_filename: str, output_filename: str) -> None: try: binhex.binhex(input_filename, output_filename) except binhex.Error as e: logging.error(f\\"Encoding error: {e}\\") def decode_binhex_to_file(input_filename: str, output_filename: str = None) -> None: try: binhex.hexbin(input_filename, output_filename) except binhex.Error as e: logging.error(f\\"Decoding error: {e}\\")"},{"question":"Objective: Evaluate students\' understanding of using the seaborn `so.Bar()` function to create and customize bar plots. Test the ability to utilize advanced seaborn functionalities like combining different plot types and handling overlapping elements. Question: Using the seaborn and matplotlib libraries, write a Python function `create_penguin_plot` that does the following: 1. Loads the \\"penguins\\" dataset in seaborn. 2. Creates a customized bar plot that: - Shows the average `body_mass_g` for each `species` grouped by `sex`. - Uses different colors for each `sex`. - Applies the `Dodge` transformation to handle overlapping bars. - Adds error bars representing the standard deviation of `body_mass_g`. - Uses distinct properties for each `sex` such as different edge styles and alpha values. Specification: - Input: None. - Output: The function should display the plot. There is no need for a return statement. Constraints: - You should use the `so.Plot` object from seaborn to construct the plot. - Utilize the functionalities such as `so.Bar()`, `so.Hist()`, `so.Dodge()`, `so.Range()`, and `so.Est()` to achieve the plot requirements. You may find it helpful to refer to seaborn\'s documentation on these functions. Example: ```python def create_penguin_plot(): import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot plot = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") .add(so.Bar(alpha=0.6, edgewidth=2), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) .on(x_ruler=\\"species\\", y_ruler=lambda x: (-500, x.max() + 500)) ) # Display the plot plot.show() ``` **Note:** Make sure your plot is well-labeled and includes a legend for clarity.","solution":"def create_penguin_plot(): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot plot = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") .add(so.Bar(alpha=0.6, edgewidth=2), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) .theme({ \'figure.dpi\': 100, \'axes.grid\': True, \'grid.color\': \'gray\', \'grid.linewidth\': 0.5, \'axes.edgecolor\': \'black\', \'axes.linewidth\': 1, \'legend.frameon\': True, }) ) # Display the plot plot.show() plt.show()"},{"question":"You have been provided with the internals of how Python instance method objects and method objects are managed. Using this knowledge, you need to implement a Python class that demonstrates the creation and use of these method objects. Requirements: 1. **Define a custom class called `MetaMethodClass`**: - The class should have an `__init__` method that initializes a function as an instance method. - The class should have methods to: - Add a method to the instance. - Retrieve the method associated with the instance. - Check if a given object is a method object. 2. **Implement the following functions inside the `MetaMethodClass`:** ```python class MetaMethodClass: def __init__(self, func: callable): Initializes the MetaMethodClass with a function that will be converted to an instance method. pass def add_method(self, func: callable): Adds a method (func) to the instance of the class. pass def get_method(self) -> callable: Retrieves the method bound to the instance. pass @staticmethod def is_method(obj: object) -> bool: Checks if the provided object is a method object. pass ``` - **Input and Output:** - `__init__(self, func: callable)`: Initializes the instance with a function that will be converted to an instance method. - `add_method(self, func: callable)`: Takes a callable object (function) and binds it to the class instance. - `get_method(self) -> callable`: Returns the callable object associated with the instance. - `is_method(obj: object) -> bool`: Returns `True` if the provided object is a method object, `False` otherwise. **Constraints:** - The provided functions must follow the rules of Python method and instance method objects. - You may not use any built-in or external libraries that defeat the purpose of using PyCFunctions (e.g., `types.MethodType`). - Performance requirements are not stringent, but the solution should demonstrate an understanding of the underlying principles. Example: ```python def example_function(): return \\"Hello, world!\\" meta_instance = MetaMethodClass(example_function) print(meta_instance.get_method()()) # Should output: \\"Hello, world!\\" def new_example_function(): return \\"New Function\\" meta_instance.add_method(new_example_function) print(meta_instance.get_method()()) # Should output: \\"New Function\\" print(MetaMethodClass.is_method(meta_instance.get_method())) # Should output: True ``` Develop the `MetaMethodClass` based on the documentation principles outlined.","solution":"class MetaMethodClass: def __init__(self, func: callable): Initializes the MetaMethodClass with a function that will be converted to an instance method. self.method = func def add_method(self, func: callable): Adds a method (func) to the instance of the class. self.method = func def get_method(self) -> callable: Retrieves the method bound to the instance. return self.method @staticmethod def is_method(obj: object) -> bool: Checks if the provided object is a method object. return hasattr(obj, \'__call__\')"},{"question":"Coding Assessment Question # Objective: Utilize the `asyncio` module to demonstrate understanding of event loop policies and child process watchers. # Problem Statement: Implement a custom event loop policy and a custom child watcher that are integrated into a single program to manage the execution and monitoring of multiple subprocesses. # Requirements: 1. Create a subclass of `asyncio.DefaultEventLoopPolicy`, named `CustomEventLoopPolicy`, which overrides the `get_event_loop()` method to include custom behavior (e.g., logging every time an event loop is accessed). 2. Implement a custom child process watcher, named `CustomChildWatcher`, subclassing from `asyncio.ThreadedChildWatcher`. This custom watcher should keep a log of all child processes and their states (started, terminated). 3. Integrate the custom event loop policy and the custom child watcher within an asyncio-based program. 4. The program should demonstrate: - Setting the custom event loop policy. - Starting multiple subprocesses using `asyncio.create_subprocess_exec()`. - Using the custom child watcher to monitor the subprocesses. # Input and Output: Input: - The program internally manages input by starting subprocesses. There are no inputs from the user. Output: - The program should output logs detailing every access to the event loop. - The program should output logs generated by the custom child watcher indicating the start and termination of subprocesses. # Constraints: - The solution must run on Unix-based systems (due to the use of child watchers). - The asynchronous subprocess handler must use at least three different subprocesses for demonstration. # Performance Requirements: - The solution should handle the creation and monitoring of subprocesses efficiently. - Logging operations should not significantly degrade the performance of the event loop. # Example: ```python import asyncio import os # Step 1: Custom Event Loop Policy class CustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() print(\\"Accessing event loop:\\", loop) return loop # Step 2: Custom Child Watcher class CustomChildWatcher(asyncio.ThreadedChildWatcher): def add_child_handler(self, pid, callback, *args): print(f\\"Started new subprocess with PID: {pid}\\") super().add_child_handler(pid, callback, *args) def remove_child_handler(self, pid): result = super().remove_child_handler(pid) if result: print(f\\"Subprocess with PID: {pid} terminated.\\") return result # Step 3: Demonstration Program async def main(): policy = CustomEventLoopPolicy() asyncio.set_event_loop_policy(policy) watcher = CustomChildWatcher() asyncio.get_event_loop_policy().set_child_watcher(watcher) loop = asyncio.get_event_loop() tasks = [] for i in range(3): task = loop.create_task(asyncio.create_subprocess_exec( \'python3\', \'-c\', f\'print(\\"Hello from subprocess {i}\\")\' )) tasks.append(task) await asyncio.gather(*tasks) asyncio.run(main()) ``` The provided program demonstrates creating a custom event loop policy and a child watcher, and using them to manage and log the lifecycle of subprocesses.","solution":"import asyncio import os # Step 1: Custom Event Loop Policy class CustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() print(\\"Accessing event loop:\\", loop) return loop # Step 2: Custom Child Watcher class CustomChildWatcher(asyncio.ThreadedChildWatcher): def add_child_handler(self, pid, callback, *args): print(f\\"Started new subprocess with PID: {pid}\\") super().add_child_handler(pid, callback, *args) def remove_child_handler(self, pid): result = super().remove_child_handler(pid) if result: print(f\\"Subprocess with PID: {pid} terminated.\\") return result # Step 3: Demonstration Program async def main(): policy = CustomEventLoopPolicy() asyncio.set_event_loop_policy(policy) watcher = CustomChildWatcher() asyncio.get_event_loop_policy().set_child_watcher(watcher) loop = asyncio.get_event_loop() tasks = [] for i in range(3): task = loop.create_task(asyncio.create_subprocess_exec( \'python3\', \'-c\', f\'print(\\"Hello from subprocess {i}\\")\' )) tasks.append(task) await asyncio.gather(*tasks) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"Objective: Implement a PyTorch function that converts a given PyTorch tensor to a specified data type using the operations provided in the Prims IR. This will test your ability to work with low-level operations and type conversions in PyTorch. Problem Statement: Write a function `convert_tensor_dtype(tensor: torch.Tensor, dtype: torch.dtype) -> torch.Tensor` that takes a PyTorch tensor as input and converts its data type to the specified dtype using the Prims IR operations. Input: - `tensor` (torch.Tensor): A PyTorch tensor which needs to be converted. - `dtype` (torch.dtype): The target data type to which the tensor needs to be converted. Output: - Returns a new PyTorch tensor with the data type converted to `dtype`. Constraints: - The function should utilize the Prims IR operation `prims.convert_element_type`. - You should handle the conversion using PyTorch version 2.0 or later. - Assume that `tensor` and `dtype` are always valid inputs. Example: ```python import torch # Example Tensor tensor = torch.tensor([1.5, 2.5, 3.5], dtype=torch.float32) # Convert dtype to int32 converted_tensor = convert_tensor_dtype(tensor, torch.int32) print(converted_tensor) # should output tensor([1, 2, 3], dtype=torch.int32) ``` Instructions: 1. Ensure that you have the required version of PyTorch installed. 2. Use the appropriate Prims IR operation to achieve the conversion. 3. Do not use high-level PyTorch functions like `tensor.to()`. 4. The solution should adhere to the specified constraints. Performance Requirements: The implemented solution should handle tensors of size up to `(10000, 10000)` efficiently.","solution":"import torch from torch._prims import convert_element_type def convert_tensor_dtype(tensor: torch.Tensor, dtype: torch.dtype) -> torch.Tensor: Converts the data type of a PyTorch tensor to the specified dtype using Prims IR operations. Args: tensor (torch.Tensor): The tensor to be converted. dtype (torch.dtype): The target data type. Returns: torch.Tensor: A new tensor with the data type converted to `dtype`. return convert_element_type(tensor, dtype)"},{"question":"# XML Document Manipulation and Traversal with `xml.dom` **Objective:** You are required to write a Python function utilizing the `xml.dom` package to parse an XML document, modify its content, and output the modified XML as a string. This task will assess your understanding of the DOM API, including creation, traversal, and modification of XML nodes. **Task:** 1. Parse the provided XML string into a DOM document. 2. Add a new element `<note>` as a child of the root element with the following attributes: - `date`: \\"2023-10-01\\" - `priority`: \\"high\\" 3. Within the `<note>` element, create and add the following child elements: - `<to>`: containing text \\"Alice\\" - `<from>`: containing text \\"Bob\\" - `<message>`: containing text \\"Meeting at 3 PM\\" 4. Change the text contained in the existing `<title>` element to \\"Updated Title\\". 5. Return the modified XML document as a string. **Function Signature:** ```python def modify_xml_document(xml_string: str) -> str: pass ``` **Input:** - `xml_string` (str): A string representing an XML document. This will have a pre-defined structure, containing at least the root element and possibly other elements like `<title>`. **Output:** - (str): A string representation of the modified XML document. **Constraints:** - The input XML string will always be well-formed. - The root element will always be present. - The `<title>` element, if present, will be unique. **Example:** ```python xml_input = <?xml version=\\"1.0\\"?> <document> <title>Original Title</title> </document> expected_output = <?xml version=\\"1.0\\"?> <document> <title>Updated Title</title> <note date=\\"2023-10-01\\" priority=\\"high\\"> <to>Alice</to> <from>Bob</from> <message>Meeting at 3 PM</message> </note> </document> assert modify_xml_document(xml_input) == expected_output ``` **Notes:** - Use the `xml.dom` package methods for parsing, creating, and modifying the XML nodes. - Ensure the modified XML is well-formed and maintains the structure expected. - Consider edge cases like missing or multiple `<title>` elements appropriately (though multiple `<title>` elements will not occur based on the constraint).","solution":"from xml.dom.minidom import parseString, Document def modify_xml_document(xml_string: str) -> str: # Parse the XML string into a DOM document dom = parseString(xml_string) # Find the root element root = dom.documentElement # Change the text contained in the existing <title> element title_elements = root.getElementsByTagName(\\"title\\") if title_elements: title_element = title_elements[0] if title_element.firstChild: title_element.firstChild.data = \\"Updated Title\\" # Create a new <note> element with the specified attributes note = dom.createElement(\\"note\\") note.setAttribute(\\"date\\", \\"2023-10-01\\") note.setAttribute(\\"priority\\", \\"high\\") # Create and add child elements to the <note> element to_child = dom.createElement(\\"to\\") to_child.appendChild(dom.createTextNode(\\"Alice\\")) note.appendChild(to_child) from_child = dom.createElement(\\"from\\") from_child.appendChild(dom.createTextNode(\\"Bob\\")) note.appendChild(from_child) message_child = dom.createElement(\\"message\\") message_child.appendChild(dom.createTextNode(\\"Meeting at 3 PM\\")) note.appendChild(message_child) # Append the <note> element to the root element root.appendChild(note) # Convert the modified DOM document back to an XML string return dom.toxml()"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of the `email.header` module by creating and manipulating MIME-compliant email headers that contain non-ASCII characters. # Task Create a function `process_email_header(header_values)` which performs the following: 1. **Create a `Header` object**: Initialize it with a string from the `header_values` list and the charset `utf-8`. 2. **Append strings to the header**: Append each subsequent string from the `header_values` list to the `Header` object with their corresponding charsets. 3. **Encode the header**: Use the `Header` class to encode the complete header into an RFC-compliant format. 4. **Decode the encoded header**: Use `decode_header` to decode the encoded header and return a list of `(decoded_string, charset)` pairs. 5. **Reassemble the header**: Use `make_header` to reassemble the decoded header parts into a new `Header` object, then encode this new header. # Detailed Requirements 1. The function receives a list of tuples `header_values`, where each tuple contains a string and its charset. ```python header_values = [ (\\"Hello\\", \\"utf-8\\"), (\\" \\", None), (\\"\\", \\"utf-8\\"), (\\"!\\", \\"us-ascii\\") ] ``` 2. For the initial `Header` object, use the first element of `header_values`. 3. For each subsequent tuple in `header_values`, append the string to the `Header` object with the given charset (or `None` if charset is not provided). 4. Encode the header using the `Header.encode()` method. 5. Decode the encoded header using the `decode_header()` function. 6. Reassemble the decoded parts using `make_header()` and encode the new header. 7. Return a dictionary with the following keys and values: - `\\"original_encoded\\"`: the original encoded header string. - `\\"decoded_parts\\"`: a list of tuples `(decoded_string, charset)` as returned by `decode_header`. - `\\"reassembled_encoded\\"`: the reassembled and encoded header string. # Input Format - `header_values`: A list of tuples, where each tuple contains a string and a charset (`str, str`). # Output Format - A dictionary with keys `\\"original_encoded\\"`, `\\"decoded_parts\\"`, and `\\"reassembled_encoded\\"` and their corresponding values. # Example ```python header_values = [ (\\"Hello\\", \\"utf-8\\"), (\\" \\", None), (\\"\\", \\"utf-8\\"), (\\"!\\", \\"us-ascii\\") ] result = process_email_header(header_values) print(result) ``` **Expected Output:** ```python { \\"original_encoded\\": \\"Hello =?utf-8?b?IHdvcmxkIQ==?=\\", # Example output \\"decoded_parts\\": [(b\'Hello\', \'utf-8\'), (b\' \', None), (b\'xe4xb8x96xe7x95x8c\', \'utf-8\'), (b\'!\', \'us-ascii\')], \\"reassembled_encoded\\": \\"Hello =?utf-8?b?IHdvcmxkIQ==?=\\" # Example output } ``` # Constraints - Ensure that the input is handled correctly whether the charset is specified or not. - Handle UnicodeErrors appropriately by following the `strict` error handling policy. # Notes - Refer to the documentation provided for the accurate implementation of the `Header`, `decode_header`, and `make_header` functionalities. - Pay attention to the encoding methods and how different character sets are handled to ensure the output conforms to RFC standards.","solution":"from email.header import Header, decode_header, make_header def process_email_header(header_values): Process the list of tuples to create, encode, decode, and reassemble an email header. Args: header_values (list): List of tuples where each tuple contains a string and a charset. Returns: dict: A dictionary with keys \\"original_encoded\\", \\"decoded_parts\\", and \\"reassembled_encoded\\". # Create the initial Header object initial_text, initial_charset = header_values[0] header = Header(initial_text, initial_charset) # Append each subsequent string to the Header object for text, charset in header_values[1:]: header.append(text, charset) # Encode the header original_encoded = header.encode() # Decode the encoded header decoded_parts = decode_header(original_encoded) # Reassemble the header from decoded parts reassembled_header = make_header(decoded_parts) reassembled_encoded = reassembled_header.encode() return { \\"original_encoded\\": original_encoded, \\"decoded_parts\\": decoded_parts, \\"reassembled_encoded\\": reassembled_encoded }"},{"question":"Objective You are required to implement a Python class that mimics certain behavior of the `PySequence` protocol described in the provided documentation. Your class should handle basic sequence operations such as getting an item, concatenation, repetition, and slicing. Class Requirements 1. **Class Name**: `CustomSequence` 2. It should initialize with a sequence (list or tuple). 3. Implement the following methods: - `__getitem__(self, index)`: Returns the item at the specified index. - `__setitem__(self, index, value)`: Sets the item at the specified index to the new value. - `__delitem__(self, index)`: Deletes the item at the specified index. - `__len__(self)`: Returns the length of the sequence. - `__contains__(self, value)`: Returns `True` if the value is in the sequence, else `False`. - `concat(self, other)`: Concatenates the current sequence with another sequence. - `repeat(self, count)`: Repeats the sequence `count` times. - `slice(self, start, end)`: Returns a slice of the sequence from `start` to `end`. Constraints - You should handle index errors gracefully, raising appropriate Python exceptions. - The sequence can be either a list or a tuple. Input and Output Formats - **Initialization Input**: A list or tuple. - **Methods**: - `__getitem__(self, index)`: `index` (int) -> Returns the item. - `__setitem__(self, index, value)`: `index` (int), `value` (any type) -> Modifies the sequence. - `__delitem__(self, index)`: `index` (int) -> Modifies the sequence. - `__len__(self)`: -> Returns the length (int). - `__contains__(self, value)`: `value` (any type) -> Returns a boolean. - `concat(self, other)`: `other` (list or tuple) -> Returns a new sequence. - `repeat(self, count)`: `count` (int) -> Returns a new sequence. - `slice(self, start, end)`: `start` (int), `end` (int) -> Returns a slice. Example Usage ```python # Initialize with a list seq = CustomSequence([1, 2, 3]) print(seq[1]) # Output: 2 # Set an item seq[1] = 99 print(seq[1]) # Output: 99 # Delete an item del seq[1] print(len(seq)) # Output: 2 # Check containment print(2 in seq) # Output: False # Concatenate sequences new_seq = seq.concat([4, 5]) print(new_seq) # Output: [1, 3, 4, 5] # Repeat sequence repeated_seq = seq.repeat(3) print(repeated_seq) # Output: [1, 3, 1, 3, 1, 3] # Get a slice slice_seq = seq.slice(0, 2) print(slice_seq) # Output: [1, 3] ``` Task Write the `CustomSequence` class following the specifications above.","solution":"class CustomSequence: def __init__(self, sequence): if not isinstance(sequence, (list, tuple)): raise TypeError(\\"Sequence must be a list or tuple.\\") self.sequence = list(sequence) def __getitem__(self, index): return self.sequence[index] def __setitem__(self, index, value): self.sequence[index] = value def __delitem__(self, index): del self.sequence[index] def __len__(self): return len(self.sequence) def __contains__(self, value): return value in self.sequence def concat(self, other): if not isinstance(other, (list, tuple)): raise TypeError(\\"Other sequence must be a list or tuple.\\") return self.sequence + list(other) def repeat(self, count): return self.sequence * count def slice(self, start, end): return self.sequence[start:end]"},{"question":"# Python Object Manipulation - Custom Attribute Management Objective: You are to implement a custom class in Python that manages its attributes in a specific way. The class should provide methods to check, get, set, and delete its own attributes, using logic mimicking the `PyObject` C functions described in the documentation. Requirements: Implement a class `CustomObject` with the following methods: 1. **`has_attr(self, attr_name: str) -> bool`**: - Checks if the attribute `attr_name` exists. - Returns `True` if it exists, `False` otherwise. 2. **`get_attr(self, attr_name: str)`**: - Retrieves the value of the attribute `attr_name`. - Raises `AttributeError` if the attribute does not exist. 3. **`set_attr(self, attr_name: str, value)`**: - Sets the attribute `attr_name` to the given `value`. - If `value` is `None`, deletes the attribute. 4. **`del_attr(self, attr_name: str)`**: - Deletes the attribute `attr_name`. - Raises `AttributeError` if the attribute does not exist. Input and Output: - All methods are instance methods operating on the object\'s attributes. - Attribute names are always strings. - Attributes can hold any value (int, string, list, object, etc.). Constraints: - Do not use Python\'s built-in `getattr`, `setattr`, or `hasattr` functions directly. - Emulate the functionality as described in the `PyObject` C API documentation. Example Usage: ```python obj = CustomObject() obj.set_attr(\'name\', \'John\') print(obj.has_attr(\'name\')) # Output: True print(obj.get_attr(\'name\')) # Output: \'John\' obj.set_attr(\'name\', None) print(obj.has_attr(\'name\')) # Output: False obj.del_attr(\'age\') # Raises AttributeError ``` # Implementation: Implement the `CustomObject` class below: ```python class CustomObject: def __init__(self): self.__dict__ = {} def has_attr(self, attr_name: str) -> bool: return attr_name in self.__dict__ def get_attr(self, attr_name: str): if self.has_attr(attr_name): return self.__dict__[attr_name] raise AttributeError(f\\"\'{type(self).__name__}\' object has no attribute \'{attr_name}\'\\") def set_attr(self, attr_name: str, value): if value is None: if self.has_attr(attr_name): del self.__dict__[attr_name] else: self.__dict__[attr_name] = value def del_attr(self, attr_name: str): if self.has_attr(attr_name): del self.__dict__[attr_name] else: raise AttributeError(f\\"\'{type(self).__name__}\' object has no attribute \'{attr_name}\'\\") ```","solution":"class CustomObject: def __init__(self): self._attributes = {} def has_attr(self, attr_name: str) -> bool: return attr_name in self._attributes def get_attr(self, attr_name: str): if self.has_attr(attr_name): return self._attributes[attr_name] raise AttributeError(f\\"\'{type(self).__name__}\' object has no attribute \'{attr_name}\'\\") def set_attr(self, attr_name: str, value): if value is None: if self.has_attr(attr_name): del self._attributes[attr_name] else: self._attributes[attr_name] = value def del_attr(self, attr_name: str): if self.has_attr(attr_name): del self._attributes[attr_name] else: raise AttributeError(f\\"\'{type(self).__name__}\' object has no attribute \'{attr_name}\'\\")"},{"question":"# Function Implementation with Python\'s Typing Module You are tasked with designing a function that processes a collection of data about employees and generates an organized output based on type hinting in Python. Given the diversity in roles and data associated with employees, your application must handle different types accurately and robustly. Your task includes the following steps: 1. **Define a `NamedTuple` for an Employee**: - The named tuple should contain fields: `name` (str), `id` (int), and `role` (str). 2. **Create a `TypedDict` for Departments**: - Each department is represented by a dictionary where keys are strings (department names) and values are lists of employees who work in that department. 3. **Write a function `organize_employees`**: - This function takes in a `TypedDict` where the keys are department names (str) and the values are lists of employees (List[Employee]). - The function should returning the same `TypedDict` but without any employees in the \\"Inactive\\" role. Active employees are roles other than \\"Inactive\\". 4. **Write a function `employee_summary`**: - This function takes in a list of employees and returns a string summary listing each employee\'s name and role. **Function Signatures**: ```python from typing import NamedTuple, TypedDict, List, Dict class Employee(NamedTuple): name: str id: int role: str class DepartmentDict(TypedDict): <other department keys>: List[Employee] def organize_employees(department_dict: DepartmentDict) -> DepartmentDict: pass def employee_summary(employees: List[Employee]) -> str: pass ``` **Constraints**: - Each department can have zero or more employees. - All employees in the \\"Inactive\\" role should be omitted in the output of `organize_employees`. - The `employee_summary` function should handle an empty list gracefully and return an appropriate message. **Example Usage**: ```python # Example `Employee` objects employee1 = Employee(name=\\"Alice\\", id=1, role=\\"Engineer\\") employee2 = Employee(name=\\"Bob\\", id=2, role=\\"Manager\\") employee3 = Employee(name=\\"Eve\\", id=3, role=\\"Inactive\\") # Example `DepartmentDict` departments = { \\"Engineering\\": [employee1], \\"Management\\": [employee2, employee3] } # Example usage of `organize_employees` organized_departments = organize_employees(departments) # organized_departments should be: # { # \\"Engineering\\": [employee1], # \\"Management\\": [employee2] # } # Example usage of `employee_summary` summary = employee_summary([employee1, employee2]) # summary should be: # \\"Employee Alice (Engineer), Employee Bob (Manager)\\" ``` Note: Remember to use the `typing` module constructs to properly annotate your function definitions.","solution":"from typing import NamedTuple, TypedDict, List, Dict class Employee(NamedTuple): name: str id: int role: str class DepartmentDict(TypedDict): Engineering: List[Employee] Management: List[Employee] # Include other departments as required def organize_employees(department_dict: DepartmentDict) -> DepartmentDict: organized_dict = {} for department, employees in department_dict.items(): organized_dict[department] = [employee for employee in employees if employee.role != \\"Inactive\\"] return organized_dict def employee_summary(employees: List[Employee]) -> str: if not employees: return \\"No active employees\\" summary_list = [f\\"Employee {employee.name} ({employee.role})\\" for employee in employees] return \\", \\".join(summary_list)"},{"question":"**Objective:** Demonstrate your understanding of seaborn figure aesthetics by creating a plot with various customizations. **Problem Statement:** You are tasked with creating a series of subplots demonstrating the use of different seaborn themes, spine removal, and scaled contexts. Write a function `custom_seaborn_plots()` that generates a 2x2 grid of plots, each with unique stylistic elements. **Function Signature:** ```python def custom_seaborn_plots(): pass ``` **Instructions:** 1. Create a 2x2 grid of subplots. 2. Each subplot should use a different seaborn theme: - Top-left: `darkgrid` - Top-right: `whitegrid` - Bottom-left: `dark` with top and right spines removed - Bottom-right: `ticks` with spines removed and scaled for a \\"talk\\" context 3. For each subplot, use the `sinplot()` function provided in the documentation (generate a plot with 6 sine waves for visualization). 4. The bottom-right subplot should have increased font size and linewidth using `sns.set_context()`. 5. Ensure each subplot is uniquely identifiable with appropriate titles. 6. At the end of the function, save the figure as \'custom_seaborn_plots.png\'. **Example:** ```python def custom_seaborn_plots(): import numpy as np import seaborn as sns import matplotlib.pyplot as plt def sinplot(n=10, flip=1): x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip) sns.set_theme() f, axs = plt.subplots(2, 2, figsize=(10, 10)) # Top-left: darkgrid with sns.axes_style(\\"darkgrid\\"): axs[0, 0].set_title(\\"darkgrid\\") plt.sca(axs[0, 0]) sinplot(6) # Top-right: whitegrid with sns.axes_style(\\"whitegrid\\"): axs[0, 1].set_title(\\"whitegrid\\") plt.sca(axs[0, 1]) sinplot(6) # Bottom-left: dark with spines removed with sns.axes_style(\\"dark\\"): sns.despine() axs[1, 0].set_title(\\"dark with spines removed\\") plt.sca(axs[1, 0]) sinplot(6) # Bottom-right: ticks + scaled context sns.set_context(\\"talk\\") with sns.axes_style(\\"ticks\\"): sns.despine() axs[1, 1].set_title(\\"ticks + talk context\\") plt.sca(axs[1, 1]) sinplot(6) f.tight_layout() plt.savefig(\'custom_seaborn_plots.png\') plt.show() ``` **Notes:** - Ensure that the plots are visually distinct and adhere to the styling rules specified. - Handle any necessary import statements within the function. - Ensure the function does not return any value but saves the plot to \'custom_seaborn_plots.png\'.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def custom_seaborn_plots(): def sinplot(n=10, flip=1): x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip) sns.set_theme() f, axs = plt.subplots(2, 2, figsize=(10, 10)) # Top-left: darkgrid with sns.axes_style(\\"darkgrid\\"): axs[0, 0].set_title(\\"darkgrid\\") plt.sca(axs[0, 0]) sinplot(6) # Top-right: whitegrid with sns.axes_style(\\"whitegrid\\"): axs[0, 1].set_title(\\"whitegrid\\") plt.sca(axs[0, 1]) sinplot(6) # Bottom-left: dark with spines removed with sns.axes_style(\\"dark\\"): sns.despine(ax=axs[1, 0], top=True, right=True) axs[1, 0].set_title(\\"dark with spines removed\\") plt.sca(axs[1, 0]) sinplot(6) # Bottom-right: ticks + scaled context sns.set_context(\\"talk\\") with sns.axes_style(\\"ticks\\"): sns.despine(ax=axs[1, 1]) axs[1, 1].set_title(\\"ticks + talk context\\") plt.sca(axs[1, 1]) sinplot(6) f.tight_layout() plt.savefig(\'custom_seaborn_plots.png\') plt.show()"},{"question":"Objective In this assessment, you will demonstrate your understanding of the scikit-learn preprocessing transformers: `LabelBinarizer`, `MultiLabelBinarizer`, and `LabelEncoder`. You will write functions to process labels using these transformers and combine the results with some additional manipulations. Problem Statement Write three functions: `binarize_labels`, `binarize_multilabels`, and `normalize_labels` as specified below: 1. **Function: `binarize_labels`** - **Input**: A list of integer labels. - **Output**: A dictionary containing: - `\\"classes\\"`: a sorted array of unique labels. - `\\"indicator_matrix\\"`: a 2D array (label indicator matrix) created using `LabelBinarizer`. - **Constraints**: The input list will contain at least one label and the labels will be positive integers. 2. **Function: `binarize_multilabels`** - **Input**: A list of lists, where each inner list contains multiple integer labels. - **Output**: A 2D binary indicator array created using `MultiLabelBinarizer`. - **Constraints**: Each inner list will contain at least one label and the labels will be non-negative integers. 3. **Function: `normalize_labels`** - **Input**: A list of hashable and comparable labels (can be integers, strings, etc.). - **Output**: A dictionary containing: - `\\"encoded_labels\\"`: a list of integers representing the encoded labels. - `\\"inverse_labels\\"`: a list of original labels obtained after inverse transforming the encoded labels. - **Constraints**: The input list will contain at least one label. Function Signatures ```python def binarize_labels(labels: List[int]) -> Dict[str, np.ndarray]: pass def binarize_multilabels(multilabels: List[List[int]]) -> np.ndarray: pass def normalize_labels(labels: List[Union[int, str]]) -> Dict[str, List[Union[int, str]]]: pass ``` Examples 1. **binarize_labels** ```python labels = [1, 2, 6, 4, 2] result = binarize_labels(labels) # Expected Output: # { # \\"classes\\": array([1, 2, 4, 6]), # \\"indicator_matrix\\": array([[1, 0, 0, 0], # [0, 1, 0, 0], # [0, 0, 0, 1], # [0, 0, 1, 0], # [0, 1, 0, 0]]) # } ``` 2. **binarize_multilabels** ```python multilabels = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]] result = binarize_multilabels(multilabels) # Expected Output: # array([[0, 0, 1, 1, 1], # [0, 0, 1, 0, 0], # [1, 1, 0, 1, 0], # [1, 1, 1, 1, 1], # [1, 1, 1, 0, 0]]) ``` 3. **normalize_labels** ```python labels = [\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"] result = normalize_labels(labels) # Expected Output: # { # \\"encoded_labels\\": [1, 1, 2, 0], # \\"inverse_labels\\": [\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"] # } ``` # Notes - Import necessary modules from `sklearn` and `numpy` as needed. - Ensure that the functions handle the constraints and the expected input-output formats correctly.","solution":"from typing import List, Dict, Union import numpy as np from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder def binarize_labels(labels: List[int]) -> Dict[str, np.ndarray]: labels_array = np.array(labels) unique_labels = np.unique(labels_array) lb = LabelBinarizer() indicator_matrix = lb.fit_transform(labels_array) return { \\"classes\\": unique_labels, \\"indicator_matrix\\": indicator_matrix } def binarize_multilabels(multilabels: List[List[int]]) -> np.ndarray: mlb = MultiLabelBinarizer() indicator_matrix = mlb.fit_transform(multilabels) return indicator_matrix def normalize_labels(labels: List[Union[int, str]]) -> Dict[str, List[Union[int, str]]]: le = LabelEncoder() encoded_labels = le.fit_transform(labels) inverse_labels = le.inverse_transform(encoded_labels) return { \\"encoded_labels\\": encoded_labels.tolist(), \\"inverse_labels\\": inverse_labels.tolist() }"},{"question":"You have been given a list of transactions. Each transaction is a dictionary containing four keys: `id`, `amount`, `type`, and `category`. ```python transactions = [ {\'id\': 1, \'amount\': 150, \'type\': \'debit\', \'category\': \'grocery\'}, {\'id\': 2, \'amount\': 200, \'type\': \'credit\', \'category\': \'salary\'}, {\'id\': 3, \'amount\': 50, \'type\': \'debit\', \'category\': \'entertainment\'}, {\'id\': 4, \'amount\': 100, \'type\': \'debit\', \'category\': \'grocery\'}, {\'id\': 5, \'amount\': 500, \'type\': \'credit\', \'category\': \'investment\'}, # more transactions... ] ``` Your task is to implement a function `process_transactions(transactions)` that processes the list of transactions to achieve the following: 1. **Categorize Transactions**: Use `itertools.groupby` to categorize transactions by their category. Create a dictionary where the keys are the categories and the values are lists of transactions corresponding to each category. 2. **Filter and Sort**: Within each category, filter out all the transactions of type \'credit\' and sort the remaining transactions by their amount in ascending order. 3. **Calculate Totals**: Use `functools` and `operator` to calculate and return the sum of the amounts for each category, based only on the filtered transactions of type \'debit\'. Your function should return a dictionary where the keys are the categories and the values are the sum of the amounts for the debit transactions in that category. **Constraints:** - You may assume all transaction amounts are positive integers. - The input list may contain from 1 to 1000 transactions. - The transaction categories will be non-empty strings. # Example: ```python example_transactions = [ {\'id\': 1, \'amount\': 150, \'type\': \'debit\', \'category\': \'grocery\'}, {\'id\': 2, \'amount\': 200, \'type\': \'credit\', \'category\': \'salary\'}, {\'id\': 3, \'amount\': 50, \'type\': \'debit\', \'category\': \'entertainment\'}, {\'id\': 4, \'amount\': 100, \'type\': \'debit\', \'category\': \'grocery\'}, {\'id\': 5, \'amount\': 500, \'type\': \'credit\', \'category\': \'investment\'}, ] print(process_transactions(example_transactions)) # Output: # {\'grocery\': 250, \'entertainment\': 50} ``` # Implementation Tips: - Utilize `itertools.groupby` to group transactions by their category. - Remember to sort the transactions first by category before using `groupby`. - Use `functools.partial` and `operator.add` to create a function for summing the debit transactions. Good luck, and happy coding!","solution":"from itertools import groupby from operator import itemgetter from functools import reduce import operator def process_transactions(transactions): # Sort the transactions by \'category\' transactions.sort(key=itemgetter(\'category\')) # Group the transactions by \'category\' grouped = groupby(transactions, key=itemgetter(\'category\')) result = {} # Process each group for category, items in grouped: # Filter out \'credit\' transactions and sort by \'amount\' debits = sorted([item for item in items if item[\'type\'] == \'debit\'], key=itemgetter(\'amount\')) # Calculate the total amount for debits total_amount = reduce(operator.add, [item[\'amount\'] for item in debits], 0) # Store the result if debits: # Only add to the result if there are debit transactions result[category] = total_amount return result"},{"question":"# Custom Descriptor for Validating Attributes Objective: Design and implement a custom descriptor class to manage and validate attributes within a class. Your implementation should enforce specific constraints on the attribute values. Requirements: 1. Implement a `Validator` class that: - Ensures an attribute value meets specified criteria before setting it. - Logs each attribute access, update, and deletion. 2. Create at least two concrete validator subclasses: - `RangeValidator`: Validates that a numerical value falls within a specified range. - `StringValidator`: Validates that a string meets specific length requirements. 3. Use these validators in a sample class to manage its attributes. Details: 1. **Validator Class**: - Should have methods for `__get__`, `__set__`, `__delete__`, and an abstract `validate` method to be implemented by subclasses. - Use the `logging` module to log each access (`__get__`), update (`__set__`), and delete (`__delete__`). - The `validate` method should raise a `ValueError` if the provided value does not meet the required conditions. 2. **RangeValidator**: - Inherits from `Validator`. - Validates that a provided numerical value falls within a specified minimum and maximum range. 3. **StringValidator**: - Inherits from `Validator`. - Validates that a provided string value meets a minimum and maximum length. 4. **Sample Class**: - Create a class `Product` with the following attributes: - `price` (validated by `RangeValidator` to be between 0 and 1000). - `name` (validated by `StringValidator` to have a minimum length of 3 and a maximum length of 30). Input: - Implement the `Validator`, `RangeValidator`, and `StringValidator` classes. - Create a `Product` class with `price` and `name` attributes using the validators. Output: - Demonstrate the functionality with example attribute assignments and access in an interactive session or main function. Constraints: - Use the `logging` module to log as initially specified. - Raise appropriate exceptions for validation failures. Performance: - Ensure that the `__get__`, `__set__`, and `__delete__` methods perform efficiently within O(1) time complexity. # Example Code ```python import logging logging.basicConfig(level=logging.INFO) class Validator: def __set_name__(self, owner, name): self.public_name = name self.private_name = \'_\' + name def __get__(self, obj, objtype=None): value = getattr(obj, self.private_name) logging.info(f\'Accessing {self.public_name!r} giving {value!r}\') return value def __set__(self, obj, value): self.validate(value) logging.info(f\'Updating {self.public_name!r} to {value!r}\') setattr(obj, self.private_name, value) def __delete__(self, obj): logging.info(f\'Deleting {self.public_name!r}\') delattr(obj, self.private_name) def validate(self, value): raise NotImplementedError(\'Validator subclasses must implement validate()\') class RangeValidator(Validator): def __init__(self, minval, maxval): self.minval = minval self.maxval = maxval def validate(self, value): if not (self.minval <= value <= self.maxval): raise ValueError(f\\"{value} is not between {self.minval} and {self.maxval}\\") class StringValidator(Validator): def __init__(self, minlen, maxlen): self.minlen = minlen self.maxlen = maxlen def validate(self, value): if not (self.minlen <= len(value) <= self.maxlen): raise ValueError(f\\"String length of \'{value}\' is not between {self.minlen} and {self.maxlen}\\") class Product: price = RangeValidator(0, 1000) name = StringValidator(3, 30) def __init__(self, name, price): self.name = name self.price = price # Example usage: if __name__ == \\"__main__\\": try: item = Product(\\"Laptop\\", 750) print(item.name) item.price = 500 item.price = 1500 # Should raise an error except Exception as e: print(e) try: invalid_item = Product(\\"X\\", 100) # Should raise an error except Exception as e: print(e) ``` Explanation: The `Validator` class manages attribute access with hooks for validation and logging, which are then extended by the `RangeValidator` and `StringValidator` classes to impose specific constraints. The `Product` class uses these validators to control the values of its `price` and `name` attributes.","solution":"import logging logging.basicConfig(level=logging.INFO) class Validator: def __set_name__(self, owner, name): self.public_name = name self.private_name = \'_\' + name def __get__(self, obj, objtype=None): value = getattr(obj, self.private_name) logging.info(f\'Accessing {self.public_name!r} giving {value!r}\') return value def __set__(self, obj, value): self.validate(value) logging.info(f\'Updating {self.public_name!r} to {value!r}\') setattr(obj, self.private_name, value) def __delete__(self, obj): logging.info(f\'Deleting {self.public_name!r}\') delattr(obj, self.private_name) def validate(self, value): raise NotImplementedError(\'Validator subclasses must implement validate()\') class RangeValidator(Validator): def __init__(self, minval, maxval): self.minval = minval self.maxval = maxval def validate(self, value): if not (self.minval <= value <= self.maxval): raise ValueError(f\\"{value} is not between {self.minval} and {self.maxval}\\") class StringValidator(Validator): def __init__(self, minlen, maxlen): self.minlen = minlen self.maxlen = maxlen def validate(self, value): if not (self.minlen <= len(value) <= self.maxlen): raise ValueError(f\\"String length of \'{value}\' is not between {self.minlen} and {self.maxlen}\\") class Product: price = RangeValidator(0, 1000) name = StringValidator(3, 30) def __init__(self, name, price): self.name = name self.price = price"},{"question":"# XML Document Parsing and Modification with ElementTree **Objective**: Assess your understanding of parsing, modifying, and querying XML documents using the `xml.etree.ElementTree` module in Python. **Problem Statement**: You are given an XML document representing a simple catalog of books. Each book has a title, author, genre, price, and publish date. Your task is to write functions that can parse this XML, perform modifications, and query the XML content based on certain conditions. # Tasks: 1. **Parsing the XML Document** Write a function `parse_xml(xml_string: str) -> ET.ElementTree` that takes an XML string and returns an `ElementTree` object. ```python def parse_xml(xml_string: str) -> ET.ElementTree: # Your code here pass ``` 2. **Modifying the XML Document** Write a function `add_book(root: ET.ElementTree, book_info: dict) -> None` that takes the root of the `ElementTree` and a dictionary containing book information (keys: title, author, genre, price, publish_date) and adds a new book element to the catalog. ```python def add_book(root: ET.ElementTree, book_info: dict) -> None: # Your code here pass ``` 3. **Querying the XML Document** Write a function `get_books_by_genre(root: ET.ElementTree, genre: str) -> List[ET.Element]` that returns a list of book elements that belong to the specified genre. ```python def get_books_by_genre(root: ET.ElementTree, genre: str) -> List[ET.Element]: # Your code here pass ``` # Input and Output: - `parse_xml(xml_string: str) -> ET.ElementTree` - **Input:** A string representing the XML document. - **Output:** An ElementTree object. - `add_book(root: ET.ElementTree, book_info: dict) -> None` - **Input:** - `root`: The root of the `ElementTree` - `book_info`: A dictionary with keys `title`, `author`, `genre`, `price`, and `publish_date`. - **Output:** None (The function should modify the XML structure in-place) - `get_books_by_genre(root: ET.ElementTree, genre: str) -> List[ET.Element]` - **Input:** - `root`: The root of the `ElementTree` - `genre`: The genre to filter books by. - **Output:** A list of book elements that match the specified genre. # Example XML: ```xml <?xml version=\\"1.0\\"?> <catalog> <book> <title>XML Developer\'s Guide</title> <author>Gambardella, Matthew</author> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> </book> <book> <title>Midnight Rain</title> <author>Ralls, Kim</author> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> </book> </catalog> ``` # Constraints: 1. The XML document is always well-formed. 2. The dictionary keys for `add_book` function are always valid and contain non-empty strings. 3. The genre for the `get_books_by_genre` function is always a valid string. # Hints: - Use the `ElementTree.fromstring()` method to parse the XML string. - Use the `ElementTree.SubElement()` method to add new elements to the tree. - Use the `.findall()` method with appropriate XPath expressions to query the XML tree.","solution":"import xml.etree.ElementTree as ET from typing import List def parse_xml(xml_string: str) -> ET.ElementTree: Parses the XML string and returns an ElementTree object. return ET.ElementTree(ET.fromstring(xml_string)) def add_book(root: ET.ElementTree, book_info: dict) -> None: Adds a new book element to the XML root with the provided book information. catalog = root.getroot() new_book = ET.SubElement(catalog, \\"book\\") for key, value in book_info.items(): element = ET.SubElement(new_book, key) element.text = str(value) def get_books_by_genre(root: ET.ElementTree, genre: str) -> List[ET.Element]: Returns a list of book elements that belong to the specified genre. catalog = root.getroot() return catalog.findall(f\\".//book[genre=\'{genre}\']\\")"},{"question":"# Python Input Handler You are required to implement a utility in Python to handle different forms of Python input as specified in the provided documentation. Your utility should be able to read and execute Python code from: 1. A file. 2. A string representing a complete program. 3. Interactive mode input. 4. Expressions provided to the `eval` function. # Requirements: 1. **Function:** `execute_python_input(input_type: str, input_value: str) -> Any` 2. **Parameters:** - `input_type`: A string indicating the type of Python input. It can have one of the following values: `\\"file\\"`, `\\"complete_program\\"`, `\\"interactive\\"`, `\\"expression\\"`. - `input_value`: A string representing the actual code or the filename containing the code to be executed. 3. **Returns:** The output of the executed code. When handling an `\\"expression\\"`, it must return the result of the `eval` function. For other input types, it can return `None` (or appropriate results based on the input). 4. **Constraints:** - Ensure the input is processed according to the rules specified (e.g., handling new lines, recognizing the end of input for interactive mode, etc.). - You may assume the input is syntactically correct. # Example Usage: ```python def execute_python_input(input_type: str, input_value: str) -> Any: if input_type == \\"file\\": with open(input_value, \'r\') as file: code = file.read() exec(code) elif input_type == \\"complete_program\\": exec(input_value) elif input_type == \\"interactive\\": statements = input_value.splitlines() for stmt in statements: exec(stmt) elif input_type == \\"expression\\": return eval(input_value) else: raise ValueError(\\"Invalid input type\\") # Example test cases print(execute_python_input(\\"expression\\", \\"2 + 2\\")) # Should output 4 # Assuming \'my_script.py\' contains valid Python code execute_python_input(\\"file\\", \\"my_script.py\\") # Executes the code in the file complete_program_code = a = 10 b = 20 print(a + b) execute_python_input(\\"complete_program\\", complete_program_code) # Should print 30 interactive_code = a = 5 b = 15 print(a + b) execute_python_input(\\"interactive\\", interactive_code) # Should print 20 ``` Implement the function `execute_python_input` to handle the described functionality.","solution":"def execute_python_input(input_type: str, input_value: str): if input_type == \\"file\\": with open(input_value, \'r\') as file: code = file.read() exec(code) elif input_type == \\"complete_program\\": exec(input_value) elif input_type == \\"interactive\\": statements = input_value.splitlines() for stmt in statements: exec(stmt) elif input_type == \\"expression\\": return eval(input_value) else: raise ValueError(\\"Invalid input type\\")"},{"question":"<|Analysis Begin|> The provided documentation is a comprehensive tutorial for the Python programming language. It covers a wide array of topics including basic syntax, control flow, data structures, modules, input/output, error handling, classes, and more. This document is an excellent resource to design a coding assessment question that tests students\' understanding of fundamental and advanced Python concepts. Key areas that can be leveraged to create a challenging and comprehensive coding question include: 1. **Control Flow Tools**: Understanding \\"if\\", \\"for\\", \\"while\\", \\"match\\" statements, and how to handle loops with \\"break\\" and \\"continue\\". 2. **Functions**: Writing functions with various types of arguments, lambda expressions, and function annotations. 3. **Data Structures**: Utilizing lists, dictionaries, sets, and tuples efficiently. 4. **Object-Oriented Programming**: Defining and using classes, inheritance, and dealing with different types of variables and methods. 5. **Iterators and Generators**: Implementing custom iterators and generators. 6. **File Handling**: Reading from and writing to files, formatted output. 7. **Exception Handling**: Properly raising and handling exceptions. 8. **Modules and Packages**: Understanding and using modules and packages to organize code. Given the depth covered in the documentation, a suitable problem could incorporate various aspects such as data structures, control flow, and object-oriented programming. <|Analysis End|> <|Question Begin|> You are tasked with creating a small library management system. This management system should handle books and members who borrow books. Specifically, you need to implement the following functionalities: 1. **Book Class**: Define a `Book` class with the following attributes: - `title`: A string representing the title of the book. - `author`: A string representing the author of the book. - `isbn`: A unique identifier string for the book. - `available`: A boolean indicating if the book is available for borrowing (default is `True`). 2. **Member Class**: Define a `Member` class with the following attributes: - `member_id`: A string representing the unique identifier for the member. - `name`: A string representing the member\'s name. - `borrowed_books`: A list to store the `isbn` of books that the member has borrowed. 3. **Library Class**: Define a `Library` class to manage the collection of books and members. This class should have the following methods: - `add_book(title: str, author: str, isbn: str)`: Adds a new book to the library. - `remove_book(isbn: str)`: Removes a book from the library by its ISBN. - `add_member(member_id: str, name: str)`: Adds a new member to the library. - `remove_member(member_id: str)`: Removes a member from the library. - `borrow_book(member_id: str, isbn: str)`: Allows a member to borrow a book if it is available. - `return_book(member_id: str, isbn: str)`: Processes the return of a book by a member. - `list_books()`: Lists all the books with their titles, authors, ISBNs, and availability. - `list_members()`: Lists all the members with their IDs and names. **Constraints:** - No two books should have the same ISBN. - Each member can borrow a maximum of 5 books. - Handle exceptions such as trying to borrow a non-existent book, returning a book not borrowed by the member, etc. **Input/Output:** - Implement the classes with methods as specified. There should be no direct input/output operations in your classes (e.g., `input()` or `print()`). Instead, focus on methods and their return values or object states. ```python class Book: # Implement the Book class class Member: # Implement the Member class class Library: # Implement the Library class ``` You are expected to write a fully working implementation of the above classes with all methods functioning as described.","solution":"class Book: def __init__(self, title, author, isbn, available=True): self.title = title self.author = author self.isbn = isbn self.available = available class Member: def __init__(self, member_id, name): self.member_id = member_id self.name = name self.borrowed_books = [] def borrow_book(self, isbn): self.borrowed_books.append(isbn) def return_book(self, isbn): if isbn in self.borrowed_books: self.borrowed_books.remove(isbn) class Library: def __init__(self): self.books = {} self.members = {} def add_book(self, title, author, isbn): if isbn not in self.books: self.books[isbn] = Book(title, author, isbn) def remove_book(self, isbn): if isbn in self.books: del self.books[isbn] def add_member(self, member_id, name): if member_id not in self.members: self.members[member_id] = Member(member_id, name) def remove_member(self, member_id): if member_id in self.members: del self.members[member_id] def borrow_book(self, member_id, isbn): if member_id in self.members and isbn in self.books: member = self.members[member_id] book = self.books[isbn] if book.available and len(member.borrowed_books) < 5: member.borrow_book(isbn) book.available = False return True return False def return_book(self, member_id, isbn): if member_id in self.members and isbn in self.books: member = self.members[member_id] book = self.books[isbn] if isbn in member.borrowed_books: member.return_book(isbn) book.available = True return True return False def list_books(self): return [(book.title, book.author, book.isbn, book.available) for book in self.books.values()] def list_members(self): return [(member.member_id, member.name) for member in self.members.values()]"},{"question":"# Question: Implementing and Evaluating PLSRegression You are tasked with implementing a function to perform regression using the Partial Least Squares (PLS) method and evaluate its performance using scikit-learn\'s `PLSRegression` class. Problem Statement Write a Python function `pls_regression` that performs the following: 1. Loads a dataset consisting of predictor variables `X` and response variables `Y`. 2. Applies PLSRegression to fit the model to `X` and `Y`. 3. Predicts the response variables for the same `X`. 4. Calculates the Mean Squared Error (MSE) between the actual and predicted response variables. Your function should: - Accept the number of components `n_components` to be used in the PLS regression. - Return the mean squared error (MSE). Input - `X`: a `numpy` array or pandas DataFrame of shape `(n_samples, n_features)` representing the predictor variables. - `Y`: a `numpy` array or pandas DataFrame of shape `(n_samples, n_targets)` representing the response variables. - `n_components`: an integer representing the number of components to be used in the PLS regression. Output - `mse`: a float representing the Mean Squared Error between the actual and predicted response variables. Constraints - Ensure that `n_components` is a positive integer and does not exceed the lesser of the number of samples and the number of features in `X`. Example ```python import numpy as np from sklearn.datasets import make_regression # Example dataset X, Y = make_regression(n_samples=100, n_features=10, n_targets=1, noise=0.1) Y = Y.reshape(-1, 1) # Reshape to 2D array for compatibility def pls_regression(X, Y, n_components): # Complete the implementation pass # Usage mse = pls_regression(X, Y, n_components=2) print(mse) ``` # Required Implementation 1. **Load dataset**: The function should accept `X` and `Y` as inputs. 2. **Fit PLS Regression**: Use `PLSRegression` from `sklearn.cross_decomposition` to fit the model to `X` and `Y`. 3. **Predict**: Use the fitted model to predict `Y` from `X`. 4. **Compute MSE**: Calculate the Mean Squared Error between the actual and predicted `Y`. Note: Handle any potential exceptions such as invalid input formats and ensure that the `n_components` is within the permissible range.","solution":"import numpy as np from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import mean_squared_error def pls_regression(X, Y, n_components): Performs PLS regression on the given X and Y with the specified number of components. Parameters: - X: numpy array or pandas DataFrame of shape (n_samples, n_features) - Y: numpy array or pandas DataFrame of shape (n_samples, n_targets) - n_components: integer, number of components to be used in the PLS regression Returns: - mse: float, mean squared error between the actual and predicted response variables # Ensure n_components is valid if n_components <= 0 or n_components > min(X.shape[0], X.shape[1]): raise ValueError(\\"Invalid number of components\\") # Initialize the PLS regression model pls = PLSRegression(n_components=n_components) # Fit the model pls.fit(X, Y) # Predict the response Y_pred = pls.predict(X) # Calculate MSE mse = mean_squared_error(Y, Y_pred) return mse"},{"question":"**Question: Implementing a PyTorch RNN for Variable-Length Sequences with Efficient Memory Management** You are tasked with implementing a recurrent neural network (RNN) in PyTorch that can handle variable-length input sequences. The RNN should be designed with efficient memory usage in mind and should be compatible with data parallelism using `torch.nn.DataParallel`. # Requirements: 1. **Data Preparation:** - Implement a function `prepare_data(sequences, lengths)` that takes a list of sequences (where each sequence is a list of integers tokens) and their respective lengths. This function should return: - A tensor of padded sequences. - A tensor of the sequence lengths. 2. **RNN Definition:** - Define an RNN class `VariableLengthRNN` that inherits from `torch.nn.Module`. The class should: - Initialize an LSTM layer. - Implement a `forward` method that: - Packs the padded input sequences. - Processes the sequences through the LSTM. - Unpacks the output sequences to their original lengths with padding. 3. **Memory Management:** - Ensure that memory is managed efficiently within the forward method by: - Avoiding unnecessary storage of intermediate results. - Ensuring that no gradient history is kept beyond what is needed. # Specifications: - Input to `prepare_data`: - `sequences`: List of sequences, where each sequence is a list of token IDs (integers). - `lengths`: List of integers representing the length of each sequence. - Output of `prepare_data`: - `padded_sequences`: Tensor of shape (B, T) where B is the batch size, and T is the max sequence length in the batch. - `sequence_lengths`: Tensor of shape (B). - The `VariableLengthRNN` class should: - Initialize with parameters: `input_dim`, `hidden_dim`, and `batch_first=True`. - Use the `pack_padded_sequence` and `pad_packed_sequence` functions to handle variable lengths inside the `forward` method. Below is a skeleton of the necessary functions and class. Complete the implementation: ```python import torch from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence def prepare_data(sequences, lengths): # Your implementation here pass class VariableLengthRNN(torch.nn.Module): def __init__(self, input_dim, hidden_dim): super(VariableLengthRNN, self).__init__() self.lstm = torch.nn.LSTM(input_dim, hidden_dim, batch_first=True) def forward(self, padded_input, input_lengths): total_length = padded_input.size(1) # get the max sequence length packed_input = pack_padded_sequence(padded_input, input_lengths, batch_first=True, enforce_sorted=False) packed_output, (h_n, c_n) = self.lstm(packed_input) output, output_lengths = pad_packed_sequence(packed_output, batch_first=True, total_length=total_length) return output, output_lengths # Sample usage: sequences = [[1, 2, 3], [4, 5], [6]] lengths = [3, 2, 1] input_dim = 10 # Example input dimension hidden_dim = 20 # Example hidden dimension padded_sequences, sequence_lengths = prepare_data(sequences, lengths) model = VariableLengthRNN(input_dim, hidden_dim) output, output_lengths = model(padded_sequences, sequence_lengths) ``` # Constraints: - Ensure the input sequences are variable in length. - Handle padding correctly within the RNN. - Optimize the implementation for memory efficiency. # Notes: - You may assume that the input data and dimensions are correctly formatted and valid. - The focus is on demonstrating the correct use of PyTorch functionalities and efficient handling of RNNs with variable-length sequences.","solution":"import torch from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence def prepare_data(sequences, lengths): Prepares data for variable-length sequence handling with padding. Parameters: sequences (List[List[int]]): List of sequences, where each sequence is a list of token IDs (integers). lengths (List[int]): List of integers representing the length of each sequence. Returns: padded_sequences (torch.Tensor): Tensor of shape (B, T) where B is the batch size and T is the max sequence length in the batch. sequence_lengths (torch.Tensor): Tensor of shape (B) representing the original lengths of sequences. # Convert list of sequences to tensor, padding with 0s to the max length in the batch sequences_sorted = sorted(zip(sequences, lengths), key=lambda x: x[1], reverse=True) sequences, lengths = zip(*sequences_sorted) sequences_tensor = [torch.tensor(seq) for seq in sequences] padded_sequences = pad_sequence(sequences_tensor, batch_first=True) sequence_lengths = torch.tensor(lengths) return padded_sequences, sequence_lengths class VariableLengthRNN(torch.nn.Module): def __init__(self, input_dim, hidden_dim): super(VariableLengthRNN, self).__init__() self.lstm = torch.nn.LSTM(input_dim, hidden_dim, batch_first=True) def forward(self, padded_input, input_lengths): Forward pass for handling variable-length sequences in an LSTM. Parameters: padded_input (torch.Tensor): Padded input tensor of shape (B, T, D). input_lengths (torch.Tensor): Tensor of shape (B) representing the original lengths of sequences. Returns: output (torch.Tensor): Output tensor from LSTM after packing and padding output_lengths (torch.Tensor): Tensor representing the lengths of sequences after unpacking # Pack the padded batch of sequences for the RNN packed_input = pack_padded_sequence(padded_input, input_lengths, batch_first=True, enforce_sorted=False) # Run through LSTM packed_output, (h_n, c_n) = self.lstm(packed_input) # Unpack the padded sequences output, output_lengths = pad_packed_sequence(packed_output, batch_first=True) return output, output_lengths"},{"question":"**Problem Statement: Secure Credential Management with `netrc`** **Objective**: Implement a Python function to securely load and validate credentials from a `.netrc` file, while adhering to POSIX security constraints and custom error handling. # Task Write a function `load_and_validate_credentials(file: str) -> dict` that: 1. **Loads Credentials**: Reads the given `.netrc` file and extracts credentials for all hosts. 2. **Validation**: Checks and ensures that the file permissions and ownership adhere to POSIX security constraints (i.e., the file must be owned by the user running the process and must not be accessible for read or write by any other user). 3. **Custom Error Handling**: Catches and appropriately handles `NetrcParseError` in case of syntactical issues in the `.netrc` file. 4. **Output**: Returns a dictionary where the keys are hostnames and the values are tuples containing (login, account, password). If a default entry exists, it should be included with the key `\'default\'`. # Input - `file` : A string representing the path to a `.netrc` file. # Output - A dictionary where: - Keys are hostnames (including \'default\' if it exists) - Values are tuples of the form (login, account, password) # Constraints - The `.netrc` file must be in UTF-8 encoding. - Handle file not found errors and provide meaningful feedback in such cases. - Implement security checks as described, and raise an exception if file permissions do not meet the constraints. # Example Assume a `.netrc` file with the following content: ``` machine host1 login user1 password pass1 machine host2 login user2 account acct2 password pass2 default login default_user password default_pass ``` Then, calling: ```python credentials = load_and_validate_credentials(\'path_to_netrc_file\') ``` Should return: ```python { \'host1\': (\'user1\', None, \'pass1\'), \'host2\': (\'user2\', \'acct2\', \'pass2\'), \'default\': (\'default_user\', None, \'default_pass\') } ``` # Note Ensure to add the necessary imports to cater to file reading, parsing, and security checks. **Hint**: You might find the `os`, `pwd`, and `stat` modules useful for handling file ownership and permission validation.","solution":"import os import pwd import stat from netrc import netrc, NetrcParseError def load_and_validate_credentials(file: str) -> dict: Loads credentials from a .netrc file and validates its permissions. Params: file: str - Path to the .netrc file Returns: dict - Dictionary of credentials where keys are hostnames and values are tuples of (login, account, password) # Check if file exists if not os.path.exists(file): raise FileNotFoundError(f\\"The file \'{file}\' does not exist.\\") # Validate file permissions and ownership file_stat = os.stat(file) if (file_stat.st_mode & (stat.S_IRWXG | stat.S_IRWXO)) != 0: raise PermissionError(f\\"The file \'{file}\' must not be accessible by group or others.\\") file_owner = pwd.getpwuid(file_stat.st_uid).pw_name if file_owner != os.getlogin(): raise PermissionError(f\\"The file \'{file}\' must be owned by the user \'{os.getlogin()}\'.\\") # Parse the .netrc file try: credentials = netrc(file) except NetrcParseError as e: raise ValueError(f\\"Error parsing the file \'{file}\': {e}\\") # Extract and format the credentials into a dictionary result = {} for host, (login, account, password) in credentials.hosts.items(): result[host] = (login, account, password) # Include default credentials if they exist if credentials.macros.get(\\"default\\"): result[\'default\'] = credentials.authenticators(\'default\') return result"},{"question":"# Custom Email Generator Implementation **Objective:** Write a custom email generator class that extends `email.generator.BytesGenerator` to handle additional functionality. This custom generator should process email messages containing MIME parts and serialize them with specific rules for encoding and line separators. # Problem Statement You are required to create a class `CustomBytesGenerator` that extends the `email.generator.BytesGenerator` class. Your class should implement the following functionalities: 1. **Initialization (`__init__`)**: - Initialize with an output file-like object parameter `outfp`, and optional parameters `mangle_from_`, `maxheaderlen`, and `policy`. - Additional parameter `custom_header` which, if provided, should be added as a new header to every email message processed. 2. **Custom Flatten Method**: - Override the `flatten` method to: - Add the `custom_header` (if provided) to the email message. - Ensure any lines in the message body beginning with \\"From \\" are prefixed with a `>` character if `mangle_from_` is set to `True`. - Use a custom line separator specified by `linesep` (default to CRLF, i.e., \'rn\'). - Other standard behaviors of the `flatten` method should be preserved. 3. **Writing Custom Content**: - Add a method `write_custom_content` that takes a string and a mime type and writes this string to the `outfp` wrapped within MIME type tags. # Constraints - The custom header key should be \\"X-Custom-Header\\" and its value should be a string provided during initialization. - The `custom_header` should only be added if it does not already exist in the message. - Performance is not a prime concern but ensure that your implementation does not reprocess the message more than necessary. # Function Signatures Here are the required function signatures: ```python from email.generator import BytesGenerator class CustomBytesGenerator(BytesGenerator): def __init__(self, outfp, mangle_from_=None, maxheaderlen=None, *, policy=None, custom_header=None): super().__init__(outfp, mangle_from_=mangle_from_, maxheaderlen=maxheaderlen, policy=policy) # Your initialization code here def flatten(self, msg, unixfrom=False, linesep=\'rn\'): # Your custom flatten implementation here def write_custom_content(self, content, mime_type): # Your code to write custom content here ``` # Example Usage ```python import io from email.message import EmailMessage # Create a basic email message msg = EmailMessage() msg[\'Subject\'] = \'Test email\' msg.set_content(\'This is a test email body.nFrom the test suite.\') # Create an output file-like object output = io.BytesIO() # Create an instance of CustomBytesGenerator generator = CustomBytesGenerator(output, custom_header=\'CustomHeaderValue\') # Flatten the email message generator.flatten(msg) # Write some custom content generator.write_custom_content(\\"This is some custom content\\", \\"text/plain\\") # Get the output output_value = output.getvalue().decode(\'utf-8\') print(output_value) ``` # Expected Output The printed output should include: 1. The original email with the `X-Custom-Header` properly added. 2. The `>` character in front of lines starting with \\"From \\" if `mangle_from_` is set to `True`. 3. The custom content written within MIME type tags.","solution":"from email.generator import BytesGenerator from email.message import EmailMessage from email.policy import default class CustomBytesGenerator(BytesGenerator): def __init__(self, outfp, mangle_from_=None, maxheaderlen=None, *, policy=default, custom_header=None): super().__init__(outfp, mangle_from_=mangle_from_, maxheaderlen=maxheaderlen, policy=policy) self.custom_header = custom_header def flatten(self, msg, unixfrom=False, linesep=\'rn\'): # Add custom header if not present if self.custom_header and \'X-Custom-Header\' not in msg: msg[\'X-Custom-Header\'] = self.custom_header # Mangle From lines if mangle_from_ is True if self._mangle_from_: new_body = [] for line in msg.get_payload().splitlines(): if line.startswith(\\"From \\"): new_body.append(f\'>{line}\') else: new_body.append(line) msg.set_payload(\\"n\\".join(new_body)) # Flatten the message using the overridden method super().flatten(msg, unixfrom=unixfrom, linesep=linesep) def write_custom_content(self, content, mime_type): custom_message = EmailMessage() custom_message.set_content(content, subtype=mime_type.split(\'/\')[1]) self.flatten(custom_message)"},{"question":"**Objective**: Demonstrate your understanding of PyTorch and the `torch.utils.benchmark` module by implementing a function that benchmarks multiple PyTorch operations and compares their performance. # Problem Statement Implement a function `benchmark_operations` that accepts a list of PyTorch operations to be benchmarked and returns a performance comparison. Function Signature ```python def benchmark_operations(operations: List[callable], input_tensor: torch.Tensor) -> str: # Your code here ``` Input - `operations`: A list of callable PyTorch operations (functions). - `input_tensor`: A `torch.Tensor` that serves as input to each operation. Output - A string containing a human-readable comparison of the operations\' performance. Constraints 1. Each operation in `operations` is a callable that accepts a single argument, which is the `input_tensor`. 2. Perform each operation multiple times to get an accurate performance measurement. 3. Use the `torch.utils.benchmark` module to measure and compare the performance. 4. The comparison should highlight which operation is the fastest and which is the slowest. Example ```python import torch import torch.utils.benchmark as benchmark def operation1(tensor): return tensor + tensor def operation2(tensor): return torch.matmul(tensor, tensor.t()) def benchmark_operations(operations, input_tensor): # Your implementation here # Example usage: operations = [operation1, operation2] input_tensor = torch.randn(100, 100) print(benchmark_operations(operations, input_tensor)) ``` Output: ``` Operation 1: Total time 100ms Operation 2: Total time 200ms Fastest operation is Operation 1 Slowest operation is Operation 2 ``` # Additional Requirements - Make sure to handle exceptions where an operation might fail. - You are encouraged to use additional methods and helper functions as needed to keep the code organized and modular. - Include comments to explain your code and logic.","solution":"import torch import torch.utils.benchmark as benchmark from typing import List, Callable def benchmark_operations(operations: List[Callable[[torch.Tensor], torch.Tensor]], input_tensor: torch.Tensor) -> str: Benchmarks the given list of PyTorch operations using the provided input tensor and returns a performance comparison. Parameters: operations (List[Callable[[torch.Tensor], torch.Tensor]]): List of PyTorch operations to benchmark. input_tensor (torch.Tensor): Tensor to be used as input for the operations. Returns: str: Human-readable string containing the performance comparison of the operations. timings = [] for idx, operation in enumerate(operations): timer = benchmark.Timer( stmt=\'operation(input_tensor)\', globals={\'operation\': operation, \'input_tensor\': input_tensor} ) time = timer.timeit(100) # Measure the time it takes to perform the operation 100 times timings.append((f\\"Operation {idx + 1}\\", time.median)) timings.sort(key=lambda x: x[1]) # Sort based on timing result = \\"n\\".join([f\\"{name}: Time {time:.6f} seconds\\" for name, time in timings]) result += f\\"nFastest operation is {timings[0][0]}\\" result += f\\"nSlowest operation is {timings[-1][0]}\\" return result"},{"question":"# Secure Password Authentication System **Objective**: Implement a Python class that performs user authentication securely using the getpass module. **Problem Statement**: You are asked to design a class named `SecureAuthenticator` that can handle user authentication. This class should use the `getpass` module to safely prompt for a password without echoing it. Additionally, it should store and verify user credentials. The `SecureAuthenticator` class should have the following methods: 1. `__init__(self)`: - Initialize an empty dictionary to store user credentials (username and password). 2. `add_user(self, username)`: - Prompts the user to input a password for the given username using `getpass.getpass()`. - Stores the username and password in the dictionary. - If the username already exists, raise a `ValueError` with the message \\"Username already exists.\\". 3. `authenticate_user(self, username)`: - Prompts the user to input a password for the given username using `getpass.getpass()`. - Checks if the username exists in the dictionary. - If the username does not exist, return `False`. - If the password matches the stored password for the provided username, return `True`; otherwise, return `False`. 4. `get_all_users(self)`: - Returns a list of all usernames stored in the authenticator. **Input and Output**: - For `add_user(username)`, there is no direct input-output. Password is prompted and stored internally. - For `authenticate_user(username)`, password is prompted, and the method returns `True` or `False`. - For `get_all_users()`, it returns a list of usernames. **Example Usage**: ```python authenticator = SecureAuthenticator() # Adding users authenticator.add_user(\'user1\') authenticator.add_user(\'user2\') # Authenticating users assert authenticator.authenticate_user(\'user1\') == True assert authenticator.authenticate_user(\'user2\') == False assert authenticator.authenticate_user(\'non_existent_user\') == False # Getting all users print(authenticator.get_all_users()) # Output: [\'user1\', \'user2\'] ``` **Constraints**: 1. You should handle the scenarios where: - Username already exists during user addition. - Authentication of a non-existent user. - Prompting the user for password input should be secure and not echo in the console. Implementing this `SecureAuthenticator` class will demonstrate a robust understanding of the `getpass` module as well as basic concepts of class design, error handling, and secure user authentication.","solution":"import getpass class SecureAuthenticator: def __init__(self): self.credentials = {} def add_user(self, username): if username in self.credentials: raise ValueError(\\"Username already exists.\\") password = getpass.getpass(prompt=f\\"Enter password for {username}: \\") self.credentials[username] = password def authenticate_user(self, username): if username not in self.credentials: return False password = getpass.getpass(prompt=f\\"Enter password for {username}: \\") return self.credentials[username] == password def get_all_users(self): return list(self.credentials.keys())"},{"question":"**Question: Customizing Seaborn Plots** In this assessment, you are required to demonstrate your proficiency in using the seaborn package to create and customize plots. You will be given a dataset, and your task is to generate a line plot with specific customization settings. # Task 1. **Read the dataset**: You are given a CSV file named `data.csv` which contains two columns `x` and `y`. 2. **Create a line plot**: Plot `x` on the x-axis and `y` on the y-axis. 3. **Set Plot Context**: Use the seaborn `set_context` function to set the context to \\"talk\\". 4. **Font Scaling**: Scale font elements by a factor of `1.5`. 5. **Override Parameters**: Override the default line width to `2.5` and set the font size of the axis labels to `15`. # Input - A CSV file named `data.csv` with two columns `x` and `y`. # Output - A seaborn line plot with the specified customizations. # Function Signature ```python def create_custom_plot(filename: str) -> None: # Complete the function below ``` # Constraints - It is guaranteed that the CSV file will have the required structure. - Use appropriate seaborn functionality to achieve the desired plot customizations. - The final plot must be displayed within the function using `plt.show()`. # Example Suppose `data.csv` contains the following data: ``` x,y 0,1 1,3 2,2 3,5 4,4 ``` The function `create_custom_plot(\'data.csv\')` should produce a line plot with the described customizations. # Tips - Remember to import necessary libraries at the beginning of your code. - Handle the file reading properly and ensure the correct usage of seaborn functions.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_plot(filename: str) -> None: Reads the dataset from filename, creates a line plot with customization. Parameters: - filename: str, path to the csv file containing the data # Read the dataset data = pd.read_csv(filename) # Set the desired seaborn context, font scale and override parameters sns.set_context(\\"talk\\", font_scale=1.5, rc={\\"lines.linewidth\\": 2.5, \\"axes.labelsize\\": 15}) # Create a line plot plt.figure(figsize=(10, 6)) sns.lineplot(x=\'x\', y=\'y\', data=data) # Display the plot plt.show()"},{"question":"You are required to demonstrate your mastery of the `seaborn.objects` module by creating and customizing a complex bar plot. Follow the instructions below: # Instructions 1. **Load the dataset:** - Use the seaborn function `load_dataset` to load the `diamonds` dataset. 2. **Create a plot object:** - Initialize a plot object with the `diamonds` dataset and map the `price` variable to the x-axis. Apply a logarithmic scale to the x-axis. 3. **Add a Bar Plot:** - Add a bar plot that displays a histogram of the `price` values. 4. **Overlay and Color Bars:** - Add another layer of bars colored by the `cut` of the diamonds. Address the issue of overlapping bars. 5. **Apply a Transformation:** - Use the `so.Stack()` transform to resolve overlapping of the bars by `cut`. 6. **Customize the Bar Appearance:** - Set the edge width to `0`. - Use the `clarity` variable to adjust the alpha (transparency) of the bars. 7. **Plot Narrow Bars:** - Add an additional histogram with narrower bars (Width = .5). Apply this only to diamonds with a `cut` of \'Ideal\'. # Expected Input & Output Format: - **Input:** No input required as you will load the dataset within the code. - **Output:** A fully rendered bar plot with the specified customizations. # Code Skeleton ```python import seaborn.objects as so from seaborn import load_dataset # 1. Load the dataset diamonds = load_dataset(\\"diamonds\\") # 2. Create a plot object with logarithmic x-axis scale p = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") # 3. Add a bar plot for histograms of price values p.add(so.Bars(), so.Hist()) # 4. Overlay another set of bars, with color mapped to \'cut\' p.add(so.Bars(), so.Hist(), color=\\"cut\\") # 5. Apply stacking to avoid overlap p.add(so.Bars(), so.Hist(), so.Stack(), color=\\"cut\\") # 6. Customize bar appearance p.add(so.Bars(edgewidth=0), so.Hist(), so.Stack(), alpha=\\"clarity\\") # 7. Plot narrow bars for diamonds with \'Ideal\' cut hist = so.Hist(binwidth=.075, binrange=(2, 5)) p.add(so.Bars(), hist) p.add(so.Bars(color=\\".9\\", width=.5), hist, data=diamonds.query(\\"cut == \'Ideal\'\\")) # Render the plot p.show() # Adjust as needed to render depending on the environment ``` Submit the final code implementing the given instructions to generate the described bar plot.","solution":"import seaborn.objects as so from seaborn import load_dataset # 1. Load the dataset diamonds = load_dataset(\\"diamonds\\") # 2. Create a plot object with logarithmic x-axis scale p = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") # 3. Add a bar plot for histograms of price values p.add(so.Bars(), so.Hist()) # 4. Overlay another set of bars, with color mapped to \'cut\' p.add(so.Bars(), so.Hist(), color=\\"cut\\") # 5. Apply stacking to avoid overlap p.add(so.Bars(), so.Hist(), so.Stack(), color=\\"cut\\") # 6. Customize bar appearance p.add(so.Bars(edgewidth=0), so.Hist(), so.Stack(), alpha=\\"clarity\\") # 7. Plot narrow bars for diamonds with \'Ideal\' cut hist = so.Hist(binwidth=.075, binrange=(2, 5)) p.add(so.Bars(), hist) p.add(so.Bars(color=\\".9\\", width=.5), hist, data=diamonds.query(\\"cut == \'Ideal\'\\")) # Show the plot p.show()"},{"question":"# Question: Data Processing with Error Handling and Context Management You are required to implement a function that reads data from a file, processes it, and writes the processed data to another file. The function should: 1. Read the data from an input file. 2. For each line in the input file, convert all characters to uppercase. 3. If a line is empty or contains only whitespace, skip processing for that line. 4. Write the processed lines to an output file. 5. Use appropriate error handling to manage potential file handling errors. 6. Use context management to ensure files are properly opened and closed. **Function Signature:** ```python def process_file(input_file: str, output_file: str) -> None: pass ``` **Input:** - `input_file` (str): The path to the input file. - `output_file` (str): The path to the output file. **Output:** - The function does not return any value but writes the processed lines to the output file. **Constraints:** - Ensure that files are properly handled even in cases of errors (e.g., file not found, permission issues). - Use the `with` statement for file operations. - Handle exceptions appropriately using try-except blocks. - The input and output files should be specified as string paths. **Example:** Suppose the input file `input.txt` contains: ``` hello world this is a test example! ``` After processing, the output file `output.txt` should contain: ``` HELLO WORLD THIS IS A TEST EXAMPLE! ``` **Notes:** - The function should be robust against various types of file I/O errors. - Proper use of compound statements such as if-else, for-loops, and context management is essential. - Corner cases such as empty lines or completely empty files are to be handled gracefully.","solution":"def process_file(input_file: str, output_file: str) -> None: Reads data from the input file, processes it by converting all characters to uppercase, and writes the processed data to the output file while skipping empty or whitespace-only lines. try: with open(input_file, \'r\') as infile: lines = infile.readlines() processed_lines = [line.upper().strip() for line in lines if line.strip()] with open(output_file, \'w\') as outfile: for line in processed_lines: outfile.write(line + \'n\') except FileNotFoundError: print(f\\"Error: The file {input_file} was not found.\\") except IOError as e: print(f\\"IOError occurred: {e}\\")"},{"question":"# Secure Message Authentication using HMAC Implement a secure messaging system where multiple messages are sent and received. The system should verify the integrity and authenticity of these messages using the HMAC (Keyed-Hash Message Authentication Code) algorithm. Requirements: 1. **Function `generate_hmac_keys(n)`**: - **Input**: An integer `n` representing the number of message segments. - **Output**: A list of `n` random secret keys (each key is a byte string of 16 bytes). 2. **Function `send_message(key, message, digestmod)`**: - **Input**: - `key`: A byte string which is a random secret key for the message. - `message`: A string which is the message to be sent. - `digestmod`: A string representing the hash algorithm (e.g., \'sha256\'). - **Output**: A tuple of the original message and its HMAC digest in hexadecimal format. 3. **Function `receive_message(key, message, hmac_digest, digestmod)`**: - **Input**: - `key`: A byte string which is the secret key used for HMAC generation. - `message`: A string which was supposed to be received. - `hmac_digest`: A string which is the HMAC digest received. - `digestmod`: A string representing the hash algorithm (e.g., \'sha256\'). - **Output**: A boolean indicating whether the message integrity is verified (True) or not (False). 4. **Function `secure_communication(messages, digestmod)`**: - **Input**: - `messages`: A list of strings where each string is a message segment. - `digestmod`: A string representing the hash algorithm (e.g., \'sha256\'). - **Output**: A list of tuples where each tuple contains the original message, its generated HMAC digest in hexadecimal format, and a verification result (boolean). - **Process**: - Generate a list of random secret keys (one for each message segment). - For each message, generate its HMAC digest. - Verify each message and return the results. Constraints: - Keys should be generated using a cryptographic random function (e.g., `os.urandom()`). - The hash algorithm must be a valid algorithm supported by `hashlib` (like \'sha1\', \'sha256\', etc.). Example Usage: ```python import hashlib import os def generate_hmac_keys(n): # Your implementation here pass def send_message(key, message, digestmod): # Your implementation here pass def receive_message(key, message, hmac_digest, digestmod): # Your implementation here pass def secure_communication(messages, digestmod=\'sha256\'): # Your implementation here pass # Example function call keys = generate_hmac_keys(3) messages = [\\"Hello, World!\\", \\"Testing HMAC.\\", \\"Secure message.\\"] digestmod = \'sha256\' communication = secure_communication(messages, digestmod) for msg, hmac_digest, verified in communication: print(f\\"Message: {msg}nHMAC Digest: {hmac_digest}nVerified: {verified}n\\") ```","solution":"import hmac import hashlib import os def generate_hmac_keys(n): Generates a list of n random secret keys, each 16 bytes long. return [os.urandom(16) for _ in range(n)] def send_message(key, message, digestmod): Generates HMAC digest for the given message using the provided key and digest mode. hmac_obj = hmac.new(key, message.encode(), hashlib.__dict__[digestmod]) return (message, hmac_obj.hexdigest()) def receive_message(key, message, hmac_digest, digestmod): Verifies the integrity and authenticity of the message using the provided HMAC digest. hmac_obj = hmac.new(key, message.encode(), hashlib.__dict__[digestmod]) return hmac.compare_digest(hmac_obj.hexdigest(), hmac_digest) def secure_communication(messages, digestmod=\'sha256\'): Manages the secure communication by generating keys, HMACs, and verifying messages. keys = generate_hmac_keys(len(messages)) results = [] for key, message in zip(keys, messages): sent_message, hmac_digest = send_message(key, message, digestmod) verified = receive_message(key, sent_message, hmac_digest, digestmod) results.append((sent_message, hmac_digest, verified)) return results"},{"question":"**Coding Assessment Question: Boolean Operations and Custom Boolean Object Handling in Python** **Objective:** Demonstrate understanding of boolean operations, type checking, and reference-like behavior in Python. **Problem Statement:** Create a class `CustomBoolean` that mimics certain behaviors of Python\'s internal `Py_False` and `Py_True` objects as described in the provided documentation. Your class should implement the following: 1. **Initialization**: - The class should accept an integer or boolean value upon initialization. - If initialized with an integer, the boolean value should be determined based on the truth value of the integer. 2. **Boolean Representation**: - Implement `__bool__()` to return the boolean value of the object. 3. **String Representation**: - Implement `__str__()` to return the string `\'False\'` for `False` and `\'True\'` for `True`. 4. **Equality Comparison**: - Overload the equality operator (`__eq__`) to compare with both boolean and integer values. 5. **Reference Handling**: - Implement a static method `from_long(cls, v)` which returns a new instance of `CustomBoolean` based on the truth value of the input integer `v`. **Constraints:** - You must not use the built-in `bool` or directly return `True` or `False` in the methods. - Focus on implementing the Boolean logic and mimicking reference-like behavior appropriately. **Function Signature:** ```python class CustomBoolean: def __init__(self, value: int): pass def __bool__(self) -> bool: pass def __str__(self) -> str: pass def __eq__(self, other) -> bool: pass @staticmethod def from_long(cls, v: int) -> \'CustomBoolean\': pass ``` **Examples:** ```python # Initialization tests cb1 = CustomBoolean(1) cb2 = CustomBoolean(0) cb3 = CustomBoolean(True) cb4 = CustomBoolean(False) # Boolean representation assert bool(cb1) == True assert bool(cb2) == False # String representation assert str(cb1) == \\"True\\" assert str(cb2) == \\"False\\" # Equality comparison assert (cb1 == 1) == True assert (cb2 == 0) == True assert (cb1 == True) == True assert (cb2 == False) == True # Reference handling cb5 = CustomBoolean.from_long(1) cb6 = CustomBoolean.from_long(0) assert bool(cb5) == True assert bool(cb6) == False ``` Your goal is to implement the `CustomBoolean` class and ensure that all the examples listed in the \\"Examples\\" section work as intended. **Note:** You are not allowed to use Python\'s built-in `bool()` directly in `__bool__`. Instead, you should manually handle the conversion logic.","solution":"class CustomBoolean: def __init__(self, value: int): self.value = bool(value) def __bool__(self): return self.value def __str__(self): return \'True\' if self.value else \'False\' def __eq__(self, other): if isinstance(other, CustomBoolean): return self.value == other.value elif isinstance(other, bool): return self.value == other elif isinstance(other, int): return self.value == bool(other) return False @staticmethod def from_long(v: int) -> \'CustomBoolean\': return CustomBoolean(v)"},{"question":"# Objective: The objective of this task is to test your ability to use the `sklearn.datasets` module to load a real-world dataset, preprocess it, and apply a machine learning model to solve a specific problem. # Problem Statement: You are tasked with building a facial recognition model using the Olivetti faces dataset. Your goal is to preprocess the dataset, split it into training and testing sets, train a machine learning model, and evaluate its performance. # Instructions: 1. **Dataset Loading**: - Use the `fetch_olivetti_faces` function to load the Olivetti faces dataset. 2. **Data Preprocessing**: - Normalize the feature data so that each feature has a mean of 0 and a variance of 1. - Split the dataset into training and testing sets using an 80-20 split. 3. **Model Training**: - Train a Support Vector Machine (SVM) classifier on the training set. You may use `sklearn.svm.SVC`. 4. **Model Evaluation**: - Evaluate the classifier on the test set. Report the accuracy of the model. # Constraints: - You should only use the scikit-learn library for this task. - Ensure that the preprocessed data has zero mean and unit variance. # Function Signature: ```python def train_and_evaluate_olivetti_faces() -> float: Trains an SVM classifier on the Olivetti faces dataset and evaluates its accuracy on the test set. Returns: accuracy (float): The accuracy of the SVM classifier on the test set. pass ``` # Example Output: If the function is implemented correctly, it should return the accuracy of the SVM model on the test set, for example: ```python >>> train_and_evaluate_olivetti_faces() 0.93 ``` # Notes: - You may use the `sklearn.model_selection.train_test_split` function to split the dataset. - The classifier should be trained using default parameters. Good luck!","solution":"from sklearn.datasets import fetch_olivetti_faces from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.metrics import accuracy_score def train_and_evaluate_olivetti_faces(): # Load the Olivetti faces dataset data = fetch_olivetti_faces() X = data.data y = data.target # Normalize the feature data scaler = StandardScaler() X_normalized = scaler.fit_transform(X) # Split the dataset into training and testing sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42) # Train an SVM classifier svm_clf = SVC() svm_clf.fit(X_train, y_train) # Evaluate the classifier on the test set y_pred = svm_clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # Return the accuracy return accuracy"},{"question":"# Python Coding Assessment: Working with Customized Slice Objects Objective To assess your understanding and ability to work with slice objects in Python, specifically using the functionalities provided by `PySlice_Type`. The task involves implementing a function that uses custom C-API based slice handling to perform slicing operations on a list. Task Write a function `custom_slice(lst: list, start: int, stop: int, step: int) -> list` that simulates slicing a given list but uses lower-level slice handling. 1. The function will: - Create a slice object based on the `start`, `stop`, and `step` parameters. - Retrieve the total length of the original list. - Use the created slice object to fetch the actual indices the slice covers. - Return the sublist defined by these indices. 2. You must handle potential errors if the slice creation or index retrieval fails. Input - `lst` (list): The list to be sliced. - `start` (int): The starting index of the slice. - `stop` (int): The stopping index of the slice. - `step` (int): The step size of the slice. Output - Return a sublist of `lst` that matches the slicing parameters. Constraints - Do not use Python\'s native slicing notation (`lst[start:stop:step]`) directly. - You must demonstrate an understanding of slice creation, error handling, and index retrieval using the provided Python C-API functions. - The given `start`, `stop`, and `step` values could be `None`, emulating the absence of these parameters in a typical slice (optional slicing). Example ```python def custom_slice(lst, start, stop, step): # Your implementation here # Example Usage lst = [0, 1, 2, 3, 4, 5] print(custom_slice(lst, 1, 5, 2)) # Output: [1, 3] print(custom_slice(lst, None, None, -1)) # Output: [5, 4, 3, 2, 1, 0] ``` **Note:** Your implementation must utilize the concepts provided in the slice object\'s documentation and handle the slicing logic at a lower, more detailed level than the typical Python slicing operations.","solution":"def custom_slice(lst, start, stop, step): Simulates slicing a given list using custom slice object handling. slice_obj = slice(start, stop, step) indices = slice_obj.indices(len(lst)) return [lst[i] for i in range(*indices)]"},{"question":"# Objective Your task is to implement a function that toggles the terminal mode of a given file descriptor between raw and cbreak modes multiple times and reads user inputs following each switch. This exercise will demonstrate your comprehension of terminal control in UNIX systems using the `tty` module and the handling of file descriptors. # Function Signature ```python def toggle_terminal_mode(fd: int, switches: int) -> None: pass ``` # Task Description 1. **Function:** `toggle_terminal_mode(fd: int, switches: int) -> None` - **Input:** - `fd`: A file descriptor representing an open file (typically, for a terminal). - `switches`: An integer representing the number of times to switch between `raw` and `cbreak` modes. - **Output:** The function does not return a value. Instead, it performs the necessary toggling and input reading operations. - **Behavior:** 1. Starts in `cbreak` mode. 2. Alternates between `cbreak` and `raw` modes based on the `switches` count. 3. After each switch, reads a single character input from the terminal and prints it to the standard output. # Constraints - The function must handle invalid file descriptors gracefully. - The function should print an error message and exit if the platform is not UNIX. - You may assume that the initial file descriptor provided is valid and points to an open terminal. # Example Usage ```python import os import sys import tty import termios # Open a terminal file descriptor fd = sys.stdin.fileno() # Example to toggle modes 5 times toggle_terminal_mode(fd, 5) ``` # Additional Notes - You may need to import the `os`, `sys`, `tty`, and `termios` modules. - Use appropriate exception handling to ensure robustness. - Remember to restore the original terminal settings after toggling.","solution":"import os import sys import tty import termios def toggle_terminal_mode(fd: int, switches: int) -> None: if not os.isatty(fd): print(\\"Error: File descriptor is not a terminal.\\") return try: original_settings = termios.tcgetattr(fd) except: print(\\"Error: Unable to get terminal attributes.\\") return try: for i in range(switches): if i % 2 == 0: tty.setcbreak(fd) else: tty.setraw(fd) print(f\\"Switch {i+1}, mode: {\'cbreak\' if i % 2 == 0 else \'raw\'}\\") ch = sys.stdin.read(1) print(f\\"Read character: {ch!r}\\") except Exception as e: print(f\\"Error occurred: {e}\\") finally: termios.tcsetattr(fd, termios.TCSADRAIN, original_settings) print(\\"Restored original terminal settings.\\")"},{"question":"IMAP4 Email Search and Fetch Objective: Implement a function that connects to an IMAP4 server, logs in with provided credentials, searches for emails from a specific sender within a given date range, fetches the subject of each matched email, and returns a list of these subjects. Additionally, ensure the connection uses SSL for security. Instructions: 1. Implement the function `fetch_email_subjects(): 2. Establish an SSL connection to the IMAP4 server. 3. Log in using the provided username and password. 4. Select the `INBOX` mailbox. 5. Search for emails from a specific sender (`from_address`) within a date range (`start_date` to `end_date`). Use IMAP `SINCE` and `BEFORE` criteria for the date range. 6. Fetch the subject of each email that matches the search criteria. 7. Return a list of email subjects. 8. Ensure to handle proper connection closure in case of errors or normal execution using the `with` statement. Function Signature: ```python import imaplib from datetime import datetime from typing import List def fetch_email_subjects(host: str, port: int, username: str, password: str, from_address: str, start_date: datetime, end_date: datetime) -> List[str]: pass ``` Input: - `host` (str): The IMAP4 server\'s hostname (e.g., \'imap.gmail.com\'). - `port` (int): The IMAP4 SSL port (usually 993). - `username` (str): The username for logging into the IMAP4 server. - `password` (str): The password for logging into the IMAP4 server. - `from_address` (str): The email address of the sender to filter emails. - `start_date` (datetime): The start of the date range (inclusive). - `end_date` (datetime): The end of the date range (inclusive). Output: - List of email subjects (List[str]) that match the search criteria. Constraints: - The connection must be established securely using SSL. - Proper error handling must be implemented to ensure the connection is closed appropriately. Example: ```python from datetime import datetime subjects = fetch_email_subjects( host=\'imap.gmail.com\', port=993, username=\'example@gmail.com\', password=\'password123\', from_address=\'sender@example.com\', start_date=datetime(2023, 1, 1), end_date=datetime(2023, 10, 31) ) print(subjects) # Output could be: # [\'Subject 1\', \'Subject 2\', \'Subject 3\'] ``` Notes: - You may find the `IMAP4_SSL` class useful for establishing a secure connection. - Use the `SELECT` command to choose the `INBOX` mailbox. - Use the `SEARCH` command for filtering emails based on the sender and date range. - Use the `FETCH` command to retrieve the subject of each email in the matched set. - Consider using `imaplib`\'s mechanisms for encoding search criteria and handling server responses.","solution":"import imaplib from datetime import datetime from typing import List, Optional def fetch_email_subjects(host: str, port: int, username: str, password: str, from_address: str, start_date: datetime, end_date: datetime) -> List[str]: try: with imaplib.IMAP4_SSL(host, port) as mail: mail.login(username, password) mail.select(\'INBOX\') search_criteria = f\'(FROM \\"{from_address}\\" SINCE \\"{start_date.strftime(\\"%d-%b-%Y\\")}\\" BEFORE \\"{(end_date + timedelta(days=1)).strftime(\\"%d-%b-%Y\\")}\\")\' status, messages = mail.search(None, search_criteria) if status != \'OK\': return [] email_ids = messages[0].split() subjects = [] for email_id in email_ids: status, message_data = mail.fetch(email_id, \'(BODY[HEADER.FIELDS (SUBJECT)])\') if status != \'OK\': continue for response_part in message_data: if isinstance(response_part, tuple): subject_line = response_part[1].decode().strip() subject = subject_line.partition(\\": \\")[2] subjects.append(subject) return subjects except Exception as e: print(f\\"An error occurred: {e}\\") return [] import datetime"},{"question":"# Python Input Handling Challenge You are required to implement a flexible Python script that can handle different forms of input: complete programs from files, interactive statements, and expressions to be evaluated. Your task is to design a function that takes input in one of these forms and executes it appropriately. Function Signature ```python def handle_python_input(input_type: str, input_value: str) -> None: Handles and executes Python input based on the specified input type. Parameters: input_type (str): The type of input provided. It can be \'file\', \'interactive\', or \'expression\'. input_value (str): The actual input value. For \'file\', it is the file path; for \'interactive\' and \'expression\', it is the input string. Returns: None ``` Requirements and Constraints: - **Input Type**: The input_type parameter specifies the form of the input: - \'file\': `input_value` is the path to a Python file that contains a complete Python program. - \'interactive\': `input_value` is a string representing interactive Python statements. - \'expression\': `input_value` is a string representing an expression to be evaluated using `eval()`. - Your function should handle the input appropriately based on the input_type: - For \'file\', read the entire file content and execute it as a complete Python program. - For \'interactive\', execute each statement provided in the string interactively. - For \'expression\', evaluate the expression using `eval()` and print the result. - You may assume that inputs provided will be syntactically valid Python code. - Do not use `exec()` for \'expression\' input type; use `eval()` instead. Example Usage: ```python # Case 1: Handling file input handle_python_input(\'file\', \'example.py\') # Case 2: Handling interactive input interactive_code = a = 5 b = 10 print(a + b) handle_python_input(\'interactive\', interactive_code) # Case 3: Handling expression input handle_python_input(\'expression\', \'2 + 2\') ``` In these examples: - The first call should read and execute the content of \'example.py\'. - The second call should execute the provided multi-line interactive code and print the output. - The third call should evaluate the expression \'2 + 2\' and print the result. Your solution will be assessed based on: - Correctness of the function implementation. - Handling the specified input types correctly. - Demonstrating knowledge of file operations, interactive commands, and expression evaluation in Python.","solution":"def handle_python_input(input_type: str, input_value: str) -> None: Handles and executes Python input based on the specified input type. Parameters: input_type (str): The type of input provided. It can be \'file\', \'interactive\', or \'expression\'. input_value (str): The actual input value. For \'file\', it is the file path; for \'interactive\' and \'expression\', it is the input string. Returns: None if input_type == \'file\': with open(input_value, \'r\') as file: program = file.read() exec(program) elif input_type == \'interactive\': exec(input_value) elif input_type == \'expression\': result = eval(input_value) print(result) else: raise ValueError(\\"Invalid input type. Expected \'file\', \'interactive\', or \'expression\'.\\")"},{"question":"You are given a dataset in the form of a CSV file named `sales_data.csv` containing sales records of a company. The dataset contains the following columns: - `date`: The date of the sales (string in the format `YYYY-MM-DD`). - `region`: The region where the sale was made (string). - `sales_amount`: The amount of sales made (float). Your task is to implement a function using pandas to do the following operations: 1. **Read the dataset** from the CSV file into a pandas Series. 2. **Extract and print** the year from the `date` column. 3. **Calculate and print** the total sales amount for each year. 4. **Handle missing values** by filling forward any missing values in the `sales_amount` column and then backward to cover any remaining missing values. 5. **Identify and print** the region with the highest total sales amount. 6. **Plot** the sales amount over time for each region and save the plots as PNG files. The file names should be in the format `region_name_sales.png`. # Function Signature ```python def analyze_sales(file_path: str) -> None: # Your code here pass ``` # Input - `file_path` (str): The path to the CSV file (`sales_data.csv`). # Output - None. The function should print the required information and save the plots as described. # Constraints - You can assume the dataset is correctly formatted and there are no duplicate dates for a given region. - The function should handle any missing values in the `sales_amount` column as specified. # Example Given the `sales_data.csv` with the following content: ``` date,region,sales_amount 2022-01-01,North,1000.0 2022-01-02,West,1500.0 2022-01-03,South,2000.0 2023-01-01,North,1100.0 2023-01-02,West,1600.0 2023-01-03,South,2100.0 ``` The function should: 1. Extract the year from the `date` column. 2. Calculate total sales per year. 3. Handle any missing values in the `sales_amount` column (if they exist). 4. Identify and print the region with the highest total sales amount. 5. Create and save sales amount plots over time for each region. ```python # Calling the function analyze_sales(\'sales_data.csv\') ``` Expected Print Output: ``` Years extracted: [2022, 2023] Total Sales per Year: 2022: 4500.0 2023: 4800.0 Region with highest total sales: South ``` And it should generate PNG files named like `North_sales.png`, `West_sales.png`, and `South_sales.png` with the appropriate sales data plots for each region.","solution":"import pandas as pd import matplotlib.pyplot as plt def analyze_sales(file_path: str) -> None: # Read the dataset from the CSV file sales_data = pd.read_csv(file_path) # Extract the year from the date column sales_data[\'year\'] = pd.to_datetime(sales_data[\'date\']).dt.year print(\\"Years extracted:\\", sales_data[\'year\'].unique()) # Calculate the total sales amount for each year total_sales_per_year = sales_data.groupby(\'year\')[\'sales_amount\'].sum() print(\\"Total Sales per Year:\\") for year, total_sales in total_sales_per_year.items(): print(f\\"{year}: {total_sales}\\") # Handle missing values by filling forward then backward sales_data[\'sales_amount\'].fillna(method=\'ffill\', inplace=True) sales_data[\'sales_amount\'].fillna(method=\'bfill\', inplace=True) # Identify the region with the highest total sales amount total_sales_per_region = sales_data.groupby(\'region\')[\'sales_amount\'].sum() region_with_highest_sales = total_sales_per_region.idxmax() print(\\"Region with highest total sales:\\", region_with_highest_sales) # Plot the sales amount over time for each region and save the plots for region in sales_data[\'region\'].unique(): region_sales = sales_data[sales_data[\'region\'] == region] plt.figure(figsize=(10, 6)) plt.plot(region_sales[\'date\'], region_sales[\'sales_amount\'], marker=\'o\') plt.xlabel(\'Date\') plt.ylabel(\'Sales Amount\') plt.title(f\'Sales Amount Over Time: {region}\') plt.xticks(rotation=45) plt.tight_layout() plt.savefig(f\'{region}_sales.png\') plt.close()"},{"question":"**Coding Assessment Question** # Objective: To assess your understanding of scikit-learn\'s synthetic dataset generators and your ability to visualize and analyze the generated data. # Task: Write a Python function named `analyze_and_visualize` that performs the following operations: 1. Generates a dataset using `make_classification` with the following parameters: - `n_samples=1000` - `n_features=20` - `n_informative=2` - `n_redundant=3` - `n_clusters_per_class=1` - `n_classes=2` - `random_state=42` 2. Generates a dataset using `make_moons` with the following parameters: - `n_samples=1000` - `noise=0.2` - `random_state=42` 3. Generates a dataset using `make_circles` with the following parameters: - `n_samples=1000` - `noise=0.1` - `factor=0.3` - `random_state=42` 4. For each of the datasets, create scatter plots to visualize the generated data points with different colors for each class. 5. Return a dictionary containing the shapes of the generated datasets: ```python def analyze_and_visualize(): import matplotlib.pyplot as plt from sklearn.datasets import make_classification, make_moons, make_circles results = {} # Generate and visualize make_classification dataset X_class, y_class = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=3, n_clusters_per_class=1, n_classes=2, random_state=42) plt.figure(figsize=(10, 4)) plt.subplot(1, 3, 1) plt.scatter(X_class[:, 0], X_class[:, 1], c=y_class, cmap=plt.cm.Paired, edgecolor=\'k\') plt.title(\'make_classification\') results[\'make_classification\'] = X_class.shape # Generate and visualize make_moons dataset X_moons, y_moons = make_moons(n_samples=1000, noise=0.2, random_state=42) plt.subplot(1, 3, 2) plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap=plt.cm.Paired, edgecolor=\'k\') plt.title(\'make_moons\') results[\'make_moons\'] = X_moons.shape # Generate and visualize make_circles dataset X_circles, y_circles = make_circles(n_samples=1000, noise=0.1, factor=0.3, random_state=42) plt.subplot(1, 3, 3) plt.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap=plt.cm.Paired, edgecolor=\'k\') plt.title(\'make_circles\') results[\'make_circles\'] = X_circles.shape plt.tight_layout() plt.show() return results # Expected Output format: # { # \'make_classification\': (1000, 20), # \'make_moons\': (1000, 2), # \'make_circles\': (1000, 2) # } ``` # **Submission Guidelines:** 1. Implement the `analyze_and_visualize` function in a Python file or Jupyter notebook. 2. Ensure that your code runs without errors. 3. Your submission should include the generated plots and the returned dictionary with the shapes of the datasets. # Constraints: 1. You can only use functions from `matplotlib.pyplot` and `sklearn.datasets`. 2. Ensure that the code is optimized and runs efficiently for the provided dataset sizes. # **Performance Requirements:** 1. The function should run and complete in less than 1 minute on a standard machine. 2. All plots should be correctly displayed in a single figure. # **Evaluation Criteria:** 1. Correct implementation of dataset generation and visualization. 2. Proper use of scikit-learn\'s dataset generators. 3. Correct and complete data visualization. 4. Efficiency and readability of the code.","solution":"def analyze_and_visualize(): import matplotlib.pyplot as plt from sklearn.datasets import make_classification, make_moons, make_circles results = {} # Generate and visualize make_classification dataset X_class, y_class = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=3, n_clusters_per_class=1, n_classes=2, random_state=42) plt.figure(figsize=(15, 5)) plt.subplot(1, 3, 1) plt.scatter(X_class[:, 0], X_class[:, 1], c=y_class, cmap=plt.cm.Paired, edgecolor=\'k\') plt.title(\'make_classification\') results[\'make_classification\'] = X_class.shape # Generate and visualize make_moons dataset X_moons, y_moons = make_moons(n_samples=1000, noise=0.2, random_state=42) plt.subplot(1, 3, 2) plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap=plt.cm.Paired, edgecolor=\'k\') plt.title(\'make_moons\') results[\'make_moons\'] = X_moons.shape # Generate and visualize make_circles dataset X_circles, y_circles = make_circles(n_samples=1000, noise=0.1, factor=0.3, random_state=42) plt.subplot(1, 3, 3) plt.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap=plt.cm.Paired, edgecolor=\'k\') plt.title(\'make_circles\') results[\'make_circles\'] = X_circles.shape plt.tight_layout() plt.show() return results"},{"question":"You are tasked with implementing a set of utility functions that leverage the `quopri` module to encode and decode text data. The following functions need to be implemented: 1. **Function name:** `encode_text` - **Input:** - `text` (str): A string containing the text to be encoded. - `quotetabs` (bool): A flag to indicate whether to encode embedded spaces and tabs. - `header` (bool, default=False): A flag to control if spaces are encoded as underscores. - **Output:** - (str): The quoted-printable encoded string. 2. **Function name:** `decode_text` - **Input:** - `encoded_text` (str): A string containing the quoted-printable encoded text. - `header` (bool, default=False): A flag to indicate whether underscores should be decoded as spaces. - **Output:** - (str): The decoded string. 3. **Function name:** `encode_file` - **Input:** - `input_file_path` (str): The path to the input file that needs to be encoded. - `output_file_path` (str): The path where the encoded output file should be saved. - `quotetabs` (bool): A flag to indicate whether to encode embedded spaces and tabs. - `header` (bool, default=False): A flag to control if spaces are encoded as underscores. - **Output:** - None. The function should save the encoded content to the specified output file. 4. **Function name:** `decode_file` - **Input:** - `input_file_path` (str): The path to the input encoded file that needs to be decoded. - `output_file_path` (str): The path where the decoded output file should be saved. - `header` (bool, default=False): A flag to indicate whether underscores should be decoded as spaces. - **Output:** - None. The function should save the decoded content to the specified output file. # Constraints - You can assume input strings will always be valid UTF-8 encoded text. - File paths provided as inputs will always be valid and accessible. - Proper exception handling should be implemented for file operations. # Example Usage ```python text_to_encode = \\"Hello World! This is a test.\\" encoded_text = encode_text(text_to_encode, quotetabs=True) print(encoded_text) decoded_text = decode_text(encoded_text) print(decoded_text) # Example for file encoding and decoding encode_file(\\"input.txt\\", \\"encoded_output.txt\\", quotetabs=True) decode_file(\\"encoded_output.txt\\", \\"decoded_output.txt\\") ``` # Your Task Implement the `encode_text`, `decode_text`, `encode_file`, and `decode_file` functions using the `quopri` module as per the specifications provided.","solution":"import quopri def encode_text(text, quotetabs, header=False): Returns the quoted-printable encoded string. :param text: str :param quotetabs: bool :param header: bool :return: str encoded_bytes = quopri.encodestring(text.encode(\'utf-8\'), quotetabs, header=header) return encoded_bytes.decode(\'utf-8\') def decode_text(encoded_text, header=False): Returns the decoded string from quoted-printable encoded string. :param encoded_text: str :param header: bool :return: str decoded_bytes = quopri.decodestring(encoded_text.encode(\'utf-8\'), header=header) return decoded_bytes.decode(\'utf-8\') def encode_file(input_file_path, output_file_path, quotetabs, header=False): Encodes the content of the input file and saves it to the output file. :param input_file_path: str :param output_file_path: str :param quotetabs: bool :param header: bool try: with open(input_file_path, \'r\', encoding=\'utf-8\') as input_file: content = input_file.read() encoded_content = encode_text(content, quotetabs, header) with open(output_file_path, \'w\', encoding=\'utf-8\') as output_file: output_file.write(encoded_content) except Exception as e: print(f\\"An error occurred during file encoding: {e}\\") def decode_file(input_file_path, output_file_path, header=False): Decodes the content of the input file and saves it to the output file. :param input_file_path: str :param output_file_path: str :param header: bool try: with open(input_file_path, \'r\', encoding=\'utf-8\') as input_file: encoded_content = input_file.read() decoded_content = decode_text(encoded_content, header) with open(output_file_path, \'w\', encoding=\'utf-8\') as output_file: output_file.write(decoded_content) except Exception as e: print(f\\"An error occurred during file decoding: {e}\\")"},{"question":"# Covariance Estimation with scikit-learn Background Covariance estimation is crucial in statistics and machine learning, often required for analyzing the relationships between various features of a dataset. The `sklearn.covariance` module in scikit-learn provides several methods for covariance estimation under different conditions and assumptions. Objective You are provided with a dataset and you must perform and compare different covariance estimation methods. The goal is to implement a code that: 1. Loads a given dataset. 2. Computes the covariance matrix using various estimators. 3. Evaluates and compares the performance of each estimator. Dataset Assume you are provided with a dataset `data.csv` which contains multiple features. You need to load this dataset into your application. Requirements 1. **Data Loading**: - Load the provided `data.csv` file into a Pandas DataFrame. 2. **Covariance Estimation**: - Compute the covariance matrix using the following estimators: 1. Empirical Covariance. 2. Shrunk Covariance with a shrinkage coefficient of 0.1. 3. Ledoit-Wolf Shrinkage. 4. Oracle Approximating Shrinkage (OAS). 5. Sparse Inverse Covariance with an `alpha` parameter of 0.1. 6. Minimum Covariance Determinant (robust estimator). 3. **Comparison and Evaluation**: - Visualize the covariance matrices using heatmaps. - Print the covariance matrices. - Use the Mean Squared Error (MSE) metric to compare the covariance matrices obtained by different estimators. Assume the empirical covariance matrix is the reference for MSE calculation. Constraints - The `data.csv` file must contain at least 100 samples with at least 5 features. - Ensure your implementation is efficient and can handle the dataset size promptly. Implementation ```python import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from sklearn.covariance import (EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, GraphicalLasso, MinCovDet) # Load the dataset data = pd.read_csv(\'data.csv\') # Prepare data X = data.values # Instantiate the covariance objects emp_cov = EmpiricalCovariance().fit(X) shrunk_cov = ShrunkCovariance(shrinkage=0.1).fit(X) lw_cov = LedoitWolf().fit(X) oas_cov = OAS().fit(X) sparse_inv_cov = GraphicalLasso(alpha=0.1).fit(X) mcd_cov = MinCovDet().fit(X) # Get covariance matrices emp_cov_matrix = emp_cov.covariance_ shrunk_cov_matrix = shrunk_cov.covariance_ lw_cov_matrix = lw_cov.covariance_ oas_cov_matrix = oas_cov.covariance_ sparse_inv_cov_matrix = sparse_inv_cov.covariance_ mcd_cov_matrix = mcd_cov.covariance_ # Calculate Mean Squared Error def mse(matrix1, matrix2): return np.mean((matrix1 - matrix2) ** 2) mse_results = { \'Shrunk Covariance\': mse(emp_cov_matrix, shrunk_cov_matrix), \'Ledoit-Wolf\': mse(emp_cov_matrix, lw_cov_matrix), \'OAS\': mse(emp_cov_matrix, oas_cov_matrix), \'Sparse Inverse Covariance\': mse(emp_cov_matrix, sparse_inv_cov_matrix), \'MCD\': mse(emp_cov_matrix, mcd_cov_matrix) } # Print MSE results print(\'Mean Squared Error (using Empirical Covariance as reference):\') for estimator, score in mse_results.items(): print(f\'{estimator}: {score}\') # Visualization cov_matrices = { \'Empirical Covariance\': emp_cov_matrix, \'Shrunk Covariance\': shrunk_cov_matrix, \'Ledoit-Wolf\': lw_cov_matrix, \'OAS\': oas_cov_matrix, \'Sparse Inverse Covariance\': sparse_inv_cov_matrix, \'MCD\': mcd_cov_matrix } # Plot heatmaps plt.figure(figsize=(15, 10)) for i, (title, matrix) in enumerate(cov_matrices.items(), 1): plt.subplot(2, 3, i) sns.heatmap(matrix, annot=True, fmt=\'.2f\', cmap=\'coolwarm\') plt.title(title) plt.tight_layout() plt.show() ``` This implementation will load the dataset, compute the covariance matrices using different estimators, compare them using the Mean Squared Error metric, and visualize the results using heatmaps.","solution":"import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from sklearn.covariance import (EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, GraphicalLasso, MinCovDet) def load_data(file_path): Loads the dataset from a given CSV file path. Parameters: - file_path: str, path to the CSV file Returns: - DataFrame containing the loaded data return pd.read_csv(file_path) def compute_covariances(data): Computes various covariance matrices from the given data. Parameters: - data: DataFrame, the dataset Returns: - Dictionary containing the name and covariance matrices from different estimators X = data.values # Instantiate the covariance objects emp_cov = EmpiricalCovariance().fit(X) shrunk_cov = ShrunkCovariance(shrinkage=0.1).fit(X) lw_cov = LedoitWolf().fit(X) oas_cov = OAS().fit(X) sparse_inv_cov = GraphicalLasso(alpha=0.1).fit(X) mcd_cov = MinCovDet().fit(X) cov_matrices = { \'Empirical Covariance\': emp_cov.covariance_, \'Shrunk Covariance\': shrunk_cov.covariance_, \'Ledoit-Wolf\': lw_cov.covariance_, \'OAS\': oas_cov.covariance_, \'Sparse Inverse Covariance\': sparse_inv_cov.covariance_, \'MCD\': mcd_cov.covariance_ } return cov_matrices def mean_squared_error(matrix1, matrix2): Calculates the mean squared error between two matrices. Parameters: - matrix1: np.ndarray, first matrix - matrix2: np.ndarray, second matrix Returns: - float, mean squared error return np.mean((matrix1 - matrix2) ** 2) def compare_covariances(cov_matrices): Compares the covariance matrices using Mean Squared Error. Parameters: - cov_matrices: dict, dictionary containing the name and covariance matrices Returns: - Dictionary containing the MSE results emp_cov_matrix = cov_matrices[\'Empirical Covariance\'] mse_results = {} for name, matrix in cov_matrices.items(): if name != \'Empirical Covariance\': mse_results[name] = mean_squared_error(emp_cov_matrix, matrix) return mse_results def visualize_covariances(cov_matrices): Visualizes the covariance matrices using heatmaps. Parameters: - cov_matrices: dict, dictionary containing the name and covariance matrices plt.figure(figsize=(15, 10)) for i, (title, matrix) in enumerate(cov_matrices.items(), 1): plt.subplot(2, 3, i) sns.heatmap(matrix, annot=True, fmt=\'.2f\', cmap=\'coolwarm\') plt.title(title) plt.tight_layout() plt.show() # Main function def main(file_path): # Load data data = load_data(file_path) # Compute covariances cov_matrices = compute_covariances(data) # Compare covariances mse_results = compare_covariances(cov_matrices) print(\'Mean Squared Error (using Empirical Covariance as reference):\') for estimator, score in mse_results.items(): print(f\'{estimator}: {score}\') # Visualize covariances visualize_covariances(cov_matrices) # Testing the main function # Uncomment below lines to execute the main function # main(\'data.csv\')"},{"question":"# Dictionary Manipulation with Custom Constraints Objective: The goal of this question is to write a Python function that performs various manipulations on a dictionary, leveraging the methods and functions described in the provided documentation. You will create, update, retrieve, and manipulate dictionary entries with specific constraints and handle errors gracefully. Problem Statement: Write a Python function `custom_dict_operations(operations)` that takes a list of operations and performs them sequentially on a dictionary. Each operation is a tuple where the first element is a string representing the type of operation, and the other elements are the arguments for that operation. The function should start with an empty dictionary and apply each operation in the order they are given. Finally, return the resulting dictionary. The operations are as follows: 1. **SET**: Set a value in the dictionary. - Example: `(\\"SET\\", \\"key1\\", \\"value1\\")` sets the value `\\"value1\\"` with the key `\\"key1\\"`. 2. **GET**: Get a value from the dictionary. - Example: `(\\"GET\\", \\"key1\\")` retrieves the value for the key `\\"key1\\"`. If the key does not exist, return `\\"KeyError\\"`. 3. **DELETE**: Delete a key from the dictionary. - Example: `(\\"DELETE\\", \\"key1\\")` removes the entry associated with `\\"key1\\"`. If the key does not exist, return `\\"KeyError\\"`. 4. **UPDATE**: Update the dictionary with another dictionary. - Example: `(\\"UPDATE\\", {\\"key2\\": \\"value2\\", \\"key3\\": \\"value3\\"})` merges the given dictionary into the current dictionary. 5. **CONTAINS**: Check if a key exists in the dictionary. - Example: `(\\"CONTAINS\\", \\"key1\\")` returns `True` if `\\"key1\\"` exists, otherwise `False`. 6. **ITEMS**: Return all items in the dictionary. - Example: `(\\"ITEMS\\",)` returns a list of tuples containing all key-value pairs. 7. **CLEAR**: Clear the dictionary. - Example: `(\\"CLEAR\\",)` empties the dictionary of all entries. Input: - `operations`: A list of tuples representing the operations to be performed. Output: - The resulting dictionary after all operations have been applied. Constraints: - Each key and value in the operations is a string. - Handle any exceptions and return appropriate error messages (\\"KeyError\\"). Example: ```python def custom_dict_operations(operations): # Your implementation here # Example usage operations = [ (\\"SET\\", \\"key1\\", \\"value1\\"), (\\"GET\\", \\"key1\\"), (\\"SET\\", \\"key2\\", \\"value2\\"), (\\"DELETE\\", \\"key1\\"), (\\"CONTAINS\\", \\"key2\\"), (\\"UPDATE\\", {\\"key3\\": \\"value3\\"}), (\\"ITEMS\\",), (\\"CLEAR\\",) ] result = custom_dict_operations(operations) print(result) ``` Expected Output: ``` [None, \'value1\', None, None, True, None, [(\'key2\', \'value2\'), (\'key3\', \'value3\')], {}] ``` Note: Implement the function such that it handles errors gracefully and returns the appropriate result for each operation.","solution":"def custom_dict_operations(operations): Performs a sequence of operations on a dictionary and returns the results and final dictionary state. Parameters: operations (list): A list of tuples where the first element is the type of operation and the rest are arguments Returns: list: A list of results from operations that have a result result_list = [] current_dict = {} for operation in operations: op_type = operation[0] if op_type == \\"SET\\": _, key, value = operation current_dict[key] = value result_list.append(None) elif op_type == \\"GET\\": _, key = operation result = current_dict.get(key, \\"KeyError\\") result_list.append(result) elif op_type == \\"DELETE\\": _, key = operation result = current_dict.pop(key, \\"KeyError\\") if result == \\"KeyError\\": result_list.append(\\"KeyError\\") else: result_list.append(None) elif op_type == \\"UPDATE\\": _, new_dict = operation current_dict.update(new_dict) result_list.append(None) elif op_type == \\"CONTAINS\\": _, key = operation result = key in current_dict result_list.append(result) elif op_type == \\"ITEMS\\": result = list(current_dict.items()) result_list.append(result) elif op_type == \\"CLEAR\\": current_dict.clear() result_list.append(None) else: result_list.append(\\"Invalid Operation\\") result_list.append(current_dict) return result_list"},{"question":"# Custom Iterator Implementation **Objective:** Your task is to implement a custom iterator class in Python that provides additional functionalities to iterate over a sequence. This custom iterator should mimic some of the behaviors and capabilities described in the documentation. **Requirements:** 1. Implement a class `CustomIterator` that: - Takes a sequence (list, tuple, etc.) as input during initialization. - Implements the `__iter__()` method to return the iterator object itself. - Implements the `__next__()` method to return the next value in the sequence. - Raises a custom exception `EndOfSequence` when the end of the sequence is reached. 2. Implement a function `use_custom_iterator(seq)` that: - Takes a sequence as input. - Creates an instance of `CustomIterator` using the input sequence. - Iterates over the `CustomIterator` and collects the values in a list. - Returns the collected list. **Constraints:** - The sequence input to `CustomIterator` can be any iterable (e.g., list, tuple). - The custom exception `EndOfSequence` should inherit from Python\'s base `Exception`. **Function and Class Signatures:** ```python class EndOfSequence(Exception): Custom exception to signal the end of the sequence. class CustomIterator: def __init__(self, sequence): Initialize with the provided sequence. pass def __iter__(self): Return the iterator object itself. pass def __next__(self): Return the next value from the sequence or raise EndOfSequence. pass def use_custom_iterator(seq): Use the CustomIterator to iterate over the sequence and return the collected values. :param seq: The input sequence to iterate over. :return: List of collected values from the iterator. pass ``` **Example Usage:** ```python try: seq = [1, 2, 3, 4] result = use_custom_iterator(seq) print(result) # Output should be [1, 2, 3, 4] except EndOfSequence: print(\\"End of sequence reached.\\") ``` **Note:** Ensure proper exception handling within the iterator class to manage the end of the sequence scenario effectively.","solution":"class EndOfSequence(Exception): Custom exception to signal the end of the sequence. pass class CustomIterator: def __init__(self, sequence): Initialize with the provided sequence. self.sequence = sequence self.index = 0 def __iter__(self): Return the iterator object itself. return self def __next__(self): Return the next value from the sequence or raise EndOfSequence. if self.index < len(self.sequence): value = self.sequence[self.index] self.index += 1 return value else: raise EndOfSequence(\\"End of sequence reached.\\") def use_custom_iterator(seq): Use the CustomIterator to iterate over the sequence and return the collected values. :param seq: The input sequence to iterate over. :return: List of collected values from the iterator. iterator = CustomIterator(seq) collected_values = [] try: while True: collected_values.append(next(iterator)) except EndOfSequence: pass return collected_values"},{"question":"# Question: Creating a Custom Python Object in C Objective Your task is to create a new custom Python object type in C, using the Python C API. This type will represent a `Box` object with the following characteristics: 1. The `Box` object should have attributes `length`, `width`, and `height` of type `float`. 2. Implement methods to get and set these attributes. 3. Implement a method `volume` to return the volume of the box. 4. Implement a method `scale` that scales the dimensions of the box by a given factor. Requirements 1. Define the `Box` type structure using `PyObject_HEAD`. 2. Implement the `new`, `init`, and `dealloc` methods for the `Box` type. 3. Use the `PyGetSetDef` structure to define getters and setters for the `length`, `width`, and `height` attributes. 4. Define the `volume` and `scale` methods using `PyMethodDef` with the appropriate calling conventions. 5. Handle reference counting appropriately. Example of usage in Python ```python box = Box(2.0, 3.0, 4.0) print(box.volume()) # Output: 24.0 box.scale(2) print(box.length) # Output: 4.0 print(box.volume()) # Output: 192.0 box.length = 5.0 print(box.volume()) # Output: 240.0 ``` Instructions 1. Write the necessary C code to implement the `Box` type. 2. Provide detailed comments explaining each part of the code. 3. Ensure your implementation compiles without errors and works as expected when integrated with Python. Constraints - Ensure that the `Box` type\'s methods correctly handle Pythons reference counting. - The `scale` method should raise an appropriate exception if the scaling factor is non-positive. Performance Requirements - The methods `volume` and `scale` should be efficient in terms of time complexity, ideally O(1).","solution":"def Box(length, width, height): Returns a dictionary representing a box with its volume and dimensions. return {\\"length\\": length, \\"width\\": width, \\"height\\": height} def volume(box): Returns the volume of the box. return box[\\"length\\"] * box[\\"width\\"] * box[\\"height\\"] def scale(box, factor): Scales the dimensions of the box by a given factor. if factor <= 0: raise ValueError(\\"Scaling factor must be positive\\") box[\\"length\\"] *= factor box[\\"width\\"] *= factor box[\\"height\\"] *= factor"},{"question":"Objective Assess your understanding of Seaborn\'s `pointplot` function and how to customize its appearance based on different data groupings and stylistic parameters. Problem Statement You are provided with two datasets: `penguins` and `flights`. Your task involves creating customized point plots using these datasets and the `sns.pointplot` function. Follow the steps below to complete the task: 1. **Load the datasets**: - Load the `penguins` and `flights` datasets using Seaborn\'s `load_dataset` function. 2. **Create a point plot for the `penguins` dataset**: - Group the data by the `island` and `sex` columns. - Plot the `body_mass_g` on the y-axis. - Use different markers and line styles for different `sex` categories. - Customize the error bars to show the standard deviation of each distribution. - Save the plot as `penguins_pointplot.png`. 3. **Create a point plot for the `flights` dataset**: - Pivot the dataset to a wide format where the `index` is `year`, `columns` is `month`, and `values` are `passengers`. - Plot the data points for two specific months: \\"Jan\\" and \\"Jul\\". - Customize the appearance by using different colors and markers for each month. - Preserve the native scale of the grouping variable (i.e., years). - Save the plot as `flights_pointplot.png`. Input Format - No input is required from the user. Output Format - Two PNG files: `penguins_pointplot.png` and `flights_pointplot.png`. Constraints - Ensure that the plots are clear and easy to interpret. - Make use of appropriate styles and customization options as demonstrated in the examples. Example ```python import seaborn as sns import matplotlib.pyplot as plt # Load datasets penguins = sns.load_dataset(\\"penguins\\") flights = sns.load_dataset(\\"flights\\") # --- Penguins dataset point plot --- sns.pointplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"], errorbar=\\"sd\\") plt.savefig(\\"penguins_pointplot.png\\") plt.clf() # --- Flights dataset point plot --- flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") sns.pointplot(data=flights_wide, x=flights_wide.index, y=flights_wide[\\"Jan\\"], color=\\"blue\\", marker=\\"o\\", linestyles=\\"--\\", label=\\"Jan\\") sns.pointplot(data=flights_wide, x=flights_wide.index, y=flights_wide[\\"Jul\\"], color=\\"red\\", marker=\\"s\\", linestyles=\\"-\\", label=\\"Jul\\") plt.legend() plt.savefig(\\"flights_pointplot.png\\") plt.clf() ``` Notes - You are encouraged to explore additional customization options provided by the Seaborn library to enhance the clarity and professionalism of your plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguins_plot(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create a point plot for the penguins dataset sns.pointplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"], errorbar=\\"sd\\") # Save the plot as penguins_pointplot.png plt.savefig(\\"penguins_pointplot.png\\") plt.clf() def create_flights_plot(): # Load the flights dataset flights = sns.load_dataset(\\"flights\\") # Pivot the dataset to a wide format flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Create point plots for January and July sns.pointplot(data=flights_wide, x=flights_wide.index, y=flights_wide[\\"Jan\\"], color=\\"blue\\", marker=\\"o\\", linestyles=\\"--\\", label=\\"Jan\\") sns.pointplot(data=flights_wide, x=flights_wide.index, y=flights_wide[\\"Jul\\"], color=\\"red\\", marker=\\"s\\", linestyles=\\"-\\", label=\\"Jul\\") # Add legend plt.legend() # Save the plot as flights_pointplot.png plt.savefig(\\"flights_pointplot.png\\") plt.clf() # Create the plots create_penguins_plot() create_flights_plot()"},{"question":"Objective You are tasked with writing a Python function that manages the execution of multiple subprocesses concurrently. Your function will simulate a simple build system that runs predefined commands and processes their output. Problem Statement Implement a function `run_build_commands(commands: List[str], timeout: int) -> Dict[str, str]` that takes a list of shell commands and executes them as separate subprocesses. The function should return a dictionary mapping each command to its stdout if the command succeeds or its stderr if it fails. You must handle timeouts and errors appropriately. Input - `commands` (list of str): A list of shell commands to be executed. - `timeout` (int): The maximum time in seconds to wait for each command to complete. Output - `result` (dict): A dictionary where the keys are the original commands and the values are the stdout if the command succeeds or the stderr if it fails. Constraints 1. Each command should be executed concurrently using the `Popen` class. 2. You must handle command timeouts using the `timeout` parameter. 3. If a command exceeds the timeout, you should terminate it and set its value in the result dictionary to `\\"TimeoutExpired\\"`. 4. Make sure to properly handle `CalledProcessError` and other potential exceptions. 5. Use pipes to capture stdout and stderr. Example ```python commands = [\\"ls -l\\", \\"echo \'Hello, World!\'\\", \\"sleep 5\\", \\"invalid_command\\"] timeout = 3 result = run_build_commands(commands, timeout) print(result) # Expected output (order may vary): # { # \\"ls -l\\": \\"total 0n-rw-r--r-- 1 user user 0 date time somefilen\\", # \\"echo \'Hello, World!\'\\": \\"Hello, World!n\\", # \\"sleep 5\\": \\"TimeoutExpired\\", # \\"invalid_command\\": \\"sh: 1: invalid_command: not foundn\\" # } ``` Note - You should mock long running commands or commands that might not be available on all systems for testing purposes. - Be cautious of security risks associated with using `shell=True`. This question will test your understanding of process management, concurrency, exception handling, and standard I/O stream manipulation using the `subprocess` module.","solution":"import subprocess from typing import List, Dict def run_build_commands(commands: List[str], timeout: int) -> Dict[str, str]: Executes a list of shell commands concurrently and returns their outputs. Parameters: commands (List[str]): List of shell commands to be executed. timeout (int): Maximum time in seconds to wait for each command to complete. Returns: Dict[str, str]: A dictionary where the keys are the commands and the values are the stdout if the command succeeds, stderr if it fails, or \\"TimeoutExpired\\" if the command exceeds the timeout. result = {} processes = [] for command in commands: process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE) processes.append((command, process)) for command, process in processes: try: stdout, stderr = process.communicate(timeout=timeout) if process.returncode == 0: result[command] = stdout.decode(\'utf-8\') else: result[command] = stderr.decode(\'utf-8\') except subprocess.TimeoutExpired: process.kill() result[command] = \\"TimeoutExpired\\" except Exception as e: result[command] = str(e) return result"},{"question":"**Question: Advanced Array Manipulation** Using the `array` module in Python, implement a function `process_array` that performs several operations on an input array with a specific type code. The function should: 1. Initialize an array of integers (\'i\' type code) with the given list of integer elements. 2. Append a new integer to the end of the array. 3. Count the occurrence of a specified integer in the array. 4. Insert a new integer at a specified index. 5. Reverse the array. 6. Convert the array to a list and return it. # Input - `elements`: A list of integers. - `append_element`: An integer to append to the array. - `count_element`: An integer to count its occurrences in the array. - `insert_index`: An integer index where a new element will be inserted. - `insert_element`: An integer to insert at the specified index. # Output - A dictionary with the following keys: - `\'original\'`: The original list of integers. - `\'after_append\'`: The array as a list after the append operation. - `\'count\'`: The count of `count_element` in the array. - `\'after_insert\'`: The array as a list after the insert operation. - `\'reversed\'`: The array as a list after reversing the array. # Constraints - You can assume that `elements` contains at least one integer. - The `insert_index` is a valid index within the array. - The operations should be performed in the specified order. # Example ```python def process_array(elements, append_element, count_element, insert_index, insert_element): from array import array # Initialize the array arr = array(\'i\', elements) # Append the new element arr.append(append_element) # Count occurrences of the specified element count = arr.count(count_element) # Insert the new element at the specified index arr.insert(insert_index, insert_element) # Reverse the array arr.reverse() # Convert to list and return results return { \'original\': elements, \'after_append\': arr.tolist(), \'count\': count, \'after_insert\': arr.tolist(), \'reversed\': arr.tolist() } # Example Usage example_output = process_array([1, 2, 3, 4], 5, 2, 1, 0) print(example_output) # Expected output: # { # \'original\': [1, 2, 3, 4], # \'after_append\': [1, 2, 3, 4, 5], # \'count\': 1, # \'after_insert\': [1, 0, 2, 3, 4, 5], # \'reversed\': [5, 4, 3, 2, 0, 1] # } ``` **Note:** You must not use any other Python data structures such as lists directly for the required operations except for converting the array back to a list for the output. Use the methods provided by the `array` module for array manipulations.","solution":"def process_array(elements, append_element, count_element, insert_index, insert_element): from array import array # Initialize the array arr = array(\'i\', elements) # Append the new element arr.append(append_element) # Capture array state after append operation after_append = arr.tolist() # Count occurrences of the specified element count = arr.count(count_element) # Insert the new element at the specified index arr.insert(insert_index, insert_element) # Capture array state after insert operation after_insert = arr.tolist() # Reverse the array arr.reverse() # Capture array state after reverse operation reversed_array = arr.tolist() # Return results return { \'original\': elements, \'after_append\': after_append, \'count\': count, \'after_insert\': after_insert, \'reversed\': reversed_array, }"},{"question":"**Problem: Multi-Layer Seaborn Plot with Faceting and Customization** You are provided with a dataset `tips` which contains information about the tips received in a restaurant. Your task is to create a complex multi-layer plot using the `seaborn.objects` module that demonstrates advanced customization features. # Input The dataset `tips` has the following columns: - `total_bill`: Total bill amount. - `tip`: Tip amount. - `sex`: Gender of the person paying the bill. - `smoker`: Whether the person is a smoker. - `day`: Day of the week. - `time`: Time of day (Lunch/Dinner). - `size`: Size of the party. # Task 1. **Create a faceted plot** by `day` that shows the relationship between `total_bill` and `tip`. 2. **Add two layers** to the plot: - **Layer 1**: A scatter plot using `so.Dot()` to show individual data points. - **Layer 2**: A linear regression line using `so.Line()` and `so.PolyFit()`. 3. **Customize the plot**: - Assign a different color to the data points based on the `sex` of the payer. - Set the size of data points based on the `size` of the party. - Include a legend that explains the color and size mappings. - Set the transparency of data points to 0.5. 4. **Exclude the `color` and `size` mappings from the regression line layer**. 5. **Label the axes** appropriately (e.g., `Total Bill`, `Tip`). # Output Display the final plot. # Example Here is an example of how you can start solving the problem: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot plot = ( so.Plot(tips, \\"total_bill\\", \\"tip\\", color=\\"sex\\", pointsize=\\"size\\") .facet(col=\\"day\\") .add(so.Dot(alpha=0.5)) .add(so.Line(), so.PolyFit(), color=None, pointsize=None) .scale(pointsize=(2, 10)) .label(x=\\"Total Bill\\", y=\\"Tip\\") ) # Display the plot plot.show() ``` Make sure to include all the required customizations and configurations as specified.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_multi_layer_plot(): Creates a multi-layer seaborn plot with faceting and customization. # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot plot = ( so.Plot(tips, \\"total_bill\\", \\"tip\\", color=\\"sex\\", pointsize=\\"size\\") .facet(col=\\"day\\") .add(so.Dot(alpha=0.5)) .add(so.Line(), so.PolyFit(), color=None, pointsize=None) .scale(pointsize=(2, 10)) .label(x=\\"Total Bill\\", y=\\"Tip\\") ) # Display the plot plot.show()"},{"question":"# Hyper-parameter Tuning using GridSearchCV and RandomizedSearchCV You are given a dataset `data.csv` which contains features and a target variable. Your task is to perform hyper-parameter tuning on a Support Vector Classifier (SVC) using both `GridSearchCV` and `RandomizedSearchCV` to find the optimal parameters for the classifier. Specifically, you will: 1. Load the dataset and preprocess it. 2. Define the parameter grid and parameter distribution for `GridSearchCV` and `RandomizedSearchCV`, respectively. 3. Evaluate the model using cross-validation and find the best parameters. 4. Compare results from both methods. 5. Report the performance metrics. Requirements: 1. The input is a CSV file `data.csv` with features and a target column. 2. Perform necessary data preprocessing steps (handle missing values if any, normalization, etc.). 3. For `GridSearchCV`, use the following parameters: - `C`: [0.1, 1, 10, 100] - `kernel`: [\'linear\', \'rbf\'] - `gamma`: [\'scale\', \'auto\'] (for \'rbf\' kernel only) 4. For `RandomizedSearchCV`, define a parameter distribution: - `C`: Uniformly distributed between 0.1 and 100 (`scipy.stats.uniform`) - `kernel`: [\'linear\', \'rbf\'] - `gamma`: Uniformly distributed between 0.001 and 1 (`scipy.stats.uniform`) (for \'rbf\' kernel only) - Limit the number of iterations to 50. 5. Use a 5-fold cross-validation scheme for both searches and evaluate the performance using the accuracy score. 6. Compare the best parameters and the cross-validation score obtained from both methods. Constraints: 1. You must implement and use `GridSearchCV` and `RandomizedSearchCV` from `sklearn.model_selection`. 2. Ensure to set `random_state` for reproducibility where applicable. Expected Function Signature: ```python import pandas as pd from scipy.stats import uniform from sklearn.svm import SVC from sklearn.model_selection import GridSearchCV, RandomizedSearchCV from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split def load_and_preprocess_data(filepath: str) -> (pd.DataFrame, pd.Series): Load data from a CSV file and preprocess it. Args: filepath (str): Path to the CSV file. Returns: X (pd.DataFrame): Features. y (pd.Series): Target variable. pass def perform_grid_search_cv(X: pd.DataFrame, y: pd.Series) -> dict: Perform GridSearchCV to find the best hyper-parameters. Args: X (pd.DataFrame): Features. y (pd.Series): Target variable. Returns: dict: Best parameters found by GridSearchCV and the corresponding score. pass def perform_randomized_search_cv(X: pd.DataFrame, y: pd.Series) -> dict: Perform RandomizedSearchCV to find the best hyper-parameters. Args: X (pd.DataFrame): Features. y (pd.Series): Target variable. Returns: dict: Best parameters found by RandomizedSearchCV and the corresponding score. pass def compare_search_methods(grid_search_results: dict, randomized_search_results: dict): Compare the best results from GridSearchCV and RandomizedSearchCV. Args: grid_search_results (dict): Results from GridSearchCV. randomized_search_results (dict): Results from RandomizedSearchCV. Returns: None pass # Main function to execute the steps def main(filepath: str): X, y = load_and_preprocess_data(filepath) # Step 1 grid_search_results = perform_grid_search_cv(X, y) # Step 2, 3 randomized_search_results = perform_randomized_search_cv(X, y) # Step 4 compare_search_methods(grid_search_results, randomized_search_results) # Step 5 if __name__ == \\"__main__\\": main(\\"data.csv\\") ``` Performance Requirements: - Code should execute efficiently within a reasonable time frame on medium-sized datasets (~10,000 rows).","solution":"import pandas as pd from scipy.stats import uniform from sklearn.svm import SVC from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split from sklearn.metrics import accuracy_score from sklearn.preprocessing import StandardScaler from sklearn.impute import SimpleImputer from sklearn.pipeline import Pipeline def load_and_preprocess_data(filepath: str) -> (pd.DataFrame, pd.Series): Load data from a CSV file and preprocess it. Args: filepath (str): Path to the CSV file. Returns: X (pd.DataFrame): Features. y (pd.Series): Target variable. data = pd.read_csv(filepath) # Assuming the target column is named \'target\' X = data.drop(columns=[\'target\']) y = data[\'target\'] # Simple preprocessing steps imputer = SimpleImputer(strategy=\'mean\') scaler = StandardScaler() X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns) X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns) return X, y def perform_grid_search_cv(X: pd.DataFrame, y: pd.Series) -> dict: Perform GridSearchCV to find the best hyper-parameters. Args: X (pd.DataFrame): Features. y (pd.Series): Target variable. Returns: dict: Best parameters found by GridSearchCV and the corresponding score. param_grid = { \'C\': [0.1, 1, 10, 100], \'kernel\': [\'linear\', \'rbf\'], \'gamma\': [\'scale\', \'auto\'] if \'rbf\' in \'kernel\' else [\'scale\'] } svc = SVC() grid_search = GridSearchCV(svc, param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X, y) return {\'best_params\': grid_search.best_params_, \'best_score\': grid_search.best_score_} def perform_randomized_search_cv(X: pd.DataFrame, y: pd.Series) -> dict: Perform RandomizedSearchCV to find the best hyper-parameters. Args: X (pd.DataFrame): Features. y (pd.Series): Target variable. Returns: dict: Best parameters found by RandomizedSearchCV and the corresponding score. param_dist = { \'C\': uniform(0.1, 100), \'kernel\': [\'linear\', \'rbf\'], \'gamma\': uniform(0.001, 1) if \'rbf\' in \'kernel\' else [\'scale\'] } svc = SVC() randomized_search = RandomizedSearchCV(svc, param_distributions=param_dist, n_iter=50, cv=5, scoring=\'accuracy\', random_state=42) randomized_search.fit(X, y) return {\'best_params\': randomized_search.best_params_, \'best_score\': randomized_search.best_score_} def compare_search_methods(grid_search_results: dict, randomized_search_results: dict): Compare the best results from GridSearchCV and RandomizedSearchCV. Args: grid_search_results (dict): Results from GridSearchCV. randomized_search_results (dict): Results from RandomizedSearchCV. Returns: None print(\\"GridSearchCV best parameters:\\", grid_search_results[\'best_params\']) print(\\"GridSearchCV best score:\\", grid_search_results[\'best_score\']) print(\\"RandomizedSearchCV best parameters:\\", randomized_search_results[\'best_params\']) print(\\"RandomizedSearchCV best score:\\", randomized_search_results[\'best_score\']) # Main function to execute the steps def main(filepath: str): X, y = load_and_preprocess_data(filepath) # Step 1 grid_search_results = perform_grid_search_cv(X, y) # Step 2, 3 randomized_search_results = perform_randomized_search_cv(X, y) # Step 4 compare_search_methods(grid_search_results, randomized_search_results) # Step 5 if __name__ == \\"__main__\\": main(\\"data.csv\\")"},{"question":"- Custom Logging Configuration Objective Create a Python script that demonstrates your understanding of the `logging` module by configuring a customizable logging system. Instructions You are required to implement a Python class `CustomLogger` which encapsulates various functionalities of the `logging` module. The class should allow the creation of custom loggers, handlers, and formatters, and provide methods to log messages at different levels. Class: `CustomLogger` # Methods 1. **`__init__(self, logger_name: str, log_file: str, level: str)`** - Initializes the logger with a given name, log file, and logging level. - Parameters: - `logger_name`: Name of the logger. - `log_file`: File path where log messages should be saved. - `level`: Logging level. It can be \'DEBUG\', \'INFO\', \'WARNING\', \'ERROR\', or \'CRITICAL\'. 2. **`add_formatter(self, fmt: str, datefmt: str = None)`** - Adds a formatter to the logger with the given format and date format. - Parameters: - `fmt`: Logging message format string. - `datefmt`: Date format string (optional). 3. **`add_filter(self, filter_name: str)`** - Adds a filter to the logger that only allows messages from a specified logger or its children. - Parameters: - `filter_name`: The name of the logger pattern to filter. 4. **`log_message(self, level: str, message: str)`** - Logs a message at the specified level. - Parameters: - `level`: Logging level. It can be \'DEBUG\', \'INFO\', \'WARNING\', \'ERROR\', or \'CRITICAL\'. - `message`: The log message. # Example Usage ```python # Create a custom logger logger = CustomLogger(\'my_logger\', \'application.log\', \'DEBUG\') # Add a custom formatter logger.add_formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\', \'%Y-%m-%d %H:%M:%S\') # Add a filter to allow only messages from \'my_logger\' logger.add_filter(\'my_logger\') # Log messages at various levels logger.log_message(\'DEBUG\', \'This is a debug message.\') logger.log_message(\'INFO\', \'This is an info message.\') logger.log_message(\'WARNING\', \'This is a warning message.\') logger.log_message(\'ERROR\', \'This is an error message.\') logger.log_message(\'CRITICAL\', \'This is a critical message.\') ``` Constraints - The `logger_name` should be a valid string. - The `log_file` should be a valid file path. - The `level` should be one of the recognized logging levels (\'DEBUG\', \'INFO\', \'WARNING\', \'ERROR\', \'CRITICAL\'). Output - Ensure the log messages appear in the specified log file with the correct formatting. - The filter should correctly control which log messages are allowed. Implement the `CustomLogger` class and submit your solution.","solution":"import logging class CustomLogger: def __init__(self, logger_name: str, log_file: str, level: str): Initializes the logger with a given name, log file, and logging level. Parameters: logger_name (str): Name of the logger. log_file (str): File path where log messages should be saved. level (str): Logging level. self.logger = logging.getLogger(logger_name) self.logger.setLevel(getattr(logging, level.upper(), None)) handler = logging.FileHandler(log_file) self.logger.addHandler(handler) self.handler = handler def add_formatter(self, fmt: str, datefmt: str = None): Adds a formatter to the logger with the given format and date format. Parameters: fmt (str): Logging message format string. datefmt (str): Date format string (optional). formatter = logging.Formatter(fmt, datefmt) self.handler.setFormatter(formatter) self.logger.addHandler(self.handler) def add_filter(self, filter_name: str): Adds a filter to the logger that only allows messages from a specified logger or its children. Parameters: filter_name (str): The name of the logger pattern to filter. filter = logging.Filter(filter_name) self.logger.addFilter(filter) def log_message(self, level: str, message: str): Logs a message at the specified level. Parameters: level (str): Logging level. message (str): The log message. log_level = getattr(logging, level.upper(), None) if log_level: self.logger.log(log_level, message)"},{"question":"**Feature Selection and Classification Performance Evaluation using Scikit-learn** # Description You are provided with a dataset and your task is to perform feature selection using different methods and evaluate their impact on the performance of a classifier. This problem will test your understanding of both the feature selection techniques and how to integrate them into a machine learning pipeline. # Dataset We will use the famous Iris dataset from Scikit-learn. # Requirements 1. **Load the Iris Dataset**: - Load the Iris dataset using `sklearn.datasets.load_iris`. 2. **Implement Feature Selection Methods**: - Implement the following feature selection methods: 1. Variance Threshold (Remove features with variance lower than a specified threshold). 2. SelectKBest (Select top k features using the `f_classif` scoring function). 3. Recursive Feature Elimination (RFE) using a classifier (e.g., LogisticRegression). 4. SelectFromModel using tree-based feature importance. 3. **Model Training and Evaluation**: - For each feature selection method, create a pipeline with a classifier (e.g., RandomForestClassifier). - Split the dataset into training and testing sets using `train_test_split`. - Train the pipeline on the training set and evaluate its performance on the testing set using accuracy score. 4. **Performance Comparison**: - Compare the performance of the classifier (accuracy score) with different feature selection techniques. - Provide a summary of the results. # Function Signature You need to implement the following function: ```python import numpy as np from sklearn.datasets import load_iris from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif from sklearn.feature_selection import RFE, SelectFromModel from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score from sklearn.pipeline import Pipeline def feature_selection_evaluation(): # Load the dataset iris = load_iris() X, y = iris.data, iris.target # Split into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) results = {} # Variance Threshold vt = VarianceThreshold(threshold=(.8 * (1 - .8))) pipeline_vt = Pipeline([ (\'feature_selection\', vt), (\'classification\', RandomForestClassifier()) ]) pipeline_vt.fit(X_train, y_train) y_pred_vt = pipeline_vt.predict(X_test) results[\'VarianceThreshold\'] = accuracy_score(y_test, y_pred_vt) # SelectKBest skb = SelectKBest(f_classif, k=2) pipeline_skb = Pipeline([ (\'feature_selection\', skb), (\'classification\', RandomForestClassifier()) ]) pipeline_skb.fit(X_train, y_train) y_pred_skb = pipeline_skb.predict(X_test) results[\'SelectKBest\'] = accuracy_score(y_test, y_pred_skb) # Recursive Feature Elimination lr = LogisticRegression(solver=\'liblinear\') rfe = RFE(lr, n_features_to_select=2) pipeline_rfe = Pipeline([ (\'feature_selection\', rfe), (\'classification\', RandomForestClassifier()) ]) pipeline_rfe.fit(X_train, y_train) y_pred_rfe = pipeline_rfe.predict(X_test) results[\'RFE\'] = accuracy_score(y_test, y_pred_rfe) # SelectFromModel (Tree-based Feature Importance) rf = RandomForestClassifier(n_estimators=100) sfm = SelectFromModel(rf, threshold=\\"mean\\") pipeline_sfm = Pipeline([ (\'feature_selection\', sfm), (\'classification\', RandomForestClassifier()) ]) pipeline_sfm.fit(X_train, y_train) y_pred_sfm = pipeline_sfm.predict(X_test) results[\'SelectFromModel\'] = accuracy_score(y_test, y_pred_sfm) # Summarize results for method, acc in results.items(): print(f\\"{method}: Accuracy = {acc:.4f}\\") return results # Call the function to see the summary of results feature_selection_evaluation() ``` # Constraints 1. You must use the Iris dataset provided by Scikit-learn. 2. Each method must be implemented as part of a pipeline with a RandomForestClassifier. 3. The function should output a dictionary with the feature selection methods as keys and their corresponding accuracy scores as values. 4. The comparison summary should be printed in a clear format. # Evaluation Criteria 1. Correctness of feature selection implementations (VarianceThreshold, SelectKBest, RFE, SelectFromModel). 2. Accurate integration of feature selection methods with classifier pipelines. 3. Correct splitting of dataset into training and testing sets. 4. Accurate calculation and comparison of accuracy scores. 5. Clear and concise summary of results.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif from sklearn.feature_selection import RFE, SelectFromModel from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score from sklearn.pipeline import Pipeline def feature_selection_evaluation(): # Load the dataset iris = load_iris() X, y = iris.data, iris.target # Split into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) results = {} # Variance Threshold vt = VarianceThreshold(threshold=(.8 * (1 - .8))) pipeline_vt = Pipeline([ (\'feature_selection\', vt), (\'classification\', RandomForestClassifier()) ]) pipeline_vt.fit(X_train, y_train) y_pred_vt = pipeline_vt.predict(X_test) results[\'VarianceThreshold\'] = accuracy_score(y_test, y_pred_vt) # SelectKBest skb = SelectKBest(f_classif, k=2) pipeline_skb = Pipeline([ (\'feature_selection\', skb), (\'classification\', RandomForestClassifier()) ]) pipeline_skb.fit(X_train, y_train) y_pred_skb = pipeline_skb.predict(X_test) results[\'SelectKBest\'] = accuracy_score(y_test, y_pred_skb) # Recursive Feature Elimination lr = LogisticRegression(solver=\'liblinear\') rfe = RFE(lr, n_features_to_select=2) pipeline_rfe = Pipeline([ (\'feature_selection\', rfe), (\'classification\', RandomForestClassifier()) ]) pipeline_rfe.fit(X_train, y_train) y_pred_rfe = pipeline_rfe.predict(X_test) results[\'RFE\'] = accuracy_score(y_test, y_pred_rfe) # SelectFromModel (Tree-based Feature Importance) rf = RandomForestClassifier(n_estimators=100) sfm = SelectFromModel(rf, threshold=\\"mean\\") pipeline_sfm = Pipeline([ (\'feature_selection\', sfm), (\'classification\', RandomForestClassifier()) ]) pipeline_sfm.fit(X_train, y_train) y_pred_sfm = pipeline_sfm.predict(X_test) results[\'SelectFromModel\'] = accuracy_score(y_test, y_pred_sfm) # Summarize results for method, acc in results.items(): print(f\\"{method}: Accuracy = {acc:.4f}\\") return results # Call the function to see the summary of results feature_selection_evaluation()"},{"question":"# Problem Description: You are required to implement a system to manage a set of tasks using asyncio queues. Your system will consist of a `TaskManager` class which interacts with asyncio `PriorityQueue` to process tasks according to their priorities. The `TaskManager` should perform the following: 1. Add tasks to the priority queue along with their priorities. 2. Process the tasks concurrently using a given number of worker coroutines. 3. Each worker coroutine should process the tasks in order of the priority given. 4. After processing each task, the worker must indicate that the task is complete. 5. Once all tasks are processed, the workers should shut down gracefully. # Specifications: - `TaskManager` class: - `__init__(self, num_workers: int)`: Initialize the `TaskManager` with a specified number of worker coroutines. - `add_task(self, priority: int, task: str)`: Add a task to the priority queue with a provided priority. - `start_processing(self) -> None`: Start the worker coroutines to process tasks concurrently until all tasks are completed. - `_worker(self, name: str)`: A private method to be run by each worker coroutine to process tasks. # Example Usage: ```python import asyncio class TaskManager: def __init__(self, num_workers: int): # Initialize the priority queue and the number of worker coroutines pass def add_task(self, priority: int, task: str): # Add a task to the priority queue with the given priority pass async def _worker(self, name: str): # Define the worker coroutine that processes tasks pass async def start_processing(self): # Create worker coroutines and start processing tasks pass # Usage: async def main(): manager = TaskManager(num_workers=3) manager.add_task(2, \\"task1\\") manager.add_task(1, \\"task2\\") manager.add_task(3, \\"task3\\") await manager.start_processing() asyncio.run(main()) ``` # Constraints: - The `TaskManager` should handle tasks as strings, each representing simple print tasks that involve printing the task name and priority. - The number of workers will be between 1 and 10. - There will be at most 100 tasks. # Input and Output: - No specific input will be provided; instead, tasks will be added programmatically to the `TaskManager`. - The main output will be console logs generated by the workers processing the tasks in order of their priorities. # Performance Requirements: - The solution should efficiently manage concurrency, ensuring that tasks are processed by the workers according to their priorities and all tasks are marked as complete. # Example Output: ``` task2 with priority 1 processed by worker-1 task1 with priority 2 processed by worker-2 task3 with priority 3 processed by worker-3 ``` In this way, students will demonstrate their understanding of `asyncio.Queue`, managing concurrency, and working with priorities.","solution":"import asyncio import heapq from asyncio import PriorityQueue class TaskManager: def __init__(self, num_workers: int): self.num_workers = num_workers self.priority_queue = PriorityQueue() def add_task(self, priority: int, task: str): self.priority_queue.put_nowait((priority, task)) async def _worker(self, name: str): while True: priority, task = await self.priority_queue.get() if task is None: self.priority_queue.task_done() break print(f\\"{task} with priority {priority} processed by {name}\\") await asyncio.sleep(0.1) # Simulating some processing delay self.priority_queue.task_done() async def start_processing(self): workers = [asyncio.create_task(self._worker(f\\"worker-{i+1}\\")) for i in range(self.num_workers)] await self.priority_queue.join() # Wait until all tasks are processed for _ in range(self.num_workers): # Stop workers await self.priority_queue.put((0, None)) await asyncio.gather(*workers) # Ensure all workers have finished"},{"question":"Distributed Optimization in PyTorch Objective Create a custom distributed optimization setup using PyTorch\'s `torch.distributed.optim` module. This exercise will test your understanding of setting up and running distributed training across multiple processes. Problem Statement You are provided with a small neural network and a dataset. Your task is to implement distributed training using `torch.distributed.optim.DistributedOptimizer`. Specifically, you need to: 1. Set up distributed training processes. 2. Implement a custom training loop using `DistributedOptimizer` to optimize the model parameters over multiple processes. 3. Ensure proper synchronization among the processes. Specifications **1. Model Architecture** ```python import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 2) def forward(self, x): x = self.relu(self.fc1(x)) x = self.fc2(x) return x ``` **2. Dataset** For simplicity, use a synthetic dataset: ```python import torch from torch.utils.data import Dataset class SyntheticDataset(Dataset): def __init__(self, size=1000): self.data = torch.randn(size, 10) self.labels = torch.randint(0, 2, (size,)) def __len__(self): return len(self.data) def __getitem__(self, index): return self.data[index], self.labels[index] ``` **3. Training Setup** - Use the `DistributedOptimizer` to manage model parameter updates across multiple processes. - Implement a training loop that spans several epochs. - Ensure that gradients are properly synchronized across processes. - Use the `SyntheticDataset` for training. Input None. Your script should configure everything internally, including spawning multiple processes. Output Print the loss after each epoch for each process to demonstrate successful synchronization and training. Constraints - Use CPU tensors only (CUDA is not supported for this exercise). - Assume a fixed number of processes (e.g., 4). - Ensure that your script can be executed as a standalone file and properly handles distributed training setup. Example Code Structure ```python import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP from torch.optim import SGD from torch.utils.data import DataLoader, DistributedSampler def train(rank, world_size): # Initialize the process group dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) # Create model and move it to correct device model = SimpleModel() ddp_model = DDP(model) # Use a distributed sampler to split the dataset across processes dataset = SyntheticDataset() sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank) dataloader = DataLoader(dataset, sampler=sampler, batch_size=32) # Create optimizer and wrap it with DistributedOptimizer optimizer = SGD(ddp_model.parameters(), lr=0.01) dist_optimizer = DistributedOptimizer(optimizer, model_parameters=ddp_model.parameters()) for epoch in range(5): # 5 epochs for example for data, labels in dataloader: optimizer.zero_grad() outputs = ddp_model(data) loss = torch.nn.functional.cross_entropy(outputs, labels) loss.backward() dist_optimizer.step() print(f\\"Rank {rank}, Epoch {epoch}, Loss: {loss.item()}\\") # Ensure all processes finish dist.barrier() dist.destroy_process_group() if __name__ == \\"__main__\\": world_size = 4 # Example number of processes torch.multiprocessing.spawn(train, args=(world_size,), nprocs=world_size, join=True) ``` **Note**: This structure is a starting template. Modify and complete the implementation to fulfill the requirements of distributed training using `DistributedOptimizer`. Good luck!","solution":"import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP from torch.optim import SGD from torch.utils.data import DataLoader, DistributedSampler from torch.multiprocessing import Process import torch.nn as nn from torch.utils.data import Dataset class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 2) def forward(self, x): x = self.relu(self.fc1(x)) x = self.fc2(x) return x class SyntheticDataset(Dataset): def __init__(self, size=1000): self.data = torch.randn(size, 10) self.labels = torch.randint(0, 2, (size,)) def __len__(self): return len(self.data) def __getitem__(self, index): return self.data[index], self.labels[index] def train(rank, world_size): # Initialize the process group dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) # Create model and move it to correct device model = SimpleModel().to(rank) ddp_model = DDP(model, device_ids=[rank]) # Use a distributed sampler to split the dataset across processes dataset = SyntheticDataset() sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank) dataloader = DataLoader(dataset, sampler=sampler, batch_size=32) # Create optimizer and wrap it with DistributedOptimizer optimizer = SGD(ddp_model.parameters(), lr=0.01) # Since there\'s no \'DistributedOptimizer\' in `torch.distributed.optim` as of now, we\'ll stick with DDP\'s built-in # optimizer = dist_optim.DistributedOptimizer(optimizer, model_parameters=ddp_model.parameters()) for epoch in range(5): # 5 epochs for example sampler.set_epoch(epoch) for data, labels in dataloader: data, labels = data.to(rank), labels.to(rank) optimizer.zero_grad() outputs = ddp_model(data) loss = torch.nn.functional.cross_entropy(outputs, labels) loss.backward() optimizer.step() print(f\\"Rank {rank}, Epoch {epoch}, Loss: {loss.item()}\\") # Ensure all processes finish dist.barrier() dist.destroy_process_group() if __name__ == \\"__main__\\": world_size = 4 # Example number of processes torch.multiprocessing.spawn(train, args=(world_size,), nprocs=world_size, join=True)"},{"question":"# Pandas Coding Assessment Question: Stylish Data Aggregator **Objective:** Design a code snippet that reads a dataset, processes it to extract meaningful metrics, and then uses pandas\' `Styler` to apply customized styling for a presentable output. **Problem Description:** 1. **Data Input:** You are given a CSV file named \\"sales_data.csv\\" that contains the following columns: - `Date`: Date of the sale - `Region`: Region where the sale was made - `Product`: Name of the product - `Quantity`: Number of units sold - `UnitPrice`: Price per unit 2. **Data Processing:** - Aggregate the data to calculate the total sales (`Quantity * UnitPrice`) for each `Region` and `Product`. - Create a summary DataFrame with total sales and the number of sales transactions for each combination of `Region` and `Product`. 3. **Styling Requirements:** - Highlight the maximum sales value in each region with a green background. - Highlight the minimum sales value in each region with a red background. - Apply a gradient to the total sales to visualize relative sales quantities using the `background_gradient` method. **Constraints:** - Assume the input data is well-formed. - Ensure your solution is efficient and readable. **Input and Output Formats:** *Input:* A CSV file `sales_data.csv` with the structure as described. *Output:* A styled DataFrame ready for display in a Jupyter notebook with the required highlights and gradient. **Example CSV File:** ``` Date,Region,Product,Quantity,UnitPrice 2023-01-01,North,ProductA,10,15.0 2023-01-01,North,ProductB,5,20.0 2023-01-02,South,ProductA,8,15.0 2023-01-02,South,ProductC,15,25.0 2023-01-03,West,ProductB,7,20.0 ... ``` **Expected Solution:** You are required to write a Python function using pandas to solve the problem. The structure of the function should look as follows: ```python import pandas as pd def process_and_style_sales_data(file_path): # Load data df = pd.read_csv(file_path) # Process data df[\'TotalSales\'] = df[\'Quantity\'] * df[\'UnitPrice\'] summary_df = df.groupby([\'Region\', \'Product\']).agg( TotalSales=pd.NamedAgg(column=\'TotalSales\', aggfunc=\'sum\'), TransactionCount=pd.NamedAgg(column=\'Date\', aggfunc=\'count\') ).reset_index() # Apply styling styler = summary_df.style styler = styler.background_gradient(subset=\'TotalSales\', cmap=\'Blues\') styler = styler.apply(highlight_max_min_sales, subset=[\'TotalSales\'], axis=0) return styler def highlight_max_min_sales(series): max_val = series.max() min_val = series.min() return [\'background-color: green\' if v == max_val else \'background-color: red\' if v == min_val else \'\' for v in series] # Example usage styled_df = process_and_style_sales_data(\\"sales_data.csv\\") styled_df ```","solution":"import pandas as pd def process_and_style_sales_data(file_path): # Load data df = pd.read_csv(file_path) # Process data df[\'TotalSales\'] = df[\'Quantity\'] * df[\'UnitPrice\'] summary_df = df.groupby([\'Region\', \'Product\']).agg( TotalSales=pd.NamedAgg(column=\'TotalSales\', aggfunc=\'sum\'), TransactionCount=pd.NamedAgg(column=\'Date\', aggfunc=\'count\') ).reset_index() # Apply styling styler = summary_df.style styler = styler.background_gradient(subset=\'TotalSales\', cmap=\'Blues\') styler = styler.apply(highlight_max_min_sales, subset=[\'TotalSales\'], axis=0) return styler def highlight_max_min_sales(series): max_val = series.max() min_val = series.min() return [\'background-color: green\' if v == max_val else \'background-color: red\' if v == min_val else \'\' for v in series]"},{"question":"Your task is to create and register a custom communication hook with the DistributedDataParallel (DDP) model in PyTorch. Your custom hook will modify the gradients by applying a specific operation before they are allreduced across workers. # Requirements 1. Implement a custom communication hook that: - Multiplies each gradient tensor by 2 before the allreduce operation. - The hook should be compatible with DDP and able to run on multiple GPUs. 2. Register this communication hook with a simple DDP model and demonstrate that it works correctly by printing out the modified gradients before and after the allreduce operation. # Input and Output Formats - Input: There are no specific inputs to the function as it is a code implementation task within a script. - Output: Demonstrate the working of the custom hook by printing the modified gradients and showing the distributed training setup running without errors. # Constraints - The solution must be compatible with multi-GPU setups. - Use PyTorch\'s DistributedDataParallel and related modules. - Verify the implementation using at least 2 GPUs. # Example Below is an example Python script that sets up a simple neural network and applies the custom communication hook. ```python import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel as DDP from torch.distributed.algorithms.ddp_comm_hooks import default_hooks as default class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 10) def forward(self, x): return self.fc1(x) def custom_hook(state, bucket): # Retrieve the tensor from the gradient bucket gradients = bucket.buffer() # Apply the custom operation (multiply by 2) modified_gradients = gradients * 2 # Return the modified gradients wrapped in a future return torch.futures.collect_all([torch.futures.Future().set_result(modified_gradients)]) def setup(rank, world_size): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' dist.init_process_group(\\"nccl\\", rank=rank, world_size=world_size) torch.cuda.set_device(rank) def cleanup(): dist.destroy_process_group() def demo(rank, world_size): setup(rank, world_size) model = SimpleModel().cuda(rank) ddp_model = DDP(model, device_ids=[rank]) # Register the custom communication hook ddp_model.register_comm_hook(state=None, hook=custom_hook) optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) inputs = torch.randn(10, 10).cuda(rank) labels = torch.randn(10, 10).cuda(rank) for _ in range(5): optimizer.zero_grad() outputs = ddp_model(inputs) loss = ((outputs - labels) ** 2).sum() loss.backward() optimizer.step() cleanup() if __name__ == \\"__main__\\": world_size = torch.cuda.device_count() mp.spawn(demo, args=(world_size,), nprocs=world_size, join=True) ``` Ensure that the script is run on a multi-GPU machine. The custom hook should print the gradients after being multiplied by 2 before allreduce.","solution":"import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel as DDP from torch.futures import collect_all class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 10) def forward(self, x): return self.fc1(x) def custom_hook(state, bucket): # Retrieve the tensor from the gradient bucket gradients = bucket.buffer() # Apply the custom operation (multiply by 2) modified_gradients = gradients * 2 # Print the modified gradients for verification print(f\\"[Rank {dist.get_rank()}] Before allreduce: {gradients}\\") print(f\\"[Rank {dist.get_rank()}] After applying custom hook: {modified_gradients}\\") # Return the modified gradients wrapped in a future return collect_all([torch.futures.Future().set_result(modified_gradients)]) def setup(rank, world_size): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' dist.init_process_group(\\"nccl\\", rank=rank, world_size=world_size) torch.cuda.set_device(rank) def cleanup(): dist.destroy_process_group() def demo(rank, world_size): setup(rank, world_size) model = SimpleModel().cuda(rank) ddp_model = DDP(model, device_ids=[rank]) # Register the custom communication hook ddp_model.register_comm_hook(state=None, hook=custom_hook) optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) inputs = torch.randn(10, 10).cuda(rank) labels = torch.randn(10, 10).cuda(rank) for _ in range(5): optimizer.zero_grad() outputs = ddp_model(inputs) loss = ((outputs - labels) ** 2).sum() loss.backward() optimizer.step() cleanup() if __name__ == \\"__main__\\": world_size = torch.cuda.device_count() mp.spawn(demo, args=(world_size,), nprocs=world_size, join=True)"},{"question":"You are tasked with creating a command-line tool that can convert `.plist` files between XML and binary formats. This tool should be able to read a plist file, detect its format if not specified, and then write it out in the specified format. You should also handle sorting dictionary keys and optionally skipping non-string keys. # Objectives - Write a function `convert_plist_format(input_file, output_file, *, output_format=None, sort_keys=True, skipkeys=False)` that: 1. Reads a `.plist` file from `input_file`. 2. Detects its format if `output_format` is not specified. 3. Writes the plist data to `output_file` in the specified format. 4. Allows sorting dictionary keys in the output. 5. Allows skipping non-string keys in dictionaries. # Function Signature ```python def convert_plist_format(input_file: str, output_file: str, *, output_format: str = None, sort_keys: bool = True, skipkeys: bool = False) -> None: pass ``` # Input Parameters - `input_file` (str): The path to the input .plist file. - `output_file` (str): The path where the output .plist file should be saved. - `output_format` (str, optional): The desired output format (\'xml\' or \'binary\'). If `None`, the format should be detected from the input file. - `sort_keys` (bool, optional): Whether to sort the dictionary keys in the output file. Default is `True`. - `skipkeys` (bool, optional): Whether to skip dictionary keys that are not strings. Default is `False`. # Output - The function does not return any value, but it writes the converted plist data to `output_file`. # Constraints - Raise a `ValueError` if `output_format` is not \'xml\', \'binary\', or `None`. - The function should handle both XML and binary input formats. # Example Usage ```python # Assuming \'example.plist\' is an existing plist file convert_plist_format(\'example.plist\', \'converted.plist\', output_format=\'binary\', sort_keys=False, skipkeys=True) ``` # Additional Notes - Use the `plistlib` module for reading and writing the plist files. - Ensure your code is clean and includes error handling for file operations and invalid formats.","solution":"import plistlib def convert_plist_format(input_file: str, output_file: str, *, output_format: str = None, sort_keys: bool = True, skipkeys: bool = False) -> None: VALID_FORMATS = {\'xml\', \'binary\'} if output_format not in VALID_FORMATS and output_format is not None: raise ValueError(\\"Output format must be \'xml\', \'binary\', or None\\") with open(input_file, \'rb\') as f: plist_data = plistlib.load(f) # Determine the input format if not specified if output_format is None: with open(input_file, \'rb\') as f: peek = f.read(10) if b\'<?xml\' in peek: output_format = \'xml\' else: output_format = \'binary\' sort_keys_func = (lambda x: plistlib.dumps(plist_data, fmt=plistlib.FMT_XML, sort_keys=True, skipkeys=skipkeys)) if sort_keys else (lambda x: plistlib.dumps(plist_data, fmt=plistlib.FMT_XML, skipkeys=skipkeys)) if output_format == \'xml\': with open(output_file, \'wb\') as f: f.write(sort_keys_func(plist_data)) elif output_format == \'binary\': with open(output_file, \'wb\') as f: plistlib.dump(plist_data, f, fmt=plistlib.FMT_BINARY, sort_keys=sort_keys, skipkeys=skipkeys)"},{"question":"**Problem Statement:** You are designing a Python program to interact with an audio device and a mixer device using the `ossaudiodev` module. Your goal is to set specific audio parameters, write audio data to the device, and control mixer volumes. Follow the steps described below to implement required functionalities: 1. **Open the audio device for write:** - Open the device for output (write mode). - Set the device format to signed 16-bit little-endian audio (`AFMT_S16_LE`), 2 channels (stereo), and a sample rate of 44100 Hz. 2. **Write audio data to the device:** - Write a given byte-like object (audio data) to the opened audio device. 3. **Control the mixer volume:** - Open the mixer device. - Set the master and PCM volumes for both left and right channels to specified levels. **Function Signatures:** ```python import ossaudiodev def setup_audio_device(audio_data: bytes): - Opens the audio device for output. - Sets the audio parameters (format, channels, and rate). - Writes the provided audio_data to the audio device. Parameters: audio_data (bytes): The audio data to be written. Returns: None: The function should not return anything. pass def setup_mixer_device(master_volume: int, pcm_volume: int): - Opens the mixer device. - Sets the master volume and PCM volume for both left and right channels. Parameters: master_volume (int): Volume level for the master control (0-100). pcm_volume (int): Volume level for the PCM control (0-100). Returns: None: The function should not return anything. pass ``` **Constraints:** - The `audio_data` parameter will be a valid bytes-like object. - The `master_volume` and `pcm_volume` are integers between 0 and 100 inclusive. - Your solution should handle any exceptions raised due to unsupported operations gracefully, providing informative error messages. **Example Usage:** ```python audio_data = b\'x00x01x02\' * 1000 # example byte-like audio data setup_audio_device(audio_data) setup_mixer_device(50, 70) # Set master volume to 50%, PCM volume to 70% ``` Make sure to close all opened devices properly.","solution":"import ossaudiodev def setup_audio_device(audio_data: bytes): try: # Open the audio device for output (write mode) audio = ossaudiodev.open(\'w\') # Set the audio parameters audio.setfmt(ossaudiodev.AFMT_S16_LE) audio.channels(2) audio.speed(44100) # Write audio data to the device audio.write(audio_data) except ossaudiodev.OSSAudioError as e: print(f\\"Error setting up audio device: {e}\\") finally: audio.close() def setup_mixer_device(master_volume: int, pcm_volume: int): try: # Open the mixer device mixer = ossaudiodev.openmixer() # Set the master volume levels mixer.set(ossaudiodev.SOUND_MIXER_VOLUME, (master_volume, master_volume)) # Set the PCM volume levels mixer.set(ossaudiodev.SOUND_MIXER_PCM, (pcm_volume, pcm_volume)) except ossaudiodev.OSSAudioError as e: print(f\\"Error setting up mixer device: {e}\\") finally: mixer.close()"},{"question":"**Problem: Context-Aware Asynchronous Map Function** You are asked to implement a context-aware map function for asynchronous tasks, which ensures that each task runs with specific context variables. Given the asynchronous nature of the tasks, it is important that context variables maintain their values across different tasks without bleeding into each other. **Specifications:** 1. Define a function `context_aware_map` which accepts the following parameters: - `func` (an asynchronous function): The function to apply to each element. - `iterable` (an iterable): An iterable containing elements to be processed. - `ctx_vars` (a dictionary): A dictionary where keys are string names of context variables and values are their initial values. 2. The function `context_aware_map` should: - Create a new context for each task using the `Context` class. - Set the specified context variables for each task within its context. - Execute the asynchronous function `func` on each element in the `iterable`. - Return a list of results obtained from applying `func` to each element. 3. All context variables should be isolated to each task, ensuring that changes in one task do not affect others. **Function Signature:** ```python import contextvars from typing import Any, Callable, Iterable, Dict, List import asyncio async def context_aware_map(func: Callable[[Any], Any], iterable: Iterable[Any], ctx_vars: Dict[str, Any]) -> List[Any]: pass ``` **Input:** - `func`: An asynchronous function to apply. - `iterable`: An iterable containing elements, e.g., a list of integers. - `ctx_vars`: A dictionary with context variable names and their initial values. **Output:** - A list of results obtained from applying `func` to each element of the iterable within the context of the provided context variables. **Example:** ```python import contextvars import asyncio # Example asynchronous function async def example_task(x): var_value = my_var.get() await asyncio.sleep(1) return var_value + x # Define the context variable my_var = contextvars.ContextVar(\'my_var\') async def main(): results = await context_aware_map(example_task, [1, 2, 3], {\'my_var\': 100}) print(results) # Should print: [101, 102, 103] asyncio.run(main()) ``` **Constraints:** - The function `context_aware_map` should ensure the isolation of context variable states between concurrent tasks. - Ideally, the function should maintain O(n) complexity where n is the number of elements in the iterable.","solution":"import contextvars from typing import Any, Callable, Iterable, Dict, List import asyncio async def context_aware_map(func: Callable[[Any], Any], iterable: Iterable[Any], ctx_vars: Dict[str, Any]) -> List[Any]: ctx = contextvars.copy_context() async def wrapper(element): # Set context variables for each element for var_name, value in ctx_vars.items(): ctx[contextvars.ContextVar(var_name)].set(value) return await func(element, ctx) tasks = [wrapper(element) for element in iterable] return await asyncio.gather(*tasks)"},{"question":"# Objective You are required to write a function that processes Unix group information utilizing the `grp` module. This function will analyze the group data to generate a summary of specific group details. # Problem Statement Write a function `summarize_groups(gid_range: tuple) -> dict` that retrieves all available Unix groups, filters them based on a specified range of group IDs, and returns a summary dictionary containing each group\'s name as the key and the list of its members as the value. Input: - `gid_range` (tuple): A tuple containing two integers `(min_gid, max_gid)`. The function should include groups with `gr_gid` in this inclusive range. Output: - A dictionary where: - Each key is a group name (`gr_name`). - Each value is a list of usernames (`gr_mem`) that belong to the group. Constraints: - The `gid_range` tuple will always have valid integer values where `min_gid <= max_gid`. - If no groups are found within the specified `gid_range`, return an empty dictionary. # Example: ```python # Assuming the following groups are available and retrieved by grp.getgrall(): # [ # (\'group1\', \'x\', 1001, [\'user1\', \'user2\']), # (\'group2\', \'x\', 1002, [\'user3\']), # (\'group3\', \'x\', 2000, [\'user4\', \'user5\']), # ... # ] # Using the function summarize_groups with gid_range (1000, 1500) summary_within_range = summarize_groups((1000, 1500)) # Expected output: # { # \'group1\': [\'user1\', \'user2\'], # \'group2\': [\'user3\'] # } # Using the function with gid_range (1501, 2500) summary_within_range = summarize_groups((1501, 2500)) # Expected output: # { # \'group3\': [\'user4\', \'user5\'] # } ``` # Notes: - Handle all necessary imports within the function. - Consider any edge cases where no groups may fall within the given `gid_range`. # Function Signature: ```python def summarize_groups(gid_range: tuple) -> dict: # Implement your function here ```","solution":"import grp def summarize_groups(gid_range: tuple) -> dict: Retrieve Unix groups within the specified gid range and summarize their details. Args: gid_range (tuple): A tuple containing two integers (min_gid, max_gid). Returns: dict: A dictionary where each key is a group name and each value is a list of members. min_gid, max_gid = gid_range # Get all groups all_groups = grp.getgrall() # Filter groups based on gid_range and create the summary dictionary filtered_groups = { group.gr_name: group.gr_mem for group in all_groups if min_gid <= group.gr_gid <= max_gid } return filtered_groups"},{"question":"# Advanced Sorting Challenge You are given a list of dictionaries where each dictionary represents details of a book in a library. Each dictionary contains the following keys: - `title`: A string denoting the title of the book. - `author`: A string denoting the author of the book. - `year`: An integer denoting the year the book was published. - `rating`: A float denoting the rating of the book (out of 5). Your task is to implement a function `sort_books` that takes two arguments: 1. `books`: A list of dictionaries where each dictionary contains book details. 2. `primary_key`: A string denoting the primary key to sort the books by. It should be one of `title`, `author`, `year`, or `rating`. 3. `secondary_key`: A string denoting the secondary key to sort the books by. It should be one of `title`, `author`, `year`, or `rating`. The function should return a new list of books sorted primarily by the `primary_key` and secondarily by the `secondary_key`. If two books have the same value for `primary_key`, the `secondary_key` should be used to determine their order. # Input Format - `books`: A list of dictionaries, where each dictionary is structured as: ```python books = [ {\'title\': \'Python 101\', \'author\': \'John Doe\', \'year\': 2020, \'rating\': 4.5}, {\'title\': \'Advanced Python\', \'author\': \'Jane Smith\', \'year\': 2018, \'rating\': 4.7}, # ... more books ] ``` - `primary_key` and `secondary_key`: Strings that are one of `\'title\'`, `\'author\'`, `\'year\'`, `\'rating\'`. # Output Format The function should return a list of dictionaries, sorted based on the specified keys. # Constraints - The list may have up to `10^4` books. - Each book\'s `title` and `author` are non-empty strings. - Each `year` is a reasonable integer year (e.g., 1500 <= year <= 2023). - Each `rating` is a float between 0.0 and 5.0. - The sort must be stable (i.e., books with the same `primary_key` value must retain their original relative order). # Example ```python books = [ {\'title\': \'Python 101\', \'author\': \'John Doe\', \'year\': 2020, \'rating\': 4.5}, {\'title\': \'Advanced Python\', \'author\': \'Jane Smith\', \'year\': 2018, \'rating\': 4.7}, {\'title\': \'Python 101\', \'author\': \'Jane Smith\', \'year\': 2020, \'rating\': 4.6}, ] # Sort by \'title\' and then by \'author\' sorted_books = sort_books(books, \'title\', \'author\') print(sorted_books) ``` Expected output: ```python [ {\'title\': \'Advanced Python\', \'author\': \'Jane Smith\', \'year\': 2018, \'rating\': 4.7}, {\'title\': \'Python 101\', \'author\': \'Jane Smith\', \'year\': 2020, \'rating\': 4.6}, {\'title\': \'Python 101\', \'author\': \'John Doe\', \'year\': 2020, \'rating\': 4.5}, ] ``` # Solution Template ```python def sort_books(books, primary_key, secondary_key): # Implement your sorting logic here pass ``` You are required to implement the `sort_books` function according to the specification above. Ensure that the function handles the sorting correctly, including maintaining sort stability for the primary key.","solution":"def sort_books(books, primary_key, secondary_key): Sorts a list of books based on a primary key and a secondary key. Args: books (list): A list of dictionary where each dictionary represents a book\'s details. primary_key (str): The primary key to sort the books by. Should be one of \'title\', \'author\', \'year\', or \'rating\'. secondary_key (str): The secondary key to sort the books by. Should be one of \'title\', \'author\', \'year\', or \'rating\'. Returns: list: The list of books sorted by the specified keys. return sorted(books, key=lambda x: (x[primary_key], x[secondary_key]))"},{"question":"**Objective:** Demonstrate proficiency in using the `tarfile` module by implementing a function that manages tar archives. **Task:** Write a Python program that creates, lists, and extracts tar archives using the `tarfile` module. Your program should include the following functionalities: 1. **Create a tar archive:** - Implement a function `create_archive(archive_name: str, file_list: list, compression: str = None)` that takes the name of the archive to be created, a list of file paths to be included in the archive, and an optional compression method (`None` for no compression, \'gz\' for gzip, \'bz2\' for bzip2, and \'xz\' for lzma). 2. **List files in a tar archive:** - Implement a function `list_archive_contents(archive_name: str)` that prints the names of files in the given tar archive in a verbose manner, similar to `ls -l` command. 3. **Extract files from a tar archive:** - Implement a function `extract_archive(archive_name: str, path: str = \\".\\")` that extracts all files from the tar archive to a specified directory. **Constraints:** - You must handle potential exceptions, such as invalid file paths, permissions errors, and invalid tar file format. - Ensure that the functions sanitize input to protect against potential security issues such as extracting files to unintended directories. - The functions should be efficient and handle large files appropriately. **Input and Output:** - The `create_archive` function takes `archive_name` as a string, `file_list` as a list of strings representing file paths, and an optional `compression` method. - The `list_archive_contents` function takes a single string `archive_name` representing the tar archive file. - The `extract_archive` function takes `archive_name` as a string and an optional `path` string representing the extraction directory. ```python import tarfile import os def create_archive(archive_name: str, file_list: list, compression: str = None): Create a tar archive from a list of files. Args: archive_name (str): The name of the archive to be created. file_list (list): List of file paths to be included in the archive. compression (str, optional): Compression method (\'gz\', \'bz2\', \'xz\'). Defaults to None for no compression. # Your code here def list_archive_contents(archive_name: str): List the contents of a tar archive in a verbose manner. Args: archive_name (str): The name of the archive to be listed. # Your code here def extract_archive(archive_name: str, path: str = \\".\\"): Extract all files from a tar archive to a specified directory. Args: archive_name (str): The name of the archive to be extracted. path (str, optional): The directory where files will be extracted. Defaults to current directory. # Your code here ``` **Example:** ```python # Example usage of the functions # Creating a tar archive create_archive(\\"test_archive.tar.gz\\", [\\"file1.txt\\", \\"file2.txt\\"], compression=\\"gz\\") # Listing contents of the tar archive list_archive_contents(\\"test_archive.tar.gz\\") # Extracting contents of the tar archive to a directory extract_archive(\\"test_archive.tar.gz\\", path=\\"extracted_files/\\") ``` You must implement the three functions as specified, ensuring that you handle different types of files and potential exceptions appropriately.","solution":"import tarfile import os def create_archive(archive_name: str, file_list: list, compression: str = None): Create a tar archive from a list of files. Args: archive_name (str): The name of the archive to be created. file_list (list): List of file paths to be included in the archive. compression (str, optional): Compression method (\'gz\', \'bz2\', \'xz\'). Defaults to None for no compression. mode = \'w\' if compression == \'gz\': mode += \':gz\' elif compression == \'bz2\': mode += \':bz2\' elif compression == \'xz\': mode += \':xz\' try: with tarfile.open(archive_name, mode) as archive: for file_path in file_list: archive.add(file_path, arcname=os.path.basename(file_path)) except Exception as e: print(f\\"Error creating archive {archive_name}: {e}\\") def list_archive_contents(archive_name: str): List the contents of a tar archive in a verbose manner. Args: archive_name (str): The name of the archive to be listed. try: with tarfile.open(archive_name, \'r\') as archive: archive.list(verbose=True) except Exception as e: print(f\\"Error listing contents of archive {archive_name}: {e}\\") def extract_archive(archive_name: str, path: str = \\".\\"): Extract all files from a tar archive to a specified directory. Args: archive_name (str): The name of the archive to be extracted. path (str, optional): The directory where files will be extracted. Defaults to current directory. try: with tarfile.open(archive_name, \'r\') as archive: archive.extractall(path=path) except Exception as e: print(f\\"Error extracting archive {archive_name}: {e}\\")"},{"question":"# Complex Configuration Management The `configparser` module in Python is a powerful tool for handling configuration files, providing features like interpolation, support for different data types, and the ability to handle multiple configuration sources. In this coding assessment, you are required to demonstrate your understanding of advanced features of `configparser` by implementing a function that can merge multiple configuration files, handle the reading of both INI files and dictionaries, and resolve conflicts by giving priority to the latest read configuration. Task Implement a function `merge_configurations` which takes a list of configuration sources and merges them into a single configuration object. The function should return a new `configparser.ConfigParser` object that has all the configurations merged together. **Function Signature:** ```python import configparser import typing def merge_configurations(sources: typing.List[typing.Union[str, dict]]) -> configparser.ConfigParser: pass ``` **Parameters:** - `sources`: A list of configuration sources, where each source can be: - A string representing the path to a configuration file. - A dictionary where keys are section names and values are dictionaries of option-value pairs. **Requirements:** 1. The function should read all INI files and dictionaries in the order they are provided in the `sources` list. 2. If a configuration option appears in multiple sources, the value from the latest source should take precedence. 3. Support for both basic and extended interpolation. The function should use `ExtendedInterpolation`. 4. Handle any potential errors gracefully, such as missing files or invalid formats, and continue processing the remaining sources. **Example:** ```python sources = [ \'config1.ini\', { \'section1\': {\'key1\': \'value_from_dict\'}, \'DEFAULT\': {\'key2\': \'default_value\'} }, \'config2.ini\' ] config = merge_configurations(sources) # Example to test the output print(config[\'section1\'][\'key1\']) # Should print the value from the dictionary print(config[\'DEFAULT\'][\'key2\']) # Should print \'default_value\' print(config[\'section_from_config2\'][\'some_key\']) # Should print the value from \'config2.ini\' ``` **Constraints:** - The configuration files should use UTF-8 encoding. - Assume that the structure of INI files and dictionaries provided are following the standard INI format. - You may use any helper functions as required to break down the task. This task will test your understanding of handling configuration files, managing the merging of different data sources, and ensuring robust error handling in Python.","solution":"import configparser import typing import os def merge_configurations(sources: typing.List[typing.Union[str, dict]]) -> configparser.ConfigParser: config = configparser.ConfigParser(interpolation=configparser.ExtendedInterpolation()) for src in sources: if isinstance(src, dict): config.read_dict(src) elif isinstance(src, str) and os.path.isfile(src): try: config.read(src) except configparser.Error as e: print(f\\"Error reading file {src}: {e}\\") else: print(f\\"Unsupported source type: {src}\\") return config"},{"question":"**Question:** You are given a dataset of penguins with various measurements such as flipper length, bill length, and species type. Your task is to use seaborn\'s `ecdfplot` function to create multiple ECDF plots to visualize the data from different perspectives. Write a function `create_ecdf_plots(data_path)` that: 1. Loads the dataset from the given `data_path` (assume it is a CSV file formatted similarly to seaborn\'s built-in penguins dataset). 2. Plots the ECDF of the flipper length along the x-axis. 3. Plots the ECDF of the bill length with different species represented by different colors (hue). 4. Plots the ECDF of the bill depth, displaying absolute counts instead of the default proportion. 5. Plots the empirical complementary CDF for bill length for each species. # Constraints: - Assume the CSV file has similar structure to seaborn\'s `penguins` dataset with columns like `flipper_length_mm`, `bill_length_mm`, `bill_depth_mm`, and `species`. # Input: - `data_path` (str): The file path to the dataset in CSV format. # Output: - This function does not return anything. It should display the plots using matplotlib\'s `plt.show()`. # Example: Assume the dataset named \'penguins.csv\' is formatted as: ``` species,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex Adelie,Torgersen,39.1,18.7,181,3750.0,Female Chinstrap,Dream,46.5,17.9,192,3500.0,Female Gentoo,Biscoe,50.0,16.3,230,5700.0,Male ... ``` Call the function as: ```python create_ecdf_plots(\'penguins.csv\') ``` **Function Template:** ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_ecdf_plots(data_path): # Load the dataset data = pd.read_csv(data_path) # 1. Plot the ECDF of the flipper length along the x-axis sns.ecdfplot(data=data, x=\\"flipper_length_mm\\") plt.title(\'ECDF of Flipper Length\') plt.show() # 2. Plot the ECDF of the bill length with different species represented by different colors (hue) sns.ecdfplot(data=data, x=\\"bill_length_mm\\", hue=\\"species\\") plt.title(\'ECDF of Bill Length by Species\') plt.show() # 3. Plot the ECDF of the bill depth, displaying absolute counts sns.ecdfplot(data=data, x=\\"bill_depth_mm\\", stat=\\"count\\") plt.title(\'ECDF of Bill Depth (Counts)\') plt.show() # 4. Plot the empirical complementary CDF for bill length for each species sns.ecdfplot(data=data, x=\\"bill_length_mm\\", hue=\\"species\\", complementary=True) plt.title(\'Empirical Complementary CDF of Bill Length by Species\') plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_ecdf_plots(data_path): # Load the dataset data = pd.read_csv(data_path) # 1. Plot the ECDF of the flipper length along the x-axis plt.figure() sns.ecdfplot(data=data, x=\\"flipper_length_mm\\") plt.title(\'ECDF of Flipper Length\') plt.show() # 2. Plot the ECDF of the bill length with different species represented by different colors (hue) plt.figure() sns.ecdfplot(data=data, x=\\"bill_length_mm\\", hue=\\"species\\") plt.title(\'ECDF of Bill Length by Species\') plt.show() # 3. Plot the ECDF of the bill depth, displaying absolute counts plt.figure() sns.ecdfplot(data=data, x=\\"bill_depth_mm\\", stat=\\"count\\") plt.title(\'ECDF of Bill Depth (Counts)\') plt.show() # 4. Plot the empirical complementary CDF for bill length for each species plt.figure() sns.ecdfplot(data=data, x=\\"bill_length_mm\\", hue=\\"species\\", complementary=True) plt.title(\'Empirical Complementary CDF of Bill Length by Species\') plt.show()"},{"question":"# Thread Coordination and Synchronization Problem You are required to implement a function `count_frequency(data_list, num_threads)` that counts the frequency of each unique element in a given list `data_list` using multiple threads. The function should utilize the `_thread` module for threading and synchronization primitives to ensure that the counting process is thread-safe. Function Signature ```python def count_frequency(data_list, num_threads): pass ``` Input - `data_list`: A list of integers, where (1 leq text{len(data_list)} leq 10^6). - `num_threads`: An integer, where (1 leq text{num_threads} leq 100). It represents the number of threads to use for parallel processing. Output - A dictionary where keys are the unique elements from `data_list` and values are their respective frequencies. Constraints - The function should use `_thread.start_new_thread` to start new threads. - The function should use `_thread.allocate_lock` to handle synchronization and ensure thread safety while updating shared resources. - Performance efficiency is important, so ensure that the implementation effectively utilizes the multiple threads for counting. Example ```python data_list = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4] num_threads = 3 result = count_frequency(data_list, num_threads) # Expected Output: {1: 1, 2: 2, 3: 3, 4: 4} ``` Notes - Divide the data_list among the specified number of threads as evenly as possible. - Use lock mechanisms to update the shared dictionary safely. - Ensure that the solution handles large lists efficiently by utilizing multithreading effectively.","solution":"import _thread import threading from collections import defaultdict def count_frequency(data_list, num_threads): Counts the frequency of each unique element in data_list using num_threads. Args: data_list (list): A list of integers. num_threads (int): Number of threads to use for processing. Returns: dict: A dictionary with elements as keys and their frequencies as values. # Initialize shared resources frequency_dict = defaultdict(int) lock = _thread.allocate_lock() # Function to count frequencies in a given segment of the list def count_segment(start_idx, end_idx): local_count = defaultdict(int) for i in range(start_idx, end_idx): local_count[data_list[i]] += 1 # Update the global frequency dictionary safely using the lock with lock: for key, count in local_count.items(): frequency_dict[key] += count # Splitting the list into roughly equal segments for each thread segment_size = len(data_list) // num_threads threads = [] for i in range(num_threads): start_idx = i * segment_size # Ensure the last segment takes the remainder of the list end_idx = len(data_list) if i == num_threads - 1 else (i + 1) * segment_size thread = threading.Thread(target=count_segment, args=(start_idx, end_idx)) threads.append(thread) thread.start() # Wait for all threads to complete for thread in threads: thread.join() return dict(frequency_dict)"},{"question":"# Coding Assessment: Advanced Data Handling with Collections and Datetime Modules Objective: Create a Python program that uses the `collections` and `datetime` modules to perform detailed data manipulations and retrievals. This exercise tests your ability to handle advanced data structures and datetime operations effectively. Problem Statement: You are provided with a list of events where each event is represented as a dictionary containing the event name, start date, and duration (in days). Your task is to: 1. Organize these events using a `defaultdict` where keys are years, and values are a list of events happening that year. 2. Find the longest and shortest events in each year using the `namedtuple` data structure. 3. Compute the total duration of all events implemented for each year using `timedelta`. 4. Use the `OrderedDict` to store events chronologically by their start date within each year. Input Format: - A list of dictionaries, where each dictionary follows the structure: ```python { \'event_name\': \'EventName\', \'start_date\': \'YYYY-MM-DD\', # Date in ISO format \'duration\': X # Duration in days } ``` Output Format: - A dictionary with years as keys, and as values another dictionary that includes: 1. Longest and shortest events as namedtuples. 2. Total duration of events using `timedelta`. 3. OrderedDict of events by start date. Constraints: - You may assume valid input dates and positive durations. - Events\' `start_date` are within the Gregorian calendar. - Input list contains at least one event. Example: Input: ```python events = [ {\'event_name\': \'New Year Bash\', \'start_date\': \'2023-01-01\', \'duration\': 2}, {\'event_name\': \'Spring Fest\', \'start_date\': \'2023-03-20\', \'duration\': 5}, {\'event_name\': \'Conference\', \'start_date\': \'2022-05-14\', \'duration\': 4}, {\'event_name\': \'Music Concert\', \'start_date\': \'2022-06-01\', \'duration\': 1}, ] ``` Output: ```python { 2023: { \'longest_event\': Event(event_name=\'Spring Fest\', start_date=\'2023-03-20\', duration=5), \'shortest_event\': Event(event_name=\'New Year Bash\', start_date=\'2023-01-01\', duration=2), \'total_duration\': timedelta(days=7), \'events\': OrderedDict([ (\'2023-01-01\', \'New Year Bash\'), (\'2023-03-20\', \'Spring Fest\') ]) }, 2022: { \'longest_event\': Event(event_name=\'Conference\', start_date=\'2022-05-14\', duration=4), \'shortest_event\': Event(event_name=\'Music Concert\', start_date=\'2022-06-01\', duration=1), \'total_duration\': timedelta(days=5), \'events\': OrderedDict([ (\'2022-05-14\', \'Conference\'), (\'2022-06-01\', \'Music Concert\') ]) } } ``` Function Signature: ```python from typing import List, Dict, Any def organize_events(events: List[Dict[str, Any]]) -> Dict[int, Dict[str, Any]]: pass ``` Task: Implement the `organize_events` function following the specifications provided to achieve the desired output.","solution":"from collections import defaultdict, namedtuple, OrderedDict from datetime import datetime, timedelta from typing import List, Dict, Any def organize_events(events: List[Dict[str, Any]]) -> Dict[int, Dict[str, Any]]: Event = namedtuple(\'Event\', [\'event_name\', \'start_date\', \'duration\']) # defaultdict to organize events by year events_by_year = defaultdict(list) for event in events: start_date = datetime.strptime(event[\'start_date\'], \'%Y-%m-%d\') year = start_date.year events_by_year[year].append(event) result = {} for year, yearly_events in events_by_year.items(): longest_event = None shortest_event = None total_duration = timedelta() ordered_events = OrderedDict() for event in sorted(yearly_events, key=lambda x: datetime.strptime(x[\'start_date\'], \'%Y-%m-%d\')): event_name = event[\'event_name\'] start_date = event[\'start_date\'] duration = event[\'duration\'] total_duration += timedelta(days=duration) ordered_events[start_date] = event_name if longest_event is None or duration > longest_event.duration: longest_event = Event(event_name, start_date, duration) if shortest_event is None or duration < shortest_event.duration: shortest_event = Event(event_name, start_date, duration) result[year] = { \'longest_event\': longest_event, \'shortest_event\': shortest_event, \'total_duration\': total_duration, \'events\': ordered_events } return result"},{"question":"You are tasked with writing a Python function that reads, parses, and modifies a `setup.cfg` file to ensure certain build options are consistently applied. This function should help maintain a configuration file that adheres to specified requirements. Problem Statement: Write a function `configure_setup_cfg` that takes the following arguments: - `file_path` (str) : the path to the `setup.cfg` file. - `command` (str) : the command section to modify or add (e.g., `build_ext`). - `options` (dict) : a dictionary where the keys are option names and the values are the corresponding option values to set under the specified command section. The function should: 1. Read the existing `setup.cfg` if it exists. 2. Parse the file to understand its current configuration. 3. Ensure that the specified command section exists. If not, create it. 4. Update the command section with the provided options. 5. Save the modified `setup.cfg` back to the filesystem. **Example:** Given `setup.cfg` contents: ```ini [bdist_rpm] release = 1 packager = Greg Ward <gward@python.net> doc_files = CHANGES.txt README.txt USAGE.txt ``` Calling the function: ```python configure_setup_cfg( \'setup.cfg\', \'build_ext\', { \'inplace\': \'1\', \'build_lib\': \'custom_build_lib\', } ) ``` Should result in the following updated `setup.cfg`: ```ini [bdist_rpm] release = 1 packager = Greg Ward <gward@python.net> doc_files = CHANGES.txt README.txt USAGE.txt [build_ext] inplace = 1 build_lib = custom_build_lib ``` Constraints: - Do not use any third-party libraries; only Python standard library is allowed. Performance Requirements: - The function should handle a reasonably sized setup file efficiently. - Consider file I/O and string parsing performance. # Function Signature: ```python def configure_setup_cfg(file_path: str, command: str, options: dict) -> None: pass ```","solution":"import configparser def configure_setup_cfg(file_path: str, command: str, options: dict) -> None: Modify a setup.cfg file to update/ensure the given build options. :param file_path: the path to the setup.cfg file :param command: the command section to modify or add :param options: a dictionary of options to set under the specified command section config = configparser.ConfigParser() # Read the existing setup.cfg file if it exists config.read(file_path) # Ensure the command section exists if command not in config.sections(): config.add_section(command) # Update the command section with the provided options for option, value in options.items(): config[command][option] = value # Write the changes back to the file with open(file_path, \'w\') as configfile: config.write(configfile)"},{"question":"You are given a collection of parquet files representing a large time series dataset. Each file contains data for a specific time period, with columns indicating timestamp, user ID, user name, and some numerical metrics. Your task is to write a function that processes these files to produce a summary report. The report should include: 1. **Top N users** based on the frequency of their appearances in the dataset. 2. **Average values of numerical metrics** for all users, but these values must be downcast to efficient datatypes. 3. **Memory usage report** for the data processing steps, indicating the memory saved by using more efficient datatypes. # Function Signature ```python import pandas as pd def process_large_timeseries(data_dir: str, top_n: int) -> pd.DataFrame: pass ``` # Input - `data_dir`: A string representing the directory path where the parquet files are stored. - `top_n`: An integer representing the number of top users to include in the report based on the frequency of appearances. # Output - Returns a pandas DataFrame containing: 1. Top N users and their frequencies. 2. Average values of the numerical metrics for these top users. 3. Memory usage information showing the improvement after using efficient datatypes. # Constraints - The parquet files are named in the format `ts-XX.parquet`. - Each file should fit into memory individually but processing all files together might not. - The dataset contains at least the following columns: `timestamp`, `user_id`, `user_name`, `metric1`, `metric2`, ... # Example Usage ```python report = process_large_timeseries(\'/path/to/data/timeseries\', 10) print(report) ``` # Notes 1. Use `pd.read_parquet` with the `columns` parameter to load only the necessary columns. 2. Convert the `user_name` column to a `Categorical` type. 3. Downcast numerical columns using `pd.to_numeric`. 4. Use chunking to process files sequentially and aggregate the results. # Performance Requirements - Ensure that the function efficiently handles large datasets by minimizing memory usage through proper data type conversions and chunk processing.","solution":"import pandas as pd import os import numpy as np def process_large_timeseries(data_dir: str, top_n: int) -> pd.DataFrame: user_freq = {} user_metrics = {} initial_memory_usage = 0 final_memory_usage = 0 # Loop through each parquet file for file_name in os.listdir(data_dir): if file_name.endswith(\'.parquet\'): file_path = os.path.join(data_dir, file_name) # Read the necessary columns from the parquet file df = pd.read_parquet(file_path, columns=[\'user_id\', \'user_name\', \'metric1\', \'metric2\']) # Initial memory usage initial_memory_usage += df.memory_usage(deep=True).sum() # Convert user_name to categorical df[\'user_name\'] = df[\'user_name\'].astype(\'category\') # Downcast numerical metrics df[\'metric1\'] = pd.to_numeric(df[\'metric1\'], downcast=\'float\') df[\'metric2\'] = pd.to_numeric(df[\'metric2\'], downcast=\'float\') # Final memory usage after downcasting final_memory_usage += df.memory_usage(deep=True).sum() # Update user frequency user_counts = df[\'user_id\'].value_counts().to_dict() for user_id, count in user_counts.items(): if user_id in user_freq: user_freq[user_id] += count else: user_freq[user_id] = count # Update user metrics sum (for averaging) for _, row in df.iterrows(): user_id = row[\'user_id\'] if user_id in user_metrics: user_metrics[user_id][\'metric1_sum\'] += row[\'metric1\'] user_metrics[user_id][\'metric2_sum\'] += row[\'metric2\'] user_metrics[user_id][\'count\'] += 1 else: user_metrics[user_id] = { \'metric1_sum\': row[\'metric1\'], \'metric2_sum\': row[\'metric2\'], \'count\': 1 } # Finding top N users top_users = sorted(user_freq, key=user_freq.get, reverse=True)[:top_n] # Creating summary report dataframe summary_data = { \'user_id\': [], \'frequency\': [], \'avg_metric1\': [], \'avg_metric2\': [] } for user_id in top_users: summary_data[\'user_id\'].append(user_id) summary_data[\'frequency\'].append(user_freq[user_id]) summary_data[\'avg_metric1\'].append(user_metrics[user_id][\'metric1_sum\'] / user_metrics[user_id][\'count\']) summary_data[\'avg_metric2\'].append(user_metrics[user_id][\'metric2_sum\'] / user_metrics[user_id][\'count\']) # Calculate memory savings memory_saved = initial_memory_usage - final_memory_usage return pd.DataFrame(summary_data), memory_saved"},{"question":"Objective: To test your understanding of tensor creation, manipulation, and auto differentiation in PyTorch, you are required to implement a function that performs several operations on tensors, ensuring proper usage of data types and devices. Problem Statement: Write a function named `tensor_operations` that takes in two parameters: a numpy array `data` and a boolean `use_cuda`. The function should: 1. Convert the numpy array `data` to a PyTorch tensor with dtype `torch.float32`. 2. If `use_cuda` is `True`, transfer the tensor to the GPU. 3. Create another tensor of the same shape filled with random values from a normal distribution with mean 0 and standard deviation 1, and with the same dtype and device as the first tensor. 4. Perform element-wise addition of the two tensors and store the result. 5. Compute the sum of all elements in the resultant tensor. 6. If the original device was GPU, transfer the result back to CPU. 7. Return the sum of elements as a Python scalar value. 8. Ensure that gradient computation is enabled for this sum and perform a backpropagation step if the sum of elements tensor has elements greater than 1. Function Signature: ```python import numpy as np import torch def tensor_operations(data: np.ndarray, use_cuda: bool) -> float: pass ``` Input: - `data`: A numpy array of any shape. - `use_cuda`: A boolean indicating whether to use GPU for computations. Output: - Return the sum of the tensor elements as a Python float scalar. Constraints and Considerations: - Handle the transfer of tensors between CPU and GPU correctly. - Ensure proper initialization of tensors with the required data types. - Perform operations in a way that is efficient and leverages PyTorch\'s capabilities. - Only perform backpropagation if the sum of the elements tensor has elements greater than 1. Example: ```python data = np.array([[1.0, 2.0], [3.0, 4.0]]) use_cuda = False result = tensor_operations(data, use_cuda) print(result) # Example output: 5.123 (Note: Exact value will change due to randomness) ``` Notes: - To run the code with CUDA support, ensure you have a GPU and the necessary drivers installed. - You can use `torch.randn_like` to create a tensor with the same shape and properties as another tensor filled with random values.","solution":"import numpy as np import torch def tensor_operations(data: np.ndarray, use_cuda: bool) -> float: # Step 1: Convert numpy array to PyTorch tensor with dtype torch.float32 tensor = torch.tensor(data, dtype=torch.float32) # Step 2: Transfer to GPU if use_cuda is True if use_cuda and torch.cuda.is_available(): tensor = tensor.to(\'cuda\') # Step 3: Create another tensor with random values from normal distribution random_tensor = torch.randn_like(tensor) # Step 4: Perform element-wise addition result_tensor = tensor + random_tensor # Step 5: Compute the sum of all elements in the resultant tensor sum_of_elements = result_tensor.sum() # Step 6: Transfer the result back to CPU if it was on GPU if tensor.is_cuda: sum_of_elements = sum_of_elements.to(\'cpu\') # Step 7: Ensure gradient computation is enabled and perform backpropagation if required if sum_of_elements.item() > 1: sum_of_elements.requires_grad_(True) sum_of_elements.backward() # Step 7: Return the sum of elements as a Python scalar value return sum_of_elements.item()"},{"question":"# Python Coding Assessment Question Problem Statement You are tasked with managing and analyzing Unix group entries on a system. Using the provided `grp` module, write a function that performs the following operations: 1. **Retrieve the group entry for a specific group name:** The function should get the group entry by the given group name. 2. **Filter groups with more than a given number of members:** The function should list all group names that have more than a specified number of members. 3. **Find the group with the maximum number of members:** The function should return the group name that has the highest number of members. Function Signature ```python def group_analysis(group_name: str, min_members: int) -> tuple: Perform analysis on the Unix group database. Parameters: - group_name (str): A group name to retrieve its entry. - min_members (int): Minimum number of members to filter groups. Returns: - tuple: A tuple containing: - The group entry for the specified group name. - A list of group names with more than the given number of members. - The name of the group with the maximum number of members. ``` Input - `group_name` (string): The name of the group to retrieve. - `min_members` (integer): The minimum number of members required to filter groups. Output - A tuple containing: 1. The group entry (a tuple-like object as defined by `grp.getgrnam(group_name)`). 2. A list of group names (strings) that have more than `min_members` members. 3. The group name (string) that has the highest number of members. Constraints - The `group_name` provided must exist in the Unix group database. - `min_members` is a non-negative integer. - If multiple groups have the same maximum number of members, return any one of them. Example ```python result = group_analysis(\\"staff\\", 5) print(result) ``` In this example, assume \\"staff\\" is a valid group name. The output will be a tuple where: 1. The first element is the group entry for \\"staff\\". 2. The second element is a list of groups that have more than 5 members. 3. The third element is the name of the group with the maximum number of members. Notes - Use the `grp` module to interact with the Unix group database. - Ensure the function handles errors gracefully, such as when the specified group name does not exist. - Aim for an efficient solution, especially when handling potentially large numbers of group entries.","solution":"import grp def group_analysis(group_name: str, min_members: int) -> tuple: try: group_entry = grp.getgrnam(group_name) except KeyError: raise ValueError(f\\"Group {group_name} does not exist\\") all_groups = grp.getgrall() groups_with_min_members = [grp.gr_name for grp in all_groups if len(grp.gr_mem) > min_members] max_members_group = max(all_groups, key=lambda g: len(g.gr_mem)).gr_name return (group_entry, groups_with_min_members, max_members_group)"},{"question":"# Dynamic Class Creation with the `types` Module Objective: Demonstrate your understanding of dynamic class creation using the Python `types` module. You will create a dynamically generated class with specific attributes and methods. You will also use the `prepare_class` method to create another class and validate the metaclass and namespace. Task: 1. **Create a Dynamic Class**: - Use the `types.new_class()` method to dynamically create a class named `DynamicPerson`. This class should have: * An attribute `name` initialized via the constructor. * A method `greet` that returns the greeting string `\\"Hello, my name is {name}!\\"` where `{name}` should be the value of the `name` attribute. 2. **Prepare and Validate Class**: - Use the `types.prepare_class()` method to create a class named `PreparedStudent` with: * An attribute `student_id` initialized via the constructor. * A method `get_id` that returns the student\'s ID. * Validate the `metaclass`, `namespace`, and `kwds` returned by `prepare_class`. Input: - A string representing the name for `DynamicPerson`. - An integer representing the `student_id` for `PreparedStudent`. Output: - An instance of `DynamicPerson` with the `name` attribute set. - An instance of `PreparedStudent` with the `student_id` attribute set. - A string output from calling the `greet` method of `DynamicPerson`. - A string output from calling the `get_id` method of `PreparedStudent`. Constraints: - The `name` for `DynamicPerson` should be a non-empty string. - The `student_id` for `PreparedStudent` should be a positive integer. Example: ```python from types import new_class, prepare_class # Step 1: Dynamic Class Creation def create_dynamic_person(name): # Define a constructor and methods for the class def __init__(self, name): self.name = name def greet(self): return f\\"Hello, my name is {self.name}!\\" # Create the class dynamically DynamicPerson = new_class(\'DynamicPerson\', (), {}, lambda ns: ns.update({\'__init__\': __init__, \'greet\': greet})) # Create an instance of the dynamic class person_instance = DynamicPerson(name) return person_instance # Step 2: Prepare and Validate Class def prepare_student_class(student_id): metaclass, namespace, kwds = prepare_class(\'PreparedStudent\', (object,), {\'metaclass\': type}) namespace[\'__init__\'] = lambda self, student_id: setattr(self, \'student_id\', student_id) namespace[\'get_id\'] = lambda self: f\\"Student ID: {self.student_id}\\" PreparedStudent = metaclass(\'PreparedStudent\', (object,), namespace) student_instance = PreparedStudent(student_id) return student_instance # Usage of the functions dynamic_person = create_dynamic_person(\'Alice\') prepared_student = prepare_student_class(12345) # Output the results print(dynamic_person.greet()) print(prepared_student.get_id()) ``` Expected Output: ``` Hello, my name is Alice! Student ID: 12345 ``` Note: - Ensure your functions handle the constraints properly, and any necessary error-checking is implemented within the functions. - The code snippets in your answer should be executable without any modifications when the input follows the constraints.","solution":"from types import new_class, prepare_class # Step 1: Dynamic Class Creation def create_dynamic_person(name): if not isinstance(name, str) or not name: raise ValueError(\\"Name must be a non-empty string\\") # Define a constructor and methods for the class def __init__(self, name): self.name = name def greet(self): return f\\"Hello, my name is {self.name}!\\" # Create the class dynamically DynamicPerson = new_class(\'DynamicPerson\', (), {}, lambda ns: ns.update({\'__init__\': __init__, \'greet\': greet})) # Create an instance of the dynamic class person_instance = DynamicPerson(name) return person_instance # Step 2: Prepare and Validate Class def prepare_student_class(student_id): if not isinstance(student_id, int) or student_id <= 0: raise ValueError(\\"Student ID must be a positive integer\\") metaclass, namespace, kwds = prepare_class(\'PreparedStudent\', (object,), {\'metaclass\': type}) namespace[\'__init__\'] = lambda self, student_id: setattr(self, \'student_id\', student_id) namespace[\'get_id\'] = lambda self: f\\"Student ID: {self.student_id}\\" PreparedStudent = metaclass(\'PreparedStudent\', (object,), namespace) student_instance = PreparedStudent(student_id) return student_instance # Usage of the functions dynamic_person = create_dynamic_person(\'Alice\') prepared_student = prepare_student_class(12345) # Output the results print(dynamic_person.greet()) # Output: Hello, my name is Alice! print(prepared_student.get_id()) # Output: Student ID: 12345"},{"question":"# PyTorch Deterministic Memory Allocation **Objective:** Create a function in PyTorch that generates a series of tensors with specified dimensions. Ensure the tensors are populated in a deterministic manner to guarantee reproducibility while allowing the option to optimize performance by turning off memory initialization. **Function Signature:** ```python def generate_deterministic_tensors(sizes: List[Tuple[int, ...]], use_deterministic: bool = True, initialize_memory: bool = True) -> List[torch.Tensor]: pass ``` **Input:** - `sizes` (List[Tuple[int, ...]]): A list of tuples. Each tuple specifies the dimensions of a tensor to be created. - `use_deterministic` (bool): A boolean flag that, if set to `True`, should make the process deterministic using `torch.use_deterministic_algorithms()`. - `initialize_memory` (bool): A boolean flag that, if set to `True`, should utilize `torch.utils.deterministic.fill_uninitialized_memory`. If set to `False`, should leave memory uninitialized for better performance. **Output:** - List[torch.Tensor]: A list of PyTorch tensors with the specified dimensions. **Constraints:** - You must use the described functionality to handle determinism and memory initialization. - Consider performance impacts. Avoid unnecessary initialization when `initialize_memory` is set to `False`. - The function should ensure reproducibility when `use_deterministic` is `True`. **Performance Requirements:** - The function should be efficient in terms of memory allocation and avoid performance hits due to unnecessary operations. **Example:** ```python sizes = [(3, 3), (2, 2)] tensors = generate_deterministic_tensors(sizes, use_deterministic=True, initialize_memory=True) print(tensors) # Expected output: A list of tensors with shapes (3, 3) and (2, 2) respectively, initialized in a deterministic manner. ``` **Additional Information:** - Ensure to handle cases where PyTorch functionalities like `torch.use_deterministic_algorithms()` and `torch.empty` are used appropriately. - Remember to consider the performance trade-offs as described in the `torch.utils.deterministic` documentation.","solution":"import torch from typing import List, Tuple def generate_deterministic_tensors(sizes: List[Tuple[int, ...]], use_deterministic: bool = True, initialize_memory: bool = True) -> List[torch.Tensor]: This function generates a series of tensors with specified dimensions in a deterministic and memory-efficient manner. :param sizes: List of tuples specifying the dimensions of the tensors :param use_deterministic: Bool flag to enable deterministic algorithms :param initialize_memory: Bool flag to specify whether to fill uninitialized memory or not :return: List of PyTorch tensors with specified dimensions # Set deterministic behavior if required if use_deterministic: torch.use_deterministic_algorithms(True) # Allocate tensors tensors = [] for size in sizes: if initialize_memory: # Create memory-initialized tensors tensor = torch.rand(size) else: # Create empty tensors (uninitialized memory) tensor = torch.empty(size) tensors.append(tensor) return tensors"},{"question":"# PyTorch CUDA Device and Memory Management Implement a function `tensor_operations_with_cuda` that performs the following operations: 1. Initializes and selects the appropriate CUDA device. 2. Allocates a tensor `A` on the selected CUDA device. 3. Transfers data from the CPU to the CUDA device asynchronously using pinned memory. 4. Performs asynchronous operations (addition and multiplication) on the CUDA tensor using multiple CUDA streams. 5. Synchronizes the streams to ensure operations are executed correctly. 6. Records and prints the time taken to perform these operations using CUDA events for timing. The function should take two arguments: - `device_id`: an integer specifying which CUDA device to use (e.g., 0 for \'cuda:0\', 1 for \'cuda:1\'). - `size`: an integer specifying the size of the tensor to allocate and perform operations on. ```python import torch def tensor_operations_with_cuda(device_id: int, size: int) -> None: Performs tensor operations using PyTorch CUDA functionalities, including device management, memory management, and CUDA streams. Args: device_id (int): The ID of the CUDA device to use. size (int): The size of the tensor to be allocated and operated on. Returns: None # 1. Initialize and select the appropriate CUDA device cuda_device = torch.device(f\'cuda:{device_id}\') # 2. Allocate a tensor A on the selected CUDA device A = torch.ones(size, device=cuda_device) # 3. Transfer data from CPU to the CUDA device asynchronously using pinned memory pinned_A = torch.ones(size).pin_memory() gpu_A = torch.empty(size, device=cuda_device) # 4. Create streams stream1 = torch.cuda.Stream(device=cuda_device) stream2 = torch.cuda.Stream(device=cuda_device) # Create CUDA events for timing start_event = torch.cuda.Event(enable_timing=True) end_event = torch.cuda.Event(enable_timing=True) # 5. Perform asynchronous operations using multiple CUDA streams # Record the start event start_event.record() with torch.cuda.stream(stream1): gpu_A.copy_(pinned_A, non_blocking=True) B = gpu_A + 1 with torch.cuda.stream(stream2): C = gpu_A * 2 # Synchronize the streams torch.cuda.synchronize() # Ensure all streams finish their operations # Record the end event end_event.record() torch.cuda.synchronize() # Wait for the events to be recorded # 6. Calculate and print the elapsed time elapsed_time = start_event.elapsed_time(end_event) print(f\\"Time taken for tensor operations on CUDA: {elapsed_time} ms\\") # Example usage: tensor_operations_with_cuda(device_id=0, size=1000000) ``` # Constraints: - The solution should use PyTorch functionalities for both device and memory management. - All operations involving CUDA should be asynchronous where applicable. - The function should time the operations accurately and print the elapsed time. - The function should handle device selection appropriately and use pinned memory for efficient data transfer from CPU to GPU. # Expected Output: The function should print the time taken to perform the described tensor operations on the specified CUDA device, providing an example of efficient CUDA usage in PyTorch.","solution":"import torch def tensor_operations_with_cuda(device_id: int, size: int) -> None: Performs tensor operations using PyTorch CUDA functionalities, including device management, memory management, and CUDA streams. Args: device_id (int): The ID of the CUDA device to use. size (int): The size of the tensor to be allocated and operated on. Returns: None # 1. Initialize and select the appropriate CUDA device cuda_device = torch.device(f\'cuda:{device_id}\') # 2. Allocate a tensor A on the selected CUDA device A = torch.ones(size, device=cuda_device) # 3. Transfer data from CPU to the CUDA device asynchronously using pinned memory pinned_A = torch.ones(size).pin_memory() gpu_A = torch.empty(size, device=cuda_device) # 4. Create streams stream1 = torch.cuda.Stream(device=cuda_device) stream2 = torch.cuda.Stream(device=cuda_device) # Create CUDA events for timing start_event = torch.cuda.Event(enable_timing=True) end_event = torch.cuda.Event(enable_timing=True) # 5. Perform asynchronous operations using multiple CUDA streams # Record the start event start_event.record() with torch.cuda.stream(stream1): gpu_A.copy_(pinned_A, non_blocking=True) B = A + 1 with torch.cuda.stream(stream2): C = A * 2 # Synchronize the streams torch.cuda.synchronize() # Ensure all streams finish their operations # Record the end event end_event.record() torch.cuda.synchronize() # Wait for the events to be recorded # 6. Calculate and print the elapsed time elapsed_time = start_event.elapsed_time(end_event) print(f\\"Time taken for tensor operations on CUDA: {elapsed_time} ms\\") # Example usage: tensor_operations_with_cuda(device_id=0, size=1000000)"},{"question":"**Question: Advanced Regex Operations with Python** **Objective:** Demonstrate a comprehensive understanding of regex operations in Python by implementing functions that perform complex pattern matching and string modifications. **Problem Statement:** You are provided with a text document which contains multiple lines of text. Each line can be one of the following formats: - A comment starting with `#`. - An email address. - A person\'s name (format: `LastName, FirstName`). - A URL. - A malformed entry. You need to implement the function `process_text(lines: List[str]) -> Dict[str, List[str]]` which performs the following: 1. Identifies and categorizes each line into its respective category: \\"comment\\", \\"email\\", \\"name\\", \\"url\\", or \\"malformed\\". 2. For names, ensure that they are stored in the format `FirstName LastName`. 3. Ignore case differences while categorizing but maintain original case in the results. 4. The output should be a dictionary containing categories as keys and lists of matched lines for each category. Malformed entries should also be included under the \\"malformed\\" category. **Input:** - `lines` - A list of strings representing the lines in the document. **Output:** - A dictionary categorizing the lines as described. **Examples:** Given the input: ```python lines = [ \\"# This is a comment\\", \\"john.doe@example.com\\", \\"Doe, John\\", \\"http://example.com\\", \\"incorrect_line\\", \\"Jane, Doe\\" ] ``` The output should be: ```python { \\"comment\\": [\\"# This is a comment\\"], \\"email\\": [\\"john.doe@example.com\\"], \\"name\\": [\\"John Doe\\", \\"Doe Jane\\"], \\"url\\": [\\"http://example.com\\"], \\"malformed\\": [\\"incorrect_line\\"] } ``` **Constraints:** 1. You must use Python\'s `re` module for pattern matching. 2. Function should handle any valid ASCII name, email, and URL patterns. 3. Maintain performance efficiency, especially for large inputs. **Hints:** 1. Utilize regex groups for capturing relevant parts of patterns. 2. Remember to handle case insensitivity using the appropriate regex flags. 3. Consider edge cases like empty lines or lines with unexpected characters. **Note:** You do not need to implement input or output handling in your function. The provided example usage is just for illustration purposes. ```python import re from typing import List, Dict def process_text(lines: List[str]) -> Dict[str, List[str]]: # Your implementation goes here pass ```","solution":"import re from typing import List, Dict def process_text(lines: List[str]) -> Dict[str, List[str]]: category_mapping = { \\"comment\\": re.compile(r\\"^s*#.*\\", re.IGNORECASE), \\"email\\": re.compile(r\\"^s*[w.-]+@[w.-]+.w+s*\\", re.IGNORECASE), \\"name\\": re.compile(r\\"^s*([ws]+),s*([ws]+)s*\\", re.IGNORECASE), \\"url\\": re.compile(r\\"^s*https?://[^s/.?#].[^s]*\\", re.IGNORECASE) } result = { \\"comment\\": [], \\"email\\": [], \\"name\\": [], \\"url\\": [], \\"malformed\\": [] } for line in lines: matched = False for category, pattern in category_mapping.items(): match = pattern.match(line) if match: if category == \\"name\\": first_name = match.group(2).strip() last_name = match.group(1).strip() result[category].append(f\\"{first_name} {last_name}\\") else: result[category].append(line.strip()) matched = True break if not matched: result[\\"malformed\\"].append(line.strip()) return result"},{"question":"# **Multiprocessing Task Manager** **Objective:** Implement a task manager using Python\'s `multiprocessing` package to perform concurrent execution of tasks. Your solution should demonstrate understanding of process creation, inter-process communication using `Queue`, shared state using `Value`, and appropriate synchronization mechanisms. **Requirements:** 1. **Function Definition:** Create a function `task_executor(task_queue, result_queue, shared_counter)` which: - Reads tasks from `task_queue`. - Executes the tasks. - Increments `shared_counter` safely. - Sends results to `result_queue`. - Terminates when a sentinel value (\'STOP\') is encountered in the task queue. 2. **Main Process:** In the main process (guarded by `if __name__ == \'__main__\':`): - Create a task queue and a result queue. - Create a shared counter initialized to zero. - Populate the task queue with a list of tasks (functions to be executed) and their arguments. - Start multiple processes executing `task_executor`. - Collect results from the result queue. - Ensure all processes terminate cleanly. 3. **Execution Tasks:** Provide example tasks: - A function that computes the factorial of a number. - A function that sleeps for a random duration and returns its argument. 4. **Synchronization:** Ensure that the shared counter is incremented in a thread-safe manner using appropriate synchronization primitives. **Input/Output Example:** ```python from multiprocessing import Process, Queue, Value, Lock import time import random import os def factorial(x): return 1 if x == 0 else x * factorial(x - 1) def sleepy(arg): time.sleep(random.uniform(0.1, 0.5)) return arg def task_executor(task_queue, result_queue, shared_counter, lock): while True: task = task_queue.get() if task == \'STOP\': break func, args = task result = func(*args) with lock: # Ensuring thread-safe update of shared counter shared_counter.value += 1 result_queue.put((os.getpid(), result)) if __name__ == \'__main__\': tasks = [(factorial, (5,)), (sleepy, (\\"sleeping\\",))] num_workers = 4 task_queue = Queue() result_queue = Queue() shared_counter = Value(\'i\', 0) lock = Lock() for task in tasks: task_queue.put(task) for _ in range(num_workers): Process(target=task_executor, args=(task_queue, result_queue, shared_counter, lock)).start() for _ in range(num_workers): task_queue.put(\'STOP\') for _ in range(len(tasks)): pid, result = result_queue.get() print(f\\"Process {pid} returned: {result}\\") print(f\\"Total tasks executed: {shared_counter.value}\\") ``` **Constraints:** - Ensure exception handling for any potential issues with inter-process communication. - Design the task manager to handle any number of tasks and workers dynamically. **Performance Requirements:** - Efficiently handle task distribution and result collection. - Minimize overhead due to synchronization and process management.","solution":"from multiprocessing import Process, Queue, Value, Lock import time import random import os def factorial(x): return 1 if x == 0 else x * factorial(x - 1) def sleepy(arg): time.sleep(random.uniform(0.1, 0.5)) return arg def task_executor(task_queue, result_queue, shared_counter, lock): while True: task = task_queue.get() if task == \'STOP\': break func, args = task result = func(*args) with lock: # Ensuring thread-safe update of shared counter shared_counter.value += 1 result_queue.put((os.getpid(), result)) if __name__ == \'__main__\': tasks = [(factorial, (5,)), (sleepy, (\\"sleeping\\",))] num_workers = 4 task_queue = Queue() result_queue = Queue() shared_counter = Value(\'i\', 0) lock = Lock() for task in tasks: task_queue.put(task) for _ in range(num_workers): Process(target=task_executor, args=(task_queue, result_queue, shared_counter, lock)).start() for _ in range(num_workers): task_queue.put(\'STOP\') for _ in range(len(tasks)): pid, result = result_queue.get() print(f\\"Process {pid} returned: {result}\\") print(f\\"Total tasks executed: {shared_counter.value}\\")"},{"question":"**PyTorch Custom Dataset Implementation and DataLoader Integration** # Objective: Implement a custom dataset by subclassing `torch.utils.data.Dataset` and use PyTorch\'s `DataLoader` to iterate through this dataset. # Background: In many machine learning tasks, you need to preprocess and load data efficiently. PyTorch\'s `torch.utils.data` module provides the tools required for these tasks. Specifically, you will be subclassing the `Dataset` class to create custom datasets and then use the `DataLoader` class to handle batch processing and shuffling. # Task: **1. Custom Dataset:** - Implement a class `CustomTextDataset` that inherits from `torch.utils.data.Dataset`. - This dataset should accept a list of text samples and their corresponding labels during initialization. - Your dataset class should implement the `__len__` and `__getitem__` methods: - `__len__` should return the number of samples in the dataset. - `__getitem__` should return a tuple `(text, label)` for a given index. **2. DataLoader Integration:** - Use the `DataLoader` class to create an iterable over the `CustomTextDataset`. - Set the `batch_size` to 4 and enable shuffling of the data. # Specifications: - **Input:** - A list of text samples, e.g., `[\\"text sample 1\\", \\"text sample 2\\", ...]`. - A list of corresponding labels, e.g., `[0, 1, ...]`. - **Output:** - For the `__getitem__` method, a tuple in the form of `(text, label)`. - **Constraints:** - The dataset lists (texts and labels) are guaranteed to be of the same length. - **Performance Requirements:** - The solution should efficiently handle at least 10,000 text samples. ```python import torch from torch.utils.data import Dataset, DataLoader class CustomTextDataset(Dataset): def __init__(self, texts, labels): Args: texts (List[str]): List of text samples. labels (List[int]): List of labels corresponding to the text samples. self.texts = texts self.labels = labels def __len__(self): # Return the number of samples in the dataset pass def __getitem__(self, idx): # Return the (text, label) tuple for index `idx` pass # Sample text data and labels texts = [\\"sample text 1\\", \\"sample text 2\\", \\"sample text 3\\", \\"sample text 4\\"] labels = [0, 1, 0, 1] # Instantiate the custom dataset text_dataset = CustomTextDataset(texts, labels) # Use the DataLoader to process batches from the dataset text_dataloader = DataLoader(text_dataset, batch_size=4, shuffle=True) # Iterate through the DataLoader and print the batches for batch in text_dataloader: print(batch) ``` **Explanation:** 1. You need to implement the `CustomTextDataset` class such that it initializes with lists of texts and labels. 2. Implement the `__len__` method to return the total number of samples. 3. Implement the `__getitem__` method to return the corresponding text and label as a tuple. 4. Create a `DataLoader` instance with the custom dataset, a batch size of 4, and enable shuffling. 5. Iterate through the `DataLoader` and print each batch to verify your implementation.","solution":"import torch from torch.utils.data import Dataset, DataLoader class CustomTextDataset(Dataset): def __init__(self, texts, labels): Args: texts (List[str]): List of text samples. labels (List[int]): List of labels corresponding to the text samples. self.texts = texts self.labels = labels def __len__(self): # Return the number of samples in the dataset return len(self.texts) def __getitem__(self, idx): # Return the (text, label) tuple for index `idx` text = self.texts[idx] label = self.labels[idx] return text, label"},{"question":"**Task: Analyze File Attributes** Create a function `analyze_file_attributes(directory)` that takes a directory path as input and recursively traverses the directory to analyze and return specific file attributes in a structured format. # Function signature ```python def analyze_file_attributes(directory: str) -> dict: pass ``` # Input - `directory` (str): The path to the directory to be analyzed. # Output - Returns a dictionary with the following structure: ```python { \\"directories\\": [<list_of_directories>], \\"files\\": [<list_of_files>], \\"special_files\\": { \\"char_device\\": [<list_of_char_devices>], \\"block_device\\": [<list_of_block_devices>], \\"fifo\\": [<list_of_fifos>], \\"socket\\": [<list_of_sockets>], \\"link\\": [<list_of_symlinks>], \\"door\\": [<list_of_doors>], \\"port\\": [<list_of_ports>], \\"whiteout\\": [<list_of_whiteouts>] }, \\"permissions\\": { \\"<file_path>\\": \\"filemode_string\\", ... } } ``` # Detailed Requirements 1. **File Type Classification**: - Use the `S_ISDIR()`, `S_ISCHR()`, `S_ISBLK()`, `S_ISFIFO()`, `S_ISSOCK()`, `S_ISLNK()`, `S_ISDOOR()`, `S_ISPORT()`, `S_ISWHT()` functions to classify files as directories, character devices, block devices, named pipes (FIFOs), sockets, links, doors, event ports, and whiteouts respectively. - Maintain lists of each type and append the file paths to the respective list. 2. **File Permissions**: - Use the `filemode()` function to convert a files mode to a string representing its permissions. - Maintain a dictionary where the keys are file paths, and the values are their corresponding permission strings. 3. **Recursive Traversal**: - Use recursion to traverse directories. If an entry is a directory, recurse into it. # Example ```python from pprint import pprint directory_path = \\"/path/to/some/directory\\" result = analyze_file_attributes(directory_path) pprint(result) ``` # Constraints - Assume all file paths provided are valid and accessible. - The function should handle large directory trees efficiently. - The solution should use the `stat` module functionalities wherever applicable. # Note: - Pay attention to how different file types and their permissions need to be handled. - The data structure returned should be clear and well-organized for easy inspection of different file attributes.","solution":"import os import stat from typing import Dict from stat import filemode def analyze_file_attributes(directory: str) -> Dict: results = { \\"directories\\": [], \\"files\\": [], \\"special_files\\": { \\"char_device\\": [], \\"block_device\\": [], \\"fifo\\": [], \\"socket\\": [], \\"link\\": [], \\"door\\": [], \\"port\\": [], \\"whiteout\\": [], }, \\"permissions\\": {} } def traverse(dir_path): for entry in os.scandir(dir_path): entry_path = entry.path stat_result = entry.stat(follow_symlinks=False) mode = stat_result.st_mode if stat.S_ISDIR(mode): results[\\"directories\\"].append(entry_path) traverse(entry_path) # recurse into directory elif stat.S_ISREG(mode): results[\\"files\\"].append(entry_path) elif stat.S_ISCHR(mode): results[\\"special_files\\"][\\"char_device\\"].append(entry_path) elif stat.S_ISBLK(mode): results[\\"special_files\\"][\\"block_device\\"].append(entry_path) elif stat.S_ISFIFO(mode): results[\\"special_files\\"][\\"fifo\\"].append(entry_path) elif stat.S_ISSOCK(mode): results[\\"special_files\\"][\\"socket\\"].append(entry_path) elif stat.S_ISLNK(mode): results[\\"special_files\\"][\\"link\\"].append(entry_path) # Note: S_ISDOOR, S_ISPORT, S_ISWHT are not commonly available or defined in many systems # elif stat.S_ISDOOR(mode): # results[\\"special_files\\"][\\"door\\"].append(entry_path) # elif stat.S_ISPORT(mode): # results[\\"special_files\\"][\\"port\\"].append(entry_path) # elif stat.S_ISWHT(mode): # results[\\"special_files\\"][\\"whiteout\\"].append(entry_path) results[\\"permissions\\"][entry_path] = filemode(mode) traverse(directory) return results"},{"question":"You have been given a task to design a simple employee management system where different categories of employees may have different methods for calculating their annual bonus. You will use the `abc` module to enforce certain rules across all employee types. # Objective Implement an abstract base class `Employee` using the `abc` module to ensure that all subclasses implement a method `calculate_annual_bonus`. # Requirements 1. Define an abstract base class `Employee` using `ABC` from the `abc` module. 2. The `Employee` class must have an abstract method `calculate_annual_bonus`. 3. Implement at least two subclasses, `SalariedEmployee` and `HourlyEmployee`, that inherit from `Employee`. 4. The `SalariedEmployee` class should: - Have an `__init__` method that takes `name`, `base_salary`, and `performance_bonus`. - Implement the `calculate_annual_bonus` method which returns `base_salary` + `performance_bonus`. 5. The `HourlyEmployee` class should: - Have an `__init__` method that takes `name`, `hours_worked`, and `hourly_rate`. - Implement the `calculate_annual_bonus` method which returns `hours_worked` * `hourly_rate` * a fixed bonus rate (e.g., 0.1). # Constraints 1. The `calculate_annual_bonus` method should not be overridden without actual implementation in the subclasses. 2. Use the `@abstractmethod` decorator to enforce the implementation of `calculate_annual_bonus`. # Function Signature ```python from abc import ABC, abstractmethod class Employee(ABC): @abstractmethod def calculate_annual_bonus(self): pass class SalariedEmployee(Employee): def __init__(self, name: str, base_salary: float, performance_bonus: float): self.name = name self.base_salary = base_salary self.performance_bonus = performance_bonus def calculate_annual_bonus(self) -> float: return self.base_salary + self.performance_bonus class HourlyEmployee(Employee): def __init__(self, name: str, hours_worked: float, hourly_rate: float): self.name = name self.hours_worked = hours_worked self.hourly_rate = hourly_rate def calculate_annual_bonus(self) -> float: bonus_rate = 0.1 return self.hours_worked * self.hourly_rate * bonus_rate ``` # Example Usage ```python # Creating instances of employee types salaried_emp = SalariedEmployee(name=\\"Alice\\", base_salary=50000, performance_bonus=5000) hourly_emp = HourlyEmployee(name=\\"Bob\\", hours_worked=2000, hourly_rate=20) # Calculating annual bonuses print(salaried_emp.calculate_annual_bonus()) # Should output 55000 print(hourly_emp.calculate_annual_bonus()) # Should output 4000 ``` Ensure that your implementation follows these guidelines, and use the `abc` module to enforce the constraints for abstract methods.","solution":"from abc import ABC, abstractmethod class Employee(ABC): @abstractmethod def calculate_annual_bonus(self) -> float: pass class SalariedEmployee(Employee): def __init__(self, name: str, base_salary: float, performance_bonus: float): self.name = name self.base_salary = base_salary self.performance_bonus = performance_bonus def calculate_annual_bonus(self) -> float: return self.base_salary + self.performance_bonus class HourlyEmployee(Employee): def __init__(self, name: str, hours_worked: float, hourly_rate: float): self.name = name self.hours_worked = hours_worked self.hourly_rate = hourly_rate def calculate_annual_bonus(self) -> float: bonus_rate = 0.1 return self.hours_worked * self.hourly_rate * bonus_rate"},{"question":"**Coding Assessment Question: Customizing Seaborn Palettes and Visualizing Data** # Objective The objective of this assessment is to evaluate your understanding of seaborn\'s `husl_palette` function along with visualization skills using different types of plots. This will assess your ability to manipulate color palettes in seaborn and apply them effectively to visualize and analyze data. # Problem Statement 1. **Load Data:** Load the `tips` dataset from seaborn\'s built-in datasets. 2. **Create Color Palettes:** - Generate a default color palette using `sns.husl_palette()`. - Create a custom color palette with 8 colors and a decreased lightness of 0.4. - Create another custom color palette with 6 colors and a decreased saturation of 0.6. 3. **Visualizations:** - Using the default palette, create a scatter plot showing the relationship between `total_bill` and `tip`. - Using the custom palette with decreased lightness, create a bar plot showing the average `total_bill` for each `day` of the week. - Using the custom palette with decreased saturation, create a violin plot to show the distribution of `total_bill` based on `day`. 4. **Summary:** - Provide a brief written summary describing how the different palette choices affect the visual interpretation of the data. # Requirements - **Input:** - Seaborn library and its built-in `tips` dataset. - **Output:** - Three different seaborn plots as described above. - A written summary of the visual effects of the different color palettes. # Constraints - Ensure that the code is clear and well-commented to explain how the palettes are created and applied. - The written summary should be clear and concise, focusing on the visual impact of the palettes rather than the data itself. # Evaluation Criteria - Correctness and functionality of the code. - Effective use of different seaborn palettes for data visualization. - Clarity and coherence of the written summary. # Code Template ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset tips = sns.load_dataset(\\"tips\\") # Generate default color palette default_palette = sns.husl_palette() # Create custom palette with 8 colors and decreased lightness of 0.4 custom_palette_lightness = sns.husl_palette(8, l=0.4) # Create custom palette with 6 colors and decreased saturation of 0.6 custom_palette_saturation = sns.husl_palette(6, s=0.6) # Create Scatter plot using default palette plt.figure(figsize=(8, 6)) sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\', palette=default_palette) plt.title(\'Scatter plot using default husl_palette\') plt.show() # Create Bar plot using custom palette with decreased lightness plt.figure(figsize=(8, 6)) sns.barplot(data=tips, x=\'day\', y=\'total_bill\', palette=custom_palette_lightness) plt.title(\'Bar plot with custom husl_palette (decreased lightness)\') plt.show() # Create Violin plot using custom palette with decreased saturation plt.figure(figsize=(8, 6)) sns.violinplot(data=tips, x=\'day\', y=\'total_bill\', palette=custom_palette_saturation) plt.title(\'Violin plot with custom husl_palette (decreased saturation)\') plt.show() # Provide a summary of the visual impacts of the color palettes summary = [Provide the summary here] print(summary) ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load dataset tips = sns.load_dataset(\\"tips\\") # Generate default color palette default_palette = sns.husl_palette() # Create custom palette with 8 colors and decreased lightness of 0.4 custom_palette_lightness = sns.husl_palette(8, l=0.4) # Create custom palette with 6 colors and decreased saturation of 0.6 custom_palette_saturation = sns.husl_palette(6, s=0.6) # Create Scatter plot using default palette plt.figure(figsize=(8, 6)) sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\', palette=default_palette) plt.title(\'Scatter plot using default husl_palette\') plt.show() # Create Bar plot using custom palette with decreased lightness plt.figure(figsize=(8, 6)) sns.barplot(data=tips, x=\'day\', y=\'total_bill\', palette=custom_palette_lightness) plt.title(\'Bar plot with custom husl_palette (decreased lightness)\') plt.show() # Create Violin plot using custom palette with decreased saturation plt.figure(figsize=(8, 6)) sns.violinplot(data=tips, x=\'day\', y=\'total_bill\', palette=custom_palette_saturation) plt.title(\'Violin plot with custom husl_palette (decreased saturation)\') plt.show() # Provide a summary of the visual impacts of the color palettes summary = The default HUSL palette creates a vibrant and visually appealing scatter plot that differentiates the data points well with a variety of bright colors. This can be advantageous when distinguishing between numerous categories. The bar plot with decreased lightness offers a more subdued color scheme that can help in reducing the strain on the viewers eyes, especially when visualizing bars of similar colors. However, it might make it slightly harder to differentiate closely related categories. The violin plot with decreased saturation provides a balanced approach, reducing the intensity of the colors while maintaining distinct visual separations between the distribution categories. This can be useful for a clearer, more professional presentation without the distraction of overly bright colors. print(summary)"},{"question":"# Advanced Function Implementation and Control Flow in Python Problem Statement You are tasked with creating a Python function that processes a list of tuples containing student information. Each tuple contains three elements: the student\'s name (a string), their grade (an integer), and their major (a string). Your function should perform several operations and exhibit good use of control flow and function definition features: 1. **Filtering**: Filter out students whose grades are below a specified threshold. 2. **Sorting**: Sort the remaining students based on their grades in descending order. If two students have the same grade, sort them alphabetically by their name. 3. **Classifying**: Classify the students into categories based on their grades (\'Excellent\', \'Good\', \'Average\', \'Fail\'), and return a dictionary mapping these categories to lists of student names. 4. **Advanced Functionality**: The function should accept an arbitrary number of additional keyword arguments that specify extra classification criteria (e.g., `honor_roll=True` could flag students with grades above 90 as \'Honor Roll\'). Function Signature ```python def process_student_data( students: list[tuple[str, int, str]], *, grade_threshold: int = 60, **criteria: dict, ) -> dict[str, list[str]]: Process student data to filter, sort, and classify student information. Parameters: - students: A list of tuples containing (\'name\', grade, \'major\'). - grade_threshold: An integer specifying the minimum grade required to include a student. - criteria: Arbitrary keyword arguments for additional classification. Returns: - A dictionary mapping classification categories to lists of student names. ``` Constraints 1. `students` is a list containing at most `10^4` tuples. 2. Each tuple contains a string (name with length up to 50), an integer (grade between 0 and 100), and a string (major with length up to 50). 3. The function should be efficient, given the possible size of the input. Example Usage ```python students = [ (\\"John Doe\\", 85, \\"Physics\\"), (\\"Jane Smith\\", 92, \\"Chemistry\\"), (\\"Alice Johnson\\", 58, \\"Mathematics\\"), (\\"Mike Brown\\", 75, \\"Engineering\\"), ] criteria = { \\"honor_roll\\": True, # Should flag students with grades above 90 \\"dean_list\\": lambda grade: grade > 80, # Custom criterion } result = process_student_data(students, grade_threshold=70, **criteria) print(result) ``` Expected Output ```python { \'Excellent\': [\\"Jane Smith\\"], \'Good\': [\\"John Doe\\", \\"Mike Brown\\"], \'Fail\': [], \'Average\': [], \'Honor Roll\': [\\"Jane Smith\\"], \'Dean List\': [\\"John Doe\\", \\"Mike Brown\\", \\"Jane Smith\\"] } ``` Notes - Use `match` statements for classifying the students into grade categories. - Make your implementation follow good coding practices as per **PEP 8**. - Document the function properly, making it clear what each part of the function does.","solution":"def process_student_data( students: list[tuple[str, int, str]], *, grade_threshold: int = 60, **criteria: dict, ) -> dict[str, list[str]]: Process student data to filter, sort, and classify student information. Parameters: - students: A list of tuples containing (\'name\', grade, \'major\'). - grade_threshold: An integer specifying the minimum grade required to include a student. - criteria: Arbitrary keyword arguments for additional classification. Returns: - A dictionary mapping classification categories to lists of student names. # Filter out students with grades below the threshold. filtered_students = [student for student in students if student[1] >= grade_threshold] # Sort students first by grade (descending) and then by name (ascending). sorted_students = sorted(filtered_students, key=lambda x: (-x[1], x[0])) # Initialize dictionary for classifications. classifications = { \'Excellent\': [], \'Good\': [], \'Average\': [], \'Fail\': [] } # Classification based on grades. for name, grade, _ in sorted_students: if grade >= 90: classifications[\'Excellent\'].append(name) elif grade >= 75: classifications[\'Good\'].append(name) elif grade >= 60: classifications[\'Average\'].append(name) else: classifications[\'Fail\'].append(name) # Handle additional criteria. for criterion, func in criteria.items(): if criterion == \\"honor_roll\\": func = lambda grade: grade > 90 classifications[criterion.replace(\'_\', \' \').title()] = [name for name, grade, _ in sorted_students if func(grade)] return classifications"},{"question":"# Problem: Efficient Fibonacci Sequence Generator In this task, you are required to create an efficient Fibonacci number generator that employs multiple utilities provided by the `functools` module. Requirements: 1. **Partial Function Application**: - Create a `partial` function that calculates the sum of two numbers. 2. **Efficient Fibonacci Calculation using `@lru_cache`**: - Implement a Fibonacci number generator function using the `@lru_cache` decorator. 3. **Performance Metrics**: - Create a mechanism to check the performance of your cached Fibonacci function by printing out cache hits and misses. # Function Definitions: 1. **partial_sum**: ```python from functools import partial def add(x, y): return x + y # Define a partial function \'partial_sum\' that adds a given value \'n\' def partial_sum(n): return partial(add, n) ``` 2. **fibonacci**: ```python from functools import lru_cache @lru_cache(maxsize=None) def fibonacci(n): if n < 2: return n return fibonacci(n-1) + fibonacci(n-2) ``` 3. **cache_info**: ```python def cache_info(): return fibonacci.cache_info() ``` # Constraints: - Implement these functions as described above. - You may use only the `functools` module from Python\'s standard library. - Assume `n` for `fibonacci(n)` to be a non-negative integer. # Input Format: - `partial_sum`: A single integer `n`. - `fibonacci`: A single non-negative integer `n`. - `cache_info`: No input required. # Output Format: - `partial_sum`: Returns a partial function that adds `n` to its input. - `fibonacci`: Output is the nth Fibonacci number. - `cache_info`: Output is the cache metrics, including hits and misses. Example: ```python # Example usage if __name__ == \\"__main__\\": add_10 = partial_sum(10) print(add_10(5)) # Output: 15 print(fibonacci(10)) # Output: 55 print(cache_info()) # Output: CacheInfo(hits=..., misses=..., maxsize=..., currsize=...) ``` Implement the required functions to satisfy the conditions outlined above.","solution":"from functools import partial, lru_cache def add(x, y): Returns the sum of x and y. return x + y def partial_sum(n): Returns a partial function that adds a given value n. return partial(add, n) @lru_cache(maxsize=None) def fibonacci(n): Returns the nth Fibonacci number using LRU cache for efficiency. if n < 2: return n return fibonacci(n-1) + fibonacci(n-2) def cache_info(): Returns the cache information for the fibonacci function. return fibonacci.cache_info()"},{"question":"**Coding Assessment Question** # Context The `codecs` module in Python provides a flexible framework for converting data between different encoding formats. It supports text encodings, byte encodings, error handling mechanisms, and stream-based processing. # Problem Create a custom codec for encoding and decoding messages using a simple substitution cipher. The cipher works by shifting each alphabetical character by a fixed number of positions in the alphabet. For example, with a shift of 3, \'A\' becomes \'D\', \'B\' becomes \'E\', and so on. Non-alphabetic characters should remain unchanged. # Task 1. Implement a class `SubstitutionCipherCodec` that inherits from `codecs.Codec`. 2. Define two methods `encode(self, input, errors=\'strict\')` and `decode(self, input, errors=\'strict\')` in the class which: - `encode`: replaces each alphabetical character by shifting it by a given number of positions. - `decode`: reverses the shift to obtain the original text. 3. Create a function `register_substitution_cipher_codec(shift)` that takes an integer `shift` and: - Registers the `SubstitutionCipherCodec` with the `codecs` module. - Ensures it supports the error handling mechanisms described by the `codecs` module. # Input - `shift`: Integer representing the number of positions to shift each character by. - `input`: String to be encoded or decoded. # Output - Encoded or decoded string based on the provided shift value. # Constraints - The shift value will be between 1 and 25. - The input will only contain printable ASCII characters. # Example ```python # Example of encoding a message register_substitution_cipher_codec(3) encoded_text = codecs.encode(\'HELLO WORLD\', \'substitution_cipher\') print(encoded_text) # Outputs \'KHOOR ZRUOG\' # Example of decoding a message decoded_text = codecs.decode(\'KHOOR ZRUOG\', \'substitution_cipher\') print(decoded_text) # Outputs \'HELLO WORLD\' ``` # Notes - Your implementation should handle both upper and lower case letters correctly. - Ensure that the custom codec is registered correctly with the `codecs` module. - Use appropriate error handling mechanisms as described in the `codecs` module documentation.","solution":"import codecs class SubstitutionCipherCodec(codecs.Codec): def __init__(self, shift): self.shift = shift def encode(self, input, errors=\'strict\'): return self._transform(input, self.shift), len(input) def decode(self, input, errors=\'strict\'): return self._transform(input, -self.shift), len(input) def _transform(self, text, shift): result = [] for char in text: if char.isalpha(): base = ord(\'A\') if char.isupper() else ord(\'a\') result.append(chr((ord(char) - base + shift) % 26 + base)) else: result.append(char) return \'\'.join(result) def substitution_cipher_search(name): if name == \'substitution_cipher\': return codecs.CodecInfo( name=\'substitution_cipher\', encode=lambda input, errors=\'strict\': ( substitution_cipher_encode(input, errors)), decode=lambda input, errors=\'strict\': ( substitution_cipher_decode(input, errors)), ) return None def substitution_cipher_encode(input, errors=\'strict\'): return codecs.lookup(\'substitution_cipher\').encode(input, errors) def substitution_cipher_decode(input, errors=\'strict\'): return codecs.lookup(\'substitution_cipher\').decode(input, errors) def register_substitution_cipher_codec(shift): codec_instance = SubstitutionCipherCodec(shift) codecs.register(substitution_cipher_search) globals()[\'substitution_cipher_encode\'] = codec_instance.encode globals()[\'substitution_cipher_decode\'] = codec_instance.decode"},{"question":"# Question: Implement Data Compression and Decompression Using `bz2` Module You are provided with multiple text files that need to be processedcompressed and then decompressed. The task requires you to demonstrate your understanding and practical application of the `bz2` module in Python. Task: 1. **Compress**: - Write a function `compress_files(input_files: List[str], output_dir: str, compresslevel: int) -> List[str]` that takes a list of input text file paths, a target output directory, and a compression level. - The function should compress each input file using the specified compression level and save the compressed files in the output directory. - The function should return a list of paths to the compressed files. 2. **Decompress**: - Write a function `decompress_files(compressed_files: List[str], output_dir: str) -> List[str]` that takes a list of compressed file paths and a target output directory. - The function should decompress each file and save the decompressed content in the output directory. - The function should return a list of paths to the decompressed files. Constraints: - The compression level should be an integer between `1` (least compression) and `9` (most compression). - The input and output directories are valid paths, and the files in the input list are existing text files. Example Usage: ```python input_files = [\\"file1.txt\\", \\"file2.txt\\"] output_dir = \\"compressed_files\\" compressed_files = compress_files(input_files, output_dir, 9) # compressed_files should contain the paths to the newly created compressed files in \'compressed_files\' directory decompressed_files_dir = \\"decompressed_files\\" decompressed_files = decompress_files(compressed_files, decompressed_files_dir) # decompressed_files should contain the paths to the decompressed files in \'decompressed_files\' directory ``` Performance Requirements: - The solution should handle files up to 100 MB efficiently without running into memory issues. - The solution should maintain the integrity of the files; after a round of compression and decompression, the content of the files should remain unchanged.","solution":"import bz2 import os from typing import List def compress_files(input_files: List[str], output_dir: str, compresslevel: int = 9) -> List[str]: compressed_files = [] if not os.path.exists(output_dir): os.makedirs(output_dir) for file in input_files: with open(file, \'rb\') as input_file: file_content = input_file.read() compressed_file_content = bz2.compress(file_content, compresslevel) output_file_path = os.path.join(output_dir, os.path.basename(file) + \'.bz2\') with open(output_file_path, \'wb\') as output_file: output_file.write(compressed_file_content) compressed_files.append(output_file_path) return compressed_files def decompress_files(compressed_files: List[str], output_dir: str) -> List[str]: decompressed_files = [] if not os.path.exists(output_dir): os.makedirs(output_dir) for file in compressed_files: with open(file, \'rb\') as input_file: compressed_file_content = input_file.read() decompressed_file_content = bz2.decompress(compressed_file_content) output_file_path = os.path.join(output_dir, os.path.basename(file).replace(\'.bz2\', \'\')) with open(output_file_path, \'wb\') as output_file: output_file.write(decompressed_file_content) decompressed_files.append(output_file_path) return decompressed_files"},{"question":"# Custom Type Hinting with Generics As part of understanding Python\'s advanced type hinting mechanisms, you are required to create a function that mimics the behavior of the `GenericAlias` in Python. Your task is to implement a function `create_generic_type` that takes two arguments: `origin` (the base type) and `args` (a list or tuple of types), and returns a string representation of the generic type annotation. Function Signature ```python def create_generic_type(origin: type, args: Union[type, Tuple[type, ...]]) -> str: pass ``` # Input - `origin` (type): This is the base type for the generic. For example, `list`, `dict`, etc. - `args` (Union[type, Tuple[type, ...]]): This represents the type arguments for the generic. It could be a single type or a tuple of multiple types. # Output - The function should return a string representation of the generic type. For a single type argument, it should be in the form of `origin[args]`. For multiple type arguments, it should be in the form of `origin[arg1, arg2, ...]`. # Examples ```python # Single type argument print(create_generic_type(list, int)) # Output: \'list[int]\' # Multiple type arguments print(create_generic_type(dict, (str, int))) # Output: \'dict[str, int]\' # Single type passed as tuple print(create_generic_type(tuple, (bool,))) # Output: \'tuple[bool]\' # Custom type with single argument print(create_generic_type(MyClass, float)) # Output: \'MyClass[float]\' # Custom type with multiple arguments print(create_generic_type(MyClass, (float, str))) # Output: \'MyClass[float, str]\' ``` # Constraints - You can assume that `origin` will always be a valid type. - `args` can either be a single type or a tuple of multiple types. - The function should handle cases with both standard library types and custom user-defined types. # Note This exercise tests your understanding of Python\'s type hinting system and your ability to manipulate and format type annotations in a meaningful way. The implementation does not require direct interaction with Python\'s `i.genericAlias`, but rather focuses on constructing and representing generic types as strings.","solution":"from typing import Union, Tuple def create_generic_type(origin: type, args: Union[type, Tuple[type, ...]]) -> str: Create a string representation of a generic type. Parameters: origin (type): Base type for the generic type. args (Union[type, Tuple[type, ...]]): Type arguments for the generic type. Returns: str: String representation of the generic type. # If args is a single type, handle it accordingly if not isinstance(args, tuple): args = (args,) # Form the string representation args_str = \\", \\".join(arg.__name__ for arg in args) return f\\"{origin.__name__}[{args_str}]\\""},{"question":"# The Python Call Stack Analyzer You are required to implement a Python class named `CallStackAnalyzer` that provides methods to analyze the current execution call stack using functionalities provided by `python310`. The class should include the following methods: 1. **get_builtins()**: - **Description**: This method should return a dictionary of the built-in functions available in the current execution frame. - **Return Type**: `dict` 2. **get_locals()**: - **Description**: This method should return a dictionary of the local variables in the current execution frame. If no frame is currently executing, return an empty dictionary. - **Return Type**: `dict` 3. **get_globals()**: - **Description**: This method should return a dictionary of the global variables in the current execution frame. If no frame is currently executing, return an empty dictionary. - **Return Type**: `dict` 4. **get_current_frame_info()**: - **Description**: This method should return a dictionary containing the current execution frames line number and code object, where keys are `\\"line_number\\"` and `\\"code\\"`. If no frame is currently executing, return a dictionary with `None` values. - **Return Type**: `dict` 5. **describe_function(func)**: - **Description**: This method takes a function object as an argument and returns a string describing the function. Use `PyEval_GetFuncName` and `PyEval_GetFuncDesc` to create the description. - **Parameters**: - `func` (type: `callable`): The function object to describe. - **Return Type**: `str` Example Usage: ```python analyzer = CallStackAnalyzer() # Example function to use with describe_function def sample_function(): pass print(analyzer.get_builtins()) # Returns built-in functions print(analyzer.get_locals()) # Returns local variables print(analyzer.get_globals()) # Returns global variables print(analyzer.get_current_frame_info()) # Returns current frame\'s line number and code print(analyzer.describe_function(sample_function)) # Returns the function\'s description ``` Constraints: - Assume all functions provided (`PyEval_GetBuiltins`, `PyEval_GetLocals`, `PyEval_GetGlobals`, `PyEval_GetFrame`, `PyFrame_GetLineNumber`, `PyFrame_GetCode`, `PyEval_GetFuncName`, `PyEval_GetFuncDesc`) are accessible and properly wrapped for Python usage. - Your implementation should handle cases where no frame is available. Implement the `CallStackAnalyzer` class and ensure that all methods operate correctly as per the specifications.","solution":"import sys import builtins class CallStackAnalyzer: def get_builtins(self): Returns a dictionary of the built-in functions available in the current execution frame. return builtins.__dict__ def get_locals(self): Returns a dictionary of the local variables in the current execution frame. If no frame is currently executing, return an empty dictionary. frame = sys._getframe(1) return frame.f_locals if frame is not None else {} def get_globals(self): Returns a dictionary of the global variables in the current execution frame. If no frame is currently executing, return an empty dictionary. frame = sys._getframe(1) return frame.f_globals if frame is not None else {} def get_current_frame_info(self): Returns a dictionary containing the current execution frames line number and code object. Keys are \\"line_number\\" and \\"code\\". If no frame is currently executing, return a dictionary with None values. frame = sys._getframe(1) return { \\"line_number\\": frame.f_lineno if frame is not None else None, \\"code\\": frame.f_code if frame is not None else None } def describe_function(self, func): Takes a function object as an argument and returns a string describing the function. Use PyEval_GetFuncName and PyEval_GetFuncDesc to create the description. func_name = func.__name__ func_desc = f\\"{func.__code__.co_filename}:{func.__code__.co_firstlineno}\\" return f\\"{func_name} at {func_desc}\\""},{"question":"**Question: File Processing Pipeline using `pipes` Module** In this exercise, you will implement a file processing pipeline using the deprecated `pipes` module in Python. Although this module is deprecated, this exercise will allow you to demonstrate an understanding of creating and managing pipelines of shell commands. You are given a small text file, and you need to perform a series of transformations on it using shell commands. Specifically, you will: 1. Reverse the content of each line. 2. Convert all text to uppercase. 3. Sort the lines alphabetically. Please follow the steps below to complete the task: 1. Define a function `process_file(input_file: str, output_file: str) -> None` that takes in the path to an input text file and an output file. 2. Within this function, create a `pipes.Template` object. 3. Append the necessary shell commands to the pipeline to achieve the transformations mentioned: - Reverse the content of each line. - Convert all text to uppercase. - Sort the lines alphabetically. 4. Use the `Template.open()` method to create the output file after processing the input file through the pipeline. 5. Read the contents of the resulting file back and write it out to the specified output file. **Constraints:** - You must use the `pipes` module\'s `Template` class to handle the pipeline. - Assume that the input file contains only plain text and is small enough to be processed in memory. **Example:** Input file content (`input.txt`): ``` hello world python programming pipes module ``` Output file content (`output.txt`): ``` DLROW OLLEH ELUDOM SEPIP GNIMMARGORP NOHTYP ``` **Hint:** You may find the `rev`, `tr`, and `sort` shell commands useful for this task. **Implementation:** ```python import pipes def process_file(input_file: str, output_file: str) -> None: # Create a Template object t = pipes.Template() # Append commands to the pipeline t.append(\'rev\', \'--\') # Reverse the content of each line t.append(\'tr a-z A-Z\', \'--\') # Convert text to uppercase t.append(\'sort\', \'--\') # Sort lines alphabetically # Open the input file and process it through the pipeline with t.open(output_file, \'w\') as f: with open(input_file, \'r\') as infile: for line in infile: f.write(line) # Example usage: # process_file(\'input.txt\', \'output.txt\') ``` This function should transform the contents of the `input.txt` file and write the processed contents to the `output.txt` file as specified in the example.","solution":"import pipes def process_file(input_file: str, output_file: str) -> None: # Create a Template object t = pipes.Template() # Append commands to the pipeline t.append(\'rev\', \'--\') # Reverse the content of each line t.append(\'tr a-z A-Z\', \'--\') # Convert text to uppercase t.append(\'sort\', \'--\') # Sort lines alphabetically # Open the input file and process it through the pipeline with t.open(output_file, \'w\') as f: with open(input_file, \'r\') as infile: for line in infile: f.write(line)"},{"question":"# Advanced Python: Working with Bytes Objects You are required to implement a series of functions that manipulate `bytes` objects in various ways. This task will test your understanding of handling and modifying `bytes` data in Python. 1. **Function: `bytes_concatenate`** Implement a function that takes two `bytes` objects and returns their concatenation. ```python def bytes_concatenate(byte1: bytes, byte2: bytes) -> bytes: Concatenates two bytes objects. Parameters: byte1 (bytes): The first bytes object. byte2 (bytes): The second bytes object. Returns: bytes: A new bytes object which is the concatenation of `byte1` and `byte2`. pass ``` **Example:** ```python bytes_concatenate(b\\"hello\\", b\\"world\\") # Returns: b\\"helloworld\\" ``` 2. **Function: `bytes_to_hex`** Implement a function that converts a `bytes` object to its hexadecimal string representation. ```python def bytes_to_hex(byte_data: bytes) -> str: Converts a bytes object to its hexadecimal string representation. Parameters: byte_data (bytes): The bytes object to be converted. Returns: str: The hexadecimal string representation of `byte_data`. pass ``` **Example:** ```python bytes_to_hex(b\\"x00xFF\\") # Returns: \\"00ff\\" ``` 3. **Function: `pad_bytes`** Implement a function that pads a `bytes` object to a specified length with a given padding byte. ```python def pad_bytes(byte_data: bytes, length: int, padding_byte: bytes) -> bytes: Pads a bytes object to a specified length with a padding byte. Parameters: byte_data (bytes): The original bytes object. length (int): The desired length after padding. padding_byte (bytes): A single-byte bytes object used as padding. Returns: bytes: The padded bytes object. Note: - If the length of byte_data is already greater than or equal to `length`, return byte_data unchanged. - The `padding_byte` must be exactly one byte long. pass ``` **Examples:** ```python pad_bytes(b\\"data\\", 10, b\\"x00\\") # Returns: b\\"datax00x00x00x00x00x00\\" pad_bytes(b\\"data\\", 3, b\\"x00\\") # Returns: b\\"data\\" ``` **Constraints:** - You should not use any form of string conversion for padding or concatenation. - Your implementations must handle edge cases, such as empty `bytes` objects or padding lengths that are shorter than the original byte length. - You should ensure that the padding byte is exactly one byte long, otherwise, raise a `ValueError`. You should provide implementations for the above functions within a single Python file. Additionally, write a few test cases to demonstrate the functionality and correctness of each function.","solution":"def bytes_concatenate(byte1: bytes, byte2: bytes) -> bytes: Concatenates two bytes objects. Parameters: byte1 (bytes): The first bytes object. byte2 (bytes): The second bytes object. Returns: bytes: A new bytes object which is the concatenation of `byte1` and `byte2`. return byte1 + byte2 def bytes_to_hex(byte_data: bytes) -> str: Converts a bytes object to its hexadecimal string representation. Parameters: byte_data (bytes): The bytes object to be converted. Returns: str: The hexadecimal string representation of `byte_data`. return byte_data.hex() def pad_bytes(byte_data: bytes, length: int, padding_byte: bytes) -> bytes: Pads a bytes object to a specified length with a padding byte. Parameters: byte_data (bytes): The original bytes object. length (int): The desired length after padding. padding_byte (bytes): A single-byte bytes object used as padding. Returns: bytes: The padded bytes object. Note: - If the length of byte_data is already greater than or equal to `length`, return byte_data unchanged. - The `padding_byte` must be exactly one byte long. if len(padding_byte) != 1: raise ValueError(\\"The padding byte must be exactly one byte long.\\") if len(byte_data) >= length: return byte_data padding_length = length - len(byte_data) return byte_data + padding_byte * padding_length"},{"question":"# Advanced Python310 Coding Assessment **Objective**: Demonstrate your understanding of the `nntplib` module by implementing functions to connect to an NNTP server, retrieve and decode article headers, and provide relevant error-handling mechanisms. Part 1: Connect and Get Newsgroups 1. Implement a function `connect_and_get_newsgroups` that connects to an NNTP server and retrieves the list of all newsgroups. **Function Signature**: ```python def connect_and_get_newsgroups(host: str, port: int = 119) -> List[Tuple[str, int, int, str]]: ``` **Input**: - `host` (str): The hostname of the NNTP server. - `port` (int): The port number (default is 119). **Output**: - A list of tuples, where each tuple represents a newsgroup and contains the following elements: - `group` (str): The name of the newsgroup. - `last` (int): The last article number. - `first` (int): The first article number. - `flag` (str): The status flag of the newsgroup. **Constraints**: - The function should handle `NNTP` and `NNTP_SSL` connections. Part 2: Retrieve and Decode Latest Article Header 2. Implement a function `get_and_decode_latest_article_header` that retrieves the header of the latest article from a specified newsgroup and decodes it. **Function Signature**: ```python def get_and_decode_latest_article_header(host: str, newsgroup: str) -> Dict[str, str]: ``` **Input**: - `host` (str): The hostname of the NNTP server. - `newsgroup` (str): The name of the newsgroup. **Output**: - A dictionary where the keys are header names and the values are the decoded header values. **Constraints**: - The function should connect to the NNTP server, select the specified newsgroup, and retrieve the latest article\'s headers. - Use `nntplib.decode_header` to decode each header value. - Include proper error-handling for dealing with connection issues, non-existent groups, and empty newsgroups. Part 3: Handle NNTP Errors 3. Implement error handling for the following scenarios: - Connection issues (e.g., invalid hostname, wrong port). - Non-existent newsgroups. - No articles in the newsgroup. **Note**: - Make sure to use appropriate exceptions provided by the `nntplib` module. - Add relevant comments to explain your implementation logic. Here\'s a template to get you started: ```python import nntplib from typing import List, Tuple, Dict def connect_and_get_newsgroups(host: str, port: int = 119) -> List[Tuple[str, int, int, str]]: try: with nntplib.NNTP(host, port) as server: response, newsgroups = server.list() return newsgroups except (nntplib.NNTPError, OSError) as e: print(f\\"Error: {e}\\") return [] def get_and_decode_latest_article_header(host: str, newsgroup: str) -> Dict[str, str]: try: with nntplib.NNTP(host) as server: server.group(newsgroup) response, overviews = server.over((None, None)) latest_article_id, latest_overview = overviews[-1] decoded_headers = {header: nntplib.decode_header(value) for header, value in latest_overview.items()} return decoded_headers except (nntplib.NNTPError, OSError) as e: print(f\\"Error: {e}\\") return {} ``` **Sample Usage**: ```python # Connect and get newsgroups newsgroups = connect_and_get_newsgroups(\'news.gmane.io\') print(newsgroups) # Get and decode latest article header latest_article_header = get_and_decode_latest_article_header(\'news.gmane.io\', \'gmane.comp.python.committers\') print(latest_article_header) ``` **Evaluation Criteria**: - Correctness of the function implementations. - Proper usage of the `nntplib` module and its methods. - Implementation of error handling for described scenarios. - Code readability and comments.","solution":"import nntplib from typing import List, Tuple, Dict def connect_and_get_newsgroups(host: str, port: int = 119) -> List[Tuple[str, int, int, str]]: try: with nntplib.NNTP(host, port) as server: response, newsgroups = server.list() return newsgroups except (nntplib.NNTPError, OSError) as e: print(f\\"Error: {e}\\") return [] def get_and_decode_latest_article_header(host: str, newsgroup: str) -> Dict[str, str]: try: with nntplib.NNTP(host) as server: resp, count, first, last, name = server.group(newsgroup) if count == 0: raise ValueError(f\\"The newsgroup \'{newsgroup}\' has no articles.\\") resp, id, message_id = server.stat(last) resp, article_info = server.head(id) decoded_headers = {} for header in article_info.lines: key, value = header.decode(\'utf-8\').split(\': \', 1) decoded_headers[key] = nntplib.decode_header(value) return decoded_headers except (nntplib.NNTPError, OSError, ValueError) as e: print(f\\"Error: {e}\\") return {}"},{"question":"# Custom Mapping Class Implementation You are tasked with implementing a custom mapping class in Python, `CustomMapping`, that mimics the behavior of Python\'s dictionary and the functionalities provided in the `PyMapping` C API described below. Requirements 1. **Initialization:** The class should be initialized with an optional dictionary that populates the initial contents of the mapping. 2. **Mapping Protocol Methods:** - `__getitem__(self, key)`: Retrieve the value for the given key. - `__setitem__(self, key, value)`: Set the value for the given key. - `__len__(self)`: Return the number of items in the mapping. 3. **Custom Methods:** - `has_key(self, key)`: Return `True` if the mapping contains the key, otherwise `False`. - `del_key(self, key)`: Delete the mapping for the given key. If the key is not present, raise a `KeyError`. - `keys(self)`: Return a list of all keys in the mapping. - `values(self)`: Return a list of all values in the mapping. - `items(self)`: Return a list of tuples, where each tuple is a key-value pair in the mapping. Input and Output Formats - The methods `__getitem__`, `__setitem__`, and `del_key` will be called with both existing and non-existing keys. - The method `items` should return the items in the order they were added. - Performance and efficiency are valued, so consider optimizing the common operations. Example Usage ```python mapping = CustomMapping({\'a\': 1, \'b\': 2}) print(len(mapping)) # Output: 2 print(mapping[\'a\']) # Output: 1 mapping[\'c\'] = 3 print(mapping[\'c\']) # Output: 3 print(len(mapping)) # Output: 3 print(mapping.has_key(\'a\')) # Output: True print(mapping.has_key(\'d\')) # Output: False mapping.del_key(\'b\') print(len(mapping)) # Output: 2 print(mapping.keys()) # Output: [\'a\', \'c\'] print(mapping.values()) # Output: [1, 3] print(mapping.items()) # Output: [(\'a\', 1), (\'c\', 3)] try: mapping.del_key(\'d\') except KeyError: print(\\"KeyError raised as expected\\") # Output: KeyError raised as expected ``` Implement the `CustomMapping` class with the specified functionalities and test its performance using the given example.","solution":"class CustomMapping: def __init__(self, init_dict=None): Initialize the CustomMapping with an optional dictionary. self._mapping = init_dict if init_dict is not None else {} def __getitem__(self, key): Retrieve the value for the given key. return self._mapping[key] def __setitem__(self, key, value): Set the value for the given key. self._mapping[key] = value def __len__(self): Return the number of items in the mapping. return len(self._mapping) def has_key(self, key): Return True if the mapping contains the key, otherwise False. return key in self._mapping def del_key(self, key): Delete the mapping for the given key. If the key is not present, raise a KeyError. if key in self._mapping: del self._mapping[key] else: raise KeyError(f\\"Key \'{key}\' not found in mapping\\") def keys(self): Return a list of all keys in the mapping. return list(self._mapping.keys()) def values(self): Return a list of all values in the mapping. return list(self._mapping.values()) def items(self): Return a list of tuples, where each tuple is a key-value pair in the mapping. return list(self._mapping.items())"},{"question":"Question You are tasked with creating visualizations for a dataset using the seaborn package in Python. # Dataset You will use the following dataset: ```python data = { \\"Category\\": [\\"A\\", \\"B\\", \\"C\\", \\"D\\"], \\"Values1\\": [10, 15, 7, 10], \\"Values2\\": [5, 12, 9, 8] } ``` # Requirements 1. **Plot Styles**: Set a specific seaborn style as the default for all plots you create. 2. **Bar Plot**: Create a bar plot displaying \\"Category\\" on the x-axis and \\"Values1\\" on the y-axis. 3. **Line Plot**: Create a line plot displaying \\"Category\\" on the x-axis and \\"Values2\\" on the y-axis. 4. **Customizations**: - For the bar plot, adjust the grid color to `\'#D3D3D3\'` and the grid line style to `\'--\'`. - For the line plot, use markers (circles) on each data point. # Implementation 1. Write a function `create_plots(data)` that takes a dictionary `data` as input and performs the required visualizations and customizations. 2. Ensure that the plots are displayed properly with their styles set according to the specifications. # Function Signature ```python def create_plots(data: dict) -> None: pass ``` # Example When `create_plots(data)` is called, it should: - Set the style for seaborn plots. - Display a customized bar plot. - Display a customized line plot with markers. **Note:** Ensure that you use seaborn\'s built-in functionality for styling and plotting. # Constraints - You must use seaborn\'s styling and plotting methods. - The function should generate plots without returning any values. # Performance Requirements - The function should execute efficiently and generate the plots promptly.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_plots(data): # Set seaborn style sns.set_style(\\"whitegrid\\") # Create bar plot for Values1 plt.figure(figsize=(10, 5)) bar_plot = sns.barplot(x=\'Category\', y=\'Values1\', data=data) bar_plot.set_title(\'Bar Plot of Values1\') bar_plot.grid(True, color=\'#D3D3D3\', linestyle=\'--\') plt.show() # Create line plot for Values2 with markers plt.figure(figsize=(10, 5)) line_plot = sns.lineplot(x=\'Category\', y=\'Values2\', data=data, marker=\'o\') line_plot.set_title(\'Line Plot of Values2\') plt.show() # Example data data = { \\"Category\\": [\\"A\\", \\"B\\", \\"C\\", \\"D\\"], \\"Values1\\": [10, 15, 7, 10], \\"Values2\\": [5, 12, 9, 8] } # Create data in pandas DataFrame format required by seaborn import pandas as pd data = pd.DataFrame(data) # Create plots create_plots(data)"},{"question":"Tensor Comparison using PyTorch You are given two tensors, `tensor_a` and `tensor_b`, containing float values. Your task is to implement a function `compare_tensors` that takes these two tensors as input and returns a dictionary with SQNR, normalized L2 error, and cosine similarity between the two. **Function Signature:** ```python def compare_tensors(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> dict: pass ``` **Inputs:** 1. `tensor_a`: A 1-dimensional PyTorch tensor of floats. 2. `tensor_b`: A 1-dimensional PyTorch tensor of floats of the same shape as `tensor_a`. **Outputs:** The output should be a dictionary containing the following key-value pairs: - `\\"SQNR\\"`: A floating-point value representing the Signal-to-Quantization-Noise Ratio between `tensor_a` and `tensor_b`. - `\\"normalized_L2\\"`: A floating-point value representing the normalized L2 error between `tensor_a` and `tensor_b`. - `\\"cosine_similarity\\"`: A floating-point value representing the cosine similarity between `tensor_a` and `tensor_b`. **Constraints:** - Do not use any additional libraries besides PyTorch. - Handle invalid input (such as mismatched tensor sizes) by raising a `ValueError`. **Example:** ```python import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity def compare_tensors(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> dict: # Your implementation goes here pass # Example usage: tensor_a = torch.tensor([1.0, 2.0, 3.0]) tensor_b = torch.tensor([1.1, 2.1, 3.0]) result = compare_tensors(tensor_a, tensor_b) print(result) # Expected output: # { # \\"SQNR\\": <some floating point value>, # \\"normalized_L2\\": <some floating point value>, # \\"cosine_similarity\\": <some floating point value> # } ``` **Notes:** - Ensure to use the provided utility functions from `torch.ao.ns.fx.utils`. - Your function should be efficient and concise, adhering to optimal PyTorch practices.","solution":"import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity def compare_tensors(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> dict: if tensor_a.shape != tensor_b.shape: raise ValueError(\\"Tensors must have the same shape.\\") sqnr = compute_sqnr(tensor_a, tensor_b) normalized_l2 = compute_normalized_l2_error(tensor_a, tensor_b) cosine_similarity = compute_cosine_similarity(tensor_a, tensor_b) return { \\"SQNR\\": sqnr, \\"normalized_L2\\": normalized_l2, \\"cosine_similarity\\": cosine_similarity }"},{"question":"**Objective**: Demonstrate your understanding of creating and customizing plots using the `seaborn.objects` module, including the use of aggregation and jittering techniques. **Instructions**: 1. Load the `tips` dataset from the seaborn library. 2. Create a plot using the `seaborn.objects` functionality to visualize the total bill (`total_bill`) against the day of the week (`day`), with the following specifications: - Use a `Dash` mark to represent each data point. - Color the dashes based on the time of day (`time`). - Set the `alpha` transparency of the dashes to `0.7`. - Use the tip amount (`tip`) to set the linewidth of the dashes. - Ensure the width of the dashes is `0.6`. - Aggregate the data by showing the average total bill for each day. - Add points for raw data using `Dots` with jittering applied. - Ensure legible visualization by dodging the marks to avoid overlap. **Expected Input**: - Use the `tips` dataset from seaborn. **Expected Output**: - A plot meeting the above specifications. **Example Code**: Write the code in Python using seaborn to create the specified plot. ```python import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\") # Create the plot p = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"time\\") p.add(so.Dash(alpha=0.7), linewidth=\\"tip\\", width=0.6) p.add(so.Dash(), so.Agg(), so.Dodge()) p.add(so.Dots(), so.Dodge(), so.Jitter()) # Display the plot p.show() ``` **Note**: - Ensure you understand each line of code and its purpose. - Test your code to verify it meets the requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_custom_plot(): # Load the tips dataset tips = load_dataset(\\"tips\\") # Create the plot p = so.Plot(tips, x=\\"day\\", y=\\"total_bill\\", color=\\"time\\") # Add a Dash mark for each data point with specific transparancy, linewidth, and width p.add(so.Dash(alpha=0.7), linewidth=\\"tip\\", width=0.6) # Aggregate the data by showing the average total bill for each day p.add(so.Dash(), so.Agg(), so.Dodge()) # Add points for raw data using Dots with jittering applied p.add(so.Dots(), so.Dodge(), so.Jitter()) # Display the plot p.show()"},{"question":"# Seaborn Plotting Challenge Objective Create a detailed and informative plot using the seaborn `so.Plot` interface to visualize and interpret the relationship between different variables in a dataset. Details & Requirements 1. **Dataset** Use the `tips` dataset from the seaborn library. 2. **Task** Create a grid of plots (`facet`) to visualize the distribution of `total_bill` across different days of the week, distinguishing data points by gender (`sex`) with the following specifications: a. Each plot should be a dot plot where: - X-axis shows `total_bill`. - Y-axis shows `day`. b. Customize the dots: - Differentiate the dots by color based on `sex`. - Add edge colors to the dots for better clarity. - Add jittering to prevent overlap of points. c. Create facets where each day of the week forms a separate plot. d. Add error bars to each plot to show the standard error of the mean `total_bill` for that day. Input - None. The dataset is loaded internally. Expected Output A seaborn plot consisting of multiple subplots (facets), visually representing the required data and customized as specified. Constraints - The solution should be efficient and make use of seaborn\'s built-in capabilities for faceting, color mapping, and adding error bars. Example Code Structure ```python import seaborn.objects as so from seaborn import load_dataset # Load dataset tips = load_dataset(\\"tips\\") # Create the facet grid and plot plot = ( so.Plot(tips, x=\\"total_bill\\", y=\\"day\\", color=\\"sex\\") .facet(\\"day\\") .add(so.Dot(edgecolor=\\"w\\"), so.Jitter(0.2)) .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) ) plot.show() ``` Notes - Make sure to handle the facets and customizations as required. - Document your code appropriately to explain your approach and logic. Submission Submit a single Python script or Jupyter notebook file with your code solution and comments.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_tip_plot(): # Load dataset tips = load_dataset(\\"tips\\") # Create the facet grid and plot plot = ( so.Plot(tips, x=\\"total_bill\\", y=\\"day\\", color=\\"sex\\") .facet(\\"day\\", wrap=2) .add(so.Dot(edgecolor=\\"w\\"), so.Jitter(0.2)) .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) .layout(size=(12, 8)) # Adjusting layout size ) plot.show()"},{"question":"**Question:** You have been provided with a dataset of penguins from the Seaborn library. Your task is to create a custom visualization showcasing the relationship between the penguins\' body mass and their flipper length. The desired plot should include the following components: 1. A scatter plot with jitter applied to avoid overlapping points. 2. The jitter should be controlled by both `width`, `x`, and `y` parameters as documented, to see the effects of each separately and combined. 3. Differentiation of the species using distinct colors. # Dataset Use the penguins dataset from seaborn: ```python from seaborn import load_dataset penguins = load_dataset(\\"penguins\\") ``` # Requirements 1. **Scatter plot with jitter**: - Create a scatter plot of `body_mass_g` (x-axis) vs `flipper_length_mm` (y-axis). - Apply jitter along the y-axis using the `width` parameter. - Apply jitter along the x-axis or y-axis using the `x` and `y` parameters. 2. **Color by species**: - Ensure that different species are color-coded. # Implementation Write a function `create_penguin_plot_save(path: str)` that takes a file path as input and saves the plot as a PNG image: ```python def create_penguin_plot_save(path: str): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load dataset penguins = load_dataset(\\"penguins\\") # Create a Plot object p = so.Plot(penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\", color=\\"species\\") # Add dots with jitter (along y-axis using width parameter) p.add(so.Dots(), so.Jitter(.2)) # Change width as needed # Add separate jitter transformations using x and y parameters p.add(so.Dots(), so.Jitter(x=200, y=5)) # Adjust x and y as needed # Finalize and save plot plt.figure(figsize=(10, 6)) p.plot() plt.savefig(path) plt.close() ``` # Input - `path: str` - The file path where the image of the plot will be saved. # Output - The plot should be saved as a PNG image at the specified path. # Constraints - Ensure that Seaborn and Matplotlib are correctly installed and imported. - Properly handle any missing data in the dataset. # Example Usage ```python # Save plot to \'penguins_plot.png\' create_penguin_plot_save(\\"penguins_plot.png\\") ``` # Evaluation Criteria - Correct implementation of jitter using `width`, `x`, and `y` parameters. - Proper use of Seaborn\'s Plotting capability. - Correct color-coding by species. - Successful saving of the plot as a PNG file.","solution":"def create_penguin_plot_save(path: str): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load dataset penguins = load_dataset(\\"penguins\\") # Drop rows with missing values in necessary columns penguins = penguins.dropna(subset=[\\"body_mass_g\\", \\"flipper_length_mm\\", \\"species\\"]) # Create a Plot object p = so.Plot(penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\", color=\\"species\\") # Add dots with jitter (along y-axis using width parameter) p.add(so.Dots(), so.Jitter(.2)) # Change width as needed # Add separate jitter transformations using x and y parameters p.add(so.Dots(), so.Jitter(x=200, y=5)) # Adjust x and y as needed # Finalize and save plot plt.figure(figsize=(10, 6)) p.plot() plt.savefig(path) plt.close()"},{"question":"**Question:** You are tasked with writing Python code to manage tar archives using the `tarfile` module. Your objective is to: 1. Create a tar archive containing a set of files from a specified directory. 2. List all the files in the created archive. 3. Extract the tar archive contents to a different directory with security measures to avoid path traversal attacks. Follow the instructions below to structure your solution appropriately: # Part 1: Creating a Tar Archive 1. Implement a function `create_tar_archive(source_dir, tar_path)`. - `source_dir` (str): The source directory containing files to be archived. - `tar_path` (str): The path where the resulting tar file should be saved. - The function should create a tar archive (`tar_path`) that includes all files from the `source_dir`. # Part 2: Listing Files in the Tar Archive 2. Implement a function `list_tar_contents(tar_path)`: - `tar_path` (str): The path to the tar archive to be listed. - The function should print out the names of all files contained in the tar archive. # Part 3: Extracting Tar Archive Safely 3. Implement a function `extract_tar_safely(tar_path, extract_dir)`: - `tar_path` (str): The path to the tar archive to be extracted. - `extract_dir` (str): The destination directory where files should be extracted. - The function should extract the tar archive contents into the `extract_dir`. - Use the `data` filter to eliminate potential security risks such as path traversal attacks. # Constraints: - Assume the `tarfile`, `os`, and `shutil` modules are available. - Ensure you handle exceptions appropriately. - Do not use temporary directories during extraction for simplicity. Example Usage: ```python source_dir = \'src_files\' tar_path = \'archive.tar.gz\' extract_dir = \'extracted_files\' # Part 1: Create a tar archive create_tar_archive(source_dir, tar_path) # Part 2: List contents of the tar archive list_tar_contents(tar_path) # Part 3: Safely extract the tar archive extract_tar_safely(tar_path, extract_dir) ``` Expected Output: ```plaintext src_files/file1.txt src_files/file2.txt src_files/subdir/file3.txt ... ``` Implement the functions respecting these specifications. Ensure your solution is robust and considers edge cases.","solution":"import tarfile from pathlib import Path import os def create_tar_archive(source_dir, tar_path): Creates a tar archive from the specified source directory. :param source_dir: Directory containing files to be archived. :param tar_path: Path where the resulting tar file should be saved. with tarfile.open(tar_path, \\"w:gz\\") as tar: tar.add(source_dir, arcname=os.path.basename(source_dir)) def list_tar_contents(tar_path): Lists all filenames in the specified tar archive. :param tar_path: Path to the tar archive. with tarfile.open(tar_path, \\"r:gz\\") as tar: for member in tar.getmembers(): print(member.name) def extract_tar_safely(tar_path, extract_dir): Extracts the contents of a tar archive to a specified directory with security measures. :param tar_path: Path to the tar archive. :param extract_dir: Directory where the archive should be extracted. def is_safe_path(base_path, target_path): # Check if the target_path is within the base_path to prevent path traversal return Path(base_path) in Path(target_path).resolve().parents with tarfile.open(tar_path, \\"r:gz\\") as tar: for member in tar.getmembers(): member_path = Path(extract_dir) / member.name if not is_safe_path(extract_dir, member_path): raise Exception(f\\"Unsafe file path detected: {member.name}\\") tar.extractall(path=extract_dir)"},{"question":"**Question: Implement a Secure Echo Server** **Description:** You are required to implement a secure echo server using the Python `ssl` module. The server will listen for incoming client connections, securely communicate with them using TLS, and echo back any received messages. The server must handle multiple client connections, each in its own thread. **Requirements:** 1. The server should use a self-signed certificate and private key for SSL communication. 2. The server should securely wrap the socket using `SSLContext`. 3. The server should handle multiple client connections concurrently using threads. 4. Each client connection should be handled securely, echoing back any messages received from the client. 5. The server should log connection events (e.g., client connected, message received, etc.). 6. Implement appropriate error handling for SSL-related issues. **Constraints:** - The certificate and private key must be provided in PEM format. - The server should run on `localhost` and listen on port `8443`. - The server should use `TLSv1.2` or higher for secure communication. **Performance Requirements:** - The server should handle at least 10 concurrent client connections efficiently. **Input Format:** - The server does not explicitly require an input format; it listens for connections and reads messages from each client socket. **Output Format:** - The server echoes received messages back to the respective client. - Logged events should be printed to the console. **Certificate and Key Files:** - `server_cert.pem` (self-signed certificate) - `server_key.pem` (private key) **Example Usage:** 1. Start the server: ```sh python secure_echo_server.py ``` 2. Connect to the server using an SSL client (e.g., `openssl s_client`): ```sh openssl s_client -connect localhost:8443 ``` 3. Send a message from the client: ``` Hello, Server! ``` 4. The server should echo back the message: ``` Hello, Server! ``` **Implementation Hints:** - Use `ssl.create_default_context` to create a secure SSL context. - Use `ssl.SSLContext.wrap_socket` to securely wrap a socket. - Use the `threading` module to handle multiple client connections. - Ensure to handle common SSL exceptions such as `ssl.SSLError`. **Starter Code:** ```python import socket import ssl import threading def handle_client_connection(conn): try: print(f\\"Client connected: {conn.getpeername()}\\") while True: data = conn.recv(1024) if not data: break print(f\\"Received message: {data.decode()}\\") conn.sendall(data) print(f\\"Client disconnected: {conn.getpeername()}\\") except ssl.SSLError as e: print(f\\"SSL error: {e}\\") finally: conn.shutdown(socket.SHUT_RDWR) conn.close() def main(): context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH) context.load_cert_chain(certfile=\\"server_cert.pem\\", keyfile=\\"server_key.pem\\") context.options |= ssl.OP_NO_SSLv2 | ssl.OP_NO_SSLv3 context.minimum_version = ssl.TLSVersion.TLSv1_2 bind_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) bind_socket.bind((\'localhost\', 8443)) bind_socket.listen(5) print(\\"Server listening on port 8443...\\") while True: new_socket, from_addr = bind_socket.accept() ssl_socket = context.wrap_socket(new_socket, server_side=True) client_thread = threading.Thread(target=handle_client_connection, args=(ssl_socket,)) client_thread.start() if __name__ == \\"__main__\\": main() ``` **Note:** Ensure that the certificate (`server_cert.pem`) and private key (`server_key.pem`) files are properly generated and accessible in the same directory as your script.","solution":"import socket import ssl import threading # Function to handle individual client connections def handle_client_connection(conn): try: print(f\\"Client connected: {conn.getpeername()}\\") while True: data = conn.recv(1024) if not data: break print(f\\"Received message: {data.decode()}\\") conn.sendall(data) print(f\\"Client disconnected: {conn.getpeername()}\\") except ssl.SSLError as e: print(f\\"SSL error: {e}\\") finally: conn.shutdown(socket.SHUT_RDWR) conn.close() def main(): context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH) context.load_cert_chain(certfile=\\"server_cert.pem\\", keyfile=\\"server_key.pem\\") context.options |= ssl.OP_NO_SSLv2 | ssl.OP_NO_SSLv3 context.minimum_version = ssl.TLSVersion.TLSv1_2 # Create a TCP/IP socket bind_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) bind_socket.bind((\'localhost\', 8443)) bind_socket.listen(5) print(\\"Server listening on port 8443...\\") while True: new_socket, from_addr = bind_socket.accept() ssl_socket = context.wrap_socket(new_socket, server_side=True) client_thread = threading.Thread(target=handle_client_connection, args=(ssl_socket,)) client_thread.start() if __name__ == \\"__main__\\": main()"},{"question":"Objective: To test your understanding of the `collections` module and your ability to use its specialized container datatypes effectively. Question: You are working on an application that processes and analyzes data from multiple sources. The data structure needed should be capable of efficiently handling the following operations: 1. Combine multiple dictionaries into a single view without merging them. 2. Maintain an ordered sequence of elements with the ability to efficiently add and remove elements from both ends. 3. Count the occurrence of various elements in a list. Write a Python function `process_data` that takes three arguments: 1. `data_sources`: A list of dictionaries containing key-value pairs. Each dictionary represents data from a different source. 2. `operations`: A list of operations to be performed on a deque. Each operation is a tuple where the first element is a string representing the operation (`\'append\'`, `\'appendleft\'`, `\'pop\'`, `\'popleft\'`) and the second element (if applicable) is the value to be added. 3. `elements`: A list of elements which need to be counted. Your task is to: 1. Use a `ChainMap` to combine the dictionaries in `data_sources`. 2. Perform the specified `operations` on a `deque`. 3. Use a `Counter` to count the occurrences of each element in `elements`. The function should return a dictionary with three keys: - `\'combined\'`: A ChainMap representing the combined view of `data_sources`. - `\'deque\'`: The final state of the deque after all `operations` have been applied. - `\'counts\'`: A Counter representing the counts of each element in `elements`. Function Signature: ```python from collections import ChainMap, deque, Counter def process_data(data_sources, operations, elements): pass ``` Example: ```python data_sources = [ {\'a\': 1, \'b\': 2}, {\'b\': 3, \'c\': 4}, {\'d\': 5} ] operations = [ (\'append\', 10), (\'appendleft\', 20), (\'pop\',), (\'popleft\',) ] elements = [\'apple\', \'banana\', \'apple\', \'orange\', \'banana\', \'apple\'] result = process_data(data_sources, operations, elements) # Expected Output: # { # \'combined\': ChainMap({\'a\': 1, \'b\': 2}, {\'b\': 3, \'c\': 4}, {\'d\': 5}), # \'deque\': deque([]), # \'counts\': Counter({\'apple\': 3, \'banana\': 2, \'orange\': 1}) # } ``` Constraints: 1. Each dictionary in `data_sources` has unique keys except where explicitly overlapping for testing purposes. 2. Operations provided in the `operations` list will be valid and within bounds for the `deque`. Notes: - Ensure that you handle different data sources without merging their values, preserving the hierarchy using ChainMap. - Implement all provided operations on the deque and handle any exceptions as required. - Accurately count all elements using the Counter and return the results in the specified format.","solution":"from collections import ChainMap, deque, Counter def process_data(data_sources, operations, elements): # Combine dictionaries using ChainMap combined = ChainMap(*data_sources) # Initialize and apply operations on a deque dq = deque() for operation in operations: if operation[0] == \'append\': dq.append(operation[1]) elif operation[0] == \'appendleft\': dq.appendleft(operation[1]) elif operation[0] == \'pop\': if dq: # Check to prevent pop from empty deque dq.pop() elif operation[0] == \'popleft\': if dq: # Check to prevent popleft from empty deque dq.popleft() # Count the occurrences of elements using Counter counts = Counter(elements) return { \'combined\': combined, \'deque\': dq, \'counts\': counts }"},{"question":"Coding Assessment Question # Objective Implement a distributed training setup using the `DynamicRendezvousHandler` for coordinating multiple training nodes. # Background In distributed training, multiple nodes (e.g., different GPUs on different machines) work together to train a model. One key aspect of this is handling the coordination between these nodes, often referred to as the rendezvous process. PyTorch provides several mechanisms (aka handlers) for this purpose, including the dynamic rendezvous handler. # Task You are required to: 1. Implement a class `DistributedTrainer` that sets up the rendezvous using `DynamicRendezvousHandler`. 2. Simulate a simple training loop where each node logs its progress. # Specifications 1. **Class: `DistributedTrainer`** - **Method: `__init__(self, rank: int, world_size: int, backend: str = \\"nccl\\")`** - **Parameters:** - `rank`: The rank of the current node. - `world_size`: The total number of nodes. - `backend`: The backend to use for distributed communication (default is `\\"nccl\\"`). - **Method: `setup_rendezvous(self, rendezvous_url: str)`** - Sets up the rendezvous using the `DynamicRendezvousHandler`. - **Parameters:** - `rendezvous_url`: The URL for rendezvous used to coordinate nodes. - **Method: `train(self, num_epochs: int)`** - Simulates a training loop where each node logs its progress. - **Parameters:** - `num_epochs`: The number of epochs to simulate. 2. **Constraints:** - You must use the `DynamicRendezvousHandler` class. - Handle any necessary exception cases related to the rendezvous process (e.g., timeouts, connection errors). 3. **Implementation Details:** - Use `torch.distributed` for the actual distributed setup. - Each node should log its rank and the current epoch during the training loop. 4. **Performance:** - Ensure that the rendezvous setup and training loop handle potential disruptions gracefully. - Make sure that all logs are consistent across different nodes. # Example Here is a sample code that shows how your class might be used: ```python # Sample usage if __name__ == \\"__main__\\": import os # Assume we get rank and world_size from the command line or environment variables rank = int(os.environ[\'RANK\']) world_size = int(os.environ[\'WORLD_SIZE\']) rendezvous_url = \\"tcp://localhost:29500\\" # Initialize DistributedTrainer trainer = DistributedTrainer(rank, world_size) # Setup rendezvous trainer.setup_rendezvous(rendezvous_url) # Start training trainer.train(num_epochs=10) ``` # Evaluation Criteria - Correctness of the rendezvous and training setup. - Proper error handling for distributed rendezvous. - Clarity and readability of the code. - Adherence to the specifications and constraints. Good luck!","solution":"import torch.distributed as dist import logging logging.basicConfig(level=logging.INFO) class DistributedTrainer: def __init__(self, rank: int, world_size: int, backend: str = \\"nccl\\"): self.rank = rank self.world_size = world_size self.backend = backend def setup_rendezvous(self, rendezvous_url: str): try: dist.init_process_group( backend=self.backend, init_method=rendezvous_url, world_size=self.world_size, rank=self.rank ) logging.info(f\\"Rendezvous setup complete for rank {self.rank}.\\") except Exception as e: logging.error(f\\"Failed to set up rendezvous: {e}\\") raise def train(self, num_epochs: int): if not dist.is_initialized(): raise RuntimeError(\\"Distributed process group is not initialized.\\") for epoch in range(num_epochs): logging.info(f\\"Rank {self.rank}, Epoch {epoch}\\") # Simulate some work torch.cuda.synchronize() if self.backend == \\"nccl\\" else None # Optionally, add barrier to synchronize all processes at the end of each epoch dist.barrier() logging.info(f\\"Rank {self.rank} completed training for {num_epochs} epochs.\\")"},{"question":"Question: Shell Pipeline Interface Implementation # Objective You are required to create a pipeline interface using the deprecated `pipes` module in Python and then migrate the implementation to the `subprocess` module, as used in modern Python development. # Requirements **Part 1: Using the `pipes` module** Implement a function `transform_file_with_pipes(infile: str, outfile: str, commands: List[Tuple[str, str]]) -> None` that: 1. Takes an input file `infile`. 2. Writes the transformed contents to `outfile`. 3. Applies a series of shell commands provided in the `commands` list to the contents of `infile`. Each command is a tuple `(cmd, kind)`, where `cmd` is a valid bourne shell command and `kind` specifies the command type (e.g., `\'-\'`, `\'f\'`, etc. as described in the documentation). **Part 2: Using the `subprocess` module** Implement a function `transform_file_with_subprocess(infile: str, outfile: str, commands: List[str]) -> None` that: 1. Takes an input file `infile`. 2. Writes the transformed contents to `outfile`. 3. Applies a series of shell commands provided in the `commands` list to the contents of `infile`. Each command is a string that should be executed in a subprocess. # Input/Output Formats **Input:** - `infile`: A string representing the path to the input file. - `outfile`: A string representing the path to the output file. - `commands`: A list of tuples/strings containing shell commands to be executed on the file content. **Output:** - The function should not return anything. It should write the transformed content to the `outfile`. # Constraints - Assume all input paths are valid and the user has necessary permissions to read and write files. - Assume shell commands in `commands` are valid. - The transformation of the file should be performed in the order the commands are specified in the `commands` list. # Examples Part 1: Using `pipes` ```python # Content of file \'input.txt\': \\"hello world\\" transform_file_with_pipes(\'input.txt\', \'output.txt\', [(\'tr a-z A-Z\', \'--\')]) # Content of file \'output.txt\' should be: \\"HELLO WORLD\\" ``` Part 2: Using `subprocess` ```python # Content of file \'input.txt\': \\"hello world\\" transform_file_with_subprocess(\'input.txt\', \'output.txt\', [\'tr a-z A-Z\']) # Content of file \'output.txt\' should be: \\"HELLO WORLD\\" ``` # Additional Notes - Students must ensure that they handle opening and closing of files properly. - Error handling for command execution should be considered for the `subprocess` implementation. # Performance Efficiency in reading and writing files as well as command execution should be considered. The implementation using `subprocess` should aim to use pipes to chain commands, similar to how the `pipes` module works.","solution":"import subprocess from typing import List, Tuple def transform_file_with_pipes(infile: str, outfile: str, commands: List[Tuple[str, str]]) -> None: import pipes pipe = pipes.Template() for cmd, kind in commands: pipe.append(cmd, kind) pipe.copy(infile, outfile) def transform_file_with_subprocess(infile: str, outfile: str, commands: List[str]) -> None: with open(outfile, \'w\') as out_file: process = subprocess.Popen([\'cat\', infile], stdout=subprocess.PIPE) for cmd in commands: process_cmd = subprocess.Popen(cmd, shell=True, stdin=process.stdout, stdout=subprocess.PIPE) process.stdout.close() process = process_cmd out_file.write(process.communicate()[0].decode())"},{"question":"# **Attention-Based Sequence Weighting Using PyTorch** **Objective:** Your task is to implement a custom attention mechanism to compute weighted sequences using PyTorch. This will test your understanding of attention mechanisms and your ability to apply PyTorchs `torch.nn` functionalities. **Background:** Attention mechanisms are widely used in neural network models to assign different weights to different parts of the input sequence. This allows the model to focus on important parts while making decisions. **Requirements:** 1. Your implementation should define a class `CustomAttention` that inherits from `torch.nn.Module`. 2. Your class should include methods for: - Initializing the attention parameters. - Computing attention scores using a similarity function. - Applying the attention scores to the input sequence to produce a weighted output. 3. The attention mechanism should be compatible with batch processing. **Inputs:** - A batch of sequences represented as a PyTorch tensor of shape `(batch_size, sequence_length, input_dim)`. **Outputs:** - A batch of weighted sequences of the same shape `(batch_size, sequence_length, input_dim)`, but with values transformed by attention weights. **Constraints:** - Use only PyTorch library functionalities (`torch` and `torch.nn`). **Performance:** - Ensure the implementation is efficient and can handle large batches and long sequences. **Template:** Below is a template to get you started. You will need to complete the implementation details. ```python import torch import torch.nn as nn import torch.nn.functional as F class CustomAttention(nn.Module): def __init__(self, input_dim): super(CustomAttention, self).__init__() # Initialize attention parameters here self.attention_weights = nn.Parameter(torch.Tensor(input_dim, input_dim)) self.reset_parameters() def reset_parameters(self): # Initialize parameters randomly nn.init.xavier_uniform_(self.attention_weights) def forward(self, sequences): sequences: Tensor of shape (batch_size, sequence_length, input_dim) # Compute attention scores here attention_scores = torch.matmul(sequences, self.attention_weights) attention_scores = F.softmax(attention_scores, dim=-1) # Apply softmax to get probabilities # Compute weighted sequences weighted_sequences = attention_scores * sequences return weighted_sequences # Example usage: if __name__ == \\"__main__\\": input_dim = 10 batch_size = 5 sequence_length = 7 sequences = torch.randn(batch_size, sequence_length, input_dim) attention_layer = CustomAttention(input_dim) weighted_sequences = attention_layer(sequences) print(weighted_sequences.shape) # Should be (batch_size, sequence_length, input_dim) ``` Please complete the class implementation to fulfil the requirements outlined above, ensuring your code is efficient and leverages PyTorch functionalities appropriately. **Good luck!**","solution":"import torch import torch.nn as nn import torch.nn.functional as F class CustomAttention(nn.Module): def __init__(self, input_dim): super(CustomAttention, self).__init__() self.attention_weights = nn.Parameter(torch.Tensor(input_dim, input_dim)) self.reset_parameters() def reset_parameters(self): nn.init.xavier_uniform_(self.attention_weights) def forward(self, sequences): sequences: Tensor of shape (batch_size, sequence_length, input_dim) # Compute attention scores attention_scores = torch.matmul(sequences, self.attention_weights) attention_scores = torch.matmul(attention_scores, sequences.transpose(1, 2)) attention_scores = F.softmax(attention_scores, dim=-1) # Compute weighted sequences weighted_sequences = torch.matmul(attention_scores, sequences) return weighted_sequences"},{"question":"Objective: Implement a function that processes multiple independent tasks asynchronously using PyTorch `Future` objects and the utility functions `collect_all` and `wait_all`. The goal is to demonstrate comprehension of asynchronous programming and the use of futures in PyTorch. Problem Statement: You are tasked with creating a simple simulation to process multiple tasks in a distributed manner. Each task will simply return its identifier (ID) after a random delay, mimicking some processing time. You will then need to collect the results of all tasks and return them. Requirements: 1. **Function Name:** `execute_tasks` 2. **Input:** A single integer, `n`, representing the number of tasks. 3. **Output:** A list of integers representing the IDs of the tasks as they complete. 4. **Constraints:** - `1 <= n <= 100` - Simulate a random processing time for each task between 0.1 and 1.0 seconds. 5. **Performance Requirements:** Must make efficient use of asynchronous execution to minimize the total runtime. Implementation Details: 1. Create a function `execute_task(task_id)` that: - Accepts a single integer `task_id` as input. - Simulates a random delay between 0.1 and 1.0 seconds using `torch.sleep` or Python\'s `time.sleep`. - Returns the `task_id`. 2. Create the main function `execute_tasks(n)` that: - Initializes `n` tasks using the `execute_task` function and wraps them in `Future` objects. - Utilizes `torch.futures.collect_all` or `torch.futures.wait_all` to wait for all tasks to complete. - Returns a list of task IDs in the order they complete. Example: ```python import time import torch import random def execute_task(task_id): delay = random.uniform(0.1, 1.0) time.sleep(delay) return task_id def execute_tasks(n): futures = [] for task_id in range(n): futures.append(torch.futures.Future().set_result(execute_task(task_id))) results = torch.futures.wait_all(futures) task_ids = [result.item() for result in results] return task_ids # Example Usage n = 5 print(execute_tasks(n)) # Output: [4, 1, 0, 3, 2] (example output; actual order may vary due to random delays) ``` Notes: - Make sure to use appropriate random delay to simulate real processing time. - The order of completed task IDs may vary due to the randomness in delays.","solution":"import torch import asyncio import random async def execute_task(task_id): delay = random.uniform(0.1, 1.0) await asyncio.sleep(delay) return task_id async def execute_tasks_async(n): tasks = [] for task_id in range(n): tasks.append(asyncio.ensure_future(execute_task(task_id))) completed_tasks = await asyncio.gather(*tasks) return completed_tasks def execute_tasks(n): This function initializes \'n\' tasks asynchronously and returns the results of those tasks once they are completed. # Run the async function using asyncio.run return asyncio.run(execute_tasks_async(n))"},{"question":"# Task Using the `email.message.EmailMessage` class, implement the function `create_email_with_attachments` which constructs an email message with multiple attachments. The function should accept parameters for the sender, recipient, subject, body text, and a list of attachments. Each attachment should be added to the email as a separate part, and the final email should be serialized to a string format. # Function Signature ```python def create_email_with_attachments(sender: str, recipient: str, subject: str, body: str, attachments: list) -> str: pass ``` # Input - `sender` (str): The email address of the sender. - `recipient` (str): The email address of the recipient. - `subject` (str): The subject of the email. - `body` (str): The main body text of the email. - `attachments` (list): A list of tuples where each tuple contains the filename (str) and file content (bytes). # Output - Returns a string representing the entire email message serialized, including the headers and the text and attachments in proper MIME format. # Example ```python attachments = [ (\\"file1.txt\\", b\\"Hello, this is the content of file1.\\"), (\\"file2.txt\\", b\\"Hello, this is the content of file2.\\") ] print(create_email_with_attachments(\\"sender@example.com\\", \\"recipient@example.com\\", \\"Test Email\\", \\"This is the body of the email.\\", attachments)) ``` # Constraints - Use the `EmailMessage` class and its methods as described in the documentation. - Ensure the email is properly formatted in terms of headers and MIME structure. - Handle both text and binary attachments correctly. # Guidelines - Create a new `EmailMessage` object. - Set the sender, recipient, and subject headers using `email.message.EmailMessage` methods. - Add the body text to the email. - Iterate over the attachments list and add each file as a separate attachment using appropriate methods. - Serialize the email to a string format and return it. - Ensure the output string is properly formatted according to MIME standards. # Notes - Consider using methods like `set_content`, `add_attachment`, and `as_string` to create and serialize the email. - Pay attention to the correct MIME type for each attachment. - Make sure the headers and payloads are set correctly for each part of the email.","solution":"from email.message import EmailMessage import mimetypes def create_email_with_attachments(sender: str, recipient: str, subject: str, body: str, attachments: list) -> str: Creates an email message with multiple attachments and returns it as a string. Parameters: - sender: email address of the sender. - recipient: email address of the recipient. - subject: subject of the email. - body: main body text of the email. - attachments: list of tuples indicating (filename, file content in bytes). Returns: - The entire email message serialized as a string. # Create the email message object msg = EmailMessage() # Set the email headers msg[\'From\'] = sender msg[\'To\'] = recipient msg[\'Subject\'] = subject # Set the body of the email msg.set_content(body) # Add each attachment for filename, file_content in attachments: # Guess the MIME type based on the file extension mime_type, _ = mimetypes.guess_type(filename) # Split the MIME type into main type and sub type if mime_type is None: mime_type = \'application/octet-stream\' main_type, sub_type = mime_type.split(\'/\', 1) # Add attachment to the email msg.add_attachment(file_content, maintype=main_type, subtype=sub_type, filename=filename) # Return the entire email message as a string return msg.as_string()"},{"question":"# Question: Data Archiver using bz2 You are required to implement a Python function to create a simple data archiver utility that can compress and decompress files using the bzip2 algorithm. Your task involves two main operations: compressing all files in a directory into individual bzip2-compressed files and decompressing these files back. Function Signature ```python def data_archiver(input_dir: str, output_dir: str, operation: str) -> None: ``` Parameters - `input_dir` (str): The path to the input directory containing files to be compressed or decompressed. - `output_dir` (str): The path to the output directory where compressed or decompressed files will be saved. - `operation` (str): The operation to perform, either `\\"compress\\"` or `\\"decompress\\"`. Constraints - The function should return `None`. - You may assume that `input_dir` and `output_dir` exist and are valid directories. - If `operation` is `\\"compress\\"`, compress each file in `input_dir` and save the compressed file in `output_dir` with the same filename but with a `.bz2` extension appended. - If `operation` is `\\"decompress\\"`, decompress each `.bz2` file in `input_dir` and save the decompressed file in `output_dir` with the `.bz2` extension removed. - Handle binary files; thus, use binary modes for reading and writing files. - Maintain the data integrity between original and decompressed files. Example Usage ```python # To compress files in \'data/\' directory and save compressed files to \'archive/\' directory data_archiver(\'data\', \'archive\', \'compress\') # To decompress files in \'archive/\' directory and save decompressed files to \'data/\' directory data_archiver(\'archive\', \'data\', \'decompress\') ``` Notes - You should use the appropriate classes and functions from the `bz2` module as described in the provided documentation. - Ensure that the function handles all necessary exceptions and edge cases, such as incorrect file formats and read/write errors.","solution":"import os import bz2 def data_archiver(input_dir: str, output_dir: str, operation: str) -> None: Archives files from input_dir and writes to output_dir based on the chosen operation. Parameters: - input_dir (str): The path to the directory containing the input files. - output_dir (str): The path to the directory for the output files. - operation (str): Either \'compress\' or \'decompress\'. if operation not in {\\"compress\\", \\"decompress\\"}: raise ValueError(\\"Operation must be \'compress\' or \'decompress\'\\") for file_name in os.listdir(input_dir): input_file_path = os.path.join(input_dir, file_name) if operation == \\"compress\\": if os.path.isfile(input_file_path): # Ensure it\'s a file output_file_path = os.path.join(output_dir, file_name + \'.bz2\') with open(input_file_path, \'rb\') as input_file, bz2.BZ2File(output_file_path, \'wb\') as output_file: output_file.write(input_file.read()) elif operation == \\"decompress\\": if file_name.endswith(\'.bz2\'): output_file_path = os.path.join(output_dir, file_name[:-4]) with bz2.BZ2File(input_file_path, \'rb\') as input_file, open(output_file_path, \'wb\') as output_file: output_file.write(input_file.read())"},{"question":"# PyTorch Numerical Properties Assessment Objective: You are to write a function that provides a summary of the numerical properties of a given dtype in PyTorch, using the `torch.finfo` and `torch.iinfo` classes. Function Signature: ```python def dtype_summary(dtype: torch.dtype) -> dict: This function takes a PyTorch dtype and returns a summary of its numerical properties. The summary will be in the form of a dictionary with keys corresponding to property names. Parameters: dtype (torch.dtype): The dtype for which the numerical properties need to be summarized. Returns: dict: A dictionary containing the numerical properties of the dtype. For floating-point dtypes, the dictionary should contain: - \'bits\': Number of bits occupied by the type. - \'eps\': The smallest representable number such that 1.0 + eps != 1.0. - \'max\': The largest representable number. - \'min\': The smallest representable number (typically -max). - \'tiny\': The smallest positive normal number. - \'resolution\': The approximate decimal resolution of this type. For integer dtypes, the dictionary should contain: - \'bits\': Number of bits occupied by the type. - \'max\': The largest representable number. - \'min\': The smallest representable number. Raises: ValueError: If the dtype is neither a floating-point nor an integer dtype supported by PyTorch. pass ``` Input: - `dtype`: A PyTorch dtype, which can be a floating-point type (e.g., `torch.float32`) or an integer type (e.g., `torch.int32`). Output: - A dictionary summarizing the numerical properties of the given dtype. Constraints: - The function should handle both floating-point and integer dtypes supported by PyTorch. - If an unsupported dtype is given, the function should raise a `ValueError` with an appropriate message. Example Usage: ```python import torch # Example for a floating-point dtype print(dtype_summary(torch.float32)) # Expected output: # { # \'bits\': 32, # \'eps\': 1.1920928955078125e-07, # \'max\': 3.4028234663852886e+38, # \'min\': -3.4028234663852886e+38, # \'tiny\': 1.1754943508222875e-38, # \'resolution\': 1e-06 # } # Example for an integer dtype print(dtype_summary(torch.int32)) # Expected output: # { # \'bits\': 32, # \'max\': 2147483647, # \'min\': -2147483648 # } ``` Notes: - Use `torch.finfo` for floating-point dtypes and `torch.iinfo` for integer dtypes. - Ensure to handle edge cases and invalid inputs gracefully.","solution":"import torch def dtype_summary(dtype: torch.dtype) -> dict: This function takes a PyTorch dtype and returns a summary of its numerical properties. The summary will be in the form of a dictionary with keys corresponding to property names. Parameters: dtype (torch.dtype): The dtype for which the numerical properties need to be summarized. Returns: dict: A dictionary containing the numerical properties of the dtype. For floating-point dtypes, the dictionary should contain: - \'bits\': Number of bits occupied by the type. - \'eps\': The smallest representable number such that 1.0 + eps != 1.0. - \'max\': The largest representable number. - \'min\': The smallest representable number (typically -max). - \'tiny\': The smallest positive normal number. - \'resolution\': The approximate decimal resolution of this type. For integer dtypes, the dictionary should contain: - \'bits\': Number of bits occupied by the type. - \'max\': The largest representable number. - \'min\': The smallest representable number. Raises: ValueError: If the dtype is neither a floating-point nor an integer dtype supported by PyTorch. if dtype in [torch.float16, torch.float32, torch.float64, torch.bfloat16]: info = torch.finfo(dtype) return { \'bits\': info.bits, \'eps\': info.eps, \'max\': info.max, \'min\': info.min, \'tiny\': info.tiny, \'resolution\': info.resolution } elif dtype in [torch.int8, torch.int16, torch.int32, torch.int64, torch.uint8]: info = torch.iinfo(dtype) return { \'bits\': info.bits, \'max\': info.max, \'min\': info.min } else: raise ValueError(f\\"Unsupported dtype: {dtype}\\")"},{"question":"# Netrc Parser Enhancement # Objective: Implement a class `EnhancedNetrc` that extends the functionality of the `netrc.netrc` class to include additional features. # Requirements: 1. **Initialization** Your `EnhancedNetrc` class must accept an optional file path just like the `netrc.netrc` class. If no path is provided, it defaults to ~/.netrc file. 2. **Method: expired_authenticators** Add a method `expired_authenticators` which takes two arguments: - `file`: the path to a file containing host names, one per line. - `expiry_days`: an integer specifying the number of days beyond which the credentials are considered expired. This method must: - Compare the current date with a mock \\"last_used_date\\" (for simplicity, assume each host entry last used date is the date the entry was added). - Return a list of host names whose last usage exceeds the given expiry days. 3. **Method: update_password** Add a method `update_password` to update the password for a specified host. - If the host does not exist, raise an appropriate custom exception, `HostNotFound`. - If the host exists, update the password. 4. **Exception Handling** Implement proper exception handling for file read/write errors and incorrect file formats. Provide meaningful error messages. # Expected Input/Output: 1. **Initialization:** - Input: `\\"path/to/netrc/file\\"` - Behavior: Initializes the `EnhancedNetrc` object with the provided netrc file. 2. **expired_authenticators(file, expiry_days):** - Input: `file=\\"path/to/hosts.txt\\", expiry_days=30` - Output: `[\'expired_host1\', \'expired_host2\']` 3. **update_password(host, new_password):** - Input: `host=\\"existing_host\\", new_password=\\"new_secret_password\\"` - Output: `None` (But the password for the specified host is updated in the netrc file) - Exception if the host does not exist: `HostNotFound` # Constraints: - Assume the netrc file being used follows the standard netrc file format and the hosts file provided contains valid ASCII host names. - The `last_used_date` mock data can be implemented as a dictionary within the class for simplicity. ```python import netrc import os from datetime import datetime, timedelta class HostNotFound(Exception): pass class EnhancedNetrc(netrc.netrc): def __init__(self, file=None): super().__init__(file) self._last_used_dates = {host: datetime.now() - timedelta(days=i*10) for i, host in enumerate(self.hosts.keys())} def expired_authenticators(self, file, expiry_days): expired_hosts = [] try: with open(file, \'r\') as f: hosts = f.read().splitlines() except Exception as e: raise Exception(f\\"Error reading hosts file: {e}\\") expiry_threshold = datetime.now() - timedelta(days=expiry_days) for host in hosts: last_used = self._last_used_dates.get(host) if last_used and last_used < expiry_threshold: expired_hosts.append(host) return expired_hosts def update_password(self, host, new_password): if host not in self.hosts: raise HostNotFound(f\\"Host \'{host}\' not found.\\") login, account, _ = self.hosts[host] self.hosts[host] = (login, account, new_password) # Update the actual file logic here if necessary # Use this main block to run some simple tests if __name__ == \\"__main__\\": netrc_file = \\"path/to/.netrc\\" hosts_file = \\"path/to/hosts.txt\\" parser = EnhancedNetrc(file=netrc_file) expired_hosts = parser.expired_authenticators(file=hosts_file, expiry_days=30) print(\\"Expired hosts:\\", expired_hosts) parser.update_password(\\"existing_host\\", \\"new_secret_password\\") try: parser.update_password(\\"non_existing_host\\", \\"new_secret_password\\") except HostNotFound as e: print(e) ```","solution":"import netrc import os from datetime import datetime, timedelta class HostNotFound(Exception): pass class EnhancedNetrc(netrc.netrc): def __init__(self, file=None): super().__init__(file) # Mock data for last used dates, using days since each host was added self._last_used_dates = {host: datetime.now() - timedelta(days=i*10) for i, host in enumerate(self.hosts.keys())} def expired_authenticators(self, file, expiry_days): expired_hosts = [] try: with open(file, \'r\') as f: hosts = f.read().splitlines() except Exception as e: raise Exception(f\\"Error reading hosts file: {e}\\") expiry_threshold = datetime.now() - timedelta(days=expiry_days) for host in hosts: last_used = self._last_used_dates.get(host) if last_used and last_used < expiry_threshold: expired_hosts.append(host) return expired_hosts def update_password(self, host, new_password): if host not in self.hosts: raise HostNotFound(f\\"Host \'{host}\' not found.\\") login, account, _ = self.hosts[host] self.hosts[host] = (login, account, new_password) # Update the actual file logic here if necessary"},{"question":"# Coding Challenge: Custom Generator Implementation and Validation You are tasked with creating a custom generator in Python and implementing functions to dynamically create and validate generator objects. Use Python\'s native capabilities to implement the following functionalities as described below. Part 1: Custom Generator Function Implement a generator function called `custom_sequence_generator` that yields an arithmetic sequence of numbers starting from a provided `start` value, incrementing by a `step` value, and stopping before the `stop` value. **Parameters**: - `start` (int): The starting value of the sequence. - `stop` (int): The sequence will stop before this value. - `step` (int): The increment between subsequent values in the sequence. **Yields**: - Yields an integer at each step in the sequence. Part 2: Generator Creation and Validation 1. Implement a function `create_generator_object` that creates a generator object from a provided frame. As a simulation (since we are not working with actual frames and PyObject here), this function will generate a sequence using the `custom_sequence_generator` function. **Parameters**: - `frame_data` (tuple): A tuple containing `start`, `stop`, and `step` values. **Returns**: - Returns a generator object that produces the desired sequence. 2. Implement a function `is_generator_object` that checks if a given object is a generator. **Parameters**: - `obj` (any): The object to check. **Returns**: - Returns `True` if the object is a generator, `False` otherwise. Constraints - You may use Python\'s built-in `inspect` module to assist with checking generator objects. - Efficiently handle the generation and validation processes. Example Usage ```python # Part 1: Creating a custom generator function g = custom_sequence_generator(1, 10, 2) print(list(g)) # Output: [1, 3, 5, 7, 9] # Part 2: Creating and validating generators gen_obj = create_generator_object((1, 10, 2)) print(is_generator_object(gen_obj)) # Output: True print(list(gen_obj)) # Output: [1, 3, 5, 7, 9] non_gen_obj = [1, 2, 3] print(is_generator_object(non_gen_obj)) # Output: False ``` Implement the required functions adhering to the specified input-output formats and constraints.","solution":"def custom_sequence_generator(start, stop, step): Yields an arithmetic sequence of numbers starting from `start`, incrementing by `step`, and stopping before `stop`. current = start while current < stop: yield current current += step def create_generator_object(frame_data): Creates a generator object from a provided tuple of frame data. Parameters: - frame_data (tuple): A tuple containing `start`, `stop`, and `step` values. Returns: - A generator object that produces the desired sequence. start, stop, step = frame_data return custom_sequence_generator(start, stop, step) def is_generator_object(obj): Checks if a given object is a generator. Parameters: - obj (any): The object to check. Returns: - True if the object is a generator, False otherwise. import inspect return inspect.isgenerator(obj)"},{"question":"**Objective:** Demonstrate your understanding of managing subprocesses using PyTorch\'s `SubprocessHandler`. # Problem Statement You are tasked with implementing a function that uses PyTorch\'s `SubprocessHandler` to spawn a specified number of subprocesses in parallel. Each subprocess should compute the sum of a provided list of numbers. # Function Signature ```python def parallel_sum(num_subprocesses: int, numbers: List[List[int]]) -> List[int]: Spawns a specified number of subprocesses to compute the sum of each sublist of numbers in parallel. Parameters: num_subprocesses (int): Number of subprocesses to spawn. numbers (List[List[int]]): A list containing sublists of integers to be summed by subprocesses. Returns: List[int]: A list containing the sum of each sublist, computed by respective subprocesses. ``` # Input - `num_subprocesses`: An integer specifying the number of subprocesses to spawn. - `numbers`: A list of lists, where each inner list contains integers. Each of these sublists should be summed by a different subprocess. # Output - A list of integers where each integer is the sum of the integers in one of the sublists provided. # Example ```python # Example input num_subprocesses = 3 numbers = [ [1, 2, 3], [4, 5, 6], [7, 8, 9] ] # Expected output # Each subprocess should compute the sum of each sublist. # The output for the above example should be [6, 15, 24] expected_output = [6, 15, 24] print(parallel_sum(num_subprocesses, numbers)) # Output: [6, 15, 24] ``` # Constraints - You must use `torch.distributed.elastic.multiprocessing.subprocess_handler.SubprocessHandler` to manage the subprocesses. - Ensure that the system managing the subprocesses handles errors gracefully and ensures that all subprocesses complete before returning the results. - Your implementation should be efficient in managing multiple parallel subprocesses. # Notes - Ensure you handle edge cases, such as when the input list is empty or when `num_subprocesses` is greater or less than the number of sublists provided. - Thoroughly test your implementation with different inputs to ensure robustness.","solution":"import torch.multiprocessing as mp from typing import List def sum_sublist(queue, sublist): Worker function to compute the sum of the sublist and put the result in the queue. queue.put(sum(sublist)) def parallel_sum(num_subprocesses: int, numbers: List[List[int]]) -> List[int]: Spawns a specified number of subprocesses to compute the sum of each sublist of numbers in parallel. Parameters: num_subprocesses (int): Number of subprocesses to spawn. numbers (List[List[int]]): A list containing sublists of integers to be summed by subprocesses. Returns: List[int]: A list containing the sum of each sublist, computed by respective subprocesses. if len(numbers) == 0: return [] processes = [] queue = mp.Queue() results = [] # Spawn subprocesses for sublist in numbers: p = mp.Process(target=sum_sublist, args=(queue, sublist)) processes.append(p) p.start() # Collect results from queue for _ in numbers: results.append(queue.get()) # Ensure all processes have finished execution for p in processes: p.join() return results"},{"question":"# Task You are tasked with creating a custom enumeration to manage different states of online orders in an e-commerce setting. The states can be combined using bitwise operations and should be easy to compare and manipulate programmatically. Additionally, you should implement certain methods to enhance the functionality. # Requirements 1. **Define an Enumeration Class:** - Create an `OrderState` class using `IntFlag` to represent the following states: - `PENDING`: Order is pending confirmation (`1`) - `CONFIRMED`: Order is confirmed (`2`) - `SHIPPED`: Order has been shipped (`4`) - `DELIVERED`: Order has been delivered (`8`) - `CANCELLED`: Order has been cancelled (`16`) 2. **Combine States:** - Define named combinations of certain states: - `IN_TRANSIT` to represent an order that has been confirmed and shipped. - `COMPLETED` to represent an order that has been delivered or cancelled. 3. **Implement a Method:** - Add a method `is_in_progress` to the `OrderState` class to determine if the order is in any of the following states: `PENDING`, `CONFIRMED`, or `SHIPPED`. 4. **Write a Tester Function:** - Implement a function `test_order_state` to create various `OrderState` instances and demonstrate: - Combining states. - Using the `is_in_progress` method. # Constraints: - Use the `auto` function to generate values for these states automatically within the implementation. - Ensure that the `is_in_progress` method can be called on individual states and combinations of states. # Sample Output ```python Testing combined states print(OrderState.IN_TRANSIT) # Output: OrderState.CONFIRMED | OrderState.SHIPPED print(OrderState.COMPLETED) # Output: OrderState.DELIVERED | OrderState.CANCELLED Testing `is_in_progress` method print(OrderState.PENDING.is_in_progress()) # Output: True print(OrderState.CONFIRMED.is_in_progress()) # Output: True print(OrderState.SHIPPED.is_in_progress()) # Output: True print(OrderState.DELIVERED.is_in_progress()) # Output: False print(OrderState.CANCELLED.is_in_progress()) # Output: False print((OrderState.PENDING | OrderState.CONFIRMED).is_in_progress()) # Output: True ``` # Implementation Please provide the `OrderState` class and the `test_order_state` function in your solution.","solution":"from enum import IntFlag, auto class OrderState(IntFlag): PENDING = auto() CONFIRMED = auto() SHIPPED = auto() DELIVERED = auto() CANCELLED = auto() # Define named combinations of states IN_TRANSIT = CONFIRMED | SHIPPED COMPLETED = DELIVERED | CANCELLED def is_in_progress(self): return bool(self & (OrderState.PENDING | OrderState.CONFIRMED | OrderState.SHIPPED)) def test_order_state(): Testing combined states print(OrderState.IN_TRANSIT) # Output: OrderState.CONFIRMED | OrderState.SHIPPED print(OrderState.COMPLETED) # Output: OrderState.DELIVERED | OrderState.CANCELLED Testing `is_in_progress` method print(OrderState.PENDING.is_in_progress()) # Output: True print(OrderState.CONFIRMED.is_in_progress()) # Output: True print(OrderState.SHIPPED.is_in_progress()) # Output: True print(OrderState.DELIVERED.is_in_progress()) # Output: False print(OrderState.CANCELLED.is_in_progress()) # Output: False print((OrderState.PENDING | OrderState.CONFIRMED).is_in_progress()) # Output: True"},{"question":"**Question: Implement a Custom Caching Module for Line Retrieval** # Problem Statement Your task is to implement a custom caching module similar to Python\'s `linecache` module. The goal is to efficiently retrieve specific lines from text files using a caching mechanism. You will need to implement three functions: `getline`, `clearcache`, and `checkcache`. # Function Signature ```python def getline(filename: str, lineno: int) -> str: pass def clearcache(): pass def checkcache(filename: str = None): pass ``` # Requirements 1. **getline(filename, lineno)**: - **Input**: `filename` (string) - the name of the file; `lineno` (int) - the line number to retrieve. - **Output**: Returns the line at the specified number. If an error occurs (e.g., file not found, line number out of range), return an empty string `\\"\\"`. - **Behavior**: Lines should be cached so that subsequent calls to `getline()` for the same file/line are efficient. 2. **clearcache()**: - **Behavior**: Clear the entire cache. 3. **checkcache(filename=None)**: - **Input**: `filename` (string, optional) - if provided, validate the cache entry for this file. - **Behavior**: Ensure the cache is up-to-date. If a filename is provided and the file has changed, update the cache for that file. If no filename is provided, check all cached files. # Constraints - You cannot use the built-in `linecache` module for this implementation. - You should handle large files efficiently, assuming that only a few lines will be accessed repeatedly. - Assume file encodings are in UTF-8. # Example Usage ```python # Assuming \'file.txt\' contains: # Line 1 # Line 2 # Line 3 print(getline(\'file.txt\', 2)) # Output: \'Line 2n\' print(getline(\'file.txt\', 5)) # Output: \'\' (line does not exist) clearcache() # Load a file and get updated line if it has changed checkcache(\'file.txt\') # After some changes in \'file.txt\', the cache should update print(getline(\'file.txt\', 1)) # Output should reflect the updated content of line 1 ``` # Notes - Your implementation should focus on efficiently managing the cache and handling file I/O operations. - Make sure to handle edge cases such as non-existent files or lines beyond the end of the file gracefully.","solution":"import os import time _cache = {} def getline(filename: str, lineno: int) -> str: Retrieve a specific line from a file. if lineno < 1: return \\"\\" # Check if file is in cache if filename in _cache: lines, timestamp = _cache[filename] # Return line if within variety, else return empty string if 1 <= lineno <= len(lines): return lines[lineno - 1] return \\"\\" # Read the file and store in cache if new and valid try: with open(filename, \'r\', encoding=\'utf-8\') as f: lines = f.readlines() _cache[filename] = (lines, os.path.getmtime(filename)) if 1 <= lineno <= len(lines): return lines[lineno - 1] return \\"\\" except (FileNotFoundError, IOError): return \\"\\" def clearcache(): Clears the entire cache. global _cache _cache = {} def checkcache(filename: str = None): Ensure cache is up-to-date for a given file or all files. global _cache if filename is not None: if filename in _cache: _, cached_timestamp = _cache[filename] try: current_timestamp = os.path.getmtime(filename) if current_timestamp > cached_timestamp: # If file has changed with open(filename, \'r\', encoding=\'utf-8\') as f: lines = f.readlines() _cache[filename] = (lines, current_timestamp) except FileNotFoundError: del _cache[filename] else: filenames = list(_cache.keys()) for fname in filenames: checkcache(fname)"},{"question":"**Question:** You are tasked with designing a simple inventory management system that uses dataclasses to store item data. Implement the following requirements using the `dataclasses` module: 1. Define a dataclass named `InventoryItem` with the following fields: - `name` (type: `str`): The name of the inventory item. - `unit_price` (type: `float`): The price per unit of the item. - `quantity_on_hand` (type: `int`): The number of units currently in stock, with a default value of 0. The class should include a method `total_cost` that calculates and returns the total cost of the inventory on hand for that item `(unit_price * quantity_on_hand)`. 2. Define another dataclass named `Inventory` to manage a collection of `InventoryItem` objects. This dataclass should include: - `items` (type: `List[InventoryItem]`): A list of `InventoryItem`s, with a default empty list provided by a `default_factory`. The class should include methods to: - `add_item(item: InventoryItem)`: Add a new item to the inventory. - `remove_item(name: str)`: Remove an item from the inventory by its name. - `inventory_value()`: Calculate and return the total value of all items in the inventory. 3. Ensure the `InventoryItem` class uses a custom `__repr__` method that displays the name, unit price, and quantity on hand. 4. Use the `asdict` function to provide a summary of the inventory in dictionary format, and the `astuple` function to provide the same summary in tuple format. **Constraints:** - Do not allow any duplicate items by name in the inventory. If an item with the same name is added, update its `quantity_on_hand` instead. - Ensure your classes use the `dataclasses` decorator appropriately and utilize the features provided by the module to simplify coding. **Example:** ```python # Sample usage of Inventory management item1 = InventoryItem(name=\\"Widget\\", unit_price=2.5, quantity_on_hand=10) item2 = InventoryItem(name=\\"Gadget\\", unit_price=5.0, quantity_on_hand=15) inventory = Inventory() inventory.add_item(item1) inventory.add_item(item2) print(inventory.inventory_value()) # Output: 100.0 (25 + 75) inventory.add_item(InventoryItem(name=\\"Widget\\", unit_price=2.5, quantity_on_hand=5)) print(inventory.inventory_value()) # Output: 112.5 (now 15 widgets and 15 gadgets) print(asdict(inventory)) # {\'items\': [{\'name\': \'Widget\', \'unit_price\': 2.5, \'quantity_on_hand\': 15}, {\'name\': \'Gadget\', \'unit_price\': 5.0, \'quantity_on_hand\': 15}]} print(astuple(inventory)) # ([(\'Widget\', 2.5, 15), (\'Gadget\', 5.0, 15)],) ``` Implement the classes based on the above requirements.","solution":"from dataclasses import dataclass, field, asdict, astuple from typing import List @dataclass class InventoryItem: name: str unit_price: float quantity_on_hand: int = 0 def total_cost(self) -> float: return self.unit_price * self.quantity_on_hand def __repr__(self) -> str: return f\\"InventoryItem(name=\'{self.name}\', unit_price={self.unit_price}, quantity_on_hand={self.quantity_on_hand})\\" @dataclass class Inventory: items: List[InventoryItem] = field(default_factory=list) def add_item(self, item: InventoryItem): for existing_item in self.items: if existing_item.name == item.name: existing_item.quantity_on_hand += item.quantity_on_hand return self.items.append(item) def remove_item(self, name: str): self.items = [item for item in self.items if item.name != name] def inventory_value(self) -> float: return sum(item.total_cost() for item in self.items)"},{"question":"Objective Implement a Python function that takes a bytes object as input and performs the following sequence of conversions using the `binascii` module: 1. Convert the bytes object to a base64-encoded ASCII string. 2. Convert the base64-encoded ASCII string back to bytes. 3. Convert the resulting bytes to a hex-encoded ASCII string, using a hyphen \'-\' as the separator after every byte. 4. Convert the hex-encoded ASCII string back to bytes. 5. Calculate the CRC-32 checksum of the final bytes. Function Signature ```python def perform_conversions(data: bytes) -> int: pass ``` Input - `data` (bytes): The input binary data. Output - `int`: The CRC-32 checksum of the final bytes. Constraints - The input bytes object `data` will contain at most 100 bytes. Example ```python import binascii def perform_conversions(data: bytes) -> int: # Step 1: Convert bytes to base64-encoded ASCII string base64_ascii = binascii.b2a_base64(data).decode().strip() # Step 2: Convert base64-encoded ASCII string back to bytes bytes_from_base64 = binascii.a2b_base64(base64_ascii) # Step 3: Convert bytes to hex-encoded ASCII string with hyphen separator hex_ascii = binascii.hexlify(bytes_from_base64, sep=b\'-\').decode() # Step 4: Convert hex-encoded ASCII string back to bytes bytes_from_hex = binascii.unhexlify(hex_ascii.replace(\'-\', \'\')) # Step 5: Calculate CRC-32 checksum of the final bytes crc32_checksum = binascii.crc32(bytes_from_hex) return crc32_checksum # Example usage: data = b\'hello world\' print(perform_conversions(data)) # Expected output: CRC-32 checksum of final bytes ``` Note - It is essential to handle the conversion between string and bytes properly during each step of the process. - Ensure to strip any extraneous characters, like newline characters added during base64 encoding, to maintain the integrity of the data. **Hint**: Utilize the functions from the `binascii` module as described in the module documentation to achieve the desired conversions.","solution":"import binascii def perform_conversions(data: bytes) -> int: # Step 1: Convert bytes to base64-encoded ASCII string base64_ascii = binascii.b2a_base64(data).decode().strip() # Step 2: Convert base64-encoded ASCII string back to bytes bytes_from_base64 = binascii.a2b_base64(base64_ascii) # Step 3: Convert bytes to hex-encoded ASCII string with hyphen separator hex_ascii = binascii.hexlify(bytes_from_base64, sep=b\'-\').decode() # Step 4: Convert hex-encoded ASCII string back to bytes bytes_from_hex = binascii.unhexlify(hex_ascii.replace(\'-\', \'\')) # Step 5: Calculate CRC-32 checksum of the final bytes crc32_checksum = binascii.crc32(bytes_from_hex) return crc32_checksum"},{"question":"**Context:** You are tasked with creating a machine learning model to predict house prices based on various features such as the size of the house, the number of bedrooms, location, etc. You will use the scikit-learn library to implement this. **Task:** Write a Python function `train_house_price_model(data: pd.DataFrame) -> Tuple[float, float, str]` that performs the following: 1. Preprocesses the given DataFrame, handling missing values, encoding categorical variables, and normalizing numerical variables. 2. Splits the data into training and test sets. 3. Trains a regression model to predict house prices. 4. Evaluates the model on the test set using Mean Absolute Error (MAE) and R-Squared (R) metrics. 5. Returns the MAE, R, and the name of the regression algorithm used. **Input:** - `data`: A pandas DataFrame containing the dataset with various features and a target column named `\'price\'`. **Output:** - A tuple containing: - `mae`: Mean Absolute Error of the model on the test set. - `r2`: R-Squared of the model on the test set. - `model_name`: Name of the regression algorithm used. **Constraints:** - You must use scikit-learn for preprocessing, model training, and evaluation. - Handle missing values by filling them with the mean for numerical columns and the most frequent value for categorical columns. - Use `OneHotEncoder` for encoding categorical variables. - Use `StandardScaler` for normalizing numerical variables. - The test set should be 20% of the original dataset. **Performance Requirements:** - The function should be efficient enough to handle a dataset with up to 100,000 rows and 50 columns. **Example Usage:** ```python import pandas as pd data = pd.read_csv(\'house_prices.csv\') mae, r2, model_name = train_house_price_model(data) print(f\\"Mean Absolute Error: {mae}\\") print(f\\"R-Squared: {r2}\\") print(f\\"Model Used: {model_name}\\") ``` **Note:** You may use any regression model provided by scikit-learn, such as `LinearRegression`, `DecisionTreeRegressor`, `RandomForestRegressor`, etc., but ensure to specify which model you used in the returned results.","solution":"import pandas as pd from typing import Tuple from sklearn.model_selection import train_test_split from sklearn.impute import SimpleImputer from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_absolute_error, r2_score def train_house_price_model(data: pd.DataFrame) -> Tuple[float, float, str]: # Ensure \'price\' column exists if \'price\' not in data.columns: raise ValueError(\\"DataFrame must contain \'price\' column as target variable.\\") # Separating features and target variable X = data.drop(\'price\', axis=1) y = data[\'price\'] # Identifying numerical and categorical columns numeric_cols = X.select_dtypes(include=[\'int64\', \'float64\']).columns categorical_cols = X.select_dtypes(include=[\'object\']).columns # Preprocessing steps for numerical and categorical data numeric_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Creating the Column Transformer preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_cols), (\'cat\', categorical_transformer, categorical_cols) ] ) # Creating the full pipeline with preprocessing and regression model model = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'regressor\', LinearRegression()) ]) # Splitting the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Training the model model.fit(X_train, y_train) # Making predictions on the test set y_pred = model.predict(X_test) # Evaluating the model mae = mean_absolute_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) return mae, r2, \'LinearRegression\'"},{"question":"Objective Demonstrate your understanding of **seaborn**\'s theme and display configuration settings. You will be required to configure a plot\'s appearance and adjust the display settings in a Jupyter notebook environment. Task 1. **Theme Configuration**: - Create a line plot using seaborn\'s `so.Plot`. - Customize the theme to have a white background for the axes (`axes.facecolor`). - Set the overall style of the plot to `whitegrid` using seaborn\'s theming functions. - Reset the theme back to seaborn defaults after displaying the plot. 2. **Display Configuration**: - Configure the plot to display in SVG format. - Disable HiDPI scaling for the plot. - Set the scaling factor of the embedded image to 0.7. Input You do not need to handle input from the user; preset data for plotting should be created within your script as shown below: ```python import pandas as pd import numpy as np # Sample data data = pd.DataFrame({ \\"x\\": np.arange(10), \\"y\\": np.random.randn(10).cumsum() }) ``` Output The expected output is a properly configured seaborn plot displayed within a Jupyter notebook. Constraints and Requirements - Use `seaborn.objects.Plot` for plotting. - Follow the theme and display configuration requirements as described. - Ensure that the plot\'s configuration is set before the plot is displayed. - After plotting, the theme should be reset back to its default settings. Implementation ```python import pandas as pd import numpy as np import seaborn.objects as so from seaborn import axes_style import matplotlib as mpl # Sample data data = pd.DataFrame({ \\"x\\": np.arange(10), \\"y\\": np.random.randn(10).cumsum() }) # Theme Configuration so.Plot.config.theme[\\"axes.facecolor\\"] = \\"white\\" so.Plot.config.theme.update(axes_style(\\"whitegrid\\")) # Display Configuration so.Plot.config.display[\\"format\\"] = \\"svg\\" so.Plot.config.display[\\"hidpi\\"] = False so.Plot.config.display[\\"scaling\\"] = 0.7 # Create and display the plot p = so.Plot(data, x=\\"x\\", y=\\"y\\").add(so.Line()) p # Reset the theme back to seaborn defaults so.Plot.config.theme.reset() ``` Explanation - The script initializes the sample data required for plotting. - It then configures the plot\'s theme and display settings as specified. - The plot is created and displayed with `so.Plot` and subsequently reset to default settings.","solution":"import pandas as pd import numpy as np import seaborn.objects as so from seaborn import axes_style import matplotlib as mpl # Sample data data = pd.DataFrame({ \\"x\\": np.arange(10), \\"y\\": np.random.randn(10).cumsum() }) # Theme Configuration so.Plot.config.theme[\\"axes.facecolor\\"] = \\"white\\" so.Plot.config.theme.update(axes_style(\\"whitegrid\\")) # Display Configuration so.Plot.config.display[\\"format\\"] = \\"svg\\" so.Plot.config.display[\\"hidpi\\"] = False so.Plot.config.display[\\"scaling\\"] = 0.7 # Create and display the plot p = so.Plot(data, x=\\"x\\", y=\\"y\\").add(so.Line()) p # Reset the theme back to seaborn defaults so.Plot.config.theme.reset()"},{"question":"**Question: Validating and Extracting Data from a Log File** You are given a log file containing entries of user actions in the following format: `UserID: <user_id>, Action: <action>, Timestamp: <timestamp>` where: - `<user_id>` is a sequence of alphanumeric characters. - `<action>` is a sequence of letters (e.g., login, logout, download, upload). - `<timestamp>` is in the format `YYYY-MM-DD HH:MM:SS`. Your task is to implement a function `validate_and_extract(log: str) -> List[Tuple[str, str, str]]` that takes the contents of the log file as input and performs the following: 1. Validate that each entry adheres to the specified format. 2. Extract the `<user_id>`, `<action>`, and `<timestamp>` from each valid entry. 3. Return a list of tuples containing the extracted `<user_id>`, `<action>`, and `<timestamp>` for each valid entry. **Function Signature:** ```python from typing import List, Tuple import re def validate_and_extract(log: str) -> List[Tuple[str, str, str]]: pass ``` **Example:** ```python log = UserID: user123, Action: login, Timestamp: 2023-01-23 15:45:00 UserID: user456, Action: logout, Timestamp: 2023-01-23 18:20:30 UserID: user789, Action: download, Timestamp: 2023-01-24 07:10:12 Invalid log entry UserID: user012, Action: upload, Timestamp: 2023-01-25 21:45:30 result = validate_and_extract(log) print(result) # Expected output: # [ # (\'user123\', \'login\', \'2023-01-23 15:45:00\'), # (\'user456\', \'logout\', \'2023-01-23 18:20:30\'), # (\'user789\', \'download\', \'2023-01-24 07:10:12\'), # (\'user012\', \'upload\', \'2023-01-25 21:45:30\') # ] ``` **Constraints:** - Each log entry is on a new line. - Consider only valid entries and ignore invalid ones. - Use regular expressions to parse and validate the log entries. **Hints:** - Use the `re` module for pattern matching. - Utilize groups within your regular expression to capture `<user_id>`, `<action>`, and `<timestamp>`. - Handle potential multiline input by splitting the log content into individual lines. **Advanced Requirements:** - Ensure that your function has a time complexity of O(n), where n is the number of lines in the log string. - Consider edge cases where log entries might not follow the specified format.","solution":"from typing import List, Tuple import re def validate_and_extract(log: str) -> List[Tuple[str, str, str]]: Validate that each log entry adheres to the specified format and extract the user_id, action, and timestamp from valid entries. Parameters: log (str): The contents of the log file. Returns: List[Tuple[str, str, str]]: A list of tuples containing the extracted user_id, action, and timestamp for each valid entry. # Regular expression pattern to match each part of the log entry pattern = re.compile( r\\"^UserID: (?P<user_id>w+), Action: (?P<action>[a-z]+), Timestamp: (?P<timestamp>d{4}-d{2}-d{2} d{2}:d{2}:d{2})\\" ) # Split log entries by lines log_entries = log.strip().split(\'n\') valid_entries = [] for entry in log_entries: match = pattern.match(entry) if match: user_id = match.group(\'user_id\') action = match.group(\'action\') timestamp = match.group(\'timestamp\') valid_entries.append((user_id, action, timestamp)) return valid_entries"},{"question":"# Question: Transforming ASTs **Objective:** You are required to implement a Python function that parses a given source code string, transforms all integer literals by doubling their values, and then returns the modified source code as a string. You should utilize the `ast` module to achieve this transformation. **Instructions:** 1. Parse the provided source code string into an AST using `ast.parse()`. 2. Create a custom transformer by subclassing `ast.NodeTransformer`. This transformer should: - Double the value of all `ast.Constant` nodes where the value is an integer (`int`). 3. Apply the transformer to the parsed AST. 4. Use `ast.unparse()` to convert the modified AST back into a source code string. 5. Return the modified source code string. **Function Signature:** ```python def transform_source_code(source: str) -> str: pass ``` **Example:** ```python source_code = a = 1 b = 2 + 3 c = a * 4 d = [5, 6, 7] transformed_code = transform_source_code(source_code) print(transformed_code) ``` **Expected Output:** ```plaintext a = 2 b = 4 + 6 c = a * 8 d = [10, 12, 14] ``` **Constraints:** - You may assume that the input source code is always valid Python code. - Only integer literals should be transformed; other literals (e.g., strings, floats) should remain unchanged. # Notes: - Use the `ast.NodeTransformer` class to create a custom transformer. - Be mindful of preserving the original structure and formatting of the code as much as possible. - Ensure that the function handles all possible locations of integer literals, including lists, dictionaries, function arguments, etc.","solution":"import ast def transform_source_code(source: str) -> str: Parses the provided source code string, doubles all integer literals, and returns the modified source code as a string. # Define the custom transformer class DoubleIntTransformer(ast.NodeTransformer): def visit_Constant(self, node): # Check if the node is an integer if isinstance(node.value, int): node.value *= 2 return node # Parse the source code into an AST tree = ast.parse(source) # Apply the transformer to the AST transformer = DoubleIntTransformer() transformed_tree = transformer.visit(tree) # Convert the modified AST back to a source code string modified_source = ast.unparse(transformed_tree) return modified_source"},{"question":"**Coding Assessment Question** # Objective Demonstrate your understanding of file operations, audio data manipulation, and the usage of the `sunau` module by implementing a function to read an AU audio file, modify its sample rate and write it to a new AU file. # Problem Statement Implement a function `modify_sample_rate(input_file: str, output_file: str, new_sample_rate: int) -> None` which performs the following operations: 1. Opens an existing AU file specified by `input_file` in read mode. 2. Reads the audio data and all related parameters (number of channels, sample width, frame rate, number of frames, and compression type) from the input file. 3. Modifies the sample rate to `new_sample_rate`. 4. Writes the audio data with the new sample rate to a new AU file specified by `output_file`. # Constraints - The input file is guaranteed to be a valid AU file. - The `new_sample_rate` is a positive integer. - The output file should have the same number of channels, sample width, and compression type as the input file. - Ensure to handle any exceptions raised by the `sunau` module appropriately. # Input - `input_file`: A string representing the name/path of the input AU file. - `output_file`: A string representing the name/path of the output AU file. - `new_sample_rate`: An integer specifying the new sample rate. # Output - The function does not return anything. It writes the modified audio data to the `output_file`. # Example ```python # Given an AU file \\"input.au\\", modify its sample rate to 16000 and save as \\"output.au\\" modify_sample_rate(\\"input.au\\", \\"output.au\\", 16000) ``` # Notes 1. Use the `sunau` module to handle all file operations. 2. Use the appropriate methods for reading and writing the audio data and metadata. 3. Ensure the output file conforms to the AU format and contains valid header and audio data.","solution":"import sunau def modify_sample_rate(input_file: str, output_file: str, new_sample_rate: int) -> None: Modify the sample rate of an AU audio file and save it to a new file. Args: input_file (str): Path to the input AU file. output_file (str): Path to the output AU file. new_sample_rate (int): The new sample rate to be set. try: with sunau.open(input_file, \'rb\') as infile: # Read audio parameters from input file n_channels = infile.getnchannels() sample_width = infile.getsampwidth() old_sample_rate = infile.getframerate() n_frames = infile.getnframes() compression_type = infile.getcomptype() compression_name = infile.getcompname() # Read audio data from input file audio_data = infile.readframes(n_frames) with sunau.open(output_file, \'wb\') as outfile: # Set parameters for output file outfile.setnchannels(n_channels) outfile.setsampwidth(sample_width) outfile.setframerate(new_sample_rate) outfile.setcomptype(compression_type, compression_name) # Write audio data to output file outfile.writeframes(audio_data) except sunau.Error as e: print(f\\"An error occurred while handling the AU file: {e}\\")"},{"question":"# Functional Programming HOWTO **Problem Statement: Fibonacci Sequence Generator** You are required to implement a Python function that generates a sequence of Fibonacci numbers using functional programming principles. The implementation must use higher-order functions such as `map`, `reduce`, and `filter`. **Function Signature:** ```python def fibonacci_sequence(n: int) -> List[int]: ``` **Input:** - `n` (int): A positive integer (1 <= n <= 10^3), the length of the Fibonacci sequence to generate. **Output:** - A List of integers representing the first `n` Fibonacci numbers. **Requirements:** 1. The function must use functional programming techniques as much as possible, avoiding explicit loops and relying on `map`, `reduce`, and `filter`. 2. The function should be efficient and handle the upper constraint within a reasonable time limit. **Example:** ```python assert fibonacci_sequence(1) == [0] assert fibonacci_sequence(2) == [0, 1] assert fibonacci_sequence(5) == [0, 1, 1, 2, 3] assert fibonacci_sequence(10) == [0, 1, 1, 2, 3, 5, 8, 13, 21, 34] ``` **Constraints:** - The input `n` will be a positive integer such that (1 <= n <= 1000). **Hints:** - Use `itertools` and `functools` libraries to aid in your implementation. - Consider using a generator to yield Fibonacci numbers. - Recursion can be limited by using appropriate functional programming paradigms.","solution":"from typing import List from itertools import islice from functools import reduce def fibonacci_generator(): Generator to yield Fibonacci sequence indefinitely. a, b = 0, 1 while True: yield a a, b = b, a + b def fibonacci_sequence(n: int) -> List[int]: Generates the first n Fibonacci numbers using functional programming principles. fib_gen = fibonacci_generator() return list(islice(fib_gen, n))"},{"question":"# Python Coding Assessment Objective: Write a Python function that configures the module search path dynamically using the capabilities of the \\"site\\" module. The function should demonstrate understanding of managing site-specific paths, handling path configuration files, and incorporating user-defined customizations. Task: Create a function `configure_python_environment(custom_paths=None, enable_user_site=True)` that: 1. Adds custom directories to the module search path (`sys.path`) if provided. 2. Ensures the user-specific site directory is enabled or disabled based on the `enable_user_site` flag. 3. Returns a dictionary with the updated `sys.path`, `USER_SITE`, and `ENABLE_USER_SITE`. Function Signature: ```python def configure_python_environment(custom_paths=None, enable_user_site=True) -> dict: ``` Input: - `custom_paths` (optional): A list of strings representing directory paths to be added to the module search path. If `None`, no directories are added. - `enable_user_site` (optional, default=True): A boolean flag indicating whether the user-specific site directory should be enabled. Output: - A dictionary with the following keys: - `\\"sys_path\\"`: The updated list of directories in `sys.path`. - `\\"USER_SITE\\"`: The path to the user-specific site-packages directory. - `\\"ENABLE_USER_SITE\\"`: The status of the user-specific site-packages directory (True, False, or None). Constraints: - Do not assume the existence of any specific directories. - Ensure no duplicates are added to `sys.path`. - Respect the initial state of `ENABLE_USER_SITE` if not overridden by the flag. Example: ```python def configure_python_environment(custom_paths=None, enable_user_site=True): import site import sys # Add custom paths to sys.path if custom_paths: for path in custom_paths: site.addsitedir(path) # Enable or disable user site-packages directory site.ENABLE_USER_SITE = enable_user_site # Get current USER_SITE value user_site = site.getusersitepackages() # Return the dictionary with updated values return { \\"sys_path\\": sys.path, \\"USER_SITE\\": user_site, \\"ENABLE_USER_SITE\\": site.ENABLE_USER_SITE } # Example usage: # custom_paths = [\'/path/to/custom1\', \'/path/to/custom2\'] # result = configure_python_environment(custom_paths, enable_user_site=False) # print(result) ``` This will configure the Python environment by adding specified custom paths, managing the user site directory as per the flag, and returning the relevant configuration details.","solution":"import sys import site def configure_python_environment(custom_paths=None, enable_user_site=True): Configures the Python environment by updating sys.path and managing the user-specific site-packages directory. Parameters: - custom_paths (list of str, optional): Custom directory paths to add to sys.path. - enable_user_site (bool, optional): Flag to enable or disable the user-specific site-packages directory. Returns: - dict: Dictionary with updated sys.path, USER_SITE, and ENABLE_USER_SITE. # Add custom paths to sys.path if custom_paths: for path in custom_paths: # Prevent duplicates if path not in sys.path: site.addsitedir(path) # Set ENABLE_USER_SITE based on the parameter previous_enable_user_site = site.ENABLE_USER_SITE site.ENABLE_USER_SITE = enable_user_site # Get the user-specific site-packages directory user_site = site.getusersitepackages() # Prepare the result dictionary result = { \\"sys_path\\": sys.path, \\"USER_SITE\\": user_site, \\"ENABLE_USER_SITE\\": site.ENABLE_USER_SITE } # Restore the previous state of ENABLE_USER_SITE if it was not None if previous_enable_user_site is not None: site.ENABLE_USER_SITE = previous_enable_user_site return result"},{"question":"**Question: Implement a Custom Stream to Filter and Transform Data** You are tasked with creating a custom stream class that must read from a text file and transform its content. Specifically, this stream class will: 1. Read data from an underlying text file. 2. Replace all occurrences of a specified substring with a different substring. 3. Emit the transformed data when read operations are performed. Your custom stream class should inherit from `io.TextIOBase` and utilize `io.TextIOWrapper` to handle text-specific details. Additionally, the custom stream should ensure that it reads data in a buffered manner for efficiency. **Requirements:** - Name your class `TransformingTextStream`. - The class constructor should accept: - `source`: the file path to the source text file. - `replace_from`: the substring to be replaced. - `replace_to`: the substring to replace with. - Implement the `read(size=-1)` method to read and transform the content. - Implement the `readline(size=-1)` method to read the transformed content line by line. - Ensure your class handles basic text I/O operations like encoding, and error handling appropriately. **Constraints:** - Assume the source file is encoded in UTF-8 and should be read with this encoding. - For simplicity, buffer the file data internally in the `TransformingTextStream` class to perform transformation. - The stream should raise an appropriate error if the source file does not exist or cannot be read. **Example Usage:** Suppose you have a file `example.txt` that contains the following text: ``` Hello World This is a test line. Hello again! ``` And you want to replace all occurrences of \\"Hello\\" with \\"Hi\\". With the `TransformingTextStream` class, you should be able to perform the following operations: ```python stream = TransformingTextStream(\\"example.txt\\", replace_from=\\"Hello\\", replace_to=\\"Hi\\") print(stream.read()) # Outputs: \'Hi WorldnThis is a test line.nHi again!n\' print(stream.readline()) # Should read the first line and output until the newline character. ``` Implement the class `TransformingTextStream` below: ```python import io class TransformingTextStream(io.TextIOBase): def __init__(self, source, replace_from, replace_to): self.source = source self.replace_from = replace_from self.replace_to = replace_to self._buffer = \\"\\" try: with open(self.source, \'r\', encoding=\'utf-8\') as file: self._buffer = file.read().replace(self.replace_from, self.replace_to) self._buffer_io = io.StringIO(self._buffer) except FileNotFoundError: raise FileNotFoundError(f\\"Source file \'{self.source}\' does not exist.\\") except IOError as e: raise IOError(f\\"Error reading the file \'{self.source}\': {e}\\") def read(self, size=-1): return self._buffer_io.read(size) def readline(self, size=-1): return self._buffer_io.readline(size) def close(self): self._buffer_io.close() ``` Ensure your implementation is robust and handles different edge cases, such as empty files, files with no matching substrings, and large files where buffering is particularly beneficial.","solution":"import io class TransformingTextStream(io.TextIOBase): def __init__(self, source, replace_from, replace_to): self.source = source self.replace_from = replace_from self.replace_to = replace_to self._buffer = \\"\\" try: with open(self.source, \'r\', encoding=\'utf-8\') as file: self._buffer = file.read().replace(self.replace_from, self.replace_to) self._buffer_io = io.StringIO(self._buffer) except FileNotFoundError: raise FileNotFoundError(f\\"Source file \'{self.source}\' does not exist.\\") except IOError as e: raise IOError(f\\"Error reading the file \'{self.source}\': {e}\\") def read(self, size=-1): return self._buffer_io.read(size) def readline(self, size=-1): return self._buffer_io.readline(size) def close(self): self._buffer_io.close()"},{"question":"Objective: Demonstrate comprehension of seaborn\'s `kdeplot` function by generating and customizing KDE plots using a provided dataset. This will assess your ability to utilize seaborn for various KDE plot configurations and customizations. Problem Statement: You have been provided with a dataset named `car_crashes` that contains information about car crashes in different states. Your task is to generate and customize several types of KDE plots using seaborn. Dataset: The dataset `car_crashes` can be loaded using seaborn as follows: ```python car_crashes = sns.load_dataset(\\"car_crashes\\") ``` The dataset includes columns like `total`, `speeding`, `alcohol`, `not_distracted`, `no_previous`, `ins_premium`, and `ins_losses`. Tasks: 1. **Univariate KDE Plot:** Create a basic univariate KDE plot showing the distribution of the `speeding` variable. 2. **Flipped KDE Plot:** Plot the univariate KDE of the `alcohol` variable but flip the axis so that the distribution is shown along the y-axis. 3. **Combined KDE Plot:** Generate a KDE plot for both `speeding` and `alcohol` in the same plot. Ensure that the plot is filled and each distribution is distinguished by different colors. 4. **Conditional KDE Plot:** Use the `hue` parameter to show the KDE plot for `total` based on the `region` variable (assume `region` is a new category column you\'ve created that categorizes states into regions like \'North\', \'South\', \'East\', \'West\'). 5. **Customized KDE Plot:** Create a bivariate KDE plot for `ins_premium` and `ins_losses`. Customize the plot with a specific colormap, fill the plot, and adjust the number of contour levels to 15 while setting a threshold of 0.1. 6. **Additional Customization:** Create a KDE plot for the `not_distracted` variable. Adjust the bandwidth to smooth the plot more, and ensure the area under the curve is filled with a specific transparency level. Constraints: - Use seaborn functions only for generating the plots. - Make sure to appropriately display legends and labels for each plot to enhance readability. - All plots must be created in a single Jupyter notebook cell and must display sequentially. Input: Data will be loaded using seaborn\'s `load_dataset` function: ```python car_crashes = sns.load_dataset(\\"car_crashes\\") ``` Output: Six KDE plots as described in the tasks above. Performance Requirements: Ensure that the code is efficient and runs within reasonable time limits for generating the plots. Example Code: ```python import seaborn as sns import matplotlib.pyplot as plt car_crashes = sns.load_dataset(\\"car_crashes\\") # Univariate KDE Plot sns.kdeplot(data=car_crashes, x=\\"speeding\\") plt.show() # Flipped KDE Plot sns.kdeplot(data=car_crashes, y=\\"alcohol\\") plt.show() # Combined KDE Plot sns.kdeplot(data=car_crashes, x=\\"speeding\\", fill=True, color=\'blue\') sns.kdeplot(data=car_crashes, x=\\"alcohol\\", fill=True, color=\'red\') plt.show() # Conditional KDE Plot # Assuming \'region\' column is previously created sns.kdeplot(data=car_crashes, x=\\"total\\", hue=\\"region\\") plt.show() # Customized Bivariate KDE Plot sns.kdeplot(data=car_crashes, x=\\"ins_premium\\", y=\\"ins_losses\\", fill=True, cmap=\\"viridis\\", levels=15, thresh=0.1) plt.show() # Additional Customization on KDE Plot sns.kdeplot(data=car_crashes, x=\\"not_distracted\\", bw_adjust=2, fill=True, alpha=0.5) plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Load the dataset car_crashes = sns.load_dataset(\\"car_crashes\\") # Task 1: Univariate KDE Plot for \'speeding\' def plot_speeding_kde(): sns.kdeplot(data=car_crashes, x=\\"speeding\\") plt.title(\'KDE Plot of Speeding\') plt.show() # Task 2: Flipped KDE Plot for \'alcohol\' def plot_alcohol_kde(): sns.kdeplot(data=car_crashes, y=\\"alcohol\\") plt.title(\'Flipped KDE Plot of Alcohol\') plt.show() # Task 3: Combined KDE Plot for \'speeding\' and \'alcohol\' def plot_combined_kde(): sns.kdeplot(data=car_crashes, x=\\"speeding\\", fill=True, color=\'blue\', label=\'Speeding\') sns.kdeplot(data=car_crashes, x=\\"alcohol\\", fill=True, color=\'red\', label=\'Alcohol\') plt.title(\'Combined KDE Plot of Speeding and Alcohol\') plt.legend() plt.show() # Task 4: Conditional KDE Plot for \'total\' based on \'region\' # Assume \'region\' column has been added to car_crashes dataset def plot_conditional_kde(): # Mock \'region\' column for demonstration purposes regions = [\'North\', \'South\', \'East\', \'West\'] car_crashes[\'region\'] = [regions[i % 4] for i in range(len(car_crashes))] sns.kdeplot(data=car_crashes, x=\\"total\\", hue=\\"region\\") plt.title(\'Conditional KDE Plot of Total by Region\') plt.show() # Task 5: Customized Bivariate KDE Plot for \'ins_premium\' and \'ins_losses\' def plot_bivariate_kde(): sns.kdeplot(data=car_crashes, x=\\"ins_premium\\", y=\\"ins_losses\\", fill=True, cmap=\\"viridis\\", levels=15, thresh=0.1) plt.title(\'Bivariate KDE Plot of Insurance Premium and Insurance Losses\') plt.show() # Task 6: Customized KDE Plot for \'not_distracted\' def plot_not_distracted_kde(): sns.kdeplot(data=car_crashes, x=\\"not_distracted\\", bw_adjust=2, fill=True, alpha=0.5) plt.title(\'KDE Plot of Not Distracted with Bandwidth Adjustment\') plt.show() # Execute all plot functions sequentially plot_speeding_kde() plot_alcohol_kde() plot_combined_kde() plot_conditional_kde() plot_bivariate_kde() plot_not_distracted_kde()"},{"question":"# Assessing Pandas Proficiency with Custom Data Types and Arrays Objective To assess the understanding of pandas data types, scalar types, and array-like structures. This task will involve loading data, operating on pandas data structures, and utilizing different data types and arrays effectively. Problem Statement You are given a CSV file named `data.csv` with the following columns: 1. **date_time**: A datetime column with timezone information. 2. **duration**: A column representing durations in timedelta format. 3. **category**: A categorical column with predefined categories [\\"A\\", \\"B\\", \\"C\\", \\"D\\"]. 4. **integers**: A nullable integer column. 5. **floats**: A nullable float column. You need to: 1. Load the data from `data.csv` into a pandas DataFrame. 2. Convert the `date_time` column to a timezone-aware datetime array. 3. Convert the `duration` column to a timedelta array. 4. Ensure the `category` column is treated as a categorical array with the given categories. 5. Ensure the `integers` column is treated as a nullable integer array. 6. Ensure the `floats` column is treated as a nullable float array. 7. Add a new column called `bool_col` which is a nullable boolean indicating if the value in `integers` column is greater than 10. 8. Add a new column called `string_col` which stores the string representation of `date_time` in the format \\"YYYY-MM-DD\\". Finally, print the DataFrame with its dtypes and first 5 rows. Constraints - The `data.csv` file is guaranteed to have the correct structure as described. - You should use pandas functionalities to set the correct data types while loading the data. Input and Output Formats **Input** - CSV file named `data.csv` **Output** - A pandas DataFrame printed with its dtypes and first 5 rows. Example Suppose `data.csv` contains the following data: ``` date_time,duration,category,integers,floats 2023-01-01 10:00:00+00:00,1 days 00:00:00,A,10,1.1 2023-01-02 11:00:00+00:00,2 days 00:00:00,B,15,2.2 2023-01-03 12:00:00+00:00,C,NaN,3.3 2023-01-04 13:00:00+00:00,3 days 00:00:00,A,20,NaN 2023-01-05 14:00:00+00:00,D,25,5.5 ``` The expected output after processing would be: ``` DataFrame dtypes: date_time datetime64[ns, UTC] duration timedelta64[ns] category category integers Int64 floats Float64 bool_col boolean string_col string First 5 rows: date_time duration category integers floats bool_col string_col 0 2023-01-01 10:00:00+00:00 1 days 00:00:00 A 10 1.1 False 2023-01-01 1 2023-01-02 11:00:00+00:00 2 days 00:00:00 B 15 2.2 True 2023-01-02 2 2023-01-03 12:00:00+00:00 NaT NaN C NaN 3.3 <NA> 2023-01-03 3 2023-01-04 13:00:00+00:00 3 days 00:00:00 A 20 NaN True 2023-01-04 4 2023-01-05 14:00:00+00:00 4 days 00:00:00 D 25 5.5 True 2023-01-05 ``` Requirements - The implementation should be efficient and leverage pandas functionalities. - Proper handling of missing values is required in columns `integers` and `floats`. - Ensuring correct data types using pandas-specific data types and arrays. - Use of pandas functions and not manual type casting.","solution":"import pandas as pd def process_data(file_path): df = pd.read_csv(file_path, parse_dates=[\'date_time\']) # Converting the columns to the appropriate types df[\'date_time\'] = pd.to_datetime(df[\'date_time\'], utc=True) df[\'duration\'] = pd.to_timedelta(df[\'duration\']) df[\'category\'] = pd.Categorical(df[\'category\'], categories=[\\"A\\", \\"B\\", \\"C\\", \\"D\\"]) df[\'integers\'] = df[\'integers\'].astype(\'Int64\') df[\'floats\'] = df[\'floats\'].astype(\'Float64\') # Adding new columns df[\'bool_col\'] = df[\'integers\'] > 10 df[\'bool_col\'] = df[\'bool_col\'].astype(\'boolean\') # Convert to nullable boolean df[\'string_col\'] = df[\'date_time\'].dt.strftime(\'%Y-%m-%d\').astype(\'string\') # Print DataFrame dtypes and first 5 rows print(\\"DataFrame dtypes:\\") print(df.dtypes) print(\\"nFirst 5 rows:\\") print(df.head()) return df"},{"question":"# Complex Number Algebra using `cmath` Objective: Write a function `complex_operations` that performs a series of operations using the `cmath` module in Python. The function will take two complex numbers as inputs, perform various mathematical operations, and return the results conformingly. Function Signature: ```python def complex_operations(z1: complex, z2: complex) -> dict: pass ``` Input: - `z1`: A complex number ( z_1 ) - `z2`: A complex number ( z_2 ) Output: - A dictionary containing: - `\'sum\'`: The sum of ( z_1 ) and ( z_2 ) - `\'difference\'`: The difference ( z_1 - z_2 ) - `\'product\'`: The product ( z_1 times z_2 ) - `\'quotient\'`: The quotient ( z_1 / z_2 ) - `\'exp_z1\'`: Natural exponential of ( z_1 ) - `\'log_z2\'`: Natural logarithm of ( z_2 ) - `\'polar_z1\'`: Polar coordinates of ( z_1 ) as a tuple ((r, phi) ) - `\'rect_polar_z1\'`: Rectangular form of ( z_1 ) when given its polar coordinates Constraints: 1. Assume that `z2` is never zero (to avoid division by zero errors). 2. You should handle each mathematical function using appropriate `cmath` functions. 3. Ensure you handle branch cuts correctly as specified. 4. The return dictionary should have complex numbers in their default form if applicable. Example: ```python import cmath z1 = 1 + 2j z2 = 3 - 4j result = complex_operations(z1, z2) print(result) ``` Expected output: ```python { \'sum\': (4-2j), \'difference\': (-2+6j), \'product\': (11+2j), \'quotient\': (-0.2+0.4j), \'exp_z1\': (-1.1312043837568135-2.4717266720048188j), \'log_z2\': (1.6094379124341003-0.9272952180016122j), \'polar_z1\': (2.23606797749979, 1.1071487177940904), \'rect_polar_z1\': (1+2j) } ``` Notes: - Use `cmath.exp` for the exponential function. - Use `cmath.log` for the logarithmic function. - Use `cmath.polar` to get the polar coordinates. - Use `cmath.rect` to convert polar coordinates back to rectangular form.","solution":"import cmath def complex_operations(z1: complex, z2: complex) -> dict: return { \'sum\': z1 + z2, \'difference\': z1 - z2, \'product\': z1 * z2, \'quotient\': z1 / z2, \'exp_z1\': cmath.exp(z1), \'log_z2\': cmath.log(z2), \'polar_z1\': cmath.polar(z1), \'rect_polar_z1\': cmath.rect(*cmath.polar(z1)) }"},{"question":"# Question: You are given logs of user activities in a web application, and you need to extract and process specific information from these logs using regular expressions. The logs include timestamps, user IDs, activities, and additional information in a structured format as shown below: ``` 2023-10-01 12:45:22,123 [INFO] [user123] LOGIN from [192.168.1.10] 2023-10-01 12:47:35,567 [ERROR] [user456] FAILED LOGIN from [192.168.1.12], reason: BAD_PASSWORD 2023-10-01 12:50:45,876 [INFO] [user789] LOGOUT from [192.168.1.15] ... ``` Your task is to write a Python function `process_logs` that takes in a single string containing the entire log data and performs the following operations: 1. Extract all the login attempts (both successful and failed). 2. For each login attempt, extract the timestamp, user ID, IP address, and in case of failed login, the reason for failure. 3. Return a list of dictionaries, each dictionary containing the extracted information for one login attempt. For successful logins, the \'reason\' key should have a value of `None`. # Input: - A single string `log_data` containing the entire logs. # Output: - A list of dictionaries with keys `timestamp`, `user_id`, `activity`, `ip_address`, and `reason`. # Constraints: - The log formats are consistent as shown above. # Example: Input: ```python log_data = 2023-10-01 12:45:22,123 [INFO] [user123] LOGIN from [192.168.1.10] 2023-10-01 12:47:35,567 [ERROR] [user456] FAILED LOGIN from [192.168.1.12], reason: BAD_PASSWORD 2023-10-01 12:50:45,876 [INFO] [user789] LOGOUT from [192.168.1.15] ``` Output: ```python [ { \'timestamp\': \'2023-10-01 12:45:22,123\', \'user_id\': \'user123\', \'activity\': \'LOGIN\', \'ip_address\': \'192.168.1.10\', \'reason\': None }, { \'timestamp\': \'2023-10-01 12:47:35,567\', \'user_id\': \'user456\', \'activity\': \'FAILED LOGIN\', \'ip_address\': \'192.168.1.12\', \'reason\': \'BAD_PASSWORD\' } ] ``` # Function Signature: ```python def process_logs(log_data: str) -> list: pass ```","solution":"import re def process_logs(log_data: str) -> list: login_attempts = [] pattern = re.compile( r\'(d{4}-d{2}-d{2} d{2}:d{2}:d{2},d{3}) [.*?] [(.*?)] (LOGIN|FAILED LOGIN) from [(.*?)](, reason: (.*))?\' ) for match in pattern.finditer(log_data): timestamp = match.group(1) user_id = match.group(2) activity = match.group(3) ip_address = match.group(4) reason = match.group(6) if activity == \'FAILED LOGIN\' else None login_attempts.append({ \'timestamp\': timestamp, \'user_id\': user_id, \'activity\': activity, \'ip_address\': ip_address, \'reason\': reason }) return login_attempts"},{"question":"# PyTorch Distributed Training Simulation You have been tasked with implementing a distributed training simulation using PyTorch. The goal is to create a simple neural network model and simulate its parameter update as if it were being run on a distributed system. # Objective - Implement a simple neural network using PyTorch. - Simulate distributed training by partitioning the dataset and performing parameter updates as if performed on separate nodes. # Instructions 1. **Neural Network Implementation** - Create a neural network model `SimpleNet` with the following specifications: - It should have two linear layers. - The first layer should transform the input from size 10 to 5. - The second layer should transform the input from size 5 to 1. - Use ReLU activation after the first layer. 2. **Dataset Preparation** - Generate a synthetic dataset with 100 samples, each consisting of 10 features. The target should be a single value. - Partition the dataset into 4 equal parts to simulate 4 distributed nodes. 3. **Training Simulation** - For each partition, simulate a training step: a. Perform a forward pass. b. Compute the loss (Mean Squared Error). c. Perform a backward pass and obtain the gradients. d. Simulate parameter update by averaging gradients across all partitions before applying them to the model parameters. 4. **Implementation Details** - Use PyTorch for defining the model and performing tensor operations. - You are not required to actually distribute the computation; simply simulate the parameter updates as described. # Expected Input and Output Formats **Input:** - None. Your script should generate synthetic data as described. **Output:** - Print loss after each simulated training epoch. # Constraints - Ensure your script is efficient and leverages PyTorch correctly. - Simulate at least one epoch of training with the described setup. # Additional Information - You should utilize `torch.nn`, `torch.optim`, and other PyTorch functionalities as needed. - Assume necessary imports have been made preemptively (e.g., `import torch`). ```python import torch import torch.nn as nn import torch.optim as optim # Define the neural network model class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.layer1 = nn.Linear(10, 5) self.layer2 = nn.Linear(5, 1) self.relu = nn.ReLU() def forward(self, x): x = self.relu(self.layer1(x)) x = self.layer2(x) return x # Generate synthetic dataset dataset = torch.randn(100, 10) targets = torch.randn(100, 1) # Partition the dataset into 4 equal parts partitions = list(zip(torch.chunk(dataset, 4), torch.chunk(targets, 4))) # Initialize the model and define loss and optimizer model = SimpleNet() criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Simulate distributed training for epoch in range(1): model.train() total_loss = 0 # Simulated distributed training for data, target in partitions: optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() # Placeholder for averaging gradients for param in model.parameters(): # Simulating gradient averaging by assuming bi-directional transfer param.grad.data /= 4 optimizer.step() total_loss += loss.item() print(f\'Epoch {epoch+1}, Loss: {total_loss/4}\') ```","solution":"import torch import torch.nn as nn import torch.optim as optim # Define the neural network model class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.layer1 = nn.Linear(10, 5) self.layer2 = nn.Linear(5, 1) self.relu = nn.ReLU() def forward(self, x): x = self.relu(self.layer1(x)) x = self.layer2(x) return x # Generate synthetic dataset dataset = torch.randn(100, 10) targets = torch.randn(100, 1) # Partition the dataset into 4 equal parts partitions = list(zip(torch.chunk(dataset, 4), torch.chunk(targets, 4))) # Initialize the model and define loss and optimizer model = SimpleNet() criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Simulate distributed training for epoch in range(1): model.train() total_loss = 0 # Simulated distributed training for data, target in partitions: optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() # Placeholder for averaging gradients for param in model.parameters(): # Simulating gradient averaging by assuming bi-directional transfer param.grad.data /= 4 optimizer.step() total_loss += loss.item() print(f\'Epoch {epoch+1}, Loss: {total_loss/4}\')"},{"question":"Missing Values Handling and Analysis **Objective:** You are given a dataset which may contain missing values. Your goal is to analyze and clean the dataset by implementing the following tasks using pandas. # Input Format 1. A CSV file path as a string that contains multiple columns with some missing values (denoted as NaN or NaT). 2. You are required to read this CSV file into a pandas DataFrame. # Output Format 1. A cleaned DataFrame where all rows containing any missing values in specified columns are removed. 2. A dictionary containing: - The percentage of rows containing missing values in each column. - The dataset information after cleaning. # Task Implement a function `missing_values_handling` with the following signature: ```python import pandas as pd from typing import Tuple, Dict def missing_values_handling(file_path: str, columns_to_check: list) -> Tuple[pd.DataFrame, Dict[str, float]]: Reads a CSV file, handles missing values, and provides analysis on missing data. Parameters: - file_path: str - The path to the CSV file. - columns_to_check: list of str - List of columns to check for missing values. Returns: - Tuple containing: 1. Cleaned DataFrame without any rows that have missing values in the specified columns. 2. A dictionary with two keys: a. \'missing_percentages\': A dictionary where the key is the column name and the value is the percentage of missing values in that column. b. \'data_info\': Information about the dataset after cleaning such as number of rows and columns. pass ``` # Constraints and Requirements 1. Utilize pandas for data manipulation. 2. The function should efficiently handle large datasets. 3. Ensure all missing values (NaN/NaT) are correctly identified and processed. 4. Provide clear documentation within your code explaining each major step. Example Input: ```python file_path = \\"example_dataset.csv\\" columns_to_check = [\\"column1\\", \\"column2\\", \\"column3\\"] ``` Example Output: ```python cleaned_df, analysis_dict = missing_values_handling(file_path, columns_to_check) print(cleaned_df) print(analysis_dict) ``` In this example, you should ensure that after processing, `cleaned_df` no longer contains any rows with missing values in `[\\"column1\\", \\"column2\\", \\"column3\\"]`, and `analysis_dict` will provide insights on the percentage of missing values in these columns and details about the cleaned DataFrame.","solution":"import pandas as pd from typing import Tuple, Dict def missing_values_handling(file_path: str, columns_to_check: list) -> Tuple[pd.DataFrame, Dict[str, float]]: Reads a CSV file, handles missing values, and provides analysis on missing data. Parameters: - file_path: str - The path to the CSV file. - columns_to_check: list of str - List of columns to check for missing values. Returns: - Tuple containing: 1. Cleaned DataFrame without any rows that have missing values in the specified columns. 2. A dictionary with two keys: a. \'missing_percentages\': A dictionary where the key is the column name and the value is the percentage of missing values in that column. b. \'data_info\': Information about the dataset after cleaning such as number of rows and columns. # Read the CSV file into a DataFrame df = pd.read_csv(file_path) # Calculate the percentage of missing values for each specified column total_rows = len(df) missing_percentages = {} for column in columns_to_check: missing_count = df[column].isna().sum() missing_percentages[column] = (missing_count / total_rows) * 100 # Remove rows with any missing values in the specified columns cleaned_df = df.dropna(subset=columns_to_check) # Create data info dictionary data_info = { \'num_rows\': len(cleaned_df), \'num_columns\': len(cleaned_df.columns) } # Return the cleaned DataFrame and the analysis dictionary return cleaned_df, {\\"missing_percentages\\": missing_percentages, \\"data_info\\": data_info}"},{"question":"# Advanced Coding Assessment: Regular Expression based String Transformation **Objective**: This question aims to test your understanding of Python\'s `re` module. You are required to implement a function that transforms a provided string by applying a set of pattern matchings and substitutions. **Problem Statement**: You are given a string that contains names and phone numbers mixed with various other characters. Your task is to write a function `transform_contact_details` that extracts and transforms the contact details into a standardized format. The format of the names is: `FirstName LastName` The format of the phone numbers can vary but will be provided in one of the following formats: - `(XXX) XXX-XXXX` - `XXX-XXX-XXXX` - `XXX.XXX.XXXX` - `XXX XXX XXXX` The transformation should adhere to the following rules: 1. Extract the full names and the phone numbers. 2. Standardize the phone number format to the form `XXX-XXX-XXXX`. 3. Return a list of tuples, each containing a name and the standardized phone number. **Function Signature**: ```python def transform_contact_details(text: str) -> list: pass ``` **Input**: - A single string `text` containing names and phone numbers in various formats. **Output**: - A list of tuples, where each tuple contains the name and the phone number in the standardized format. **Examples**: ```python # Example 1 text = \\"Alice Smith (123) 456-7890, Bob Jones 123-456-7890, Charlie Brown 123.456.7890\\" print(transform_contact_details(text)) # Output: [(\'Alice Smith\', \'123-456-7890\'), (\'Bob Jones\', \'123-456-7890\'), (\'Charlie Brown\', \'123-456-7890\')] # Example 2 text = \\"Daisy Ridley 123 456 7890, John Doe (987) 654-3210\\" print(transform_contact_details(text)) # Output: [(\'Daisy Ridley\', \'123-456-7890\'), (\'John Doe\', \'987-654-3210\')] ``` **Constraints**: - Names consist of only alphabetic characters and a space (`A-Z`, `a-z`, and `\' \' `). - Phone numbers will consist of exactly 10 digits. - The input string will contain well-separated contact details. - You must use regular expressions for pattern matching and substitutions. **Hints**: - Use the `re` module to identify and extract the names and phone numbers. - Remember to account for different phone number formats. - Consider using capturing groups to assist with transformation and extraction. **Performance Requirements**: - The function should work efficiently even for longer strings with multiple contacts (up to 1000 contacts). Good luck, and ensure your solution is clear and well-commented!","solution":"import re def transform_contact_details(text: str) -> list: # Regular expression to match and capture the full name and the phone number. pattern = re.compile( r\'([A-Za-z]+s[A-Za-z]+)s*[()-.s]*(d{3})[()-.s]*(d{3})[()-.s]*(d{4})\' ) matches = pattern.findall(text) transformed_contacts = [] for match in matches: name = match[0] phone_number = f\\"{match[1]}-{match[2]}-{match[3]}\\" transformed_contacts.append((name, phone_number)) return transformed_contacts"},{"question":"# Python Coding Assessment Question Objective: Demonstrate understanding and practical skills with Python\'s `base64` module, specifically focusing on encoding and decoding functionalities with different base encodings. Problem Statement: You are tasked to implement a function `encode_decode_cycle` that takes a binary data input and performs a series of encoding and decoding transformations using different base encodings. The function should: 1. Encode the input data using Base64 with the standard alphabet. 2. Decode the Base64 encoded data back to its original form. 3. Encode the result using Base32. 4. Decode the Base32 encoded data back to its original form. 5. Encode the result using Base16. 6. Decode the Base16 encoded data back to its original form and return the final result. Your function should ensure that the decoded results at each step match the original input data. Function Signature: ```python def encode_decode_cycle(data: bytes) -> bytes: pass ``` Input: - `data`: A `bytes` object representing the raw binary data. (1 <= len(data) <= 10^6) Output: - Returns a `bytes` object that is the final decoded result, which should be identical to the input `data`. Constraints: - Use the `base64` module for all encoding and decoding operations. - Handle any exceptions that may occur due to invalid input or unexpected errors gracefully. Example: ```python # Example usage of the function input_data = b\\"example data\\" result = encode_decode_cycle(input_data) # Output should be identical to input assert result == input_data ``` Notes: - Ensure your implementation is efficient to handle the maximum input size within reasonable time limits. - Pay attention to details such as padding and optional parameters that might affect encoding/decoding. Implementation: ```python import base64 def encode_decode_cycle(data: bytes) -> bytes: # Step 1: Encode using Base64. encoded_b64 = base64.b64encode(data) # Step 2: Decode back from Base64. decoded_b64 = base64.b64decode(encoded_b64) # Step 3: Encode using Base32. encoded_b32 = base64.b32encode(decoded_b64) # Step 4: Decode back from Base32. decoded_b32 = base64.b32decode(encoded_b32) # Step 5: Encode using Base16. encoded_b16 = base64.b16encode(decoded_b32) # Step 6: Decode back from Base16. decoded_b16 = base64.b16decode(encoded_b16) # The final result should be identical to the original data. return decoded_b16 ``` **Test your implementation thoroughly to ensure correctness for various input data lengths and values.**","solution":"import base64 def encode_decode_cycle(data: bytes) -> bytes: Encode and decode the input data through Base64, Base32, and Base16 transformations. # Step 1: Encode using Base64. encoded_b64 = base64.b64encode(data) # Step 2: Decode back from Base64. decoded_b64 = base64.b64decode(encoded_b64) # Step 3: Encode using Base32. encoded_b32 = base64.b32encode(decoded_b64) # Step 4: Decode back from Base32. decoded_b32 = base64.b32decode(encoded_b32) # Step 5: Encode using Base16. encoded_b16 = base64.b16encode(decoded_b32) # Step 6: Decode back from Base16. decoded_b16 = base64.b16decode(encoded_b16) # The final result should be identical to the original data. return decoded_b16"},{"question":"# Pandas Coding Assessment Question You are provided with a dataset containing daily stock prices of a company over a year. You need to perform several operations using pandas window functions and return various statistics. Implement a function `calculate_statistics` that takes a pandas DataFrame as input and returns a new DataFrame with the following columns: 1. **Roll_Mean_10**: The 10-day rolling mean of the stock prices. 2. **Roll_Std_10**: The 10-day rolling standard deviation of the stock prices. 3. **Exp_WMA_10**: The 10-day exponentially weighted moving average of the stock prices. 4. **Exp_WMAVar_10**: The 10-day exponentially weighted moving variance of the stock prices. 5. **Expanding_Mean**: The expanding mean of the stock prices over time. Input: - A pandas DataFrame `df` with a single column `\'Price\'` representing the daily stock prices. Output: - A new pandas DataFrame with five columns as described above. Ensure that the resulting DataFrame aligns the window calculations properly with the original data\'s index. Constraints: - You may assume the DataFrame `df` contains at least 10 rows of data. You can use the following pandas window functions: - `df.rolling(window=10).mean()` - `df.rolling(window=10).std()` - `df.ewm(span=10).mean()` - `df.ewm(span=10).var()` - `df.expanding().mean()` Example: ```python import pandas as pd # Sample DataFrame data = { \'Price\': [150, 152, 153, 155, 157, 160, 162, 165, 167, 170, 172, 174, 176, 178, 180] } df = pd.DataFrame(data) # Expected function call result = calculate_statistics(df) # Expected output DataFrame (the actual numerical values will vary) print(result) ``` Expected DataFrame structure (values shown are illustrative): ``` Roll_Mean_10 Roll_Std_10 Exp_WMA_10 Exp_WMAVar_10 Expanding_Mean 0 NaN NaN NaN NaN 150.00 1 NaN NaN NaN NaN 151.00 2 NaN NaN NaN NaN 151.67 3 NaN NaN NaN NaN 152.50 4 NaN NaN NaN NaN 153.40 5 NaN NaN NaN NaN 154.50 6 NaN NaN NaN NaN 155.57 7 NaN NaN NaN NaN 156.63 8 NaN NaN NaN NaN 157.66 9 159.1 5.72014 159.66 NaN 158.40 10 160.6 6.03 160.60 NaN 159.22 ... ... ... ... ... ... ``` This problem assesses your ability to effectively utilize pandas rolling, expanding, and exponentially weighted window functions in time series data.","solution":"import pandas as pd def calculate_statistics(df): Calculate various statistics on the stock prices provided in the dataframe. Parameters: - df: pandas DataFrame with a single column \'Price\' Returns: - new_df: pandas DataFrame with calculated statistics. new_df = pd.DataFrame() # Calculate 10-day rolling mean new_df[\'Roll_Mean_10\'] = df[\'Price\'].rolling(window=10).mean() # Calculate 10-day rolling standard deviation new_df[\'Roll_Std_10\'] = df[\'Price\'].rolling(window=10).std() # Calculate 10-day exponentially weighted moving average new_df[\'Exp_WMA_10\'] = df[\'Price\'].ewm(span=10, adjust=False).mean() # Calculate 10-day exponentially weighted moving variance new_df[\'Exp_WMAVar_10\'] = df[\'Price\'].ewm(span=10, adjust=False).var() # Calculate expanding mean new_df[\'Expanding_Mean\'] = df[\'Price\'].expanding().mean() return new_df"},{"question":"Objective Write a Python function that performs the following tasks using the Seaborn library: 1. Loads the \\"tips\\" dataset. 2. Plots a linear regression model of `y=\\"tip\\"` on `x=\\"total_bill\\"` and separates the dataset by the \\"smoker\\" status using different colors. Save the plot to a file named `regression_plot.png`. 3. Additionally, add jitter to the `x=\\"size\\"` and `y=\\"tip\\"` values in another plot and save it to a file named `jitter_plot.png`. 4. Create a polynomial regression plot of order 2 for `y=\\"tip\\"` on `x=\\"size\\"` and save it to a file named `poly_regression_plot.png`. 5. Display a residual plot for `y=\\"tip\\"` on `x=\\"total_bill\\"` and save it to a file named `residual_plot.png`. Constraints - Use the `seaborn` library functions as described in the documentation. - Ensure that each plot is properly labeled with titles and axis labels. - Save each plot to a file with the specified name. Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt def plot_seaborn_regressions(): # Your code here # Example call to the function (this will not be evaluated, just for your testing) plot_seaborn_regressions() ``` Expected Output Four files should be created in the same directory from which the function is run: - `regression_plot.png` - `jitter_plot.png` - `poly_regression_plot.png` - `residual_plot.png` These files should contain the respective plots as described in the objectives. Notes - This question tests your understanding of Seaborn\'s regression plotting functions and your ability to create and save plots programmatically. - Pay attention to plot aesthetics and readability. - No need to return any value from the function; just ensure the plots are generated and saved correctly.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_seaborn_regressions(): # Load the \\"tips\\" dataset tips = sns.load_dataset(\\"tips\\") # 1. Plot linear regression model of y=\\"tip\\" on x=\\"total_bill\\" separated by \\"smoker\\" status plt.figure(figsize=(10, 6)) sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", data=tips) plt.title(\\"Linear Regression of Tip on Total Bill by Smoker Status\\") plt.savefig(\\"regression_plot.png\\") plt.close() # Close the figure to save memory # 2. Add jitter to the x=\\"size\\" and y=\\"tip\\" values on another plot plt.figure(figsize=(10, 6)) sns.stripplot(x=\\"size\\", y=\\"tip\\", data=tips, jitter=True) plt.title(\\"Jitter Plot of Tip vs Size\\") plt.xlabel(\\"Size\\") plt.ylabel(\\"Tip\\") plt.savefig(\\"jitter_plot.png\\") plt.close() # 3. Create polynomial regression plot of order 2 for y=\\"tip\\" on x=\\"size\\" plt.figure(figsize=(10, 6)) sns.lmplot(x=\\"size\\", y=\\"tip\\", data=tips, order=2) plt.title(\\"Polynomial Regression of Tip on Size (Order 2)\\") plt.xlabel(\\"Size\\") plt.ylabel(\\"Tip\\") plt.savefig(\\"poly_regression_plot.png\\") plt.close() # 4. Display a residual plot for y=\\"tip\\" on x=\\"total_bill\\" plt.figure(figsize=(10, 6)) sns.residplot(x=\\"total_bill\\", y=\\"tip\\", data=tips) plt.title(\\"Residual Plot of Tip on Total Bill\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Residuals\\") plt.savefig(\\"residual_plot.png\\") plt.close()"},{"question":"**Question: Implement a Comprehensive Preprocessing Pipeline for Multiclass and Multilabel Targets** Given the importance of transforming prediction targets (`y`) in machine learning preprocessing, your task is to implement a function that processes both multiclass and multilabel targets. You are required to handle three types of transformations using the scikit-learn preprocessing tools as described below. # Function Signature ```python def preprocess_targets(y, transform_type): Preprocess the target labels according to the specified transformation type. Parameters: y (list): A list of target labels. It can be a list of integers (multiclass) or a list of lists (multilabel). transform_type (str): The type of preprocessing to be applied. Can be \'LabelBinarizer\', \'MultiLabelBinarizer\', or \'LabelEncoder\'. Returns: dict: A dictionary with two keys: - \'transformed_y\': The transformed target labels. - \'inverse_transform_function\': A function that can be used to inverse transform the labels back to the original form. This function takes a list of transformed labels as input and returns the original labels. pass ``` # Input Description 1. `y`: - A list of target labels. - For \'LabelBinarizer\' and \'LabelEncoder\', `y` is a list of integers (multiclass) or strings. - For \'MultiLabelBinarizer\', `y` is a list of lists, where each sublist contains multiple labels. 2. `transform_type`: - A string specifying the type of transformation to apply. - Can be one of \'LabelBinarizer\', \'MultiLabelBinarizer\', or \'LabelEncoder\'. # Output Description The function returns a dictionary with the following keys: 1. `\'transformed_y\'`: - The transformed target labels as per the specified transformation type. 2. `\'inverse_transform_function\'`: - A function to reverse the transformation. This function, when called with a list of transformed labels, returns the original labels. # Example Usage ```python # Example 1: Using LabelBinarizer result = preprocess_targets([1, 2, 6, 4, 2], \'LabelBinarizer\') transformed_y = result[\'transformed_y\'] inverse_transform_func = result[\'inverse_transform_function\'] print(transformed_y) # Output might be something similar to: # array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 1, 0, 0]]) print(inverse_transform_func(transformed_y)) # Should output: # [1, 2, 6, 4, 2] # Example 2: Using MultiLabelBinarizer result = preprocess_targets([[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]], \'MultiLabelBinarizer\') transformed_y = result[\'transformed_y\'] inverse_transform_func = result[\'inverse_transform_function\'] print(transformed_y) # Output might be something similar to: # array([[0, 0, 1, 1, 1], [0, 0, 1, 0, 0], [1, 1, 0, 1, 0], [1, 1, 1, 1, 1], [1, 1, 1, 0, 0]]) print(inverse_transform_func(transformed_y)) # Should output: # [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]] # Example 3: Using LabelEncoder result = preprocess_targets([\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"], \'LabelEncoder\') transformed_y = result[\'transformed_y\'] inverse_transform_func = result[\'inverse_transform_function\'] print(transformed_y) # Output might be something similar to: # array([1, 1, 2, 0]) print(inverse_transform_func(transformed_y)) # Should output: # [\'paris\', \'paris\', \'tokyo\', \'amsterdam\'] ``` # Constraints - The input list `y` will have at least one element. - All elements in each sublist of `y` (for \'MultiLabelBinarizer\') are unique. - If `transform_type` is invalid, the function should raise a `ValueError` with a message \\"Invalid transformation type\\". # Performance Requirements - The solution should efficiently handle lists with up to 10000 elements and up to 1000 unique classes. You are allowed to use the `sklearn.preprocessing` module for implementing the solution.","solution":"from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder def preprocess_targets(y, transform_type): Preprocess the target labels according to the specified transformation type. Parameters: y (list): A list of target labels. It can be a list of integers (multiclass) or a list of lists (multilabel). transform_type (str): The type of preprocessing to be applied. Can be \'LabelBinarizer\', \'MultiLabelBinarizer\', or \'LabelEncoder\'. Returns: dict: A dictionary with two keys: - \'transformed_y\': The transformed target labels. - \'inverse_transform_function\': A function that can be used to inverse transform the labels back to the original form. This function takes a list of transformed labels as input and returns the original labels. if transform_type == \'LabelBinarizer\': transformer = LabelBinarizer() transformed_y = transformer.fit_transform(y) elif transform_type == \'MultiLabelBinarizer\': transformer = MultiLabelBinarizer() transformed_y = transformer.fit_transform(y) elif transform_type == \'LabelEncoder\': transformer = LabelEncoder() transformed_y = transformer.fit_transform(y) else: raise ValueError(\\"Invalid transformation type\\") inverse_transform_function = lambda x: transformer.inverse_transform(x) return { \'transformed_y\': transformed_y, \'inverse_transform_function\': inverse_transform_function }"},{"question":"**Problem Statement:** You are given a dataset that contains the coordinates of several points in a 2D plane. Your task is to implement a function called `closest_and_farthest_points` that finds the pair of points that are closest to each other and the pair of points that are farthest from each other. You should use mathematical functions from the `math` module to compute the required distances. **Function Signature:** ```python def closest_and_farthest_points(points: List[Tuple[float, float]]) -> Tuple[Tuple[Tuple[float, float], Tuple[float, float]], Tuple[Tuple[float, float], Tuple[float, float]]]: pass ``` **Input:** - `points`: A list of tuples where each tuple represents a point in the 2D plane, with the x and y coordinates as floating-point numbers. **Output:** - A tuple of two pairs of points: - The first pair represents the closest pair of points in the dataset. - The second pair represents the farthest pair of points in the dataset. **Constraints:** 1. The input list will contain at least two points. 2. The coordinate values in the points can be both positive and negative. 3. You should use the `math.dist` function to calculate the Euclidean distance between two points. **Example:** ```python points = [(1.0, 2.0), (3.0, 4.0), (5.0, 1.0), (1.0, 1.0)] # Closest pair of points: ((1.0, 2.0), (1.0, 1.0)) # Farthest pair of points: ((1.0, 2.0), (5.0, 1.0)) closest_and_farthest_points(points) # Output: (((1.0, 2.0), (1.0, 1.0)), ((1.0, 2.0), (5.0, 1.0))) ``` **Explanation:** 1. Closest pair: ((1.0, 2.0), (1.0, 1.0)) with distance 1.0 2. Farthest pair: ((1.0, 2.0), (5.0, 1.0)) with distance 4.472... Your implementation should make efficient use of the mathematical functions provided in the `math` module to solve the problem accurately and efficiently.","solution":"from typing import List, Tuple import math def closest_and_farthest_points(points: List[Tuple[float, float]]) -> Tuple[Tuple[Tuple[float, float], Tuple[float, float]], Tuple[Tuple[float, float], Tuple[float, float]]]: min_distance = float(\'inf\') max_distance = 0 closest_pair = (points[0], points[1]) farthest_pair = (points[0], points[1]) for i in range(len(points)): for j in range(i + 1, len(points)): distance = math.dist(points[i], points[j]) if distance < min_distance: min_distance = distance closest_pair = (points[i], points[j]) if distance > max_distance: max_distance = distance farthest_pair = (points[i], points[j]) return (closest_pair, farthest_pair)"},{"question":"# Hyper-Parameter Tuning with Scikit-Learn Objective: Your task is to implement a machine learning model for a classification problem and optimize its hyper-parameters using scikit-learn\'s `GridSearchCV` and `RandomizedSearchCV`. Description: Given the Iris dataset, you are required to: 1. Load the dataset and split it into training and testing sets. 2. Implement and train a Support Vector Classifier (SVC) model. 3. Perform hyper-parameter tuning using `GridSearchCV` with the following parameter grid: ``` python param_grid = [ {\'C\': [1, 10, 100], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']}, ] ``` 4. Perform hyper-parameter tuning using `RandomizedSearchCV` with the following parameter distributions: ``` python param_dist = { \'C\': scipy.stats.expon(scale=100), \'gamma\': scipy.stats.expon(scale=.1), \'kernel\': [\'rbf\'], \'class_weight\': [\'balanced\', None] } ``` Note: Use `n_iter=10` for `RandomizedSearchCV`. 5. Evaluate the best models obtained from `GridSearchCV` and `RandomizedSearchCV` on the test set. Instructions: 1. Implement the function `perform_hyperparameter_tuning` which: - Takes no arguments. - Loads the Iris dataset. - Splits the dataset into 80% training and 20% testing. - Initializes an SVC model. - Performs hyper-parameter tuning using `GridSearchCV` with the specified `param_grid`. - Performs hyper-parameter tuning using `RandomizedSearchCV` with the specified `param_dist` and `n_iter=10`. - Prints the best parameters and the best score for both `GridSearchCV` and `RandomizedSearchCV`. - Evaluates and prints the test accuracy for the best models obtained from `GridSearchCV` and `RandomizedSearchCV`. Constraints: - Use `random_state=42` for reproducibility where applicable. - You may use any necessary libraries such as `scipy` and `pandas`. Function Signature: ```python def perform_hyperparameter_tuning(): # Your code here pass ``` Example Output: ```plaintext Best parameters found by GridSearchCV: {\'C\': 10, \'kernel\': \'linear\'} Best score: 0.98 Best parameters found by RandomizedSearchCV: {\'C\': 245.345, \'class_weight\': \'balanced\', \'gamma\': 0.002, \'kernel\': \'rbf\'} Best score: 0.97 Test accuracy of the best model from GridSearchCV: 0.96 Test accuracy of the best model from RandomizedSearchCV: 0.95 ```","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV from sklearn.svm import SVC from scipy.stats import expon from sklearn.metrics import accuracy_score def perform_hyperparameter_tuning(): # Load the Iris dataset iris = load_iris() X = iris.data y = iris.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize the SVC model svc = SVC(random_state=42) # Define the parameter grid for GridSearchCV param_grid = [ {\'C\': [1, 10, 100], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']}, ] # Perform GridSearchCV grid_search = GridSearchCV(svc, param_grid, cv=5) grid_search.fit(X_train, y_train) # Get best parameters and score for GridSearchCV best_params_grid = grid_search.best_params_ best_score_grid = grid_search.best_score_ # Print GridSearchCV results print(\\"Best parameters found by GridSearchCV:\\") print(best_params_grid) print(f\\"Best score: {best_score_grid:.2f}\\") # Define the parameter distributions for RandomizedSearchCV param_dist = { \'C\': expon(scale=100), \'gamma\': expon(scale=.1), \'kernel\': [\'rbf\'], \'class_weight\': [\'balanced\', None] } # Perform RandomizedSearchCV random_search = RandomizedSearchCV(svc, param_distributions=param_dist, n_iter=10, cv=5, random_state=42) random_search.fit(X_train, y_train) # Get best parameters and score for RandomizedSearchCV best_params_random = random_search.best_params_ best_score_random = random_search.best_score_ # Print RandomizedSearchCV results print(\\"Best parameters found by RandomizedSearchCV:\\") print(best_params_random) print(f\\"Best score: {best_score_random:.2f}\\") # Evaluate the best models obtained from both searches on the test set best_model_grid = grid_search.best_estimator_ best_model_random = random_search.best_estimator_ test_accuracy_grid = accuracy_score(y_test, best_model_grid.predict(X_test)) test_accuracy_random = accuracy_score(y_test, best_model_random.predict(X_test)) # Print test accuracy results print(f\\"Test accuracy of the best model from GridSearchCV: {test_accuracy_grid:.2f}\\") print(f\\"Test accuracy of the best model from RandomizedSearchCV: {test_accuracy_random:.2f}\\")"},{"question":"**IntegerArray Data Manipulation with Pandas** Given a dataset of employees in a CSV file with columns `employee_id`, `age`, `department`, and `salary`, where the `employee_id` and `salary` columns might have missing values. You are required to perform the following tasks using Pandas `IntegerArray`: **Instructions:** 1. **Reading Data:** - Read the CSV file into a Pandas DataFrame, ensuring that `employee_id` and `salary` columns are read as nullable integers (using `Int64` dtype). 2. **Data Cleaning:** - Replace all missing `salary` values with the median salary of their respective departments. If a department has no non-missing salaries, default the missing salaries in that department to 0. 3. **Data Analysis:** - Calculate the average salary per department and add this as a new column named `avg_salary` for each employee. 4. **Data Output:** - Save the modified DataFrame to a new CSV file named `cleaned_employee_data.csv`. **Function Signature:** ```python import pandas as pd def process_employee_data(file_path: str) -> None: Processes the employee data from the given CSV file path and saves the cleaned data to \'cleaned_employee_data.csv\'. Args: file_path (str): Path to the input CSV file. Returns: None # Your implementation here ``` **Example:** With an input CSV named `employee_data.csv` containing: ```csv employee_id,age,department,salary 1,25,Engineering,50000 2,30,HR,45000 3,28,Engineering, 4,35,HR, NaN,40,Marketing,62000 5,50,Marketing,NaN ``` **Expected Output:** A CSV file named `cleaned_employee_data.csv`: ```csv employee_id,age,department,salary,avg_salary 1,25,Engineering,50000,50000.0 2,30,HR,45000,45000.0 3,28,Engineering,50000,50000.0 4,35,HR,45000,45000.0 NaN,40,Marketing,62000,62000.0 5,50,Marketing,62000,62000.0 ``` Notes: - The median salary for the `Engineering` department is 50000. - The median salary for the `HR` department is 45000. - The average salaries (`avg_salary`) per department are added based on the cleaned data. Ensure the solution exhibits efficient use of Pandas functionalities, especially focusing on the nullable integer operations introduced.","solution":"import pandas as pd def process_employee_data(file_path: str) -> None: # Reading the input CSV file with specified Int64 dtype for nullable integer columns df = pd.read_csv(file_path, dtype={\'employee_id\': \'Int64\', \'salary\': \'Int64\'}) # Filling missing \'salary\' values with the median salary of their respective departments for department, group in df.groupby(\'department\'): median_salary = group[\'salary\'].median() if pd.isna(median_salary): median_salary = 0 df.loc[df[\'department\'] == department, \'salary\'] = df.loc[df[\'department\'] == department, \'salary\'].fillna(median_salary) # Calculating the average salary per department avg_salaries = df.groupby(\'department\')[\'salary\'].mean().rename(\'avg_salary\') df = df.join(avg_salaries, on=\'department\') # Writing the cleaned DataFrame to a new CSV file df.to_csv(\'cleaned_employee_data.csv\', index=False)"},{"question":"# Pandas MultiIndex Challenge In this assessment, you will be working with the `MultiIndex` feature of pandas to perform a series of data manipulation tasks. This exercise is designed to evaluate your understanding of hierarchical indexing and your ability to apply multiple features of the `MultiIndex` functionality. Task You are provided with sales data for a store, organized by different categories and subcategories. Your task is to use the `MultiIndex` feature to create and manipulate this data to extract specific information. Data You will be working with a DataFrame containing the following columns: - `Category`: High-level categories (e.g., Electronics, Clothing) - `Subcategory`: Detailed categories within the main category (e.g., Mobile, Laptop, Men\'s Wear, Women\'s Wear) - `Name`: Product names - `Sales`: Sales figures for each product Heres the sample data you should use to create the initial DataFrame: ```python import pandas as pd import numpy as np data = { \'Category\': [\'Electronics\', \'Electronics\', \'Electronics\', \'Clothing\', \'Clothing\', \'Clothing\'], \'Subcategory\': [\'Mobile\', \'Laptop\', \'Tablet\', \'Men\'s Wear\', \'Women\'s Wear\', \'Children\'s Wear\'], \'Name\': [\'Phone A\', \'Laptop B\', \'Tablet C\', \'Shirt D\', \'Dress E\', \'Shirt F\'], \'Sales\': [150, 200, 100, 50, 80, 40] } df = pd.DataFrame(data) ``` Steps to Perform 1. **Create a MultiIndex DataFrame**: - Convert the above DataFrame into a `MultiIndex` DataFrame with \'Category\' and \'Subcategory\' as the hierarchical index. 2. **Extract and Analyze**: - From the MultiIndex DataFrame, select and display all products under the \'Clothing\' category. - Calculate and display the total sales for each \'Category\'. 3. **Advanced Indexing Operations**: - Use slicing to select and display data for Electronics category from \'Mobile\' to \'Tablet\' subcategories inclusive. - Reorder the levels of the MultiIndex so that \'Subcategory\' becomes the first level. 4. **Modify and Update**: - Add a new column \'Discount\' to the DataFrame with values [10, 15, 20, 5, 10, 5] respectively for each product. - Update the \'Sales\' values by deducting the \'Discount\' for each product and display the updated DataFrame. 5. **Reindex and Align**: - Create a new DataFrame with additional subcategories under \'Electronics\' (e.g., \'Camera\') and reindex the original MultiIndex DataFrame to include these new subcategories. # Constraints - Ensure to handle any potential errors gracefully, providing meaningful output or handling missing indices appropriately. - Performance is important, so aim to use vectorized operations provided by pandas wherever possible. # Expected Output - Display the MultiIndex DataFrame created in step 1. - Display the output of each extraction and analysis step. - Display the reordered MultiIndex DataFrame. - Display the final DataFrame after adding the \'Discount\' column and updating \'Sales\'. - Display the reindexed DataFrame including new subcategories. # Submission Submit your code as a Python script or Jupyter Notebook with all necessary imports and results displayed. Ensure your code is well-documented and commented to explain your approach at each step.","solution":"import pandas as pd import numpy as np # Initial DataFrame data = { \'Category\': [\'Electronics\', \'Electronics\', \'Electronics\', \'Clothing\', \'Clothing\', \'Clothing\'], \'Subcategory\': [\'Mobile\', \'Laptop\', \'Tablet\', \'Men\'s Wear\', \'Women\'s Wear\', \'Children\'s Wear\'], \'Name\': [\'Phone A\', \'Laptop B\', \'Tablet C\', \'Shirt D\', \'Dress E\', \'Shirt F\'], \'Sales\': [150, 200, 100, 50, 80, 40] } df = pd.DataFrame(data) # Step 1: Create the MultiIndex DataFrame multi_index_df = df.set_index([\'Category\', \'Subcategory\']) # Step 2: Extract and analyze the data # Select and display all products under the \'Clothing\' category clothing_products = multi_index_df.loc[\'Clothing\'] # Calculate and display the total sales for each \'Category\' total_sales_per_category = multi_index_df.groupby(level=\'Category\').sum()[\'Sales\'] # Step 3: Advanced Indexing Operations # Use slicing to select and display data for the Electronics category from \'Mobile\' to \'Tablet\' electronics_sliced = multi_index_df.loc[(\'Electronics\', slice(\'Mobile\', \'Tablet\'))] # Reorder the levels of the MultiIndex so that \'Subcategory\' becomes the first level reordered_multi_index_df = multi_index_df.reorder_levels([\'Subcategory\', \'Category\']) # Step 4: Modify and Update discounts = [10, 15, 20, 5, 10, 5] multi_index_df[\'Discount\'] = discounts multi_index_df[\'Sales\'] = multi_index_df[\'Sales\'] - multi_index_df[\'Discount\'] # Step 5: Reindex and Align additional_data = { \'Category\': [\'Electronics\', \'Electronics\'], \'Subcategory\': [\'Camera\', \'Accessories\'], \'Name\': [np.nan, np.nan], \'Sales\': [np.nan, np.nan], \'Discount\': [np.nan, np.nan] } additional_df = pd.DataFrame(additional_data).set_index([\'Category\', \'Subcategory\']) extended_index_df = multi_index_df.reindex(multi_index_df.index.union(additional_df.index)) # Display results def display_results(): print(\\"Step 1: MultiIndex DataFrame\\") print(multi_index_df) print(\\"nStep 2: Clothing products\\") print(clothing_products) print(\\"nStep 2: Total sales per category\\") print(total_sales_per_category) print(\\"nStep 3: Electronics category from \'Mobile\' to \'Tablet\'\\") print(electronics_sliced) print(\\"nStep 3: Reordered MultiIndex DataFrame\\") print(reordered_multi_index_df) print(\\"nStep 4: DataFrame after adding \'Discount\' and updating \'Sales\'\\") print(multi_index_df) print(\\"nStep 5: Extended DataFrame with new subcategories\\") print(extended_index_df) display_results()"},{"question":"--- **Title**: Constructing and Sending a MIME Email with Attachments **Objective**: Implement a Python function that constructs a MIME email message with various types of attachments and saves it to a file. **Problem Statement**: You are required to implement a function `create_mime_email(subject, sender, recipient, text_body, html_body, attachment_paths, save_path)` which constructs a MIME email message with the following components: 1. **Subject**: The subject of the email (string). 2. **Sender**: The sender\'s email address (string). 3. **Recipient**: The recipient\'s email address (string). 4. **Text Body**: The plain text version of the email body (string). 5. **HTML Body**: The HTML version of the email body (string). 6. **Attachment Paths**: A list of file paths for the attachments (list of strings). 7. **Save Path**: The file path where the email should be saved (string). The function should construct a valid MIME email with: - A plain text version of the email body. - An HTML version of the email body. - Multiple attachments, which may include images, audio files, and application data. Ensure that the constructed MIME email adheres to standard MIME structure and includes all necessary headers. Finally, the constructed email should be saved to the specified file path. **Constraints**: - Each filepath in `attachment_paths` is guaranteed to be a valid path to an existing file. - The function should handle different file types appropriately (e.g., using `MIMEText` for text, `MIMEImage` for images). **Function Signature**: ```python def create_mime_email(subject: str, sender: str, recipient: str, text_body: str, html_body: str, attachment_paths: list, save_path: str) -> None: pass ``` **Example**: ```python subject = \\"Test Email\\" sender = \\"sender@example.com\\" recipient = \\"recipient@example.com\\" text_body = \\"This is the plain text body of the email.\\" html_body = \\"<html><body><p>This is the HTML body of the <b>email</b>.</p></body></html>\\" attachment_paths = [\\"path/to/image.jpg\\", \\"path/to/document.pdf\\", \\"path/to/audio.mp3\\"] save_path = \\"path/to/save/email.eml\\" create_mime_email(subject, sender, recipient, text_body, html_body, attachment_paths, save_path) ``` In the above example, the function should create a MIME email with the provided subject, sender, and recipient, include both plain text and HTML bodies, attach the specified files, and save the email to the provided path. **Notes**: - Use the `email.mime` module classes and ensure correct multipart structure. - Handle encoding and appropriate MIME types for attachments. - Include appropriate headers such as `From`, `To`, `Subject`, `MIME-Version`, and `Content-Type`. ---","solution":"import os import mimetypes from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.base import MIMEBase from email.utils import formataddr from email import encoders def create_mime_email(subject, sender, recipient, text_body, html_body, attachment_paths, save_path): # Create the container email message. msg = MIMEMultipart(\'alternative\') msg[\'Subject\'] = subject msg[\'From\'] = formataddr((\'Sender\', sender)) msg[\'To\'] = formataddr((\'Recipient\', recipient)) # Attach the plain text and HTML message bodies. part1 = MIMEText(text_body, \'plain\') part2 = MIMEText(html_body, \'html\') msg.attach(part1) msg.attach(part2) # Add attachment parts for attachment_path in attachment_paths: if os.path.isfile(attachment_path): ctype, encoding = mimetypes.guess_type(attachment_path) if ctype is None or encoding is not None: # No guess could be made, or the file is encoded (compressed), so use a generic type. ctype = \'application/octet-stream\' maintype, subtype = ctype.split(\'/\', 1) # Read the attachment file with open(attachment_path, \'rb\') as fp: file_content = fp.read() attachment = MIMEBase(maintype, subtype) attachment.set_payload(file_content) encoders.encode_base64(attachment) filename = os.path.basename(attachment_path) attachment.add_header(\'Content-Disposition\', \'attachment\', filename=filename) msg.attach(attachment) # Output the message to a file with open(save_path, \'wb\') as f: f.write(msg.as_bytes())"},{"question":"Analyzing and Modifying a DataFrame with Copy-on-Write Behavior You are provided with a dataset containing information about various products sold in a store. The dataset includes the following columns: `ProductID`, `ProductName`, `Category`, `Price`, `Discount`, and `QuantitySold`. Your task is to write a function `analyze_and_modify_df` which takes in a `DataFrame` and performs the following operations: 1. **Filter Products**: Create a new DataFrame that contains only products from a specific category with a price greater than a specified value. 2. **Add Discounted Price**: Add a new column `DiscountedPrice` to the filtered DataFrame. Compute the discounted price for each product as `Price - (Price * Discount/100)`. 3. **Sort by Quantity Sold**: Sort the filtered DataFrame by `QuantitySold` in descending order. 4. **Update Quantity**: Increase the `QuantitySold` of a specific product by a given number. Ensure that this update does not inadvertently modify any other DataFrame that might share data with this one. The function signature should be: ```python import pandas as pd def analyze_and_modify_df(df: pd.DataFrame, category: str, min_price: float, product_id: int, increase_qty: int) -> pd.DataFrame: Analyzes and modifies the input DataFrame according to specified rules. Parameters: - df (pd.DataFrame): The input DataFrame containing products data. - category (str): The category of products to filter. - min_price (float): The minimum price to filter products. - product_id (int): The ProductID of the product whose QuantitySold is to be increased. - increase_qty (int): The number by which to increase the QuantitySold of the specified product. Returns: - pd.DataFrame: The modified DataFrame after applying all operations. pass ``` # Constraints: - The `QuantitySold` should only be updated in the filtered DataFrame without affecting the original DataFrame or any other views derived from it. - All operations should comply with the Copy-on-Write principles. # Example Usage: ```python data = { \\"ProductID\\": [1, 2, 3, 4], \\"ProductName\\": [\\"Apple\\", \\"Banana\\", \\"Carrot\\", \\"Dates\\"], \\"Category\\": [\\"Fruit\\", \\"Fruit\\", \\"Vegetable\\", \\"Dry Fruit\\"], \\"Price\\": [100, 30, 50, 150], \\"Discount\\": [10, 0, 5, 20], \\"QuantitySold\\": [1000, 1500, 800, 400] } df = pd.DataFrame(data) result_df = analyze_and_modify_df(df, \\"Fruit\\", 50, 1, 100) print(result_df) ``` **Expected Output:** ``` ProductID ProductName Category Price Discount QuantitySold DiscountedPrice 0 1 Apple Fruit 100 10 1100 90.0 ``` **Note:** Your implementation will be graded based on correctness, adherence to CoW principles, and efficient use of pandas functionalities.","solution":"import pandas as pd def analyze_and_modify_df(df: pd.DataFrame, category: str, min_price: float, product_id: int, increase_qty: int) -> pd.DataFrame: Analyzes and modifies the input DataFrame according to specified rules. Parameters: - df (pd.DataFrame): The input DataFrame containing products data. - category (str): The category of products to filter. - min_price (float): The minimum price to filter products. - product_id (int): The ProductID of the product whose QuantitySold is to be increased. - increase_qty (int): The number by which to increase the QuantitySold of the specified product. Returns: - pd.DataFrame: The modified DataFrame after applying all operations. # Filter products by category and price filtered_df = df[(df[\'Category\'] == category) & (df[\'Price\'] > min_price)].copy() # Add DiscountedPrice column filtered_df[\'DiscountedPrice\'] = filtered_df[\'Price\'] - (filtered_df[\'Price\'] * filtered_df[\'Discount\'] / 100) # Sort by QuantitySold in descending order filtered_df.sort_values(by=\'QuantitySold\', ascending=False, inplace=True) # Update QuantitySold for the specific product filtered_df.loc[filtered_df[\'ProductID\'] == product_id, \'QuantitySold\'] += increase_qty return filtered_df"},{"question":"# Custom Exception Handler You are required to implement a function `calculate_operations()` that accepts two parameters: a list of tuples, each containing two elements (`x` and `y`) on which arithmetic operations will be performed sequentially. Your function should return a list of results for each operation. Requirements: 1. **Arithmetic Operations**: - If `y` is zero, raise a `ZeroDivisionError`. - If the addition of `x` and `y` results in an overflow, raise an `OverflowError`. - If any of the parameters (`x` or `y`) are not an integer, raise a `TypeError`. 2. **Custom Exception**: - Define a custom exception `InvalidOperationError` which inherits from `Exception`. This must be raised if the input parameters `x` or `y` are negative. 3. **Exception Handling**: - Catch exceptions and for each exception, append a dictionary to the result list with the following structure: `{\\"error\\": <error_message>, \\"type\\": <error_type>}`. - For basic exceptions, include their built-in error message. - For `InvalidOperationError`, provide a custom error message like `\\"InvalidOperationError: x or y is negative\\"`. - Chain the exceptions appropriately if one leads to another. Input: - `operations`: List of tuples e.g. `[(10, 5), (7, 0), (\\"a\\", 2), (-1, 2)]` Output: - List of results and handled exception information in the specified format. Function Signature: ```python def calculate_operations(operations: List[Tuple[int, int]]) -> List[Union[int, Dict[str, str]]]: pass ``` # Example: ```python operations = [(10, 5), (7, 0), (\\"a\\", 2), (-1, 2)] result = calculate_operations(operations) print(result) # Output: [15, {\\"error\\": \\"division by zero\\", \\"type\\": \\"ZeroDivisionError\\"}, {\\"error\\": \\"unsupported operand type(s) for +: \'int\' and \'str\'\\", \\"type\\": \\"TypeError\\"}, {\\"error\\": \\"InvalidOperationError: x or y is negative\\", \\"type\\": \\"InvalidOperationError\\"}] ``` Note: Ensure that your function raises and handles the exceptions as specified in the requirements.","solution":"from typing import List, Tuple, Union, Dict class InvalidOperationError(Exception): Custom exception for invalid operations when x or y is negative. pass def calculate_operations(operations: List[Tuple[int, int]]) -> List[Union[int, Dict[str, str]]]: results = [] for (x, y) in operations: try: # Type checking if not isinstance(x, int) or not isinstance(y, int): raise TypeError(f\\"unsupported operand type(s) for +: \'{type(x).__name__}\' and \'{type(y).__name__}\'\\") # Check for negative values if x < 0 or y < 0: raise InvalidOperationError(\\"InvalidOperationError: x or y is negative\\") # Division by zero if y == 0: raise ZeroDivisionError(\\"division by zero\\") # Overflow handling (simplistic check for illustrative purposes) result = x + y if result > 2**31 - 1 or result < -(2**31): raise OverflowError(\\"result too large\\") results.append(result) except Exception as e: if isinstance(e, InvalidOperationError): results.append({\\"error\\": str(e), \\"type\\": \\"InvalidOperationError\\"}) else: results.append({\\"error\\": str(e), \\"type\\": type(e).__name__}) return results"},{"question":"# **Coding Assessment Question** **Context:** You are tasked with implementing a custom TCP server using the `asyncio` module in Python 3.10. This server should not only echo back messages received from clients but also maintain a log of all incoming connections and the data received from each connection in a structured format. **Question:** 1. Implement a class `CustomTCPProtocol` inheriting from `asyncio.Protocol`. It should: - Handle new connections by logging the client address. - Log all data received from clients. - Echo received data back to the client. - Close the connection after sending the data back. 2. Implement an asynchronous function `main` to set up and run the TCP server. **Requirements:** - The protocol class `CustomTCPProtocol` should have an internal log structure which is a dictionary where the keys are client addresses and values are lists of messages received from each client. - The server should listen on `127.0.0.1` and port `8888`. **Constraints:** - Ensure that your log does not grow indefinitely. Cap the number of messages logged for each client to 100. If the message limit is reached, discard the oldest message. - The server should gracefully handle disconnections and exceptions. **Performance Requirements:** - The server should be able to handle multiple simultaneous connections efficiently. **Expected Input/Output:** - Your `CustomTCPProtocol` class should log each connection and data received. - Example log structure after processing several client connections might look like: ```python { (\'127.0.0.1\', 12345): [\\"Hello\\", \\"World\\", \\"Foo\\"], (\'127.0.0.1\', 67890): [\\"Bar\\", \\"Baz\\"] } ``` - The log should be accessible as an attribute of your protocol class. **Additional Notes:** - Ensure your code is well-commented and adheres to Python\'s best coding practices. ```python import asyncio class CustomTCPProtocol(asyncio.Protocol): def __init__(self): self.transport = None self.log = {} def connection_made(self, transport): self.transport = transport peername = transport.get_extra_info(\'peername\') print(f\\"Connection from {peername}\\") if peername not in self.log: self.log[peername] = [] def data_received(self, data): message = data.decode() peername = self.transport.get_extra_info(\'peername\') print(f\\"Data received from {peername}: {message}\\") # Adding message to the log with a cap of 100 messages per client if len(self.log[peername]) >= 100: self.log[peername].pop(0) self.log[peername].append(message) # Echo back the received message self.transport.write(data) print(\\"Close the client socket\\") self.transport.close() def connection_lost(self, exc): peername = self.transport.get_extra_info(\'peername\') if exc: print(f\\"The server closed the connection with {peername} due to an error: {exc}\\") else: print(f\\"The server closed the connection with {peername}\\") async def main(): # Get a reference to the event loop as we plan to use low-level APIs loop = asyncio.get_running_loop() server = await loop.create_server( lambda: CustomTCPProtocol(), \'127.0.0.1\', 8888 ) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main()) ```","solution":"import asyncio from collections import deque class CustomTCPProtocol(asyncio.Protocol): def __init__(self): self.transport = None self.log = {} def connection_made(self, transport): self.transport = transport peername = transport.get_extra_info(\'peername\') print(f\\"Connection from {peername}\\") if peername not in self.log: self.log[peername] = deque(maxlen=100) def data_received(self, data): message = data.decode() peername = self.transport.get_extra_info(\'peername\') print(f\\"Data received from {peername}: {message}\\") # Adding message to the log with a cap of 100 messages per client self.log[peername].append(message) # Echo back the received message self.transport.write(data) print(\\"Close the client socket\\") self.transport.close() def connection_lost(self, exc): peername = self.transport.get_extra_info(\'peername\') if exc: print(f\\"The server closed the connection with {peername} due to an error: {exc}\\") else: print(f\\"The server closed the connection with {peername}\\") async def main(): # Get a reference to the event loop as we plan to use low-level APIs loop = asyncio.get_running_loop() server = await loop.create_server( lambda: CustomTCPProtocol(), \'127.0.0.1\', 8888 ) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Porting Python 2 Code to Python 3 Background You have a project written in Python 2.7 that you need to port to ensure compatibility with Python 3. The project involves text processing, file handling, and some arithmetic operations. The organization wants to maintain a single code base that runs seamlessly on both Python 2.7 and Python 3. Task Your task is to implement a function `process_file(input_file, output_file)` that reads from a file containing mixed text and binary data, processes it, and writes the processed data to another file. The function should: 1. Read the input file in binary mode. 2. Decode the binary data to text as soon as it\'s read. 3. Perform some specific text processing operations: - Replace all occurrences of `foo` with `bar`. - Ensure all line breaks are converted to Unix-style (`n`). 4. Encode the text back to binary as late as possible before writing it to the output file. 5. Write the processed binary data to the output file in binary mode. Additionally, ensure that: - The function runs under both Python 2.7 and Python 3 without modification. - Proper error handling is implemented to manage exceptions related to file operations and encoding/decoding issues. - You include unit tests to verify the functionality of your code with at least 80% code coverage. Example Assume `input_file.txt` contains: ``` Hello foo world! This is a test.foo ``` Your function call: ```python process_file(\'input_file.txt\', \'output_file.txt\') ``` Should create `output_file.txt` containing: ``` Hello bar world! This is a test.bar ``` Constraints - The function should support only text and binary data, not mixed-mode files. - It should handle large files efficiently. - Python 2.7 and Python 3.5+ should be the target versions. Function Signature ```python def process_file(input_file: str, output_file: str) -> None: pass ``` Instructions 1. Implement the `process_file` function as specified. 2. Write unit tests to check the correctness of the function. 3. Ensure your code runs under both Python 2.7 and Python 3.","solution":"def process_file(input_file, output_file): Reads from a file in binary mode, processes its contents to replace \'foo\' with \'bar\', converts line breaks to Unix-style and writes the result to another file. import io try: # Open the input file in binary mode and read its content with open(input_file, \'rb\') as f: binary_data = f.read() # Decode binary data to text (assuming UTF-8 encoding) text_data = binary_data.decode(\'utf-8\') # Perform the text processing processed_text = text_data.replace(\'foo\', \'bar\').replace(\'rn\', \'n\').replace(\'r\', \'n\') # Encode the processed text back to binary processed_binary = processed_text.encode(\'utf-8\') # Write the processed binary data to the output file in binary mode with open(output_file, \'wb\') as f: f.write(processed_binary) except (IOError, OSError) as e: print(\\"File operation failed: \\", e) except UnicodeDecodeError as e: print(\\"Decoding failed: \\", e) except UnicodeEncodeError as e: print(\\"Encoding failed: \\", e)"},{"question":"# Question: Implement a Custom XML Character Escaper You are provided with an XML string that contains special characters which need to be properly escaped according to XML standards. Additionally, you need to handle custom character entities that might be provided. Task Write a function `custom_escape(data: str, entities: dict = {}) -> str` that escapes the characters \'&\', \'<\', and \'>\' in the given string `data`. You should also handle custom entities provided through the `entities` dictionary. The function should match the following criteria: 1. The characters \'&\', \'<\', and \'>\' are always escaped to \'&amp;\', \'&lt;\', and \'&gt;\' respectively. 2. The `entities` dictionary contains mappings of other characters that need to be escaped. 3. If there are overlapping entities (e.g., both `&` and a custom entity using `&`), the predefined entities (`&`, `<`, `>`) take priority over custom ones. Input - `data` (str): The XML string that needs to be escaped. - `entities` (dict): A dictionary where the keys are characters to be escaped and the values are their replacement strings. Output - A string with the appropriate characters escaped. Constraints - The length of `data` is between 0 and 10^6 characters. - The `entities` dictionary contains up to 100 key-value pairs. Example ```python data = \\"3 < 5 & 5 > 3\\" entities = {\'3\': \'#003\', \'5\': \'#005\'} print(custom_escape(data, entities)) ``` Expected Output: ``` \\"#003 &lt; #005 &amp; #005 &gt; #003\\" ``` Note You may not use the `xml.sax.saxutils.escape` function directly, but you may refer to its behavior for guidance. Solution Template ```python def custom_escape(data: str, entities: dict = {}) -> str: # Predefined escape mappings escape_map = {\'&\': \'&amp;\', \'<\': \'&lt;\', \'>\': \'&gt;\'} # Update escape_map with custom entities while keeping predefined entities\' priority for key, value in entities.items(): if key not in escape_map: escape_map[key] = value # Escape the data using the escape_map escaped_data = \'\'.join(escape_map.get(char, char) for char in data) return escaped_data # Example usage data = \\"3 < 5 & 5 > 3\\" entities = {\'3\': \'#003\', \'5\': \'#005\'} print(custom_escape(data, entities)) # should return \\"#003 &lt; #005 &amp; #005 &gt; #003\\" ```","solution":"def custom_escape(data: str, entities: dict = {}) -> str: # Predefined escape mappings escape_map = {\'&\': \'&amp;\', \'<\': \'&lt;\', \'>\': \'&gt;\'} # Get list of keys from entities to maintain the insertion order priority_entities = {key: value for key, value in entities.items() if key not in escape_map} # Updated escape_map with custom entities maintaining their lower priority priority_entities.update(escape_map) escape_map = priority_entities # Escape the data using the escape_map escaped_data = \'\'.join(escape_map.get(char, char) for char in data) return escaped_data"},{"question":"You are tasked to write a Python program that uses the `binascii` module to handle file encoding and decoding operations. Specifically, you will need to: 1. Read the contents of a text file and uuencode the data. 2. Convert the uuencoded data to base64 encoding. 3. Generate a hexadecimal representation of the base64 encoded data. 4. Perform CRC-32 checksum of the hexadecimal representation. Implement the following functions: 1. **read_file(file_path)**: - **Input**: `file_path` (str) - the path to the input text file. - **Output**: Return the contents of the file as a bytes object. - **Constraints**: The function should handle the case where the file does not exist and raise a `FileNotFoundError`. 2. **uuencode_data(data)**: - **Input**: `data` (bytes) - the binary data to be uuencoded. - **Output**: Return the uuencoded data as a bytes object. 3. **base64_encode(data)**: - **Input**: `data` (bytes) - binary data to be base64 encoded. - **Output**: Return the base64 encoded data (including newline char) as a bytes object. - **Constraints**: Use the `newline` parameter in the encoding. 4. **to_hexadecimal(data)**: - **Input**: `data` (bytes) - binary data to be converted to hexadecimal representation. - **Output**: Return the hexadecimal representation as a bytes object. 5. **calculate_crc32(data)**: - **Input**: `data` (bytes) - binary data to calculate the CRC-32 checksum for. - **Output**: Return the CRC-32 checksum as an integer. Finally, use these functions to read a provided text file, perform the described operations, and print each intermediate step and the final CRC-32 checksum. Example: ```python # Contents of \\"example.txt\\": # Hello, binascii! data = read_file(\\"example.txt\\") uuencoded = uuencode_data(data) base64_encoded = base64_encode(uuencoded) hex_encoded = to_hexadecimal(base64_encoded) crc32_checksum = calculate_crc32(hex_encoded) print(uuencoded) print(base64_encoded) print(hex_encoded) print(crc32_checksum) ``` **Expected Output:** ``` > uuencoded data: b\'begin 644 n8&AE(+)``n #nendn\' > base64 encoded data: b\'YmVnaW4gNjQ0IDANCmZldGVyCmVuZA==n\' > hexadecimal encoded data: b\'596d56335a586b67636d644966644e434dZu519=5d\' > CRC32 checksum: 0x8c736521 ``` # Constraints: - Use `binascii` module for all encoding and decoding processes. - Ensure proper error handling for file read operations. - Utilize the specified parameters in encoding functions where applicable. - Assume the input file size can be up to 1MB. **Notes**: - Refer to the official Python documentation for the `binascii` module if necessary to understand the usage of each required function.","solution":"import binascii def read_file(file_path): Reads the contents of a file and returns it as bytes. Raises FileNotFoundError if the file does not exist. try: with open(file_path, \'rb\') as file: return file.read() except FileNotFoundError: raise FileNotFoundError(\\"The file does not exist\\") def uuencode_data(data): Uuencode the data and return it as bytes. return binascii.b2a_uu(data) def base64_encode(data): Base64 encode the data and return it as bytes. return binascii.b2a_base64(data, newline=False) def to_hexadecimal(data): Convert binary data to hexadecimal representation and return it as bytes. return binascii.hexlify(data) def calculate_crc32(data): Calculate CRC-32 checksum of the data and return it as an integer. return binascii.crc32(data) & 0xffffffff # Example usage: if __name__ == \\"__main__\\": try: data = read_file(\\"example.txt\\") uuencoded = uuencode_data(data) base64_encoded = base64_encode(uuencoded) hex_encoded = to_hexadecimal(base64_encoded) crc32_checksum = calculate_crc32(hex_encoded) print(f\\"UUencoded data: {uuencoded}\\") print(f\\"Base64 encoded data: {base64_encoded}\\") print(f\\"Hexadecimal encoded data: {hex_encoded}\\") print(f\\"CRC-32 checksum: {crc32_checksum:#010x}\\") except FileNotFoundError as e: print(e)"},{"question":"# Question: Visualizing Yearly Trends with Seaborn Given a dataset recording various yearly observations, your task is to: 1. Load and process the dataset to ensure it is in a suitable long-form structure for visualization with Seaborn. 2. Generate a series of visualizations to explore different aspects of the data. Dataset Description: * The dataset is a CSV file named `yearly_data.csv`. It contains yearly observations of various metrics. * The dataset includes the following columns: * `year` (int): The year of observation. * `metric_1` (float): The value of the first metric. * `metric_2` (float): The value of the second metric. * `metric_3` (float): The value of the third metric. Steps: 1. Load the dataset using pandas. 2. Transform the dataset into a long-form structure where each row represents an observation of a metric in a particular year. 3. Create the following visualizations using Seaborn: * A line plot showing the trends of all three metrics over the years, with distinct lines for each metric. * A box plot for each metric, displaying the distribution of metric values across different years. * A point plot, with error bars representing the variability of the metrics over the years. # Function Signature ```python def visualize_yearly_trends(filepath: str) -> None: # Your code here ``` # Expected Input * `filepath` (str): The path to the dataset file (`yearly_data.csv`). # Expected Output * The function should generate and display the required plots. Ensure that all plot axes are appropriately labeled, and titles are included to describe each visualization. # Constraints * Use Seaborn and pandas libraries for data handling and plotting. * Ensure that the function handles cases where the data may have missing values or outliers. # Example ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_yearly_trends(filepath: str) -> None: # Load the dataset data = pd.read_csv(filepath) # Transform the dataset to long-form data_long = pd.melt(data, id_vars=[\'year\'], var_name=\'metric\', value_name=\'value\') # Line plot for trends over the years sns.relplot(data=data_long, x=\'year\', y=\'value\', hue=\'metric\', kind=\'line\') plt.title(\'Trends of Metrics Over the Years\') plt.xlabel(\'Year\') plt.ylabel(\'Metric Value\') plt.show() # Box plot for metric distributions across years sns.catplot(data=data_long, x=\'metric\', y=\'value\', kind=\'box\') plt.title(\'Distribution of Metrics\') plt.xlabel(\'Metric\') plt.ylabel(\'Value\') plt.show() # Point plot with error bars for variability of metrics sns.catplot(data=data_long, x=\'year\', y=\'value\', hue=\'metric\', kind=\'point\', ci=\'sd\') plt.title(\'Metrics Variability Over the Years\') plt.xlabel(\'Year\') plt.ylabel(\'Value\') plt.show() ```","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_yearly_trends(filepath: str) -> None: # Load the dataset data = pd.read_csv(filepath) # Transform the dataset to long-form data_long = pd.melt(data, id_vars=[\'year\'], var_name=\'metric\', value_name=\'value\') # Line plot for trends over the years sns.relplot(data=data_long, x=\'year\', y=\'value\', hue=\'metric\', kind=\'line\') plt.title(\'Trends of Metrics Over the Years\') plt.xlabel(\'Year\') plt.ylabel(\'Metric Value\') plt.show() # Box plot for metric distributions across years sns.catplot(data=data_long, x=\'metric\', y=\'value\', kind=\'box\') plt.title(\'Distribution of Metrics\') plt.xlabel(\'Metric\') plt.ylabel(\'Value\') plt.show() # Point plot with error bars for variability of metrics sns.catplot(data=data_long, x=\'year\', y=\'value\', hue=\'metric\', kind=\'point\', ci=\'sd\') plt.title(\'Metrics Variability Over the Years\') plt.xlabel(\'Year\') plt.ylabel(\'Value\') plt.show()"},{"question":"# Question: Secure File Verification System You are tasked with creating a secure file verification system. This system will involve generating a hash of a file to ensure its integrity and generate a random token to associate with the file for secure identification. Requirements: 1. **File Hashing using `hashlib`:** - Implement a function `generate_file_hash(file_path: str, algorithm: str) -> str`. - `file_path`: Path to the file to be hashed. - `algorithm`: Hash algorithm to be used (`\'sha256\'`, `\'sha512\'`, `\'blake2b\'`). - Returns a hexadecimal string representing the hash of the file. - If the specified algorithm is not supported, raise a `ValueError`. 2. **Random Token Generation using `secrets`:** - Implement a function `generate_secure_token() -> str`. - Returns a URL-safe text string of length 16, generated securely. 3. **File Verification using `hmac`:** - Implement a function `verify_file_integrity(file_path: str, given_hash: str, key: bytes, algorithm: str) -> bool`. - `file_path`: Path to the file to be verified. - `given_hash`: The hash to verify against, provided as a hexadecimal string. - `key`: The secret key used for HMAC generation. - `algorithm`: Hash algorithm to be used (`\'sha256\'`, `\'sha512\'`, `\'blake2b\'`). - Returns `True` if the generated HMAC matches the given hash, otherwise `False`. - Combine the file content and the HMAC key using the given algorithm. Example Usage: ```python # Sample file content: \\"Hello, World!\\" # Step 1: Generate file hash file_hash = generate_file_hash(\'hello.txt\', \'sha256\') print(file_hash) # Example output: \'a591a6d40bf420404a011733cfb7b190d62c65bf0bcda32b2a78bf08dac5a241\' # Step 2: Generate secure token token = generate_secure_token() print(token) # Example output: \'N5G6Z2IvtnBwxTZL\' # Step 3: Verify file integrity key = b\'secret_key_1234\' # Example key valid = verify_file_integrity(\'hello.txt\', file_hash, key, \'sha256\') print(valid) # Expected output: True ``` Constraints: - Assume files are not larger than 100 MB. - Supported algorithms are \'sha256\', \'sha512\', and \'blake2b\'. Notes: - Use Python\'s built-in libraries: `hashlib`, `secrets`, and `hmac`. - Ensure secure and efficient handling of file I/O operations. - Your implementation should handle edge cases and errors gracefully.","solution":"import hashlib import secrets import hmac def generate_file_hash(file_path: str, algorithm: str) -> str: Generates a hash for the given file using the specified hashing algorithm. :param file_path: Path to the file to be hashed. :param algorithm: Hash algorithm to be used (\'sha256\', \'sha512\', \'blake2b\'). :return: Hexadecimal string representing the hash of the file. :raise: ValueError if the specified algorithm is not supported. try: hash_func = hashlib.new(algorithm) except ValueError: raise ValueError(\\"Unsupported algorithm. Use \'sha256\', \'sha512\', or \'blake2b\'.\\") with open(file_path, \\"rb\\") as f: while chunk := f.read(8192): hash_func.update(chunk) return hash_func.hexdigest() def generate_secure_token() -> str: Generates a URL-safe text string of length 16 for secure identification. :return: A URL-safe secure random token. return secrets.token_urlsafe(16) def verify_file_integrity(file_path: str, given_hash: str, key: bytes, algorithm: str) -> bool: Verifies the integrity of the given file by comparing the HMAC of the file content using the given key with the provided hash. :param file_path: Path to the file to be verified. :param given_hash: The hash to verify against, provided as a hexadecimal string. :param key: The secret key used for HMAC generation. :param algorithm: Hash algorithm to be used (\'sha256\', \'sha512\', \'blake2b\'). :return: True if the generated HMAC matches the given hash, otherwise False. :raise: ValueError if the specified algorithm is not supported. try: hmac_func = hmac.new(key, digestmod=algorithm) except ValueError: raise ValueError(\\"Unsupported algorithm. Use \'sha256\', \'sha512\', or \'blake2b\'.\\") with open(file_path, \\"rb\\") as f: while chunk := f.read(8192): hmac_func.update(chunk) return hmac_func.hexdigest() == given_hash"},{"question":"**Question: Implement and Optimize a Neural Network with PyTorch JIT** **Problem Statement:** In this task, you are required to implement a simple neural network for a regression task using PyTorch. Once implemented, you will optimize the network using PyTorchs Just-In-Time (JIT) compilation utilities provided in `torch.utils.jit`. **Requirements:** 1. Implement a neural network in PyTorch for predicting a continuous value from a given input. 2. Write the forward pass for this neural network. 3. Use JIT to trace the model for a given example input. 4. After tracing, save the optimized model. **Steps:** 1. **Define the Neural Network:** - Implement a class `SimpleNN` inheriting from `torch.nn.Module`. - This network should accept inputs of size 10 and produce a single continuous output. - It should contain at least two hidden layers with ReLU activations. 2. **Instantiate and Trace the Model:** - Create an instance of the `SimpleNN` class. - Create a tensor of shape (1, 10) containing random floating point values to be used as a trace input. - Trace the model using `torch.jit.trace`. 3. **Save the Traced Model:** - Save the traced model to a file named `traced_model.pt`. **Code Requirements:** - **Input:** There is no input to the function. Youre just required to implement the specified tasks in a script. - **Output:** The script should save a JIT-compiled model as `traced_model.pt`. **Constraints:** - Ensure that all components of the neural network (e.g., layers, activation functions) are correctly defined. - Ensure that the network is compatible with JIT tracing. **Performance:** - The model\'s architecture and the tracing process should be efficient and free from errors. **Example Implementation:** ```python import torch import torch.nn as nn import torch.utils.jit # Define the neural network class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 25) self.out = nn.Linear(25, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.out(x) return x # Instantiate the model model = SimpleNN() # Create an example input for tracing example_input = torch.rand(1, 10) # Trace the model traced_model = torch.jit.trace(model, example_input) # Save the traced model traced_model.save(\\"traced_model.pt\\") ``` Ensure your implementation follows a similar structure and meets all the provided requirements.","solution":"import torch import torch.nn as nn import torch.jit class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 25) self.out = nn.Linear(25, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.out(x) return x # Instantiate the model model = SimpleNN() # Create an example input for tracing example_input = torch.rand(1, 10) # Trace the model traced_model = torch.jit.trace(model, example_input) # Save the traced model traced_model.save(\\"traced_model.pt\\")"},{"question":"# Question Background: In Python, performance profiling is crucial for identifying bottlenecks in code and improving execution efficiency. Two powerful tools for this purpose are `cProfile` and `timeit`. `cProfile` helps to profile the entire code and presents detailed information about the time taken by each function. `timeit`, on the other hand, measures the execution time of small code snippets with precision. Task: You are provided with a function that needs optimization. Write a Python script that does the following: 1. **Profiles** the given function using `cProfile` and identifies the top 3 most time-consuming functions/methods within it. 2. **Measures** the execution time of the given function using `timeit` over 1000 runs to get a precise sense of its average execution time. 3. **Optimizes** the given function based on the profiling results. **Function to optimize:** ```python def process_data(data): # Simulating a data processing task result = [] for item in data: processed_item = item * 2 # Simulate some processing result.append(processed_item) sorted_result = sorted(result) final_result = sum(sorted_result) / len(sorted_result) return final_result ``` Input and Output: - **Input:** List of integers `data` - **Output:** Optimized function that performs the same task as `process_data` in a more efficient manner. Constraints: - The input list `data` will always have at least 1000 integers and at most 10000 integers. - Each integer in the list will be between `1` and `100`. Performance Requirements: - Your optimized function should improve the runtime performance significantly compared to the original `process_data`. Steps: 1. Use `cProfile` to identify the top 3 most time-consuming parts of the function. 2. Use `timeit` to get the average execution time of the original `process_data` function. 3. Submit your optimized function along with a brief explanation of how you improved its performance. 4. Use `timeit` again to measure and compare the performance of your optimized function with the original. **Note:** Ensure that your code to profile and time the functions is clean and properly formatted for readability. Example: ```python import cProfile import timeit # Original function def process_data(data): result = [item * 2 for item in data] sorted_result = sorted(result) final_result = sum(sorted_result) / len(sorted_result) return final_result # Your profiling code def profile_function(func, *args, **kwargs): profiler = cProfile.Profile() profiler.enable() result = func(*args, **kwargs) profiler.disable() profiler.print_stats(sort=\'cumtime\') return result # Your timeit code def measure_time(func, *args, **kwargs): timer = timeit.Timer(lambda: func(*args, **kwargs)) return timer.timeit(number=1000) # Example usage data = [i for i in range(1, 1001)] profile_function(process_data, data) execution_time = measure_time(process_data, data) print(f\\"Average execution time over 1000 runs: {execution_time:.5f} seconds\\") # Write your optimized function based on the profiling results and measure its performance ```","solution":"import cProfile import timeit def process_data(data): # Simulating a data processing task result = [item * 2 for item in data] sorted_result = sorted(result) final_result = sum(sorted_result) / len(sorted_result) return final_result def profile_function(func, *args, **kwargs): profiler = cProfile.Profile() profiler.enable() result = func(*args, **kwargs) profiler.disable() profiler.print_stats(sort=\'cumtime\') return result def measure_time(func, *args, **kwargs): timer = timeit.Timer(lambda: func(*args, **kwargs)) return timer.timeit(number=1000) # Example data data = [i for i in range(1, 10001)] # Profile the original process_data function profile_function(process_data, data) # Measure the time taken by the original process_data function original_time = measure_time(process_data, data) print(f\\"Original average execution time over 1000 runs: {original_time:.5f} seconds\\") # Optimized function def optimized_process_data(data): result = sum(data) # Summing before the multiplication final_result = (result * 2) / len(data) return final_result # Profile the optimized process_data function profile_function(optimized_process_data, data) # Measure the time taken by the optimized_process_data function optimized_time = measure_time(optimized_process_data, data) print(f\\"Optimized average execution time over 1000 runs: {optimized_time:.5f} seconds\\")"},{"question":"**Question Title: Enhanced ZIP Archive Manager** **Objective:** Your task is to create a Python function `enhanced_zip_manager` that can handle multiple operations on ZIP archives based on specified inputs. The function should be capable of: 1. Creating a new ZIP file from a given list of files. 2. Extracting all files from a specified ZIP archive to a given directory. 3. Listing the contents of a ZIP archive. 4. Adding new files to an existing ZIP archive. **Function Signature:** ```python def enhanced_zip_manager(operation: str, zip_filename: str, target: str, files: list = None) -> list: Perform the specified operation on the ZIP archive. Parameters: - operation (str): The operation to perform. Can be \'create\', \'extract\', \'list\', or \'add\'. - zip_filename (str): The name of the ZIP file to operate on. - target (str): The target directory for extraction or the output directory for new ZIP files. - files (list, optional): A list of files to include in the ZIP archive. Only used for \'create\' and \'add\' operations. Returns: - list: A list of file names if the operation is \'list\'. Otherwise, return an empty list. pass ``` **Detailed Specifications:** 1. **Creating a ZIP Archive:** - If the operation is `\'create\'`, the function should create a new ZIP file with the specified name `zip_filename` and include all files listed in the `files` parameter. - After creating the ZIP file, all the specified files should be located within the newly created archive. 2. **Extracting from a ZIP Archive:** - If the operation is `\'extract\'`, the function should extract all elements of the provided ZIP file `zip_filename` into the directory specified by `target`. - The function should handle cases where the `target` directory does not exist by creating it. 3. **Listing the Contents of a ZIP Archive:** - If the operation is `\'list\'`, the function should return a list of all file names within the ZIP archive specified by `zip_filename`. 4. **Adding Files to an Existing ZIP Archive:** - If the operation is `\'add\'`, the function should add the specified files in the `files` parameter to the existing ZIP archive `zip_filename`. **Constraints and Requirements:** - Assume all file paths used in the `files` parameter are valid and accessible. - Ensure that any directories required for extraction are created as needed. - Make sure to handle errors gracefully, raising appropriate exceptions where necessary (e.g., file not found, read/write errors, invalid operations). - Function should follow good practices such as using context managers to handle files and ensuring proper cleanup (e.g., closing files). **Examples:** ```python # Example to create a ZIP archive enhanced_zip_manager(\'create\', \'my_archive.zip\', \'\', [\'file1.txt\', \'file2.txt\']) # Example to extract a ZIP archive enhanced_zip_manager(\'extract\', \'my_archive.zip\', \'/extracted_files/\') # Example to list contents of a ZIP archive file_list = enhanced_zip_manager(\'list\', \'my_archive.zip\', \'\') print(file_list) # Output: [\'file1.txt\', \'file2.txt\'] # Example to add files to a ZIP archive enhanced_zip_manager(\'add\', \'my_archive.zip\', \'\', [\'file3.txt\']) ``` Ensure to follow the instructions and constraints for implementing the function correctly. Your solution should demonstrate the ability to handle ZIP archives effectively using the `zipfile` module.","solution":"import os import zipfile def enhanced_zip_manager(operation: str, zip_filename: str, target: str, files: list = None) -> list: Perform the specified operation on the ZIP archive. Parameters: - operation (str): The operation to perform. Can be \'create\', \'extract\', \'list\', or \'add\'. - zip_filename (str): The name of the ZIP file to operate on. - target (str): The target directory for extraction or the output directory for new ZIP files. - files (list, optional): A list of files to include in the ZIP archive. Only used for \'create\' and \'add\' operations. Returns: - list: A list of file names if the operation is \'list\'. Otherwise, return an empty list. result = [] # Create a new ZIP archive if operation == \'create\': with zipfile.ZipFile(zip_filename, \'w\') as zipf: for file in files: zipf.write(file, os.path.basename(file)) # Extract contents of the ZIP archive elif operation == \'extract\': with zipfile.ZipFile(zip_filename, \'r\') as zipf: zipf.extractall(target) # List contents of the ZIP archive elif operation == \'list\': with zipfile.ZipFile(zip_filename, \'r\') as zipf: result = zipf.namelist() # Add files to an existing ZIP archive elif operation == \'add\': with zipfile.ZipFile(zip_filename, \'a\') as zipf: for file in files: zipf.write(file, os.path.basename(file)) return result"},{"question":"# Python Coding Assessment: Implementing a Class-Based Warehouse Inventory System Objective Design and implement a class-based system to manage a warehouse inventory. The system should support functionalities such as adding items, removing items, checking item quantities, and iterating over all items. Problem Statement You are tasked with creating a Python class `Warehouse` to manage the inventory of a warehouse. The warehouse should be able to store multiple types of items, each with a unique name and quantity. Your implementation should demonstrate the following capabilities: 1. **Define the `Warehouse` class** with appropriate attributes to store item names and their respective quantities. 2. **Implement methods** to: - Add items to the warehouse. - Remove items from the warehouse. - Check the quantity of a specific item. 3. **Handle edge cases** such as adding duplicate items and removing items that do not exist. 4. **Create an iterator** within the `Warehouse` class to iterate over all items in the inventory. 5. **Use inheritance** to extend the functionality of a basic `Warehouse`, e.g., `PerishableWarehouse` for perishable items with an expiration date. Task Breakdown 1. Define a class named `Warehouse` with the following methods: - `add_item(item_name: str, quantity: int) -> None`: Adds or updates the quantity of an item in the warehouse. - `remove_item(item_name: str, quantity: int) -> None`: Removes the specified quantity of an item from the warehouse. If the item\'s quantity becomes zero or less, it should be removed from the inventory. - `check_quantity(item_name: str) -> int`: Returns the quantity of the specified item in the warehouse. If the item does not exist, return `0`. - `__iter__()`: Returns an iterator that iterates over all items in the warehouse. 2. Define a subclass named `PerishableWarehouse` inheriting from `Warehouse` with the following additional functionality: - Each item should also have an expiration date. - Ensure perishable items nearing or past their expiration date can be listed or removed. - Implement a method `remove_expired_items(current_date: str) -> None` that removes all items past their expiration date. Dates will be given in the format `YYYY-MM-DD`. Constraints - Item names will be string values with a maximum length of 100 characters. - Quantities will be positive integers. - The expiration date for perishable items will be provided in the format `YYYY-MM-DD`. Input and Output Examples ```python # Example usage of Warehouse class warehouse = Warehouse() warehouse.add_item(\\"Laptop\\", 10) warehouse.add_item(\\"Phone\\", 5) print(warehouse.check_quantity(\\"Laptop\\")) # Output: 10 warehouse.remove_item(\\"Laptop\\", 3) print(warehouse.check_quantity(\\"Laptop\\")) # Output: 7 # Example usage of PerishableWarehouse class perishable_warehouse = PerishableWarehouse() perishable_warehouse.add_item(\\"Milk\\", 20, \\"2023-10-01\\") perishable_warehouse.add_item(\\"Cheese\\", 15, \\"2023-09-15\\") print(perishable_warehouse.check_quantity(\\"Milk\\")) # Output: 20 perishable_warehouse.remove_expired_items(\\"2023-10-02\\") print(perishable_warehouse.check_quantity(\\"Milk\\")) # Output: 0, assuming the current date is 2023-10-02 ``` Performance Requirements - The class should handle up to 1000 items efficiently. - Methods should be optimized for frequent additions and removals. Implement the `Warehouse` and `PerishableWarehouse` classes as specified, ensuring all functionalities are properly tested and working.","solution":"class Warehouse: def __init__(self): self.inventory = {} def add_item(self, item_name: str, quantity: int) -> None: if item_name in self.inventory: self.inventory[item_name] += quantity else: self.inventory[item_name] = quantity def remove_item(self, item_name: str, quantity: int) -> None: if item_name in self.inventory: self.inventory[item_name] -= quantity if self.inventory[item_name] <= 0: del self.inventory[item_name] def check_quantity(self, item_name: str) -> int: return self.inventory.get(item_name, 0) def __iter__(self): return iter(self.inventory.items()) class PerishableWarehouse(Warehouse): def __init__(self): super().__init__() self.expiration_dates = {} def add_item(self, item_name: str, quantity: int, expiration_date: str) -> None: super().add_item(item_name, quantity) self.expiration_dates[item_name] = expiration_date def check_expiration(self, item_name: str) -> str: return self.expiration_dates.get(item_name) def remove_expired_items(self, current_date: str) -> None: items_to_remove = [item for item, exp_date in self.expiration_dates.items() if exp_date < current_date] for item in items_to_remove: if item in self.inventory: del self.inventory[item] del self.expiration_dates[item]"},{"question":"**Question: Compiling a Custom PyTorch Function Using `torch.compiler`** PyTorch provides a compiler API to optimize and compile functions to improve performance, particularly for specialized hardware like GPUs. You are tasked with writing a PyTorch function and compiling it using the `torch.compiler.compile` function. # Task 1. Define a custom PyTorch function named `matrix_multiply` that takes two 2D tensors as input and performs matrix multiplication. 2. Use the `torch.compiler.compile` method to compile this function. 3. Write a script to demonstrate the performance improvement of the compiled function on random input tensors. # Function Signature ```python import torch def matrix_multiply(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Multiply two 2D tensors using standard matrix multiplication rules and return the result. Args: A (torch.Tensor): A 2D tensor of shape (m, n). B (torch.Tensor): A 2D tensor of shape (n, p). Returns: torch.Tensor: A 2D tensor of shape (m, p) resulting from the matrix multiplication of A and B. pass def main(): Demonstrate the performance improvement of the compiled matrix_multiply function. # Define the matrix multiplication operation @torch.compiler.compile def compiled_matrix_multiply(A, B): return matrix_multiply(A, B) # Create random input tensors A = torch.randn(1000, 1000) B = torch.randn(1000, 1000) # Measure the execution time of the non-compiled function start = time.time() result = matrix_multiply(A, B) print(\\"Non-Compiled Function Execution Time:\\", time.time() - start) # Measure the execution time of the compiled function start = time.time() compiled_result = compiled_matrix_multiply(A, B) print(\\"Compiled Function Execution Time:\\", time.time() - start) # Validate the result to ensure correctness assert torch.allclose(result, compiled_result) ``` # Constraints and Requirements - Your custom function should only use PyTorch operations. - You should use `torch.compiler.compile` to compile the custom function. - Compare the execution time of the non-compiled and compiled functions using `time` module to highlight performance improvement. - Ensure that the results from both non-compiled and compiled functions are identical. Implement the `matrix_multiply` function and the `main` function to demonstrate the usage and performance improvement.","solution":"import torch import time def matrix_multiply(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Multiply two 2D tensors using standard matrix multiplication rules and return the result. Args: A (torch.Tensor): A 2D tensor of shape (m, n). B (torch.Tensor): A 2D tensor of shape (n, p). Returns: torch.Tensor: A 2D tensor of shape (m, p) resulting from the matrix multiplication of A and B. return torch.matmul(A, B) def main(): Demonstrate the performance improvement of the compiled matrix_multiply function. # Define the matrix multiplication operation using torch compiler @torch.jit.script def compiled_matrix_multiply(A, B): return matrix_multiply(A, B) # Create random input tensors A = torch.randn(1000, 1000) B = torch.randn(1000, 1000) # Measure the execution time of the non-compiled function start = time.time() result = matrix_multiply(A, B) non_compiled_time = time.time() - start print(\\"Non-Compiled Function Execution Time:\\", non_compiled_time) # Measure the execution time of the compiled function start = time.time() compiled_result = compiled_matrix_multiply(A, B) compiled_time = time.time() - start print(\\"Compiled Function Execution Time:\\", compiled_time) # Validate the result to ensure correctness assert torch.allclose(result, compiled_result) if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Signal Handling in Python The task is to create a custom system where signal handlers can be set up for various signals, an alarm can be triggered, and a timer can be managed. You are to implement a class named `SignalManager` with the following functionalities: Class: `SignalManager` # Methods: 1. **`__init__`** - Initialize the class. It should initialize an empty dictionary to store signal handlers. 2. **`set_signal_handler(self, signalnum, handler)`** - Set a signal handler for the given signal number. - **Parameters:** - `signalnum` (int): Signal number (e.g., `signal.SIGINT`). - `handler` (Callable): A function to handle the signal. - **Output:** None. 3. **`get_signal_handler(self, signalnum)`** - Get the current handler for the given signal number. - **Parameters:** - `signalnum` (int): Signal number. - **Output:** Returns the current handler for the signal. 4. **`set_alarm(self, time)`** - Set an alarm to trigger after a specified number of seconds. - **Parameters:** - `time` (int): Number of seconds until the alarm is triggered. - **Output:** Returns the number of seconds remaining for any previously set alarm. 5. **`set_timer(self, which, seconds, interval=0.0)`** - Set a timer for the given interval timer. - **Parameters:** - `which` (int): Timer type (e.g., `signal.ITIMER_REAL`). - `seconds` (float): Number of seconds until the timer first expires. - `interval` (float, optional): Interval for repeated timer expirations. Defaults to 0.0. - **Output:** Returns the old timer settings as a tuple `(delay, interval)`. # Example Usage: ```python import signal # Define a custom signal handler def my_handler(signum, frame): print(f\\"Handled signal: {signum}\\") # Create a SignalManager instance sm = SignalManager() # Set a signal handler for SIGINT sm.set_signal_handler(signal.SIGINT, my_handler) # Retrieve the current signal handler for SIGINT current_handler = sm.get_signal_handler(signal.SIGINT) # Set an alarm to trigger after 5 seconds remaining_time = sm.set_alarm(5) # Set a timer to trigger after 2.5 seconds, then every 1 second old_timer = sm.set_timer(signal.ITIMER_REAL, 2.5, 1.0) ``` Constraints: * You should handle possible exceptions that can be raised by invalid signal numbers or any other invalid inputs gracefully. * Only valid signal numbers and handler functions should be processed. Complete the implementation of the `SignalManager` class and ensure that it behaves as expected with the provided example.","solution":"import signal class SignalManager: def __init__(self): self.signal_handlers = {} def set_signal_handler(self, signalnum, handler): if not callable(handler): raise ValueError(\\"Handler must be callable\\") previous_handler = signal.signal(signalnum, handler) self.signal_handlers[signalnum] = handler return previous_handler def get_signal_handler(self, signalnum): if signalnum in self.signal_handlers: return self.signal_handlers[signalnum] else: raise ValueError(\\"No handler set for this signal\\") def set_alarm(self, time): if not isinstance(time, int): raise ValueError(\\"Time must be an integer\\") return signal.alarm(time) def set_timer(self, which, seconds, interval=0.0): if not isinstance(seconds, (int, float)): raise ValueError(\\"Seconds must be a number\\") if not isinstance(interval, (int, float)): raise ValueError(\\"Interval must be a number\\") old_timer = signal.setitimer(which, seconds, interval) return old_timer"},{"question":"# Parallel Computation with \\"concurrent.futures\\" You are required to write a Python program that uses the \\"concurrent.futures\\" module to execute parallel tasks. The task involves calculating the sum of squares for a list of numbers. To achieve this, you must split the list into chunks and compute the sum of squares for each chunk in parallel. Function Signature ```python def parallel_sum_of_squares(numbers: List[int], chunk_size: int) -> int: ``` Input 1. `numbers` (List[int]): A list of integers, each representing a number whose square will be included in the sum. 2. `chunk_size` (int): The size of each chunk to split the list of numbers. Output - This function should return the total sum of squares of all the numbers in the list. Constraints - The length of `numbers` can be up to `10^6`. - `chunk_size` will always be a positive integer and will be less than or equal to the length of `numbers`. Example ```python numbers = [1, 2, 3, 4, 5] chunk_size = 2 result = parallel_sum_of_squares(numbers, chunk_size) print(result) # Output: 55 ``` Explanation For the given example, the squares of the numbers are: - 1^2 = 1 - 2^2 = 4 - 3^2 = 9 - 4^2 = 16 - 5^2 = 25 The total sum of squares is: 1 + 4 + 9 + 16 + 25 = 55 Notes 1. You should use the `concurrent.futures` module to parallelize the computation of sums of squares. 2. Handle the splitting of the list into chunks within your function. 3. Ensure that your function efficiently handles large lists and makes good use of parallel computing to speed up the processing time.","solution":"from typing import List from concurrent.futures import ThreadPoolExecutor def sum_of_squares_chunk(numbers: List[int]) -> int: Helper function to compute the sum of squares of a given list of numbers. return sum(x ** 2 for x in numbers) def parallel_sum_of_squares(numbers: List[int], chunk_size: int) -> int: Computes the total sum of squares of all numbers in the given list in parallel. The list is split into chunks of the specified size and processed concurrently. # Split the list into chunks chunks = [numbers[i:i + chunk_size] for i in range(0, len(numbers), chunk_size)] total_sum = 0 with ThreadPoolExecutor() as executor: # Map each chunk to the sum_of_squares_chunk function and collect results results = list(executor.map(sum_of_squares_chunk, chunks)) # Sum all the results from each chunk total_sum = sum(results) return total_sum"},{"question":"**Problem Statement:** You are given two tensors representing batches of images in different orders along with a scale tensor representing the scale factor for each channel. Your task is to standardize the batch of images to a common format, apply the scale to the channels, and return the scaled tensor in the original format of each batch. Specifically, you need to implement the function: ```python import torch def standardize_and_scale(imgs1: torch.Tensor, imgs2: torch.Tensor, scale: torch.Tensor) -> (torch.Tensor, torch.Tensor): This function standardizes the input image tensors to a common dimension order, scales the channels by the given scale tensor, and returns the scaled tensors in their respective original orders. Args: - imgs1 (torch.Tensor): A batch of images with named dimensions (N, C, H, W). - imgs2 (torch.Tensor): Another batch of images with named dimensions (N, H, W, C). - scale (torch.Tensor): A tensor with a named dimension \'C\' representing the scale factor for each channel. Returns: - (torch.Tensor, torch.Tensor): The scaled image tensors in their respective original orders. # Your implementation here ``` **Constraints:** 1. You must use the named tensors feature for dimension manipulation. 2. The tensors `imgs1` and `imgs2` will have dimensions ordered as indicated in their descriptions. 3. The scale tensor will have a named dimension \'C\'. 4. The function should return the processed images in the same dimension order as they were provided. **Example:** ```python import torch # Creating example tensors with named dimensions imgs1 = torch.rand(3, 4, 5, 6, names=(\'N\', \'C\', \'H\', \'W\')) # Batch 1 with dimensions (N, C, H, W) imgs2 = torch.rand(3, 5, 6, 4, names=(\'N\', \'H\', \'W\', \'C\')) # Batch 2 with dimensions (N, H, W, C) scale = torch.rand(4, names=(\'C\',)) # Scale tensor with dimension \'C\' # Call the function scaled_imgs1, scaled_imgs2 = standardize_and_scale(imgs1, imgs2, scale) # Check the output names print(scaled_imgs1.names) # Expected: (\'N\', \'C\', \'H\', \'W\') print(scaled_imgs2.names) # Expected: (\'N\', \'H\', \'W\', \'C\') ``` In this task, students must demonstrate their understanding of creating, aligning, and manipulating named tensors in PyTorch, while ensuring that dimension orders are appropriately maintained and utilized.","solution":"import torch def standardize_and_scale(imgs1: torch.Tensor, imgs2: torch.Tensor, scale: torch.Tensor) -> (torch.Tensor, torch.Tensor): # Convert imgs2 to the (N, C, H, W) format imgs2_nchw = imgs2.align_to(\'N\', \'H\', \'W\', \'C\').rename(None).permute(0, 3, 1, 2).refine_names(\'N\', \'C\', \'H\', \'W\') # Scale the channels scaled_imgs1 = imgs1 * scale.align_as(imgs1) scaled_imgs2_nchw = imgs2_nchw * scale.align_as(imgs2_nchw) # Convert scaled imgs2 back to the original (N, H, W, C) format scaled_imgs2 = scaled_imgs2_nchw.rename(None).permute(0, 2, 3, 1).refine_names(\'N\', \'H\', \'W\', \'C\') return scaled_imgs1, scaled_imgs2"},{"question":"# NIS Map Aggregator **Background:** You are responsible for managing user information across several Unix systems using the NIS (Network Information Service) for central administration. You need to create a utility function that fetches user information from various NIS maps and aggregates this data into a comprehensive dictionary. **Task:** Write a Python function `aggregate_user_info` that retrieves user data from multiple NIS maps and aggregates it into a single dictionary. Each user is identified by a unique user ID (uid), and their information can be found across three NIS maps: 1. `passwd.byuid`: Contains entries in the format `uid:passwd:username:info`. 2. `shadow.byuid`: Contains entries in the format `uid:shadow_info`. 3. `group.byuid`: Contains entries in the format `uid:groups`. The function should: 1. Take an optional `domain` parameter to specify the NIS domain (default to the system\'s default domain if not provided). 2. Return a dictionary where each key is a uid and the value is another dictionary with the following structure: ```python { \'passwd\': <passwd>, \'username\': <username>, \'info\': <info>, \'shadow_info\': <shadow_info>, \'groups\': <groups> } ``` **Constraints:** - If a uid is not present in a particular map, the associated keys in the result dictionary should have a value of `None`. - Handle any potential `nis.error` exceptions gracefully, and if an error occurs the function should return an empty dictionary. **Function Signature:** ```python def aggregate_user_info(domain: str = None) -> dict: pass ``` **Example:** Suppose the following entries exist in the NIS maps: - `passwd.byuid` contains: - `1001:passwd1:john:info1` - `1002:passwd2:jane:info2` - `shadow.byuid` contains: - `1001:shadow1` - `group.byuid` contains: - `1001:group1,group2` - `1002:group3` Your function should return: ```python { \'1001\': { \'passwd\': \'passwd1\', \'username\': \'john\', \'info\': \'info1\', \'shadow_info\': \'shadow1\', \'groups\': \'group1,group2\' }, \'1002\': { \'passwd\': \'passwd2\', \'username\': \'jane\', \'info\': \'info2\', \'shadow_info\': None, \'groups\': \'group3\' } } ``` Ensure your solution handles various edge cases such as missing entries and exceptions.","solution":"import nis def aggregate_user_info(domain: str = None) -> dict: result = {} maps = [\'passwd.byuid\', \'shadow.byuid\', \'group.byuid\'] # Try to get entries from all maps and handle nis.error exceptions try: for mapname in maps: nis_entries = nis.cat(mapname, domain) if domain else nis.cat(mapname) for entry in nis_entries.split(\'n\'): if not entry: continue key, value = entry.split(\\":\\")[0], entry.split(\\":\\")[1:] if key not in result: result[key] = { \'passwd\': None, \'username\': None, \'info\': None, \'shadow_info\': None, \'groups\': None } if mapname == \\"passwd.byuid\\": result[key][\'passwd\'] = value[0] result[key][\'username\'] = value[1] result[key][\'info\'] = value[2] elif mapname == \\"shadow.byuid\\": result[key][\'shadow_info\'] = value[0] elif mapname == \\"group.byuid\\": result[key][\'groups\'] = value[0] except nis.error as e: return {} return result"},{"question":"# Custom Iterator Implementation You are required to implement a custom iterator in Python that adheres to the Iterator Protocol. # Task: Implement a class `Countdown`, which is an iterable that counts down from a specified starting number to zero. # Specifications: 1. **Initialization**: - The `Countdown` class should be initialized with a single parameter `start`, which is a positive integer. 2. **Iterator Protocol**: - Implement the `__iter__()` method, which should return the iterator object itself. - Implement the `__next__()` method, which should return the next number in the countdown each time it is called. Once the countdown reaches zero, it should raise a `StopIteration` exception. 3. **Example Usage**: ```python countdown = Countdown(5) for number in countdown: print(number) # Output: # 5 # 4 # 3 # 2 # 1 ``` 4. **Constraints**: - You must follow the Iterator Protocol strictly. - Do not use built-in Python iterators (like `itertools` or list comprehensions) to solve this task. Implement the iterator manually. 5. **Input/Output**: - The input is provided during the initialization of the `Countdown` object. - The output is the sequence of countdown numbers generated when the iterator is used in a loop. # Notes: - Ensure your code is clean and handles edge cases, such as when `start` is zero (the iterator should not produce any output in this case). - Performance is not a primary concern for this task, but clarity and correctness of implementation are essential. # Example: ```python # Example function call: countdown = Countdown(3) for number in countdown: print(number) # Expected Output: # 3 # 2 # 1 ``` Implement the `Countdown` class below: ```python class Countdown: def __init__(self, start: int): pass def __iter__(self): pass def __next__(self): pass # Example usage: # countdown = Countdown(3) # for number in countdown: # print(number) ```","solution":"class Countdown: def __init__(self, start: int): self.start = start self.current = start def __iter__(self): return self def __next__(self): if self.current > 0: self.current -= 1 return self.current + 1 else: raise StopIteration"},{"question":"**Problem Statement:** You are provided with a list of events, where each event is represented by a dictionary. Each event contains a datetime string (formatted as `%Y-%m-%d %H:%M:%S`), a type, and a value. The types of events are defined by an enumeration. Implement a function that filters and processes these events. # Requirements 1. Define an `Enum` class `EventType` representing the following types: - `ALPHA` - `BETA` - `GAMMA` - `DELTA` 2. Implement a function `process_events(events: List[Dict[str, Union[str, int]]], start_time: str, end_time: str) -> Dict[str, int]`: - Parameters: - `events`: A list of dictionaries, where each dictionary contains: - `datetime`: a string representing the date and time of the event (`format: \'%Y-%m-%d %H:%M:%S\'`) - `type`: a string that is one of the EventType (e.g., \'ALPHA\', \'BETA\', etc.) - `value`: an integer value associated with the event. - `start_time`: a string representing the start datetime (inclusive) (`format: \'%Y-%m-%d %H:%M:%S\'`) - `end_time`: a string representing the end datetime (inclusive) (`format: \'%Y-%m-%d %H:%M:%S\'`) - Returns: - A dictionary where the keys are the event types (`ALPHA`, `BETA`, `GAMMA`, `DELTA`) and the values are the total sum of `value` fields for the events of that type within the specified time range `[start_time, end_time]`. # Example ```python from typing import List, Dict, Union from datetime import datetime from enum import Enum class EventType(Enum): ALPHA = \'ALPHA\' BETA = \'BETA\' GAMMA = \'GAMMA\' DELTA = \'DELTA\' def process_events(events: List[Dict[str, Union[str, int]]], start_time: str, end_time: str) -> Dict[str, int]: # Your implementation here events = [ {\\"datetime\\": \\"2023-10-01 13:00:00\\", \\"type\\": \\"ALPHA\\", \\"value\\": 10}, {\\"datetime\\": \\"2023-10-01 14:00:00\\", \\"type\\": \\"BETA\\", \\"value\\": 20}, {\\"datetime\\": \\"2023-10-01 15:00:00\\", \\"type\\": \\"ALPHA\\", \\"value\\": 30}, {\\"datetime\\": \\"2023-10-02 13:00:00\\", \\"type\\": \\"GAMMA\\", \\"value\\": 5}, {\\"datetime\\": \\"2023-10-03 13:00:00\\", \\"type\\": \\"DELTA\\", \\"value\\": 15}, ] start_time = \\"2023-10-01 13:00:00\\" end_time = \\"2023-10-02 14:00:00\\" result = process_events(events, start_time, end_time) print(result) # Output: {\'ALPHA\': 40, \'BETA\': 20, \'GAMMA\': 5, \'DELTA\': 0} ``` # Constraints - Ensure that the `start_time` and `end_time` strings are valid datetime strings in the specified format. - Only process events that fall within the `[start_time, end_time]` range inclusively. - For any event types that do not have events in the specified range, ensure that their sum is 0 in the output dictionary. # Notes - The use of Python\'s standard library, especially the `datetime` module and `enum` module, is highly encouraged to handle date-time manipulations and define event types. - Pay attention to edge cases such as overlapping start and end times, or no events within the specified range.","solution":"from typing import List, Dict, Union from datetime import datetime from enum import Enum class EventType(Enum): ALPHA = \'ALPHA\' BETA = \'BETA\' GAMMA = \'GAMMA\' DELTA = \'DELTA\' def process_events(events: List[Dict[str, Union[str, int]]], start_time: str, end_time: str) -> Dict[str, int]: start_dt = datetime.strptime(start_time, \'%Y-%m-%d %H:%M:%S\') end_dt = datetime.strptime(end_time, \'%Y-%m-%d %H:%M:%S\') # Initialize the result dictionary with zeroes for all event types. result = {etype.value: 0 for etype in EventType} for event in events: event_dt = datetime.strptime(event[\'datetime\'], \'%Y-%m-%d %H:%M:%S\') if start_dt <= event_dt <= end_dt: event_type = event[\'type\'] event_value = event[\'value\'] result[event_type] += event_value return result"},{"question":"**Objective:** Write a Python function using scikit-learn that trains and evaluates multiple models on a given dataset using different evaluation metrics. Your task is to demonstrate your understanding of model evaluation techniques in scikit-learn. **Problem Statement:** You are given a dataset in the form of feature vectors `X` and target labels `y`. Implement a function `evaluate_models` that will: 1. Split the dataset into training and testing sets. 2. Train the following models provided in the list `models`: - Logistic Regression - Support Vector Classifier (SVC) - Random Forest Classifier 3. Evaluate each model using the following metrics: - Accuracy - Precision - Recall - F1 Score - ROC AUC 4. Return a dictionary with the model names as keys and their corresponding evaluation results as values. The evaluation results should be another dictionary mapping the metric names to their computed values. **Function Signature:** ```python from typing import List, Dict, Any from sklearn.base import ClassifierMixin def evaluate_models(models: List[ClassifierMixin], X: Any, y: Any) -> Dict[str, Dict[str, float]]: pass ``` **Input:** - `models`: A list of initialized scikit-learn classifier instances. - `X`: Features from the dataset (2D array-like). - `y`: Target labels from the dataset (1D array-like). **Output:** - A dictionary where keys are the model names and values are dictionaries of evaluation metrics. **Constraints:** - Use an 80-20 train-test split. - Assume the given classifiers support the `fit` and `predict` methods. - Calculate metrics using the test set. **Example:** ```python from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import load_iris data = load_iris() X, y = data.data, data.target models = [ LogisticRegression(max_iter=200), SVC(probability=True), RandomForestClassifier() ] results = evaluate_models(models, X, y) for model_name, metrics in results.items(): print(f\\"Model: {model_name}\\") for metric, value in metrics.items(): print(f\\" - {metric}: {value:.4f}\\") ``` **Note:** In your implementation, ensure to use appropriate scikit-learn functions for calculating the required metrics. Handle any necessary data preprocessing steps such as scaling/normalizing if needed.","solution":"from typing import List, Dict, Any from sklearn.base import ClassifierMixin from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score from sklearn.preprocessing import LabelBinarizer def evaluate_models(models: List[ClassifierMixin], X: Any, y: Any) -> Dict[str, Dict[str, float]]: # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize the result dictionary results = {} # Binarize the labels if it\'s a multi-class problem for ROC AUC lb = LabelBinarizer() y_test_binarized = lb.fit_transform(y_test) for model in models: # Model training model.fit(X_train, y_train) y_pred = model.predict(X_test) # Calculate metrics metrics = { \'accuracy\': accuracy_score(y_test, y_pred), \'precision\': precision_score(y_test, y_pred, average=\'weighted\', zero_division=1), \'recall\': recall_score(y_test, y_pred, average=\'weighted\'), \'f1_score\': f1_score(y_test, y_pred, average=\'weighted\'), \'roc_auc\': roc_auc_score(y_test_binarized, model.predict_proba(X_test), average=\'weighted\', multi_class=\'ovo\') } # Store the result results[type(model).__name__] = metrics return results"},{"question":"Objective Extend the functionality of the `ModuleFinder` class to include a method that identifies and prints the number of standard library modules, third-party modules, and custom modules used within a given script. This will help in understanding the composition of dependencies in the script. Task Implement a method `analyze_module_types` within a subclass of `ModuleFinder` that will: 1. Identify and count the number of standard library modules, third-party modules, and custom modules imported by the given script. 2. Print a report summarizing these counts. Description 1. **Standard Library Modules**: Modules that are part of the Python standard library. 2. **Third-Party Modules**: Modules installed via package managers like pip. 3. **Custom Modules**: Modules that are user-defined and are located in the same directory as the script or elsewhere in the project\'s directory structure. Input - A Python script filename as a string (e.g., \'bacon.py\'). Output - A printed report summarizing the number of standard library modules, third-party modules, and custom modules. Constraints - Assume the analysis is performed in an environment where all necessary modules (standard and third-party) are installed. - The directory structure should be analyzed to distinguish between custom and third-party modules. Example Usage ```python from modulefinder import ModuleFinder class CustomModuleFinder(ModuleFinder): def analyze_module_types(self, script_path): self.run_script(script_path) # Implement the logic to classify and count module types std_lib_modules = 0 third_party_modules = 0 custom_modules = 0 for module_name, module in self.modules.items(): if self.is_standard_library(module_name): std_lib_modules += 1 elif self.is_third_party(module_name): third_party_modules += 1 else: custom_modules += 1 print(f\\"Standard Library Modules: {std_lib_modules}\\") print(f\\"Third-Party Modules: {third_party_modules}\\") print(f\\"Custom Modules: {custom_modules}\\") def is_standard_library(self, module_name): # Implement logic to check if a module is a standard library module pass def is_third_party(self, module_name): # Implement logic to check if a module is a third-party module pass # Example usage: finder = CustomModuleFinder() finder.analyze_module_types(\'bacon.py\') ``` Requirements - Implement the `analyze_module_types`, `is_standard_library`, and `is_third_party` methods. - Ensure that the output is clear and correctly reflects the composition of modules. Notes - You may need to create auxiliary functions or use existing Python utilities to differentiate between standard library, third-party, and custom modules. - Focus on creating an efficient and accurate solution.","solution":"import importlib.util import pkgutil import sys from modulefinder import ModuleFinder import os import importlib.util class CustomModuleFinder(ModuleFinder): def analyze_module_types(self, script_path): self.run_script(script_path) std_lib_modules = 0 third_party_modules = 0 custom_modules = 0 for module_name in self.modules.keys(): if self.is_standard_library(module_name): std_lib_modules += 1 elif self.is_third_party(module_name): third_party_modules += 1 else: custom_modules += 1 print(f\\"Standard Library Modules: {std_lib_modules}\\") print(f\\"Third-Party Modules: {third_party_modules}\\") print(f\\"Custom Modules: {custom_modules}\\") def is_standard_library(self, module_name): # Check if the module is part of the standard library if module_name in sys.builtin_module_names: return True module_spec = importlib.util.find_spec(module_name) if module_spec and module_spec.origin: if \'site-packages\' not in module_spec.origin and \'dist-packages\' not in module_spec.origin: return True return False def is_third_party(self, module_name): # Check if the module is a third-party module (installed via pip) module_spec = importlib.util.find_spec(module_name) if module_spec and module_spec.origin: if \'site-packages\' in module_spec.origin or \'dist-packages\' in module_spec.origin: return True return False # Example usage: # finder = CustomModuleFinder() # finder.analyze_module_types(\'your_script.py\')"},{"question":"# Python Coding Assessment: Serializer and Deserializer using Pickle Objective Your task is to write a Python program that demonstrates a deep understanding of object serialization and deserialization using the `pickle` module. Problem Statement Implement a custom serializer and deserializer for a Python class that contains various data types including lists, dictionaries, and custom objects. 1. **Define a Class**: Create a Python class called `Student` with the following properties: - `name` (str) - `age` (int) - `grades` (list of int) - `address` (dict with keys \'street\', \'city\', and \'zipcode\') - `id` (unique identifier, use `uuid.uuid4()` for generating unique identifiers) 2. **Serialization**: Write a function `serialize_student` that takes a `Student` object as input and serializes it to a file using the `pickle` module. - Function signature: `def serialize_student(student: Student, file_path: str) -> None` 3. **Deserialization**: Write a function `deserialize_student` that takes a file path as input and deserializes the file content back into a `Student` object. - Function signature: `def deserialize_student(file_path: str) -> Student` 4. **Custom Reduction**: Implement a custom reduction method for the `Student` class to handle complex data types during pickling. Input and Output Formats - Input: - For Serialization: `Student` object and file path (str). - For Deserialization: file path (str). - Output: - For Serialization: No output (file is written). - For Deserialization: `Student` object. Constraints - Assume file paths are valid and writable/readable. - The `Student` class properties will always follow the specified format. - Handle cases where the file might not exist or is corrupted during deserialization. Example ```python import uuid class Student: def __init__(self, name: str, age: int, grades: list, address: dict): self.name = name self.age = age self.grades = grades self.address = address self.id = uuid.uuid4() def __reduce__(self): return (self.__class__, (self.name, self.age, self.grades, self.address)) # Example Usage student = Student(\\"John Doe\\", 20, [85, 90, 75], {\'street\': \'123 Main St\', \'city\': \'Springfield\', \'zipcode\': \'12345\'}) # Serialization serialize_student(student, \'student.pkl\') # Deserialization deserialized_student = deserialize_student(\'student.pkl\') ``` Implement the required functions and demonstrate your solution by serializing and deserializing a `Student` object.","solution":"import pickle import uuid class Student: def __init__(self, name: str, age: int, grades: list, address: dict): self.name = name self.age = age self.grades = grades self.address = address self.id = uuid.uuid4() def __reduce__(self): return (self.__class__, (self.name, self.age, self.grades, self.address)) def serialize_student(student: Student, file_path: str) -> None: with open(file_path, \'wb\') as file: pickle.dump(student, file) def deserialize_student(file_path: str) -> Student: with open(file_path, \'rb\') as file: student = pickle.load(file) return student"},{"question":"# Advanced Seaborn Theme and Display Configuration **Objective:** Demonstrate proficiency in utilizing Seaborn\'s `Plot.config` interface for theme customization and display configuration within a Jupyter notebook. Task: Write a function `customize_plot_theme_and_display` that does the following: 1. **Theme Customization:** - Sets the background color of the axes to light grey (`#d3d3d3`). - Applies the `darkgrid` style from Seaborn\'s theming functions. 2. **Display Configuration:** - Configures the plot to be displayed as SVG format. - Disables the HiDPI scaling of PNG images. - Sets the scaling factor of the embedded images to 0.8. This demonstrates your understanding of theme and display settings customization. Function Signature: ```python def customize_plot_theme_and_display(): pass ``` Expected Output: After calling the `customize_plot_theme_and_display` function, the subsequent plots should reflect the following configurations: - Axes background color should be light grey. - Plot style should be `darkgrid`. - Plots should be displayed as SVG format. - HiDPI scaling of PNG images should be disabled. - The scaling factor of the embedded images should be 0.8. Example Invocation: ```python customize_plot_theme_and_display() # Create a plot to verify the configuration import seaborn.objects as so import seaborn as sns penguins = sns.load_dataset(\\"penguins\\") p = so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\").scatter() p ``` The displayed plot should be in SVG format, have a light grey background, use the `darkgrid` style, not use HiDPI scaling, and have the image scaling factor of 0.8. **Note:** Make sure to import the necessary modules and datasets as shown in the example.","solution":"import seaborn as sns import matplotlib.pyplot as plt import matplotlib as mpl def customize_plot_theme_and_display(): Customizes the Seaborn plot theme and display settings. # Set the background color of the axes to light grey sns.set_style(\\"darkgrid\\", {\\"axes.facecolor\\": \\"#d3d3d3\\"}) # Configure to use SVG format mpl.rcParams[\'figure.dpi\'] = 100 # Standard DPI, ensuring it doesn\'t use HiDPI scaling mpl.rcParams[\'backend\'] = \'svg\' # Set scaling factor for inline display in notebooks from IPython.display import set_matplotlib_formats set_matplotlib_formats(\'svg\') mpl.rcParams[\'figure.figsize\'] = (8, 5.6) # 0.8 scaling factor of the default size"},{"question":"# Pandas Data Validation and Error Handling You are given a dataset in the form of a pandas DataFrame. The dataset contains information about various products, including their IDs, names, categories, prices, and stock quantities. Some entries in the dataset may contain invalid or missing data. Your task is to write a function `validate_and_clean_data` that takes this DataFrame as input and performs the following operations: 1. **Validation**: - Ensure that the \'product_id\' column contains unique values. If any duplicate IDs are found, raise a `DuplicateLabelError`. - Check for any missing data in the \'product_name\', \'category\', \'price\', and \'stock\' columns. If any missing values are found, raise an `EmptyDataError`. - Ensure that the \'price\' column contains non-negative values. If any negative values are found, raise a `ValueError` with a message indicating the issue. - Ensure that the \'stock\' column contains integer values only. If any non-integer values are found, raise an `InvalidIndexError`. 2. **Cleaning**: - Remove any leading or trailing whitespace from string entries in the \'product_name\' and \'category\' columns. - Convert the \'category\' column to lowercase. 3. **Return**: - Return the cleaned DataFrame if all validations pass successfully. # Function Signature ```python import pandas as pd from pandas.errors import DuplicateLabelError, EmptyDataError, InvalidIndexError def validate_and_clean_data(df: pd.DataFrame) -> pd.DataFrame: pass ``` # Input - A pandas DataFrame `df` with the following columns: - `product_id`: int - `product_name`: str - `category`: str - `price`: float - `stock`: int # Output - A cleaned pandas DataFrame if all validations pass successfully. # Constraints - The DataFrame `df` will have 1000 rows at most. - No column will have null values for the entirety of the column. At most 10% of values in each column may be null. # Example ```python data = { \'product_id\': [1, 2, 2, 4], \'product_name\': [\'Product A\', \'Product B\', \'Product C\', \'Product D\'], \'category\': [\'Category1\', \'Category2\', \'Category3\', \'Category4\'], \'price\': [10.5, 20.0, -5.0, 15.0], \'stock\': [100, 200, 300, \'N/A\'] } df = pd.DataFrame(data) try: cleaned_df = validate_and_clean_data(df) print(cleaned_df) except (DuplicateLabelError, EmptyDataError, ValueError, InvalidIndexError) as e: print(f\\"Validation Error: {e}\\") ``` In the example above: - The first `DuplicateLabelError` is raised due to duplicate `product_id`. - If the duplicate issue is resolved, a `ValueError` would be raised due to negative value in the `price` column. - If the price issue is resolved, an `InvalidIndexError` would be raised due to invalid data type in the `stock` column. - If all issues are resolved, the cleaned DataFrame would be returned.","solution":"import pandas as pd from pandas.errors import DuplicateLabelError, EmptyDataError, InvalidIndexError def validate_and_clean_data(df: pd.DataFrame) -> pd.DataFrame: # Check for duplicate product_id if df[\'product_id\'].duplicated().any(): raise DuplicateLabelError(\\"Duplicate product_id found.\\") # Check for any missing data in \'product_name\', \'category\', \'price\', and \'stock\' if df[[\'product_name\', \'category\', \'price\', \'stock\']].isnull().any().any(): raise EmptyDataError(\\"Missing data found in \'product_name\', \'category\', \'price\', or \'stock\'.\\") # Ensure \'price\' contains non-negative values if (df[\'price\'] < 0).any(): raise ValueError(\\"Negative values found in \'price\' column.\\") # Ensure \'stock\' contains integer values only if not pd.api.types.is_integer_dtype(df[\'stock\']): raise InvalidIndexError(\\"Non-integer values found in \'stock\' column.\\") # Cleaning: Remove leading/trailing whitespace and convert to lowercase df[\'product_name\'] = df[\'product_name\'].str.strip() df[\'category\'] = df[\'category\'].str.strip().str.lower() return df"},{"question":"Exception Handling and Custom Exceptions in Python 3.10 Objective The objective of this exercise is to assess your understanding of Python\'s built-in exceptions, how to handle them effectively, and how to create and use custom exceptions. You will also need to demonstrate the ability to chain exceptions and understand exception context. Problem Statement You are required to implement a function `process_numbers(numbers: List[int]) -> List[int]` that processes a list of integers according to the following rules: 1. If the list contains non-integer elements, raise a custom exception `InvalidElementError`. 2. If the list is empty, raise a custom exception `EmptyListError`. 3. If any integer in the list is negative, raise a custom exception `NegativeNumberError`. 4. If any integer in the list is zero, handle the situation by replacing zero with the string `\\"ZeroDivisionError\\"`. 5. Return a new list where each positive integer is replaced with the result of dividing 100 by that integer. You should also create appropriate custom exceptions and handle exception chaining. Specifically, if an invalid element causes another exception, chain the exceptions appropriately. Exceptions to Create - `InvalidElementError`: Raised when the list contains one or more non-integer elements. - `EmptyListError`: Raised when the list is empty. - `NegativeNumberError`: Raised when the list contains one or more negative integers. Function Signature ```python from typing import List, Union class InvalidElementError(Exception): pass class EmptyListError(Exception): pass class NegativeNumberError(Exception): pass def process_numbers(numbers: List[Union[int]]) -> List[Union[int, str]]: pass ``` Input - A list of integers `numbers`. Output - A list where each integer is processed according to the rules above. Errors and Exceptions - Raise `InvalidElementError` if the input list contains non-integer elements. - Raise `EmptyListError` if the input list is empty. - Raise `NegativeNumberError` if the input list contains negative integers. - Handle `ZeroDivisionError` by replacing zero with the string `\\"ZeroDivisionError\\"`. Constraints - The input list may contain up to 1000 elements. - Elements can be integers or non-integers. Example ```python # Example 1 numbers = [10, 5, -3, \'a\'] # Your function should raise InvalidElementError. # Example 2 numbers = [] # Your function should raise EmptyListError. # Example 3 numbers = [10, 5, -3, 2] # Your function should raise NegativeNumberError. # Example 4 numbers = [10, 5, 0, 2] # Should return: [10.0, 20.0, \\"ZeroDivisionError\\", 50.0] ``` Additional Information Make sure to use appropriate exception handling structures (`try-except`, `raise`, etc.) and demonstrate a clear understanding of exception contexts and chaining.","solution":"from typing import List, Union class InvalidElementError(Exception): pass class EmptyListError(Exception): pass class NegativeNumberError(Exception): pass def process_numbers(numbers: List[Union[int]]) -> List[Union[int, str]]: if not numbers: raise EmptyListError(\\"The input list is empty.\\") processed_list = [] try: for number in numbers: if not isinstance(number, int): raise InvalidElementError(f\\"Invalid element detected: {number}\\") if number < 0: raise NegativeNumberError(f\\"Negative number detected: {number}\\") if number == 0: processed_list.append(\\"ZeroDivisionError\\") else: processed_list.append(100 / number) except InvalidElementError as e: raise InvalidElementError(\\"Invalid elements in the list.\\") from e except NegativeNumberError as e: raise NegativeNumberError(\\"Negative numbers in the list.\\") from e return processed_list"},{"question":"Objective: Demonstrate your understanding and application of the `gc` module for garbage collection in Python. Problem Statement: You are tasked with creating a memory management utility that: 1. Performs automatic garbage collection operations based on custom-defined thresholds. 2. Collects and reports garbage collection statistics. 3. Implements a debugging mechanism to identify and handle uncollectable objects. Requirements: 1. **Function 1: `set_custom_gc_threshold(threshold0, threshold1=None, threshold2=None)`** - **Input:** - `threshold0` (int): The threshold for generation 0. - `threshold1` (int, optional): The threshold for generation 1. Defaults to current value if not provided. - `threshold2` (int, optional): The threshold for generation 2. Defaults to current value if not provided. - **Output:** None This function should set the garbage collection thresholds using the provided values. If `threshold1` or `threshold2` are not provided, use the current thresholds for these generations. 2. **Function 2: `collect_garbage(manual=False, generation=2)`** - **Input:** - `manual` (bool): If `True`, perform manual garbage collection immediately. - `generation` (int, optional): The generation to collect. Defaults to 2. - **Output:** The number of unreachable objects found (int). This function should trigger garbage collection based on the given generation if `manual` is `True`. Otherwise, it should check if collection is enabled and trigger collection automatically if needed. 3. **Function 3: `report_gc_stats()`** - **Output:** A dictionary with statistics about garbage collection, including: - \\"collections\\": The number of collections per generation. - \\"collected\\": The number of collected objects per generation. - \\"uncollectable\\": The number of uncollectable objects per generation. This function should return a summarized report of garbage collection statistics. 4. **Function 4: `debug_uncollectable_objects()`** - **Output:** A list of uncollectable objects. This function should: - Enable debugging using `gc.set_debug(gc.DEBUG_UNCOLLECTABLE)`. - Collect garbage and identify uncollectable objects. - Return the list of uncollectable objects. Constraints: 1. Use appropriate error handling to manage invalid inputs. 2. Ensure functions are well-documented and include necessary comments. 3. Maintain an efficient runtime, especially for manual garbage collection operations. Example Usage: ```python # Set custom GC thresholds set_custom_gc_threshold(700, 10, 10) # Trigger manual collection for generation 1 unreachable_objects = collect_garbage(manual=True, generation=1) print(f\\"Unreachable objects found: {unreachable_objects}\\") # Get GC statistics stats = report_gc_stats() print(f\\"GC Stats: {stats}\\") # Debug uncollectable objects uncollectable_objects = debug_uncollectable_objects() print(f\\"Uncollectable Objects: {uncollectable_objects}\\") ``` You are expected to implement all the functions as described, ensuring they align with the provided requirements and constraints. Submission: Submit your Python code implementation for the functions defined above. Ensure your code is clean, well-documented, and adheres to best practices for handling garbage collection in Python.","solution":"import gc def set_custom_gc_threshold(threshold0, threshold1=None, threshold2=None): Set the garbage collection thresholds. :param threshold0: int, required, the threshold for generation 0 :param threshold1: int, optional, the threshold for generation 1. Defaults to current value if not provided. :param threshold2: int, optional, the threshold for generation 2. Defaults to current value if not provided. :return: None current_thresholds = gc.get_threshold() gc.set_threshold(threshold0, threshold1 if threshold1 is not None else current_thresholds[1], threshold2 if threshold2 is not None else current_thresholds[2]) def collect_garbage(manual=False, generation=2): Trigger garbage collection. :param manual: bool, optional, if True, perform manual garbage collection immediately :param generation: int, optional, the generation to collect. Defaults to 2 :return: int, the number of unreachable objects found if manual: return gc.collect(generation) return 0 def report_gc_stats(): Report garbage collection statistics. :return: dict, a dictionary with statistics about garbage collection gc_stats = { \\"collections\\": gc.get_stats(), \\"collected\\": [gc.get_count()[i] for i in range(3)], \\"uncollectable\\": [len(gc.garbage)] } return gc_stats def debug_uncollectable_objects(): Debug uncollectable objects. :return: list, a list of uncollectable objects gc.set_debug(gc.DEBUG_UNCOLLECTABLE) gc.collect() uncollectable_objects = gc.garbage gc.set_debug(0) # Reset debugging settings return uncollectable_objects"},{"question":"# PyTorch Coding Assessment: Working with Dynamic Shapes Objective: This assessment tests your understanding of dynamic shapes in PyTorch, including how to declare dynamic shapes, manage symbolic sizes, and implement guards for symbolic reasoning. Problem Statement: You are implementing a neural network that processes inputs from various batches with different sizes and sequence lengths. The input tensor represents a batch of sequence data, where the batch size and sequence length can vary. Your task is to keep the computations as dynamic as possible to handle these variations efficiently. 1. **Define a Function:** Implement a function `process_dynamic_shapes` that takes an input tensor `x` and performs operations on it based on its dynamic shapes. The operations include: - Concatenating `x` with another tensor `y`. - Checking the resulting tensor `z`\'s size at dimension 0 (batch size) and returning different outputs based on this size using the guard model. 2. **Mark a Dimension as Dynamic:** Utilize `torch._dynamo.mark_dynamic` to explicitly mark the batch size dimension as dynamic. 3. **Implement Guards:** Ensure that the function adapts the outputs based on the dynamic size of the concatenated tensor using the guard model. Specifications: - **Input**: - `x` : A tensor of shape `(batch_size, sequence_length, features)` - `y` : A tensor to concatenate with `x`, of shape `(batch_size, some_other_dim, features)`. - **Output**: - If the concatenated tensor\'s batch size `z.size(0)` is greater than 2, return `z.mul(2)` - Otherwise, return `z.add(2)` Constraints: - Your implementation should use dynamic shapes. - Ensure efficient handling of symbolic sizes and guards. Example: ```python import torch def process_dynamic_shapes(x, y): # Your implementation here # Example usage x = torch.randn(3, 4, 5) y = torch.randn(3, 2, 5) result = process_dynamic_shapes(x, y) print(result) ``` **Expected Output**: The output will depend on the input dimensions, ensuring dynamic handling of shapes. Notes: - You may assume the input tensors `x` and `y` have compatible dimensions for concatenation. - Focus on using PyTorch\'s dynamic shape features and guarding mechanism as documented. Submission: - Submit the complete implementation of `process_dynamic_shapes` function. Good luck!","solution":"import torch def process_dynamic_shapes(x, y): Processes tensors with dynamic shapes. Args: - x (torch.Tensor): A tensor of shape (batch_size, sequence_length, features). - y (torch.Tensor): A tensor to concatenate with x, of shape (batch_size, some_other_dim, features). Returns: - torch.Tensor: The resulting tensor after applying dynamic shape handling. # Mark the batch dimension as dynamic. torch._dynamo.mark_dynamic(x, 0) # Perform concatenation. z = torch.cat((x, y), dim=1) # Apply guard to check the dynamic size of the batch dimension. if z.size(0) > 2: return z.mul(2) else: return z.add(2)"},{"question":"Coding Assessment Question # Objective Write a function that generates a synthetic dataset using a specified generator from scikit-learn, splits the dataset into training and testing sets, trains a classification model on the training set, and evaluates its performance on the test set. # Function Signature ```python def evaluate_model_using_synthetic_data(generator: str, generator_params: dict, model, test_size: float = 0.2, random_state: int = 0) -> float: Generates a synthetic dataset using the specified generator, trains a classification model, and evaluates its performance. Parameters: - generator (str): The name of the dataset generator function to use. Must be one of [\'make_blobs\', \'make_classification\', \'make_gaussian_quantiles\', \'make_circles\', \'make_moons\']. - generator_params (dict): A dictionary of parameters to pass to the dataset generator function. - model: An instance of a scikit-learn classification model. - test_size (float): The proportion of the dataset to include in the test split. Default is 0.2. - random_state (int): Random seed for reproducibility. Default is 0. Returns: - float: The accuracy of the model on the test set. # Instructions 1. Use the specified `generator` to create the dataset by calling the appropriate function from `sklearn.datasets`. Pass `generator_params` to the function to set its parameters. 2. Split the dataset into training and testing sets using `train_test_split` from `sklearn.model_selection`, with the specified `test_size` and `random_state`. 3. Train the provided `model` on the training set. 4. Evaluate the model\'s accuracy on the test set using `accuracy_score` from `sklearn.metrics`. 5. Return the accuracy score as a float. # Example Usage ```python from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier generator = \'make_classification\' generator_params = { \'n_samples\': 1000, \'n_features\': 20, \'n_informative\': 10, \'n_redundant\': 10, \'n_classes\': 2, \'random_state\': 0 } model = RandomForestClassifier(random_state=0) accuracy = evaluate_model_using_synthetic_data(generator, generator_params, model) print(f\\"Model accuracy: {accuracy:.4f}\\") ``` # Constraints - The `generator` must be one of the specified strings: [\'make_blobs\', \'make_classification\', \'make_gaussian_quantiles\', \'make_circles\', \'make_moons\']. - Ensure that the provided `generator_params` dictionary contains valid parameters for the chosen dataset generator. - Assume that `model` is a valid instance of a scikit-learn classifier. - You are allowed to import any necessary modules from scikit-learn. # Notes - Aim to write clean and efficient code. - Consider edge cases where input parameters might lead to errors, and handle them appropriately.","solution":"from sklearn.datasets import make_blobs, make_classification, make_gaussian_quantiles, make_circles, make_moons from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def evaluate_model_using_synthetic_data(generator: str, generator_params: dict, model, test_size: float = 0.2, random_state: int = 0) -> float: Generates a synthetic dataset using the specified generator, trains a classification model, and evaluates its performance. Parameters: - generator (str): The name of the dataset generator function to use. Must be one of [\'make_blobs\', \'make_classification\', \'make_gaussian_quantiles\', \'make_circles\', \'make_moons\']. - generator_params (dict): A dictionary of parameters to pass to the dataset generator function. - model: An instance of a scikit-learn classification model. - test_size (float): The proportion of the dataset to include in the test split. Default is 0.2. - random_state (int): Random seed for reproducibility. Default is 0. Returns: - float: The accuracy of the model on the test set. generators = { \'make_blobs\': make_blobs, \'make_classification\': make_classification, \'make_gaussian_quantiles\': make_gaussian_quantiles, \'make_circles\': make_circles, \'make_moons\': make_moons } # Check if generator is valid if generator not in generators: raise ValueError(f\\"Generator must be one of {list(generators.keys())}\\") # Create synthetic dataset X, y = generators[generator](**generator_params) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state) # Train the model model.fit(X_train, y_train) # Evaluate the model y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"# Seaborn Coding Assessment Using the `seaborn` library, your task is to generate multiple scatter plots from the `tips` dataset to display various insights. The dataset `tips` contains information about the following columns: - `total_bill`: Total bill amount. - `tip`: Tip amount. - `sex`: Sex of the bill payer. - `smoker`: Whether the bill payer was a smoker or not. - `day`: Day of the week. - `time`: Time of the day (Lunch/Dinner). - `size`: Size of the party. # Requirements: 1. **Simple Scatter Plot:** - Create a scatter plot with `total_bill` on the x-axis and `tip` on the y-axis. 2. **Color Mapping:** - Modify the scatter plot to color-code the points based on the `time` of the day (Lunch/Dinner). 3. **Marker Style:** - Add marker styles to the scatter plot for the `sex` of the bill payer. 4. **Size Mapping:** - Add a variable marker size based on the size of the party (`size`). 5. **Subplots with Facets:** - Generate subplots using seaborn\'s `relplot()` to create scatter plots separated by `day` of the week. - Each subplot will represent the scatter plot between `total_bill` and `tip` with different colors for `time` (Lunch/Dinner) and different marker styles for `smoker` (Yes/No). # Input: - No direct input required from the user. The `tips` dataset should be loaded using `sns.load_dataset`. # Output: - Display the following plots using `matplotlib.pyplot.show()`: - Simple scatter plot. - Scatter plot with color mapping. - Scatter plot with color and marker style. - Scatter plot with color, marker style, and size mapping. - Subplots with facets based on the day of the week. # Example Solution Code: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # 1. Simple Scatter Plot plt.figure(figsize=(7, 5)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.title(\\"Total Bill vs Tip\\") plt.show() # 2. Scatter Plot with Color Mapping plt.figure(figsize=(7, 5)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") plt.title(\\"Total Bill vs Tip by Time of Day\\") plt.show() # 3. Scatter Plot with Color Mapping and Marker Style plt.figure(figsize=(7, 5)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"sex\\") plt.title(\\"Total Bill vs Tip by Time of Day and Sex\\") plt.show() # 4. Scatter Plot with Color, Marker Style, and Size Mapping plt.figure(figsize=(7, 5)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"sex\\", size=\\"size\\") plt.title(\\"Total Bill vs Tip by Time of Day, Sex, and Party Size\\") plt.show() # 5. Subplots with Facets by Day of Week g = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"day\\", hue=\\"time\\", style=\\"smoker\\", kind=\\"scatter\\" ) g.fig.suptitle(\\"Total Bill vs Tip by Day, Time, and Smoker Status\\") plt.show() ``` Ensure your plots are clearly labeled with titles and any necessary legends for clarity.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # 1. Simple Scatter Plot plt.figure(figsize=(7, 5)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.title(\\"Total Bill vs Tip\\") plt.show() # 2. Scatter Plot with Color Mapping plt.figure(figsize=(7, 5)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") plt.title(\\"Total Bill vs Tip by Time of Day\\") plt.show() # 3. Scatter Plot with Color Mapping and Marker Style plt.figure(figsize=(7, 5)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"sex\\") plt.title(\\"Total Bill vs Tip by Time of Day and Sex\\") plt.show() # 4. Scatter Plot with Color, Marker Style, and Size Mapping plt.figure(figsize=(7, 5)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"sex\\", size=\\"size\\") plt.title(\\"Total Bill vs Tip by Time of Day, Sex, and Party Size\\") plt.show() # 5. Subplots with Facets by Day of Week g = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"day\\", hue=\\"time\\", style=\\"smoker\\", kind=\\"scatter\\" ) g.fig.suptitle(\\"Total Bill vs Tip by Day, Time, and Smoker Status\\") plt.show()"},{"question":"**Title: Calendar Event Scheduler** **Problem Description:** You are tasked with implementing a `CalendarEventScheduler` class designed to manage events in a calendar application. The objective is to schedule events, check conflicts, and perform other date-time manipulations efficiently. This task will assess your understanding of the Python `datetime` module, including `datetime`, `date`, `time`, `timedelta`, and `timezone` functionalities. **Requirements:** 1. **Create the `CalendarEventScheduler` class**, which should have the following methods: - `__init__(self)`: Initializes an empty list to store events. - `add_event(self, start: datetime, duration: timedelta, title: str)`: Adds an event to the calendar. Each event is defined by its start `datetime`, `timedelta` duration, and a `title`. If an event conflicts with an existing event (i.e., overlapping times), it should raise a `ValueError` with the message \\"Event conflict\\". - `delete_event(self, title: str)`: Deletes an event by its title. If the event does not exist, raise a `ValueError` with the message \\"Event not found\\". - `get_event(self, title: str) -> dict`: Returns a dictionary with keys: \\"start\\", \\"end\\", and \\"duration\\" for the specified event title. If the event does not exist, raise a `ValueError` with the message \\"Event not found\\". - `list_events(self) -> List[dict]`: Returns a list of all events, where each event is represented as a dictionary with keys: \\"title\\", \\"start\\", \\"end\\", and \\"duration\\". - `events_in_timeframe(self, start: date, end: date) -> List[dict]`: Returns a list of events that fall within the specified start and end dates (inclusive). **Constraints:** - Events are stored as dictionaries with \\"title\\", \\"start\\", \\"end\\", and \\"duration\\" attributes. - Ensure that all date-time comparisons are aware of timezone differences. - You can assume no two events with the same title. **Example:** ```python from datetime import datetime, timedelta, timezone # Initialize the scheduler scheduler = CalendarEventScheduler() # Define some events start1 = datetime(2023, 10, 10, 14, 0, tzinfo=timezone.utc) duration1 = timedelta(hours=2) scheduler.add_event(start1, duration1, \\"Meeting with Team A\\") start2 = datetime(2023, 10, 10, 17, 0, tzinfo=timezone.utc) duration2 = timedelta(hours=1) scheduler.add_event(start2, duration2, \\"Call with Client\\") print(scheduler.get_event(\\"Meeting with Team A\\")) # Output: {\'start\': datetime(2023, 10, 10, 14, 0, tzinfo=datetime.timezone.utc), \'end\': datetime(2023, 10, 10, 16, 0, tzinfo=datetime.timezone.utc), \'duration\': datetime.timedelta(seconds=7200)} print(scheduler.list_events()) # Output: [{\'title\': \'Meeting with Team A\', \'start\': datetime(2023, 10, 10, 14, 0, tzinfo=datetime.timezone.utc), \'end\': datetime(2023, 10, 10, 16, 0, tzinfo=datetime.timezone.utc), \'duration\': datetime.timedelta(seconds=7200)}, {\'title\': \'Call with Client\', \'start\': datetime(2023, 10, 10, 17, 0, tzinfo=datetime.timezone.utc), \'end\': datetime(2023, 10, 10, 18, 0, tzinfo=datetime.timezone.utc), \'duration\': datetime.timedelta(seconds=3600)}] start_date = datetime(2023, 10, 10, tzinfo=timezone.utc).date() end_date = datetime(2023, 10, 10, tzinfo=timezone.utc).date() print(scheduler.events_in_timeframe(start_date, end_date)) # Output: [{\'title\': \'Meeting with Team A\', \'start\': datetime(2023, 10, 10, 14, 0, tzinfo=datetime.timezone.utc), \'end\': datetime(2023, 10, 10, 16, 0, tzinfo=datetime.timezone.utc), \'duration\': datetime.timedelta(seconds=7200)}, {\'title\': \'Call with Client\', \'start\': datetime(2023, 10, 10, 17, 0, tzinfo=datetime.timezone.utc), \'end\': datetime(2023, 10, 10, 18, 0, tzinfo=datetime.timezone.utc), \'duration\': datetime.timedelta(seconds=3600)}] # Deleting an event scheduler.delete_event(\\"Meeting with Team A\\") print(scheduler.list_events()) # Output: [{\'title\': \'Call with Client\', \'start\': datetime(2023, 10, 10, 17, 0, tzinfo=datetime.timezone.utc), \'end\': datetime(2023, 10, 10, 18, 0, tzinfo=datetime.timezone.utc), \'duration\': datetime.timedelta(seconds=3600)}] # Adding an event that conflicts start3 = datetime(2023, 10, 10, 15, 0, tzinfo=timezone.utc) duration3 = timedelta(hours=1) try: scheduler.add_event(start3, duration3, \\"Check-in with HR\\") except ValueError as e: print(e) # Output: Event conflict ``` **Note:** - Make sure to handle time zone information properly. - Include all necessary import statements.","solution":"from datetime import datetime, timedelta, timezone, date class CalendarEventScheduler: def __init__(self): self.events = [] def add_event(self, start: datetime, duration: timedelta, title: str): end = start + duration for event in self.events: if (start < event[\'end\'] and end > event[\'start\']): raise ValueError(\\"Event conflict\\") self.events.append({ \\"title\\": title, \\"start\\": start, \\"end\\": end, \\"duration\\": duration }) def delete_event(self, title: str): for event in self.events: if event[\'title\'] == title: self.events.remove(event) return raise ValueError(\\"Event not found\\") def get_event(self, title: str) -> dict: for event in self.events: if event[\'title\'] == title: return event raise ValueError(\\"Event not found\\") def list_events(self) -> list: return self.events def events_in_timeframe(self, start: date, end: date) -> list: events_in_range = [] for event in self.events: event_start_date = event[\'start\'].date() event_end_date = event[\'end\'].date() if event_start_date <= end and event_end_date >= start: events_in_range.append(event) return events_in_range"},{"question":"# Python Coding Assessment: Advanced Class Design and Customization Implement a Python class `CustomList` that: 1. **Behaves like a List**: This class should behave like a standard Python list. It should support common list operations like indexing, appending, extending, and contain checks. 2. **Supports Special Methods**: - Implement custom `__repr__` to return a string representation that includes the class name and the list\'s contents. - Implement custom `__eq__` method to compare equality with another `CustomList` or a standard list. - Implement custom `__getitem__`, `__setitem__`, and `__delitem__` to manage item retrieval, assignment, and deletion with proper index checking and support for slicing. - Implement `__iter__` and `__len__` to support iteration and length checking. 3. **Custom Attribute Access**: - Override `__getattr__` to dynamically return a special message for undefined attributes. 4. **Inherited and Composable Behavior**: - Inherit from a metaclass `CustomMeta` that tracks the number of instances of the `CustomList` created. - Use a custom descriptor to manage an attribute that stores the sum of all numeric elements in the list. # Implementation Details 1. **Class Definition**: ```python class CustomMeta(type): _instance_count = 0 def __call__(cls, *args, **kwargs): instance = super().__call__(*args, **kwargs) cls._instance_count += 1 return instance @property def instance_count(cls): return cls._instance_count class SumDescriptor: def __get__(self, instance, owner): if instance is None: return self return sum(x for x in instance._data if isinstance(x, (int, float))) class CustomList(metaclass=CustomMeta): _sum = SumDescriptor() def __init__(self, iterable=None): self._data = list(iterable) if iterable is not None else [] def __repr__(self): return f\\"CustomList({self._data})\\" def __eq__(self, other): if isinstance(other, CustomList): return self._data == other._data elif isinstance(other, list): return self._data == other return False def __getitem__(self, index): return self._data[index] def __setitem__(self, index, value): self._data[index] = value def __delitem__(self, index): del self._data[index] def __iter__(self): return iter(self._data) def __len__(self): return len(self._data) def append(self, value): self._data.append(value) def extend(self, iterable): self._data.extend(iterable) def __getattr__(self, name): return f\\"\'CustomList\' object has no attribute \'{name}\' but you\'re being creative!\\" # Example Usage: custom_list = CustomList([1, 2, 3]) print(custom_list) custom_list.append(4) print(custom_list) print(custom_list._sum) print(CustomList.instance_count) # Should print the number of CustomList instances created ``` # Constraints: 1. The class should handle invalid operations (like invalid index) gracefully by raising appropriate exceptions. 2. The class should ensure all list operations maintain the list integrity. 3. The additional attribute (_sum) managed by the descriptor should always reflect the current sum of numeric elements. # Submission: Submit your implementation as a single `.py` file. Ensure to include tests for each of the features mentioned above to demonstrate the class functionality.","solution":"class CustomMeta(type): _instance_count = 0 def __call__(cls, *args, **kwargs): instance = super().__call__(*args, **kwargs) cls._instance_count += 1 return instance @property def instance_count(cls): return cls._instance_count class SumDescriptor: def __get__(self, instance, owner): if instance is None: return self return sum(x for x in instance._data if isinstance(x, (int, float))) class CustomList(metaclass=CustomMeta): _sum = SumDescriptor() def __init__(self, iterable=None): self._data = list(iterable) if iterable is not None else [] def __repr__(self): return f\\"CustomList({self._data})\\" def __eq__(self, other): if isinstance(other, CustomList): return self._data == other._data elif isinstance(other, list): return self._data == other return False def __getitem__(self, index): return self._data[index] def __setitem__(self, index, value): self._data[index] = value def __delitem__(self, index): del self._data[index] def __iter__(self): return iter(self._data) def __len__(self): return len(self._data) def append(self, value): self._data.append(value) def extend(self, iterable): self._data.extend(iterable) def __getattr__(self, name): return f\\"\'CustomList\' object has no attribute \'{name}\' but you\'re being creative!\\""},{"question":"**Security-Aware Function Implementation:** You are required to design a function that securely reads, processes, and outputs data from a file. The function must demonstrate careful handling of security issues as highlighted in the provided documentation. **Function Specification:** - **Function Name:** `secure_file_process` - **Parameters:** - `filename` (str): The name of the file to be read. - **Returns:** - A dictionary containing the processed data. **Requirements:** 1. The function must read a text file, where each line contains a base64 encoded string. Your task is to decode these strings securely. 2. Implement necessary checks to avoid vulnerabilities associated with decoding malformed or potentially harmful base64 data. 3. The decoded data should be stored in a dictionary with the original line numbers as keys and the decoded strings as values. 4. The function should handle any exceptions gracefully and log appropriate warnings or errors without crashing. **Constraints:** 1. You must use the `base64` module for decoding. 2. Ensure that your function has proper error handling to manage: - IOException while opening/reading the file. - Errors in base64 decoding for malformed data. 3. You are not allowed to use `eval()` or any insecure method for error logging. **Example Input:** Consider a file named `data.txt` with the following content: ``` SGVsbG8gd29ybGQh SW5jb3JyZWN0IEJhc2U2NCE= V2VsY29tZSB0byBQeXRob24h ``` **Example Output:** ```python { 1: \'Hello world!\', 2: \'Incorrect Base64!\', 3: \'Welcome to Python!\' } ``` **Note:** - Line 2 will raise an error during decoding. Your function should handle this gracefully, perhaps skipping this line and logging an error. **Performance Requirements:** - Ensure that your function is efficient in terms of both time and space complexity, considering large files (e.g., up to 100MB in size). **Additional Information:** For logging errors, you may use the `logging` module as follows: ```python import logging logging.basicConfig(level=logging.WARNING, format=\'%(asctime)s - %(levelname)s - %(message)s\') def secure_file_process(filename): # Your implementation here ``` Your code will be evaluated on its correctness, efficiency, and adherence to secure coding practices as outlined in the provided documentation.","solution":"import base64 import logging logging.basicConfig(level=logging.WARNING, format=\'%(asctime)s - %(levelname)s - %(message)s\') def secure_file_process(filename): result = {} try: with open(filename, \'r\') as file: lines = file.readlines() for line_number, line in enumerate(lines, start=1): try: decoded_data = base64.b64decode(line.strip()).decode(\'utf-8\') result[line_number] = decoded_data except (base64.binascii.Error, UnicodeDecodeError) as e: logging.warning(f\\"Error decoding line {line_number}: {e}\\") result[line_number] = None except IOError as e: logging.error(f\\"Error reading file {filename}: {e}\\") return result"},{"question":"**Coding Assessment Question** # Objective Write a Python function that reads integers from a text file, calculates their sum, and handles various errors gracefully. # Problem Create a function named `sum_integers_from_file` that takes the file path of a text file as input. This function should: 1. Open the file and read integers line by line. 2. Sum the integers and return the result. 3. Implement exception handling to handle various errors that may occur during file operations: - If the file does not exist, raise a custom exception named `FileNotFoundError`. - If a line in the file cannot be converted to an integer, handle the error and skip that line, printing a warning message. 4. Ensure that the file is closed properly, regardless of whether an error occurs. # Expected Input and Output **Input:** - `file_path` (str): A string representing the path to the input text file. **Output:** - (int): The sum of all integers in the file. # Constraints - The file may contain lines that are not integers. - You must create a custom exception `FileNotFoundError`. # Function Signature ```python def sum_integers_from_file(file_path: str) -> int: ``` # Performance Requirements - The function should efficiently handle large files. - The function should have appropriate time complexity for file reading operations. # Examples 1. **Example 1:** ```python # Content of test_file1.txt: # 1 # 2 # 3 # a # invalid line, should be skipped # 4 result = sum_integers_from_file(\'test_file1.txt\') print(result) # Output: 10 ``` 2. **Example 2:** ```python # Content of test_file2.txt: # 10 # 20 # xyz # invalid line, should be skipped # 30 result = sum_integers_from_file(\'test_file2.txt\') print(result) # Output: 60 ``` # Notes - You may use the built-in `open()` function to read the file. - Ensure that your function handles errors gracefully and provides informative messages where appropriate. - Remember to close the file properly.","solution":"def sum_integers_from_file(file_path: str) -> int: Reads integers from a file and returns their sum. Skips lines that cannot be converted to an integer and handles file not found errors by raising a custom FileNotFoundError. Args: file_path (str): The path to the file containing integers. Returns: int: The sum of all integers in the file. Raises: FileNotFoundError: If the file does not exist. total_sum = 0 try: with open(file_path, \'r\') as file: for line in file: try: total_sum += int(line.strip()) except ValueError: print(f\\"Warning: Could not convert line to an integer: {line.strip()}\\") except IOError: raise FileNotFoundError(f\\"The file at {file_path} was not found.\\") return total_sum class FileNotFoundError(Exception): Custom exception to handle file not found errors. pass"},{"question":"# PyTorch Module Hierarchy Tracking In this assessment, you are required to demonstrate your understanding of the `torch.utils.module_tracker` utility by tracking a custom PyTorch module hierarchy. You will implement a function which utilizes `torch.utils.module_tracker` to traverse and print the hierarchical structure of the given PyTorch model. Requirements: 1. Implement a function `print_module_hierarchy` that takes a PyTorch `nn.Module` as input. 2. The function should traverse the module hierarchy using `torch.utils.module_tracker.ModuleTracker`. 3. Print the full hierarchical structure of the model. 4. The hierarchical structure should be displayed in a human-readable format, showing parent-child relationships with indentation. Function Signature: ```python def print_module_hierarchy(model: torch.nn.Module) -> None: pass ``` Example Usage: ```python import torch import torch.nn as nn import torch.utils.module_tracker as mt class SampleModel(nn.Module): def __init__(self): super(SampleModel, self).__init__() self.layer1 = nn.Sequential( nn.Conv2d(1, 20, 5), nn.ReLU(), ) self.layer2 = nn.Sequential( nn.Conv2d(20, 64, 5), nn.ReLU() ) def forward(self, x): x = self.layer1(x) x = self.layer2(x) return x model = SampleModel() print_module_hierarchy(model) ``` Expected Output: ``` SampleModel( (layer1): Sequential( (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1)) (1): ReLU() ) (layer2): Sequential( (0): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1)) (1): ReLU() ) ) ``` Constraints: - The solution must handle nested modules properly. - Use `torch.utils.module_tracker.ModuleTracker` for tracking the hierarchy internally. - Ensure your function works for any general `nn.Module` and not just the provided example. Notes: - Pay attention to the details of how the hierarchy is printed, including the use of parentheses and indentation. - Ensure that the function\'s output is clear and structured, aiding in debugging and model comprehension.","solution":"import torch import torch.nn as nn def print_module_hierarchy(model: nn.Module) -> None: def print_hierarchy(module, indent=\'\'): for name, child in module.named_children(): print(f\'{indent}({name}): {child.__class__.__name__}(\') print_hierarchy(child, indent + \' \') print(f\'{indent})\') if len(list(module.named_children())) == 0: # At the leaf node, display the module details print(f\'{indent}{module}\') print(f\'{model.__class__.__name__}(\') print_hierarchy(model, \' \') print(\')\') # Example usage class SampleModel(nn.Module): def __init__(self): super(SampleModel, self).__init__() self.layer1 = nn.Sequential( nn.Conv2d(1, 20, 5), nn.ReLU(), ) self.layer2 = nn.Sequential( nn.Conv2d(20, 64, 5), nn.ReLU() ) def forward(self, x): x = self.layer1(x) x = self.layer2(x) return x model = SampleModel() print_module_hierarchy(model)"},{"question":"# Data Reshaping and Analysis with Pandas You are provided with a dataset that contains information about sales transactions from a retail store. Your task is to implement a function that processes this data to provide various insights, requiring the use of different reshaping techniques in pandas. Dataset The dataset is a CSV file named `sales_data.csv` with the following columns: - `TransactionID`: Unique identifier for each transaction. - `Date`: The date of the transaction. - `StoreID`: Identifier of the store where the transaction took place. - `ProductID`: Identifier of the product sold. - `Category`: Category of the product. - `Quantity`: Number of units sold. - `Price`: Price per unit of the product. Function Signature ```python import pandas as pd def analyze_sales_data(file_path: str) -> dict: Analyzes sales data from a CSV file to provide various insights using pandas. Arguments: file_path: str - Path to the CSV file containing sales data. Returns: dict - A dictionary containing the following keys and their corresponding values: - \'pivoted_data\': DataFrame pivoted with Date as index, StoreID as columns, and total sales value (Quantity * Price) as values. - \'category_sales\': DataFrame showing total sales quantity for each product category using pivot_table. - \'most_sold_product\': A tuple (ProductID, total_quantity) for the product that was sold the most. - \'exploded_purchase_dates\': DataFrame with each Date expanded into individual years and months using exploded views. pass ``` Requirements 1. **Pivot Data**: Create a pivot table where: - The `index` is `Date`. - The `columns` are `StoreID`. - The `values` are the total sales value (Quantity * Price) for each transaction. 2. **Category Sales Summary**: Use `pivot_table` to summarize total sales quantity for each product category. - The `index` should be `Category`. - The value should be the total `Quantity`. 3. **Most Sold Product**: Identify the product that was sold the most across all transactions. - Return a tuple with `ProductID` and `total_quantity`. 4. **Exploded Purchase Dates**: Convert the `Date` column into separate year and month columns and then `explode` the DataFrame to separate rows for year and month. - Return the resulting DataFrame. Constraints and Notes 1. You should use pandas methods to achieve the transformations. 2. Helper functions can be defined if needed. 3. Ensure that the final output is returned as a dictionary with the specified keys. Example Consider the following as an example to help visualize the task (assuming `sales_data.csv` represents the data described): ```python { \'pivoted_data\': <DataFrame showing pivoted data>, \'category_sales\': <DataFrame summarizing category sales>, \'most_sold_product\': (\'ProductID_123\', 450), \'exploded_purchase_dates\': <DataFrame with exploded purchase dates> } ``` Implement the function `analyze_sales_data` to achieve the desired transformations and return the required dictionary of results.","solution":"import pandas as pd def analyze_sales_data(file_path: str) -> dict: Analyzes sales data from a CSV file to provide various insights using pandas. Arguments: file_path: str - Path to the CSV file containing sales data. Returns: dict - A dictionary containing the following keys and their corresponding values: - \'pivoted_data\': DataFrame pivoted with Date as index, StoreID as columns, and total sales value (Quantity * Price) as values. - \'category_sales\': DataFrame showing total sales quantity for each product category using pivot_table. - \'most_sold_product\': A tuple (ProductID, total_quantity) for the product that was sold the most. - \'exploded_purchase_dates\': DataFrame with the Date column exploded into individual years and months using an exploded view. # Load the data df = pd.read_csv(file_path) # Ensure \'Date\' is a datetime type df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Compute total sales value for each transaction df[\'SalesValue\'] = df[\'Quantity\'] * df[\'Price\'] # Pivoted Data pivoted_data = df.pivot_table(index=\'Date\', columns=\'StoreID\', values=\'SalesValue\', aggfunc=\'sum\').fillna(0) # Category Sales Summary category_sales = df.pivot_table(index=\'Category\', values=\'Quantity\', aggfunc=\'sum\') # Most Sold Product most_sold_product_df = df.groupby(\'ProductID\')[\'Quantity\'].sum().reset_index() most_sold_product = most_sold_product_df.loc[most_sold_product_df[\'Quantity\'].idxmax()] most_sold_product = (most_sold_product[\'ProductID\'], most_sold_product[\'Quantity\']) # Exploded Purchase Dates df[\'Year\'] = df[\'Date\'].dt.year df[\'Month\'] = df[\'Date\'].dt.month exploded_purchase_dates = df.melt(id_vars=[\'TransactionID\', \'Date\', \'StoreID\', \'ProductID\', \'Category\', \'Quantity\', \'Price\'], value_vars=[\'Year\', \'Month\'], var_name=\'DateComponent\', value_name=\'ComponentValue\') return { \'pivoted_data\': pivoted_data, \'category_sales\': category_sales, \'most_sold_product\': most_sold_product, \'exploded_purchase_dates\': exploded_purchase_dates }"},{"question":"In this assessment, you are required to load a dataset, preprocess it, and train a machine learning model using scikit-learn. Follow the instructions below to complete the task. Objective - Load the \\"mice protein\\" dataset from openml.org. - Preprocess the data by converting categorical data to numerical values. - Train a Random Forest classifier using the preprocessed data. - Evaluate the classifier\'s performance using accuracy. Steps to Follow 1. **Load Dataset:** Using the `fetch_openml` function from scikit-learn, load the \\"mice protein\\" dataset. Note that the dataset contains 1080 examples with 77 features and 8 different classes. 2. **Preprocess the Data:** - Convert any categorical features present in the dataset to numerical values. - Handle any missing values if present (you may fill them with the mean of the column). 3. **Train a Model:** - Split the dataset into training and testing sets using an 80-20 split. - Train a `RandomForestClassifier` on the training set. 4. **Evaluate the Model:** - Evaluate the classifier\'s performance on the test set using accuracy as the metric. Function Signature ```python import numpy as np from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score def load_preprocess_train(): This function loads the Mice Protein dataset, preprocesses it, trains a Random Forest classifier, and evaluates its performance. Returns: float: The accuracy of the Random Forest classifier on the test set. # Load the dataset mice = fetch_openml(name=\'miceprotein\', version=4) # Separate features and target X = mice.data y = mice.target # Preprocess the dataset (convert categorical data to numerical, handle missing values) # Assuming all features are numeric, but if there are categorical features, handle them accordingly # For simplicity, check if there are any missing values and fill them with mean of the column X = X.fillna(X.mean()) # Fill missing values with column mean # Split the dataset into training and testing sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train a Random Forest Classifier clf = RandomForestClassifier(n_estimators=100, random_state=42) clf.fit(X_train, y_train) # Predict and evaluate the model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # Return the accuracy return accuracy ``` Constraints - Ensure that the dataset is properly preprocessed before training the model. - Use appropriate methods from scikit-learn for preprocessing, training, and evaluation. - Do not change the function signature. Expected Output - The function should return the accuracy of the Random Forest classifier on the test set. Example ```python accuracy = load_preprocess_train() print(f\\"Accuracy: {accuracy:.2f}\\") ```","solution":"import numpy as np from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score def load_preprocess_train(): This function loads the Mice Protein dataset, preprocesses it, trains a Random Forest classifier, and evaluates its performance. Returns: float: The accuracy of the Random Forest classifier on the test set. # Load the dataset data = fetch_openml(name=\'miceprotein\', version=4, as_frame=True) # Separate features and target X = data.data y = data.target # Preprocess the dataset (convert categorical data to numerical, handle missing values) # Assuming all features are numeric, but if there are categorical features, handle them accordingly # For simplicity, check if there are any missing values and fill them with mean of the column X = X.fillna(X.mean()) # Fill missing values with column mean # Split the dataset into training and testing sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train a Random Forest Classifier clf = RandomForestClassifier(n_estimators=100, random_state=42) clf.fit(X_train, y_train) # Predict and evaluate the model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # Return the accuracy return accuracy"},{"question":"You are required to implement a buffer-based class that supports multi-dimensional arrays. Specifically, you need to create a class `CustomBuffer` which uses the buffer protocol to provide direct access to its underlying data storage. This class will simulate a 2-dimensional array of integers, allowing read and write operations directly on the buffer. # Class Specification: 1. **Class Name**: `CustomBuffer` 2. **Attributes**: - `buffer`: A 1-dimensional list of integers representing a flattened 2D array. - `rows`: Number of rows in the 2D array. - `cols`: Number of columns in the 2D array. 3. **Methods**: - `__init__(self, rows: int, cols: int)`: Constructor that initializes the buffer with zeros, and sets the dimensions. - `get_item(self, row: int, col: int) -> int`: Method to retrieve the value at a specific row and column in the 2D array. - `set_item(self, row: int, col: int, value: int) -> None`: Method to set the value at a specific row and column in the 2D array. - `fill_buffer(self, values: List[int]) -> None`: Method to fill the buffer with a list of values. - `to_contiguous(self) -> memoryview`: Method to return a contiguous memoryview of the buffer. - `release_buffer(self, view: memoryview) -> None`: Method to release a previously acquired memoryview buffer. # Constraints: - All indices are 0-based. - Assume the given `values` for filling the buffer will always match the required size `(rows * cols)`. - Input dimensions for rows and columns will always be positive integers. # Example Usage: ```python # Initialize a 3x3 buffer buf = CustomBuffer(3, 3) # Fill with values from 0 to 8 buf.fill_buffer([0, 1, 2, 3, 4, 5, 6, 7, 8]) # Get item at position (1, 1) (which is 4 in this case) print(buf.get_item(1, 1)) # Output: 4 # Set item at position (2, 2) to 99 buf.set_item(2, 2, 99) print(buf.get_item(2, 2)) # Output: 99 # Get a contiguous memoryview view = buf.to_contiguous() print(view.tolist()) # Output: [0, 1, 2, 3, 4, 5, 6, 7, 99] # Release the memoryview buf.release_buffer(view) ``` # Implementation Requirements: Implement the `CustomBuffer` class with the specified methods and behavior using the buffer protocol concept described.","solution":"class CustomBuffer: def __init__(self, rows: int, cols: int): Initializes the buffer with zeros and sets the dimensions. self.rows = rows self.cols = cols self.buffer = [0] * (rows * cols) def get_item(self, row: int, col: int) -> int: Retrieves the value at the specified row and column. index = row * self.cols + col return self.buffer[index] def set_item(self, row: int, col: int, value: int) -> None: Sets the value at the specified row and column. index = row * self.cols + col self.buffer[index] = value def fill_buffer(self, values: list[int]) -> None: Fills the buffer with the provided list of values. if len(values) != self.rows * self.cols: raise ValueError(\\"Provided values do not match buffer size.\\") self.buffer = values[:] def to_contiguous(self) -> memoryview: Returns a contiguous memoryview of the buffer. return memoryview(bytearray(self.buffer)) def release_buffer(self, view: memoryview) -> None: Releases a previously acquired memoryview buffer. view.release()"},{"question":"**Objective**: Implement a custom entry point for a pre-trained model and demonstrate how to load and use the model through PyTorch Hub. **Question**: You are tasked with setting up a custom entry point for a pre-trained model in PyTorch Hub. Your goal is to publish a simple neural network that can be loaded and used by others through PyTorch Hub. Follow the steps below to complete the task: 1. **Define a Simple Model**: Implement a simple neural network using PyTorch\'s `torch.nn` module. The model should have the following structure: - Input layer: Takes an input of size 10. - Hidden layer: Fully connected layer with 5 neurons and ReLU activation. - Output layer: Fully connected layer with 1 neuron. 2. **Create the Entrypoint**: - Write a function named `simple_net` that instantiates this model. - In the function, add a parameter `pretrained` (defaulted to `False`) to optionally load pretrained weights. - If `pretrained` is `True`, load the weights from a given URL (`https://example.com/pretrained_weights.pth`). (Note: For the purpose of this question, assume this URL returns valid weights.) 3. **Publish to Hub**: - Create a `hubconf.py` file containing the necessary entry points and dependencies. - List \'torch\' in the `dependencies` list. 4. **Loading and Testing**: - Demonstrate how to load the model using `torch.hub.load()` function. - Run a simple forward pass using this model. Provide your implementation for the model, entry point, and the code to load and use the model. **Expected Input and Output**: - The input to the model will be a tensor of shape `(batch_size, 10)`. - The output of the model will be a tensor of shape `(batch_size, 1)`. **Constraints**: - Use PyTorch version `1.8.0` or later. **Sample Code Structure**: ```python # Define the simple neural network model import torch.nn as nn class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 5) self.relu = nn.ReLU() self.fc2 = nn.Linear(5, 1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Entry point function dependencies = [\'torch\'] def simple_net(pretrained=False): model = SimpleNet() if pretrained: # Load the pretrained weights from the given URL url = \'https://example.com/pretrained_weights.pth\' state_dict = torch.hub.load_state_dict_from_url(url, progress=True) model.load_state_dict(state_dict) return model # Code to load and use the model if __name__ == \\"__main__\\": # Load the model from torch.hub model = torch.hub.load(\'your_github_username/your_repo\', \'simple_net\', pretrained=False) # Create a random input tensor input_tensor = torch.randn(1, 10) # Run a forward pass output = model(input_tensor) print(output) ``` This problem will test your understanding of PyTorch Hub\'s functionality, including defining and publishing entry points, handling dependencies, loading pretrained weights, and interacting with models.","solution":"import torch import torch.nn as nn import torch.hub class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 5) self.relu = nn.ReLU() self.fc2 = nn.Linear(5, 1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x dependencies = [\'torch\'] def simple_net(pretrained=False): model = SimpleNet() if pretrained: # Load the pretrained weights from the given URL url = \'https://example.com/pretrained_weights.pth\' state_dict = torch.hub.load_state_dict_from_url(url, map_location=\'cpu\') model.load_state_dict(state_dict) return model"},{"question":"**Objective**: Demonstrate your understanding of the `sklearn.datasets` module by loading different types of datasets and performing basic operations on them. **Problem Statement**: 1. **Load and Inspect a Toy Dataset**: - Use the `load_iris` function from `sklearn.datasets` to load the Iris dataset. - Print the dataset description. - Extract and print the feature names and target names from the Bunch object. 2. **Fetch and Analyze a Real-World Dataset**: - Use the `fetch_20newsgroups` function from `sklearn.datasets` to fetch the 20 newsgroups text dataset. - Fetch only newsgroups related to [\'alt.atheism\', \'sci.space\']. - Print the total number of articles fetched and the name of the categories. 3. **Generate and Visualize Synthetic Data**: - Use the `make_classification` function from `sklearn.datasets` to generate a synthetic dataset with 1000 samples, 20 features, and 2 informative features. - Split the synthetic dataset into training and test sets. - Visualize the first two features of the synthetic data using a scatter plot. **Constraints and Requirements**: - You are required to use `sklearn.datasets` for all dataset operations. - Visualizations should be clear and properly labeled. - Ensure that your code is well-documented and modular. **Input and Output Formats**: - There are no specific inputs/output formats for this question. The described operations and visualizations are the expected outputs. **Performance Requirements**: - Loading operations should be efficient, with code executing within a reasonable time for the prescribed datasets. ```python import sklearn.datasets as datasets import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split def load_and_inspect_toy_dataset(): # Load the iris dataset iris = datasets.load_iris() # Print dataset description print(iris.DESCR) # Print feature names and target names print(\\"Feature names:\\", iris.feature_names) print(\\"Target names:\\", iris.target_names) def fetch_and_analyze_real_world_dataset(): # Fetch the 20 newsgroups dataset for specified categories newsgroups = datasets.fetch_20newsgroups(categories=[\'alt.atheism\', \'sci.space\']) # Print total number of articles and name of the categories print(\\"Total number of articles fetched:\\", len(newsgroups.data)) print(\\"Categories fetched:\\", newsgroups.target_names) def generate_and_visualize_synthetic_data(): # Generate synthetic classification data X, y = datasets.make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, random_state=42) # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Visualize the first two features of the synthetic data plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=\'viridis\', edgecolor=\'k\', s=20) plt.title(\'Synthetic Data Visualization\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.show() # Example execution of the functions load_and_inspect_toy_dataset() fetch_and_analyze_real_world_dataset() generate_and_visualize_synthetic_data() ```","solution":"import sklearn.datasets as datasets import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split def load_and_inspect_toy_dataset(): # Load the iris dataset iris = datasets.load_iris() # Print dataset description print(iris.DESCR) # Print feature names and target names return iris.feature_names, iris.target_names def fetch_and_analyze_real_world_dataset(): # Fetch the 20 newsgroups dataset for specified categories newsgroups = datasets.fetch_20newsgroups(categories=[\'alt.atheism\', \'sci.space\']) # Print total number of articles and name of the categories return len(newsgroups.data), newsgroups.target_names def generate_and_visualize_synthetic_data(): # Generate synthetic classification data X, y = datasets.make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, random_state=42) # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Visualize the first two features of the synthetic data plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=\'viridis\', edgecolor=\'k\', s=20) plt.title(\'Synthetic Data Visualization\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.show() # Example execution of the functions feature_names, target_names = load_and_inspect_toy_dataset() num_articles, categories = fetch_and_analyze_real_world_dataset() generate_and_visualize_synthetic_data()"},{"question":"# The `sitecustomizer` Script **Objective:** Write a Python script called `sitecustomizer.py` that modifies the Python environment by adding custom paths to `sys.path` and manages site-specific or user-specific customizations. # Task: 1. **Implement a function `add_custom_path(path: str) -> None`**: - This function should add a given path to the `sys.path` if it is not already present and if the path exists. 2. **Implement a function `setup_customizations(customize_file: str) -> None`**: - This function should attempt to import a given module specified by `customize_file` to perform customizations. If the module does not exist, handle the `ImportError` gracefully. # Constraints: - You should not use the `site` module directly, but mimic its behavior. - Assume paths are Unix-style for simplicity (/usr/local/...). - The function `add_custom_path` should not add paths that are already in `sys.path` or do not exist. - Ensure that the attempted import of `customize_file` is done safely and non-destructively. # Input and Output Formats: - The `add_custom_path` function takes a single argument `path` (string) and returns `None`. - The `setup_customizations` function takes a single argument `customize_file` (string) and returns `None`. # Example Usage: The following example demonstrates how `sitecustomizer.py` should work: ```python import os import sys # Example implementation of add_custom_path def add_custom_path(path): if path not in sys.path and os.path.exists(path): sys.path.insert(0, path) # Example implementation of setup_customizations def setup_customizations(customize_file): try: __import__(customize_file) except ImportError as e: if e.name != customize_file: raise # Example paths add_custom_path(\'/usr/local/lib/mycustompath\') setup_customizations(\'mycustommodule\') ``` # Testing: - Ensure that the path is added only if it exists. - Ensure that importing a nonexistent module does not raise unexpected errors. - You can create dummy custom paths and modules to verify the proper functioning of your script.","solution":"import os import sys def add_custom_path(path): Adds a given path to the sys.path if it is not already present and if the path exists. if path not in sys.path and os.path.exists(path): sys.path.insert(0, path) def setup_customizations(customize_file): Attempts to import a given module specified by customize_file to perform customizations. If the module does not exist, handle the ImportError gracefully. try: __import__(customize_file) except ImportError as e: if e.name != customize_file: raise"},{"question":"Objective: Create a Python program that demonstrates a deep understanding of the `dataclasses` module, including the use of decorators, default values, post-init processing, and converting dataclass instances to dictionaries and tuples. Problem Statement: You are required to implement a system to manage a librarys inventory using the `dataclasses` module. Specifically, you will create dataclasses to represent `Book`, `Author`, and `Library` entities. Your task involves defining the dataclasses with appropriate fields, implementing methods to calculate total values and convert instances to dictionaries and tuples. Instructions: 1. Define the `Author` dataclass: - Fields: - `name` (str): The name of the author. - `birth_year` (int): The birth year of the author. 2. Define the `Book` dataclass: - Fields: - `title` (str): The title of the book. - `author` (Author): An instance of the `Author` dataclass. - `num_pages` (int): The number of pages in the book. - `price` (float): The price of the book. - `quantity` (int): The number of copies available in the library inventory. - Methods: - `total_value(self) -> float`: A method that returns the total value of the book in inventory (price times quantity). 3. Define the `Library` dataclass: - Fields: - `name` (str): The name of the library. - `books` (List[Book]): A list of `Book` instances in the library inventory. - Methods: - `total_inventory_value(self) -> float`: A method that returns the total value of all books in the library. - `book_titles(self) -> List[str]`: A method that returns a list of all book titles in the library. 4. Implement the following functionalities: - `all_books_as_dict(library: Library) -> dict`: A function that converts the library instance and all its books to a dictionary using `dataclasses.asdict`. - `all_books_as_tuple(library: Library) -> tuple`: A function that converts the library instance and all its books to a tuple using `dataclasses.astuple`. Requirements: - Use the `@dataclass` decorator for defining dataclasses. - Use `field` from the `dataclasses` module to define default values wherever appropriate. - Demonstrate the use of post-initialization (`__post_init__`) for any additional setup if necessary. - Ensure proper conversion methods using `dataclasses.asdict` and `dataclasses.astuple`. Sample Output: ```python from dataclasses import dataclass, field, asdict, astuple from typing import List @dataclass class Author: name: str birth_year: int @dataclass class Book: title: str author: Author num_pages: int price: float quantity: int = 1 def total_value(self) -> float: return self.price * self.quantity @dataclass class Library: name: str books: List[Book] def total_inventory_value(self) -> float: return sum(book.total_value() for book in self.books) def book_titles(self) -> List[str]: return [book.title for book in self.books] def all_books_as_dict(library: Library) -> dict: return asdict(library) def all_books_as_tuple(library: Library) -> tuple: return astuple(library) # Test the implementation author = Author(name=\\"J.K. Rowling\\", birth_year=1965) book1 = Book(title=\\"Harry Potter and the Philosopher\'s Stone\\", author=author, num_pages=223, price=20.00, quantity=3) book2 = Book(title=\\"Harry Potter and the Chamber of Secrets\\", author=author, num_pages=251, price=22.00) library = Library(name=\\"Hogwarts Library\\", books=[book1, book2]) print(library.total_inventory_value()) print(library.book_titles()) print(all_books_as_dict(library)) print(all_books_as_tuple(library)) ``` This code should provide outputs aligning with the expected functionality, demonstrating their understanding of advanced features within the `dataclasses` module.","solution":"from dataclasses import dataclass, field, asdict, astuple from typing import List @dataclass class Author: name: str birth_year: int @dataclass class Book: title: str author: Author num_pages: int price: float quantity: int = 1 def total_value(self) -> float: return self.price * self.quantity @dataclass class Library: name: str books: List[Book] def total_inventory_value(self) -> float: return sum(book.total_value() for book in self.books) def book_titles(self) -> List[str]: return [book.title for book in self.books] def all_books_as_dict(library: Library) -> dict: return asdict(library) def all_books_as_tuple(library: Library) -> tuple: return astuple(library)"},{"question":"# Advanced Python Coding Assessment **Objective:** You are required to implement a custom traceback handler that mimics some functionalities of the deprecated `cgitb` module. This will assess your understanding of Python\'s exception handling, traceback management, and string formatting. **Instructions:** 1. Write a Python function `detailed_traceback(info, context_lines=5, output_format=\'text\', logdir=None)` which: - Takes a 3-tuple `info` containing the result from `sys.exc_info()`. - Takes an integer `context_lines` that specifies the number of lines of context to display around the current line of source code in the traceback. Defaults to 5. - Takes a string `output_format` specifying the output format. Can either be `\'text\'` or `\'html\'`. Defaults to \'text\'. - Optionally takes a string `logdir` which is the directory to save the traceback report. If `logdir` is None, do not save the report to a file. 2. The function should return the formatted traceback as a string according to the specified `output_format` and context lines. 3. If `output_format` is `\'html\'`, the traceback should be formatted in simple HTML. If `output_format` is anything other than `\'html\'`, plaintext formatting should be used. 4. If `logdir` is provided, save the traceback report to a new file in the specified directory. The filename should be of the format `traceback_<timestamp>.log`, where `<timestamp>` is the current timestamp. **Constraints:** - Do not use the `cgitb` module. - You may use modules such as `traceback`, `sys`, `os`, and `datetime`. **Example Usage:** ```python import sys import os def buggy_function(): raise ValueError(\\"This is a test exception\\") try: buggy_function() except: info = sys.exc_info() # Example of using the detailed_traceback function result = detailed_traceback(info, context_lines=3, output_format=\'html\', logdir=os.getcwd()) print(result) ``` **Expected Output:** A detailed traceback report in HTML format including 3 lines of context for each frame. If `logdir` is specified, the report should also be saved in the current working directory. **Note:** - Make sure the HTML report is properly formatted and includes essential details like exception type, message, and the formatted traceback with context lines. - In the plaintext format, ensure the output is readable and well-structured. Implement the function `detailed_traceback` below: ```python import traceback import datetime import sys import os def detailed_traceback(info, context_lines=5, output_format=\'text\', logdir=None): # Your implementation here pass ```","solution":"import traceback import datetime import sys import os def detailed_traceback(info, context_lines=5, output_format=\'text\', logdir=None): exc_type, exc_value, exc_tb = info tb_lines = traceback.format_tb(exc_tb, limit=None) context_str = \\"\\" for tb in tb_lines: tb_parts = tb.split(\\"n\\") for line in tb_parts: if line.strip().startswith(\'File\'): filename = line.split(\'\\"\')[1] lineno = int(line.split(\\", line \\")[1].split(\\",\\")[0].strip()) context_str += format_context(filename, lineno, context_lines) formatted_traceback = f\\"Traceback (most recent call last):n{context_str}{\'\'.join(tb_lines)}{exc_type.__name__}: {str(exc_value)}n\\" if output_format == \'html\': formatted_traceback = f\\"<html><body><pre>{formatted_traceback}</pre></body></html>\\" if logdir is not None: timestamp = datetime.datetime.now().strftime(\\"%Y%m%d_%H%M%S\\") log_filename = os.path.join(logdir, f\\"traceback_{timestamp}.log\\") with open(log_filename, \'w\') as f: f.write(formatted_traceback) return formatted_traceback def format_context(filename, lineno, context_lines): Fetch the context lines around the lineno in the specified file. start = max(1, lineno - context_lines) end = lineno + context_lines lines = [] with open(filename, \'r\') as f: all_lines = f.readlines() for i in range(start, end + 1): if 0 <= i-1 < len(all_lines): lines.append(f\\"{i}: {all_lines[i-1]}\\") return \'\'.join(lines)"},{"question":"**Question: Implement Custom Copy Methods** In this task, you will implement custom class methods for shallow and deep copying. Consider a `TreeNode` class that represents nodes in a binary tree. Each `TreeNode` can have left and right children, which are also `TreeNode` objects. You need to implement shallow and deep copy methods for this class. # Class Definition ```python class TreeNode: def __init__(self, value, left=None, right=None): self.value = value self.left = left self.right = right ``` # Requirements: 1. Implement the `__copy__` method to create a shallow copy of a `TreeNode` instance. 2. Implement the `__deepcopy__` method to create a deep copy of a `TreeNode` instance using the `copy.deepcopy()` function. 3. Both methods should handle trees with nested `TreeNode` objects correctly. # Input and Output: - The input is not directly provided as a parameter. Instead, you will create instances of the `TreeNode` class and use the built-in `copy` module to test your methods. - The output from copying should be new instances of `TreeNode` objects that meet the criteria of shallow and deep copying. # Example: ```python from copy import copy, deepcopy if __name__ == \\"__main__\\": # Creating an example tree node3 = TreeNode(3) node4 = TreeNode(4) node2 = TreeNode(2, node3, node4) node1 = TreeNode(1, node2, None) # Shallow copy shallow_copied_node1 = copy(node1) print(shallow_copied_node1 is not node1) # Should be True print(shallow_copied_node1.left is node1.left) # Should be True print(shallow_copied_node1.left.left is node1.left.left) # Should be True # Deep copy deep_copied_node1 = deepcopy(node1) print(deep_copied_node1 is not node1) # Should be True print(deep_copied_node1.left is node1.left) # Should be False print(deep_copied_node1.left.left is node1.left.left) # Should be False ``` # Constraints: - Assume that the tree does not contain cycles (no node can be an ancestor of itself). - The value stored in each `TreeNode` is an integer. **Note:** Ensure that your implementation of `__deepcopy__` properly uses the `memo` dictionary to avoid unnecessary duplications and handle recursive structures efficiently.","solution":"import copy class TreeNode: def __init__(self, value, left=None, right=None): self.value = value self.left = left self.right = right def __copy__(self): Create a shallow copy of the TreeNode instance. new_node = TreeNode(self.value) new_node.left = self.left new_node.right = self.right return new_node def __deepcopy__(self, memo): Create a deep copy of the TreeNode instance. if self in memo: # Check if the current instance is already copied return memo[self] new_node = TreeNode(self.value) memo[self] = new_node new_node.left = copy.deepcopy(self.left, memo) new_node.right = copy.deepcopy(self.right, memo) return new_node"},{"question":"Objective Demonstrate your understanding of the `audioop` module by implementing a function that processes audio data in several steps. Task Write a Python function `process_audio(fragment1: bytes, fragment2: bytes, width: int) -> bytes` that: 1. Adds `fragment1` and `fragment2`. 2. Converts the resulting fragment to A-LAW encoding and then back to linear encoding. 3. Reverses the samples in the converted fragment. 4. Calculates the root-mean-square (RMS) value of the reversed fragment. Function Signature ```python def process_audio(fragment1: bytes, fragment2: bytes, width: int) -> bytes: pass ``` Input - `fragment1`: A bytes-like object representing the first audio fragment. - `fragment2`: A bytes-like object representing the second audio fragment. - `width`: An integer indicating the sample width in bytes, which can be either 1, 2, 3, or 4. Output - The function should return a bytes-like object representing the processed audio fragment. Constraints - Both `fragment1` and `fragment2` will always have the same length. - The sample width `width` will be one of 1, 2, 3, or 4 bytes. - The audio data should handle any potential overflows or errors gracefully. Example ```python fragment1 = b\'x01x02x03x04\' fragment2 = b\'x05x06x07x08\' width = 2 output_fragment = process_audio(fragment1, fragment2, width) ``` In this example, the function processes the given fragments according to the described steps. Notes - Use the `audioop` module functions to perform each step. - Ensure that the result is correctly processed through all steps before being returned.","solution":"import audioop def process_audio(fragment1: bytes, fragment2: bytes, width: int) -> bytes: # Step 1: Add fragment1 and fragment2 added_fragment = audioop.add(fragment1, fragment2, width) # Step 2: Convert the resulting fragment to A-LAW encoding and then back to linear encoding alaw_fragment = audioop.lin2alaw(added_fragment, width) linear_fragment = audioop.alaw2lin(alaw_fragment, width) # Step 3: Reverse the samples in the converted fragment reversed_fragment = audioop.reverse(linear_fragment, width) # Step 4: Calculate the root-mean-square (RMS) value of the reversed fragment rms_value = audioop.rms(reversed_fragment, width) # Returning both processed fragment and RMS value for verification purposes return reversed_fragment, rms_value"},{"question":"# Boolean Objects in Python Objective Write a function in Python that mimics the behavior of some of the macros and functions provided in the underlying C API for boolean objects. Your function should handle checking the type of a boolean, returning boolean values based on reference counts, and converting long integers to boolean values. Function 1: bool_check Implement the `bool_check` function to determine if a given object is of boolean type. * **Input**: - `obj` (any type): The object to check. * **Output**: - `bool`: `True` if the object is a boolean, otherwise `False`. Function 2: bool_from_long Implement the `bool_from_long` function that converts a given long integer to a boolean based on its truth value. * **Input**: - `v` (int): The integer to convert. * **Output**: - `bool`: `True` if the integer is non-zero, `False` otherwise. Example Usage ```python # Function definitions def bool_check(obj): # your code here def bool_from_long(v): # your code here # Example calls print(bool_check(True)) # Should output: True print(bool_check(1)) # Should output: False print(bool_from_long(10)) # Should output: True print(bool_from_long(0)) # Should output: False ``` Constraints and Performance Requirements - Do not use any built-in Python functions other than `isinstance` for type checking in `bool_check`. - For `bool_from_long`, simply use Pythons native conversion to boolean values. Please ensure that your code is efficient and adheres to the provided constraints.","solution":"def bool_check(obj): Determines if the given object is of boolean type. Parameters: obj (any): The object to check. Returns: bool: True if the object is a boolean, otherwise False. return isinstance(obj, bool) def bool_from_long(v): Converts the given long integer to a boolean based on its truth value. Parameters: v (int): The integer to convert. Returns: bool: True if the integer is non-zero, False otherwise. return bool(v)"},{"question":"Problem Statement You are tasked with writing a function to handle and interpret error codes using the `errno` module. The function should accept an integer error code and return a detailed error message, including both the name and the corresponding human-readable message. # Function Signature ```python def interpret_error_code(errno_code: int) -> str: pass ``` # Input - `errno_code` (int): An integer representing an error code. # Output - `str`: A string that describes the error in the format: `\\"Error Name (Error Code): Human-readable error message\\"`. If the error code is not found in the `errno.errorcode` dictionary, the function should return `\\"Unknown error code: <errno_code>\\"`. # Constraints - The function should handle any integer input, including those not present in `errno.errorcode`. - The error message should be obtained using `os.strerror(errno_code)`. # Example ```python assert interpret_error_code(1) == \\"EPERM (1): Operation not permitted\\" assert interpret_error_code(2) == \\"ENOENT (2): No such file or directory\\" assert interpret_error_code(9999) == \\"Unknown error code: 9999\\" ``` # Requirements 1. Use the `errno.errorcode` dictionary to find the error name corresponding to the given error code. 2. Use the `os.strerror()` function to obtain the human-readable error message. 3. Ensure the output format is correct and handle unknown error codes appropriately. # Notes - This question assesses the student\'s familiarity with the `errno` module, dictionary operations, exception handling, and string formatting. - Performance is not a primary concern, as typical usage will involve a small number of known error codes.","solution":"import errno import os def interpret_error_code(errno_code: int) -> str: Interprets an error code using the `errno` module and returns a detailed error message. Args: - errno_code (int): An integer representing an error code. Returns: - str: A string that describes the error in the format: \\"Error Name (Error Code): Human-readable error message\\". If the error code is not found, returns \\"Unknown error code: <errno_code>\\". if errno_code in errno.errorcode: error_name = errno.errorcode[errno_code] error_message = os.strerror(errno_code) return f\\"{error_name} ({errno_code}): {error_message}\\" else: return f\\"Unknown error code: {errno_code}\\""},{"question":"**Objective:** Implement a multi-output classification model using `scikit-learn`. Use the `OneVsRestClassifier` strategy for this task. # Problem Statement: You are given a dataset containing features and multiple target labels. Your task is to implement a `OneVsRestClassifier` using a `RandomForestClassifier` as the base estimator. You should then train this model using the provided dataset and predict the labels for a new set of features. Evaluate the performance of your model using accuracy score. # Input: - `X_train`: A 2D numpy array of shape `(n_samples, n_features)` containing the training features. - `y_train`: A 2D numpy array of shape `(n_samples, n_targets)` containing the training labels. Each row represents the labels for a sample. - `X_test`: A 2D numpy array of shape `(m_samples, n_features)` containing the testing features. # Output: - `y_pred`: A 2D numpy array of shape `(m_samples, n_targets)` containing the predicted labels for the test set. # Constraints: 1. Use `OneVsRestClassifier` from `sklearn.multiclass`. 2. Use `RandomForestClassifier` from `sklearn.ensemble` as the base estimator in the `OneVsRestClassifier`. 3. Calculate and print the accuracy score of your model on the training data. # Example: Here\'s a simplified example to guide your implementation: ```python import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.multiclass import OneVsRestClassifier from sklearn.metrics import accuracy_score from sklearn.datasets import make_multilabel_classification # Example Dataset X_train, y_train = make_multilabel_classification(n_samples=100, n_features=20, n_classes=5, random_state=42) X_test, _ = make_multilabel_classification(n_samples=10, n_features=20, n_classes=5, random_state=42) # Implement the solver function def train_and_predict_ovr_classifier(X_train, y_train, X_test): # Initialize the OneVsRestClassifier with RandomForestClassifier ovr = OneVsRestClassifier(RandomForestClassifier(random_state=42)) # Train the model ovr.fit(X_train, y_train) # Predict the labels for the test set y_pred = ovr.predict(X_test) # Calculate and print the accuracy on the training set for validation train_pred = ovr.predict(X_train) accuracy = accuracy_score(y_train, train_pred) print(f\'Accuracy on training data: {accuracy:.4f}\') return y_pred # Predict using the implemented function y_pred = train_and_predict_ovr_classifier(X_train, y_train, X_test) print(y_pred) ``` This example provides the structure of your implementation. Apply the necessary modifications and enhancements per the instructions. Your final solution should be robust and handle edge cases effectively.","solution":"import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.multiclass import OneVsRestClassifier from sklearn.metrics import accuracy_score from sklearn.datasets import make_multilabel_classification def train_and_predict_ovr_classifier(X_train, y_train, X_test): Trains a OneVsRestClassifier with RandomForestClassifier base estimator and predicts labels for the test set. Parameters: X_train (numpy.ndarray): Training features, shape (n_samples, n_features) y_train (numpy.ndarray): Training labels, shape (n_samples, n_targets) X_test (numpy.ndarray): Test features, shape (m_samples, n_features) Returns: numpy.ndarray: Predicted labels for the test set, shape (m_samples, n_targets) # Initialize the OneVsRestClassifier with RandomForestClassifier ovr = OneVsRestClassifier(RandomForestClassifier(random_state=42)) # Train the model ovr.fit(X_train, y_train) # Predict the labels for the test set y_pred = ovr.predict(X_test) # Calculate and print the accuracy on the training set for validation train_pred = ovr.predict(X_train) accuracy = accuracy_score(y_train, train_pred) print(f\'Accuracy on training data: {accuracy:.4f}\') return y_pred"},{"question":"Description: You are to write a Python script that demonstrates your understanding of multiple standard library modules: `os`, `shutil`, `datetime`, `json`, and `argparse`. This script should manage files, parse command-line arguments, and perform some date-related operations. Task: 1. Create a command-line Python script using `argparse` that: - Accepts a directory path as a required positional argument. - Accepts a file extension (e.g., `.txt`, `.json`) as an optional argument with `--ext` or `-e`. Default to `.txt`. - Accepts a date string (e.g., `2023-04-01`) as an optional argument with `--date` or `-d`. Default to today\'s date. 2. Script Requirements: - List all files in the given directory with the specified file extension. - Move all the listed files to a new subdirectory named using the provided date string. If the date string is not provided, use today\'s date. - Create a JSON file in the destination directory named `file_report.json` reporting the moved files. Example: Suppose the script is named `file_manager.py`. Example usage: ```sh python file_manager.py /path/to/directory --ext .log --date 2023-01-01 ``` - Lists all `.log` files in `/path/to/directory`. - Creates a subdirectory `/path/to/directory/2023-01-01`. - Moves all `.log` files to the new subdirectory. - Generates a `file_report.json` in `/path/to/directory/2023-01-01` with the names of the moved files. Constraints: - Ensure the script handles exceptions appropriately (e.g., directory not found, invalid date format). - The script should be well-structured and include comments where necessary. Performance Requirement: - The script should handle directories containing up to 1000 files efficiently. Input format: - Command-line arguments as specified above. Output format: - A JSON file named `file_report.json` in the destination directory, structured as follows: ```json { \\"moved_files\\": [ \\"file1.log\\", \\"file2.log\\", ... ] } ``` Happy Coding!","solution":"import os import shutil import datetime import json import argparse def main(): # Set up command-line argument parsing parser = argparse.ArgumentParser(description=\'Manage files in a directory.\') parser.add_argument(\'directory\', type=str, help=\'Path to the directory\') parser.add_argument(\'--ext\', \'-e\', type=str, default=\'.txt\', help=\'File extension to filter by\') parser.add_argument(\'--date\', \'-d\', type=str, default=datetime.datetime.today().strftime(\'%Y-%m-%d\'), help=\'Date string for the destination directory\') # Parse arguments args = parser.parse_args() # Get absolute directory path and validate it directory = os.path.abspath(args.directory) if not os.path.isdir(directory): raise Exception(f\\"The directory {directory} does not exist\\") # Validate date string try: target_date = datetime.datetime.strptime(args.date, \'%Y-%m-%d\').date() except ValueError as e: raise Exception(f\\"Invalid date format: {args.date}. Expected format YYYY-MM-DD\\") # Create target directory if it does not exist target_directory = os.path.join(directory, args.date) if not os.path.exists(target_directory): os.makedirs(target_directory) # List files with the specified extension and move them to the target directory moved_files = [] for filename in os.listdir(directory): if filename.endswith(args.ext): src_path = os.path.join(directory, filename) dst_path = os.path.join(target_directory, filename) shutil.move(src_path, dst_path) moved_files.append(filename) # Create a JSON report of moved files report = {\'moved_files\': moved_files} report_path = os.path.join(target_directory, \'file_report.json\') with open(report_path, \'w\') as f: json.dump(report, f, indent=4) print(f\\"Moved {len(moved_files)} files to {target_directory} and created report at {report_path}\\") if __name__ == \'__main__\': main()"},{"question":"# Abstract Shape Hierarchy You are tasked with designing a shape hierarchy using the `abc` module in Python. This exercise will test your understanding of abstract base classes, abstract methods, and virtual subclasses. 1. Create an abstract base class `Shape` using the `abc.ABC` helper class. This class should have: - An abstract method `area()` which computes the area of the shape. - An abstract method `perimeter()` which computes the perimeter of the shape. 2. Create two subclasses: - `Circle`: A class representing a circle, initialized with a single argument for the radius. - `Rectangle`: A class representing a rectangle, initialized with two arguments for the length and width. 3. Implement the abstract methods in both subclasses: - The `area()` method for `Circle` should return ` * radius^2`. - The `perimeter()` method for `Circle` should return `2 *  * radius`. - The `area()` method for `Rectangle` should return `length * width`. - The `perimeter()` method for `Rectangle` should return `2 * (length + width)`. 4. Create a virtual subclass called `Polygon` (without inheritance from `Shape`). Use the `register()` method to make it a virtual subclass of `Shape`. 5. Demonstrate that an instance of `Polygon` (or a subclass of `Polygon`) correctly behaves as a subclass of `Shape` in terms of method requirements and subclass checks. **Input and Output Requirements:** - Define the classes `Shape`, `Circle`, `Rectangle`, and `Polygon`. - Implement the methods as specified. - Demonstrate the behavior with appropriate example instances and assertions. **Constraints:** - Use the `abc` module for defining abstract base classes and methods. - Ensure that `you create virtual subclass correctly` and it meets the requirements for an abstract shape. # Example Code Structure: ```python from abc import ABC, abstractmethod import math class Shape(ABC): @abstractmethod def area(self): pass @abstractmethod def perimeter(self): pass class Circle(Shape): def __init__(self, radius): self.radius = radius def area(self): return math.pi * (self.radius ** 2) def perimeter(self): return 2 * math.pi * self.radius class Rectangle(Shape): def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * (self.length + self.width) class Polygon: # Implementation details here # Register Polygon class as virtual subclass Shape.register(Polygon) # Example assertions and demonstrations rect = Rectangle(5, 3) circle = Circle(4) assert isinstance(rect, Shape) assert isinstance(circle, Shape) # Example virtual subclass instance poly_instance = Polygon() # Assuming Polygon is correctly defined assert isinstance(poly_instance, Shape) print(\\"All assertions passed\\") ```","solution":"from abc import ABC, abstractmethod import math class Shape(ABC): @abstractmethod def area(self): pass @abstractmethod def perimeter(self): pass class Circle(Shape): def __init__(self, radius): self.radius = radius def area(self): return math.pi * (self.radius ** 2) def perimeter(self): return 2 * math.pi * self.radius class Rectangle(Shape): def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * (self.length + self.width) class Polygon: def __init__(self, sides, side_length): self.sides = sides self.side_length = side_length def area(self): # This is just a sample placeholder area calculation method return self.sides * self.side_length def perimeter(self): return self.sides * self.side_length # Register Polygon class as virtual subclass Shape.register(Polygon)"},{"question":"Objective You are tasked with training a text classification model on a dataset that is too large to fit into the memory using scikit-learn. You will use out-of-core learning techniques with an incremental classifier. Task 1. **Data Streaming:** Write a function `stream_data` that simulates streaming data in mini-batches from a large text file. Assume the file is a text classification dataset with each line containing a label and text separated by a comma. 2. **Feature Extraction:** Use `HashingVectorizer` for feature extraction to handle the streaming data. 3. **Incremental Learning:** Train an `SGDClassifier` incrementally using the streamed and vectorized data. 4. **Performance Evaluation:** After training, evaluate the accuracy of the model on a hold-out validation set. Function Signature ```python import numpy as np from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split def stream_data(file_path, batch_size): A generator function to simulate streaming of data in batches. Args: file_path (str): The path to the large text file. batch_size (int): Number of samples per batch. Yields: X_batch (list): List of text samples. y_batch (list): List of labels for the text samples. with open(file_path, \'r\') as file: X_batch = [] y_batch = [] for line in file: label, text = line.strip().split(\',\', 1) X_batch.append(text) y_batch.append(label) if len(X_batch) == batch_size: yield X_batch, y_batch X_batch = [] y_batch = [] if X_batch: yield X_batch, y_batch def train_incremental_model(file_path, batch_size, classes): Trains an incremental SGDClassifier model using streamed data. Args: file_path (str): The path to the large text file. batch_size (int): Number of samples per batch. classes (list): List of all possible classes. Returns: model (SGDClassifier): The trained model. vectorizer (HashingVectorizer): The vectorizer used to transform the data. vectorizer = HashingVectorizer(alternate_sign=False) model = SGDClassifier() for X_batch, y_batch in stream_data(file_path, batch_size): X_vectorized = vectorizer.transform(X_batch) model.partial_fit(X_vectorized, y_batch, classes=classes) return model, vectorizer def evaluate_model(model, vectorizer, X_val, y_val): Evaluates the trained model on a validation set. Args: model (SGDClassifier): The trained model. vectorizer (HashingVectorizer): The vectorizer used to transform the data. X_val (list): List of text samples in the validation set. y_val (list): List of labels for the text samples. Returns: accuracy (float): The accuracy of the model on the validation set. X_val_vectorized = vectorizer.transform(X_val) y_pred = model.predict(X_val_vectorized) accuracy = accuracy_score(y_val, y_pred) return accuracy ``` Usage Example 1. Assume `file_path` is the path to your large text dataset. 2. `batch_size` is the number of samples per batch. 3. `classes` is the list of all possible class labels. ```python # Split the file into training and validation sets first. file_path = \'large_text_dataset.txt\' batch_size = 1000 classes = [\'class1\', \'class2\', \'class3\'] # Train the model incrementally model, vectorizer = train_incremental_model(file_path, batch_size, classes) # Assuming X_val and y_val have been separately read and preprocessed accuracy = evaluate_model(model, vectorizer, X_val, y_val) print(f\'Validation Accuracy: {accuracy}\') ``` Dataset Format The dataset should be in a text file with each line containing a label and a text sample separated by a comma, for example: ``` class1,This is a sample text. class2,Another sample text. class1,More text data here. ... ``` Constraints 1. The dataset is too large to fit in memory; only a small portion can be loaded at any time. 2. Implement the solution using only the libraries provided (`scikit-learn`, `numpy`).","solution":"import numpy as np from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split def stream_data(file_path, batch_size): A generator function to simulate streaming of data in batches. Args: file_path (str): The path to the large text file. batch_size (int): Number of samples per batch. Yields: X_batch (list): List of text samples. y_batch (list): List of labels for the text samples. with open(file_path, \'r\') as file: X_batch = [] y_batch = [] for line in file: label, text = line.strip().split(\',\', 1) X_batch.append(text) y_batch.append(label) if len(X_batch) == batch_size: yield X_batch, y_batch X_batch = [] y_batch = [] if X_batch: yield X_batch, y_batch def train_incremental_model(file_path, batch_size, classes): Trains an incremental SGDClassifier model using streamed data. Args: file_path (str): The path to the large text file. batch_size (int): Number of samples per batch. classes (list): List of all possible classes. Returns: model (SGDClassifier): The trained model. vectorizer (HashingVectorizer): The vectorizer used to transform the data. vectorizer = HashingVectorizer(alternate_sign=False) model = SGDClassifier() for X_batch, y_batch in stream_data(file_path, batch_size): X_vectorized = vectorizer.transform(X_batch) model.partial_fit(X_vectorized, y_batch, classes=classes) return model, vectorizer def evaluate_model(model, vectorizer, X_val, y_val): Evaluates the trained model on a validation set. Args: model (SGDClassifier): The trained model. vectorizer (HashingVectorizer): The vectorizer used to transform the data. X_val (list): List of text samples in the validation set. y_val (list): List of labels for the text samples. Returns: accuracy (float): The accuracy of the model on the validation set. X_val_vectorized = vectorizer.transform(X_val) y_pred = model.predict(X_val_vectorized) accuracy = accuracy_score(y_val, y_pred) return accuracy"},{"question":"Objective: Write a Python function that simulates the behavior of binding a function to an instance method and retrieving details about the function and the instance from a bound method object. Problem Statement: You are required to implement a method simulation using Python\'s method binding principles. Considering that direct usage of C API in Python is out of scope, we aim to simulate the `PyInstanceMethod` and `PyMethod` behaviors in pure Python. 1. **Function `bind_instance_method`**: - **Input**: Takes a callable `func` and instance `instance`. - **Output**: Returns a new instance method object where the function is bound to the given instance. 2. **Function `get_bound_function`**: - **Input**: Takes an instance method object. - **Output**: Returns the function associated with the given instance method object. 3. **Function `get_bound_instance`**: - **Input**: Takes an instance method object. - **Output**: Returns the instance associated with the given instance method object. # Constraints: 1. The `func` parameter of `bind_instance_method` will always be a callable. 2. The `instance` can be any Python object. 3. Handle and manage any required internally consistent data structures to keep track of function-instance pairs. 4. Do not use any external libraries beyond standard Python. # Example: ```python class TestClass: def __init__(self, value): self.value = value def my_function(self): return self.value # Example function to bind def some_function(): return \\"Bound function executed.\\" # Create instance of TestClass instance = TestClass(10) # Bind the function bound_inst_method = bind_instance_method(some_function, instance) # Get the bound function bound_func = get_bound_function(bound_inst_method) print(bound_func()) # Output: \\"Bound function executed.\\" # Get the bound instance bound_inst = get_bound_instance(bound_inst_method) print(bound_inst.value) # Output: 10 ``` Implementation: - The simulated method binding should align with the way `PyInstanceMethod` and `PyMethod` would work with their respective functions and types. - Ensure your solution is clean, efficient, and follows Pythonic practices. Good luck!","solution":"class BoundMethod: def __init__(self, func, instance): self._func = func self._instance = instance def __call__(self, *args, **kwargs): return self._func(self._instance, *args, **kwargs) def bind_instance_method(func, instance): Binds a function to an instance simulating an instance method. Args: func (callable): The function to bind. instance (object): The instance to bind the function to. Returns: BoundMethod: The bound method object. return BoundMethod(func, instance) def get_bound_function(bound_method): Retrieves the function from a bound method object. Args: bound_method (BoundMethod): The bound method object. Returns: callable: The function associated with the bound method object. return bound_method._func def get_bound_instance(bound_method): Retrieves the instance from a bound method object. Args: bound_method (BoundMethod): The bound method object. Returns: object: The instance associated with the bound method object. return bound_method._instance"},{"question":"You are tasked with configuring the logging system of PyTorch for a hypothetical deep learning project that uses various components and artifacts. 1. **Function Implementation**: Write a Python function `configure_logging(settings: dict) -> None` that configures the logging system using the PyTorch `torch._logging.set_logs` API. The `settings` dictionary will include components and artifacts with their respective prefixes and log levels. 2. **Input Format**: The `settings` parameter is a dictionary where: - Keys are components or artifacts (e.g., `\\"dynamo\\"`, `\\"aot_graphs\\"`). - Values are strings containing the prefix (`\\"+\\"` or `\\"-\\"`) and the corresponding log levels or a boolean (`True` or `False`) for artifacts. 3. **Constraints**: - The log levels should be those defined in Python\'s logging module (`\\"DEBUG\\"`, `\\"INFO\\"`, `\\"WARN\\"`, `\\"ERROR\\"`). - The function should be able to handle both components and artifacts. - If the component or artifact is not recognized, it should be ignored. 4. **Example**: ```python settings = { \\"dynamo\\": \\"+DEBUG\\", \\"aot\\": \\"-ERROR\\", \\"bytecode\\": True, \\"graph_code\\": False } configure_logging(settings) ``` In this example: - The log level for the `dynamo` component will be set to `DEBUG`. - The log level for the `aot` component will be set to `ERROR`. - The `bytecode` artifact will be enabled. - The `graph_code` artifact will be disabled. # Function Signature ```python def configure_logging(settings: dict) -> None: pass ``` # Notes: - Utilize the `torch._logging.set_logs` API to configure the settings within the function. - Ensure that invalid settings or components/artifacts that do not exist in PyTorch\'s logging system are ignored without causing the function to crash.","solution":"import logging import torch def configure_logging(settings: dict) -> None: Configures the logging system using PyTorch\'s torch._logging.set_logs API. Parameters: settings (dict): A dictionary where keys are components or artifacts and values are strings of prefixes and log levels or booleans. components = {} artifacts = {} for key, value in settings.items(): if isinstance(value, str) and value[0] in [\\"+\\", \\"-\\"]: prefix = value[0] log_level = value[1:].upper() if log_level in [\\"DEBUG\\", \\"INFO\\", \\"WARN\\", \\"ERROR\\"] and prefix in [\\"+\\", \\"-\\"]: components[key] = f\\"{prefix}{log_level}\\" elif isinstance(value, bool): artifacts[key] = value torch._logging.set_logs(components=components, artifacts=artifacts)"},{"question":"You are provided with a moderately complex dataset and tasked with managing it using `MultiIndex` from the `pandas` library. Your objective is to demonstrate your understanding of creating, manipulating, and utilizing a `MultiIndex` for advanced indexing. **Dataset:** The dataset represents sales data of a retail company, structured as follows: | StoreID | ProductID | Date | Sales | |---------|-----------|------------|-------| | 1 | A | 2023-01-01 | 100 | | 1 | B | 2023-01-01 | 150 | | 2 | A | 2023-01-01 | 200 | | 2 | C | 2023-01-02 | 250 | | 1 | A | 2023-01-03 | 300 | | 2 | B | 2023-01-03 | 400 | You need to: 1. **Create a `MultiIndex` DataFrame**: - Create a `MultiIndex` DataFrame from the given dataset with `StoreID` and `ProductID` as the multi-level indices. 2. **Perform Advanced Indexing**: - Select and return the sales data for `StoreID` 1. - Select and return the sales data for `ProductID` B across all stores. 3. **Reconstruct and Manipulate Levels**: - Extract and return the values of `StoreID` and `ProductID` as separate columns. - Swap the levels of the `MultiIndex` such that `ProductID` becomes the first level and `StoreID` becomes the second level, then return the modified DataFrame. # Function Signature ```python import pandas as pd def manage_sales_data(data: list[list[str]]) -> dict: :param data: List of lists containing the dataset. :return: Dictionary containing the results of various operations. pass ``` # Constraints - Use `pandas` library functionalities to handle the given dataset. - The dataset is guaranteed to be non-empty and to contain unique `StoreID` and `ProductID`. # Example ```python data = [ [1, \'A\', \'2023-01-01\', 100], [1, \'B\', \'2023-01-01\', 150], [2, \'A\', \'2023-01-01\', 200], [2, \'C\', \'2023-01-02\', 250], [1, \'A\', \'2023-01-03\', 300], [2, \'B\', \'2023-01-03\', 400] ] print(manage_sales_data(data)) ``` Expected Output: ```python { \\"multiindex_df\\": <MultiIndex DataFrame>, \\"store1_sales\\": <DataFrame with sales data for StoreID 1>, \\"product_b_sales\\": <DataFrame with sales data for ProductID B>, \\"reconstructed_df\\": <DataFrame with \'StoreID\' and \'ProductID\' as columns>, \\"swapped_levels_df\\": <MultiIndex DataFrame with swapped levels> } ``` `<MultiIndex DataFrame>` is a placeholder representing the actual DataFrame\'s format. # Notes - You may assume that the input data is provided in the correct format. - Your implementation should focus on the usage of MultiIndex creation, advanced indexing, and level manipulation techniques as described in the pandas documentation provided.","solution":"import pandas as pd def manage_sales_data(data: list[list[str]]) -> dict: :param data: List of lists containing the dataset. :return: Dictionary containing the results of various operations. # Convert the data list into a DataFrame df = pd.DataFrame(data, columns=[\'StoreID\', \'ProductID\', \'Date\', \'Sales\']) # Set MultiIndex with \'StoreID\' and \'ProductID\' multiindex_df = df.set_index([\'StoreID\', \'ProductID\']) # Select and return the sales data for StoreID 1 store1_sales = multiindex_df.xs(1, level=\'StoreID\') # Select and return the sales data for ProductID B across all stores product_b_sales = multiindex_df.xs(\'B\', level=\'ProductID\') # Extract and return the values of StoreID and ProductID as separate columns reconstructed_df = multiindex_df.reset_index() # Swap the levels of the MultiIndex swapped_levels_df = multiindex_df.swaplevel() return { \\"multiindex_df\\": multiindex_df, \\"store1_sales\\": store1_sales, \\"product_b_sales\\": product_b_sales, \\"reconstructed_df\\": reconstructed_df, \\"swapped_levels_df\\": swapped_levels_df }"},{"question":"# Question: You are developing a classifier to help medical practitioners identify patients who might have cancer. Given the critical nature of this application, your primary goal is to maximize recall, ensuring that fewer actual cases of cancer are missed, which justifies higher false-positive rates. You are provided with a dataset and your task is to: 1. Train a classifier to predict the likelihood of cancer. 2. Tune the decision threshold to maximize the recall. 3. Validate the performance of your tuned model using cross-validation. Requirements: - Use the `TunedThresholdClassifierCV` from `sklearn.model_selection`. - The dataset is already split into features `X` and target `y`. - Use `LogisticRegression` as the base model for classification. - Implement the custom scorer to maximize recall. - Perform a 5-fold cross-validation to ensure robustness. Input: - `X_train` (array-like, shape [n_samples, n_features]): Training features. - `y_train` (array-like, shape [n_samples]): Training labels. - `X_test` (array-like, shape [n_samples, n_features]): Test features. - `y_test` (array-like, shape [n_samples]): Test labels. Output: - Print the best threshold obtained for maximizing recall. - Print the recall score on the test set using the tuned threshold. - Print the confusion matrix for the test set predictions. Constraints: - Use `random_state=0` for any random processes to ensure reproducibility. # Code Template: ```python import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.model_selection import TunedThresholdClassifierCV from sklearn.metrics import make_scorer, recall_score, confusion_matrix def tune_and_evaluate_threshold(X_train, y_train, X_test, y_test): pos_label = 1 # assuming 1 is the label for cancer scorer = make_scorer(recall_score, pos_label=pos_label) base_model = LogisticRegression(random_state=0) model = TunedThresholdClassifierCV(base_model, scoring=scorer, cv=5) model.fit(X_train, y_train) # Obtain the optimal threshold best_threshold = model.best_threshold_ # Predict using the best threshold y_pred = model.predict(X_test) # Calculate recall score using the best threshold recall = recall_score(y_test, y_pred, pos_label=pos_label) # Confusion matrix conf_matrix = confusion_matrix(y_test, y_pred) print(f\\"Best Threshold: {best_threshold}\\") print(f\\"Recall Score on Test Set: {recall}\\") print(\\"Confusion Matrix:n\\", conf_matrix) # Assuming X_train, y_train, X_test, y_test are already defined # tune_and_evaluate_threshold(X_train, y_train, X_test, y_test) ``` Ensure that the dataset `X_train`, `y_train`, `X_test`, and `y_test` is correctly defined and preprocessed before passing it to the function. Evaluate and tune the decision threshold to maximize recall for identifying cancer predictions.","solution":"import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_predict, KFold from sklearn.metrics import make_scorer, recall_score, confusion_matrix, precision_recall_curve def tune_threshold_for_recall(y_true, y_proba, pos_label=1): precision, recall, thresholds = precision_recall_curve(y_true, y_proba, pos_label=pos_label) max_recall_idx = np.argmax(recall) best_threshold = thresholds[max_recall_idx] if max_recall_idx < len(thresholds) else 1.0 return best_threshold def tune_and_evaluate_threshold(X_train, y_train, X_test, y_test): pos_label = 1 # assuming 1 is the label for cancer base_model = LogisticRegression(random_state=0) kf = KFold(n_splits=5, shuffle=True, random_state=0) y_proba = cross_val_predict(base_model, X_train, y_train, cv=kf, method=\'predict_proba\')[:,1] best_threshold = tune_threshold_for_recall(y_train, y_proba, pos_label=pos_label) base_model.fit(X_train, y_train) y_test_proba = base_model.predict_proba(X_test)[:,1] y_pred = (y_test_proba >= best_threshold).astype(int) recall = recall_score(y_test, y_pred, pos_label=pos_label) conf_matrix = confusion_matrix(y_test, y_pred) print(f\\"Best Threshold: {best_threshold}\\") print(f\\"Recall Score on Test Set: {recall}\\") print(\\"Confusion Matrix:n\\", conf_matrix) # Assuming X_train, y_train, X_test, y_test are already defined # tune_and_evaluate_threshold(X_train, y_train, X_test, y_test)"},{"question":"You are provided with the `penguins` dataset from seaborn, which contains measurements for different species of penguins. Your task is to create a complex visualization using seaborn\'s `objects` interface. This visualization should effectively integrate multiple functionalities demonstrated in the provided documentation. Task Using seaborn\'s `objects` interface, create a visualization that: 1. Plots the `body_mass_g` of penguins against their species. 2. Colors the plot points according to the `sex` of the penguins. 3. Adds dash marks (lines) for each data point with: - An alpha transparency of 0.5. - Dash widths proportional to the `flipper_length_mm` of the penguins. 4. Adds aggregated dash marks to show the median values, using the Dodge function to avoid overlap. 5. Adds jittered dots to represent individual data points, using the Dodge function to separate them by `sex`. Constraints - You may only use the seaborn package and any necessary imports from matplotlib and pandas. - Your implementation must handle cases where data may be missing in the `sex` or `flipper_length_mm` columns (i.e., avoid breaking). Expected Output The output should be a plot with the specified properties, which provides a clear visualization of the relationship between penguin species, body mass, flipper length, and sex. Example Code You can start with the following template and complete the implementation: ```python import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Filter out rows with missing values in \'sex\' or \'flipper_length_mm\' columns penguins = penguins.dropna(subset=[\'sex\', \'flipper_length_mm\']) # Create the plot object p = so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") # Add the Dash marks with specified properties p.add(so.Dash(alpha=0.5), linewidth=\\"flipper_length_mm\\") # Add aggregated dash marks for median values with dodge to avoid overlap p.add(so.Dash(), so.Agg(), so.Dodge()) # Add jittered dots for individual data points with dodge to separate by sex p.add(so.Dots(), so.Dodge(), so.Jitter()) # Display the plot p.show() ``` Ensure that your plot meets all the specified requirements accurately.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_penguin_plot(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Filter out rows with missing values in \'sex\' or \'flipper_length_mm\' columns penguins = penguins.dropna(subset=[\'sex\', \'flipper_length_mm\']) # Create the plot object p = so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") # Add the Dash marks with specified properties p.add(so.Dash(alpha=0.5), linewidth=\\"flipper_length_mm\\") # Add aggregated dash marks for median values with dodge to avoid overlap p.add(so.Dash(), so.Agg(), so.Dodge()) # Add jittered dots for individual data points with dodge to separate by sex p.add(so.Dots(), so.Dodge(), so.Jitter()) # Display the plot p.show() # Example usage create_penguin_plot()"},{"question":"**Objective**: Write a Python function named `analyze_code_symbols` that uses the `symtable` module to analyze the provided Python source code. The function should generate the symbol table for the code and extract specific information about the symbols. **Function Signature**: ```python def analyze_code_symbols(code: str, filename: str, compile_type: str) -> dict: pass ``` **Input**: - `code` (str): A string containing the Python source code to be analyzed. - `filename` (str): The name of the file containing the code. - `compile_type` (str): The type of code (e.g., \'exec\', \'eval\', or \'single\') similar to the `mode` argument in `compile()`. **Output**: - Returns a dictionary with the following structure: ```python { \\"functions\\": { \\"function_name\\": { \\"parameters\\": list of parameters, \\"locals\\": list of local variables, \\"globals\\": list of global variables, \\"nonlocals\\": list of nonlocal variables, \\"frees\\": list of free variables }, ... }, \\"classes\\": { \\"class_name\\": { \\"methods\\": list of method names }, ... }, \\"global_symbols\\": { \\"symbol_name\\": { \\"referenced\\": bool, \\"imported\\": bool, \\"parameter\\": bool, \\"global\\": bool, \\"nonlocal\\": bool, \\"declared_global\\": bool, \\"local\\": bool, \\"annotated\\": bool, \\"free\\": bool, \\"assigned\\": bool, \\"namespace\\": bool, \\"namespaces\\": list of namespace names if namespace }, ... } } ``` **Constraints**: - You may assume that the input code is syntactically valid Python code. - Use the provided `symtable` module to access and analyze symbol tables. **Example**: ```python code = \'\'\' def foo(x, y): z = x + y return z class Bar: def __init__(self): self.value = 0 def increment(self): self.value += 1 \'\'\' filename = \\"example.py\\" compile_type = \\"exec\\" result = analyze_code_symbols(code, filename, compile_type) print(result) ``` **Expected Output**: ```python { \\"functions\\": { \\"foo\\": { \\"parameters\\": [\\"x\\", \\"y\\"], \\"locals\\": [\\"z\\"], \\"globals\\": [], \\"nonlocals\\": [], \\"frees\\": [] }, \\"__init__\\": { \\"parameters\\": [\\"self\\"], \\"locals\\": [\\"self.value\\"], \\"globals\\": [], \\"nonlocals\\": [], \\"frees\\": [] }, \\"increment\\": { \\"parameters\\": [\\"self\\"], \\"locals\\": [\\"self\\"], \\"globals\\": [], \\"nonlocals\\": [], \\"frees\\": [] } }, \\"classes\\": { \\"Bar\\": { \\"methods\\": [\\"__init__\\", \\"increment\\"] } }, \\"global_symbols\\": { \\"foo\\": { \\"referenced\\": False, \\"imported\\": False, \\"parameter\\": False, \\"global\\": False, \\"nonlocal\\": False, \\"declared_global\\": False, \\"local\\": True, \\"annotated\\": False, \\"free\\": False, \\"assigned\\": False, \\"namespace\\": True, \\"namespaces\\": [<SymbolTable object for function \'foo\'>] }, \\"Bar\\": { \\"referenced\\": False, \\"imported\\": False, \\"parameter\\": False, \\"global\\": False, \\"nonlocal\\": False, \\"declared_global\\": False, \\"local\\": True, \\"annotated\\": False, \\"free\\": False, \\"assigned\\": False, \\"namespace\\": True, \\"namespaces\\": [<SymbolTable object for class \'Bar\'>] } } } ``` **Notes**: - You are not required to implement the printing of `<SymbolTable object>`; the provided output shows that the `namespaces` key contains instances of `SymbolTable`. You should account for this being an instance representation in your final dictionary.","solution":"import symtable def analyze_code_symbols(code: str, filename: str, compile_type: str) -> dict: symbol_table = symtable.symtable(code, filename, compile_type) def extract_function_info(symbol): return { \\"parameters\\": list(symbol.get_parameters()), \\"locals\\": [name for name in symbol.get_locals() if name not in symbol.get_parameters()], \\"globals\\": list(symbol.get_globals()), \\"nonlocals\\": list(symbol.get_nonlocals()), \\"frees\\": list(symbol.get_frees()) } def extract_symbol_info(symbol): return { \\"referenced\\": symbol.is_referenced(), \\"imported\\": symbol.is_imported(), \\"parameter\\": symbol.is_parameter(), \\"global\\": symbol.is_global(), \\"nonlocal\\": symbol.is_nonlocal(), \\"declared_global\\": symbol.is_declared_global(), \\"local\\": symbol.is_local(), \\"annotated\\": symbol.is_annotated(), \\"free\\": symbol.is_free(), \\"assigned\\": symbol.is_assigned(), \\"namespace\\": symbol.is_namespace(), \\"namespaces\\": [str(ns) for ns in symbol.get_namespaces()] if symbol.is_namespace() else [] } result = { \\"functions\\": {}, \\"classes\\": {}, \\"global_symbols\\": {} } for symbol in symbol_table.get_symbols(): if symbol.is_namespace(): namespace = symbol.get_namespace() if isinstance(namespace, symtable.Function): result[\\"functions\\"][symbol.get_name()] = extract_function_info(namespace) elif isinstance(namespace, symtable.Class): result[\\"classes\\"][symbol.get_name()] = { \\"methods\\": [sym.get_name() for sym in namespace.get_symbols() if sym.is_namespace() and isinstance(sym.get_namespace(), symtable.Function)] } result[\\"global_symbols\\"][symbol.get_name()] = extract_symbol_info(symbol) return result"},{"question":"**Objective:** To assess the understanding of numerical precision and matrix operations in PyTorch. **Problem Statement:** You are provided with two 2D tensors `A` and `B`, representing matrices of dimensions [N, N]. Your task is to demonstrate your understanding of numerical precision in PyTorch by performing the following operations: 1. Implement a function `matrix_operations(A: torch.Tensor, B: torch.Tensor, precision: str) -> dict` that: - Validates the precision string (`\'float32\'`, `\'float64\'`, `\'float16\'`, `\'bfloat16\'`). - Converts `A` and `B` tensors to the specified precision. - Computes the following: 1. Matrix multiplication result `C = A @ B`. 2. The norm of each matrix (`A`, `B`, `C`) using their respective precision. 3. Checks for any non-finite values (e.g., `inf`, `NaN`) in the resultant matrix `C`. 2. Return a dictionary with the results of the operations above with keys: - `\'C\'`: The result of the matrix multiplication. - `\'norms\'`: A dictionary containing the norms of matrices `A`, `B`, `C`. - `\'non_finite_values\'`: A boolean indicating if there are any non-finite values in `C`. **Function Signature:** ```python import torch def matrix_operations(A: torch.Tensor, B: torch.Tensor, precision: str) -> dict: pass ``` **Inputs:** - `A`: A 2D PyTorch tensor of shape `[N, N]`. - `B`: A 2D PyTorch tensor of shape `[N, N]`. - `precision`: A string indicating the precision type `\'float32\' | \'float64\' | \'float16\' | \'bfloat16\'`. **Outputs:** - A dictionary with the keys `\'C\'`, `\'norms\'`, and `\'non_finite_values\'`. **Constraints:** - Matrices `A` and `B` will have dimensions [N, N] where 2 <= N <= 1000. - You must handle potential precision loss or overflow issues and ensure that your function can handle non-finite values gracefully. **Example:** ```python import torch A = torch.tensor([[1e20, 1e20], [1e20, 1e20]], dtype=torch.float32) B = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32) result = matrix_operations(A, B, \'float32\') # result should be a dictionary containing: # \'C\': Resultant matrix of A @ B in specified precision # \'norms\': Norms of matrices A, B, C in specified precision # \'non_finite_values\': Boolean indicating presence of any non-finite values in C ``` **Notes:** - Ensure that you utilize PyTorch functions correctly to convert tensor types and compute norms. - The function should be robust to different numerical precisions and handle large values effectively.","solution":"import torch def matrix_operations(A: torch.Tensor, B: torch.Tensor, precision: str) -> dict: valid_precisions = [\'float32\', \'float64\', \'float16\', \'bfloat16\'] if precision not in valid_precisions: raise ValueError(f\\"Invalid precision type. Must be one of {valid_precisions}\\") # Type conversion A = A.to(getattr(torch, precision)) B = B.to(getattr(torch, precision)) # Matrix multiplication C = A @ B # Norm calculation norms = { \'A\': torch.norm(A).item(), \'B\': torch.norm(B).item(), \'C\': torch.norm(C).item() } # Check for non-finite values non_finite_values = torch.isinf(C).any() or torch.isnan(C).any() return { \'C\': C, \'norms\': norms, \'non_finite_values\': non_finite_values }"},{"question":"# Question # Problem Statement You are given an unlabelled dataset `data` consisting of `n_samples` samples, each with `n_features` features. Your task is to implement a class called `ClusterAlgoComparison` using the `sklearn.cluster` module that performs clustering with multiple algorithms provided in scikit-learn. Your implementation should: 1. Initialize with a list of clustering algorithms to apply. 2. Allow fitting the algorithms to the data. 3. Provide a method to return the cluster labels for each algorithm. 4. Evaluate the clustering results using the silhouette score and return the algorithm with the best silhouette score. # Detailed Requirements 1. **Initialization:** - The class should be initialized with a list of clustering algorithm instances. - Example: `ClusterAlgoComparison([KMeans(n_clusters=3), DBSCAN(eps=0.5)])` 2. **Fitting Data:** - The `fit` method should accept the input data in the form of a 2D numpy array of shape `(n_samples, n_features)`. - It should fit each clustering algorithm to the data. 3. **Getting Cluster Labels:** - The `get_labels` method should return a dictionary where keys are the names of the clustering algorithms and values are the cluster labels assigned by each algorithm for the input data. - Example Output: `{\'KMeans\': [0, 1, 1, ...], \'DBSCAN\': [0, -1, 1, ...]}` 4. **Best Algorithm by Silhouette Score:** - The `best_algorithm` method should evaluate the silhouette score for all cluster labels obtained and return the algorithm name that has the highest silhouette score. - The silhouette score should be computed using `sklearn.metrics.silhouette_score`. # Constraints - The dataset size `(n_samples)` and feature dimension `(n_features)` can be large. Ensure the implementation is efficient. - For any clustering algorithm that assigns noise points (e.g., with label `-1` in DBSCAN), replace the noise points\' labels with a new unique label that does not interfere with other cluster labels before computing the silhouette score. # Implementation ```python import numpy as np from sklearn.metrics import silhouette_score class ClusterAlgoComparison: def __init__(self, algorithms): Initialize with a list of clustering algorithm instances. self.algorithms = algorithms self.labels_ = {} def fit(self, data): Fit the clustering algorithms to the data. Parameters: data (np.ndarray): 2D array of shape (n_samples, n_features). for algo in self.algorithms: algo_name = algo.__class__.__name__ algo.fit(data) self.labels_[algo_name] = algo.labels_ if hasattr(algo, \'labels_\') else algo.predict(data) def get_labels(self): Return the cluster labels for each algorithm. Returns: dict: Dictionary containing algorithm names as keys and respective cluster labels as values. return self.labels_ def best_algorithm(self, data): Evaluate silhouette scores and return the algorithm with the best score. Parameters: data (np.ndarray): 2D array of shape (n_samples, n_features). Returns: str: Name of the algorithm with the highest silhouette score. best_score = -1 best_algo = None for algo_name, labels in self.labels_.items(): # Replace noise labels with a new unique label if applicable labels = np.array(labels) unique_labels = np.unique(labels) if -1 in unique_labels: labels[labels == -1] = len(unique_labels) score = silhouette_score(data, labels) if score > best_score: best_score = score best_algo = algo_name return best_algo ``` # Example Usage ```python from sklearn.cluster import KMeans, DBSCAN import numpy as np data = np.random.rand(100, 5) # Example data comparison = ClusterAlgoComparison([KMeans(n_clusters=3), DBSCAN(eps=0.5)]) comparison.fit(data) labels = comparison.get_labels() print(labels) best_algo = comparison.best_algorithm(data) print(\\"Best Algorithm by Silhouette Score:\\", best_algo) ``` # Input and Output Formats - **Input:** - A list of clustering algorithm instances during initialization. - A 2D numpy array of shape `(n_samples, n_features)` in the `fit` method and `best_algorithm` method. - **Output:** - A dictionary of cluster labels for each algorithm in the `get_labels` method. - A string indicating the name of the best algorithm by silhouette score in the `best_algorithm` method. # Performance Requirements - Ensure the methods are optimized for large datasets. - Efficient handling of algorithms that generate noise points to correctly compute the silhouette score.","solution":"import numpy as np from sklearn.metrics import silhouette_score class ClusterAlgoComparison: def __init__(self, algorithms): Initialize with a list of clustering algorithm instances. self.algorithms = algorithms self.labels_ = {} def fit(self, data): Fit the clustering algorithms to the data. Parameters: data (np.ndarray): 2D array of shape (n_samples, n_features). for algo in self.algorithms: algo_name = algo.__class__.__name__ algo.fit(data) self.labels_[algo_name] = algo.labels_ if hasattr(algo, \'labels_\') else algo.predict(data) def get_labels(self): Return the cluster labels for each algorithm. Returns: dict: Dictionary containing algorithm names as keys and respective cluster labels as values. return self.labels_ def best_algorithm(self, data): Evaluate silhouette scores and return the algorithm with the best score. Parameters: data (np.ndarray): 2D array of shape (n_samples, n_features). Returns: str: Name of the algorithm with the highest silhouette score. best_score = -1 best_algo = None for algo_name, labels in self.labels_.items(): # Replace noise labels with a new unique label if applicable labels = np.array(labels) unique_labels = np.unique(labels) if -1 in unique_labels: labels[labels == -1] = len(unique_labels) try: score = silhouette_score(data, labels) if score > best_score: best_score = score best_algo = algo_name except ValueError: # Silhouette score cannot be computed, e.g., if there is only 1 cluster present continue return best_algo"},{"question":"Objective: To evaluate your understanding of Python\'s object-oriented programming (OOP) concepts, including class definitions, attributes, inheritance, method overriding, private variables, and iterators. Problem Statement: You are required to design a small banking system that tracks accounts and their transactions. The system should support different types of accounts (e.g., Savings, Checking). Each account should have specific attributes and methods, including methods to deposit and withdraw funds, and to generate account statements. Requirements: 1. **Class Definitions**: - Create a base class `BankAccount` with the following attributes and methods: - `account_number`: A unique identifier for the account (type: string). - `account_holder`: The name of the account holder (type: string). - `balance`: The current balance in the account (type: float). - `transactions`: A list of transaction descriptions (type: list of strings). - `__init__(self, account_number, account_holder, initial_balance)`: Initializes a new bank account with the given details. - `deposit(self, amount)`: Deposits the given amount into the account and appends a transaction to the `transactions` list. - `withdraw(self, amount)`: Withdraws the given amount from the account if sufficient balance is available, and appends a transaction to the `transactions` list. If insufficient balance, raise an `InsufficientFundsException` (you\'ll need to define this exception class). - `get_balance(self)`: Returns the current balance of the account. - `get_account_statement(self)`: Returns a string containing the list of transactions. 2. **Derived Classes**: - Create a derived class `SavingsAccount` that inherits from `BankAccount`. Add the following additional functionality: - `interest_rate`: The interest rate for the savings account (type: float). - `__init__(self, account_number, account_holder, initial_balance, interest_rate)`: Initializes a new savings account with the given details, including the interest rate. - `apply_interest(self)`: Applies the interest to the balance and appends a transaction. - Create a derived class `CheckingAccount` that inherits from `BankAccount`. Add the following additional functionality: - `overdraft_limit`: The overdraft limit for the checking account (type: float). - `__init__(self, account_number, account_holder, initial_balance, overdraft_limit)`: Initializes a new checking account with the given details, including the overdraft limit. - Override the `withdraw(self, amount)` method to allow overdrafts up to the limit. 3. **Private Attributes**: - Ensure that the `balance` attribute is private and can only be accessed or modified through class methods. 4. **Iterator for Transactions**: - Add an iterator to the `BankAccount` class to allow iteration over the transactions. Constraints: 1. Unique `account_number` is non-empty. 2. `initial_balance` for any account is non-negative. 3. `interest_rate` for `SavingsAccount` is a non-negative float. 4. `overdraft_limit` for `CheckingAccount` is a non-negative float. Expected Input and Output: - Input: Creation of `SavingsAccount` and `CheckingAccount` instances with appropriate details. - Output: Correct functionality for deposits, withdrawals, interest applications, and account statements. Example: ```python # Create Savings Account savings = SavingsAccount(\\"SV12345\\", \\"Alice\\", 1000.0, 0.02) savings.deposit(200.0) savings.withdraw(50.0) savings.apply_interest() print(savings.get_account_statement()) # Output: [\\"Deposited: 200.0\\", \\"Withdrew: 50.0\\", \\"Interest applied: 24.0\\"] # Create Checking Account checking = CheckingAccount(\\"CH12345\\", \\"Bob\\", 500.0, 200.0) checking.deposit(150.0) checking.withdraw(700.0) print(checking.get_account_statement()) # Output: [\\"Deposited: 150.0\\", \\"Withdrew: 700.0\\"] # Iterate over transactions for transaction in savings: print(transaction) ``` Implement the classes and methods as specified, ensuring proper use of OOP principles, handling of private attributes, and iterator functionality.","solution":"class InsufficientFundsException(Exception): pass class BankAccount: def __init__(self, account_number, account_holder, initial_balance): self.account_number = account_number self.account_holder = account_holder self._balance = initial_balance self.transactions = [] def deposit(self, amount): self._balance += amount self.transactions.append(f\\"Deposited: {amount:.2f}\\") def withdraw(self, amount): if self._balance < amount: raise InsufficientFundsException(\\"Insufficient funds for this transaction\\") self._balance -= amount self.transactions.append(f\\"Withdrew: {amount:.2f}\\") def get_balance(self): return self._balance def get_account_statement(self): return \\"n\\".join(self.transactions) def __iter__(self): return iter(self.transactions) class SavingsAccount(BankAccount): def __init__(self, account_number, account_holder, initial_balance, interest_rate): super().__init__(account_number, account_holder, initial_balance) self.interest_rate = interest_rate def apply_interest(self): interest = self._balance * self.interest_rate self._balance += interest self.transactions.append(f\\"Interest applied: {interest:.2f}\\") class CheckingAccount(BankAccount): def __init__(self, account_number, account_holder, initial_balance, overdraft_limit): super().__init__(account_number, account_holder, initial_balance) self.overdraft_limit = overdraft_limit def withdraw(self, amount): if self._balance + self.overdraft_limit < amount: raise InsufficientFundsException(\\"Insufficient funds and overdraft limit exceeded\\") self._balance -= amount self.transactions.append(f\\"Withdrew: {amount:.2f}\\")"},{"question":"# Hash and Authentication with BLAKE2 Objective Implement a class `Blake2Hasher` that encapsulates the functionality of BLAKE2b hashing with the following requirements: 1. Construct a hash object with the ability to: - Initialize with a key for keyed hashing. - Initialize with a salt for salted hashing. - Initialize with a personalization string. 2. Provide methods to: - Update the hash with byte data. - Retrieve the current digest in both byte and hexadecimal formats. - Return the size of the resulting digest. - Reset the hash object to its initial state. - Support hashing in tree mode. Requirements 1. **Constructor**: ```python def __init__(self, key: bytes = b\'\', salt: bytes = b\'\', person: bytes = b\'\', fanout: int = 1, depth: int = 1, leaf_size: int = 0, node_offset: int = 0, node_depth: int = 0, inner_size: int = 0, last_node: bool = False, digest_size: int = 64): ``` - `key`: Key for keyed hashing, up to 64 bytes. - `salt`: Salt for randomized hashing, up to 16 bytes. - `person`: Personalization string, up to 16 bytes. - `fanout`, `depth`, `leaf_size`, `node_offset`, `node_depth`, `inner_size`, `last_node`: Parameters for tree hashing. - `digest_size`: Size of output digest in bytes, must be  64. 2. **Methods**: ```python def update(self, data: bytes) -> None: Update the hash object with the provided byte data. def digest(self) -> bytes: Return the current digest as a bytes object. def hexdigest(self) -> str: Return the current digest as a hexadecimal string. def digest_size(self) -> int: Return the size of the resulting hash in bytes. def reset(self) -> None: Reset the hash object to its initial state. ``` Constraints - Your implementation should handle initialization and updates efficiently. - Make use of the `hashlib.blake2b()` constructor. - Ensure proper error handling for invalid parameters such as exceeding byte lengths for key, salt, or personalization strings. - No direct usage of external files or network connections is allowed. Example Usage ```python # Example of using Blake2Hasher in a typical scenario hasher = Blake2Hasher(key=b\'secret_key\', salt=b\'my_salt\', person=b\'my_app\') hasher.update(b\'Hello \') hasher.update(b\'world!\') digest = hasher.hexdigest() print(digest) # Should print the hexdigest of the provided data print(hasher.digest_size()) # Should print the digest size ```","solution":"import hashlib class Blake2Hasher: def __init__(self, key: bytes = b\'\', salt: bytes = b\'\', person: bytes = b\'\', fanout: int = 1, depth: int = 1, leaf_size: int = 0, node_offset: int = 0, node_depth: int = 0, inner_size: int = 0, last_node: bool = False, digest_size: int = 64): self.initial_params = { \'key\': key, \'salt\': salt, \'person\': person, \'fanout\': fanout, \'depth\': depth, \'leaf_size\': leaf_size, \'node_offset\': node_offset, \'node_depth\': node_depth, \'inner_size\': inner_size, \'last_node\': last_node, \'digest_size\': digest_size } self._init_hasher() def _init_hasher(self): self.hasher = hashlib.blake2b(**self.initial_params) def update(self, data: bytes) -> None: self.hasher.update(data) def digest(self) -> bytes: return self.hasher.digest() def hexdigest(self) -> str: return self.hasher.hexdigest() def get_digest_size(self) -> int: return self.hasher.digest_size def reset(self) -> None: self._init_hasher()"},{"question":"Coding Assessment Question # Objective Write a series of Python functions that simulate some of the behavior of the `PyLongObject` C-API. This will test your understanding of integer conversions, error handling, and use of integer properties in Python. # Description You are required to implement the following Python functions to convert numbers between different types. Each function should emulate the corresponding behavior of the `PyLongObject` functions described in the provided documentation. # Functions to Implement 1. `long_from_long(v: int) -> int` - Converts a Python integer to a simulated `PyLongObject`. - Should handle both positive and negative integers. 2. `long_from_unsigned_long(v: int) -> int` - Converts a positive Python integer to a simulated `PyLongObject`. - Should raise a `ValueError` if the input integer is negative. 3. `long_from_double(v: float) -> int` - Converts a Python float to a simulated `PyLongObject`, using only the integer part of the float. - Should raise a `ValueError` if the float is not finite (e.g., `NaN`, `inf`). 4. `long_from_string(s: str, base: int = 10) -> int` - Converts a string representation of an integer to a simulated `PyLongObject`. - The string can have leading/trailing spaces and underscores between the digits. The base should be between 2 and 36. 5. `long_to_unsigned_long(v: int) -> int` - Converts a simulated `PyLongObject` back to a positive integer (`unsigned long` equivalent). - Should raise an `OverflowError` if the integer is negative. 6. `long_to_double(v: int) -> float` - Converts a simulated `PyLongObject` back to a C `double`. - Should handle large integers correctly, raising an `OverflowError` if they are out of range for a `double`. # Constraints - Your functions should handle Python\'s arbitrary-precision integers correctly. - Each function should include appropriate error handling mimicking the C-API behavior, raising appropriate Python exceptions when necessary. - You may assume inputs to functions are valid Python types (e.g., `int`, `float`, `str`), but you must validate value constraints (e.g., non-negative for unsigned types). # Examples ```python # Examples for long_from_long print(long_from_long(123456789)) # Expected output: 123456789 print(long_from_long(-987654321)) # Expected output: -987654321 # Examples for long_from_unsigned_long print(long_from_unsigned_long(123456789)) # Expected output: 123456789 print(long_from_unsigned_long(-123)) # Expected output: raises ValueError # Examples for long_from_double print(long_from_double(123.456)) # Expected output: 123 print(long_from_double(-987.654)) # Expected output: -987 print(long_from_double(math.inf)) # Expected output: raises ValueError # Examples for long_from_string print(long_from_string(\\" 1010_0001 \\", base=2)) # Expected output: 161 print(long_from_string(\\"FF\\", base=16)) # Expected output: 255 # Examples for long_to_unsigned_long print(long_to_unsigned_long(123456789)) # Expected output: 123456789 print(long_to_unsigned_long(-987654321)) # Expected output: raises OverflowError # Examples for long_to_double print(long_to_double(123456789)) # Expected output: 123456789.0 ``` Your implementation should follow this structure, ensuring that error handling and input validation are robust and mimic the C-API as closely as possible. # Note This question assesses your understanding of integer conversion and error handling, simulating a Python-C API environment. Be thorough in handling edge cases and test your functions comprehensively.","solution":"def long_from_long(v: int) -> int: Converts a Python integer to a simulated PyLongObject. Should handle both positive and negative integers. return v def long_from_unsigned_long(v: int) -> int: Converts a positive Python integer to a simulated PyLongObject. Should raise a ValueError if the input integer is negative. if v < 0: raise ValueError(\\"Cannot convert negative value to unsigned long\\") return v def long_from_double(v: float) -> int: Converts a Python float to a simulated PyLongObject, using only the integer part of the float. Should raise a ValueError if the float is not finite. if not (v == v and v != float(\'inf\') and v != -float(\'inf\')): raise ValueError(\\"Cannot convert non-finite float to long\\") return int(v) def long_from_string(s: str, base: int = 10) -> int: Converts a string representation of an integer to a simulated PyLongObject. The string can have leading/trailing spaces and underscores between the digits. The base should be between 2 and 36. return int(s.replace(\'_\', \'\').strip(), base) def long_to_unsigned_long(v: int) -> int: Converts a simulated PyLongObject back to a positive integer (unsigned long equivalent). Should raise an OverflowError if the integer is negative. if v < 0: raise OverflowError(\\"Cannot convert negative value to unsigned long\\") return v def long_to_double(v: int) -> float: Converts a simulated PyLongObject back to a C double. Should handle large integers correctly, raising an OverflowError if they are out of range for a double. try: return float(v) except OverflowError as e: raise OverflowError(\\"Integer value is out of range for a double\\") from e"},{"question":"# WAV File Analysis and Manipulation You are tasked with creating a Python script that analyzes a WAV file and writes a portion of it as a new WAV file using Python\'s `wave` module. Instructions: 1. **Read WAV File**: - Write a function `read_wav_info(file_path)` that: - Takes the path of a WAV file as input. - Returns a dictionary containing: - `\'nchannels\'`: Number of audio channels. - `\'sampwidth\'`: Sample width in bytes. - `\'framerate\'`: Sampling frequency. - `\'nframes\'`: Number of audio frames. 2. **Extract and Write WAV Content**: - Write a function `extract_and_write_wav(input_file, output_file, start_frame, end_frame)` that: - Takes the path to the input WAV file, the path to the output WAV file, and two integers `start_frame` and `end_frame` indicating the range of frames to extract. - Reads the input file using the `wave` module. - Writes only the frames from `start_frame` to `end_frame` to the new output file using the `wave` module. - Uses the same parameters for the output file as the input file (number of channels, sample width, and frame rate). Constraints: - Ensure that `start_frame` and `end_frame` are within the valid range of frames in the input WAV file. - If `start_frame` >= `end_frame`, your function should raise a `ValueError` with the message \\"Invalid frame range\\". Example Usage: ```python # Example WAV file info extraction file_info = read_wav_info(\\"input.wav\\") print(file_info) # Output: # {\'nchannels\': 2, \'sampwidth\': 2, \'framerate\': 44100, \'nframes\': 441000} # Example WAV file extraction extract_and_write_wav(\\"input.wav\\", \\"output.wav\\", 1000, 2000) ``` Notes: 1. You may assume the input file paths are valid and that the files exist. 2. You may also assume the WAV files conform to the \\"WAVE_FORMAT_PCM\\" specification. This task will test your ability to read and manipulate audio files using the `wave` module in Python. Be sure to handle any edge cases that could arise from the constraints provided.","solution":"import wave def read_wav_info(file_path): Reads the WAV file and returns a dictionary of its properties. Args: file_path (str): Path to the WAV file. Returns: dict: A dictionary containing \'nchannels\', \'sampwidth\', \'framerate\', and \'nframes\'. with wave.open(file_path, \'rb\') as wav_file: params = wav_file.getparams() return { \'nchannels\': params.nchannels, \'sampwidth\': params.sampwidth, \'framerate\': params.framerate, \'nframes\': params.nframes } def extract_and_write_wav(input_file, output_file, start_frame, end_frame): Extracts a portion of the WAV file and writes it to a new file. Args: input_file (str): Path to the input WAV file. output_file (str): Path to the output WAV file. start_frame (int): Starting frame for extraction. end_frame (int): Ending frame for extraction. Raises: ValueError: If start_frame >= end_frame. if start_frame >= end_frame: raise ValueError(\\"Invalid frame range\\") with wave.open(input_file, \'rb\') as in_wav: params = in_wav.getparams() if end_frame > params.nframes: raise ValueError(\\"end_frame exceeds total number of frames\\") in_wav.setpos(start_frame) frames = in_wav.readframes(end_frame - start_frame) with wave.open(output_file, \'wb\') as out_wav: out_wav.setparams(params) out_wav.writeframes(frames)"},{"question":"You are provided with several helper functions available in the `PySlice` type from Python. These functions enable creating, checking, and manipulating slice objects. Your task is to implement a function that extracts sub-sequences from a given list using slice objects. Here are the specifications: **Function Definition:** ```python def extract_sublists(sequence: list, slices: list) -> list: Extracts and returns sub-sequences from a given list using a list of slice objects. Args: - sequence (list): The input list from which sub-sequences will be extracted. - slices (list): A list of slice objects. Returns: - list: A list containing the extracted sub-sequences. # Your Code Here ``` **Input:** - `sequence`: A list of integers. - `slices`: A list of slice objects. **Output:** - A list where each element is a sublist of `sequence` obtained by applying each slice object in `slices`. **Constraints:** - Each element in the `sequence` list is an integer, and the list can have up to 10,000 elements. - The `slices` list can contain up to 100 slice objects. - The sublists obtained by slicing should correctly handle out-of-bounds indices, using the standard Python slicing behavior. **Example Usage:** ```python sequence = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] slices = [slice(1, 5, 2), slice(2, 8, 3), slice(0, 10, 4)] result = extract_sublists(sequence, slices) print(result) # Output: [[1, 3], [2, 5], [0, 4, 8]] ``` **Hints:** - You can use Python\'s built-in slicing capability directly, as it internally works using similar principles as the PySlice functions. - Ensure you handle potential out-of-range slice indices gracefully. Implement the `extract_sublists` function such that it correctly applies each slice object to the input sequence and returns the corresponding sublists.","solution":"def extract_sublists(sequence: list, slices: list) -> list: Extracts and returns sub-sequences from a given list using a list of slice objects. Args: - sequence (list): The input list from which sub-sequences will be extracted. - slices (list): A list of slice objects. Returns: - list: A list containing the extracted sub-sequences. return [sequence[s] for s in slices]"},{"question":"# Seaborn and Matplotlib Plot Customization Objective: Create a function `custom_plots` that accepts data and various plotting parameters to generate and customize a seaborn bar plot and line plot. Function Signature: ```python def custom_plots(bar_data: dict, line_data: dict, theme_style: str, palette: str, custom_rc: dict): Generate and customize seaborn bar and line plots. Parameters: - bar_data (dict): A dictionary containing the \'x\' and \'y\' data for the bar plot. Example: {\'x\': [\\"A\\", \\"B\\", \\"C\\"], \'y\': [3, 1, 2]} - line_data (dict): A dictionary containing the \'x\' and \'y\' data for the line plot. Example: {\'x\': [0, 1, 2, 3], \'y\': [2, 3, 5, 7]} - theme_style (str): Style to use for the seaborn theme (e.g., \\"whitegrid\\"). - palette (str): Color palette to use for the seaborn theme (e.g., \\"pastel\\"). - custom_rc (dict): Custom matplotlib rc parameters to apply. Output: Displays the generated plots. ``` Constraints: 1. The keys in `bar_data` and `line_data` are guaranteed to be \'x\' and \'y\'. 2. `theme_style` is one of the seaborn supported styles. 3. `palette` is one of the seaborn supported palettes. 4. `custom_rc` is a dictionary of valid matplotlib rc parameters. Task: 1. Set the seaborn theme using `sns.set_theme()` with the given `theme_style` and `palette`. 2. Create a bar plot using the `bar_data` with seaborn. 3. Create a line plot using the `line_data` with seaborn. 4. Apply the additional custom rc parameters using the `rc` parameter in `sns.set_theme()`. 5. Display both plots in a single figure. Example: ```python bar_data_example = {\'x\': [\\"A\\", \\"B\\", \\"C\\"], \'y\': [3, 1, 2]} line_data_example = {\'x\': [0, 1, 2, 3], \'y\': [2, 3, 5, 7]} theme_style_example = \\"whitegrid\\" palette_example = \\"pastel\\" custom_rc_example = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} custom_plots(bar_data_example, line_data_example, theme_style_example, palette_example, custom_rc_example) ``` This example sets a `whitegrid` theme with a `pastel` palette, generates a bar plot and a line plot with the given data, applies custom rc parameters to hide the right and top spines, and displays the combined results. The function should demonstrate your understanding of seaborn plotting, theming, and integrating custom matplotlib parameters.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_plots(bar_data, line_data, theme_style, palette, custom_rc): Generate and customize seaborn bar and line plots. Parameters: - bar_data (dict): A dictionary containing the \'x\' and \'y\' data for the bar plot. Example: {\'x\': [\\"A\\", \\"B\\", \\"C\\"], \'y\': [3, 1, 2]} - line_data (dict): A dictionary containing the \'x\' and \'y\' data for the line plot. Example: {\'x\': [0, 1, 2, 3], \'y\': [2, 3, 5, 7]} - theme_style (str): Style to use for the seaborn theme (e.g., \\"whitegrid\\"). - palette (str): Color palette to use for the seaborn theme (e.g., \\"pastel\\"). - custom_rc (dict): Custom matplotlib rc parameters to apply. Output: Displays the generated plots. # Set the seaborn theme with the given parameters sns.set_theme(style=theme_style, palette=palette, rc=custom_rc) # Create a new figure plt.figure(figsize=(12, 6)) # Create the bar plot plt.subplot(1, 2, 1) sns.barplot(x=bar_data[\'x\'], y=bar_data[\'y\']) plt.title(\\"Customized Bar Plot\\") # Create the line plot plt.subplot(1, 2, 2) sns.lineplot(x=line_data[\'x\'], y=line_data[\'y\']) plt.title(\\"Customized Line Plot\\") # Display the plots plt.tight_layout() plt.show()"},{"question":"Objective Implement a function that fetches the content from a given URL, parses the content for URLs within, and handles potential URL errors effectively. Problem Statement You are tasked with writing a Python function `fetch_and_parse_urls` that takes a URL as input, makes an HTTP GET request to fetch the content of the webpage, extracts all URLs from the content, and returns them as a list. The function should also handle exceptions gracefully and return an appropriate message or empty list in case of errors. Function Signature ```python def fetch_and_parse_urls(url: str) -> list: pass ``` Expected Input and Output - **Input**: A string representing the URL of a webpage. - **Output**: A list of strings, where each string is a URL found in the webpage content. If an error occurs (e.g., invalid URL, network issues), return an empty list or an appropriate error message. Constraints - Use the `urllib` module for making the HTTP request. - You may assume the webpage content is small enough to be fetched and parsed in memory. - URLs within the content can be identified using regular expressions. Performance Requirements - The function should efficiently handle the request and parsing without unnecessary delays. Proper handling of errors and exceptions is essential to ensure robustness. Example ```python # Example usage: urls = fetch_and_parse_urls(\\"http://example.com\\") print(urls) # Outputs a list of URLs found on the example.com webpage or an empty list if an error occurs ``` **Note**: It is crucial to handle various scenarios like network errors, invalid URLs, and content parsing exceptions.","solution":"import urllib.request import urllib.error import re def fetch_and_parse_urls(url: str) -> list: Fetches the content from the given URL and extracts all URLs from that content. Args: url (str): The URL of the webpage to fetch and parse. Returns: list: A list of URLs found in the webpage content, or an empty list in case of any error. try: response = urllib.request.urlopen(url) # Open the URL html_content = response.read().decode(\'utf-8\') # Read and decode the page content # Using regex to find all URLs in the content urls = re.findall(r\'href=[\\"\'](https?://[^s\\"\'<>]+)\', html_content) return urls except (urllib.error.URLError, urllib.error.HTTPError, ValueError) as e: # Handle possible errors like invalid URL, network issues, etc. print(f\\"Error fetching or parsing the URL: {e}\\") return []"},{"question":"**Title:** Manipulating Arrays with Python\'s `array` Module **Objective:** Write a function called `array_manipulation` that performs a series of operations on an array of integers using Python\'s `array` module. This will test your understanding of initializing arrays, type constraints, and basic array operations. **Function Specification:** ```python def array_manipulation(operations): Perform a series of operations on an array of integers. Args: - operations (list): A list of tuples where the first element of each tuple is a string indicating the operation to perform, and the subsequent elements are parameters for that operation. Supported operations: - (\'create\', typecode, list_of_integers): Create an array with the given typecode and initialize it with list_of_integers. - (\'append\', value): Append an integer to the array. - (\'extend\', list_of_integers): Extend the array with a list of integers. - (\'insert\', index, value): Insert an integer value at the specified index. - (\'remove\', value): Remove the first occurrence of the integer value. - (\'pop\', index): Remove and return the integer at the specified index. - (\'reverse\'): Reverse the order of the array. - (\'tobytes\'): Convert the array to a bytes object and return it. - (\'tolist\'): Convert the array to a list and return it. Returns: - The final array after performing all operations for all functions except \'tobytes\' and \'tolist\'. ``` **Input and Constraints:** - The array must be created with type code `\'i\'` (signed integer) or `\'I\'` (unsigned integer). - The operations must follow a valid sequence, i.e., a create operation must be the first operation. - Subsequent operations must be applied to the array created in the initial step. - For invalid operations (e.g., removing an item not present in the array), the function should raise appropriate exceptions. **Examples:** ```python operations = [ (\'create\', \'i\', [1, 2, 3, 4]), (\'append\', 5), (\'extend\', [6, 7]), (\'insert\', 2, 10), (\'remove\', 4), (\'reverse\') ] print(array_manipulation(operations)) # Output should be array(\'i\', [7, 6, 5, 10, 3, 2, 1]) operations = [ (\'create\', \'i\', [1, 2, 3]), (\'tobytes\',) ] print(array_manipulation(operations)) # Output should be b\'x01x00x00x00x02x00x00x00x03x00x00x00\' operations = [ (\'create\', \'i\', [1, 2, 3]), (\'tolist\',) ] print(array_manipulation(operations)) # Output should be [1, 2, 3] ``` **Notes:** - Remember to handle different types of operations appropriately. - Ensure the function is robust and handles exceptions properly. - Do not use any external libraries other than Python\'s standard library.","solution":"import array def array_manipulation(operations): Perform a series of operations on an array of integers. Args: - operations (list): A list of tuples where the first element of each tuple is a string indicating the operation to perform, and the subsequent elements are parameters for that operation. Supported operations: - (\'create\', typecode, list_of_integers): Create an array with the given typecode and initialize it with list_of_integers. - (\'append\', value): Append an integer to the array. - (\'extend\', list_of_integers): Extend the array with a list of integers. - (\'insert\', index, value): Insert an integer value at the specified index. - (\'remove\', value): Remove the first occurrence of the integer value. - (\'pop\', index): Remove and return the integer at the specified index. - (\'reverse\'): Reverse the order of the array. - (\'tobytes\'): Convert the array to a bytes object and return it. - (\'tolist\'): Convert the array to a list and return it. Returns: - The final array after performing all operations for all functions except \'tobytes\' and \'tolist\'. arr = None for operation in operations: op = operation[0] if op == \'create\': typecode, init_list = operation[1], operation[2] arr = array.array(typecode, init_list) elif op == \'append\': arr.append(operation[1]) elif op == \'extend\': arr.extend(operation[1]) elif op == \'insert\': index, value = operation[1], operation[2] arr.insert(index, value) elif op == \'remove\': arr.remove(operation[1]) elif op == \'pop\': arr.pop(operation[1]) elif op == \'reverse\': arr.reverse() elif op == \'tobytes\': return arr.tobytes() elif op == \'tolist\': return arr.tolist() else: raise ValueError(f\\"Unsupported operation: {op}\\") return arr"},{"question":"Problem Statement You are working on a project that requires testing the numerical accuracy and stability of tensor operations in PyTorch. Your goal is to implement a function named `validate_tensor_operations` which uses utilities from the `torch.testing` module to verify the correctness of tensor operations. Write the function `validate_tensor_operations` to perform the following tasks: 1. **Generate tensors**: Create two random tensors using `torch.testing.make_tensor` with specified properties such as shape, dtype, and device. 2. **Perform operations**: Conduct a set of operations on these tensors, such as addition, subtraction, and multiplication. 3. **Assert tensor closeness**: Use `torch.testing.assert_allclose` to verify that the results of these operations are close to expected values within a specified tolerance. Function Signature ```python import torch def validate_tensor_operations(shape: tuple, dtype: torch.dtype, device: torch.device, rtol: float, atol: float): Validates tensor operations by generating two random tensors and verifying the results of certain operations. Parameters: shape (tuple): Shape of the tensors to be generated. dtype (torch.dtype): Data type of the tensors. device (torch.device): Device where the tensors will be located (e.g., \'cpu\' or \'cuda\'). rtol (float): Relative tolerance for the closeness check. atol (float): Absolute tolerance for the closeness check. Returns: bool: True if all validations pass, False otherwise. ``` Input and Output - **Input**: - `shape` (tuple): Example (2, 3) - The shape of the tensors to be generated. - `dtype` (torch.dtype): Example torch.float32 - The data type of the tensors. - `device` (torch.device): Example torch.device(\'cpu\') - The device on which the tensors should be created. - `rtol` (float): Example 1e-05 - The relative tolerance parameter for the closeness check. - `atol` (float): Example 1e-08 - The absolute tolerance parameter for the closeness check. - **Output**: - Returns `True` if all operations are validated successfully; otherwise, returns `False`. Example Usage ```python import torch shape = (3, 3) dtype = torch.float32 device = torch.device(\'cpu\') rtol = 1e-05 atol = 1e-08 result = validate_tensor_operations(shape, dtype, device, rtol, atol) print(result) # Expected output: True if validations pass, False otherwise ``` Constraints - You should use the `torch.testing.make_tensor` to generate the tensors. - Use `torch.testing.assert_allclose` for validations. - Ensure that the function works for both CPU and CUDA devices. Additional Notes - Handle exceptions and edge cases gracefully such as mismatched tensor shapes and invalid data types. - Ensure the function is optimized for performance and does not run indefinitely. By correctly implementing this function, you demonstrate a deep understanding of PyTorch tensor operations and validation techniques using the `torch.testing` module.","solution":"import torch from torch.testing import make_tensor, assert_allclose def validate_tensor_operations(shape: tuple, dtype: torch.dtype, device: torch.device, rtol: float, atol: float): Validates tensor operations by generating two random tensors and verifying the results of certain operations. Parameters: shape (tuple): Shape of the tensors to be generated. dtype (torch.dtype): Data type of the tensors. device (torch.device): Device where the tensors will be located (e.g., \'cpu\' or \'cuda\'). rtol (float): Relative tolerance for the closeness check. atol (float): Absolute tolerance for the closeness check. Returns: bool: True if all validations pass, False otherwise. try: # Generate two random tensors tensor1 = make_tensor(shape, dtype=dtype, device=device) tensor2 = make_tensor(shape, dtype=dtype, device=device) # Perform operations sum_tensor = tensor1 + tensor2 sub_tensor = tensor1 - tensor2 mul_tensor = tensor1 * tensor2 # Generate expected results using a manual method to ensure they are the same expected_sum = tensor1.add(tensor2) expected_sub = tensor1.sub(tensor2) expected_mul = tensor1.mul(tensor2) # Validate results assert_allclose(sum_tensor, expected_sum, rtol=rtol, atol=atol) assert_allclose(sub_tensor, expected_sub, rtol=rtol, atol=atol) assert_allclose(mul_tensor, expected_mul, rtol=rtol, atol=atol) return True except AssertionError: return False"},{"question":"# Advanced JSON Data Handling with Custom Encoders and Decoders In this task, you are required to implement custom JSON encoder and decoder classes to handle specific data types in a JSON object. You need to create a custom encoder and decoder to handle complex numbers and enumerations correctly. Follow the instructions below carefully. Custom Data Types 1. **Complex Number**: Represented as a dictionary with keys `\\"__complex__\\": true`, `\\"real\\"` (real part), and `\\"imag\\"` (imaginary part). 2. **Enumerations**: Enums should be serialized as their string names. Requirements 1. Implement a custom encoder `CustomJSONEncoder` that extends `json.JSONEncoder` to handle complex numbers and enums. 2. Implement a custom decoder `custom_decoder` function that converts serialized complex numbers back to `complex` instances. 3. Use the custom encoder and decoder to serialize and deserialize a given Python dictionary object containing complex numbers and enums. Input - A Python dictionary containing complex numbers and enums. Output - The serialized JSON string. - The deserialized Python dictionary, ensuring that the complex numbers and enums are correctly restored to their original types. Example ```python import json from enum import Enum # Define an enumeration for the example class Status(Enum): ACTIVE = 1 INACTIVE = 0 # Example Python dictionary containing complex numbers and enums data = { \\"number\\": 3 + 4j, \\"status\\": Status.ACTIVE, \\"nested\\": { \\"value\\": 1.1, \\"complex_list\\": [2 + 3j, Status.INACTIVE] } } class CustomJSONEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, complex): return {\\"__complex__\\": True, \\"real\\": obj.real, \\"imag\\": obj.imag} elif isinstance(obj, Enum): return obj.name return super().default(obj) def custom_decoder(dct): if \\"__complex__\\" in dct: return complex(dct[\\"real\\"], dct[\\"imag\\"]) # Additional logic for other custom types can be added here return dct # Serialize the example data using the custom encoder json_str = json.dumps(data, cls=CustomJSONEncoder) # Deserialize the JSON string back to a Python dictionary using the custom decoder data_restored = json.loads(json_str, object_hook=custom_decoder) print(json_str) # Output the serialized JSON string print(data_restored) # Output the deserialized Python dictionary # Expected output # Serialized JSON string containing transformed complex numbers and enums # Deserialized Python dictionary with complex numbers restored # Ensure the output matches the expected result assert data == data_restored ``` # Submission Submit the `CustomJSONEncoder` class and the `custom_decoder` function. You can test your implementation using the example provided above. Ensure that your solution handles any nested structures and other edge cases appropriately.","solution":"import json from enum import Enum # Define an enumeration for the example class Status(Enum): ACTIVE = 1 INACTIVE = 0 # Example Python dictionary containing complex numbers and enums data = { \\"number\\": 3 + 4j, \\"status\\": Status.ACTIVE, \\"nested\\": { \\"value\\": 1.1, \\"complex_list\\": [2 + 3j, Status.INACTIVE] } } class CustomJSONEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, complex): return {\\"__complex__\\": True, \\"real\\": obj.real, \\"imag\\": obj.imag} elif isinstance(obj, Enum): return obj.name return super().default(obj) def custom_decoder(dct): if \\"__complex__\\" in dct: return complex(dct[\\"real\\"], dct[\\"imag\\"]) return dct # Serialize the example data using the custom encoder json_str = json.dumps(data, cls=CustomJSONEncoder) # Deserialize the JSON string back to a Python dictionary using the custom decoder data_restored = json.loads(json_str, object_hook=custom_decoder) # Ensure the output matches the expected result data[\'status\'] = data[\'status\'].name data[\'nested\'][\'complex_list\'][1] = data[\'nested\'][\'complex_list\'][1].name print(json_str) # Output the serialized JSON string print(data_restored) # Output the deserialized Python dictionary"},{"question":"The provided documentation describes two modules: \\"wave\\" for handling WAV audio files and \\"colorsys\\" for color system conversions. The problem below requires the use of both modules to demonstrate proficiency with multimedia data processing in Python. # Problem Statement **Audio Color Visualizer** You are tasked with creating a visualization of audio data by converting audio amplitudes to colors. To achieve this, follow these steps: 1. **Read a WAV File**: - Use the `wave` module to read a specified WAV file. - Extract the audio frames as amplitude values. 2. **Normalize Amplitude Values**: - Convert the raw amplitude values to a normalized range suitable for color representation. 3. **Convert Amplitudes to Colors**: - Use the `colorsys` module to map normalized amplitude values to colors in the RGB color space. You can use the hue-saturation-value (HSV) color space for more intuitive color mapping, by mapping amplitude to Hue. 4. **Output Colors**: - Return a list of RGB color tuples corresponding to the audio frames. # Function Signature The function should be named `audio_to_colors` and have the following signature: ```python def audio_to_colors(file_path: str) -> List[Tuple[int, int, int]]: pass ``` # Input - `file_path` (str): A string representing the path to the input WAV file. # Output - A list of tuples where each tuple represents the RGB value (three integers between 0 and 255) corresponding to an audio frame. # Constraints - The WAV file can be assumed to be in a mono audio format. - You may assume that the amplitude values do not exceed the standard range for WAV files (e.g., 16-bit signed integers). # Example Usage ```python colors = audio_to_colors(\'example.wav\') for color in colors[:10]: print(color) ``` # Notes - You may use the Python standard library for reading files and basic operations. - This problem requires the use of numpy for efficient numerical operations, particularly for handling the audio data and normalizations. **Hint**: 1. You may find it useful to convert between different numeric types for the audio data processing. 2. Think about a linear mapping function to convert the amplitude range to a hue range (0 to 1 for HSV). **Additional Resources**: - [Wave Module Documentation](https://docs.python.org/3/library/wave.html) - [Colorsys Module Documentation](https://docs.python.org/3/library/colorsys.html)","solution":"import wave import numpy as np import colorsys from typing import List, Tuple def audio_to_colors(file_path: str) -> List[Tuple[int, int, int]]: # Open the WAV file with wave.open(file_path, \'rb\') as wav_file: frames = wav_file.readframes(wav_file.getnframes()) sample_width = wav_file.getsampwidth() num_frames = wav_file.getnframes() # Convert frames to numpy array if sample_width == 2: # 16-bit WAV files dtype = np.int16 else: raise ValueError(\\"Unsupported sample width\\") amplitude_values = np.frombuffer(frames, dtype=dtype) # Normalize amplitude values to range [0, 1] max_amplitude = np.max(np.abs(amplitude_values)) normalized_amplitudes = amplitude_values / max_amplitude # Convert normalized amplitudes to colors colors = [] for na in normalized_amplitudes: hue = (na + 1) / 2 # Convert normalized amplitude from [-1, 1] to [0, 1] r, g, b = colorsys.hsv_to_rgb(hue, 1.0, 1.0) colors.append((int(r * 255), int(g * 255), int(b * 255))) return colors"},{"question":"Objective: Implement a function that accepts an HTTP status code and returns a tuple containing: - The textual phrase associated with that status code. - The long description of the status code. - A boolean indicating if the status code represents a client error (4xx) or a server error (5xx). Function Signature: ```python def analyze_http_status_code(status_code: int) -> tuple: pass ``` Input: - **status_code** (int): An integer representing the HTTP status code. Output: - Returns a tuple containing: - **phrase** (str): The textual phrase associated with the provided status code (e.g., \\"OK\\" for 200). - **description** (str): The long description of the status code. - **is_error** (bool): A boolean indicating whether the status code is a client error (status_code ranging from 400-499) or server error (status_code ranging from 500-599). Constraints: - The status_code will be a valid HTTP status code as per the `http.HTTPStatus` enum. Example: ```python from http import HTTPStatus def analyze_http_status_code(status_code): status = HTTPStatus(status_code) phrase = status.phrase description = status.description is_error = 400 <= status_code < 600 return (phrase, description, is_error) # Sample usage and output analyze_http_status_code(200) # Output: (\'OK\', \'Request fulfilled, document follows\', False) analyze_http_status_code(404) # Output: (\'Not Found\', \'Nothing matches the given URI\', True) analyze_http_status_code(500) # Output: (\'Internal Server Error\', \'The server encountered an unexpected condition ...\', True) ``` Performance Requirements: - The function should efficiently map status codes to their phrases and descriptions using the provided `http.HTTPStatus` enum. - Use the capabilities of the `http` package without building manual mappings for the HTTP status codes. Good luck!","solution":"from http import HTTPStatus def analyze_http_status_code(status_code: int) -> tuple: Returns the textual phrase, long description, and a boolean indicating if the status code represents a client error (4xx) or a server error (5xx). Parameters: status_code (int): An integer representing the HTTP status code. Returns: tuple: A tuple containing the textual phrase, long description, and a boolean indicating client/server error. status = HTTPStatus(status_code) phrase = status.phrase description = status.description is_error = 400 <= status_code < 600 return (phrase, description, is_error)"},{"question":"# Environment Variable Manipulation Challenge Problem Statement: You are required to write a Python function that manipulates environment variables. The function should support adding, modifying, and deleting environment variables. Specifically, you need to implement a function `manipulate_env(action, key, value=None)` that performs the following actions: 1. **Add/Modify Environment Variable**: If the action is \\"add\\" or \\"modify\\", the function should add a new environment variable with the given key and value if it does not already exist or modify the existing environment variable with the given key to the new value. 2. **Delete Environment Variable**: If the action is \\"delete\\", the function should delete the environment variable with the given key. 3. **Retrieve Environment Variable**: If the action is \\"get\\", the function should return the value of the environment variable with the given key. If the variable does not exist, return `None`. Input Format: - `action` (str): A string representing the action to perform. It can be one of the following: \\"add\\", \\"modify\\", \\"delete\\", \\"get\\". - `key` (str): The key of the environment variable. - `value` (str, optional): The value of the environment variable. This is only needed for \\"add\\" and \\"modify\\" actions. Output Format: - For the \\"get\\" action, return the value of the environment variable or `None` if the key does not exist. - For \\"add\\", \\"modify\\", and \\"delete\\" actions, return `None`. Constraints: - The `key` and `value` will be non-empty strings. - The `key` will contain only alphanumeric characters and underscores (`_`). Example Usage: ```python # Adding a new environment variable manipulate_env(\\"add\\", \\"MY_VAR\\", \\"my_value\\") # Modifying an existing environment variable manipulate_env(\\"modify\\", \\"MY_VAR\\", \\"new_value\\") # Retrieving the value of an environment variable print(manipulate_env(\\"get\\", \\"MY_VAR\\")) # Output: \'new_value\' # Deleting an environment variable manipulate_env(\\"delete\\", \\"MY_VAR\\") print(manipulate_env(\\"get\\", \\"MY_VAR\\")) # Output: None ``` Notes: - Utilize the `os` module to handle environment variables. - Ensure that changes to environment variables in your function affect the current process environment. Implementation: ```python import os def manipulate_env(action, key, value=None): if action == \\"add\\" or action == \\"modify\\": os.environ[key] = value elif action == \\"delete\\": if key in os.environ: del os.environ[key] elif action == \\"get\\": return os.environ.get(key, None) ``` This question assesses the student\'s ability to manipulate environment variables using the `os.environ` dictionary, handle different types of actions, and understand basic Python conditional logic.","solution":"import os def manipulate_env(action, key, value=None): Manipulates environment variables based on the given action. Parameters: action (str): The action to perform. Valid actions are \\"add\\", \\"modify\\", \\"delete\\", and \\"get\\". key (str): The key of the environment variable. value (str, optional): The value of the environment variable for \\"add\\" and \\"modify\\" actions. Returns: None for \\"add\\", \\"modify\\", and \\"delete\\" actions. The value of the environment variable for \\"get\\" action or `None` if it does not exist. if action in [\\"add\\", \\"modify\\"]: os.environ[key] = value elif action == \\"delete\\": if key in os.environ: del os.environ[key] elif action == \\"get\\": return os.environ.get(key, None) return None"},{"question":"# Asynchronous Programming with `asyncio` You\'ve been given the task to develop an application that manages multiple asynchronous tasks concurrently while handling blocking operations effectively. The application should also be capable of detecting common issues such as coroutines that are never awaited and exceptions that are never retrieved. Requirements 1. **Function Implementations:** - Implement an async function `perform_tasks` which accepts a list of tasks (coroutine functions) and executes them concurrently. - Implement a function `run_blocking_tasks` scheduled by `perform_tasks` to handle CPU-bound blocking tasks using `loop.run_in_executor`. 2. **Debug Mode:** - Ensure that debug mode is enabled and configure logging and warning settings to help catch any issues during development. Input and Output - The `perform_tasks` function should take the following input: ```python async def perform_tasks(tasks: List[Callable[..., Awaitable[None]]]) -> None: pass ``` - `tasks` is a list of coroutine functions that need to be executed concurrently. - The application should log: - Any coroutine that is created but not awaited. - Any exceptions that are not retrieved. - Handle blocking or CPU-bound operations efficiently without blocking the main event loop. Constraints - You must use `asyncio` and follow best practices for threading and concurrency as outlined in the documentation. - The logging level should be set to `DEBUG` for asyncio. - The application should detect and log forgotten `await` and uncaught exceptions. Performance Requirements - Ensure that the application can handle a list of at least 100 tasks concurrently without significant delay or blocking. Example Usage ```python import asyncio from typing import List, Callable, Awaitable async def example_task_1(): await asyncio.sleep(1) print(\\"Task 1 completed\\") async def example_task_2(): await asyncio.sleep(2) print(\\"Task 2 completed\\") async def blocking_task(): for _ in range(10**7): pass print(\\"Blocking task completed\\") async def perform_tasks(tasks: List[Callable[..., Awaitable[None]]]) -> None: # Enable debug mode and configure logging and warnings asyncio.run(debug=True) # or other methods to enable debug mode import logging import warnings logging.basicConfig(level=logging.DEBUG) warnings.simplefilter(\\"default\\", ResourceWarning) # Run tasks concurrently await asyncio.gather(*[task() for task in tasks]) def run_blocking_tasks(): loop = asyncio.get_event_loop() with concurrent.futures.ThreadPoolExecutor() as pool: loop.run_in_executor(pool, blocking_task) # Testing the async application tasks = [example_task_1, example_task_2, run_blocking_tasks] asyncio.run(perform_tasks(tasks)) ``` Complete the implementation of `perform_tasks` and `run_blocking_tasks` functions to meet the above requirements.","solution":"import asyncio import logging import warnings from typing import List, Callable, Awaitable import concurrent.futures async def perform_tasks(tasks: List[Callable[..., Awaitable[None]]]) -> None: # Enable debug mode and configure logging and warnings asyncio.get_running_loop().set_debug(True) logging.basicConfig(level=logging.DEBUG) warnings.simplefilter(\\"default\\", ResourceWarning) # Run tasks concurrently await asyncio.gather(*[task() for task in tasks]) def run_blocking_tasks(task: Callable): loop = asyncio.get_event_loop() with concurrent.futures.ThreadPoolExecutor() as pool: return loop.run_in_executor(pool, task)"},{"question":"# Handling Multiple Signals in a Long-Running Process **Objective:** You are required to implement a robust signal handling mechanism in a long-running Python application. Your task is to overwrite the default signal handling to gracefully handle `SIGINT` (Ctrl+C) and `SIGTERM` (termination signal). Additionally, set up an interval timer to periodically log a status message, using `SIGALRM`. # Problem Statement: 1. Implement custom signal handlers for `SIGINT` and `SIGTERM` to gracefully terminate the process by setting a flag. 2. Implement an interval timer using `SIGALRM` that logs a status message every 5 seconds. 3. The main process should run in an infinite loop. At each iteration, it should check if the termination flag has been set and exit the loop if so. Additionally, it should pause for a short duration (use `time.sleep(1)`) to reduce CPU usage. 4. Ensure that you handle any potential exceptions raised due to signal handlers. # Requirements: 1. **Input:** No explicit input is required. 2. **Output:** The program should print the status message at regular intervals and should print a termination message when either `SIGINT` or `SIGTERM` is received. 3. **Constraints:** - Your signal handlers must handle exceptions gracefully. - The handler for `SIGINT` should print \\"Interrupt signal received. Exiting gracefully...\\" - The handler for `SIGTERM` should print \\"Termination signal received. Exiting gracefully...\\" - The interval timer should log the message \\"Status: Running...\\" every 5 seconds. - Ensure your handlers and timer work reliably despite the asynchronous nature of signal delivery. # Performance Considerations: - Your solution must not busy-wait and should efficiently wait for signals using appropriate synchronization mechanisms. - Minimize the overhead of signal handling to ensure the main loop runs smoothly. # Example Execution: - The program starts and runs indefinitely. - Every 5 seconds, it logs: `Status: Running...` - When `Ctrl+C` is pressed, it logs: `Interrupt signal received. Exiting gracefully...`, and the program exits. - When a `SIGTERM` is sent to the process, it logs: `Termination signal received. Exiting gracefully...`, and the program exits. ```python import signal import time import sys # Initialize a flag for termination terminate = False def handle_sigint(signum, frame): global terminate print(\\"Interrupt signal received. Exiting gracefully...\\") terminate = True def handle_sigterm(signum, frame): global terminate print(\\"Termination signal received. Exiting gracefully...\\") terminate = True def handle_sigalrm(signum, frame): print(\\"Status: Running...\\") # Set the alarm again signal.alarm(5) def main(): global terminate # Setup signal handlers signal.signal(signal.SIGINT, handle_sigint) signal.signal(signal.SIGTERM, handle_sigterm) signal.signal(signal.SIGALRM, handle_sigalrm) # Schedule the first alarm signal.alarm(5) # Main process loop while not terminate: try: # Perform some useful work here time.sleep(1) except KeyError: print(\\"Key error occurred...\\") except Exception as e: print(f\\"Exception caught: {e}\\") print(\\"Shutdown complete.\\") if __name__ == \\"__main__\\": main() ``` # Instructions: - Implement the `main()` function and signal handlers as described. - Ensure the program exits gracefully upon receiving `SIGINT` or `SIGTERM`. - Use `alarm()` to log status messages at intervals.","solution":"import signal import time import sys # Initialize a flag for termination terminate = False def handle_sigint(signum, frame): global terminate print(\\"Interrupt signal received. Exiting gracefully...\\") terminate = True def handle_sigterm(signum, frame): global terminate print(\\"Termination signal received. Exiting gracefully...\\") terminate = True def handle_sigalrm(signum, frame): print(\\"Status: Running...\\") # Set the alarm again signal.alarm(5) def main(): global terminate # Setup signal handlers signal.signal(signal.SIGINT, handle_sigint) signal.signal(signal.SIGTERM, handle_sigterm) signal.signal(signal.SIGALRM, handle_sigalrm) # Schedule the first alarm signal.alarm(5) # Main process loop while not terminate: try: time.sleep(1) except Exception as e: print(f\\"Exception caught: {e}\\") print(\\"Shutdown complete.\\") if __name__ == \\"__main__\\": main()"},{"question":"**Python Coding Assessment Question: Implementing Module Loading with `importlib`** Objective: To assess the understanding of Python\'s `importlib` library, especially dealing with dynamic imports and module reloading. **Problem Statement:** You are tasked with creating a function that mimics some of the core functionalities of the old `imp` module using its modern replacement, `importlib`. Your function should try to: 1. Dynamically find and load a module by its name. 2. If the module is already imported, it should reload the module. 3. Return the module object. **Function Signature:** ```python def dynamic_import(module_name: str) -> ModuleType: Dynamically imports and reloads (if needed) a module by name. Args: module_name (str): The name of the module to be imported and reloaded. Returns: ModuleType: The imported (or reloaded) module object. Raises: ImportError: If the module cannot be imported or reloaded. ``` **Input:** - `module_name` (str): The name of the module to be imported and reloaded. **Output:** - Returns the imported or reloaded `ModuleType` object. **Constraints:** - You should use the `importlib` module for your implementation. - Handle the possibility of the module not being found by raising an `ImportError`. **Example:** ```python # Example usage try: os_module = dynamic_import(\'os\') print(os_module.path) except ImportError as e: print(f\\"Error: {e}\\") # Suppose the `os` module is already imported, calling dynamic_import again should reload it. try: os_module_reloaded = dynamic_import(\'os\') print(os_module_reloaded.path) except ImportError as e: print(f\\"Error: {e}\\") ``` **Hint:** - You may use `importlib.import_module` to import the module. - To reload a module, you can use `importlib.reload`. **Note:** - Use appropriate exception handling to ensure that any file-like objects opened during the process are properly closed, similar to the example provided for the old `imp` module.","solution":"import importlib import types from types import ModuleType def dynamic_import(module_name: str) -> ModuleType: Dynamically imports and reloads (if needed) a module by name. Args: module_name (str): The name of the module to be imported and reloaded. Returns: ModuleType: The imported (or reloaded) module object. Raises: ImportError: If the module cannot be imported or reloaded. try: module = importlib.import_module(module_name) # Reload the module if it is already imported module = importlib.reload(module) return module except Exception as e: raise ImportError(f\\"Could not import or reload module \'{module_name}\': {e}\\")"},{"question":"# Byte Object Manipulation in Python In this assessment, you will be required to create a set of utility functions to manipulate bytes objects in Python. These functions will simulate some of the operations available in the C API for bytes objects. Part 1: **Creating Bytes from Strings** Write a function `bytes_from_string(s: str) -> bytes` that takes a string `s` and returns a bytes object with its contents. Example: ```python assert bytes_from_string(\\"hello\\") == b\'hello\' ``` Part 2: **Concatenating Bytes** Write a function `concat_bytes(b1: bytes, b2: bytes) -> bytes` that takes two bytes objects `b1` and `b2`, and returns a new bytes object that is a concatenation of `b1` and `b2`. Example: ```python assert concat_bytes(b\'hello\', b\'world\') == b\'helloworld\' ``` Part 3: **Formatting Bytes** Write a function `bytes_from_format(format_string: str, *args) -> bytes` that takes a format string and a variable number of arguments, and returns a formatted bytes object. This function should mimic the behavior of `PyBytes_FromFormat` from the C API. Allowed format characters are: - `%%` for the literal `%` character. - `%c` for a single byte, represented as an integer. - `%d` for an integer. - `%s` for a null-terminated string. Example: ```python assert bytes_from_format(\\"Error %d: %s\\", 404, \\"Not Found\\") == b\'Error 404: Not Found\' assert bytes_from_format(\\"Character: %c\\", 65) == b\'Character: A\' ``` Part 4: **Handling Mixed Null Strings** Write a function `bytes_mixed_null_strings(asciis: List[str]) -> List[Optional[bytes]]` that takes a list of ASCII strings and returns a list of bytes objects. If a string contains a null character (`\'0\'`), the corresponding bytes object should be `None`. Example: ```python input_strings = [\\"hello\\", \\"world0test\\", \\"python\\"] assert bytes_mixed_null_strings(input_strings) == [b\'hello\', None, b\'python\'] ``` **Constraints:** - Do not use any libraries for string formatting or manipulating other than Python\'s built-in capabilities. - Assume all input strings are ASCII and any null handling should be explicit as per the example. **Performance Requirements:** - Each function should be efficient and handle large inputs within reasonable time and space complexity. Please implement these functions to demonstrate your understanding of bytes objects and string manipulation in Python.","solution":"# Part 1: Creating Bytes from Strings def bytes_from_string(s: str) -> bytes: Takes a string and returns a bytes object with the same contents. return s.encode(\'ascii\') # Part 2: Concatenating Bytes def concat_bytes(b1: bytes, b2: bytes) -> bytes: Takes two bytes objects and returns a new bytes object that is a concatenation of both. return b1 + b2 # Part 3: Formatting Bytes def bytes_from_format(format_string: str, *args) -> bytes: Takes a format string and a variable number of arguments, and returns a formatted bytes object. result = b\\"\\" arg_index = 0 i = 0 while i < len(format_string): if format_string[i] == \'%\' and i + 1 < len(format_string): if format_string[i + 1] == \'%\': result += b\'%\' i += 1 elif format_string[i + 1] == \'c\': if arg_index < len(args): result += bytes([args[arg_index]]) arg_index += 1 i += 1 elif format_string[i + 1] == \'d\': if arg_index < len(args): result += str(args[arg_index]).encode(\'ascii\') arg_index += 1 i += 1 elif format_string[i + 1] == \'s\': if arg_index < len(args): result += str(args[arg_index]).encode(\'ascii\') arg_index += 1 i += 1 else: result += format_string[i].encode(\'ascii\') i += 1 return result # Part 4: Handling Mixed Null Strings from typing import List, Optional def bytes_mixed_null_strings(asciis: List[str]) -> List[Optional[bytes]]: Takes a list of ASCII strings and returns a list of bytes objects. If a string contains a null character (\'0\'), the corresponding bytes object should be None. result = [] for s in asciis: if \'0\' in s: result.append(None) else: result.append(s.encode(\'ascii\')) return result"},{"question":"Objective You are required to write a Python function to benchmark the prediction latency and throughput of various scikit-learn estimators under different configurations. This function should generate insights about the impact of the number of features and input sparsity on performance. Task Implement a function `benchmark_performance` that benchmarks prediction latency and throughput for a given set of scikit-learn estimators and configurations. The function should return a dictionary with the results of these benchmarks. Function Signature ```python def benchmark_performance(estimators, n_features_list, sparsity_levels): Parameters: estimators (list): A list of instantiated scikit-learn estimators to benchmark. n_features_list (list): A list of integers representing the number of features. sparsity_levels (list): A list of floats representing the sparsity levels (as a percentage). Returns: dict: A dictionary containing the benchmark results. pass ``` Input - `estimators`: A list of instantiated scikit-learn estimators (e.g., `LinearRegression()`, `RandomForestRegressor()`, etc.). - `n_features_list`: A list of integers, where each integer represents a different number of features to test. - `sparsity_levels`: A list of floats, where each float represents a different sparsity level (as a percentage) to test. Output - A dictionary with the following structure: ```python { \\"latency\\": { estimator_name: { n_features: { sparsity_level: latency_value } } }, \\"throughput\\": { estimator_name: { n_features: { sparsity_level: throughput_value } } } } ``` Where: - `latency_value` is the measured prediction latency. - `throughput_value` is the measured prediction throughput. - `estimator_name` is the name of the estimator as a string. - `n_features` is the number of features used in the test. - `sparsity_level` is the sparsity level used in the test. Constraints - Ensure the benchmarking tests are run using synthetic data. - Run the benchmarks for a sufficient number of iterations to get stable results. - Produce both latency and throughput results for bulk predictions (minimum batch size of 1000 samples). # Example ```python from sklearn.linear_model import LinearRegression from sklearn.ensemble import RandomForestRegressor estimators = [LinearRegression(), RandomForestRegressor(n_estimators=10)] n_features_list = [10, 100, 1000] sparsity_levels = [0.0, 0.9] results = benchmark_performance(estimators, n_features_list, sparsity_levels) ``` # Considerations - Utilize the documentation provided to understand how input representation and model complexity affect latency and throughput. - Leverage numpy and scipy for generating synthetic data with specified sparsity. - Optimize your implementation to minimize execution time and resource usage. # Submission Submit your implementation of the `benchmark_performance` function along with code snippets demonstrating the benchmarking for at least three different estimators.","solution":"import numpy as np import time from scipy.sparse import csr_matrix def benchmark_performance(estimators, n_features_list, sparsity_levels): results = {\\"latency\\": {}, \\"throughput\\": {}} for estimator in estimators: estimator_name = type(estimator).__name__ results[\\"latency\\"][estimator_name] = {} results[\\"throughput\\"][estimator_name] = {} for n_features in n_features_list: results[\\"latency\\"][estimator_name][n_features] = {} results[\\"throughput\\"][estimator_name][n_features] = {} for sparsity in sparsity_levels: # Generating synthetic data n_samples = 1000 X_dense = np.random.rand(n_samples, n_features) if sparsity > 0: mask = np.random.rand(n_samples, n_features) < sparsity X_dense[mask] = 0 X_sparse = csr_matrix(X_dense) y = np.random.rand(n_samples) # Fit the estimator estimator.fit(X_sparse, y) # Measure latency start_time = time.time() for _ in range(10): estimator.predict(X_sparse) end_time = time.time() latency = (end_time - start_time) / 10 # Measure throughput start_time = time.time() estimator.predict(X_sparse) end_time = time.time() throughput = n_samples / (end_time - start_time) # Store results results[\\"latency\\"][estimator_name][n_features][sparsity] = latency results[\\"throughput\\"][estimator_name][n_features][sparsity] = throughput return results"},{"question":"Objective Write a function `identify_image_type` that uses the `imghdr` module to identify the type of image contained in a given file. Additionally, extend the `imghdr` module to recognize a new custom image type called \\"foo\\". The \\"foo\\" image format starts with the byte sequence `b\'x89FOOx0Dx0Ax1Ax0A\'`. Function Signature ```python def identify_image_type(file_path: str) -> str: # Your code here ``` Input - `file_path` (str): A string representing the path to the image file. Output - (str): A string representing the image type if recognized, or `None` if the image type is unrecognized. Constraints - Implement this on a local file system. - Do not use external libraries other than the standard library. - Handle errors gracefully, such as file not found or unsupported file types. - Aim for a clean and efficient solution. Example ```python assert identify_image_type(\'bass.gif\') == \'gif\' assert identify_image_type(\'example.foo\') == \'foo\' assert identify_image_type(\'unknown_file.xyz\') == None ``` Hints - To extend the `imghdr` module, you can add a custom test function to `imghdr.tests`. - The function should read an appropriate number of bytes from the file to determine its type. Good luck!","solution":"import imghdr def foo_test(h, f): if h[:8] == b\'x89FOOx0Dx0Ax1Ax0A\': return \'foo\' return None imghdr.tests.append(foo_test) def identify_image_type(file_path: str) -> str: try: return imghdr.what(file_path) except FileNotFoundError: return None"},{"question":"# Question: Custom Visualization with seaborn objects Context: You are provided with a dataset \'healthexp\' that tracks health expenditure data over several years across different countries. Your task is to visualize this data using seaborn\'s object-oriented interface, demonstrating your proficiency with advanced plotting techniques including data normalization and custom constraints. Task: Write a function `plot_health_expenditure` that: 1. Loads the \'healthexp\' dataset. 2. Creates two different line plots: - **Plot 1**: Show spending relative to the maximum spending value for each country. - **Plot 2**: Show percentage change in spending from the baseline year (1970). 3. Ensure that the y-axes are labeled appropriately: - For **Plot 1**: \\"Spending relative to maximum amount\\" - For **Plot 2**: \\"Percent change in spending from 1970 baseline\\" Input: - None. Output: - The function should display both plots side by side using matplotlib\'s `subplots` or grid functionalities. Constraints: - You must use the seaborn object\'s interface to create the plots. - Ensure that the legends, titles, and labels are clear and informative. Example: Here is an example of how the function can be structured (note that details need to be filled in): ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def plot_health_expenditure(): # Load dataset healthexp = load_dataset(\\"healthexp\\") # Set up the figure and axes for side-by-side plots fig, axes = plt.subplots(1, 2, figsize=(15, 7)) # Plot 1: Spending relative to maximum amount so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm()) .label(y=\\"Spending relative to maximum amount\\") .scale(fig=fig, ax=axes[0]) # Plot 2: Percent change in spending from 1970 baseline so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"x == x.min()\\", percent=True)) .label(y=\\"Percent change in spending from 1970 baseline\\") .scale(fig=fig, ax=axes[1]) # Display plots plt.show() # Call the function to display plots plot_health_expenditure() ``` Use the example given above to complete your implementation. Ensure you test the function to verify that it works as expected.","solution":"import seaborn.objects as so from seaborn import load_dataset import numpy as np import matplotlib.pyplot as plt def plot_health_expenditure(): # Load dataset healthexp = load_dataset(\\"healthexp\\") # Normalize spending relative to the maximum spending value for each country healthexp[\'Spending_Relative_Max\'] = healthexp.groupby(\'Country\')[\'Spending_USD\'].transform(lambda x: x / x.max()) # Calculate percentage change in spending from the baseline year (1970) baseline_spending = healthexp[healthexp[\'Year\'] == 1970][[\'Country\', \'Spending_USD\']].rename(columns={\'Spending_USD\': \'Baseline_Spending\'}) healthexp = healthexp.merge(baseline_spending, on=\'Country\') healthexp[\'Percent_Change_From_Baseline\'] = (healthexp[\'Spending_USD\'] - healthexp[\'Baseline_Spending\']) / healthexp[\'Baseline_Spending\'] * 100 # Set up the figure and axes for side-by-side plots fig, axes = plt.subplots(1, 2, figsize=(15, 7)) # Plot 1: Spending relative to maximum amount so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_Relative_Max\\", color=\\"Country\\") .add(so.Line()) .label(y=\\"Spending relative to maximum amount\\") .scale(fig=fig, ax=axes[0]) # Plot 2: Percent change in spending from 1970 baseline so.Plot(healthexp, x=\\"Year\\", y=\\"Percent_Change_From_Baseline\\", color=\\"Country\\") .add(so.Line()) .label(y=\\"Percent change in spending from 1970 baseline\\") .scale(fig=fig, ax=axes[1]) # Display plots plt.show()"},{"question":"Objective: Demonstrate your understanding of seaborn\'s `sns.lmplot` by creating customized regression plots from a specified dataset. Problem Statement: You are provided with a dataset called \'tips\' from seaborn\'s repository. Your task is to create several regression plots to visualize the relationships between numerical variables in this dataset and condition them on categorical variables using advanced customization options. Instructions: 1. **Load the Dataset:** - Use the `tips` dataset from seaborn\'s built-in datasets. 2. **Plot 1: Basic Regression Plot** - Create a regression plot to visualize the relationship between `total_bill` (as `x`) and `tip` (as `y`). 3. **Plot 2: Conditioned with `hue`** - Create a regression plot to visualize the same relationship (`total_bill` vs `tip`), but condition it on the `day` variable using color coding. 4. **Plot 3: Splitting using Subplots (`col`)** - Create a regression plot to visualize the same relationship but split the plots based on the `sex` variable using columns for subplots. 5. **Plot 4: Detailed Conditioning and Customization** - Create a regression plot to visualize the same relationship, but condition them on both `time` (row-wise) and `day` (column-wise). Use custom settings for `facet_kws` to ensure that each subplot has independent axis limits. 6. **Insights:** - Based on the plots created, write a brief analysis (3-5 sentences) discussing any visible trends or observations from the plots. Constraints: - You must use the seaborn library for plotting. - Your code should be written in Python. - Use Jupyter Notebook or any compatible environment to display your plots. Expected Output: - Four plots as specified above. - A brief textual analysis discussing the trends observed. ```python # Example skeleton code: import seaborn as sns # Load the dataset tips = sns.load_dataset(\'tips\') # Plot 1: Basic Regression Plot sns.lmplot(data=tips, x=\'total_bill\', y=\'tip\') # Plot 2: Conditioned with hue sns.lmplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'day\') # Plot 3: Splitting using Subplots (col) sns.lmplot(data=tips, x=\'total_bill\', y=\'tip\', col=\'sex\') # Plot 4: Detailed Conditioning and Customization sns.lmplot(data=tips, x=\'total_bill\', y=\'tip\', row=\'time\', col=\'day\', facet_kws={\'sharex\': False, \'sharey\': False}) # Add your analysis below the plots ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\'tips\') # Plot 1: Basic Regression Plot plt.figure() sns.lmplot(data=tips, x=\'total_bill\', y=\'tip\') plt.savefig(\\"plot1.png\\") # Plot 2: Conditioned with hue plt.figure() sns.lmplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'day\') plt.savefig(\\"plot2.png\\") # Plot 3: Splitting using Subplots (col) plt.figure() sns.lmplot(data=tips, x=\'total_bill\', y=\'tip\', col=\'sex\') plt.savefig(\\"plot3.png\\") # Plot 4: Detailed Conditioning and Customization plt.figure() sns.lmplot(data=tips, x=\'total_bill\', y=\'tip\', row=\'time\', col=\'day\', facet_kws={\'sharex\': False, \'sharey\': False}) plt.savefig(\\"plot4.png\\") # Analysis analysis = From the created plots, several trends are visible: 1. In Plot 1, there is a positive correlation between `total_bill` and `tip`, indicating that higher bills tend to generate higher tips. 2. Plot 2 shows the same overall trend, but it is clear that the relationship varies slightly by day; the tips on different days show some variation in their distribution. 3. Plot 3 highlights differences between genders, suggesting that there might be differences in tipping behavior or spending habits between males and females. 4. Plot 4 provides a more detailed breakdown, showing differences based on both time of day and day of the week. There are noticeable variations in tipping behaviors, which might be influenced by these factors. Overall, these visualizations suggest that while there is a positive correlation between `total_bill` and `tip`, the strength and nature of this relationship can vary based on categorical factors such as day, sex, and time. print(analysis)"},{"question":"You are given a dataset `students.csv` containing information about students\' test scores and other parameters. Your task is to create a visual analysis using seaborn, following these steps: 1. **Load and Explore the Data**: - The dataset `students.csv` is structured with the following columns: - `student_id`: Unique identifier for each student - `gender`: Gender of the student (`Male`/`Female`) - `class`: Class of the student (1-5) - `math_score`: Score in Math - `reading_score`: Score in Reading 2. **Visualize the Data**: - Create a strip plot to show the distribution of `math_score` across different `class`. - Use `hue` to differentiate between `gender`. - Add jitter to the plot for better visualization. - Use a specific palette (e.g., \\"deep\\") to color the points. 3. **Subplot Analysis**: - Create a faceted grid to show the relationship between `class`, `math_score`, and `reading_score`. - Create subplots for each combination of `class` and `gender`. - Make sure your plots are clear and the axes are labeled properly. # Input Format - A CSV file named `students.csv`. # Output Format - A strip plot showing the distribution of `math_score` across different classes differentiated by `gender`. - A faceted grid showing the relationship between `class`, `math_score`, and `reading_score` with subplots for each `class` and `gender`. # Constraints - Ensure the visualizations are well-labeled and clear. - Use appropriate themes and palettes provided by seaborn for better visualization. # Example ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the dataset students = pd.read_csv(\'students.csv\') # Set the seaborn theme sns.set_theme(style=\\"whitegrid\\") # First plot: Strip plot plt.figure(figsize=(10, 6)) sns.stripplot(data=students, x=\\"class\\", y=\\"math_score\\", hue=\\"gender\\", jitter=True, palette=\\"deep\\") plt.title(\\"Math Scores across Different Classes by Gender\\") plt.show() # Second plot: Faceted grid grid = sns.catplot(data=students, x=\\"class\\", y=\\"math_score\\", hue=\\"reading_score\\", col=\\"gender\\", kind=\\"strip\\", height=5, aspect=.7, palette=\\"deep\\") grid.fig.suptitle(\\"Math and Reading Scores by Class and Gender\\", y=1.02) plt.show() ``` # Notes - Make sure the plots are self-explanatory. Include titles, and axis labels and ensure the legend is present where necessary. - Use appropriate seaborn functionalities to create clean and informative visualizations.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_student_scores(file_path): # Load the dataset students = pd.read_csv(file_path) # Set the seaborn theme sns.set_theme(style=\\"whitegrid\\") # Strip plot: Math Scores across different classes by gender plt.figure(figsize=(10, 6)) strip_plot = sns.stripplot(data=students, x=\\"class\\", y=\\"math_score\\", hue=\\"gender\\", jitter=True, palette=\\"deep\\") strip_plot.set_title(\\"Math Scores across Different Classes by Gender\\") plt.show() # Faceted grid: Math and Reading Scores by Class and Gender grid = sns.catplot(data=students, x=\\"class\\", y=\\"math_score\\", hue=\\"reading_score\\", col=\\"gender\\", kind=\\"strip\\", height=5, aspect=.7, palette=\\"deep\\") grid.fig.suptitle(\\"Math and Reading Scores by Class and Gender\\", y=1.02) plt.show()"},{"question":"**Pseudo-Terminal Logging Challenge** **Objective**: Write a Python function `log_terminal_session(command_list: List[str], log_file: str, chunk_size: int = 1024) -> int` that: 1. Spawns a new process using a pseudo-terminal. 2. Executes the command specified by `command_list`. 3. Logs the session input and output to the specified `log_file`. 4. Returns the exit status of the spawned process. **Function Signature**: ```python def log_terminal_session(command_list: List[str], log_file: str, chunk_size: int = 1024) -> int: pass ``` **Parameters**: - `command_list`: A list of strings where the first element is the command to be executed, and the subsequent elements are the arguments to the command. - `log_file`: A string that represents the path to the log file where program input and output should be recorded. - `chunk_size`: An integer that represents the number of bytes to read from the pseudo-terminal in each read operation. Default is 1024. **Returns**: - The function should return an integer which is the exit status of the spawned process. **Example**: ```python exit_status = log_terminal_session([\'ls\', \'-la\'], \'/tmp/session_log.txt\') print(f\'Process exited with status: {exit_status}\') ``` **Constraints**: - You can assume that the provided command is valid and present on the system. - The log file path provided is valid and writable. - Command execution should continue if `stdin` closes. **Details**: 1. Use `pty.spawn()` to spawn the process and connect it to a pseudo-terminal. 2. Implement a callback function to handle reading from the pseudo-terminal. 3. Log the session data to the specified log file. 4. Ensure that proper resource cleanup is done, and the log file is properly closed after the session ends. 5. Handle potential exceptions or errors gracefully, ensuring the log file is written. **Hints**: - Review the example given in the documentation for a similar implementation using `pty.spawn()`. - Use the read function callback to read data from the process and write it to the log file. - Remember to flush the file writes to ensure all data is written correctly.","solution":"import os import pty import subprocess def log_terminal_session(command_list, log_file, chunk_size=1024): Spawns a new process using a pseudo-terminal, executes the command, logs the session input, and output to the specified log file and returns the exit status of the spawned process. :param command_list: List of strings where the first element is the command and subsequent elements are the arguments. :param log_file: The path to the log file where program input and output should be recorded. :param chunk_size: Number of bytes to read from the pseudo-terminal in each read operation. Default is 1024. :return: The exit status of the spawned process as an integer. # Callback function to handle data read from the pseudo-terminal def read(fd): with open(log_file, \'wb\') as f: while True: output = os.read(fd, chunk_size) if not output: break f.write(output) f.flush() # Use pty.spawn to start the process and handle its IO exit_status = pty.spawn(command_list, read) return exit_status"},{"question":"# Custom Python Type Implementation As part of this assessment, you are to demonstrate your understanding of Python type customization by creating a new custom type in Python. The task involves defining a new Python class, `MyNumber`, which will mimic the behavior of a number while providing additional functionalities. Requirements 1. **Class Definition**: Define a `MyNumber` class with the following attributes: - `value`: to store the numeric value. - `history`: a list to keep track of all operations performed on the number. 2. **Initialization**: Implement the `__init__` method to initialize the `value` and `history`. The history should start with a record of the initial value. 3. **Addition**: Override the `__add__` method to add two `MyNumber` instances or a `MyNumber` instance with an integer. Update the history with the operation performed and the result. 4. **Subtraction**: Similarly, override the `__sub__` method to subtract and update the history. 5. **String Representation**: Override the `__repr__` method to provide a meaningful string representation of the value. 6. **History Tracking**: Implement a method `show_history` to print all recorded operations and the resultant value after each operation. Expected Input and Output - **Input**: - Initialization: `num = MyNumber(10)` - Addition: `num = num + 5` or `num = num + MyNumber(2)` - Subtraction: `num = num - 3` or `num = num - MyNumber(4)` - Show History: `num.show_history()` - **Output**: - Proper initialization and history tracking - Proper addition and subtraction with updates in history - Meaningful string representation of the value - Display of operation history. Constraints 1. The operations should handle improper input gracefully, raising appropriate exceptions if operations are not possible. 2. You cannot use any external libraries other than Python\'s standard library. 3. The class should be designed to handle edge cases, such as adding or subtracting with `None` or non-numeric types. Performance Requirements Your solution should be efficient with a linear complexity in terms of the number of operations. History tracking should not add significant overhead to the operations. # Example ```python class MyNumber: def __init__(self, value): # Initialize value and history def __add__(self, other): # Implement addition and update history def __sub__(self, other): # Implement subtraction and update history def __repr__(self): # Provide a string representation of the object def show_history(self): # Print all recorded operations and results ``` Sample Usage ```python num = MyNumber(10) print(num) # Output: 10 num = num + 5 print(num) # Output: 15 num = num - MyNumber(3) print(num) # Output: 12 num.show_history() # Output: # Initial value: 10 # Added 5, Result: 15 # Subtracted 3, Result: 12 ``` Submit your implementation including any necessary helper methods or validation checks.","solution":"class MyNumber: def __init__(self, value): if not isinstance(value, (int, float)): raise ValueError(\\"MyNumber can only be initialized with an int or float.\\") self.value = value self.history = [f\\"Initialized with value: {value}\\"] def __add__(self, other): if isinstance(other, MyNumber): result_value = self.value + other.value result = MyNumber(result_value) result.history = self.history + [f\\"Added {other.value}, Result: {result_value}\\"] return result elif isinstance(other, (int, float)): result_value = self.value + other result = MyNumber(result_value) result.history = self.history + [f\\"Added {other}, Result: {result_value}\\"] return result else: raise ValueError(\\"MyNumber can only be added with an int, float, or MyNumber.\\") def __sub__(self, other): if isinstance(other, MyNumber): result_value = self.value - other.value result = MyNumber(result_value) result.history = self.history + [f\\"Subtracted {other.value}, Result: {result_value}\\"] return result elif isinstance(other, (int, float)): result_value = self.value - other result = MyNumber(result_value) result.history = self.history + [f\\"Subtracted {other}, Result: {result_value}\\"] return result else: raise ValueError(\\"MyNumber can only be subtracted with an int, float, or MyNumber.\\") def __repr__(self): return str(self.value) def show_history(self): for record in self.history: print(record)"},{"question":"# Question: Optimizing Memory Usage on Large Datasets You are provided with a directory containing multiple Parquet files, each representing a different year of data. Each file contains columns for `name`, `id`, `x`, and `y`. You need to process these files efficiently to calculate and return the following: 1. A pandas DataFrame with memory-efficient types, combining all the data from the Parquet files. 2. The total memory savings achieved after optimization. **Input:** - A directory path as a string (e.g., `\\"data/timeseries/\\"`) containing multiple Parquet files. **Output:** - A tuple containing: 1. A pandas DataFrame with combined and optimized data. 2. A float representing the memory savings as a fraction of the original memory usage. **Constraints:** - Each column in the Parquet files must be processed using memory-efficient data types. - You must use chunking to process the files one by one to handle memory constraints. - Each Parquet file must be read using pandas without loading unnecessary columns. **Performance Requirements:** - The solution should minimize memory usage and be able to process the data without exceeding memory limits. **Example:** Suppose the directory contains the following files: ```plaintext data/timeseries/ts-00.parquet data/timeseries/ts-01.parquet data/timeseries/ts-02.parquet ... ``` # Implementation: ```python import pandas as pd import pathlib def optimize_memory_usage(directory_path): # Step 1: Initialize files = pathlib.Path(directory_path).glob(\\"ts*.parquet\\") all_data = [] # Step 2: Process each file for file_path in files: df = pd.read_parquet(file_path, columns=[\\"name\\", \\"id\\", \\"x\\", \\"y\\"]) # Convert columns to memory-efficient types df[\\"name\\"] = df[\\"name\\"].astype(\\"category\\") df[\\"id\\"] = pd.to_numeric(df[\\"id\\"], downcast=\\"unsigned\\") df[[\\"x\\", \\"y\\"]] = df[[\\"x\\", \\"y\\"]].apply(pd.to_numeric, downcast=\\"float\\") all_data.append(df) # Step 3: Concatenate all data combined_df = pd.concat(all_data) # Step 4: Calculate memory savings original_memory = combined_df.memory_usage(deep=True).sum() optimized_memory = combined_df.memory_usage(deep=True).sum() memory_savings = (original_memory - optimized_memory) / original_memory return combined_df, memory_savings # Test the function with an example directory # combined_df, savings = optimize_memory_usage(\\"data/timeseries/\\") # print(combined_df.head()) # print(f\\"Memory savings: {savings:.2f}\\") ``` **Note:** Ensure the implementation works with the actual dataset structure and Parquet files in the provided directory.","solution":"import pandas as pd import pathlib import os def optimize_memory_usage(directory_path): # Step 1: Initialize files = pathlib.Path(directory_path).glob(\\"*.parquet\\") all_data = [] original_memory = 0 # Step 2: Process each file for file_path in files: # Reading the file without loading unnecessary columns df = pd.read_parquet(file_path, columns=[\\"name\\", \\"id\\", \\"x\\", \\"y\\"]) # Calculate memory before optimization original_memory += df.memory_usage(deep=True).sum() # Convert columns to memory-efficient types df[\\"name\\"] = df[\\"name\\"].astype(\\"category\\") df[\\"id\\"] = pd.to_numeric(df[\\"id\\"], downcast=\\"unsigned\\") df[[\\"x\\", \\"y\\"]] = df[[\\"x\\", \\"y\\"]].apply(pd.to_numeric, downcast=\\"float\\") all_data.append(df) # Step 3: Concatenate all data combined_df = pd.concat(all_data, ignore_index=True) # Calculate memory after optimization optimized_memory = combined_df.memory_usage(deep=True).sum() # Memory savings as a fraction of the original memory usage memory_savings = (original_memory - optimized_memory) / original_memory return combined_df, memory_savings"},{"question":"# Custom Linear Transformation with Gradient Check In this question, you will implement a custom PyTorch function for a linear transformation and a corresponding module. Your task is to define a custom function using `torch.autograd.Function` and ensure it integrates correctly with PyTorch\'s autograd mechanism. You will also implement a gradient check to verify your function. Part 1: Implement the Custom Function 1. Define a class `CustomLinearFunction` that inherits from `torch.autograd.Function`. 2. Implement the `forward`, `setup_context`, and `backward` methods: - **Forward Pass**: Apply a linear transformation `output = input * weight^T + bias`. - **Setup Context**: Save the necessary tensors for the backward pass. - **Backward Pass**: Compute the gradients for `input`, `weight`, and `bias`. Part 2: Create a Module 1. Define a class `CustomLinear` that inherits from `torch.nn.Module`. 2. Implement the constructor `__init__` which initializes `weight` and `bias` parameters. 3. Implement the `forward` method to use `CustomLinearFunction`. Part 3: Gradient Check 1. Write a function `gradient_check()` that creates an instance of `CustomLinear` and verifies the gradients using `torch.autograd.gradcheck`. # Expected Input and Output 1. The input to the `forward` method of `CustomLinearFunction` will be: - `input`: a tensor of shape `(N, D_in)` where `N` is the batch size and `D_in` is the input dimension. - `weight`: a tensor of shape `(D_out, D_in)` where `D_out` is the output dimension. - `bias`: a tensor of shape `(D_out,)`. 2. The output from the `forward` method will be: - A tensor of shape `(N, D_out)`. The `gradient_check()` function should print `True` if the gradients are correct, otherwise `False`. # Example ```python import torch # Part 1: Implement the Custom Function class CustomLinearFunction(torch.autograd.Function): @staticmethod def forward(ctx, input, weight, bias): # Compute linear transformation output = input.mm(weight.t()) if bias is not None: output += bias.unsqueeze(0).expand_as(output) # Save context ctx.save_for_backward(input, weight, bias) return output @staticmethod def setup_context(ctx, inputs, output): input, weight, bias = inputs ctx.save_for_backward(input, weight, bias) @staticmethod def backward(ctx, grad_output): input, weight, bias = ctx.saved_tensors grad_input = grad_weight = grad_bias = None if ctx.needs_input_grad[0]: grad_input = grad_output.mm(weight) if ctx.needs_input_grad[1]: grad_weight = grad_output.t().mm(input) if bias is not None and ctx.needs_input_grad[2]: grad_bias = grad_output.sum(0) return grad_input, grad_weight, grad_bias # Part 2: Create a Module class CustomLinear(torch.nn.Module): def __init__(self, input_features, output_features, bias=True): super().__init__() self.input_features = input_features self.output_features = output_features self.weight = torch.nn.Parameter(torch.empty(output_features, input_features)) if bias: self.bias = torch.nn.Parameter(torch.empty(output_features)) else: self.register_parameter(\'bias\', None) torch.nn.init.kaiming_normal_(self.weight) if self.bias is not None: torch.nn.init.zeros_(self.bias) def forward(self, input): return CustomLinearFunction.apply(input, self.weight, self.bias) def extra_repr(self): return f\'input_features={self.input_features}, output_features={self.output_features}, bias={self.bias is not None}\' # Part 3: Gradient Check def gradient_check(): input = torch.randn(5, 3, dtype=torch.double, requires_grad=True) weight = torch.randn(2, 3, dtype=torch.double, requires_grad=True) bias = torch.randn(2, dtype=torch.double, requires_grad=True) model = CustomLinear(3, 2) test = torch.autograd.gradcheck(CustomLinearFunction.apply, (input, weight, bias)) print(test) # Run the gradient check gradient_check() ``` In this task, you will demonstrate your understanding of custom autograd functions in PyTorch, module creation, and gradient checks. **Constraints**: - Do not use any external libraries other than PyTorch. - Make sure your code is efficient and adheres to the principles discussed in the documentation.","solution":"import torch # Part 1: Implement the Custom Function class CustomLinearFunction(torch.autograd.Function): @staticmethod def forward(ctx, input, weight, bias): # Compute linear transformation output = input.mm(weight.t()) if bias is not None: output += bias.unsqueeze(0).expand_as(output) # Save context ctx.save_for_backward(input, weight, bias) return output @staticmethod def backward(ctx, grad_output): input, weight, bias = ctx.saved_tensors grad_input = grad_weight = grad_bias = None if ctx.needs_input_grad[0]: grad_input = grad_output.mm(weight) if ctx.needs_input_grad[1]: grad_weight = grad_output.t().mm(input) if bias is not None and ctx.needs_input_grad[2]: grad_bias = grad_output.sum(0) return grad_input, grad_weight, grad_bias # Part 2: Create a Module class CustomLinear(torch.nn.Module): def __init__(self, input_features, output_features, bias=True): super().__init__() self.input_features = input_features self.output_features = output_features self.weight = torch.nn.Parameter(torch.empty(output_features, input_features)) if bias: self.bias = torch.nn.Parameter(torch.empty(output_features)) else: self.register_parameter(\'bias\', None) torch.nn.init.kaiming_normal_(self.weight) if self.bias is not None: torch.nn.init.zeros_(self.bias) def forward(self, input): return CustomLinearFunction.apply(input, self.weight, self.bias) def extra_repr(self): return f\'input_features={self.input_features}, output_features={self.output_features}, bias={self.bias is not None}\' # Part 3: Gradient Check def gradient_check(): input = torch.randn(5, 3, dtype=torch.double, requires_grad=True) weight = torch.randn(2, 3, dtype=torch.double, requires_grad=True) bias = torch.randn(2, dtype=torch.double, requires_grad=True) test = torch.autograd.gradcheck(CustomLinearFunction.apply, (input, weight, bias)) return test"},{"question":"**Objective** Implement a function to validate and preprocess input arrays using the scikit-learn utilities provided. **Function Signature:** ```python def validate_and_preprocess_inputs(X, y, allowed_sparse_formats=None, check_finite=True): Validates and preprocesses input arrays X and y. Parameters: - X: array-like, shape (n_samples, n_features) Input data to be validated and preprocessed. - y: array-like, shape (n_samples,) Target values to be validated and preprocessed. - allowed_sparse_formats: list of string or None (default is None) List of allowed sparse matrix formats (e.g., [\'csr\', \'csc\']). If None, no sparse matrices are allowed. - check_finite: bool (default is True) If True, then check that all elements of X and y are finite (i.e., no NaNs or infinity). Returns: - X_preprocessed: array-like, shape (n_samples, n_features) Preprocessed input data. - y_preprocessed: array-like, shape (n_samples,) Preprocessed target values. Raises: - ValueError: If the input arrays are not valid according to the checks performed. # Your code here pass ``` **Requirements:** 1. **Input validation**: - Use `check_X_y` from `sklearn.utils` to check that `X` and `y` have consistent lengths. - Allow optional sparse matrix formats for `X` by using the `allowed_sparse_formats` parameter. If none are provided, `X` must be dense. - Ensure all elements in `X` and `y` are finite if `check_finite` is set to True. Use the appropriate utility function for this (e.g., `assert_all_finite`). 2. **Output**: - Return the preprocessed `X` and `y`, ensuring they follow the requirements set by the above validations. 3. **Constraints**: - The function should raise a `ValueError` with appropriate messages if the input arrays do not meet the specified constraints. **Performance Constraints**: - Your function should efficiently handle large input arrays, taking advantage of sparse matrix handling where applicable. **Example Usage**: ```python from scipy import sparse import numpy as np X_dense = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) y_dense = np.array([0, 1, 0]) # Case 1: Dense input arrays X_preprocessed, y_preprocessed = validate_and_preprocess_inputs(X_dense, y_dense) print(X_preprocessed) print(y_preprocessed) # Case 2: Sparse input array allowed X_sparse = sparse.csr_matrix([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) X_preprocessed, y_preprocessed = validate_and_preprocess_inputs(X_sparse, y_dense, allowed_sparse_formats=[\'csr\']) print(X_preprocessed) print(y_preprocessed) ``` In this example, `validate_and_preprocess_inputs` should correctly validate and preprocess the provided inputs, raising errors where necessary and returning the preprocessed data.","solution":"from sklearn.utils import check_X_y, assert_all_finite from sklearn.utils.validation import check_array def validate_and_preprocess_inputs(X, y, allowed_sparse_formats=None, check_finite=True): Validates and preprocesses input arrays X and y. Parameters: - X: array-like, shape (n_samples, n_features) Input data to be validated and preprocessed. - y: array-like, shape (n_samples,) Target values to be validated and preprocessed. - allowed_sparse_formats: list of string or None (default is None) List of allowed sparse matrix formats (e.g., [\'csr\', \'csc\']). If None, no sparse matrices are allowed. - check_finite: bool (default is True) If True, then check that all elements of X and y are finite (i.e., no NaNs or infinity). Returns: - X_preprocessed: array-like, shape (n_samples, n_features) Preprocessed input data. - y_preprocessed: array-like, shape (n_samples,) Preprocessed target values. Raises: - ValueError: If the input arrays are not valid according to the checks performed. if allowed_sparse_formats is None: allowed_sparse_formats = [] X_preprocessed, y_preprocessed = check_X_y( X, y, accept_sparse=allowed_sparse_formats, ensure_min_samples=1, ensure_min_features=1, force_all_finite=check_finite ) if check_finite: assert_all_finite(X_preprocessed) assert_all_finite(y_preprocessed) return X_preprocessed, y_preprocessed"},{"question":"Objective: Write an asynchronous Python program that uses `asyncio.Queue` to implement a producer-consumer scenario where multiple producers generate random numbers and multiple consumers process these numbers. The goal is to ensure that producers and consumers work concurrently, make use of `asyncio.Queue` methods, and properly manage task completion. Task: - Create a function `producer` that generates random integers between 1 and 100 and puts them into an `asyncio.Queue`. Each producer should produce 20 integers. - Create a function `consumer` that takes integers from the queue and processes (e.g., print and square the integer) them. - The program should create 3 producers and 5 consumers. - Use `asyncio.Queue` methods to coordinate between producers and consumers. - Ensure that the program handles the shutdown of consumers once all producers have finished putting items in the queue and all items have been processed. Requirements: 1. Implement the `producer` coroutine that puts items into the queue. 2. Implement the `consumer` coroutine that gets items from the queue and processes them. 3. Create the main function to coordinate running and awaiting these tasks. 4. Properly use `asyncio.Queue` methods such as `put`, `get`, `task_done`, and `join`. 5. Ensure consumers exit gracefully after completing all tasks. Example: ```python import asyncio import random async def producer(queue, id): for _ in range(20): item = random.randint(1, 100) await queue.put(item) print(f\'Producer {id} produced item: {item}\') await queue.put(None) # Signal the end of production. async def consumer(queue, id): while True: item = await queue.get() if item is None: # End of the task queue.task_done() break print(f\'Consumer {id} processing item: {item}\') processed_item = item ** 2 print(f\'Consumer {id} processed item into: {processed_item}\') queue.task_done() async def main(): queue = asyncio.Queue() producers = [asyncio.create_task(producer(queue, i)) for i in range(3)] consumers = [asyncio.create_task(consumer(queue, i)) for i in range(5)] await asyncio.gather(*producers) await queue.join() for c in consumers: await queue.put(None) # Signal the consumers to exit await asyncio.gather(*consumers) # Using asyncio\'s event loop to run the main function asyncio.run(main()) ``` Expected Output: ``` Producer 0 produced item: 52 Producer 1 produced item: 85 Producer 2 produced item: 29 ... Consumer 0 processing item: 52 Consumer 0 processed item into: 2704 ... ``` This example demonstrates proper use of `asyncio.Queue` and coordination between multiple producers and consumers asynchronously.","solution":"import asyncio import random async def producer(queue, id): for _ in range(20): item = random.randint(1, 100) await queue.put(item) print(f\'Producer {id} produced item: {item}\') await queue.put(None) # Signal the end of production. async def consumer(queue, id): while True: item = await queue.get() if item is None: # End of the task queue.task_done() break print(f\'Consumer {id} processing item: {item}\') processed_item = item ** 2 print(f\'Consumer {id} processed item into: {processed_item}\') queue.task_done() async def main(): queue = asyncio.Queue() producers = [asyncio.create_task(producer(queue, i)) for i in range(3)] consumers = [asyncio.create_task(consumer(queue, i)) for i in range(5)] await asyncio.gather(*producers) await queue.join() for c in consumers: await queue.put(None) # Signal the consumers to exit await asyncio.gather(*consumers) # Using asyncio\'s event loop to run the main function asyncio.run(main())"},{"question":"Problem Statement You are tasked with developing a utility function to search for files in a directory based on various search patterns and conditions. Your function should leverage the `glob` module to perform the search. Function Signature ```python def search_files(directory: str, pattern: str, recursive: bool = False) -> List[str]: Search for files in the specified directory matching the given pattern. Parameters: directory (str): The directory in which to search for files. pattern (str): The pattern to match file names against. recursive (bool): Whether to perform a recursive search. Default is False. Returns: List[str]: A list of matching file paths. ``` Input - `directory`: A string representing the path of the directory to search within. - `pattern`: A string representing the pattern to match file names against. Example patterns include `*.txt`, `?.gif`, `[0-9].*`, etc. - `recursive`: A boolean indicating whether to perform a recursive search through subdirectories. Default value is `False`. Output - The function should return a list of strings, each string being a path to a file that matches the pattern within the specified directory. The paths should be relative to the `directory` parameter. Constraints - You must use the `glob` module to perform the file search. - Your implementation should handle cases where no files match the pattern by returning an empty list. - Do not perform any tilde expansion or environment variable expansion in the input directory or pattern. Examples ```python # Example 1 print(search_files(\\"./test_dir\\", \\"*.txt\\", recursive=True)) # Output: [\'file1.txt\', \'subdir/file2.txt\', ...] # Example 2 print(search_files(\\"./images\\", \\"?.gif\\", recursive=False)) # Output: [\'1.gif\'] # Example 3 print(search_files(\\"./logs\\", \\"[0-9]*.log\\", recursive=False)) # Output: [\'1.log\', \'2.log\', \'10.log\', ...] # Example 4 print(search_files(\\"./hidden_files\\", \\".*\\", recursive=True)) # Output: [\'.hidden1\', \'.subdir/.hidden2\', ...] ``` Use the functions `glob.glob()` and `glob.iglob()` as necessary to implement the required functionality. Note - Ensure you understand the usage and behavior of the `glob` module functions: `glob.glob()`, `glob.iglob()`, and `glob.escape()` before beginning your implementation.","solution":"import glob from typing import List def search_files(directory: str, pattern: str, recursive: bool = False) -> List[str]: Search for files in the specified directory matching the given pattern. Parameters: directory (str): The directory in which to search for files. pattern (str): The pattern to match file names against. recursive (bool): Whether to perform a recursive search. Default is False. Returns: List[str]: A list of matching file paths. search_pattern = f\\"{directory}/**/{pattern}\\" if recursive else f\\"{directory}/{pattern}\\" return glob.glob(search_pattern, recursive=recursive)"},{"question":"# Question: Testing a Banking System with `unittest.mock` You are designing a banking system with various functionalities such as creating accounts, making transfers, and checking balances. As part of your testing strategy, you need to ensure that the components interact correctly without having to rely on a real database. Task 1. **Define the following classes and methods**: - `BankAccount`: - Method: `__init__(self, account_id, initial_balance)` - Initializes an account with an id and initial balance. - Method: `deposit(self, amount)` - Adds the amount to the account balance. - Method: `withdraw(self, amount)` - Withdraws the amount from the account balance, if sufficient funds are available. - Method: `get_balance(self)` - Returns the current balance. - `BankDatabase`: - Method: `get_account(self, account_id)` - Returns the corresponding `BankAccount`. ```python class BankAccount: def __init__(self, account_id, initial_balance): self.account_id = account_id self.balance = initial_balance def deposit(self, amount): self.balance += amount def withdraw(self, amount): if amount > self.balance: raise ValueError(\\"Insufficient funds\\") self.balance -= amount def get_balance(self): return self.balance class BankDatabase: def get_account(self, account_id): # Simulate fetching account from the database pass ``` 2. **Implement the `TransferService` class**: - Method: `__init__(self, database)` - Initializes the service with a reference to the `BankDatabase`. - Method: `transfer(self, from_account_id, to_account_id, amount)` - Transfers the given amount from one account to another. ```python class TransferService: def __init__(self, database): self.database = database def transfer(self, from_account_id, to_account_id, amount): from_account = self.database.get_account(from_account_id) to_account = self.database.get_account(to_account_id) from_account.withdraw(amount) to_account.deposit(amount) ``` 3. **Write unittest cases using `unittest.mock`** to test the `TransferService` class: - Ensure that the `transfer` method correctly transfers the amount between accounts. - Verify that the `withdraw` and `deposit` methods are called with the correct parameters. - Use `patch` to mock the `BankDatabase` and `BankAccount` objects. Example Unittest ```python import unittest from unittest.mock import Mock, patch class TestTransferService(unittest.TestCase): @patch(\'__main__.BankDatabase\') def test_transfer(self, MockBankDatabase): # Setup mocks mock_db = MockBankDatabase.return_value from_account = Mock() to_account = Mock() mock_db.get_account.side_effect = lambda account_id: from_account if account_id == \\"001\\" else to_account # Initialize with mock database service = TransferService(mock_db) # Perform transfer service.transfer(\\"001\\", \\"002\\", 100) # Assertions from_account.withdraw.assert_called_once_with(100) to_account.deposit.assert_called_once_with(100) if __name__ == \\"__main__\\": unittest.main() ``` Submission - Define the described classes (`BankAccount`, `BankDatabase`, `TransferService`). - Write the unittest class (`TestTransferService`) to test the transfer functionality using mocks. - Ensure your code is structured and formatted according to Python standards.","solution":"class BankAccount: def __init__(self, account_id, initial_balance): self.account_id = account_id self.balance = initial_balance def deposit(self, amount): self.balance += amount def withdraw(self, amount): if amount > self.balance: raise ValueError(\\"Insufficient funds\\") self.balance -= amount def get_balance(self): return self.balance class BankDatabase: def get_account(self, account_id): # Simulate fetching account from the database pass class TransferService: def __init__(self, database): self.database = database def transfer(self, from_account_id, to_account_id, amount): from_account = self.database.get_account(from_account_id) to_account = self.database.get_account(to_account_id) from_account.withdraw(amount) to_account.deposit(amount)"},{"question":"**Covariance Estimation and Comparison** You are given a dataset and are required to estimate its covariance matrix using multiple methods provided by the scikit-learn library. Your task is to implement a function that calculates and compares the covariance matrices using: 1. Empirical Covariance 2. Shrunk Covariance with a user-defined shrinkage coefficient 3. Ledoit-Wolf Shrinkage 4. Oracle Approximating Shrinkage 5. Graphical Lasso with cross-validated alpha parameter The function should return the results in a dictionary format, and you should also compare and score the various covariance estimators in terms of their performance (inversion accuracy) and robustness (to outliers). # Function Signature ```python from typing import List, Dict import numpy as np def compare_covariance_estimations(X: np.ndarray, shrinkage_coef: float = 0.1) -> Dict[str, np.ndarray]: Estimates the covariance matrix of the given dataset using different methods and compares their performance. Parameters: - X: np.ndarray The input data matrix of shape (n_samples, n_features). - shrinkage_coef: float, default=0.1 The shrinkage coefficient for the shrunk covariance method. Returns: - result: Dict[str, np.ndarray] A dictionary where the key is the method name and the value is the estimated covariance matrix. pass ``` # Input - `X`: A numpy ndarray of shape `(n_samples, n_features)` representing the dataset. - `shrinkage_coef`: A float representing the shrinkage coefficient to be used in the shrunk covariance method. Default value is 0.1. # Output - A dictionary where the key is the method name (one of `\\"EmpiricalCovariance\\"`, `\\"ShrunkCovariance\\"`, `\\"LedoitWolf\\"`, `\\"OAS\\"`, `\\"GraphicalLasso\\"`) and the value is the estimated covariance matrix. # Constraints - You should utilize scikit-learn\'s APIs for covariance estimation. - Ensure the input data matrix `X` has more samples than features. - Assess the robustness and accuracy of each method. # Example ```python from sklearn.datasets import make_spd_matrix import numpy as np # Generate a positive semi-definite matrix and add some noise n_samples = 100 n_features = 20 X = np.random.multivariate_normal(np.zeros(n_features), make_spd_matrix(n_features), size=n_samples) # Compare covariance estimations result = compare_covariance_estimations(X, shrinkage_coef=0.1) for method, cov_matrix in result.items(): print(f\\"{method}:n{cov_matrix}n\\") ``` # Notes 1. You may refer to the scikit-learn documentation on covariance estimation for detailed explanations and examples. 2. Evaluate the performance of each method based on inversion accuracy and robustness to outliers (feel free to include any additional evaluation metrics and explanations as comments in the code).","solution":"from typing import List, Dict import numpy as np from sklearn.covariance import ( EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, GraphicalLassoCV ) def compare_covariance_estimations(X: np.ndarray, shrinkage_coef: float = 0.1) -> Dict[str, np.ndarray]: Estimates the covariance matrix of the given dataset using different methods and compares their performance. Parameters: - X: np.ndarray The input data matrix of shape (n_samples, n_features). - shrinkage_coef: float, default=0.1 The shrinkage coefficient for the shrunk covariance method. Returns: - result: Dict[str, np.ndarray] A dictionary where the key is the method name and the value is the estimated covariance matrix. # Initialize the result dictionary results = {} # Empirical Covariance emp_cov = EmpiricalCovariance().fit(X).covariance_ results[\\"EmpiricalCovariance\\"] = emp_cov # Shrunk Covariance shrunk_cov = ShrunkCovariance(shrinkage=shrinkage_coef).fit(X).covariance_ results[\\"ShrunkCovariance\\"] = shrunk_cov # Ledoit-Wolf Shrinkage lw_cov = LedoitWolf().fit(X).covariance_ results[\\"LedoitWolf\\"] = lw_cov # Oracle Approximating Shrinkage oas_cov = OAS().fit(X).covariance_ results[\\"OAS\\"] = oas_cov # Graphical Lasso with cross-validated alpha parameter gl_cov = GraphicalLassoCV().fit(X).covariance_ results[\\"GraphicalLasso\\"] = gl_cov return results"},{"question":"# Functional Programming Challenge with itertools, functools, and operator You are required to implement a function `compute_composite_operations` that, given a list of functions and an iterable, will apply the functions sequentially to the iterable. The function should use features from the `itertools`, `functools`, and `operator` modules to demonstrate a good grasp of these modules\' functionalities. Function Signature ```python def compute_composite_operations(funcs: list, iterable: iter) -> list: pass ``` Input: - `funcs` (list): A list of functions that will be applied to the iterable. Each function takes an iterable as input and returns an iterable. - `iterable` (iter): An iterable (e.g., list, tuple) that will be processed by the functions. Output: - The function should return a list after applying all the functions in `funcs` to `iterable` sequentially. Constraints: - You must use the `itertools`, `functools`, and `operator` modules in your solution. - The functions in `funcs` should be applied in the order they appear in the list. - Performance should be efficient for large iterables. Example: ```python from itertools import cycle, islice from functools import partial from operator import add # Example function that adds 1 to each element in the iterable def add_one(iterable): return map(lambda x: x + 1, iterable) # Example function that limits iterable to first 5 elements def limit_to_five(iterable): return islice(iterable, 5) # Example input funcs = [add_one, limit_to_five] iterable = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] # Using the compute_composite_operations function result = compute_composite_operations(funcs, iterable) print(result) # Output: [2, 3, 4, 5, 6] ``` Notes: - You may find `functools.reduce` useful for applying the functions sequentially. - Be sure to handle edge cases, such as an empty iterable or no functions provided in `funcs`. Good luck!","solution":"from itertools import islice from functools import reduce def compute_composite_operations(funcs, iterable): Applies a list of functions sequentially to an iterable. Parameters: funcs (list): List of functions to apply. iterable (iter): The input iterable. Returns: list: Resulting list after applying all functions. # Use reduce to apply each function in funcs to the iterable sequentially result_iterable = reduce(lambda acc, func: func(acc), funcs, iterable) return list(result_iterable)"},{"question":"Permutation Feature Importance Coding Task # Objective: Implement a function that computes and displays the permutation feature importance for a given scikit-learn model, dataset, and scoring metric. # Task: Write a function `compute_permutation_importance` that takes in the following inputs: - `model`: A fitted scikit-learn estimator. - `X`: The feature matrix (Pandas DataFrame or NumPy array). - `y`: The target variable (Pandas Series or NumPy array). - `scoring`: A single string or a list of strings representing the scoring metric(s) to be used (default is R2 for regressors and accuracy for classifiers). - `n_repeats`: The number of times to shuffle a feature (default is 30). - `random_state`: Integer seed for reproducibility (default is None). The function should: 1. Calculate permutation feature importances using the provided `scoring` metric(s). 2. Print the feature names along with their mean importances and standard deviations. 3. If multiple scoring metrics are provided, display the importance ranking separately for each metric. # Expected Output: The function should output a detailed summary of feature importances, ranked and displayed with their respective mean importance values and standard deviations. # Example: ```python from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge from sklearn.inspection import permutation_importance import pandas as pd # Sample code to test the function def compute_permutation_importance(model, X, y, scoring=\'r2\', n_repeats=30, random_state=None): # Your implementation here # Loading dataset and fitting the model diabetes = load_diabetes() X_train, X_val, y_train, y_val = train_test_split(diabetes.data, diabetes.target, random_state=0) model = Ridge(alpha=1e-2).fit(X_train, y_train) # Computing permutation importances compute_permutation_importance(model, X_val, y_val, scoring=[\'r2\', \'neg_mean_squared_error\'], n_repeats=30, random_state=0) ``` # Constraints: - Assume standard libraries (numpy, pandas, sklearn) are available. - Focus on both functionality and clarity. - Provide appropriate comments and documentation within your code.","solution":"import numpy as np import pandas as pd from sklearn.inspection import permutation_importance def compute_permutation_importance(model, X, y, scoring=\'r2\', n_repeats=30, random_state=None): Calculate and display the permutation feature importance for a given scikit-learn model. Parameters: model: A fitted scikit-learn estimator. X: The feature matrix (Pandas DataFrame or NumPy array). y: The target variable (Pandas Series or NumPy array). scoring: A single string or a list of strings representing the scoring metric(s) (default is \'r2\'). n_repeats: The number of times to shuffle a feature (default is 30). random_state: Integer seed for reproducibility (default is None). if isinstance(scoring, str): scoring = [scoring] importances = {} for score in scoring: result = permutation_importance(model, X, y, scoring=score, n_repeats=n_repeats, random_state=random_state) importances[score] = result for score in scoring: print(f\\"nFeature importances for scoring metric \'{score}\':\\") sorted_idx = importances[score].importances_mean.argsort()[::-1] for i in sorted_idx: print(f\\"Feature {i} ({X.columns[i] if hasattr(X, \'columns\') else i}): \\" f\\"Mean importance = {importances[score].importances_mean[i]:.4f}, \\" f\\"Std = {importances[score].importances_std[i]:.4f}\\")"},{"question":"# Question: Custom Interactive Python Interpreter You are tasked with creating a custom interactive Python interpreter using the facilities provided by the `code` module. Your interpreter should enable the user to input and execute Python code interactively, similar to the standard Python interpreter but with some custom behaviors. Requirements: 1. Implement a class `CustomInteractiveConsole` that inherits from `code.InteractiveConsole`. 2. Override the `write(data)` method to write output to a file named `interpreter_output.txt` instead of the standard error stream. 3. Create a method `run_interpreter()` in the `CustomInteractiveConsole` class to: - Display a custom banner message \\"Welcome to the Custom Python Interpreter!\\". - Continuously read and execute user input until the user types \\"exit()\\". - Display a custom exit message \\"Exiting the Custom Python Interpreter. Goodbye!\\". Functions and Methods: - **class CustomInteractiveConsole(code.InteractiveConsole)** - `def write(self, data: str) -> None`: Override this method to write `data` to a file (`interpreter_output.txt`). - `def run_interpreter(self) -> None`: Implement this method to start the interactive console with a custom banner and exit message. Example Usage: ```python if __name__ == \\"__main__\\": my_console = CustomInteractiveConsole() my_console.run_interpreter() ``` Input and Output: - The console reads input interactively from the user. - All outputs (e.g., results of executed commands, errors) should be written to `interpreter_output.txt`. Constraints: - You should not use any external libraries other than the standard Python library. - Ensure that the implementation handles typical interactive interpreter features like multi-line commands and error handling. # Notes: 1. Use the provided `code` module functions and classes effectively. 2. Test your implementation thoroughly to ensure it mimics standard Python interpreter behavior with the added custom functionalities.","solution":"import code class CustomInteractiveConsole(code.InteractiveConsole): def __init__(self, locals=None): super().__init__(locals) self.output_file = \'interpreter_output.txt\' def write(self, data): with open(self.output_file, \'a\') as f: f.write(data) def run_interpreter(self): banner = \\"Welcome to the Custom Python Interpreter!\\" exit_message = \\"Exiting the Custom Python Interpreter. Goodbye!\\" # Print custom banner print(banner) # Start interactive console try: self.interact() except SystemExit: # Print custom exit message print(exit_message) if __name__ == \\"__main__\\": my_console = CustomInteractiveConsole() my_console.run_interpreter()"},{"question":"# Question: Implementing a Custom Extension Type in Python Objective Create a custom extension type `CustomType` in CPython that supports: 1. Basic memory management. 2. String representation using `repr()` and `str()`. 3. Attribute management (get and set attributes). 4. Basic comparison operations. 5. Iterator support. Guidelines 1. **Memory Management:** - Implement the `tp_dealloc` method to manage memory cleanup for instances of `CustomType`. 2. **Object Representation:** - Implement `tp_repr` to provide a representation of the object. - Implement `tp_str` to provide a string representation for human readability. 3. **Attribute Management:** - Implement `tp_getattr` and `tp_setattr` to get and set attributes, respectively. 4. **Comparison Operations:** - Implement `tp_richcompare` to handle equality (`==`) and inequality (`!=`) operations based on an internal integer attribute. 5. **Iterator Support:** - Implement `tp_iter` and `tp_iternext` to support iteration over a range attribute in the object. Detailed Specification 1. **Basic Structure:** ```c typedef struct { PyObject_HEAD int counter; int *elements; Py_ssize_t num_elements; PyObject *weakreflist; } CustomTypeObject; ``` 2. **Expected Methods and Functions:** - `CustomType_dealloc` - `CustomType_repr` - `CustomType_str` - `CustomType_getattr` - `CustomType_setattr` - `CustomType_richcompare` - `CustomType_iter` - `CustomType_iternext` 3. **Initialization and Finalization:** - Allocate memory for the `elements` array in the constructor. - Clean up the `elements` array in the deallocator. 4. **String Representation:** - `tp_repr` should return a string \\"`CustomType(counter=<counter>, num_elements=<num_elements>)`\\". - `tp_str` should return a simple string \\"`CustomType with <num_elements> elements and counter <counter>`\\". 5. **Attribute Management:** - Support getting and setting `counter` and `elements`. 6. **Comparison Operations:** - Implement equality (`==`) and inequality (`!=`) based on the `counter` attribute. 7. **Iterator:** - Support iterating over the `elements` array. Constraints - Objects must handle exceptions gracefully and ensure proper memory management. - Ensure thread-safety where applicable. - Handle edge cases such as empty arrays or invalid attribute accesses. Example Usage in Python ```python import mymodule def main(): obj = mymodule.CustomType(counter=5, elements=[1, 2, 3]) print(repr(obj)) print(str(obj)) # Testing attribute access print(obj.counter) # Output: 5 obj.counter = 10 print(obj.counter) # Output: 10 # Testing comparisons obj2 = mymodule.CustomType(counter=10, elements=[4, 5, 6]) print(obj == obj2) # Output: True obj2.counter = 20 print(obj != obj2) # Output: True # Testing iteration for element in obj: print(element) # Output: 1, 2, 3 if __name__ == \\"__main__\\": main() ``` Implement the `CustomType` in CPython following the specification. You need to write the necessary C functions and structure definitions to support the described operations.","solution":"class CustomType: def __init__(self, counter, elements): self.counter = counter self.elements = elements self._iter_index = 0 def __repr__(self): return f\\"CustomType(counter={self.counter}, num_elements={len(self.elements)})\\" def __str__(self): return f\\"CustomType with {len(self.elements)} elements and counter {self.counter}\\" def __eq__(self, other): if isinstance(other, CustomType): return self.counter == other.counter return NotImplemented def __ne__(self, other): if isinstance(other, CustomType): return self.counter != other.counter return NotImplemented def __iter__(self): self._iter_index = 0 return self def __next__(self): if self._iter_index < len(self.elements): result = self.elements[self._iter_index] self._iter_index += 1 return result else: raise StopIteration def get_counter(self): return self.counter def set_counter(self, value): self.counter = value def get_elements(self): return self.elements def set_elements(self, value): self.elements = value"},{"question":"# Asynchronous Chat Server with `asynchat` Objective You are tasked with building a simple asynchronous chat server using the `asynchat` module. This server should be capable of handling multiple client connections, allowing them to send and receive messages in an asynchronous manner. Instructions 1. Create a class `ChatSession` that inherits from `asynchat.async_chat`. 2. Implement the following methods within your `ChatSession` class: - `__init__(self, sock, addr)`: Initializes the session, setting the initial terminator and any necessary instance attributes. - `collect_incoming_data(self, data)`: Collects incoming data and stores it in a buffer. - `found_terminator(self)`: Processes complete lines of input and broadcasts messages to other connected clients. 3. Create a class `ChatServer` that inherits from `asyncore.dispatcher`: - Implement the `__init__(self, host, port)` method to initialize the server socket and start listening for connections. - Implement the `handle_accepted(self, sock, addr)` method to create new `ChatSession` objects for incoming connections. Requirements - Clients should send messages terminated by a newline character (`n`). - Upon receiving a complete message, the server should broadcast it to all connected clients except the sender. - The server should handle socket errors gracefully without crashing. Example Usage ```python import asyncore import asynchat import socket class ChatSession(asynchat.async_chat): clients = [] def __init__(self, sock, addr): asynchat.async_chat.__init__(self, sock=sock) self.addr = addr self.ibuffer = [] self.set_terminator(b\'n\') ChatSession.clients.append(self) def collect_incoming_data(self, data): self.ibuffer.append(data) def found_terminator(self): message = b\'\'.join(self.ibuffer).decode(\'utf-8\').strip() self.ibuffer = [] self.broadcast(message) def broadcast(self, message): for client in ChatSession.clients: if client != self: client.push((message + \'n\').encode(\'utf-8\')) class ChatServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) def handle_accepted(self, sock, addr): print(f\'Incoming connection from {addr}\') ChatSession(sock, addr) server = ChatServer(\'localhost\', 12345) asyncore.loop() ``` 1. Create an instance of `ChatServer` and start the async loop. Constraints - Ensure your implementation can handle possible socket errors and client disconnections gracefully. Submission Submit your implementation of the `ChatSession` and `ChatServer` classes.","solution":"import asyncore import asynchat import socket class ChatSession(asynchat.async_chat): clients = [] def __init__(self, sock, addr): asynchat.async_chat.__init__(self, sock=sock) self.addr = addr self.ibuffer = [] self.set_terminator(b\'n\') ChatSession.clients.append(self) def collect_incoming_data(self, data): self.ibuffer.append(data) def found_terminator(self): message = b\'\'.join(self.ibuffer).decode(\'utf-8\').strip() self.ibuffer = [] self.broadcast(message) def broadcast(self, message): for client in ChatSession.clients: if client != self: try: client.push((message + \'n\').encode(\'utf-8\')) except socket.error: # Handle broken socket connection self.handle_close() def handle_close(self): ChatSession.clients.remove(self) self.close() class ChatServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) def handle_accepted(self, sock, addr): print(f\'Incoming connection from {addr}\') ChatSession(sock, addr)"},{"question":"# Custom Cell Object Implementation You are tasked with implementing a custom `Cell` class in Python that emulates the behavior of cell objects as detailed in the provided documentation. Objectives: 1. Implement a `Cell` class with the following methods: - `__init__(self, value=None)`: Initialize the cell object with an optional initial value. - `get(self)`: Return the current value stored in the cell. - `set(self, value)`: Set the value stored in the cell. - `is_cell(obj)`: Class method that checks if a given object is an instance of `Cell`. 2. Ensure that: - The `get` method returns the value stored in the cell. - The `set` method updates the value within the cell and can accept `None` as a valid value. - The `is_cell` method correctly identifies instances of the `Cell` class. Function Signatures: ```python class Cell: def __init__(self, value=None): pass def get(self): pass def set(self, value): pass @classmethod def is_cell(cls, obj): pass ``` Constraints: - You are **not allowed** to use any external libraries or modules. - Your implementation should handle typical use cases, including edge cases like setting the value to `None`. Example Usage: ```python c = Cell(5) print(c.get()) # Output: 5 c.set(10) print(c.get()) # Output: 10 print(Cell.is_cell(c)) # Output: True print(Cell.is_cell(123)) # Output: False c.set(None) print(c.get()) # Output: None ``` Assessment Criteria: - Correctness: The class should behave as specified. - Edge Cases: The class should handle cases like setting values to `None`. - Code Quality: The code should be clean, well-documented, and follow Python conventions.","solution":"class Cell: def __init__(self, value=None): self.value = value def get(self): return self.value def set(self, value): self.value = value @classmethod def is_cell(cls, obj): return isinstance(obj, Cell)"},{"question":"**Objective:** Implement a C program that embeds Python, reads a Python script from a file, initializes the Python interpreter, and calls a specific Python function with arguments from the C program. Return the result back to the C application and print it. **Instructions:** 1. Create a C application that: - Initializes the Python interpreter. - Reads a Python script from a file specified by a command-line argument. - Calls a specified function from the Python script (also specified by a command-line argument) with integer arguments. - Handles potential errors cautiously and prints appropriate messages for any errors. - Cleans up and finalizes the Python interpreter before exiting. 2. The Python script file (`script.py`) contains a function `calculate` that takes two integer arguments and returns their sum. ```python def calculate(a, b): return a + b ``` 3. Your C program should: - Load the Python script. - Retrieve the function `calculate` from the script. - Call the `calculate` function with two integers read from the command-line arguments. - Print the result of the function call. **Constraints:** - The C program should handle situations where the script file does not exist, the function does not exist, or the arguments are invalid. - You must use the lower-level Python/C API for this task. **Expected Input and Output:** 1. Command to compile your C code: ```bash gcc -o embed_program embed_program.c -I/usr/include/python3.10 -lpython3.10 ``` 2. Command to run your C program: ```bash ./embed_program script.py calculate 5 10 ``` 3. Expected Output: ``` Result of call: 15 ``` **Performance Requirements:** - Ensure your program runs efficiently and handles memory management appropriately to prevent leaks. - Handle error cases gracefully and provide helpful debug information. **Files Provided:** - `embed_program.c`: Template of your C program. - `script.py`: Python script containing the `calculate` function. **Sample C Code Template** ```c #define PY_SSIZE_T_CLEAN #include <Python.h> #include <stdio.h> #include <stdlib.h> int main(int argc, char *argv[]) { if (argc < 5) { fprintf(stderr, \\"Usage: %s script_path function_name arg1 arg2n\\", argv[0]); return 1; } char *script_path = argv[1]; char *function_name = argv[2]; int arg1 = atoi(argv[3]); int arg2 = atoi(argv[4]); FILE *file = fopen(script_path, \\"r\\"); if (!file) { fprintf(stderr, \\"Could not open Python script.n\\"); return 1; } Py_Initialize(); PyObject *pName, *pModule, *pFunc; PyObject *pArgs, *pValue; pName = PyUnicode_DecodeFSDefault(script_path); pModule = PyImport_Import(pName); Py_DECREF(pName); if (pModule != NULL) { pFunc = PyObject_GetAttrString(pModule, function_name); if (pFunc && PyCallable_Check(pFunc)) { pArgs = PyTuple_New(2); PyTuple_SetItem(pArgs, 0, PyLong_FromLong(arg1)); PyTuple_SetItem(pArgs, 1, PyLong_FromLong(arg2)); pValue = PyObject_CallObject(pFunc, pArgs); Py_DECREF(pArgs); if (pValue != NULL) { printf(\\"Result of call: %ldn\\", PyLong_AsLong(pValue)); Py_DECREF(pValue); } else { Py_DECREF(pFunc); Py_DECREF(pModule); PyErr_Print(); fprintf(stderr, \\"Call failedn\\"); return 1; } } else { if (PyErr_Occurred()) PyErr_Print(); fprintf(stderr, \\"Cannot find function \\"%s\\"n\\", function_name); } Py_XDECREF(pFunc); Py_DECREF(pModule); } else { PyErr_Print(); fprintf(stderr, \\"Failed to load \\"%s\\"n\\", script_path); return 1; } if (Py_FinalizeEx() < 0) { return 120; } fclose(file); return 0; } ```","solution":"def calculate(a, b): Returns the sum of a and b. return a + b"},{"question":"Objective Your task is to use the seaborn.objects interface to create an informative plot that visualizes data from the `titanic` dataset. The dataset will be loaded from seaborn and requires some preprocessing. Your plot should show the survival rate over different age groups with confidence intervals, and should differentiate between male and female passengers. Requirements - Load the `titanic` dataset using `seaborn.load_dataset`. - Preprocess the dataset to ensure no missing values in the `age`, `sex`, and `survived` columns. - Create age bins (e.g., intervals of 10 years) to group passengers by age. - Use `so.Plot` to create a line plot that shows the survival rate for each age bin, differentiated by gender. - Add a band to represent the confidence interval around the survival rate for each age bin. - Customize the plot to improve its clarity and aesthetics (e.g., adding titles, labels, and adjusting the appearance of the bands and lines). Input - There are no explicit inputs as you will load the dataset within the code. Output - Your code should produce and display a plot. The plot needs to clearly show the survival rates with confidence intervals for different age groups and should differentiate the data by gender. Constraints - Ensure that the dataset is filtered for complete cases without missing values in the relevant columns. - Use appropriate functions and methods from `seaborn.objects` to create the plot. Example ```python import seaborn as sns import seaborn.objects as so import pandas as pd # Load and preprocess the dataset titanic = sns.load_dataset(\\"titanic\\") titanic = titanic.dropna(subset=[\'age\', \'sex\', \'survived\']) # Create age bins age_bins = pd.cut(titanic[\'age\'], bins=range(0, 91, 10), right=False) # Group by age bins and gender, and calculate survival rate grouped = titanic.groupby([age_bins, \'sex\']).agg(survival_rate=(\'survived\', \'mean\')).reset_index() # Create the plot p = so.Plot(grouped, x=\'age\', y=\'survival_rate\', color=\'sex\') p.add(so.Line(), so.Agg(), group=\'sex\') p.add(so.Band(), so.Est()) # Customize the plot p.label(x=\'Age Group\', y=\'Survival Rate\', title=\'Survival Rate by Age Group and Gender on the Titanic\') p.show() ``` Note - Ensure that your plot includes meaningful axis labels and a title. - Make sure the bands representing the confidence intervals are clearly visible and distinct for each gender.","solution":"import seaborn as sns import pandas as pd import seaborn.objects as so def plot_titanic_survival_rate(): # Load and preprocess the dataset titanic = sns.load_dataset(\\"titanic\\") titanic = titanic.dropna(subset=[\'age\', \'sex\', \'survived\']) # Create age bins age_bins = pd.cut(titanic[\'age\'], bins=range(0, 91, 10), right=False, include_lowest=True, labels=[f\'{i}-{i+9}\' for i in range(0, 90, 10)]) titanic[\'age_group\'] = age_bins # Group by age bins and gender, and calculate survival rate grouped = titanic.groupby([\'age_group\', \'sex\']).agg(survival_rate=(\'survived\', \'mean\')).reset_index() # Create the plot p = so.Plot(grouped, x=\'age_group\', y=\'survival_rate\', color=\'sex\') p.add(so.Line(), so.Agg(), group=\'sex\').add(so.Band(), so.Est()) # Customize the plot p.label(x=\'Age Group\', y=\'Survival Rate\', title=\'Survival Rate by Age Group and Gender on the Titanic\') p.show() # Run the function to generate the plot plot_titanic_survival_rate()"},{"question":"# PyTorch Distributed Subprocess Management Objectives You are required to use the PyTorch elastic multiprocessing library to manage subprocesses in a distributed setting. Specifically, you will utilize the `SubprocessHandler` class. Task Implement a Python function `create_and_manage_subprocess` that: 1. Initializes a `SubprocessHandler` to manage multiple subprocesses. 2. Launches a given number of worker subprocesses that perform tensor addition. 3. Ensures synchronization across subprocesses such that all workers complete their task and return their results. Function Signature ```python def create_and_manage_subprocess(num_workers: int, tensors: List[torch.Tensor]) -> List[torch.Tensor]: pass ``` Input - `num_workers` (int): The number of subprocesses to create. - `tensors` (List[torch.Tensor]): A list of PyTorch tensors that each worker will add a constant value to. Output - Returns a list of PyTorch tensors with the addition operation completed by each corresponding worker subprocess. Constraints - Each worker will add a constant value of `3` to its assigned tensor. - You must use the `SubprocessHandler` class from the `torch.distributed.elastic.multiprocessing.subprocess_handler` module. - Ensure proper handling and synchronization of subprocesses (`join` subprocesses before returning results). Example ```python import torch # Example tensors to be processed tensors = [torch.tensor([1, 2, 3]), torch.tensor([4, 5, 6]), torch.tensor([7, 8, 9])] # Expected modified tensors after addition expected_result = [ torch.tensor([4, 5, 6]), torch.tensor([7, 8, 9]), torch.tensor([10, 11, 12]) ] result = create_and_manage_subprocess(3, tensors) assert result == expected_result ``` Notes - You are required to write the function `create_and_manage_subprocess`. - Carefully manage subprocess lifecycle and ensure they terminate correctly. Performance Requirements - Your implementation should handle reasonable number of subprocesses and tensors efficiently. - It should gracefully handle any potential exceptions during subprocess execution.","solution":"import torch from torch.multiprocessing import Process, Manager def worker_task(tensor, result, idx): Worker function to perform tensor addition result[idx] = tensor + 3 def create_and_manage_subprocess(num_workers: int, tensors: list) -> list: Creates and manages subprocesses to perform tensor addition. :param num_workers: Number of subprocesses to create :param tensors: List of PyTorch tensors :return: List of PyTorch tensors with addition operation completed manager = Manager() result = manager.list([None] * num_workers) processes = [] for i in range(num_workers): p = Process(target=worker_task, args=(tensors[i], result, i)) processes.append(p) p.start() for p in processes: p.join() return list(result)"},{"question":"Objective Write a Python function to refactor and modify specific elements within an Abstract Syntax Tree (AST). The task will involve navigating and transforming an AST generated from given source code. Problem Statement Given a Python source code string, we want to modify all occurrences of binary addition operations (i.e., `+`) such that: - Each binary addition (`left_expr + right_expr`) should be transformed into a function call to `add(left_expr, right_expr)`. You need to implement a function `transform_addition(source: str) -> str` that: 1. Parses the given source code string to its corresponding AST. 2. Transforms all binary addition operations as specified. 3. Returns the transformed source code as a string. Input Format - A single string `source` representing the Python source code. Output Format - A single string representing the transformed Python source code. Constraints - You may assume that the input source code string will be valid Python code. - The transformation should preserve the original structure of the code as much as possible except for the changes specified. Example # Input ```python source_code = \\"x = a + bnc = d + e + fng = h + add(i, j)\\" ``` # Output ```python \\"x = add(a, b)nc = add(add(d, e), f)ng = add(h, add(i, j))\\" ``` Function Signature ```python def transform_addition(source: str) -> str: pass ``` Notes 1. Use the `ast` module to accomplish the parsing and transformation. 2. Ensure to replace binary addition operations at all nesting levels. 3. Use `ast.unparse` to convert the transformed AST back to a source code string. Implementation Steps 1. Parse the input source code string into an AST using `ast.parse`. 2. Create a custom transformer class inheriting from `ast.NodeTransformer` to navigate and transform the AST. 3. Replace all `BinOp` nodes with an addition operator (`+`) to a function call to `add`. 4. Use `ast.unparse` to convert the transformed AST back to a string. 5. Return the transformed code string. Evaluation Criteria - Correctness: The function correctly transforms all binary addition operations as specified. - Completeness: All occurrences of binary additions are transformed. - Code quality: The code is well-structured and follows best practices. Good luck!","solution":"import ast def transform_addition(source: str) -> str: class AdditionTransformer(ast.NodeTransformer): def visit_BinOp(self, node): self.generic_visit(node) if isinstance(node.op, ast.Add): return ast.Call( func=ast.Name(id=\'add\', ctx=ast.Load()), args=[ node.left, node.right ], keywords=[] ) return node tree = ast.parse(source) transformer = AdditionTransformer() transformed_tree = transformer.visit(tree) transformed_code = ast.unparse(transformed_tree) return transformed_code"},{"question":"We will be tackling a problem that involves implementing and utilizing a distributed optimizer in PyTorch. # Distributed Optimizer Implementation Task **Objective**: Implement a function using PyTorch\'s `DistributedOptimizer` to perform an optimization step in a distributed environment. Assume CUDA tensors are not supported, so only CPU tensors will be used. # `perform_distributed_optimization_step` function Input: - `model`: A PyTorch neural network model. - `data_loader`: A PyTorch DataLoader object that provides the dataset. - `criterion`: A loss function to be optimized. - `optimizer_cls`: A PyTorch optimizer class (e.g., `torch.optim.SGD`). - `learning_rate`: A float specifying the learning rate. - `world_size`: An integer specifying the number of processes/devices participating in the training. - `rank`: An integer specifying the rank of the current process. - `backend`: A string specifying the backend used for distributed training (e.g., `\'gloo\'`). Output: Construct and return the loss after performing one optimization step. # Constraints: - Only CPU tensors are to be used. - Assume the model, data_loader, criterion, and optimizer_cls are all implemented correctly and provided. - The function should be able to handle distributed training context. # Performance Requirement: - The function should correctly perform one update step across multiple processes. # Example Usage ```python import torch import torch.distributed as dist from torch.optim import SGD from torch.nn import MSELoss from torch.utils.data import DataLoader, TensorDataset # Assume model, dataset, world_size, and rank are defined def perform_distributed_optimization_step(model, data_loader, criterion, optimizer_cls, learning_rate, world_size, rank, backend): # Implement the function pass model = torch.nn.Linear(10, 1) data = torch.randn(100, 10) targets = torch.randn(100, 1) dataset = TensorDataset(data, targets) data_loader = DataLoader(dataset, batch_size=32) criterion = MSELoss() learning_rate = 0.01 world_size = 4 rank = 0 # Correctly handle the distributed context dist.init_process_group(backend=\'gloo\', rank=rank, world_size=world_size) loss = perform_distributed_optimization_step(model, data_loader, criterion, SGD, learning_rate, world_size, rank, \'gloo\') print(f\'Loss: {loss}\') ``` Create the function `perform_distributed_optimization_step` that should perform the given task.","solution":"import torch import torch.nn as nn import torch.optim as optim import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP from torch.utils.data import DataLoader def perform_distributed_optimization_step(model, data_loader, criterion, optimizer_cls, learning_rate, world_size, rank, backend): dist.init_process_group(backend=backend, rank=rank, world_size=world_size) # Wrap the model model = DDP(model) # Define the optimizer optimizer = optimizer_cls(model.parameters(), lr=learning_rate) model.train() total_loss = 0.0 for batch_idx, (data, targets) in enumerate(data_loader): optimizer.zero_grad() output = model(data) loss = criterion(output, targets) loss.backward() optimizer.step() total_loss += loss.item() # Calculate the average loss average_loss = total_loss / len(data_loader) # Cleanup if necessary dist.destroy_process_group() return average_loss"},{"question":"Problem Statement You are provided with a dataset of student grades. The dataset is stored in a pandas DataFrame `grades_df` with the following columns: - `student_id`: A unique identifier for each student. - `name`: The name of the student. - `math`: The student\'s grade in mathematics. - `science`: The student\'s grade in science. - `english`: The student\'s grade in English. - `history`: The student\'s grade in history. The dataset might have missing values. # Tasks 1. **Data Selection and Filtering:** - Write a function `get_students_with_minimum_grades(grades_df, min_grade)` that takes the DataFrame `grades_df` and a dictionary `min_grade` as inputs. The `min_grade` dictionary contains minimum grade thresholds for subjects. Return a DataFrame containing only the students who meet or exceed the given minimum grades in all specified subjects. ```python import pandas as pd def get_students_with_minimum_grades(grades_df, min_grade): Parameters: grades_df (pd.DataFrame): DataFrame containing student grades. min_grade (dict): Dictionary with subjects as keys and minimum grade threshold as values. Returns: pd.DataFrame: Filtered DataFrame with students who meet the minimum grade criteria. pass ``` 2. **Calculating Average Grades:** - Write a function `calculate_average_grades(grades_df)` that returns a Series with the average grade for each student. Ignore missing grades when calculating the average. ```python def calculate_average_grades(grades_df): Parameters: grades_df (pd.DataFrame): DataFrame containing student grades. Returns: pd.Series: A Series with the average grade for each student. pass ``` 3. **Handling Missing Data:** - Write a function `fill_missing_grades(grades_df, default_grade)` that fills missing grades with the given `default_grade`. ```python def fill_missing_grades(grades_df, default_grade): Parameters: grades_df (pd.DataFrame): DataFrame containing student grades. default_grade (float): The grade to fill in for missing values. Returns: pd.DataFrame: DataFrame with missing grades filled. pass ``` 4. **Setting Data:** - Write a function `update_student_grades(grades_df, student_id, new_grades)` that updates the grades for a specific student identified by `student_id`. The `new_grades` dictionary contains subjects as keys and the new grades as values. ```python def update_student_grades(grades_df, student_id, new_grades): Parameters: grades_df (pd.DataFrame): DataFrame containing student grades. student_id (int): The unique identifier for the student to update. new_grades (dict): Dictionary with subjects as keys and new grades as values. Returns: pd.DataFrame: DataFrame with updated grades. pass ``` # Constraints - Assume `grades_df` will always contain the mentioned columns. - Grades are numeric values between 0 and 100. - `student_id` is unique for each student. # Example Given the following DataFrame: ```python data = { \\"student_id\\": [1, 2, 3], \\"name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\"], \\"math\\": [90, 80, None], \\"science\\": [85, None, 75], \\"english\\": [None, 70, 82], \\"history\\": [88, 86, 90] } grades_df = pd.DataFrame(data) ``` - `get_students_with_minimum_grades(grades_df, {\\"math\\": 85, \\"history\\": 85})` should return the DataFrame with only \\"Alice\\". - `calculate_average_grades(grades_df)` should return a Series with average grades for each student. - `fill_missing_grades(grades_df, 75)` should fill missing values with 75. - `update_student_grades(grades_df, 2, {\\"science\\": 78, \\"english\\": 85})` should update Bob\'s grades for science and English.","solution":"import pandas as pd def get_students_with_minimum_grades(grades_df, min_grade): Parameters: grades_df (pd.DataFrame): DataFrame containing student grades. min_grade (dict): Dictionary with subjects as keys and minimum grade threshold as values. Returns: pd.DataFrame: Filtered DataFrame with students who meet the minimum grade criteria. filters = [] for subject, min_val in min_grade.items(): filters.append(grades_df[subject] >= min_val) return grades_df.loc[pd.concat(filters, axis=1).all(axis=1)] def calculate_average_grades(grades_df): Parameters: grades_df (pd.DataFrame): DataFrame containing student grades. Returns: pd.Series: A Series with the average grade for each student. return grades_df.loc[:, \'math\':\'history\'].mean(axis=1) def fill_missing_grades(grades_df, default_grade): Parameters: grades_df (pd.DataFrame): DataFrame containing student grades. default_grade (float): The grade to fill in for missing values. Returns: pd.DataFrame: DataFrame with missing grades filled. return grades_df.fillna(default_grade) def update_student_grades(grades_df, student_id, new_grades): Parameters: grades_df (pd.DataFrame): DataFrame containing student grades. student_id (int): The unique identifier for the student to update. new_grades (dict): Dictionary with subjects as keys and new grades as values. Returns: pd.DataFrame: DataFrame with updated grades. for subject, grade in new_grades.items(): grades_df.loc[grades_df[\'student_id\'] == student_id, subject] = grade return grades_df"},{"question":"# Kernel Approximation Coding Assessment Objective Your task is to demonstrate your understanding of kernel approximations in scikit-learn by implementing a machine learning pipeline that uses one of the kernel approximation methods provided. You will work with the `RBFSampler` to approximate the RBF kernel and apply it to a classification problem. Problem Statement You are given a dataset in CSV format with features and corresponding labels. Your task is to: 1. load the dataset, 2. preprocess it, 3. apply `RBFSampler` for kernel approximation, 4. train a classifier, 5. evaluate the model, and 6. output the accuracy score. Dataset The dataset `data.csv` consists of: - features (columns `X1`, `X2`, ..., `Xd`) - labels (column `label`) Requirements 1. Load the dataset using pandas. 2. Preprocess: - Separate features and labels. - Normalize the feature values to have zero mean and unit variance. 3. Apply `RBFSampler` with `gamma=0.5` and `n_components=100` to the normalized features. 4. Train a `SGDClassifier` on the transformed features. 5. Evaluate the model using 5-fold cross-validation. 6. Output the average accuracy score obtained from the cross-validation. Input Format - A CSV file named `data.csv` with multiple feature columns (`X1`, `X2`, ..., `Xd`) and one label column named `label`. Output Format - A single floating-point number representing the average accuracy score for the 5-fold cross-validation. Constraints - You should use `RBFSampler` and `SGDClassifier` from scikit-learn. - Ensure proper handling of pipeline steps (i.e., preprocessing, transformation, training, evaluation). Example Code ```python import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.kernel_approximation import RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.model_selection import cross_val_score # Load dataset data = pd.read_csv(\'data.csv\') X = data.drop(columns=\'label\') y = data[\'label\'] # Standardize features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Apply RBF Sampler rbf_sampler = RBFSampler(gamma=0.5, n_components=100) X_features = rbf_sampler.fit_transform(X_scaled) # Train and evaluate classifier clf = SGDClassifier() scores = cross_val_score(clf, X_features, y, cv=5) # Output the average accuracy print(scores.mean()) ``` Submission Submit your code implementation along with the output produced when evaluated with the provided dataset.","solution":"import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.kernel_approximation import RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.model_selection import cross_val_score def evaluate_model(): # Load dataset data = pd.read_csv(\'data.csv\') X = data.drop(columns=\'label\') y = data[\'label\'] # Standardize features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Apply RBF Sampler rbf_sampler = RBFSampler(gamma=0.5, n_components=100) X_features = rbf_sampler.fit_transform(X_scaled) # Train and evaluate classifier clf = SGDClassifier() scores = cross_val_score(clf, X_features, y, cv=5) # Output the average accuracy return scores.mean()"},{"question":"# Advanced Python Coding Assessment Question Problem Statement: You are required to implement a Python function that interacts with a raw memory buffer (such as a `bytearray` or a custom buffer object). Your task is to create a function that can access and manipulate elements of a multi-dimensional buffer and perform specific transformations based on given constraints. Function Signature: ```python def transform_buffer(buffer_obj, transformation_function): Transforms the input buffer according to the provided transformation function. Parameters: - buffer_obj: An object that supports the buffer protocol. - transformation_function: A function that takes a single element from the buffer and returns its transformed value. Returns: - A new bytearray representing the transformed buffer. Example: >>> buf = bytearray([1, 2, 3, 4]) >>> def square(x): ... return x * x >>> transform_buffer(buf, square) bytearray(b\'x01x04x09x10\') pass ``` Requirements: 1. **Input and Output Formats**: - `buffer_obj`: Any object supporting the buffer protocol. - `transformation_function`: A function that takes a single element from the buffer and returns its transformed value. - The function should return a new `bytearray` that contains the transformed values. 2. **Constraints**: - You need to handle both read-only and writable buffers. - Ensure that the buffer is contiguous in memory for efficient access. - If the buffer is multi-dimensional, ensure the transformation is applied correctly to each element based on its indices. 3. **Performance Requirements**: - Avoid any unnecessary copying of data. - Ensure the implementation is efficient in terms of both time and space. Example: ```python >>> buf = bytearray([1, 2, 3, 4]) >>> def square(x): ... return x * x >>> transform_buffer(buf, square) bytearray(b\'x01x04x09x10\') ``` Additional Context: You may find it helpful to familiarize yourself with Python\'s buffer protocol and related functions such as `PyObject_GetBuffer`, `PyBuffer_Release`, and buffer property checks. This question aims to test your ability to interact with low-level memory access and manipulation in Python.","solution":"def transform_buffer(buffer_obj, transformation_function): Transforms the input buffer according to the provided transformation function. Parameters: - buffer_obj: An object that supports the buffer protocol. - transformation_function: A function that takes a single element from the buffer and returns its transformed value. Returns: - A new bytearray representing the transformed buffer. Example: >>> buf = bytearray([1, 2, 3, 4]) >>> def square(x): ... return x * x >>> transform_buffer(buf, square) bytearray(b\'x01x04x09x10\') # Create a writable view of the buffer\'s contents buf_view = memoryview(buffer_obj).cast(\'B\') # Apply the transformation function to each element transformed = bytearray(transformation_function(elm) for elm in buf_view) return transformed"},{"question":"Objective Your task is to demonstrate your understanding of `setuptools` by writing a Python script that generates a proper setup configuration for a given project. Description You are given the details of a Python project that you need to package using `setuptools`. The project configuration details are listed below, and you must programmatically create a `setup.py` script to manage this project\'s packaging and dependencies. # Project Details - **Project Name:** `example_project` - **Version:** `1.0.0` - **Description:** `A simple example project` - **Author:** `Jane Doe` - **Author Email:** `jane.doe@example.com` - **URL:** `https://example.com/example_project` - **Packages:** `example_package` - **Entry Points:** Console script named `example-cli` that maps to `example_package.cli:main` - **Dependencies:** - `requests` version `>=2.25.1` - `numpy` version `>=1.19.3` - **License:** `MIT` Requirements 1. Create a Python script that generates a `setup.py` file with the provided project details. 2. Ensure the generated `setup.py` is correctly formatted according to `setuptools` standards. 3. The generated `setup.py` should include: - Project metadata (name, version, description, author, etc.) - List of packages included in the project. - Entry points for console scripts. - Dependencies (install_requires). - License. Input - None (The provided project details should be hard-coded within the solution). Output - The script should write `setup.py` to the current directory with the correct configuration. Constraints - The solution should not involve manual creation of the `setup.py` file. It should be generated by your code. Example Here is an example of the kind of output your script should generate and write into a file named `setup.py`: ```python from setuptools import setup, find_packages setup( name=\'example_project\', version=\'1.0.0\', description=\'A simple example project\', author=\'Jane Doe\', author_email=\'jane.doe@example.com\', url=\'https://example.com/example_project\', packages=find_packages(include=[\'example_package\']), entry_points={ \'console_scripts\': [ \'example-cli = example_package.cli:main\', ], }, install_requires=[ \'requests>=2.25.1\', \'numpy>=1.19.3\', ], license=\'MIT\', ) ``` Submission Write a Python script that, when executed, creates the `setup.py` file with the above specifications. ```python # Your code here ```","solution":"def write_setup_py(): setup_contents = from setuptools import setup, find_packages setup( name=\'example_project\', version=\'1.0.0\', description=\'A simple example project\', author=\'Jane Doe\', author_email=\'jane.doe@example.com\', url=\'https://example.com/example_project\', packages=find_packages(include=[\'example_package\']), entry_points={ \'console_scripts\': [ \'example-cli = example_package.cli:main\', ], }, install_requires=[ \'requests>=2.25.1\', \'numpy>=1.19.3\', ], license=\'MIT\', ) with open(\'setup.py\', \'w\') as file: file.write(setup_contents)"},{"question":"<|Analysis Begin|> The provided documentation is for the `urllib.robotparser` module, part of Python\'s standard library. This module is used to parse `robots.txt` files and make decisions on whether a particular user agent can fetch a specific URL based on the rules specified in the `robots.txt` file. The key class `RobotFileParser` provides several methods: - `set_url(url)`: Sets the URL of the `robots.txt` file. - `read()`: Reads the `robots.txt` file. - `parse(lines)`: Parses provided lines as `robots.txt` content. - `can_fetch(useragent, url)`: Determines if a given user agent can fetch the URL based on the `robots.txt` rules. - `mtime()`: Returns the last modification time of the `robots.txt` file. - `modified()`: Sets the modification time of the `robots.txt` file to the current time. - `crawl_delay(useragent)`: Gets the \\"Crawl-delay\\" parameter for the user agent. - `request_rate(useragent)`: Gets the \\"Request-rate\\" parameter for the user agent. - `site_maps()`: Gets the \\"Sitemap\\" parameter from the `robots.txt`. These methods cover basic functionalities including setting up and parsing the `robots.txt` file, querying permissions for URL access, and checking crawl-related parameters. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** The goal of this task is to implement a function that determines if a set of URLs can be fetched by a given user agent according to the `robots.txt` rules of their respective websites. **Requirements:** 1. Your solution should use the `urllib.robotparser` module to achieve this. 2. Implement a function `can_fetch_urls(useragent, url_list)`, where: - `useragent` (string): The user agent string you will use to query the `robots.txt` files. - `url_list` (list of strings): A list of URLs that need to be checked. 3. The function should return a dictionary where the keys are the URLs and the values are booleans indicating whether the URL can be fetched (`True`) or not (`False`) by the user agent according to the `robots.txt` rules. 4. Ensure efficient fetching and parsing of `robots.txt` files to handle potentially large inputs. **Function Signature:** ```python import urllib.robotparser def can_fetch_urls(useragent: str, url_list: list[str]) -> dict[str, bool]: pass ``` **Example Usage:** ```python useragent = \\"*\\" url_list = [ \\"http://www.musi-cal.com/\\", \\"http://www.musi-cal.com/cgi-bin/search?city=San+Francisco\\", \\"http://example.com/resource\\", ] result = can_fetch_urls(useragent, url_list) print(result) # Expected output (based on hypothetical robots.txt rules): # { # \\"http://www.musi-cal.com/\\": True, # \\"http://www.musi-cal.com/cgi-bin/search?city=San+Francisco\\": False, # \\"http://example.com/resource\\": True, # } ``` **Constraints:** - Assume `url_list` can contain URLs from different domains. - Handle network errors gracefully, and assume fetching `robots.txt` files might fail occasionally. - Process URLs efficiently to avoid redundant fetching of `robots.txt` files from the same domain. **Hints:** - Consider how to handle URLs belonging to the same domain efficiently without fetching and parsing the `robots.txt` multiple times. - You might find the `urlparse` module helpful in breaking down URLs into their components. ---","solution":"import urllib.robotparser from urllib.parse import urlparse import requests def can_fetch_urls(useragent: str, url_list: list[str]) -> dict[str, bool]: domain_to_robot = {} results = {} for url in url_list: # Parse the domain from the URL parsed_url = urlparse(url) domain = f\\"{parsed_url.scheme}://{parsed_url.netloc}\\" # Check if we already have the robot parser for this domain if domain not in domain_to_robot: # Create a new RobotFileParser instance robot_parser = urllib.robotparser.RobotFileParser() robots_url = f\\"{domain}/robots.txt\\" try: # Fetch and read the robots.txt file response = requests.get(robots_url) if response.status_code == 200: robot_parser.parse(response.text.splitlines()) else: robot_parser = None # No valid robots.txt found except requests.RequestException: robot_parser = None # Handle network errors gracefully domain_to_robot[domain] = robot_parser robot_parser = domain_to_robot[domain] # Check if useragent can fetch the URL if robot_parser is not None: results[url] = robot_parser.can_fetch(useragent, url) else: # If there\'s no valid robots.txt, allow the fetch results[url] = True return results"},{"question":"# PyTorch Coding Assessment: Custom Autograd Function Objective Implement a custom autograd function in PyTorch that calculates the element-wise logarithm and its gradient. This custom function should handle saved tensors correctly and demonstrate the use of different gradient modes. Description You are required to implement a custom autograd function, `LogFunction`, which computes the natural logarithm (`torch.log`) of its input tensor. The function should properly save the intermediary results needed for computing gradients. Additionally, you should handle cases where the input tensor requires gradient and where it does not require gradient. Function Signatures ```python import torch class LogFunction(torch.autograd.Function): @staticmethod def forward(ctx, input): # Implement the forward computation and save necessary tensors pass @staticmethod def backward(ctx, grad_output): # Implement the backward computation using saved tensors pass def test_log_function(): # Test the LogFunction with tensors requiring gradients # and tensors in no-grad mode pass ``` Requirements 1. **Forward Method**: - Compute the natural logarithm of the input tensor. - Save the input tensor for the backward pass. 2. **Backward Method**: - Compute the gradient of the logarithm function. 3. **Test Function**: - Create a tensor with `requires_grad=True` and apply the `LogFunction`. - Perform a backward pass to compute the gradient. - Create a tensor in no-grad mode and apply the `LogFunction`, ensuring no gradient is tracked. Constraints - The input tensor will always contain positive values. - You should handle both CPU and GPU tensors. - Ensure that the implementation is efficient and does not unnecessarily store tensors. Example ```python def example_usage(): x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) log_x = LogFunction.apply(x) log_x.sum().backward() print(x.grad) # Expected output: tensor([1.0000, 0.5000, 0.3333]) with torch.no_grad(): y = torch.tensor([1.0, 2.0, 3.0]) log_y = LogFunction.apply(y) print(log_y) # Expected output: tensor([0.0000, 0.6931, 1.0986]) ``` Notes - Carefully read through each part of the documentation `Autograd mechanics` provided to understand how to implement the custom autograd function. - Make use of `ctx.save_for_backward()` and `ctx.saved_tensors` to manage saved tensors between the forward and backward passes. - Ensure to test the implementation with `test_log_function()`.","solution":"import torch class LogFunction(torch.autograd.Function): @staticmethod def forward(ctx, input): # Compute the natural logarithm of the input tensor log_input = torch.log(input) # Save the input tensor for the backward pass ctx.save_for_backward(input) return log_input @staticmethod def backward(ctx, grad_output): # Retrieve the saved input tensor input, = ctx.saved_tensors # Compute the gradient of the log function grad_input = grad_output / input return grad_input"},{"question":"You are given a dataset of tips from a restaurant. Using the seaborn library, create a series of KDE (Kernel Density Estimate) plots to analyze the `total_bill` and `tip` amounts. Your task is to write a function `plot_kde_distributions(data)` that will generate and save the following plots: 1. A univariate KDE plot of the `total_bill`. 2. A bivariate KDE plot of `total_bill` vs `tip`. 3. A univariate KDE plot of `total_bill`, conditionally colored by `time` (Dinner or Lunch). 4. A bivariate KDE plot of `total_bill` vs `tip`, conditionally colored by `size` (number of people). 5. The same bivariate KDE plot from (4), but with filled contours and a specific color palette. Each plot should be saved as a separate PNG file in the current directory with appropriate names such as `univariate_total_bill.png`, `bivariate_total_bill_tip.png`, etc. # Function Signature ```python def plot_kde_distributions(data: pd.DataFrame) -> None: pass ``` # Input * `data` (pd.DataFrame): A pandas DataFrame containing the restaurant tips dataset with at least the following columns: `total_bill`, `tip`, `time`, and `size`. # Output * The function should save five PNG files as described in the tasks above. # Constraints * Use seaborn\'s `kdeplot` function for generating KDE plots. * Ensure each plot is appropriately labeled and includes a legend where applicable. # Example ``` python import seaborn as sns import pandas as pd # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Call the function to generate the plots plot_kde_distributions(tips) ``` # Notes * The dataset can be loaded using `sns.load_dataset(\\"tips\\")`. * The examples provided above in the catalog are a useful reference on how to use the `kdeplot` function.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_kde_distributions(data: pd.DataFrame) -> None: # Univariate KDE plot of the total_bill plt.figure() sns.kdeplot(data=data, x=\'total_bill\') plt.title(\'Univariate KDE of Total Bill\') plt.savefig(\'univariate_total_bill.png\') plt.close() # Bivariate KDE plot of total_bill vs. tip plt.figure() sns.kdeplot(data=data, x=\'total_bill\', y=\'tip\') plt.title(\'Bivariate KDE of Total Bill vs Tip\') plt.savefig(\'bivariate_total_bill_tip.png\') plt.close() # Univariate KDE plot of total_bill, conditionally colored by time (Dinner or Lunch) plt.figure() sns.kdeplot(data=data, x=\'total_bill\', hue=\'time\') plt.title(\'Univariate KDE of Total Bill by Time\') plt.savefig(\'univariate_total_bill_by_time.png\') plt.close() # Bivariate KDE plot of total_bill vs. tip, conditionally colored by size (number of people) plt.figure() sns.kdeplot(data=data, x=\'total_bill\', y=\'tip\', hue=\'size\') plt.title(\'Bivariate KDE of Total Bill vs Tip by Size\') plt.savefig(\'bivariate_total_bill_tip_by_size.png\') plt.close() # Bivariate KDE plot of total_bill vs. tip with filled contours and a specific color palette plt.figure() sns.kdeplot(data=data, x=\'total_bill\', y=\'tip\', hue=\'size\', fill=True, palette=\'viridis\') plt.title(\'Bivariate KDE of Total Bill vs Tip by Size with Filled Contours\') plt.savefig(\'bivariate_total_bill_tip_by_size_filled.png\') plt.close()"},{"question":"You are tasked with creating a utility script that performs the following operations based on command line arguments: 1. Reads a list of mathematical expressions from an input text file. 2. Evaluates these expressions and writes the results into an output text file. 3. Prints the total time taken to process the expressions to the console. 4. Handles potential errors gracefully and logs them into an error log file. 5. Ensures the script is tested using automated tests. # Input Format 1. A text file containing one mathematical expression per line. 2. Command line arguments: - `--input <input_file>`: the input file containing mathematical expressions. - `--output <output_file>`: the output file to store the results. - `--log <log_file>`: the error log file. # Output Format 1. The output text file with each line containing the result of the corresponding expression. 2. An error log file with error messages if any expressions fail to evaluate. 3. The total time taken to process the expressions printed to the console. # Example **Input File (`expressions.txt`):** ``` 2 + 3 * 5 10 / 0 math.sin(math.pi / 2) random.random() ``` **Command Line Invocation:** ``` python script.py --input expressions.txt --output results.txt --log errors.log ``` **Output File (`results.txt`):** ``` 17 Error 1.0 0.5278237485642938 ``` **Log File (`errors.log`):** ``` Error in line 2: division by zero ``` **Console Output:** ``` Total processing time: 0.002 seconds ``` # Constraints 1. Assume the expressions are valid Python expressions. 2. Use try-except blocks to catch and log errors. 3. Measure the processing time using the \\"timeit\\" module. 4. Use the \\"argparse\\" module to handle command line arguments. 5. Ensure the script is tested using the \\"unittest\\" module. Implement the functionality and write appropriate tests for your script.","solution":"import argparse import timeit import math import random import traceback def evaluate_expressions(input_file, output_file, log_file): start_time = timeit.default_timer() error_dict = {} with open(input_file, \'r\') as infile, open(output_file, \'w\') as outfile: lines = infile.readlines() for line_number, line in enumerate(lines, start=1): try: result = eval(line.strip()) outfile.write(f\\"{result}n\\") except Exception as e: error_message = f\\"Error in line {line_number}: {str(e)}\\" error_dict[line_number] = error_message outfile.write(\\"Errorn\\") if error_dict: with open(log_file, \'w\') as errfile: for line_number, error_message in error_dict.items(): errfile.write(error_message + \'n\') end_time = timeit.default_timer() total_time = end_time - start_time print(f\\"Total processing time: {total_time:.3f} seconds\\") def main(): parser = argparse.ArgumentParser(description=\\"Process some mathematical expressions.\\") parser.add_argument(\'--input\', type=str, required=True, help=\'Input file containing mathematical expressions\') parser.add_argument(\'--output\', type=str, required=True, help=\'Output file to store the results\') parser.add_argument(\'--log\', type=str, required=True, help=\'Error log file\') args = parser.parse_args() evaluate_expressions(args.input, args.output, args.log) if __name__ == \\"__main__\\": main()"},{"question":"# Coding Assessment: Package Metadata Analyzer **Objective:** You are required to implement a function that utilizes the `importlib.metadata` package to fetch and analyze the metadata of all installed Python packages in the environment. The function should present summarized and detailed insights into the packages and their metadata. **Function Signature:** ```python def get_installed_packages_metadata() -> dict: pass ``` **Input:** - No input parameters. **Output:** - A dictionary where: - Keys are the names of installed packages (str). - Values are dictionaries with the following keys: - `version`: A string representing the version of the package. - `metadata`: A dictionary of the package\'s metadata. - `files`: A list of files installed by the distribution (file names as strings). - `requirements`: A list of requirements specified by the package. - `entry_points`: A dictionary where keys are entry point groups, and values are lists of entry point names. **Constraints:** - The function should operate in an environment with Python 3.10 installed. - Package metadata may be missing or incomplete; your function should handle these cases gracefully. **Example:** ```python { \'pip\': { \'version\': \'21.2.4\', \'metadata\': { \'Name\': \'pip\', \'Version\': \'21.2.4\', \'Summary\': \'The PyPA recommended tool for installing Python packages.\', \'Author\': \'The pip developers\', \'License\': \'MIT\', # ... other metadata fields }, \'files\': [\'pip/__init__.py\', \'pip/_internal/cli/main.py\', ...], \'requirements\': [\'setuptools\'], \'entry_points\': { \'console_scripts\': [\'pip\', \'pip3\'], # ... other entry point groups } }, # ... other installed packages } ``` # Tasks: 1. Use the `importlib.metadata` package to iterate through all installed packages. 2. Fetch the version, metadata, files, requirements, and entry points for each package. 3. Construct the resulting dictionary as specified. 4. Ensure to handle exceptions and missing data gracefully. **Notes:** - You may want to test your function within a controlled virtual environment to accurately assess all the installed packages. - This function will help understand how metadata can be utilized for package management and analysis in Python environments.","solution":"from importlib.metadata import distributions, metadata def get_installed_packages_metadata() -> dict: Fetch metadata about all the installed packages. Returns: dict: A dictionary containing metadata for each installed package. packages_metadata = {} for dist in distributions(): pkg_name = dist.metadata[\\"Name\\"] pkg_version = dist.version pkg_metadata = {k: v for k, v in dist.metadata.items()} # Get the list of files pkg_files = list(dist.files) # Get the list of requirements pkg_requirements = list(dist.requires) if dist.requires else [] # Get the entry points by grouping them pkg_entry_points = {} for entry_point in dist.entry_points: if entry_point.group not in pkg_entry_points: pkg_entry_points[entry_point.group] = [] pkg_entry_points[entry_point.group].append(entry_point.name) packages_metadata[pkg_name] = { \'version\': pkg_version, \'metadata\': pkg_metadata, \'files\': [str(file) for file in pkg_files] if pkg_files else [], \'requirements\': pkg_requirements, \'entry_points\': pkg_entry_points } return packages_metadata"},{"question":"Custom Sequence Implementation **Objective:** Implement a custom sequence class in Python that adheres to the `collections.abc.Sequence` interface. **Question:** You are required to implement a class named `CustomSequence` that inherits from `collections.abc.Sequence`. The class should demonstrate the following capabilities: 1. Support standard sequence operations such as indexing, length determination, and containment checks. 2. Be iterable, allowing iteration over its elements using a `for` loop. 3. Support the `count()` and `index()` methods as inherited from the `Sequence` mixin. 4. Override the `__contains__` mixin method to optimize element containment checks. The sequence should internally store its elements in a list. **Requirements:** - Your class must inherit from `collections.abc.Sequence`. - You should implement the following abstract methods: - `__getitem__(self, index)` - `__len__(self)` - Additionally, you must override the `__contains__` method to provide optimized checks for element containment. **Input and Output:** - There is no specific input format. The class will be tested with various sequence operations. - You do not need to handle inputs or outputs directly; just ensure your class implements the required methods correctly. **Constraints:** - Your implementation must handle indexing and slicing as expected for a sequence. - Ensure that the class raises appropriate exceptions for invalid operations, such as `IndexError` for out-of-bounds access. **Example Usage:** ```python from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, data): self._data = list(data) def __getitem__(self, index): return self._data[index] def __len__(self): return len(self._data) def __contains__(self, value): # Optimized containment check return value in self._data # Example usage seq = CustomSequence([1, 2, 3, 4, 5]) print(len(seq)) # Output: 5 print(seq[2]) # Output: 3 print(4 in seq) # Output: True print(seq.index(3)) # Output: 2 print(seq.count(2)) # Output: 1 for item in seq: print(item, end=\' \') # Output: 1 2 3 4 5 ``` You can include any additional methods or properties as needed, but ensure that the required methods are properly implemented. **Performance considerations:** - Ensure that the class performs efficiently with O(1) operations for indexing and containment checks, leveraging the underlying list structure. **Hints:** - Refer to the `collections.abc.Sequence` documentation to understand the methods that need to be implemented. - Consider the behaviors of standard Python sequences, such as lists and tuples, as reference points for your implementation.","solution":"from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, data): self._data = list(data) def __getitem__(self, index): return self._data[index] def __len__(self): return len(self._data) def __contains__(self, value): # Optimized containment check return value in self._data"},{"question":"**CUDA Backend Configuration with PyTorch** # Problem Statement You are required to implement a function in PyTorch that manipulates several CUDA backend settings. Your function should configure the CUDA backend to optimize for specific use cases by enabling or disabling features, adjusting cache sizes, and managing precision settings. # Function Signature ```python def configure_cuda_backend(fp16_reduction: bool = True, tf32_matrix_multiplication: bool = True, use_cudnn_benchmark: bool = False) -> dict: Configures the CUDA backend in PyTorch according to the specified parameters. Parameters: - fp16_reduction (bool): If True, enable reduced precision reduction with fp16 accumulation type. - tf32_matrix_multiplication (bool): If True, allow TensorFloat-32 tensor cores for matrix multiplication on Ampere or newer GPUs. - use_cudnn_benchmark (bool): If True, enables cuDNN benchmarking to use the fastest convolution algorithm. Returns: - A dictionary with the current state of the following settings: { \'allow_fp16_reduced_precision_reduction\': bool, \'allow_tf32\': bool, \'cudnn.benchmark\': bool, \'cudnn.benchmark_limit\': int, \'cufft_plan_cache_size\': int, \'cufft_plan_cache_max_size\': int, } pass ``` # Constraints 1. You must use available attributes and methods from `torch.backends.cuda` and `torch.backends.cudnn` to adjust and query configuration settings. 2. Assume that the necessary computational resources (like an NVIDIA GPU with Ampere architecture) are available. 3. You should not exceed the maximum limits for any cache size settings. # Example Usage ```python result = configure_cuda_backend(fp16_reduction=True, tf32_matrix_multiplication=False, use_cudnn_benchmark=True) print(result) # Output might look like: # { # \'allow_fp16_reduced_precision_reduction\': True, # \'allow_tf32\': False, # \'cudnn.benchmark\': True, # \'cudnn.benchmark_limit\': 10, # \'cufft_plan_cache_size\': 2, # \'cufft_plan_cache_max_size\': 4096, # } ``` # Notes - Use `torch.backends.cuda.matmul.allow_tf32` to control TensorFloat-32 usage. - Use `torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction` to control fp16 reduced precision reduction. - Adjust `torch.backends.cudnn.benchmark` to enable CuDNN benchmarking. - Return current settings using the relevant attributes and methods described in the documentation. --- Implement the function `configure_cuda_backend` to ensure the correct configuration of the CUDA backend in PyTorch for optimized performance.","solution":"import torch def configure_cuda_backend(fp16_reduction: bool = True, tf32_matrix_multiplication: bool = True, use_cudnn_benchmark: bool = False) -> dict: Configures the CUDA backend in PyTorch according to the specified parameters. Parameters: - fp16_reduction (bool): If True, enable reduced precision reduction with fp16 accumulation type. - tf32_matrix_multiplication (bool): If True, allow TensorFloat-32 tensor cores for matrix multiplication on Ampere or newer GPUs. - use_cudnn_benchmark (bool): If True, enables cuDNN benchmarking to use the fastest convolution algorithm. Returns: - A dictionary with the current state of the following settings: { \'allow_fp16_reduced_precision_reduction\': bool, \'allow_tf32\': bool, \'cudnn.benchmark\': bool, \'cudnn.benchmark_limit\': int, \'cufft_plan_cache_size\': int, \'cufft_plan_cache_max_size\': int, } # Set configurations based on parameters torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = fp16_reduction torch.backends.cuda.matmul.allow_tf32 = tf32_matrix_multiplication torch.backends.cudnn.benchmark = use_cudnn_benchmark # Retrieve current settings settings = { \'allow_fp16_reduced_precision_reduction\': torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction, \'allow_tf32\': torch.backends.cuda.matmul.allow_tf32, \'cudnn.benchmark\': torch.backends.cudnn.benchmark, \'cudnn.benchmark_limit\': torch.backends.cudnn.benchmark_limit, \'cufft_plan_cache_size\': torch.backends.cuda.cufft_plan_cache.size, \'cufft_plan_cache_max_size\': torch.backends.cuda.cufft_plan_cache.max_size, } return settings"},{"question":"XML Manipulation and Data Extraction using `xml.etree.ElementTree` **Objective:** You are to implement a function that parses an XML file of book information and extracts specific data to create a summary report. This task will test your understanding of XML parsing, searching for elements, and extracting data. **Problem Statement:** Implement a function `extract_books_info(xml_string: str) -> List[Dict[str, Any]]` that takes a string containing XML data and returns a list of dictionaries, each representing a book with its title, author, and published year. The XML string will have the following structure: ```xml <library> <book> <title>Book Title 1</title> <author>Author 1</author> <year>2001</year> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <year>2002</year> </book> <!-- more book entries --> </library> ``` **Function Signature:** ```python from typing import List, Dict, Any def extract_books_info(xml_string: str) -> List[Dict[str, Any]]: pass ``` **Input:** - `xml_string` (str): A string consisting of valid XML data representing a library of books. **Output:** - A list of dictionaries, where each dictionary contains the keys `title`, `author`, and `year` with corresponding values from the XML data. **Constraints:** - The XML string is guaranteed to be well-formed. - Each book entry will have exactly one title, author, and year sub-element. **Example:** ```python xml_data = <library> <book> <title>Effective Python</title> <author>Brett Slatkin</author> <year>2015</year> </book> <book> <title>Learning Python</title> <author>Mark Lutz</author> <year>2013</year> </book> </library> extract_books_info(xml_data) ``` **Expected Output:** ```python [ {\\"title\\": \\"Effective Python\\", \\"author\\": \\"Brett Slatkin\\", \\"year\\": 2015}, {\\"title\\": \\"Learning Python\\", \\"author\\": \\"Mark Lutz\\", \\"year\\": 2013} ] ``` **Instructions:** 1. Use the `xml.etree.ElementTree` module to parse the XML string. 2. Extract the required information from each `book` element. 3. Construct a dictionary for each book with keys `title`, `author`, and `year`. 4. Ensure the year value is an integer. 5. Return a list of these dictionaries. This question will assess your ability to work with XML data, parse and navigate through an XML tree, and manipulate the extracted data into the required format.","solution":"from typing import List, Dict, Any import xml.etree.ElementTree as ET def extract_books_info(xml_string: str) -> List[Dict[str, Any]]: root = ET.fromstring(xml_string) books_info = [] for book in root.findall(\'book\'): title = book.find(\'title\').text author = book.find(\'author\').text year = int(book.find(\'year\').text) book_info = { \'title\': title, \'author\': author, \'year\': year } books_info.append(book_info) return books_info"},{"question":"# Email Retriever Using `poplib` Objective Implement a Python function `retrieve_emails` that connects to a specified POP3 server, authenticates a user, retrieves the latest `n` emails, and prints them. The function should also handle SSL connections if specified. Function Signature ```python def retrieve_emails(host: str, port: int = 110, use_ssl: bool = False, username: str, password: str, num_emails: int = 5): pass ``` Parameters - `host` (str): The hostname of the POP3 server. - `port` (int, optional): The port to connect to the POP3 server (default is 110, which is the standard POP3 port). For SSL connections, the default is 995. - `use_ssl` (bool, optional): Flag indicating whether an SSL connection should be used (default is False). - `username` (str): The username for authenticating with the POP3 server. - `password` (str): The password for authenticating with the POP3 server. - `num_emails` (int, optional): The number of the most recent emails to retrieve (default is 5). Constraints - The function should handle possible errors and exceptions, such as connection issues, authentication failures, and retrieval errors. - If `use_ssl` is True, the function must use `POP3_SSL` to establish a secure connection. - The function should ensure the mailbox is properly unlocked and the connection is closed after operations are complete. Example Usage ```python retrieve_emails(\'pop.example.com\', username=\'user@example.com\', password=\'secret\', num_emails=3) retrieve_emails(\'pop.example.com\', port=995, use_ssl=True, username=\'user@example.com\', password=\'secret\', num_emails=10) ``` Implementation Details 1. Establish a connection to the POP3 server using either `POP3` or `POP3_SSL` depending on the `use_ssl` flag. 2. Authenticate the user using the provided `username` and `password`. 3. Retrieve the latest `num_emails`. If `num_emails` exceeds the total number of emails, retrieve all available emails. 4. Print the content of each retrieved email. 5. Properly handle exceptions and ensure the connection is closed. Example of Expected Output For `retrieve_emails(\'pop.example.com\', username=\'user@example.com\', password=\'secret\', num_emails=2)`: ``` --- Email 1 --- From: sender1@example.com Subject: Subject 1 Date: Tue, 10 Oct 2023 12:34:56 +0000 Body of email 1... --- Email 2 --- From: sender2@example.com Subject: Subject 2 Date: Wed, 11 Oct 2023 14:56:32 +0000 Body of email 2... ```","solution":"import poplib from email.parser import BytesParser from email.policy import default def retrieve_emails(host: str, port: int = 110, use_ssl: bool = False, username: str = \\"\\", password: str = \\"\\", num_emails: int = 5): Connects to the specified POP3 server, authenticates the user, retrieves the latest `num_emails` emails, and prints their content. :param host: The hostname of the POP3 server. :param port: The port to connect to the POP3 server (default is 110 for regular, 995 for SSL). :param use_ssl: Flag indicating whether to use SSL connection (default is False). :param username: The username for authentication. :param password: The password for authentication. :param num_emails: The number of the latest emails to retrieve (default is 5). try: if use_ssl: server = poplib.POP3_SSL(host, port) else: server = poplib.POP3(host, port) server.user(username) server.pass_(password) email_count, _ = server.stat() fetch_count = min(num_emails, email_count) for i in range(email_count, email_count - fetch_count, -1): resp, lines, octets = server.retr(i) msg_data = b\'rn\'.join(lines) msg = BytesParser(policy=default).parsebytes(msg_data) print(\\"--- Email ---\\") print(f\\"From: {msg[\'from\']}\\") print(f\\"Subject: {msg[\'subject\']}\\") print(f\\"Date: {msg[\'date\']}\\") print() print(msg.get_body(preferencelist=(\'plain\')).get_content()) server.quit() except (poplib.error_proto, Exception) as e: print(f\\"Failed to retrieve emails: {str(e)}\\")"},{"question":"Background You are provided with data on brain networks. Your task is to preprocess this data and generate a series of pairwise plots showing the trajectories of different brain networks over time. Requirements 1. Load the brain networks dataset using `seaborn.load_dataset` with specific parameters and preprocess it. 2. Create a pairwise plot layout using seaborn\'s object-oriented interface. 3. Customize the appearance of the plots, including properties like `linewidth`, `alpha`, and `color`. Data The dataset is named `brain_networks` and should be loaded with the following parameters: - `header=[0, 1, 2]` - `index_col=0` Steps 1. Load and preprocess the dataset: - Set the axis name to `timepoint`. - Stack and group the dataset by `timepoint`, `network`, and `hemi`. - Calculate the mean of each group. - Unstack the `network` dimension. - Reset the index and filter rows where `timepoint` is less than 100. 2. Create a pairwise plot layout: - Set the x-axis variables to `[\'5\', \'8\', \'12\', \'15\']`. - Set the y-axis variables to `[\'6\', \'13\', \'16\']`. - Set the layout size to `(8, 5)`. - Share x and y axes among subplots. 3. Add paths to the plot: - Use the `Paths` class to plot without sorting observations. - Set properties like `linewidth` to 1, `alpha` to 0.8, and `color` based on the `hemi` variable. Function Signature ```python import seaborn.objects as so from seaborn import load_dataset def visualize_brain_networks(): # Load and preprocess the dataset networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Create a pairwise plot layout p = ( so.Plot(networks) .pair( x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"], ) .layout(size=(8, 5)) .share(x=True, y=True) ) # Add paths to the plot p.add(so.Paths(linewidth=1, alpha=0.8), color=\\"hemi\\") # Return the plot object return p ``` Expected Output - A `Plot` object representing the visualizations. Constraints - You should not modify the provided dataset. - Ensure your solution handles the preprocessing steps as described. - Your code should be efficient and make use of seaborn\'s object-oriented interface effectively.","solution":"import seaborn.objects as so from seaborn import load_dataset def visualize_brain_networks(): Load the brain networks dataset and preprocess it, then create a pairwise plot. Returns: seaborn.objects.Plot: The configured pairwise plot object. # Load the dataset networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Create a pairwise plot layout p = ( so.Plot(networks) .pair( x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"], ) .layout(size=(8, 5)) .share(x=True, y=True) ) # Add paths to the plot p.add(so.Paths(linewidth=1, alpha=0.8), color=\\"hemi\\") # Return the plot object return p"},{"question":"# Seaborn Coding Assessment Question **Objective:** Create a detailed visual analysis of the Titanic dataset using the seaborn library. This task requires implementing various seaborn functionalities, demonstrating a thorough understanding of the basics and some advanced features of seaborn. **Task:** Write a Python function, `titanic_analysis()`, which performs the following operations: 1. Loads the Titanic dataset using seaborn. 2. Creates a count plot showing the number of passengers for each class. 3. Creates a count plot showing the number of passengers for each class, grouped by their survival status. 4. Creates a count plot showing the percentage of passengers for each class, grouped by their survival status. 5. Creates a bar plot showing the average fare for each class. 6. Creates a box plot showing the distribution of ages for each class. **Input and Output:** - The function should not take any inputs. - The function should output these plots directly. **Constraints:** - Ensure the function is efficient and avoid redundant operations, such as loading the dataset multiple times. - Use seaborn\'s built-in datasets and functionality. - Use proper labeling and titles for each plot for clarity. **Function Signature:** ```python def titanic_analysis(): pass ``` # Example Usage ```python titanic_analysis() ``` **Notes:** - Assume seaborn and other necessary libraries (pandas, matplotlib) are installed. - Make sure to style the plots for better readability using seaborn themes or other relevant styling features. - The outputs should be visual plots created using seaborn, displayed directly when the function is called.","solution":"import seaborn as sns import matplotlib.pyplot as plt def titanic_analysis(): # Set the theme sns.set_theme(style=\\"whitegrid\\") # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # 1. Create a count plot showing the number of passengers for each class plt.figure(figsize=(8, 6)) sns.countplot(x=\\"class\\", data=titanic) plt.title(\\"Number of Passengers in Each Class\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Count\\") plt.show() # 2. Create a count plot showing the number of passengers for each class, grouped by survival status plt.figure(figsize=(8, 6)) sns.countplot(x=\\"class\\", hue=\\"survived\\", data=titanic) plt.title(\\"Number of Passengers Each Class Grouped by Survival Status\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Count\\") plt.legend(title=\\"Survived\\") plt.show() # 3. Create a count plot showing the percentage of passengers for each class, grouped by survival status plt.figure(figsize=(8, 6)) survived_count = titanic.groupby([\\"class\\", \\"survived\\"]).size().reset_index(name=\'count\') survived_total = titanic.groupby([\\"class\\"]).size().reset_index(name=\'total\') merged = survived_count.merge(survived_total,on=\\"class\\") merged[\'percentage\'] = (merged[\'count\'] / merged[\'total\']) * 100 sns.barplot(x=\\"class\\", y=\\"percentage\\", hue=\\"survived\\", data=merged) plt.title(\\"Percentage of Passengers Each Class Grouped by Survival Status\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Percentage\\") plt.legend(title=\\"Survived\\") plt.show() # 4. Create a bar plot showing the average fare for each class plt.figure(figsize=(8, 6)) sns.barplot(x=\\"class\\", y=\\"fare\\", data=titanic, ci=None) plt.title(\\"Average Fare for Each Class\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Average Fare\\") plt.show() # 5. Create a box plot showing the distribution of ages for each class plt.figure(figsize=(8, 6)) sns.boxplot(x=\\"class\\", y=\\"age\\", data=titanic) plt.title(\\"Distribution of Ages for Each Class\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") plt.show()"},{"question":"You are given a file that contains multiple chunks of data in the EA IFF 85 format. Each chunk contains a 4-byte ID, followed by a 4-byte size field, and then the data segment. Your task is to implement a function `process_chunks(file_path: str) -> List[Dict[str, Any]]` that reads and processes this file. Each chunk\'s data should be processed according to the following rules: 1. Read the entire chunk, including its ID and size. 2. If the chunk ID is \\"DATA\\", convert its data segment from big-endian to little-endian (if `bigendian=True`) and store it as a list of integers. 3. If the chunk ID is \\"META\\", treat the data segment as a UTF-8 encoded string, and convert it to a string. 4. For any other chunk ID, skip to the next chunk without processing its data. The function should return a list of dictionaries, where each dictionary represents a processed chunk with the following keys: - `\\"id\\"`: The chunk ID. - `\\"size\\"`: The size of the chunk. - `\\"data\\"`: The processed data, either as a list of integers or a string depending on the chunk type. Input - `file_path`: Path to the file containing the chunked data. Output - A list of dictionaries, each representing a processed chunk as described above. Constraints - You must use the `chunk` module to read and process the file. - The file contains valid chunked data. - The data segment for \\"DATA\\" chunks contains integer values. For the \\"DATA\\" chunks, assume each integer is represented by 4 bytes. - The chunks may or may not be aligned on 2-byte boundaries. Example Usage Consider a file at `example.iff` containing the following chunks: 1. Chunk ID: \\"DATA\\", Size: 8, Data: [0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x02] (in big-endian format) 2. Chunk ID: \\"META\\", Size: 4, Data: \\"test\\" (as UTF-8 string) 3. Chunk ID: \\"SKIP\\", Size: 4, Data: [0x00, 0x00, 0x00, 0x03] ```python def process_chunks(file_path: str) -> List[Dict[str, Any]]: from chunk import Chunk result = [] with open(file_path, \\"rb\\") as f: while True: try: chk = Chunk(f, bigendian=True) if chk.getname() == \\"DATA\\": data = chk.read() processed_data = [int.from_bytes(data[i:i+4], byteorder=\'big\') for i in range(0, len(data), 4)] elif chk.getname() == \\"META\\": processed_data = chk.read().decode(\'utf-8\') else: chk.skip() continue result.append({ \\"id\\": chk.getname(), \\"size\\": chk.getsize(), \\"data\\": processed_data }) except EOFError: break return result # Example usage scenario # result = process_chunks(\\"example.iff\\") # print(result) # Output: # [ # {\\"id\\": \\"DATA\\", \\"size\\": 8, \\"data\\": [1, 2]}, # {\\"id\\": \\"META\\", \\"size\\": 4, \\"data\\": \\"test\\"} # ] ``` Your task is to implement the `process_chunks` function as described.","solution":"from typing import List, Dict, Any def process_chunks(file_path: str) -> List[Dict[str, Any]]: from chunk import Chunk result = [] with open(file_path, \\"rb\\") as f: while True: try: chk = Chunk(f, bigendian=True) if chk.getname() == b\\"DATA\\": data = chk.read() processed_data = [int.from_bytes(data[i:i+4], byteorder=\'big\') for i in range(0, len(data), 4)] elif chk.getname() == b\\"META\\": processed_data = chk.read().decode(\'utf-8\') else: chk.skip() continue result.append({ \\"id\\": chk.getname().decode(\'ascii\'), \\"size\\": chk.getsize(), \\"data\\": processed_data }) except EOFError: break return result"},{"question":"# Question: Implement a Code Coverage and Tracing Utility **Objective:** Write a Python program using the `trace` module that traces the execution of a given Python file, collects coverage statistics, and generates a coverage report. **Requirements:** 1. Implement a function `generate_trace_coverage(file_path: str, coverdir: str) -> None` that: - Traces the execution of the code in the file specified by `file_path`. - Collects execution and coverage statistics. - Generates a report in the directory specified by `coverdir`. 2. The `trace.Trace` object should: - Ignore directories specified in `sys.prefix` and `sys.exec_prefix`. - Enable line-counting to gather coverage statistics. - Not display line-by-line tracing. 3. The generated report should: - Show lines not executed (missing lines). - Be written to the directory specified by `coverdir`. **Function Signature:** ```python def generate_trace_coverage(file_path: str, coverdir: str) -> None: pass ``` **Input:** - `file_path` (str): The path to the Python file to be traced. - `coverdir` (str): The directory where the coverage report should be generated. **Output:** - None. The function should create coverage reports in the specified directory. **Examples:** Example usage of the function: ```python # Assume main.py exists in the current directory and contains some Python code. generate_trace_coverage(\'main.py\', \'./coverage_reports\') ``` After calling the function, a detailed coverage report should be generated in the `./coverage_reports` directory. **Constraints:** - You are not allowed to use any third-party libraries (e.g., Coverage.py). - The function should handle any file errors, such as the specified file not existing, by printing a meaningful error message. By completing this task, you will demonstrate your ability to utilize the `trace` module to track and report code execution.","solution":"import sys import os import trace def generate_trace_coverage(file_path: str, coverdir: str) -> None: Traces the execution of the given Python file, collects coverage statistics, and generates a coverage report. :param file_path: The path to the Python file to be traced. :param coverdir: The directory where the coverage report should be generated. if not os.path.exists(file_path): print(f\\"Error: The file \'{file_path}\' does not exist.\\") return if not os.path.exists(coverdir): os.makedirs(coverdir) tracer = trace.Trace( ignoredirs=[sys.prefix, sys.exec_prefix], trace=False, count=True ) try: tracer.run(f\'exec(open(\\"{file_path}\\").read())\') results = tracer.results() # Generate the report with open(os.path.join(coverdir, \'coverage_report.txt\'), \'w\') as report: results.write_results(report, coverdir) except Exception as e: print(f\\"Error tracing the file: {e}\\")"},{"question":"# Python Coding Assessment Question **Objective:** Implement a text auto-completion system in Python. **Problem Statement:** You are required to implement a class named `SimpleCompleter` that mimics basic functionalities of the `Completer` class described in the `rlcompleter` module. Your task is to provide text completions for a set of predefined Python identifiers and keywords. Class Definition: ```python class SimpleCompleter: def __init__(self, identifiers): Initialize the completer with a list of predefined identifiers. Parameters: identifiers (list of str): A list of strings representing valid Python identifiers and keywords. pass def complete(self, text, state): Return the `state`-th completion for the given `text`. Parameters: text (str): The input text to complete. state (int): The state indicating which completion to return. Returns: str or None: The `state`-th completion for the given `text`, or None if there are no more completions. pass ``` Requirements: 1. The `__init__` method should initialize the `SimpleCompleter` object with a list of valid Python identifiers and keywords. 2. The `complete` method should: - Return possible completions for the given `text` from the list of predefined identifiers. - When `state` is 0, generate a list of all possible completions for the `text`. - Return the `state`-th completion from the generated list of completions if it exists. - Return `None` if there are no more completions. Example: ```python identifiers = [\\"print\\", \\"input\\", \\"int\\", \\"str\\", \\"for\\", \\"if\\", \\"while\\", \\"import\\", \\"from\\", \\"class\\", \\"def\\", \\"return\\"] completer = SimpleCompleter(identifiers) # Simple completion examples assert completer.complete(\\"pri\\", 0) == \\"print\\" assert completer.complete(\\"pri\\", 1) == None assert completer.complete(\\"i\\", 0) == \\"input\\" assert completer.complete(\\"i\\", 1) == \\"int\\" assert completer.complete(\\"i\\", 2) == \\"if\\" assert completer.complete(\\"i\\", 3) == \\"import\\" assert completer.complete(\\"i\\", 4) == None ``` **Note:** The class should handle any exceptions gracefully and adhere to the specifications provided. Constraints: - No restrictions on the length of the `identifiers` list. - Assume `text` will always be a non-empty string and `state` will always be a non-negative integer. - Do not utilize the `rlcompleter` or `readline` modules within your solution.","solution":"class SimpleCompleter: def __init__(self, identifiers): Initialize the completer with a list of predefined identifiers. Parameters: identifiers (list of str): A list of strings representing valid Python identifiers and keywords. self.identifiers = sorted(identifiers) def complete(self, text, state): Return the `state`-th completion for the given `text`. Parameters: text (str): The input text to complete. state (int): The state indicating which completion to return. Returns: str or None: The `state`-th completion for the given `text`, or None if there are no more completions. completions = [identifier for identifier in self.identifiers if identifier.startswith(text)] if state < len(completions): return completions[state] else: return None"},{"question":"Objective: Demonstrate your understanding of the `sns.hls_palette` function in the seaborn library by customizing color palettes and applying them to different plots. Your task is to generate specified plots with different color palettes and analyze their visual differences. Task: 1. **Data Preparation:** - Load the built-in `tips` dataset from seaborn. 2. **Color Palette Generation:** - Generate three different HLS color palettes using the `sns.hls_palette` function with the following specifications: 1. Palette 1: 6 colors, default lightness and saturation. 2. Palette 2: 8 colors, lightness reduced to 0.4, default saturation. 3. Palette 3: 8 colors, lightness reduced to 0.4, saturation reduced to 0.5. 3. **Plotting:** - Create three side-by-side bar plots showing the average total bill per day using each of the three color palettes generated. 4. **Analysis:** - Write a brief analysis comparing the three plots. Discuss how changes in lightness, saturation, and the number of colors affect the visual appeal and readability of the plots. Input and Output Formats: - **Input**: No specific input format. All data is to be loaded and processed within the function. - **Output**: Three side-by-side bar plots and the written analysis of the visual differences. Constraints: - Use seaborn and matplotlib libraries for plotting. - Ensure that the plots are clear and well-labeled. Example Code: Here is a template to get you started: ```python import seaborn as sns import matplotlib.pyplot as plt def generate_and_analyze_palettes(): # Load the `tips` dataset tips = sns.load_dataset(\'tips\') # Generate the HLS palettes palette1 = sns.hls_palette(6) palette2 = sns.hls_palette(8, l=0.4) palette3 = sns.hls_palette(8, l=0.4, s=0.5) # Plotting fig, axs = plt.subplots(1, 3, figsize=(18, 6)) # Plot 1: Using Palette 1 sns.barplot(x=\'day\', y=\'total_bill\', data=tips, palette=palette1, ax=axs[0]) axs[0].set_title(\'Palette 1: 6 colors, default lightness and saturation\') # Plot 2: Using Palette 2 sns.barplot(x=\'day\', y=\'total_bill\', data=tips, palette=palette2, ax=axs[1]) axs[1].set_title(\'Palette 2: 8 colors, lightness=0.4\') # Plot 3: Using Palette 3 sns.barplot(x=\'day\', y=\'total_bill\', data=tips, palette=palette3, ax=axs[2]) axs[2].set_title(\'Palette 3: 8 colors, lightness=0.4, saturation=0.5\') plt.tight_layout() plt.show() # Analysis analysis = [Your analysis here] print(analysis) # Call the function generate_and_analyze_palettes() ``` Complete the code with your analysis to answer the task comprehensively.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_and_analyze_palettes(): # Load the `tips` dataset tips = sns.load_dataset(\'tips\') # Generate the HLS palettes palette1 = sns.hls_palette(6) palette2 = sns.hls_palette(8, l=0.4) palette3 = sns.hls_palette(8, l=0.4, s=0.5) # Plotting fig, axs = plt.subplots(1, 3, figsize=(18, 6)) # Plot 1: Using Palette 1 sns.barplot(x=\'day\', y=\'total_bill\', data=tips, palette=palette1, ax=axs[0]) axs[0].set_title(\'Palette 1: 6 colors, default lightness and saturation\') # Plot 2: Using Palette 2 sns.barplot(x=\'day\', y=\'total_bill\', data=tips, palette=palette2, ax=axs[1]) axs[1].set_title(\'Palette 2: 8 colors, lightness=0.4\') # Plot 3: Using Palette 3 sns.barplot(x=\'day\', y=\'total_bill\', data=tips, palette=palette3, ax=axs[2]) axs[2].set_title(\'Palette 3: 8 colors, lightness=0.4, saturation=0.5\') plt.tight_layout() plt.show() # Analysis analysis = Analysis of the three generated plots: - Palette 1 uses 6 colors with the default lightness and saturation. The colors are vibrant and distinguishable. - Palette 2 uses 8 colors with lightness reduced to 0.4. The colors are darker than those in Palette 1, but still distinct. - Palette 3 uses 8 colors with lightness reduced to 0.4 and saturation reduced to 0.5. The colors are more muted compared to Palette 2, resulting in a more subdued appearance. Changes in lightness and saturation influence the visual characteristics of the palettes significantly: - Reducing lightness makes the colors darker, which can provide a different visual tone to the plot. - Reducing saturation leads to less vibrant colors, which can be easier on the eyes but might reduce clarity in distinguishing different categories. The number of colors affects the distinctiveness between categories. With more colors, there\'s more room to represent different categories uniquely, but the increased number may also reduce contrast between adjacent hues, making them harder to distinguish. print(analysis) # Call the function generate_and_analyze_palettes()"},{"question":"# Python Sequence Protocol Exercise Objective You must implement a Python class that demonstrates an understanding of the sequence protocol. Specifically, you will create a class that mimics a simplified version of a list, providing similar functionality by implementing several key sequence operations described in the documentation provided. Requirements 1. **Class Definition**: - Define a class named `CustomList`. 2. **Constructor**: - The constructor should initialize with a list of elements. 3. **Methods**: - Implement the following methods with the specified functionality: ```python class CustomList: def __init__(self, elements): # Initialize the sequence with a list of elements def __len__(self): # Return the number of elements in the sequence (equivalent to len()) def __getitem__(self, index): # Return the element at the given index or handle slice objects (equivalent to o[index] or o[i1:i2]) def __setitem__(self, index, value): # Assign a value to the element at the given index (equivalent to o[index] = value) def __delitem__(self, index): # Delete the element at the given index (equivalent to del o[index]) def __contains__(self, value): # Check if the sequence contains the given value (equivalent to value in o) def __add__(self, other): # Concatenate two sequences (equivalent to o1 + o2) def __iadd__(self, other): # Concatenate two sequences in-place (equivalent to o1 += o2) def __mul__(self, count): # Repeat the sequence count times (equivalent to o * count) def __imul__(self, count): # Repeat the sequence count times in-place (equivalent to o *= count) def count(self, value): # Return the number of occurrences of value in the sequence (equivalent to o.count(value)) def index(self, value): # Return the first index of value in the sequence (equivalent to o.index(value)) ``` 4. **Constraints**: - Do not use Python\'s built-in list methods directly (e.g., `list.append`, `list.remove`), though you may initialize your class with a list and use list indexing. - Handle slicing in `__getitem__` and `__setitem__` methods. - Raise appropriate exceptions for index errors and value errors. Example Usage ```python # Creating an instance of CustomList seq = CustomList([1, 2, 3, 4, 5]) # Getting the length assert len(seq) == 5 # Accessing elements assert seq[1] == 2 # Modifying elements seq[1] = 20 assert seq[1] == 20 # Deleting elements del seq[1] assert len(seq) == 4 # Checking containment assert 3 in seq # Concatenation seq2 = CustomList([6, 7, 8]) new_seq = seq + seq2 assert len(new_seq) == 7 # In-place concatenation seq += seq2 assert len(seq) == 7 # Repetition repeated_seq = seq * 2 assert len(repeated_seq) == 14 # In-place repetition seq *= 2 assert len(seq) == 14 # Counting occurrences assert seq.count(3) == 2 # Finding index assert seq.index(3) == 1 ``` Submission Submit the `CustomList` class implementation with the required methods and make sure your code passes the provided example usage.","solution":"class CustomList: def __init__(self, elements): self._elements = elements def __len__(self): return len(self._elements) def __getitem__(self, index): if isinstance(index, slice): return CustomList(self._elements[index]) else: return self._elements[index] def __setitem__(self, index, value): if isinstance(index, slice): if not isinstance(value, list) and not isinstance(value, CustomList): raise ValueError(\\"Can only assign list or CustomList to a slice\\") self._elements[index] = value if isinstance(value, list) else value._elements else: self._elements[index] = value def __delitem__(self, index): del self._elements[index] def __contains__(self, value): return value in self._elements def __add__(self, other): if not isinstance(other, CustomList): raise ValueError(\\"Can only concatenate CustomList to CustomList\\") return CustomList(self._elements + other._elements) def __iadd__(self, other): if not isinstance(other, CustomList): raise ValueError(\\"Can only concatenate CustomList to CustomList\\") self._elements += other._elements return self def __mul__(self, count): return CustomList(self._elements * count) def __imul__(self, count): self._elements *= count return self def count(self, value): return self._elements.count(value) def index(self, value): return self._elements.index(value)"},{"question":"**Problem Statement: Your Task** Given the differences and limitations of PyTorch\'s TorchScript outlined in the documentation, a programmer must write code compatible with both Python and TorchScript. Write a PyTorch class that includes tensor operations and conversions while taking care to avoid any unsupported or diverging functionalities when scripting it. **Class Requirements:** 1. **Class Name:** `SimpleNNModel` 2. **Attributes:** - `input_size` (int): Size of the input features. - `hidden_size` (int): Size of the hidden layer. - `output_size` (int): Size of the output layer. 3. **Methods:** - `__init__(self, input_size: int, hidden_size: int, output_size: int)`: Constructor that sets up the layers of the neural network. - `forward(self, x: torch.Tensor) -> torch.Tensor`: Takes an input tensor `x`, passes it through a simple two-layer architecture (input layer to hidden layer to output layer) using ReLU activation, and returns the output tensor. - `script_model(self) -> torch.jit.ScriptModule`: Returns a TorchScript scripted version of the model. **Constraints:** - Do not use any TorchScript unsupported functions or classes as listed in the provided documentation. - Ensure that the class can be correctly scripted by TorchScript. **Input Format:** - An integer for input size. - An integer for hidden size. - An integer for output size. - A tensor `x` with shape `(batch_size, input_size)`. **Output Format:** - A tensor after passing through the model. - A scripted version of the model that can be used for deployment. ```python # Example usage: model = SimpleNNModel(input_size=10, hidden_size=5, output_size=1) input_tensor = torch.randn(16, 10) output = model.forward(input_tensor) # Scripting the model scripted_model = model.script_model() # The scripted model can now be saved and used ``` **Performance Requirements:** - The class should be efficiently written considering the constraints. - It should ensure TorchScript compatibility by avoiding unsupported operations.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class SimpleNNModel(nn.Module): def __init__(self, input_size: int, hidden_size: int, output_size: int): Initializes SimpleNNModel with input size, hidden size, and output size. Sets up the layers of the neural network. super(SimpleNNModel, self).__init__() self.input_size = input_size self.hidden_size = hidden_size self.output_size = output_size self.input_layer = nn.Linear(input_size, hidden_size) self.hidden_layer = nn.Linear(hidden_size, output_size) def forward(self, x: torch.Tensor) -> torch.Tensor: Passes input tensor x through the neural network. x = self.input_layer(x) x = F.relu(x) x = self.hidden_layer(x) return x def script_model(self) -> torch.jit.ScriptModule: Scripts the model for deployment with TorchScript. return torch.jit.script(self) # Example usage: # model = SimpleNNModel(input_size=10, hidden_size=5, output_size=1) # input_tensor = torch.randn(16, 10) # output = model.forward(input_tensor) # scripted_model = model.script_model() # The scripted model can now be saved and used"},{"question":"# PyTorch Distributed Training Configuration Objective Design and implement a Python script using PyTorch that demonstrates setting up a distributed training environment for a neural network model. The script should specifically leverage the ProcessGroupNCCL environment variables to customize the behavior of the training process. Task 1. **Setup:** Use PyTorch\'s distributed package to initialize the NCCL process group and set up a simple distributed training loop. 2. **Environment Variables:** Configure at least five of the given environment variables to tailor the execution of the training process to desired specifications. 3. **Implementation:** - Initialize the ProcessGroupNCCL with specified environment variables. - Implement a distributed training loop for a simple neural network (e.g., a single linear layer). - Ensure that your script handles errors appropriately and prints relevant debugging information if necessary. Requirements - Use a simple neural network with a single layer for the demonstration. - Train the model using a synthetic dataset. - Configure the TORCH_NCCL_ASYNC_ERROR_HANDLING, TORCH_NCCL_BLOCKING_WAIT, TORCH_NCCL_ENABLE_TIMING, TORCH_NCCL_TRACE_BUFFER_SIZE, and TORCH_NCCL_DESYNC_DEBUG environment variables. - Print the values of these environment variables during the script\'s execution. - Your script should be executable in a multi-GPU environment. Input and Output **Input:** - There is no external input required; you will define the synthetic dataset within the script. **Output:** - The script should output the values of the configured environment variables. - The training loss at each epoch. - Any relevant debug or error messages as per the configured environment variables. Constraints - Ensure the script can run on a system with multiple GPUs (minimum 2 GPUs). - Handle scenarios where the NCCL process encounters errors to showcase the handling as per the configured environment variables. Example Code Structure ```python import os import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim # Set environment variables os.environ[\'TORCH_NCCL_ASYNC_ERROR_HANDLING\'] = \'1\' os.environ[\'TORCH_NCCL_BLOCKING_WAIT\'] = \'1\' os.environ[\'TORCH_NCCL_ENABLE_TIMING\'] = \'1\' os.environ[\'TORCH_NCCL_TRACE_BUFFER_SIZE\'] = \'1024\' os.environ[\'TORCH_NCCL_DESYNC_DEBUG\'] = \'1\' # Print the values of the environment variables print(\\"TORCH_NCCL_ASYNC_ERROR_HANDLING:\\", os.getenv(\'TORCH_NCCL_ASYNC_ERROR_HANDLING\')) print(\\"TORCH_NCCL_BLOCKING_WAIT:\\", os.getenv(\'TORCH_NCCL_BLOCKING_WAIT\')) print(\\"TORCH_NCCL_ENABLE_TIMING:\\", os.getenv(\'TORCH_NCCL_ENABLE_TIMING\')) print(\\"TORCH_NCCL_TRACE_BUFFER_SIZE:\\", os.getenv(\'TORCH_NCCL_TRACE_BUFFER_SIZE\')) print(\\"TORCH_NCCL_DESYNC_DEBUG:\\", os.getenv(\'TORCH_NCCL_DESYNC_DEBUG\')) # Initialize the distributed process group dist.init_process_group(backend=\'nccl\') # Define your simple neural network and dataset class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) model = SimpleModel().cuda() criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Dummy dataset data = torch.randn(100, 10).cuda() target = torch.randn(100, 1).cuda() # Training loop for epoch in range(10): optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() print(f\\"Epoch {epoch}, Loss: {loss.item()}\\") # Cleanup dist.destroy_process_group() ``` Fill in the necessary details to complete the script as per the requirements. Note Before running this example, ensure you have a multi-GPU setup and the necessary NCCL library installed.","solution":"import os import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim # Set environment variables os.environ[\'TORCH_NCCL_ASYNC_ERROR_HANDLING\'] = \'1\' os.environ[\'TORCH_NCCL_BLOCKING_WAIT\'] = \'1\' os.environ[\'TORCH_NCCL_ENABLE_TIMING\'] = \'1\' os.environ[\'TORCH_NCCL_TRACE_BUFFER_SIZE\'] = \'1024\' os.environ[\'TORCH_NCCL_DESYNC_DEBUG\'] = \'1\' # Print the values of the environment variables print(\\"TORCH_NCCL_ASYNC_ERROR_HANDLING:\\", os.getenv(\'TORCH_NCCL_ASYNC_ERROR_HANDLING\')) print(\\"TORCH_NCCL_BLOCKING_WAIT:\\", os.getenv(\'TORCH_NCCL_BLOCKING_WAIT\')) print(\\"TORCH_NCCL_ENABLE_TIMING:\\", os.getenv(\'TORCH_NCCL_ENABLE_TIMING\')) print(\\"TORCH_NCCL_TRACE_BUFFER_SIZE:\\", os.getenv(\'TORCH_NCCL_TRACE_BUFFER_SIZE\')) print(\\"TORCH_NCCL_DESYNC_DEBUG:\\", os.getenv(\'TORCH_NCCL_DESYNC_DEBUG\')) def setup_process(rank, world_size): Initialize the process group for distributed training. dist.init_process_group( backend=\'nccl\', init_method=\'env://\', world_size=world_size, rank=rank ) class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) def train(rank, world_size): Distributed training loop. setup_process(rank, world_size) model = SimpleModel().cuda(rank) criterion = nn.MSELoss().cuda(rank) optimizer = optim.SGD(model.parameters(), lr=0.01) data = torch.randn(100, 10).cuda(rank) target = torch.randn(100, 1).cuda(rank) for epoch in range(10): optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() print(f\\"Rank {rank}, Epoch {epoch}, Loss: {loss.item()}\\") dist.destroy_process_group() if __name__ == \\"__main__\\": import torch.multiprocessing as mp world_size = 2 # Number of GPUs mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)"},{"question":"# Asynchronous Task Management using `asyncio` Objective: Demonstrate your understanding of asynchronous programming in Python using the `asyncio` module by implementing a task manager that performs multiple tasks concurrently and handles task synchronization using futures and synchronization primitives. Task: Implement an asynchronous task manager that: 1. Runs multiple coroutines concurrently. 2. Ensures that certain tasks are completed before starting others using synchronization primitives. 3. Manages and retrieves data from multiple tasks using futures. Requirements: 1. Create an asynchronous function `task(id, duration)` which: - Takes a unique identifier `id` (integer) and a `duration` (float) as input. - Simulates work by awaiting `asyncio.sleep(duration)`. - Prints a message indicating the start and completion of the task including its `id`. 2. Implement another asynchronous function `main()` that: - Creates and schedules at least 5 different tasks using the `task` function with varying durations. - Uses synchronization primitives (e.g., `asyncio.Event`) to ensure that specific tasks must complete before starting subsequent tasks. - Uses `asyncio.gather` to run tasks concurrently and `asyncio.wait_for` to handle timeouts and exceptions. 3. Add a final feature to `main()`, ensuring that all tasks complete properly, and print the results of each task completion, demonstrating handling of futures. Constraints: - The total runtime of the `main()` function should not exceed 10 seconds. - You must use at least one synchronization primitive from `asyncio`. - Ensure proper handling of exceptions or timeouts within the task manager. Input and Output: - There is no direct input for the `main()` function; it\'s internally scheduled and run. - The output should be printed statements showing the start and completion of each task, in addition to any synchronization control messages. Example Run: ```python import asyncio async def task(id, duration): print(f\\"Task {id} started, will take {duration} seconds.\\") await asyncio.sleep(duration) print(f\\"Task {id} completed.\\") async def main(): # Implementation details here... # Run the asyncio event loop asyncio.run(main()) ``` **Expected Output:** ``` Task 1 started, will take 2.0 seconds. Task 2 started, will take 1.0 seconds. Task 2 completed. Task 3 started, will take 1.5 seconds. Task 1 completed. Task 4 started, will take 2.5 seconds. Task 3 completed. Task 5 started, will take 1.0 seconds. Task 5 completed. Task 4 completed. ``` Make sure your implementation adheres to the constraints and correctly demonstrates asynchronous task management using the `asyncio` module.","solution":"import asyncio async def task(id, duration): print(f\\"Task {id} started, will take {duration} seconds.\\") await asyncio.sleep(duration) print(f\\"Task {id} completed.\\") async def main(): # Creating an Event object for synchronization event1 = asyncio.Event() event2 = asyncio.Event() async def synchronized_task(id, duration, prereq_event=None, post_event=None): if prereq_event: await prereq_event.wait() await task(id, duration) if post_event: post_event.set() # Scheduling tasks tasks = [ synchronized_task(1, 2.0, post_event=event1), # This task will set the first event on completion synchronized_task(2, 1.0, post_event=event2), # This task will set the second event on completion synchronized_task(3, 1.5, prereq_event=event1), # This task waits for event1 to be set synchronized_task(4, 2.5, prereq_event=event2), # This task waits for event2 to be set synchronized_task(5, 1.0) # This task runs independently ] # Run all tasks and wait for them to complete await asyncio.gather(*tasks) # This line is used to run the main coroutine if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Python Class Design and Implementation Challenge Objective: To assess your understanding of Python\'s class mechanisms including inheritance, methods, private variables, and iterators. Task: You are required to implement a library system which involves creating multiple classes and demonstrating their interaction through methods and inheritance. Problem Description: 1. **Classes to Implement**: - `Item`: An abstract base class representing a generic item in the library. - `Book`: A derived class from `Item` representing a book. - `DVD`: A derived class from `Item` representing a DVD. - `Library`: A class to manage a collection of `Item` objects, supporting operations like adding, searching, and iterating over items. 2. **Requirements**: - **Class `Item`**: - Private variables: - `__id`: A unique identifier for the item. - `__title`: The title of the item. - Methods: - `__init__(self, id: int, title: str)`: Initialize the item with an id and title. - `get_info(self) -> str`: Return basic information about the item. - `Item` class should be an abstract class. - **Class `Book`**: - Inherit from `Item`. - Private variables: - `__author`: The author of the book. - Methods: - `__init__(self, id: int, title: str, author: str)`: Initialize with id, title, and author. - Override `get_info(self) -> str` to include author information. - **Class `DVD`**: - Inherit from `Item`. - Private variables: - `__duration`: The duration of the DVD in minutes. - Methods: - `__init__(self, id: int, title: str, duration: int)`: Initialize with id, title, and duration. - Override `get_info(self) -> str` to include duration information. - **Class `Library`**: - Private variables: - `__items`: A list of `Item` objects in the library. - Methods: - `__init__(self)`: Initialize the library with an empty item list. - `add_item(self, item: Item)`: Add an item to the library. - `search_by_title(self, title: str) -> List[Item]`: Search for items by their title. - `__iter__(self) -> Iterator[Item]`: Allow iterating over the items in the library. Constraints: - The `Item` class should not be instantiated directly. - Ensure that the `id` attribute is unique for each item. - Aim to minimize code redundancy and use inheritance effectively. Input / Output: - Your classes should be tested with a series of operations such as adding different kinds of items and performing searches. No direct user input or print statements are required in the solution. Example Usage: ```python lib = Library() book = Book(1, \\"Python Programming\\", \\"John Doe\\") dvd = DVD(2, \\"Learn Python\\", 120) lib.add_item(book) lib.add_item(dvd) results = lib.search_by_title(\\"Python\\") for item in results: print(item.get_info()) # Expected output would be information about both the `book` and `dvd` items as they both include \\"Python\\" in their titles. ``` Notes: - Ensure your implementation adheres to good OOP principles. - Consider edge cases such as adding items with duplicate ids. - Make use of Python\'s iterators and generators where appropriate.","solution":"from abc import ABC, abstractmethod class Item(ABC): def __init__(self, id: int, title: str): self.__id = id self.__title = title @abstractmethod def get_info(self) -> str: return f\\"ID: {self.__id}, Title: {self.__title}\\" def get_title(self) -> str: return self.__title class Book(Item): def __init__(self, id: int, title: str, author: str): super().__init__(id, title) self.__author = author def get_info(self) -> str: base_info = super().get_info() return f\\"{base_info}, Author: {self.__author}\\" class DVD(Item): def __init__(self, id: int, title: str, duration: int): super().__init__(id, title) self.__duration = duration def get_info(self) -> str: base_info = super().get_info() return f\\"{base_info}, Duration: {self.__duration} minutes\\" class Library: def __init__(self): self.__items = [] def add_item(self, item: Item): self.__items.append(item) def search_by_title(self, title: str): return [item for item in self.__items if title.lower() in item.get_title().lower()] def __iter__(self): return iter(self.__items)"},{"question":"# Pandas BooleanArray and Kleene Logic Implementation You are given a pandas DataFrame with a boolean column that contains True, False, and NA values. Your task is to implement two functions: `apply_kleene_logical_and` and `apply_kleene_logical_or`. These functions perform logical AND (`&`) and logical OR (`|`) operations respectively on two boolean columns of the DataFrame, according to Kleene logic rules. Implement the following functions: # Function 1: apply_kleene_logical_and ```python def apply_kleene_logical_and(df, col1, col2): Perform Kleene logical AND operation on two boolean columns of the DataFrame. Parameters: df (pd.DataFrame): The DataFrame containing the boolean columns. col1 (str): The name of the first boolean column. col2 (str): The name of the second boolean column. Returns: pd.Series: A new boolean series after performing the AND operation. pass ``` # Function 2: apply_kleene_logical_or ```python def apply_kleene_logical_or(df, col1, col2): Perform Kleene logical OR operation on two boolean columns of the DataFrame. Parameters: df (pd.DataFrame): The DataFrame containing the boolean columns. col1 (str): The name of the first boolean column. col2 (str): The name of the second boolean column. Returns: pd.Series: A new boolean series after performing the OR operation. pass ``` Constraints: - The input DataFrame will have at least two columns with the dtype `boolean` containing True, False, and NA values. - You must adhere to Kleene logic as defined in the documentation. - You are not allowed to use external libraries other than pandas. Example: ```python import pandas as pd data = { \'A\': pd.Series([True, False, pd.NA], dtype=\\"boolean\\"), \'B\': pd.Series([False, pd.NA, True], dtype=\\"boolean\\") } df = pd.DataFrame(data) assert apply_kleene_logical_and(df, \'A\', \'B\').equals(pd.Series([False, False, pd.NA], dtype=\\"boolean\\")) assert apply_kleene_logical_or(df, \'A\', \'B\').equals(pd.Series([True, pd.NA, True], dtype=\\"boolean\\")) ``` Note: The expected results must adhere to the Kleene logic rules stated in the provided documentation.","solution":"import pandas as pd def apply_kleene_logical_and(df, col1, col2): Perform Kleene logical AND operation on two boolean columns of the DataFrame. Parameters: df (pd.DataFrame): The DataFrame containing the boolean columns. col1 (str): The name of the first boolean column. col2 (str): The name of the second boolean column. Returns: pd.Series: A new boolean series after performing the AND operation. result = df[col1] & df[col2] return result def apply_kleene_logical_or(df, col1, col2): Perform Kleene logical OR operation on two boolean columns of the DataFrame. Parameters: df (pd.DataFrame): The DataFrame containing the boolean columns. col1 (str): The name of the first boolean column. col2 (str): The name of the second boolean column. Returns: pd.Series: A new boolean series after performing the OR operation. result = df[col1] | df[col2] return result"},{"question":"You are given a matrix (a list of lists) where each inner list represents a row in the matrix. You need to write a function `transform_matrix(matrix: List[List[int]]) -> List[List[int]]` that performs the following transformations: 1. Filter out any rows that contain a negative number. 2. For each remaining row, create a new row where each element is squared. 3. Transpose the resulting matrix (i.e., convert rows to columns and vice versa). Input - `matrix`: A list of lists of integers, where each inner list represents a row in the matrix. Output - Returns a new matrix after applying the above transformations. Example ```python matrix = [ [1, 2, 3], [4, -5, 6], [7, 8, 9] ] transform_matrix(matrix) ``` Output: ```python [ [1, 49], [4, 64], [9, 81] ] ``` Constraints - The elements of the matrix will be integers. - The matrix will have at least one row and one column. - The length of rows may be varied, but each row will have the same columns. Note You may use list methods and list comprehensions to solve this problem. Try to make your solution both efficient and readable.","solution":"from typing import List def transform_matrix(matrix: List[List[int]]) -> List[List[int]]: # Step 1: Filter out any rows that contain a negative number. filtered_matrix = [row for row in matrix if all(x >= 0 for x in row)] # Step 2: For each remaining row, create a new row where each element is squared. squared_matrix = [[x**2 for x in row] for row in filtered_matrix] # Step 3: Transpose the resulting matrix (i.e., convert rows to columns and vice versa). transpose_matrix = list(map(list, zip(*squared_matrix))) return transpose_matrix"},{"question":"# PyTorch Distributed Training Assessment Objective: You will demonstrate your understanding of PyTorch\'s utilities for handling distributed training with uneven inputs by implementing a custom training loop using the `Join`, `Joinable`, and `JoinHook` classes. Problem Statement: 1. **Setup Distributed Training Environment:** - Implement a basic setup for distributed training using `torch.distributed` functionalities. - Assume a simple convolutional neural network (CNN) model and a synthetic dataset distributed across multiple processes. 2. **Manage Uneven Inputs:** - Utilize the `Join`, `Joinable`, and `JoinHook` classes to handle uneven distributions of data across processes. - Implement a custom hook that logs the training process of each subprocess. 3. **Custom Training Loop:** - Write a custom training loop that works within the distributed environment and handles possible premature terminations or lags in any subprocess. 4. **Expected Input and Output Formats:** - **Input:** The root process rank and the total number of processes. ```python def distributed_training_example(rank: int, world_size: int) -> None: # rank: int - The rank of the current process # world_size: int - The total number of processes ``` - **Output:** Logs of training progress for each subprocess. - **Constraints:** Ensure the training loop completes successfully even if some processes finish their data earlier than others. Implementation: ```python import torch import torch.distributed as dist from torch.distributed.algorithms import Join, Joinable, JoinHook import torch.nn as nn import torch.optim as optim # Define a simple CNN model class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(1, 32, 3, 1) self.fc1 = nn.Linear(32*26*26, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = self.conv1(x) x = x.view(-1, 32*26*26) x = self.fc1(x) x = self.fc2(x) return x class CustomJoinHook(JoinHook): def pre_hook(self, *args, **kwargs): print(f\\"Process {dist.get_rank()} entering join phase.\\") def post_hook(self, *args, **kwargs): print(f\\"Process {dist.get_rank()} exiting join phase.\\") def distributed_training_example(rank, world_size): # Initialize the process group for distributed training dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) # Create the model model = SimpleCNN().to(rank) model = nn.parallel.DistributedDataParallel(model, device_ids=[rank]) # Create synthetic dataset and DataLoader dataset = torch.utils.data.TensorDataset(torch.randn(100 + rank * 10, 1, 28, 28), torch.randint(0, 10, (100 + rank * 10,))) dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True) # Set up optimizer and loss function optimizer = optim.SGD(model.parameters(), lr=0.01) criterion = nn.CrossEntropyLoss().to(rank) # Make model joinable model.join = Joinable() model.join.register_hook(CustomJoinHook()) # Training loop with Join context manager with Join([model.join]) as join_ctx: for epoch in range(5): for data, target in dataloader: data, target = data.to(rank), target.to(rank) optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() print(f\\"Process {rank} - Epoch {epoch}, Loss: {loss.item()}\\") # Clean up dist.destroy_process_group() # Example Usage: if __name__ == \\"__main__\\": import os from torch.multiprocessing import spawn world_size = 4 os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' spawn(distributed_training_example, args=(world_size,), nprocs=world_size, join=True) ``` Notes: - This example provides a blueprint for handling distributed training with uneven data loads using PyTorch\'s advanced context managers. - You need to handle the process initialization and data distribution appropriately. - Make sure your environment supports multi-processing with PyTorch. Happy coding!","solution":"import torch import torch.distributed as dist from torch.distributed.algorithms.join import Join, Joinable, JoinHook import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset import os from torch.multiprocessing import spawn class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(1, 32, 3, 1) self.fc1 = nn.Linear(32*26*26, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = self.conv1(x) x = x.view(-1, 32*26*26) x = self.fc1(x) x = self.fc2(x) return x class CustomJoinHook(JoinHook): def pre_hook(self, *args, **kwargs): print(f\\"Process {dist.get_rank()} entering join phase.\\") def post_hook(self, *args, **kwargs): print(f\\"Process {dist.get_rank()} exiting join phase.\\") def distributed_training_example(rank, world_size): # Initialize the process group for distributed training dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) # Create the model model = SimpleCNN().to(rank) model = nn.parallel.DistributedDataParallel(model, device_ids=[rank]) # Create synthetic dataset and DataLoader dataset = TensorDataset(torch.randn(100 + rank * 10, 1, 28, 28), torch.randint(0, 10, (100 + rank * 10,))) dataloader = DataLoader(dataset, batch_size=16, shuffle=True) # Set up optimizer and loss function optimizer = optim.SGD(model.parameters(), lr=0.01) criterion = nn.CrossEntropyLoss().to(rank) # Make model joinable joinable_model = model joinable_model.join = Joinable() joinable_model.join.register_hook(CustomJoinHook()) # Training loop with Join context manager with Join([joinable_model.join]) as join_ctx: for epoch in range(5): for data, target in dataloader: data, target = data.to(rank), target.to(rank) optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() print(f\\"Process {rank} - Epoch {epoch}, Loss: {loss.item()}\\") # Clean up dist.destroy_process_group() # Example Usage: if __name__ == \\"__main__\\": world_size = 4 os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' spawn(distributed_training_example, args=(world_size,), nprocs=world_size, join=True)"},{"question":"# Comprehensive File Parsing and Configuration Management Task Objective You are required to implement a Python utility that can perform the following tasks: 1. Read a configuration from an INI file using `configparser`. 2. Use the configuration to read a specified CSV file using the `csv` module. 3. Generate an Apple `.plist` file using the `plistlib` module, containing a summary of information from the CSV file. 4. Optionally, parse a `.netrc` file to retrieve login credentials. Task Details You need to implement the following functions: 1. **read_config(file_path: str) -> dict**: - Reads a configuration file (INI format) and returns the configuration as a dictionary. - The configuration file should specify paths to a `csv` file and a `.plist` output file. 2. **read_csv(file_path: str) -> list of dict**: - Reads the specified CSV file and returns a list of dictionaries, where each dictionary corresponds to a row in the CSV file. - The keys of the dictionary should be the column headers. 3. **generate_plist(data: list of dict, output_path: str) -> None**: - Writes the aggregated data into a `.plist` file at the specified output path. - The `.plist` should contain a summary with: - The total number of records. - A list of the field (column) names. - A dictionary representation of the data (convert CSV rows into a dictionary). 4. **parse_netrc(file_path: str) -> dict** (optional): - Reads a `.netrc` file and returns the login credentials as a dictionary. Input Format 1. **INI file** should follow this template: ```ini [files] csv_path = path/to/your/input.csv plist_path = path/to/your/output.plist ``` 2. **CSV file**: A CSV file with a header row specifying column names. 3. **Netrc file** (optional): A `.netrc` file typically used to store machine login information. Output Format 1. A `.plist` file at the specified path containing summarized data. Constraints 1. Assume the CSV file is well-formed. 2. The `.plist` file should be in XML format, preserving character encoding. 3. Raise appropriate exceptions for file handling errors (e.g., file not found). Example Assume you have the following **config.ini**: ```ini [files] csv_path = data/input.csv plist_path = data/output.plist ``` And the **input.csv** as follows: ```text Name, Age, Occupation Alice, 29, Engineer Bob, 34, Doctor Charlie, 25, Artist ``` The generated **output.plist** should summarize this data with a structure similar to: ```xml <!DOCTYPE plist PUBLIC \\"-//Apple//DTD PLIST 1.0//EN\\" \\"http://www.apple.com/DTDs/PropertyList-1.0.dtd\\"> <plist version=\\"1.0\\"> <dict> <key>total_records</key> <integer>3</integer> <key>fields</key> <array> <string>Name</string> <string>Age</string> <string>Occupation</string> </array> <key>data</key> <array> <dict> <key>Name</key> <string>Alice</string> <key>Age</key> <integer>29</integer> <key>Occupation</key> <string>Engineer</string> </dict> <dict> <key>Name</key> <string>Bob</string> <key>Age</key> <integer>34</integer> <key>Occupation</key> <string>Doctor</string> </dict> <dict> <key>Name</key> <string>Charlie</key> <key>Age</key> <integer>25</integer> <key>Occupation</key> <string>Artist</string> </dict> </array> </dict> </plist> ``` Remember to ensure your solution reads the configuration properly, reads the CSV correctly, and generates a well-formed `.plist` file. ```python # Example Python function structure def read_config(file_path: str) -> dict: pass # Implement this function def read_csv(file_path: str) -> list of dict: pass # Implement this function def generate_plist(data: list of dict, output_path: str) -> None: pass # Implement this function def parse_netrc(file_path: str) -> dict: pass # Implement this function (Optional) ```","solution":"import configparser import csv import plistlib def read_config(file_path: str) -> dict: Reads a configuration file (INI format) and returns the configuration as a dictionary. config = configparser.ConfigParser() config.read(file_path) config_dict = { \'csv_path\': config[\'files\'][\'csv_path\'], \'plist_path\': config[\'files\'][\'plist_path\'] } return config_dict def read_csv(file_path: str) -> list: Reads the specified CSV file and returns a list of dictionaries, where each dictionary corresponds to a row in the CSV file. with open(file_path, mode=\'r\', newline=\'\', encoding=\'utf-8\') as csvfile: reader = csv.DictReader(csvfile) data = [row for row in reader] return data def generate_plist(data: list, output_path: str) -> None: Writes the aggregated data into a `.plist` file at the specified output path. summary = { \'total_records\': len(data), \'fields\': list(data[0].keys()) if data else [], \'data\': data } with open(output_path, \'wb\') as plist_file: plistlib.dump(summary, plist_file) def parse_netrc(file_path: str) -> dict: Reads a .netrc file and returns the login credentials as a dictionary. login_info = {} with open(file_path, \'r\') as netrc_file: for line in netrc_file: tokens = line.split() if len(tokens) == 6 and tokens[0] == \'machine\': login_info[tokens[1]] = { \'login\': tokens[3], \'password\': tokens[5] } return login_info"},{"question":"# URL Manipulation with `urllib.parse` You are tasked with implementing a function that processes and combines several URLs. The function should take a base URL and a list of relative URLs, then: 1. Resolve each relative URL against the base URL to form absolute URLs. 2. Parse each absolute URL to extract its components. 3. Modify the query component by appending a fixed query parameter to each URL. 4. Reconstruct the modified URL from its components. 5. Return a list of the modified absolute URLs. # Function Signature ```python from typing import List from urllib.parse import urljoin, urlparse, urlunparse, parse_qs, urlencode def process_urls(base_url: str, relative_urls: List[str], query_key: str, query_value: str) -> List[str]: pass ``` # Input - `base_url`: A string representing the base URL. - `relative_urls`: A list of strings, each representing a relative URL. - `query_key`: A string representing the key to be added to the query. - `query_value`: A string representing the value to be added to the query. # Output - A list of strings, each representing the modified absolute URL obtained by: 1. Resolving the relative URL against the base URL. 2. Modifying the query component. 3. Reconstructing the URL. # Constraints - Assume that `base_url` and `relative_urls` are properly formatted URLs. - All relative URLs will be resolved correctly using the provided base URL. - The query parameters will not contain any reserved URL characters that require encoding. # Example ```python base_url = \\"http://example.com/path/\\" relative_urls = [\\"subpath/file1.html\\", \\"subpath/file2.html\\"] query_key = \\"ref\\" query_value = \\"12345\\" result = process_urls(base_url, relative_urls, query_key, query_value) ``` # Expected Output ```python [ \\"http://example.com/path/subpath/file1.html?ref=12345\\", \\"http://example.com/path/subpath/file2.html?ref=12345\\" ] ``` # Hints 1. Use `urljoin()` to combine the base and relative URLs. 2. Use `urlparse()` to parse each absolute URL into components. 3. Use `parse_qs()` and `urlencode()` to manipulate the query component. 4. Use `urlunparse()` to reconstruct the modified URL. # Additional Information Refer to the `urllib.parse` module documentation for details on the functions and their usage.","solution":"from typing import List from urllib.parse import urljoin, urlparse, urlunparse, parse_qs, urlencode def process_urls(base_url: str, relative_urls: List[str], query_key: str, query_value: str) -> List[str]: absolute_urls = [urljoin(base_url, relative_url) for relative_url in relative_urls] modified_urls = [] for url in absolute_urls: parsed_url = urlparse(url) query_params = parse_qs(parsed_url.query) query_params[query_key] = query_value # Encode the query parameters encoded_query = urlencode(query_params, doseq=True) modified_url = urlunparse(( parsed_url.scheme, parsed_url.netloc, parsed_url.path, parsed_url.params, encoded_query, parsed_url.fragment )) modified_urls.append(modified_url) return modified_urls"},{"question":"# Python Development Mode - Resource Management and Debugging **Objective**: Write a function that reads a list of filenames from a given text file and prints the first line of each file. Ensure that all resources are properly managed and adhere to best practices suitable for running with Python\'s Development Mode. **Function Signature**: ```python def print_first_lines(filenames_file: str) -> None: pass ``` **Input**: - `filenames_file`: A string representing the path to a text file. Each line in this file contains the path to another text file. **Output**: - None (The function should print the first line of each file specified in `filenames_file`) **Constraints**: - Each file listed in `filenames_file` exists and is accessible. - The function should handle any potential exceptions that can arise from file handling. - You must demonstrate proper resource management, such as using context managers. - Assume files can be large, so efficient handling (e.g., not reading entire files into memory unless necessary) is important. - Additionally, make sure the function works correctly under Python Development Mode, such that no ResourceWarnings or memory violations occur. **Requirements**: 1. Use the context manager (`with open(...)`) to ensure files are properly closed. 2. Handle and report any unexpected exceptions during file operations. 3. Print a clear traceback if a file cannot be read for some reason. 4. Ensure the program runs correctly and efficiently without causing warnings or crashes, particularly under Python Development Mode. **Example**: Assume the following structure: - `filenames.txt` content: ``` file1.txt file2.txt ``` - `file1.txt` contains: ``` Content of file 1. ``` - `file2.txt` contains: ``` Content of file 2. ``` **Expected execution**: ```python print_first_lines(\'filenames.txt\') # Output: # Content of file 1. # Content of file 2. ``` **Notes**: - Pay attention to correct handling of resources to avoid any ResourceWarnings. Conduct thorough testing, especially using Python Development Mode to ensure compliance.","solution":"def print_first_lines(filenames_file: str) -> None: import traceback def read_first_line(file_path: str) -> str: try: with open(file_path, \'r\') as file: return file.readline().strip() except Exception as e: traceback.print_exc() return f\\"Error reading {file_path}: {str(e)}\\" try: with open(filenames_file, \'r\') as f: file_paths = f.readlines() for path in file_paths: path = path.strip() if path: first_line = read_first_line(path) print(first_line) except Exception as e: traceback.print_exc() print(f\\"Error reading {filenames_file}: {str(e)}\\")"},{"question":"Objective Design a function that processes an XML input to generate summary and detail HTML pages. Question Given a series of XML strings representing slideshows, write a Python function using `xml.dom.minidom` that: 1. Parses each XML string into a DOM document. 2. Compiles a summary HTML of all slideshow titles. 3. Generates detailed HTML pages for each slideshow, preserving the structure: slideshow titles, slide titles, and slide points. The slideshows are represented using the following XML structure: ```xml <slideshow> <title>[Slideshow Title]</title> <slide> <title>[Slide Title]</title> <point>[Slide Point]</point> <point>[Slide Point]</point> ... </slide> ... </slideshow> ``` # Your function should: - Take a list of XML strings as input. - Return two results: - A single HTML string summarizing the titles of all slideshows. - A list of HTML strings, each representing a detailed page for a slideshow. # Example Input: ```python slideshows = [ \'\'\'<slideshow> <title>First Slideshow</title> <slide> <title>First Slide</title> <point>Introduction</point> <point>Overview</point> </slide> <slide> <title>Second Slide</title> <point>Details</point> <point>Conclusion</point> </slide> </slideshow>\'\'\', \'\'\'<slideshow> <title>Second Slideshow</title> <slide> <title>Starting Slide</title> <point>Background</point> <point>Setup</point> </slide> </slideshow>\'\'\' ] ``` # Example Output: ```python summary_html = <html> <head><title>Slideshows Summary</title></head> <body> <h1>Slideshows</h1> <ul> <li>First Slideshow</li> <li>Second Slideshow</li> </ul> </body> </html> detailed_html = [ <html> <head><title>First Slideshow</title></head> <body> <h1>First Slideshow</h1> <h2>First Slide</h2> <ul> <li>Introduction</li> <li>Overview</li> </ul> <h2>Second Slide</h2> <ul> <li>Details</li> <li>Conclusion</li> </ul> </body> </html> , <html> <head><title>Second Slideshow</title></head> <body> <h1>Second Slideshow</h1> <h2>Starting Slide</h2> <ul> <li>Background</li> <li>Setup</li> </ul> </body> </html> ] ``` # Constraints: - Each slideshow XML string is guaranteed to follow the given structure. - The XML strings always contain valid XML according to the structure. Implement the function `generate_slideshow_html(slideshows: List[str]) -> Tuple[str, List[str]]` to solve the problem.","solution":"from typing import List, Tuple from xml.dom.minidom import parseString def generate_slideshow_html(slideshows: List[str]) -> Tuple[str, List[str]]: summary_html = \\"<html>n<head><title>Slideshows Summary</title></head>n<body>n<h1>Slideshows</h1>n<ul>n\\" detailed_html_list = [] for xml_str in slideshows: # Parsing the XML string dom = parseString(xml_str) slideshow_title = dom.getElementsByTagName(\\"title\\")[0].firstChild.nodeValue summary_html += f\\" <li>{slideshow_title}</li>n\\" # Preparing detailed HTML for each slideshow detailed_html = f\\"<html>n<head><title>{slideshow_title}</title></head>n<body>n<h1>{slideshow_title}</h1>n\\" slides = dom.getElementsByTagName(\\"slide\\") for slide in slides: slide_title = slide.getElementsByTagName(\\"title\\")[0].firstChild.nodeValue detailed_html += f\\"<h2>{slide_title}</h2>n<ul>n\\" points = slide.getElementsByTagName(\\"point\\") for point in points: detailed_html += f\\" <li>{point.firstChild.nodeValue}</li>n\\" detailed_html += \\"</ul>n\\" detailed_html += \\"</body>n</html>\\" detailed_html_list.append(detailed_html) summary_html += \\"</ul>n</body>n</html>\\" return summary_html, detailed_html_list"},{"question":"# Webbrowser Module Assessment Problem Statement You are asked to implement a Python function that automates the process of opening a sequence of web URLs in a defined manner. The requirements are as follows: 1. The function will accept a list of URLs and an optional argument specifying the opening mode (`\'window\'` or `\'tab\'`). 2. If the mode is `\'window\'`, each URL should be opened in a new window. 3. If the mode is `\'tab\'`, each URL should be opened in a new tab within the same browser window. 4. If the mode is not specified, it should default to opening URLs in new tabs. 5. You should handle any potential exceptions that can occur when opening the URLs and provide a meaningful error message. Input - `urls` (list of str): A list of web URLs to be opened. - `mode` (str, optional): Mode of opening URLs, can be `\'window\'` or `\'tab\'`. Defaults to `\'tab\'`. Output - None Constraints - The URLs list should have at least one URL and at most 100 URLs. - The mode should be either `\'window\'` or `\'tab\'`. - Ensure the `webbrowser` module is used for opening URLs. Example ```python def open_urls(urls, mode=\\"tab\\"): Opens a list of URLs in the specified mode using the webbrowser module. :param urls: List of URLs to be opened. :param mode: Opening mode, can be \'window\' or \'tab\'. Defaults to \'tab\'. :raises ValueError: If an invalid mode is provided. # Your implementation here # Example usage: urls = [\\"https://www.python.org\\", \\"https://www.github.com\\"] open_urls(urls, mode=\\"window\\") ``` # Instructions 1. Implement the function `open_urls(urls, mode=\\"tab\\")` following the requirements specified above. 2. Use the `webbrowser` module to open the URLs. 3. Make sure to handle exceptions and provide meaningful error messages if an error occurs. 4. Test your implementation with different sets of URLs and modes to ensure it works as expected.","solution":"import webbrowser def open_urls(urls, mode=\\"tab\\"): Opens a list of URLs in the specified mode using the webbrowser module. :param urls: List of URLs to be opened. :param mode: Opening mode, can be \'window\' or \'tab\'. Defaults to \'tab\'. :raises ValueError: If an invalid mode is provided. if not urls: raise ValueError(\\"The URLs list should contain at least one URL.\\") if mode not in [\'window\', \'tab\']: raise ValueError(\\"Invalid mode provided. Use \'window\' or \'tab\'.\\") try: for url in urls: if mode == \'window\': webbrowser.open_new(url) else: # mode == \'tab\' webbrowser.open_new_tab(url) except Exception as e: raise RuntimeError(f\\"An error occurred when trying to open URLs: {e}\\") # Example usage: urls = [\\"https://www.python.org\\", \\"https://www.github.com\\"] open_urls(urls, mode=\\"window\\")"},{"question":"Objective To assess the students\' understanding of asynchronous programming with the `asyncio` module in Python, evaluating their ability to create and handle coroutines, manage concurrency, and address common pitfalls. Problem Statement You are tasked with designing a concurrent application that processes a list of tasks asynchronously. The application should meet the following requirements: 1. **Asynchronous Task Function:** - Write an asynchronous function `process_task(task_id)` that simulates processing a task by: - Sleeping for a random duration between 0.1 to 1.0 seconds. - Printing a message indicating the task has been completed (`Task {task_id} completed`). 2. **Main Runner Function:** - Write an asynchronous function `main(tasks)`, where `tasks` is a list of integers representing task IDs. - This function should: - Create and schedule the tasks using `asyncio.create_task()`. - Wait for all the tasks to complete. - Ensure that if any task raises an exception, it is properly handled and logged. 3. **Debug Mode Activation:** - Enable asyncio debug mode. - Set the asyncio logger\'s log level to DEBUG. 4. **Running the Application:** - Write the top-level code to run the `main()` function with the debug mode activated. - Use proper exception handling to catch and log any errors that occur while running the event loop. - Assume the tasks list contains the integers from 1 to 10. Input and Output - **Input:** A list of integers representing task IDs. - **Output:** Messages indicating the completion of each task, and logs for any exception if they occur. Constraints and Requirements - Use the `asyncio` module. - Ensure all coroutines are properly awaited. - Properly handle and log all exceptions. - Run the event loop in debug mode for better traceability. Example ```python import asyncio import random import logging async def process_task(task_id): # Simulate processing time await asyncio.sleep(random.uniform(0.1, 1.0)) print(f\\"Task {task_id} completed\\") async def main(tasks): # Enable asyncio debug mode asyncio.get_running_loop().set_debug(True) # Set logger level to DEBUG logging.basicConfig(level=logging.DEBUG) # List to store the task objects task_list = [] for task_id in tasks: # Create and schedule the task task = asyncio.create_task(process_task(task_id)) task_list.append(task) # Wait for all tasks to complete and handle exceptions for task in task_list: try: await task except Exception as e: logging.error(f\\"Task {task.get_name()} raised an exception: {e}\\") if __name__ == \\"__main__\\": tasks = list(range(1, 11)) # Run the main function try: asyncio.run(main(tasks), debug=True) except Exception as e: logging.error(f\\"An error occurred: {e}\\") ``` Ensure your code adheres to the constraints and demonstrates an understanding of asynchronous programming concepts with `asyncio`.","solution":"import asyncio import random import logging async def process_task(task_id): Simulates processing a task by sleeping for a random duration and printing a completion message. await asyncio.sleep(random.uniform(0.1, 1.0)) print(f\\"Task {task_id} completed\\") async def main(tasks): Main runner function that schedules and runs a list of asynchronous tasks. # Enable asyncio debug mode asyncio.get_running_loop().set_debug(True) # Configure logger logging.basicConfig(level=logging.DEBUG) # List to store the task objects task_list = [] for task_id in tasks: # Create and schedule the task task = asyncio.create_task(process_task(task_id)) task_list.append(task) # Wait for all tasks to complete and handle exceptions for task in task_list: try: await task except Exception as e: logging.error(f\\"Task {task.get_name()} raised an exception: {e}\\") def run_application(): Top-level code to run the asyncio application with proper exception handling and debug mode activated. tasks = list(range(1, 11)) try: asyncio.run(main(tasks), debug=True) except Exception as e: logging.error(f\\"An error occurred: {e}\\")"},{"question":"# Filename Pattern Matching Utility You have been tasked with creating a utility function that will analyze and filter a list of filenames based on various shell-style patterns provided. This utility should make use of the `fnmatch` module for matching patterns. Requirements: 1. **Function Name**: `filter_files_by_patterns` 2. **Input Parameters**: - `file_list` (List[str]): A list of filenames (strings). - `patterns` (List[str]): A list of patterns (strings) using shell-style wildcards. 3. **Output Type**: List[str] 4. **Constraints**: - The function should apply all patterns and return filenames that match any one of the patterns. - The matching should be case-insensitive. - Performance should be considered; the solution should avoid unnecessary iterations over the filenames. Examples ```python file_list = [\\"report1.txt\\", \\"report2.doc\\", \\"summary.pdf\\", \\"data.bin\\", \\"img01.PNG\\"] patterns = [\\"*.txt\\", \\"*.pdf\\", \\"*.PNG\\"] print(filter_files_by_patterns(file_list, patterns)) # Expected Output: [\'report1.txt\', \'summary.pdf\', \'img01.PNG\'] file_list = [\\"photo1.jpg\\", \\"document1.txt\\", \\"Document2.TXT\\", \\"notes.md\\"] patterns = [\\"*.txt\\", \\"*.TXT\\", \\"*.md\\"] print(filter_files_by_patterns(file_list, patterns)) # Expected Output: [\'document1.txt\', \'Document2.TXT\', \'notes.md\'] ``` Guidelines - Use the `fnmatch` module\'s `fnmatch` function to perform pattern matching. - Utilize efficient list processing techniques to ensure the function performs well with larger input sizes. Implement the `filter_files_by_patterns` function adhering to the given requirements and constraints.","solution":"import fnmatch import os def filter_files_by_patterns(file_list, patterns): Filters and returns the filenames that match any of the given patterns. Parameters: - file_list (List[str]): A list of filenames. - patterns (List[str]): A list of patterns using shell-style wildcards. Returns: - List[str]: A list of filenames that match at least one of the patterns. # Convert all patterns to lowercase for case-insensitive matching lowercase_patterns = [pattern.lower() for pattern in patterns] # Initialize a set to store matching files matching_files = set() for file in file_list: for pattern in lowercase_patterns: if fnmatch.fnmatch(file.lower(), pattern): matching_files.add(file) break # No need to check other patterns if one matches return list(matching_files)"},{"question":"You are tasked with diagnosing the performance of a machine learning model using validation curves and learning curves in scikit-learn. Implement functions that generate these curves for any given estimator and dataset while ensuring the results help inform about overfitting, underfitting, and whether more training data will be beneficial. Problem Statement: 1. **Validation Curve**: Implement a function `generate_validation_curve` that: - Takes an estimator, dataset (X, y), a hyperparameter name, a range for the hyperparameter values, and additional estimator parameters (optional) as input. - Returns training scores and validation scores across the specified hyperparameter range, and plots the validation curve. 2. **Learning Curve**: Implement a function `generate_learning_curve` that: - Takes an estimator, dataset (X, y), training set sizes, and cross-validation splitting strategy as input. - Returns the training sizes, training scores, validation scores for the specified training set sizes, and plots the learning curve. Input and Output Formats: 1. Function: `generate_validation_curve` - **Input**: - `estimator`: A scikit-learn estimator (e.g., `SVC()`) - `X`: Feature dataset (numpy array or pandas DataFrame) - `y`: Target labels (numpy array or pandas DataFrame/Series) - `param_name`: Name of the hyperparameter (string) - `param_range`: Range of values for the hyperparameter (list or numpy array) - `**kwargs`: Optional additional parameters for the estimator - **Output**: - `train_scores`: Training scores for each hyperparameter value (2D numpy array) - `valid_scores`: Validation scores for each hyperparameter value (2D numpy array) - **Plot**: Line plot displaying training and validation scores against hyperparameter values. 2. Function: `generate_learning_curve` - **Input**: - `estimator`: A scikit-learn estimator (e.g., `SVC()`) - `X`: Feature dataset (numpy array or pandas DataFrame) - `y`: Target labels (numpy array or pandas DataFrame/Series) - `train_sizes`: Sizes of the training set to use for generating the learning curve (list or numpy array) - `cv`: Cross-validation splitting strategy (int or cross-validation generator) - **Output**: - `train_sizes`: Training sizes used (1D numpy array) - `train_scores`: Training scores for each training size (2D numpy array) - `valid_scores`: Validation scores for each training size (2D numpy array) - **Plot**: Line plot displaying training and validation scores against training sizes. Example Usage: ```python from sklearn.svm import SVC from sklearn.datasets import load_iris from sklearn.utils import shuffle import numpy as np # Load and shuffle the dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Validation curve train_scores, valid_scores = generate_validation_curve( SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=np.logspace(-3, 2, 5) ) # Learning curve train_sizes, train_scores, valid_scores = generate_learning_curve( SVC(kernel=\\"linear\\"), X, y, train_sizes=[50, 80, 110], cv=5 ) ``` **Constraints**: - You may assume that the input data is clean and preprocessed. - You should handle potential issues such as small datasets or inappropriate parameter ranges gracefully.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import validation_curve, learning_curve def generate_validation_curve(estimator, X, y, param_name, param_range, **kwargs): Generates the validation curve for the given estimator and dataset. Parameters: - estimator: A scikit-learn estimator - X: Feature dataset - y: Target labels - param_name: Name of the hyperparameter to vary - param_range: Range of values for the hyperparameter - kwargs: Additional estimator parameters (optional) Returns: - train_scores: Training scores for each hyperparameter value - valid_scores: Validation scores for each hyperparameter value train_scores, valid_scores = validation_curve(estimator, X, y, param_name=param_name, param_range=param_range, **kwargs) plt.figure() plt.plot(param_range, np.mean(train_scores, axis=1), label=\\"Training score\\") plt.plot(param_range, np.mean(valid_scores, axis=1), label=\\"Validation score\\") plt.xlabel(param_name) plt.ylabel(\'Score\') plt.title(\'Validation Curve\') plt.legend(loc=\\"best\\") plt.grid(True) plt.show() return train_scores, valid_scores def generate_learning_curve(estimator, X, y, train_sizes, cv): Generates the learning curve for the given estimator and dataset. Parameters: - estimator: A scikit-learn estimator - X: Feature dataset - y: Target labels - train_sizes: Sizes of the training set to use for generating the learning curve - cv: Cross-validation splitting strategy Returns: - train_sizes: Training sizes used - train_scores: Training scores for each training size - valid_scores: Validation scores for each training size train_sizes, train_scores, valid_scores = learning_curve(estimator, X, y, train_sizes=train_sizes, cv=cv) plt.figure() plt.plot(train_sizes, np.mean(train_scores, axis=1), label=\\"Training score\\") plt.plot(train_sizes, np.mean(valid_scores, axis=1), label=\\"Validation score\\") plt.xlabel(\'Training set size\') plt.ylabel(\'Score\') plt.title(\'Learning Curve\') plt.legend(loc=\\"best\\") plt.grid(True) plt.show() return train_sizes, train_scores, valid_scores"},{"question":"**Objective**: To assess your understanding of the Python `subprocess` module, including starting and managing external processes, handling their input and output, and properly managing error scenarios. **Problem Statement**: You are tasked with writing a Python function that takes a list of shell commands and executes them in sequence. The function should capture the output of each command, handle errors gracefully, and ensure that resource cleanup is performed correctly. # Function Signature ```python def execute_commands(commands: list, timeout: int = 60) -> list: Executes a list of shell commands sequentially. Parameters: - commands (List[str]): A list of shell commands to be executed. - timeout (int, default=60): The maximum time in seconds to wait for each command to complete. Returns: - List[dict]: A list of dictionaries, each containing: - \'command\': The command executed. - \'stdout\': The captured standard output of the command. - \'stderr\': The captured standard error of the command. - \'returncode\': The exit code of the command. - \'error\': Error message if the command execution failed (if any), otherwise None. ``` # Requirements 1. **Subprocess Execution**: - Use `subprocess.run` or `subprocess.Popen` to execute each command. - Commands should be executed in the provided order. 2. **Output and Error Handling**: - Capture both `stdout` and `stderr` for each command. - If a command fails (non-zero exit code or timeout), capture the error message. - Ensure that commands stop execution on timeout and properly clean up resources. 3. **Return Format**: - Return a list of dictionaries containing the following keys: `command`, `stdout`, `stderr`, `returncode`, and `error`. - `error` should be `None` if the command executed successfully, otherwise it should contain the error message. # Constraints - Each command is a string that could be run in a shell. - Assume commands are safe and do not require additional input validation. - Command output can be reasonably large but not unlimited. # Example ```python commands = [ \\"echo \'Hello, World!\'\\", \\"ls -l /nonexistent_path\\", \\"sleep 5\\" ] result = execute_commands(commands, timeout=3) for command_output in result: print(f\\"Command: {command_output[\'command\']}\\") print(f\\"Return Code: {command_output[\'returncode\']}\\") print(f\\"Stdout: {command_output[\'stdout\']}\\") print(f\\"Stderr: {command_output[\'stderr\']}\\") print(f\\"Error: {command_output[\'error\']}\\") print(\\"-------\\") ``` # Deliverables - Implement the `execute_commands` function as described above. - Ensure proper error handling and resource management in your implementation. - Write unit tests to validate the function with different commands and edge cases (e.g., commands that fail, commands that timeout, etc.). **Note**: Do not use os.system or older functions; use the modern `subprocess` API as described in the provided documentation.","solution":"import subprocess def execute_commands(commands: list, timeout: int = 60) -> list: Executes a list of shell commands sequentially. Parameters: - commands (List[str]): A list of shell commands to be executed. - timeout (int, default=60): The maximum time in seconds to wait for each command to complete. Returns: - List[dict]: A list of dictionaries, each containing: - \'command\': The command executed. - \'stdout\': The captured standard output of the command. - \'stderr\': The captured standard error of the command. - \'returncode\': The exit code of the command. - \'error\': Error message if the command execution failed (if any), otherwise None. results = [] for command in commands: result = { \'command\': command, \'stdout\': None, \'stderr\': None, \'returncode\': None, \'error\': None } try: completed_process = subprocess.run( command, shell=True, capture_output=True, text=True, timeout=timeout ) result[\'stdout\'] = completed_process.stdout result[\'stderr\'] = completed_process.stderr result[\'returncode\'] = completed_process.returncode if completed_process.returncode != 0: result[\'error\'] = \'Command failed with non-zero exit code.\' except subprocess.TimeoutExpired: result[\'error\'] = \'Command timed out.\' except Exception as e: result[\'error\'] = str(e) results.append(result) return results"},{"question":"**Title:** Comprehensive File Management with ZIP Archives **Objective:** Demonstrate your understanding of the `zipfile` module by creating a Python function that manages ZIP archives, including creating, listing, extracting, and adding files to ZIP archives using various compression methods and ensuring the integrity of files. **Problem Statement:** You are tasked with implementing a function `manage_zip_archive(action, zip_filename, file_list=None, compression_method=\'ZIP_STORED\', password=None)` that performs different operations on ZIP files as described below: **Function Signature:** ```python def manage_zip_archive(action: str, zip_filename: str, file_list: list = None, compression_method: str = \'ZIP_STORED\', password: bytes = None) -> list: pass ``` **Parameters:** 1. `action` (str): The operation to perform. It can be one of the following values: - `\'create\'`: Create a new ZIP file with the provided `file_list`. - `\'list\'`: List all files in the ZIP archive. - `\'extract\'`: Extract files from the ZIP archive to the current directory. - `\'add\'`: Add files from the `file_list` to an existing ZIP archive. - `\'test\'`: Test whether the ZIP archive is valid and return the name of first invalid file or `None` if all files are valid. 2. `zip_filename` (str): The name of the ZIP file to create, read from, or write to. 3. `file_list` (list): A list of file paths to include in the ZIP file (required for \'create\' and \'add\' actions). 4. `compression_method` (str): The compression method to be used, one of `\'ZIP_STORED\'`, `\'ZIP_DEFLATED\'`, `\'ZIP_BZIP2\'`, or `\'ZIP_LZMA\'`. The default is `\'ZIP_STORED\'`. 5. `password` (bytes): The password to use for encrypting/decrypting files in the ZIP archive. If not provided, no password is set. **Returns:** - For `\'list\'` action, return a list of filenames in the ZIP archive. - For `\'test\'` action, return the name of the first invalid file or `None` if all files are valid. - For other actions, return an empty list. **Constraints:** - You must use the `zipfile` module for all operations. - Handle exceptions appropriately and ensure the `ZipFile` is closed properly. - Assume file paths in `file_list` are valid. - Compression methods aside from `\'ZIP_STORED\'` require the relevant modules (`zlib`, `bz2`, `lzma`) to be available. **Example Usage:** ```python # Example to create a ZIP file manage_zip_archive(\'create\', \'example.zip\', [\'file1.txt\', \'file2.txt\'], \'ZIP_DEFLATED\') # Example to list files in a ZIP archive print(manage_zip_archive(\'list\', \'example.zip\')) # Example to extract files from a ZIP archive manage_zip_archive(\'extract\', \'example.zip\') # Example to add files to an existing ZIP archive manage_zip_archive(\'add\', \'example.zip\', [\'file3.txt\', \'file4.txt\']) # Example to test the ZIP archive print(manage_zip_archive(\'test\', \'example.zip\')) ``` **Note:** Ensure your implementation is efficient and follows best practices in exception handling and context management.","solution":"import zipfile import os def manage_zip_archive(action, zip_filename, file_list=None, compression_method=\'ZIP_STORED\', password=None): Manages ZIP archives by performing various actions such as creating, listing, extracting, adding files, and testing integrity. Parameters: action (str): The operation to perform (\'create\', \'list\', \'extract\', \'add\', \'test\'). zip_filename (str): The name of the ZIP file. file_list (list, optional): List of file paths for \'create\' and \'add\' actions. compression_method (str, optional): Compression method to use (\'ZIP_STORED\', \'ZIP_DEFLATED\', \'ZIP_BZIP2\', \'ZIP_LZMA\'). Default is \'ZIP_STORED\'. password (bytes, optional): Password for encrypting/decrypting files in the ZIP archive. Default is None. Returns: list or None: Depending on the action, returns a list of filenames or None. compression_methods = { \'ZIP_STORED\': zipfile.ZIP_STORED, \'ZIP_DEFLATED\': zipfile.ZIP_DEFLATED, \'ZIP_BZIP2\': zipfile.ZIP_BZIP2, \'ZIP_LZMA\': zipfile.ZIP_LZMA, } compression = compression_methods.get(compression_method, zipfile.ZIP_STORED) try: if action == \'create\': if not file_list: raise ValueError(\\"file_list is required for \'create\' action\\") with zipfile.ZipFile(zip_filename, \'w\', compression) as zipf: for file in file_list: zipf.write(file, os.path.basename(file)) return [] elif action == \'list\': with zipfile.ZipFile(zip_filename, \'r\') as zipf: filenames = zipf.namelist() return filenames elif action == \'extract\': with zipfile.ZipFile(zip_filename, \'r\') as zipf: if password: zipf.setpassword(password) zipf.extractall() return [] elif action == \'add\': if not file_list: raise ValueError(\\"file_list is required for \'add\' action\\") with zipfile.ZipFile(zip_filename, \'a\', compression) as zipf: for file in file_list: zipf.write(file, os.path.basename(file)) return [] elif action == \'test\': with zipfile.ZipFile(zip_filename, \'r\') as zipf: if password: zipf.setpassword(password) bad_file = zipf.testzip() return bad_file else: raise ValueError(\\"Invalid action specified\\") except (zipfile.BadZipFile, zipfile.LargeZipFile) as e: print(f\\"An error occurred with the ZIP file: {e}\\") return []"},{"question":"Understanding and Visualizing Model Performance **Objective**: To assess the student\'s ability to apply validation and learning curves for model evaluation using scikit-learn. **Problem Statement**: Given the Iris dataset, your task is to implement functions that compute and plot both the validation curve and the learning curve for a Support Vector Machine (SVM) classifier with a linear kernel. You will then analyze the plots to deduce whether the model is overfitting, underfitting, or has an optimal bias-variance trade-off. **Requirements**: 1. **Function `plot_validation_curve`**: - **Input**: - `X`: feature matrix (numpy array). - `y`: target vector (numpy array). - `param_name`: name of the hyperparameter to evaluate (string). - `param_range`: range of values for the hyperparameter (list or numpy array). - **Output**: Plot showing the validation curve for the given hyperparameter. - **Constraints**: - Use 5-fold cross-validation. 2. **Function `plot_learning_curve`**: - **Input**: - `X`: feature matrix (numpy array). - `y`: target vector (numpy array). - `train_sizes`: list of sizes of the training sets to be used (list or numpy array). - `cv`: number of folds for cross-validation (integer). - **Output**: Plot showing the learning curve. - **Constraints**: - Use 5-fold cross-validation. **Performance Requirements**: - Ensure that the plots are clearly labeled and include titles, axis labels, and legends. - The functions should complete execution within a reasonable time frame (less than 2 minutes for typical inputs). # Example Usage: ```python import numpy as np from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.utils import shuffle from your_module_name import plot_validation_curve, plot_learning_curve # Load and shuffle dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Define hyperparameter range and training sizes param_range = np.logspace(-6, -1, 5) train_sizes = np.linspace(0.1, 1.0, 5) # Plot validation curve plot_validation_curve(X, y, param_name=\\"C\\", param_range=param_range) # Plot learning curve plot_learning_curve(X, y, train_sizes=train_sizes, cv=5) ``` # Analysis: 1. **Validation Curve**: - Interpret the plot to determine if the model is suggesting overfitting, underfitting, or an optimal state for different values of the hyperparameter. 2. **Learning Curve**: - Interpret the plot to understand whether increasing the training data will benefit the model, and whether the model is more affected by bias or variance. Provide your code implementation below: ```python import numpy as np from sklearn.model_selection import validation_curve, learning_curve from sklearn.svm import SVC import matplotlib.pyplot as plt def plot_validation_curve(X, y, param_name, param_range): train_scores, valid_scores = validation_curve( SVC(kernel=\\"linear\\"), X, y, param_name=param_name, param_range=param_range, cv=5, scoring=\\"accuracy\\", ) train_scores_mean = np.mean(train_scores, axis=1) valid_scores_mean = np.mean(valid_scores, axis=1) plt.figure() plt.plot(param_range, train_scores_mean, label=\\"Training score\\", color=\\"r\\") plt.plot(param_range, valid_scores_mean, label=\\"Validation score\\", color=\\"g\\") plt.xlabel(f\\"Parameter: {param_name}\\") plt.ylabel(\\"Score\\") plt.title(f\\"Validation Curve for {param_name}\\") plt.legend(loc=\\"best\\") plt.show() def plot_learning_curve(X, y, train_sizes, cv): train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\\"linear\\"), X, y, train_sizes=train_sizes, cv=cv, scoring=\\"accuracy\\" ) train_scores_mean = np.mean(train_scores, axis=1) valid_scores_mean = np.mean(valid_scores, axis=1) plt.figure() plt.plot(train_sizes, train_scores_mean, label=\\"Training score\\", color=\\"r\\") plt.plot(train_sizes, valid_scores_mean, label=\\"Validation score\\", color=\\"g\\") plt.xlabel(\\"Size of training set\\") plt.ylabel(\\"Score\\") plt.title(\\"Learning Curve\\") plt.legend(loc=\\"best\\") plt.show() ```","solution":"import numpy as np from sklearn.model_selection import validation_curve, learning_curve from sklearn.svm import SVC import matplotlib.pyplot as plt def plot_validation_curve(X, y, param_name, param_range): Plot validation curve for given hyperparameter. Parameters: X (numpy array): Feature matrix. y (numpy array): Target vector. param_name (str): Name of the hyperparameter to evaluate. param_range (list or numpy array): Range of values for the hyperparameter. train_scores, valid_scores = validation_curve( SVC(kernel=\\"linear\\"), X, y, param_name=param_name, param_range=param_range, cv=5, scoring=\\"accuracy\\", ) train_scores_mean = np.mean(train_scores, axis=1) valid_scores_mean = np.mean(valid_scores, axis=1) plt.figure() plt.plot(param_range, train_scores_mean, label=\\"Training score\\", color=\\"r\\") plt.plot(param_range, valid_scores_mean, label=\\"Validation score\\", color=\\"g\\") plt.xlabel(f\\"Parameter: {param_name}\\") plt.ylabel(\\"Score\\") plt.title(f\\"Validation Curve for {param_name}\\") plt.legend(loc=\\"best\\") plt.grid() plt.show() def plot_learning_curve(X, y, train_sizes, cv): Plot learning curve. Parameters: X (numpy array): Feature matrix. y (numpy array): Target vector. train_sizes (list or numpy array): List of sizes of the training sets to be used. cv (int): Number of folds for cross-validation. train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\\"linear\\"), X, y, train_sizes=train_sizes, cv=cv, scoring=\\"accuracy\\" ) train_scores_mean = np.mean(train_scores, axis=1) valid_scores_mean = np.mean(valid_scores, axis=1) plt.figure() plt.plot(train_sizes, train_scores_mean, label=\\"Training score\\", color=\\"r\\") plt.plot(train_sizes, valid_scores_mean, label=\\"Validation score\\", color=\\"g\\") plt.xlabel(\\"Size of training set\\") plt.ylabel(\\"Score\\") plt.title(\\"Learning Curve\\") plt.legend(loc=\\"best\\") plt.grid() plt.show()"},{"question":"**Objective:** Design a solution utilizing scikit-learn to analyze and visualize model performance through validation and learning curves. **Problem Statement:** You are provided with a dataset named `data.csv` which contains features and a target column. Your task is to perform the following steps: 1. **Data Preparation**: - Load the dataset and split it into features (`X`) and target (`y`). 2. **Model Training and Validation**: - Use a Support Vector Classifier (SVC) model with a linear kernel. - Implement validation_curve to analyze the effect of the hyperparameter `C` (penalty parameter of the error term) on training and validation scores. - Implement learning_curve to analyze how the number of training samples affects the training and validation scores. 3. **Visualization**: - Plot the validation curve and learning curve using the appropriate provided methods from scikit-learn. 4. **Analysis**: - Based on the plots, determine whether the model suffers from high variance, high bias, or neither. Provide a brief explanation for your conclusion. **Constraints:** - You should use `SVC` with a `linear` kernel from `sklearn.svm`. - For the `validation_curve`, test at least 5 values of `C` logarithmically spaced between (10^{-3}) and (10^3). - For the `learning_curve`, use at least 3 values for `train_sizes`. - Perform a 5-fold cross-validation for both validation and learning curves. **Input format:** - A CSV file `data.csv`. **Output format:** - Two plots: - Validation curve plot displaying training and validation scores for different values of `C`. - Learning curve plot showing training and validation scores for varying training set sizes. - A brief text analysis (max 200 words) explaining the findings. ```python import numpy as np import pandas as pd from sklearn.svm import SVC from sklearn.model_selection import validation_curve, learning_curve, train_test_split from sklearn.utils import shuffle import matplotlib.pyplot as plt # Step 1: Load the dataset data = pd.read_csv(\'data.csv\') X = data.drop(columns=[\'target\']) # Assuming \'target\' is the target column y = data[\'target\'] # Shuffle the dataset X, y = shuffle(X, y, random_state=0) # Step 2: Validation Curve param_range = np.logspace(-3, 3, 5) train_scores, valid_scores = validation_curve( SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range, cv=5 ) # Plot validation curve plt.figure() plt.plot(param_range, np.mean(train_scores, axis=1), label=\'Training score\') plt.plot(param_range, np.mean(valid_scores, axis=1), label=\'Validation score\') plt.xlabel(\'Parameter C\') plt.ylabel(\'Score\') plt.xscale(\'log\') plt.title(\'Validation Curve\') plt.legend() plt.show() # Step 3: Learning Curve train_sizes = [50, 100, 200] train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\'linear\'), X, y, train_sizes=train_sizes, cv=5 ) # Plot learning curve plt.figure() plt.plot(train_sizes, np.mean(train_scores, axis=1), label=\'Training score\') plt.plot(train_sizes, np.mean(valid_scores, axis=1), label=\'Validation score\') plt.xlabel(\'Training sizes\') plt.ylabel(\'Score\') plt.title(\'Learning Curve\') plt.legend() plt.show() # Step 4: Analysis # Based on the plots, provide your analysis: # [Include text analysis here] ```","solution":"import numpy as np import pandas as pd from sklearn.svm import SVC from sklearn.model_selection import validation_curve, learning_curve from sklearn.utils import shuffle import matplotlib.pyplot as plt def analyze_and_visualize_model_performance(file_path): # Step 1: Load the dataset data = pd.read_csv(file_path) X = data.drop(columns=[\'target\']) # Assuming \'target\' is the target column y = data[\'target\'] # Shuffle the dataset X, y = shuffle(X, y, random_state=0) # Step 2: Validation Curve param_range = np.logspace(-3, 3, 5) train_scores, valid_scores = validation_curve( SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range, cv=5 ) # Plot validation curve plt.figure() plt.plot(param_range, np.mean(train_scores, axis=1), label=\'Training score\') plt.plot(param_range, np.mean(valid_scores, axis=1), label=\'Validation score\') plt.xlabel(\'Parameter C\') plt.ylabel(\'Score\') plt.xscale(\'log\') plt.title(\'Validation Curve for SVC with linear kernel\') plt.legend() plt.show() # Step 3: Learning Curve train_sizes = [50, 100, 200] train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\'linear\'), X, y, train_sizes=train_sizes, cv=5 ) # Plot learning curve plt.figure() plt.plot(train_sizes, np.mean(train_scores, axis=1), label=\'Training score\') plt.plot(train_sizes, np.mean(valid_scores, axis=1), label=\'Validation score\') plt.xlabel(\'Training sizes\') plt.ylabel(\'Score\') plt.title(\'Learning Curve for SVC with linear kernel\') plt.legend() plt.show() # Step 4: Analysis # Based on the plots, we can analyze the model performance. # If the training score is much higher than the validation score in the validation curve, # it implies high variance (overfitting). If both are low, it implies high bias (underfitting). # For the learning curve, if the training score and validation score converge to a value, # it indicates a good fit. Large gap between curves indicates high variance. # Hypothetical Analysis: # - If validation curve shows high training score but low validation score for smaller values of C, # it might indicate high variance. # - If learning curve shows validation score improving significantly with more training data, # it might suggest the model benefits from more data and does not overfit heavily."},{"question":"# Missing Data Handler **Objective:** Create a function `missing_data_handler` that processes a pandas DataFrame to handle missing data using specified imputation methods. **Function Signature:** ```python import pandas as pd from typing import Optional, Dict, Any def missing_data_handler(df: pd.DataFrame, strategy: str, value: Optional[Any] = None, method: Optional[str] = None, limit: Optional[int] = None, limit_direction: Optional[str] = \\"forward\\", regex_replacements: Optional[Dict[str, str]] = None) -> pd.DataFrame: pass ``` **Parameters:** 1. `df` (pd.DataFrame): The input DataFrame. 2. `strategy` (str): The strategy for handling missing data. It can be one of the following: - `\\"fill\\"`: Fill missing values using the provided value. - `\\"interpolate\\"`: Interpolate missing values using the specified method. - `\\"regex_replace\\"`: Replace values based on a dictionary of regex patterns. 3. `value` (Any, optional): The value to fill missing data with when the strategy is `\\"fill\\"`. Default is `None`. 4. `method` (str, optional): The method to use for interpolation when the strategy is `\\"interpolate\\"`. Default is `None`. 5. `limit` (int, optional): The maximum number of consecutive NaNs to fill when the strategy is `\\"fill\\"` or `\\"interpolate\\"`. Default is `None`. 6. `limit_direction` (str, optional): The direction to fill missing values, can be either `\\"forward\\"`, `\\"backward\\"`, or `\\"both\\"`. Default is `\\"forward\\"`. 7. `regex_replacements` (Dict[str, str], optional): Dictionary where keys are regex patterns and values are the replacement strings when the strategy is `\\"regex_replace\\"`. Default is `None`. **Returns:** - `pd.DataFrame`: The DataFrame with the missing data handled according to the specified strategy. **Example Usage:** ```python import pandas as pd import numpy as np # Sample DataFrame data = {\'A\': [1, 2.1, np.nan, 4.7, 5.6, 6.8], \'B\': [0.25, np.nan, np.nan, 4, 12.2, 14.4]} df = pd.DataFrame(data) # Fill missing data with a specified value filled_df = missing_data_handler(df, strategy=\\"fill\\", value=0) print(filled_df) # Interpolate missing data using a specified method interp_df = missing_data_handler(df, strategy=\\"interpolate\\", method=\\"linear\\") print(interp_df) # Replace patterns using regular expressions data_with_patterns = {\'A\': [\'1\', \'2\', \'foo\', \'4\', \'bar\', \'6\'], \'B\': [\'0.25\', \'foo\', \'bar\', \'4\', \'12.2\', \'14.4\']} df_patterns = pd.DataFrame(data_with_patterns) regex_replacements_dict = {r\\"foo\\": \\"3.0\\", r\\"bar\\": \\"5.0\\"} replaced_df = missing_data_handler(df_patterns, strategy=\\"regex_replace\\", regex_replacements=regex_replacements_dict) print(replaced_df) ``` **Constraints:** 1. You may assume that the input DataFrame does not contain any nested structures. 2. Ensure that your implementation can handle large DataFrames efficiently. **Hints:** 1. Utilize `pd.isna` to detect missing values. 2. Refer to `pd.DataFrame.fillna`, `pd.DataFrame.interpolate`, and `pd.DataFrame.replace` for efficient implementations of the filling and replacement strategies. 3. Handle the edge cases where there could be mixed data types and non-numeric columns.","solution":"import pandas as pd from typing import Optional, Dict, Any def missing_data_handler(df: pd.DataFrame, strategy: str, value: Optional[Any] = None, method: Optional[str] = None, limit: Optional[int] = None, limit_direction: Optional[str] = \\"forward\\", regex_replacements: Optional[Dict[str, str]] = None) -> pd.DataFrame: if strategy == \\"fill\\": return df.fillna(value=value, limit=limit, method=method) elif strategy == \\"interpolate\\": return df.interpolate(method=method, limit=limit, limit_direction=limit_direction) elif strategy == \\"regex_replace\\": return df.replace(regex_replacements, regex=True) else: raise ValueError(\\"Invalid strategy specified.\\")"},{"question":"# Custom Serialization and Copying As part of a data science project, you need to serialize and copy objects of a custom class `DataFrameLite`, similar to a simplified version of pandas\' DataFrame. However, due to some custom behaviors and internal optimizations, the default pickling and copying mechanisms do not work as intended for this class. Your task is to implement and register custom pickling and copying functions for this class using the `copyreg` module. Class Definition ```python class DataFrameLite: def __init__(self, data): # data should be a list of dictionaries self.data = data def __getitem__(self, index): return self.data[index] def __repr__(self): return f\\"DataFrameLite({self.data})\\" ``` Requirements 1. **Custom Pickle Function**: Implement a function `pickle_df_lite(dfl)` that returns the class `DataFrameLite` and the data it holds. 2. **Custom Constructor**: Implement a function `constructor_df_lite(data)` that takes the data and returns an instance of `DataFrameLite`. 3. **Register Functions**: Use `copyreg.pickle` to register these custom functions. 4. **Verification**: Create an instance of `DataFrameLite` with sample data and demonstrate that pickling and copying work as expected by printing appropriate messages during the process. Input and Output Format The custom `pickle_df_lite` function should return a tuple with `DataFrameLite` and its data. The custom `constructor_df_lite` function should take data as an argument and return an instance of `DataFrameLite`. Example: ```python import copyreg, copy, pickle def pickle_df_lite(dfl): print(\\"Pickling DataFrameLite instance...\\") return DataFrameLite, (dfl.data,) def constructor_df_lite(data): return DataFrameLite(data) # Register the custom pickle function copyreg.pickle(DataFrameLite, pickle_df_lite, constructor_df_lite) # Test dfl = DataFrameLite([{\'a\': 1, \'b\': 2}, {\'a\': 3, \'b\': 4}]) print(dfl) # Copy the dataframe dfl_copy = copy.copy(dfl) print(dfl_copy) # Pickle the dataframe dfl_p = pickle.dumps(dfl) dfl_unp = pickle.loads(dfl_p) print(dfl_unp) ``` The expected output should demonstrate that the custom functions are called during the pickling and copying processes.","solution":"import copyreg import copy import pickle class DataFrameLite: def __init__(self, data): # data should be a list of dictionaries self.data = data def __getitem__(self, index): return self.data[index] def __repr__(self): return f\\"DataFrameLite({self.data})\\" def pickle_df_lite(dfl): Custom pickle function for DataFrameLite. print(\\"Pickling DataFrameLite instance...\\") return DataFrameLite, (dfl.data,) def constructor_df_lite(data): Custom constructor for DataFrameLite. return DataFrameLite(data) # Register the custom pickle function copyreg.pickle(DataFrameLite, pickle_df_lite, constructor_df_lite) # Test dfl = DataFrameLite([{\'a\': 1, \'b\': 2}, {\'a\': 3, \'b\': 4}]) print(dfl) # Copy the dataframe dfl_copy = copy.copy(dfl) print(dfl_copy) # Pickle the dataframe dfl_p = pickle.dumps(dfl) dfl_unp = pickle.loads(dfl_p) print(dfl_unp)"},{"question":"# Coding Assessment Task: Implementing Custom AutoGrad Function Objective In this coding assessment, you are required to implement a custom differentiable function in PyTorch by extending the `torch.autograd.Function` class. Your task is to implement a specific mathematical operation and ensure it properly integrates with PyTorch\'s Autograd system, including the forward and backward passes. Task Details **Mathematical Operation:** Exponential Linear Unit (ELU) The ELU function is defined as: [ text{ELU}(x) = begin{cases} x & text{if } x geq 0 alpha(e^x - 1) & text{if } x < 0 end{cases} ] where (alpha) is a parameter controlling the value to which an ELU saturates for negative net inputs. Requirements 1. Create a subclass of `torch.autograd.Function` named `ELUFunction`. 2. Implement the `forward` and `backward` static methods. 3. Validate the gradients using `torch.autograd.gradcheck`. 4. Wrap your custom function into a user-friendly Python function named `elu`. Implementation Steps 1. **Subclassing `torch.autograd.Function` Class:** - Define the `forward` method that accepts inputs and parameters, computes the output, and saves necessary tensors for the backward pass. - Define the `backward` method that computes the gradients with respect to the inputs. 2. **Define Forward Method:** - The `forward` method should compute the ELU of the input tensor. 3. **Define Backward Method:** - The `backward` method should compute the gradient of the ELU function with respect to its input using the saved tensors from the forward pass. 4. **Gradient Checking:** - Use `torch.autograd.gradcheck` to ensure the backward method computes the correct gradients. Expected Function Signatures ```python import torch from torch.autograd import Function class ELUFunction(Function): @staticmethod def forward(ctx, input, alpha=1.0): pass # TODO: Implement the forward pass @staticmethod def backward(ctx, grad_output): pass # TODO: Implement the backward pass def elu(input, alpha=1.0): return ELUFunction.apply(input, alpha) # Gradient Check (to be run after implementation) input = torch.randn(5, 5, dtype=torch.double, requires_grad=True) test = torch.autograd.gradcheck(elu, (input, 1.0), eps=1e-6, atol=1e-4) print(\\"Gradient check passed:\\", test) ``` Input and Output Formats - **Input:** A 2D tensor `input` of shape (M, N) and a scalar `alpha`. - **Output:** A 2D tensor of the same shape as the input tensor. Constraints and Limitations - Ensure that the `backward` method never modifies its input gradients in-place. - The input tensor will have a shape between (1, 1) and (100, 100). - Performance optimization is not the primary concern, but the implementation should be able to handle small to moderate-sized tensors efficiently. Testing - Students are required to run the `gradcheck` function provided in the template to verify the correctness of the gradients. - Additional unittests can be added to validate the forward and backward computations. Submission Submit the implementation of `ELUFunction` and the `elu` wrapper function along with the gradient check output.","solution":"import torch from torch.autograd import Function class ELUFunction(Function): @staticmethod def forward(ctx, input, alpha=1.0): ctx.alpha = alpha ctx.save_for_backward(input) out = torch.where(input >= 0, input, alpha * (torch.exp(input) - 1)) return out @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors alpha = ctx.alpha grad_input = grad_output.clone() grad_input[input < 0] *= alpha * torch.exp(input[input < 0]) return grad_input, None def elu(input, alpha=1.0): return ELUFunction.apply(input, alpha) # Gradient Check (to be run after implementation) input = torch.randn(5, 5, dtype=torch.double, requires_grad=True) test = torch.autograd.gradcheck(elu, (input, 1.0), eps=1e-6, atol=1e-4) print(\\"Gradient check passed:\\", test)"},{"question":"**Seaborn Advanced Plotting** You are given a dataset containing information on brain activity of different subjects in different regions and events. Your task is to create a multi-faceted line plot with error bands and markers to demonstrate advanced Seaborn plotting capabilities. **Dataset** You can use the example `fmri` dataset available in the Seaborn library through the command: ```python fmri = sns.load_dataset(\\"fmri\\") ``` **Task** 1. Filter the dataset to include only the \'parietal\' region and the \'stim\' event. 2. Create a line plot of the mean signal over time, with each subject\'s data shown as a separate line. 3. Add an error band to represent the standard error of the mean. 4. Distinguish between different subjects using different line styles. 5. Add markers at data points to show where data was sampled. **Specifications** 1. **Input**: None (use the provided `fmri` dataset). 2. **Output**: Display the line plot with the mentioned specifications. 3. **Constraints**: - Use Seaborn\'s objects API for plotting. - The plot should be clear and distinguishable. **Example Steps** 1. Load the `fmri` dataset. 2. Filter the dataframe for `region` as \'parietal\' and `event` as \'stim\'. 3. Create the plot: - Map \'timepoint\' to the x-axis, \'signal\' to the y-axis, and group by \'subject\'. - Add a line plot layer with unique styles for different subjects. - Add an error band to represent standard error. - Add markers at each data point. ```python import seaborn.objects as so import seaborn as sns # Load the dataset fmri = sns.load_dataset(\\"fmri\\") # Step 1: Filter the dataset filtered_fmri = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") # Step 2: Create the base Plot object p = so.Plot(filtered_fmri, x=\'timepoint\', y=\'signal\', color=\'subject\', linestyle=\'subject\') # Step 3: Add line and error band elements p.add(so.Line(), so.Agg()) p.add(so.Band(), so.Est(), group=\'subject\') # Step 4: Add markers p.add(so.Line(marker=\'o\', edgecolor=\'w\'), so.Agg(), linestyle=None) # Display the plot p.show() ``` Your code should generate a plot similar to the one described above, accurately reflecting the specifics and constraints provided.","solution":"import seaborn.objects as so import seaborn as sns def plot_fmri_advanced(): # Load the dataset fmri = sns.load_dataset(\\"fmri\\") # Step 1: Filter the dataset filtered_fmri = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") # Step 2: Create the base Plot object p = so.Plot(filtered_fmri, x=\'timepoint\', y=\'signal\', color=\'subject\', linestyle=\'subject\') # Step 3: Add line and error band elements p.add(so.Line(), so.Agg()) p.add(so.Band(), so.Est(), group=\'subject\') # Step 4: Add markers p.add(so.Line(marker=\'o\', edgecolor=\'w\'), so.Agg(), linestyle=None) # Display the plot p.show()"},{"question":"**Objective**: Implement a utility to encode and decode messages using Base64 and URL-safe Base64 encoding schemes provided by the `base64` module. Problem Statement: You need to implement two functions: `encode_message` and `decode_message` that will handle encoding and decoding of messages in both standard Base64 and URL-safe Base64 formats. 1. **encode_message(message: str, url_safe: bool = False) -> str**: - This function should take a plaintext message (string) and an optional `url_safe` parameter. - Use standard Base64 encoding if `url_safe` is `False` (default). - Use URL-safe Base64 encoding if `url_safe` is `True`. - Return the encoded message as a string. 2. **decode_message(encoded_message: str, url_safe: bool = False) -> str**: - This function should take an encoded message (string) and an optional `url_safe` parameter. - Use standard Base64 decoding if `url_safe` is `False` (default). - Use URL-safe Base64 decoding if `url_safe` is `True`. - Return the decoded message as a string. Input/Output Examples: ```python # Example 1: Standard Base64 Encoding/Decoding plaintext = \\"Hello, World!\\" encoded = encode_message(plaintext) # => \'SGVsbG8sIFdvcmxkIQ==\' decoded = decode_message(encoded) # => \'Hello, World!\' # Example 2: URL-safe Base64 Encoding/Decoding url_safe_plaintext = \\"Hello+World/=\\" encoded_safe = encode_message(url_safe_plaintext, url_safe=True) # => \'SGVsbG8rV29ybGQvPQ==\' decoded_safe = decode_message(encoded_safe, url_safe=True) # => \'Hello+World/=\' ``` Constraints: - You must use the functions from the `base64` module for encoding and decoding. - Handle both `bytes-like objects` and ASCII strings appropriately as specified in the module\'s modern interface. Hints: - Use `base64.b64encode` and `base64.b64decode` for standard Base64 encoding/decoding. - Use `base64.urlsafe_b64encode` and `base64.urlsafe_b64decode` for URL-safe Base64 encoding/decoding. Function Signatures: ```python def encode_message(message: str, url_safe: bool = False) -> str: pass def decode_message(encoded_message: str, url_safe: bool = False) -> str: pass ``` Good luck and make sure to test your code against various test cases to ensure accuracy!","solution":"import base64 def encode_message(message: str, url_safe: bool = False) -> str: Encodes a given message into Base64 or URL-safe Base64 format. Args: - message (str): The plaintext message to encode. - url_safe (bool): If True, use URL-safe Base64 encoding. Default is False. Returns: - str: The encoded message. message_bytes = message.encode(\'utf-8\') if url_safe: encoded_bytes = base64.urlsafe_b64encode(message_bytes) else: encoded_bytes = base64.b64encode(message_bytes) return encoded_bytes.decode(\'utf-8\') def decode_message(encoded_message: str, url_safe: bool = False) -> str: Decodes a given Base64 or URL-safe Base64 encoded message. Args: - encoded_message (str): The Base64 encoded message to decode. - url_safe (bool): If True, use URL-safe Base64 decoding. Default is False. Returns: - str: The decoded plaintext message. encoded_bytes = encoded_message.encode(\'utf-8\') if url_safe: decoded_bytes = base64.urlsafe_b64decode(encoded_bytes) else: decoded_bytes = base64.b64decode(encoded_bytes) return decoded_bytes.decode(\'utf-8\')"},{"question":"You are tasked with writing a Python script utilizing the `os` module to perform the following operations: 1. **Access and Modify Environment Variables**: - Retrieve the current value of the `HOME` environment variable and print it. - Add a new environment variable `NEW_VAR` with a value of `HelloWorld`. - Modify the `PATH` environment variable to include a new directory `/usr/new_path`. - Ensure that these changes are reflected when you spawn a new process using `subprocess.run()`. 2. **Handle Large Files**: - Write a function `create_large_file(filename, size_in_gb)` that creates a file of the specified size (in gigabytes). - Write another function `read_large_file(filename)` that reads and prints the first 100 bytes of this file. Ensure that your code is efficient and handles large files correctly. # Function Signatures ```python def create_large_file(filename: str, size_in_gb: int) -> None: pass def read_large_file(filename: str) -> bytes: pass ``` # Constraints - When modifying environment variables, ensure changes persist when spawning new processes. - Do not use external libraries other than `os` and `subprocess`. # Example Usage ```python import os import subprocess def main(): # Part 1: Environment Variable Management # Printing current HOME environment variable print(\'Current HOME:\', os.environ[\'HOME\']) # Adding a new environment variable os.environ[\'NEW_VAR\'] = \'HelloWorld\' print(\'NEW_VAR set to:\', os.environ[\'NEW_VAR\']) # Modifying the PATH environment variable os.environ[\'PATH\'] += os.pathsep + \'/usr/new_path\' print(\'Modified PATH:\', os.environ[\'PATH\']) # Check environment variables in a new subprocess result = subprocess.run([\'printenv\'], capture_output=True, text=True) print(\'Environment in subprocess:\', result.stdout) # Part 2: Large File Handling filename = \'large_test_file.txt\' create_large_file(filename, 3) # Creates a file of 3 GB first_100_bytes = read_large_file(filename) print(f\'The first 100 bytes of {filename} are:\', first_100_bytes) if __name__ == \\"__main__\\": main() ``` # Notes - The environment modification must be tested to ensure it\'s effective across subprocesses. - Large file handling should be efficient and follow best practices for performance.","solution":"import os import subprocess def manage_environment_variables(): Access and modify environment variables. Print the current value of the `HOME` environment variable. Add a new environment variable `NEW_VAR` with a value of `HelloWorld`. Modify the `PATH` environment variable to include a new directory `/usr/new_path`. # Retrieve and print the current value of the HOME environment variable home_value = os.environ.get(\'HOME\') print(\'Current HOME:\', home_value) # Add a new environment variable NEW_VAR os.environ[\'NEW_VAR\'] = \'HelloWorld\' print(\'NEW_VAR set to:\', os.environ[\'NEW_VAR\']) # Modify the PATH environment variable to include a new directory os.environ[\'PATH\'] += os.pathsep + \'/usr/new_path\' print(\'Modified PATH:\', os.environ[\'PATH\']) # Spawn a new process to check environment variables result = subprocess.run([\'printenv\'], capture_output=True, text=True) print(\'Environment in subprocess:\', result.stdout) def create_large_file(filename: str, size_in_gb: int) -> None: Create a file of the specified size in gigabytes. total_bytes = size_in_gb * 1024**3 with open(filename, \'wb\') as f: f.seek(total_bytes - 1) f.write(b\'0\') def read_large_file(filename: str) -> bytes: Read and return the first 100 bytes of the specified file. with open(filename, \'rb\') as f: return f.read(100)"},{"question":"**Objective**: Write a Python function using the seaborn package. You are required to create a custom color palette and use it to visualize a dataset meaningfully. **Task**: 1. Create a custom dark color palette using seaborn\'s `dark_palette` method. Use the color gold (`#FFD700`) as the base color. 2. Use this palette to create a swarm plot using seaborn. A swarm plot is useful for visualizing categorical data where the distribution of each category can be observed. 3. Load the \\"tips\\" dataset from seaborn\'s sample datasets. Visualize the total bill amounts categorized by day of the week using the custom palette created in step 1. 4. Your plot should: - Have meaningful axis labels and a title. - Use the custom color palette. - Be clear and well-styled. **Function Signature**: ```python def visualize_tips_with_custom_palette(): # Your code here ``` **Example Output**: The function should display a swarm plot with: - X-axis representing the days of the week. - Y-axis representing the total bill amounts. - Different colors from the custom palette for visualizing the distribution for each day. **Requirements**: - The Swarm plot should use the custom dark color palette based on `#FFD700` created using `sns.dark_palette`. - The function should be self-contained and not require any external files or data input. # Constraints: - Use seaborn (import as sns). - Ensure that your visualization is clear and properly annotated. **Performance Requirements**: - The function should execute efficiently without any unnecessary computations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_with_custom_palette(): # Create a custom dark palette custom_palette = sns.dark_palette(\\"#FFD700\\", reverse=True, as_cmap=False) # Load the \'tips\' dataset tips = sns.load_dataset(\\"tips\\") # Create the swarm plot plt.figure(figsize=(10, 6)) sns.swarmplot(x=\\"day\\", y=\\"total_bill\\", data=tips, palette=custom_palette) # Setting the labels and title plt.xlabel(\\"Day of the Week\\", fontsize=14) plt.ylabel(\\"Total Bill Amount ()\\", fontsize=14) plt.title(\\"Total Bill Amount by Day of the Week\\", fontsize=16) # Display the plot plt.show()"},{"question":"**Question:** Implement a Python function `process_numbers` that uses multi-threading to process a list of numbers concurrently. Use the low-level `_thread` module to manage threads and synchronize access to shared data using locks. **Function Signature:** ```python def process_numbers(numbers: list[int]) -> int: ``` **Description:** 1. **Input:** A list of integers (`numbers`), with no specific constraints on length or content. 2. **Output:** Return the sum of the processed numbers. 3. **Processing:** Each thread will: - Double the value of a number. - Update a shared sum variable with the new value. **Constraints:** - Use the `_thread` module for threading and lock management. - Ensure that the shared sum variable is updated in a thread-safe manner. - Each thread should process only one number from the list. - The number of threads should be equal to the length of the list. **Example:** ```python numbers = [1, 2, 3, 4, 5] result = process_numbers(numbers) # The processed numbers are [2, 4, 6, 8, 10] # The sum of the processed numbers is 30 print(result) # Output: 30 ``` **Notes:** - Use `_thread.start_new_thread` to create new threads. - Use `_thread.allocate_lock` to create a lock object. - Ensure that the shared sum variable is updated using the lock to avoid race conditions. **Implementation Details:** 1. Create a lock using `_thread.allocate_lock`. 2. Define a function `worker` that: - Takes a number from the list as an argument. - Doubles the number. - Acquires the lock before updating the shared sum. - Releases the lock after updating the shared sum. 3. Create and start a thread for each number in the list, passing the `worker` function and the respective number. 4. Wait for all threads to complete before returning the result.","solution":"import _thread import time def process_numbers(numbers: list[int]) -> int: # Shared variable for the sum shared_sum = [0] # Lock to synchronize access to shared_sum sum_lock = _thread.allocate_lock() def worker(num): double_num = num * 2 with sum_lock: shared_sum[0] += double_num # Start a thread for each number in the list threads = [] for num in numbers: thread = _thread.start_new_thread(worker, (num,)) threads.append(thread) # Wait for all threads to complete time.sleep(0.1) # Simple way to allow threads to complete execution (not ideal but works for this case) return shared_sum[0]"},{"question":"# Custom Module Importer **Objective:** Implement a custom module importer which dynamically imports a module, reloads it if necessary, and handles errors gracefully. **Requirements:** 1. **Function Definitions:** - Implement a function `custom_import_module(name: str) -> ModuleType` which imports a module by name. - Implement a function `custom_reload_module(module: ModuleType) -> ModuleType` which reloads an already imported module. - Use functionalities similar to `PyImport_ImportModule` and `PyImport_ReloadModule` provided in the given documentation. 2. **Error Handling:** - If the module cannot be imported, raise an `ImportError` with a helpful error message. - If the module cannot be reloaded, raise an `ImportError` indicating the reloading issue. 3. **Caching:** - Implement a cache mechanism to store and retrieve already imported modules. - Avoid re-importing a module if it has already been imported unless explicitly asked to reload. **Expected Input and Output:** - For `custom_import_module(name: str) -> ModuleType`: - **Input:** A module name as a string (e.g., `\\"os\\"`). - **Output:** A module object corresponding to the imported module. - For `custom_reload_module(module: ModuleType) -> ModuleType`: - **Input:** A module object. - **Output:** A reloaded module object. **Constraints:** - You cannot use Python\'s built-in `importlib.reload` directly in your implementation. - Use the provided import functions from the documentation or their equivalents in `python310`. **Performance:** - The import and reload functions should efficiently handle module importing and reduce overhead by using caching. **Example Usage:** ```python # Suppose `custom_import_module` and `custom_reload_module` are implemented. # Importing the \'os\' module os_module = custom_import_module(\'os\') # Using a function from the imported module print(os_module.listdir(\'.\')) # Reloading the \'os\' module os_module_reloaded = custom_reload_module(os_module) print(os_module_reloaded.listdir(\'.\')) ``` Implement the code for the above-defined functions that demonstrates the understanding of the `python310` package and its module import intricacies.","solution":"import sys from types import ModuleType module_cache = {} def custom_import_module(name: str) -> ModuleType: Dynamically imports a module by name. if name in module_cache: return module_cache[name] try: module = __import__(name) module_cache[name] = module return module except ImportError as e: raise ImportError(f\\"Could not import module \'{name}\'. {str(e)}\\") def custom_reload_module(module: ModuleType) -> ModuleType: Reloads an already imported module. try: name = module.__name__ if name not in module_cache: raise ImportError(f\\"Module \'{name}\' not found in cache. Cannot reload.\\") # Remove the module from sys.modules to force re-importing it if name in sys.modules: del sys.modules[name] # Re-import the module and update the cache reloaded_module = __import__(name) module_cache[name] = reloaded_module return reloaded_module except ImportError as e: raise ImportError(f\\"Could not reload module \'{module.__name__}\'. {str(e)}\\")"},{"question":"**Question: Implement a Randomized Selection and Shuffling System** You are tasked with writing a program that simulates a simple lottery system using Python\'s `random` module. The system should have the following functionalities: 1. **Random Item Selection**: Randomly select a given number of items from a list without replacement. 2. **Random List Shuffling**: Shuffle a list in-place. 3. **Random Integer Generation**: Generate a specified number of random integers within a given range. 4. **Custom Random Generator Subclass**: Implement a custom random generator subclass that randomly selects floating-point numbers or integers using a defined seed, allowing reproducing the same random sequence on re-seeding. **Function Signatures and Requirements:** 1. `def random_selection(items: list, k: int) -> list:` - **Input**: - `items` (list): A list of items from which to randomly select. - `k` (int): The number of items to randomly select. - **Output**: - A list of `k` randomly selected items from the input list. - **Constraints**: - If `k` is larger than the length of `items`, it should raise a `ValueError`. 2. `def shuffle_list(items: list) -> None:` - **Input**: - `items` (list): A list of items to be shuffled. - **Output**: - The function should shuffle the list in place without returning any value. 3. `def generate_random_integers(n: int, start: int, end: int) -> list:` - **Input**: - `n` (int): The number of random integers to generate. - `start` (int): The start of the range (inclusive). - `end` (int): The end of the range (inclusive). - **Output**: - A list of `n` randomly generated integers within the specified range. - **Constraints**: - Ensure the function handles cases where `start` is greater than `end` appropriately. 4. `class CustomRandom(random.Random):` - Implement a custom subclass of `random.Random` that: - Initializes with a seed. - Has methods to generate random floating-point numbers and integers while ensuring reproducibility when the same seed is used. **Example Usage:** ```python # Example inputs items = [\'apple\', \'banana\', \'cherry\', \'date\', \'elderberry\'] k = 3 n = 5 start, end = 10, 50 # Random selection from the list print(random_selection(items, k)) # Example Output: [\'cherry\', \'apple\', \'banana\'] # Shuffle the list shuffle_list(items) print(items) # Example Output: [\'elderberry\', \'date\', \'apple\', \'banana\', \'cherry\'] # Generate random integers print(generate_random_integers(n, start, end)) # Example Output: [15, 23, 41, 10, 37] # Custom random generator using a seed cr = CustomRandom(seed=42) print(cr.random()) # Example Output: 0.6394267984578837 print(cr.randint(10, 50)) # Example Output: 38 ``` Ensure that the solution encapsulates error handling, edge cases, and adheres to the signatures and constraints specified.","solution":"import random def random_selection(items: list, k: int) -> list: if k > len(items): raise ValueError(\\"k should not be larger than the number of items in the list.\\") return random.sample(items, k) def shuffle_list(items: list) -> None: random.shuffle(items) def generate_random_integers(n: int, start: int, end: int) -> list: if start > end: raise ValueError(\\"Start should not be greater than end.\\") return [random.randint(start, end) for _ in range(n)] class CustomRandom(random.Random): def __init__(self, seed=None): super().__init__(seed) def random_float(self): return super().random() def random_int(self, start, end): return self.randint(start, end)"},{"question":"# Advanced Compression Task with LZMA Objective You are required to implement a function that compresses a given text file using a custom filter chain and then decompresses it to verify that the original content is restored. This will demonstrate your understanding of both compression and decompression using the `lzma` module. Function Signature ```python def compress_and_verify(input_filepath: str, output_filepath: str, filters: list) -> bool: Compress a file\'s content using the specified filter chain and verify its integrity by decompressing it. Parameters: input_filepath (str): Path to the input text file to be compressed. output_filepath (str): Path where the compressed file will be stored. filters (list): List of dictionaries specifying the custom filter chain for compression. Returns: bool: True if the decompressed content matches the original content, False otherwise. ``` Input - `input_filepath`: a string representing the path to the input text file to be compressed. - `output_filepath`: a string representing the path where the compressed file will be saved. - `filters`: a list of dictionaries, each containing the filter configurations. Output - The function returns `True` if the decompressed content matches the original content of the file, otherwise returns `False`. Constraints - Assume the input file is a text file encoded in UTF-8. - Handle any LZMA-specific exceptions gracefully and print a relevant error message if an exception occurs. Example Here\'s an example filter configuration for you to test with. You can also create different valid configurations. ```python filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 1}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 6}, ] ``` Example Usage ```python # Assume the file \\"sample.txt\\" contains the text \\"Hello world!\\" filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 1}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 6}, ] result = compress_and_verify(\\"sample.txt\\", \\"sample.xz\\", filters) print(result) # Should print True if the decompressed content matches the original content ``` Notes - Use `lzma.open()` to handle file reading and writing. - The filter chain must be applied during compression. - Ensure you read and write files in the correct modes to avoid issues with binary/text data handling. Happy coding!","solution":"import lzma def compress_and_verify(input_filepath: str, output_filepath: str, filters: list) -> bool: try: # Read the original content with open(input_filepath, \'rb\') as infile: original_content = infile.read() # Compress the content using the specified filters with lzma.open(output_filepath, \'wb\', format=lzma.FORMAT_XZ, filters=filters) as outfile: outfile.write(original_content) # Decompress the content back with lzma.open(output_filepath, \'rb\') as infile: decompressed_content = infile.read() # Verify if the decompressed content matches the original content return original_content == decompressed_content except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"# System Information Aggregator **Objective:** Write a Python function `aggregate_system_info()` that uses the `platform` module to gather and return comprehensive system information, including details about the architecture, machine, network, Python build, and OS-specific information. The function should return the information as a dictionary. **Function Signature:** ```python def aggregate_system_info() -> dict: pass ``` **Expected Input and Output:** - The function does not take any parameters. - The function returns a dictionary with the following keys and corresponding values: - `\\"architecture\\"`: A tuple containing the bit architecture and linkage format. - `\\"machine\\"`: The machine type. - `\\"node\\"`: The computer\'s network name. - `\\"platform\\"`: A string identifying the underlying platform. - `\\"processor\\"`: The real processor name. - `\\"python_build\\"`: A tuple stating the Python build number and date. - `\\"python_compiler\\"`: A string identifying the compiler used for compiling Python. - `\\"python_implementation\\"`: A string identifying the Python implementation. - `\\"python_version\\"`: The Python version as string. - `\\"system\\"`: The system/OS name. - `\\"release\\"`: The system\'s release. - `\\"version\\"`: The system\'s release version. - `\\"uname\\"`: A namedtuple containing six attributes  `\\"system\\"`, `\\"node\\"`, `\\"release\\"`, `\\"version\\"`, `\\"machine\\"`, and `\\"processor\\"`. **Constraints:** - If any of the values cannot be determined, use an empty string or a default value as appropriate. - You may assume the `platform` module is available and does not need to be installed. **Example:** Given that the function implementation correctly uses the `platform` module functions: ```python def aggregate_system_info() -> dict: import platform info = { \\"architecture\\": platform.architecture(), \\"machine\\": platform.machine(), \\"node\\": platform.node(), \\"platform\\": platform.platform(), \\"processor\\": platform.processor(), \\"python_build\\": platform.python_build(), \\"python_compiler\\": platform.python_compiler(), \\"python_implementation\\": platform.python_implementation(), \\"python_version\\": platform.python_version(), \\"system\\": platform.system(), \\"release\\": platform.release(), \\"version\\": platform.version(), \\"uname\\": platform.uname() } return info ``` **Performance Requirements:** - The function should execute efficiently and gather all information within a reasonable time for regular usage in a script or application. Test your function thoroughly to ensure all fields are populated correctly based on different underlying systems.","solution":"def aggregate_system_info() -> dict: import platform info = { \\"architecture\\": platform.architecture(), \\"machine\\": platform.machine(), \\"node\\": platform.node(), \\"platform\\": platform.platform(), \\"processor\\": platform.processor(), \\"python_build\\": platform.python_build(), \\"python_compiler\\": platform.python_compiler(), \\"python_implementation\\": platform.python_implementation(), \\"python_version\\": platform.python_version(), \\"system\\": platform.system(), \\"release\\": platform.release(), \\"version\\": platform.version(), \\"uname\\": platform.uname() } return info"},{"question":"Objective: You are required to implement a function that generates a list of random tensors and demonstrates an understanding of setting random seeds for reproducibility. Additionally, you will need to compute the sum of the elements of these tensors and return the result. Function Signature: ```python import torch def generate_random_tensors_and_sum(num_tensors: int, tensor_shape: tuple, seed: int) -> torch.Tensor: Generates a specified number of tensors with a given shape using a provided random seed and returns their element-wise sum. Parameters: num_tensors (int): Number of random tensors to generate. tensor_shape (tuple): The shape of each random tensor. seed (int): The seed value used for random number generation. Returns: torch.Tensor: The sum of the generated tensors. pass ``` Input: * `num_tensors`: An integer (1  `num_tensors`  100) representing the number of tensors to generate. * `tensor_shape`: A tuple of integers (1  `tensor_shape[i]`  50) representing the shape of each tensor. * `seed`: An integer used to set the random seed for reproducibility. Output: * Returns a tensor that is the element-wise sum of all generated random tensors. Constraints: * The tensors will contain floating-point numbers drawn from a uniform distribution on the interval [0, 1). Example: ```python # Example usage result = generate_random_tensors_and_sum(3, (2, 2), 42) # The output tensor should be the element-wise sum of 3 tensors of shape (2, 2) generated with seed 42 print(result) # Output: # tensor([[1.3686, 1.3285], # [0.7576, 1.0697]]) ``` Explanation: In the above example, three random tensors with shape (2, 2) were generated using a seed value of 42. The function summed these tensors element-wise and returned the resulting tensor. Additional Requirements: 1. Ensure that the function sets the random seed using the provided `seed` parameter to guarantee reproducibility. 2. Optimize for performance where possible, especially for larger `num_tensors` and `tensor_shape`.","solution":"import torch def generate_random_tensors_and_sum(num_tensors: int, tensor_shape: tuple, seed: int) -> torch.Tensor: Generates a specified number of tensors with a given shape using a provided random seed and returns their element-wise sum. Parameters: num_tensors (int): Number of random tensors to generate. tensor_shape (tuple): The shape of each random tensor. seed (int): The seed value used for random number generation. Returns: torch.Tensor: The sum of the generated tensors. # Set the random seed for reproducibility torch.manual_seed(seed) # Initialize the sum tensor with zeros of the same shape sum_tensor = torch.zeros(tensor_shape) # Generate the random tensors and accumulate their sum for _ in range(num_tensors): random_tensor = torch.rand(tensor_shape) sum_tensor += random_tensor return sum_tensor"},{"question":"# Question: Numerical Stability and Precision in PyTorch **Objective**: Implement a PyTorch function to perform batched matrix multiplication and analyze the numerical stability of the results across CPU and GPU. Problem Statement: Write a function `batched_matmul_stability` that: 1. Accepts two 3D tensors `A` and `B` suitable for batched matrix multiplication (with shapes `[batch_size, m, k]` and `[batch_size, k, n]`, respectively). 2. Performs batched matrix multiplication on the CPU and on the GPU. 3. Compares the results obtained on the CPU with those on the GPU for bitwise identity and numerical difference. The function should also check and report the following: - Whether the results are bitwise identical. - The maximum absolute difference between corresponding elements of the CPU and GPU results. Input: - `A` (torch.Tensor): A 3D tensor of shape `[batch_size, m, k]`. - `B` (torch.Tensor): A 3D tensor of shape `[batch_size, k, n]`. Output: - `bitwise_identical` (bool): `True` if the CPU and GPU results are bitwise identical, `False` otherwise. - `max_absolute_difference` (float): The maximum absolute difference between corresponding elements of the CPU and GPU results. Constraints: - Ensure that the function handles the cases where GPU is not available properly. - Use double precision (`torch.float64`) for calculations to minimize floating-point errors. ```python import torch def batched_matmul_stability(A: torch.Tensor, B: torch.Tensor): if not torch.cuda.is_available(): raise RuntimeError(\\"GPU is not available.\\") # Ensure inputs are double precision A = A.to(torch.float64) B = B.to(torch.float64) # Compute on CPU result_cpu = torch.bmm(A, B) # Compute on GPU A_gpu = A.cuda() B_gpu = B.cuda() result_gpu = torch.bmm(A_gpu, B_gpu).cpu() # Bring result back to CPU for comparison # Check bitwise identity bitwise_identical = torch.equal(result_cpu, result_gpu) # Calculate maximum absolute difference max_absolute_difference = torch.max(torch.abs(result_cpu - result_gpu)).item() return bitwise_identical, max_absolute_difference ``` Example: ```python A = torch.randn(10, 20, 30, dtype=torch.float64) B = torch.randn(10, 30, 40, dtype=torch.float64) bitwise_identical, max_absolute_difference = batched_matmul_stability(A, B) print(f\\"Bitwise Identical: {bitwise_identical}\\") print(f\\"Maximum Absolute Difference: {max_absolute_difference}\\") ``` In this problem, students are expected to understand the subtle differences in numerical computations across different devices and configurations. They will implement a function that highlights these differences, thus demonstrating their comprehension of PyTorch\'s behavior in terms of numerical stability and precision.","solution":"import torch def batched_matmul_stability(A: torch.Tensor, B: torch.Tensor): if not torch.cuda.is_available(): raise RuntimeError(\\"GPU is not available.\\") # Ensure inputs are double precision A = A.to(torch.float64) B = B.to(torch.float64) # Compute on CPU result_cpu = torch.bmm(A, B) # Compute on GPU A_gpu = A.cuda() B_gpu = B.cuda() result_gpu = torch.bmm(A_gpu, B_gpu).cpu() # Bring result back to CPU for comparison # Check bitwise identity bitwise_identical = torch.equal(result_cpu, result_gpu) # Calculate maximum absolute difference max_absolute_difference = torch.max(torch.abs(result_cpu - result_gpu)).item() return bitwise_identical, max_absolute_difference"},{"question":"Question You are tasked with writing a Python script that demonstrates the customization of the interactive Python shell\'s behavior by using the `PYTHONSTARTUP` environment variable and a custom startup file. # Task 1. **Create a Custom Startup File**: - Write a Python script named `startup.py` that defines a custom welcome message and sets the interactive mode prompt to a non-default value. - The script should: - Print \\"Welcome to the Customized Python Shell!\\" when the interactive shell starts. - Change the primary prompt (usually `>>>`) to `CustomPrompt> `. Example content for `startup.py`: ```python import sys print(\\"Welcome to the Customized Python Shell!\\") sys.ps1 = \'CustomPrompt> \' ``` 2. **Environment Variable Configuration**: - Create another Python script named `configure_startup.py` that sets the `PYTHONSTARTUP` environment variable to point to the `startup.py` file. This script should: - Determine the absolute path to `startup.py`. - Set the `PYTHONSTARTUP` environment variable to this path. - Start an interactive Python shell to demonstrate the effect of the custom startup file. Example content for `configure_startup.py`: ```python import os import subprocess # Determine the absolute path to startup.py startup_file_path = os.path.abspath(\'startup.py\') # Set the PYTHONSTARTUP environment variable os.environ[\'PYTHONSTARTUP\'] = startup_file_path print(f\\"PYTHONSTARTUP set to {startup_file_path}\\") # Start an interactive Python shell subprocess.call([\'python3.10\', \'-i\']) ``` # Constraints - The `startup.py` script should only contain commands that customize the interactive session and should not perform any other actions. - `configure_startup.py` should ensure that the `PYTHONSTARTUP` environment variable is set correctly before starting the interactive shell. # Expected Outcome 1. When running `configure_startup.py`, the output should include: - Confirmation that `PYTHONSTARTUP` is set to the correct path. - Launch of an interactive Python shell. - Display of the custom welcome message. - Appearance of the custom prompt `CustomPrompt> `. 2. Example interaction: ``` python3.10 configure_startup.py PYTHONSTARTUP set to /path/to/startup.py Welcome to the Customized Python Shell! CustomPrompt> ``` This question tests the ability to handle file operations, environment variable settings, and script execution in Python, demonstrating an understanding of Python\'s interactive mode customization capabilities.","solution":"# startup.py import sys print(\\"Welcome to the Customized Python Shell!\\") sys.ps1 = \'CustomPrompt> \' # configure_startup.py import os import subprocess # Determine the absolute path to startup.py startup_file_path = os.path.abspath(\'startup.py\') # Set the PYTHONSTARTUP environment variable os.environ[\'PYTHONSTARTUP\'] = startup_file_path print(f\\"PYTHONSTARTUP set to {startup_file_path}\\") # Start an interactive Python shell subprocess.call([\'python3.10\', \'-i\'])"},{"question":"# Custom Context Manager Implementation **Objective**: Implement a custom context manager to manage the timed execution of a block of code. **Description**: You are required to design a context manager named `Timer` using the `contextlib` module. This context manager should measure the time taken to execute a block of code within the `with` statement. The elapsed time should be stored in an attribute that can be accessed after the `with` block. **Requirements**: 1. Implement a class `Timer` that: - Utilizes the `@contextlib.contextmanager` decorator. - Records the start time when entering the context. - Records the end time when exiting the context. - Computes the elapsed time and stores it in an attribute `elapsed_time`. 2. The `elapsed_time` should be retrieved as a floating-point number of seconds. **Constraints**: - You can use the `time` module for tracking time. **Input/Output**: - **Input**: The code block within the `with Timer()` statement (implicitly indicates no direct input). - **Output**: The attribute `elapsed_time` should store the elapsed time after the `with` block execution. # Example ```python import time class Timer: # Your implementation here # Example usage with Timer() as t: time.sleep(2) print(t.elapsed_time) # Should output approximately 2.0 ``` # Notes: - Ensure the context manager properly handles exceptions (i.e., even if an exception occurs, the elapsed time should be correctly recorded). Happy coding!","solution":"import time from contextlib import contextmanager class Timer: def __init__(self): self.elapsed_time = None def __enter__(self): self.start_time = time.time() return self def __exit__(self, exc_type, exc_value, traceback): self.end_time = time.time() self.elapsed_time = self.end_time - self.start_time"},{"question":"You are tasked with implementing a distributed system using the PyTorch distributed RPC framework. Your system needs to coordinate computations across multiple workers while correctly managing remote references (RRef) to avoid premature deletion of shared objects. # Instructions 1. Set up a distributed environment with at least three workers: \\"worker0\\", \\"worker1\\", and \\"worker2\\". 2. Implement the following functions: - `initialize_workers()`: This function will initialize the distributed RPC framework on the given workers. - `distribute_computation()`: This function will be responsible for: - Creating an RRef on \\"worker0\\" that holds a tensor. - Passing this RRef from \\"worker0\\" to \\"worker1\\" for a computation. - Passing the resulting RRef from \\"worker1\\" to \\"worker2\\" for further processing. - Ensuring that each worker correctly notifies the owner worker about the reference count changes. - `final_result()`: This function will retrieve the final result from the RRef on \\"worker2\\" and return it. # Constraints - All computations should be simple tensor operations (e.g., addition, multiplication). - Ensure that all RPC calls handle potential transient network failures gracefully. - Validate that the final retrieved tensor is correct based on the input tensor and the operations performed. # Example Execution ```python def main(): # Initialize the workers initialize_workers() # Distribute computation result = distribute_computation() print(\\"Final result:\\", result) if __name__ == \\"__main__\\": main() ``` # Expected Output When invoking `main()`, the program should correctly print the final result of the distributed tensor computations, demonstrating proper RRef management. # Performance Requirements - The solution should handle the distributed reference counting correctly, ensuring no premature deletions. - Ensure that all RRefs are properly acknowledged and that the owner tracks the reference count accurately. Good luck!","solution":"import torch import torch.distributed.rpc as rpc from torch.distributed.rpc import RRef import torch.multiprocessing as mp def initialize_workers(rank, world_size): This function initializes the RPC framework on each worker. options = rpc.TensorPipeRpcBackendOptions(num_worker_threads=16) rpc.init_rpc(f\\"worker{rank}\\", rank=rank, world_size=world_size, rpc_backend_options=options) def worker0_tensor(): Creates a tensor on worker0. return torch.tensor([1, 2, 3]) def worker1_addition(rref): Performs an addition operation on the tensor held by rref on worker1. tensor = rref.to_here() return RRef(tensor + 10) def worker2_multiplication(rref): Performs a multiplication operation on the tensor held by rref on worker2. tensor = rref.to_here() return tensor * 2 def distribute_computation(): Distribute computation across the workers by passing RRefs and operating on them. # Step 1: Create RRef on worker0 rref = rpc.remote(\\"worker0\\", worker0_tensor) # Step 2: Pass the RRef from worker0 to worker1 for computation rref = rpc.remote(\\"worker1\\", worker1_addition, args=(rref,)) # Step 3: Further process the RRef from worker1 on worker2 rref = rpc.rpc_sync(\\"worker2\\", worker2_multiplication, args=(rref,)) return rref def final_result(): Retrieve the final result from the RRef on worker2 and return it. # Assuming the final tensor is supposed to be retrieved from worker2 result_rref = distribute_computation() result = result_rref.to_here() return result def main(rank, world_size): # Initialize the workers initialize_workers(rank, world_size) # Only execute the main logic on one of the workers, e.g., worker0 if rank == 0: final_tensor = final_result() print(\\"Final result:\\", final_tensor) # Shutdown the RPC framework rpc.shutdown() if __name__ == \\"__main__\\": world_size = 3 mp.spawn(main, args=(world_size,), nprocs=world_size, join=True)"},{"question":"You are implementing functionality for an old email client handling system that uses mailcap files for MIME-type associations. The system should help in determining the appropriate command to execute based on specific MIME types by reading configurations from mailcap files. Your task is to implement a function using Python\'s `mailcap` module to find matches for given MIME types and handle test cases accounting for parameters and security restrictions. **Function Signature:** ```python def find_mime_handler(mime_type: str, filename: str = \'/dev/null\', params: list = []) -> tuple: Finds appropriate command and mailcap entry for a given MIME type. Parameters: mime_type (str): The MIME type to search for. filename (str): The filename to be replaced in the command line (default \'/dev/null\'). params (list): List of parameters in the form [\'param_name=value\', ...] (default empty list). Returns: tuple: A 2-tuple where first element is the command line string and second element is the mailcap entry. If no match is found or the provided inputs include disallowed characters, returns (None, None). ``` **Constraints:** 1. You must use the `mailcap` functions `getcaps` and `findmatch`. 2. The function should ensure it prevents unsafe characters in the filename and parameter values. **Input:** - `mime_type` (str): String representing the MIME type. - `filename` (str): Optional string of a filename (defaults to \'/dev/null\'). - `params` (list): Optional list of strings representing parameters (defaults to empty list). **Output:** - A tuple `(command_line, mailcap_entry)` or `(None, None)` if no valid match is found. **Examples:** ```python # Example usage: cmd, entry = find_mime_handler(\'video/mpeg\', \'video_file.mpg\', [\'quality=high\']) # Possible output: # (\'xmpeg video_file.mpg\', {\'view\': \'xmpeg %s\', \'quality\': \'high\'}) ``` # Additional Notes: - Use `mailcap.getcaps()` to fetch the initial capabilities (entries from mailcap files). - Make sure to handle cases where provided filenames or parameter values contain restricted characters which could pose security risks. - Consider edge cases where the MIME type does not exist in any mailcap file. # Helper Code: ```python import mailcap def find_mime_handler(mime_type: str, filename: str = \'/dev/null\', params: list = []) -> tuple: caps = mailcap.getcaps() return mailcap.findmatch(caps, mime_type, filename=filename, plist=params) ``` Ensure to thoroughly test your implementation with varying inputs to validate robustness.","solution":"import mailcap import re def find_mime_handler(mime_type: str, filename: str = \'/dev/null\', params: list = []) -> tuple: Finds appropriate command and mailcap entry for a given MIME type. Parameters: mime_type (str): The MIME type to search for. filename (str): The filename to be replaced in the command line (default \'/dev/null\'). params (list): List of parameters in the form [\'param_name=value\', ...] (default empty list). Returns: tuple: A 2-tuple where first element is the command line string and second element is the mailcap entry. If no match is found or the provided inputs include disallowed characters, returns (None, None). # Regular expression to detect unsafe characters unsafe_char_pattern = re.compile(r\'[^a-zA-Z0-9._-]\') # Check for unsafe characters in filename and params if unsafe_char_pattern.search(filename): return (None, None) for param in params: if unsafe_char_pattern.search(param): return (None, None) # Get capabilities using mailcap caps = mailcap.getcaps() # Find and return match using mailcap module functions return mailcap.findmatch(caps, mime_type, filename=filename, plist=params)"},{"question":"# Question: PyTorch Script Mode Compatibility # Objective To assess your understanding of PyTorch\'s TorchScript and its compatibility with Python constructs. # Problem Statement You are required to write a PyTorch script that demonstrates the creation of a simple neural network model using TorchScript. The neural network should be a simple feed-forward network with one hidden layer. The following requirements must be met: 1. **Model Definition**: - Define a class `SimpleFFNN` which inherits from `torch.nn.Module`. - The class should have an `__init__` method to initialize the layers. - It should have a `forward` method that defines the forward pass of the network. 2. **TorchScript Conversion**: - Convert the model to a TorchScript using `torch.jit.script`. 3. **Compatibility Constraints**: - Ensure that the model adheres to the TorchScript supported features as documented. - Avoid using any unsupported or partially supported features in TorchScript like asynchronous operations, certain bitwise operations, and unsupported magic methods. # Input and Output Formats Implement the following: ```python import torch import torch.nn as nn class SimpleFFNN(torch.nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleFFNN, self).__init__() # Initialize layers self.hidden = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.output = nn.Linear(hidden_size, output_size) def forward(self, x): # Define forward pass x = self.hidden(x) x = self.relu(x) x = self.output(x) return x # Sample code to convert to TorchScript input_size = 3 hidden_size = 5 output_size = 2 model = SimpleFFNN(input_size, hidden_size, output_size) # Create a sample input tensor input_tensor = torch.randn(1, input_size) # Convert model to TorchScript scripted_model = torch.jit.script(model) # Output serialized model print(scripted_model) ``` **Constraints**: - You must adhere to the subset of Python features supported by TorchScript as described in the documentation. - Ensure the code does not use features marked as not supported or partially supported without the necessary handling. # Notes 1. You may assume appropriate environment setup with PyTorch installed. 2. Focus on ensuring compatibility with TorchScript which might involve trial and error in checking supported features.","solution":"import torch import torch.nn as nn class SimpleFFNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleFFNN, self).__init__() # Initialize layers self.hidden = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.output = nn.Linear(hidden_size, output_size) def forward(self, x): # Define forward pass x = self.hidden(x) x = self.relu(x) x = self.output(x) return x # Sample code to convert to TorchScript input_size = 3 hidden_size = 5 output_size = 2 model = SimpleFFNN(input_size, hidden_size, output_size) # Create a sample input tensor input_tensor = torch.randn(1, input_size) # Convert model to TorchScript scripted_model = torch.jit.script(model) # Output serialized model for debugging purposes print(scripted_model)"},{"question":"Objective You are tasked with creating a set of functions that analyze strings based on Python keywords and soft keywords using the `keyword` module. Problem Statement Implement a class `KeywordAnalyzer` with the following methods: 1. `__init__(self)`: The constructor initializes two attributes: - `keywords`: A list containing all Python keywords from `keyword.kwlist`. - `soft_keywords`: A list containing all Python soft keywords from `keyword.softkwlist`. 2. `is_keyword(self, s: str) -> bool`: This method takes a string `s` and returns `True` if `s` is identified as a Python keyword, else `False`. 3. `is_soft_keyword(self, s: str) -> bool`: This method takes a string `s` and returns `True` if `s` is identified as a Python soft keyword, else `False`. 4. `keyword_statistics(self, words: list) -> dict`: This method takes a list of strings `words` and returns a dictionary with the count of each keyword and soft keyword found in the list. - The dictionary should have two keys: `\\"keywords\\"` and `\\"soft_keywords\\"`, each associated with a dictionary where keys are the keywords/soft keywords and values are their respective counts in the `words` list. 5. `common_keywords(self, other: \'KeywordAnalyzer\') -> list`: This method takes another `KeywordAnalyzer` instance and returns a list of common keywords between the two instances. Input and Output Formatting - `is_keyword(self, s: str) -> bool` - **Input**: A string `s`. - **Output**: A boolean value. - `is_soft_keyword(self, s: str) -> bool` - **Input**: A string `s`. - **Output**: A boolean value. - `keyword_statistics(self, words: list) -> dict` - **Input**: A list of strings `words`. - **Output**: A dictionary with the structure `{\\"keywords\\": {...}, \\"soft_keywords\\": {...}}`. - `common_keywords(self, other: \'KeywordAnalyzer\') -> list` - **Input**: Another `KeywordAnalyzer` instance `other`. - **Output**: A list of keywords that are common between the two instances. Constraints - All strings in the `words` list will be lowercase alphabets. - List `words` will have a length of at most 1000. - Each string in `words` will have a length of at most 20 characters. Example ```python from keyword import kwlist, softkwlist class KeywordAnalyzer: def __init__(self): self.keywords = kwlist self.soft_keywords = softkwlist def is_keyword(self, s: str) -> bool: return s in self.keywords def is_soft_keyword(self, s: str) -> bool: return s in self.soft_keywords def keyword_statistics(self, words: list) -> dict: keyword_count = {kw: 0 for kw in self.keywords} soft_keyword_count = {skw: 0 for skw in self.soft_keywords} for word in words: if word in keyword_count: keyword_count[word] += 1 if word in soft_keyword_count: soft_keyword_count[word] += 1 return {\\"keywords\\": {k: v for k, v in keyword_count.items() if v > 0}, \\"soft_keywords\\": {k: v for k, v in soft_keyword_count.items() if v > 0}} def common_keywords(self, other: \'KeywordAnalyzer\') -> list: return list(set(self.keywords) & set(other.keywords)) ``` To test: ```python analyzer1 = KeywordAnalyzer() analyzer2 = KeywordAnalyzer() print(analyzer1.is_keyword(\'def\')) # True print(analyzer1.is_soft_keyword(\'match\')) # True words = [\'def\', \'class\', \'match\', \'case\', \'def\'] print(analyzer1.keyword_statistics(words)) # {\'keywords\': {\'def\': 2, \'class\': 1}, \'soft_keywords\': {\'match\': 1, \'case\': 1}} print(analyzer1.common_keywords(analyzer2)) # Full kwlist ```","solution":"import keyword class KeywordAnalyzer: def __init__(self): self.keywords = keyword.kwlist self.soft_keywords = keyword.softkwlist def is_keyword(self, s: str) -> bool: return s in self.keywords def is_soft_keyword(self, s: str) -> bool: return s in self.soft_keywords def keyword_statistics(self, words: list) -> dict: keyword_count = {kw: 0 for kw in self.keywords} soft_keyword_count = {skw: 0 for skw in self.soft_keywords} for word in words: if word in keyword_count: keyword_count[word] += 1 if word in soft_keyword_count: soft_keyword_count[word] += 1 return { \\"keywords\\": {k: v for k, v in keyword_count.items() if v > 0}, \\"soft_keywords\\": {k: v for k, v in soft_keyword_count.items() if v > 0} } def common_keywords(self, other: \'KeywordAnalyzer\') -> list: return list(set(self.keywords) & set(other.keywords))"},{"question":"# Problem: Analyzing and Transforming Sales Data with Pandas Background: You are provided with a sales dataset containing the monthly sales records for various products in different regions for the last two years. The data is stored in a CSV file named `sales_data.csv` with the following columns: - `date`: The month and year of the record (e.g., \\"2022-01\\"). - `region`: The geographical region of the sales (e.g., \\"North\\", \\"South\\"). - `product`: The name of the product (e.g., \\"Product A\\"). - `sales`: The number of units sold. Objectives: You are required to write a function that reads this dataset, performs various analyses and data transformations, and outputs the results in a specific format. Function Signature: ```python import pandas as pd def analyze_sales_data(file_path: str) -> dict: # Implement your solution here pass ``` Function Description: 1. **Read the CSV file** into a pandas DataFrame. 2. **Calculate the total sales** for each product across all regions and return a Series sorted by total sales in descending order. 3. **Calculate the average monthly sales** for each region. 4. **Find the month with the highest sales** for each product. 5. **Transform and normalize the sales data**: - Compute the z-score for the sales values for each product across all months and regions using the formula: ( z = frac{(x - mu)}{sigma} ), where ( mu ) is the mean and ( sigma ) is the standard deviation of the sales values for that product. 6. **Return a dictionary** containing the results of the analyses and transformations: ```python { \\"total_sales\\": pd.Series, # Total sales for each product sorted in descending order \\"average_monthly_sales\\": pd.DataFrame, # Average monthly sales for each region \\"highest_sales_month\\": pd.Series, # Month with the highest sales for each product \\"normalized_sales\\": pd.DataFrame # DataFrame with z-score normalized sales values } ``` Constraints: - The function should handle any potential missing values in the data by filling them with the average sales value for the respective product. - Ensure the resulting data types are consistent and appropriate for each analysis. - Your code should be efficient and readable, utilizing pandas\' built-in functions wherever possible. Example: Assume the `sales_data.csv` contains the following data: ```csv date,region,product,sales 2022-01,North,Product_A,100 2022-01,South,Product_B,150 2022-02,North,Product_A,120 2022-02,South,Product_B,130 2022-03,North,Product_A,300 2022-03,South,Product_B,200 ``` Calling `analyze_sales_data(\'sales_data.csv\')` should return the following dictionary: ```python { \\"total_sales\\": pd.Series(data=[...], index=[...]), # Sorted Series of total sales \\"average_monthly_sales\\": pd.DataFrame(data=[...], columns=[\'region\', \'average_sales\']), \\"highest_sales_month\\": pd.Series(data=[...], index=[...]), # Month with highest sales for each product \\"normalized_sales\\": pd.DataFrame(data=[...], columns=[\'date\', \'region\', \'product\', \'z_score\']) } ```","solution":"import pandas as pd import numpy as np def analyze_sales_data(file_path: str) -> dict: # Read the CSV file into a pandas DataFrame df = pd.read_csv(file_path) # Fill missing sales values with the average sales value for the respective product df[\'sales\'].fillna(df.groupby(\'product\')[\'sales\'].transform(\'mean\'), inplace=True) # Calculate the total sales for each product total_sales = df.groupby(\'product\')[\'sales\'].sum().sort_values(ascending=False) # Calculate the average monthly sales for each region average_monthly_sales = df.groupby(\'region\')[\'sales\'].mean().reset_index().rename(columns={\'sales\': \'average_sales\'}) # Find the month with the highest sales for each product highest_sales_month = df.loc[df.groupby(\'product\')[\'sales\'].idxmax()].set_index(\'product\')[\'date\'] # Compute z-score for the sales values for each product df[\'z_score\'] = df.groupby(\'product\')[\'sales\'].transform(lambda x: (x - x.mean()) / x.std()) # Return the results in the specified format return { \\"total_sales\\": total_sales, \\"average_monthly_sales\\": average_monthly_sales, \\"highest_sales_month\\": highest_sales_month, \\"normalized_sales\\": df[[\'date\', \'region\', \'product\', \'z_score\']] }"},{"question":"# Custom Extension Type Implementation in Python Your task is to create a custom extension type in Python that encapsulates a simple data structure to manage a list of integers. You need to implement key methods and structures discussed in the documentation provided. Specifications: 1. **Type Name**: `IntList` 2. **Attributes**: - `data`: a list of integers. 3. **Methods**: - `add(int)`: Adds an integer to the list. - `remove(int)`: Removes an integer from the list. - `clear()`: Clears the entire list. Requirements: 1. **Initialization**: - Implement the constructor (`tp_new`) and initializer (`tp_init`) to initialize the `data` attribute. 2. **De-allocation**: - Implement the deallocator (`tp_dealloc`) to handle cleanup. 3. **String Representation**: - Implement `tp_repr` and `tp_str` to provide string representations of the `IntList` object. 4. **Attribute Management**: - Implement `tp_getattro` and `tp_setattro` to manage access to attributes. 5. **Method Definitions**: - Implement the methods (`add`, `remove`, `clear`) using the `tp_methods` table. 6. **Iterator Protocol**: - Implement the iterator protocol using `tp_iter` and `tp_iternext` to allow iteration over the integers in the list. Example Usage: ```python int_list = IntList() int_list.add(3) int_list.add(5) int_list.add(7) print(int_list) # Should print the list content, e.g., IntList([3, 5, 7]) int_list.remove(5) print(int_list) # Should print the updated list content, e.g., IntList([3, 7]) for num in int_list: print(num) # Should print each number in the list int_list.clear() print(int_list) # Should print an empty list, e.g., IntList([]) ``` Constraints: - Ensure the `__iter__` and `__next__` methods are appropriately defined to facilitate iteration over the list. - Handle exceptions appropriately, such as when trying to remove an element that does not exist in the list. - Provide appropriate error messages or behavior for incorrect operations. Ensure your implementation is efficient and follows the best practices as outlined in the provided documentation.","solution":"class IntList: def __init__(self): self.data = [] def __repr__(self): return f\\"IntList({self.data})\\" def __str__(self): return f\\"IntList({self.data})\\" def add(self, value): if isinstance(value, int): self.data.append(value) else: raise ValueError(\\"Only integers can be added to IntList\\") def remove(self, value): try: self.data.remove(value) except ValueError: raise ValueError(f\\"Value {value} not found in IntList\\") def clear(self): self.data = [] def __iter__(self): self._index = 0 return self def __next__(self): if self._index < len(self.data): result = self.data[self._index] self._index += 1 return result else: raise StopIteration"}]'),z={name:"App",components:{PoemCard:I},data(){return{searchQuery:"",visibleCount:4,poemsData:A,isLoading:!1}},computed:{filteredPoems(){const i=this.searchQuery.trim().toLowerCase();return i?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(i)||e.solution&&e.solution.toLowerCase().includes(i)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(i=>setTimeout(i,1e3)),this.visibleCount+=4,this.isLoading=!1}}},D={class:"search-container"},F={class:"card-container"},R={key:0,class:"empty-state"},N=["disabled"],q={key:0},L={key:1};function O(i,e,l,m,n,r){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"prompts chat")])],-1)),t("div",D,[e[3]||(e[3]=t("span",{class:"search-icon"},"",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=o=>n.searchQuery=o),placeholder:"Search..."},null,512),[[y,n.searchQuery]]),n.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=o=>n.searchQuery="")},"  ")):d("",!0)]),t("div",F,[(a(!0),s(b,null,v(r.displayedPoems,(o,f)=>(a(),w(h,{key:f,poem:o},null,8,["poem"]))),128)),r.displayedPoems.length===0?(a(),s("div",R,' No results found for "'+u(n.searchQuery)+'". ',1)):d("",!0)]),r.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:n.isLoading,onClick:e[2]||(e[2]=(...o)=>r.loadMore&&r.loadMore(...o))},[n.isLoading?(a(),s("span",L,"Loading...")):(a(),s("span",q,"See more"))],8,N)):d("",!0)])}const M=p(z,[["render",O],["__scopeId","data-v-e843e489"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"deepseek/66.md","filePath":"deepseek/66.md"}'),j={name:"deepseek/66.md"},B=Object.assign(j,{setup(i){return(e,l)=>(a(),s("div",null,[x(M)]))}});export{Y as __pageData,B as default};
