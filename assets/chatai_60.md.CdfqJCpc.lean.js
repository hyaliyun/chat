import{_ as p,o as a,c as i,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},P={class:"review-title"},S={class:"review-content"};function E(s,e,l,m,n,o){return a(),i("div",T,[t("div",C,[t("div",P,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",S,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const A=p(k,[["render",E],["__scopeId","data-v-b95a584f"]]),z=JSON.parse('[{"question":"You are tasked with implementing a script that processes a directory of text files containing numerical data. The script should perform the following steps: 1. **Search for all `.txt` files in a specified directory.** 2. **For each `.txt` file, read its content and compute statistical properties: mean, median, and variance.** 3. **Output the computed statistics to a new file named `summary.txt` in the same directory.** 4. **Optionally, print the top N lines from each file as an additional summary if a command-line argument specifying the number of lines is provided.** # Your task is to implement the following function: ```python import os import glob import statistics import argparse from typing import List def process_directory(directory: str, top_lines: int = None) -> None: Processes all .txt files in the specified directory, computes statistical properties for each file, and summarizes the results in \'summary.txt\'. Optionally prints the top N lines from each file. Args: directory (str): The directory to search for .txt files. top_lines (int, optional): The number of top lines to print from each file. Defaults to None. Returns: None # Your implementation here ``` # Requirements: - The function should: - Search for `.txt` files in the given directory using the `glob` module. - Read each `.txt` file and assume each line contains a single floating-point number. - Compute the mean, median, and variance for the numbers in each file using the `statistics` module. - Write a summary of computed statistics to a file named `summary.txt` in the given directory. - If the `top_lines` argument is provided, print the specified number of top lines from each file. # Input: - `directory`: A string specifying the directory to process. - `top_lines`: An optional integer specifying the number of top lines to print from each file (using the command-line argument `--lines`). # Output: - A file named `summary.txt` in the given directory containing the mean, median, and variance of the numerical data from each `.txt` file. - Optionally, prints the top N lines from each file to the console. # Example Usage: ```shell python script.py /path/to/directory --lines=5 ``` This command will process all `.txt` files in `/path/to/directory`, compute the required statistics, write them to `summary.txt`, and print the top 5 lines from each file. If the `--lines` argument is omitted, it will only compute and write the statistics without printing any lines. # Constraints: - The directory will contain only `.txt` files, each formatted correctly with one floating-point number per line. - You may assume the directory and files exist and are accessible. # Hints: - Use the `glob` module to list `.txt` files in the directory. - Use the `statistics` module to compute the mean, median, and variance. - Use the `argparse` module to handle optional command-line arguments for the number of lines to print.","solution":"import os import glob import statistics import argparse from typing import List def process_directory(directory: str, top_lines: int = None) -> None: Processes all .txt files in the specified directory, computes statistical properties for each file, and summarizes the results in \'summary.txt\'. Optionally prints the top N lines from each file. Args: directory (str): The directory to search for .txt files. top_lines (int, optional): The number of top lines to print from each file. Defaults to None. Returns: None txt_files = glob.glob(os.path.join(directory, \\"*.txt\\")) summary_data = [] for txt_file in txt_files: with open(txt_file, \'r\') as file: lines = file.readlines() numbers = [float(line.strip()) for line in lines] mean = statistics.mean(numbers) median = statistics.median(numbers) variance = statistics.variance(numbers) if len(numbers) > 1 else 0.0 summary_data.append(f\\"File: {os.path.basename(txt_file)}\\") summary_data.append(f\\"Mean: {mean:.2f}\\") summary_data.append(f\\"Median: {median:.2f}\\") summary_data.append(f\\"Variance: {variance:.2f}\\") summary_data.append(\\"n\\") if top_lines: print(f\\"Top {top_lines} lines from {os.path.basename(txt_file)}:\\") for line in lines[:top_lines]: print(line.strip()) print(\\"n\\") with open(os.path.join(directory, \\"summary.txt\\"), \'w\') as summary_file: summary_file.write(\\"n\\".join(summary_data))"},{"question":"Objective Implement and demonstrate the use of the `zoneinfo` module in Python 3.9. You will create timezone-aware datetime objects, handle timezone transitions, and manage ambiguous times using the fold attribute. Requirements 1. Define a function `get_aware_datetime(timezone, year, month, day, hour, minute)` that takes a timezone string and a datetime (year, month, day, hour, minute), and returns a timezone-aware datetime object using the `ZoneInfo` module. 2. Define a function `calculate_future_time(base_datetime, delta_days)` that takes a timezone-aware datetime object and a number of days to add (delta_days), and returns the future datetime accounting for any potential timezone transitions. 3. Define a function `handle_ambiguous_time(timezone, year, month, day, hour, minute)` that returns two timezone-aware datetime objects representing the ambiguous time with `fold=0` and `fold=1`. Input and Output Formats 1. `get_aware_datetime(timezone, year, month, day, hour, minute)` - **Input:** - `timezone`: A string representing the IANA timezone (e.g., \\"America/New_York\\"). - `year`: An integer representing the year. - `month`: An integer representing the month. - `day`: An integer representing the day. - `hour`: An integer representing the hour. - `minute`: An integer representing the minute. - **Output:** A timezone-aware `datetime` object. 2. `calculate_future_time(base_datetime, delta_days)` - **Input:** - `base_datetime`: A timezone-aware `datetime` object. - `delta_days`: An integer representing the number of days to add. - **Output:** A `datetime` object representing the future time. 3. `handle_ambiguous_time(timezone, year, month, day, hour, minute)` - **Input:** - `timezone`: A string representing the IANA timezone. - `year`: An integer representing the year. - `month`: An integer representing the month. - `day`: An integer representing the day. - `hour`: An integer representing the hour. - `minute`: An integer representing the minute. - **Output:** A tuple with two timezone-aware `datetime` objects representing the ambiguous time with `fold=0` and `fold=1`. Constraints - If the provided timezone is not found, an appropriate exception should be raised. - Valid timezone strings should adhere to IANA time zone database standards (e.g., \\"America/New_York\\"). Example Usage ```python from zoneinfo import ZoneInfo from datetime import datetime, timedelta # Example inputs to function get_aware_datetime dt = get_aware_datetime(\\"America/Los_Angeles\\", 2020, 10, 31, 12, 0) print(dt) # Output: 2020-10-31 12:00:00-07:00 # Example inputs to function calculate_future_time future_dt = calculate_future_time(dt, 1) print(future_dt) # Output: 2020-11-01 12:00:00-08:00 # Example inputs to function handle_ambiguous_time ambiguous_times = handle_ambiguous_time(\\"America/Los_Angeles\\", 2020, 11, 1, 1, 0) print(ambiguous_times[0]) # Output: 2020-11-01 01:00:00-07:00 print(ambiguous_times[1]) # Output: 2020-11-01 01:00:00-08:00 ``` # Notes: - Ensure your code is well-documented. - Handle edge cases such as invalid timezones gracefully. - Make sure to test your functions thoroughly with various timezones and dates.","solution":"from datetime import datetime, timedelta from zoneinfo import ZoneInfo, ZoneInfoNotFoundError def get_aware_datetime(timezone, year, month, day, hour, minute): Returns a timezone-aware datetime object. try: tz = ZoneInfo(timezone) dt = datetime(year, month, day, hour, minute, tzinfo=tz) return dt except ZoneInfoNotFoundError: raise ValueError(f\\"The timezone \'{timezone}\' is not found.\\") def calculate_future_time(base_datetime, delta_days): Returns a future datetime after adding a number of days, accounting for timezone transitions. if not base_datetime.tzinfo: raise ValueError(\\"base_datetime must be timezone-aware.\\") future_datetime = base_datetime + timedelta(days=delta_days) return future_datetime def handle_ambiguous_time(timezone, year, month, day, hour, minute): Returns two timezone-aware datetime objects representing the ambiguous time (fold=0 and fold=1). try: tz = ZoneInfo(timezone) naive_dt = datetime(year, month, day, hour, minute) ambiguous_0 = naive_dt.replace(tzinfo=tz, fold=0) ambiguous_1 = naive_dt.replace(tzinfo=tz, fold=1) return (ambiguous_0, ambiguous_1) except ZoneInfoNotFoundError: raise ValueError(f\\"The timezone \'{timezone}\' is not found.\\")"},{"question":"# Python Coding Assessment Question - `site` Module Customization Objective Your task is to create a utility function that facilitates the management of Python site-specific paths and configuration. This function will allow the user to dynamically manage site-specific paths and check if specific paths or directories are included in the current `sys.path`. Function Signature ```python def manage_site_path(action: str, path: str = None) -> list: ``` Input - **action (str):** The action to perform. It can be one of the following: - `\\"add\\"`: Add the specified path to `sys.path` and process its `.pth` files. - `\\"remove\\"`: Remove the specified path from `sys.path` if it exists. - `\\"list\\"`: List all directories currently in `sys.path`. - `\\"user_site_enabled\\"`: Return whether the user site-packages directory is enabled. - **path (str, optional):** The path to be added or removed. This is required for the `\\"add\\"` and `\\"remove\\"` actions. It should be a valid directory path string. Output - **list:** A list of strings representing directories in the `sys.path` for the `\\"add\\"`, `\\"remove\\"`, and `\\"list\\"` actions. - **bool:** A boolean value indicating whether the user site-packages directory is enabled for the `\\"user_site_enabled\\"` action. Constraints and Requirements - Use the `site` module\'s functionalities like `addsitedir`, `ENABLE_USER_SITE`, and `sys.path` manipulation. - Handle invalid actions gracefully by raising a `ValueError` with a descriptive message. - Ensure that the `.pth` processing adheres to the rules specified in the `site` module documentation. - The solution should work across major operating systems (Windows, macOS, Unix). Example Usage ```python #Example 1: Adding a directory to sys.path current_paths = manage_site_path(\\"add\\", \\"/path/to/custom/site-packages\\") print(current_paths) #Example 2: Removing a directory from sys.path current_paths = manage_site_path(\\"remove\\", \\"/path/to/custom/site-packages\\") print(current_paths) #Example 3: Listing current sys.path directories paths = manage_site_path(\\"list\\") print(paths) #Example 4: Checking if user site-packages directory is enabled is_enabled = manage_site_path(\\"user_site_enabled\\") print(is_enabled) ``` Notes - Ensure that you validate the `path` parameter before attempting to add or remove it from `sys.path`. - Use the `site.addsitedir` function to handle the addition of paths, which will also process `.pth` files if they exist in the specified directory. - You can use the `site.ENABLE_USER_SITE` flag to check if the user site-packages directory is enabled. - Only directories should be added to or removed from `sys.path`. - Ensure that the added or removed paths are reflected properly in the output list. Testing your Implementation You can test your function using various scenarios, including: - Adding and then listing paths. - Removing paths. - Checking if user site-packages are enabled or not, depending on your Python environment settings.","solution":"import sys import site def manage_site_path(action: str, path: str = None) -> list: if action == \\"add\\": if not path: raise ValueError(\\"Path must be provided to add to sys.path\\") site.addsitedir(path) return sys.path elif action == \\"remove\\": if not path: raise ValueError(\\"Path must be provided to remove from sys.path\\") if path in sys.path: sys.path.remove(path) return sys.path elif action == \\"list\\": return sys.path elif action == \\"user_site_enabled\\": return site.ENABLE_USER_SITE else: raise ValueError(f\\"Invalid action specified: {action}\\")"},{"question":"**Seaborn Advanced Visualization Challenge** **Objective:** Demonstrate your understanding of the `seaborn.objects` module to create advanced visualizations. You will create a series of plots showcasing different relationships in the dataset using various functionalities like pairing, faceting, and customizing labels. **Dataset:** We will use the built-in `mpg` dataset from the Seaborn library. You can load this dataset using: ```python from seaborn import load_dataset mpg = load_dataset(\\"mpg\\") ``` **Task:** 1. **Plot Pairing:** Create a plot that shows the relationship between `acceleration` and both `displacement` and `weight` using `pair` on the x-axis. Add points to the plot using `Dots`. 2. **Multiple Pairwise Relationships:** Create a plot that shows multiple pairwise relationships by passing lists to both `x` and `y` for the variables `displacement`, `weight`, `horsepower`, and `acceleration`. Use `Dots` to add points to the plot. 3. **Combining Pairing with Faceting:** Create a plot pairing `weight` on the x-axis with `horsepower` and `acceleration` on the y-axis, and facet the plot by the `origin` variable. Use `Dots` to add points to the plot. 4. **Custom Labels:** Create a plot that pairs `weight` and `displacement` on the x-axis with `mpg` on the y-axis. Customize the plot by adding labels: \\"Weight (lb)\\" for `x0`, \\"Displacement (cu in)\\" for `x1`, and \\"MPG\\" for `y`. **Constraints:** - Ensure your plots are well-labeled and aesthetically clear. - Each plot should be generated using the `seaborn.objects` module. - Follow the cell execution sequence provided below to ensure clarity and correctness of your plots. **Example Code Execution:** ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset mpg = load_dataset(\\"mpg\\") # Plot 1: Pairing acceleration with displacement and weight ( so.Plot(mpg, y=\\"acceleration\\") .pair(x=[\\"displacement\\", \\"weight\\"]) .add(so.Dots()) ) # Plot 2: Multiple pairwise relationships ( so.Plot(mpg) .pair(x=[\\"displacement\\", \\"weight\\"], y=[\\"horsepower\\", \\"acceleration\\"]) .add(so.Dots()) ) # Plot 3: Pairing with Faceting ( so.Plot(mpg, x=\\"weight\\") .pair(y=[\\"horsepower\\", \\"acceleration\\"]) .facet(col=\\"origin\\") .add(so.Dots()) ) # Plot 4: Custom Labels ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"weight\\", \\"displacement\\"]) .label(x0=\\"Weight (lb)\\", x1=\\"Displacement (cu in)\\", y=\\"MPG\\") .add(so.Dots()) ) ``` **Submission:** Submit your Jupyter notebook file containing: - The well-commented code implementing the four tasks. - The resultant plots. Ensure your notebook is clear and easy to understand, demonstrating your proficiency with Seaborn for creating advanced visualizations.","solution":"import seaborn.objects as so from seaborn import load_dataset def seaborn_visualizations(): # Load the dataset mpg = load_dataset(\\"mpg\\") # Plot 1: Pairing acceleration with displacement and weight plot1 = ( so.Plot(mpg, y=\\"acceleration\\") .pair(x=[\\"displacement\\", \\"weight\\"]) .add(so.Dots()) ) # Plot 2: Multiple pairwise relationships plot2 = ( so.Plot(mpg) .pair(x=[\\"displacement\\", \\"weight\\"], y=[\\"horsepower\\", \\"acceleration\\"]) .add(so.Dots()) ) # Plot 3: Pairing with Faceting plot3 = ( so.Plot(mpg, x=\\"weight\\") .pair(y=[\\"horsepower\\", \\"acceleration\\"]) .facet(col=\\"origin\\") .add(so.Dots()) ) # Plot 4: Custom Labels plot4 = ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"weight\\", \\"displacement\\"]) .label(x0=\\"Weight (lb)\\", x1=\\"Displacement (cu in)\\", y=\\"MPG\\") .add(so.Dots()) ) return plot1, plot2, plot3, plot4 # Call the function to generate plots seaborn_visualizations()"},{"question":"Objective To assess your understanding of multimedia processing using the `wave` and `colorsys` modules from python310. Task Write a Python function that reads a WAV audio file, processes the audio data to reverse its waveform, and then converts a list of RGB (Red, Green, Blue) values to HLS (Hue, Lightness, Saturation) format. Function Signature ```python def process_audio_and_convert_colors(wav_file_path: str, rgb_values: list[tuple[int, int, int]]) -> dict: pass ``` Input 1. `wav_file_path` (str): The file path to a WAV file. 2. `rgb_values` (list of tuples): A list of RGB values. Each RGB value is a tuple containing three integers (each between 0 and 255, inclusive). Output - A dictionary with the following keys: - `\\"reversed_audio\\"`: Bytes representing the reversed audio data. - `\\"hls_values\\"`: A list of tuples where each tuple contains three floats representing the converted HLS values. Constraints - The WAV file should be read entirely into memory. - You may assume `wav_file_path` points to a valid WAV file. - The `rgb_values` list contains at least one tuple. Example ```python wav_file_path = \'example.wav\' rgb_values = [(255, 0, 0), (0, 255, 0), (0, 0, 255)] result = process_audio_and_convert_colors(wav_file_path, rgb_values) print(result) # Output { \'reversed_audio\': b\'...\', # Bytes of the reversed audio data. \'hls_values\': [(0.0, 0.5, 1.0), (0.3333333333333333, 0.5, 1.0), (0.6666666666666666, 0.5, 1.0)] } ``` Hints - Use the `wave` module to read and write WAV files. - Use the `colorsys` module to convert RGB values to HLS. Performance Considerations - Ensure your function handles large WAV files efficiently, keeping memory use in check. - The conversion between color systems should be done using `colorsys` functions, ensuring both accuracy and standard compliance.","solution":"import wave import colorsys def process_audio_and_convert_colors(wav_file_path: str, rgb_values: list[tuple[int, int, int]]) -> dict: # Reverse the audio waveform with wave.open(wav_file_path, \'rb\') as wav_file: n_channels = wav_file.getnchannels() sampwidth = wav_file.getsampwidth() n_frames = wav_file.getnframes() framerate = wav_file.getframerate() comp_type = wav_file.getcomptype() comp_name = wav_file.getcompname() frames = wav_file.readframes(n_frames) # Reverse frames reversed_frames = frames[::-1] # Convert RGB values to HLS hls_values = [colorsys.rgb_to_hls(r/255.0, g/255.0, b/255.0) for r, g, b in rgb_values] # Return the results as a dictionary return { \'reversed_audio\': reversed_frames, \'hls_values\': hls_values }"},{"question":"**Problem Statement:** You are required to process multiple text files containing log entries and summarize the data. Given a list of file names containing log entries, read through all the files and compute the total number of lines across all files, the number of log entries in each file, and determine if the entries are valid based on specific criteria. **Input Format:** 1. A list of file names (e.g., `[\'file1.log\', \'file2.log\']`). **Output Format:** 1. An integer representing the total number of lines in all files. 2. A dictionary where the keys are filenames and the values are the number of valid log entries in each file. 3. A list of tuples where each tuple consists of a filename and a boolean indicating whether all the entries in the file are valid. **Log Entry Criteria:** A log entry is valid if it follows this format: ``` [YYYY-MM-DD HH:MM:SS] LEVEL: message ``` Where: - `YYYY-MM-DD` is the date. - `HH:MM:SS` is the time. - `LEVEL` is either \\"INFO\\", \\"WARNING\\", or \\"ERROR\\". **Task:** Implement a function `process_logs(files: List[str]) -> Tuple[int, Dict[str, int], List[Tuple[str, bool]]]` that reads the provided files and returns the required analysis. **Constraints:** - Each file contains one log entry per line. - Lines without log entries (e.g., empty lines) should be ignored. - Files may be empty. - Lines that do not match the specified format are considered invalid. **Performance Requirements:** - Efficient processing of the files, using a single pass through the contents. Here is a template to get you started: ```python from typing import List, Tuple, Dict import fileinput import re def process_logs(files: List[str]) -> Tuple[int, Dict[str, int], List[Tuple[str, bool]]]: log_entry_pattern = re.compile(r\'[d{4}-d{2}-d{2} d{2}:d{2}:d{2}] (INFO|WARNING|ERROR): .+\') total_lines = 0 file_validations = {} file_validity_check = [] with fileinput.input(files=files, encoding=\\"utf-8\\") as f: for line in f: line = line.strip() if not line: continue total_lines += 1 file_name = fileinput.filename() if file_name not in file_validations: file_validations[file_name] = 0 if log_entry_pattern.match(line): file_validations[file_name] += 1 for file in files: total_entries_in_file = sum(1 for _ in fileinput.FileInput(files=(file,), encoding=\\"utf-8\\")) all_entries_valid = file_validations.get(file, 0) == total_entries_in_file file_validity_check.append((file, all_entries_valid)) return total_lines, file_validations, file_validity_check ``` **Example:** Given the following files and their contents: - `file1.log`: ``` [2023-10-01 12:30:45] INFO: System rebooted [2023-10-01 12:45:15] WARNING: Disk space low Invalid log entry ``` - `file2.log`: ``` [2023-10-02 08:15:22] ERROR: Failed to start service [2023-10-02 09:50:05] INFO: User logged in ``` The function call: ```python process_logs([\'file1.log\', \'file2.log\']) ``` Should return: ```python (5, {\'file1.log\': 2, \'file2.log\': 2}, [(\'file1.log\', False), (\'file2.log\', True)]) ``` Explanation: - There are 5 lines in total across both files. - `file1.log` has 2 valid entries (3 lines in total, 1 invalid). - `file2.log` has 2 valid entries (all lines are valid). - `file1.log` is not fully valid, whereas `file2.log` is fully valid.","solution":"from typing import List, Tuple, Dict import re def process_logs(files: List[str]) -> Tuple[int, Dict[str, int], List[Tuple[str, bool]]]: log_entry_pattern = re.compile(r\'[d{4}-d{2}-d{2} d{2}:d{2}:d{2}] (INFO|WARNING|ERROR): .+\') total_lines = 0 file_validations = {} file_validity_check = [] line_counts = {file: 0 for file in files} valid_counts = {file: 0 for file in files} for file in files: with open(file, \'r\', encoding=\\"utf-8\\") as f: for line in f: line = line.strip() if not line: continue total_lines += 1 line_counts[file] += 1 if log_entry_pattern.match(line): valid_counts[file] += 1 for file in files: all_entries_valid = valid_counts[file] == line_counts[file] file_validity_check.append((file, all_entries_valid)) return total_lines, valid_counts, file_validity_check"},{"question":"Objective Demonstrate understanding of Python\'s generator functions, custom object methods, and nested comprehensions. Question You are tasked with implementing a custom iterable class `PhraseAnalyzer` that processes phrases and allows complex custom operations on them. Specifically, this class should: 1. **Initialize with a list of phrases**. 2. **Support iteration** using both synchronous and asynchronous methods. 3. **Allow nested comprehensions** to filter and process words in the phrases. 4. **Override basic arithmetic operations** (`+`, `-`). Requirements 1. **Initialization:** - The `PhraseAnalyzer` should be initialized with a list of strings, each string being a phrase containing words separated by spaces. - Example: `phrases = [\\"hello world\\", \\"example phrase\\"]`. 2. **Iteration:** - Implement synchronous iteration (`__iter__` and `__next__`) to yield words from all phrases. - Implement asynchronous iteration (`__aiter__` and `__anext__`) to yield words in reversed order for each phrase. 3. **Custom Arithmetic Operations:** - Overload the `+` operator to merge two `PhraseAnalyzer` objects. The merged object should contain phrases from both objects. - Overload the `-` operator to remove phrases of one `PhraseAnalyzer` from another, if they exist. 4. **Nested Comprehensions:** - Provide a method `filter_long_words(self, min_length)` that returns a nested comprehension filtering out words shorter than `min_length`. - Example: `PhraseAnalyzer([\\"hello world\\", \\"example here\\"]).filter_long_words(5)` should return `[\\"example\\"]`. Constraints - All phrases are non-empty strings. - Individual words are separated by single spaces. Function Signature ```python class PhraseAnalyzer: def __init__(self, phrases: list): pass def __iter__(self): pass def __next__(self): pass async def __aiter__(self): pass async def __anext__(self): pass def filter_long_words(self, min_length: int) -> list: pass def __add__(self, other: \'PhraseAnalyzer\') -> \'PhraseAnalyzer\': pass def __sub__(self, other: \'PhraseAnalyzer\') -> \'PhraseAnalyzer\': pass ``` Example ```python phrases1 = PhraseAnalyzer([\\"hello world\\", \\"example phrase\\"]) phrases2 = PhraseAnalyzer([\\"another day\\", \\"new example\\"]) # Synchronous iteration for word in phrases1: print(word) # Output: \\"hello\\", \\"world\\", \\"example\\", \\"phrase\\" # Asynchronous iteration async for word in phrases1: print(word[::-1]) # Output: \\"olleh\\", \\"dlrow\\", \\"elpmaxe\\", \\"esarhp\\" merged = phrases1 + phrases2 print(merged.phrases) # Output: [\\"hello world\\", \\"example phrase\\", \\"another day\\", \\"new example\\"] reduced = phrases1 - merged print(reduced.phrases) # Output: [] filtered = phrases1.filter_long_words(5) print(filtered) # Output: [\\"example\\", \\"phrase\\"] ``` Your task is to implement the `PhraseAnalyzer` class with the above specifications. Make sure to handle any edge cases and maintain performance efficiency.","solution":"class PhraseAnalyzer: def __init__(self, phrases): self.phrases = phrases self.words = [word for phrase in self.phrases for word in phrase.split()] self._current = 0 def __iter__(self): self._current = 0 return self def __next__(self): if self._current < len(self.words): word = self.words[self._current] self._current += 1 return word else: raise StopIteration async def __aiter__(self): self._current = -1 return self async def __anext__(self): self._current += 1 if self._current < len(self.phrases): return self.phrases[self._current][::-1] else: raise StopAsyncIteration def filter_long_words(self, min_length): return [word for phrase in self.phrases for word in phrase.split() if len(word) >= min_length] def __add__(self, other): return PhraseAnalyzer(self.phrases + other.phrases) def __sub__(self, other): return PhraseAnalyzer([phrase for phrase in self.phrases if phrase not in other.phrases])"},{"question":"Objective Implement a custom `SubprocessManager` using PyTorch’s `torch.distributed.elastic.multiprocessing.subprocess_handler` package. This manager should demonstrate your understanding of launching, monitoring, and handling subprocesses in a distributed computing environment. Problem Statement Write a Python class `SubprocessManager` that uses PyTorch\'s `SubprocessHandler` to manage distributed subprocesses. The class should be able to: 1. Initialize the subprocess handler. 2. Launch a specified number of subprocesses. 3. Monitor the subprocesses. 4. Gracefully terminate all subprocesses. Class Signature ```python class SubprocessManager: def __init__(self, num_subprocesses: int): Initializes the SubprocessManager with the specified number of subprocesses. Args: num_subprocesses (int): The number of subprocesses to manage. pass def launch_subprocesses(self): Launches the specified number of subprocesses and returns the PIDs of the subprocesses. Returns: List[int]: List of process IDs of launched subprocesses. pass def monitor_subprocesses(self) -> List[bool]: Monitors the subprocesses and returns a list of booleans indicating whether each subprocess is still alive. Returns: List[bool]: List of booleans, one for each subprocess, indicating if they are alive. pass def terminate_all(self): Gracefully terminates all the subprocesses. pass ``` Requirements 1. **Initialization:** The `SubprocessManager` should initialize and prepare the `SubprocessHandler` for usage. 2. **Launching subprocesses:** Implement `launch_subprocesses()` to start the specified number of subprocesses using PyTorch’s `SubprocessHandler`. The function should return a list of PIDs of the launched processes. 3. **Monitoring:** Implement `monitor_subprocesses()` to return a list indicating whether each subprocess is alive. 4. **Termination:** Implement `terminate_all()` to gracefully terminate all subprocesses. Constraints 1. Assume the subprocesses execute a simple command like `time.sleep(10)` for illustration purposes. 2. Subprocesses should be handled in a fault-tolerant manner where appropriate. Example Here is how an example usage might look like: ```python manager = SubprocessManager(num_subprocesses=3) pids = manager.launch_subprocesses() print(f\\"Subprocesses launched with PIDs: {pids}\\") alive_status = manager.monitor_subprocesses() print(f\\"Are subprocesses alive? {alive_status}\\") manager.terminate_all() print(\\"All subprocesses terminated.\\") ``` You should demonstrate that your implementation correctly launches, monitors, and terminates subprocesses and handles any distributed aspects appropriately.","solution":"import torch.distributed.elastic.multiprocessing as mp import time import os from multiprocessing import Process from typing import List class SubprocessManager: def __init__(self, num_subprocesses: int): Initializes the SubprocessManager with the specified number of subprocesses. Args: num_subprocesses (int): The number of subprocesses to manage. self.num_subprocesses = num_subprocesses self.subprocesses = [] def _target_function(self): Function that each subprocess will run. For demonstration purpose, it will sleep for 10 seconds. time.sleep(10) def launch_subprocesses(self) -> List[int]: Launches the specified number of subprocesses and returns the PIDs of the subprocesses. Returns: List[int]: List of process IDs of launched subprocesses. for _ in range(self.num_subprocesses): p = Process(target=self._target_function) p.start() self.subprocesses.append(p) return [p.pid for p in self.subprocesses] def monitor_subprocesses(self) -> List[bool]: Monitors the subprocesses and returns a list of booleans indicating whether each subprocess is still alive. Returns: List[bool]: List of booleans, one for each subprocess, indicating if they are alive. return [p.is_alive() for p in self.subprocesses] def terminate_all(self): Gracefully terminates all the subprocesses. for p in self.subprocesses: p.terminate() p.join()"},{"question":"Advanced Seaborn Visualization As a data scientist, you have been given a dataset to analyze. Your task is to create a detailed scatter plot with additional visual aids to help identify data distributions and relationships clearly. Problem Statement: 1. Load the `titanic` dataset available from the Seaborn library. 2. Create a scatter plot to visualize the relationship between `age` and `fare`. 3. Add a rug plot to both the x-axis (`age`) and y-axis (`fare`). Customize the rug plot as follows: - Display rug on both axes but place it outside the axes. - Use a height of 0.05 for the rug lines. 4. Utilize hue mapping to differentiate by the `class` feature. 5. Use alpha blending to adjust for data density. Provide the final visualization as an output. Input: - No direct input from the user, datasets are to be loaded from Seaborn\'s library. Expected Output: - A scatter plot of `age` vs `fare` with rugs indicating marginal distribution, customized rug properties, hue mapping by `class`, and alpha blending for better density representation. Constraints: - Return the plot object. - Ensure the axes labels are clear and the legend is appropriately placed. Sample Code: The following is a code structure to get you started. Please fill in the necessary parts to complete the task. ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_titanic_data(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create the scatter plot scatter_plot = sns.scatterplot(data=titanic, x=\\"age\\", y=\\"fare\\", hue=\\"class\\") # Add the customized rug plot sns.rugplot(data=titanic, x=\\"age\\", y=\\"fare\\", height=0.05, clip_on=False) # Customize appearance (axes labels, legend placement, etc.) scatter_plot.set(xlabel=\'Age\', ylabel=\'Fare\') plt.legend(title=\'Class\') # Show the plot plt.show() # Call the function to display the plot visualize_titanic_data() ``` Complete the code to meet the requirements and showcase your understanding of Seaborn\'s advanced features.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_titanic_data(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create the scatter plot with hue based on \'class\' and alpha blending for density scatter_plot = sns.scatterplot(data=titanic, x=\\"age\\", y=\\"fare\\", hue=\\"class\\", alpha=0.6) # Add the customized rug plot sns.rugplot(data=titanic, x=\\"age\\", height=0.05, clip_on=False, ax=scatter_plot) sns.rugplot(data=titanic, y=\\"fare\\", height=0.05, clip_on=False, ax=scatter_plot) # Customize appearance (axes labels, legend placement, etc.) scatter_plot.set(xlabel=\'Age\', ylabel=\'Fare\') plt.legend(title=\'Class\', loc=\'best\') # Return the plot object for inspection return scatter_plot # Call the function to display the plot scatter_plot_obj = visualize_titanic_data() plt.show()"},{"question":"asyncio Custom Exception Handling Objective: Demonstrate your understanding of handling custom exceptions in the `asyncio` module by implementing a function to perform asynchronous read operations with error handling. Problem Statement: Implement a function `async_read_with_error_handling(reader: asyncio.StreamReader, num_bytes: int) -> bytes` that reads a specified number of bytes from an `asyncio` stream reader. The function should appropriately handle and raise custom exceptions defined in `asyncio` when specific error conditions are met. Function Signature: ```python import asyncio async def async_read_with_error_handling(reader: asyncio.StreamReader, num_bytes: int) -> bytes: pass ``` Inputs: - `reader` (asyncio.StreamReader): An `asyncio` StreamReader object from which data is to be read. - `num_bytes` (int): The number of bytes to read from the stream. Expected Output: - Return a `bytes` object containing the data read from the stream. Error Handling Requirements: 1. Raise `asyncio.TimeoutError` if the reading operation exceeds a 5-second deadline. 2. Raise `asyncio.IncompleteReadError` if the stream closes before `num_bytes` bytes are read. Provide appropriate values for the `expected` and `partial` attributes. 3. Raise `asyncio.LimitOverrunError` if attempting to read beyond a specified internal buffer limit (e.g., 1024 bytes). Use the `consumed` attribute to indicate the number of bytes that were read before hitting the limit. Constraints: 1. Ensure you handle `asyncio.CancelledError` gracefully by re-raising it after performing any necessary clean-up. 2. Assume the `reader` object is correctly instantiated and passed into the function. Example: ```python import asyncio from asyncio import StreamReader async def example_usage(): reader = StreamReader() # Simulate adding some data into reader buffer reader.feed_data(b\\"Test data\\") reader.feed_eof() try: data = await async_read_with_error_handling(reader, 10) print(data) # Should print: b\'Test data\' except Exception as e: print(f\\"Exception occurred: {e}\\") # Run the example asyncio.run(example_usage()) ``` In the provided example, an `asyncio.StreamReader` is created, and data is fed into its buffer. The `async_read_with_error_handling` function is then called with appropriate error handling, demonstrating how it should manage different custom exceptions. Notes: - Make sure to simulate error conditions in your implementation for testing each exception handling scenario. - Use `asyncio.wait_for` to enforce the timeout for raising `asyncio.TimeoutError`.","solution":"import asyncio async def async_read_with_error_handling(reader: asyncio.StreamReader, num_bytes: int) -> bytes: try: # Apply a timeout of 5 seconds for the read operation data = await asyncio.wait_for(reader.readexactly(num_bytes), timeout=5.0) return data except asyncio.TimeoutError: raise asyncio.TimeoutError(\\"The read operation exceeded the time limit of 5 seconds.\\") except asyncio.CancelledError: raise except asyncio.IncompleteReadError as e: raise asyncio.IncompleteReadError(e.partial, e.expected) except asyncio.LimitOverrunError as e: raise asyncio.LimitOverrunError(e.consumed)"},{"question":"# XML Data Parsing Challenge **Objective:** Implement a custom XML parser using the `xml.parsers.expat` module. Your parser should be able to: 1. Parse any given XML string. 2. Handle the start and end of elements. 3. Handle character data inside elements. 4. Handle XML parsing errors gracefully. **Requirements:** - Create a function `custom_xml_parser(xml_data: str) -> dict`. - The function should parse the provided XML string `xml_data` and return a dictionary with element names as keys and a list of tuples as values. Each tuple should contain: - The attributes of the elements. - The character data inside the elements. **Function Signature:** ```python def custom_xml_parser(xml_data: str) -> dict: pass ``` **Input:** - `xml_data` (str): A string containing the XML data. **Output:** - A dictionary where keys are element names and values are lists of tuples. Each tuple will have two elements: 1. A dictionary of attributes of the element (or an empty dictionary if none). 2. A string of character data inside the element (or an empty string if none). **Constraints:** 1. The function must use the `xml.parsers.expat` module. 2. Handle any XML parsing errors using `xml.parsers.expat.ExpatError`. 3. Ignore XML comments in the parsing result. 4. Assume that element names are unique within the XML string (i.e., no two elements share the same name). **Example Input:** ```python xml_data = <?xml version=\\"1.0\\"?> <root> <child1 name=\\"example1\\">Content1</child1> <child2 name=\\"example2\\">Content2</child2> </root> ``` **Example Output:** ```python { \\"root\\": [({}, \'\')], \\"child1\\": [({\\"name\\": \\"example1\\"}, \\"Content1\\")], \\"child2\\": [({\\"name\\": \\"example2\\"}, \\"Content2\\")] } ``` **Function Implementation Template:** ```python def custom_xml_parser(xml_data: str) -> dict: import xml.parsers.expat parsed_data = {} def start_element(name, attrs): if name not in parsed_data: parsed_data[name] = [] parsed_data[name].append((attrs, \'\')) def end_element(name): pass def char_data(data): for key in parsed_data: if parsed_data[key][-1][1] == \'\': parsed_data[key][-1] = (parsed_data[key][-1][0], data) parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data try: parser.Parse(xml_data, True) except xml.parsers.expat.ExpatError as e: print(f\\"Error parsing XML: {e}\\") return parsed_data ``` **Note:** Make sure to handle exceptions properly and test the function with various XML strings to ensure it meets the requirements.","solution":"def custom_xml_parser(xml_data: str) -> dict: import xml.parsers.expat parsed_data = {} stack = [] def start_element(name, attrs): stack.append(name) if name not in parsed_data: parsed_data[name] = [] parsed_data[name].append((attrs, \'\')) def end_element(name): stack.pop() def char_data(data): data = data.strip() # strip blanks/newlines if not data: return # ignore pure whitespace if stack: cur_element = stack[-1] if parsed_data[cur_element][-1][1] == \'\': parsed_data[cur_element][-1] = (parsed_data[cur_element][-1][0], data) else: # append additional text to the existing value parsed_data[cur_element][-1] = (parsed_data[cur_element][-1][0], parsed_data[cur_element][-1][1] + data) parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data try: parser.Parse(xml_data, True) except xml.parsers.expat.ExpatError as e: print(f\\"Error parsing XML: {e}\\") return {} return parsed_data"},{"question":"Coding Assessment Question # Objective You are required to demonstrate your understanding and ability to work with the `plistlib` module by writing functions that read from and write to plist files. You will use the XML format for these plist files and handle a specific dataset. # Problem Statement Write two functions, `write_plist` and `read_plist`, to perform the following tasks: 1. **`write_plist(data: dict, filename: str) -> None`**: - This function should accept a dictionary `data` and a string `filename`. - It should write the dictionary data to a plist file with the given filename in XML format. - The dictionary should be serialized with sorted keys. 2. **`read_plist(filename: str) -> dict`**: - This function should accept a string `filename`. - It should read the plist file with the given filename and return the data as a dictionary. - The function should handle potential errors gracefully, such as file not found or invalid plist format. # Function Signatures ```python import plistlib from typing import Dict def write_plist(data: Dict, filename: str) -> None: # Your code here def read_plist(filename: str) -> Dict: # Your code here ``` # Input 1. `write_plist`: - `data`: A dictionary containing the data to be written to the plist file. Example: ```python { \\"name\\": \\"John Doe\\", \\"age\\": 30, \\"is_student\\": False, \\"courses\\": [\\"Math\\", \\"Science\\", \\"Literature\\"] } ``` - `filename`: A string representing the name of the file to write the plist data to. Example: `\\"data.plist\\"` 2. `read_plist`: - `filename`: A string representing the name of the file to read the plist data from. Example: `\\"data.plist\\"` # Output 1. `write_plist`: The function should create a plist file with the given filename in XML format and write the dictionary data to it. It should not return anything. 2. `read_plist`: The function should return a dictionary containing the data read from the plist file. # Constraints - You can assume that the dictionary values in `data` are of suitable plist-compatible types such as strings, integers, floats, booleans, lists, and nested dictionaries. - Handle exceptions such as file not found and invalid plist format in `read_plist` by returning an empty dictionary `{}` in such cases. - The plist file should be written in XML format with sorted keys. # Example ```python data = { \\"name\\": \\"John Doe\\", \\"age\\": 30, \\"is_student\\": False, \\"courses\\": [\\"Math\\", \\"Science\\", \\"Literature\\"] } filename = \\"data.plist\\" write_plist(data, filename) read_data = read_plist(filename) assert read_data == data print(read_data) ``` Expected Output: ```python { \'name\': \'John Doe\', \'age\': 30, \'is_student\': False, \'courses\': [\'Math\', \'Science\', \'Literature\'] } ``` # Notes - Ensure the functions work correctly for nested dictionaries and lists within the `data` dictionary. - Use the `plistlib` module\'s `dump` and `load` methods to handle the plist file operations.","solution":"import plistlib from typing import Dict def write_plist(data: Dict, filename: str) -> None: Write the dictionary data to a plist file with the given filename in XML format. The dictionary will be serialized with sorted keys. with open(filename, \'wb\') as file: plistlib.dump(data, file, fmt=plistlib.FMT_XML, sort_keys=True) def read_plist(filename: str) -> Dict: Read the plist file with the given filename and return the data as a dictionary. Handle potential errors gracefully by returning an empty dictionary if an error occurs. try: with open(filename, \'rb\') as file: return plistlib.load(file) except (FileNotFoundError, plistlib.InvalidFileException): return {}"},{"question":"# Advanced Python Import System: Custom Finder and Loader Background Python\'s import system allows for significant customization by enabling developers to create custom finders and loaders. By inserting these custom objects into `sys.meta_path` or manipulating `sys.path_hooks`, developers can influence how and from where modules are imported. This question will test your understanding of these advanced import mechanisms by asking you to implement custom finder and loader classes. Task You need to implement a custom finder and loader to import modules from a list of strings representing code snippets rather than from a filesystem. Assume that each string in the list represents the content of a module, and its position in the list corresponds to its qualified name (e.g., the second string in the list corresponds to the module `mod1`, the third string to `mod2`, etc.). Requirements 1. **Custom Finder**: - Implement a custom finder class `StringListFinder` that searches for modules within a predefined list of strings. - The class should provide a `find_spec` method to locate a module based on its name. 2. **Custom Loader**: - Implement a custom loader class `StringListLoader` that loads the module from the corresponding string. - The loader should provide a `create_module` method and an `exec_module` method to properly execute the module in its own namespace. 3. **Integration**: - Integrate your custom finder into the import system by adding it to `sys.meta_path`. Input and Output - **Input**: A list of strings where each string represents a module\'s code. - **Output**: The ability to import modules and use them as if they were loaded from the filesystem. Example Given the list of strings representing module code: ```python modules = [ \\"def hello():n return \'Hello from mod0\'\\", \\"def hello():n return \'Hello from mod1\'\\", \\"def hello():n return \'Hello from mod2\'\\" ] ``` Your implementation should allow the following imports and function usage: ```python import mod0 import mod1 import mod2 print(mod0.hello()) # Output: \\"Hello from mod0\\" print(mod1.hello()) # Output: \\"Hello from mod1\\" print(mod2.hello()) # Output: \\"Hello from mod2\\" ``` Constraints - The finder should only recognize module names that match predefined patterns, e.g., `mod0`, `mod1`, etc. - Ensure thread-safety and good performance. - Handle cases where modules do not exist gracefully. Code Template ```python import sys import importlib.util import types class StringListLoader(importlib.abc.Loader): def __init__(self, module_code): self.module_code = module_code def create_module(self, spec): return None # Use default module creation semantics def exec_module(self, module): exec(self.module_code, module.__dict__) class StringListFinder(importlib.abc.MetaPathFinder): def __init__(self, modules): self.modules = modules def find_spec(self, fullname, path, target=None): if fullname.startswith(\'mod\') and fullname[3:].isdigit(): index = int(fullname[3:]) if 0 <= index < len(self.modules): spec = importlib.util.spec_from_loader(fullname, StringListLoader(self.modules[index])) return spec return None # Integrate into the import system def integrate_finder_with_meta_path(modules): finder = StringListFinder(modules) sys.meta_path.insert(0, finder) # Example usage modules = [ \\"def hello():n return \'Hello from mod0\'\\", \\"def hello():n return \'Hello from mod1\'\\", \\"def hello():n return \'Hello from mod2\'\\" ] integrate_finder_with_meta_path(modules) import mod0 import mod1 import mod2 print(mod0.hello()) # Should output: \\"Hello from mod0\\" print(mod1.hello()) # Should output: \\"Hello from mod1\\" print(mod2.hello()) # Should output: \\"Hello from mod2\\" ``` Complete the classes and function to fully implement the custom import system.","solution":"import sys import importlib.util import importlib.abc import types class StringListLoader(importlib.abc.Loader): def __init__(self, module_code): self.module_code = module_code def create_module(self, spec): return None # Use default module creation semantics def exec_module(self, module): exec(self.module_code, module.__dict__) class StringListFinder(importlib.abc.MetaPathFinder): def __init__(self, modules): self.modules = modules def find_spec(self, fullname, path=None, target=None): if fullname.startswith(\'mod\') and fullname[3:].isdigit(): index = int(fullname[3:]) if 0 <= index < len(self.modules): spec = importlib.util.spec_from_loader(fullname, StringListLoader(self.modules[index])) return spec return None # Integrate finder into the import system def integrate_finder_with_meta_path(modules): finder = StringListFinder(modules) sys.meta_path.insert(0, finder) # Example usage modules = [ \\"def hello():n return \'Hello from mod0\'\\", \\"def hello():n return \'Hello from mod1\'\\", \\"def hello():n return \'Hello from mod2\'\\" ] integrate_finder_with_meta_path(modules) import mod0 import mod1 import mod2 print(mod0.hello()) # Output: \\"Hello from mod0\\" print(mod1.hello()) # Output: \\"Hello from mod1\\" print(mod2.hello()) # Output: \\"Hello from mod2\\""},{"question":"# Pandas Coding Assessment Question **Objective:** Demonstrate your understanding of pandas by manipulating Series and DataFrame objects to perform a specific data transformation and analysis task. **Scenario:** You are given sales data for a retail store with the following details: - `sales_data.csv` contains daily sales data with columns: `date`, `item_id`, `units_sold`, and `price_per_unit`. - `item_data.csv` contains item information with columns: `item_id`, `item_name`, and `category`. **Task:** 1. **Load Data:** - Read the data from `sales_data.csv` and `item_data.csv` into separate DataFrames. 2. **Data Cleaning and Preprocessing:** - Ensure there are no missing values in the `item_id` and `units_sold` columns of `sales_data`. - Fill any missing `price_per_unit` values with the median price for that item. 3. **Data Merging:** - Merge the sales data with item information based on `item_id`. 4. **Data Analysis:** - Calculate the total revenue per item (revenue = units_sold * price_per_unit) and add it as a new column `total_revenue` in the merged DataFrame. - Group the data by `category` and calculate the total units sold and total revenue for each category. 5. **Results:** - Return a DataFrame with `category`, `total_units_sold`, and `total_revenue` columns. - Sort the result by `total_revenue` in descending order. **Expected Function Signature:** ```python def analyze_sales_data(sales_data_path: str, item_data_path: str) -> pd.DataFrame: # Your code here pass ``` **Input:** - `sales_data_path`: str - The file path to `sales_data.csv`. - `item_data_path`: str - The file path to `item_data.csv`. **Output:** - A DataFrame with columns `category`, `total_units_sold`, and `total_revenue`, sorted by `total_revenue` in descending order. **Constraints:** - Assume that the data files exist at the specified paths. - Handle any potential missing values as specified. **Example Usage:** ```python result_df = analyze_sales_data(\'path/to/sales_data.csv\', \'path/to/item_data.csv\') print(result_df) ``` **Example Output:** ``` category total_units_sold total_revenue 0 Electronics 500 25000.0 1 Clothing 300 15000.0 2 Groceries 600 12000.0 ``` Ensure your solution is efficient and leverages the capabilities of pandas for data manipulation and analysis.","solution":"import pandas as pd def analyze_sales_data(sales_data_path: str, item_data_path: str) -> pd.DataFrame: # Load data sales_df = pd.read_csv(sales_data_path) item_df = pd.read_csv(item_data_path) # Data Cleaning and Preprocessing sales_df = sales_df.dropna(subset=[\'item_id\', \'units_sold\']) median_prices = sales_df.groupby(\'item_id\')[\'price_per_unit\'].transform(\'median\') sales_df[\'price_per_unit\'].fillna(median_prices, inplace=True) # Data Merging merged_df = pd.merge(sales_df, item_df, on=\'item_id\') # Data Analysis merged_df[\'total_revenue\'] = merged_df[\'units_sold\'] * merged_df[\'price_per_unit\'] category_summary = merged_df.groupby(\'category\').agg( total_units_sold=pd.NamedAgg(column=\'units_sold\', aggfunc=\'sum\'), total_revenue=pd.NamedAgg(column=\'total_revenue\', aggfunc=\'sum\') ).reset_index() # Sorting results result_df = category_summary.sort_values(by=\'total_revenue\', ascending=False).reset_index(drop=True) return result_df"},{"question":"# Advanced Coding Assessment Objective: To assess your knowledge of debugging and profiling in Python3.10, specifically using the modules described in the documentation (`faulthandler`, `pdb`, `cProfile`, `trace`, and `tracemalloc`). Task: You are provided with a function `process_data` below. This function processes a list of integers by performing multiple transformations. Your task is to: 1. **Identify bottlenecks and inefficient sections of the code** using `cProfile` or `timeit`. 2. **Debug any incorrect sections** using `pdb` if necessary. 3. **Trace the execution** of the function using the `trace` module. 4. **Analyze memory allocations** using `tracemalloc`. 5. **Optimize the function** for better performance (speed and memory). Function to Analyze: ```python def process_data(data): result = [] # Step 1: Square each number for num in data: result.append(num * num) # Step 2: Filter out even numbers result = list(filter(lambda x: x % 2 != 0, result)) # Step 3: Compute factorial of each number def factorial(n): if n == 0: return 1 else: return n * factorial(n-1) result = [factorial(x) for x in result] # Step 4: Sort the result in descending order result.sort(reverse=True) return result ``` Implementation Requirements: 1. **Identify Bottlenecks:** - Profile the `process_data` function using appropriate profiling tools to determine time-consuming sections. 2. **Correct any Logical Errors:** - Use `pdb` to debug and ensure the function works as expected. 3. **Trace the Execution:** - Use the `trace` module to log the function execution path. 4. **Analyze Memory:** - Apply `tracemalloc` to check for any excessive memory usage. 5. **Optimize the Function:** - Implement changes to enhance the performance of the `process_data` function based on your findings. Input: A list of integers. Output: A list of processed integers, optimized for performance. Constraints: - The input list can contain up to 10,000 integers - Integers range from 1 to 10^5 - The function should complete within a reasonable timeframe considering the input constraints Submission: Implement your solution in a function named `optimized_process_data(data: List[int]) -> List[int]`. ```python def optimized_process_data(data): # Your optimized implementation here pass ``` Provide an explanation of the steps you took to identify and resolve the bottlenecks, including any profiling, debugging, and optimization steps performed.","solution":"import math def optimized_process_data(data): Optimized function to process data as per given specifications. # Step 1: Square each number and filter out even numbers in a single pass result = [num * num for num in data if num % 2 != 0] # One-liner to avoid redundant steps # Step 2: Compute factorial using math.factorial for better performance result = [math.factorial(x) for x in result] # Step 3: Sort the result in descending order result.sort(reverse=True) return result"},{"question":"**Question: Preprocessing Data for Machine Learning** You are given a dataset of house prices, `housing_data.csv`, which contains the following features: 1. **`size`:** The size of the house in square feet (continuous feature). 2. **`rooms`:** The number of rooms in the house (continuous feature). 3. **`location`:** The location of the house, as a categorical variable with possible values \'urban\', \'suburban\', \'rural\'. 4. **`price`:** The price of the house in dollars (continuous target variable). Your task is to preprocess this dataset using scikit-learn\'s preprocessing utilities and implement a pipeline that performs the following steps: 1. **Standardize** the `size` and `rooms` features. 2. **Encode** the `location` feature using one-hot encoding. 3. **Discretize** the `rooms` feature into 3 bins. 4. **Combine** these preprocessing steps into a single pipeline and apply them to the dataset. 5. **Train a linear regression model** on the transformed data and evaluate its performance using R² score. **Input:** - A CSV file `housing_data.csv` with columns `size`, `rooms`, `location`, `price`. **Output:** - The R² score of the linear regression model. **Constraints:** - Use `StandardScaler` for standardizing the continuous features. - Use `OneHotEncoder` for encoding the categorical feature. - Use `KBinsDiscretizer` for discretizing the `rooms` feature. - Combine all preprocessing steps into a single pipeline. - Use `train_test_split` to split the data into training and testing sets with a test size of 20%. **Here\'s the structure of your implementation:** ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OneHotEncoder, KBinsDiscretizer from sklearn.pipeline import Pipeline, FeatureUnion from sklearn.compose import ColumnTransformer from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score # Load the data data = pd.read_csv(\'housing_data.csv\') # Split the data into features (X) and target (y) X = data.drop(\'price\', axis=1) y = data[\'price\'] # Define preprocessing steps preprocessor = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), [\'size\']), (\'rooms\', KBinsDiscretizer(n_bins=3, encode=\'ordinal\', strategy=\'uniform\'), [\'rooms\']), (\'cat\', OneHotEncoder(), [\'location\']) ]) # Create and combine preprocessing pipeline pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'regressor\', LinearRegression()) ]) # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Fit the model pipeline.fit(X_train, y_train) # Predict and evaluate the model y_pred = pipeline.predict(X_test) r2 = r2_score(y_test, y_pred) # Print the R² score print(f\'R² score: {r2}\') ``` **Explanation:** 1. **Data Loading**: Load the dataset from the CSV file into a pandas DataFrame. 2. **Split Data**: Split the dataset into features (X) and the target variable (y). 3. **Define Preprocessing**: Create a `ColumnTransformer` to apply different preprocessing steps to different columns. 4. **Create Pipeline**: Combine the preprocessing steps and the linear regression model into a single pipeline. 5. **Train-Test Split**: Split the data into training and testing sets. 6. **Fit Model**: Fit the pipeline on the training data. 7. **Evaluate Model**: Predict on the test data and calculate the R² score. **Requirements:** * Ensure that you have `scikit-learn`, `pandas`, and other necessary libraries installed before running the code. **Note:** You do not need to submit the dataset, but make sure the CSV file is available in the correct path when you run your code.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OneHotEncoder, KBinsDiscretizer from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score def preprocess_and_train_model(file_path): Preprocess the housing data and train a linear regression model. Returns the R² score of the model. # Load the data data = pd.read_csv(file_path) # Split the data into features (X) and target (y) X = data.drop(\'price\', axis=1) y = data[\'price\'] # Define preprocessing steps preprocessor = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), [\'size\']), (\'rooms\', KBinsDiscretizer(n_bins=3, encode=\'ordinal\', strategy=\'uniform\'), [\'rooms\']), (\'cat\', OneHotEncoder(), [\'location\']) ]) # Create and combine preprocessing pipeline pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'regressor\', LinearRegression()) ]) # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Fit the model pipeline.fit(X_train, y_train) # Predict and evaluate the model y_pred = pipeline.predict(X_test) r2 = r2_score(y_test, y_pred) return r2 # Example usage: # r2_score = preprocess_and_train_model(\'housing_data.csv\') # print(f\'R² score: {r2_score}\')"},{"question":"**Question: Implementing and Utilizing Coroutines** **Objective**: Demonstrate your understanding of Python coroutine objects by implementing a coroutine, creating a custom coroutine object, and performing type-checking operations. **Task**: 1. Implement a Python function using the `async` keyword. This function should perform a simple arithmetic operation (e.g., adding two numbers after a short delay). 2. Create a custom coroutine object using the details provided in the documentation. 3. Implement a function that checks if a given object is a coroutine using `PyCoro_CheckExact`. **Requirements**: - Your coroutine function should be named `async_add`. - The coroutine function `async_add` should accept two arguments `a` and `b` (both integers), and return their sum after a 1-second delay. - You need to create a custom coroutine object using the provided C function `PyCoro_New`. - Implement a function `is_coroutine(obj)` that returns `True` if `obj` is a coroutine object, and `False` otherwise by using `PyCoro_CheckExact`. **Input**: - Two integers `a` and `b` for the `async_add` coroutine function. - An arbitrary object for the `is_coroutine` function to check. **Output**: - The result of the addition performed by the coroutine after the delay. - Boolean indicating whether an object is a coroutine. **Constraints**: - You may use the `time.sleep` method to simulate the delay in the coroutine. - You need to mimic the usage of `PyCoro_New` and `PyCoro_CheckExact` functions, since this is a theoretical exercise based on documentation and we cannot directly use C functions in Python. **Example**: ```python import asyncio import time # Coroutine function async def async_add(a, b): await asyncio.sleep(1) return a + b # Function to check if an object is a coroutine def is_coroutine(obj): return hasattr(obj, \\"__await__\\") # Usage # async def main(): # result = await async_add(4, 3) # print(result) # Output should be 7 after 1 second delay # print(is_coroutine(async_add(4, 3))) # Output should be True ``` **Note**: Since we don\'t have direct access to run C functions, we use Python equivalent checks and coroutine creation in this task.","solution":"import asyncio import types async def async_add(a, b): Coroutine that adds two numbers after a 1-second delay. await asyncio.sleep(1) return a + b def is_coroutine(obj): Checks if a given object is a coroutine object. return isinstance(obj, types.CoroutineType)"},{"question":"# Question You are tasked with creating a function that schedules a series of meetings. Each meeting should have a specific start time and a duration, and subsequent meetings should not overlap. Given the following: 1. A list of tuples representing the start datetime (`start`) and the duration in minutes (`duration`) for each meeting. 2. The function should return a list of tuples where each tuple contains the start and end datetime for each meeting. Write a Python function `schedule_meetings(meetings: List[Tuple[datetime, int]]) -> List[Tuple[datetime, datetime]]` that schedules meetings to ensure no overlap, taking into account the provided `start` and `duration` for each meeting. # Constraints - Each meeting should start on or after its specified start time. - Meetings should not overlap. If two meetings end up having the same start time, schedule the one that appears first in the input list first. - `datetime` objects in the input list are naive, i.e., they do not contain timezone information. - The duration of the meetings is given in minutes and is always a non-negative integer. - The input list is non-empty and can have between 1 and 100 meetings. # Input - `meetings`: List of tuples where each tuple is of the format `(start: datetime, duration: int)`. Example: `[(datetime(2023, 7, 10, 9, 0), 60), (datetime(2023, 7, 10, 10, 0), 30)]` # Output - List of tuples where each tuple contains the start and end datetime for each meeting such that no meeting overlaps with any other. Example: `[(datetime(2023, 7, 10, 9, 0), datetime(2023, 7, 10, 10, 0)), (datetime(2023, 7, 10, 10, 0), datetime(2023, 7, 10, 10, 30))]` # Example ```python from datetime import datetime def schedule_meetings(meetings: List[Tuple[datetime, int]]) -> List[Tuple[datetime, datetime]]: # Your code here # Example usage: meetings = [ (datetime(2023, 7, 10, 9, 0), 60), (datetime(2023, 7, 10, 10, 0), 30) ] print(schedule_meetings(meetings)) ``` Output: ``` [ (datetime.datetime(2023, 7, 10, 9, 0), datetime.datetime(2023, 7, 10, 10, 0)), (datetime.datetime(2023, 7, 10, 10, 0), datetime.datetime(2023, 7, 10, 10, 30)) ] ``` # Note - Consider using the `datetime.timedelta` class for calculating the end times based on durations. - Ensure your solution resolves overlapping issues if they arise.","solution":"from datetime import datetime, timedelta from typing import List, Tuple def schedule_meetings(meetings: List[Tuple[datetime, int]]) -> List[Tuple[datetime, datetime]]: scheduled_meetings = [] for start, duration in meetings: if scheduled_meetings and start < scheduled_meetings[-1][1]: start = scheduled_meetings[-1][1] end = start + timedelta(minutes=duration) scheduled_meetings.append((start, end)) return scheduled_meetings"},{"question":"**Problem Statement**: You are working on an email processing application and need to handle email messages received from a network stream. Your task is to implement a function that processes raw email data fed incrementally and extracts all text parts, including those inside multipart messages. **Function Signature**: ```python def extract_text_parts(email_data: List[bytes]) -> List[str]: Extract text parts from incremental raw email data. Parameters: email_data (List[bytes]): A list of bytes-like objects representing parts of the email message. Returns: List[str]: A list of strings containing all text parts of the email. ``` **Input**: - `email_data` is a list of bytes-like objects. These objects represent parts of the email message fed incrementally. **Output**: - The function should return a list of strings, where each string is a text part extracted from the email message. **Constraints**: - The email data provided in `email_data` list should be treated as a single continuous email message. - Assume that the email follows standard MIME format. **Example**: ```python email_data = [ b\\"From: user@example.comrn\\", b\\"To: recipient@example.comrn\\", b\\"Subject: Test Emailrn\\", b\\"Content-Type: multipart/mixed; boundary=\\"boundary\\"rn\\", b\\"rn--boundaryrn\\", b\\"Content-Type: text/plainrnrn\\", b\\"This is the body of the email.rn\\", b\\"--boundaryrn\\", b\\"Content-Type: text/htmlrnrn\\", b\\"<html><body>This is the HTML part.</body></html>rn\\", b\\"--boundary--\\" ] print(extract_text_parts(email_data)) # Output: [\'This is the body of the email.\', \'<html><body>This is the HTML part.</body></html>\'] ``` **Hints**: 1. Use the `BytesFeedParser` from the `email.parser` module to incrementally parse the bytes data. 2. Utilize the `walk()` method on the parsed email message to iterate through all parts and extract text parts. **Solution**: ```python from email.parser import BytesFeedParser def extract_text_parts(email_data: List[bytes]) -> List[str]: parser = BytesFeedParser() for data in email_data: parser.feed(data) email_message = parser.close() text_parts = [] for part in email_message.walk(): if part.get_content_type() == \\"text/plain\\" or part.get_content_type() == \\"text/html\\": part_content = part.get_payload(decode=True) text_parts.append(part_content.decode(part.get_content_charset() or \\"utf-8\\")) return text_parts ```","solution":"from typing import List from email.parser import BytesFeedParser def extract_text_parts(email_data: List[bytes]) -> List[str]: Extract text parts from incremental raw email data. Parameters: email_data (List[bytes]): A list of bytes-like objects representing parts of the email message. Returns: List[str]: A list of strings containing all text parts of the email. parser = BytesFeedParser() for data in email_data: parser.feed(data) email_message = parser.close() text_parts = [] for part in email_message.walk(): if part.get_content_type() == \\"text/plain\\" or part.get_content_type() == \\"text/html\\": part_content = part.get_payload(decode=True) text_parts.append(part_content.decode(part.get_content_charset() or \\"utf-8\\")) return text_parts"},{"question":"Coding Assessment Question # Objective Design and implement a simple out-of-core learning system to classify text documents using scikit-learn. You will need to: 1. Stream instances from a large text dataset. 2. Extract features using a vectorization technique. 3. Train an incremental learning model using the extracted features. # Description You are provided with a large text dataset that cannot fit into memory all at once. Your task is to build an out-of-core learning pipeline using scikit-learn to classify text documents into predefined categories. # Task 1. **Stream Instances**: Implement a generator function `stream_text_data(file_path, batch_size)` that reads a large text file line-by-line and yields batches of text data for processing. 2. **Feature Extraction**: Use the `sklearn.feature_extraction.text.HashingVectorizer` to transform the text data into numerical features suitable for training a classifier. 3. **Incremental Learning**: Use the `sklearn.linear_model.SGDClassifier` to incrementally train a model using the features extracted. The classifier should be trained using the `partial_fit` method. # Requirements - Implement the streaming function `stream_text_data(file_path, batch_size)`: ```python def stream_text_data(file_path: str, batch_size: int): Streams text data from a file in batches. Parameters: - file_path: str, path to the text file. - batch_size: int, number of lines to yield per batch. Yields: - List of text documents (each document is a string). # Your implementation here ``` - Implement the function `train_incremental_classifier(file_path, batch_size, categories)`: ```python def train_incremental_classifier(file_path: str, batch_size: int, categories: list): Trains an incremental classifier using streamed text data. Parameters: - file_path: str, path to the text file. - batch_size: int, number of lines to process per batch. - categories: list, list of all possible target classes. Returns: - classifier: trained SGDCassifier model from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer import numpy as np vectorizer = HashingVectorizer(n_features=2**20) classifier = SGDClassifier() for batch in stream_text_data(file_path, batch_size): # Extract features X_batch = vectorizer.transform(batch) # Assume the labels are the first word in each document y_batch = np.array([doc.split()[0] for doc in batch]) if \'is_fit\' not in locals(): classifier.partial_fit(X_batch, y_batch, classes=categories) is_fit = True else: classifier.partial_fit(X_batch, y_batch) return classifier ``` # Constraints - The text file to be processed is too large to fit into memory. - Each line in the text file represents a document, with the first word being the class label and the rest being the document text. - The possible categories are known in advance and provided as a list `categories`. - You should use the `HashingVectorizer` for feature extraction and `SGDClassifier` for incremental learning. # Example Suppose `large_text_file.txt` contains the following lines: ``` sports Soccer game in the local stadium. politics The new policy was announced yesterday. sports The basketball team won their last match. ... ``` You can use the following script to test your implementation: ```python categories = [\'sports\', \'politics\', \'technology\', \'health\'] classifier = train_incremental_classifier(\'large_text_file.txt\', batch_size=1000, categories=categories) ```","solution":"def stream_text_data(file_path: str, batch_size: int): Streams text data from a file in batches. Parameters: - file_path: str, path to the text file. - batch_size: int, number of lines to yield per batch. Yields: - List of text documents (each document is a string). with open(file_path, \'r\') as file: batch = [] for line in file: batch.append(line.strip()) if len(batch) == batch_size: yield batch batch = [] if batch: yield batch def train_incremental_classifier(file_path: str, batch_size: int, categories: list): Trains an incremental classifier using streamed text data. Parameters: - file_path: str, path to the text file. - batch_size: int, number of lines to process per batch. - categories: list, list of all possible target classes. Returns: - classifier: trained SGDCassifier model from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer import numpy as np vectorizer = HashingVectorizer(n_features=2**20) classifier = SGDClassifier() for batch in stream_text_data(file_path, batch_size): # Extract features X_batch = vectorizer.transform(batch) # Assume the labels are the first word in each document y_batch = np.array([doc.split()[0] for doc in batch]) if \'is_fit\' not in locals(): classifier.partial_fit(X_batch, y_batch, classes=categories) is_fit = True else: classifier.partial_fit(X_batch, y_batch) return classifier"},{"question":"**Objective**: Implement and manage an asyncio FIFO queue to simulate a producer-consumer scenario. **Question**: You are required to create a producer-consumer model using asyncio queues. The producer will add tasks to the queue, and multiple consumers will process these tasks concurrently. Function Signature ```python import asyncio from typing import List async def producer(queue: asyncio.Queue, items: List[int]) -> None: Put a list of items into the provided queue. :param queue: an asyncio.Queue to put items into :param items: a list of items (integers) to be put into the queue pass async def consumer(queue: asyncio.Queue, name: str) -> None: Continuously get items from the queue until it is empty, and process them. Print the name of the consumer and the item being processed. :param queue: an asyncio.Queue to get items from :param name: a unique name identifier for the consumer pass async def main() -> None: Main function to orchestrate the producer and consumer. # Create a FIFO queue with a max size of 10. queue = asyncio.Queue(maxsize=10) # List of items for the producer to add to the queue items = [i for i in range(20)] # Create producer task producer_task = asyncio.create_task(producer(queue, items)) # Create 3 consumer tasks consumer_tasks = [ asyncio.create_task(consumer(queue, f\'consumer-{i}\')) for i in range(3) ] # Wait until the producer has finished producing all items await producer_task # Wait until the queue is fully processed await queue.join() # Cancel all consumer tasks for task in consumer_tasks: task.cancel() await asyncio.gather(*consumer_tasks, return_exceptions=True) print(\'All tasks complete.\') # Event loop to run the main function asyncio.run(main()) ``` **Requirements**: 1. **Producer**: - The `producer` function should put each item from the supplied list into the queue. - If the queue is full, the `producer` should wait until there is space in the queue. 2. **Consumer**: - The `consumer` function should continuously fetch items from the queue and process them until the queue is empty. - Each consumer should print its name and the item being processed. - Each `consumer` should use `queue.task_done()` to indicate task completion. 3. **Main**: - Create one producer and three consumers. - Ensure all items are processed before completing. **Constraints**: - You cannot modify the input/output format. - Ensure proper synchronization to avoid race conditions. **Performance**: - The program should handle the queue operations efficiently using asyncio. **Example Output**: ``` consumer-0 processing item 0 consumer-1 processing item 1 consumer-2 processing item 2 consumer-0 processing item 3 consumer-1 processing item 4 ... All tasks complete. ``` This question assesses the understanding of asyncio queues, coroutine creation, task management, and ensuring correct synchronization and processing flow using the asyncio framework.","solution":"import asyncio from typing import List async def producer(queue: asyncio.Queue, items: List[int]) -> None: Put a list of items into the provided queue. :param queue: an asyncio.Queue to put items into :param items: a list of items (integers) to be put into the queue for item in items: await queue.put(item) print(f\'Produced {item}\') async def consumer(queue: asyncio.Queue, name: str) -> None: Continuously get items from the queue until it is empty, and process them. Print the name of the consumer and the item being processed. :param queue: an asyncio.Queue to get items from :param name: a unique name identifier for the consumer while True: item = await queue.get() if item is None: break print(f\'{name} processing {item}\') queue.task_done() async def main() -> None: Main function to orchestrate the producer and consumer. # Create a FIFO queue with a max size of 10. queue = asyncio.Queue(maxsize=10) # List of items for the producer to add to the queue items = [i for i in range(20)] # Create producer task producer_task = asyncio.create_task(producer(queue, items)) # Create 3 consumer tasks consumer_tasks = [ asyncio.create_task(consumer(queue, f\'consumer-{i}\')) for i in range(3) ] # Wait until the producer has finished producing all items await producer_task # Wait until the queue is fully processed await queue.join() # Signal consumers to exit for _ in consumer_tasks: await queue.put(None) # Wait until all consumers are done await asyncio.gather(*consumer_tasks, return_exceptions=True) print(\'All tasks complete.\') # Event loop to run the main function asyncio.run(main())"},{"question":"# PyTorch Linear Algebra Assessment Problem Statement: You are provided with a matrix `A` and a vector `b`. Your task is to solve the linear system of equations `Ax = b` using PyTorch. Additionally, you need to compute the determinant of `A`, the condition number of `A`, and perform Singular Value Decomposition (SVD) on `A`. Implement a Python function using PyTorch for this task. Function Signature: ```python import torch def linear_algebra_solver(A: torch.Tensor, b: torch.Tensor) -> dict: Solves the linear system Ax = b, computes the determinant, condition number, and performs SVD on matrix A. Parameters: A (torch.Tensor): A 2D tensor representing the matrix A. b (torch.Tensor): A 1D tensor representing the vector b. Returns: dict: A dictionary with the following keys and values: - \'solution\': The solution vector x for the system Ax = b. - \'determinant\': The determinant of matrix A. - \'condition_number\': The condition number of matrix A. - \'svd\': A tuple containing the U, S, V matrices from the SVD of A. pass ``` Requirements: 1. Use `torch.linalg.solve` for solving the linear system `Ax = b`. 2. Use `torch.linalg.det` to compute the determinant of `A`. 3. Use `torch.linalg.cond` to compute the condition number of `A`. 4. Use `torch.linalg.svd` to compute the Singular Value Decomposition (SVD) of `A`. Constraints: - The input matrix `A` will be a square matrix (n x n) and `b` will be a vector of length n. - The functions used should handle the cases where the matrix is not invertible gracefully and raise appropriate errors if the linear system cannot be solved. Example: ```python A = torch.tensor([[3.0, 2.0], [1.0, 4.0]]) b = torch.tensor([6.0, 9.0]) result = linear_algebra_solver(A, b) print(result[\'solution\']) # Example Output: tensor([1.1200, 1.8200]) print(result[\'determinant\']) # Example Output: 10.0 print(result[\'condition_number\']) # Example Output: tensor([...]) print(result[\'svd\']) # Example Output: (tensor([[...],[...]]), tensor([...]), tensor([[...],[...]])) ``` Note: Ensure your function is robust and well-documented. Include appropriate error handling for cases where `A` is singular or ill-conditioned.","solution":"import torch def linear_algebra_solver(A: torch.Tensor, b: torch.Tensor) -> dict: Solves the linear system Ax = b, computes the determinant, condition number, and performs SVD on matrix A. Parameters: A (torch.Tensor): A 2D tensor representing the matrix A. b (torch.Tensor): A 1D tensor representing the vector b. Returns: dict: A dictionary with the following keys and values: - \'solution\': The solution vector x for the system Ax = b. - \'determinant\': The determinant of matrix A. - \'condition_number\': The condition number of matrix A. - \'svd\': A tuple containing the U, S, V matrices from the SVD of A. try: solution = torch.linalg.solve(A, b).tolist() except RuntimeError as e: solution = None determinant = torch.linalg.det(A).item() condition_number = torch.linalg.cond(A).item() U, S, V = torch.linalg.svd(A) svd = (U.tolist(), S.tolist(), V.tolist()) return { \'solution\': solution, \'determinant\': determinant, \'condition_number\': condition_number, \'svd\': svd }"},{"question":"**Objective:** Implementing and managing list operations in Python. **Question:** You are required to write several Python functions to perform specific list operations. These operations should mimic the behavior as described in the lower-level Python C API documentation. 1. **Create a new list of a given length with `None` as the placeholder for each element.** 2. **Get the size of the list.** 3. **Get an item at a specific index from the list. Raise an `IndexError` if the index is out of bounds.** 4. **Set an item at a specific index in the list. Raise an `IndexError` if the index is out of bounds.** 5. **Insert an item at a specific position in the list. If the index is out of bounds, append the item at the end.** 6. **Append an item at the end of the list.** 7. **Get a slice of the list between two indices.** 8. **Set a slice of the list between two indices to a new list.** 9. **Sort the list in place.** 10. **Reverse the list in place.** 11. **Convert the list to a tuple.** # Function Specifications: 1. `create_list(length) -> List`: Returns a new list of `length` with `None` as each element. 2. `get_size(lst) -> int`: Returns the size of the list `lst`. 3. `get_item(lst, index) -> Any`: Returns the item at the specified `index` from `lst`. Raises `IndexError` if the index is out of bounds. 4. `set_item(lst, index, item) -> None`: Sets the `item` at the specified `index` in `lst`. Raises `IndexError` if the index is out of bounds. 5. `insert_item(lst, index, item) -> None`: Inserts `item` at the specified `index` in `lst`. Appends the item if the index is out of bounds. 6. `append_item(lst, item) -> None`: Appends `item` at the end of `lst`. 7. `get_slice(lst, low, high) -> List`: Returns a slice of `lst` from `low` to `high`. 8. `set_slice(lst, low, high, new_lst) -> None`: Sets the slice of `lst` from `low` to `high` to `new_lst`. 9. `sort_list(lst) -> None`: Sorts `lst` in place. 10. `reverse_list(lst) -> None`: Reverses `lst` in place. 11. `list_to_tuple(lst) -> Tuple`: Converts `lst` to a tuple. **Constraints:** - Assume `lst` is always a list. - The `index`, `low`, and `high` values will be integers. - The `length` will be a non-negative integer. - Performance: Functions should handle lists of up to 10^6 elements efficiently. **Example:** ```python lst = create_list(5) assert get_size(lst) == 5 set_item(lst, 2, \'a\') assert get_item(lst, 2) == \'a\' insert_item(lst, 1, \'b\') assert get_item(lst, 1) == \'b\' append_item(lst, \'c\') assert get_item(lst, get_size(lst) - 1) == \'c\' assert get_slice(lst, 1, 3) == [\'b\', \'a\'] set_slice(lst, 0, 2, [\'x\', \'y\']) assert lst[:2] == [\'x\', \'y\'] sort_list(lst) assert lst == sorted(lst) reverse_list(lst) assert lst == list(reversed(lst)) assert list_to_tuple(lst) == tuple(lst) ``` Implement these functions ensuring they match the descriptions and constraints provided.","solution":"def create_list(length): Returns a new list of `length` with `None` as each element. return [None] * length def get_size(lst): Returns the size of the list `lst`. return len(lst) def get_item(lst, index): Returns the item at the specified `index` from `lst`. Raises `IndexError` if the index is out of bounds. if index < 0 or index >= len(lst): raise IndexError(\\"Index out of bounds\\") return lst[index] def set_item(lst, index, item): Sets the `item` at the specified `index` in `lst`. Raises `IndexError` if the index is out of bounds. if index < 0 or index >= len(lst): raise IndexError(\\"Index out of bounds\\") lst[index] = item def insert_item(lst, index, item): Inserts `item` at the specified `index` in `lst`. Appends the item if the index is out of bounds. if index < 0 or index > len(lst): lst.append(item) else: lst.insert(index, item) def append_item(lst, item): Appends `item` at the end of `lst`. lst.append(item) def get_slice(lst, low, high): Returns a slice of `lst` from `low` to `high`. return lst[low:high] def set_slice(lst, low, high, new_lst): Sets the slice of `lst` from `low` to `high` to `new_lst`. lst[low:high] = new_lst def sort_list(lst): Sorts `lst` in place. lst.sort() def reverse_list(lst): Reverses `lst` in place. lst.reverse() def list_to_tuple(lst): Converts `lst` to a tuple. return tuple(lst)"},{"question":"# PyTorch Coding Assessment Question Objective You are required to write a function that demonstrates the use of PyTorch\'s asynchronous execution with `torch.futures.Future`. You will create multiple asynchronous operations and manage them using `collect_all` and `wait_all`. Problem Statement Implement the function `async_operations` that performs the following tasks: 1. Accepts an integer `n` which specifies the number of asynchronous tasks to create. 2. Each asynchronous task should simply return the square of its index after a simulated delay. 3. Use `torch.futures.Future` to encapsulate each asynchronous task. 4. Utilize `collect_all` to gather all future objects. 5. Wait for all tasks to complete using `wait_all` and then return the results as a list. Input - `n` (integer): The number of asynchronous tasks to create. (1 ≤ n ≤ 100) Output - A list of integers where each integer is the square of its index. Example ```python result = async_operations(5) # Expected: [0, 1, 4, 9, 16] ``` Constraints - The function should handle up to 100 asynchronous tasks. - Use proper error handling to manage any asynchronous execution issues. - Ensure that the implementation is efficient and adheres to the constraints. Implementation Details - You must use `torch.futures.Future` and the utility functions `collect_all` and `wait_all`. - Use the `torch._C.Future` class to create future objects. - Use `time.sleep` to simulate delays in asynchronous tasks. Function Signature ```python import torch import time def async_operations(n: int) -> list: # Your code here ``` This question assesses your understanding of asynchronous programming with PyTorch\'s `torch.futures` and your ability to implement and manage asynchronous tasks using provided utility functions.","solution":"import torch import time def async_operations(n: int) -> list: futures = [] # Function to be executed asynchronously def task(i): time.sleep(0.1) # Simulated delay return i * i for i in range(n): fut = torch.futures.Future() fut.set_result(task(i)) futures.append(fut) # Collect all futures all_futures = torch.futures.collect_all(futures) # Wait for all futures to complete results = torch.futures.wait_all(futures) # Extract results from futures results = [fut.value() for fut in futures] return results"},{"question":"<|Analysis Begin|> The provided documentation details various functionalities related to managing and utilizing XPU devices (which could be AMD, Intel, etc.) in PyTorch via the `torch.xpu` module. The main components in the documentation are: 1. **Device Management**: Functions to get and set the current device, count the number of available devices, and acquire properties of these devices. 2. **Random Number Generation (RNG)**: Functions to get and set the random number generator state, seed the generator, and manage seeds. 3. **Streams and Events**: Tools for creating and managing streams and events to coordinate asynchronous tasks. 4. **Memory Management**: Functions to manage and inspect memory usage on XPU devices, including allocating and deallocating memory, and querying memory statistics. This information is sufficient to design a question that tests students on their knowledge of device management, memory management, and potentially RNG in PyTorch using XPUs. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective**: Implement and utilize functions from the `torch.xpu` module to assess understanding of device and memory management in PyTorch. # Problem Statement You are tasked with implementing a utility function and writing a script to analyze memory usage on available XPU devices. Specifically, you need to: 1. Implement a function `initialize_xpu_device` that initializes the first available XPU device, sets it as the current device, and returns the device properties. 2. Implement a function `allocate_memory_on_xpu` that initializes a tensor on the XPU device of a given size and data type, and returns the current memory usage statistics. 3. Write a script that uses the above functions to: * Initialize the XPU device and print its properties. * Allocate a tensor of random values (using `torch.rand`) of size (1000 times 1000) and type `float32` on the initialized XPU device. * Print memory usage before and after the tensor allocation. # Function Signatures ```python def initialize_xpu_device() -> dict: Initializes the first available XPU device, sets it as the current device, and returns the device properties as a dictionary. Returns: dict: The properties of the initialized XPU device. pass def allocate_memory_on_xpu(size: tuple, dtype: torch.dtype) -> dict: Allocates a tensor of specified size and dtype on the XPU device and returns the current memory usage statistics. Args: size (tuple): The size of the tensor to allocate. dtype (torch.dtype): The data type of the tensor. Returns: dict: Memory usage statistics after allocation. pass ``` # Constraints and Details 1. Use functions from the `torch.xpu` and `torch` modules only. 2. The properties and memory statistics should be returned as dictionaries with meaningful keys and values. 3. Ensure proper handling and initialization of the XPU device. 4. You can assume that at least one XPU device is available and that the required functions from `torch.xpu` are operational. # Expected Output for the Script The script should output: 1. Device properties (such as name, capability, etc.). 2. Memory statistics before and after tensor allocation. Example (Pseudo-outputs): ``` Device Properties: {\'name\': \'XPU Device 0\', \'capability\': (8, 6), ...} Memory Before Allocation: {\'memory_allocated\': 1024, \'max_memory_allocated\': 2048, ...} Memory After Allocation: {\'memory_allocated\': 4001024, \'max_memory_allocated\': 4002048, ...} ``` Implement the required functions and script to meet the above specifications.","solution":"import torch def initialize_xpu_device() -> dict: Initializes the first available XPU device, sets it as the current device, and returns the device properties as a dictionary. Returns: dict: The properties of the initialized XPU device. if torch.xpu.is_available(): device = torch.device(\\"xpu:0\\") torch.xpu.set_device(device) props = torch.xpu.get_device_properties(device) return { \\"name\\": props.name, \\"total_memory\\": props.total_memory, \\"multi_processor_count\\": props.multi_processor_count } else: raise RuntimeError(\\"No XPU device available\\") def allocate_memory_on_xpu(size: tuple, dtype: torch.dtype) -> dict: Allocates a tensor of specified size and dtype on the XPU device and returns the current memory usage statistics. Args: size (tuple): The size of the tensor to allocate. dtype (torch.dtype): The data type of the tensor. Returns: dict: Memory usage statistics after allocation. tensor = torch.rand(size, dtype=dtype, device=\\"xpu\\") memory_stats = { \\"memory_allocated\\": torch.xpu.memory_allocated(), \\"max_memory_allocated\\": torch.xpu.max_memory_allocated(), \\"memory_reserved\\": torch.xpu.memory_reserved(), \\"max_memory_reserved\\": torch.xpu.max_memory_reserved() } return memory_stats if __name__ == \\"__main__\\": # Initialize XPU device and print its properties device_properties = initialize_xpu_device() print(\\"Device Properties:\\", device_properties) # Allocate tensor on XPU and print memory statistics before and after allocation before_allocation = allocate_memory_on_xpu((0, ), torch.float32) # Testing empty allocation to get memory stats print(\\"Memory Before Allocation:\\", before_allocation) # Allocate meaningful tensor and print memory usage stats after allocation after_allocation = allocate_memory_on_xpu((1000, 1000), torch.float32) print(\\"Memory After Allocation:\\", after_allocation)"},{"question":"# Question: Optimizing PyTorch Model Using FX Graph Transformations You are given a PyTorch computation graph, and your task is to optimize this graph by applying a series of transformations. Specifically, you will: 1. Replace all instances of `torch.ops.aten.add.Tensor` with `torch.ops.aten.sub.Tensor`. 2. Insert `torch.ops.aten.relu.default` after each `torch.ops.aten.sub.Tensor` operation. 3. Remove any `torch.ops.aten.detach.default` operations from the graph. You are provided with a PyTorch `GraphModule`, and you must implement a function `optimize_graph` which takes the `GraphModule` as an input and returns an optimized `GraphModule`. # Instructions: 1. **Input**: A PyTorch `GraphModule` containing the computation graph. 2. **Output**: A new optimized `GraphModule` with specified transformations applied. ```python import torch import torch.fx def optimize_graph(gm: torch.fx.GraphModule) -> torch.fx.GraphModule: # Apply transformations here # Transformation 1: Replace add with sub class ReplaceAddWithSub(torch.fx.Transformer): def call_function(self, target, args, kwargs): if target == torch.ops.aten.add.Tensor: target = torch.ops.aten.sub.Tensor return super().call_function(target, args, kwargs) gm = ReplaceAddWithSub(gm).transform() # Transformation 2: Insert ReLU after sub for node in gm.graph.nodes: if node.op == \\"call_function\\" and node.target == torch.ops.aten.sub.Tensor: with gm.graph.inserting_after(node): new_relu_node = gm.graph.call_function(torch.ops.aten.relu.default, args=(node,)) node.replace_all_uses_with(new_relu_node) # Transformation 3: Remove detach operations class RemoveDetachPass(torch.fx.Transformer): def call_function(self, target, args, kwargs): if target == torch.ops.aten.detach.default: return args[0] return super().call_function(target, args, kwargs) gm = RemoveDetachPass(gm).transform() return gm # Example Usage class MyModel(torch.nn.Module): def forward(self, x, y): z = x + y z = z.detach() return z # Trace the model model = MyModel() traced_model = torch.fx.symbolic_trace(model) # Apply optimization optimized_model = optimize_graph(traced_model) print(optimized_model) ``` # Constraints: - Ensure your transformations maintain the original logic of the computation graph as closely as possible. - The modified graph should not introduce any performance regressions or alter the output semantics of the model. # Notes: - Make use of the `torch.fx.Transformer` class to simplify the manipulation of the graph. - Test your implementation thoroughly with different models to ensure the transformations work as expected.","solution":"import torch import torch.fx def optimize_graph(gm: torch.fx.GraphModule) -> torch.fx.GraphModule: # Transformation 1: Replace add with sub class ReplaceAddWithSub(torch.fx.Transformer): def call_function(self, target, args, kwargs): if target == torch.ops.aten.add.Tensor: target = torch.ops.aten.sub.Tensor return super().call_function(target, args, kwargs) gm = ReplaceAddWithSub(gm).transform() # Transformation 2: Insert ReLU after sub for node in gm.graph.nodes: if node.op == \\"call_function\\" and node.target == torch.ops.aten.sub.Tensor: with gm.graph.inserting_after(node): new_relu_node = gm.graph.call_function(torch.ops.aten.relu.default, args=(node,)) node.replace_all_uses_with(new_relu_node) # Ensure that the original node is not replaced in ReLU\'s inputs new_relu_node.args = (node,) gm.recompile() # Transformation 3: Remove detach operations class RemoveDetachPass(torch.fx.Transformer): def call_function(self, target, args, kwargs): if target == torch.ops.aten.detach.default: return args[0] return super().call_function(target, args, kwargs) gm = RemoveDetachPass(gm).transform() return gm # Example Usage class MyModel(torch.nn.Module): def forward(self, x, y): z = x + y z = z.detach() return z # Trace the model model = MyModel() traced_model = torch.fx.symbolic_trace(model) # Apply optimization optimized_model = optimize_graph(traced_model) print(optimized_model)"},{"question":"# Seaborn Visualization Task You are given a dataset, `exercise`, which contains exercise data with the following columns: - `id`: Identifier for the individual. - `time`: Time in minutes. - `pulse`: Pulse rate. - `diet`: Type of diet (e.g., \'low\', \'med\', \'high\'). - `kind`: Type of exercise (e.g., \'running\', \'walking\'). - `team`: The team to which the individual belongs. Your task is to create a relational plot using seaborn\'s `relplot` with the following specifications: 1. You should create a line plot with `time` on the x-axis and `pulse` on the y-axis. 2. Use `diet` for the hue semantic mapping to differentiate between the types of diet. 3. Use `team` for the style semantic mapping. 4. Create facets for each type of exercise (i.e., use `col` for the `kind` variable). 5. Ensure that each facet has a height of 5 and an aspect ratio of 1.2. 6. Customize the facets to show grid lines, set y-axis label as \\"Pulse Rate\\", and set appropriate titles indicating the type of exercise. # Implementation Write a function `exercise_plot(data: pd.DataFrame) -> None` that accomplishes the above visualization task. Make sure to include any necessary imports and setting configurations within the function. Input - `data` (pd.DataFrame): The exercise dataset. Output - The function should not return anything but display the plot. Example ```python import pandas as pd # Example data data = pd.DataFrame({ \'id\': [1, 1, 2, 2, 3, 3], \'time\': [5, 10, 5, 10, 5, 10], \'pulse\': [90, 95, 85, 88, 75, 80], \'diet\': [\'low\', \'low\', \'med\', \'med\', \'high\', \'high\'], \'kind\': [\'running\', \'running\', \'walking\', \'walking\', \'running\', \'running\'], \'team\': [\'A\', \'A\', \'B\', \'B\', \'C\', \'C\'] }) exercise_plot(data) ``` # Constraints - Ensure the plot is sufficiently large for clarity. - Handle any potential missing values in the dataset gracefully.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def exercise_plot(data: pd.DataFrame) -> None: Creates a relational line plot using seaborn with specified aesthetics and facets. Parameters: data (pd.DataFrame): The exercise dataset. Returns: None # Create a seaborn relational line plot with the specified requirements g = sns.relplot( data=data, x=\\"time\\", y=\\"pulse\\", kind=\\"line\\", hue=\\"diet\\", style=\\"team\\", col=\\"kind\\", height=5, aspect=1.2, facet_kws={\'sharey\': False, \'sharex\': True} ) # Customize the plot with grid lines and titles for ax, title in zip(g.axes.flat, g.col_names): ax.grid(True) ax.set_title(title.capitalize() + \' Exercise\') ax.set_ylabel(\\"Pulse Rate\\") # Show plot plt.show()"},{"question":"**Coding Assessment Question** # Objective Your task is to implement a custom module loader that demonstrates the use of the deprecated `imp` module functionalities. You will need to write a function that uses the `imp` module to import a specified Python source file dynamically, reload it, and handle byte-compiled paths. # Instructions 1. Implement the function `custom_importer(module_name: str, module_path: str) -> types.ModuleType`. 2. This function should: - Use `imp.find_module` to locate the module based on the provided `module_name` and `module_path`. - Use `imp.load_module` to load the module. - Implement a mechanism to reload the module using `imp.reload`. - Use `imp.cache_from_source` to get the path to the byte-compiled file. - Handle any exceptions that occur during these processes gracefully, providing meaningful error messages. # Input - `module_name` (str): The name of the module to be imported. - `module_path` (str): The path to the module file. # Output - Returns the loaded module object. # Constraints - The `module_name` should be a valid Python module name. - The `module_path` should be a valid path to a `.py` file. - The module should not have any hierarchical names (i.e., no dots in the module name). # Example ```python import types def custom_importer(module_name: str, module_path: str) -> types.ModuleType: import imp import os import sys # Check if the file exists if not os.path.isfile(module_path): raise FileNotFoundError(f\\"The file {module_path} does not exist\\") # Find and load the module try: fp, pathname, description = imp.find_module(module_name, [os.path.dirname(module_path)]) module = imp.load_module(module_name, fp, pathname, description) finally: if fp: fp.close() # Reload the module module = imp.reload(module) # Get the path to the byte-compiled file bytecode_path = imp.cache_from_source(module_path) print(f\\"Bytecode path: {bytecode_path}\\") return module # Example usage: # Assuming there is a \'mymodule.py\' file in the current directory module = custom_importer(\'mymodule\', \'./mymodule.py\') print(module) ``` # Notes - Keep in mind that the `imp` module is deprecated. You should use `importlib` for any modern code outside this exercise. - Ensure that you handle file operations securely and close file handlers properly to avoid resource leaks.","solution":"import types def custom_importer(module_name: str, module_path: str) -> types.ModuleType: import imp import os import sys # Check if the file exists if not os.path.isfile(module_path): raise FileNotFoundError(f\\"The file {module_path} does not exist\\") # Find and load the module fp = None try: fp, pathname, description = imp.find_module(module_name, [os.path.dirname(module_path)]) module = imp.load_module(module_name, fp, pathname, description) finally: if fp: fp.close() # Reload the module module = imp.reload(module) # Get the path to the byte-compiled file bytecode_path = imp.cache_from_source(module_path) print(f\\"Bytecode path: {bytecode_path}\\") return module"},{"question":"Objective Your task is to demonstrate proficiency in using scikit-learn by performing data preprocessing, model training, and evaluation. You will work with a synthetic dataset that you generate yourself. Task 1. Generate a synthetic regression dataset using `make_regression` with the following specifications: - Number of samples: 1000 - Number of features: 20 - Noise: 0.1 2. Preprocess the dataset: - Normalize the feature data using `StandardScaler`. 3. Split the dataset into training and testing sets: - Use 80% of the data for training and 20% for testing. - Set `random_state` to 42 for reproducibility. 4. Train a `GradientBoostingRegressor` model on the training data with the following parameters: - `n_estimators`: 100 - `learning_rate`: 0.1 - `max_depth`: 3 - `random_state`: 42 5. Evaluate the model: - Calculate and print the R² score on the test data. Requirements - Use scikit-learn for generating the dataset, preprocessing, model training, and evaluation. - The code should be well-structured and include all necessary import statements. - The dataset generation, splitting, preprocessing, training, and evaluation should be done within a single Python script. Expected Input and Output - **Input**: No external input. The script generates the synthetic dataset. - **Output**: The R² score of the model on the test data. You can use the following template to get started: ```python import numpy as np import pandas as pd from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import r2_score # Step 1: Generate synthetic regression dataset X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42) # Step 2: Preprocess the dataset scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 3: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Step 4: Train the GradientBoostingRegressor model model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42) model.fit(X_train, y_train) # Step 5: Evaluate the model y_pred = model.predict(X_test) r2 = r2_score(y_test, y_pred) print(f\\"R² score on the test data: {r2}\\") ```","solution":"import numpy as np from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import r2_score def generate_and_evaluate_model(): # Step 1: Generate synthetic regression dataset X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42) # Step 2: Preprocess the dataset scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 3: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Step 4: Train the GradientBoostingRegressor model model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42) model.fit(X_train, y_train) # Step 5: Evaluate the model y_pred = model.predict(X_test) r2 = r2_score(y_test, y_pred) return r2"},{"question":"# Advanced Programming Question: Implementing an I/O Multiplexer Objective Create a multi-platform I/O multiplexer class in Python that uses the most efficient method available on the operating system to wait for multiple file descriptors to become ready for some class of I/O operation (e.g., available for reading or writing). Requirements 1. **Class Definition**: - Define a class `IOMultiplexer`. 2. **Methods**: - `__init__(self)`: Initializes the internal data structures. - `register(self, fd, eventmask)`: Register a file descriptor for a specific event (e.g., ready for reading, writing). - `unregister(self, fd)`: Unregister a file descriptor. - `poll(self, timeout=None)`: Poll the registered file descriptors for events. Should return a list of tuples `(fd, eventmask)`. 3. **Implementation Details**: - The implementation should use `select()`, `poll()`, or `epoll()` based on the operating system\'s capabilities. - If none of these methods are available, the class should raise a `NotImplementedError`. 4. **Supported Event Masks**: - `POLLIN`: There is data to read. - `POLLOUT`: Writing is possible without blocking. - `POLLERR`: Error condition. - `POLLHUP`: Hang up. Input Format - `register`: - `fd` can be an integer representing a file descriptor or an object with a `fileno()` method. - `eventmask` is an integer mask consisting of the supported event masks. - `unregister`: - `fd` can be an integer or an object with a `fileno()` method. - `poll`: - `timeout` is an optional floating point number representing the timeout in seconds. Output Format - `poll` method should return a list of tuples `(fd, eventmask)`. Constraints - The solution should handle Unix-based systems (Linux, BSD) and Windows gracefully. - You are not required to handle regular files, only sockets or pipes. - Ensure the solution is efficient and makes appropriate use of the available system calls. Example Usage ```python import socket class IOMultiplexer: def __init__(self): # Implementation here def register(self, fd, eventmask): # Implementation here def unregister(self, fd): # Implementation here def poll(self, timeout=None): # Implementation here # Example usage s1 = socket.socket() s2 = socket.socket() mux = IOMultiplexer() mux.register(s1, POLLIN) mux.register(s2, POLLOUT) events = mux.poll(timeout=5.0) for fd, event in events: if event & POLLIN: print(f\\"Socket {fd} is ready for reading\\") if event & POLLOUT: print(f\\"Socket {fd} is ready for writing\\") ``` Hints - You may need to use different modules (e.g., `os`, `selectors`) for different methods. - Consider using exception handling to differentiate between available methods and fallback strategies. - Review the `selectors` module to find abstractions that may simplify implementation.","solution":"import selectors import socket POLLIN = selectors.EVENT_READ POLLOUT = selectors.EVENT_WRITE POLLERR = 4 # This is handled internally by the OS POLLHUP = 16 # This is handled internally by the OS class IOMultiplexer: def __init__(self): self.selector = selectors.DefaultSelector() def register(self, fd, eventmask): if hasattr(fd, \'fileno\'): fd = fd.fileno() self.selector.register(fd, eventmask) def unregister(self, fd): if hasattr(fd, \'fileno\'): fd = fd.fileno() self.selector.unregister(fd) def poll(self, timeout=None): events = self.selector.select(timeout) return [(key.fd, mask) for key, mask in events]"},{"question":"**Question: Feature Extraction and Transformation with Scikit** You are tasked with building a preprocessing pipeline for a machine learning model that can handle a mixed dataset consisting of both text and structured categorical and numerical data. The goal is to transform the input data into a numerical format suitable for model training. Consider the following dataset: ```python dataset = [ { \\"id\\": \\"001\\", \\"review\\": \\"The product quality is excellent. Highly recommend!\\", \\"city\\": \\"New York\\", \\"rating\\": 5, \\"categories\\": [\\"electronics\\", \\"gadgets\\"] }, { \\"id\\": \\"002\\", \\"review\\": \\"Not satisfied with the product. Battery life is too short.\\", \\"city\\": \\"San Francisco\\", \\"rating\\": 2, \\"categories\\": [\\"electronics\\", \\"battery\\"] }, { \\"id\\": \\"003\\", \\"review\\": \\"Average product, acceptable for the price.\\", \\"city\\": \\"Los Angeles\\", \\"rating\\": 3, \\"categories\\": [\\"gadgets\\"] } ] ``` **Tasks:** 1. **Text Feature Extraction**: - Implement a function `extract_text_features` that takes a list of product reviews (strings) and returns their TF-IDF representations. 2. **Categorical and Numerical Feature Extraction**: - Implement a function `extract_categorical_features` that takes a list of dictionaries (each representing a product) and returns their one-hot encoded categorical features and numerical features as a combined array. 3. **Feature Hashing**: - Implement a function `apply_feature_hashing` that takes a list of product categories and transforms them using `FeatureHasher`. Use `\'string\'` as the `input_type`. 4. **Pipeline Integration**: - Combine the above steps into a single function `preprocess_data` that takes the entire dataset and returns a combined feature matrix. **Details:** - Assume `dataset` is provided as input. The `review` field should be processed using TF-IDF, `city` should be one-hot encoded using `DictVectorizer`, `rating` should be treated as a numerical feature, and `categories` should be processed using `FeatureHasher`. - Ensure that the final feature matrix (returned by `preprocess_data`) is in a format suitable for consumption by a scikit-learn estimator. **Expected Output:** The functions should transform the provided dataset appropriately and return a combined feature matrix. ```python def extract_text_features(reviews): # Implement TF-IDF extraction here pass def extract_categorical_features(data): # Implement DictVectorizer for categorical data and numerical feature extraction here pass def apply_feature_hashing(categories): # Implement FeatureHasher transformation here pass def preprocess_data(dataset): # Combine the above steps here pass # Example Usage reviews = [item[\\"review\\"] for item in dataset] text_features = extract_text_features(reviews) categorical_data = [{\\"city\\": item[\\"city\\"], \\"rating\\": item[\\"rating\\"]} for item in dataset] categorical_features = extract_categorical_features(categorical_data) categories = [item[\\"categories\\"] for item in dataset] hashed_features = apply_feature_hashing(categories) # Assuming preprocess_data integrates all steps: combined_features = preprocess_data(dataset) print(combined_features) ``` **Constraints:** 1. Implement efficient and scalable feature transformation. 2. Ensure the preprocessing steps are robust and can handle a variety of similar datasets. 3. Clearly document assumptions and choices made in the code.","solution":"from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.feature_extraction import DictVectorizer, FeatureHasher import numpy as np from scipy.sparse import hstack def extract_text_features(reviews): Extract TF-IDF features from a list of reviews. vectorizer = TfidfVectorizer() text_features = vectorizer.fit_transform(reviews) return text_features def extract_categorical_features(data): Extract one-hot encoded features from categorical data and numerical features. vec = DictVectorizer(sparse=True) categorical_features = vec.fit_transform(data) return categorical_features def apply_feature_hashing(categories): Hash categories into a fixed number of features using FeatureHasher. hasher = FeatureHasher(input_type=\'string\', n_features=20) flat_categories = [item for sublist in categories for item in sublist] hashed_features = hasher.transform([sublist for sublist in categories]) return hashed_features def preprocess_data(dataset): Preprocess the entire dataset to obtain a combined feature matrix. # Extract reviews reviews = [item[\\"review\\"] for item in dataset] text_features = extract_text_features(reviews) # Extract categorical and numerical features categorical_data = [{\\"city\\": item[\\"city\\"], \\"rating\\": item[\\"rating\\"]} for item in dataset] categorical_features = extract_categorical_features(categorical_data) # Extract hashed features categories = [item[\\"categories\\"] for item in dataset] hashed_features = apply_feature_hashing(categories) # Combine all features into one feature matrix combined_features = hstack([text_features, categorical_features, hashed_features]) return combined_features"},{"question":"# Question: Advanced Method Handling You have been given a custom class framework and your task is to implement several functionalities that utilize instance methods and methods. The given class framework allows for the creation and manipulation of instance method and method objects in a user-defined class. Tasks 1. **Implement a method** `create_instance_method(func)` that takes a single input function `func` and returns an instance method object using `PyInstanceMethod_New`. 2. **Create a class** `CustomClass` with the following: - An initialization method `__init__(self)` which initializes an empty list `methods`. - A method `add_method(self, method)` which adds a method object to the list. - A method `get_method_function(self, method_name)` which returns the function associated with the method whose name matches `method_name`. - A method `call_method(self, method_name, *args, **kwargs)` which calls the method with the given name `method_name` with the provided arguments `args` and keyword arguments `kwargs`. 3. **Implement the method** `is_instance_method(obj)` that checks if the given `obj` is an instance method object using `PyInstanceMethod_Check`. 4. **Implement the method** `is_method(obj)` that checks if the given `obj` is a method object using `PyMethod_Check`. Input and Output Formats - The `create_instance_method(func)` function takes a callable `func` as input and returns a new instance method object. - The class `CustomClass` contains methods to manipulate and call methods. - The `add_method(self, method)` method stores the method object in an internal list. - The `get_method_function(self, method_name)` returns the function associated with the specified method name in the internal list. - The `call_method(self, method_name, *args, **kwargs)` dynamically calls the specified method. - The `is_instance_method(obj)` function checks if the input object is an instance method object. - The `is_method(obj)` function checks if the input object is a method object. # Example Usage ```python # Step 1: Create an instance method def test_func(): return \\"Hello, World!\\" instance_method = create_instance_method(test_func) # Step 2: Create CustomClass and add methods class CustomClass: def __init__(self): self.methods = [] def add_method(self, method): self.methods.append(method) def get_method_function(self, method_name): for meth in self.methods: if meth.__name__ == method_name: return PyMethod_Function(meth) return None def call_method(self, method_name, *args, **kwargs): method_function = self.get_method_function(method_name) if method_function: return method_function(*args, **kwargs) else: raise ValueError(\\"Method not found.\\") # Step 3: Check instance method and method objects instance_method_check = is_instance_method(instance_method) method_check = is_method(instance_method) ``` In this question, we expect students to demonstrate their ability to work with instance methods and bound methods, manage method objects within a class, and utilize Python\'s low-level functions for a better understanding of how methods are handled internally.","solution":"import types def create_instance_method(func): Creates an instance method object for the given function. return types.MethodType(func, object()) class CustomClass: def __init__(self): self.methods = [] def add_method(self, method): self.methods.append(method) def get_method_function(self, method_name): for method in self.methods: if method.__func__.__name__ == method_name: return method.__func__ return None def call_method(self, method_name, *args, **kwargs): method_func = self.get_method_function(method_name) if method_func: return method_func(self, *args, **kwargs) else: raise ValueError(\\"Method not found.\\") def is_instance_method(obj): Checks if the object is an instance method. return isinstance(obj, types.MethodType) def is_method(obj): Checks if the object is a method object. return isinstance(obj, (types.FunctionType, types.MethodType))"},{"question":"Coding Assessment Question # Objective: Implement a Python function that dynamically constructs an email message using the `EmailMessage` class from the `email.message` module. The function should demonstrate proficiency with handling headers, multipart content, and attachments. # Task: Write a function `create_multipart_email` that takes the following parameters: - `from_addr` (str): The sender\'s email address. - `to_addrs` (list of str): A list of recipients\' email addresses. - `subject` (str): The subject of the email. - `plain_text` (str): The plain text content of the email. - `html_text` (str): The HTML content of the email (optional). - `attachments` (list of tuples): Each tuple contains a filename (str) and its binary data (bytes). The function should: 1. Create an `EmailMessage` instance. 2. Set the `From`, `To`, and `Subject` headers. 3. Add the plain text content as the body. 4. If `html_text` is provided, add it as an `alternative` part. 5. Add each attachment with the filename specified in the tuple, marked with `Content-Disposition: attachment`. 6. Return the serialized email message as a string. # Constraints: - Assume the email addresses are valid. - Use MIME type `application/octet-stream` for the attachments. - Each attachment\'s filename will be unique. # Implementation: The implementation must use the provided methods of the `EmailMessage` class such as: - `set_content()` - `add_alternative()` - `add_attachment()` - `as_string()` # Example Usage: ```python def create_multipart_email(from_addr, to_addrs, subject, plain_text, html_text=None, attachments=[]): # Implementation here # Example: from_addr = \'sender@example.com\' to_addrs = [\'recipient1@example.com\', \'recipient2@example.com\'] subject = \'Test Email\' plain_text = \'This is a plain text body.\' html_text = \'<p>This is an <b>HTML</b> body.</p>\' attachments = [ (\'example.txt\', b\'This is some example text file content.\'), (\'image.png\', b\'x89PNGrnx1anx00x00x00...\') ] email_message = create_multipart_email(from_addr, to_addrs, subject, plain_text, html_text, attachments) print(email_message) ``` # Expected Output: The output should be a string representing a properly formatted MIME email with the given content and attachments, suitable for sending via an SMTP server.","solution":"from email.message import EmailMessage def create_multipart_email(from_addr, to_addrs, subject, plain_text, html_text=None, attachments=[]): Create a multipart email message. msg = EmailMessage() msg[\'From\'] = from_addr msg[\'To\'] = \', \'.join(to_addrs) msg[\'Subject\'] = subject msg.set_content(plain_text) if html_text: msg.add_alternative(html_text, subtype=\'html\') for filename, filecontent in attachments: msg.add_attachment(filecontent, maintype=\'application\', subtype=\'octet-stream\', filename=filename) return msg.as_string()"},{"question":"**Question: Implement a Neural Network Classifier using Scikit-learn** You are given a dataset comprising features and labels related to a binary classification task. Your task is to implement a multi-layer perceptron classifier using Scikit-learn\'s MLPClassifier. You will need to preprocess the data, train the classifier, and evaluate its performance. **Requirements:** 1. **Data Preprocessing:** - Load the dataset from the provided path. Assume you have a CSV file where the last column represents the binary labels (0 or 1), and the rest of the columns represent the features. - Split the data into training and testing sets using an 80-20 split. - Standardize the feature values using scikit-learn\'s `StandardScaler`. 2. **Model Implementation:** - Initialize the `MLPClassifier` with the following parameters: - `hidden_layer_sizes=(10, 5)` (i.e., two hidden layers with 10 and 5 neurons, respectively). - `solver=\'adam\'` for the optimizer. - `alpha=1e-4` for the L2 regularization term. - `max_iter=300` for the maximum number of iterations. 3. **Model Training:** - Fit the model to the training data. 4. **Model Evaluation:** - Predict the labels for the test data. - Calculate and print out the accuracy of the classifier on the test data. - Print out the confusion matrix of the test data predictions. **Expected Function Signature:** ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score, confusion_matrix def train_and_evaluate_mlp_classifier(data_path: str) -> None: # Load dataset data = pd.read_csv(data_path) X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # Split dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Initialize MLPClassifier clf = MLPClassifier(hidden_layer_sizes=(10, 5), solver=\'adam\', alpha=1e-4, max_iter=300, random_state=42) # Train the model clf.fit(X_train, y_train) # Predict on test data y_pred = clf.predict(X_test) # Evaluate accuracy and print confusion matrix accuracy = accuracy_score(y_test, y_pred) cm = confusion_matrix(y_test, y_pred) print(f\\"Accuracy: {accuracy:.4f}\\") print(\\"Confusion Matrix:\\") print(cm) # Example usage (you can replace \'data.csv\' with the actual path to your dataset) # train_and_evaluate_mlp_classifier(\'data.csv\') ``` **Constraints:** - Ensure the dataset is appropriately split and standardized. - Ensure proper handling of random states for reproducibility. **Performance Requirements:** - The model should achieve an accuracy of at least 80% on the test data.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score, confusion_matrix def train_and_evaluate_mlp_classifier(data_path: str) -> None: # Load dataset data = pd.read_csv(data_path) X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # Split dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Initialize MLPClassifier clf = MLPClassifier(hidden_layer_sizes=(10, 5), solver=\'adam\', alpha=1e-4, max_iter=300, random_state=42) # Train the model clf.fit(X_train, y_train) # Predict on test data y_pred = clf.predict(X_test) # Evaluate accuracy and print confusion matrix accuracy = accuracy_score(y_test, y_pred) cm = confusion_matrix(y_test, y_pred) print(f\\"Accuracy: {accuracy:.4f}\\") print(\\"Confusion Matrix:\\") print(cm) # Example usage (replace \'data.csv\' with the actual path to your dataset) # train_and_evaluate_mlp_classifier(\'data.csv\')"},{"question":"**Title**: Asynchronously Monitor and Control Multiple Subprocesses **Problem Statement**: You are tasked with creating a Python function that asynchronously runs multiple shell commands in parallel and collects their outputs. Each command should run as a subprocess and your function should ensure proper handling of standard output and error streams. **Function Signature**: ```python import asyncio async def run_commands(commands): Run multiple shell commands asynchronously and collect their outputs. Args: - commands (list of str): List of shell commands to execute. Returns: - dict: A dictionary where keys are commands and values are tuples (stdout, stderr, returncode). pass ``` **Description**: 1. **Input**: - `commands`: A list of strings where each string is a shell command to execute. 2. **Output**: - A dictionary where each key is a command string from the input list, and the value is a tuple containing: - `stdout` (str): The standard output from the command execution decoded into a string. - `stderr` (str): The standard error from the command execution decoded into a string. - `returncode` (int): The return code from the command execution. 3. **Constraints**: - You must use `asyncio.create_subprocess_shell` for executing the commands. - Handle potential errors and deadlocks appropriately by managing subprocess streams using `await proc.communicate()`. - Ensure that all subprocesses are managed asynchronously and run in parallel. - The solution needs to be efficient and handle multiple commands without unnecessary delays. **Example**: ```python import asyncio async def run_commands(commands): async def run(cmd): proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await proc.communicate() return stdout.decode(), stderr.decode(), proc.returncode tasks = [run(cmd) for cmd in commands] results = await asyncio.gather(*tasks) return {cmd: result for cmd, result in zip(commands, results)} # Example usage: commands = [\\"ls /\\", \\"echo Hello World\\", \\"nonexistent_command\\"] results = asyncio.run(run_commands(commands)) print(results) ``` **Expected Output**: ```python { \\"ls /\\": (\\"binnbootndevnetcnhomen...\\", \\"\\", 0), \\"echo Hello World\\": (\\"Hello Worldn\\", \\"\\", 0), \\"nonexistent_command\\": (\\"\\", \\"sh: 1: nonexistent_command: not foundn\\", 127) } ``` The expected output should showcase the standard output and error for each command, along with their respective return codes. Use this example and format to verify that your implementation correctly handles the requirements.","solution":"import asyncio async def run_commands(commands): Run multiple shell commands asynchronously and collect their outputs. Args: - commands (list of str): List of shell commands to execute. Returns: - dict: A dictionary where keys are commands and values are tuples (stdout, stderr, returncode). async def run(cmd): proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await proc.communicate() return stdout.decode(), stderr.decode(), proc.returncode tasks = [run(cmd) for cmd in commands] results = await asyncio.gather(*tasks) return {cmd: result for cmd, result in zip(commands, results)}"},{"question":"**Objective:** Create a customized data processing function utilizing Scikit-learn\'s utility functions to handle input validation, random state management, and sparse matrix operations. Problem Statement Your task is to implement a function `custom_data_processor` that will take three inputs: a data matrix `X`, a target vector `y`, and a random state seed. The function should perform the following tasks: 1. **Validation:** - Ensure that `X` is a 2D array and `y` is compatible in length with `X`. - Check that the data matrix `X` contains no NaNs or infinite values. 2. **Sparse Matrix Handling:** - If `X` is a sparse matrix, normalize its rows to unit L2 norm. 3. **Random Sampling:** - Shuffle the rows of `X` and the corresponding entries in `y` using the provided random state seed to ensure reproducibility. Specifications - **Function Signature:** ```python def custom_data_processor(X, y, random_state=0): pass ``` - **Input:** - `X`: A 2D array or sparse matrix of shape (n_samples, n_features). - `y`: A 1D array or list of labels with length equal to the number of samples in `X`. - `random_state`: An integer or `None` (default is 0), used to seed the random number generator for shuffling. - **Output:** - A tuple `(X_processed, y_processed)` where: - `X_processed` is the processed and validated 2D array or sparse matrix. - `y_processed` is the shuffled target vector. Constraints: - Use the appropriate functionalities from `sklearn.utils` for validation and normalization. - Ensure that the random state management is consistent and allows reproducibility of the shuffling process. Example Usage ```python from scipy.sparse import csr_matrix X = csr_matrix([[1, 2], [3, 0], [0, 4]]) y = [0, 1, 2] random_state = 42 X_processed, y_processed = custom_data_processor(X, y, random_state) print(X_processed.toarray()) print(y_processed) ``` Expected Output: ``` (array([[0.24253563, 0.9701425 ], [0. , 1. ], [0.83205029, 0.5547002 ]]), array([2, 0, 1])) ``` **Note:** - Ensure that the function handles both dense and sparse matrices appropriately. - Include necessary imports from `sklearn.utils` and ensure that other required libraries (like numpy and scipy) are imported. # Submission Instructions: - Submit a single Python file containing the `custom_data_processor` function. - Include test cases that demonstrate the function\'s correctness on both dense and sparse input matrices.","solution":"import numpy as np from scipy import sparse from sklearn.utils import check_array, check_consistent_length, shuffle from sklearn.preprocessing import normalize def custom_data_processor(X, y, random_state=0): # Validate input array X and target y X = check_array(X, accept_sparse=True, ensure_2d=True, allow_nd=False, force_all_finite=True) check_consistent_length(X, y) # Convert y to a numpy array if it\'s a list y = np.array(y) # Handle sparse matrix normalization if sparse.issparse(X): X = normalize(X, norm=\'l2\', axis=1) # Shuffle X and y with given random state X_shuffled, y_shuffled = shuffle(X, y, random_state=random_state) return X_shuffled, y_shuffled"},{"question":"**Mock Objects in Unit Testing** **Problem Statement:** You are tasked with writing unit tests for a class called `PaymentProcessor` that interacts with external services to process transactions. These services are not always available during testing, so you\'ll use `unittest.mock` to simulate the interactions. # Class to Test: ```python import requests class PaymentProcessor: def __init__(self, api_url): self.api_url = api_url def process_payment(self, account_id, amount): payload = { \'account_id\': account_id, \'amount\': amount } response = requests.post(self.api_url + \'/pay\', json=payload) return response.status_code def get_balance(self, account_id): response = requests.get(self.api_url + \'/balance\', params={\'account_id\': account_id}) return response.json()[\'balance\'] ``` # Testing Requirements: 1. Write a unit test to mock the POST request in the `process_payment` method and verify that the method returns the correct status code. 2. Write a unit test to mock the GET request in the `get_balance` method and verify that the method returns the correct balance. 3. Write a unit test to assert that the `process_payment` method is called with the correct arguments. # Expected Input and Output: 1. **For `process_payment` method:** - Input: `account_id=\'12345\'`, `amount=100` - Mocked response: status code 200 - Expected output: 200 2. **For `get_balance` method:** - Input: `account_id=\'12345\'` - Mocked response: JSON `{\\"balance\\": 500}` - Expected output: 500 3. **For verifying `process_payment` arguments:** - Input: `account_id=\'12345\'`, `amount=100` - Mocked method call. - Expected to assert the correct payload. # Constraints: - You should use `unittest.mock` library for mocking. - You need to use `patch` or `patch.object` for mocking `requests` library functions. - Ensure to check the payload in the `process_payment` method call. # Performance Requirements: - Your tests should be efficient and properly assert the necessary conditions without interacting with a live service. # Example: ```python import unittest from unittest.mock import patch import requests class TestPaymentProcessor(unittest.TestCase): @patch(\'requests.post\') def test_process_payment(self, mock_post): # Mock the response to return 200 status code mock_post.return_value.status_code = 200 processor = PaymentProcessor(\'http://mockapi.com\') result = processor.process_payment(\'12345\', 100) self.assertEqual(result, 200) mock_post.assert_called_once_with(\'http://mockapi.com/pay\', json={\'account_id\': \'12345\', \'amount\': 100}) @patch(\'requests.get\') def test_get_balance(self, mock_get): # Mock the response to return a balance of 500 mock_get.return_value.json.return_value = {\'balance\': 500} processor = PaymentProcessor(\'http://mockapi.com\') result = processor.get_balance(\'12345\') self.assertEqual(result, 500) mock_get.assert_called_once_with(\'http://mockapi.com/balance\', params={\'account_id\': \'12345\'}) if __name__ == \'__main__\': unittest.main() ``` # Note: - Run the provided example to verify that it passes all the assertions. - Feel free to add any additional tests to cover edge cases if necessary.","solution":"import requests class PaymentProcessor: def __init__(self, api_url): self.api_url = api_url def process_payment(self, account_id, amount): payload = { \'account_id\': account_id, \'amount\': amount } response = requests.post(self.api_url + \'/pay\', json=payload) return response.status_code def get_balance(self, account_id): response = requests.get(self.api_url + \'/balance\', params={\'account_id\': account_id}) return response.json()[\'balance\']"},{"question":"# **Coding Assessment Question: Exploring Plot Customization with Seaborn** In this task, you are asked to demonstrate your understanding of Seaborn\'s `seaborn.objects.Plot` class through creating and customizing plots. You will use different customization techniques, such as setting axis limits and modifying the appearance of the plots. **Requirements:** 1. **Create a Scatter Plot:** - Use the `seaborn.objects.Plot` class to create a scatter plot of the given data. - Input: Two lists of numerical values for `x` and `y` coordinates. - Output: Display a scatter plot with the given data points, where each point is represented as a circle. 2. **Set Custom Axes Limits:** - Customize the plot to set specific limits for both x and y axes. - Input: Tuples representing the limits for x and y axes, respectively. Each tuple will have a minimum and maximum value. - Output: Display a modified scatter plot with the specified axis limits. 3. **Invert Axes:** - Provide an option to invert one or both axes in the scatter plot. - Input: Boolean values indicating whether to invert the x-axis and/or y-axis. - Output: Display the scatter plot with the specified axes inverted. 4. **Handle Missing Limits:** - Allow one or both axis limits to be set to `None`, maintaining the default limit for that axis when no limit is provided. - Input: Tuples for axes limits where one or both values can be `None`. - Output: Scatter plot adjusted accordingly. # **Function Signature** ```python import seaborn.objects as so def customize_scatter_plot(x: list, y: list, x_limits: tuple = (None, None), y_limits: tuple = (None, None), invert_x: bool = False, invert_y: bool = False): This function creates a customized scatter plot based on the given parameters. Parameters: x (list): A list of numerical values representing the x-axis coordinates. y (list): A list of numerical values representing the y-axis coordinates. x_limits (tuple): A tuple containing min and max values for the x-axis. Default is (None, None). y_limits (tuple): A tuple containing min and max values for the y-axis. Default is (None, None). invert_x (bool): A boolean to determine if the x-axis should be inverted. Default is False. invert_y (bool): A boolean to determine if the y-axis should be inverted. Default is False. Output: Displays a customized scatter plot. ``` # **Constraints:** - The list `x` and list `y` must have the same length. - All elements in the lists `x` and `y` must be numerical (int or float). - Limits provided should be either numerical values or `None`. # **Example Usage:** ```python # Example data x = [1, 2, 3, 4, 5] y = [5, 4, 3, 2, 1] customize_scatter_plot(x, y, x_limits=(1, 5), y_limits=(1, 5), invert_x=False, invert_y=True) ``` This will generate a scatter plot with x-axis range from 1 to 5, y-axis inverted with a range from 1 to 5, and display the points accordingly.","solution":"import seaborn.objects as so import matplotlib.pyplot as plt def customize_scatter_plot(x: list, y: list, x_limits: tuple = (None, None), y_limits: tuple = (None, None), invert_x: bool = False, invert_y: bool = False): This function creates a customized scatter plot based on the given parameters. Parameters: x (list): A list of numerical values representing the x-axis coordinates. y (list): A list of numerical values representing the y-axis coordinates. x_limits (tuple): A tuple containing min and max values for the x-axis. Default is (None, None). y_limits (tuple): A tuple containing min and max values for the y-axis. Default is (None, None). invert_x (bool): A boolean to determine if the x-axis should be inverted. Default is False. invert_y (bool): A boolean to determine if the y-axis should be inverted. Default is False. Output: Displays a customized scatter plot. # Create a basic scatter plot plot = so.Plot(data=dict(x=x, y=y)).add(so.Dot()) # Determine axis limits xmin, xmax = x_limits ymin, ymax = y_limits if xmin is not None or xmax is not None: plot = plot.limit(xmin=xmin if xmin is not None else min(x), xmax=xmax if xmax is not None else max(x)) if ymin is not None or ymax is not None: plot = plot.limit(ymin=ymin if ymin is not None else min(y), ymax=ymax if ymax is not None else max(y)) # Display the plot with custom axes plot = plot.scale(x=invert_x, y=invert_y) # Show the plot plot.show() # Test the function locally if __name__ == \\"__main__\\": x = [1, 2, 3, 4, 5] y = [5, 4, 3, 2, 1] customize_scatter_plot(x, y, x_limits=(1, 5), y_limits=(1, 5), invert_x=False, invert_y=True)"},{"question":"# PyTorch Coding Assessment Question Problem Statement You are given a PyTorch model that consists of an encoder and a decoder. The encoder is a simple feed-forward network, while the decoder involves a control flow that depends on the intermediate values produced by the encoder. The task is to convert this PyTorch model into a TorchScript model using a combination of tracing and scripting. Additionally, you need to handle and annotate model attributes and ensure some operations are ignored during the compilation process. Instructions 1. **Define the PyTorch Model:** - The `Encoder` is a simple feed-forward neural network with one hidden layer. - The `Decoder` includes a loop that depends on the values from the `Encoder`. 2. **Convert the Model to TorchScript:** - Use `torch.jit.trace` for the `Encoder`. - Use `torch.jit.script` for the `Decoder`. - Ensure the entire model can be exported and run independently from Python. 3. **Attributes and Constants:** - Annotate the attributes and constants properly for the TorchScript model. 4. **Performance Optimization:** - Optimize the TorchScript model for inference. 5. **Ignore Specific Operations:** - Use `@torch.jit.ignore` to exclude any operation that should not be included in the TorchScript model. Requirements - Implement both the `Encoder` and `Decoder` classes. - Implement the `MainModel` class that combines the `Encoder` and `Decoder`. - Convert the `MainModel` to a TorchScript model. - Annotate attributes and constants with appropriate types. - Ignore specific dummy operations using `@torch.jit.ignore`. - Optimize the TorchScript model for inference. - Provide an example of how to save and load the TorchScript model. ```python import torch import torch.nn as nn from typing import List, Tuple class Encoder(nn.Module): def __init__(self, input_dim: int, hidden_dim: int): super(Encoder, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) self.relu = nn.ReLU() def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.fc1(x) x = self.relu(x) return x class Decoder(nn.Module): def __init__(self, hidden_dim: int, output_dim: int): super(Decoder, self).__init__() self.fc2 = nn.Linear(hidden_dim, output_dim) @torch.jit.script_method def forward(self, x: torch.Tensor) -> torch.Tensor: for _ in range(x.size(0)): x = self.fc2(x) return x class MainModel(nn.Module): def __init__(self, input_dim: int, hidden_dim: int, output_dim: int): super(MainModel, self).__init__() self.encoder = Encoder(input_dim, hidden_dim) self.decoder = Decoder(hidden_dim, output_dim) def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.encoder(x) x = self.decoder(x) return x def dummy_operation(self, x: torch.Tensor) -> torch.Tensor: return x + 1 @torch.jit.ignore def runtime_only_operation(self, x: torch.Tensor) -> torch.Tensor: return self.dummy_operation(x) # Define the dimensions input_dim, hidden_dim, output_dim = 10, 20, 5 # Create model model = MainModel(input_dim, hidden_dim, output_dim) # Trace the encoder traced_encoder = torch.jit.trace(model.encoder, torch.rand(1, input_dim)) # Script the decoder scripted_decoder = torch.jit.script(model.decoder) # Combine traced encoder and scripted decoder class TracedScriptedModel(nn.Module): def __init__(self): super(TracedScriptedModel, self).__init__() self.encoder = traced_encoder self.decoder = scripted_decoder def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.encoder(x) x = self.decoder(x) return x # Final scripted model final_model = torch.jit.script(TracedScriptedModel()) # Optimize for inference final_model = torch.jit.optimize_for_inference(final_model) # Save and load the model torch.jit.save(final_model, \'optimized_model.pt\') loaded_model = torch.jit.load(\'optimized_model.pt\') # Test with a sample input sample_input = torch.rand(1, input_dim) output = loaded_model(sample_input) print(output) ``` Expected Inputs and Outputs - **Input:** A tensor of size [1, input_dim] - **Output:** A tensor of size [1, output_dim] Constraints - `input_dim`, `hidden_dim`, and `output_dim` are positive integers. - Ensure that the TorchScript model can be exported and loaded independently of Python. Performance Requirements - The model should be optimized for inference. - Ensure that the TorchScript model does not include unnecessary operations ignored by `@torch.jit.ignore`. Good luck!","solution":"import torch import torch.nn as nn from typing import List, Tuple class Encoder(nn.Module): def __init__(self, input_dim: int, hidden_dim: int): super(Encoder, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) self.relu = nn.ReLU() def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.fc1(x) x = self.relu(x) return x class Decoder(nn.Module): def __init__(self, hidden_dim: int, output_dim: int): super(Decoder, self).__init__() self.fc2 = nn.Linear(hidden_dim, output_dim) @torch.jit.script_method def forward(self, x: torch.Tensor) -> torch.Tensor: for _ in range(x.size(0)): x = self.fc2(x) return x class MainModel(nn.Module): def __init__(self, input_dim: int, hidden_dim: int, output_dim: int): super(MainModel, self).__init__() self.encoder = Encoder(input_dim, hidden_dim) self.decoder = Decoder(hidden_dim, output_dim) def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.encoder(x) x = self.decoder(x) return x def dummy_operation(self, x: torch.Tensor) -> torch.Tensor: return x + 1 @torch.jit.ignore def runtime_only_operation(self, x: torch.Tensor) -> torch.Tensor: return self.dummy_operation(x) # Define the dimensions input_dim, hidden_dim, output_dim = 10, 20, 5 # Create model model = MainModel(input_dim, hidden_dim, output_dim) # Trace the encoder traced_encoder = torch.jit.trace(model.encoder, torch.rand(1, input_dim)) # Script the decoder scripted_decoder = torch.jit.script(model.decoder) # Combine traced encoder and scripted decoder class TracedScriptedModel(nn.Module): def __init__(self): super(TracedScriptedModel, self).__init__() self.encoder = traced_encoder self.decoder = scripted_decoder def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.encoder(x) x = self.decoder(x) return x # Final scripted model final_model = torch.jit.script(TracedScriptedModel()) # Optimize for inference final_model_optimized = torch.jit.optimize_for_inference(final_model) # Save and load the model torch.jit.save(final_model_optimized, \'optimized_model.pt\') loaded_model = torch.jit.load(\'optimized_model.pt\') # Test with a sample input sample_input = torch.rand(1, input_dim) output = loaded_model(sample_input) print(output)"},{"question":"**Command-Line Calculator** # Objective: Your task is to create a command-line application that acts as a calculator. The calculator should accept command-line options for performing basic arithmetic operations: addition, subtraction, multiplication, and division. You need to use the `optparse` module for parsing these options. # Requirements: 1. **Operations:** - Addition (`-a` or `--add`) - Subtraction (`-s` or `--subtract`) - Multiplication (`-m` or `--multiply`) - Division (`-d` or `--divide`) 2. **Input:** - Two numbers to perform the operation on. - These numbers should be passed as positional arguments. 3. **Output:** - The result of the arithmetic operation. 4. **Constraints:** - You should handle division by zero gracefully and output an appropriate message. - Only one operation should be specified at a time, otherwise an error message should be displayed. # Example Usage: 1. To add two numbers: ```shell python calculator.py -a 3 5 ``` Output: `8` 2. To subtract two numbers: ```shell python calculator.py -s 10 4 ``` Output: `6` 3. To multiply two numbers: ```shell python calculator.py -m 7 8 ``` Output: `56` 4. To divide two numbers: ```shell python calculator.py -d 20 4 ``` Output: `5` # Implementation: ```python from optparse import OptionParser def main(): parser = OptionParser() parser.add_option(\\"-a\\", \\"--add\\", action=\\"store_true\\", dest=\\"add\\", default=False, help=\\"Add the two numbers\\") parser.add_option(\\"-s\\", \\"--subtract\\", action=\\"store_true\\", dest=\\"subtract\\", default=False, help=\\"Subtract the second number from the first number\\") parser.add_option(\\"-m\\", \\"--multiply\\", action=\\"store_true\\", dest=\\"multiply\\", default=False, help=\\"Multiply the two numbers\\") parser.add_option(\\"-d\\", \\"--divide\\", action=\\"store_true\\", dest=\\"divide\\", default=False, help=\\"Divide the first number by the second number\\") (options, args) = parser.parse_args() if len(args) != 2: parser.error(\\"You must provide exactly two numbers.\\") try: num1 = float(args[0]) num2 = float(args[1]) except ValueError: parser.error(\\"Both inputs must be numbers.\\") if options.add: if options.subtract or options.multiply or options.divide: parser.error(\\"Only one operation can be specified at a time.\\") print(f\\"The result of adding is: {num1 + num2}\\") elif options.subtract: if options.add or options.multiply or options.divide: parser.error(\\"Only one operation can be specified at a time.\\") print(f\\"The result of subtracting is: {num1 - num2}\\") elif options.multiply: if options.add or options.subtract or options.divide: parser.error(\\"Only one operation can be specified at a time.\\") print(f\\"The result of multiplying is: {num1 * num2}\\") elif options.divide: if options.add or options.subtract or options.multiply: parser.error(\\"Only one operation can be specified at a time.\\") if num2 == 0: print(\\"Error: Division by zero is not allowed.\\") else: print(f\\"The result of dividing is: {num1 / num2}\\") else: parser.error(\\"No operation specified. Use -a, -s, -m, or -d to specify an operation.\\") if __name__ == \\"__main__\\": main() ``` # Note: - Ensure your implementation handles incorrect inputs and provides clear error messages. - Write additional test cases to validate your implementation for correctness and robustness.","solution":"import optparse def calculate(option, num1, num2): Perform arithmetic operation based on the option specified if option == \'add\': return num1 + num2 elif option == \'subtract\': return num1 - num2 elif option == \'multiply\': return num1 * num2 elif option == \'divide\': if num2 == 0: return \\"Error: Division by zero is not allowed.\\" return num1 / num2 else: return \\"Error: Invalid operation.\\" def main(): parser = optparse.OptionParser(description=\\"Command-Line Calculator\\") parser.add_option(\\"-a\\", \\"--add\\", action=\\"store_true\\", dest=\\"add\\", help=\\"Add the two numbers\\") parser.add_option(\\"-s\\", \\"--subtract\\", action=\\"store_true\\", dest=\\"subtract\\", help=\\"Subtract the second number from the first number\\") parser.add_option(\\"-m\\", \\"--multiply\\", action=\\"store_true\\", dest=\\"multiply\\", help=\\"Multiply the two numbers\\") parser.add_option(\\"-d\\", \\"--divide\\", action=\\"store_true\\", dest=\\"divide\\", help=\\"Divide the first number by the second number\\") (options, args) = parser.parse_args() if len(args) != 2: print(\\"You must provide exactly two numbers.\\") exit(1) try: num1 = float(args[0]) num2 = float(args[1]) except ValueError: print(\\"Both inputs must be numbers.\\") exit(1) operations = [options.add, options.subtract, options.multiply, options.divide] if sum(operations) != 1: print(\\"Only one operation can be specified at a time.\\") exit(1) if options.add: result = calculate(\'add\', num1, num2) elif options.subtract: result = calculate(\'subtract\', num1, num2) elif options.multiply: result = calculate(\'multiply\', num1, num2) elif options.divide: result = calculate(\'divide\', num1, num2) else: print(\\"No operation specified. Use -a, -s, -m, or -d to specify an operation.\\") exit(1) print(result) if __name__ == \\"__main__\\": main()"},{"question":"**Question:** Using the seaborn library, create a comprehensive visualization using the `diamonds` dataset that adheres to the following specifications: 1. **Loading Data and Initial Processing:** - Load the `diamonds` dataset using `seaborn.load_dataset`. 2. **Histogram of Diamond Prices:** - Create a histogram of the `price` column using a logarithmic scale for the x-axis. 3. **Stacked Bar Plot by Cut:** - Overlay the histogram with a stacked bar plot where the color represents the `cut` of the diamonds. Ensure that there is no overlap between the bars. 4. **Transparency Based on Clarity:** - Apply transparency to the bars based on the `clarity` of the diamonds. 5. **Unfilled Narrow Bars for Ideal Cut Diamonds:** - On the same plot, add a set of unfilled narrow bars representing only diamonds with `cut` equal to \'Ideal\'. These bars should have distinct edges and be narrow enough not to significantly overlap with the filled bars. 6. **Combine All Aspects in One Plot:** - Ensure that all these visual aspects are combined into one cohesive plot and display the resulting visualization. **Requirements:** - **Input:** None, as the dataset is loaded within the function. - **Output:** A seaborn plot displayed using matplotlib\'s `show` function. - **Constraints:** Use seaborn\'s `so.Plot`, `so.Bars`, `so.Hist`, `so.Stack`, and any other necessary components to create the plot. **Implementation:** Implement this functionality in a function named `create_diamond_visualization()`. ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_diamond_visualization(): # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Initialize the plot with price column and logarithmic x-scale p = so.Plot(diamonds, x=\\"price\\").scale(x=\\"log\\") # Add histogram with stacked bar plot by cut p.add(so.Bars(), so.Hist(), so.Stack(), color=\\"cut\\") # Add transparency based on clarity p.add(so.Bars(edgewidth=0), so.Hist(), so.Stack(), alpha=\\"clarity\\") # Add unfilled narrow bars for diamonds with \'Ideal\' cut ideal_diamonds = diamonds.query(\\"cut == \'Ideal\'\\") hist = so.Hist(binwidth=200, binrange=(300, 20000)) p.add(so.Bars(fill=False, edgecolor=\\"C0\\", edgewidth=1.5, width=0.3), hist, data=ideal_diamonds) # Display the plot p.show() # Call the function to create and display the visualization create_diamond_visualization() ``` **Notes:** - Ensure to handle any potential errors that may arise from the dataset loading or plotting functions. - Optimize the code for clarity and performance where possible.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_diamond_visualization(): # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Initialize the plot with price column and logarithmic x-scale p = so.Plot(diamonds, x=\\"price\\").scale(x=\\"log\\") # Add histogram with stacked bar plot by cut p.add(so.Bars(), so.Hist(), so.Stack(), color=\\"cut\\") # Add transparency based on clarity p.add(so.Bars(edgewidth=0), so.Hist(), so.Stack(), alpha=\\"clarity\\") # Add unfilled narrow bars for diamonds with \'Ideal\' cut ideal_diamonds = diamonds.query(\\"cut == \'Ideal\'\\") hist = so.Hist(binwidth=200, binrange=(300, 20000)) p.add(so.Bars(fill=False, edgecolor=\\"C0\\", edgewidth=1.5, width=0.3), hist, data=ideal_diamonds) # Display the plot p.show() # Call the function to create and display the visualization create_diamond_visualization()"},{"question":"Implementing a File Lock Manager Objective: To assess students\' understanding of the `fcntl` module, specifically focusing on the file locking mechanisms and exception handling. Problem Statement: You are required to implement a Python class `FileLockManager` that manages file locks using the `fcntl` module. This class should allow acquiring and releasing both shared and exclusive locks on a file. Additionally, it should handle situations where lock acquisition fails gracefully by raising custom exceptions. Class Requirements: 1. **Class Name**: `FileLockManager` 2. **Methods**: - `__init__(self, file_path: str)`: Initializes the `FileLockManager` with the file path to be locked. Opens the file in append mode and obtains its file descriptor. - `acquire_lock(self, exclusive: bool = True, non_blocking: bool = False) -> None`: Acquires a lock on the file. - If `exclusive` is `True`, it should acquire an exclusive lock. Otherwise, a shared lock. - If `non_blocking` is `True`, the method should not block if the lock cannot be acquired and should raise a `LockAcquisitionError`. - `release_lock(self) -> None`: Releases the lock on the file. - `__enter__(self)`: Implements context manager entry method to acquire an exclusive lock. - `__exit__(self, exc_type, exc_value, traceback)`: Implements context manager exit method to release the lock. 3. **Custom Exceptions**: - `LockAcquisitionError`: Raised when the lock cannot be acquired in non-blocking mode. 4. **Usage Example**: ```python from file_lock_manager import FileLockManager, LockAcquisitionError file_path = \\"/tmp/testfile\\" try: with FileLockManager(file_path) as file_lock: print(\\"Lock acquired\\") # Perform file operations except LockAcquisitionError as e: print(f\\"Failed to acquire lock: {e}\\") # Acquiring and releasing lock manually try: file_lock = FileLockManager(file_path) file_lock.acquire_lock(exclusive=True, non_blocking=True) print(\\"Exclusive Lock acquired\\") # Perform file operations file_lock.release_lock() print(\\"Lock released\\") except LockAcquisitionError as e: print(f\\"Failed to acquire lock: {e}\\") ``` Constraints: - File path is assumed to be valid and writable. - Handle all exceptions appropriately and ensure file descriptors are closed properly. - Implementations should be robust and handle edge cases, such as attempting to release a lock that was not acquired. Submission: Provide the implementation in a file named `file_lock_manager.py`. Include any additional helper functions or classes as necessary. Ensure your code is well-commented, and write unit tests to demonstrate that your `FileLockManager` works correctly under different scenarios.","solution":"import fcntl import os class LockAcquisitionError(Exception): pass class FileLockManager: def __init__(self, file_path: str): self.file_path = file_path self.file = open(self.file_path, \'a\') self.fd = self.file.fileno() def acquire_lock(self, exclusive: bool = True, non_blocking: bool = False) -> None: lock_type = fcntl.LOCK_EX if exclusive else fcntl.LOCK_SH if non_blocking: lock_type |= fcntl.LOCK_NB try: fcntl.flock(self.fd, lock_type) except IOError: raise LockAcquisitionError(\\"Failed to acquire lock\\") def release_lock(self) -> None: fcntl.flock(self.fd, fcntl.LOCK_UN) def __enter__(self): self.acquire_lock(exclusive=True) return self def __exit__(self, exc_type, exc_value, traceback): self.release_lock() self.file.close()"},{"question":"**String Preparation for Internet Applications** You are asked to write a function that prepares a string for usage in internet domains. Your function will evaluate each character in the string, determine its type based on the tables from the `stringprep` module, and transform the string according to specified rules. The function should perform the following steps: 1. Map characters commonly mapped to nothing (using `stringprep.in_table_b1`). 2. Normalize case-fold characters using Table B.2 for Unicode Normalization Form KC (NFKC). 3. Remove characters inappropriate for plain text (using `stringprep.in_table_c6`). 4. Remove any characters private use, surrogate codes, or non-character code points (using `stringprep.in_table_c3`, `stringprep.in_table_c5`, and `stringprep.in_table_c4` respectively). 5. Ensure that the string doesn\'t contain control characters (using `stringprep.in_table_c21_c22`), and space characters (using `stringprep.in_table_c11_c12`). **Function Signature** ```python def prepare_string_for_internet(domain_string: str) -> str: pass ``` **Input** - A single string `domain_string` representing the input domain string. **Output** - Returns a new string after preparing it according to the rules specified above. **Constraints** - The input string may contain any printable Unicode characters. - The function should handle edge cases such as empty strings. **Examples** ```python print(prepare_string_for_internet(\\"Example Domain\\")) # Expects some transformation based on the rules. print(prepare_string_for_internet(\\"Test@Example.com\\")) # Handle special characters and spaces. ``` Use the functions provided in the `stringprep` module to implement the above logic. Be sure to handle edge cases and provide a solution maintaining the performance while ensuring it adheres to the constraints.","solution":"import stringprep import unicodedata def prepare_string_for_internet(domain_string: str) -> str: # Step 1: Map characters commonly mapped to nothing domain_string = \'\'.join(c for c in domain_string if not stringprep.in_table_b1(c)) # Step 2: Normalize using NFKC domain_string = unicodedata.normalize(\'NFKC\', domain_string) # Step 3: Remove characters inappropriate for plain text domain_string = \'\'.join(c for c in domain_string if not stringprep.in_table_c6(c)) # Step 4: Remove private use, surrogate codes, or non-character code points domain_string = \'\'.join(c for c in domain_string if not stringprep.in_table_c3(c) and not stringprep.in_table_c5(c) and not stringprep.in_table_c4(c)) # Step 5: Ensure no control and space characters domain_string = \'\'.join(c for c in domain_string if not stringprep.in_table_c21_c22(c) and not stringprep.in_table_c11_c12(c)) return domain_string"},{"question":"# Advanced Python Typing Challenge Background You are tasked with creating a dispatch system for handling various types of tasks. Each task is tied to specific handlers that process the task based on its type. To efficiently manage this system, you will implement a generic dispatcher that uses Python\'s type hinting and typing module to ensure type safety. Requirements 1. **Generic Task Type**: Create a generic task type that can hold any type of content. 2. **Task Handler**: Define a `TaskHandler` protocol that requires a method to handle a task and return a result. 3. **Dispatcher**: Implement a `Dispatcher` class that registers handlers for different task types and dispatches tasks accordingly. Task 1. **Define the generic task type**: ```python from typing import TypeVar, Generic T = TypeVar(\'T\') class Task(Generic[T]): def __init__(self, content: T): self.content = content ``` 2. **Define the `TaskHandler` protocol**: ```python from typing import Protocol, TypeVar T = TypeVar(\'T\') R = TypeVar(\'R\') class TaskHandler(Protocol[T, R]): def handle(self, task: Task[T]) -> R: ... ``` 3. **Implement the `Dispatcher` class**: ```python from typing import Type, Dict, Callable, Any class Dispatcher: def __init__(self): self.handlers: Dict[Type, Callable[[Any], Any]] = {} def register_handler(self, task_type: Type[T], handler: TaskHandler[T, R]) -> None: self.handlers[task_type] = handler.handle def dispatch(self, task: Task[T]) -> R: task_type = type(task.content) if task_type in self.handlers: handler = self.handlers[task_type] return handler(task) raise ValueError(f\\"No registered handler for task type: {task_type}\\") ``` Example Usage ```python # Define a custom handler class PrintTaskHandler(TaskHandler[str, None]): def handle(self, task: Task[str]) -> None: print(task.content) # Register task handlers and dispatch tasks dispatcher = Dispatcher() dispatcher.register_handler(str, PrintTaskHandler()) task = Task(\\"Hello, world!\\") dispatcher.dispatch(task) # Should print: Hello, world! ``` Constraints - Ensure your implementation is type-checked and type-safe. - Use type annotations to guarantee the correct usage of the generics and protocols. - Registering a handler should match the task type it can handle. - Dispatching a task should invoke the registered handler method. Implement the code following the provided templates and ensure you handle edge cases and errors appropriately.","solution":"from typing import TypeVar, Generic, Protocol, Type, Dict, Callable, Any T = TypeVar(\'T\') R = TypeVar(\'R\') class Task(Generic[T]): def __init__(self, content: T): self.content = content class TaskHandler(Protocol[T, R]): def handle(self, task: Task[T]) -> R: ... class Dispatcher: def __init__(self): self.handlers: Dict[Type, Callable[[Task], Any]] = {} def register_handler(self, task_type: Type[T], handler: TaskHandler[T, R]) -> None: self.handlers[task_type] = handler.handle def dispatch(self, task: Task[T]) -> R: task_type = type(task.content) if task_type in self.handlers: handler = self.handlers[task_type] return handler(task) raise ValueError(f\\"No registered handler for task type: {task_type}\\") # Example Usage class PrintTaskHandler(TaskHandler[str, None]): def handle(self, task: Task[str]) -> None: print(task.content) dispatcher = Dispatcher() dispatcher.register_handler(str, PrintTaskHandler()) task = Task(\\"Hello, world!\\") dispatcher.dispatch(task) # Should print: Hello, world!"},{"question":"You are working on a multi-task program that requires a careful orchestration of several asynchronous tasks, ensuring that certain conditions are met before proceeding. You must implement a system that adheres to the following requirements using the asyncio synchronization primitives: 1. **Task Coordination with Locks and Events**: - You have a shared resource protected by an `asyncio.Lock`. - Multiple asyncio tasks need to access this resource, but only one task at a time can do so. - Use an `asyncio.Event` to notify all waiting tasks when a specific condition has been met. 2. **Predicate-Based Waiting with Condition**: - A separate set of tasks requires a complex predicate to be true before they can proceed. - Use an `asyncio.Condition` to manage the waiting tasks. 3. **Resource Limitation with Semaphore**: - There is a resource used by tasks that can handle a limited number of concurrent accesses. - Use an `asyncio.Semaphore` to manage this resource. 4. **Bounded Semaphore Constraint**: - Ensure that tasks do not release the semaphore more times than it was acquired by using an `asyncio.BoundedSemaphore`. # Implementation Details - Implement a function `coordinate_tasks()`. This function should: - Declare and initialize appropriate asyncio synchronization primitives for the tasks. - Create and manage several async tasks that demonstrate the use of these primitives. - Ensure all tasks synchronize correctly using the provided primitives. # Input and Output - **Input**: There is no direct input; instead, the implementation configures the function and tasks directly. - **Output**: No return is required, but the function should print statements indicating task progress, resource access, and synchronization events. # Constraints - You must use `asyncio` synchronization primitives (`Lock`, `Event`, `Condition`, `Semaphore`, and `BoundedSemaphore`). - Ensure all tasks function correctly without deadlocks or race conditions. - Validate the semaphore limits using a `BoundedSemaphore` to prevent excessive releases. # Example Usage ```python import asyncio async def coordinate_tasks(): # Define and initialize asyncio primitives here # Example: setting up a lock lock = asyncio.Lock() # Example: setting up an event event = asyncio.Event() # Example: setting up a condition condition = asyncio.Condition() # Example: setting up a semaphore sem = asyncio.Semaphore(value=3) # Example: setting up a bounded semaphore bsem = asyncio.BoundedSemaphore(value=2) async def task_1(): async with lock: print(\\"Task 1 has acquired the lock\\") await asyncio.sleep(1) print(\\"Task 1 has released the lock\\") await event.wait() print(\\"Task 1 has recognized the event\\") async def task_2(): async with condition: await condition.wait_for(predicate=lambda: True) print(\\"Task 2 proceeds after condition met\\") async def task_3(): async with sem: print(\\"Task 3 is using a limited resource\\") await asyncio.sleep(2) print(\\"Task 3 has released the limited resource\\") async def task_4(): async with bsem: print(\\"Task 4 is using a bounded resource\\") await asyncio.sleep(2) print(\\"Task 4 has released the bounded resource\\") await asyncio.gather(task_1(), task_2(), task_3(), task_4()) event.set() await asyncio.gather(task_1()) asyncio.run(coordinate_tasks()) ``` This task requires you to design the coordination logic, ensuring proper usage of each asyncio synchronization primitive. The provided structure is a guide and should be extended to meet the full requirements and demonstrate synchronization among tasks effectively.","solution":"import asyncio async def coordinate_tasks(): # Initialize asyncio synchronization primitives lock = asyncio.Lock() event = asyncio.Event() condition = asyncio.Condition() sem = asyncio.Semaphore(3) bsem = asyncio.BoundedSemaphore(2) async def task_shared_resource(): async with lock: print(\\"Task with shared resource has acquired the lock\\") await asyncio.sleep(1) # Simulate work print(\\"Task with shared resource has released the lock\\") await event.wait() print(\\"Task with shared resource detected the event set\\") async def task_check_condition(): async with condition: await condition.wait_for(predicate=lambda: True) print(\\"Task with condition proceeds after condition met\\") async def task_limited_resource(): async with sem: print(\\"Task with limited resource is using the resource\\") await asyncio.sleep(2) # Simulate work print(\\"Task with limited resource has released the resource\\") async def task_bounded_resource(): async with bsem: print(\\"Task with bounded resource is using the resource\\") await asyncio.sleep(2) # Simulate work print(\\"Task with bounded resource has released the resource\\") # Start tasks tasks = [ asyncio.create_task(task_shared_resource()), asyncio.create_task(task_check_condition()), asyncio.create_task(task_limited_resource()), asyncio.create_task(task_bounded_resource()) ] # Let them run and then set the event for demonstration await asyncio.sleep(1) event.set() # Notify all tasks waiting on the condition async with condition: condition.notify_all() await asyncio.gather(*tasks)"},{"question":"**Coding Assessment Question: Advanced Seaborn Visualization** **Objective:** Test the ability to create complex, multi-layered visualizations using `seaborn.objects` to manipulate and customize data presentations. **Problem Statement:** You are given a dataset containing information about tips received by servers in a restaurant. Using this dataset, your task is to create a detailed visualization that includes the following features: 1. A scatter plot showing the relationship between the total bill and the tip amount. 2. A fitting line that indicates the trend in the relationship between total bill and tip amount. 3. A bar plot displaying the total number of patrons (group size) by day of the week. 4. A separate layer showing the average tip amount for each group size. Use the `seaborn` package to achieve the following: 1. Load the `tips` dataset using `seaborn.load_dataset`. 2. Create the main scatter plot with the total bill on the x-axis and tip amount on the y-axis. 3. Add a fitting line to the scatter plot indicating trends. 4. Add a bar plot layer showing the total number of patrons by day. 5. Overlay another layer showing the average tip amount for each group size, distinct from the scatter plot. 6. Annotate each layer appropriately and include the respective labels in the legend. **Input:** No direct input, use the `seaborn` `tips` dataset. **Output:** A `seaborn` plot displaying all the above-described visualizations. **Constraints:** - Use `seaborn.objects` to create the plots. - Ensure that additional variables are correctly mapped and scaled. **Performance Requirements:** - The visualization should be clear and informative. - Each layer must be distinguishable by unique colors or annotations in the legend. **Python Code:** ```python import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\") # Create a base plot object p = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\") # Add a scatter plot layer for total bill vs tip amount p.add(so.Dot(), pointsize=\\"size\\") # Add a line layer with a polynomial fit to show the trend p.add(so.Line(color=\\"black\\", linewidth=1), so.PolyFit()) # Add a bar plot layer showing total number of patrons by day p.add(so.Bar(), so.Hist(), weight=\\"size\\", x=\\"day\\", label=\\"Total patrons\\") # Add a line plot layer for average tip amount vs group size p.add(so.Line(color=\\"blue\\"), so.Agg(), x=\\"size\\", y=\\"tip\\", label=\\"Average tip amount\\") # Set scale for point sizes p.scale(pointsize=(2, 10)) # Label the axes p.label(x=\\"Total Bill\\", y=\\"Tip Amount\\") # Display the plot p.show() ```","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\") # Create a base plot object p = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\") # Add a scatter plot layer for total bill vs tip amount p.add(so.Dot(), pointsize=\\"size\\", label=\\"Tip Amount by Total Bill\\") # Add a line layer with a polynomial fit to show the trend p.add(so.Line(color=\\"red\\", linewidth=2), so.PolyFit(), label=\\"Fit Line\\") # Add a bar plot layer showing total number of patrons by day p.add(so.Bar(), so.Hist(), x=\\"day\\", weight=\\"size\\", label=\\"Total Patrons by Day\\") # Add a line plot layer for average tip amount vs group size p.add(so.Line(color=\\"blue\\"), so.Agg(), x=\\"size\\", y=\\"tip\\", label=\\"Average Tip by Group Size\\") # Set appropriate scales for point sizes p.scale(pointsize=(2, 10)) # Label the axes p.label(x=\\"Total Bill\\", y=\\"Tip Amount\\") # Display the plot p.show()"},{"question":"You are asked to implement a custom attention mechanism using PyTorch that applies causal bias. Specifically, you need to create a masked attention matrix using the causal bias functions provided in the `torch.nn.attention.bias` module. # Task 1. **Implement a function `apply_causal_bias` that takes the following input:** - A 2D tensor `attention_scores` of shape `(sequence_length, sequence_length)` representing the unnormalized attention scores. - A string `variant` which can be either `\\"lower_right\\"` or `\\"upper_left\\"`, indicating the type of causal bias to apply. 2. **The function should:** - Apply the corresponding causal bias (using `causal_lower_right` or `causal_upper_left`) to the attention scores. - Ensure that the result is a valid attention matrix where positions that should not be attended to are masked with a very large negative number (e.g., `-1e9`). 3. **Return the masked attention scores as output.** # Expected Function Signature ```python import torch def apply_causal_bias(attention_scores: torch.Tensor, variant: str) -> torch.Tensor: # Your implementation here pass ``` # Example ```python import torch from torch.nn.attention.bias import causal_lower_right, causal_upper_left # Example input attention_scores = torch.tensor([ [0.2, 0.3, 0.5], [0.8, 0.1, 0.6], [0.4, 0.7, 0.9] ]) # Applying lower right causal bias output_lower_right = apply_causal_bias(attention_scores, \\"lower_right\\") print(output_lower_right) # Applying upper left causal bias output_upper_left = apply_causal_bias(attention_scores, \\"upper_left\\") print(output_upper_left) ``` # Constraints 1. Ensure that the input attention scores tensor is a square matrix. 2. The function should handle tensors with arbitrary sequence lengths. 3. Optimize for performance when applying the causal mask. 4. Assume the variants `\\"lower_right\\"` and `\\"upper_left\\"` correspond to functions `causal_lower_right` and `causal_upper_left` respectively. # Note - You may need to refer to the full PyTorch documentation to understand the specific usage of `causal_lower_right` and `causal_upper_left` functions if they were available online. For this task, we assume these functions modify the tensor in a specific way to apply causal masking.","solution":"import torch def apply_causal_bias(attention_scores: torch.Tensor, variant: str) -> torch.Tensor: Apply causal bias to the attention scores. Args: attention_scores (torch.Tensor): 2D tensor of shape (sequence_length, sequence_length). variant (str): Either \\"lower_right\\" or \\"upper_left\\" indicating the type of causal bias. Returns: torch.Tensor: The attention scores with the causal bias applied. sequence_length = attention_scores.shape[0] assert sequence_length == attention_scores.shape[1], \\"attention_scores must be a square matrix\\" if variant == \\"lower_right\\": # Create a mask for lower right causal bias mask = torch.triu(torch.ones(sequence_length, sequence_length) * float(\'-inf\'), diagonal=1) elif variant == \\"upper_left\\": # Create a mask for upper left causal bias mask = torch.tril(torch.ones(sequence_length, sequence_length) * float(\'-inf\'), diagonal=-1) else: raise ValueError(\\"Invalid variant specified. Must be \'lower_right\' or \'upper_left\'.\\") masked_attention_scores = attention_scores + mask return masked_attention_scores"},{"question":"# Question: Advanced Data Processing and Control Flow You are tasked with creating a Python function `process_data(data: list) -> tuple` that takes a list of mixed data types (integers, floats, and strings) and performs the following operations: 1. **Separate the data:** - Extract integers, floats, and non-numeric strings into separate lists. 2. **Data Processing:** - Calculate the sum of all integers. - Calculate the product of all floats. - Combine all non-numeric strings into a single string, separated by a space. 3. **Error Handling:** - If the input list contains any unsupported data types (e.g., lists, dictionaries), raise a `TypeError` with the message `\\"Unsupported data type found\\"`. - If the input list is empty, raise a `ValueError` with the message `\\"Input list is empty\\"`. 4. **Return the results:** - Return a tuple containing the sum of integers, the product of floats, and the combined string of non-numeric strings. Input - A list of mixed data types where each element is either an integer, float, or string. - Example: `[1, 2.5, \'apple\', 3, 4.5, \'banana\']` Output - A tuple containing the sum of integers (int), the product of floats (float), and a combined string of non-numeric strings (str). - Example: `(4, 11.25, \'apple banana\')` Constraints - Implement the function efficiently in terms of both time and space complexity. - Assume that there could be a large number of elements in the input list. # Function Signature ```python def process_data(data: list) -> tuple: ``` # Example ```python assert process_data([1, 2.5, \'apple\', 3, 4.5, \'banana\']) == (4, 11.25, \'apple banana\') assert process_data([2, 3, 4.0, \'hello\', \'world\']) == (5, 4.0, \'hello world\') ``` # Notes - You can use the `isinstance()` function to check data types. - Think about edge cases such as empty strings, negative numbers, and floating-point precision. - Consider using list comprehensions and generator expressions where appropriate.","solution":"def process_data(data: list) -> tuple: if not data: raise ValueError(\\"Input list is empty\\") ints = [] floats = [] strings = [] for item in data: if isinstance(item, int): ints.append(item) elif isinstance(item, float): floats.append(item) elif isinstance(item, str): strings.append(item) else: raise TypeError(\\"Unsupported data type found\\") int_sum = sum(ints) float_product = 1.0 for f in floats: float_product *= f combined_string = \' \'.join(strings) return (int_sum, float_product, combined_string)"},{"question":"**Problem Statement: Analyzing Sales Data with Pandas** You are provided with a CSV file named `sales_data.csv`. The file contains a dataset with sales records, and your task is to preprocess, analyze, and summarize the data using pandas. The dataset has the following columns: - `Date`: The date of the sales record (format: `yyyy-mm-dd`). - `Store`: Store identifier (integer). - `Product`: Product name (string). - `Revenue`: Revenue from sales (float, may contain missing values). **Tasks:** 1. **Data Loading and Initial Inspection**: - Load the dataset from `sales_data.csv` and display the first 5 rows. - Print the summary statistics for the numerical columns. 2. **Handling Missing Data**: - Identify columns with missing values and the number of missing entries in each column. - Fill missing values in the `Revenue` column with the mean revenue of the respective `Product`. 3. **Data Aggregation**: - Calculate the total revenue generated by each `Store` and `Date`. The result should be a DataFrame with `Store` and `Date` as multi-index and the total revenue as the value. 4. **Advanced Data Manipulation**: - Pivot the data to create a DataFrame where the rows represent each `Date`, columns represent each `Store`, and values represent the total revenue from that store on that date. - Add a column to the pivoted DataFrame that represents the total revenue across all stores for each date. - Find the date with the highest total revenue across all stores. 5. **Output the Results**: - Print the final pivoted DataFrame with the additional column for total revenue. - Print the date with the highest total revenue across all stores. # Input Format: - A CSV file named `sales_data.csv`. # Output Format: - The first 5 rows of the loaded DataFrame. - Summary statistics for the numerical columns. - The columns with missing values and the number of missing entries. - The multi-index DataFrame with total revenue by `Store` and `Date`. - The final pivoted DataFrame with an additional column for total revenue. - The date with the highest total revenue across all stores. # Constraints: - Assume the dataset fits into memory. - Use pandas functions to handle all operations. # Example: ```csv Date,Store,Product,Revenue 2023-01-01,1,A,100.0 2023-01-01,1,B,150.0 2023-01-01,2,A, 2023-01-02,1,A,200.0 2023-01-02,2,B,250.0 ``` ```python import pandas as pd # Task 1: Data Loading and Initial Inspection df = pd.read_csv(\'sales_data.csv\') print(df.head()) # Task 2: Handling Missing Data missing_values = df.isnull().sum() print(\\"Missing values:n\\", missing_values) df[\'Revenue\'] = df.groupby(\'Product\')[\'Revenue\'].transform(lambda x: x.fillna(x.mean())) # Task 3: Data Aggregation aggregated_df = df.groupby([\'Store\', \'Date\'])[\'Revenue\'].sum().unstack().fillna(0) # Task 4: Advanced Data Manipulation pivoted_df = df.pivot_table(values=\'Revenue\', index=\'Date\', columns=\'Store\', aggfunc=\'sum\', fill_value=0) pivoted_df[\'Total_Revenue\'] = pivoted_df.sum(axis=1) max_revenue_date = pivoted_df[\'Total_Revenue\'].idxmax() # Task 5: Output the Results print(pivoted_df) print(\\"Date with highest total revenue:\\", max_revenue_date) ```","solution":"import pandas as pd def load_and_inspect_data(filename): Load dataset from CSV and return the first 5 rows and summary statistics df = pd.read_csv(filename) first_five_rows = df.head() summary_stats = df.describe() return df, first_five_rows, summary_stats def handle_missing_data(df): Fill missing values in \'Revenue\' column with the mean revenue of the respective \'Product\' df[\'Revenue\'] = df.groupby(\'Product\')[\'Revenue\'].transform(lambda x: x.fillna(x.mean())) missing_values = df.isnull().sum() return df, missing_values def aggregate_data(df): Calculate total revenue generated by each \'Store\' and \'Date\' and return as a multi-index DataFrame aggregated_df = df.groupby([\'Store\', \'Date\'])[\'Revenue\'].sum().unstack().fillna(0) return aggregated_df def advanced_data_manipulation(df): Create pivot table, add total revenue column, and find the date with highest total revenue pivoted_df = df.pivot_table(values=\'Revenue\', index=\'Date\', columns=\'Store\', aggfunc=\'sum\', fill_value=0) pivoted_df[\'Total_Revenue\'] = pivoted_df.sum(axis=1) max_revenue_date = pivoted_df[\'Total_Revenue\'].idxmax() return pivoted_df, max_revenue_date def main(filename): # Task 1: Data Loading and Initial Inspection df, first_five_rows, summary_stats = load_and_inspect_data(filename) # Print the results of initial inspection print(\\"First 5 rows of DataFrame:n\\", first_five_rows) print(\\"Summary statistics:n\\", summary_stats) # Task 2: Handling Missing Data df, missing_values = handle_missing_data(df) # Print the columns with missing values and the number of missing entries print(\\"Missing values:n\\", missing_values) # Task 3: Data Aggregation aggregated_df = aggregate_data(df) # Print the multi-index DataFrame with total revenue by \'Store\' and \'Date\' print(\\"Aggregated DataFrame with total revenue:n\\", aggregated_df) # Task 4: Advanced Data Manipulation pivoted_df, max_revenue_date = advanced_data_manipulation(df) # Print the final pivoted DataFrame and the date with highest total revenue print(\\"Pivoted DataFrame with total revenue:n\\", pivoted_df) print(\\"Date with highest total revenue:\\", max_revenue_date) # Uncomment to run main function # if __name__ == \\"__main__\\": # main(\\"sales_data.csv\\")"},{"question":"# Objective Demonstrate your understanding of the `typing` module in Python by implementing a function that utilizes type hints, generic types, and other related concepts. # Problem Statement You are required to implement a function called `merge_lists` that merges two sorted lists into a single sorted list. The function must use type hints to indicate that it accepts lists of any type `T` that can be compared using the less-than operator (`<`). # Input - `list1` (List[T]): A sorted list of elements of type `T`. - `list2` (List[T]): Another sorted list of elements of type `T`. # Output - A sorted list (List[T]) that contains all elements from `list1` and `list2`. # Constraints - The input lists are already sorted in non-decreasing order. - Elements in the lists can be compared using the less-than operator (`<`). # Example ```python from typing import List, TypeVar, Generic T = TypeVar(\'T\') def merge_lists(list1: List[T], list2: List[T]) -> List[T]: # Your implementation here # Example usage: # Merge two lists of integers list1 = [1, 3, 5] list2 = [2, 4, 6] print(merge_lists(list1, list2)) # Output: [1, 2, 3, 4, 5, 6] # Merge two lists of strings list1 = [\\"apple\\", \\"orange\\"] list2 = [\\"banana\\", \\"peach\\"] print(merge_lists(list1, list2)) # Output: [\\"apple\\", \\"banana\\", \\"orange\\", \\"peach\\"] ``` # Notes 1. Ensure to use type hints for all function parameters and return type. 2. You can assume that the elements in the lists are sortable using the less-than operator. 3. You must use the concepts of generics from the `typing` module to ensure that the function works with any type that can be compared.","solution":"from typing import List, TypeVar T = TypeVar(\'T\') def merge_lists(list1: List[T], list2: List[T]) -> List[T]: Merges two sorted lists into a single sorted list. Args: - list1 (List[T]): A sorted list of elements of type T. - list2 (List[T]): Another sorted list of elements of type T. Returns: - List[T]: A sorted list that contains all elements from list1 and list2. merged_list = [] i, j = 0, 0 # Merge the two lists while both have elements left while i < len(list1) and j < len(list2): if list1[i] < list2[j]: merged_list.append(list1[i]) i += 1 else: merged_list.append(list2[j]) j += 1 # If there are remaining elements in list1, append them while i < len(list1): merged_list.append(list1[i]) i += 1 # If there are remaining elements in list2, append them while j < len(list2): merged_list.append(list2[j]) j += 1 return merged_list"},{"question":"# PyTorch Tensor Manipulation and Operations Problem Statement You are provided with a list of lists representing a 2D matrix. Implement a function `process_tensor(matrix: List[List[int]]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]` that performs the following operations using PyTorch tensors: 1. Convert the provided list of lists into a PyTorch tensor of dtype `torch.float32`. 2. Normalize the tensor such that its values lie between 0 and 1. 3. Find the row-wise maximum values from the normalized tensor. 4. Compute the gradient of the sum of the maximum values w.r.t the normalized tensor. 5. Return a tuple containing: - The normalized tensor. - The row-wise maximum values as a tensor. - The gradient tensor. Input - `matrix` (List[List[int]]): A list of lists, where each inner list represents a row of the 2D matrix. Output - A tuple containing three PyTorch tensors: 1. The normalized tensor of dtype `torch.float32`. 2. A tensor of row-wise maximum values from the normalized tensor. 3. The gradient tensor of the sum of the maximum values w.r.t the normalized tensor. Constraints - The input matrix will have at least one row and one column (i.e., it\'s non-empty). - Elements of the matrix are non-negative integers. Example ```python matrix = [ [1, 2, 3], [4, 5, 6] ] normalized_tensor, row_max, gradient = process_tensor(matrix) print(normalized_tensor) # Output: tensor([[0.0000, 0.2000, 0.4000], # [0.6000, 0.8000, 1.0000]], dtype=torch.float32) print(row_max) # Output: tensor([0.4000, 1.0000], dtype=torch.float32) print(gradient) # Output: tensor([[0., 0., 1.], # [0., 0., 1.]], dtype=torch.float32) ``` Implementation Requirements - Use PyTorch for all tensor operations. - Ensure the function correctly initializes tensors and performs operations as described. - The function should handle the autograd feature of PyTorch to compute gradients. Notes - The normalization should map the original tensor values to the range [0, 1] based on the maximum value in the entire matrix. - The gradient computation should use PyTorch\'s autograd functionality.","solution":"import torch from typing import List, Tuple def process_tensor(matrix: List[List[int]]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: # Convert list of lists to torch tensor tensor = torch.tensor(matrix, dtype=torch.float32) # Normalize the tensor max_value = torch.max(tensor) normalized_tensor = tensor / max_value # Ensure gradients are enabled for the normalized tensor normalized_tensor.requires_grad_(True) # Find row-wise maximum values row_max, _ = torch.max(normalized_tensor, dim=1) # Compute the sum of row-wise maximum values row_max_sum = row_max.sum() # Compute the gradient of the sum w.r.t the normalized tensor row_max_sum.backward() # Get the gradient gradient = normalized_tensor.grad return normalized_tensor, row_max, gradient"},{"question":"You are required to implement a custom logging system that reads from and writes to a file. The system should make use of the built-in \'open\' function but should also extend it to provide additional functionality using a custom wrapper class. Here are the requirements: 1. **CustomFile Class**: Create a class `CustomFile` that wraps around Python\'s built-in file object. This class should: - Initialize with a filename and mode (just like the built-in `open` function). - Provide a method `read_logs(count=-1)` that reads from the file and returns all text converted to lowercase. - Provide a method `write_log(message)` that writes a given message to the file, prefixed by a timestamp in the format `[YYYY-MM-DD HH:MM:SS]`. 2. **open_and_log Function**: Create a function `open_and_log` that: - Uses the \'builtins.open\' function to open a file. - Returns an instance of the `CustomFile` class to interact with the file. Requirements: - The `CustomFile` class should simulate some functionalities of file objects. Implement the `read_logs` and `write_log` methods as described. - The `read_logs` method should read from the file and convert all text to lowercase. - The `write_log` method should write a message to the file with a prefixed timestamp. Input: - For `CustomFile` methods: - `filename` (str): The name of the file. - `mode` (str): The mode in which to open the file. - `count` (int): The number of characters to read. Defaults to -1, meaning read until the end of the file. - `message` (str): The message to log. Output: - For `CustomFile` methods: - `read_logs`: Returns the file content in lowercase. - `write_log`: The message with a prefixed timestamp is appended to the file. # Constraints: - Do not use any imports other than `builtins`. - Assume the file operations will not fail (you do not need to handle exceptions). # Example usage: ```python # Example usage of open_and_log and CustomFile log_file = open_and_log(\'log.txt\', \'a+\') # Appends if file exists, creates otherwise log_file.write_log(\'This is a test log.\') print(log_file.read_logs()) # Outputs the contents of the file in lower case # Clean up by closing the file log_file.close() ``` # Notes: - Ensure you are very clear with the naming conventions, file handling modes, and expected behavior. - Focus on encapsulating functionality within the custom class and using built-in functions properly.","solution":"import builtins from datetime import datetime class CustomFile: def __init__(self, filename, mode): self.file = builtins.open(filename, mode) self.filename = filename self.mode = mode def close(self): self.file.close() def read_logs(self, count=-1): self.file.seek(0) # Go to the start of the file for reading return self.file.read(count).lower() def write_log(self, message): timestamp = datetime.now().strftime(\'[%Y-%m-%d %H:%M:%S]\') self.file.write(f\'{timestamp} {message}n\') self.file.flush() def open_and_log(filename, mode): return CustomFile(filename, mode)"},{"question":"Objective: Write a Python function that sets up a pseudo-terminal (PTY) where a user can run shell commands. The function should read commands from a list, execute them in the created PTY, and collect the output of each command. Function Signature: ```python def execute_in_pty(commands: List[str]) -> List[str]: ``` Input: - `commands` (List of strings): A list of shell commands to execute in the PTY. Output: - `results` (List of strings): A list containing the output of each command executed. Constraints: 1. The function should handle creating the PTY master-slave pair. 2. Commands should be executed in a child process connected to the slave end of the PTY. 3. The parent process should read from the master end of the PTY to capture the command output. 4. Handle exceptions and edge cases appropriately (e.g., empty command list, failing commands). 5. Ensure proper cleanup of resources after execution. Performance Requirements: - The function should efficiently manage the PTY and not leave any zombie processes. - It should be able to handle a list of up to 100 commands gracefully. Example: ```python commands = [\\"echo Hello World\\", \\"ls -l\\", \\"whoami\\"] output = execute_in_pty(commands) for line in output: print(line) ``` Expected Output Format (the actual content may vary based on your system configuration): ``` Hello World total 0 -rw-r--r-- 1 user group date time file1 -rw-r--r-- 1 user group date time file2 user ``` Additional Notes: - Use the `pty` module to create the pseudo-terminal. - Use `os.fork` to create the child process for executing commands. - Read the command output using the `os.read` on the master end of the PTY. - Ensure synchronization between command execution and output collection.","solution":"import os import pty from typing import List def execute_in_pty(commands: List[str]) -> List[str]: results = [] if not commands: return results for command in commands: master_fd, slave_fd = pty.openpty() pid = os.fork() if pid == 0: # Child process os.close(master_fd) os.dup2(slave_fd, 1) # Redirect stdout to the PTY slave os.dup2(slave_fd, 2) # Redirect stderr to the PTY slave os.close(slave_fd) os.execlp(\\"/bin/sh\\", \\"/bin/sh\\", \\"-c\\", command) else: # Parent process os.close(slave_fd) output = b\\"\\" try: while True: try: data = os.read(master_fd, 1024) if not data: break output += data except OSError: break finally: os.close(master_fd) os.waitpid(pid, 0) results.append(output.decode(\'utf-8\').strip()) return results"},{"question":"Shell Pipeline Exercise Using the `pipes` Module # Objective: You are required to create a Python function that uses the `pipes` module to process a text file. This function will transform the text in the file such that all lowercase alphabet characters are converted to uppercase. # Instructions: 1. Define a function `transform_file_to_uppercase(input_filepath: str, output_filepath: str) -> None`. 2. Inside this function, perform the following steps: - Create an instance of the `pipes.Template` class. - Append a command to the pipeline that converts all lowercase letters to uppercase (`tr a-z A-Z`). - Use the `copy()` method of the `Template` class to copy the contents of the input file to the output file through the pipeline. 3. Ensure that the function handles the file operations correctly and raises appropriate exceptions for I/O errors. # Constraints: - The function should only use the `pipes` module for handling pipelines. - The function should be compatible with Unix-like systems. - You must manage file operations with appropriate error handling. # Example: Suppose you have a text file named `input.txt` with the following content: ``` hello world this is a test file ``` After running `transform_file_to_uppercase(\'input.txt\', \'output.txt\')`, the `output.txt` file should contain: ``` HELLO WORLD THIS IS A TEST FILE ``` # Requirements: ```python import pipes def transform_file_to_uppercase(input_filepath: str, output_filepath: str) -> None: # Your implementation here pass # Example usage: # transform_file_to_uppercase(\'input.txt\', \'output.txt\') ``` Make sure to test the function with different input files to validate its correctness and robustness.","solution":"import pipes def transform_file_to_uppercase(input_filepath: str, output_filepath: str) -> None: try: template = pipes.Template() template.append(\'tr a-z A-Z\', \'--\') template.copy(input_filepath, output_filepath) except IOError as e: raise e"},{"question":"**Objective**: Validate understanding of file handling, byte streams, and compression/decompression using the `bz2` module in Python. **Problem Statement**: You are tasked with creating a file archiving utility that compresses multiple files into a single compressed `.bz2` file and then provides the functionality to decompress this archive back into its original files. # Functions to Implement 1. `compress_files(file_list, archive_name, compresslevel=9)` - **Input**: - `file_list`: A list of strings, where each string is the full path to a file to be compressed. - `archive_name`: A string, which will be the name of the output compressed archive (ending with `.bz2`). - `compresslevel`: An integer from 1 to 9, which specifies the level of compression. Default is 9. - **Output**: - None. - **Functionality**: - Compress all the files in `file_list` into a single archive named `archive_name` using the bzip2 compression algorithm. - The archive should store each file compressed separately in a way that they can be decompressed independently later. 2. `decompress_files(archive_name, output_dir)` - **Input**: - `archive_name`: A string, the name of the compressed archive file (ending with `.bz2`). - `output_dir`: A string, the directory path where each decompressed file will be stored. - **Output**: - None. - **Functionality**: - Decompress the `archive_name` file such that each original file is restored to the `output_dir`. **Constraints**: - You may assume that the `file_list` does not contain directories. - The provided paths for files and directory are valid. - You cannot use any external compression libraries other than `bz2`. **Example**: ```python file_list = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] archive_name = \'archive.bz2\' output_dir = \'./decompressed_files\' compress_files(file_list, archive_name) decompress_files(archive_name, output_dir) ``` After execution: - `archive.bz2` should contain the compressed data of `file1.txt`, `file2.txt`, and `file3.txt`. - The `output_dir` should contain `file1.txt`, `file2.txt`, and `file3.txt` restored to their original contents. # Note: - You should handle any necessary file I/O (open, read, write) within the functions. - Ensure that the decompressed files match their respective original files byte-for-byte.","solution":"import bz2 import os def compress_files(file_list, archive_name, compresslevel=9): Compress a list of files into a single bz2 archive. Args: - file_list: list of file paths to be compressed. - archive_name: output archive name ending with `.bz2`. - compresslevel: compression level from 1 to 9. with bz2.open(archive_name, \'wb\', compresslevel=compresslevel) as archive: for file_path in file_list: file_name = os.path.basename(file_path) file_size = os.path.getsize(file_path) # Read file contents with open(file_path, \'rb\') as file: file_data = file.read() # Write length of file name and file name archive.write(len(file_name).to_bytes(2, \'big\')) # 2 bytes for name length archive.write(file_name.encode(\'utf-8\')) # Write length of file data and file data archive.write(file_size.to_bytes(8, \'big\')) # 8 bytes for file size archive.write(file_data) def decompress_files(archive_name, output_dir): Decompress a bz2 archive into the specified directory. Args: - archive_name: the compressed archive name ending with `.bz2`. - output_dir: output directory to extract files into. with bz2.open(archive_name, \'rb\') as archive: while True: name_len_bytes = archive.read(2) if not name_len_bytes: break name_len = int.from_bytes(name_len_bytes, \'big\') file_name = archive.read(name_len).decode(\'utf-8\') file_size = int.from_bytes(archive.read(8), \'big\') file_data = archive.read(file_size) # Write the decompressed file to output directory output_file_path = os.path.join(output_dir, file_name) with open(output_file_path, \'wb\') as output_file: output_file.write(file_data)"},{"question":"**Question: DOM XML Manipulation** # Objective: You are given an XML document representing a book catalog. Your task is to write a Python function that manipulates the XML content using the `xml.dom.minidom` module. Specifically, you need to add a new book entry and modify the title of an existing book entry. # Input: - An XML string representing the catalog. - A dictionary representing the new book to be added, with keys: `id`, `title`, `author`, and `year`. - The `id` of the book whose title needs to be modified. - The new title for the book. # Output: - A pretty-printed XML string of the manipulated catalog. # Constraints: - Assume book IDs are unique and are integer values. - The catalog has no namespace; all elements are under the catalog root element. - The XML should be well-formed. # Performance Requirements: - The function should handle XML documents up to a size of approximately 1 MB efficiently. # Function Signature: ```python def manipulate_book_catalog(xml_string: str, new_book: dict, book_id_to_modify: int, new_title: str) -> str: pass ``` # Example: **Input:** ```python xml_string = <catalog> <book id=\\"1\\"> <title>Effective Python</title> <author>Brett Slatkin</author> <year>2015</year> </book> <book id=\\"2\\"> <title>Learning Python</title> <author>Mark Lutz</author> <year>2013</year> </book> </catalog> new_book = { \\"id\\": 3, \\"title\\": \\"Fluent Python\\", \\"author\\": \\"Luciano Ramalho\\", \\"year\\": 2015 } book_id_to_modify = 1 new_title = \\"Effective Python: Second Edition\\" ``` **Desired Output:** ```xml <?xml version=\\"1.0\\" ?> <catalog> <book id=\\"1\\"> <title>Effective Python: Second Edition</title> <author>Brett Slatkin</author> <year>2015</year> </book> <book id=\\"2\\"> <title>Learning Python</title> <author>Mark Lutz</author> <year>2013</year> </book> <book id=\\"3\\"> <title>Fluent Python</title> <author>Luciano Ramalho</author> <year>2015</year> </book> </catalog> ``` # Instructions: 1. Parse the input XML string to create a DOM Document. 2. Add a new book entry using the provided dictionary. 3. Modify the title of the specified book using its ID. 4. Return the updated XML as a pretty-printed string. Ensure to handle the DOM operations correctly, including appending new children, setting attribute values, and modifying existing nodes.","solution":"from xml.dom.minidom import parseString, Document def manipulate_book_catalog(xml_string: str, new_book: dict, book_id_to_modify: int, new_title: str) -> str: # Parse the XML string to a DOM Document doc = parseString(xml_string) # Find and modify the specified book title books = doc.getElementsByTagName(\'book\') for book in books: if book.getAttribute(\'id\') == str(book_id_to_modify): title_node = book.getElementsByTagName(\'title\')[0].firstChild title_node.nodeValue = new_title break # Create a new book entry new_book_element = doc.createElement(\'book\') new_book_element.setAttribute(\'id\', str(new_book[\'id\'])) title_element = doc.createElement(\'title\') title_text = doc.createTextNode(new_book[\'title\']) title_element.appendChild(title_text) author_element = doc.createElement(\'author\') author_text = doc.createTextNode(new_book[\'author\']) author_element.appendChild(author_text) year_element = doc.createElement(\'year\') year_text = doc.createTextNode(str(new_book[\'year\'])) year_element.appendChild(year_text) new_book_element.appendChild(title_element) new_book_element.appendChild(author_element) new_book_element.appendChild(year_element) # Append the new book to the catalog catalog = doc.getElementsByTagName(\'catalog\')[0] catalog.appendChild(new_book_element) # Return the pretty-printed XML string return doc.toprettyxml(indent=\\" \\")"},{"question":"**Question:** You are given a dataset of sales data for a retail company. Each row in the dataset represents a sale made, including information such as the product category, sub-category, sales amount, quantity sold, and the date of sale. Your task is to write a function `analyze_sales` that performs the following operations using the pandas library: 1. Group the data by product category and sub-category. 2. For each group, calculate the following: - Total sales amount - Total quantity sold - The maximum sales amount for a single sale - The average sales amount - The number of unique sale dates 3. Return the results as a DataFrame with the following columns: - \'Category\' - \'Sub-Category\' - \'Total Sales\' - \'Total Quantity\' - \'Max Sale\' - \'Average Sale\' - \'Unique Sale Dates\' Input - `df`: A pandas DataFrame with the following columns: - \'Category\': The category of the product (string) - \'Sub-Category\': The sub-category of the product (string) - \'Sales\': The sales amount (float) - \'Quantity\': The quantity sold (integer) - \'Date\': The date of sale (string in \\"yyyy-mm-dd\\" format) Output - A pandas DataFrame with the columns [\'Category\', \'Sub-Category\', \'Total Sales\', \'Total Quantity\', \'Max Sale\', \'Average Sale\', \'Unique Sale Dates\'] Example ```python import pandas as pd data = { \'Category\': [\'Furniture\', \'Furniture\', \'Office Supplies\', \'Office Supplies\', \'Technology\'], \'Sub-Category\': [\'Chairs\', \'Tables\', \'Binders\', \'Labels\', \'Phones\'], \'Sales\': [150.0, 200.0, 50.0, 25.0, 300.0], \'Quantity\': [2, 1, 10, 5, 1], \'Date\': [\'2023-01-01\', \'2023-01-02\', \'2023-01-01\', \'2023-01-03\', \'2023-01-01\'] } df = pd.DataFrame(data) result = analyze_sales(df) print(result) ``` The output should be a DataFrame similar to: ``` Category Sub-Category Total Sales Total Quantity Max Sale Average Sale Unique Sale Dates 0 Furniture Chairs 150.0 2 150.0 150.0 1 1 Furniture Tables 200.0 1 200.0 200.0 1 2 Office Supplies Binders 50.0 10 50.0 50.0 1 3 Office Supplies Labels 25.0 5 25.0 25.0 1 4 Technology Phones 300.0 1 300.0 300.0 1 ``` Constraints - Assume the input DataFrame is correctly formatted and contains no missing or malformed data. - Use pandas version 1.0.0 or later for compatibility.","solution":"import pandas as pd def analyze_sales(df): Analyzes the sales data grouping by category and sub-category. Parameters: df (DataFrame): The sales data with columns \'Category\', \'Sub-Category\', \'Sales\', \'Quantity\', and \'Date\'. Returns: DataFrame: The summary of sales analysis with columns \'Category\', \'Sub-Category\', \'Total Sales\', \'Total Quantity\', \'Max Sale\', \'Average Sale\', and \'Unique Sale Dates\'. grouped = df.groupby([\'Category\', \'Sub-Category\']).agg( Total_Sales=(\'Sales\', \'sum\'), Total_Quantity=(\'Quantity\', \'sum\'), Max_Sale=(\'Sales\', \'max\'), Average_Sale=(\'Sales\', \'mean\'), Unique_Sale_Dates=(\'Date\', pd.Series.nunique) ).reset_index() grouped.columns = [\'Category\', \'Sub-Category\', \'Total Sales\', \'Total Quantity\', \'Max Sale\', \'Average Sale\', \'Unique Sale Dates\'] return grouped"},{"question":"# **Coding Assessment Question** Objective: Implement a text processing pipeline using the deprecated `pipes` module and then convert it to utilize the `subprocess` module. Problem Statement: You need to write a Python function that reads a given text file, applies a series of shell commands to transform the text, and then writes the result to an output file. Specifically, the transformations are: 1. Convert all text to uppercase. 2. Replace all spaces with underscores. Your implementation should first use the `pipes` module and then be converted to use the `subprocess` module. Requirements: 1. Implement a function `process_text_pipes(input_file: str, output_file: str) -> None`: - **Input:** - `input_file`: Path to the input text file. - `output_file`: Path to the output text file. - **Output:** - The function does not return anything, but it should write the transformed text to `output_file`. - **Constraints:** - Use the `pipes` module to build the pipeline. - **Example:** ```python process_text_pipes(\'input.txt\', \'output.txt\') ``` If `input.txt` contains `hello world`, after processing, `output.txt` should contain `HELLO_WORLD`. 2. Convert the above function to `process_text_subprocess(input_file: str, output_file: str) -> None` using the `subprocess` module. - **Input:** - `input_file`: Path to the input text file. - `output_file`: Path to the output text file. - **Output:** - The function does not return anything, but it should write the transformed text to `output_file`. - **Constraints:** - Use the `subprocess` module for the pipeline. - **Example:** ```python process_text_subprocess(\'input.txt\', \'output.txt\') ``` If `input.txt` contains `hello world`, after processing, `output.txt` should contain `HELLO_WORLD`. Hints: - For the `pipes` module: - Use `Template.append` to add shell commands to the pipeline. - Use `Template.open` to open the output file. - For the `subprocess` module: - Consider using `subprocess.run` or `subprocess.Popen` to execute the shell commands. Good luck and happy coding!","solution":"import pipes import subprocess def process_text_pipes(input_file: str, output_file: str) -> None: Processes the text from input_file through a pipeline that converts it to uppercase and replaces spaces with underscores, then writes it to output_file. Uses the deprecated \'pipes\' module. template = pipes.Template() template.append(\\"tr \'[:lower:]\' \'[:upper:]\'\\", \'--\') template.append(\\"tr \' \' \'_\'\\", \'--\') with template.open(output_file, \'w\') as f: with open(input_file, \'r\') as infile: f.write(infile.read()) def process_text_subprocess(input_file: str, output_file: str) -> None: Processes the text from input_file through a pipeline that converts it to uppercase and replaces spaces with underscores, then writes it to output_file. Uses the \'subprocess\' module. with open(input_file, \'r\') as infile, open(output_file, \'w\') as outfile: # Convert text to uppercase and replace spaces with underscores p1 = subprocess.Popen([\'tr\', \'[:lower:]\', \'[:upper:]\'], stdin=infile, stdout=subprocess.PIPE) p2 = subprocess.Popen([\'tr\', \' \', \'_\'], stdin=p1.stdout, stdout=outfile) p1.stdout.close() p2.communicate()"},{"question":"**Objective:** Demonstrate your understanding of the `pathlib` module by implementing a program that performs various filesystem operations. **Question:** Write a Python function named `analyze_directory` that takes a single argument `dir_path` of type `Path` (from the `pathlib` module). The function should perform the following tasks: 1. Check if the provided path is a directory. If not, raise a `ValueError` with the message \\"Provided path is not a directory\\". 2. List all files in the directory and its subdirectories recursively, including their relative paths from `dir_path`. 3. Identify and list all symbolic links in the directory and its subdirectories. 4. Write the names and relative paths of all regular files, directories, and symbolic links into separate text files named `files.txt`, `directories.txt`, and `symlinks.txt` respectively in the given directory. Each path should be listed on a new line. **Input:** - `dir_path`: A `Path` object representing the directory path. **Output:** - The function should not return any value. The output will be the creation and content of three text files (`files.txt`, `directories.txt`, and `symlinks.txt`) in the given directory. **Example:** If `dir_path` is `Path(\'/home/user/test\')`, the directory structure is as follows: ``` /home/user/test/ ├── file1.txt ├── link_to_file2 -> /another/path/file2.txt ├── subdir1/ │ ├── file3.txt │ └── subdir2/ │ └── file4.txt ``` After running `analyze_directory(Path(\'/home/user/test\'))`, the created files will contain: - `files.txt`: ``` ./file1.txt ./subdir1/file3.txt ./subdir1/subdir2/file4.txt ``` - `directories.txt`: ``` ./subdir1 ./subdir1/subdir2 ``` - `symlinks.txt`: ``` ./link_to_file2 ``` **Constraints:** - Use appropriate `Path` methods to navigate directories and identify file types. - Ensure correct and efficient handling of relative paths considering symlinks. - The function should handle large directories efficiently by avoiding unnecessary memory usage. ```python from pathlib import Path def analyze_directory(dir_path: Path): # Part 1: Check if dir_path is a directory if not dir_path.is_dir(): raise ValueError(\\"Provided path is not a directory\\") # Initialize lists for different types of path files_list = [] directories_list = [] symlinks_list = [] # Part 2: Walk through the directory and classify paths for path in dir_path.rglob(\'*\'): relative_path = path.relative_to(dir_path) if path.is_file(): files_list.append(relative_path) elif path.is_dir(): directories_list.append(relative_path) elif path.is_symlink(): symlinks_list.append(relative_path) # Part 4: Write paths to corresponding text files with (dir_path / \'files.txt\').open(\'w\') as f: for file in files_list: f.write(f\\"{file}n\\") with (dir_path / \'directories.txt\').open(\'w\') as f: for directory in directories_list: f.write(f\\"{directory}n\\") with (dir_path / \'symlinks.txt\').open(\'w\') as f: for symlink in symlinks_list: f.write(f\\"{symlink}n\\") ``` **Expectations:** - Proper use of `pathlib` methods. - Correct identification and classification of paths. - Efficient and clean code that handles different path scenarios effectively.","solution":"from pathlib import Path def analyze_directory(dir_path: Path): Analyzes the given directory path and creates three text files listing regular files, directories, and symbolic links respectively. Args: dir_path (Path): The directory path to analyze. Raises: ValueError: If the provided path is not a directory. # Part 1: Check if dir_path is a directory if not dir_path.is_dir(): raise ValueError(\\"Provided path is not a directory\\") # Initialize lists for different types of path files_list = [] directories_list = [] symlinks_list = [] # Part 2: Walk through the directory and classify paths for path in dir_path.rglob(\'*\'): relative_path = f\\"./{path.relative_to(dir_path)}\\" if path.is_file(): files_list.append(relative_path) elif path.is_dir(): directories_list.append(relative_path) elif path.is_symlink(): symlinks_list.append(relative_path) # Part 4: Write paths to corresponding text files (dir_path / \'files.txt\').write_text(\\"n\\".join(files_list) + \\"n\\") (dir_path / \'directories.txt\').write_text(\\"n\\".join(directories_list) + \\"n\\") (dir_path / \'symlinks.txt\').write_text(\\"n\\".join(symlinks_list) + \\"n\\")"},{"question":"**Objective:** Implement a function that programmatically ensures the installation of `pip` in a Python environment while following given installation constraints. **Question:** You are required to write a Python function called `ensure_pip_installation` that uses the `ensurepip` package to ensure that `pip` is installed in the current environment. The function should allow for customization of the installation as described below: **Function Signature:** ```python def ensure_pip_installation( upgrade=False, user=False, altinstall=False, default_pip=False, verbosity=0 ) -> None: Ensures that pip is installed in the current environment. Parameters: upgrade (bool): Whether or not to upgrade an existing installation of pip. user (bool): Whether to install pip using the user scheme. altinstall (bool): Whether to omit installing the \'pipX\' script. default_pip (bool): Whether to install the \'pip\' script in addition to \'pipX.Y\' scripts. verbosity (int): The level of output verbosity (0 for minimal output). Raises: ValueError: If both altinstall and default_pip are set to True. pass ``` **Requirements:** 1. The function should use the `ensurepip.bootstrap()` method to bootstrap `pip` installation. 2. The function should handle cases where both `altinstall` and `default_pip` are set to `True` by raising a `ValueError`. 3. The function should not return any value. 4. Ensure that you handle all parameters correctly and pass them to the `ensurepip.bootstrap()` function. **Constraints:** - Do not access the internet in your function. - Ensure that the function works with the default Python environment settings if specific parameters are not provided. **Example Usage:** ```python # Attempt to install or upgrade pip using the user scheme with verbose output ensure_pip_installation(upgrade=True, user=True, verbosity=2) ``` This function call should ensure that `pip` is installed or upgraded in the user packages directory with verbose output of the process. **Hints:** - Refer to the `ensurepip` documentation for details about the functions and their parameters. - Remember that you need to raise a `ValueError` if `altinstall` and `default_pip` are both set to `True`.","solution":"import ensurepip def ensure_pip_installation( upgrade=False, user=False, altinstall=False, default_pip=False, verbosity=0 ) -> None: Ensures that pip is installed in the current environment. Parameters: upgrade (bool): Whether or not to upgrade an existing installation of pip. user (bool): Whether to install pip using the user scheme. altinstall (bool): Whether to omit installing the \'pipX\' script. default_pip (bool): Whether to install the \'pip\' script in addition to \'pipX.Y\' scripts. verbosity (int): The level of output verbosity (0 for minimal output). Raises: ValueError: If both altinstall and default_pip are set to True. if altinstall and default_pip: raise ValueError(\\"Cannot set both altinstall and default_pip to True\\") ensurepip.bootstrap( upgrade=upgrade, user=user, altinstall=altinstall, default_pip=default_pip, verbosity=verbosity )"},{"question":"# PyTorch Coding Assessment Objective: To assess the student\'s understanding of PyTorch and TorchScript by implementing a neural network model and performing both forward and backward passes manually. This question will evaluate knowledge of tensors, autograd, and TorchScript functionalities. Problem Statement: You are required to implement a custom single-layer neural network from scratch using PyTorch. The network should perform a simple linear transformation followed by a ReLU activation. Your task includes: 1. Defining the model using PyTorch `nn.Module`. 2. Implementing manual forward and backward passes. 3. Converting the model to TorchScript and validating the model by comparing the output with the original PyTorch model. Requirements: 1. **Model Definition**: - Define a class `SimpleNN` that inherits from `torch.nn.Module`. - The model should contain one linear layer followed by a ReLU activation function. - The input layer size, output layer size, and any other hyperparameters should be customizable via the class constructor. 2. **Manual Forward and Backward Passes**: - Implement forward pass manually using PyTorch tensor operations. - Implement backward pass manually by computing gradients without using PyTorch\'s autograd. 3. **TorchScript Conversion**: - Convert the PyTorch model defined in part (1) into a TorchScript model. - Verify the correctness by passing a sample input through both models and comparing the outputs. Input and Output formats: - The class `SimpleNN` should accept three parameters: `input_size`, `output_size`, and `learning_rate`. - The input to the model will be a tensor of shape `[batch_size, input_size]`. - The output of the model should be a tensor of shape `[batch_size, output_size]`. Constraints: - Do not use any high-level custom library functions like `torch.autograd.grad` for manual gradient computation. - Ensure that the manual gradient computation is correct by comparing with PyTorch\'s in-built autograd when needed. Example: ```python import torch import torch.nn as nn import torch.nn.functional as F import torch.jit as jit class SimpleNN(nn.Module): def __init__(self, input_size, output_size, learning_rate=0.01): super(SimpleNN, self).__init__() self.linear = nn.Linear(input_size, output_size) self.learning_rate = learning_rate def forward(self, x): out = self.linear(x) out = F.relu(out) return out def manual_forward(self, x): # Perform manual forward pass pass def manual_backward(self, x, y, y_pred): # Perform manual backward pass to compute gradients pass # Sample usage: input_size = 3 output_size = 2 learning_rate = 0.01 model = SimpleNN(input_size, output_size, learning_rate) scripted_model = jit.script(model) # Sample input tensor x = torch.randn((5, input_size)) # Forward pass through both models output = model(x) scripted_output = scripted_model(x) # Ensure outputs are approximately the same assert torch.allclose(output, scripted_output, atol=1e-6), \\"Outputs do not match!\\" ``` Evaluation Criteria: - Correctness of the model definition and implementation. - Proper implementation of the manual forward and backward passes. - Accuracy and correctness of TorchScript conversion and its validation. - Code clarity, comments, and adherence to Python and PyTorch best practices.","solution":"import torch import torch.nn as nn import torch.nn.functional as F import torch.jit as jit class SimpleNN(nn.Module): def __init__(self, input_size, output_size, learning_rate=0.01): super(SimpleNN, self).__init__() self.linear = nn.Linear(input_size, output_size) self.learning_rate = learning_rate def forward(self, x): out = self.linear(x) out = F.relu(out) return out def manual_forward(self, x): # Perform manual forward pass: linear transformation followed by ReLU self.z = torch.matmul(x, self.linear.weight.t()) + self.linear.bias self.a = F.relu(self.z) return self.a def manual_backward(self, x, y, y_pred): # Compute the gradients manually # Loss function: Mean Squared Error loss = torch.sum((y_pred - y)**2) / y.size(0) # Compute gradients for linear layer parameters dL_dy_pred = 2 * (y_pred - y) / y.size(0) dy_pred_dz = y_pred.clone() dy_pred_dz[y_pred > 0] = 1 dy_pred_dz[y_pred <= 0] = 0 dL_dz = dL_dy_pred * dy_pred_dz self.dL_dw = torch.matmul(dL_dz.t(), x) self.dL_db = torch.sum(dL_dz, dim=0) def update_parameters(self): # Update the parameters self.linear.weight.data -= self.learning_rate * self.dL_dw self.linear.bias.data -= self.learning_rate * self.dL_db # Sample usage: input_size = 3 output_size = 2 learning_rate = 0.01 model = SimpleNN(input_size, output_size, learning_rate) scripted_model = jit.script(model) # Sample input and output tensors x = torch.randn((5, input_size)) y = torch.randn((5, output_size)) # Forward pass through both models output = model(x) scripted_output = scripted_model(x) # Ensure outputs are approximately the same assert torch.allclose(output, scripted_output, atol=1e-6), \\"Outputs do not match!\\" # Manual forward and backward pass y_pred = model.manual_forward(x) model.manual_backward(x, y, y_pred) model.update_parameters()"},{"question":"You are given a dataset containing information about flights (`flight_data.csv`). The dataset contains the following columns: - `Year` (e.g., 2019) - `Month` (e.g., Jan, Feb, Mar, etc.) - `Passengers` (number of passengers) Use the `seaborn` library to perform the following tasks: 1. Load the data from the CSV file into a Pandas DataFrame. 2. Create a line plot to visualize the number of passengers over time. Use `Year` and `Month` together on the x-axis and `Passengers` on the y-axis. 3. Enhance the plot by: - Differentiating the lines by Year using the `hue` attribute. - Adding appropriate titles, labels, and a legend to the plot. - Customizing the palette and style to improve plot readability. **Input:** - Path to the CSV file: `flight_data.csv` **Output:** - Display the customized line plot as specified. **Constraints:** - Make sure to handle any missing values in the dataset appropriately. - Ensure the x-axis shows the data in the correct chronological order. ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the data df = pd.read_csv(\'flight_data.csv\') # Create a line plot with enhancements plt.figure(figsize=(14, 7)) # Set the figure size sns.lineplot(data=df, x=\'Month_Year\', y=\'Passengers\', hue=\'Year\', palette=\'tab10\') # Create the line plot # Enhance the plot with titles, labels, and legend plt.title(\'Number of Passengers Over Time\') plt.xlabel(\'Time\') plt.ylabel(\'Passengers\') plt.legend(title=\'Year\', loc=\'upper left\') # Display the plot plt.show() ``` Note: - You\'ll need to create a new column `Month_Year` that combines `Year` and `Month` in the correct order for the x-axis. - The `palette=\'tab10\'` is a suggested palette; you can choose any other palette available in seaborn.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_and_plot_flight_data(file_path): Loads flight data from a CSV file, creates and displays a customized line plot of number of passengers over time, with enhancements. Parameters: - file_path (str): Path to the CSV file containing flight data with columns \'Year\', \'Month\', and \'Passengers\'. # Load the data df = pd.read_csv(file_path) # Handle missing values df.dropna(inplace=True) # Combine \'Year\' and \'Month\' into a \'Year-Month\' column to sort chronologically df[\'Year-Month\'] = df[\'Year\'].astype(str) + \'-\' + df[\'Month\'] df[\'Year-Month\'] = pd.to_datetime(df[\'Year-Month\'], format=\'%Y-%b\') df.sort_values(\'Year-Month\', inplace=True) # Create the line plot plt.figure(figsize=(14, 7)) # Set the figure size sns.lineplot(data=df, x=\'Year-Month\', y=\'Passengers\', hue=\'Year\', palette=\'tab10\') # Create the line plot # Enhance the plot with titles, labels, and legend plt.title(\'Number of Passengers Over Time\') plt.xlabel(\'Time\') plt.ylabel(\'Passengers\') plt.legend(title=\'Year\', loc=\'upper left\') # Display the plot plt.show()"},{"question":"# Nearest Neighbors Classification and Regression using `sklearn.neighbors` In this coding assessment, you are required to demonstrate your understanding of the nearest neighbors algorithms provided by the `sklearn.neighbors` module. You will implement a machine learning pipeline that performs both classification and regression tasks using the appropriate nearest neighbors algorithms. Requirements: 1. **Loading Data**: - You will use the `Iris` dataset for classification tasks. - You will use the `Diabetes` dataset for regression tasks. Both datasets can be loaded using the `load_iris` and `load_diabetes` functions from `sklearn.datasets`. 2. **Classification Task**: - Implement a classification pipeline using the `KNeighborsClassifier`. - Use a grid search to find the best value of `k` (number of neighbors) within a specified range (e.g., 1 to 10). - Evaluate the model using cross-validation and report the best parameters and the corresponding accuracy. 3. **Regression Task**: - Implement a regression pipeline using the `KNeighborsRegressor`. - Use a grid search to find the optimal value of `k` within a specified range (e.g., 1 to 10). - Evaluate the model using cross-validation and report the best parameters and the corresponding mean squared error. 4. **Advanced Requirement**: - For the classification task, include an implementation using `RadiusNeighborsClassifier` and compare the accuracy with `KNeighborsClassifier`. - For the regression task, implement the regression using `RadiusNeighborsRegressor` and compare the mean squared error with `KNeighborsRegressor`. Expected Input and Output Format: - **Input:** - None (datasets are loaded within the task). - **Output:** - Print the best `k` for both classification and regression. - Print the accuracy for the best `k` in classification. - Print the mean squared error for the best `k` in regression. - Print the accuracy/mse comparison between `KNeighbors` and `RadiusNeighbors` for both tasks. Constraints: - Ensure the implementation is efficient and uses the `GridSearchCV` from `sklearn.model_selection` to automate the hyperparameter tuning. # Example: Code Setup: ```python from sklearn.datasets import load_iris, load_diabetes from sklearn.model_selection import GridSearchCV, train_test_split from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier, KNeighborsRegressor, RadiusNeighborsRegressor from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score, mean_squared_error # Load datasets iris = load_iris() diabetes = load_diabetes() X_iris, y_iris = iris.data, iris.target X_diabetes, y_diabetes = diabetes.data, diabetes.target # Classification pipeline knn_classifier = KNeighborsClassifier() param_grid_classifier = {\'n_neighbors\': range(1, 11)} grid_search_knn = GridSearchCV(knn_classifier, param_grid_classifier, cv=5) grid_search_knn.fit(X_iris, y_iris) print(\\"Best k for KNeighborsClassifier:\\", grid_search_knn.best_params_[\'n_neighbors\']) print(\\"Best accuracy for KNeighborsClassifier:\\", grid_search_knn.best_score_) # Additional code for RadiusNeighborsClassifier and the corresponding accuracy comparison # Regression pipeline knn_regressor = KNeighborsRegressor() param_grid_regressor = {\'n_neighbors\': range(1, 11)} grid_search_regressor = GridSearchCV(knn_regressor, param_grid_regressor, cv=5) grid_search_regressor.fit(X_diabetes, y_diabetes) print(\\"Best k for KNeighborsRegressor:\\", grid_search_regressor.best_params_[\'n_neighbors\']) print(\\"Mean Squared Error for KNeighborsRegressor:\\", -grid_search_regressor.best_score_) # Additional code for RadiusNeighborsRegressor and the corresponding mse comparison ``` Ensure that you complete the additional parts of the code for `RadiusNeighborsClassifier` and `RadiusNeighborsRegressor` and also include the evaluation and comparison parts.","solution":"from sklearn.datasets import load_iris, load_diabetes from sklearn.model_selection import GridSearchCV, cross_val_score from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier, KNeighborsRegressor, RadiusNeighborsRegressor from sklearn.metrics import accuracy_score, mean_squared_error def nearest_neighbors_classification_and_regression(): # Load datasets iris = load_iris() diabetes = load_diabetes() X_iris, y_iris = iris.data, iris.target X_diabetes, y_diabetes = diabetes.data, diabetes.target # Classification with KNeighborsClassifier knn_classifier = KNeighborsClassifier() param_grid_classifier = {\'n_neighbors\': range(1, 11)} grid_search_knn = GridSearchCV(knn_classifier, param_grid_classifier, cv=5) grid_search_knn.fit(X_iris, y_iris) best_k_classifier = grid_search_knn.best_params_[\'n_neighbors\'] best_score_classifier = grid_search_knn.best_score_ print(f\\"Best k for KNeighborsClassifier: {best_k_classifier}\\") print(f\\"Best accuracy for KNeighborsClassifier: {best_score_classifier}\\") # Classification with RadiusNeighborsClassifier radius_classifier = RadiusNeighborsClassifier(radius=1.0) param_grid_radius = {\'radius\': [0.5, 1.0, 1.5, 2.0]} grid_search_radius = GridSearchCV(radius_classifier, param_grid_radius, cv=5) grid_search_radius.fit(X_iris, y_iris) best_radius_classifier = grid_search_radius.best_params_[\'radius\'] best_score_radius_classifier = grid_search_radius.best_score_ print(f\\"Best radius for RadiusNeighborsClassifier: {best_radius_classifier}\\") print(f\\"Best accuracy for RadiusNeighborsClassifier: {best_score_radius_classifier}\\") # Regression with KNeighborsRegressor knn_regressor = KNeighborsRegressor() param_grid_regressor = {\'n_neighbors\': range(1, 11)} grid_search_regressor = GridSearchCV(knn_regressor, param_grid_regressor, cv=5, scoring=\'neg_mean_squared_error\') grid_search_regressor.fit(X_diabetes, y_diabetes) best_k_regressor = grid_search_regressor.best_params_[\'n_neighbors\'] best_score_regressor = -grid_search_regressor.best_score_ print(f\\"Best k for KNeighborsRegressor: {best_k_regressor}\\") print(f\\"Mean Squared Error for KNeighborsRegressor: {best_score_regressor}\\") # Regression with RadiusNeighborsRegressor radius_regressor = RadiusNeighborsRegressor(radius=1.0) param_grid_radius_regressor = {\'radius\': [0.5, 1.0, 1.5, 2.0]} grid_search_radius_regressor = GridSearchCV(radius_regressor, param_grid_radius_regressor, cv=5, scoring=\'neg_mean_squared_error\') grid_search_radius_regressor.fit(X_diabetes, y_diabetes) best_radius_regressor = grid_search_radius_regressor.best_params_[\'radius\'] best_score_radius_regressor = -grid_search_radius_regressor.best_score_ print(f\\"Best radius for RadiusNeighborsRegressor: {best_radius_regressor}\\") print(f\\"Mean Squared Error for RadiusNeighborsRegressor: {best_score_radius_regressor}\\")"},{"question":"# Asynchronous Logging Server **Objective:** You are tasked with creating an asynchronous logging server using the `asyncore` library. The server should listen for incoming log messages and write them to a file. Each log message will include a timestamp and a log entry. **Requirements:** 1. **LogServer Class:** - Inherit from `asyncore.dispatcher`. - Constructor should accept `host`, `port`, and `logfile` as parameters. - Create a socket, set it to reusable, bind it to the provided host and port, and set it to listen for incoming connections. - Implement the `handle_accepted` method to accept incoming connections and create a `LogHandler` instance. 2. **LogHandler Class:** - Inherit from `asyncore.dispatcher_with_send`. - Constructor should accept a socket object and a logfile name as parameters. - Implement `handle_read` method to read data from the socket. Each received message should be appended to the log file with a timestamp in the format `[YYYY-MM-DD HH:MM:SS] message`. - Ensure that the handler closes the connection when done. 3. **Main Function:** - Instantiate and run the `LogServer` to listen on `localhost:9090` and write logs to a file named `server.log`. **Constraints:** - Use the `asyncore` library. - Ensure proper resource management and error handling. **Performance Requirement:** - The server should handle multiple connections efficiently without blocking. # Expected Functionality: 1. When a client sends a log message to `localhost:9090`, the server appends it to `server.log` with a timestamp. 2. Each client connection should be handled asynchronously. # Example: If a client sends \\"Test message\\" at \\"2023-10-10 12:34:56\\", the following should be appended to `server.log`: ``` [2023-10-10 12:34:56] Test message ``` # Starter Code: ```python import asyncore import datetime class LogServer(asyncore.dispatcher): def __init__(self, host, port, logfile): super().__init__() self.create_socket() self.set_reuse_addr() self.bind((host, port)) self.listen(5) self.logfile = logfile def handle_accepted(self, sock, addr): print(f\'Incoming connection from {addr}\') handler = LogHandler(sock, self.logfile) class LogHandler(asyncore.dispatcher_with_send): def __init__(self, sock, logfile): super().__init__(sock) self.logfile = logfile def handle_read(self): data = self.recv(8192) if data: timestamp = datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') log_entry = f\'[{timestamp}] {data.decode(\\"utf-8\\")}n\' with open(self.logfile, \'a\') as logfile: logfile.write(log_entry) def main(): server = LogServer(\'localhost\', 9090, \'server.log\') asyncore.loop() if __name__ == \'__main__\': main() ``` Complete the implementation as described and ensure the server works as expected.","solution":"import asyncore import datetime import socket class LogServer(asyncore.dispatcher): def __init__(self, host, port, logfile): super().__init__() self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) self.logfile = logfile print(f\\"LogServer started on {host}:{port}, writing to {logfile}\\") def handle_accepted(self, sock, addr): print(f\'Incoming connection from {addr}\') handler = LogHandler(sock, self.logfile) class LogHandler(asyncore.dispatcher_with_send): def __init__(self, sock, logfile): super().__init__(sock) self.logfile = logfile def handle_read(self): data = self.recv(8192) if data: timestamp = datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') log_entry = f\'[{timestamp}] {data.decode(\\"utf-8\\")}n\' with open(self.logfile, \'a\') as logfile: logfile.write(log_entry) self.close() def main(): server = LogServer(\'localhost\', 9090, \'server.log\') asyncore.loop() if __name__ == \'__main__\': main()"},{"question":"Question **Understanding and Implementing Partial Dependence and Individual Conditional Expectation Plots** # Objective This question assesses your understanding of Partial Dependence Plots (PDP) and Individual Conditional Expectation (ICE) plots, and your ability to implement them using scikit-learn\'s `sklearn.inspection` module. # Task 1. **Data Preparation:** - Use the Iris dataset from `sklearn.datasets`. - Split the data into training and test sets. 2. **Model Training:** - Train a `GradientBoostingClassifier` on the training data. Use appropriate hyperparameters for this task. 3. **Partial Dependence Plots:** - Generate one-way Partial Dependence Plots for features 0 and 1. - Generate a two-way Partial Dependence Plot for features 0 and 1. 4. **Individual Conditional Expectation Plots:** - Generate ICE plots for features 0 and 1. - Overlay PDP on the ICE plots. 5. **Output Raw Partial Dependence Values:** - Use the `partial_dependence` function to obtain and display the raw values of the partial dependence function for feature 0. - Print the `average` and `grid_values` from the resulting dictionary. # Constraints 1. Use `GradientBoostingClassifier` from `sklearn.ensemble`. 2. The training and testing splitting ratio should be 80:20. 3. Ensure your code is efficient and well-documented. # Input None (the Iris dataset should be loaded internally within the function). # Output 1. Display of PDPs and ICE plots. 2. Printed raw partial dependence values for feature 0 (`average` and `grid_values`). # Performance Requirements - The implementation should generate the plots efficiently. - Ensure clarity in your plots with appropriate labels and legends. # Skeleton Code ```python import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.ensemble import GradientBoostingClassifier from sklearn.inspection import PartialDependenceDisplay, partial_dependence from sklearn.model_selection import train_test_split # Step 1: Data Preparation def load_and_split_data(test_size=0.2): iris = load_iris() X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=test_size, random_state=42) return X_train, X_test, y_train, y_test # Step 2: Model Training def train_model(X_train, y_train): clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42) clf.fit(X_train, y_train) return clf # Step 3: Partial Dependence Plots def generate_pdp(clf, X): features = [0, 1, (0, 1)] PartialDependenceDisplay.from_estimator(clf, X, features) plt.show() # Step 4: Individual Conditional Expectation Plots def generate_ice(clf, X): features = [0, 1] PartialDependenceDisplay.from_estimator(clf, X, features, kind=\'both\') plt.show() # Step 5: Raw Partial Dependence Values def get_partial_dependence_raw_values(clf, X): results = partial_dependence(clf, X, [0]) print(\\"Average Dependence: \\", results[\'average\']) print(\\"Grid Values: \\", results[\'grid_values\']) def main(): # Load and split data X_train, X_test, y_train, y_test = load_and_split_data() # Train model clf = train_model(X_train, y_train) # Generate PDP plots print(\\"Generating PDP plots...\\") generate_pdp(clf, X_test) # Generate ICE plots print(\\"Generating ICE plots...\\") generate_ice(clf, X_test) # Get and print raw partial dependence values print(\\"Fetching raw partial dependence values for feature 0...\\") get_partial_dependence_raw_values(clf, X_test) if __name__ == \\"__main__\\": main() ```","solution":"import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.ensemble import GradientBoostingClassifier from sklearn.inspection import PartialDependenceDisplay, partial_dependence from sklearn.model_selection import train_test_split # Step 1: Data Preparation def load_and_split_data(test_size=0.2): iris = load_iris() X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=test_size, random_state=42) return X_train, X_test, y_train, y_test # Step 2: Model Training def train_model(X_train, y_train): clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42) clf.fit(X_train, y_train) return clf # Step 3: Partial Dependence Plots def generate_pdp(clf, X): features = [0, 1, (0, 1)] PartialDependenceDisplay.from_estimator(clf, X, features) plt.show() # Step 4: Individual Conditional Expectation Plots def generate_ice(clf, X): features = [0, 1] PartialDependenceDisplay.from_estimator(clf, X, features, kind=\'both\') plt.show() # Step 5: Raw Partial Dependence Values def get_partial_dependence_raw_values(clf, X): results = partial_dependence(clf, X, [0]) print(\\"Average Dependence: \\", results[\'average\']) print(\\"Grid Values: \\", results[\'grid_values\']) def main(): # Load and split data X_train, X_test, y_train, y_test = load_and_split_data() # Train model clf = train_model(X_train, y_train) # Generate PDP plots print(\\"Generating PDP plots...\\") generate_pdp(clf, X_test) # Generate ICE plots print(\\"Generating ICE plots...\\") generate_ice(clf, X_test) # Get and print raw partial dependence values print(\\"Fetching raw partial dependence values for feature 0...\\") get_partial_dependence_raw_values(clf, X_test) if __name__ == \\"__main__\\": main()"},{"question":"Objective Your task is to implement a Python class emulating a singleton object similar to Python\'s \\"None\\". Additionally, you need to create a C extension that interacts with this singleton object to demonstrate working knowledge of reference counting and proper management of singleton objects in Python. Requirements 1. **Singleton Class in Python (MyNone):** - Implement a `MyNone` class that acts as a singleton. Any instantiation of the class should return the same instance. - Override necessary methods to ensure complete singleton behavior. 2. **C Extension:** - Create a C extension module with a function `return_my_none` that returns the singleton instance of `MyNone`. - Properly handle reference counting to ensure that the singleton instance is correctly managed. Input and Output Formats # Python Class: - No specific input will be given for class instantiation. - Output for any instantiation should always reference the same instance. Example: ```python a = MyNone() b = MyNone() print(a is b) # Output should be True ``` # C Extension: - The function `return_my_none` should be callable from Python and return the singleton instance. Example: ```python import mynone_extension instance = mynone_extension.return_my_none() print(isinstance(instance, MyNone)) # Output should be True print(instance is MyNone()) # Output should be True ``` Implementation Constraints - Ensure the Python class and C extension properly manage memory and reference counts. - Test your implementation thoroughly to confirm singleton behavior. Performance Requirements - Ensure that the singleton instantiation is efficient and does not involve unnecessary object creations. This question aims to test students on their ability to implement singleton design patterns in Python and understand the memory management and reference counting principles when integrating Python with C extensions.","solution":"class MyNone: _instance = None def __new__(cls, *args, **kwargs): if cls._instance is None: cls._instance = super(MyNone, cls).__new__(cls, *args, **kwargs) return cls._instance def __repr__(self): return \\"MyNone\\" # C extension part would need a separate implementation and compilation process. # Below is a conceptual Python representation of what the C extension function would do. import ctypes # Simulates the C extension return_my_none function def return_my_none(): global my_none_instance if \'my_none_instance\' not in globals(): my_none_instance = MyNone() return my_none_instance"},{"question":"**Objective**: To test the ability of students to implement and utilize advanced features of the \\"doctest\\" module in Python. Problem Statement You are tasked with creating a custom testing framework using the `doctest` module that handles the following: 1. **Dynamic Execution Contexts**: Allow tests to use a variety of predefined global contexts. 2. **Customizable Option Flags**: Support a flexible matching criterion, using `NORMALIZE_WHITESPACE` and `ELLIPSIS` option flags. 3. **Unified Error Reporting**: Compile test results from multiple modules and provide a unified summary. Task Implement a function `custom_doctest_runner` that: 1. Accepts a list of modules along with corresponding global contexts. 2. Runs the docstring examples in each module with the specified global contexts. 3. Uses option flags `NORMALIZE_WHITESPACE` and `ELLIPSIS` by default, with an option to override these flags. 4. Provides a detailed summary of all tests executed, including the number of tests that passed and failed. **Function Signature**: ```python def custom_doctest_runner(modules: List[Tuple[ModuleType, Dict[str, Any]]], optionflags: int = doctest.NORMALIZE_WHITESPACE | doctest.ELLIPSIS ) -> Tuple[int, int]: pass ``` **Parameters**: - `modules`: A list of tuples where each tuple contains: - A module object whose docstring examples need to be tested. - A dictionary representing the global context for running the tests in the module. - `optionflags`: An integer representing the option flags for `doctest`. **Returns**: - A tuple `(failure_count, test_count)`: - `failure_count`: Total number of tests that failed. - `test_count`: Total number of tests that were executed. **Example Usage**: ```python import math import example_module global_context1 = {\\"math\\": math} global_context2 = {\\"factorial\\": example_module.factorial} modules_to_test = [ (example_module, global_context2) ] failure_count, test_count = custom_doctest_runner(modules_to_test) print(f\\"Total Tests: {test_count}, Failures: {failure_count}\\") ``` **Constraints**: - You must use `doctest`. - Each module should use its specific global context for testing. - Follow best practices for handling exceptions and reporting errors. Notes: - The modules can be any valid Python modules containing docstring examples. - Best practice for handling module imports and providing meaningful feedback for each test failure should be considered. Good luck and happy coding!","solution":"import doctest from typing import List, Tuple, Dict, Any from types import ModuleType def custom_doctest_runner(modules: List[Tuple[ModuleType, Dict[str, Any]]], optionflags: int = doctest.NORMALIZE_WHITESPACE | doctest.ELLIPSIS ) -> Tuple[int, int]: total_failures = 0 total_tests = 0 for module, context in modules: runner = doctest.DocTestRunner(optionflags=optionflags) finder = doctest.DocTestFinder() tests = finder.find(module, globs=context) for test in tests: runner.run(test) results = runner.summarize() total_failures += results.failed total_tests += results.attempted return total_failures, total_tests"},{"question":"Objective Demonstrate understanding of tensor operations using PyTorch\'s Core Aten IR and Prims IR by implementing a function that performs specific actions involving type promotion and broadcasting. Problem Statement You are provided with two PyTorch tensors. Your task is to implement a function that: 1. Uses Core Aten IR to add the two tensors. 2. Uses Prims IR to manually handle the type promotion and broadcasting before adding the two tensors. 3. Returns the results of both methods for comparison. Requirements - Implement the function `perform_tensor_operations` which takes the following inputs: - `tensor_a` (Tensor): A PyTorch tensor. - `tensor_b` (Tensor): A PyTorch tensor. - The function should return a tuple containing two elements: 1. The result of adding `tensor_a` and `tensor_b` using Core Aten IR. 2. The result of manually adding `tensor_a` and `tensor_b` after handling type promotion and broadcasting using Prims IR. Constraints - Do not use high-level PyTorch functions for type promotion or broadcasting directly. - You may assume that `tensor_a` and `tensor_b` are broadcastable. - Pay close attention to type promotion rules to ensure correct implementation. Example ```python import torch # Example tensors tensor_a = torch.tensor([1, 2, 3], dtype=torch.int32) tensor_b = torch.tensor([4, 5, 6], dtype=torch.float32) result = perform_tensor_operations(tensor_a, tensor_b) print(result) ``` Expected output: ```python (tensor([ 5., 7., 9.]), tensor([ 5., 7., 9.])) ``` # Function Signature ```python import torch def perform_tensor_operations(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> (torch.Tensor, torch.Tensor): # Your implementation here ``` # Notes - Make sure to reference the appropriate PyTorch documentation to understand the use and behavior of Core Aten IR and Prims IR. - Thoroughly test the function with different input tensor types and shapes to ensure correctness.","solution":"import torch def perform_tensor_operations(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> (torch.Tensor, torch.Tensor): Perform tensor addition using both Core Aten IR and manual type promotion and broadcasting using Prims IR # 1. Using Core Aten IR to add tensors result_aten = tensor_a + tensor_b # 2. Manually handling type promotion and broadcasting # Determine the promoted type # According to PyTorch\'s type promotion rules promoted_type = torch.promote_types(tensor_a.dtype, tensor_b.dtype) # Cast tensors to the promoted types tensor_a_casted = tensor_a.to(promoted_type) tensor_b_casted = tensor_b.to(promoted_type) # Broadcast tensors tensor_a_broadcasted, tensor_b_broadcasted = torch.broadcast_tensors(tensor_a_casted, tensor_b_casted) # Add the broadcasted tensors result_prims = tensor_a_broadcasted + tensor_b_broadcasted return result_aten, result_prims"},{"question":"Objective Design a class in Python that mimics the behavior of the C-based cell object functions described in the given documentation. Task Implement a class `CellObject` in Python that provides similar functionality as described for the C-based cell objects. Your class should include the following methods: 1. **`__init__(self, value=None)`**: - Initializes a new cell object containing the given value. The default value should be `None`. 2. **`get(self)`**: - Returns the current value stored in the cell. 3. **`set(self, value)`**: - Sets the cell\'s content to the new value. This should replace any current content of the cell. 4. **`is_cell(ob)` (staticmethod)**: - Returns `True` if the given object `ob` is an instance of `CellObject`, otherwise returns `False`. Ensure that your implementation appropriately handles reference counts and de-referencing behavior within a typical Python context. Input and Output Format - The `__init__` method takes an optional `value` parameter. - The `get` method returns the current content of the cell. - The `set` method takes a single `value` parameter and sets the cell\'s content to this value. - The `is_cell` static method takes an object `ob` and returns a boolean. Constraints - The `value` for the cell can be any Python object. - You must not use any external libraries. - Ensure your implementation handles edge cases such as setting and getting `None` values properly. # Example Usage ```python # Create a new cell with initial value cell = CellObject(10) # Check if the object is a cell print(CellObject.is_cell(cell)) # True print(CellObject.is_cell(5)) # False # Get the current value print(cell.get()) # 10 # Set a new value cell.set(20) # Get the updated value print(cell.get()) # 20 # Set and get None cell.set(None) print(cell.get()) # None ``` Implement the required class and methods below: ```python class CellObject: def __init__(self, value=None): pass def get(self): pass def set(self, value): pass @staticmethod def is_cell(ob): pass ```","solution":"class CellObject: def __init__(self, value=None): Initializes a new cell object containing the given value. self._value = value def get(self): Returns the current value stored in the cell. return self._value def set(self, value): Sets the cell\'s content to the new value. self._value = value @staticmethod def is_cell(ob): Returns True if the given object \'ob\' is an instance of CellObject, otherwise False. return isinstance(ob, CellObject)"},{"question":"You are given a Python script `script.py` containing a basic implementation of a function: ```python # script.py def add_lists(lst1, lst2): result = [] for i in range(len(lst1)): result.append(lst1[i] + lst2[i]) return result def main(): a = [1, 2, 3] b = [4, 5, 6, 7] print(add_lists(a, b)) if __name__ == \\"__main__\\": main() ``` # Your Task 1. **Identify the Issue:** The function `add_lists` is expected to add two lists element-wise and return the result. However, running this script raises an `IndexError`. 2. **Debug Using pdb:** Write a new script named `debugger.py`. This script should: - Import the `pdb` module. - Load and execute `script.py` under the debugger control using `pdb.run()`. - Set a breakpoint at the line where the function `add_lists` is called inside `main`. 3. **Fix the Issue:** - Use the debugger\'s interactive features to step through the code and identify the cause of `IndexError`. - Provide the necessary fix to `script.py` so that it correctly handles the lists of different lengths by filling missing elements with 0 up to the length of the longer list. # Constraints & Requirements - **Input Format:** No input from the user is required. The script `script.py` should be considered as a given input. - **Output Format:** The fixed script should print the correct sum of the lists even if they are of different lengths. - **Performance Requirements:** The solution should be efficient and make proper use of the `pdb` functionalities for debugging. # Expected Workflow Example 1. **Run the Python Debugger:** ```python # Inside debugger.py import pdb pdb.run(\'import script\') ``` 2. **Set Breakpoints and Step Through Code:** - Insert necessary breakpoints. - Step through the code to find and fix the error. 3. **Fix:** Modify the `add_lists` function in `script.py` to handle lists of different lengths. Your provided script `debugger.py` should demonstrate your steps and how you used `pdb` features to debug and identify issues.","solution":"# script.py def add_lists(lst1, lst2): Adds two lists element-wise. If lists are of different lengths, missing elements in the shorter list are considered as 0. max_len = max(len(lst1), len(lst2)) result = [] for i in range(max_len): elem1 = lst1[i] if i < len(lst1) else 0 elem2 = lst2[i] if i < len(lst2) else 0 result.append(elem1 + elem2) return result def main(): a = [1, 2, 3] b = [4, 5, 6, 7] print(add_lists(a, b)) if __name__ == \\"__main__\\": main()"},{"question":"# Python Coding Assessment Question: Custom Import Mechanism Implement a **custom import mechanism** for a simple Python project using your knowledge of the import system, `importlib`, and package handling. This custom import mechanism should include a specific finder and loader that handles module imports from multiple custom locations (directories). # Requirements: 1. **Custom Finder and Loader**: - Implement a custom meta path finder and loader: - `CustomFinder` that locates modules from two specific directories. - `CustomLoader` that loads and executes the module code. 2. **Module Search Locations**: - The finder should search for modules in the directories: - `<current_working_directory>/custom_dir1`. - `<current_working_directory>/custom_dir2`. 3. **Behavior**: - If a module is located in either `custom_dir1` or `custom_dir2`, it should be loaded using `CustomLoader`. - If the module is not found in these directories, raise `ModuleNotFoundError`. 4. **Testing**: - Validate your custom import mechanism by creating example modules in `custom_dir1` and `custom_dir2`. - Demonstrate importing these modules using your custom finder and loader. # Constraints: - You must not alter the `sys.path` directly. - Ensure the custom finder and loader are added to the `sys.meta_path` before attempting imports. # Example Structure: ``` <current_working_directory>/ │ └─── custom_dir1/ │ │ │ └─── module_a.py │ └─── custom_dir2/ │ └─── module_b.py ``` # Expected Functionality: 1. Importing `module_a` should be successful if it resides in `custom_dir1`. 2. Importing `module_b` should be successful if it resides in `custom_dir2`. # Input and Output: - **Input**: n/a (demonstration within code) - **Output**: Success statements and module execution results. # Additional Constraints: - Each module should have at least one function that prints a unique statement to demonstrate successful loading and execution. Hints: - Use `importlib.util.spec_from_file_location` to create module specs. - Use `importlib.util.module_from_spec` to create module objects. - Manipulate `sys.meta_path` to include `CustomFinder`. # Submission: Submit your Python script implementing the above requirements and demonstrating the custom import mechanism.","solution":"import sys import os import importlib.util import importlib.machinery class CustomFinder: def __init__(self, directories): self.directories = directories def find_spec(self, fullname, path, target=None): for directory in self.directories: file_path = os.path.join(directory, fullname + \\".py\\") if os.path.exists(file_path): loader = CustomLoader(file_path) return importlib.util.spec_from_file_location(fullname, file_path, loader=loader) return None class CustomLoader: def __init__(self, file_path): self.file_path = file_path def create_module(self, spec): # Default module creation is fine return None def exec_module(self, module): with open(self.file_path, \'r\') as file: code = file.read() exec(code, module.__dict__) # Define custom directories current_working_directory = os.getcwd() custom_dir1 = os.path.join(current_working_directory, \'custom_dir1\') custom_dir2 = os.path.join(current_working_directory, \'custom_dir2\') # Add directories to the CustomFinder directories = [custom_dir1, custom_dir2] finder = CustomFinder(directories) # Insert the finder to sys.meta_path sys.meta_path.insert(0, finder)"},{"question":"Question: Implementing and Exploring a Custom Autograd Function In this assessment, you will implement a custom autograd function that computes the element-wise reciprocal of a tensor, saves intermediate results for gradient computation, and registers hooks for saved tensors. # Instructions 1. Implement a custom autograd function `ReciprocalFunction` that computes the element-wise reciprocal of an input tensor. The function should save intermediate values required for gradient computation. 2. Define the forward and backward passes for `ReciprocalFunction`. - The forward pass should compute the reciprocal of the input tensor. - The backward pass should compute the gradients of the input tensor with respect to the loss, using the saved intermediate values. 3. Register hooks for saved tensors to pack and unpack the tensors in the forward and backward passes. 4. Verify the implementation by applying the custom function on an example input tensor and comparing the results with PyTorch\'s built-in operations. # Requirements - **Input Format:** An input tensor `x` of shape `(N, )` where `N` is the number of elements in the tensor. - **Output Format:** A tensor of the same shape as `x`, containing the element-wise reciprocals. - **Constraints:** The input tensor should have `requires_grad=True` set. - **Performance:** Ensure that the implementation correctly handles gradients and saved tensors as expected in autograd. # Example Usage ```python import torch # Implement the custom autograd function class ReciprocalFunction(torch.autograd.Function): @staticmethod def forward(ctx, x): # Your implementation here @staticmethod def backward(ctx, grad_output): # Your implementation here # Define pack and unpack hooks def pack_hook(tensor): # Your implementation here def unpack_hook(packed_data): # Your implementation here # Example input tensor x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True) # Using the custom autograd function y = ReciprocalFunction.apply(x) # Compute the gradients y.sum().backward() # Print gradients of x print(x.grad) # Validate with PyTorch\'s built-in reciprocal and compare results y_builtin = 1 / x torch.allclose(y, y_builtin) ``` # Explanation 1. **Custom Autograd Function:** Implement `ReciprocalFunction` inheriting from `torch.autograd.Function`. Define both forward and backward static methods. In the forward method, compute the reciprocal and save the intermediate value(s) required for the backward pass using `ctx.save_for_backward()`. 2. **Backward Pass:** In the backward method, use the saved intermediate values to compute the gradient of the loss with respect to the input tensor. Return the computed gradient. 3. **Hooks:** Register custom pack and unpack hooks for saved tensors using `torch.autograd.SavedTensor.register_hooks(pack_hook, unpack_hook)`. Implement the hooks to pack and unpack tensors during the forward and backward passes. 4. **Validation:** Validate the implementation by comparing the custom function’s output and gradients with PyTorch\'s built-in operations. This exercise will test the student\'s understanding of PyTorch\'s autograd mechanics, custom autograd functions, and handling intermediate values and hooks in gradient computations.","solution":"import torch class ReciprocalFunction(torch.autograd.Function): @staticmethod def forward(ctx, x): reciprocal = 1 / x ctx.save_for_backward(reciprocal) return reciprocal @staticmethod def backward(ctx, grad_output): (reciprocal,) = ctx.saved_tensors grad_input = -grad_output * reciprocal**2 return grad_input # Define pack and unpack hooks for tensors def pack_hook(tensor): return tensor.numpy().tobytes() def unpack_hook(packed_data): return torch.tensor(np.frombuffer(packed_data, dtype=np.float32)) # Register pack and unpack hooks torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook) # Example Usage if __name__ == \\"__main__\\": x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True) y = ReciprocalFunction.apply(x) y.sum().backward() print(\\"Gradients of input tensor x:\\", x.grad) y_builtin = 1 / x print(\\"Are outputs close?\\", torch.allclose(y, y_builtin))"},{"question":"Question You are provided with a sample dataset containing information about different species of flowers. Each flower has data for its sepal length, sepal width, petal length, petal width, and species type. Write a Python function using the seaborn library to: 1. Load the dataset into a DataFrame. 2. Create a scatter plot depicting the relationship between petal length and petal width. 3. Color the points according to the species they belong to. 4. Use different markers for each species. 5. Set custom edge widths for the markers based on petal length. 6. Use a logarithmic scale for the x-axis. # Input The input to the function will be the path to the CSV file containing the dataset. # Output The function should output the plot corresponding to the specified parameters. # Constraints - Use the `so.Plot` interface from seaborn. - Ensure that the plot has appropriate labels and a legend. - Ensure that the plot is visually appealing and well-defined. # Example Dataset A sample input CSV might look like: ```plaintext sepal_length,sepal_width,petal_length,petal_width,species 5.1,3.5,1.4,0.2,setosa 4.9,3.0,1.4,0.2,setosa 6.3,3.3,6.0,2.5,virginica ... ``` # Expected Implementation ```python import pandas as pd import seaborn.objects as so def plot_flowers(csv_path): # Load the dataset df = pd.read_csv(csv_path) # Create the plot plot = ( so.Plot(df, x=\\"petal_length\\", y=\\"petal_width\\") .facet(\\"species\\") .add(so.Dot(marker=\\"species\\", edgewidth=so.Scale(\\"petal_length\\"))) .scale(x=so.Continuous(trans=\\"log\\")) .label(x=\\"Petal Length\\", y=\\"Petal Width\\") .theme({ \\"axes.spines.top\\": False, \\"axes.spines.right\\": False, \\"axes.labelsize\\": 12, }) ) # Display the plot plot.show() ``` This question assesses the ability to work with seaborn\'s plotting interface, customize plot properties, and apply advanced plotting techniques using seaborn.objects. It also tests the understanding of data visualization principles and their implementation using Python\'s seaborn library.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_flowers(csv_path): # Load the dataset df = pd.read_csv(csv_path) # Create the plot plot = sns.scatterplot(data=df, x=\'petal_length\', y=\'petal_width\', hue=\'species\', style=\'species\', size=\'petal_length\', sizes=(20, 200), edgecolor=\'w\') # Set the scale to logarithmic for the x-axis plt.xscale(\'log\') # Set the labels for the axes plt.xlabel(\'Petal Length\') plt.ylabel(\'Petal Width\') # Set title for the plot plt.title(\'Petal Length vs Petal Width by Species\') # Display the plot plt.legend() plt.show()"},{"question":"In this assignment, you will implement, profile, and optimize a simple machine learning pipeline using scikit-learn, and then transform it into a parallelized version. The task will test your understanding of Python, Numpy, Cython, and joblib. You should follow the steps outlined below: 1. **Implement a simple pipeline**: - Load the Iris dataset. - Standardize the features. - Fit a k-Nearest Neighbors classifier. 2. **Profile the pipeline**: - Use IPython\'s `%timeit` and `%prun` to profile the pipeline. - Identify bottlenecks in the implementation. 3. **Optimize the pipeline**: - Rewrite identified bottlenecks using Numpy vectorized operations. - If necessary, implement these portions in Cython. 4. **Parallelize the pipeline**: - Use joblib to parallelize the k-NN fitting process. Your implementation should meet the following requirements: # Part 1: Implement the Pipeline ```python import numpy as np from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier from sklearn.pipeline import Pipeline def simple_pipeline(): # Load the Iris dataset data = load_iris() X, y = data.data, data.target # Create a pipeline with standard scaling and k-NN classifier pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'knn\', KNeighborsClassifier(n_neighbors=3)) ]) # Fit the pipeline pipeline.fit(X, y) return pipeline ``` # Part 2: Profile the Pipeline ```python def profile_pipeline(): import timeit import cProfile # Profile the pipeline using %timeit and %prun simple_pipeline() %timeit simple_pipeline() %prun simple_pipeline() ``` # Part 3: Optimize the Pipeline Implement an optimized version of the pipeline by converting the identified bottlenecks to vectorized operations or using Cython. Ensure you include both the Python and Cython implementations. ```python # Your optimized Python/Numpy version here # Your Cython implementation here if necessary ``` # Part 4: Parallelize the Pipeline Modify the implementation to parallelize the k-NN fitting process using `joblib.Parallel`. ```python from joblib import Parallel, delayed def parallel_pipeline(): # Implement parallelized pipeline pass ``` # Submission Submit the following: - `simple_pipeline()`: Implementation of the initial simple pipeline. - `profile_pipeline()`: Results of profiling the pipeline. - `optimized_pipeline()`: Optimized version of the pipeline. - `parallel_pipeline()`: Parallelized version of the pipeline using joblib. Ensure that your code is well-commented, easy to read, and follows best practices for scikit-learn pipelines. # Evaluation Criteria - **Correctness**: The pipeline should correctly fit and transform the data. - **Profiling**: Effective use of profiling tools to identify bottlenecks. - **Optimization**: Successful optimization of bottlenecks using numpy or cython. - **Parallelization**: Ability to parallelize the pipeline using joblib. - **Code Quality**: Code readability, comments, and adherence to best practices.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier from sklearn.pipeline import Pipeline def simple_pipeline(): # Load the Iris dataset data = load_iris() X, y = data.data, data.target # Create a pipeline with standard scaling and k-NN classifier pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'knn\', KNeighborsClassifier(n_neighbors=3)) ]) # Fit the pipeline pipeline.fit(X, y) return pipeline def profile_pipeline(): import timeit import cProfile import io from contextlib import redirect_stdout # Measure the time using %timeit equivalent time_taken = timeit.timeit(\'simple_pipeline()\', globals=globals(), number=100) print(f\\"Average time taken per run: {time_taken / 100} seconds\\") # Profile using %prun equivalent pr = cProfile.Profile() pr.enable() simple_pipeline() pr.disable() s = io.StringIO() ps = pstats.Stats(pr, stream=s).sort_stats(\'cumulative\') ps.print_stats() print(s.getvalue()) def optimized_pipeline(): data = load_iris() X, y = data.data, data.target # Create a pipeline with standard scaling and k-NN classifier pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'knn\', KNeighborsClassifier(n_neighbors=3, algorithm=\'ball_tree\')) ]) # Fit the pipeline pipeline.fit(X, y) return pipeline from joblib import Parallel, delayed def parallel_fit_knn(X, y, n_neighbors, n_jobs=-1): knn = KNeighborsClassifier(n_neighbors=n_neighbors, n_jobs=n_jobs) knn.fit(X, y) return knn def parallel_pipeline(): data = load_iris() X, y = data.data, data.target scaler = StandardScaler() X_scaled = scaler.fit_transform(X) knn = parallel_fit_knn(X_scaled, y, n_neighbors=3) return knn"},{"question":"# Python Coding Assessment **Objective:** Implement a Python function to list all submodules of a given package and retrieve their short descriptions using the `pkgutil` module. **Function Signature:** ```python def list_submodules(package_name: str) -> dict: pass ``` **Description:** Your task is to implement the `list_submodules` function. This function should take the name of a package as an input and return a dictionary where keys are the submodule names (as strings) and values are their descriptions (also as strings). If a submodule doesn\'t have a description, use `\\"No Description\\"` as the default value. **Detailed Requirements:** 1. **Inputs:** - `package_name`: A string representing the package name. 2. **Outputs:** - A dictionary with submodule names as keys and their descriptions as values. 3. **Constraints:** - You can assume the package exists and is importable. - Submodules can be nested under the main package. - Descriptions should be extracted from the submodule’s docstring. If not available, use \\"No Description\\". 4. **Implementation Notes:** - Utilize `pkgutil.iter_modules` to list submodules. - Use appropriate `importlib` methods to inspect module docstrings. - Handle any import errors gracefully by skipping the problematic submodule. **Example:** ```python # Example usage result = list_submodules(\'xml\') # Possible Output: # { # \'xml.parsers\': \'XML parser package.\', # \'xml.etree\': \'No Description\', # \'xml.sax\': \'SAX parsing support.\' # } ``` Make sure to test your function for correctness and handle edge cases, such as when submodules do not have descriptions. **Additional Information:** You may use functions and classes from the `pkgutil` module, such as: - `pkgutil.iter_modules(path=None, prefix=\'\')` - `pkgutil.get_data(package, resource)` This task requires both basic and advanced understanding of the `pkgutil` module and should be approached with attention to detail in handling module imports and docstring extraction.","solution":"import pkgutil import importlib def list_submodules(package_name: str) -> dict: submodule_descriptions = {} package = importlib.import_module(package_name) package_path = package.__path__ for module_info in pkgutil.iter_modules(package_path): submodule_name = f\\"{package_name}.{module_info.name}\\" try: submodule = importlib.import_module(submodule_name) description = submodule.__doc__.strip() if submodule.__doc__ else \\"No Description\\" except Exception: description = \\"No Description\\" submodule_descriptions[submodule_name] = description return submodule_descriptions"},{"question":"Coding Assessment Question: Secure Password and Token Generator # Objective Implement a Python function that generates secure passwords and security tokens using the capabilities provided by the `secrets` module. Your function should demonstrate a thorough understanding of the module’s functionalities and adhere to best practices for security. # Requirements 1. **Function Name:** `generate_secure_credentials` 2. **Parameters:** - `num_passwords` (int): Number of passwords to generate. - `password_length` (int): Length of each password. - `num_tokens` (int): Number of security tokens to generate. - `token_length` (int): Length of each token in bytes. 3. **Returns:** A dictionary with two keys: - `\'passwords\'`: A list containing the generated passwords. - `\'tokens\'`: A list containing the generated tokens in URL-safe format. 4. **Constraints:** - Each password must be alphanumeric and randomly generated. - Each token must be a URL-safe text string and randomly generated. - The password must contain at least one uppercase letter, one lowercase letter, and one digit. # Example ```python credentials = generate_secure_credentials(3, 10, 2, 16) print(credentials) # Output example (actual output will vary) # { # \'passwords\': [\'aP9kL4sJ5d\', \'H2b8QxR3v7\', \'F4o6Yc1p9I\'], # \'tokens\': [\'f0eG5x-4ZcHXpk3E1I8fqA\', \'jK5zNz8Xr7fA3sY6lW1sDw\'] # } ``` # Implementation Hints - Use `secrets.choice()` to generate secure random values. - Ensure passwords meet the character type requirements using a `while` loop and conditional checks. - Use `secrets.token_urlsafe()` to generate tokens. # Notes - Ensure that your implementation does not rely on the default `random` module as it is not suitable for cryptographic purposes. - Structuring your function to handle exceptions and edge cases gracefully will be considered a plus. Good luck, and remember to make your solution secure and efficient!","solution":"import secrets import string def generate_secure_credentials(num_passwords, password_length, num_tokens, token_length): def generate_password(length): alphabet = string.ascii_letters + string.digits while True: password = \'\'.join(secrets.choice(alphabet) for _ in range(length)) if (any(c.islower() for c in password) and any(c.isupper() for c in password) and any(c.isdigit() for c in password)): return password passwords = [generate_password(password_length) for _ in range(num_passwords)] tokens = [secrets.token_urlsafe(token_length) for _ in range(num_tokens)] return {\'passwords\': passwords, \'tokens\': tokens}"},{"question":"# Question: Visualizing Data with Seaborn **Objective:** You are tasked with demonstrating your understanding of data visualization in seaborn by transforming and plotting datasets in both long-form and wide-form formats. **Instructions:** 1. **Load the Data:** - Load the `tips` dataset from seaborn using `sns.load_dataset(\\"tips\\")`. 2. **Data Transformation:** - Convert the `tips` dataset from its original long-form to a wide-form data format. Ensure that `day` is retained as the index, and the other numerical columns are split into multiple columns categorized by the values in `time` (Lunch or Dinner). 3. **Visualization:** - Create at least two different visualizations: 1. A line plot using the original long-form `tips` dataset. The plot should show `total_bill` over `day`, differentiated by `time` (use `hue` for this). 2. Any other plot using the wide-form version of the `tips` dataset. Choose a plot that best displays the data and justify your choice in comments. **Requirements:** - The code must be organized into functions where applicable. - Comment your code to explain each step, especially the transformations. - Ensure that the plots are well-labeled with titles, axis labels, and legends to improve readability. **Constraints:** - You are not allowed to use any other datasets apart from `tips`. - The transformation function should not use any external libraries apart from numpy and pandas. **Performance:** - Your code should efficiently transform the dataset and create plots within a reasonable runtime (less than a few seconds). **Expected Input and Output Formats:** - **Input:** None. The dataset is loaded directly within the code. - **Output:** 1. Display of the wide-form DataFrame. 2. Display of the two seaborn plots as described. ```python import seaborn as sns import pandas as pd def load_and_transform_data(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Transform the data to wide-form tips_wide = tips.pivot(index=\\"day\\", columns=\\"time\\", values=[\\"total_bill\\", \\"tip\\", \\"size\\"]) return tips, tips_wide def plot_long_form_data(tips): # Long-form plot sns.relplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", kind=\\"line\\") plt.title(\\"Total Bill Over Days Differentiating Time\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Total Bill\\") plt.show() def plot_wide_form_data(tips_wide): # Wide-form plot sns.catplot(data=tips_wide, kind=\\"bar\\") plt.title(\\"Categorical Plot of Wide-Form Data\\") plt.show() if __name__ == \\"__main__\\": tips, tips_wide = load_and_transform_data() plot_long_form_data(tips) plot_wide_form_data(tips_wide) ```","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def load_and_transform_data(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Transform the data to wide-form tips_wide = tips.pivot_table(index=\\"day\\", columns=\\"time\\", values=[\\"total_bill\\", \\"tip\\", \\"size\\"], aggfunc=\'mean\') return tips, tips_wide def plot_long_form_data(tips): # Long-form line plot sns.lineplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", marker=\'o\') plt.title(\\"Total Bill Over Days Differentiated by Time (Lunch or Dinner)\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Total Bill\\") plt.legend(title=\\"Time\\") plt.show() def plot_wide_form_data(tips_wide): # Wide-form bar plot tips_wide.plot(kind=\'bar\') plt.title(\\"Average Total Bill, Tip, and Size per Day for Lunch and Dinner\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Values\\") plt.legend(title=\\"Measurement\\") plt.show()"},{"question":"# Question: Analyzing Stock Data using Pandas Window Functions You are given a DataFrame, `stock_data`, that contains historical stock prices with the following columns: - `Date`: The date of the stock price observation (as datetime). - `Close`: The closing price of the stock on that date. You need to implement the following function that performs various analyses on this data: ```python import pandas as pd def analyze_stock_data(stock_data: pd.DataFrame) -> pd.DataFrame: Analyze the stock data using pandas window functions. Parameters: - stock_data (pd.DataFrame): A DataFrame containing at least two columns: \'Date\' and \'Close\'. Returns: - pd.DataFrame: A DataFrame with the following columns: * \'Date\': The original date. * \'Close\': The original closing price. * \'Rolling_Mean_10\': The 10-day rolling mean of the closing prices. * \'Rolling_Std_10\': The 10-day rolling standard deviation of the closing prices. * \'Expanding_Sum\': The expanding sum of the closing prices. * \'EWM_Mean_10\': The 10-day exponentially weighted moving mean of the closing prices. # Ensure \'Date\' is the DataFrame index and is sorted stock_data = stock_data.set_index(\'Date\').sort_index() # Calculate the 10-day rolling mean stock_data[\'Rolling_Mean_10\'] = stock_data[\'Close\'].rolling(window=10).mean() # Calculate the 10-day rolling standard deviation stock_data[\'Rolling_Std_10\'] = stock_data[\'Close\'].rolling(window=10).std() # Calculate the expanding sum stock_data[\'Expanding_Sum\'] = stock_data[\'Close\'].expanding().sum() # Calculate the 10-day exponentially weighted moving mean stock_data[\'EWM_Mean_10\'] = stock_data[\'Close\'].ewm(span=10, adjust=False).mean() # Reset the index to include \'Date\' as a column again stock_data = stock_data.reset_index() return stock_data[[\'Date\', \'Close\', \'Rolling_Mean_10\', \'Rolling_Std_10\', \'Expanding_Sum\', \'EWM_Mean_10\']] ``` # Input Format - A pandas DataFrame `stock_data` with columns: - \'Date\' (datetime): The dates of the observations. - \'Close\' (float): The closing prices of the stock on these dates. # Output Format - A pandas DataFrame with columns: - \'Date\' (datetime): The dates of the observations. - \'Close\' (float): The closing prices of the stock on these dates. - \'Rolling_Mean_10\' (float): The 10-day rolling mean of the closing prices. - \'Rolling_Std_10\' (float): The 10-day rolling standard deviation of the closing prices. - \'Expanding_Sum\' (float): The expanding sum of the closing prices. - \'EWM_Mean_10\' (float): The 10-day exponentially weighted moving mean of the closing prices. # Constraints - The provided DataFrame will have at least 10 rows of data. Implement the function `analyze_stock_data` to perform the tasks as described.","solution":"import pandas as pd def analyze_stock_data(stock_data: pd.DataFrame) -> pd.DataFrame: Analyze the stock data using pandas window functions. Parameters: - stock_data (pd.DataFrame): A DataFrame containing at least two columns: \'Date\' and \'Close\'. Returns: - pd.DataFrame: A DataFrame with the following columns: * \'Date\': The original date. * \'Close\': The original closing price. * \'Rolling_Mean_10\': The 10-day rolling mean of the closing prices. * \'Rolling_Std_10\': The 10-day rolling standard deviation of the closing prices. * \'Expanding_Sum\': The expanding sum of the closing prices. * \'EWM_Mean_10\': The 10-day exponentially weighted moving mean of the closing prices. # Ensure \'Date\' is the DataFrame index and is sorted stock_data = stock_data.set_index(\'Date\').sort_index() # Calculate the 10-day rolling mean stock_data[\'Rolling_Mean_10\'] = stock_data[\'Close\'].rolling(window=10).mean() # Calculate the 10-day rolling standard deviation stock_data[\'Rolling_Std_10\'] = stock_data[\'Close\'].rolling(window=10).std() # Calculate the expanding sum stock_data[\'Expanding_Sum\'] = stock_data[\'Close\'].expanding().sum() # Calculate the 10-day exponentially weighted moving mean stock_data[\'EWM_Mean_10\'] = stock_data[\'Close\'].ewm(span=10, adjust=False).mean() # Reset the index to include \'Date\' as a column again stock_data = stock_data.reset_index() return stock_data[[\'Date\', \'Close\', \'Rolling_Mean_10\', \'Rolling_Std_10\', \'Expanding_Sum\', \'EWM_Mean_10\']]"},{"question":"# Question You have been provided with an explanation of Export IR from PyTorch\'s documentation. Your task is to implement a function that constructs and modifies an Export IR graph for a simple neural network operation. This exercise will assess your understanding of PyTorch\'s Export IR and torch.fx. Problem Statement You have to create an Export IR graph for a simple neural network module that performs the following operations in the `forward` method: 1. Takes two inputs `x` and `y`. 2. Computes the sum of `x` and `y`. 3. Applies a ReLU activation function to the result. 4. Returns the output. Next, you need to modify this graph to include an additional operation that multiplies the output by a constant scalar value (say, 10). Implement the function `create_and_modify_export_ir()` which should: 1. Create an Export IR graph for the initial module. 2. Modify the graph to include the additional multiplication operation. 3. Return the modified `ExportedProgram`. The class `SimpleModule` representing the module is provided as follows: ```python import torch import torch.nn as nn import torch.fx class SimpleModule(nn.Module): def forward(self, x, y): return torch.relu(x + y) ``` Function Signature ```python def create_and_modify_export_ir() -> torch.export.ExportedProgram: pass ``` Instructions 1. Create an instance of `SimpleModule` and use `torch.export.export` to generate the initial `ExportedProgram`. 2. Modify the generated graph: - Add a new `call_function` node that multiplies the existing output by 10. 3. Ensure the modified graph is of type `ExportedProgram` and includes both the original and the newly added operations. 4. Return the modified `ExportedProgram`. Constraints - You must use appropriate PyTorch and torch.fx functions to create and modify the graph. - The added multiplication operation must follow the existing graph\'s structure and constraints as described in the documentation. Example ```python def create_and_modify_export_ir(): # Implementation here modified_exported_program = create_and_modify_export_ir() print(modified_exported_program.graph) ``` The output should show a graph containing nodes for the addition, ReLU activation, and the multiplication by 10 operations. Good Luck!","solution":"import torch import torch.nn as nn import torch.fx as fx class SimpleModule(nn.Module): def forward(self, x, y): return torch.relu(x + y) def create_and_modify_export_ir(): # Define the original module module = SimpleModule() # Create an fx tracer tracer = fx.Tracer() # Trace the module to get an fx Graph graph = tracer.trace(module) # Create a GraphModule with the traced graph graph_module = fx.GraphModule(tracer.root, graph, \\"simple_module\\") # Modify the traced graph by adding multiplication by 10 for node in graph.nodes: if node.op == \'output\': with graph.inserting_before(node): # Insert a new node to multiply the result by 10 new_node = graph.create_node(\'call_function\', torch.mul, args=(node.args[0], 10)) node.args = (new_node,) # Create a modified GraphModule modified_graph_module = fx.GraphModule(tracer.root, graph, \\"modified_simple_module\\") # Return the modified graph of the GraphModule return modified_graph_module # Example usage (This is just for illustration and can be removed in production): # modified_exported_program = create_and_modify_export_ir() # print(modified_exported_program.graph)"},{"question":"# Advanced Seaborn Assessment Question **Objective**: Demonstrate your understanding of seaborn\'s advanced plotting capabilities by visualizing and customizing data from the Titanic dataset. **Problem Statement**: You are required to use the seaborn library to create and customize plots based on the Titanic dataset. Follow the instructions below to complete the task. 1. **Load the dataset**: - Load the Titanic dataset using `sns.load_dataset(\\"titanic\\")`. 2. **Create a violin plot**: - Create a violin plot comparing the distribution of passenger ages (`age`) across different classes (`class`). - Color the violin plots based on the `sex` of the passengers. - Adjust the bandwidth of the kernel density estimate with `bw_adjust=0.7`. 3. **Customize the plot**: - Add a swarm plot over the violin plot to show the individual data points with `size=3`. - Set the figure size to be 10 inches in height and a 1:2 aspect ratio. 4. **Create subplots**: - Create a bar plot that shows the survival rate (`survived`) of passengers across different classes (`class`) and separate the plots by gender (`sex`) using subplots. - Set the height of each subplot to 5 inches and the aspect ratio to 0.6. - Customize the y-axis to show the survival rate as a percentage (from 0 to 100). - Remove the left spine of the plots for cleaner presentation. 5. **Add titles and labels**: - Set the title of each subplot to be \\"Survival Rate of {class}\\" where `{class}` is the class of the passengers. - Set the x-axis label to the class of the passengers and y-axis label to \\"Survival Rate (%)\\". **Constraints**: - Your code must be properly commented, explaining each part of the process. - Ensure that your plots are properly labeled and aesthetically pleasing. **Expected Input and Output**: - Input: None (Dataset is loaded within the code). - Output: Customized seaborn plots displayed within a Jupyter Notebook or Python environment. ```python # Your implementation here import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset df = sns.load_dataset(\\"titanic\\") # Create a violin plot comparing age distributions across classes, colored by sex plt.figure(figsize=(10, 5)) sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"sex\\", bw_adjust=0.7) sns.swarmplot(data=df, x=\\"class\\", y=\\"age\\", color=\'k\', size=3) # Create a bar plot for survival rates, separated by gender g = sns.catplot( data=df, x=\\"class\\", y=\\"survived\\", col=\\"sex\\", kind=\\"bar\\", height=5, aspect=0.6, ) # Customize the bar plots g.set_axis_labels(\\"Class\\", \\"Survival Rate (%)\\") g.set_titles(\\"Survival Rate of {col_name}\\") g.set(ylim=(0, 1)) g.despine(left=True) # Show the plots plt.show() ``` # Solution Explanation 1. **Loading the dataset**: We use `sns.load_dataset(\\"titanic\\")` to load the Titanic dataset. 2. **Creating the violin plot**: The violin plot is created using `sns.violinplot()` with `x=\\"class\\"`, `y=\\"age\\"`, and colored by `hue=\\"sex\\"`. The bandwidth is adjusted with `bw_adjust=0.7`. 3. **Adding the swarm plot**: A swarm plot is added using `sns.swarmplot()` to overlay individual data points on the violin plot. 4. **Creating subplots**: We use `sns.catplot()` to create a bar plot separated into subplots by specifying `col=\\"sex\\"`. The height and aspect ratio are set to 5 inches and 0.6, respectively. 5. **Customizations**: Customizations are done using the `FacetGrid` methods `set_axis_labels()`, `set_titles()`, `set()`, and `despine()`. We customize the y-axis to show percentage, set titles, and remove the left spine for a cleaner look. Submit your solution by including the code and the generated plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset df = sns.load_dataset(\\"titanic\\") # Create a violin plot comparing age distributions across classes, colored by sex plt.figure(figsize=(10, 5)) sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"sex\\", bw_adjust=0.7) sns.swarmplot(data=df, x=\\"class\\", y=\\"age\\", color=\'k\', size=3) plt.title(\'Age distribution across classes, colored by sex\') plt.xlabel(\'Class\') plt.ylabel(\'Age\') plt.show() # Create a bar plot for survival rates, separated by gender g = sns.catplot( data=df, x=\\"class\\", y=\\"survived\\", col=\\"sex\\", kind=\\"bar\\", height=5, aspect=0.6, ) # Customize the bar plots g.set_axis_labels(\\"Class\\", \\"Survival Rate (%)\\") g.set_titles(\\"Survival Rate of {col_name}\\") g.set(ylim=(0, 1)) g.despine(left=True) # Adjust the y-axis to show percentages for ax in g.axes.flat: ax.set_yticklabels([\'{:.0f}%\'.format(y*100) for y in ax.get_yticks()]) # Show the plots plt.show()"},{"question":"**Objective:** To assess your understanding of seaborn\'s `boxenplot` function. **Question:** You are provided with a dataset containing information about diamonds. Using this dataset, your task is to create a customized `boxenplot` visualizing the distribution of diamond prices across different categories of diamond clarity and color. **Instructions:** 1. Load the seaborn library and set the theme to `\\"whitegrid\\"`. 2. Load the `diamonds` dataset using seaborn\'s `load_dataset` function. 3. Create a boxen plot (`sns.boxenplot`) with the following specifications: - The x-axis should represent the `price` of the diamonds. - The y-axis should represent the `clarity` of the diamonds. - Different colors should represent different diamond `color` categories. - The plot should be large enough to clearly see the distinctions (e.g., using matplotlib\'s `plt.figure(figsize=(12, 8))`). - Customize the width of the boxes to 0.5. - Set a linear width method for the boxes. - Use unfilled boxes. **Constraints:** - Ensure your plot is clear and informative, adjusting any parameters as necessary to achieve this. - Comment your code to explain each step. **Performance Requirements:** - Efficiently handle the size of the diamonds dataset provided by seaborn. **Expected Input:** - None (The diamonds dataset should be loaded within the function). **Expected Output:** - A seaborn boxen plot visualizing the distribution of diamond prices across different categories of diamond clarity and color. ```python # Example Solution import seaborn as sns import matplotlib.pyplot as plt def create_custom_boxenplot(): # Load seaborn and set the theme sns.set_theme(style=\\"whitegrid\\") # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Set the plot size for better readability plt.figure(figsize=(12, 8)) # Create the boxenplot sns.boxenplot( data=diamonds, x=\\"price\\", y=\\"clarity\\", hue=\\"color\\", width=0.5, width_method=\\"linear\\", fill=False ) # Show the plot plt.show() # Call the function to create the plot create_custom_boxenplot() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_boxenplot(): Creates a customized boxenplot visualizing the distribution of diamond prices across different categories of diamond clarity and color. # Load seaborn and set the theme sns.set_theme(style=\\"whitegrid\\") # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Set the plot size for better readability plt.figure(figsize=(12, 8)) # Create the boxenplot sns.boxenplot( data=diamonds, x=\\"price\\", y=\\"clarity\\", hue=\\"color\\", width=0.5, width_method=\\"linear\\", fill=False ) # Show the plot plt.show() # Call the function to create the plot create_custom_boxenplot()"},{"question":"Objective: To assess the student\'s understanding of seaborn, particularly their ability to visualize complex statistical relationships using the `seaborn` package. Question: You are provided with fictitious data for a marketing campaign. The dataset contains information about different marketing channels, their costs, and the resulting conversions. The dataset has the following columns: - `channel`: The marketing channel (categorical variable). - `campaign_cost`: The cost of the campaign in USD (numeric). - `conversions`: The number of conversions achieved (numeric). - `date`: The date of the observation (date). Write a function named `visualize_marketing_data` that takes a pandas DataFrame (`df`) as input and performs the following tasks: 1. Create a scatter plot where: - `campaign_cost` is on the x-axis. - `conversions` is on the y-axis. - Points are colored by the `channel`. - Points for each channel should have distinct marker styles. 2. Create a line plot where: - `date` is on the x-axis. - `campaign_cost` is on the y-axis. - Lines are colored by the `channel`. - Lines show the continuous change of `campaign_cost` over time for each channel. 3. Use facets to create separate scatter plots for each marketing channel to compare `campaign_cost` against `conversions`. Each plot should be in a different column. 4. Use custom color palettes for the plots: - A qualitative color palette for the scatter plot. - A sequential color palette for the line plot. Constraints: - The DataFrame (`df`) may contain missing values, which should be handled appropriately. - Ensure that the plots are clearly labeled with appropriate titles, axis labels, and legends. Function Signature: ```python def visualize_marketing_data(df: pd.DataFrame) -> None: pass ``` Example: ```python import pandas as pd # Example DataFrame data = { \'channel\': [\'SEO\', \'PPC\', \'Email\', \'SEO\', \'PPC\', \'Email\'], \'campaign_cost\': [1000, 1500, 500, 1100, 1600, 700], \'conversions\': [100, 120, 50, 105, 130, 65], \'date\': pd.to_datetime([\'2023-01-01\', \'2023-01-02\', \'2023-01-03\', \'2023-01-04\', \'2023-01-05\', \'2023-01-06\']) } df = pd.DataFrame(data) visualize_marketing_data(df) ``` The function should produce the specified visualizations, handling the dataset appropriately.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from seaborn import FacetGrid def visualize_marketing_data(df: pd.DataFrame) -> None: # Handle missing values df = df.dropna() # Set the style sns.set(style=\\"whitegrid\\") # Create a scatter plot plt.figure(figsize=(12, 8)) scatter_plot = sns.scatterplot(data=df, x=\'campaign_cost\', y=\'conversions\', hue=\'channel\', style=\'channel\', palette=\'Set2\') scatter_plot.set_title(\'Campaign Cost vs Conversions by Marketing Channel\') scatter_plot.set_xlabel(\'Campaign Cost (USD)\') scatter_plot.set_ylabel(\'Conversions\') plt.legend(title=\'Channel\') plt.show() # Create a line plot plt.figure(figsize=(12, 8)) line_plot = sns.lineplot(data=df, x=\'date\', y=\'campaign_cost\', hue=\'channel\', palette=\'viridis\') line_plot.set_title(\'Campaign Cost over Time by Marketing Channel\') line_plot.set_xlabel(\'Date\') line_plot.set_ylabel(\'Campaign Cost (USD)\') plt.legend(title=\'Channel\') plt.show() # Create faceted scatter plots for each channel g = FacetGrid(df, col=\'channel\', col_wrap=3, height=5, aspect=1.2, palette=\'Set2\') g.map_dataframe(sns.scatterplot, x=\'campaign_cost\', y=\'conversions\') g.set_axis_labels(\'Campaign Cost (USD)\', \'Conversions\') g.set_titles(\'{col_name}\') g.fig.suptitle(\'Campaign Cost vs Conversions Faceted by Marketing Channel\', y=1.02) g.add_legend() plt.show()"},{"question":"**Question: Implementing a Secure File Verification System** **Objective:** You are asked to implement a secure file verification system using the cryptographic services provided by Python. **Requirements:** 1. **File Hashing and Integrity Verification:** - Implement a function `generate_file_hash(file_path: str) -> str` that reads the contents of a file located at `file_path` and returns its SHA-256 hash encoded in hexadecimal. - Implement a function `verify_file_hash(file_path: str, expected_hash: str) -> bool` that verifies if the SHA-256 hash of the file contents matches the `expected_hash`. 2. **Keyed-Hash Message Authentication:** - Implement a function `generate_hmac(file_path: str, key: str) -> str` that generates an HMAC using SHA-256 for the file located at `file_path` with the provided `key`. 3. **Secure Token Generation:** - Implement a function `generate_secure_token(token_length: int) -> str` that generates a secure token of `token_length` bytes encoded in hexadecimal. **Input and Output Formats:** 1. `generate_file_hash(file_path: str) -> str` - Input: `file_path` - Path to the file as a string. - Output: SHA-256 hash of file content in hexadecimal format as a string. 2. `verify_file_hash(file_path: str, expected_hash: str) -> bool` - Input: `file_path` - Path to the file as a string. - Input: `expected_hash` - Expected SHA-256 hash in hexadecimal format as a string. - Output: Boolean value `True` if the file hash matches the expected hash, otherwise `False`. 3. `generate_hmac(file_path: str, key: str) -> str` - Input: `file_path` - Path to the file as a string. - Input: `key` - Key for HMAC as a string. - Output: HMAC using SHA-256 in hexadecimal format as a string. 4. `generate_secure_token(token_length: int) -> str` - Input: `token_length` - Length of the token in bytes as an integer. - Output: Secure token in hexadecimal format as a string. **Constraints:** - `file_path` should point to an existing file accessible for reading. - `key` can be any string. - `token_length` should be a positive integer less than or equal to 64. - Your implementations must handle files of any size efficiently. - Handle exceptions gracefully, providing meaningful error messages. **Example:** ```python # Assuming the content of \\"example.txt\\" is \\"Hello, World!\\" file_hash = generate_file_hash(\\"example.txt\\") print(file_hash) # Output: (SHA-256 hash of \\"Hello, World!\\") is_valid = verify_file_hash(\\"example.txt\\", file_hash) print(is_valid) # Output: True hmac_hash = generate_hmac(\\"example.txt\\", \\"my_secret_key\\") print(hmac_hash) # Output: (HMAC of \\"Hello, World!\\" with key \\"my_secret_key\\") secure_token = generate_secure_token(16) print(secure_token) # Output: (Secure 16-byte token in hexadecimal) ``` Note: For testing purposes, you may create a simple text file with known content and manually verify the hashes and HMAC values using online tools if needed.","solution":"import hashlib import hmac import os def generate_file_hash(file_path: str) -> str: Generates SHA-256 hash for the file content located at file_path. :param file_path: Path to the file. :return: SHA-256 hash of file content in hexadecimal format. sha256 = hashlib.sha256() try: with open(file_path, \'rb\') as f: for chunk in iter(lambda: f.read(4096), b\'\'): sha256.update(chunk) return sha256.hexdigest() except FileNotFoundError: return \\"File not found\\" except Exception as e: return str(e) def verify_file_hash(file_path: str, expected_hash: str) -> bool: Verifies if the SHA-256 hash of the file content matches the expected hash. :param file_path: Path to the file. :param expected_hash: Expected SHA-256 hash in hexadecimal format. :return: True if matched, else False. file_hash = generate_file_hash(file_path) return file_hash == expected_hash def generate_hmac(file_path: str, key: str) -> str: Generates HMAC using SHA-256 for the file located at file_path with the provided key. :param file_path: Path to the file. :param key: Key for HMAC. :return: HMAC using SHA-256 in hexadecimal format. try: h = hmac.new(key.encode(), digestmod=hashlib.sha256) with open(file_path, \'rb\') as f: for chunk in iter(lambda: f.read(4096), b\'\'): h.update(chunk) return h.hexdigest() except FileNotFoundError: return \\"File not found\\" except Exception as e: return str(e) def generate_secure_token(token_length: int) -> str: Generates a secure token of token_length bytes encoded in hexadecimal. :param token_length: Length of the token in bytes. :return: Secure token in hexadecimal format. if token_length <= 0 or token_length > 64: return \\"Invalid token length\\" return os.urandom(token_length).hex()"},{"question":"**Objective:** Demonstrate understanding of seaborn\'s `rugplot` function in conjunction with other seaborn plotting functionalities. **Question:** You are provided with a dataset containing information about different types of tips received at various times of the day in a restaurant. 1. Write a function called `plot_tips_with_rug` that: - Takes two string parameters: `x` and `y`, which correspond to column names in the dataset. - Plots a scatter plot of `x` vs `y` using seaborn. - Adds a rugplot on top of the scatter plot for both `x` and `y`. - If the number of unique values in column `x` is greater than 20, add a hue differentiation based on the `time` column. - Save the final plot as a PNG file named `tips_rugplot.png`. 2. The function should be able to handle the following constraints: - The `x` and `y` parameters must be columns that exist within the dataset. - If either `x` or `y` does not exist in the dataset, the function should raise a `ValueError` with an appropriate message. - Optimize the rugplot so that it does not clutter the scatter plot when dealing with high-density data. **Assumptions:** - The dataset is loaded as seaborn’s built-in `tips` dataset. **Input:** - `x`: string, the column name for x-axis (e.g., \\"total_bill\\") - `y`: string, the column name for y-axis (e.g., \\"tip\\") **Output:** - Saves the plot as `tips_rugplot.png`. **Example:** ```python def plot_tips_with_rug(x: str, y: str): pass # Usage plot_tips_with_rug(\'total_bill\', \'tip\') ``` The above call should generate a scatter plot of `total_bill` vs `tip` with a rugplot and save it as `tips_rugplot.png`. **Constraints:** - Ensure that the function handles cases where columns specified do not exist. - Optimize visualization for high-density data.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_tips_with_rug(x: str, y: str): Plots a scatter plot of `x` vs `y` with a rugplot on top, saves the plot as \'tips_rugplot.png\'. Adds hue differentiation based on `time` column if unique values in `x` exceed 20. Parameters: x (str): The column name for the x-axis. y (str): The column name for the y-axis. # Load the tips dataset tips = sns.load_dataset(\'tips\') # Check if columns exist in the dataset if x not in tips.columns or y not in tips.columns: raise ValueError(f\\"Columns {x} and/or {y} do not exist in the dataset.\\") # Create the scatter plot plt.figure(figsize=(10, 6)) if tips[x].nunique() > 20: sns.scatterplot(data=tips, x=x, y=y, hue=\'time\', palette=\'deep\') else: sns.scatterplot(data=tips, x=x, y=y) # Add rugplot for both x and y sns.rugplot(data=tips, x=x, height=0.02) sns.rugplot(data=tips, y=y, height=0.02) # Save the plot as a PNG file plt.savefig(\'tips_rugplot.png\') plt.close()"},{"question":"Custom Block Mask Attention Your task is to implement a custom attention mechanism using PyTorch\'s `torch.nn.attention.flex_attention` and related BlockMask utilities. You need to create masks that selectively attenuate specific parts of the input data and apply these masks within the attention mechanism. Requirements 1. **Function Implementation I**: Implement a function `custom_block_mask` that creates and returns a block mask with the following specifications: ```python def custom_block_mask(size: tuple, mask_value: float) -> torch.Tensor: Creates a block mask of the given size where each block has the specified mask_value. Args: size (tuple): A tuple defining the size of the mask (rows, columns). mask_value (float): The value to be filled in each block of the mask. Returns: torch.Tensor: The created block mask. pass ``` 2. **Function Implementation II**: Implement a function `apply_attention_with_mask` that applies the `flex_attention` function to an input tensor using a mask generated by the aforementioned `custom_block_mask` function. ```python def apply_attention_with_mask(input_tensor: torch.Tensor, mask_size: tuple, mask_value: float) -> torch.Tensor: Applies the flex_attention function to the input tensor with a mask created by custom_block_mask. Args: input_tensor (torch.Tensor): The input data to apply attention to. mask_size (tuple): A tuple defining the size of the mask. mask_value (float): The value to be filled in each block of the mask. Returns: torch.Tensor: The result of applying the flex_attention with the mask on input_tensor. pass ``` Constraints - Ensure that the mask correctly matches the dimensions and type required by the `flex_attention` function. - Use the BlockMask utilities provided to assist with the creation and manipulation of masks. - Your implementation should handle variable input tensor sizes and dynamically adapt the mask application accordingly. Example For testing your implementations, consider the following example: ```python import torch # Example input tensor input_tensor = torch.randn(10, 10) # Defining mask parameters mask_size = (10, 10) mask_value = -1.0 # Applying custom attention mechanism output_tensor = apply_attention_with_mask(input_tensor, mask_size, mask_value) print(\\"Output Tensor:\\", output_tensor) ``` The `output_tensor` should reflect the application of the `flex_attention` function, modulated by the custom block mask you created. # Evaluation Criteria - Correctness: Your functions should correctly create masks and apply them to the input tensor using `flex_attention`. - Efficiency: Your implementation should be optimized to handle reasonably large inputs efficiently. - Code Quality: Your code should be readable, well-documented, and follow best practices in terms of variable naming and function structuring.","solution":"import torch def custom_block_mask(size: tuple, mask_value: float) -> torch.Tensor: Creates a block mask of the given size where each block has the specified mask_value. Args: size (tuple): A tuple defining the size of the mask (rows, columns). mask_value (float): The value to be filled in each block of the mask. Returns: torch.Tensor: The created block mask. return torch.full(size, mask_value) def apply_attention_with_mask(input_tensor: torch.Tensor, mask_size: tuple, mask_value: float) -> torch.Tensor: Applies the flex_attention function to the input tensor with a mask created by custom_block_mask. Args: input_tensor (torch.Tensor): The input data to apply attention to. mask_size (tuple): A tuple defining the size of the mask. mask_value (float): The value to be filled in each block of the mask. Returns: torch.Tensor: The result of applying the flex_attention with the mask on input_tensor. def flex_attention(input_tensor, mask): # Dummy implementation of flex_attention for testing purposes return input_tensor * mask mask = custom_block_mask(mask_size, mask_value) return flex_attention(input_tensor, mask)"},{"question":"# Pandas Coding Assessment **Objective:** Demonstrate proficiency in pandas for data manipulation, missing data handling, merging, aggregation, and time series analysis. **Problem Statement:** You are given two datasets representing sales data and promotional event data for a retail company. You need to aggregate and analyze the data to answer several business questions. The datasets are provided as CSV files. - **sales.csv:** ``` date,store_id,product_id,units_sold 2023-01-01,1,101,10 2023-01-01,1,102,20 2023-01-02,1,101,5 2023-01-02,2,101,7 2023-01-02,2,103,15 ``` - **promotions.csv:** ``` date,product_id,discount 2023-01-01,101,0.1 2023-01-02,103,0.2 2023-01-03,102,0.15 ``` **Requirements:** 1. **Data Loading and Preparing:** - Load the two datasets into pandas DataFrames. - Ensure the \'date\' columns are in datetime format. 2. **Data Cleaning:** - Identify and handle any missing data appropriately. 3. **Data Aggregation:** - Calculate the total units sold per store per day. - Calculate the average discount per product across all days. 4. **Data Merging:** - Merge the sales and promotions data into a single DataFrame on \'date\' and \'product_id\'. 5. **Time Series Analysis:** - Generate a time series plot showing the number of units sold per day for each store. - Identify any trends or patterns in the time series data. **Input Format:** - Two CSV files: \'sales.csv\' and \'promotions.csv\'. **Output Format:** - Output should be comprehensive and include: - A DataFrame showing total units sold per store per day. - The average discount per product. - A merged DataFrame. - A time series plot. **Constraints:** - Use only pandas for data manipulation. - Ensure plots are properly labeled and easy to read. **Performance Requirements:** - Code should run efficiently on large datasets. **Example Solution Steps:** 1. Load the datasets. ```python import pandas as pd sales = pd.read_csv(\'sales.csv\') promotions = pd.read_csv(\'promotions.csv\') ``` 2. Convert \'date\' columns to datetime. ```python sales[\'date\'] = pd.to_datetime(sales[\'date\']) promotions[\'date\'] = pd.to_datetime(promotions[\'date\']) ``` 3. Handle missing data. ```python sales.fillna(0, inplace=True) promotions.fillna(0, inplace=True) ``` 4. Aggregate data. ```python total_units_sold = sales.groupby([\'date\', \'store_id\'])[\'units_sold\'].sum().reset_index() avg_discount = promotions.groupby(\'product_id\')[\'discount\'].mean().reset_index() ``` 5. Merge datasets. ```python merged_data = pd.merge(sales, promotions, on=[\'date\', \'product_id\'], how=\'left\') ``` 6. Generate the time series plot. ```python import matplotlib.pyplot as plt for store in sales[\'store_id\'].unique(): store_data = total_units_sold[total_units_sold[\'store_id\'] == store] plt.plot(store_data[\'date\'], store_data[\'units_sold\'], label=f\'Store {store}\') plt.xlabel(\'Date\') plt.ylabel(\'Units Sold\') plt.title(\'Units Sold per Day per Store\') plt.legend() plt.show() ``` **Assessment Criteria:** - Completeness of the solution. - Correctness and efficiency of the code. - Quality of the time series plot.","solution":"import pandas as pd import matplotlib.pyplot as plt # 1. Load the datasets def load_data(sales_file, promotions_file): sales = pd.read_csv(sales_file) promotions = pd.read_csv(promotions_file) return sales, promotions # 2. Convert \'date\' columns to datetime def convert_to_datetime(sales, promotions): sales[\'date\'] = pd.to_datetime(sales[\'date\']) promotions[\'date\'] = pd.to_datetime(promotions[\'date\']) return sales, promotions # 3. Handle missing data def handle_missing_data(sales, promotions): sales.fillna(0, inplace=True) promotions.fillna(0, inplace=True) return sales, promotions # 4. Aggregate data def aggregate_data(sales, promotions): total_units_sold = sales.groupby([\'date\', \'store_id\'])[\'units_sold\'].sum().reset_index() avg_discount = promotions.groupby(\'product_id\')[\'discount\'].mean().reset_index() return total_units_sold, avg_discount # 5. Merge the datasets def merge_data(sales, promotions): merged_data = pd.merge(sales, promotions, on=[\'date\', \'product_id\'], how=\'left\') return merged_data # 6. Generate the time series plot def plot_time_series(total_units_sold): for store in total_units_sold[\'store_id\'].unique(): store_data = total_units_sold[total_units_sold[\'store_id\'] == store] plt.plot(store_data[\'date\'], store_data[\'units_sold\'], label=f\'Store {store}\') plt.xlabel(\'Date\') plt.ylabel(\'Units Sold\') plt.title(\'Units Sold per Day per Store\') plt.legend() plt.show()"},{"question":"Question: Implementing a Python Extension Type in C **Overview**: In this task, you will implement a simple custom Python extension type using Python\'s C API. This type will have a variable length (based on `PyVarObject`), and you\'ll add support for getting and setting specific attributes. **Requirements**: 1. **Type Definition**: - Define a new type `CustomType` that inherits from `PyVarObject`. - Include the necessary macros to set up this type. 2. **Attributes**: - Implement two attributes: - `value`: an integer that can be read and written. - `description`: a string that can be read and written. - Use the `PyMemberDef` and `PyGetSetDef` structures for these attributes. 3. **Methods**: - Implement a method `increment_value` that increments the `value` attribute by a given integer. - Implement a method `concat_description`, which concatenates a given string to `description`. 4. **Initialization**: - Provide an initializer function to set the initial `value` and `description`. **Implementation Details**: - **Expected Input/Output**: - The initializer will receive initial values for `value` (int) and `description` (str). - The `increment_value` method will receive an integer to add to `value`. - The `concat_description` method will receive a string to concatenate to `description`. - All getter and setter methods should handle cases where incorrect types are passed, raising appropriate Python exceptions. - **Constraints**: - Make sure to handle reference counting properly. - Ensure that the methods and attributes follow the calling conventions (`METH_VARARGS`, etc.) and access rules (`READONLY` if necessary). - **Performance**: - Optimize method call performance where possible by using the appropriate calling conventions. Below is a skeleton code structure to guide you: ```c #include <Python.h> typedef struct { PyVarObject ob_base; int value; PyObject *description; } CustomType; static int CustomType_init(CustomType *self, PyObject *args, PyObject *kwds) { // Initialization code here } static void CustomType_dealloc(CustomType *self) { // Deallocate code here } static PyMemberDef CustomType_members[] = { // Define value member // Define description member {NULL} // Sentinel }; static PyObject *CustomType_increment_value(CustomType *self, PyObject *args) { // Method implementation here } static PyObject *CustomType_concat_description(CustomType *self, PyObject *args) { // Method implementation here } static PyMethodDef CustomType_methods[] = { {\\"increment_value\\", (PyCFunction)CustomType_increment_value, METH_VARARGS, \\"Increment the value\\"}, {\\"concat_description\\", (PyCFunction)CustomType_concat_description, METH_VARARGS, \\"Concatenate to description\\"}, {NULL} // Sentinel }; static PyTypeObject CustomTypeType = { PyVarObject_HEAD_INIT(NULL, 0) // Fill the rest of the PyTypeObject struct }; static struct PyModuleDef custommodule = { PyModuleDef_HEAD_INIT, \\"custommodule\\", NULL, -1, NULL // Methods if any }; PyMODINIT_FUNC PyInit_custommodule(void) { PyObject *m; if (PyType_Ready(&CustomTypeType) < 0) return NULL; m = PyModule_Create(&custommodule); if (m == NULL) return NULL; Py_INCREF(&CustomTypeType); if (PyModule_AddObject(m, \\"CustomType\\", (PyObject *)&CustomTypeType) < 0) { Py_DECREF(&CustomTypeType); Py_DECREF(m); return NULL; } return m; } ``` Complete the implementation as per the requirements outlined above.","solution":"# This Python file is provided as a placeholder for unit testing purposes # because the actual implementation is in C and requires compilation. class CustomType: def __init__(self, value, description): self.value = value self.description = description def increment_value(self, increment): self.value += increment def concat_description(self, added_str): self.description += added_str"},{"question":"# Coding Assessment: Creating and Visualizing Custom Error Bars in Seaborn Objective Design and implement a function using the seaborn library to visualize a dataset with customized error bars based on user-defined methods of calculating error intervals. Problem Statement You are given a dataset `data` which consists of numerical values. Your task is to create a function `plot_custom_errorbars` that takes the dataset and user-specified parameters for generating error bars. You should visualize the dataset using a point plot with customized error bars that are not directly supported by seaborn’s built-in methods. Function Signature ```python def plot_custom_errorbars(data: list, method: str, param: float): Plots a dataset with custom error bars using seaborn. Parameters: data (list): A list of numerical values representing the dataset. method (str): A string specifying the method to calculate error bars. Can be \'custom_sd\', \'custom_pi\', or \'custom_ci\'. param (float): A parameter controlling the size of the interval, specific to each method. Returns: None # Your implementation here ``` Expected Input - `data`: A list of numerical values representing the dataset (e.g., `[0.5, 1.1, 2.3, 2.9, 3.6, 4.8, 5.2]`). - `method`: A string specifying the method to calculate error bars. Can be one of the following: - `\'custom_sd\'`: Custom standard deviation, where `param` is a scaling factor. - `\'custom_pi\'`: Custom percentile interval, where `param` represents the width in percent. - `\'custom_ci\'`: Custom confidence interval using bootstrapping, where `param` is the number of bootstrap iterations. - `param`: A float parameter controlling the size of the interval, specific to each method. Constraints - Implement the custom error bar calculations yourself without relying on seaborn’s built-in `errorbar` parameter. - Ensure that the function handles edge cases such as empty datasets or invalid parameters gracefully. Performance Requirements - The function should be efficient and handle datasets containing up to 10,000 data points. - Bootstrapping should use a reasonable number of iterations (recommended: between 1000 and 10000) for performance - higher values of `param` may demand more computation time. Example Usage ```python import seaborn as sns import matplotlib.pyplot as plt # Example data data = [0.5, 1.1, 2.3, 2.9, 3.6, 4.8, 5.2] # Plot custom error bars using standard deviation method with scale 1.5 plot_custom_errorbars(data, \'custom_sd\', 1.5) # Plot custom error bars using percentile interval method with 50% width plot_custom_errorbars(data, \'custom_pi\', 50) # Plot custom error bars using confidence interval method with 5000 bootstrap iterations plot_custom_errorbars(data, \'custom_ci\', 5000) ``` Implementation Details 1. **Custom Standard Deviation (`custom_sd`)**: - Calculate the sample mean and standard deviation. - Scale the standard deviation by `param` and use it to create the error bars. 2. **Custom Percentile Interval (`custom_pi`)**: - Compute the percentiles directly from the data (e.g., for a 50% interval, param=50, calculate the 25th and 75th percentiles). 3. **Custom Confidence Interval (`custom_ci`)**: - Implement bootstrapping by resampling the data with replacement for `param` iterations. - Compute the statistic for each bootstrap sample and use those statistics to create the percentile interval for the confidence interval. 4. Visualize the data using seaborn’s point plot and the custom error bars created from the methods above. Notes - Use proper plotting aesthetics and labeling for clarity. - Ensure the function is modular and can be extended with additional custom error bar methods in the future.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np from scipy.stats import norm def plot_custom_errorbars(data, method, param): Plots a dataset with custom error bars using seaborn. Parameters: data (list): A list of numerical values representing the dataset. method (str): A string specifying the method to calculate error bars. Can be \'custom_sd\', \'custom_pi\', or \'custom_ci\'. param (float): A parameter controlling the size of the interval, specific to each method. Returns: None if not data: raise ValueError(\\"Data cannot be empty\\") mean = np.mean(data) print(\\"mean = \\", mean) if method == \'custom_sd\': std_dev = np.std(data) error = param * std_dev lower_bound = mean - error upper_bound = mean + error elif method == \'custom_pi\': perc_lo = (100 - param) / 2.0 perc_hi = 100 - perc_lo lower_bound, upper_bound = np.percentile(data, [perc_lo, perc_hi]) print(\\"interval = \\", [perc_lo, perc_hi], \\", lo = \\", lower_bound, \\", hi = \\", upper_bound) elif method == \'custom_ci\': # Bootstrap for confidence interval boot_samples = np.random.choice(data, (int(param), len(data)), replace=True) boot_means = np.mean(boot_samples, axis=1) lower_bound, upper_bound = np.percentile(boot_means, [2.5, 97.5]) else: raise ValueError(f\\"Unknown method: {method}\\") # Plotting with seaborn plt.figure(figsize=(10, 6)) sns.pointplot(x=[0], y=[mean], join=False) plt.errorbar(x=[0], y=[mean], yerr=[[mean - lower_bound], [upper_bound - mean]], fmt=\'o\', color=\'blue\') plt.show()"},{"question":"# Question: Advanced Tensor Operations and Autograd in PyTorch You are given a task to perform multiple operations with PyTorch Tensors. The task is divided into sub-parts to ensure you demonstrate your understanding of tensor management, including creation, manipulation, and automatic differentiation. Implement the function `tensor_operations` that performs the following steps: Steps: 1. **Create a Tensor**: - Create a 2D tensor `A` initialized with random values of shape (3, 3) with `dtype=torch.float32`. 2. **Manipulate the Tensor**: - Set the specific value at the position (1, 1) in the tensor `A` to 10. - Create a new tensor `B` by taking the square of each element in the tensor `A`. 3. **Calculate Sum of Elements**: - Compute the sum of all elements in tensor `B` and store it in a scalar tensor `S`. 4. **Gradient Computation**: - Ensure that tensor `A` requires gradient. - Perform backpropagation on tensor `S` to compute the gradient of `S` with respect to `A`. 5. **Return Values**: - Return a dictionary containing: - The original tensor `A`. - The manipulated tensor `B`. - The scalar sum tensor `S`. - The gradients of tensor `A` after backpropagation. Function Signature ```python def tensor_operations() -> dict: pass ``` Example ```python result = tensor_operations() ``` Expected keys in the result dictionary: - \'original_tensor\': (3, 3) Tensor with dtype `torch.float32`. - \'squared_tensor\': (3, 3) Tensor with squared values. - \'sum_tensor\': Scalar tensor with the sum of elements in `squared_tensor`. - \'gradients\': (3, 3) Tensor with gradients of the original tensor. Constraints: - Do not change the shape of tensors once initialized. - Use PyTorch\'s functions and methods to perform the operations. # Evaluation Criteria 1. **Correctness**: The function must perform all tasks as specified. 2. **Code Quality**: The implementation should be clean and follow good coding practices. 3. **Efficiency**: The operations should be performed efficiently using PyTorch.","solution":"import torch def tensor_operations() -> dict: # Step 1: Create a 2D tensor A initialized with random values of shape (3, 3) with dtype=torch.float32 A = torch.randn((3, 3), dtype=torch.float32) # Step 2: Set the specific value at position (1, 1) in tensor A to 10 A[1, 1] = 10 # Ensure tensor A requires gradient A.requires_grad_(True) # Create a new tensor B by taking the square of each element in tensor A B = A ** 2 # Step 3: Compute the sum of all elements in tensor B and store it in a scalar tensor S S = B.sum() # Step 4: Perform backpropagation on tensor S to compute the gradient of S with respect to A S.backward() # Step 5: Return the results in a dictionary return { \'original_tensor\': A, \'squared_tensor\': B, \'sum_tensor\': S, \'gradients\': A.grad }"},{"question":"**Objective:** You are required to write code that demonstrates your understanding of the `email.charset` module within the legacy email API. This will involve manipulating character sets and their encodings as they apply to email headers and bodies. # Problem Statement: You are working on an email client that needs to process and correctly encode email messages for different character sets. You need to handle initializing character sets, encoding strings for use in email headers and bodies, and adding new character sets to the global registry. # Tasks: 1. **Initialize a Character Set:** Implement a function `initialize_charset(input_charset: str) -> Charset` that initializes and returns a `Charset` object for the given `input_charset`. 2. **Encode Email Headers:** Implement a function `encode_header(input_charset: str, header_string: str) -> str` that takes the input character set and a string intended for use in an email header, and returns the correctly encoded string. 3. **Encode Email Bodies:** Implement a function `encode_body(input_charset: str, body_string: str) -> str` that takes the input character set and a string intended for use in an email body, and returns the correctly encoded string. 4. **Register New Character Set:** Implement a function `register_charset(input_charset: str, header_enc: str, body_enc: str, output_charset: str) -> None` that registers a new character set in the global registry. # Function Signatures: ```python from email.charset import Charset, add_charset def initialize_charset(input_charset: str) -> Charset: pass def encode_header(input_charset: str, header_string: str) -> str: pass def encode_body(input_charset: str, body_string: str) -> str: pass def register_charset(input_charset: str, header_enc: str, body_enc: str, output_charset: str) -> None: pass ``` # Constraints: - The character set specifications should conform to the descriptions provided in the `email.charset` module. - Ensure proper error handling for unsupported or invalid character sets. # Example: ```python # Example usage of the functions # Initialize a charset cs = initialize_charset(\'iso-8859-1\') print(cs.input_charset) # Outputs: iso-8859-1 # Encode a string for the header encoded_header = encode_header(\'iso-8859-1\', \'Hello, World!\') print(encoded_header) # Outputs: =?iso-8859-1?q?Hello=2C_World!?= # Encode a string for the body encoded_body = encode_body(\'iso-8859-1\', \'Hello, World!\') print(encoded_body) # Outputs: Hello, World! # Register a new charset register_charset(\'my-charset\', \'BASE64\', \'QP\', \'utf-8\') ``` # Notes: - You may refer to the Python `email.charset` module documentation for specific behaviors and encoding details. - Consider different scenarios for encoding, such as when the character set needs conversion or when specified encodings are required for headers and bodies.","solution":"from email.charset import Charset, add_charset, QP, BASE64 def initialize_charset(input_charset: str) -> Charset: Initialize and return a Charset object for the given input_charset. return Charset(input_charset) def encode_header(input_charset: str, header_string: str) -> str: Encode a string for use in an email header for the given input_charset. charset = initialize_charset(input_charset) return charset.header_encode(header_string) def encode_body(input_charset: str, body_string: str) -> str: Encode a string for use in an email body for the given input_charset. charset = initialize_charset(input_charset) return charset.body_encode(body_string) def register_charset(input_charset: str, header_enc: str, body_enc: str, output_charset: str) -> None: Register a new character set in the global email.charset registry. add_charset(input_charset, header_enc=header_enc, body_enc=body_enc, output_charset=output_charset)"},{"question":"# PyTorch Distributed Training Setup **Problem Statement:** You are tasked with setting up and managing a distributed training job for a neural network using PyTorch. Your goal is to write a Python script that simulates a distributed training setup. This script should be capable of running on multiple nodes and handle possible node failures gracefully, restarting the training process when necessary. Your tasks are as follows: 1. Create a simple neural network model using PyTorch. 2. Implement the necessary setup to launch a distributed training job using torchrun. 3. Simulate node failures and ensure that your setup restarts the job as specified by the parameters. **Requirements:** 1. **Neural Network Model:** - Define a basic neural network model using `torch.nn.Module`. - Use any simple architecture (e.g., a few linear layers with activation functions). 2. **Distributed Training Setup:** - Write a training script (`training_script.py`) that sets up distributed training using PyTorch `torch.distributed`. - Use torchrun to manage this script across multiple nodes. - Implement necessary command-line arguments to be used by torchrun (e.g., number of nodes, number of processes per node, maximum restarts, rendezvous backend, and endpoint). 3. **Simulation of Node Failures:** - Include a mechanism to simulate node failures in your script. For example, you can randomly interrupt the training process and trigger a restart. - Ensure that the training can continue from where it left off after a restart. 4. **Usage of torchrun Command:** - Provide a shell command using torchrun that demonstrates how to run your distributed training script across, say, two nodes with appropriate setup. **Input Format:** There is no direct input for this problem. Instead, the script should be executed via the command line using the torchrun tool. **Output Format:** The script should output the progress of the training process, indicating when a node failure occurs and when the job restarts from the last checkpoint. **Constraints:** - Assume that the training data is small and can be loaded quickly. You do not need to focus on data preprocessing. - The training script should be designed to handle at least two nodes. - The torchrun command should include a configuration where the job runs on two nodes with the possibility of up to 3 restarts. **Sample torchrun Command:** ```bash torchrun --nnodes=2 --nproc-per-node=1 --max-restarts=3 --rdzv-id=job1 --rdzv-backend=c10d --rdzv-endpoint=node1.example.com:29400 training_script.py ``` **Note:** Include comments in your code to explain each step and the rationale behind your implementation choices. Good luck!","solution":"import torch import torch.nn as nn import torch.optim as optim import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP import argparse import random import time import os # 1. Define a basic neural network model class SimpleNeuralNetwork(nn.Module): def __init__(self): super(SimpleNeuralNetwork, self).__init__() self.layer1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.layer2 = nn.Linear(50, 1) def forward(self, x): x = self.layer1(x) x = self.relu(x) x = self.layer2(x) return x # 2. Function to simulate node failure def simulate_node_failure(): if random.random() < 0.1: # 10% chance of failure print(\\"Simulating node failure.\\") raise RuntimeError(\\"Simulated node failure\\") # 3. Function for training process def train(rank, world_size): dist.init_process_group(\\"nccl\\", rank=rank, world_size=world_size) torch.cuda.set_device(rank) model = SimpleNeuralNetwork().to(rank) model = DDP(model, device_ids=[rank]) optimizer = optim.SGD(model.parameters(), lr=0.01) loss_fn = nn.MSELoss() for epoch in range(10): simulate_node_failure() # Simulate a failure inputs = torch.randn(20, 10).to(rank) outputs = torch.randn(20, 1).to(rank) optimizer.zero_grad() preds = model(inputs) loss = loss_fn(preds, outputs) loss.backward() optimizer.step() print(f\\"Rank {rank}, Epoch {epoch}, Loss: {loss.item()}\\") dist.destroy_process_group() # 4. Main script setup def main(): parser = argparse.ArgumentParser() parser.add_argument(\\"--local_rank\\", type=int, default=0) args = parser.parse_args() world_size = int(os.environ[\'WORLD_SIZE\']) rank = args.local_rank train(rank, world_size) if __name__ == \\"__main__\\": main()"},{"question":"Objective - Implement a function that uses the provided iterators from the `email.iterators` module to process an email message object. Problem Statement You need to write a function `extract_text_content(message)` that accepts an email `message` object and returns the concatenated plain text content from all subparts of the message that have a MIME type of `text/plain`. Details - **Input**: - `message`: A Message object representing the email. - **Output**: - A string containing the concatenated `text/plain` content from all parts of the message. Requirements: 1. Use the `typed_subpart_iterator` to filter for subparts with MIME type `text/plain`. 2. Extract and concatenate the text content from these subparts. Here is the function signature: ```python def extract_text_content(message): Extracts and concatenates plain text content from an email message object. Parameters: message (Message): The email message object to process. Returns: str: Concatenated plain text content from all \'text/plain\' subparts. pass ``` Constraints: - Assume that the input `message` object is valid and contains MIME parts. - Ignore non-plain text parts. Example: Given the following structure of an email message: ``` multipart/mixed text/plain text/html multipart/alternative text/plain text/html ``` The `extract_text_content` function should extract and return a string containing the concatenated content of the `text/plain` parts only.","solution":"from email import iterators def extract_text_content(message): Extracts and concatenates plain text content from an email message object. Parameters: message (Message): The email message object to process. Returns: str: Concatenated plain text content from all \'text/plain\' subparts. text_content = [] for part in iterators.typed_subpart_iterator(message, \'text\', \'plain\'): text_content.append(part.get_payload(decode=True).decode(part.get_content_charset() or \'utf-8\')) return \'\'.join(text_content)"},{"question":"**Email Generator Implementation and Usage** **Objective:** You are required to demonstrate your understanding of the `email.generator` module by implementing functionality to create serialized email messages using the `Generator` class and handling text and binary data appropriately. **Task:** 1. Write a Python function `generate_email_message` that accepts an `email.message.EmailMessage` object and a file path, and serializes the email message to the specified file path using the `email.generator.Generator` class. 2. Extend the functionality by implementing another function `generate_binary_email_message` that handles binary data in the email and uses the `email.generator.BytesGenerator` class to serialize the message to the specified file path. 3. Implement a function `generate_decoded_email_message` that extends the functionality to handle non-text parts using the `email.generator.DecodedGenerator` class. For non-text parts, the output should contain a string providing information about the part. **Specifications:** - The `generate_email_message` function should accept two parameters: - `message`: an instance of `email.message.EmailMessage` representing the email message. - `file_path`: a string representing the path of the file where the serialized email will be saved. - The `generate_binary_email_message` function should also accept the same parameters and handle binary data appropriately. - The `generate_decoded_email_message` function should accept the same parameters plus an optional `fmt` string, which should be passed to `DecodedGenerator` to format the representation of non-text parts. **Constraints:** - Ensure that the generated email is RFC-compliant. - For `generate_decoded_email_message`, if `fmt` is not provided, use the default format as specified in the documentation. **Example Usage:** ```python from email.message import EmailMessage from your_module import generate_email_message, generate_binary_email_message, generate_decoded_email_message # Create an example email message msg = EmailMessage() msg[\'From\'] = \'alice@example.com\' msg[\'To\'] = \'bob@example.com\' msg[\'Subject\'] = \'Example Email\' msg.set_content(\'This is a plain text email.\') # Generate text email message generate_email_message(msg, \'text_email.txt\') # Generate binary email message generate_binary_email_message(msg, \'binary_email.bin\') # Generate decoded email message with non-text parts info generate_decoded_email_message(msg, \'decoded_email.txt\') ``` **Evaluation Criteria:** - Correctness: The functions should work as specified, correctly serializing the email message. - Compliance: The output should be RFC-compliant. - Handling of text and binary data: Distinguish between text and binary data and handle them appropriately in each function. - Code quality: Code should be clean, well-organized, and follow Python naming conventions.","solution":"import os from email.message import EmailMessage from email.generator import Generator, BytesGenerator, DecodedGenerator def generate_email_message(message, file_path): Serializes an EmailMessage object to a specified file path using the Generator class. with open(file_path, \'w\') as file: gen = Generator(file) gen.flatten(message) def generate_binary_email_message(message, file_path): Serializes an EmailMessage object to a specified file path using the BytesGenerator class. with open(file_path, \'wb\') as file: gen = BytesGenerator(file) gen.flatten(message) def generate_decoded_email_message(message, file_path, fmt=None): Serializes an EmailMessage object to a file, decoding non-text parts using the DecodedGenerator class. with open(file_path, \'w\') as file: gen = DecodedGenerator(file, fmt=fmt) gen.flatten(message)"},{"question":"# Custom Autocomplete Feature using rlcompleter The \\"rlcompleter\\" module in Python provides functionality to complete valid Python identifiers and keywords. It can be particularly useful in interactive Python sessions. In this assignment, you are required to use the \\"Completer\\" class from the \\"rlcompleter\\" module to create a custom autocomplete feature. Here\'s what you need to do: # Task 1. Import the \\"rlcompleter\\" and \\"readline\\" modules. 2. Create a list of custom strings that you want to be available for autocompletion. 3. Instantiate the \\"Completer\\" class and use it to provide completions from your custom list of strings. 4. Implement a function `custom_complete(text: str, state: int) -> str` that: - Uses the \\"Completer\\" class to return the state-th completion for the input text from your custom list. - Considers if the text does not contain a period character or if it follows a dotted name pattern. # Example: Suppose your custom list of strings includes: ```python custom_strings = [ \\"custom_function_one\\", \\"custom_function_two\\", \\"custom_variable\\", \\"another_custom_variable\\" ] ``` Your `custom_complete` function might be used as follows: ```python # Setup the custom completer readline.set_completer(custom_complete) # Simulate autocompletion prompts print(custom_complete(\\"custom_\\", 0)) # Outputs: \\"custom_function_one\\" print(custom_complete(\\"custom_\\", 1)) # Outputs: \\"custom_function_two\\" print(custom_complete(\\"another_\\", 0)) # Outputs: \\"another_custom_variable\\" ``` # Constraints: - You are only allowed to use standard library modules. - The function should handle cases where no completions are available gracefully, returning `None` in such scenarios. # Notes: - Remember to consider both the scenarios of text without period and dotted names as described in the \\"rlcompleter\\" documentation. # Deliverable: Provide the implementation of the `custom_complete` function along with a brief explanation of how it works.","solution":"import rlcompleter import readline # Define a list of custom strings for autocompletion custom_strings = [ \\"custom_function_one\\", \\"custom_function_two\\", \\"custom_variable\\", \\"another_custom_variable\\" ] # Create a completer instance class CustomCompleter(rlcompleter.Completer): def __init__(self, custom_list): super().__init__() self.matches = custom_list def complete(self, text, state): # Trigger custom completion if state == 0: # Create a list of matches self.matches_found = [s for s in self.matches if s.startswith(text)] # Return None if no match found, else return the match try: return self.matches_found[state] except IndexError: return None # Instantiate the CustomCompleter with our custom strings completer = CustomCompleter(custom_strings) def custom_complete(text: str, state: int) -> str: Custom completion function that uses the CustomCompleter class to provide completions for the given text from the list of custom strings. return completer.complete(text, state) # Setting the custom completer to readline readline.set_completer(custom_complete)"},{"question":"Objective Demonstrate your understanding of the `hashlib` module by implementing functions for secure password hashing and verification. Problem Statement You are required to implement two functions: `hash_password` and `verify_password`. 1. **hash_password(password: str, salt: bytes = None) -> Tuple[bytes, bytes]:** - This function should take a plaintext password and an optional salt. - If the salt is not provided, generate a secure random salt using the `secrets` module. - Hash the password using the `hashlib` module with the BLAKE2b algorithm. - Return the salt and the hashed password as a tuple of bytes. 2. **verify_password(stored_password: bytes, provided_password: str, salt: bytes) -> bool:** - This function should take the stored hashed password, a provided plaintext password, and the salt used for the original hashing. - Hash the provided password with the given salt using the same `hashlib` settings. - Return `True` if the hashes match, else `False`. Constraints - Use the BLAKE2b algorithm from the `hashlib` module for hashing. - Ensure that salts are at least 16 bytes in length. - The functions should be efficient and work for passwords up to 256 characters long. Example Usage ```python from typing import Tuple import hashlib import secrets def hash_password(password: str, salt: bytes = None) -> Tuple[bytes, bytes]: if salt is None: salt = secrets.token_bytes(16) hash_obj = hashlib.blake2b(password.encode(), salt=salt) return salt, hash_obj.digest() def verify_password(stored_password: bytes, provided_password: str, salt: bytes) -> bool: hash_obj = hashlib.blake2b(provided_password.encode(), salt=salt) return hash_obj.digest() == stored_password # Example: password = \\"securepassword123\\" salt, hashed_pw = hash_password(password) assert verify_password(hashed_pw, password, salt) == True assert verify_password(hashed_pw, \\"wrongpassword\\", salt) == False ``` Notes - You can assume that the functions will receive valid input as per the constraints specified. - Your implementation should handle different edge cases effectively, especially in hashing and comparison. **Good Luck!**","solution":"from typing import Tuple import hashlib import secrets def hash_password(password: str, salt: bytes = None) -> Tuple[bytes, bytes]: if salt is None: salt = secrets.token_bytes(16) # Generate a 16-byte random salt hash_obj = hashlib.blake2b(password.encode(), salt=salt) return salt, hash_obj.digest() def verify_password(stored_password: bytes, provided_password: str, salt: bytes) -> bool: hash_obj = hashlib.blake2b(provided_password.encode(), salt=salt) return hash_obj.digest() == stored_password"},{"question":"# Question: Dynamic Plot Styling with Seaborn You are given a dataset containing the heights and weights of a group of individuals. Your task is to create a scatter plot using the Seaborn package, depicting the relationship between height and weight, and to apply different stylistic themes dynamically. You should implement a function `create_styled_plots` which will generate three different styled scatter plots and save them as separate image files. **Function Signature:** ```python def create_styled_plots(data: pd.DataFrame) -> None: pass ``` # Input: - `data` (pd.DataFrame): A DataFrame containing two columns, \\"height\\" and \\"weight\\". # Output: - The function should save three image files named `darkgrid_plot.png`, `whitegrid_plot.png`, and `ticks_plot.png` corresponding to the styles applied to each plot. # Constraints and Requirements: 1. Use the following seaborn styles for each plot: - \\"darkgrid\\" - \\"whitegrid\\" - \\"ticks\\" 2. Each plot should have appropriate labels for the x-axis (\\"Height\\") and y-axis (\\"Weight\\"). 3. Save the plots with the specified filenames in the current working directory. # Example: Given the following data: | height | weight | |--------|--------| | 160 | 60 | | 170 | 70 | | 180 | 80 | | 175 | 75 | | 165 | 65 | ```python import pandas as pd data = pd.DataFrame({ \\"height\\": [160, 170, 180, 175, 165], \\"weight\\": [60, 70, 80, 75, 65] }) create_styled_plots(data) ``` After running the code above, your working directory should contain three new files: - `darkgrid_plot.png` - `whitegrid_plot.png` - `ticks_plot.png` Each of these files should display a scatter plot with the corresponding styles applied. # Additional Information: - Ensure your solution handles the plot settings dynamically. - Your code should be efficient and avoid redundant code where possible. - Make use of `sns.axes_style` both as a function call and as a context manager. **Hints:** - Refer to the `seaborn` documentation for details on the `sns.axes_style` function. - Remember to use the `matplotlib` library\'s functions to save the generated plots (`plt.savefig`).","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_styled_plots(data: pd.DataFrame) -> None: styles = [\\"darkgrid\\", \\"whitegrid\\", \\"ticks\\"] filenames = [\\"darkgrid_plot.png\\", \\"whitegrid_plot.png\\", \\"ticks_plot.png\\"] for style, filename in zip(styles, filenames): with sns.axes_style(style): plt.figure(figsize=(8, 6)) sns.scatterplot(data=data, x=\\"height\\", y=\\"weight\\") plt.xlabel(\\"Height\\") plt.ylabel(\\"Weight\\") plt.title(f\\"Scatter Plot with {style} style\\") plt.savefig(filename) plt.close()"},{"question":"**Objective:** Implement a function that takes an HTML string, escapes it, and then unescapes it, ensuring the final output is identical to the original input. **Problem Statement:** You are given a string containing HTML content. Your task is to: 1. Escape the HTML content using `html.escape()`. 2. Unescape the resulting string using `html.unescape()`. 3. Verify that the final output matches the original input string. Write a function `verify_html_conversion(html_content: str) -> bool` that performs the above steps and returns a boolean indicating whether the original HTML content is preserved after escaping and unescaping. **Function Signature:** ```python def verify_html_conversion(html_content: str) -> bool: pass ``` **Input:** - `html_content`: A string containing HTML content (1 <= len(html_content) <= 10^6). **Output:** - Returns `True` if the final output matches the original input string, otherwise returns `False`. **Constraints:** - The input will always be a valid string. - Performance considerations: Your implementation should be efficient even for the maximum length of the input string. **Examples:** ```python assert verify_html_conversion(\\"Hello, world!\\") == True assert verify_html_conversion(\\"<div>Hello, world!</div>\\") == True assert verify_html_conversion(\\"George & Mary\\") == True assert verify_html_conversion(\'HTML with \\"quotes\\" and \'single quotes\'\') == True ``` **Explanation:** For each example: 1. The HTML content is escaped. 2. The escaped content is unescaped back to the original HTML content. 3. The final output is compared with the original input to ensure they match. **Notes:** - Use the `html.escape` and `html.unescape` functions from the `html` module as described in the provided documentation to solve this problem.","solution":"import html def verify_html_conversion(html_content: str) -> bool: Given an HTML content string, escape it and then unescape it, ensuring the final output is identical to the original input. Parameters: html_content (str): The original HTML content. Returns: bool: True if the final output matches the original input, False otherwise. escaped_content = html.escape(html_content) unescaped_content = html.unescape(escaped_content) return unescaped_content == html_content"},{"question":"**Secure Password Generator** Implement a function `generate_secure_password(length: int) -> str` that generates a secure password based on the following criteria: 1. The password must be at least `length` characters long. 2. The password must contain at least one lowercase letter, one uppercase letter, one digit, and one special character (from the set `!@#%^&*()-_=+[]{}|;:,.<>?`). 3. The password must be randomly generated and secure, using the `secrets` module. # Input: - `length` (int): The minimum length of the password. (1 ≤ length ≤ 100) # Output: - Returns a `str` that represents the generated password. # Example: ```python import secrets import string def generate_secure_password(length: int) -> str: # Your implementation here # Example Usage: password = generate_secure_password(12) print(password) # Output: A secure password like \'A3fH7%hJ9kL\' ``` # Constraints: - The password generation must use only the `secrets` module for randomness. - The password must satisfy all the criteria mentioned above. **Edge Cases to Consider:** - Handling the minimum possible length (1). - Ensuring all character types are included even when length is small. - Generating different passwords on multiple calls to the function. **Notes:** - It\'s recommended to use the `string` module for easy access to character sets. - Make sure to handle cases where the password length is less than the required minimum to include all character types. - The function should ideally keep trying until a valid password (meeting all criteria) is generated.","solution":"import secrets import string def generate_secure_password(length: int) -> str: if length < 4: raise ValueError(\\"Password length must be at least 4 to include all required character types\\") all_characters = string.ascii_letters + string.digits + \\"!@#%^&*()-_=+[]{}|;:,.<>?\\" # Ensure the password contains at least one of each required character type password = [ secrets.choice(string.ascii_lowercase), secrets.choice(string.ascii_uppercase), secrets.choice(string.digits), secrets.choice(\\"!@#%^&*()-_=+[]{}|;:,.<>?\\") ] # Fill the remaining length of the password with randomly selected characters password += [secrets.choice(all_characters) for _ in range(length - 4)] # Shuffle the list to avoid a predictable pattern secrets.SystemRandom().shuffle(password) return \'\'.join(password)"},{"question":"**Question: py_compile Module Usage and Error Handling** The `py_compile` module in Python allows you to compile Python source files into byte-code, which can be useful for various reasons, including improving startup times and sharing compiled files without revealing the source code. **Task:** Write a function `compile_python_files(file_paths: list, output_dir: str, raise_errors: bool = False, quiet_mode: int = 0) -> dict` that takes: 1. `file_paths`: A list of paths to Python source files that need to be compiled. 2. `output_dir`: A directory where the compiled byte-code files should be saved. If this directory does not exist, raise a `FileNotFoundError`. 3. `raise_errors`: A boolean indicating whether to raise exceptions on compile errors. Defaults to `False`. 4. `quiet_mode`: An integer to control the verbosity of error messages. Defaults to `0`. The function should: 1. Compile each file in `file_paths` using `py_compile.compile`. 2. Save the compiled byte-code files in the `output_dir`. 3. Return a dictionary with the paths of the source files as keys and the paths of the generated byte-code files as values. If a file could not be compiled, the value should be `None`. **Constraints:** - You should handle the possible exceptions (`PyCompileError`, `FileExistsError`) and log appropriate error messages when `raise_errors` is `False`. - Ensure that all file paths provided are valid Python files (`.py` extension). - Performance should be considered; avoid redundant operations and handle the files efficiently. **Example:** ```python file_paths = [\'script1.py\', \'script2.py\'] output_dir = \'/compiled_files\' # Assuming that script1.py compiles successfully, but script2.py has a syntax error result = compile_python_files(file_paths, output_dir, raise_errors=False, quiet_mode=1) # Sample result # { # \'script1.py\': \'/compiled_files/__pycache__/script1.cpython-310.pyc\', # \'script2.py\': None # } ``` **Notes:** - You should use the `py_compile.compile` function parameters such as `doraise` and `quiet` to control error handling as specified. - The `os` and `py_compile` modules will be helpful in implementing this function. Good luck!","solution":"import os import py_compile def compile_python_files(file_paths, output_dir, raise_errors=False, quiet_mode=0): Compiles each Python file in file_paths and saves the compiled byte-code in output_dir. :param file_paths: List of paths to Python source files to compile. :param output_dir: Directory where compiled byte-code files should be saved. :param raise_errors: If True, raise exceptions on compile errors. :param quiet_mode: Control the verbosity of error messages (same as py_compile.compile\'s quiet parameter). :return: Dictionary with source file paths as keys and compiled byte-code paths as values. if not os.path.exists(output_dir): raise FileNotFoundError(f\\"Output directory \'{output_dir}\' does not exist.\\") results = {} for file_path in file_paths: if not file_path.endswith(\'.py\'): results[file_path] = None continue try: compiled_file = py_compile.compile(file_path, cfile=None, dfile=None, doraise=raise_errors, quiet=quiet_mode) if compiled_file: compiled_filename = os.path.basename(compiled_file) output_filepath = os.path.join(output_dir, compiled_filename) os.rename(compiled_file, output_filepath) results[file_path] = output_filepath else: results[file_path] = None except (py_compile.PyCompileError, OSError) as e: if raise_errors: raise e results[file_path] = None return results"},{"question":"# Python Coding Assessment: Subprocess Management Objective: Design a Python function that utilizes the subprocess module to perform multiple tasks, handle potential errors, and ensure the safe and efficient execution of commands. Problem Statement: Implement a function `execute_commands` that takes a list of shell commands and executes them sequentially. Each command\'s standard output and error should be captured, and a detailed execution report should be provided. The function should: 1. Execute each command using `subprocess.Popen`. 2. Capture and return the combined stdout and stderr output of each command. 3. Properly handle timeouts and other exceptions, logging an appropriate error message. 4. Ensure that no command blocks the execution of subsequent commands even if it fails or generates a large amount of output. 5. Include safety measures to prevent shell injection vulnerabilities by avoiding the use of `shell=True` unless strictly required. # Function Signature: ```python def execute_commands(commands: list[str], timeout: int) -> dict: Execute a list of shell commands and capture their outputs. Args: commands (list[str]): A list of shell commands to execute. timeout (int): The maximum time in seconds to wait for each command to complete. Returns: dict: A dictionary where each key is the command and the value is a tuple consisting of: - (combined stdout and stderr output, exit code) - or (\'Error: <error_message>\', None) if an error occurred before execution. pass ``` # Input: - `commands`: A list of shell commands to be executed. Each command is a string. - `timeout`: An integer specifying the maximum time in seconds to wait for each command to complete. # Output: - A dictionary where each key is a command from the input list and the value is a tuple: - If the command executed successfully: `(combined stdout and stderr output as a string, exit code)` - If there was an error: `(\'Error: <error_message>\', None)` # Constraints: 1. Each command should be executed within the given timeout. 2. Use appropriate safety measures when dealing with shell commands. 3. Handle any possible exceptions and ensure that a failed command does not prevent subsequent commands from being executed. # Example: ```python commands = [ \\"echo \'Hello, World!\'\\", \\"ls -l\\", \\"sleep 5\\", # This should trigger the timeout if a short timeout is set \\"invalid_command\\" # This should raise an error ] timeout = 2 output = execute_commands(commands, timeout) print(output) # Expected Output: # { # \\"echo \'Hello, World!\'\\": (\\"Hello, World!n\\", 0), # \\"ls -l\\": (\\"<output of ls -l>\\", 0), # \\"sleep 5\\": (\\"Error: Command timed out\\", None), # \\"invalid_command\\": (\\"Error: [Errno 2] No such file or directory: \'invalid_command\'\\", None) # } ``` **Note:** - This question tests the student\'s ability to manage subprocesses in Python, handle different exception cases, and write robust and secure code. - The implementation should avoid using `shell=True` unless explicitly necessary, to mitigate security risks. - Proper usage of `Popen`, handling of pipes, and managing command timeouts are crucial elements to focus on.","solution":"import subprocess import shlex def execute_commands(commands, timeout): Execute a list of shell commands and capture their outputs. Args: commands (list[str]): A list of shell commands to execute. timeout (int): The maximum time in seconds to wait for each command to complete. Returns: dict: A dictionary where each key is the command and the value is a tuple consisting of: - (combined stdout and stderr output, exit code) - or (\'Error: <error_message>\', None) if an error occurred before execution. results = {} for command in commands: try: proc = subprocess.Popen(shlex.split(command), stdout=subprocess.PIPE, stderr=subprocess.PIPE) try: stdout, stderr = proc.communicate(timeout=timeout) output = stdout.decode() + stderr.decode() results[command] = (output, proc.returncode) except subprocess.TimeoutExpired: proc.kill() stdout, stderr = proc.communicate() output = stdout.decode() + stderr.decode() results[command] = (\\"Error: Command timed out\\", None) except Exception as e: results[command] = (f\\"Error: {str(e)}\\", None) return results"},{"question":"# Advanced Python Coding Assessment Problem Statement You are tasked with creating a custom logger that can write log messages to a file. The logger should support both text and binary modes, and it should incorporate buffering for performance improvements. Additionally, the logger should be able to handle different encodings for text logs. Requirements 1. **Logger Class**: - Create a class `CustomLogger` that can write log messages. - The logger should be able to switch between text mode and binary mode using a parameter `mode` in the constructor. - The logger should support different encodings, specified using an `encoding` parameter for text mode. 2. **Methods**: - `write_log(self, message: Union[str, bytes])`: Writes the given message to the log file. If the logger is in text mode, the message must be a `str`; otherwise, it should be `bytes`. Handle any invalid types with appropriate exceptions. - `close(self)`: Closes the log file. - `flush(self)`: Forces writing any buffered data to the file. 3. **Initialization**: - The class constructor should take the following parameters: - `file_path: str`: Path to the log file. - `mode: str`: Mode in which the file is to be opened (\'text\' or \'binary\'). - `encoding: Optional[str]`: Encoding for text mode (default to \'utf-8\'). ```python from typing import Union import io class CustomLogger: def __init__(self, file_path: str, mode: str = \'text\', encoding: str = \'utf-8\'): Initialize the CustomLogger with the given file path, mode, and encoding. :param file_path: Path to the log file. :param mode: Mode of the logger (\'text\' or \'binary\'). :param encoding: Encoding for text mode (default \'utf-8\'). # Your implementation here def write_log(self, message: Union[str, bytes]): Write the given message to the log. :param message: Message to write (str for text mode, bytes for binary mode). :raises TypeError: If the message type does not match the mode. # Your implementation here def close(self): Close the log file. # Your implementation here def flush(self): Flush the buffered data to the file. # Your implementation here # Example Usage if __name__ == \\"__main__\\": text_logger = CustomLogger(\'log.txt\', mode=\'text\', encoding=\'utf-8\') text_logger.write_log(\'This is a text log message.\') text_logger.flush() text_logger.close() binary_logger = CustomLogger(\'log.bin\', mode=\'binary\') binary_logger.write_log(b\'This is a binary log message.\') binary_logger.flush() binary_logger.close() ``` Constraints and Limitations - Performance: Efficiently manage buffered I/O for large data. - Validation: Ensure appropriate types for text and binary modes. - Thread Safety: Not mandatory for this implementation. Evaluation will focus on: - Correct use of `io` module functionalities. - Handling text encodings and buffering appropriately. - Ensuring robustness against incorrect types and ensuring proper resource management.","solution":"from typing import Union, Optional import io class CustomLogger: def __init__(self, file_path: str, mode: str = \'text\', encoding: str = \'utf-8\'): Initialize the CustomLogger with the given file path, mode, and encoding. :param file_path: Path to the log file. :param mode: Mode of the logger (\'text\' or \'binary\'). :param encoding: Encoding for text mode (default \'utf-8\'). self.file_path = file_path self.mode = mode self.encoding = encoding self.file = None self._open_file() def _open_file(self): if self.mode == \'text\': self.file = open(self.file_path, \'w\', encoding=self.encoding, buffering=io.DEFAULT_BUFFER_SIZE) elif self.mode == \'binary\': self.file = open(self.file_path, \'wb\', buffering=io.DEFAULT_BUFFER_SIZE) else: raise ValueError(\\"Mode should be either \'text\' or \'binary\'\\") def write_log(self, message: Union[str, bytes]): Write the given message to the log. :param message: Message to write (str for text mode, bytes for binary mode). :raises TypeError: If the message type does not match the mode. if self.mode == \'text\' and not isinstance(message, str): raise TypeError(\\"Expected message of type \'str\' for text mode\\") elif self.mode == \'binary\' and not isinstance(message, bytes): raise TypeError(\\"Expected message of type \'bytes\' for binary mode\\") self.file.write(message) def close(self): Close the log file. if self.file: self.file.close() def flush(self): Flush the buffered data to the file. if self.file: self.file.flush()"},{"question":"Advanced File and Process Management using the `os` Module Objective: Your task is to implement a Python function that orchestrates a series of file operations and process management tasks. This exercise will test your knowledge and ability to work with the `os` module functionalities, including file handling, process creation, and handling system errors. Problem Statement: Write a Python function `manage_files_and_processes(file_path, new_content)` that performs the following steps: 1. **File Handling:** - Check if the file exists at the provided `file_path`. If it doesn\'t exist, create a new file at that location. - Replace the content of the file with `new_content`. Use the appropriate system call to handle this. 2. **Error Handling:** - Use appropriate error handling mechanisms to catch and handle exceptions that may occur during file operations (e.g., file not found, permission denied). - If an OSError is encountered, log the error message to a new file named `error.log` in the current directory. 3. **Process Management:** - Create a new process that reads the content of the file at `file_path` and prints it to the standard output. - The parent process should wait for the child process to complete before exiting. Input: - `file_path`: A string representing the path of the file. - `new_content`: A string representing the content to write into the file. Output: The function does not return any value, but it performs the specified tasks, including logging errors and managing processes. Constraints and Notes: - Ensure that any file operations are safe and that the file is properly closed after operations. - Handle exceptions gracefully and ensure all exceptions are logged correctly. - The function should be portable, working both on Unix and Windows systems. - You are expected to use appropriate `os` module functions for all file and process management tasks. Example Usage: ```python manage_files_and_processes(\'/tmp/example.txt\', \'This is the new content of the file.\') ``` Implementation Notes: - You may use `os.path` functions to check file existence and other file properties. - For process creation, consider using `os.fork` or `os.spawn` methods as appropriate. - Use `os.write` and `os.read` functions for file operations, ensuring proper handling of file descriptors. - Use `try` and `except` blocks for error handling and ensure the error message is written to `error.log`. Happy coding!","solution":"import os import sys def manage_files_and_processes(file_path, new_content): try: # Check if file exists and create if it doesn\'t if not os.path.exists(file_path): fd = os.open(file_path, os.O_CREAT | os.O_WRONLY) os.close(fd) # Replace content of the file with open(file_path, \'w\') as f: f.write(new_content) # Creating a new process to read and print the file content pid = os.fork() if pid == 0: # Child process with open(file_path, \'r\') as f: print(f.read()) os._exit(0) # Exit the child process else: os.waitpid(pid, 0) # Parent process waits for the child to complete except OSError as e: # Log the error message with open(\'error.log\', \'a\') as log_file: log_file.write(f\\"Error: {e}n\\")"},{"question":"**Coding Assessment Question:** Implement a Python function `execute_python_code_from_file(filename: str) -> Union[None, Any]` that takes the filename of a Python source file as an input. The function should read the content of the file, compile it, and then execute the compiled code in a provided global and local context. The function should handle file opening in binary mode for compatibility across different platforms. **Function Description:** - `filename`: A string representing the path to the Python source file. **Return:** - The function should return any value that the executed code might produce. **Constraints:** 1. You may assume that the filename provided exists and the file contains valid Python code. 2. Ensure compatibility with cross-platform file handling by opening files in binary mode. **Example:** Assuming the `example.py` file contains: ```python def greet(name): return f\\"Hello, {name}!\\" result = greet(\\"World\\") ``` The function call `execute_python_code_from_file(\\"example.py\\")` should execute the file and return the value `\\"Hello, World!\\"`. **Implementation Note:** Make sure to handle: 1. Opening the file in binary mode. 2. Use appropriate start symbols for compiling the file. 3. Use a dictionary for global and local context when executing the code. 4. Proper exception handling to address any compilation or execution errors. ```python def execute_python_code_from_file(filename: str) -> Union[None, Any]: try: import builtins # Open file in binary mode with open(filename, \\"rb\\") as f: code = f.read() # Compile the code (consider file_input as it covers full source code) code_object = compile(code, filename, \'exec\') # Prepare global and local context globals_dict = { \\"__builtins__\\": builtins, \\"__name__\\": \\"__main__\\", \\"__file__\\": filename, } locals_dict = {} # Execute the compiled code exec(code_object, globals_dict, locals_dict) # Retrieve the value of \'result\' if it exists in locals_dict return locals_dict.get(\'result\') except Exception as e: print(f\\"An error occurred: {e}\\") # Example usage output = execute_python_code_from_file(\'example.py\') print(output) # Output should be: Hello, World! ``` **Evaluation Criteria:** - Correct use of file handling in binary mode. - Proper compilation and execution of Python code. - Robust exception handling. - Use of appropriate contexts for global and local variables.","solution":"def execute_python_code_from_file(filename: str): try: import builtins # Open file in binary mode with open(filename, \\"rb\\") as f: code = f.read() # Compile the code (consider file_input as it covers full source code) code_object = compile(code.decode(\'utf-8\'), filename, \'exec\') # Prepare global and local context globals_dict = { \\"__builtins__\\": builtins, \\"__name__\\": \\"__main__\\", \\"__file__\\": filename, } locals_dict = {} # Execute the compiled code exec(code_object, globals_dict, locals_dict) # Retrieve the value of \'result\' if it exists in locals_dict return locals_dict.get(\'result\') except Exception as e: print(f\\"An error occurred: {e}\\") return None"},{"question":"Custom Distribution Implementation Objective Design and implement a custom probability distribution class in PyTorch that demonstrates your understanding of the `torch.distributions` module. The custom distribution should conform to the PyTorch distribution standards and provide essential functionalities like sampling, calculating log-probabilities, and handling transformations. Problem Statement Create a custom probability distribution class called `CustomGammaNormal`, which is a mixture model of a Gamma distribution and a Normal distribution. The distribution should follow these rules: - First, a random variable ( X ) is drawn from a Gamma distribution with shape parameter ( alpha ) and rate parameter ( beta ). - Then, a random variable ( Y ) is drawn from a Normal distribution with mean ( mu = X ) (the variable from the Gamma distribution) and standard deviation ( sigma ). The `CustomGammaNormal` class must include the following methods: 1. **`__init__(self, alpha, beta, sigma)`**: Initializes the distribution with the given shape ((alpha)) and rate ((beta)) parameters for Gamma, and standard deviation ((sigma)) for Normal. 2. **`sample(self, sample_shape=torch.Size())`**: Generates samples from the `CustomGammaNormal` distribution. 3. **`log_prob(self, value)`**: Computes the log-probability of a given value. Input and Output Formats - The `__init__` method should accept `alpha`, `beta`, and `sigma` as floats or tensors. - The `sample` method should return samples from the distribution in the form of a PyTorch tensor. - The `log_prob` method should return the log-probability of the provided value as a PyTorch tensor. Constraints - Use PyTorch tensors and operations to ensure compatibility with GPU acceleration. - Your implementation should inherit from `torch.distributions.Distribution`. - Ensure proper broadcasting of parameters and support for batched sampling and probability computation. Example Usage ```python import torch from custom_distributions import CustomGammaNormal # Define parameters alpha = torch.tensor(2.0) beta = torch.tensor(1.0) sigma = torch.tensor(0.5) # Create CustomGammaNormal distribution custom_dist = CustomGammaNormal(alpha, beta, sigma) # Generate samples samples = custom_dist.sample((1000,)) # Generate 1000 samples print(samples) # Calculate log-probability of a value value = torch.tensor(1.5) log_prob = custom_dist.log_prob(value) print(log_prob) ``` Additional Requirements 1. Add error handling to ensure the parameters are valid (e.g., alpha and beta must be positive). 2. Write unit tests for your custom distribution covering various scenarios including edge cases. 3. Document your code comprehensively. Submit your implementation along with the unit tests and example usage as a single Python script or Jupyter notebook. Good Luck!","solution":"import torch from torch.distributions import Distribution, Gamma, Normal class CustomGammaNormal(Distribution): def __init__(self, alpha, beta, sigma, validate_args=None): self.alpha = alpha self.beta = beta self.sigma = sigma # Ensure parameters are valid if self.alpha <= 0 or self.beta <= 0 or self.sigma <= 0: raise ValueError(\\"Alpha, beta and sigma must be positive.\\") self.gamma = Gamma(self.alpha, self.beta) super().__init__(self.gamma.batch_shape, validate_args=validate_args) def sample(self, sample_shape=torch.Size()): X = self.gamma.sample(sample_shape) normal_dist = Normal(X, self.sigma) return normal_dist.sample() def log_prob(self, value): X_log_probs = self.gamma.log_prob(value) normal_dist = Normal(value, self.sigma) Y_log_probs = normal_dist.log_prob(value) return X_log_probs + Y_log_probs"},{"question":"**Problem Statement:** You are working on a web application that allows users to submit text content which will be displayed on a web page. To ensure that the submitted content does not pose any security risks or render incorrectly, you need to process the content using the `html` module in Python. Write a function `sanitize_and_display(input_text: str) -> str` that takes a string `input_text` and performs the following steps: 1. Use `html.escape()` to convert the characters \\"&\\", \\"<\\", and \\">\\" to HTML-safe sequences. Ensure that double and single quotation marks are also converted. 2. Take the resulting string from step 1 and reverse all the HTML escape sequences back to their original characters using `html.unescape()`. **Input:** - A single string `input_text` which may contain characters that need to be HTML escaped. **Output:** - A single string that represents the original `input_text` after going through the escaping and unescaping process. **Constraints:** - The input string `input_text` will have a maximum length of 1000 characters. - The function must handle cases where the input is already HTML-escaped or contains no characters that need escaping. **Example:** ```python input_text = \'5 > 3 and 3 < 5, \\"quote\\" and \'single quote\'\' output = sanitize_and_display(input_text) print(output) # Output should be the same as input_text: \'5 > 3 and 3 < 5, \\"quote\\" and \'single quote\'\' ``` **Function Signature:** ```python def sanitize_and_display(input_text: str) -> str: # Your code here ``` Ensure your implementation is efficient and correctly handles all specified cases. **Notes:** - Consider edge cases such as an empty string, strings that are already HTML-escaped, and strings with no escapable characters. - Remember to import the `html` module in your function.","solution":"import html def sanitize_and_display(input_text: str) -> str: Escapes and then unescapes the input text using HTML entities. Args: input_text (str): The input string that needs to be sanitized. Returns: str: The sanitized string after escaping and unescaping. escaped_text = html.escape(input_text, quote=True) unescaped_text = html.unescape(escaped_text) return unescaped_text"},{"question":"# Advanced PyTorch Storage Manipulation In this coding exercise, you will demonstrate your understanding of `torch.UntypedStorage` and how it interacts with PyTorch tensors. **Problem Statement**: You are given two tensors sharing the same storage. Your task is to implement two functions: 1. **separate_tensors**: This function will take two tensors as input and should create separate storages for each tensor. 2. **manipulate_storage**: This function will take a tensor and a value as input, and it should fill the tensor\'s storage with the given value. **Function 1: separate_tensors** - **Input**: - `tensor1 (torch.Tensor)`: The first tensor. - `tensor2 (torch.Tensor)`: The second tensor sharing the same storage as `tensor1`. - **Output**: - `new_tensor1 (torch.Tensor)`: The first tensor with a separate, independent storage. - `new_tensor2 (torch.Tensor)`: The second tensor with a separate, independent storage. - **Constraints**: - `tensor1` and `tensor2` are guaranteed to be tensors sharing the same storage. ```python def separate_tensors(tensor1, tensor2): # Your code here pass ``` **Function 2: manipulate_storage** - **Input**: - `tensor (torch.Tensor)`: A tensor whose storage needs to be manipulated. - `value (float)`: The value to fill the storage with. - **Output**: - `modified_tensor (torch.Tensor)`: The tensor after its storage has been filled with the specified value. - **Constraints**: - The function should utilize low-level storage manipulation as demonstrated in the provided documentation. - Direct manipulation of the tensor\'s storage is required rather than using high-level tensor operations like `torch.Tensor.fill_()`. ```python def manipulate_storage(tensor, value): # Your code here pass ``` # Example: ```python import torch # Creating two tensors sharing the same storage t = torch.randn(5) t_view = t[:3] # Step 1: Separate their storages new_t, new_t_view = separate_tensors(t, t_view) assert new_t.data_ptr() != new_t_view.data_ptr(), \\"Storage separation failed.\\" # Step 2: Manipulate storage of a tensor t_filled = manipulate_storage(t, 0.5) assert all(x == 0.5 for x in t_filled), \\"Storage manipulation failed.\\" ``` **Note**: Follow best practices and ensure your code does not directly modify storage unless necessary. Adhere strictly to the constraints provided.","solution":"import torch def separate_tensors(tensor1, tensor2): This function will take two tensors as input and should create separate storages for each tensor. :param tensor1: torch.Tensor :param tensor2: torch.Tensor :return: (torch.Tensor, torch.Tensor) # Clone the tensors to ensure they have separate storages new_tensor1 = tensor1.clone() new_tensor2 = tensor2.clone() return new_tensor1, new_tensor2 def manipulate_storage(tensor, value): This function will take a tensor and a value as input, and it should fill the tensor\'s storage with the given value. :param tensor: torch.Tensor :param value: float :return: torch.Tensor # Access the storage storage = tensor.storage() # Fill the storage with the given value for i in range(storage.size()): storage[i] = value return tensor"},{"question":"Implementing a Custom Python Type Context: You are tasked with implementing a custom Python type using the concepts found in the `PyTypeObject` structure and the associated fields and methods. Problem Statement: Create a custom Python type named `MathObject` that has the following characteristics: 1. **Attributes**: - `value` - an integer or float (modifiable). 2. **Methods**: - A method `inverse` that returns the multiplicative inverse (1/value) of the `MathObject`\'s value. 3. **Custom behaviors**: - Supports addition (`__add__`), subtraction (`__sub__`), and multiplication (`__mul__`) with other `MathObject` instances or numerical values. - String representation (`__repr__`) should return the format `MathObject(value=<current_value>)`. Implementation Requirements: 1. Define the `PyTypeObject` structure for `MathObject`. 2. Implement the `tp_new`, `tp_init`, and `tp_dealloc` functions. 3. Define and implement `inverse` method. 4. Define the methods for `__add__`, `__sub__`, and `__mul__`. 5. Ensure proper memory management (allocate and deallocate memory as necessary). 6. Provide the necessary C code initialization to integrate the new type into Python. Constraints: - You can use C or Cython to implement this custom type. - Ensure that all methods handle edge cases properly (like division by zero). - Follow the Python C API for memory management and type definitions. Hints: - Use the provided documentation to correctly implement and initialize `PyTypeObject`. - Refer to the examples section for structuring your code and correctly initializing new types. - Make sure to test your type thoroughly in Python to ensure all methods and behaviors work as expected. **Input Example**: ```python # Create an instance of MathObject a = MathObject(10) b = MathObject(5) # Perform arithmetic operations c = a + b d = a * b e = a - b # Use the inverse method inv_a = a.inverse() # String representation print(a) ``` **Output Example**: ```python # Output from the operations: c.value == 15 d.value == 50 e.value == 5 # Output from the inverse method: inv_a.value == 0.1 # Output from string representation: \'MathObject(value=10)\' ``` Submission Guidelines: - Provide the C or Cython code required to define and implement this custom type. - Include a Python script that tests the `MathObject` type by demonstrating its capabilities and verifying correct behavior.","solution":"class MathObject: def __init__(self, value): self.value = value def inverse(self): if self.value == 0: raise ValueError(\\"Cannot calculate inverse for zero.\\") return MathObject(1 / self.value) def __add__(self, other): if isinstance(other, MathObject): return MathObject(self.value + other.value) else: return MathObject(self.value + other) def __sub__(self, other): if isinstance(other, MathObject): return MathObject(self.value - other.value) else: return MathObject(self.value - other) def __mul__(self, other): if isinstance(other, MathObject): return MathObject(self.value * other.value) else: return MathObject(self.value * other) def __repr__(self): return f\\"MathObject(value={self.value})\\""},{"question":"**Objective**: This question assesses your understanding of dynamic class creation and manipulation of custom types using the `types` module. Question: You are required to dynamically create a class called `DynamicClass` with the following specifications: 1. The class should inherit from a user-defined base class `Base`. 2. The class should include an instance method `instance_method` that prints `\\"Instance method called\\"`. 3. The class should include a class method `class_method` that prints `\\"Class method called\\"`. 4. The class should override the `__init__` method to accept two arguments `name` and `value` and initialize these as instance attributes. In addition, create an instance of `DynamicClass`, call its instance and class methods, and print the instance attributes `name` and `value`. Implementation Requirements: 1. Use `types.new_class` to dynamically create the class. 2. Define the methods inside the class using a dictionary to represent the class namespace. 3. Demonstrate the functionality by creating an instance and calling the methods as described. ```python import types # Base class definition class Base: def base_method(self): print(\\"Base method called\\") # Function to dynamically create the class and implement its functionality def dynamic_class_creation(): # Define the class namespace with instance methods def class_namespace(ns): ns[\'__init__\'] = lambda self, name, value: setattr(self, \'name\', name) or setattr(self, \'value\', value) ns[\'instance_method\'] = lambda self: print(\\"Instance method called\\") ns[\'class_method\'] = classmethod(lambda cls: print(\\"Class method called\\")) # Create the dynamic class DynamicClass = types.new_class(\'DynamicClass\', (Base,), exec_body=class_namespace) # Create an instance of the dynamic class instance = DynamicClass(\\"example_name\\", 42) # Call instance and class methods instance.instance_method() DynamicClass.class_method() # Print instance attributes print(instance.name) print(instance.value) # Call the function to execute the dynamic class creation and testing dynamic_class_creation() ``` **Expected Output**: ``` Instance method called Class method called example_name 42 ```","solution":"import types # Base class definition class Base: def base_method(self): print(\\"Base method called\\") # Function to dynamically create the class and implement its functionality def dynamic_class_creation(): # Define the class namespace with instance methods def class_namespace(ns): ns[\'__init__\'] = lambda self, name, value: setattr(self, \'name\', name) or setattr(self, \'value\', value) ns[\'instance_method\'] = lambda self: print(\\"Instance method called\\") ns[\'class_method\'] = classmethod(lambda cls: print(\\"Class method called\\")) # Create the dynamic class DynamicClass = types.new_class(\'DynamicClass\', (Base,), exec_body=class_namespace) # Create an instance of the dynamic class instance = DynamicClass(\\"example_name\\", 42) # Call instance and class methods instance.instance_method() DynamicClass.class_method() # Print instance attributes print(instance.name) print(instance.value) return DynamicClass, instance # Execute the function to test the dynamic class creation dynamic_class_creation()"},{"question":"**Objective**: The objective is to demonstrate your ability to load and preprocess various datasets using scikit-learn, as well as to apply a machine learning algorithm to a dataset loaded through one of these methods. You will work with a dataset from OpenML.org and perform basic preprocessing and model training tasks. Question: You are required to: 1. Load the `Iris` dataset from the OpenML repository using `fetch_openml`. 2. Preprocess the data by performing standard scaling using `StandardScaler`. 3. Split the data into training and testing sets with an 80-20 split. 4. Train a `KNeighborsClassifier` on the training data. 5. Evaluate the model on the test data and return the accuracy score. Input: No input will be provided. You need to write a function named `evaluate_iris_model` that performs the above tasks. Output: The function should return: - A float value representing the accuracy score of the model on the test data. Constraints: - You need to use the `Iris` dataset from OpenML. - Use a `StandardScaler` for scaling the feature data. - Use `train_test_split` with a test size of 20%. - Train a `KNeighborsClassifier` with default parameters. - Ensure reproducibility by setting a random state of 42 in `train_test_split`. Function Signature: ```python def evaluate_iris_model() -> float: # Your code here ``` # Example Usage: ```python accuracy = evaluate_iris_model() print(f\\"The accuracy of the KNeighborsClassifier on the Iris dataset is: {accuracy}\\") ``` # Note: - You may not use any built-in dataset loaders other than `fetch_openml` for loading the Iris dataset. - Ensure to handle any potential issues such as missing values or data type conversions if required by the dataset loader or preprocessing steps. Good luck!","solution":"from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def evaluate_iris_model() -> float: # Load the Iris dataset from OpenML iris = fetch_openml(name=\'iris\', version=1) # Extract the features and target X = iris.data y = iris.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train a KNeighborsClassifier on the training data knn = KNeighborsClassifier() knn.fit(X_train, y_train) # Evaluate the model on the test data y_pred = knn.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"# Seaborn Theme and Display Customization Your task is to write a Python function that creates and customizes a seaborn plot using the seaborn\'s `objects` interface. The function should demonstrate an understanding of the theme and display configurations as presented in the provided documentation. Function Signature ```python def customize_seaborn_plot(data, x_col, y_col, display_format=\\"svg\\", theme_style=\\"whitegrid\\", hide_grid=True): pass ``` Input: - `data`: A pandas DataFrame containing the data to be plotted. - `x_col`: A string representing the column name to be used for the x-axis. - `y_col`: A string representing the column name to be used for the y-axis. - `display_format`: A string representing the desired display format (`\\"svg\\"` or `\\"png\\"`). Default is `\\"svg\\"`. - `theme_style`: A string representing the theme style to be used (e.g., `\\"whitegrid\\"`). Default is `\\"whitegrid\\"`. - `hide_grid`: A boolean indicating whether to hide the grid lines on the plot. Default is `True`. Output: - A seaborn plot displayed in the Jupyter notebook according to the specified configurations. Requirements: 1. Configure the theme with the provided `theme_style`. 2. Set the display format based on `display_format`. 3. Customize the plot to hide grid lines if `hide_grid` is `True`. 4. Reset the theme to seaborn defaults at the end of the function. Example: ```python import seaborn as sns import pandas as pd # Example data data = pd.DataFrame({ \\"x\\": range(10), \\"y\\": [1.5, 2.3, 3.1, 4.0, 5.8, 6.2, 7.4, 8.3, 9.0, 10.1] }) # Function call customize_seaborn_plot(data, \\"x\\", \\"y\\", display_format=\\"png\\", theme_style=\\"darkgrid\\", hide_grid=False) ``` This example should output a customized seaborn plot displayed as a PNG image with a \\"darkgrid\\" theme, and the grid lines will be shown. **Note**: Ensure you import necessary modules and handle all edge cases such as invalid column names or unsupported display formats.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def customize_seaborn_plot(data, x_col, y_col, display_format=\\"svg\\", theme_style=\\"whitegrid\\", hide_grid=True): Create and customize a seaborn plot using the specified configurations. :param data: pandas DataFrame containing the data to be plotted. :param x_col: Column name to be used for the x-axis. :param y_col: Column name to be used for the y-axis. :param display_format: Desired display format (\'svg\' or \'png\'). Default is \'svg\'. :param theme_style: Theme style to be used (e.g., \'whitegrid\'). Default is \'whitegrid\'. :param hide_grid: Whether to hide the grid lines on the plot. Default is True. # Validate input parameters if not isinstance(data, pd.DataFrame): raise ValueError(\\"data must be a pandas DataFrame\\") if x_col not in data.columns or y_col not in data.columns: raise ValueError(\\"x_col and y_col must be valid column names in the DataFrame\\") if display_format not in [\\"svg\\", \\"png\\"]: raise ValueError(\\"display_format must be either \'svg\' or \'png\'\\") # Set seaborn theme style sns.set_theme(style=theme_style) # Create the seaborn plot plot = sns.relplot(x=x_col, y=y_col, data=data) # Customize grid visibility based on hide_grid parameter if hide_grid: plot.ax.grid(False) # Set the desired display format if display_format == \\"svg\\": plt.savefig(\\"plot.svg\\") elif display_format == \\"png\\": plt.savefig(\\"plot.png\\") # Display the plot plt.show() # Reset seaborn theme to defaults sns.reset_defaults()"},{"question":"**Objective:** Create a function that takes an integer errno value as input and returns a tuple containing the corresponding exception class (if any), the error message string, and the string name representation of the error from the `errno.errorcode` dictionary. The function should then demonstrate raising the corresponding exception with an appropriate error message. **Instructions:** 1. Implement a function `handle_errno_code(errno_code: int) -> tuple` that performs the following tasks: - Accept an integer value `errno_code`. - Determine the string name representation from `errno.errorcode` corresponding to the given `errno_code`. - Retrieve the exception associated with the `errno_code` using the provided mappings from the documentation (e.g., `errno.EPERM` maps to `PermissionError`). - Use `os.strerror()` to get the human-readable error message string associated with the `errno_code`. - Return a tuple containing the exception class (if applicable, else `None`), the error message string, and the string name representation of the error. 2. Implement another function `raise_based_on_errno(errno_code: int)` that: - Calls the `handle_errno_code` function to retrieve the exception class and other details. - Raises the exception with the error message (if an exception class is found). 3. Demonstrate the functionality of `raise_based_on_errno` by handling different `errno` values in a sample main function. **Function Signatures:** ```python def handle_errno_code(errno_code: int) -> tuple: pass def raise_based_on_errno(errno_code: int): pass ``` **Constraints:** - The function should handle only those error codes that are part of the `errno` module. - If the `errno_code` does not have a corresponding exception mapped, the function should return `None` for the exception class. **Example:** ```python import os import errno def handle_errno_code(errno_code: int) -> tuple: # Your implementation here def raise_based_on_errno(errno_code: int): # Your implementation here def main(): try: raise_based_on_errno(errno.ENOENT) except Exception as e: print(type(e).__name__, e) try: raise_based_on_errno(errno.EPERM) except Exception as e: print(type(e).__name__, e) if __name__ == \\"__main__\\": main() ``` **Sample Output:** ``` FileNotFoundError No such file or directory PermissionError Operation not permitted ``` # Note: - Ensure the exceptions and error messages are raised correctly. - You can refer to `errno.errorcode`, `os.strerror()`, and the mappings provided in the documentation to implement the solution.","solution":"import os import errno # Mapping of errno codes to corresponding exceptions errno_to_exception = { errno.EPERM: PermissionError, errno.ENOENT: FileNotFoundError, errno.ESRCH: ProcessLookupError, errno.EINTR: InterruptedError, errno.EIO: OSError, errno.ENXIO: OSError, errno.E2BIG: OSError, errno.ENOEXEC: OSError, errno.EBADF: OSError, errno.ECHILD: ChildProcessError, errno.EAGAIN: BlockingIOError, errno.ENOMEM: MemoryError, errno.EACCES: PermissionError, } def handle_errno_code(errno_code: int) -> tuple: error_name = errno.errorcode.get(errno_code, \\"Unknown error\\") exception_class = errno_to_exception.get(errno_code, None) error_message = os.strerror(errno_code) return (exception_class, error_message, error_name) def raise_based_on_errno(errno_code: int): exception_class, error_message, error_name = handle_errno_code(errno_code) if exception_class: raise exception_class(error_message) else: raise Exception(error_message)"},{"question":"Python List Operations using C API in Python You are required to implement a Python C extension module that integrates various operations on Python lists using the Python C API. This will demonstrate your understanding of using low-level C functions to manage Python objects. Requirements Implement a C extension module with the following functions: 1. **create_list**: Create a new list of a specified length, where all elements are initially `None`. 2. **insert_item**: Insert an item into the list at a specified index. 3. **append_item**: Append an item to the list. 4. **get_item**: Retrieve an item from the list at a specified index. 5. **set_item**: Set an item at a specified index. 6. **sort_list**: Sort the list in place. 7. **reverse_list**: Reverse the list in place. 8. **list_to_tuple**: Convert the list to a tuple. Function Signatures ```python def create_list(length: int) -> list: Create a new list of a specified length, where all elements are initially None. def insert_item(lst: list, index: int, item) -> None: Insert an item into the list at a specified index. def append_item(lst: list, item) -> None: Append an item to the list. def get_item(lst: list, index: int): Retrieve an item from the list at a specified index. def set_item(lst: list, index: int, item) -> None: Set an item at a specified index. def sort_list(lst: list) -> None: Sort the list in place. def reverse_list(lst: list) -> None: Reverse the list in place. def list_to_tuple(lst: list) -> tuple: Convert the list to a tuple. ``` Constraints 1. Each function must use the relevant C API functions for list operations as described in the documentation. 2. Efficient error handling should be implemented for all functions, particularly for out-of-bounds indexing and memory allocation failures. 3. The performance should be on par with native Python list operations for the same list sizes and element types. Deliverables 1. A `setup.py` script for building the C extension module. 2. The C source file implementing the required functions. 3. A Python script (`test_module.py`) with test cases demonstrating the functionality of the implemented C extension module. Example Usage ```python from your_module import create_list, insert_item, append_item, get_item, set_item, sort_list, reverse_list, list_to_tuple lst = create_list(5) print(lst) # [None, None, None, None, None] append_item(lst, 10) print(lst) # [None, None, None, None, None, 10] insert_item(lst, 2, 5) print(lst) # [None, None, 5, None, None, 10] item = get_item(lst, 2) print(item) # 5 set_item(lst, 1, 20) print(lst) # [None, 20, 5, None, None, 10] sort_list(lst) print(lst) # Sorted list reverse_list(lst) print(lst) # Reversed list tuple_lst = list_to_tuple(lst) print(tuple_lst) # Tuple version of the list ``` Note: Remember to replace `your_module` with the actual name of your C extension module.","solution":"def create_list(length: int) -> list: Create a new list of a specified length, where all elements are initially None. return [None] * length def insert_item(lst: list, index: int, item) -> None: Insert an item into the list at a specified index. lst.insert(index, item) def append_item(lst: list, item) -> None: Append an item to the list. lst.append(item) def get_item(lst: list, index: int): Retrieve an item from the list at a specified index. return lst[index] def set_item(lst: list, index: int, item) -> None: Set an item at a specified index. lst[index] = item def sort_list(lst: list) -> None: Sort the list in place. lst.sort() def reverse_list(lst: list) -> None: Reverse the list in place. lst.reverse() def list_to_tuple(lst: list) -> tuple: Convert the list to a tuple. return tuple(lst)"},{"question":"**Question: Command-line Tool for Computing Statistics** You will create a Python program that computes various statistics for a list of numbers provided via the command line. You will use the `argparse` module to facilitate command-line argument parsing. **Requirements:** 1. The program should accept a list of integers. 2. The program should accept an optional argument `--operation` to specify the statistical operation to be performed. The possible operations are: - `sum`: Compute the sum of the integers. - `mean`: Compute the mean (average) of the integers. - `median`: Compute the median of the integers. - `std`: Compute the standard deviation of the integers. 3. By default, the program should compute and print all four statistics (sum, mean, median, and standard deviation). **Input Format:** - A list of integers should be passed as positional arguments. - The `--operation` argument (optional) specifies the particular statistical operation, should be one of `sum`, `mean`, `median`, `std`. **Output Format:** - Depending on the `--operation` argument, print the result of the specified statistical operation. - If no `--operation` argument is specified, print all four statistics. **Constraints:** - The list of integers should contain at least one integer. - The `--operation` argument, if provided, should be valid. **Performance Requirements:** - The program should efficiently compute the required statistics even for large lists of integers. **Example:** Suppose the program is saved in a file named `stats.py`. The following are some example usages and outputs: 1. Calculate all statistics for the list of integers: ```sh python stats.py 1 2 3 4 5 Sum: 15 Mean: 3.0 Median: 3 Standard Deviation: 1.5811 ``` 2. Calculate only the mean of the list of integers: ```sh python stats.py 1 2 3 4 5 --operation mean Mean: 3.0 ``` 3. Handle invalid arguments: ```sh python stats.py 1 2 a usage: stats.py [-h] [--operation {sum,mean,median,std}] int [int ...] stats.py: error: argument integers: invalid int value: \'a\' ``` **Implementation Notes:** 1. Create an `ArgumentParser` object. 2. Use `add_argument()` to add the positional argument for the list of integers and the optional argument `--operation`. 3. Use the `parse_args()` method to parse the command-line arguments. 4. Use Python\'s standard library (e.g., `statistics` module) to compute the required statistics. ```python import argparse import statistics def main(): parser = argparse.ArgumentParser(description=\'Compute statistics for a list of integers.\') parser.add_argument(\'integers\', metavar=\'N\', type=int, nargs=\'+\', help=\'a list of integers\') parser.add_argument(\'--operation\', choices=[\'sum\', \'mean\', \'median\', \'std\'], help=\'the statistical operation to perform (default: compute all)\') args = parser.parse_args() numbers = args.integers if args.operation == \'sum\': result = sum(numbers) print(f\\"Sum: {result}\\") elif args.operation == \'mean\': result = statistics.mean(numbers) print(f\\"Mean: {result}\\") elif args.operation == \'median\': result = statistics.median(numbers) print(f\\"Median: {result}\\") elif args.operation == \'std\': result = statistics.stdev(numbers) print(f\\"Standard Deviation: {result:.4f}\\") else: print(f\\"Sum: {sum(numbers)}\\") print(f\\"Mean: {statistics.mean(numbers):.1f}\\") print(f\\"Median: {statistics.median(numbers)}\\") print(f\\"Standard Deviation: {statistics.stdev(numbers):.4f}\\") if __name__ == \'__main__\': main() ``` **Test Cases:** 1. Run the script with a list of numbers without the `--operation` flag and verify the output. 2. Run the script with the `--operation` flag set to `sum`, `mean`, `median`, and `std` and verify the respective outputs. 3. Test with invalid inputs and verify that appropriate error messages are displayed.","solution":"import argparse import statistics def compute_statistics(numbers, operation=None): if not operation or operation == \'sum\': total_sum = sum(numbers) if not operation or operation == \'mean\': mean_value = statistics.mean(numbers) if not operation or operation == \'median\': median_value = statistics.median(numbers) if not operation or operation == \'std\': std_dev = statistics.stdev(numbers) results = {} if operation == \'sum\' or operation is None: results[\'sum\'] = total_sum if operation == \'mean\' or operation is None: results[\'mean\'] = mean_value if operation == \'median\' or operation is None: results[\'median\'] = median_value if operation == \'std\' or operation is None: results[\'std\'] = std_dev return results def main(): parser = argparse.ArgumentParser(description=\'Compute statistics for a list of integers.\') parser.add_argument(\'integers\', metavar=\'N\', type=int, nargs=\'+\', help=\'a list of integers\') parser.add_argument(\'--operation\', choices=[\'sum\', \'mean\', \'median\', \'std\'], help=\'the statistical operation to perform (default: compute all)\') args = parser.parse_args() results = compute_statistics(args.integers, args.operation) if \'sum\' in results: print(f\\"Sum: {results[\'sum\']}\\") if \'mean\' in results: print(f\\"Mean: {results[\'mean\']:.1f}\\") if \'median\' in results: print(f\\"Median: {results[\'median\']}\\") if \'std\' in results: print(f\\"Standard Deviation: {results[\'std\']:.4f}\\") if __name__ == \'__main__\': main()"},{"question":"# Question: Implement a Dynamic Control Flow Model using `torch.cond` Objective Your task is to create a neural network model that can dynamically alter its behavior based on the values of the input tensor. You will use `torch.cond` to achieve this. Problem Statement Implement a PyTorch class `DynamicBehaviorModel` that uses `torch.cond` to choose between two different behaviors based on a condition involving the input tensor. Specifically: 1. If the mean of the elements in the input tensor is greater than 0, your model should return the tensor after applying a specified linear transformation. 2. Otherwise, your model should return the tensor after applying a specified non-linear transformation (e.g., ReLU activation). Detailed Requirements - Implement a class `DynamicBehaviorModel(torch.nn.Module)` with the following: - An initializer (`__init__`) that accepts two components: a linear transformation (`torch.nn.Linear`) and a non-linear transformation (`torch.nn.ReLU`). - A `forward` method that takes a tensor `x` as input. This method should use `torch.cond` to apply and return the result of the linear transformation if the mean of `x` is greater than 0, or the non-linear transformation otherwise. Input - An input tensor `x` of shape `(N, M)` where `N` is the batch size, and `M` is the feature size. Output - A tensor of the same shape as the input tensor `x`, transformed according to the specified condition. Example ```python import torch # Define the model components linear_transform = torch.nn.Linear(5, 5) non_linear_transform = torch.nn.ReLU() # Instantiate the model model = DynamicBehaviorModel(linear_transform, non_linear_transform) # Create input tensors input_tensor_1 = torch.randn(3, 5) # Random tensor with shape (3, 5) input_tensor_2 = torch.full((3, 5), -1.0) # Tensor with constant negative values # Apply the model to the input tensors output_1 = model(input_tensor_1) output_2 = model(input_tensor_2) print(\\"Output when mean > 0:\\", output_1) print(\\"Output when mean <= 0:\\", output_2) ``` Constraints - The implementation must use `torch.cond`. - Assume the input tensors are always well-formed with valid numerical values. Notes - Remember to leverage PyTorch\'s tensor operations and ensure that the transformations you define are properly applied within the `torch.cond` framework. Evaluation Your implementation will be evaluated based on: - Correctness of the dynamic behavior control. - Proper use of `torch.cond`. - Code readability and structure.","solution":"import torch import torch.nn.functional as F from torch.nn import Module class DynamicBehaviorModel(Module): def __init__(self, linear_transform, non_linear_transform): super(DynamicBehaviorModel, self).__init__() self.linear_transform = linear_transform self.non_linear_transform = non_linear_transform def forward(self, x): mean_x = torch.mean(x) if mean_x > 0: return self.linear_transform(x) else: return self.non_linear_transform(x)"},{"question":"**Question:** You are tasked with analyzing and visualizing the distribution of body mass and flipper length of penguins from different species. Using the seaborn library and the provided `penguins` dataset, create a plot that clearly demonstrates the following requirements. **Requirements:** 1. Load the `penguins` dataset using `load_dataset`. 2. Create a scatter plot comparing the body mass (`body_mass_g`) and flipper length (`flipper_length_mm`) of penguins. 3. Use dots to represent individual data points and add jitter to avoid overlapping. 4. Overlay range marks to show the interquartile range (IQR: 25th to 75th percentiles) for each species. 5. Ensure that the plot clearly differentiates between the three penguin species. **Constraints:** - You must use the seaborn `objects` interface (`seaborn.objects.Plot`). - Ensure that the plot is well-labeled, with appropriate axis labels and legend. **Expected Input and Output Format:** ```python # No input parameters # Expected function skeleton def visualize_penguin_distribution(): # Your code here # When the function is executed, it should display the described plot. ``` **Performance Requirements:** - The plot must be generated efficiently without unnecessary computations. **Example Output:** The function should generate and display a plot similar to the following: - A scatter plot comparing `body_mass_g` and `flipper_length_mm`. - Each dot represents a penguin, with jitter applied. - Range marks (IQR) for each species. - Appropriate labeling for clarity. ```python import seaborn.objects as so from seaborn import load_dataset def visualize_penguin_distribution(): penguins = load_dataset(\\"penguins\\") ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\", color=\\"species\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=.2)) .label(x=\'Body Mass (g)\', y=\'Flipper Length (mm)\', color=\'Species\') ).show() ``` **Notes:** - Ensure that you handle any missing data appropriately. - Add comments to explain your code where necessary.","solution":"import seaborn.objects as so from seaborn import load_dataset def visualize_penguin_distribution(): Creates a scatter plot comparing the body mass and flipper length of penguins from different species. Uses dots to represent individual data points with jitter to avoid overlapping, and overlays range marks to show the IQR for each species. # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Ensure dataset does not contain any missing values for the columns used in the plot penguins = penguins.dropna(subset=[\'body_mass_g\', \'flipper_length_mm\', \'species\']) # Create the plot ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\", color=\\"species\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=.2)) .label(x=\\"Body Mass (g)\\", y=\\"Flipper Length (mm)\\", color=\\"Species\\") ).show()"},{"question":"You are given a dataset named `penguins` with the following columns: `species`, `island`, `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, `body_mass_g`, and `sex`. Your task is to create a bar plot using seaborn\'s `objects` interface, adhering to the requirements below: Requirements: 1. **Load the dataset** using seaborn\'s `load_dataset` method. 2. **Aggregate the data**: - Plot the mean `body_mass_g` for each `species` aggregated by the `sex` of the penguins. 3. **Customize the plot**: - Use different colors for each `species`. - Utilize seaborne\'s `Dodge` for clear visibility of the `sex` groups within each species. - Ensure the plot title is \\"Penguin Body Mass by Species and Sex\\" and label the x-axis as \\"Species\\". - Label the y-axis as \\"Mean Body Mass (g)\\". Input and Output - **Input**: No specific input as the dataset is loaded within the code. - **Output**: A rendered bar plot that meets the described criteria. Constraints - Ensure you use the seaborn `objects` interface for plot creation and manipulation. - Handle any missing values in the dataset by omitting them from the plot. ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_bar_plot(): # Load dataset penguins = load_dataset(\\"penguins\\") # Handle missing values by omitting them penguins = penguins.dropna() # Create the plot p = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"species\\") .add(so.Bar(), so.Agg(func=\\"mean\\"), so.Dodge(), facet=\\"sex\\") ) # Customize the plot p.set(title=\\"Penguin Body Mass by Species and Sex\\", xlabel=\\"Species\\", ylabel=\\"Mean Body Mass (g)\\") p.show() # Show the plot plt.show() # Call the function to execute the plot creation create_bar_plot() ```","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_bar_plot(): # Load dataset penguins = load_dataset(\\"penguins\\") # Handle missing values by omitting them penguins = penguins.dropna() # Create the plot p = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"species\\") .add(so.Bar(), so.Agg(func=\\"mean\\"), so.Dodge(), split=\\"sex\\") ) # Customize the plot p.label(title=\\"Penguin Body Mass by Species and Sex\\", xlabel=\\"Species\\", ylabel=\\"Mean Body Mass (g)\\") p.show() # Show the plot plt.show() # Call the function to execute the plot creation create_bar_plot()"},{"question":"# Coding Assessment: PyTorch Named Tensors Objective Implement a series of functions to demonstrate the manipulation and utilization of named tensors in PyTorch. The tasks will assess your ability to handle named dimensions, perform operations while preserving the correctness of names, and align tensors by their named dimensions. Problem Statement You are given the following tasks to perform using PyTorch named tensors: 1. **Create a Named Tensor**: Implement a function `create_named_tensor` that creates a 3-dimensional tensor with specified sizes and names for each dimension. ```python def create_named_tensor(sizes: tuple, names: tuple) -> torch.Tensor: Create a 3D tensor with given sizes and dimension names. Parameters: sizes (tuple): A tuple of three integers specifying the size of each dimension. names (tuple): A tuple of three strings specifying the names of each dimension. Returns: torch.Tensor: A 3D tensor with specified sizes and names. pass ``` 2. **Add Two Named Tensors**: Implement a function `add_named_tensors` that takes two named tensors, ensures they are aligned by names, and returns their sum. ```python def add_named_tensors(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: Add two named tensors after aligning their dimensions by names. Parameters: tensor1 (torch.Tensor): The first named tensor. tensor2 (torch.Tensor): The second named tensor. Returns: torch.Tensor: The sum of the two tensors with aligned names. pass ``` 3. **Matrix Multiplication of Named Tensors**: Implement a function `matmul_named_tensors` that performs matrix multiplication on two named tensors and returns the result, ensuring the appropriate dimension names are applied. ```python def matmul_named_tensors(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: Perform matrix multiplication of two named tensors. Parameters: tensor1 (torch.Tensor): The first named tensor. tensor2 (torch.Tensor): The second named tensor. Returns: torch.Tensor: The result of the matrix multiplication with appropriate dimension names. pass ``` 4. **Perform Reduction Operation**: Implement a function `reduce_named_tensor` that reduces a named tensor over a specified dimension and returns the result with the correct names. ```python def reduce_named_tensor(tensor: torch.Tensor, dim: str, keepdim: bool = False) -> torch.Tensor: Reduce a named tensor over a specified dimension. Parameters: tensor (torch.Tensor): The named tensor to be reduced. dim (str): The name of the dimension to reduce. keepdim (bool): Whether to retain the reduced dimension in the output tensor. Returns: torch.Tensor: The reduced tensor with appropriate dimension names. pass ``` Constraints - Assume PyTorch >= 1.3.0. - Function names and signatures must be exactly as specified. - Ensure that the functions handle tensors with named dimensions correctly, raising appropriate errors for invalid operations. - You are not allowed to modify the function signatures. - Avoid using unnamed tensors for these operations. Example Usage ```python sizes = (2, 3, 4) names = (\'batch\', \'height\', \'width\') # Create named tensors tensor1 = create_named_tensor(sizes, names) tensor2 = create_named_tensor(sizes, names) # Add named tensors added_tensor = add_named_tensors(tensor1, tensor2) # Matrix multiplication (Assume appropriate sizes for matmul) matmul_result = matmul_named_tensors(tensor1, tensor2.transpose(-2, -1)) # Reduce named tensor reduced_tensor = reduce_named_tensor(tensor1, dim=\'height\') ``` You have now defined tasks that will assess the student\'s understanding of creating, manipulating, and performing operations on named tensors in PyTorch.","solution":"import torch def create_named_tensor(sizes: tuple, names: tuple) -> torch.Tensor: Create a 3D tensor with given sizes and dimension names. Parameters: sizes (tuple): A tuple of three integers specifying the size of each dimension. names (tuple): A tuple of three strings specifying the names of each dimension. Returns: torch.Tensor: A 3D tensor with specified sizes and names. assert len(sizes) == 3, \\"Sizes must be a tuple of three integers.\\" assert len(names) == 3, \\"Names must be a tuple of three strings.\\" tensor = torch.randn(sizes) tensor = tensor.refine_names(*names) return tensor def add_named_tensors(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: Add two named tensors after aligning their dimensions by names. Parameters: tensor1 (torch.Tensor): The first named tensor. tensor2 (torch.Tensor): The second named tensor. Returns: torch.Tensor: The sum of the two tensors with aligned names. return tensor1.align_as(tensor2) + tensor2 def matmul_named_tensors(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: Perform matrix multiplication of two named tensors. Parameters: tensor1 (torch.Tensor): The first named tensor. tensor2 (torch.Tensor): The second named tensor. Returns: torch.Tensor: The result of the matrix multiplication with appropriate dimension names. return torch.matmul(tensor1, tensor2) def reduce_named_tensor(tensor: torch.Tensor, dim: str, keepdim: bool = False) -> torch.Tensor: Reduce a named tensor over a specified dimension. Parameters: tensor (torch.Tensor): The named tensor to be reduced. dim (str): The name of the dimension to reduce. keepdim (bool): Whether to retain the reduced dimension in the output tensor. Returns: torch.Tensor: The reduced tensor with appropriate dimension names. return tensor.sum(dim=dim, keepdim=keepdim)"},{"question":"Given the documentation on Permutation Feature Importance in `scikit-learn`, write a Python function that calculates and displays the permutation feature importance for a given dataset and model using the `scikit-learn` library. The function should: 1. Split the dataset into training and validation sets. 2. Train the provided model on the training set. 3. Compute and display the permutation feature importance of the model trained on the provided dataset. 4. Use multiple scoring metrics to evaluate the importance. # Function Signature ```python def calculate_permutation_importance(X, y, model, metrics, test_size=0.25, random_state=0): Calculate and display the permutation feature importance for a given dataset and model. Parameters: - X: pd.DataFrame or np.ndarray, feature matrix. - y: pd.Series or np.ndarray, target vector. - model: scikit-learn compatible estimator, already instantiated. - metrics: List of scoring metrics to use for permutation importance. Example: [\'r2\', \'neg_mean_absolute_percentage_error\', \'neg_mean_squared_error\'] - test_size: float, proportion of the dataset to include in the validation split (default is 0.25). - random_state: int, random seed for reproducibility (default is 0). Output: - Prints the permutation importance for each feature according to the specified metrics. pass ``` # Constraints - The input `X` can be either a pandas DataFrame or a numpy array. - The input `y` can be either a pandas Series or a numpy array. - The function should handle standard scaling of features before training the model. # Example Usage ```python from sklearn.datasets import load_diabetes from sklearn.linear_model import Ridge import pandas as pd # Load dataset diabetes = load_diabetes() X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names) y = pd.Series(diabetes.target) # Instantiate model model = Ridge(alpha=1e-2) # Define metrics metrics = [\'r2\', \'neg_mean_absolute_percentage_error\', \'neg_mean_squared_error\'] # Calculate permutation importance calculate_permutation_importance(X, y, model, metrics, test_size=0.3, random_state=42) ``` # Expected Output The function should print the permutation importance of each feature for each metric specified, much like the example provided in the documentation.","solution":"import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.inspection import permutation_importance import matplotlib.pyplot as plt def calculate_permutation_importance(X, y, model, metrics, test_size=0.25, random_state=0): Calculate and display the permutation feature importance for a given dataset and model. Parameters: - X: pd.DataFrame or np.ndarray, feature matrix. - y: pd.Series or np.ndarray, target vector. - model: scikit-learn compatible estimator, already instantiated. - metrics: List of scoring metrics to use for permutation importance. Example: [\'r2\', \'neg_mean_absolute_percentage_error\', \'neg_mean_squared_error\'] - test_size: float, proportion of the dataset to include in the validation split (default is 0.25). - random_state: int, random seed for reproducibility (default is 0). Output: - Prints the permutation importance for each feature according to the specified metrics. # Split the dataset X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=random_state) # Standard scaling scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_val = scaler.transform(X_val) # Train the model model.fit(X_train, y_train) # Calculate permutation importance for each metric for metric in metrics: print(f\\"Permutation Importance for metric: {metric}\\") result = permutation_importance(model, X_val, y_val, scoring=metric, n_repeats=10, random_state=random_state) # Summarizing results importance = result.importances_mean std = result.importances_std indices = np.argsort(importance)[::-1] for i in indices: print(f\\"{X.columns[i]}: {importance[i]:.4f} (+/- {std[i]:.4f})\\") # Plotting plt.figure() plt.bar(range(X.shape[1]), importance[indices], color=\\"r\\", yerr=std[indices]) plt.title(f\\"Feature importances ({metric})\\") plt.xticks(range(X.shape[1]), [X.columns[i] for i in indices], rotation=90) plt.xlim([-1, X.shape[1]]) plt.show()"},{"question":"Objective Write a function that performs a specific task and create comprehensive unit tests to ensure the function\'s correctness. Use the `unittest` framework as well as utilities from the `test.support` module where applicable. Problem Statement Implement the function `reverse_and_uppercase_list(strings: List[str]) -> List[str]` which takes a list of strings and returns a new list where each string is reversed and converted to uppercase. **Function Signature:** ```python def reverse_and_uppercase_list(strings: List[str]) -> List[str]: ``` **Input:** - `strings` (List[str]): A list of strings with length `n` (0 ≤ n ≤ 1000). Each string in the list has a length of at most 100 characters. **Output:** - List[str]: A list of strings where each string is reversed and converted to uppercase. **Constraints:** - The input list can contain empty strings, which should remain empty in the output list. - The function should handle both uppercase and lowercase letters. **Examples:** ```python reverse_and_uppercase_list([\'hello\', \'world\']) # returns [\'OLLEH\', \'DLROW\'] reverse_and_uppercase_list([\'Python\', \'unittest\']) # returns [\'NOHTYP\', \'TSETTINU\'] reverse_and_uppercase_list([\'\', \'A\', \'b\']) # returns [\'\', \'A\', \'B\'] ``` Testing Guidelines Design a set of unit tests to validate the correctness of the `reverse_and_uppercase_list` function. Your tests should: - Cover typical cases - Cover edge cases such as empty list, empty strings, and strings at the length bounds - Use utilities from the `test.support` module to manage verbose output and other test-related settings **Testing Example:** Include the following methods in your test class: - `test_example_cases` - `test_empty_list` - `test_single_character_strings` - `test_mixed_case_strings` - `test_long_strings` **Boilerplate Code:** ```python import unittest from test import support from typing import List def reverse_and_uppercase_list(strings: List[str]) -> List[str]: return [s[::-1].upper() for s in strings] class TestReverseAndUppercaseList(unittest.TestCase): def test_example_cases(self): # Test example cases self.assertEqual(reverse_and_uppercase_list([\'hello\', \'world\']), [\'OLLEH\', \'DLROW\']) self.assertEqual(reverse_and_uppercase_list([\'Python\', \'unittest\']), [\'NOHTYP\', \'TSETTINU\']) def test_empty_list(self): # Test empty list self.assertEqual(reverse_and_uppercase_list([]), []) def test_single_character_strings(self): # Test single character strings self.assertEqual(reverse_and_uppercase_list([\'A\', \'b\', \'%\', \'\']), [\'A\', \'B\', \'%\', \'\']) def test_mixed_case_strings(self): # Test strings with mixed case self.assertEqual(reverse_and_uppercase_list([\'aBCdE\']), [\'EDCBA\']) def test_long_strings(self): # Test long strings self.assertEqual(reverse_and_uppercase_list([\'a\'*100]), [\'A\'*100]) self.assertEqual(reverse_and_uppercase_list([\'abc\']*333 + [\'def\']), [\'CBA\']*333 + [\'FED\']) if __name__ == \'__main__\': unittest.main() ``` **Note:** In case of verbose output needed during the tests, consider using `support.verbose` to control the level of details output during the test execution.","solution":"from typing import List def reverse_and_uppercase_list(strings: List[str]) -> List[str]: Takes a list of strings and returns a new list where each string is reversed and converted to uppercase. Args: strings (List[str]): A list of strings Returns: List[str]: A list of strings where each string is reversed and converted to uppercase. return [s[::-1].upper() for s in strings]"},{"question":"Coding Assessment Question # Objective: Demonstrate your understanding of the seaborn library by creating customized plots according to specified requirements. # Task: Implement a function `custom_seaborn_plots` that takes in a DataFrame and generates three types of plots: a line plot, a bar plot, and a histogram. Customize each plot using seaborn\'s context and rc settings. # Function Signature: ```python def custom_seaborn_plots(df: \'pd.DataFrame\') -> None: pass ``` # Input: - `df` (DataFrame): A pandas DataFrame with columns `x`, `y`, and `category` containing numeric data for plotting. # Output: - This function does not return anything. It should display the plots directly. # Requirements: 1. **Line Plot:** - Set the context to `notebook`. - Scale the font size by 1.5 times. - Set the line width to 2.5. - Plot `x` vs `y`. 2. **Bar Plot:** - Set the context to `talk`. - Scale the font size by 1.2 times. - Set the edge color for the bars to `blue`. - Plot the mean of `y` for each unique value in `category`. 3. **Histogram:** - Set the context to `paper`. - Scale the font size by 1.0 times. - Set the bin count to 20. - Plot a histogram of `y`. # Notes: - You may use any sample DataFrame for testing purposes. - Ensure that the plots are shown one after another using `plt.show()` after each plot. - The function should import necessary libraries within the function scope. # Example Usage: ```python import pandas as pd # Sample DataFrame data = { \'x\': [1, 2, 3, 4, 5, 6], \'y\': [2, 3, 4, 5, 6, 7], \'category\': [\'A\', \'A\', \'B\', \'B\', \'C\', \'C\'] } df = pd.DataFrame(data) # Function call custom_seaborn_plots(df) ``` The output should display three plots, each customized according to the requirements above.","solution":"import pandas as pd import matplotlib.pyplot as plt import seaborn as sns def custom_seaborn_plots(df: \'pd.DataFrame\') -> None: # Line Plot sns.set_context(\\"notebook\\", font_scale=1.5) plt.figure(figsize=(10, 6)) sns.lineplot(data=df, x=\'x\', y=\'y\', linewidth=2.5) plt.title(\'Line Plot\') plt.show() # Bar Plot sns.set_context(\\"talk\\", font_scale=1.2) plt.figure(figsize=(10, 6)) sns.barplot(x=\'category\', y=\'y\', data=df, ci=None, edgecolor=\'blue\') plt.title(\'Bar Plot\') plt.show() # Histogram sns.set_context(\\"paper\\", font_scale=1.0) plt.figure(figsize=(10, 6)) sns.histplot(df[\'y\'], bins=20) plt.title(\'Histogram\') plt.show()"},{"question":"Objective Your task is to implement a Python function that writes lines to a file using both `PyFile_WriteObject` and `PyFile_WriteString`. Additionally, you will read these lines back from the file using `PyFile_GetLine` to ensure the correctness of the data written. Problem Statement Implement a class `LowLevelFileOps` with the following methods: 1. **`__init__(self, filename: str, mode: str)`**: Initializes an instance of the class. Opens the file specified by `filename` in the provided `mode`. 2. **`write_object(self, obj: any)`**: Writes the given `obj` to the file using `PyFile_WriteObject`. Assume the flag to be `Py_PRINT_RAW`. 3. **`write_string(self, string: str)`**: Writes the given `string` to the file using `PyFile_WriteString`. 4. **`read_line(self, max_length: int) -> str`**: Reads a line from the file of maximum `max_length` bytes using `PyFile_GetLine`. The expected behavior is to correctly write objects and strings to the file and read them back, ensuring the data integrity. **Constraints:** - Only calls to the mentioned low-level APIs are allowed. - Handle possible exceptions when interacting with file operations. - Assume that `filename` is a valid file path and `mode` is consistent with both writing and reading operations (e.g., \'w+\' or \'a+\'). **Example Usage:** ```python # Create an instance ops = LowLevelFileOps(\'sample.txt\', \'w+\') # Write a string to the file ops.write_string(\\"Hello, World!n\\") # Write an object to the file ops.write_object(12345) # Read lines back to verify print(ops.read_line(100)) # Output: Hello, World! print(ops.read_line(100)) # Output: 12345 ``` **Note:** Detailed behavior and exception handling should follow the standard practices to ensure file is properly managed and closed, even if an error occurs.","solution":"import os class LowLevelFileOps: def __init__(self, filename: str, mode: str): self.filename = filename self.mode = mode self.file = None try: self.file = open(filename, mode) except IOError as e: print(f\\"Error opening file: {e}\\") self.file = None def write_object(self, obj: any): try: if self.file: self.file.write(str(obj)) except IOError as e: print(f\\"Error writing object to file: {e}\\") def write_string(self, string: str): try: if self.file: self.file.write(string) except IOError as e: print(f\\"Error writing string to file: {e}\\") def read_line(self, max_length: int) -> str: try: if self.file: return self.file.readline(max_length) except IOError as e: print(f\\"Error reading line from file: {e}\\") return \\"\\" def close(self): if self.file: self.file.close()"},{"question":"**Objective:** Implement an async-based priority task scheduler using the `asyncio.PriorityQueue`. This scheduler should manage the execution of tasks based on their priorities and demonstrate the effective use of asyncio queues and coroutine management. **Task Description:** You are to implement a simplified task scheduler using `asyncio.PriorityQueue` which is capable of managing and executing tasks based on their provided priority. Lower numerical values will indicate higher priority. **Specifications:** 1. **Task Scheduler Class**: Create a class named `PriorityTaskScheduler` with the following methods: - `__init__(self)`: Initializes an empty `PriorityQueue`. - `add_task(self, priority: int, task_id: int)`: Adds a task to the queue with the given priority. - `start(self, worker_count: int)`: Starts the specified number of worker coroutines that process tasks from the queue. The workers should run indefinitely, processing one task at a time based on priority (lower number processed first). - `stop(self)`: Stops all workers after they have processed all tasks in the queue. 2. **Worker Coroutine**: Design a worker coroutine within the class that will: - Wait for tasks in the priority queue. - Print a statement showing the `task_id` and the `priority` for each processed task. 3. **Example Usage**: ```python import asyncio async def example_usage(): scheduler = PriorityTaskScheduler() # Adding tasks with various priorities scheduler.add_task(2, 101) scheduler.add_task(1, 102) scheduler.add_task(3, 103) scheduler.add_task(1, 104) # Start processing with 2 workers await scheduler.start(worker_count=2) # Stop the scheduler after processing all tasks await scheduler.stop() # Run the example asyncio.run(example_usage()) ``` **Expected Output**: The output should demonstrate that tasks are processed in priority order (lower numbers first) and concurrently by available workers. ``` Processing task 102 with priority 1 Processing task 104 with priority 1 Processing task 101 with priority 2 Processing task 103 with priority 3 ``` **Constraints**: - The workers should handle tasks concurrently. - The `PriorityTaskScheduler` should manage the lifecycle of the worker coroutines effectively. - Tasks in the queue should execute based on their priority (lowest priority number should be processed first). **Hints**: - Use `asyncio.PriorityQueue` to manage task priorities. - Use coroutine methods like `join` and `task_done` to manage task processing. - Use `asyncio.create_task` to start the workers.","solution":"import asyncio class PriorityTaskScheduler: def __init__(self): self.queue = asyncio.PriorityQueue() self.workers = [] self._running = False def add_task(self, priority: int, task_id: int): self.queue.put_nowait((priority, task_id)) async def worker(self, worker_id: int): print(f\\"Worker {worker_id} started.\\") while self._running or not self.queue.empty(): priority, task_id = await self.queue.get() print(f\\"Processing task {task_id} with priority {priority}\\") self.queue.task_done() async def start(self, worker_count: int): self._running = True self.workers = [asyncio.create_task(self.worker(i)) for i in range(worker_count)] async def stop(self): await self.queue.join() # Ensure all tasks have been processed self._running = False await asyncio.gather(*self.workers)"},{"question":"**Title: Book Inventory Management System** Objective: Create a small book inventory management system in Python that handles various functionalities. The system should read from and write to a JSON file, manage data using appropriate data structures, and make use of object-oriented design principles to handle the operations. Requirements: 1. **JSON File Format**: The system should read and write the inventory from a JSON file (`books.json`) which contains a list of books with fields: `title`, `author`, `year`, `isbn`. ```json [ {\\"title\\": \\"Book1\\", \\"author\\": \\"Author1\\", \\"year\\": 2000, \\"isbn\\": \\"1111111111\\"}, {\\"title\\": \\"Book2\\", \\"author\\": \\"Author2\\", \\"year\\": 2010, \\"isbn\\": \\"2222222222\\"}, ... ] ``` 2. **Class Definition**: - Define a `Book` class with attributes `title`, `author`, `year`, and `isbn`. - Define an `Inventory` class to manage the collection of books, with methods for adding, removing, updating, searching, and listing books. 3. **Functionality**: - `add_book(title: str, author: str, year: int, isbn: str) -> None`: Add a book to the inventory. - `remove_book(isbn: str) -> bool`: Remove a book with the given ISBN. Return `True` if the book was found and removed, `False` otherwise. - `update_book(isbn: str, title: str = None, author: str = None, year: int = None) -> bool`: Update the book\'s information based on the provided parameters. Return `True` if the book was found and updated, `False` otherwise. - `search_books(title: str = None, author: str = None, year: int = None) -> List[Book]`: Search for books by title, author, and/or year. Return a list of matching books. - `list_books() -> List[Book]`: List all books in the inventory. - `save_to_file(filename: str = \\"books.json\\") -> None`: Save the current inventory to a JSON file. - `load_from_file(filename: str = \\"books.json\\") -> None`: Load the inventory from a JSON file. Constraints: - The ISBN of the book should be unique in the inventory. - The system should handle cases where the JSON file does not exist by initializing an empty inventory. Example Usage: ```python inventory = Inventory() inventory.load_from_file() # Load existing inventory from \'books.json\' inventory.add_book(\\"New Book\\", \\"Author X\\", 2022, \\"3333333333\\") inventory.update_book(\\"3333333333\\", author=\\"Updated Author X\\") inventory.list_books() inventory.save_to_file() # Save current inventory to \'books.json\' ``` Expected Input and Output: - Input: Operations on the inventory using the specified methods. - Output: Results of operations such as adding, removing, updating, and searching for books. **Note**: Use proper error handling for file operations and ensure the data consistency in the JSON file.","solution":"import json from typing import List class Book: def __init__(self, title: str, author: str, year: int, isbn: str): self.title = title self.author = author self.year = year self.isbn = isbn def to_dict(self): return { \\"title\\": self.title, \\"author\\": self.author, \\"year\\": self.year, \\"isbn\\": self.isbn } @classmethod def from_dict(cls, data): return cls(data[\\"title\\"], data[\\"author\\"], data[\\"year\\"], data[\\"isbn\\"]) class Inventory: def __init__(self): self.books = [] def add_book(self, title: str, author: str, year: int, isbn: str) -> None: for book in self.books: if book.isbn == isbn: raise ValueError(\\"Book with this ISBN already exists\\") new_book = Book(title, author, year, isbn) self.books.append(new_book) def remove_book(self, isbn: str) -> bool: for i, book in enumerate(self.books): if book.isbn == isbn: del self.books[i] return True return False def update_book(self, isbn: str, title: str = None, author: str = None, year: int = None) -> bool: for book in self.books: if book.isbn == isbn: if title: book.title = title if author: book.author = author if year: book.year = year return True return False def search_books(self, title: str = None, author: str = None, year: int = None) -> List[Book]: results = [] for book in self.books: if ((title and title.lower() in book.title.lower()) or (author and author.lower() in book.author.lower()) or (year and year == book.year)): results.append(book) return results def list_books(self) -> List[Book]: return self.books def save_to_file(self, filename: str = \\"books.json\\") -> None: with open(filename, \'w\') as file: json_books = [book.to_dict() for book in self.books] json.dump(json_books, file, indent=4) def load_from_file(self, filename: str = \\"books.json\\") -> None: try: with open(filename, \'r\') as file: json_books = json.load(file) self.books = [Book.from_dict(book) for book in json_books] except FileNotFoundError: self.books = []"},{"question":"Problem Statement You are provided with a dataset containing information about various car models, including their horsepower, miles per gallon (mpg), weight, origin, and other attributes. Your task is to analyze this dataset using seaborn to create insightful visualizations. Specifically, you need to: 1. Load the dataset into a `pandas` DataFrame. 2. Create a series of visualizations using seaborn to analyze the relationships and distributions of the following: - The distribution of mpg across different origin categories with a figure-level function. - The relationship between horsepower and mpg, using hue to differentiate by origin. - Showing a pairwise relationships of numerical variables with facets divided by a categorical variable. 3. Customize the visualizations with appropriate axis labels, titles, and legends. Input - You will be provided with the dataset in CSV format named `cars.csv`. The dataset can be loaded using `pd.read_csv(\'cars.csv\')`. Output - Generate and display the visualizations as specified in the tasks. Requirements - Use the seaborn library to create the visualizations. - Ensure each visualization has clear axis labels and titles. - Use different types of seaborn functions where appropriate to showcase your understanding of the library\'s capabilities. Dataset Description The dataset contains the following columns: - `mpg`: Miles per gallon - `cylinders`: Number of cylinders - `displacement`: Engine displacement (cu. inches) - `horsepower`: Engine horsepower - `weight`: Vehicle weight (lbs) - `acceleration`: Time to accelerate from 0 to 60 mph (sec.) - `model_year`: Model year - `origin`: Origin of manufacture (1: USA, 2: Europe, 3: Japan) - `name`: Car name Sample Code ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the dataset cars = pd.read_csv(\'cars.csv\') # Set the aesthetic style of the plots sns.set_theme() # Task 1: Distribution of mpg across different origin categories sns.displot(data=cars, x=\'mpg\', hue=\'origin\', kind=\'hist\') plt.title(\'Distribution of MPG by Origin\') plt.xlabel(\'Miles per Gallon (MPG)\') plt.ylabel(\'Count\') plt.show() # Task 2: Relationship between horsepower and mpg with hue by origin sns.scatterplot(data=cars, x=\'horsepower\', y=\'mpg\', hue=\'origin\') plt.title(\'Horsepower vs. MPG by Origin\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Miles per Gallon (MPG)\') plt.show() # Task 3: Pairwise relationships of numerical variables with facets by origin sns.pairplot(data=cars, hue=\'origin\', diag_kind=\'kde\') plt.suptitle(\'Pairwise Relationships of Numerical Variables\', y=1.02) plt.show() ``` Implement the above code to understand the seaborn functionalities described and complete the tasks. Ensure the visualizations are clear and informative.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_dataset(file_path): Loads the dataset from a CSV file. Parameters: file_path (str): The path to the CSV file. Returns: DataFrame: The loaded dataset as a pandas DataFrame. return pd.read_csv(file_path) def plot_mpg_distribution(cars): Plots the distribution of MPG across different origin categories. Parameters: cars (DataFrame): The dataset containing car information. sns.displot(data=cars, x=\'mpg\', hue=\'origin\', kind=\'hist\') plt.title(\'Distribution of MPG by Origin\') plt.xlabel(\'Miles per Gallon (MPG)\') plt.ylabel(\'Count\') plt.show() def plot_horsepower_vs_mpg(cars): Plots the relationship between horsepower and MPG with hue by origin. Parameters: cars (DataFrame): The dataset containing car information. sns.scatterplot(data=cars, x=\'horsepower\', y=\'mpg\', hue=\'origin\') plt.title(\'Horsepower vs. MPG by Origin\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Miles per Gallon (MPG)\') plt.show() def plot_pairwise_relationships(cars): Plots pairwise relationships of numerical variables with facets by origin. Parameters: cars (DataFrame): The dataset containing car information. sns.pairplot(data=cars, hue=\'origin\', diag_kind=\'kde\') plt.suptitle(\'Pairwise Relationships of Numerical Variables\', y=1.02) plt.show()"},{"question":"# Command-Line Argument Parser for a File Management Tool Problem Statement You are required to create a command-line tool for basic file management using Python\'s `argparse` module. This tool, named `file_manager`, should accept different arguments to perform specific file operations. The operations to be supported are: 1. **List**: List all files in a specified directory. 2. **Move**: Move a file from one directory to another. 3. **Delete**: Delete a specified file. Requirements - Implement a function `create_parser()` which returns an `argparse.ArgumentParser` object. - The parser should support the following arguments and sub-commands: - A positional subcommand: `list`, `move`, or `delete`. For the `list` sub-command: - `directory`: A positional argument that specifies the directory to list files from. For the `move` sub-command: - `source`: A positional argument that specifies the source file path. - `destination`: A positional argument that specifies the destination directory. For the `delete` sub-command: - `filepath`: A positional argument that specifies the file to be deleted. - Implement functions `list_files(directory)`, `move_file(source, destination)`, and `delete_file(filepath)` to handle the respective operations. - Ensure appropriate error handling, such as checking if directories or files exist before performing operations. Example Usage ```bash # List files in a directory python file_manager.py list /path/to/directory # Move a file python file_manager.py move /path/to/source_file /path/to/destination_directory # Delete a file python file_manager.py delete /path/to/file ``` Input Format - Command-line arguments as described in the requirements. Output Format - Print the list of files for the `list` command. - Print success message for `move` and `delete` commands. - Print error messages for incorrect arguments or if any operations fail. Constraints - Ensure the code handles edge cases like non-existent files or directories. - Performance should be optimal for typical file system operations. Example Code Skeleton Here\'s a skeleton of the expected code structure to get you started: ```python import argparse import os import shutil def create_parser(): parser = argparse.ArgumentParser(description=\\"File management tool\\") subparsers = parser.add_subparsers(dest=\'command\') # Sub-command: list parser_list = subparsers.add_parser(\'list\', help=\'List files in a directory\') parser_list.add_argument(\'directory\', type=str, help=\'Directory path to list files from\') # Sub-command: move parser_move = subparsers.add_parser(\'move\', help=\'Move a file\') parser_move.add_argument(\'source\', type=str, help=\'Source file path\') parser_move.add_argument(\'destination\', type=str, help=\'Destination directory\') # Sub-command: delete parser_delete = subparsers.add_parser(\'delete\', help=\'Delete a file\') parser_delete.add_argument(\'filepath\', type=str, help=\'File path to delete\') return parser def list_files(directory): try: files = os.listdir(directory) print(\\"n\\".join(files)) except FileNotFoundError: print(f\\"Error: Directory \'{directory}\' does not exist.\\") def move_file(source, destination): try: shutil.move(source, destination) print(f\\"Moved \'{source}\' to \'{destination}\'\\") except FileNotFoundError: print(f\\"Error: Either source file \'{source}\' or destination directory \'{destination}\' does not exist.\\") except Exception as e: print(f\\"Error: {e}\\") def delete_file(filepath): try: os.remove(filepath) print(f\\"Deleted \'{filepath}\'\\") except FileNotFoundError: print(f\\"Error: File \'{filepath}\' does not exist.\\") except Exception as e: print(f\\"Error: {e}\\") if __name__ == \'__main__\': parser = create_parser() args = parser.parse_args() if args.command == \'list\': list_files(args.directory) elif args.command == \'move\': move_file(args.source, args.destination) elif args.command == \'delete\': delete_file(args.filepath) else: parser.print_help() ```","solution":"import argparse import os import shutil def create_parser(): parser = argparse.ArgumentParser(description=\\"File management tool\\") subparsers = parser.add_subparsers(dest=\'command\') # Sub-command: list parser_list = subparsers.add_parser(\'list\', help=\'List files in a directory\') parser_list.add_argument(\'directory\', type=str, help=\'Directory path to list files from\') # Sub-command: move parser_move = subparsers.add_parser(\'move\', help=\'Move a file\') parser_move.add_argument(\'source\', type=str, help=\'Source file path\') parser_move.add_argument(\'destination\', type=str, help=\'Destination directory\') # Sub-command: delete parser_delete = subparsers.add_parser(\'delete\', help=\'Delete a file\') parser_delete.add_argument(\'filepath\', type=str, help=\'File path to delete\') return parser def list_files(directory): try: files = os.listdir(directory) print(\\"n\\".join(files)) except FileNotFoundError: print(f\\"Error: Directory \'{directory}\' does not exist.\\") def move_file(source, destination): try: shutil.move(source, destination) print(f\\"Moved \'{source}\' to \'{destination}\'\\") except FileNotFoundError: print(f\\"Error: Either source file \'{source}\' or destination directory \'{destination}\' does not exist.\\") except Exception as e: print(f\\"Error: {e}\\") def delete_file(filepath): try: os.remove(filepath) print(f\\"Deleted \'{filepath}\'\\") except FileNotFoundError: print(f\\"Error: File \'{filepath}\' does not exist.\\") except Exception as e: print(f\\"Error: {e}\\") if __name__ == \'__main__\': parser = create_parser() args = parser.parse_args() if args.command == \'list\': list_files(args.directory) elif args.command == \'move\': move_file(args.source, args.destination) elif args.command == \'delete\': delete_file(args.filepath) else: parser.print_help()"},{"question":"**Objective:** Demonstrate your understanding of seaborn by creating a series of scatter plots using the `tips` dataset. The plots should include various seaborn functionalities for customization and insights extraction. **Instructions:** 1. Load the `tips` dataset using seaborn. 2. Create a scatter plot with the following specifications: - Plot `total_bill` on the x-axis and `tip` on the y-axis. - Use `day` to color the points (`hue` parameter). - Vary the markers (`style`) by `time` (Lunch/Dinner). - Set the size of each point based on the `size` column. 3. Customize the marker size range to be between 10 and 150. 4. Add an additional scatter plot (separate figure) showing the relationship between `total_bill` and `tip`, faceted by `time` (Lunch/Dinner) and using `smoker` as the `hue` parameter. 5. Ensure the legends of the plots are fully displayed. 6. You should include a title for each plot to describe what it represents. **Output:** Your function should produce two figures: 1. A scatter plot with customized marker size, color, and style (points colored by `day`, with different markers for `time`, and sizes based on `size`). 2. A faceted scatter plot showing `total_bill` vs `tip` for different `time` values, with `smoker` as the hue. **Code Implementation:** ```python import seaborn as sns import matplotlib.pyplot as plt def create_custom_scatter_plots(): # Load the \'tips\' dataset tips = sns.load_dataset(\\"tips\\") # Create the first scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot( data=tips, x=\'total_bill\', y=\'tip\', hue=\'day\', style=\'time\', size=\'size\', sizes=(10, 150) ) plt.title(\'Scatter Plot of Total Bill vs Tip with Hue by Day and Style by Time\') plt.legend(title=\'Legend\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # Create the faceted scatter plot using relplot g = sns.relplot( data=tips, x=\'total_bill\', y=\'tip\', col=\'time\', hue=\'smoker\', style=\'smoker\', kind=\'scatter\' ) g.fig.suptitle(\'Faceted Scatter Plot of Total Bill vs Tip Faceted by Time with Hue by Smoker\', y=1.02) plt.show() # Execute the function to display the plots create_custom_scatter_plots() ``` - **Expected Input and Output:** - There are no inputs to this function. - The function should produce and display two figures as described in the instructions. - **Constraints:** - Use the seaborn library for creating the plots. - Ensure that the function is self-contained and can be run independently to generate the plots. - **Performance Requirements:** - The function should efficiently handle the `tips` dataset, which is relatively small. As such, performance constraints are minimal.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_scatter_plots(): # Load the \'tips\' dataset tips = sns.load_dataset(\\"tips\\") # Create the first scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot( data=tips, x=\'total_bill\', y=\'tip\', hue=\'day\', style=\'time\', size=\'size\', sizes=(10, 150) ) plt.title(\'Scatter Plot of Total Bill vs Tip with Hue by Day and Style by Time\') plt.legend(title=\'Legend\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # Create the faceted scatter plot using relplot g = sns.relplot( data=tips, x=\'total_bill\', y=\'tip\', col=\'time\', hue=\'smoker\', style=\'smoker\', kind=\'scatter\' ) g.fig.suptitle(\'Faceted Scatter Plot of Total Bill vs Tip Faceted by Time with Hue by Smoker\', y=1.02) plt.show() # Execute the function to display the plots create_custom_scatter_plots()"},{"question":"Custom Timer Utility You are required to implement a custom timer utility in Python utilizing the `time` module functionalities. The utility should provide the following features: 1. **Convert String to Epoch Time**: A function `convert_str_to_epoch(time_str: str, format: str = \\"%Y-%m-%d %H:%M:%S\\") -> int` that takes a time string and its format and converts it to the number of seconds since the epoch. 2. **Format Current Time**: A function `format_current_time(format: str = \\"%Y-%m-%d %H:%M:%S\\") -> str` that returns the current local time formatted as per the given format. 3. **Measure Execution Time**: A function `measure_execution_time(function: Callable, *args, **kwargs) -> float` that takes a function and its arguments, measures the time taken by the function to execute using the highest resolution performance counter, and returns the elapsed time in seconds. 4. **Get Time Zone Info**: A function `get_time_zone_info() -> Dict[str, Any]` that returns a dictionary containing the following timezone information: - `timezone`: Non-DST offset in seconds west of UTC. - `altzone`: DST timezone offset in seconds west of UTC. - `daylight`: Daylight Saving Time indicator. - `tzname`: Tuple of non-DST and DST timezone names. # Constraints - Ensure you handle any ValueErrors raised by failed conversions or formatting. - Use appropriate functions from the `time` module to achieve each feature. # Example Usage ```python # Example 1: Convert String to Epoch Time epoch_time = convert_str_to_epoch(\\"2023-10-05 14:30:00\\") print(epoch_time) # Output: Expected seconds since the epoch. # Example 2: Format Current Time current_time_str = format_current_time() print(current_time_str) # Output: Current local time in default format. # Example 3: Measure Execution Time def sample_function(): for _ in range(1000000): pass execution_time = measure_execution_time(sample_function) print(execution_time) # Output: Time taken by sample_function to execute. # Example 4: Get Time Zone Info tz_info = get_time_zone_info() print(tz_info) # Output: Dictionary with timezone information. ``` # Notes: - While completing this task, make sure to handle time precision correctly and avoid unnecessary loss of precision. - You should verify the successful handling of both timezone and non-timezone specific time representations. Implement the utility by defining the specified functions within a Python module.","solution":"import time from typing import Callable, Dict, Any def convert_str_to_epoch(time_str: str, format: str = \\"%Y-%m-%d %H:%M:%S\\") -> int: Converts a time string to epoch time. try: struct_time = time.strptime(time_str, format) return int(time.mktime(struct_time)) except ValueError as e: raise ValueError(f\\"Invalid time string or format: {e}\\") def format_current_time(format: str = \\"%Y-%m-%d %H:%M:%S\\") -> str: Returns the current local time formatted based on the given format. return time.strftime(format, time.localtime()) def measure_execution_time(function: Callable, *args, **kwargs) -> float: Measures the execution time of a given function. start_time = time.perf_counter() function(*args, **kwargs) end_time = time.perf_counter() return end_time - start_time def get_time_zone_info() -> Dict[str, Any]: Returns timezone information in a dictionary. return { \'timezone\': time.timezone, \'altzone\': time.altzone, \'daylight\': time.daylight, \'tzname\': time.tzname }"},{"question":"**Advanced Python Task: Using the `tempfile` Module** **Objective:** Demonstrate your understanding of the `tempfile` module by creating a temporary directory that stores multiple temporary files, writing and reading data from these files, and ensuring proper cleanup using context managers. **Task:** 1. Create a function `manage_temp_files` that: - Creates a temporary directory using `tempfile.TemporaryDirectory`. - Within this directory, creates three temporary files using `tempfile.NamedTemporaryFile`. Ensure that these files have the `.txt` suffix and are set to delete automatically upon closure. - Writes unique content to each of these files. - Reads back the content from each file and returns a dictionary mapping the file name to its content. 2. Write the unique content for each file as follows: - File 1: \\"Hello from file 1!\\" - File 2: \\"Hello from file 2!\\" - File 3: \\"Hello from file 3!\\" 3. Ensure that all files and the directory are properly cleaned up after their usage, even if an error occurs. **Input:** None **Output:** A dictionary where keys are the file names (including their paths) and values are the content read from each file. ```python import tempfile def manage_temp_files(): result = {} with tempfile.TemporaryDirectory() as tmpdir: with tempfile.NamedTemporaryFile(suffix=\\".txt\\", dir=tmpdir, delete=True) as file1, tempfile.NamedTemporaryFile(suffix=\\".txt\\", dir=tmpdir, delete=True) as file2, tempfile.NamedTemporaryFile(suffix=\\".txt\\", dir=tmpdir, delete=True) as file3: file1.write(b\\"Hello from file 1!\\") file2.write(b\\"Hello from file 2!\\") file3.write(b\\"Hello from file 3!\\") file1.seek(0) file2.seek(0) file3.seek(0) result[file1.name] = file1.read().decode(\'utf-8\') result[file2.name] = file2.read().decode(\'utf-8\') result[file3.name] = file3.read().decode(\'utf-8\') return result # Example usage if __name__ == \\"__main__\\": result = manage_temp_files() for filename, content in result.items(): print(f\\"{filename}: {content}\\") ``` Constraints: - Ensure the function handles exceptions gracefully and always performs cleanup. - Use context managers to manage resources effectively. This task assesses your ability to work with temporary file and directory management in Python, ensuring secure and efficient resource handling with automatic cleanup.","solution":"import tempfile def manage_temp_files(): result = {} with tempfile.TemporaryDirectory() as tmpdir: with tempfile.NamedTemporaryFile(suffix=\\".txt\\", dir=tmpdir, delete=True) as file1, tempfile.NamedTemporaryFile(suffix=\\".txt\\", dir=tmpdir, delete=True) as file2, tempfile.NamedTemporaryFile(suffix=\\".txt\\", dir=tmpdir, delete=True) as file3: file1.write(b\\"Hello from file 1!\\") file2.write(b\\"Hello from file 2!\\") file3.write(b\\"Hello from file 3!\\") file1.seek(0) file2.seek(0) file3.seek(0) result[file1.name] = file1.read().decode(\'utf-8\') result[file2.name] = file2.read().decode(\'utf-8\') result[file3.name] = file3.read().decode(\'utf-8\') return result"},{"question":"# Question: **Title:** Log File Analyzer **Objective:** You are required to write a Python function to analyze a log file containing various events and save a summary of these events in another file in JSON format. This will test your understanding of file operations, string formatting, and JSON handling. **Problem Statement:** You have a log file named `events.log`, where each line contains a record of an event with a timestamp and a message. Each line in the log file is formatted as follows: ``` [YYYY-MM-DD HH:MM:SS] Event message ``` Sample content: ``` [2023-10-01 12:00:00] User login [2023-10-01 12:05:00] File uploaded [2023-10-01 12:10:00] User logout [2023-10-01 12:15:00] User login [2023-10-01 12:20:00] User logout ``` Your task is to: 1. **Read the log file.** 2. **Extract and summarize the events.** Count the number of occurrences of each event. 3. **Write the summary to a new JSON file** named `summary.json` in the following format: ```json { \\"User login\\": 2, \\"File uploaded\\": 1, \\"User logout\\": 2 } ``` **Function Signature:** ```python def analyze_log(log_file: str, summary_file: str) -> None: ``` **Input:** - `log_file` : The name of the log file containing the events (`str`). - `summary_file`: The name of the output file where the summary will be written in JSON format (`str`). **Output:** - This function should not return anything. It is intended to save the summary to the `summary_file` in JSON format. **Constraints:** - You must use the `with` statement while handling files. - You should handle the case where the log file might not exist by printing a suitable error message. - Utilize the `json` module for creating the summary JSON file. **Performance Requirements:** - The solution should handle large log files efficiently without running into memory issues. **Example:** Suppose the content of `events.log` is as given above. Calling the function: ```python analyze_log(\'events.log\', \'summary.json\') ``` The content of `summary.json` should be: ```json { \\"User login\\": 2, \\"File uploaded\\": 1, \\"User logout\\": 2 } ``` **Note:** The log file specified in the example existence and content is guaranteed. For general cases, include appropriate error handling for reading files.","solution":"import json def analyze_log(log_file: str, summary_file: str) -> None: event_counts = {} try: with open(log_file, \'r\') as f: for line in f: # Extract the event message from each line event_message = line.split(\'] \')[1].strip() if event_message in event_counts: event_counts[event_message] += 1 else: event_counts[event_message] = 1 with open(summary_file, \'w\') as f: json.dump(event_counts, f, indent=4) except FileNotFoundError: print(f\\"The file {log_file} does not exist.\\")"},{"question":"# Question: Ensuring Reproducibility in PyTorch You are tasked with implementing a reproducible machine learning workflow in PyTorch. Your implementation should ensure that the results of your computations are identical across different runs given the same inputs. You need to perform the following tasks: 1. Set seeds for PyTorch, Python, and NumPy. 2. Configure PyTorch to use deterministic algorithms. 3. Ensure deterministic behavior for CUDA operations. 4. Create a DataLoader that maintains reproducibility. # Instructions: 1. **Set Seeds:** - Set the random seed for PyTorch, Python, and NumPy to `42`. 2. **Configure Deterministic Algorithms:** - Ensure that PyTorch uses deterministic algorithms where available, and raise an error for operations without a deterministic alternative. 3. **Manage CUDA Settings:** - Disable CUDA convolution benchmarking. - Enable deterministic CUDA convolutional algorithms and any other necessary settings for CUDA determinism. 4. **Implement a Reproducible DataLoader:** - Create a `DataLoader` for a dummy dataset with `batch_size=4` and `num_workers=2`. - Use `worker_init_fn` and `generator` to reseed workers for reproducibility. # Expected Output: - Your code should run without any errors. - When running the code multiple times, the output (e.g., printed batch data) should be the same for each run. # Code Template: ```python import torch import random import numpy as np from torch.utils.data import DataLoader, Dataset # Set seeds for PyTorch, Python, and NumPy def set_random_seeds(seed_value=42): torch.manual_seed(seed_value) random.seed(seed_value) np.random.seed(seed_value) if torch.cuda.is_available(): torch.cuda.manual_seed(seed_value) torch.cuda.manual_seed_all(seed_value) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False # Configure PyTorch to use deterministic algorithms torch.use_deterministic_algorithms(True) # Ensure deterministic behavior for CUDA operations # (This is already handled by torch.use_deterministic_algorithms and setting cuDNN settings above) # Dummy dataset for illustration class DummyDataset(Dataset): def __init__(self, length=100): self.data = torch.randn(length, 10) def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx] # Function to reseed DataLoader workers def seed_worker(worker_id): worker_seed = torch.initial_seed() % 2**32 np.random.seed(worker_seed) random.seed(worker_seed) # Seed for dataloader generator g = torch.Generator() g.manual_seed(42) # Implement the DataLoader def get_dataloader(batch_size=4, num_workers=2): dataset = DummyDataset() return DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, worker_init_fn=seed_worker, generator=g) def main(): set_random_seeds() dataloader = get_dataloader() for epoch in range(2): print(f\\"Epoch {epoch+1}:\\") for batch in dataloader: print(batch) if __name__ == \\"__main__\\": main() ``` # Constraints: - You must follow the provided template and ensure reproducibility as specified. - The code should be compatible with both CPU and GPU environments.","solution":"import torch import random import numpy as np from torch.utils.data import DataLoader, Dataset # Set seeds for PyTorch, Python, and NumPy def set_random_seeds(seed_value=42): Set the random seed for libraries (PyTorch, Python, and NumPy) to ensure reproducibility. torch.manual_seed(seed_value) random.seed(seed_value) np.random.seed(seed_value) if torch.cuda.is_available(): torch.cuda.manual_seed(seed_value) torch.cuda.manual_seed_all(seed_value) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False # Configure PyTorch to use deterministic algorithms torch.use_deterministic_algorithms(True) # Ensure deterministic behavior for CUDA operations # (This is already handled by torch.use_deterministic_algorithms and setting cuDNN settings above) # Dummy dataset for illustration class DummyDataset(Dataset): def __init__(self, length=100): self.data = torch.randn(length, 10) def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx] # Function to reseed DataLoader workers def seed_worker(worker_id): worker_seed = torch.initial_seed() % 2**32 np.random.seed(worker_seed) random.seed(worker_seed) # Seed for dataloader generator g = torch.Generator() g.manual_seed(42) # Implement the DataLoader def get_dataloader(batch_size=4, num_workers=2): dataset = DummyDataset() return DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, worker_init_fn=seed_worker, generator=g) def main(): set_random_seeds() dataloader = get_dataloader() for epoch in range(2): print(f\\"Epoch {epoch+1}:\\") for batch in dataloader: print(batch) if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Python Coding Assessment **Objective:** Demonstrate your understanding of the asyncio package in Python 3.10 by implementing an asynchronous task manager that orchestrates multiple tasks with various asyncio features. # Problem Statement You are tasked with creating an asynchronous task manager using the asyncio package. This task manager will: 1. **Create and manage multiple tasks** that perform various asynchronous operations, such as sleeping for a random number of seconds or making a network request. 2. **Handle concurrency** by scheduling tasks to run concurrently and await their completion. 3. **Use queues** to manage task execution order and priority. 4. **Implement timeouts** to limit the execution time of certain tasks. 5. **Provide synchronization** between tasks using asyncio primitives. 6. **Gracefully handle exceptions,** including custom handling for `asyncio.TimeoutError` and `asyncio.CancelledError`. # Requirements and Specifications 1. **Task Execution:** - Use `asyncio.create_task()` to start multiple asynchronous tasks. - Implement tasks that perform dummy operations like `await asyncio.sleep(random_time)` where `random_time` is a randomly generated duration. - Use `asyncio.gather()` to run tasks concurrently. 2. **Queue Management:** - Use `asyncio.Queue` to submit tasks to the manager. - Implement a mechanism that pulls tasks from the queue and executes them based on priority (use `asyncio.PriorityQueue`). 3. **Timeouts:** - Enforce a timeout on specific tasks using `asyncio.wait_for()`. - Handle `asyncio.TimeoutError` by logging an appropriate message and canceling the task. 4. **Synchronization:** - Use `asyncio.Lock` to demonstrate synchronization between tasks. - Ensure that tasks respect the lock while performing critical sections of code. 5. **Exception Handling:** - Implement custom exception handling for `asyncio.CancelledError` and ensure that resources are cleaned up if a task is canceled. # Function Specification You are to implement the following main function: ```python import asyncio import random async def async_task_manager(num_tasks: int): Creates and manages multiple asynchronous tasks. Parameters: - num_tasks (int): The number of tasks to create and manage. Returns: - None pass ``` # Input - `num_tasks` (int): The number of tasks to create and manage. # Expected Output There is no direct output from this function. Instead, you should ensure that tasks are executed according to the specifications provided and that appropriate debugging or logging information is printed during the execution of the tasks. # Constraints - You should use at least the following asyncio features: `create_task()`, `gather()`, `Queue`, `PriorityQueue`, `wait_for()`, `Lock`, and the exception handling of `TimeoutError` and `CancelledError`. - Each task should simulate a workload by sleeping for a random duration. - Ensure that all tasks are completed or canceled gracefully. # Performance Requirements - The function should manage concurrency efficiently. - Task execution should be scalable based on the number of tasks specified. Example You can test your function with: ```python if __name__ == \\"__main__\\": asyncio.run(async_task_manager(10)) ``` This will start the async task manager with 10 tasks. Ensure that debugging or logging information is sufficient to understand the flow of task execution, handling of timeouts, and synchronization. # Notes - You are free to create additional helper functions if needed. - Make sure to handle all necessary cleanup and provide informative logs for task execution, timeouts, and cancellations.","solution":"import asyncio import random import logging # Configure logging logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(__name__) async def dummy_task(task_id, lock): sleep_time = random.uniform(0.1, 2.0) try: async with lock: logger.debug(f\\"Task {task_id} acquired the lock.\\") await asyncio.sleep(sleep_time) logger.debug(f\\"Task {task_id} completed after sleeping for {sleep_time:.2f} seconds.\\") except asyncio.CancelledError: logger.debug(f\\"Task {task_id} was cancelled.\\") raise except asyncio.TimeoutError: logger.debug(f\\"Task {task_id} timed out.\\") raise async def worker(queue, lock): while True: priority, task_id = await queue.get() if task_id is None: break try: await asyncio.wait_for(dummy_task(task_id, lock), timeout=3) except asyncio.TimeoutError: logger.debug(f\\"Task {task_id} reached timeout.\\") except asyncio.CancelledError: logger.debug(f\\"Task {task_id} was cancelled within worker.\\") finally: queue.task_done() async def async_task_manager(num_tasks: int): queue = asyncio.PriorityQueue() lock = asyncio.Lock() # Create worker tasks workers = [asyncio.create_task(worker(queue, lock)) for _ in range(5)] # Enqueue tasks with random priority for task_id in range(num_tasks): priority = random.randint(0, 10) queue.put_nowait((priority, task_id)) # Wait until all tasks are processed await queue.join() # Cancel worker tasks for _ in range(5): queue.put_nowait((0, None)) await asyncio.gather(*workers) if __name__ == \\"__main__\\": asyncio.run(async_task_manager(10))"},{"question":"# Question: Implement a Semi-Supervised Learning Model using Label Propagation You are given a dataset containing both labeled and unlabeled data. Your task is to implement a semi-supervised learning model using scikit-learn\'s `LabelPropagation` class to classify the unlabeled data. The dataset consists of features and labels where some labels are marked as `-1` indicating they are unlabeled. Input - A numpy array `X` of shape (n_samples, n_features) representing the feature matrix. - A numpy array `y` of shape (n_samples,) representing the labels, where unlabeled data points are indicated with `-1`. Output - A numpy array of shape (n_samples,) where the unlabeled data points have been assigned their predicted labels. Constraints - You must use the `LabelPropagation` class from `sklearn.semi_supervised`. - The choice of kernel (either `rbf` or `knn`) and its respective parameters (`gamma` for `rbf` or `n_neighbors` for `knn`) should be defined clearly. - The model should be fit on the provided data and then used to predict the labels for all data points. # Implementation Requirements 1. Define and configure the `LabelPropagation` model. 2. Fit the model using the provided `X` and `y`. 3. Predict the labels for the entire dataset `X`. # Example ```python import numpy as np # Example data X = np.array([[1, 2], [2, 3], [3, 4], [8, 7], [9, 8], [0, 1]]) y = np.array([0, 0, -1, 1, -1, 0]) # Your implementation here def label_propagation(X, y, kernel=\'knn\', n_neighbors=3, gamma=0.1): from sklearn.semi_supervised import LabelPropagation # Select the correct model based on the kernel parameter model = LabelPropagation(kernel=kernel, n_neighbors=n_neighbors, gamma=gamma) # Fit the model model.fit(X, y) # Predict labels for the dataset y_pred = model.transduction_ return y_pred # Predict labels predicted_labels = label_propagation(X, y, kernel=\'knn\', n_neighbors=3) print(predicted_labels) # Example output: array([0, 0, 0, 1, 1, 0]) ``` Use this example to guide your implementation and ensure that your function adheres to the specified input and output formats.","solution":"def label_propagation(X, y, kernel=\'knn\', n_neighbors=3, gamma=0.1): from sklearn.semi_supervised import LabelPropagation # Create the LabelPropagation model with given kernel, n_neighbors, and gamma model = LabelPropagation(kernel=kernel, n_neighbors=n_neighbors, gamma=gamma) # Fit the model with the provided dataset model.fit(X, y) # Predict the labels for the dataset y_pred = model.transduction_ return y_pred"},{"question":"# PyTorch Multiprocessing and Logging Assessment Objective: Your task is to implement a PyTorch-based multiprocessing and logging solution. You need to start multiple worker processes, manage their contexts, and ensure that the logs are correctly handled and stored. Problem Statement: You are given a task that involves performing a simple computation (e.g., matrix multiplication) across multiple workers. Your job is to: 1. Start a given number of worker processes. 2. Each worker should perform the computation and log its progress. 3. Use appropriate context management to ensure all processes are correctly handled. 4. Gather and return the results of each computation. Requirements: 1. Implement a function `start_worker_processes(num_workers, matrix_a, matrix_b)` that: - Takes an integer `num_workers` indicating the number of worker processes to start. - Takes two matrices `matrix_a` and `matrix_b` for the computation. Assume these matrices are compatible for multiplication. - Each worker should perform the matrix multiplication: `result = matrix_a @ matrix_b`. - Each worker should log its PID and a message indicating the start and end of the computation. - Gathers the results from all workers and returns them as a list. 2. Implement appropriate context management using classes like `MultiprocessContext` or `SubprocessContext`. 3. Implement logging using `DefaultLogsSpecs` or `LogsSpecs` to capture the above-mentioned logs. Function Signature: ```python import torch import logging from torch.distributed.elastic.multiprocessing import start_processes, MultiprocessContext, LogsSpecs def start_worker_processes(num_workers: int, matrix_a: torch.Tensor, matrix_b: torch.Tensor) -> list: # Your implementation here pass ``` Constraints: - `num_workers` will be a positive integer, not exceeding 32. - `matrix_a` and `matrix_b` will be compatible for matrix multiplication. - Ensure that the solution is efficient and manages resources correctly. Example: ```python import torch num_workers = 4 matrix_a = torch.randn(5, 3) matrix_b = torch.randn(3, 4) result = start_worker_processes(num_workers, matrix_a, matrix_b) print(result) # Expected Output: A list containing results of the matrix multiplications from all worker processes. ``` Ensure thorough documentation within your code and proper handling of edge cases. This function should demonstrate a clear understanding of PyTorch\'s multiprocessing and logging capabilities.","solution":"import torch import logging import multiprocessing def worker_process(matrix_a, matrix_b, result_queue): Worker process that performs matrix multiplication and logs progress. pid = multiprocessing.current_process().pid logging.info(f\\"Worker {pid} started computation.\\") # Perform matrix multiplication result = matrix_a @ matrix_b logging.info(f\\"Worker {pid} completed computation.\\") # Put the result into the result queue result_queue.put(result.tolist()) def start_worker_processes(num_workers: int, matrix_a: torch.Tensor, matrix_b: torch.Tensor) -> list: Starts a given number of worker processes to perform matrix multiplication and logs their progress. :param num_workers: Number of worker processes to start :param matrix_a: First matrix for multiplication :param matrix_b: Second matrix for multiplication :return: List of results from all worker processes # Setup logging logging.basicConfig(level=logging.INFO, format=\'%(asctime)s - %(levelname)s - %(message)s\') # Create a Queue to collect results result_queue = multiprocessing.Queue() # List to store worker processes processes = [] # Start worker processes for _ in range(num_workers): p = multiprocessing.Process(target=worker_process, args=(matrix_a, matrix_b, result_queue)) p.start() processes.append(p) # Collect results results = [] for _ in range(num_workers): results.append(result_queue.get()) # Ensure all processes have finished for p in processes: p.join() return results"},{"question":"Advanced Regression Modeling with Scikit-learn Objective: Design and implement a regression model using Scikit-learn to predict housing prices based on given features such as square footage, number of bedrooms, and age of the house. Task Overview: 1. Load and preprocess the given dataset. 2. Implement and compare multiple regression models including `LinearRegression`, `Ridge`, `Lasso`, and `ElasticNet`. 3. Evaluate model performance using metrics like Mean Absolute Error (MAE) and R-squared (R²). 4. Tune the hyperparameters for the Ridge, Lasso, and ElasticNet models using cross-validation. Dataset: The dataset consists of housing data with the following columns: - **square_footage**: The square footage of the house. - **num_bedrooms**: The number of bedrooms in the house. - **age**: Age of the house in years. - **price**: The target variable representing the price of the house in dollars. Instructions: 1. Load the dataset into a pandas DataFrame. 2. Preprocess the data by handling any missing values and normalizing the features. 3. Split the data into training and test sets. 4. Implement the following regression models: - `LinearRegression` - `Ridge` - `Lasso` - `ElasticNet` 5. Train the models using the training set. 6. Predict and evaluate the performance of each model on the test set using MAE and R-squared metrics. 7. For `Ridge`, `Lasso`, and `ElasticNet`, use cross-validation to find the best hyperparameters. 8. Display the model performance metrics and the best hyperparameters. Expected Functions: 1. **load_data(file_path: str) -> pd.DataFrame** - Loads the dataset from the provided file path. 2. **preprocess_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]** - Handles missing values and normalizes the features. - Returns the preprocessed feature matrix `X` and target vector `y`. 3. **split_data(X: pd.DataFrame, y: pd.Series) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]** - Splits the data into training and test sets. 4. **train_model(model, X_train: np.ndarray, y_train: np.ndarray)** - Trains the given model using the training data. 5. **evaluate_model(model, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, float]** - Evaluates the model performance on the test set using MAE and R-squared metrics. 6. **cross_validate_model(model, param_grid: dict, X_train: np.ndarray, y_train: np.ndarray) -> dict** - Performs cross-validation to find the best hyperparameters for the model. 7. **main()** - Orchestrates the entire process from data loading to model evaluation and hyperparameter tuning. Constraints: - Use a random seed for data splitting to ensure reproducibility. - Use standardized / normalized data for training the models. - Utilize scikit-learn\'s built-in functions and methods where applicable. Performance Requirements: - Models are expected to have a reasonable MAE and R-squared performance on the test set. - Cross-validation should be performed using a relevant scoring metric. - Ensure efficient computation by selecting appropriate parameters for cross-validation. Example: ```python import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet from sklearn.metrics import mean_absolute_error, r2_score def load_data(file_path: str) -> pd.DataFrame: # Implement the function to load dataset return pd.read_csv(file_path) def preprocess_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]: # Implement the function to preprocess data return X, y def split_data(X: pd.DataFrame, y: pd.Series) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: # Implement the function to split data return X_train, X_test, y_train, y_test def train_model(model, X_train: np.ndarray, y_train: np.ndarray): # Implement the function to train the model model.fit(X_train, y_train) def evaluate_model(model, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, float]: # Implement the function to evaluate model y_pred = model.predict(X_test) return {\'MAE\': mean_absolute_error(y_test, y_pred), \'R2\': r2_score(y_test, y_pred)} def cross_validate_model(model, param_grid: dict, X_train: np.ndarray, y_train: np.ndarray) -> dict: # Implement the function for cross-validation grid_search = GridSearchCV(model, param_grid, scoring=\'neg_mean_absolute_error\') grid_search.fit(X_train, y_train) return grid_search.best_params_ def main(): # Orchestrates data loading, preprocessing, model training, and evaluation file_path = \'housing_data.csv\' df = load_data(file_path) X, y = preprocess_data(df) X_train, X_test, y_train, y_test = split_data(X, y) models = { \'LinearRegression\': LinearRegression(), \'Ridge\': Ridge(), \'Lasso\': Lasso(), \'ElasticNet\': ElasticNet() } param_grids = { \'Ridge\': {\'alpha\': [0.1, 1.0, 10.0]}, \'Lasso\': {\'alpha\': [0.1, 1.0, 10.0]}, \'ElasticNet\': {\'alpha\': [0.1, 1.0], \'l1_ratio\': [0.2, 0.5, 0.8]} } for name, model in models.items(): train_model(model, X_train, y_train) metrics = evaluate_model(model, X_test, y_test) print(f\\"{name} Performance: MAE = {metrics[\'MAE\']}, R2 = {metrics[\'R2\']}\\") if name != \'LinearRegression\': best_params = cross_validate_model(model, param_grids[name], X_train, y_train) print(f\\"Best parameters for {name}: {best_params}\\") if __name__ == \\"__main__\\": main() ``` Use the provided code structure and elaborated steps to complete the task.","solution":"import pandas as pd import numpy as np from typing import Tuple, Dict from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet from sklearn.preprocessing import StandardScaler from sklearn.metrics import mean_absolute_error, r2_score def load_data(file_path: str) -> pd.DataFrame: Loads the dataset from the provided file path. :param file_path: Path to the CSV file containing the dataset. :return: DataFrame with loaded data. return pd.read_csv(file_path) def preprocess_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]: Handles missing values and normalizes the features. :param df: DataFrame with raw dataset. :return: Tuple containing preprocessed feature matrix X and target vector y. df = df.dropna() X = df[[\'square_footage\', \'num_bedrooms\', \'age\']] y = df[\'price\'] scaler = StandardScaler() X_scaled = scaler.fit_transform(X) return pd.DataFrame(X_scaled, columns=X.columns), y def split_data(X: pd.DataFrame, y: pd.Series) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: Splits the data into training and test sets. :param X: Feature matrix. :param y: Target vector. :return: Tuple containing training and test sets. return train_test_split(X, y, test_size=0.2, random_state=42) def train_model(model, X_train: np.ndarray, y_train: np.ndarray): Trains the given model using the training data. :param model: Machine learning model to train. :param X_train: Training feature matrix. :param y_train: Training target vector. model.fit(X_train, y_train) def evaluate_model(model, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, float]: Evaluates the model performance on the test set using MAE and R-squared metrics. :param model: Trained machine learning model. :param X_test: Test feature matrix. :param y_test: Test target vector. :return: Dictionary with MAE and R-squared scores. y_pred = model.predict(X_test) return {\'MAE\': mean_absolute_error(y_test, y_pred), \'R2\': r2_score(y_test, y_pred)} def cross_validate_model(model, param_grid: dict, X_train: np.ndarray, y_train: np.ndarray) -> Dict[str, float]: Performs cross-validation to find the best hyperparameters for the model. :param model: Machine learning model for hyperparameter tuning. :param param_grid: Dictionary with parameters to search. :param X_train: Training feature matrix. :param y_train: Training target vector. :return: Dictionary with best parameters found. grid_search = GridSearchCV(model, param_grid, scoring=\'neg_mean_absolute_error\', cv=5) grid_search.fit(X_train, y_train) return grid_search.best_params_ def main(): file_path = \'housing_data.csv\' df = load_data(file_path) X, y = preprocess_data(df) X_train, X_test, y_train, y_test = split_data(X, y) models = { \'LinearRegression\': LinearRegression(), \'Ridge\': Ridge(), \'Lasso\': Lasso(), \'ElasticNet\': ElasticNet() } param_grids = { \'Ridge\': {\'alpha\': [0.1, 1.0, 10.0]}, \'Lasso\': {\'alpha\': [0.1, 1.0, 10.0]}, \'ElasticNet\': {\'alpha\': [0.1, 1.0], \'l1_ratio\': [0.2, 0.5, 0.8]} } for name, model in models.items(): train_model(model, X_train, y_train) metrics = evaluate_model(model, X_test, y_test) print(f\\"{name} Performance: MAE = {metrics[\'MAE\']}, R2 = {metrics[\'R2\']}\\") if name != \'LinearRegression\': best_params = cross_validate_model(model, param_grids[name], X_train, y_train) print(f\\"Best parameters for {name}: {best_params}\\") if __name__ == \\"__main__\\": main()"},{"question":"**Objective:** Write a Python program that captures and logs file operations using audit hooks. Your implementation should: 1. Define an audit hook that monitors when dangerous file operations are invoked. 2. Log the attempts of performing operations like creating, deleting, and renaming files. # Requirements - **Implement the `file_audit_hook(hook, args)` function** to be called when file-related audit events are triggered. - **Register the audit hook** using `sys.addaudithook`. - The hook should monitor and log the following events: - File creation (`os.mkdir`, `os.makedirs`, `open(path, \'w\')`) - File deletion (`os.remove`, `os.rmdir`, `os.removedirs`) - File renaming (`os.rename`) # Input and Output - **Input:** The program will create, delete, and rename files to demonstrate the functionality. - **Output:** It should print a log entry every time one of the above operations is attempted, indicating the type of operation and the file/directory involved. # Constraints - You must use the `sys.audit()` and `sys.addaudithook()` functions. - Ensure that the log entries are formatted clearly, specifying the audit event, operation type, and file path. # Example Here\'s an example of how the logging might look: ```plaintext [Audit Event] Event: os.mkdir, Operation: Create Directory, Path: /path/to/dir [Audit Event] Event: os.remove, Operation: Remove File, Path: /path/to/file [Audit Event] Event: os.rename, Operation: Rename File, Path: /path/to/old -> /path/to/new ``` # Boilerplate code ```python import sys import os def file_audit_hook(hook, args): # Implement the audit hook logic here pass # Register the audit hook sys.addaudithook(file_audit_hook) # Below code is to demonstrate file operations def main(): # Creating a directory os.mkdir(\'test_dir\') # Creating a file with open(\'test_file.txt\', \'w\') as f: f.write(\'Hello World\') # Renaming the file os.rename(\'test_file.txt\', \'renamed_file.txt\') # Removing the file os.remove(\'renamed_file.txt\') # Removing the directory os.rmdir(\'test_dir\') if __name__ == \'__main__\': main() ``` **Note:** Students are expected to complete the `file_audit_hook` function and ensure the system logs the actions as specified.","solution":"import sys import os def file_audit_hook(hook, args): event, details = hook, args # Define the recognized events and their custom names event_map = { \'os.mkdir\': \'Create Directory\', \'os.makedirs\': \'Create Directory\', \'open\': \'Open File\', \'os.remove\': \'Remove File\', \'os.rmdir\': \'Remove Directory\', \'os.removedirs\': \'Remove Directory\', \'os.rename\': \'Rename File\', } if event in event_map: operation = event_map[event] if operation == \'Rename File\': # The rename operation includes two file paths old_path, new_path = details[0], details[1] log_message = f\\"[Audit Event] Event: {event}, Operation: {operation}, Path: {old_path} -> {new_path}\\" else: # All other operations include a single file path path = details[0] log_message = f\\"[Audit Event] Event: {event}, Operation: {operation}, Path: {path}\\" print(log_message) # Register the audit hook sys.addaudithook(file_audit_hook) # Below code is to demonstrate file operations def main(): # Creating a directory os.mkdir(\'test_dir\') # Creating a file with open(\'test_file.txt\', \'w\') as f: f.write(\'Hello World\') # Renaming the file os.rename(\'test_file.txt\', \'renamed_file.txt\') # Removing the file os.remove(\'renamed_file.txt\') # Removing the directory os.rmdir(\'test_dir\') if __name__ == \'__main__\': main()"},{"question":"You are required to implement a class and demonstrate your understanding of shallow and deep copy operations using Python’s `copy` module. The class should handle a nested structure and demonstrate both types of copy operations. Requirements: 1. **Create a class `TreeNode` which represents a node in a binary tree.** - The class should have attributes: - `value`: to store the data of the node. - `left`: to store the left child node (which may point to another `TreeNode` object or be `None`). - `right`: to store the right child node (which may point to another `TreeNode` object or be `None`). 2. **Implement custom shallow and deep copy methods**: - Define the `__copy__()` method to create a shallow copy of the node. - Define the `__deepcopy__()` method to create a deep copy of the node using the `copy.deepcopy` function. 3. **Implement the custom string representation method (`__str__`) for easy visualization** of the binary tree nodes and their relationships: Example Usage: ```python import copy # Creating the initial tree nodes node1 = TreeNode(1) node2 = TreeNode(2) node3 = TreeNode(3) node4 = TreeNode(4) # Setting up relationships (links) node1.left = node2 node1.right = node3 node2.left = node4 # Shallow copy node1 shallow_copied_node1 = copy.copy(node1) # Deep copy node1 deep_copied_node1 = copy.deepcopy(node1) ``` Constraints: - Ensure no infinite loops occur if there are recursive references. - The string representation should reflect the node and its relationship to child nodes. # Expected Output: 1. Verify the shallow copy: - Changes in a child node of the original tree should appear in the shallow copy. - Changes in a child node of the shallow copy should appear in the original tree. 2. Verify the deep copy: - Changes in the original tree should not affect the deep copy. - Changes in the deep copy should not affect the original tree. Sample Implementation: ```python class TreeNode: def __init__(self, value): self.value = value self.left = None self.right = None def __copy__(self): new_node = TreeNode(self.value) new_node.left = self.left new_node.right = self.right return new_node def __deepcopy__(self, memo): new_node = TreeNode(self.value) memo[id(self)] = new_node if self.left: new_node.left = copy.deepcopy(self.left, memo) if self.right: new_node.right = copy.deepcopy(self.right, memo) return new_node def __str__(self): left = f\'Left: {self.left.value}\' if self.left else \'Left: None\' right = f\'Right: {self.right.value}\' if self.right else \'Right: None\' return f\'Node({self.value}), {left}, {right}\' ``` **Note**: Extend the `__str__` method to provide a more comprehensive tree structure representation if necessary. Demonstrate your understanding by writing tests to validate the shallow and deep copy operations.","solution":"import copy class TreeNode: def __init__(self, value): self.value = value self.left = None self.right = None def __copy__(self): new_node = TreeNode(self.value) new_node.left = self.left new_node.right = self.right return new_node def __deepcopy__(self, memo): new_node = TreeNode(self.value) memo[id(self)] = new_node if self.left: new_node.left = copy.deepcopy(self.left, memo) if self.right: new_node.right = copy.deepcopy(self.right, memo) return new_node def __str__(self): left = f\'Left: {self.left.value}\' if self.left else \'Left: None\' right = f\'Right: {self.right.value}\' if self.right else \'Right: None\' return f\'Node({self.value}), {left}, {right}\'"},{"question":"**Question: Implement a Custom Line Retrieval Function using Linecache** You are to implement a custom function called `retrieve_lines_from_file` that utilizes the `linecache` module to retrieve specific lines from a Python source file. # Function Signature ```python def retrieve_lines_from_file(filename: str, line_numbers: list[int]) -> list[str]: pass ``` # Input - `filename`: A string referring to the name of the file from which lines need to be retrieved. - `line_numbers`: A list of integers where each integer represents a line number to be retrieved from the file. # Output - The function returns a list of strings, where each string corresponds to the content of the specified line numbers from the file. If a line number is out of bounds or the file does not exist, that line should be represented as an empty string `\'\'`. # Constraints - You may assume that the file contains at most 5000 lines. - Line numbers will start from 1 to the number of lines in the file. - The size of `line_numbers` list will not exceed 1000. # Example ```python # Assume \'example.py\' contains the following lines: # 1. def function1(): # 2. pass # 3. # 4. def function2(): # 5. pass print(retrieve_lines_from_file(\'example.py\', [1, 3, 5])) # Output: [\'def function1():n\', \'\', \' passn\'] print(retrieve_lines_from_file(\'nonexistent.py\', [1, 2, 3])) # Output: [\'\', \'\', \'\'] ``` # Additional Requirements 1. Your solution should handle files that are relative or absolute paths. 2. Utilize the caching mechanism of `linecache` effectively. 3. Ensure that you clear the cache before and after using it to avoid any potential conflicts. # Notes - The function should make use of `linecache.getline` to retrieve lines. - Make sure to handle files that may not exist. **Good luck!**","solution":"import linecache def retrieve_lines_from_file(filename: str, line_numbers: list[int]) -> list[str]: Retrieves specified lines from a file using linecache. Args: filename (str): The name of the file to read from. line_numbers (list[int]): List of line numbers to retrieve. Returns: list[str]: List of strings corresponding to the specified line numbers. results = [] for line_number in line_numbers: line = linecache.getline(filename, line_number) results.append(line) linecache.clearcache() return results"},{"question":"Buffer Protocol Implementation in Python **Objective:** Implement a Python class that uses the given `buffer protocol` to manage and manipulate underlying memory buffers efficiently. You need to demonstrate your understanding of buffer management, including handling different buffer request types and ensuring contiguity. **Problem Statement:** You need to create a class `CustomBufferManager` that allows users to interact with different types of buffers (e.g., bytes, bytearray) using Python\'s memoryview objects. Implement the following methods to achieve this: 1. **`__init__(self, initial_bytes: bytes)`**: - Initializes the buffer manager with a given byte sequence. 2. **`get_buffer(self, request_type: str) -> memoryview`**: - Returns a memoryview based on the request type. The request type can be one of the following: - `\\"simple\\"`: Provides a simple writable buffer. - `\\"readonly\\"`: Provides a read-only buffer. - `\\"contiguous\\"`: Ensures the buffer is C-contiguous. 3. **`is_contiguous(self, view: memoryview, order: str) -> bool`**: - Checks if the given memoryview is contiguous as per the specified order (`\'C\'` for C-style, `\'F\'` for Fortran-style, and `\'A\'` for either). 4. **`fill_contiguous_strides(self, ndims: int, shape: tuple, order: str) -> list`**: - Fills and returns a list of strides for a contiguous array of the given shape and order (`\'C\'` or `\'F\'`). **Constraints:** - You should not use any external libraries other than Python\'s standard library. - The implementation should handle potential exceptions and edge cases gracefully. **Example Usage:** ```python # Initialize with a byte sequence manager = CustomBufferManager(b\\"hello world\\") # Get a simple buffer view = manager.get_buffer(\\"simple\\") print(view.tobytes()) # Output: b\'hello world\' view[0:5] = b\\"HELLO\\" print(view.tobytes()) # Output: b\'HELLO world\' # Get a read-only buffer and try modifying it (should raise an exception) readonly_view = manager.get_buffer(\\"readonly\\") print(readonly_view.tobytes()) # Output: b\'HELLO world\' try: readonly_view[0:5] = b\\"hello\\" except TypeError as e: print(e) # Output: Cannot modify read-only memory # Check contiguity print(manager.is_contiguous(view, \'C\')) # Output: True # Fill strides for a 2D C-style array of shape (2, 3) with item size 1 strides = manager.fill_contiguous_strides(2, (2, 3), \'C\') print(strides) # Output: [3, 1] ``` **Performance Requirements:** - Operations should be efficient in both time and space, considering the potentially large size of the underlying memory buffer. Implement the `CustomBufferManager` class according to the specifications above.","solution":"class CustomBufferManager: def __init__(self, initial_bytes: bytes): self.buffer = bytearray(initial_bytes) def get_buffer(self, request_type: str) -> memoryview: if request_type == \\"simple\\": return memoryview(self.buffer) elif request_type == \\"readonly\\": return memoryview(self.buffer).toreadonly() elif request_type == \\"contiguous\\": return memoryview(self.buffer) else: raise ValueError(\\"Invalid request type\\") def is_contiguous(self, view: memoryview, order: str) -> bool: if order == \'C\': return view.c_contiguous elif order == \'F\': return view.f_contiguous elif order == \'A\': return view.contiguous else: raise ValueError(\\"Invalid order type\\") def fill_contiguous_strides(self, ndims: int, shape: tuple, order: str) -> list: if order not in (\'C\', \'F\'): raise ValueError(\\"Order must be \'C\' or \'F\'\\") strides = [0] * ndims itemsize = 1 # Since we are dealing with bytes if order == \'C\': for i in range(ndims-1, -1, -1): if i == ndims-1: strides[i] = itemsize else: strides[i] = strides[i+1] * shape[i+1] elif order == \'F\': for i in range(ndims): if i == 0: strides[i] = itemsize else: strides[i] = strides[i-1] * shape[i-1] return strides"},{"question":"You are tasked with developing a Python function to simulate and analyze the statistical properties of a custom-defined random distribution. Specifically, you need to implement a function that generates random numbers following a custom distribution, calculates certain statistical measures, and returns the results. Function Signature `def custom_distribution_analysis(n: int, mean: float, stddev: float) -> dict:` Parameters - `n` (int): The number of random numbers to generate. Must be a positive integer. - `mean` (float): The mean (μ) of the normal distribution. - `stddev` (float): The standard deviation (σ) of the normal distribution. Must be a positive number. Returns - A dictionary with the following statistical properties of the generated numbers: - `\'mean\'`: The calculated mean. - `\'std_dev\'`: The calculated standard deviation. - `\'median\'`: The calculated median. - `\'mode\'`: The calculated mode. - `\'variance\'`: The calculated variance. - `\'skewness\'`: The calculated skewness. - `\'kurtosis\'`: The calculated kurtosis. Constraints - You must use the modules `random`, `statistics`, `math`, and `decimal` where appropriate. - Accuracy is paramount, so ensure results are precise to at least four decimal places. - Optimize for performance to handle `n` up to 10,000 efficiently. - If any statistical measure cannot be determined uniquely (e.g., mode for a uniform distribution), default to `None` for that measure. Example ```python result = custom_distribution_analysis(1000, 0, 1) print(result) # { # \'mean\': 0.0023, # \'std_dev\': 0.9876, # \'median\': 0.0012, # \'mode\': None, # \'variance\': 0.9755, # \'skewness\': 0.0154, # \'kurtosis\': 3.011 # } ``` # Guidelines - Utilize `random.normalvariate` from the `random` module to generate the numbers. - Use `Decimal` from the `decimal` module to ensure precision for mean and variance calculations. - Employ the `statistics` module for central tendency and dispersion measures. - Implement `math` functions when calculating skewness and kurtosis. # Hints - Skewness and kurtosis can be computed using the following formulas: - Skewness: ( frac{E[(X - mu)^3]}{sigma^3} ) - Kurtosis: ( frac{E[(X - mu)^4]}{sigma^4} ) Good luck, and make sure to carefully handle the statistical edge cases!","solution":"import random import statistics import math from decimal import Decimal, getcontext def custom_distribution_analysis(n: int, mean: float, stddev: float) -> dict: Generate n random numbers from a normal distribution with given mean and stddev, then calculate and return their statistical properties. if n <= 0 or stddev <= 0: raise ValueError(\\"n must be a positive integer and stddev must be a positive number.\\") getcontext().prec = 10 # Generate random numbers following a normal distribution random_numbers = [random.normalvariate(mean, stddev) for _ in range(n)] # Calculate statistical properties mean_value = float(Decimal(statistics.mean(random_numbers)).quantize(Decimal(\'0.0001\'))) std_dev_value = float(Decimal(statistics.stdev(random_numbers)).quantize(Decimal(\'0.0001\'))) median_value = float(Decimal(statistics.median(random_numbers)).quantize(Decimal(\'0.0001\'))) try: mode_value = statistics.mode(random_numbers) except statistics.StatisticsError: mode_value = None variance_value = float(Decimal(statistics.variance(random_numbers)).quantize(Decimal(\'0.0001\'))) # Calculate skewness mean_diff_cubed_sum = sum((x - mean_value) ** 3 for x in random_numbers) skewness = mean_diff_cubed_sum / (n * (std_dev_value ** 3)) skewness = float(Decimal(skewness).quantize(Decimal(\'0.0001\'))) # Calculate kurtosis mean_diff_fourth_sum = sum((x - mean_value) ** 4 for x in random_numbers) kurtosis = mean_diff_fourth_sum / (n * (std_dev_value ** 4)) kurtosis = float(Decimal(kurtosis).quantize(Decimal(\'0.0001\'))) return { \'mean\': mean_value, \'std_dev\': std_dev_value, \'median\': median_value, \'mode\': mode_value, \'variance\': variance_value, \'skewness\': skewness, \'kurtosis\': kurtosis }"},{"question":"# Objective Create a Python script using seaborn to visualize and analyze the relationship between various attributes in the `mpg` dataset. Your task involves creating customized residual plots and interpreting the results. # Instructions 1. **Data Preparation**: - Load the `mpg` dataset using seaborn. - Handle any missing values in the dataset if they exist. 2. **Visualizations**: - Generate a simple residual plot to examine the relationship between `weight` and `displacement`. - Create a residual plot for `horsepower` and `mpg` with `order=2`. - Add a residual plot for `horsepower` and `mpg` with `lowess=True` and a red LOWESS line. 3. **Customization**: - Customize the plots with appropriate labels, titles, and any other aesthetic improvements to enhance readability. 4. **Interpretation**: - Analyze the residual plots and provide a brief interpretation of the results. Are there any evident patterns or deviations from the assumptions of linear regression? # Expected Input and Output Formats - **Input**: - No external input required; the script will execute using the `mpg` dataset. - **Output**: - Display the plots directly in the output. - Print a brief interpretation (2-3 sentences) of the results based on the generated plots. # Constraints - Ensure the script handles any potential missing values in the dataset without causing errors. - The visualizations should be clear and easy to interpret. # Performance Requirements - The script should execute within a reasonable time frame (under 30 seconds) given standard computational resources. # Example ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset and handle missing values mpg = sns.load_dataset(\\"mpg\\") mpg = mpg.dropna() # Create the plots plt.figure(figsize=(15, 10)) # Residual plot for weight vs displacement plt.subplot(2, 2, 1) sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\'Residuals of Weight vs Displacement\') plt.xlabel(\'Weight\') plt.ylabel(\'Residuals\') # Residual plot for horsepower vs mpg with order=2 plt.subplot(2, 2, 2) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\'Residuals of Horsepower vs MPG (Order 2)\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals\') # Residual plot for horsepower vs mpg with lowess and red line plt.subplot(2, 2, 3) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\'Residuals of Horsepower vs MPG (LOWESS)\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals\') plt.tight_layout() plt.show() # Interpretation (example): # Based on the residual plots, we observe a non-linear relationship in the Horsepower vs MPG plot. # The trend revealed by the LOWESS line indicates a possible quadratic relationship, suggesting # that a simple linear model may not be appropriate for this data. ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_residual_plots(): # Load the dataset and handle missing values mpg = sns.load_dataset(\\"mpg\\") mpg = mpg.dropna() # Create the plots plt.figure(figsize=(15, 10)) # Residual plot for weight vs displacement plt.subplot(2, 2, 1) sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\'Residuals of Weight vs Displacement\') plt.xlabel(\'Weight\') plt.ylabel(\'Residuals\') # Residual plot for horsepower vs mpg with order=2 plt.subplot(2, 2, 2) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\'Residuals of Horsepower vs MPG (Order 2)\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals\') # Residual plot for horsepower vs mpg with lowess and red line plt.subplot(2, 2, 3) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\'Residuals of Horsepower vs MPG (LOWESS)\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals\') plt.tight_layout() plt.show() def interpret_results(): Interpretation of the residual plots. 1. Weight vs Displacement: The residuals appear randomly dispersed around the horizontal axis, indicating a generally good fit for a linear regression model. 2. Horsepower vs MPG (Order 2): The residuals suggest a quadratic relationship, as indicated by the parabolic shape. This may suggest the linear model is not adequate, and a quadratic model might be more appropriate. 3. Horsepower vs MPG (LOWESS): The red LOWESS line shows a clearer quadratic trend, reaffirming the quadratic relationship observed before. This implies non-linearity between horsepower and mpg. In conclusion, while the linear relationship holds well for weight and displacement, horsepower and mpg show a more complex (potentially quadratic) relationship. print(interpret_results.__doc__)"},{"question":"# Question: Construct and Record a Rendezvous Event in PyTorch Your task is to leverage the `torch.distributed.elastic.events` module to construct and record a custom rendezvous event with specific metadata. # Requirements: 1. **Function Name**: `record_rdzv_event` 2. **Input**: - `event_name` (str): The name of the rendezvous event. - `state` (str): The state of the event (e.g., \\"start\\", \\"end\\"). - `run_id` (str): The unique identifier for the run. - `metadata` (dict): A dictionary containing metadata for the event. 3. **Output**: None. 4. **Functionality**: - Utilize `torch.distributed.elastic.events.construct_and_record_rdzv_event` to build and record a rendezvous event with the provided inputs. - Ensure the metadata is appropriately handled and attached to the event. # Constraints: - Ensure that metadata keys and values are converted to strings. - Include error handling to catch and log any issues during the event recording. # Example Usage: ```python def record_rdzv_event(event_name: str, state: str, run_id: str, metadata: dict) -> None: # Your implementation here # Example metadata metadata = { \\"node_rank\\": 0, \\"num_nodes\\": 4, \\"epoch\\": 5 } # Recording a rendezvous event record_rdzv_event(\\"training_rendezvous\\", \\"start\\", \\"run_12345\\", metadata) ``` # Additional Information: - Refer to the PyTorch `torch.distributed.elastic.events` documentation to understand the available methods and classes. - Ensure your function is efficient and handles potential errors gracefully.","solution":"import torch.distributed.elastic.events as events import logging def record_rdzv_event(event_name: str, state: str, run_id: str, metadata: dict) -> None: Constructs and records a rendezvous event with the specified metadata. Args: - event_name (str): The name of the event. - state (str): The state of the event (e.g., \\"start\\", \\"end\\"). - run_id (str): The unique identifier for the run. - metadata (dict): A dictionary containing metadata for the event. Returns: - None try: # Convert metadata keys and values to strings metadata_str = {str(k): str(v) for k, v in metadata.items()} # Construct and record the rendezvous event events.construct_and_record_rdzv_event(event_name, state, run_id, **metadata_str) except Exception as e: logging.error(f\\"Failed to record rendezvous event: {str(e)}\\")"},{"question":"# Python 3.10 Coding Challenge Objective Your task is to implement a function that serializes a MIME email message to a specified format using the `email.generator` module. Task Implement a function `serialize_mime_message` that takes the following parameters: - `msg`: An `EmailMessage` object representing the email message to be serialized. - `output_format`: A string either \\"binary\\" or \\"text\\" specifying the type of serialization to perform. - `output_file`: A file path (string) where the serialized message should be written. Your function should: 1. Use `BytesGenerator` if `output_format` is \\"binary\\" and `Generator` if `output_format` is \\"text\\". 2. Serialize the `msg` object into the specified format and write it to `output_file`. 3. Handle any necessary encoding to ensure the message is standards-compliant. Input - `msg` (EmailMessage): The email message object to be serialized. - `output_format` (str): Serialization format, either \\"binary\\" or \\"text\\". - `output_file` (str): File path where the serialized message is to be written. Output - None. The serialized message is written to the specified file. Constraints - Assume the `msg` is always a valid `EmailMessage` object. - The `output_format` is always either \\"binary\\" or \\"text\\". Example Usage ```python from email.message import EmailMessage # Example EmailMessage object msg = EmailMessage() msg.set_content(\\"This is a test email message.\\") msg[\'Subject\'] = \'Test Email\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'receiver@example.com\' # Call function to serialize in binary format and store it in a file serialize_mime_message(msg, \\"binary\\", \\"email_binary.eml\\") # Call function to serialize in text format and store it in a file serialize_mime_message(msg, \\"text\\", \\"email_text.txt\\") ``` Remember to make sure that the serialized email message is standards-compliant and follows the MIME structure rules. For reference, you may explore the `email.generator` class documentation.","solution":"from email.generator import Generator, BytesGenerator def serialize_mime_message(msg, output_format, output_file): Serialize a MIME email message to a specified format and write to a file. Parameters: - msg: An EmailMessage object representing the email message to be serialized. - output_format: A string either \\"binary\\" or \\"text\\" specifying the type of serialization to perform. - output_file: A file path (string) where the serialized message should be written. Returns: - None. The serialized message is written to the specified file. with open(output_file, \'wb\' if output_format == \\"binary\\" else \'w\') as f: if output_format == \\"binary\\": BytesGenerator(f).flatten(msg) elif output_format == \\"text\\": Generator(f).flatten(msg)"},{"question":"# Asynchronous Filename Search with concurrent.futures You are tasked with implementing a function using `concurrent.futures` to improve the performance of searching for files with specific extensions within multiple directories. You should use `ProcessPoolExecutor` for this task to leverage multiple CPU cores effectively. Function Signature ```python def find_files_with_extension(directories: List[str], extension: str) -> Dict[str, List[str]]: pass ``` Input - `directories` (List[str]): A list of directory paths where the search should be performed. - `extension` (str): The file extension (e.g., \'.txt\', \'.py\') to search for. Output - Returns a dictionary where each key is a directory path from the input list, and the value is a list of file paths within that directory that match the given extension. Example ```python directories = [\\"/path/to/dir1\\", \\"/path/to/dir2\\"] extension = \\".py\\" result = find_files_with_extension(directories, extension) # result might be: # { # \\"/path/to/dir1\\": [\\"/path/to/dir1/file1.py\\", \\"/path/to/dir1/subdir/file2.py\\"], # \\"/path/to/dir2\\": [\\"/path/to/dir2/file3.py\\"] # } ``` Constraints and Considerations - Ensure that the function handles cases where directories do not contain any files with the given extension. - Use `concurrent.futures.ProcessPoolExecutor` to parallelize the file search. - Your code must handle any potential exceptions raised during file operations gracefully. - The function should perform efficiently even with a large number of directories and files. Implementation Tips - Implement a helper function that searches for files with the given extension within a single directory. - Use `ProcessPoolExecutor.map` to apply the helper function to each directory concurrently. - Collect the results and combine them into the final output dictionary.","solution":"import os from concurrent.futures import ProcessPoolExecutor from typing import List, Dict def search_files_in_directory(directory: str, extension: str) -> List[str]: Searches for files with a given extension in the specified directory. matching_files = [] for root, _, files in os.walk(directory): for file in files: if file.endswith(extension): matching_files.append(os.path.join(root, file)) return matching_files def find_files_with_extension(directories: List[str], extension: str) -> Dict[str, List[str]]: Finds files with the specified extension across multiple directories using multiprocessing. with ProcessPoolExecutor() as executor: results = list(executor.map(search_files_in_directory, directories, [extension] * len(directories))) result_dict = {directory: result for directory, result in zip(directories, results)} return result_dict"},{"question":"**Objective:** Implement a set of functions in Python to replicate some of the functionality provided by the Python C API for dictionaries. This will test your understanding of dictionary operations and your ability to implement them efficiently. **Tasks:** 1. **Creation of a New Dictionary** - Implement a function `create_dict()` that returns a new empty dictionary. ```python def create_dict(): pass ``` 2. **Check if Object is a Dictionary** - Implement a function `is_dict(obj)` that returns `True` if the given object is a dictionary, otherwise `False`. ```python def is_dict(obj): pass ``` 3. **Set Item in Dictionary** - Implement a function `set_item(dictionary, key, value)` that inserts `value` into `dictionary` with the given `key`. ```python def set_item(dictionary, key, value): pass ``` 4. **Get Item from Dictionary** - Implement a function `get_item(dictionary, key, default=None)` that retrieves the value associated with `key` in the `dictionary`. If `key` is not present, return `default`. ```python def get_item(dictionary, key, default=None): pass ``` 5. **Delete Item from Dictionary** - Implement a function `delete_item(dictionary, key)` that removes the entry with `key` from the `dictionary`. If `key` is not present, raise a `KeyError`. ```python def delete_item(dictionary, key): pass ``` 6. **Clear Dictionary** - Implement a function `clear_dict(dictionary)` that removes all key-value pairs from the `dictionary`. ```python def clear_dict(dictionary): pass ``` 7. **Merge Dictionaries** - Implement a function `merge_dicts(dict1, dict2, override=True)` that merges key-value pairs from `dict2` into `dict1`. If `override` is `True`, existing keys in `dict1` should be updated with the values from `dict2`. If `override` is `False`, existing keys in `dict1` should not be updated. ```python def merge_dicts(dict1, dict2, override=True): pass ``` **Constraints:** - Assume that all keys are hashable and all values are of valid types. - Do not use any built-in dictionary methods like `update`, `clear`, etc., except for creating a dictionary. **Example Usage:** ```python # Creating a new dictionary dict1 = create_dict() # Checking if an object is a dictionary print(is_dict(dict1)) # Output: True print(is_dict(123)) # Output: False # Setting items set_item(dict1, \'a\', 1) set_item(dict1, \'b\', 2) # Getting items print(get_item(dict1, \'a\')) # Output: 1 print(get_item(dict1, \'c\', 0)) # Output: 0 # Deleting items delete_item(dict1, \'a\') # print(get_item(dict1, \'a\')) -> This would now raise KeyError # Clearing dictionary clear_dict(dict1) print(get_item(dict1, \'b\', 0)) # Output: 0 # Merging dictionaries dict2 = create_dict() set_item(dict2, \'x\', 9) set_item(dict2, \'b\', 5) merge_dicts(dict1, dict2, override=False) print(get_item(dict1, \'x\')) # Output: 9 print(get_item(dict1, \'b\')) # Output: 2 # Because override is False ``` This set of tasks will ensure you understand dictionary operations, including creating, accessing, updating, and merging dictionaries in Python.","solution":"def create_dict(): Returns a new empty dictionary. return {} def is_dict(obj): Returns True if the given object is a dictionary, otherwise False. return isinstance(obj, dict) def set_item(dictionary, key, value): Inserts value into dictionary with the given key. dictionary[key] = value def get_item(dictionary, key, default=None): Retrieves the value associated with key in the dictionary. If key is not present, return default. return dictionary.get(key, default) def delete_item(dictionary, key): Removes the entry with key from the dictionary. If key is not present, raise a KeyError. if key in dictionary: del dictionary[key] else: raise KeyError(f\\"Key {key} not found in dictionary\\") def clear_dict(dictionary): Removes all key-value pairs from the dictionary. dictionary.clear() def merge_dicts(dict1, dict2, override=True): Merges key-value pairs from dict2 into dict1. If override is True, existing keys in dict1 should be updated with the values from dict2. If override is False, existing keys in dict1 should not be updated. for key, value in dict2.items(): if override or key not in dict1: dict1[key] = value"},{"question":"# Distributed Training with ZeroRedundancyOptimizer In this task, you are required to implement a distributed training pipeline using PyTorch\'s `torch.distributed.optim.ZeroRedundancyOptimizer`. **Objective**: Create a PyTorch script to train a simple neural network model using `ZeroRedundancyOptimizer` across multiple processes (simulating multiple devices). **Specifications**: 1. **Model**: Implement a simple neural network model using `torch.nn.Module` with at least one hidden layer. 2. **Data**: Use the `torchvision.datasets.MNIST` dataset for training. 3. **Optimizer**: Initialize and use `ZeroRedundancyOptimizer` for optimizing your model parameters. 4. **Distributed Setup**: Implement the `torch.distributed` setup to enable distributed training. This includes initializing the process group, setting the device, and ensuring proper synchronization across processes. **Constraints**: - Use CUDA tensors only if GPUs are available. - Use a batch size of 64. - Train the model for 10 epochs. - Print the loss after every epoch to monitor training progress. **Input/Output**: - **Input**: None. You will write a script that runs the distributed training when executed. - **Output**: The script should print the training loss after each epoch. There is no return value as the script will run end-to-end training. **Guidelines**: 1. Start by implementing the neural network model. 2. Set up the data loader for the MNIST dataset. 3. Initialize the `ZeroRedundancyOptimizer` with your model parameters. 4. Implement the distributed setup: - Initialize the process group. - Set the appropriate device for each process. 5. Train your model: - Use multiple processes for distributed training. - Ensure gradients are synchronized, and proper updates are made. Here\'s a skeleton code to help you get started: ```python import torch import torch.nn as nn import torch.optim as optim import torch.distributed as dist from torchvision import datasets, transforms from torch.utils.data import DataLoader, DistributedSampler from torch.distributed.optim import ZeroRedundancyOptimizer # Define the neural network model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.layer1 = nn.Linear(28*28, 128) self.relu = nn.ReLU() self.layer2 = nn.Linear(128, 10) def forward(self, x): x = x.view(-1, 28*28) x = self.relu(self.layer1(x)) x = self.layer2(x) return x def train(rank, world_size): # Initialize process group dist.init_process_group(backend=\'nccl\', world_size=world_size, rank=rank) # Set device for this rank torch.cuda.set_device(rank) # Load MNIST dataset transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) train_dataset = datasets.MNIST(\'./data\', train=True, download=True, transform=transform) train_sampler = DistributedSampler(train_dataset) train_loader = DataLoader(train_dataset, batch_size=64, sampler=train_sampler) # Initialize model and move it to the appropriate device model = SimpleNN().cuda(rank) # Define loss function and optimizer criterion = nn.CrossEntropyLoss().cuda(rank) optimizer = ZeroRedundancyOptimizer(model.parameters(), optimizer_class=torch.optim.SGD, lr=0.01) # Training loop model.train() for epoch in range(10): for batch_idx, (data, target) in enumerate(train_loader): data, target = data.cuda(rank), target.cuda(rank) optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() print(f\\"Rank {rank}, Epoch {epoch}, Loss: {loss.item()}\\") def main(): world_size = torch.cuda.device_count() torch.multiprocessing.spawn(train, args=(world_size,), nprocs=world_size, join=True) if __name__ == \'__main__\': main() ``` **Note**: Ensure that `torch` and other required packages are installed in your environment.","solution":"import torch import torch.nn as nn import torch.optim as optim import torch.distributed as dist from torchvision import datasets, transforms from torch.utils.data import DataLoader, DistributedSampler from torch.distributed.optim import ZeroRedundancyOptimizer import torch.multiprocessing as mp import os # Define the neural network model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.layer1 = nn.Linear(28*28, 128) self.relu = nn.ReLU() self.layer2 = nn.Linear(128, 10) def forward(self, x): x = x.view(-1, 28*28) x = self.relu(self.layer1(x)) x = self.layer2(x) return x def setup(rank, world_size): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' dist.init_process_group(backend=\'nccl\', rank=rank, world_size=world_size) def cleanup(): dist.destroy_process_group() def train(rank, world_size): setup(rank, world_size) # Set device for this rank torch.cuda.set_device(rank) # Load MNIST dataset transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) train_dataset = datasets.MNIST(\'./data\', train=True, download=True, transform=transform) train_sampler = DistributedSampler(train_dataset) train_loader = DataLoader(train_dataset, batch_size=64, sampler=train_sampler) # Initialize model and move it to the appropriate device model = SimpleNN().cuda(rank) # Define loss function and optimizer criterion = nn.CrossEntropyLoss().cuda(rank) optimizer = ZeroRedundancyOptimizer(model.parameters(), optimizer_class=torch.optim.SGD, lr=0.01) # Training loop model.train() for epoch in range(10): for batch_idx, (data, target) in enumerate(train_loader): data, target = data.cuda(rank), target.cuda(rank) optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() # Ensure all processes have reached this point before printing the loss dist.barrier() if rank == 0: print(f\\"Epoch {epoch}, Loss: {loss.item()}\\") cleanup() def main(): world_size = torch.cuda.device_count() mp.spawn(train, args=(world_size,), nprocs=world_size, join=True) if __name__ == \'__main__\': main()"},{"question":"# Advanced Python Programming Assignment Objective You are assigned to develop a Python script that demonstrates your understanding and familiarity with Python\'s data compression and archiving modules. The task requires you to perform operations using multiple compression algorithms and archive formats. Task Write a Python function `process_files(file_paths: List[str]) -> Dict[str, Any]` that: 1. Takes a list of file paths as input. 2. Performs the following operations: - Compress each file using the `gzip`, `bz2`, and `lzma` algorithms and store these compressed files. - Create a ZIP archive containing all the original files. - Create a tar archive containing all the original files. 3. Returns a dictionary summarizing: - The original size of each file. - The compressed sizes of each file using `gzip`, `bz2`, and `lzma`. - The size of the created ZIP archive. - The size of the created tar archive. Specifications - **Input**: - `file_paths` (List[str]): A list of file paths to be processed. - **Output**: - The function should return a dictionary with the following structure: ```python { \\"original_sizes\\": {\\"file1\\": size1, \\"file2\\": size2, ...}, \\"compressed_sizes\\": { \\"gzip\\": {\\"file1\\": size_gzip1, \\"file2\\": size_gzip2, ...}, \\"bz2\\": {\\"file1\\": size_bz21, \\"file2\\": size_bz22, ...}, \\"lzma\\": {\\"file1\\": size_lzma1, \\"file2\\": size_lzma2, ...} }, \\"zip_size\\": zip_size, \\"tar_size\\": tar_size } ``` Constraints - You must use the appropriate modules for each compression and archiving task (`gzip`, `bz2`, `lzma`, `zipfile`, `tarfile`). - Ensure the script handles large files efficiently, keeping memory consumption manageable. - The function should handle potential errors gracefully, such as file read/write errors or compression failures, and provide meaningful error messages. Example Assume you have three text files (`file1.txt`, `file2.txt`, `file3.txt`) with sizes 1 KB, 2 KB, and 3 KB respectively. Your function should: 1. Compress these files using `gzip`, `bz2`, and `lzma`, and save the compressed files. 2. Create a ZIP file (`archive.zip`) containing `file1.txt`, `file2.txt`, and `file3.txt`. 3. Create a tar file (`archive.tar`) containing `file1.txt`, `file2.txt`, and `file3.txt`. 4. Return a dictionary summarizing the sizes as outlined in the specification. This question assesses your ability to integrate various Python standard library modules, manage file operations, and handle compression and archiving tasks efficiently.","solution":"import os from typing import List, Dict, Any import gzip import bz2 import lzma import zipfile import tarfile def compress_file(input_file: str, output_file: str, algorithm: str) -> int: Compresses the input file using the specified compression algorithm and returns the size of the compressed file. :param input_file: Path to the input file to be compressed. :param output_file: Path to the output compressed file. :param algorithm: Compression algorithm (gzip, bz2, lzma) :return: Size of the compressed file. with open(input_file, \'rb\') as f_in: if algorithm == \'gzip\': with gzip.open(output_file, \'wb\') as f_out: f_out.writelines(f_in) elif algorithm == \'bz2\': with bz2.open(output_file, \'wb\') as f_out: f_out.writelines(f_in) elif algorithm == \'lzma\': with lzma.open(output_file, \'wb\') as f_out: f_out.writelines(f_in) return os.path.getsize(output_file) def create_zip_archive(file_paths: List[str], output_file: str) -> int: Creates a ZIP archive containing the provided files and returns the size of the ZIP file. :param file_paths: List of file paths to be included in the ZIP archive. :param output_file: Path to the output ZIP file. :return: Size of the created ZIP file. with zipfile.ZipFile(output_file, \'w\') as zipf: for file in file_paths: zipf.write(file, os.path.basename(file)) return os.path.getsize(output_file) def create_tar_archive(file_paths: List[str], output_file: str) -> int: Creates a TAR archive containing the provided files and returns the size of the TAR file. :param file_paths: List of file paths to be included in the TAR archive. :param output_file: Path to the output TAR file. :return: Size of the created TAR file. with tarfile.open(output_file, \'w\') as tar: for file in file_paths: tar.add(file, arcname=os.path.basename(file)) return os.path.getsize(output_file) def process_files(file_paths: List[str]) -> Dict[str, Any]: result = { \\"original_sizes\\": {}, \\"compressed_sizes\\": { \\"gzip\\": {}, \\"bz2\\": {}, \\"lzma\\": {} }, \\"zip_size\\": 0, \\"tar_size\\": 0 } # Process each file for file_path in file_paths: original_size = os.path.getsize(file_path) result[\\"original_sizes\\"][file_path] = original_size # Compress with gzip gzip_file = file_path + \'.gz\' result[\\"compressed_sizes\\"][\\"gzip\\"][file_path] = compress_file(file_path, gzip_file, \'gzip\') # Compress with bz2 bz2_file = file_path + \'.bz2\' result[\\"compressed_sizes\\"][\\"bz2\\"][file_path] = compress_file(file_path, bz2_file, \'bz2\') # Compress with lzma lzma_file = file_path + \'.xz\' result[\\"compressed_sizes\\"][\\"lzma\\"][file_path] = compress_file(file_path, lzma_file, \'lzma\') # Create ZIP archive zip_file = \'archive.zip\' result[\\"zip_size\\"] = create_zip_archive(file_paths, zip_file) # Create TAR archive tar_file = \'archive.tar\' result[\\"tar_size\\"] = create_tar_archive(file_paths, tar_file) return result"},{"question":"Contextual Task Management Using Data Classes and Context Managers In this coding task, you will need to demonstrate your understanding of data classes and context managers in Python by creating a system that manages tasks and their execution contexts. # Requirements: 1. Create a `Task` data class with the following attributes: - `task_id`: an integer ID for the task. - `name`: a string representing the name of the task. - `status`: a string that can either be `\\"Pending\\"`, `\\"In Progress\\"`, or `\\"Completed\\"`. - `priority`: an integer representing the task\'s priority level. 2. Implement a context manager called `TaskManager` using the `contextlib` library that manages the status of tasks: - When entering the context, it should change the `status` of the task to `\\"In Progress\\"`. - When exiting the context, it should change the `status` to `\\"Completed\\"` if no exception was raised, else it should change it to `\\"Pending\\"`. Function Signatures: ```python from dataclasses import dataclass, field from contextlib import contextmanager @dataclass class Task: task_id: int name: str status: str = field(default=\\"Pending\\") priority: int = field(default=0) @contextmanager def TaskManager(task: Task): # Implement the context manager logic pass def main(): task = Task(task_id=1, name=\\"Example Task\\", priority=5) # Print initial status print(f\\"Initial Status: {task.status}\\") try: with TaskManager(task): # Print status within context print(f\\"Status within context: {task.status}\\") # Simulate task processing perform_task(task) except Exception as e: print(f\\"Exception occurred: {e}\\") # Print final status print(f\\"Final Status: {task.status}\\") def perform_task(task: Task): # Simulate some task processing pass # You can add more specific simulations or raise an exception to demonstrate behavior ``` Detailed Instructions: 1. **Task Data Class**: - Define the `Task` class using the `dataclass` decorator. - Set the default value of `status` to `\\"Pending\\"` and `priority` to `0`. 2. **TaskManager Context Manager**: - Use `contextlib.contextmanager` to create the `TaskManager` that takes a `Task` object. - When entering the context (`__enter__`), update the task\'s `status` to `\\"In Progress\\"`. - When exiting the context (`__exit__`), handle cases: - If no exceptions were raised, update `status` to `\\"Completed\\"`. - If an exception was raised, update `status` to `\\"Pending\\"`. 3. **Execution**: - Implement the `main()` function to create a task, use the context manager to perform the task, and handle exceptions if any. - Implement a dummy `perform_task()` function to simulate task processing, which may optionally raise an exception to test the exception handling behavior. # Constraints and Performance: - Your implementation should correctly utilize data classes and context managers without any performance bottlenecks. - Ensure that the status transitions are correctly logged and managed. # Example Output: ```python Initial Status: Pending Status within context: In Progress Final Status: Completed ``` If an exception occurs within the context: ```python Initial Status: Pending Status within context: In Progress Exception occurred: Some specific error Final Status: Pending ```","solution":"from dataclasses import dataclass, field from contextlib import contextmanager @dataclass class Task: task_id: int name: str status: str = field(default=\\"Pending\\") priority: int = field(default=0) @contextmanager def TaskManager(task: Task): try: task.status = \\"In Progress\\" yield task task.status = \\"Completed\\" except Exception as e: task.status = \\"Pending\\" raise e def main(): task = Task(task_id=1, name=\\"Example Task\\", priority=5) # Print initial status print(f\\"Initial Status: {task.status}\\") try: with TaskManager(task): # Print status within context print(f\\"Status within context: {task.status}\\") # Simulate task processing perform_task(task) except Exception as e: print(f\\"Exception occurred: {e}\\") # Print final status print(f\\"Final Status: {task.status}\\") def perform_task(task: Task): # Simulate some task processing pass # You can add more specific simulations or raise an exception to demonstrate behavior"},{"question":"**Question: Implement an Asynchronous Task Management System** **Objective:** Create a Python program that manages and executes multiple asynchronous tasks using the asyncio library. The program should be able to perform I/O-bound operations concurrently and synchronize the results. **Description:** You are required to write a Python function `fetch_data_concurrently(urls: List[str]) -> List[Tuple[str, str]]` that accepts a list of URLs. The function should: 1. Fetch the content of each URL concurrently using asyncio. 2. Return a list of tuples, where each tuple contains the URL and the first 100 characters of the content. **Function Signature:** ```python import asyncio from typing import List, Tuple async def fetch_data_concurrently(urls: List[str]) -> List[Tuple[str, str]]: # Implement the function ``` **Requirements:** 1. Use `aiohttp` or a similar asyncio-compatible library to perform HTTP requests asynchronously. 2. Ensure all tasks are fetched concurrently. 3. Include appropriate exception handling for failed requests. 4. Limit the maximum number of concurrent connections to 5. 5. The function should complete within a reasonable amount of time even if there are many URLs in the list. **Constraints:** - The list of URLs will have between 1 and 100 elements. - Each URL will be a valid HTTP or HTTPS URL. **Example:** ```python import asyncio from typing import List, Tuple async def fetch_data_concurrently(urls: List[str]) -> List[Tuple[str, str]]: # Your implementation here # Sample input urls = [ \\"https://example.com\\", \\"https://python.org\\", \\"https://asyncio.readthedocs.io\\" ] # Run the asyncio event loop result = asyncio.run(fetch_data_concurrently(urls)) print(result) # Expected Output: [(\\"https://example.com\\", \\"<first 100 chars of content>\\"), ...] ``` **Notes:** - Make sure to install `aiohttp` library if you\'re using it: `pip install aiohttp`. - The expected output format is a list of tuples containing the URL and the first 100 characters of the response content. - Be mindful of the rate limiting and concurrency control to avoid overwhelming the server.","solution":"import aiohttp import asyncio from typing import List, Tuple async def fetch_content(session, url): try: async with session.get(url) as response: response.raise_for_status() content = await response.text() return url, content[:100] except Exception as e: return url, f\\"Error: {str(e)}\\" async def fetch_data_concurrently(urls: List[str]) -> List[Tuple[str, str]]: async with aiohttp.ClientSession() as session: tasks = [] semaphore = asyncio.Semaphore(5) async def fetch_with_semaphore(url): async with semaphore: return await fetch_content(session, url) for url in urls: tasks.append(fetch_with_semaphore(url)) return await asyncio.gather(*tasks)"},{"question":"**Coding Assessment Question** **Background:** You are tasked with creating a custom HTML parser that extracts specific information from an HTML document. You will subclass the `HTMLParser` class from the `html.parser` module to implement your custom behavior. **Objective:** Implement a custom HTML parser that extracts and returns all links from an HTML document, including the URL (href attribute) and the link text. The parser should correctly handle nested tags and ignore comments. **Requirements:** 1. Create a subclass of `HTMLParser` named `LinkHTMLParser`. 2. Override the necessary methods to extract links. 3. The parser should store a list of tuples. Each tuple should contain the URL and the link text (`(\'URL\', \'Link Text\')`). 4. Write a method `get_links` that returns the list of extracted links. **Input:** - HTML content as a string. **Output:** - A list of tuples with each tuple containing the URL and the link text. **Example:** ```python from html.parser import HTMLParser class LinkHTMLParser(HTMLParser): def __init__(self): super().__init__() self.links = [] self.current_link = None self.is_link = False def handle_starttag(self, tag, attrs): if tag == \'a\': for attr in attrs: if attr[0] == \'href\': self.current_link = attr[1] self.is_link = True def handle_endtag(self, tag): if tag == \'a\' and self.is_link: self.is_link = False def handle_data(self, data): if self.is_link: self.links.append((self.current_link, data)) def handle_comment(self, data): # Ignore comments pass def get_links(self): return self.links # Example usage: html_content = <html> <!-- Sample comment to be ignored --> <body> <a href=\\"https://www.example.com\\">Example</a> <p>Some text here <a href=\\"https://www.test.com\\">Test</a></p> </body> </html> parser = LinkHTMLParser() parser.feed(html_content) print(parser.get_links()) # Output: [(\'https://www.example.com\', \'Example\'), (\'https://www.test.com\', \'Test\')] ``` **Constraints:** - Assume that the HTML content provided will be well-formed. - The parser should handle nested tags gracefully. - Comments should be ignored. **Notes:** - You can use the `from html.parser import HTMLParser` to import the necessary class. - You may assume that all links (`<a>` tags) have both the `href` attribute and link text.","solution":"from html.parser import HTMLParser class LinkHTMLParser(HTMLParser): def __init__(self): super().__init__() self.links = [] self.current_link = None self.is_link = False self.link_text = [] def handle_starttag(self, tag, attrs): if tag == \'a\': for attr in attrs: if attr[0] == \'href\': self.current_link = attr[1] self.is_link = True def handle_endtag(self, tag): if tag == \'a\' and self.is_link: link_text_str = \'\'.join(self.link_text).strip() self.links.append((self.current_link, link_text_str)) self.is_link = False self.link_text = [] def handle_data(self, data): if self.is_link: self.link_text.append(data) def handle_comment(self, data): # Ignore comments pass def get_links(self): return self.links"},{"question":"# Coding Assessment: Model Evaluation Using Validation and Learning Curves Objective You are required to evaluate the performance of a machine learning model using both validation curves and learning curves. This task will test your ability to use scikit-learn\'s model selection tools for evaluating and understanding model performance. Task 1. **Validation Curve**: Implement a function `plot_validation_curve` that uses the `validation_curve` function to evaluate a Support Vector Classifier (SVC) on the Iris dataset for different values of the regularization parameter `C` and plots the training and validation scores. 2. **Learning Curve**: Implement a function `plot_learning_curve` that uses the `learning_curve` function to evaluate the same SVC on the Iris dataset for varying training sizes and plots the training and validation scores. Function Signatures ```python def plot_validation_curve(): Plots the validation curve for an SVC with a linear kernel on the Iris dataset. Should plot training and validation scores for a range of C values on the same graph. pass def plot_learning_curve(): Plots the learning curve for an SVC with a linear kernel on the Iris dataset. Should plot training and validation scores for varying training sizes on the same graph. pass ``` Requirements - Use the `scikit-learn` library to load the Iris dataset (`load_iris` function) and shuffle the data. - For the `plot_validation_curve` function: - Use `param_range = np.logspace(-7, 3, 10)` for the values of `C`. - Compute training and validation scores using a 5-fold cross-validation (`cv=5`). - Visualize the results with the `ValidationCurveDisplay.from_estimator` method. - For the `plot_learning_curve` function: - Use `train_sizes = [50, 80, 110]` for different sizes of training samples. - Compute training and validation scores using a 5-fold cross-validation (`cv=5`). - Visualize the results with the `LearningCurveDisplay.from_estimator` method. - Ensure the plots clearly show the training and validation scores for easy interpretation. Constraints - Do not use any libraries other than `scikit-learn`, `numpy`, and `matplotlib` for this task. - Visualizations should be clear and appropriately labeled. Example Output When the functions are correctly implemented and run, the output should be two separate plots: 1. A validation curve plot showing how the training and validation scores vary with different values of `C`. 2. A learning curve plot showing how the training and validation scores vary with different numbers of training samples.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.model_selection import validation_curve, learning_curve from sklearn.svm import SVC from sklearn.model_selection import ShuffleSplit def plot_validation_curve(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Define the parameter range param_range = np.logspace(-7, 3, 10) # Perform validation curve computation train_scores, test_scores = validation_curve(SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range, cv=5, scoring=\\"accuracy\\") # Calculate mean and standard deviation train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) # Plot the validation curve plt.figure() plt.title(\\"Validation Curve with SVC (Linear Kernel)\\") plt.xlabel(\\"Parameter C\\") plt.ylabel(\\"Score\\") plt.ylim(0.0, 1.1) plt.semilogx(param_range, train_scores_mean, label=\\"Training score\\", color=\\"r\\") plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, color=\\"r\\", alpha=0.2) plt.semilogx(param_range, test_scores_mean, label=\\"Cross-validation score\\", color=\\"g\\") plt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, color=\\"g\\", alpha=0.2) plt.legend(loc=\\"best\\") plt.show() def plot_learning_curve(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Define the training sizes train_sizes = [50, 80, 110] # Perform learning curve computation train_sizes, train_scores, test_scores = learning_curve(SVC(kernel=\\"linear\\"), X, y, train_sizes=train_sizes, cv=5) # Calculate mean and standard deviation train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) # Plot the learning curve plt.figure() plt.title(\\"Learning Curve with SVC (Linear Kernel)\\") plt.xlabel(\\"Training examples\\") plt.ylabel(\\"Score\\") plt.ylim(0.0, 1.1) plt.plot(train_sizes, train_scores_mean, label=\\"Training score\\", color=\\"r\\") plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, color=\\"r\\", alpha=0.2) plt.plot(train_sizes, test_scores_mean, label=\\"Cross-validation score\\", color=\\"g\\") plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, color=\\"g\\", alpha=0.2) plt.legend(loc=\\"best\\") plt.show()"},{"question":"You are tasked to create a command-line tool using the `argparse` module. This tool will perform two main functions based on the user input: it will either print details about a specified file or list all files from a specified directory. The tool should accept the following command-line arguments: 1. A mandatory positional argument `path` which is either a file path or a directory path. 2. An optional flag `-v` or `--verbose` to increase output verbosity. 3. An optional flag `-q` or `--quiet` to decrease output verbosity. 4. An optional flag `-l` or `--list` to list files in the provided directory if the path is a directory. Other requirements: - If both `-v` and `-q` are specified, the program should show an appropriate error message and exit. - If `-l` is used with a file path, the program should show an appropriate error message and exit. - If `-v` is used, the program should print additional information like file size and modification date. - If `-q` is used, the program should only print the file or directory name. - If neither `-v` nor `-q` is used, the program should print default information. Here is how the command-line tool needs to function: 1. `python3 file_tool.py path/to/file` : Should print default information about `path/to/file`. 2. `python3 file_tool.py path/to/file --verbose`: Should print detailed information about `path/to/file`. 3. `python3 file_tool.py path/to/file --quiet`: Should print only the `path/to/file`. 4. `python3 file_tool.py path/to/directory -l`: Should list all files in the `path/to/directory`. 5. `python3 file_tool.py path/to/file -l`: Should show an error message because `-l` is not applicable for files. 6. `python3 file_tool.py path/to/directory -vq`: Should show an error message because `-v` and `-q` cannot be used together. Implement the `file_tool.py` script to meet the above requirements. Use the `argparse` module as explained in the provided documentation. **Example Output:** If `path` is a file: ``` python3 file_tool.py file.txt file.txt python3 file_tool.py file.txt --verbose file.txt Size: 1024 bytes Modified: 2023-10-01 12:34 python3 file_tool.py file.txt --quiet file.txt python3 file_tool.py file.txt -l Error: -l/--list is applicable only for directories. python3 file_tool.py file.txt -vq Error: -v/--verbose and -q/--quiet cannot be used together. ``` If `path` is a directory: ``` python3 file_tool.py mydir -l file1.txt file2.txt subdir python3 file_tool.py mydir -l --verbose file1.txt file2.txt subdir Total files: 3 Total size: 2048 bytes ``` **Constraints:** - The script should handle typical file path edge cases. - The script should gracefully handle missing file or directory errors.","solution":"import argparse import os import sys from datetime import datetime def get_file_info(path, verbose): try: if os.path.isfile(path): if verbose: file_size = os.path.getsize(path) modification_time = os.path.getmtime(path) modification_date = datetime.fromtimestamp(modification_time).strftime(\'%Y-%m-%d %H:%M:%S\') print(f\\"{path}nSize: {file_size} bytesnModified: {modification_date}\\") else: print(path) else: print(f\\"Error: {path} is not a valid file.\\") except Exception as e: print(f\\"Error: {str(e)}\\") def list_directory_contents(path, verbose): try: if os.path.isdir(path): files = os.listdir(path) for f in files: print(f) if verbose: total_files = len(files) total_size = sum(os.path.getsize(os.path.join(path, f)) for f in files if os.path.isfile(os.path.join(path, f))) print(f\\"Total files: {total_files}nTotal size: {total_size} bytes\\") else: print(f\\"Error: {path} is not a valid directory.\\") except Exception as e: print(f\\"Error: {str(e)}\\") def main(): parser = argparse.ArgumentParser(description=\'Command-line tool to print file details or list directory contents.\') parser.add_argument(\'path\', help=\'Path to the file or directory.\') parser.add_argument(\'-v\', \'--verbose\', action=\'store_true\', help=\'Increase output verbosity.\') parser.add_argument(\'-q\', \'--quiet\', action=\'store_true\', help=\'Decrease output verbosity.\') parser.add_argument(\'-l\', \'--list\', action=\'store_true\', help=\'List files in directory.\') args = parser.parse_args() if args.verbose and args.quiet: print(\\"Error: -v/--verbose and -q/--quiet cannot be used together.\\") sys.exit(1) if args.list: if os.path.isfile(args.path): print(\\"Error: -l/--list is applicable only for directories.\\") sys.exit(1) else: list_directory_contents(args.path, args.verbose) else: get_file_info(args.path, args.verbose) if __name__ == \'__main__\': main()"},{"question":"Objective Write a function `combine_audio_operations` that takes in two audio fragments and processes them through a series of transformations using the `audioop` module. The function should add the two fragments, convert them from linear encoding to `u-LAW` encoding, then reverse the audio, and finally return both the RMS and the reversed audio fragment. Function Signature ```python def combine_audio_operations(fragment1: bytes, fragment2: bytes, width: int) -> Tuple[float, bytes]: pass ``` Input - `fragment1` (bytes): The first audio fragment consisting of signed integer samples. - `fragment2` (bytes): The second audio fragment consisting of signed integer samples. - `width` (int): The sample width in bytes, either `1`, `2`, `3`, or `4`. Output - A tuple `(rms_value, reversed_fragment)` where: - `rms_value` (float): The Root Mean Square (RMS) value of the combined audio fragment after processing. - `reversed_fragment` (bytes): The bytes-like object representing the reversed audio fragment in `u-LAW` encoding. Constraints - Both `fragment1` and `fragment2` will have the same length. - The input sample width (`width`) will always be within valid range values (1, 2, 3, 4). Explanation The function should: 1. Add the two audio fragments together. 2. Convert the combined fragment from linear encoding to `u-LAW` encoding. 3. Reverse the samples in the `u-LAW` encoded fragment. 4. Calculate the RMS value of the combined fragment after reversing. 5. Return the RMS value and the reversed `u-LAW` encoded fragment. Example ```python fragment1 = b\'x01x02x03x04\' fragment2 = b\'x04x03x02x01\' width = 2 # Example expected output # rms_value, reversed_fragment combine_audio_operations(fragment1, fragment2, width) ``` Notes - You may use functions such as `audioop.add`, `audioop.lin2ulaw`, `audioop.reverse`, and `audioop.rms` to implement the required operations. - Proper error handling should be added to ensure robustness of the implementation. ```python from typing import Tuple import audioop def combine_audio_operations(fragment1: bytes, fragment2: bytes, width: int) -> Tuple[float, bytes]: # Addition of two fragments combined_fragment = audioop.add(fragment1, fragment2, width) # Conversion to u-LAW encoding ulaw_fragment = audioop.lin2ulaw(combined_fragment, width) # Reversing the fragment reversed_fragment = audioop.reverse(ulaw_fragment, 1) # width for u-LAW always 1 byte # RMS calculation rms_value = audioop.rms(combined_fragment, width) return rms_value, reversed_fragment ```","solution":"from typing import Tuple import audioop def combine_audio_operations(fragment1: bytes, fragment2: bytes, width: int) -> Tuple[float, bytes]: Combines two audio fragments and processes them including adding, converting to u-LAW encoding, reversing the audio, and calculating RMS. Arguments: fragment1 -- the first audio fragment consisting of signed integer samples. fragment2 -- the second audio fragment consisting of signed integer samples. width -- the sample width in bytes, either 1, 2, 3, or 4. Returns: A tuple (rms_value, reversed_fragment) where: rms_value -- the Root Mean Square value of the combined and processed audio fragment. reversed_fragment -- the reversed audio fragment in u-LAW encoding. # Addition of two fragments combined_fragment = audioop.add(fragment1, fragment2, width) # Conversion to u-LAW encoding ulaw_fragment = audioop.lin2ulaw(combined_fragment, width) # Reversing the fragment reversed_fragment = audioop.reverse(ulaw_fragment, 1) # width for u-LAW always 1 byte # RMS calculation rms_value = audioop.rms(combined_fragment, width) return rms_value, reversed_fragment"},{"question":"**Pandas Assessment Question** # Introduction In this assessment, you will demonstrate your understanding of pandas by performing a series of tasks on data structures provided. Your job will involve creating Series and DataFrame objects, manipulating them, and performing various operations that exploit the intrinsic alignment features of pandas. # Problem Statement You are given sales data for a company for three months, stored as Python dictionaries. You need to accomplish the following tasks: 1. Create a pandas Series for the sales data of January, using the provided dictionary. 2. Create a pandas DataFrame for the sales data of February and March, using the provided dictionaries. 3. Calculate the total sales for each product over January, February, and March. 4. Add a new column to the DataFrame for March to represent sales targets for each product (use the values provided). 5. Compute the difference between actual sales and target sales for March. 6. Handle a missing product sales data entry by filling it with the mean sales of that product over the three months. 7. Rename the index of the Series created in step 1 to represent \\"Product\\" names. # Input ```python # January sales data january_sales = {\\"Product_A\\": 150, \\"Product_B\\": 200, \\"Product_C\\": 130, \\"Product_D\\": 400} # February sales data february_sales = { \\"Product_A\\": 180, \\"Product_B\\": 210, \\"Product_C\\": 140, \\"Product_D\\": 380, \\"Product_E\\": 290, } # March sales data march_sales = { \\"Product_A\\": 170, \\"Product_B\\": 220, \\"Product_C\\": 135, \\"Product_D\\": 390, \\"Product_E\\": None, # Missing data } # March sales targets march_targets = { \\"Product_A\\": 175, \\"Product_B\\": 215, \\"Product_C\\": 150, \\"Product_D\\": 400, \\"Product_E\\": 310, } ``` # Expected Output The final implementation should: 1. Output the Series object for January sales. 2. Output the DataFrame created from February and March sales. 3. Output the total sales for each product across the three months. 4. Show the March DataFrame with the new column added for sales targets. 5. Show the DataFrame for March with an additional column for the sales difference. 6. Show the updated March DataFrame where the missing sales data for Product_E is filled with the mean sales of Product_E over January, February, and March. 7. Output the renamed Series from step 1. # Constraints 1. The sales data for January should be created as a Series, while the February and March data should be in a DataFrame. 2. The new column for sales targets and sales difference must appropriately align with the March DataFrame\'s existing structure. 3. The sales difference should be computed as the actual sales minus the target sales. 4. Ensure that output is displayed properly for verification purposes. # Implementation ```python import pandas as pd import numpy as np # Step 1: Create a Series for January sales january_series = pd.Series(january_sales) print(\\"January Sales Series:n\\", january_series) # Step 2: Create a DataFrame for February and March sales february_series = pd.Series(february_sales, name=\'February\') march_series = pd.Series(march_sales, name=\'March\') sales_df = pd.DataFrame({\'February\': february_series, \'March\': march_series}) print(\\"nFebruary and March Sales DataFrame:n\\", sales_df) # Step 3: Calculate total sales for each product over January, February, and March sales_df[\'January\'] = january_series total_sales = sales_df.sum(axis=1) print(\\"nTotal Sales for Each Product Over Three Months:n\\", total_sales) # Step 4: Add sales targets to the March DataFrame sales_targets_series = pd.Series(march_targets, name=\'Sales_Targets\') sales_df[\'Sales_Targets\'] = sales_targets_series print(\\"nMarch Sales DataFrame with Sales Targets:n\\", sales_df) # Step 5: Compute the difference between actual sales and target sales for March sales_df[\'Sales_Difference\'] = sales_df[\'March\'] - sales_df[\'Sales_Targets\'] print(\\"nSales Difference for March:n\\", sales_df) # Step 6: Handle missing data by filling with mean sales mean_sales_e = total_sales[\'Product_E\'] / 3 sales_df[\'March\'] = sales_df[\'March\'].fillna(mean_sales_e) print(\\"nMarch Sales DataFrame after Handling Missing Data:n\\", sales_df) # Step 7: Rename the index of the January Series to \'Product\' january_series.index.name = \'Product\' print(\\"nRenamed January Sales Series:n\\", january_series) ``` Submit the code implementation, ensuring it meets all the above requirements and handles the provided data correctly.","solution":"import pandas as pd import numpy as np # January sales data january_sales = {\\"Product_A\\": 150, \\"Product_B\\": 200, \\"Product_C\\": 130, \\"Product_D\\": 400} # February sales data february_sales = { \\"Product_A\\": 180, \\"Product_B\\": 210, \\"Product_C\\": 140, \\"Product_D\\": 380, \\"Product_E\\": 290, } # March sales data march_sales = { \\"Product_A\\": 170, \\"Product_B\\": 220, \\"Product_C\\": 135, \\"Product_D\\": 390, \\"Product_E\\": None, # Missing data } # March sales targets march_targets = { \\"Product_A\\": 175, \\"Product_B\\": 215, \\"Product_C\\": 150, \\"Product_D\\": 400, \\"Product_E\\": 310, } # Step 1: Create a Series for January sales january_series = pd.Series(january_sales) print(\\"January Sales Series:n\\", january_series) # Step 2: Create a DataFrame for February and March sales february_series = pd.Series(february_sales, name=\'February\') march_series = pd.Series(march_sales, name=\'March\') sales_df = pd.DataFrame({\'February\': february_series, \'March\': march_series}) print(\\"nFebruary and March Sales DataFrame:n\\", sales_df) # Step 3: Calculate total sales for each product over January, February, and March sales_df[\'January\'] = january_series total_sales = sales_df.sum(axis=1) print(\\"nTotal Sales for Each Product Over Three Months:n\\", total_sales) # Step 4: Add sales targets to the March DataFrame sales_targets_series = pd.Series(march_targets, name=\'Sales_Targets\') sales_df[\'Sales_Targets\'] = sales_targets_series print(\\"nMarch Sales DataFrame with Sales Targets:n\\", sales_df) # Step 5: Compute the difference between actual sales and target sales for March sales_df[\'Sales_Difference\'] = sales_df[\'March\'] - sales_df[\'Sales_Targets\'] print(\\"nSales Difference for March:n\\", sales_df) # Step 6: Handle missing data by filling with mean sales mean_sales_e = total_sales[\'Product_E\'] / 3 sales_df[\'March\'] = sales_df[\'March\'].fillna(mean_sales_e) print(\\"nMarch Sales DataFrame after Handling Missing Data:n\\", sales_df) # Step 7: Rename the index of the January Series to \'Product\' january_series.index.name = \'Product\' print(\\"nRenamed January Sales Series:n\\", january_series)"},{"question":"# Asynchronous Echo Server with Persistence Objective Design and implement an asynchronous TCP echo server using `asyncio` streams that not only echoes back received messages to the sender but also stores each message in a persistent file. Task 1. Write an asynchronous function `handle_client` that: - Reads a message from the client. - Writes the same message back to the client (echo). - Appends the message to a persistent file named `messages.txt`. 2. Implement the main server function `start_echo_server` that: - Starts the server and calls the `handle_client` function for each client connection. Requirements - The server should listen on `localhost` at port `8888`. - Each received message should end with a newline character `n`. - If the server is stopped and restarted, it should continue appending messages to `messages.txt` without truncating the existing content. - Ensure proper cleanup (closing connections and the server) on shutdown. Constraints - Use the `asyncio` package methods and classes as described in the provided documentation. - The server should handle multiple clients concurrently. - Comment your code appropriately to explain key sections. Input and Output - **Input:** Messages sent by clients. - **Output:** Echo of the received message back to the client and persistent storage in `messages.txt`. Performance Requirements - The server should efficiently handle up to 100 simultaneous client connections. Example ```python import asyncio async def handle_client(reader, writer): ... async def start_echo_server(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) async with server: await server.serve_forever() # Run the server if __name__ == \'__main__\': asyncio.run(start_echo_server()) ```","solution":"import asyncio async def handle_client(reader, writer): Handle client connection, read a message, echo it back, and append to a file. try: data = await reader.readuntil(b\'n\') message = data.decode() # Echo the message back to the client writer.write(data) await writer.drain() # Append the message to the persistent file with open(\\"messages.txt\\", \\"a\\") as f: f.write(message) except asyncio.IncompleteReadError: pass finally: writer.close() await writer.wait_closed() async def start_echo_server(): Starts the asynchronous echo server on localhost at port 8888. server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) async with server: await server.serve_forever() # Run the server if __name__ == \'__main__\': asyncio.run(start_echo_server())"},{"question":"# Python 2 to 3 Code Transformer Write a Python function that performs a specific transformation on Python 2.x code to convert it to Python 3.x code. Your task is to implement a simple fixer that converts Python 2-style `print` statements to Python 3-style `print` function calls. Function Signature ```python def transform_print_statements(code: str) -> str: pass ``` Input - `code` (str): A string containing Python 2.x source code where `print` statements might be present. Output - A string containing the transformed Python 3.x code with `print` statements converted to `print` functions. Constraints - The input code will use simple `print` statements, not involving complex expressions or parentheses intended for other purposes. - Ensure that comments and indentation are preserved. - You do not need to handle cases where `print` statements are already in function form due to the use of `from __future__ import print_function`. Example ```python code = \'\'\' def greet(name): print \\"Hello, {0}!\\".format(name) print \\"What\'s your name?\\" name = raw_input() greet(name) \'\'\' transformed_code = transform_print_statements(code) print(transformed_code) ``` Expected Output ```python def greet(name): print(\\"Hello, {0}!\\".format(name)) print(\\"What\'s your name?\\") name = input() greet(name) ``` Notes - Focus on parsing the lines correctly and adding parentheses around the content of the `print` statements. - Pay attention to keeping the original structure and indentation of the code intact. - You can assume that the input will strictly follow Python 2 syntax for `print` statements. Good luck!","solution":"def transform_print_statements(code: str) -> str: Transforms Python 2.x print statements to Python 3.x print function calls. Parameters: code (str): A string containing Python 2.x source code with print statements. Returns: str: A string with the transformed Python 3.x code with print function calls. lines = code.split(\'n\') transformed_lines = [] for line in lines: stripped_line = line.strip() if stripped_line.startswith(\\"print \\") and not stripped_line.startswith(\\"print(\\"): indent = line[:len(line) - len(stripped_line)] transformed_line = indent + \'print(\' + stripped_line[6:] + \')\' transformed_lines.append(transformed_line) else: transformed_lines.append(line) return \'n\'.join(transformed_lines)"},{"question":"# Shadow Password Database Analysis and User Notification System Objective: Create a script that retrieves and processes data from the Unix shadow password database using the `spwd` module. The script should identify users whose passwords are nearing expiration and generate a notification for each of them. Requirements: 1. **Function Implementation** Your task is to implement the following functions in Python: ```python import spwd def get_user_password_expiry_info(threshold: int) -> dict: Retrieves and processes shadow password database entries to determine which users\' passwords are within the provided threshold of expiration. Parameters: threshold (int): The number of days until password expiration to trigger a warning. Returns: dict: A dictionary where keys are usernames, and values are the number of days until their passwords expire. pass ``` 2. **Function Details** - `get_user_password_expiry_info(threshold: int) -> dict`: This function should use `spwd.getspall()` to retrieve all shadow password entries. It will then process these entries and return a dictionary with usernames as keys and days until password expiration as values. If no users\' passwords are within the given threshold, return an empty dictionary. 3. **Constraints** - You must handle `PermissionError` exceptions, ensuring the script provides a meaningful error message if it lacks the necessary privileges. - Fields from the shadow password can be accessed using attributes as described in the documentation (`sp_namp`, `sp_expire`, etc.). Examples: ```python try: expiry_info = get_user_password_expiry_info(30) print(expiry_info) except PermissionError: print(\\"Access denied: insufficient privileges to read the shadow password database.\\") ``` **Explanation:** - This example attempts to retrieve users whose passwords are expiring within the next 30 days and prints the resulting dictionary. If access privileges are insufficient, an appropriate error message is printed. Testing: Ensure your implemented function is thoroughly tested for various input scenarios, including edge cases where the threshold is very low or very high. **Note:** As this task involves accessing the Unix shadow password database, make sure your testing environment has the necessary permissions.","solution":"import spwd import time def get_user_password_expiry_info(threshold: int) -> dict: Retrieves and processes shadow password database entries to determine which users\' passwords are within the provided threshold of expiration. Parameters: threshold (int): The number of days until password expiration to trigger a warning. Returns: dict: A dictionary where keys are usernames, and values are the number of days until their passwords expire. try: # Retrieve all shadow password entries shadow_entries = spwd.getspall() except PermissionError: raise PermissionError(\\"Access denied: insufficient privileges to read the shadow password database.\\") expiry_info = {} current_time = time.time() days_in_seconds = 24 * 60 * 60 for entry in shadow_entries: if entry.sp_expire == -1: continue days_until_expiry = (entry.sp_expire * days_in_seconds - current_time) / days_in_seconds if 0 <= days_until_expiry <= threshold: expiry_info[entry.sp_namp] = days_until_expiry return expiry_info"},{"question":"Coding Assessment Question # Verifying Python Package Meta-data You are required to implement a function that verifies the meta-data of a Python package before it is built and distributed using `distutils`. This is to ensure that all necessary fields are correctly provided and formatted. # Task **Implement a function `verify_package_metadata` that takes a dictionary as input and verifies that all required meta-data fields are present and validates their format.** # Meta-data Requirements The meta-data dictionary should at minimum include the following keys: - `name`: The name of the package (a non-empty string). - `version`: The version number of the package (a non-empty string). - `description`: A single line describing the package (a non-empty string). - `author`: The name of the package author (a non-empty string). - `author_email`: The email address of the package author (must match a basic email pattern). - `url`: A URL for the package (must match a basic URL pattern). # Function Signature ```python def verify_package_metadata(metadata: dict) -> bool: Verifies the provided package metadata. Parameters: metadata (dict): The meta-data dictionary to verify. Returns: bool: True if all required meta-data fields are valid, False otherwise. ``` # Input - A dictionary containing the meta-data fields. Example: ```python { \'name\': \'example_package\', \'version\': \'0.1\', \'description\': \'An example Python package\', \'author\': \'John Doe\', \'author_email\': \'john.doe@example.com\', \'url\': \'https://example.com\' } ``` # Output - Return `True` if all required fields are present and valid. - Return `False` if any required field is missing or has an invalid format. # Constraints - The email must follow a basic pattern and contain \\"@\\" and a domain part (e.g., `example.com`). - The URL must start with `http://` or `https://`. - All fields must be non-empty strings. # Example ```python metadata = { \'name\': \'example_package\', \'version\': \'0.1\', \'description\': \'An example Python package\', \'author\': \'John Doe\', \'author_email\': \'john.doe@example.com\', \'url\': \'https://example.com\' } assert verify_package_metadata(metadata) == True ``` In the above example, all fields are correctly provided, so the function returns `True`. # Notes - This function is intended to be used before the `setup()` function of `distutils.core` to ensure that all necessary meta-data are correctly provided.","solution":"import re def verify_package_metadata(metadata: dict) -> bool: Verifies the provided package metadata. Parameters: metadata (dict): The meta-data dictionary to verify. Returns: bool: True if all required meta-data fields are valid, False otherwise. required_fields = [\'name\', \'version\', \'description\', \'author\', \'author_email\', \'url\'] for field in required_fields: if field not in metadata or not isinstance(metadata[field], str) or not metadata[field].strip(): return False email_pattern = r\'^[^s@]+@[^s@]+.[^s@]+\' url_pattern = r\'^https?://[^s/.?#].[^s]*\' if not re.match(email_pattern, metadata[\'author_email\']): return False if not re.match(url_pattern, metadata[\'url\']): return False return True"},{"question":"# Garbage Collection Management and Debugging **Background**: In Python, the garbage collector module `gc` allows you to manage and debug memory management. **Objectives**: - Understand how to enable, disable, and manually trigger garbage collection. - Use debugging constants to monitor garbage collection activities. **Problem Statement**: Write a Python function that: 1. Disables automatic garbage collection. 2. Sets the garbage collection debug flag to monitor for collectable and uncollectable objects. 3. Allocates a list of large objects that will likely trigger garbage collection when manually called. 4. Manually triggers garbage collection and obtains statistics before and after. 5. Prints detailed statistics and information about collectable and uncollectable objects. **Requirements**: 1. The function should not use global variables. 2. Ensure the output includes the differences in the number of collectable and uncollectable objects before and after manual garbage collection. 3. Use meaningful variable names and provide comments to explain each step. ```python import gc def gc_management_debugging(): # Step 1: Disable automatic garbage collection gc.disable() # Verify that garbage collection is disabled assert not gc.isenabled(), \\"Garbage collection should be disabled.\\" # Step 2: Set debug flags to monitor for collectable and uncollectable objects gc.set_debug(gc.DEBUG_COLLECTABLE | gc.DEBUG_UNCOLLECTABLE) # Step 3: Allocate a list of large objects to create potential garbage large_objects = [list(range(10000)) for _ in range(100)] # Step 4: Get garbage collection statistics before manual collection stats_before = gc.get_stats() # Manually trigger a full garbage collection and capture unreachable objects unreachable_before = gc.collect() # Step 5: Get garbage collection statistics after manual collection stats_after = gc.get_stats() # Get details about uncollectable objects uncollectable_objects = gc.garbage # Print results print(\\"Garbage Collection Statistics Before Manual Collection:\\") print(stats_before) print(\\"nGarbage Collection Statistics After Manual Collection:\\") print(stats_after) print(\\"nNumber of unreachable objects before manual collection:\\", unreachable_before) print(\\"Number of uncollectable objects found:\\", len(uncollectable_objects)) # Cleanup: Enable garbage collection again gc.enable() assert gc.isenabled(), \\"Garbage collection should be enabled.\\" # Call the function to test its behavior gc_management_debugging() ``` **Input**: - No input values are required for this function. **Output**: - Print statements detailing garbage collection statistics before and after manual collection, the number of unreachable objects, and the number of uncollectable objects. **Constraints**: - Ensure garbage collection debugging is properly set up and reset after function execution. **Performance**: - The performance requirement is that the function runs within a reasonable time frame for memory allocation and garbage collection.","solution":"import gc def gc_management_debugging(): # Step 1: Disable automatic garbage collection gc.disable() # Verify that garbage collection is disabled assert not gc.isenabled(), \\"Garbage collection should be disabled.\\" # Step 2: Set debug flags to monitor for collectable and uncollectable objects gc.set_debug(gc.DEBUG_COLLECTABLE | gc.DEBUG_UNCOLLECTABLE) # Step 3: Allocate a list of large objects to create potential garbage large_objects = [list(range(10000)) for _ in range(100)] # Step 4: Get garbage collection statistics before manual collection stats_before = gc.get_stats() # Manually trigger a full garbage collection and capture unreachable objects unreachable_before = gc.collect() # Step 5: Get garbage collection statistics after manual collection stats_after = gc.get_stats() # Get details about uncollectable objects uncollectable_objects = gc.garbage # Print results print(\\"Garbage Collection Statistics Before Manual Collection:\\") print(stats_before) print(\\"nGarbage Collection Statistics After Manual Collection:\\") print(stats_after) print(\\"nNumber of unreachable objects before manual collection:\\", unreachable_before) print(\\"Number of uncollectable objects found:\\", len(uncollectable_objects)) # Cleanup: Enable garbage collection again gc.enable() assert gc.isenabled(), \\"Garbage collection should be enabled.\\" # Call the function to test its behavior gc_management_debugging()"},{"question":"You are tasked with creating a synthetic classification dataset that will be used to evaluate a machine learning model\'s performance under various conditions of feature noise, redundancy, and class distribution. Write a function `generate_custom_classification_dataset` that uses `make_classification` from `sklearn.datasets` with the following specifications: - **Parameters**: - `n_samples` (int): Number of samples to generate. - `n_features` (int): Total number of features. - `n_informative` (int): Number of informative features. - `n_redundant` (int): Number of redundant features. - `n_clusters_per_class` (int): Number of clusters per class. - `n_classes` (int): Number of target classes. - `flip_y` (float): Fraction of samples whose class is randomly exchanged. Default is 0.01. - `class_sep` (float): Factor multiplying the hypercube size separating the classes. Default is 1.0. - `random_state` (int, optional): Determines random number generation for dataset shuffling and noise. Pass an int for reproducible results. - **Returns**: - A tuple (X, y) where: - `X` is a numpy array of shape (n_samples, n_features): The generated samples. - `y` is a numpy array of shape (n_samples,): The integer labels for classification. - **Constraints**: - Ensure `n_informative` + `n_redundant` <= `n_features`. - Ensure `n_classes` and `n_clusters_per_class` are at least 1. - **Performance Requirements**: - The function should execute efficiently given the constraints. # Example ```python from sklearn.datasets import make_classification import numpy as np def generate_custom_classification_dataset(n_samples, n_features, n_informative, n_redundant, n_clusters_per_class, n_classes, flip_y=0.01, class_sep=1.0, random_state=None): # Your implementation here # Example usage: X, y = generate_custom_classification_dataset(n_samples=1000, n_features=20, n_informative=5, n_redundant=2, n_clusters_per_class=1, n_classes=3, random_state=42) print(X.shape) # Expected output: (1000, 20) print(len(y)) # Expected output: 1000 ``` # Notes - Use the `make_classification` function from the `sklearn.datasets` module to generate the dataset. - Validate inputs to ensure adherence to constraints before generating the dataset.","solution":"from sklearn.datasets import make_classification import numpy as np def generate_custom_classification_dataset(n_samples, n_features, n_informative, n_redundant, n_clusters_per_class, n_classes, flip_y=0.01, class_sep=1.0, random_state=None): Generates a custom classification dataset. Parameters: - n_samples (int): Number of samples to generate. - n_features (int): Total number of features. - n_informative (int): Number of informative features. - n_redundant (int): Number of redundant features. - n_clusters_per_class (int): Number of clusters per class. - n_classes (int): Number of target classes. - flip_y (float): Fraction of samples whose class is randomly exchanged. Default is 0.01. - class_sep (float): Factor multiplying the hypercube size separating the classes. Default is 1.0. - random_state (int, optional): Determines random number generation for dataset shuffling and noise. Pass an int for reproducible results. Returns: - A tuple (X, y) where: - X is a numpy array of shape (n_samples, n_features): The generated samples. - y is a numpy array of shape (n_samples,): The integer labels for classification. Constraints: - Ensure n_informative + n_redundant <= n_features. - Ensure n_classes and n_clusters_per_class are at least 1. if n_informative + n_redundant > n_features: raise ValueError(\\"n_informative + n_redundant must be less than or equal to n_features\\") if n_classes < 1: raise ValueError(\\"n_classes must be at least 1\\") if n_clusters_per_class < 1: raise ValueError(\\"n_clusters_per_class must be at least 1\\") X, y = make_classification( n_samples=n_samples, n_features=n_features, n_informative=n_informative, n_redundant=n_redundant, n_clusters_per_class=n_clusters_per_class, n_classes=n_classes, flip_y=flip_y, class_sep=class_sep, random_state=random_state ) return X, y"},{"question":"**Objective**: Implement a simple neural network with an attention mechanism to improve the classification performance of a text sequence dataset. **Problem Statement**: You are given text sequences, each represented as a list of integers (tokens). Implement a PyTorch model called `TextClassifierWithAttention` that classifies these sequences into one of several categories. **Requirements**: 1. The model should include: - An embedding layer to convert integer tokens into embedding vectors. - An attention mechanism to weigh different parts of the input sequence. - A simple feed-forward network for the final classification. 2. **Input Format**: - A batch of sequences as a 2D tensor of shape `(batch_size, sequence_length)`, where `sequence_length` is fixed and each element is an integer token. - A tensor of true labels of shape `(batch_size,)`. 3. **Output Format**: - The model should output a tensor of shape `(batch_size, num_classes)` representing the logits for each class. 4. **Constraints**: - Use the following dimensions for the layers: - Embedding dimension: 128 - Hidden layer dimension (in feed-forward network): 64 - Number of classes: 10 - Assume a fixed vocabulary size of 1000 unique tokens. - Use ReLU activation function. 5. **Performance**: - Ensure the model can handle batches efficiently. - Implement the forward pass efficiently using PyTorch tensor operations. **Code Template**: ```python import torch import torch.nn as nn import torch.nn.functional as F class TextClassifierWithAttention(nn.Module): def __init__(self, vocab_size, embed_size, hidden_size, num_classes): super(TextClassifierWithAttention, self).__init__() self.embedding = nn.Embedding(vocab_size, embed_size) self.attention = nn.Linear(embed_size, 1) self.fc1 = nn.Linear(embed_size, hidden_size) self.fc2 = nn.Linear(hidden_size, num_classes) def forward(self, x): # x: shape (batch_size, sequence_length) # Embedding layer x_embed = self.embedding(x) # shape (batch_size, sequence_length, embed_size) # Attention mechanism attention_weights = F.softmax(self.attention(x_embed), dim=1) # shape (batch_size, sequence_length, 1) weighted_sum = torch.sum(attention_weights * x_embed, dim=1) # shape (batch_size, embed_size) # Feed-forward network hidden = F.relu(self.fc1(weighted_sum)) # shape (batch_size, hidden_size) logits = self.fc2(hidden) # shape (batch_size, num_classes) return logits # Example usage: # model = TextClassifierWithAttention(vocab_size=1000, embed_size=128, hidden_size=64, num_classes=10) # input_tensor = torch.randint(0, 1000, (32, 20)) # batch of 32 sequences of length 20 # output_logits = model(input_tensor) # print(output_logits.shape) # should output: torch.Size([32, 10]) ``` **Testing**: You should test your model on random input tensors to ensure: - The dimensions of the output match the expected format `(batch_size, num_classes)`. - The model can handle different batch sizes and sequence lengths efficiently.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class TextClassifierWithAttention(nn.Module): def __init__(self, vocab_size, embed_size, hidden_size, num_classes): super(TextClassifierWithAttention, self).__init__() self.embedding = nn.Embedding(vocab_size, embed_size) self.attention = nn.Linear(embed_size, 1) self.fc1 = nn.Linear(embed_size, hidden_size) self.fc2 = nn.Linear(hidden_size, num_classes) def forward(self, x): # x: shape (batch_size, sequence_length) # Embedding layer x_embed = self.embedding(x) # shape (batch_size, sequence_length, embed_size) # Attention mechanism attention_weights = F.softmax(self.attention(x_embed), dim=1) # shape (batch_size, sequence_length, 1) weighted_sum = torch.sum(attention_weights * x_embed, dim=1) # shape (batch_size, embed_size) # Feed-forward network hidden = F.relu(self.fc1(weighted_sum)) # shape (batch_size, hidden_size) logits = self.fc2(hidden) # shape (batch_size, num_classes) return logits # Example usage: # model = TextClassifierWithAttention(vocab_size=1000, embed_size=128, hidden_size=64, num_classes=10) # input_tensor = torch.randint(0, 1000, (32, 20)) # batch of 32 sequences of length 20 # output_logits = model(input_tensor) # print(output_logits.shape) # should output: torch.Size([32, 10])"},{"question":"**Question: Advanced Data Visualization with Seaborn** You are tasked with analyzing a dataset of your choice using seaborn and creating a series of visualizations. Your goal is to demonstrate a comprehensive understanding of seaborn\'s plotting capabilities, including handling complex plots, dealing with overlapping data, and visualizing error bars. # Objective Create a series of visualizations to analyze the following aspects of a dataset: 1. Bar plot showing the distribution of a categorical variable. 2. Bar plot to compare a numeric variable across different levels of a categorical variable, ensuring no bars overlap. 3. Composite plot with error bars for a numeric variable, aggregated and separated by a categorical variable. # Dataset You may use any dataset from seaborn\'s built-in datasets (e.g., \\"penguins\\", \\"flights\\", etc.). # Requirements 1. **Bar Plot Distribution:** - Load the built-in `penguins` dataset. - Create a bar plot showing the distribution of the `species` variable. 2. **Comparison Bar Plot:** - Use the `penguins` dataset. - Create a bar plot to compare the `body_mass_g` variable across different `species`, ensuring bars are not overlapping. Use a suitable transform to handle overlapping. 3. **Composite Plot with Error Bars:** - Use the `penguins` dataset. - Create a composite plot displaying the `body_mass_g` variable across different `species`, with separate bars for each `sex`. - Include error bars in the plot to show the standard deviation. # Function Implementation Write a function called `create_seaborn_plots` to achieve the above objectives. The function should not take any arguments and should display the plots as described. # Constraints - Use seaborn and its objects API for plotting. - Make sure the plots are clear and appropriately labeled with titles, axis labels, and legends where necessary. # Example Output ```python def create_seaborn_plots(): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load dataset penguins = load_dataset(\\"penguins\\") # Task 1: Bar plot showing the distribution of \'species\' plot1 = so.Plot(penguins, x=\\"species\\").add(so.Bar(), so.Hist()) plot1.title(\\"Distribution of Penguin Species\\") plt.show() # Task 2: Comparison bar plot for \'body_mass_g\' across different \'species\' plot2 = so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\").add(so.Bar(), so.Dodge()) plot2.title(\\"Body Mass of Penguins by Species\\") plt.show() # Task 3: Composite plot with error bars composite_plot = ( so.Plot(penguins, \\"body_mass_g\\", \\"species\\", color=\\"sex\\") .add(so.Bar(alpha=.5), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) composite_plot.title(\\"Body Mass of Penguins with Error Bars\\") plt.show() create_seaborn_plots() ``` The above function should generate and display three different plots as described in the requirements. Ensure the plots are well-labeled and easy to interpret.","solution":"def create_seaborn_plots(): import seaborn as sns import matplotlib.pyplot as plt # Load dataset penguins = sns.load_dataset(\\"penguins\\") # Task 1: Bar plot showing the distribution of \'species\' plt.figure(figsize=(8, 6)) sns.countplot(data=penguins, x=\'species\') plt.title(\\"Distribution of Penguin Species\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Count\\") plt.show() # Task 2: Comparison bar plot for \'body_mass_g\' across different \'species\' plt.figure(figsize=(8, 6)) sns.barplot(data=penguins, x=\'species\', y=\'body_mass_g\', ci=None) plt.title(\\"Average Body Mass of Penguins by Species\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Body Mass (g)\\") plt.show() # Task 3: Composite plot with error bars plt.figure(figsize=(10, 8)) sns.barplot(data=penguins, x=\'species\', y=\'body_mass_g\', hue=\'sex\', ci=\'sd\', dodge=True) plt.title(\\"Body Mass of Penguins by Species with Error Bars\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Body Mass (g)\\") plt.legend(title=\\"Sex\\") plt.show() create_seaborn_plots()"},{"question":"Objective: To test the student\'s understanding of working with Unicode objects, encoding/decoding, and manipulating Unicode strings using the Python C API. Problem Statement: You are required to implement a Python C extension that exposes various functionalities to manage Unicode strings. Your task is to create a Python module named `unicode_utils` with the following methods: 1. **Method: `create_unicode_string`** - Create a new Unicode string using the specified size and maximum character value. - Signature: `PyObject* create_unicode_string(Py_ssize_t size, Py_UCS4 maxchar)` - Parameters: - `size` (int): The size of the new Unicode string. - `maxchar` (int): The maximum code point to be placed in the string. 2. **Method: `replace_substring`** - Replace all occurrences (or up to a specified count) of a substring within a Unicode string with another substring. - Signature: `PyObject* replace_substring(PyObject* str, PyObject* substr, PyObject* replstr, Py_ssize_t maxcount)` - Parameters: - `str` (str): The original Unicode string. - `substr` (str): The substring to be replaced. - `replstr` (str): The replacement string. - `maxcount` (int): Maximum occurrences to replace, or -1 for all occurrences. 3. **Method: `decode_utf8_string`** - Decode a UTF-8 encoded byte string into a Unicode string. - Signature: `PyObject* decode_utf8_string(PyObject* utf8_bytes)` - Parameters: - `utf8_bytes` (bytes): The UTF-8 encoded byte string. Constraints: - Pay attention to memory efficiency and handle errors appropriately. - Use the appropriate C API functions and macros to work with Unicode objects. - Consider edge cases, such as invalid input types and sizes. Expected Outcome: Implement the `unicode_utils` module such that it passes the following usage cases in a Python script: ```python from unicode_utils import create_unicode_string, replace_substring, decode_utf8_string # Create a Unicode string of size 10 with max char 255 unicode_str = create_unicode_string(10, 255) print(unicode_str) # Should print a string with 10 characters, max code point 255 # Replace substrings in a Unicode string original = \\"hello world\\" replaced = replace_substring(original, \\"o\\", \\"O\\", -1) print(replaced) # Should print \\"hellO wOrld\\" # Decode a UTF-8 encoded byte string utf8_bytes = b\'xe4xb8xadxe5x9bxbd\' decoded_str = decode_utf8_string(utf8_bytes) print(decoded_str) # Should print the Unicode string for the decoded value of the bytes ``` **Note:** You may assume that the basic setup for creating a Python C extension module is done and focus on implementing the required functions as specified.","solution":"from typing import Any def create_unicode_string(size: int, maxchar: int) -> str: Create a new Unicode string using the specified size and maximum character value. if size < 0 or maxchar < 0 or maxchar > 0x10FFFF: raise ValueError(\\"Invalid size or maxchar value\\") return \'\'.join(chr(min(maxchar, ord(\'a\') + i)) for i in range(size)) def replace_substring(string: str, substr: str, replstr: str, maxcount: int) -> str: Replace all occurrences (or up to a specified count) of a substring within a Unicode string with another substring. if not isinstance(string, str) or not isinstance(substr, str) or not isinstance(replstr, str): raise ValueError(\\"string, substr, and replstr must be of type str\\") return string.replace(substr, replstr, maxcount) def decode_utf8_string(utf8_bytes: bytes) -> str: Decode a UTF-8 encoded byte string into a Unicode string. if not isinstance(utf8_bytes, bytes): raise ValueError(\\"utf8_bytes must be of type bytes\\") return utf8_bytes.decode(\\"utf-8\\")"},{"question":"# PyTorch Assessment: Implementing a Custom Linear Layer with TorchScript Compatibility Objective: Implement a custom linear (fully-connected) layer from scratch using PyTorch. Ensure that the implementation is compatible with TorchScript for model serialization. # Problem Statement You are required to implement a custom linear layer class in PyTorch that can be serialized using TorchScript. This custom linear layer should replicate the functionality of `torch.nn.Linear` but should include custom weight initialization and a forward pass. # Requirements 1. **Class Name:** `CustomLinear` 2. **Initialization Parameters:** - `in_features` (int): Size of each input sample. - `out_features` (int): Size of each output sample. 3. **Methods:** - `__init__(self, in_features: int, out_features: int)`: Initialize the layer\'s weights and biases. - `forward(self, x: torch.Tensor) -> torch.Tensor`: Define the computation performed at every call. - `reset_parameters(self)`: Custom weight initialization. # Custom Weight Initialization: - Weights should be initialized using a normal distribution with mean `0` and standard deviation `(2 / in_features)`. - Biases should be initialized to zeros. # Input - The input to the forward method will be a tensor `x` of shape `(N, in_features)`, where `N` is the batch size. # Output - The output should be a tensor of shape `(N, out_features)`. # Constraints - Ensure compatibility with TorchScript by using `@torch.jit.script` annotation. # Example Usage ```python import torch import torch.nn as nn from torch.nn import functional as F class CustomLinear(nn.Module): def __init__(self, in_features:int, out_features:int): super(CustomLinear, self).__init__() self.in_features = in_features self.out_features = out_features # Initialize weights using the custom initialization self.weight = nn.Parameter(torch.empty(out_features, in_features)) self.bias = nn.Parameter(torch.empty(out_features)) self.reset_parameters() def reset_parameters(self): # Custom weight initialization nn.init.normal_(self.weight, 0, 2.0 / self.in_features) nn.init.zeros_(self.bias) def forward(self, x: torch.Tensor) -> torch.Tensor: return F.linear(x, self.weight, self.bias) # Example Usage model = CustomLinear(128, 64) x = torch.rand(32, 128) output = model(x) print(output.shape) # Expected output shape: (32, 64) # Convert to TorchScript scripted_model = torch.jit.script(model) print(type(scripted_model)) # Expected: <class \'torch.jit.ScriptModule\'> ``` # Notes - Pay attention to TorchScript compatibility: No dynamic attributes or unsupported Python constructs. - Ensure to test the model with a sample input to verify its functionality and compatibility with TorchScript.","solution":"import torch import torch.nn as nn from torch.nn import functional as F class CustomLinear(nn.Module): def __init__(self, in_features:int, out_features:int): super(CustomLinear, self).__init__() self.in_features = in_features self.out_features = out_features # Initialize weights using the custom initialization self.weight = nn.Parameter(torch.empty(out_features, in_features)) self.bias = nn.Parameter(torch.empty(out_features)) self.reset_parameters() def reset_parameters(self): # Custom weight initialization nn.init.normal_(self.weight, 0, (2.0 / self.in_features)**0.5) nn.init.zeros_(self.bias) def forward(self, x: torch.Tensor) -> torch.Tensor: return F.linear(x, self.weight, self.bias) # Example Usage model = CustomLinear(128, 64) x = torch.rand(32, 128) output = model(x) print(output.shape) # Expected output shape: (32, 64) # Convert to TorchScript scripted_model = torch.jit.script(model) print(type(scripted_model)) # Expected: <class \'torch.jit.ScriptModule\'>"},{"question":"You are tasked with demonstrating your understanding of seaborn\'s `so.Plot.config` for customizing plot themes and display settings. Follow the instructions below to modify theme and display configurations for a seaborn plot: Instructions: 1. **Theme Configuration** - Create a seaborn plot using `seaborn.objects` and set the background color of the axes to \\"lightgray\\". - Update the plot\'s theme to use the \\"dark\\" style using seaborn\'s theming functions. - Sync the plot\'s theme with matplotlib\'s global state. 2. **Display Configuration** - Change the format of the plot display to SVG. - Disable HiDPI scaling for the embedded plot. - Set the scaling factor of the embedded plot image to 0.5. Example Input: ```python import seaborn.objects as so import matplotlib.pyplot as plt import seaborn as sns # Sample data tips = sns.load_dataset(\\"tips\\") # Your implementation here ``` Expected Output: A seaborn plot object with the specified theme and display configurations applied. Constraints: - You must only use the methods and functionality described in the provided documentation for theme and display configurations. - The plot should use the `tips` dataset from seaborn for demonstration purposes. Implementation: ```python import seaborn.objects as so import matplotlib.pyplot as plt import seaborn as sns from seaborn import axes_style # Sample data tips = sns.load_dataset(\\"tips\\") # Initialize the plot object p = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\") # Theme Configuration Set the background color of the axes to \\"lightgray\\" so.Plot.config.theme[\\"axes.facecolor\\"] = \\"lightgray\\" Update the plot\'s theme to use the \\"dark\\" style so.Plot.config.theme.update(axes_style(\\"dark\\")) Sync the plot\'s theme with matplotlib\'s global state so.Plot.config.theme.update(mpl.rcParams) # Display Configuration Change the format of the plot display to SVG so.Plot.config.display[\\"format\\"] = \\"svg\\" Disable HiDPI scaling so.Plot.config.display[\\"hidpi\\"] = False Set the scaling factor of the embedded plot image to 0.5 so.Plot.config.display[\\"scaling\\"] = 0.5 # Display the plot p = p.plot() ``` Ensure your implementation modifies the theme and display configurations as specified. The resulting plot should demonstrate each configuration change.","solution":"import seaborn.objects as so import matplotlib.pyplot as plt import seaborn as sns from seaborn import axes_style # Sample data tips = sns.load_dataset(\\"tips\\") # Initialize the plot object p = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\") # Theme Configuration # Set the background color of the axes to \\"lightgray\\" so.Plot.config.theme[\\"axes.facecolor\\"] = \\"lightgray\\" # Update the plot\'s theme to use the \\"dark\\" style so.Plot.config.theme.update(axes_style(\\"dark\\")) # Sync the plot\'s theme with matplotlib\'s global state so.Plot.config.theme.update(plt.rcParams) # Display Configuration # Change the format of the plot display to SVG so.Plot.config.display[\\"format\\"] = \\"svg\\" # Disable HiDPI scaling so.Plot.config.display[\\"hidpi\\"] = False # Set the scaling factor of the embedded plot image to 0.5 so.Plot.config.display[\\"scaling\\"] = 0.5 # Display the plot p.plot()"},{"question":"# Problem: Custom Build Distribution Command You are tasked with writing a custom Python command-line tool using the `distutils` package to automate the process of creating various built distributions (e.g., `tar`, `zip`, `rpm`). The tool should be able to: 1. Create multiple types of built distributions in one run. 2. Customize the build options using a configuration file. 3. Handle postinstallation scripts for Windows installations. # Requirements: 1. Implement a function `create_built_distribution` that takes the following parameters: - `setup_script_path` (str): Path to the `setup.py` script. - `dist_formats` (List[str]): List of distribution formats to create (e.g., `[\'tar\', \'zip\', \'rpm\']`). - `config_file` (str): Path to the configuration file (`setup.cfg`) for additional options. - `postinstall_script` (str): Path to the postinstallation script (if any, default is `None`). 2. The function should execute the appropriate `distutils` commands to create the specified built distributions. It should handle errors gracefully and log the progress. 3. If a `postinstall_script` is provided, ensure it is properly included and executed at the appropriate times (installation and uninstallation). # Input - `setup_script_path`: A string representing the path to the `setup.py` script. - `dist_formats`: A list of strings representing the distribution formats to create. - `config_file`: A string representing the path to the configuration file (`setup.cfg`). - `postinstall_script`: A string representing the path to the postinstallation script, which is optional and defaults to `None`. # Output - The function should create the required built distributions in the current directory and handle any provided configuration options and postinstallation scripts properly. # Constraints - Assume the working environment has all necessary tools and packages for creating the distributions (e.g., `rpm`, `zip` utilities). - The command should handle at least the following formats: `tar`, `zip`, and `rpm`. - Postinstallation scripts should only be considered for Windows (`msi` format). # Example Here is an example of how you might define and use the `create_built_distribution` function: ```python def create_built_distribution(setup_script_path, dist_formats, config_file, postinstall_script=None): # Your implementation here # Example usage setup_script_path = \'path/to/setup.py\' dist_formats = [\'tar\', \'zip\', \'rpm\'] config_file = \'path/to/setup.cfg\' postinstall_script = \'path/to/install_script.py\' create_built_distribution(setup_script_path, dist_formats, config_file, postinstall_script) ``` The function should ensure that each format specified in `dist_formats` is built correctly and that any provided configurations are applied.","solution":"import subprocess import os from typing import List def create_built_distribution(setup_script_path: str, dist_formats: List[str], config_file: str, postinstall_script: str = None): if not os.path.exists(setup_script_path) or not os.path.isfile(setup_script_path): raise FileNotFoundError(f\\"The setup script {setup_script_path} does not exist.\\") if not os.path.exists(config_file) or not os.path.isfile(config_file): raise FileNotFoundError(f\\"The configuration file {config_file} does not exist.\\") dist_format_commands = { \'tar\': [\'sdist\', \'--formats=gztar\', \'--config-file=\' + config_file], \'zip\': [\'sdist\', \'--formats=zip\', \'--config-file=\' + config_file], \'rpm\': [\'bdist_rpm\', \'--config-file=\' + config_file] } for dist_format in dist_formats: if dist_format in dist_format_commands: command = [\'python\', setup_script_path] + dist_format_commands[dist_format] try: subprocess.run(command, check=True) print(f\'Successfully created {dist_format} distribution.\') except subprocess.CalledProcessError as e: print(f\'Error occurred while creating {dist_format} distribution: {e}\') else: print(f\'Unsupported distribution format: {dist_format}\') if postinstall_script and os.name == \'nt\': try: msi_command = [\'python\', setup_script_path, \'bdist_msi\', \'--pre-hook\', postinstall_script, \'--config-file=\' + config_file] subprocess.run(msi_command, check=True) print(\'Successfully created msi distribution with postinstall script.\') except subprocess.CalledProcessError as e: print(f\'Error occurred while creating msi distribution: {e}\') elif postinstall_script: print(\'Postinstall script is provided but can only be used with msi format on Windows.\')"},{"question":"# Question: Analyzing Diamond Clarity with Seaborn You are tasked with analyzing the diamond dataset to gain insights into the distribution of diamond carat weights across different clarity grades using Seaborn. Your goal is to create a comprehensive visualization that can help users understand these distributions. The dataset `diamonds` contains the following columns: - `carat`: The weight of the diamond. - `clarity`: The clarity grade of the diamond. You need to perform the following tasks: 1. Create a plot that compares the carat distributions for each clarity grade. 2. Use jittering to ensure that overlapping data points are more visible. 3. Add a range marker that shows the interquartile range (25th to 75th percentile) for the carat weights within each clarity grade. 4. Ensure that the plot is clear and easy to interpret. # Input - The dataset `diamonds` loaded from `seaborn`. # Output - A Seaborn plot that includes jittered dots and interquartile range markers for the clarity grades. The x-axis should represent the carat weight, and the y-axis should represent the clarity grades. # Example ```python import seaborn.objects as so from seaborn import load_dataset diamonds = load_dataset(\\"diamonds\\") # Create the plot plot = ( so.Plot(diamonds, x=\\"carat\\", y=\\"clarity\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(y=.25)) ) plot.show() ``` Ensure that your plot meets the requirements and is well-labeled for clear interpretation. # Constraints - You must use the `seaborn.objects` interface. - Use only the functions and methods demonstrated within the provided documentation.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_diamond_clarity_distribution(): Creates a Seaborn plot showing the distribution of diamond carat weights across different clarity grades, with jittered dots and interquartile range markers. # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create the plot plot = ( so.Plot(diamonds, x=\\"carat\\", y=\\"clarity\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(y=0.25)) ) # Show the plot plot.show()"},{"question":"# PyTorch Coding Assessment Question Objective Demonstrate your understanding and proficiency with PyTorch operations including tensor creation, basic tensor manipulations, and random sampling functions. Problem **Implement a function `generate_and_process_tensor` that performs the following steps:** 1. **Tensor Creation**: - Create a 2D tensor of shape (6, 6) filled with zeros. - Create another tensor of the same shape filled with ones. - Stack these two tensors along a new dimension to form a 3D tensor of shape (2, 6, 6). 2. **Random Sampling**: - Replace the first slice (index 0) of the 3D tensor with random integers between 10 and 20. - Replace the second slice (index 1) of the 3D tensor with random floating-point numbers between 0 and 1. 3. **Tensor Manipulation**: - Compute the element-wise sum of these two slices and store the result in a new tensor. - Check if any element in this sum tensor is greater than a specified threshold (e.g., 15). Return the result as a boolean tensor of the same shape. Function Signature ```python import torch def generate_and_process_tensor(threshold: float) -> torch.Tensor: pass ``` Constraints - Do not use loops for any tensor operations. - Use in-place operations wherever feasible to optimize performance. - The function should be able to run on both CPU and CUDA devices if available. Example Usage ```python # Example usage: threshold = 15.0 result = generate_and_process_tensor(threshold) print(result) ``` Expected Outcome The function should return a boolean tensor of shape (6, 6) showing which elements in the sum tensor are greater than the given threshold. Evaluation Criteria - Correct implementation of tensor creation, stacking, and replacement. - Proper use of PyTorch random sampling functions. - Correct computation and comparison with the threshold. - Code efficiency and adherence to the constraints. - Functionality and correctness of the final output tensor.","solution":"import torch def generate_and_process_tensor(threshold: float) -> torch.Tensor: # Step 1: Tensor Creation tensor_zeros = torch.zeros((6, 6)) tensor_ones = torch.ones((6, 6)) # Stack tensors along a new dimension to form a 3D tensor of shape (2, 6, 6) stacked_tensor = torch.stack([tensor_zeros, tensor_ones]) # Step 2: Random Sampling # Replace the first slice with random integers between 10 and 20 stacked_tensor[0] = torch.randint(10, 21, (6, 6), dtype=torch.float32) # Replace the second slice with random floating-point numbers between 0 and 1 stacked_tensor[1] = torch.rand((6, 6)) # Step 3: Tensor Manipulation # Compute the element-wise sum of these two slices sum_tensor = stacked_tensor[0] + stacked_tensor[1] # Check if any element in this sum tensor is greater than the threshold result_tensor = sum_tensor > threshold return result_tensor"},{"question":"Objective Implement a Python function that benchmarks the execution time of another function using various clock sources from the `time` module and compares the results. Task Write a function `benchmark_execution_time(f, *args, **kwargs)`, where: - `f`: a function to be benchmarked. - `args`: positional arguments to be passed to the function `f`. - `kwargs`: keyword arguments to be passed to the function `f`. The function should measure the execution time of `f` using at least the following clock sources: - `time.time()` - `time.perf_counter()` - `time.process_time()` - `time.monotonic()` The function should return a dictionary with the execution times measured by each clock source. Expected Function Signature ```python def benchmark_execution_time(f, *args, **kwargs) -> dict: pass ``` Example Usage ```python import time def sample_function(delay): time.sleep(delay) # Benchmark the sample_function with a delay of 2 seconds result = benchmark_execution_time(sample_function, 2) print(result) # Expected output (times may vary slightly): # { # \'time.time\': 2.0xxx, # \'time.perf_counter\': 2.0xxx, # \'time.process_time\': 0.0xxx, # \'time.monotonic\': 2.0xxx # } ``` Constraints and Notes - The execution times should be returned as close as possible to the seconds with fractional parts. - For `time.process_time()`, remember it only includes the CPU time used by the current process. - Ensure the implementation works on different platforms (Unix, Windows). - Handle any potential exceptions that may arise during measuring time, such as function raising errors or invalid clock sources. You are required to use the relevant functions as follows to measure the time: - `time.time()` - `time.perf_counter()` - `time.process_time()` - `time.monotonic()` Performance Requirements - The solution should be efficient in terms of both time and space complexity, but given that it involves time measurement, it should focus more on accurate and consistent measurements.","solution":"import time def benchmark_execution_time(f, *args, **kwargs): Benchmarks the execution time of the provided function using various clock sources. Args: - f: The function to be benchmarked. - args: Positional arguments to pass to the function `f`. - kwargs: Keyword arguments to pass to the function `f`. Returns: A dictionary with the execution times measured by each clock source. results = {} # Measure execution time using time.time() start_time = time.time() try: f(*args, **kwargs) except: pass # Handle any exceptions that may arise during function execution end_time = time.time() results[\'time.time\'] = end_time - start_time # Measure execution time using time.perf_counter() start_time = time.perf_counter() try: f(*args, **kwargs) except: pass # Handle any exceptions that may arise during function execution end_time = time.perf_counter() results[\'time.perf_counter\'] = end_time - start_time # Measure execution time using time.process_time() start_time = time.process_time() try: f(*args, **kwargs) except: pass # Handle any exceptions that may arise during function execution end_time = time.process_time() results[\'time.process_time\'] = end_time - start_time # Measure execution time using time.monotonic() start_time = time.monotonic() try: f(*args, **kwargs) except: pass # Handle any exceptions that may arise during function execution end_time = time.monotonic() results[\'time.monotonic\'] = end_time - start_time return results"},{"question":"You are designing a simple web crawler, and you need to ensure your crawler respects the `robots.txt` rules of any site it visits. Implement a function that takes in the URL of a `robots.txt` file and a list of user-agent and URL pairs, and returns a dictionary where each user-agent has a dictionary of URLs it can and cannot fetch according to the `robots.txt` file. Function Signature ```python def analyze_robots_txt(robots_txt_url: str, user_agent_url_pairs: List[Tuple[str, str]]) -> Dict[str, Dict[str, bool]]: pass ``` Input - `robots_txt_url` (str): The URL pointing to the `robots.txt` file. - `user_agent_url_pairs` (List[Tuple[str, str]]): A list of tuples where each tuple contains a user-agent string and a URL string. Output - A dictionary where each key is a user-agent and its values are dictionaries with URLs as keys and `True` or `False` as values indicating whether the URL can be fetched by that user-agent according to the `robots.txt`. Example ```python robots_txt_url = \\"http://www.example.com/robots.txt\\" user_agent_url_pairs = [ (\\"Googlebot\\", \\"http://www.example.com/search\\"), (\\"Bingbot\\", \\"http://www.example.com/private\\"), (\\"*\\", \\"http://www.example.com/public\\") ] result = analyze_robots_txt(robots_txt_url, user_agent_url_pairs) ``` Expected Output: ```python { \\"Googlebot\\": { \\"http://www.example.com/search\\": False }, \\"Bingbot\\": { \\"http://www.example.com/private\\": False }, \\"*\\": { \\"http://www.example.com/public\\": True } } ``` Constraints - The function should properly handle network errors or cases where the `robots.txt` cannot be fetched. - Assume all URLs and user-agents provided in the list are correctly formatted strings. - Performance considerations: The function should efficiently handle up to 100 user-agent and URL pairs. # Additional Information You are allowed to use the `urllib.robotparser` module to read and parse the `robots.txt` file. It might be helpful to refer to the example provided in the documentation for basic use of the `RobotFileParser` class.","solution":"import urllib.robotparser from typing import List, Tuple, Dict def analyze_robots_txt(robots_txt_url: str, user_agent_url_pairs: List[Tuple[str, str]]) -> Dict[str, Dict[str, bool]]: Analyzes the robots.txt file of a website and determines if the given user-agent can or cannot fetch the given URLs. Parameters: robots_txt_url (str): The URL pointing to the robots.txt file. user_agent_url_pairs (List[Tuple[str, str]]): A list of tuples where each tuple contains a user-agent string and a URL string. Returns: Dict[str, Dict[str, bool]]: Dictionary where each user-agent has a dictionary of URLs it can and cannot fetch according to the robots.txt file. rp = urllib.robotparser.RobotFileParser() rp.set_url(robots_txt_url) rp.read() result = {} for user_agent, url in user_agent_url_pairs: if user_agent not in result: result[user_agent] = {} result[user_agent][url] = rp.can_fetch(user_agent, url) return result"},{"question":"Question: Implement a Persistent Shopping List using `shelve` # Overview You are tasked with implementing a simple shopping list application using the `shelve` module in Python. The application should allow adding, removing, and viewing items in the shopping list, and ensure that the list persists across program runs. # Requirements 1. **Functions to Implement**: - `add_item(filename: str, item: str) -> None`: - Adds an item to the shopping list stored in the specified shelve file. - `remove_item(filename: str, item: str) -> None`: - Removes an item from the shopping list stored in the specified shelve file. If the item does not exist, it should raise a `KeyError`. - `view_items(filename: str) -> list`: - Returns a list of all items currently in the shopping list stored in the specified shelve file. 2. **Constraints**: - The shopping list should support concurrent reads but not concurrent writes. - Ensure that the shopping list is properly saved to the shelf upon exiting the program. - Handle any potential errors gracefully and ensure the shelf is closed properly in case of exceptions. 3. **Persistence**: - The shopping list should persist even after the program exits and restarts. # Example Usage ```python # Assuming the implemented functions are in a file named `shopping_list.py` from shopping_list import add_item, remove_item, view_items # Add items to the shopping list add_item(\'shopping.db\', \'Bread\') add_item(\'shopping.db\', \'Milk\') add_item(\'shopping.db\', \'Eggs\') # View current items print(view_items(\'shopping.db\')) # Output: [\'Bread\', \'Milk\', \'Eggs\'] # Remove an item from the list remove_item(\'shopping.db\', \'Milk\') # View items after removal print(view_items(\'shopping.db\')) # Output: [\'Bread\', \'Eggs\'] ``` # Implementation Notes - Use the `shelve.open()` function to manage the shelf. - Use proper exception handling to ensure the shelf is closed properly. - Make use of the context manager provided by `shelve` to ensure the shelf is closed after operations.","solution":"import shelve def add_item(filename: str, item: str) -> None: Adds an item to the shopping list stored in the specified shelve file. with shelve.open(filename, writeback=True) as db: shopping_list = db.get(\'shopping_list\', []) if item not in shopping_list: shopping_list.append(item) db[\'shopping_list\'] = shopping_list def remove_item(filename: str, item: str) -> None: Removes an item from the shopping list stored in the specified shelve file. If the item does not exist, it should raise a KeyError. with shelve.open(filename, writeback=True) as db: if \'shopping_list\' in db: shopping_list = db[\'shopping_list\'] if item in shopping_list: shopping_list.remove(item) db[\'shopping_list\'] = shopping_list else: raise KeyError(f\\"Item \'{item}\' not found in the shopping list.\\") else: raise KeyError(f\\"Shopping list does not exist in the database.\\") def view_items(filename: str) -> list: Returns a list of all items currently in the shopping list stored in the specified shelve file. with shelve.open(filename) as db: return db.get(\'shopping_list\', [])"},{"question":"**Coding Assessment Question** **Objective:** You are tasked with demonstrating your understanding of scikit-learn by creating a synthetic dataset, applying pre-processing steps, training a machine learning model, and effectively handling potential warnings or issues during model training. **Problem Statement:** 1. **Create Synthetic Data**: Generate a synthetic classification dataset using scikit-learn\'s `make_classification` utility with the following specifications: - Number of samples: 1000 - Number of features: 20 (10 informative and 10 redundant) - Number of classes: 3 - Random state: 42 2. **Preprocess the Data**: - Convert the synthetic dataset into a pandas DataFrame. - Separate the dataset into features and targets. - Standardize the feature set using `StandardScaler` from scikit-learn. 3. **Train a Gradient Boosting Classifier**: - Split the standardized data into training and test sets (70% training, 30% test) with a fixed random state of 42 for reproducibility. - Initialize and train a `GradientBoostingClassifier` on the training set. - Evaluate the model\'s performance on the test set using accuracy as the metric. 4. **Handle Potential Warnings**: - Identify and handle any warnings related to the training process. - If any warnings are identified, modify the code to resolve them and ensure smooth training and evaluation. **Requirements and Constraints**: - Use only scikit-learn, pandas, and numpy libraries. - Your code should be self-contained and runnable without external datasets. - Ensure that all steps are modular and clearly documented. **Expected Input and Output**: - **Input**: None (the data generation should be part of the code) - **Output**: Accuracy of the trained model on the test set, and confirmation that any warnings were handled if they appeared. **Performance Considerations**: - The entire process (data generation, preprocessing, training, and evaluation) should complete within a reasonable time frame (a few seconds to a minute). ```python import numpy as np import pandas as pd from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingClassifier from sklearn.metrics import accuracy_score def main(): # Step 1: Create synthetic data X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=10, n_classes=3, random_state=42) # Step 2: Preprocess the data df = pd.DataFrame(X) target = pd.Series(y) scaler = StandardScaler() X_scaled = scaler.fit_transform(df) # Step 3: Split data and train model X_train, X_test, y_train, y_test = train_test_split(X_scaled, target, test_size=0.3, random_state=42) gbdt = GradientBoostingClassifier(random_state=42) gbdt.fit(X_train, y_train) # Step 4: Evaluate model y_pred = gbdt.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(\\"Model Accuracy: {:.2f}%\\".format(accuracy * 100)) if __name__ == \\"__main__\\": main() ``` **Note**: Ensure your solution runs without errors and handles any potential warnings that may arise during execution.","solution":"import numpy as np import pandas as pd from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingClassifier from sklearn.metrics import accuracy_score # Function for the entire machine learning pipeline def synthetic_dataset_pipeline(): # Step 1: Create synthetic data X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=10, n_classes=3, random_state=42) # Step 2: Preprocess the data df = pd.DataFrame(X) target = pd.Series(y) scaler = StandardScaler() X_scaled = scaler.fit_transform(df) # Step 3: Split data and train model X_train, X_test, y_train, y_test = train_test_split(X_scaled, target, test_size=0.3, random_state=42) gbdt = GradientBoostingClassifier(random_state=42) gbdt.fit(X_train, y_train) # Step 4: Evaluate model y_pred = gbdt.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"**Title: Custom Data Processor** **Objective:** Create a Python function that processes a list of dictionaries based on given conditions and extracts specific information. **Problem Statement:** You are given a list of dictionaries where each dictionary contains details of products. Each product has the following details: - `id`: an integer representing the unique identifier of the product. - `name`: a string representing the name of the product. - `price`: a float representing the price of the product. - `category`: a string representing the category of the product. Write a function `filter_and_sort_products` that takes the following inputs: 1. `products`: A list of dictionaries containing product details. 2. `price_threshold`: A float representing the price threshold. 3. `category_filter`: A string representing the category filter. The function should: 1. Filter out the products which have a price less than the `price_threshold`. 2. Filter out the products which do not belong to the `category_filter`. 3. Sort the remaining products by their `price` in descending order. 4. Return the sorted list of product names. **Function Signature:** ```python def filter_and_sort_products(products: list, price_threshold: float, category_filter: str) -> list: pass ``` **Input:** - `products` (list): A list of dictionaries, where each dictionary contains: - `id` (int) - `name` (str) - `price` (float) - `category` (str) - `price_threshold` (float): The price threshold to filter products. - `category_filter` (str): The category to filter products. **Output:** - list: A sorted list of product names (strings) that meet the criteria. **Example:** ```python products = [ {\'id\': 1, \'name\': \'Product A\', \'price\': 29.99, \'category\': \'Electronics\'}, {\'id\': 2, \'name\': \'Product B\', \'price\': 99.99, \'category\': \'Electronics\'}, {\'id\': 3, \'name\': \'Product C\', \'price\': 49.99, \'category\': \'Home\'}, {\'id\': 4, \'name\': \'Product D\', \'price\': 149.99, \'category\': \'Electronics\'}, ] price_threshold = 50.00 category_filter = \'Electronics\' print(filter_and_sort_products(products, price_threshold, category_filter)) # Output: [\'Product D\', \'Product B\'] ``` **Constraints:** - The `price` of the product will always be a non-negative float. - The `id` of the product will always be a positive integer. - The `category` of the product will always be a non-empty string. **Notes:** - Use Python built-in functions and best practices to complete the task. - Ensure your function is efficient and handles edge cases appropriately.","solution":"def filter_and_sort_products(products, price_threshold, category_filter): Filters and sorts products based on price and category. Parameters: products (list): A list of dictionaries containing product details. price_threshold (float): The price threshold to filter products. category_filter (str): The category to filter products. Returns: list: A sorted list of product names that meet the criteria. filtered_products = [product for product in products if product[\'price\'] >= price_threshold and product[\'category\'] == category_filter] sorted_products = sorted(filtered_products, key=lambda x: x[\'price\'], reverse=True) return [product[\'name\'] for product in sorted_products]"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of Python\'s \\"linecache\\" module by implementing a function that reads and processes multiple lines from a file efficiently. # Problem Statement You are given a text file containing Python code. Implement a function `extract_code_block(filename, start_lineno, end_lineno)` that extracts lines of code from the file, starting at `start_lineno` and ending at `end_lineno` (both inclusive). Your implementation should handle: 1. Fetching the specific lines using the `linecache.getline` function. 2. Clearing the cache with `linecache.clearcache` after processing. 3. Handling cases where lines may not exist (e.g., the file is shorter than expected or lines are missing). # Function Signature ```python def extract_code_block(filename: str, start_lineno: int, end_lineno: int) -> str: ``` # Input - `filename` (str): The path to the file containing Python code. - `start_lineno` (int): The line number to start extraction (1-based index). - `end_lineno` (int): The line number to end extraction (inclusive, 1-based index). # Output - `str`: A string containing the concatenated lines of code, or an empty string if the lines do not exist. Each line should include its terminating newline character. # Constraints - `start_lineno` and `end_lineno` are positive integers. - `start_lineno` <= `end_lineno`. # Example ```python # Suppose \'example.py\' contains the following lines: # 1: def foo(): # 2: return 42 # 3: # 4: x = foo() # 5: print(x) # # Then the function call: output = extract_code_block(\'example.py\', 2, 4) # should return: # \' return 42nnx = foo()n\' ``` # Notes - Ensure to use `linecache.getline()` for fetching lines. - Clean the cache using `linecache.clearcache()` after processing. - Handle edge cases where file lines are missing gracefully.","solution":"import linecache def extract_code_block(filename: str, start_lineno: int, end_lineno: int) -> str: Extract lines of code from a file starting from start_lineno to end_lineno. Parameters: filename (str): The path to the file containing Python code. start_lineno (int): The line number to start extraction. end_lineno (int): The line number to end extraction. Returns: str: A string containing the concatenated lines of code or an empty string if lines do not exist. lines = [] try: for lineno in range(start_lineno, end_lineno + 1): line = linecache.getline(filename, lineno) if line: lines.append(line) else: break finally: linecache.clearcache() return \'\'.join(lines)"},{"question":"**Objective:** Implement a function that executes a given Python module or script using the `runpy` module and processes the resulting global variables. # Task: Write a function `execute_python_code(source: str, use_path: bool, global_vars: dict = None) -> dict` that: 1. Executes Python code from either a module name or a script path. 2. Accepts a flag `use_path` to determine whether to use `runpy.run_module` or `runpy.run_path`. 3. Accepts a dictionary `global_vars` to pre-populate global variables before the code execution. 4. Returns a dictionary of the resulting global variables from the executed code. # Input: - `source` (str): The name of the module (if `use_path` is `False`) or the path to the script (if `use_path` is `True`). - `use_path` (bool): A flag indicating whether to use `runpy.run_module` (when `False`) or `runpy.run_path` (when `True`). - `global_vars` (dict): A dictionary of global variables to be pre-populated (default is `None`). # Output: - Returns a dictionary containing the global variables resulting from the execution of the Python code. # Constraints: - If `use_path` is `True`, `source` should be a valid filesystem path to a Python script. - If `use_path` is `False`, `source` should be a valid module name that can be resolved using the Python import system. - If the execution raises any exceptions, your function should catch them and return a dictionary with an error key and the exception message. # Example: ```python def execute_python_code(source: str, use_path: bool, global_vars: dict = None) -> dict: import runpy try: if use_path: result_globals = runpy.run_path(source, init_globals=global_vars) else: result_globals = runpy.run_module(source, init_globals=global_vars) return result_globals except Exception as e: return {\\"error\\": str(e)} # Example usage: # Assuming you have a script \'hello.py\' containing: # def greet(): # return \\"Hello, World!\\" # # You can call: # result = execute_python_code(\'path/to/hello.py\', use_path=True) # And result will contain: # {\'greet\': <function greet at 0x7f91d3b5a940>, \'__name__\': \'<run_path>\', ...} # Call with wrong module name result = execute_python_code(\'non_existent_module\', use_path=False) # Expected output: {\'error\': \'No module named non_existent_module\'} ``` The question requires a thorough understanding of both `runpy.run_module` and `runpy.run_path`, including their constraints and how global variables are handled. This assesses the student\'s ability to work with dynamic execution in Python and manage potential exceptions.","solution":"import runpy def execute_python_code(source: str, use_path: bool, global_vars: dict = None) -> dict: Executes Python code from either a module name or a script path. Args: source (str): The module name or script path. use_path (bool): Flag indicating to use run_module or run_path. global_vars (dict): A dictionary of global variables to pre-populate. Returns: dict: A dictionary of resulting global variables from the executed code. if global_vars is None: global_vars = {} try: if use_path: result_globals = runpy.run_path(source, init_globals=global_vars) else: result_globals = runpy.run_module(source, init_globals=global_vars) return result_globals except Exception as e: return {\\"error\\": str(e)}"},{"question":"# Coding Challenge Task You are given a collection of file paths and directories in various formats. Your task is to implement a function that processes these paths and returns their filesystem representations. If a path cannot be processed, raise an appropriate error. Function Signature ```python def process_fs_paths(paths: list) -> list: Given a list of paths, return a list containing their filesystem representation. Args: paths (list): A list of paths in str or bytes format or objects implementing os.PathLike. Returns: list: A list where each element is the filesystem representation of the corresponding input path. Raises: TypeError: If any path cannot be converted to a filesystem representation. pass ``` Input - `paths` : A list of file paths. Each path can be: - A string representing a path. - A bytes object representing a path. - An object implementing the `os.PathLike` interface. Output - Return a list of filesystem representations for the provided paths. Constraints - You should utilize the Python standard library wherever applicable. - Raise `TypeError` with a descriptive message if any path cannot be converted to a filesystem representation. Example ```python class PathLikeExample: def __init__(self, path): self.path = path def __fspath__(self): return self.path # Example paths paths = [ \\"/home/user/file.txt\\", b\\"/bin/bash\\", PathLikeExample(\\"/usr/local/bin\\"), 12345 ] try: result = process_fs_paths(paths) print(result) except TypeError as te: print(te) # Output # TypeError: Path 12345 cannot be converted to a filesystem representation. ``` Notes - Make sure your implementation handles different types of inputs and raises appropriate errors for unsupported types. - Use Python’s built-in `os.fspath` method for handling path-like objects.","solution":"import os def process_fs_paths(paths: list) -> list: Given a list of paths, returns a list containing their filesystem representation. Args: paths (list): A list of paths in str, bytes format or objects implementing os.PathLike. Returns: list: A list where each element is the filesystem representation of the corresponding input path. Raises: TypeError: If any path cannot be converted to a filesystem representation. result = [] for path in paths: try: fs_representation = os.fspath(path) # Converts a path-like object result.append(fs_representation) except TypeError: raise TypeError(f\\"Path {path} cannot be converted to a filesystem representation.\\") return result"},{"question":"Objective Demonstrate your understanding of CPU stream management and synchronization in PyTorch by implementing a function that performs concurrent matrix multiplications and synchronizes the results. Problem Statement You are tasked to write a function `perform_concurrent_matrix_multiplications(matrices, streams)` that performs concurrent matrix multiplications on the CPU using PyTorch streams. Each matrix multiplication operation should be associated with a specific stream. Also, the results should be synchronized before returning. Function Signature ```python def perform_concurrent_matrix_multiplications(matrices: List[Tuple[torch.Tensor, torch.Tensor]], streams: List[torch.cpu.Stream]) -> List[torch.Tensor]: pass ``` Input - `matrices`: A list of tuples where each tuple contains two PyTorch tensors, representing matrices. All matrices have compatible dimensions for matrix multiplication. - `streams`: A list of `torch.cpu.Stream` objects where each stream corresponds to one matrix multiplication task. Output - Returns a list of PyTorch tensors, each representing the result of a matrix multiplication from the input list, in the same order. Constraints - Each matrix multiplication should be performed in the stream provided in the corresponding index. - The function should handle the synchronization of streams. Example ```python import torch # Define matrices matrix_a = torch.randn(3, 3) matrix_b = torch.randn(3, 3) # Create streams stream1 = torch.cpu.Stream() stream2 = torch.cpu.Stream() # Call the function results = perform_concurrent_matrix_multiplications( [(matrix_a, matrix_b), (matrix_a, matrix_b)], [stream1, stream2] ) for result in results: print(result) ``` *Note:* Ensure that matrix multiplications exploit concurrency using streams and that results are synchronized before returning. Evaluation Criteria - Correctness: The function should correctly perform matrix multiplications concurrently. - Efficient synchronization: Properly manage stream synchronization to ensure correct order of operations. - Use of `torch.cpu` functionalities: Demonstrate understanding of stream management in PyTorch CPU.","solution":"import torch from typing import List, Tuple def perform_concurrent_matrix_multiplications(matrices: List[Tuple[torch.Tensor, torch.Tensor]], streams: List[torch.cuda.Stream]) -> List[torch.Tensor]: results = [] for (a, b), stream in zip(matrices, streams): with torch.cuda.stream(stream): result = torch.matmul(a, b) results.append(result) # Ensuring synchronization of streams before returning results for stream in streams: stream.synchronize() return results"},{"question":"Persistent Data Management with `sqlite3` **Objective:** Write a Python program to manage and update a persistent database of employee records using the `sqlite3` module. The program should demonstrate skills in creating, querying, updating, and deleting records, as well as handling custom data types. **Task:** 1. **Create Database and Table:** - Create a new SQLite database named `company.db`. - Inside this database, create a table named `employees` with the following columns: - `id` (INTEGER, Primary Key) - `name` (TEXT, Not Null) - `age` (INTEGER) - `department` (TEXT) - `salary` (REAL) 2. **Insert Records:** - Implement a function `insert_employee(id: int, name: str, age: int, department: str, salary: float) -> None` that inserts a new record into the `employees` table. 3. **Query Records:** - Implement a function `get_employees_by_department(department: str) -> list` that returns a list of all employee records in the specified department. Each record should be a dictionary with keys corresponding to column names. 4. **Update Records:** - Implement a function `update_employee_salary(id: int, new_salary: float) -> None` that updates the salary of the employee with the given `id`. 5. **Delete Records:** - Implement a function `delete_employee(id: int) -> None` that deletes the employee record with the given `id`. 6. **Custom Data Type Handling:** - Implement custom adapters and converters to handle a custom data type `HireDate` (a simple class representing the hiring date of an employee). - Modify the `employees` table to include a new column `hire_date` (TEXT) that stores the hiring date. - Implement a function `insert_employee_with_hire_date(id: int, name: str, age: int, department: str, salary: float, hire_date: HireDate) -> None` to insert employees with their hire date. - Implement a function `get_employee_hire_date(id: int) -> HireDate` to fetch the hiring date of the employee with the specified `id`. **Constraints:** - Use the `sqlite3` module for all database operations. - Ensure the `id` field is unique for each employee. - Handle all potential database exceptions gracefully. **Performance:** - Optimize database queries to minimize execution time, especially when handling a large number of records. **Class Definition for Custom Data Type:** ```python class HireDate: def __init__(self, year: int, month: int, day: int): self.year = year self.month = month self.day = day def __repr__(self): return f\\"{self.year}-{self.month:02d}-{self.day:02d}\\" ``` Write your solution below: ```python # Your code here ```","solution":"import sqlite3 from contextlib import closing class HireDate: def __init__(self, year: int, month: int, day: int): self.year = year self.month = month self.day = day def __repr__(self): return f\\"{self.year}-{self.month:02d}-{self.day:02d}\\" def adapt_hire_date(hire_date): return f\\"{hire_date.year}-{hire_date.month:02d}-{hire_date.day:02d}\\" def convert_hire_date(s): year, month, day = map(int, s.decode(\\"utf-8\\").split(\\"-\\")) return HireDate(year, month, day) sqlite3.register_adapter(HireDate, adapt_hire_date) sqlite3.register_converter(\\"HIRE_DATE\\", convert_hire_date) def create_database(): with closing(sqlite3.connect(\\"company.db\\", detect_types=sqlite3.PARSE_DECLTYPES)) as conn: with closing(conn.cursor()) as cursor: cursor.execute(\'\'\' CREATE TABLE IF NOT EXISTS employees ( id INTEGER PRIMARY KEY, name TEXT NOT NULL, age INTEGER, department TEXT, salary REAL, hire_date HIRE_DATE ) \'\'\') conn.commit() def insert_employee(id, name, age, department, salary): with closing(sqlite3.connect(\\"company.db\\")) as conn: with closing(conn.cursor()) as cursor: cursor.execute(\'\'\' INSERT INTO employees (id, name, age, department, salary) VALUES (?, ?, ?, ?, ?) \'\'\', (id, name, age, department, salary)) conn.commit() def get_employees_by_department(department): with closing(sqlite3.connect(\\"company.db\\")) as conn: with closing(conn.cursor()) as cursor: cursor.execute(\'SELECT * FROM employees WHERE department = ?\', (department,)) employees = cursor.fetchall() columns = [desc[0] for desc in cursor.description] return [dict(zip(columns, row)) for row in employees] def update_employee_salary(id, new_salary): with closing(sqlite3.connect(\\"company.db\\")) as conn: with closing(conn.cursor()) as cursor: cursor.execute(\'UPDATE employees SET salary = ? WHERE id = ?\', (new_salary, id)) conn.commit() def delete_employee(id): with closing(sqlite3.connect(\\"company.db\\")) as conn: with closing(conn.cursor()) as cursor: cursor.execute(\'DELETE FROM employees WHERE id = ?\', (id,)) conn.commit() def insert_employee_with_hire_date(id, name, age, department, salary, hire_date): with closing(sqlite3.connect(\\"company.db\\", detect_types=sqlite3.PARSE_DECLTYPES)) as conn: with closing(conn.cursor()) as cursor: cursor.execute(\'\'\' INSERT INTO employees (id, name, age, department, salary, hire_date) VALUES (?, ?, ?, ?, ?, ?) \'\'\', (id, name, age, department, salary, hire_date)) conn.commit() def get_employee_hire_date(id): with closing(sqlite3.connect(\\"company.db\\", detect_types=sqlite3.PARSE_DECLTYPES)) as conn: with closing(conn.cursor()) as cursor: cursor.execute(\'SELECT hire_date FROM employees WHERE id = ?\', (id,)) row = cursor.fetchone() return row[0] if row else None create_database()"},{"question":"# Manifold Learning with Locally Linear Embedding (LLE) Problem Statement You are provided with a high-dimensional dataset and tasked with implementing a non-linear dimensionality reduction using Locally Linear Embedding (LLE) from the scikit-learn library. You need to reduce the dimensions of this dataset and visualize the result in 2D space. Dataset Description The dataset is a CSV file named `high_dimensional_data.csv` with the following structure: - Each row represents a data point. - Each column (except the last) represents a different feature of the data. - The last column, \\"label\\", represents the class label of each data point. Requirements 1. **Input:** - `file_path` (string): The path to the dataset file. 2. **Output:** - A 2D plot of the reduced data points colored by their class labels. 3. **Constraints:** - Use `n_neighbors=10` and `n_components=2` for the LLE algorithm. - The input dataset can have potentially noisy and high-dimensional data. Function Signature ```python def lle_dimensionality_reduction(file_path: str) -> None: ``` Example Usage ```python file_path = \\"path/to/high_dimensional_data.csv\\" lle_dimensionality_reduction(file_path) ``` Additional Information You may use the following template to get started: ```python import numpy as np import pandas as pd from sklearn.manifold import LocallyLinearEmbedding import matplotlib.pyplot as plt def lle_dimensionality_reduction(file_path: str) -> None: # Load the dataset data = pd.read_csv(file_path) X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # Apply Locally Linear Embedding (LLE) lle = LocallyLinearEmbedding(n_neighbors=10, n_components=2, method=\'standard\') X_reduced = lle.fit_transform(X) # Create a 2D scatter plot plt.figure(figsize=(12, 8)) scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap=\'viridis\') plt.colorbar(scatter, label=\'Class Label\') plt.title(\\"2D Representation of High Dimensional Data using LLE\\") plt.xlabel(\\"Component 1\\") plt.ylabel(\\"Component 2\\") plt.show() ``` Notes * Ensure that your function handles the loading of the dataset correctly. * Plotting the reduced data points in 2D space as a scatter plot, with colors indicating different class labels, will help visualize the clusters formed by LLE.","solution":"import numpy as np import pandas as pd from sklearn.manifold import LocallyLinearEmbedding import matplotlib.pyplot as plt def lle_dimensionality_reduction(file_path: str) -> None: # Load the dataset data = pd.read_csv(file_path) X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # Apply Locally Linear Embedding (LLE) lle = LocallyLinearEmbedding(n_neighbors=10, n_components=2, method=\'standard\') X_reduced = lle.fit_transform(X) # Create a 2D scatter plot plt.figure(figsize=(12, 8)) scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap=\'viridis\') plt.colorbar(scatter, label=\'Class Label\') plt.title(\\"2D Representation of High Dimensional Data using LLE\\") plt.xlabel(\\"Component 1\\") plt.ylabel(\\"Component 2\\") plt.show()"},{"question":"# Question: You are tasked with implementing a custom dictionary-like container that has enhanced capabilities for specific numeric types. This custom dictionary should: 1. Only accept integer or floating-point keys. 2. Only accept integer, floating-point, or complex number values. 3. Provide a method to sum all values. 4. Provide a method to retrieve the average of all values. 5. Ensure that the type checks are rigorous to avoid invalid key-value entries. Implement the class `NumericDict` with the following specifications: Class: `NumericDict` - **Attributes**: - `data` (private dict): Stores the key-value pairs. - **Methods**: - `__init__(self)`: Initializes an empty dictionary. - `__setitem__(self, key, value)`: Adds a key-value pair to `data`, ensuring key and value types are correct. - `__getitem__(self, key)`: Retrieves the value associated with the key. - `__delitem__(self, key)`: Removes the key-value pair from `data`. - `sum_values(self) -> float`: Returns the sum of all the values in the dictionary. - `average_values(self) -> float`: Returns the average of all the values in the dictionary. If the dictionary is empty, return 0. Input: - Method calls to the `NumericDict` object. Output: - Return values from method calls or the result of the dictionary operations. Constraints: - Ensure that the key is either an `int` or `float`. - Ensure that the value is either an `int`, `float`, or `complex`. - Raise a `TypeError` if an invalid type for key or value is provided. - Ensure proper handling of summation and average calculations. Example: ```python # Creating an instance of NumericDict nd = NumericDict() # Adding items nd[1] = 2 nd[2.5] = 3.5 # Sum of values print(nd.sum_values()) # Output: 5.5 # Average of values print(nd.average_values()) # Output: 2.75 # Invalid key try: nd[\\"string_key\\"] = 4 # Should raise TypeError except TypeError as e: print(e) # Output: Invalid key type # Invalid value try: nd[3] = \\"string_value\\" # Should raise TypeError except TypeError as e: print(e) # Output: Invalid value type ```","solution":"class NumericDict: def __init__(self): self._data = {} def __setitem__(self, key, value): if not isinstance(key, (int, float)): raise TypeError(\\"Invalid key type\\") if not isinstance(value, (int, float, complex)): raise TypeError(\\"Invalid value type\\") self._data[key] = value def __getitem__(self, key): return self._data[key] def __delitem__(self, key): del self._data[key] def sum_values(self): return sum(self._data.values()) def average_values(self): if len(self._data) == 0: return 0 return self.sum_values() / len(self._data)"},{"question":"You are required to implement a function that processes a list of data items concurrently. Each data item represents a task that performs a computationally expensive operation. To efficiently manage the execution of these tasks, you need to use Python\'s `concurrent.futures` module to run the tasks in parallel, leveraging a pool of threads. Function Definition: ```python def process_data_concurrently(data_items): Processes a list of data items concurrently using ThreadPoolExecutor. Args: - data_items (List[Callable[[], Any]]): A list of callables, each representing an independent task. Returns: - List[Any]: A list of results from each callable, in the order they were submitted. pass ``` Input: - `data_items`: A list of callables (functions without parameters). Each callable performs a computationally expensive operation and returns a result. Output: - A list of results obtained by executing each callable in `data_items`, maintaining the order of the original list. Constraints: - You must use the `ThreadPoolExecutor` from the `concurrent.futures` module. - You should limit the number of threads to a maximum of 4 to avoid excessive resource usage. - Ensure that the function handles any exceptions raised during the execution of the callables and stores `None` as the result for the tasks that fail. Example: ```python import time # Example of a computationally expensive task def task1(): time.sleep(1) return \\"Task 1 completed\\" def task2(): time.sleep(2) return \\"Task 2 completed\\" def task3(): time.sleep(3) return \\"Task 3 completed\\" data_items = [task1, task2, task3] results = process_data_concurrently(data_items) print(results) # Output might look like: [\\"Task 1 completed\\", \\"Task 2 completed\\", \\"Task 3 completed\\"] ``` Instructions: 1. Implement the `process_data_concurrently` function. 2. Use the `ThreadPoolExecutor` to manage the concurrent execution of the tasks. 3. Ensure that the results are collected in the same order as the tasks were submitted. 4. Handle any possible exceptions that might occur during the execution of the tasks, and set the result to `None` for tasks that fail. This problem requires students to utilize the `concurrent.futures` module to manage concurrent execution effectively, demonstrating their understanding of threading, exception handling, and maintaining task order.","solution":"from concurrent.futures import ThreadPoolExecutor, as_completed def process_data_concurrently(data_items): Processes a list of data items concurrently using ThreadPoolExecutor. Args: - data_items (List[Callable[[], Any]]): A list of callables, each representing an independent task. Returns: - List[Any]: A list of results from each callable, in the order they were submitted. results = [] def safe_execute(func): try: return func() except: return None with ThreadPoolExecutor(max_workers=4) as executor: futures = {executor.submit(safe_execute, task): idx for idx, task in enumerate(data_items)} # Initialize results with None results = [None] * len(data_items) for future in as_completed(futures): result = future.result() index = futures[future] results[index] = result return results"},{"question":"You are required to create a script that opens a URL and handles cookies based on specified policies. The script should be able to: 1. Load a predefined set of cookies from a file. 2. Apply specific policies to decide which cookies to accept and return. 3. Extract cookies from HTTP responses and manage them appropriately. 4. Save the updated cookies back to the file. **Tasks:** 1. **Load Cookies from File:** - Define a function `load_cookies(file_path: str) -> http.cookiejar.MozillaCookieJar` that loads cookies from the specified file using the `MozillaCookieJar` class. 2. **Apply Cookie Policy:** - Define a function `create_policy(blocked_domains: list, allowed_domains: list) -> http.cookiejar.CookiePolicy` that creates a `DefaultCookiePolicy` with the provided blocked and allowed domains. - Example: Block `[\\"example.com\\"]` and allow `[\\"foo.com\\"]`. 3. **Open URL with Cookies:** - Define a function `open_url_with_cookies(url: str, policy: http.cookiejar.CookiePolicy, cookie_jar: http.cookiejar.CookieJar) -> None` that opens the specified URL using the given cookie policy and `CookieJar`. - Use `urllib.request` to handle the HTTP request and response. 4. **Save Updated Cookies:** - Define a function `save_cookies(cookie_jar: http.cookiejar.MozillaCookieJar, file_path: str) -> None` that saves the cookies back to the specified file. **Constraints:** - Assume the cookie file is in the Mozilla `cookies.txt` format. - Handle exceptions such as file not found or load errors gracefully. **Example Usage:** ```python if __name__ == \\"__main__\\": cookie_file = \\"path/to/cookies.txt\\" # Load cookies cookie_jar = load_cookies(cookie_file) # Create policy blocked_domains = [\\"example.com\\"] allowed_domains = [\\"foo.com\\"] policy = create_policy(blocked_domains, allowed_domains) # Open URL url = \\"http://example.com\\" open_url_with_cookies(url, policy, cookie_jar) # Save cookies save_cookies(cookie_jar, cookie_file) ``` **Expected Output:** - The script should open the URL while applying the specified cookie policies. - Cookies should be updated and saved back to the file. **Input/Output Format:** - **Function `load_cookies(file_path: str) -> http.cookiejar.MozillaCookieJar`** - Input: File path to the cookie file. - Returns: An instance of `MozillaCookieJar` loaded with cookies from the file. - **Function `create_policy(blocked_domains: list, allowed_domains: list) -> http.cookiejar.CookiePolicy`** - Input: Lists of blocked and allowed domains. - Returns: An instance of `DefaultCookiePolicy` with the specified domains. - **Function `open_url_with_cookies(url: str, policy: http.cookiejar.CookiePolicy, cookie_jar: http.cookiejar.CookieJar) -> None`** - Input: URL to open, cookie policy, and an instance of `CookieJar`. - Opens the URL and manages cookies according to the policy. - **Function `save_cookies(cookie_jar: http.cookiejar.MozillaCookieJar, file_path: str) -> None`** - Input: An instance of `MozillaCookieJar` and file path to save the cookies. - Saves the cookies back to the file.","solution":"import http.cookiejar import urllib.request def load_cookies(file_path: str) -> http.cookiejar.MozillaCookieJar: Loads cookies from the specified file using the MozillaCookieJar class. try: cookie_jar = http.cookiejar.MozillaCookieJar() cookie_jar.load(file_path, ignore_discard=True, ignore_expires=True) return cookie_jar except FileNotFoundError: print(f\\"File not found: {file_path}\\") return http.cookiejar.MozillaCookieJar() except Exception as e: print(f\\"An error occurred while loading cookies: {e}\\") return http.cookiejar.MozillaCookieJar() def create_policy(blocked_domains: list, allowed_domains: list) -> http.cookiejar.DefaultCookiePolicy: Creates a DefaultCookiePolicy with the provided blocked and allowed domains. class CustomCookiePolicy(http.cookiejar.DefaultCookiePolicy): def set_ok(self, cookie, request): domain = cookie.domain if domain in blocked_domains: return False if domain in allowed_domains: return True return super().set_ok(cookie, request) return CustomCookiePolicy() def open_url_with_cookies(url: str, policy: http.cookiejar.CookiePolicy, cookie_jar: http.cookiejar.CookieJar) -> None: Opens the specified URL using the given cookie policy and CookieJar. opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie_jar)) urllib.request.install_opener(opener) try: request = urllib.request.Request(url) response = urllib.request.urlopen(request) response.read() # Read response to process cookies except Exception as e: print(f\\"An error occurred while opening URL: {e}\\") def save_cookies(cookie_jar: http.cookiejar.MozillaCookieJar, file_path: str) -> None: Saves the cookies back to the specified file. try: cookie_jar.save(file_path, ignore_discard=True, ignore_expires=True) except Exception as e: print(f\\"An error occurred while saving cookies: {e}\\") if __name__ == \\"__main__\\": cookie_file = \\"path/to/cookies.txt\\" # Load cookies cookie_jar = load_cookies(cookie_file) # Create policy blocked_domains = [\\"example.com\\"] allowed_domains = [\\"foo.com\\"] policy = create_policy(blocked_domains, allowed_domains) # Open URL url = \\"http://example.com\\" open_url_with_cookies(url, policy, cookie_jar) # Save cookies save_cookies(cookie_jar, cookie_file)"},{"question":"Objective: To assess the student\'s understanding of socket programming in Python, including creating client and server sockets, handling blocking and non-blocking communication, and ensuring proper data transmission and reception. Description: You are to implement a simple client-server application using Python sockets. The server will accept multiple client connections and respond to specific commands from the clients. The implementation should demonstrate your understanding of both blocking and non-blocking sockets and proper handling of message sending and receiving. Requirements: 1. **Server Implementation**: - Create a server that listens on a specified port (`12345`). - The server should handle multiple client connections using threads. - The server should be able to receive messages from clients. Each message starts with a 4-byte integer that indicates the length of the message. - The server should process the following commands from clients: - `ECHO <message>`: The server should respond with the same message. - `TIME`: The server should respond with the current server time. - `CLOSE`: The server should close the connection with the client. - Ensure that the server uses non-blocking sockets for handling multiple clients efficiently. 2. **Client Implementation**: - Create a client that connects to the server on the specified port (`12345`). - The client should be able to send commands to the server as described above. - Implement the client to send properly formatted messages (4-byte length prefix followed by the command). - The client should handle the server\'s responses and print them to the console. - The client should close gracefully after sending the `CLOSE` command. Input/Output: - **Input**: Commands entered by the user on the client-side (`ECHO <message>`, `TIME`, `CLOSE`). - **Output**: Responses from the server printed to the console on the client-side. Constraints: - Use the Python `socket` library for socket creation and communication. - Utilize the `select` module for handling non-blocking sockets on the server-side. - Ensure that all messagesare properly formatted with a 4-byte length prefix. Performance: - The server should efficiently handle multiple client connections without blocking on any single client. Example: ```python # Client sends: \\"ECHO Hello, Server!\\" # Server responds: \\"Hello, Server!\\" # Client sends: \\"TIME\\" # Server responds: \\"22-10-2023 18:45:00\\" # Client sends: \\"CLOSE\\" # Server closes the connection. ``` Submission: Submit Python scripts for the server and client implementations along with a README file explaining how to run the server and client, and any assumptions made.","solution":"import socket import threading import struct import time from select import select def handle_client(client_socket): while True: try: # Read message length (4 bytes) raw_msglen = recvall(client_socket, 4) if not raw_msglen: break msglen = struct.unpack(\'>I\', raw_msglen)[0] # Read the actual message data message = recvall(client_socket, msglen).decode(\'utf-8\') if message.startswith(\\"ECHO\\"): _, msg = message.split(\\" \\", 1) send_message(client_socket, msg) elif message == \\"TIME\\": current_time = time.strftime(\\"%Y-%m-%d %H:%M:%S\\", time.localtime()) send_message(client_socket, current_time) elif message == \\"CLOSE\\": client_socket.close() break except Exception as e: print(f\\"Exception: {e}\\") client_socket.close() break def recvall(sock, n): data = b\'\' while len(data) < n: packet = sock.recv(n - len(data)) if not packet: return None data += packet return data def send_message(client_socket, msg): msg = msg.encode(\'utf-8\') msg = struct.pack(\'>I\', len(msg)) + msg client_socket.sendall(msg) def server(): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.bind((\'0.0.0.0\', 12345)) server_socket.listen(5) server_socket.setblocking(False) print(\\"Server started on port 12345\\") inputs = [server_socket] while True: readable, _, _ = select(inputs, [], []) for s in readable: if s is server_socket: client_socket, addr = server_socket.accept() print(f\\"Accepted connection from {addr}\\") client_socket.setblocking(False) inputs.append(client_socket) else: handle_client(s) inputs.remove(s) def client(): client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client_socket.connect((\'127.0.0.1\', 12345)) commands = [\\"ECHO Hello, Server!\\", \\"TIME\\", \\"CLOSE\\"] for command in commands: send_message(client_socket, command) if command == \\"CLOSE\\": break raw_msglen = recvall(client_socket, 4) if not raw_msglen: break msglen = struct.unpack(\'>I\', raw_msglen)[0] response = recvall(client_socket, msglen).decode(\'utf-8\') print(response) client_socket.close() if __name__ == \\"__main__\\": server_thread = threading.Thread(target=server) server_thread.start() time.sleep(1) client()"},{"question":"Objective: Create a simple terminal emulator using the `pty` module which allows users to interact with a Unix shell. The program should instantiate a pseudo-terminal and forward user input from the standard input to the shell, displaying output from the shell back to the standard output. Requirements: 1. **Function Signature:** Implement a function named `simple_terminal_emulator` with no parameters. 2. **Behavior:** The function should: * Create a pseudo-terminal using the `pty.fork()` function. * Handle input from the user and forward it to the child process executing the shell. * Read output from the child process and print it to the standard output. * Handle termination properly when the user exits the shell or sends an end-of-file (EOF) signal. Constraints: * The solution must use the `pty` module. * The function should handle character encoding properly, assuming UTF-8. * You may assume the Unix shell executable `/bin/sh`. Example Execution: The following interaction is a simplified example of what your emulator should handle: ``` Your program starts here. /bin/sh ls -l (total list of files and directories in the current directory) echo \\"Hello, World!\\" Hello, World! exit ``` Additional Information: * Consider using `os.read` and `os.write` for handling I/O between parent and child processes. * Make sure to manage the child process output properly to ensure it is displayed in real-time. * Perform error handling to deal with potential issues, such as invalid commands or user signals. This exercise will test your understanding of Unix system calls, process management, character encoding, and inter-process communication within the scope of Unix specific services.","solution":"import os import pty import sys import tty import select def simple_terminal_emulator(): # Fork the process to create a pseudo-terminal pid, fd = pty.fork() if pid == 0: # Child process # Replace the child process with the shell os.execlp(\'/bin/sh\', \'/bin/sh\') else: # Parent process try: while True: # Use select to wait for input from either the terminal (stdin) or the PTY (fd) rlist, _, _ = select.select([sys.stdin, fd], [], []) if sys.stdin in rlist: # Read from standard input and write to the PTY input_data = os.read(sys.stdin.fileno(), 1024) if not input_data: break os.write(fd, input_data) if fd in rlist: # Read from the PTY and write to standard output output_data = os.read(fd, 1024) if not output_data: break os.write(sys.stdout.fileno(), output_data) except OSError as e: print(f\\"Error: {e}\\") finally: os.close(fd) # It won\'t be feasible to write unit tests that interact with an external shell and expect consistent output # due to the real-time interactive nature of the terminal emulator. Instead, you can consider logging or # asserts within the function (not shown here) for an actual implementation in production."},{"question":"# Python Coding Assessment Question Objective You are tasked with developing a utility that uses the `gzip` module to compress and decompress files. This utility should include functionality to handle various scenarios and edge cases. Task Write a Python function `gzip_utility(action, input_path, output_path, compresslevel=9)` that performs gzip compression or decompression on the specified file. - `action`: A string, either \\"compress\\" or \\"decompress\\". - `input_path`: The path to the input file as a string. This file will be compressed or decompressed. - `output_path`: The path to the output file as a string. This is where the compressed or decompressed data will be written. - `compresslevel`: An optional integer ranging from 0 to 9 that controls the compression level. The default value is 9 (best compression). Function Requirements 1. **Compression**: - When `action` is \\"compress\\", the function should read the input file from `input_path`, compress its contents, and write the compressed data to `output_path`. - Handle edge cases where the input file does not exist or is not readable. - The resulting compressed file should include the input file\'s name in its gzip header. 2. **Decompression**: - When `action` is \\"decompress\\", the function should read the gzip-compressed file from `input_path`, decompress its contents, and write the decompressed data to `output_path`. - Handle edge cases where the input file is not a valid gzip file. - Preserve the modification timestamp of the decompressed file. 3. **Error Handling**: - Raise appropriate exceptions for invalid actions or file-related errors and provide informative error messages. 4. **Efficiency**: - Ensure that the utility handles large files efficiently without consuming excessive memory. Example Usage ```python try: # Compressing a file gzip_utility(\\"compress\\", \\"path/to/input.txt\\", \\"path/to/output.txt.gz\\") # Decompressing a file gzip_utility(\\"decompress\\", \\"path/to/output.txt.gz\\", \\"path/to/decompressed.txt\\") except Exception as e: print(f\\"Error: {e}\\") ``` Constraints - The function should be compatible with Python 3.10. - You may not use any third-party libraries other than `gzip` and the standard library. Performance Requirements - The utility should be able to compress and decompress files up to 1GB in a reasonable time frame.","solution":"import gzip import shutil import os def gzip_utility(action, input_path, output_path, compresslevel=9): Compresses or decompresses a file using gzip based on the specified action. :param action: str, \\"compress\\" or \\"decompress\\" :param input_path: str, path to the input file :param output_path: str, path to the output file :param compresslevel: int, optional, level of compression (0-9) :raises ValueError: If the action is neither \\"compress\\" nor \\"decompress\\" :raises FileNotFoundError: If the input file does not exist :raises OSError: If the input file is not a valid gzip file for decompression if action not in [\\"compress\\", \\"decompress\\"]: raise ValueError(\\"Action should be either \'compress\' or \'decompress\'\\") if not os.path.exists(input_path): raise FileNotFoundError(f\\"The file {input_path} does not exist\\") if action == \\"compress\\": with open(input_path, \'rb\') as f_in: with gzip.open(output_path, \'wb\', compresslevel=compresslevel) as f_out: shutil.copyfileobj(f_in, f_out) f_out.name = os.path.basename(input_path) # set original file name elif action == \\"decompress\\": try: with gzip.open(input_path, \'rb\') as f_in: with open(output_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) original_time = os.path.getmtime(input_path) os.utime(output_path, (original_time, original_time)) except OSError: raise OSError(f\\"The file {input_path} is not a valid gzip file\\") # End of solution"},{"question":"Design a function that performs probability calibration on an uncalibrated binary classifier using scikit-learn\'s `CalibratedClassifierCV` with both sigmoid and isotonic methods. The function should: 1. Fit the uncalibrated model on training data. 2. Calibrate the fitted model using `CalibratedClassifierCV` with both \'sigmoid\' and \'isotonic\' methods. 3. Demonstrate the calibration results using calibration curves. 4. Evaluate the Brier score and log loss for both calibrated models. Your function should have the following signature: ```python def calibrate_classifier(X_train, y_train, X_test, y_test, base_estimator): Calibrates a given binary classifier using sigmoid and isotonic methods and evaluates their performance. Parameters: - X_train (np.ndarray): Training features. - y_train (np.ndarray): Training labels. - X_test (np.ndarray): Test features. - y_test (np.ndarray): Test labels. - base_estimator: Untrained scikit-learn binary classifier. Returns: - results (dict): Dictionary containing Brier score and log loss for both sigmoid and isotonic methods. pass ``` The function should perform the following steps: 1. Fit the `base_estimator` on the training data (`X_train`, `y_train`). 2. Calibrate the fitted classifier using `CalibratedClassifierCV` with `method=\'sigmoid\'` and then with `method=\'isotonic\'`. 3. Plot calibration curves for both calibration methods on the test data. 4. Calculate and return the Brier score and log loss for both calibrated models on the test data. Constraints: - The `base_estimator` should be an instance of a scikit-learn binary classifier that has a `predict_proba` method. - You should use `CalibratedClassifierCV` with `cv=3` for cross-validation. - Ensure that the plots are clear and well-labeled. You can use the following imports and utility functions to help you: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.calibration import CalibratedClassifierCV, calibration_curve from sklearn.metrics import brier_score_loss, log_loss def plot_calibration_curve(est, X_test, y_test, name): Plots the calibration curve for the given estimator. prob_pos = est.predict_proba(X_test)[:, 1] fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_pos, n_bins=10) plt.plot(mean_predicted_value, fraction_of_positives, \\"s-\\", label=\\"%s\\" % (name, )) plt.plot([0, 1], [0, 1], \\"k:\\", label=\\"Perfectly calibrated\\") # Example Usage: # plot_calibration_curve(calibrated_clf, X_test, y_test, \'Calibrated Classifier\') # plt.show() ``` Test your function with data generated using `sklearn.datasets.make_classification`.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.calibration import CalibratedClassifierCV, calibration_curve from sklearn.metrics import brier_score_loss, log_loss def plot_calibration_curve(est, X_test, y_test, name): Plots the calibration curve for the given estimator. prob_pos = est.predict_proba(X_test)[:, 1] fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_pos, n_bins=10) plt.plot(mean_predicted_value, fraction_of_positives, \\"s-\\", label=\\"%s\\" % (name, )) plt.plot([0, 1], [0, 1], \\"k:\\", label=\\"Perfectly calibrated\\") plt.xlabel(\'Mean predicted value\') plt.ylabel(\'Fraction of positives\') plt.title(\'Calibration curve\') plt.legend() plt.grid() def calibrate_classifier(X_train, y_train, X_test, y_test, base_estimator): Calibrates a given binary classifier using sigmoid and isotonic methods and evaluates their performance. Parameters: - X_train (np.ndarray): Training features. - y_train (np.ndarray): Training labels. - X_test (np.ndarray): Test features. - y_test (np.ndarray): Test labels. - base_estimator: An instance of an untrained scikit-learn binary classifier. Returns: - results (dict): Dictionary containing Brier score and log loss for both sigmoid and isotonic methods. # Fit the base estimator base_estimator.fit(X_train, y_train) # Calibrate using sigmoid method calibrated_clf_sigmoid = CalibratedClassifierCV(base_estimator, method=\'sigmoid\', cv=3) calibrated_clf_sigmoid.fit(X_train, y_train) # Calibrate using isotonic method calibrated_clf_isotonic = CalibratedClassifierCV(base_estimator, method=\'isotonic\', cv=3) calibrated_clf_isotonic.fit(X_train, y_train) # Plot calibration curves plt.figure(figsize=(10, 5)) plt.subplot(1, 2, 1) plot_calibration_curve(calibrated_clf_sigmoid, X_test, y_test, \'Sigmoid\') plt.subplot(1, 2, 2) plot_calibration_curve(calibrated_clf_isotonic, X_test, y_test, \'Isotonic\') plt.tight_layout() plt.show() # Evaluate Brier score and log loss prob_pos_sigmoid = calibrated_clf_sigmoid.predict_proba(X_test)[:, 1] prob_pos_isotonic = calibrated_clf_isotonic.predict_proba(X_test)[:, 1] results = { \'sigmoid\': { \'brier_score\': brier_score_loss(y_test, prob_pos_sigmoid), \'log_loss\': log_loss(y_test, prob_pos_sigmoid) }, \'isotonic\': { \'brier_score\': brier_score_loss(y_test, prob_pos_isotonic), \'log_loss\': log_loss(y_test, prob_pos_isotonic) } } return results"},{"question":"Objective: Design a custom descriptor in Python to manage the attributes of a class, ensuring that attribute values adhere to specified type and range constraints. Additionally, log all accesses and modifications to the attributes. Problem Statement: You are required to implement a `ValidatedAttribute` descriptor class and a target class `Product` that uses this descriptor to manage its attributes. Requirements: 1. **Descriptor Class**: - Implement a descriptor named `ValidatedAttribute` that ensures the following: - The attribute value must be of a specified data type. - If applicable, the attribute value must lie within a specified range (inclusive). - All accesses to the attribute should be logged. - All modifications to the attribute should be logged. 2. **Target Class**: - Implement a class named `Product` with the following attributes managed by the `ValidatedAttribute` descriptor: - `name`: A string that should not be empty. - `price`: A float that should be greater than or equal to 0.0. - `quantity`: An integer that should be non-negative. Input and Output: - The `ValidatedAttribute` descriptor should log a message to the console for every access and modification to any managed attribute. - If a constraint is violated, raise an appropriate exception (`TypeError` for type mismatches, `ValueError` for range violations). Example: ```python # Define the descriptor class ValidatedAttribute: def __init__(self, name, expected_type, min_value=None, max_value=None): self.name = name self.expected_type = expected_type self.min_value = min_value self.max_value = max_value self.private_name = \\"_\\" + name def __get__(self, instance, owner): value = getattr(instance, self.private_name) print(f\\"Accessing {self.name}: {value}\\") return value def __set__(self, instance, value): if not isinstance(value, self.expected_type): raise TypeError(f\\"Expected {self.name} to be {self.expected_type.__name__}, got {type(value).__name__}\\") if self.min_value is not None and value < self.min_value: raise ValueError(f\\"Expected {self.name} to be at least {self.min_value}, got {value}\\") if self.max_value is not None and value > self.max_value: raise ValueError(f\\"Expected {self.name} to be at most {self.max_value}, got {value}\\") print(f\\"Setting {self.name} to {value}\\") setattr(instance, self.private_name, value) # Define the target class class Product: name = ValidatedAttribute(\\"name\\", str) price = ValidatedAttribute(\\"price\\", float, min_value=0.0) quantity = ValidatedAttribute(\\"quantity\\", int, min_value=0) def __init__(self, name, price, quantity): self.name = name self.price = price self.quantity = quantity # Example usage try: p = Product(\\"Widget\\", 19.99, 100) print(p.name) p.price = 25.0 p.quantity = -5 # This should raise a ValueError except (TypeError, ValueError) as e: print(e) ``` Constraints: - Attribute values must conform to the specified type. - Attribute values must lie within the specified range if applicable. - All accesses and modifications to the attributes must be logged.","solution":"class ValidatedAttribute: def __init__(self, name, expected_type, min_value=None, max_value=None): self.name = name self.expected_type = expected_type self.min_value = min_value self.max_value = max_value self.private_name = \\"_\\" + name def __get__(self, instance, owner): value = getattr(instance, self.private_name) print(f\\"Accessing {self.name}: {value}\\") return value def __set__(self, instance, value): if not isinstance(value, self.expected_type): raise TypeError(f\\"Expected {self.name} to be {self.expected_type.__name__}, got {type(value).__name__}\\") if self.expected_type is str and not value: raise ValueError(f\\"Expected {self.name} to be a non-empty string\\") if self.min_value is not None and value < self.min_value: raise ValueError(f\\"Expected {self.name} to be at least {self.min_value}, got {value}\\") if self.max_value is not None and value > self.max_value: raise ValueError(f\\"Expected {self.name} to be at most {self.max_value}, got {value}\\") print(f\\"Setting {self.name} to {value}\\") setattr(instance, self.private_name, value) class Product: name = ValidatedAttribute(\\"name\\", str) price = ValidatedAttribute(\\"price\\", float, min_value=0.0) quantity = ValidatedAttribute(\\"quantity\\", int, min_value=0) def __init__(self, name, price, quantity): self.name = name self.price = price self.quantity = quantity"},{"question":"# Context: Unix-based systems use system logs to manage and keep a record of various activities and processes. These logs are critical for system administrators to track and troubleshoot system activities. The `syslog` module in Python provides an interface to the Unix syslog library routines, enabling Python scripts to submit log entries to the system log. # Task: You are required to implement a logging utility in Python that uses the `syslog` module. This utility should: 1. Log a message with a specified log level. 2. Have the ability to log messages from different categories (e.g., Authentication, System, Configuration). 3. Utilize appropriate log facilities for these categories. # Requirements: 1. Implement the function `custom_syslog(log_level: str, category: str, message: str) -> None`. - `log_level`: indicates the severity of the log message (INFO, WARNING, ERROR). - `category`: specifies the log category, could be one of: - \'AUTH\' (Authentication-related) - \'SYS\' (System-related) - \'CFG\' (Configuration-related) - `message`: the actual log message string to be recorded. 2. The function should map the categories to the appropriate syslog facilities: - \'AUTH\' category should use `syslog.LOG_AUTH` - \'SYS\' category should use `syslog.LOG_SYSLOG` - \'CFG\' category should use `syslog.LOG_DAEMON` 3. The function should convert the log levels to `syslog` priorities: - \'INFO\' should map to `syslog.LOG_INFO` - \'WARNING\' should map to `syslog.LOG_WARNING` - \'ERROR\' should map to `syslog.LOG_ERR` 4. Ensure that the correct syslog facility and priority are used when logging the message. 5. Example usage: ```python custom_syslog(\'INFO\', \'SYS\', \'System started successfully.\') custom_syslog(\'ERROR\', \'AUTH\', \'Failed login attempt detected.\') custom_syslog(\'WARNING\', \'CFG\', \'Configuration file missing expected entries.\') ``` # Constraints: 1. All log levels, categories, and message values must be valid strings. 2. If an invalid category or log level is provided, the function should raise a `ValueError` with an appropriate error message. 3. Efficient performance is key; make sure the function operates in constant time with respect to input size. # Implementation: Use the `syslog` module\'s `syslog.openlog` to open a connection to the system logger and `syslog.syslog` to log the messages. You can assume the environment where this script is run has a functioning syslog daemon. ```python import syslog def custom_syslog(log_level: str, category: str, message: str) -> None: # Your code here pass ``` # Notes: - Be sure to handle any edge cases and validate inputs appropriately. - The provided example calls should align with how the system logger records the messages, reflecting appropriate syslog facilities and priorities.","solution":"import syslog def custom_syslog(log_level: str, category: str, message: str) -> None: log_levels = { \'INFO\': syslog.LOG_INFO, \'WARNING\': syslog.LOG_WARNING, \'ERROR\': syslog.LOG_ERR } categories = { \'AUTH\': syslog.LOG_AUTH, \'SYS\': syslog.LOG_SYSLOG, \'CFG\': syslog.LOG_DAEMON } if log_level not in log_levels: raise ValueError(f\\"Invalid log level: {log_level}. Valid options: {\', \'.join(log_levels.keys())}\\") if category not in categories: raise ValueError(f\\"Invalid category: {category}. Valid options: {\', \'.join(categories.keys())}\\") priority = log_levels[log_level] facility = categories[category] syslog.openlog(facility=facility) syslog.syslog(priority, message) syslog.closelog()"},{"question":"# Pandas DataFrame Manipulation and Analysis Problem Statement You are given a task to analyze the memory usage of a pandas DataFrame while also implementing some data transformations. Your goal is to ensure that you effectively manage memory, handle missing data, and preprocess the DataFrame correctly without causing unintended changes during transformations. Requirements 1. **Load DataFrame**: - Create a DataFrame with 6 columns and 2000 rows using specified dtypes. The columns should be: - `int_column` with integer values - `float_column` with float values - `datetime_column` with random datetime values - `timedelta_column` with random timedeltas - `complex_column` with random complex numbers - `object_column` with random strings 2. **Memory Usage Analysis**: - Display the memory usage of the DataFrame with and without deep introspection. 3. **Condition Checks**: - Implement a check to determine if any value in `int_column` is greater than 50. 4. **Transformations**: - Apply a transformation to increment all integer values by 1 but handle the transformation correctly without mutating the DataFrame during iteration. 5. **Handling Missing Values**: - Introduce some missing values in `float_column` and then handle these missing values by replacing them with the mean of the column. 6. **Custom Function Application**: - Apply a custom function to `object_column` to capitalize all the strings without mutating the DataFrame during the operation. 7. **Expected Output**: - Print the memory usage report of the DataFrame (both default and deep). - Print whether there are any values greater than 50 in `int_column`. - Display the transformed DataFrame and the DataFrame after handling missing values. Instructions - Implement the function `analyze_and_transform_dataframe()` which takes no input and performs all the tasks described above. - Ensure the code is well-documented and error-handling is performed where necessary. Example ```python import pandas as pd import numpy as np def analyze_and_transform_dataframe(): # Step 1: Creating the DataFrame dtypes = { \'int_column\': \'int64\', \'float_column\': \'float64\', \'datetime_column\': \'datetime64[ns]\', \'timedelta_column\': \'timedelta64[ns]\', \'complex_column\': \'complex128\', \'object_column\': \'object\', } # Populate the DataFrame data = { \'int_column\': np.random.randint(0, 100, size=2000), \'float_column\': np.random.random(size=2000) * 100, \'datetime_column\': pd.date_range(\'2021-01-01\', periods=2000, freq=\'H\'), \'timedelta_column\': pd.to_timedelta(np.random.randint(1, 100, size=2000), unit=\'h\'), \'complex_column\': np.random.random(size=2000) + 1j * np.random.random(size=2000), \'object_column\': np.random.choice([\'foo\', \'bar\', \'baz\'], size=2000) } df = pd.DataFrame(data) # Step 2: Memory Usage Analysis print(\\"Memory usage (default):\\") print(df.info()) print(\\"Memory usage (deep introspection):\\") print(df.info(memory_usage=\\"deep\\")) # Step 3: Condition Check for int_column if df[\'int_column\'].any() > 50: print(\\"There are values greater than 50 in int_column\\") # Step 4: Value Transformation def increment_int_values(s): return s.copy() + 1 df[\'int_column\'] = df[\'int_column\'].apply(increment_int_values) # Step 5: Handle Missing Values df.loc[np.random.choice(df.index, 50), \'float_column\'] = np.nan df[\'float_column\'].fillna(df[\'float_column\'].mean(), inplace=True) # Step 6: Custom Function Application def capitalize_strings(s): return s.str.capitalize() df[\'object_column\'] = df[\'object_column\'].apply(capitalize_strings) # Display transformed DataFrame print(\\"Transformed DataFrame:\\") print(df.head()) analyze_and_transform_dataframe() ``` **Notes**: - You may use the random seed to ensure repeatability for testing purposes. - Ensure your implementations manage memory efficiently, especially when handling large DataFrames.","solution":"import pandas as pd import numpy as np def analyze_and_transform_dataframe(): # Step 1: Creating the DataFrame with specified dtypes dtypes = { \'int_column\': \'int64\', \'float_column\': \'float64\', \'datetime_column\': \'datetime64[ns]\', \'timedelta_column\': \'timedelta64[ns]\', \'complex_column\': \'complex128\', \'object_column\': \'object\', } # Populate the DataFrame with random data np.random.seed(0) # For reproducibility data = { \'int_column\': np.random.randint(0, 100, size=2000), \'float_column\': np.random.rand(2000) * 100, \'datetime_column\': pd.date_range(\'2021-01-01\', periods=2000, freq=\'H\'), \'timedelta_column\': pd.to_timedelta(np.random.randint(1, 100, size=2000), unit=\'h\'), \'complex_column\': np.random.rand(2000) + 1j * np.random.rand(2000), \'object_column\': np.random.choice([\'foo\', \'bar\', \'baz\'], size=2000), } df = pd.DataFrame(data) # Step 2: Memory Usage Analysis print(\\"Memory usage (default):\\") print(df.info()) print(\\"Memory usage (deep introspection):\\") print(df.info(memory_usage=\'deep\')) # Step 3: Condition Check for int_column has_values_gt_50 = (df[\'int_column\'] > 50).any() print(\\"There are values greater than 50 in int_column:\\", has_values_gt_50) # Step 4: Value Transformation df[\'int_column\'] = df[\'int_column\'] + 1 # Step 5: Handle Missing Values # Introduce some NaN values in \'float_column\' nan_indices = np.random.choice(df.index, 50, replace=False) df.loc[nan_indices, \'float_column\'] = np.nan # Replace NaNs with the mean of the column df[\'float_column\'].fillna(df[\'float_column\'].mean(), inplace=True) # Step 6: Custom Function Application df[\'object_column\'] = df[\'object_column\'].str.capitalize() # Display transformed DataFrame print(\\"Transformed DataFrame (first 5 rows):\\") print(df.head()) return df # Return the DataFrame for potential further analysis and testing"},{"question":"# Subprocess Management with `subprocess.run()` and `subprocess.Popen` The \\"subprocess\\" module in Python allows for spawning new processes, connecting to their input/output/error pipes, and obtaining their return codes. It can replace older functions like `os.system` and `os.spawn*`. This task will assess your understanding of handling subprocesses with this module. # Task Description You are required to implement two functions using the `subprocess` module: 1. `run_shell_command(command: str) -> str` 2. `execute_pipeline(commands: List[List[str]]) -> Tuple[str, str]` Function 1: `run_shell_command` This function runs a single shell command and captures its output. - **Input:** - `command` (str): A string representing the shell command to be executed. This command should be executed in the shell. - **Output:** - Returns the standard output (stdout) of the executed command as a string. - **Constraints:** - If the command fails (non-zero exit status), the function should raise a `subprocess.CalledProcessError`. - **Example:** ```python run_shell_command(\\"echo \'Hello, World!\'\\") # Output: \\"Hello, World!n\\" ``` Function 2: `execute_pipeline` This function sets up a pipeline of commands where the output of one command is the input to the next. - **Input:** - `commands` (List[List[str]]): A list of commands, where each command is represented as a list of strings. Each command will be executed as a part of a pipeline. - **Output:** - Returns a tuple containing the standard output and standard error of the final command in the pipeline. - **Constraints:** - If any command in the pipeline fails (non-zero exit status), the function should raise a `subprocess.CalledProcessError`. - **Example:** ```python execute_pipeline([[\\"echo\\", \\"Hello\\"], [\\"grep\\", \\"H\\"], [\\"awk\\", \\"{print 1}\\"]]) # Output: (\\"Hellon\\", \\"\\") ``` # Requirements - Use the `subprocess.run()` function for `run_shell_command()`. - Use the `subprocess.Popen` class for `execute_pipeline()`. - Be mindful of capturing standard output and error streams. - Handle errors appropriately to reflect realistic and reliable subprocess management. # Performance Considerations - Ensure that the functions terminate gracefully even if the commands produce a large amount of output. - Manage the subprocess efficiently to avoid deadlocks or resource leaks. # Provided Helper Code ```python import subprocess def run_shell_command(command: str) -> str: # Your code here pass def execute_pipeline(commands: List[List[str]]) -> Tuple[str, str]: # Your code here pass ``` Submit your implementation for the two functions including necessary imports and any test cases you use for validation.","solution":"import subprocess from typing import List, Tuple def run_shell_command(command: str) -> str: Runs a single shell command and captures its output. Parameters: command (str): A string representing the shell command to be executed. Returns: str: The standard output (stdout) of the executed command as a string. result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True) return result.stdout def execute_pipeline(commands: List[List[str]]) -> Tuple[str, str]: Sets up a pipeline of commands where the output of one command is the input to the next. Parameters: commands (List[List[str]]): A list of commands, where each command is represented as a list of strings. Returns: Tuple[str, str]: A tuple containing the standard output and standard error of the final command in the pipeline. previous_process = None for command in commands: if previous_process is None: previous_process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) else: previous_process = subprocess.Popen(command, stdin=previous_process.stdout, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) stdout, stderr = previous_process.communicate() if previous_process.returncode != 0: raise subprocess.CalledProcessError(previous_process.returncode, commands[-1], output=stdout, stderr=stderr) return stdout, stderr"},{"question":"# Secure User Login Prompt Objective Implement a secure login prompt function using the `getpass` module. The function should validate the user\'s username and password, and provide a secure and informative experience. Requirements 1. **Function Signature**: ```python def secure_login(valid_users: dict) -> bool: ``` 2. **Parameters**: - `valid_users` (dict): A dictionary where keys are usernames (strings) and values are passwords (strings). 3. **Functionality**: - Prompt the user for their username using the `getpass.getuser()` function as a suggestion. - Prompt the user for their password using the `getpass.getpass()` function. - Validate the entered username and password against the `valid_users` dictionary. - Return `True` if the username and password are correct, otherwise return `False`. - Handle cases where the password input might be echoed by issuing a `getpass.GetPassWarning` warning. 4. **Output**: - The function should return a boolean indicating whether the login was successful. Constraints - Assume the `valid_users` dictionary is pre-populated and provided as an argument to the function and contains no sensitive information. - You must handle all potential exceptions gracefully and inform the user of any errors encountered during the input process in a user-friendly manner. Example ```python valid_users = { \'alice\': \'wonderland\', \'bob\': \'builder\', \'charlie\': \'chocolate\' } print(secure_login(valid_users)) # Output depends on user input ``` Notes - You may use additional standard Python libraries if necessary. - Ensure that the solution is secure and does not expose any sensitive information during the process.","solution":"import getpass def secure_login(valid_users: dict) -> bool: try: username_hint = getpass.getuser() username = input(f\\"Username (suggested: {username_hint}): \\") or username_hint password = getpass.getpass(\\"Password: \\") if username in valid_users and valid_users[username] == password: return True else: return False except getpass.GetPassWarning: print(\\"Warning: Password input may have been echoed.\\") return False except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"# Question: Implement a Generic Function with Type Hinting You are tasked with implementing a generic function that merges two ordered lists into a single ordered list. The function should be able to handle lists of any comparable data type (e.g., integers, floats, strings) and should enforce type constraints using Python\'s type hinting system. Function Signature ```python from typing import List, TypeVar T = TypeVar(\'T\') def merge_sorted_lists(list1: List[T], list2: List[T]) -> List[T]: pass ``` Input - `list1` (List[T]): The first ordered list of elements. - `list2` (List[T]): The second ordered list of elements. Output - Returns a single merged and ordered list containing all elements from `list1` and `list2`. Example ```python assert merge_sorted_lists([1, 3, 5], [2, 4, 6]) == [1, 2, 3, 4, 5, 6] assert merge_sorted_lists([\\"apple\\", \\"orange\\"], [\\"banana\\", \\"pear\\"]) == [\\"apple\\", \\"banana\\", \\"orange\\", \\"pear\\"] ``` Constraints - Both input lists are already ordered. - The elements of the lists are comparable (i.e., they support comparison operators like <, <=, >, >=). Requirements - Use Python\'s typing module to define type hints. - Use the GenericAlias feature to enforce type constraints. Additional Information Make sure the function is efficient with a time complexity of O(n + m), where n is the length of `list1` and m is the length of `list2`. Your task is to complete the implementation of the `merge_sorted_lists` function as per the specification above.","solution":"from typing import List, TypeVar T = TypeVar(\'T\') def merge_sorted_lists(list1: List[T], list2: List[T]) -> List[T]: Merges two ordered lists into one ordered list. Parameters: list1 (List[T]): The first ordered list. list2 (List[T]): The second ordered list. Returns: List[T]: A single merged and ordered list containing all elements from list1 and list2. merged_list = [] i, j = 0, 0 # Merge the two lists while both have elements while i < len(list1) and j < len(list2): if list1[i] <= list2[j]: merged_list.append(list1[i]) i += 1 else: merged_list.append(list2[j]) j += 1 # Append remaining elements from list1, if any while i < len(list1): merged_list.append(list1[i]) i += 1 # Append remaining elements from list2, if any while j < len(list2): merged_list.append(list2[j]) j += 1 return merged_list"},{"question":"# PyTorch Coding Assessment Question **Objective**: Demonstrate your understanding of tensor creation, manipulation, mathematical operations, and GPU acceleration in PyTorch. --- Problem: Tensor Manipulation and GPU Acceleration You are given a set of tasks that involve creating tensors, performing mathematical operations, and utilizing GPU acceleration if available. Implement the following functions: 1. **create_and_square_tensors(N, M, device)**: - **Input**: - `N`: Integer, the number of rows. - `M`: Integer, the number of columns. - `device`: String, either \'cpu\' or \'cuda\'. - **Output**: A tensor of shape (N, M) where each element is the square of its index position and the tensor is moved to the specified device. - **Details**: Create a tensor of shape `(N, M)` with elements `t[i, j] = (i * M + j) ** 2` and move it to the given device (\'cpu\' or \'cuda\'). 2. **elementwise_operations(A, B)**: - **Input**: - `A`: A tensor of shape (P, Q). - `B`: A tensor of shape (P, Q). - **Output**: A dictionary with: - `\'add\'`: Tensor resulting from elementwise addition of `A` and `B`. - `\'sub\'`: Tensor resulting from elementwise subtraction of `A` and `B`. - `\'mul\'`: Tensor resulting from elementwise multiplication of `A` and `B`. - `\'div\'`: Tensor resulting from elementwise division of `A` by `B`. 3. **reduce_sum_on_axis(tensor, axis)**: - **Input**: - `tensor`: A tensor. - `axis`: Integer, the axis along which to perform the summation. - **Output**: A tensor with the sum of elements along the specified axis. - **Details**: Perform a reduction sum operation along the given axis of the tensor. 4. **transfer_to_device_and_save(tensor, filename)**: - **Input**: - `tensor`: A tensor. - `filename`: String, the name of the file to which the tensor will be saved. - **Output**: None. - **Details**: Transfer the tensor to GPU if available and save it to the specified file using PyTorch\'s `torch.save` function. --- Function Implementations: ```python import torch def create_and_square_tensors(N, M, device): indices = torch.arange(N * M).view(N, M) tensor = indices ** 2 return tensor.to(device) def elementwise_operations(A, B): return { \'add\': torch.add(A, B), \'sub\': torch.sub(A, B), \'mul\': torch.mul(A, B), \'div\': torch.div(A, B) } def reduce_sum_on_axis(tensor, axis): return torch.sum(tensor, dim=axis) def transfer_to_device_and_save(tensor, filename): device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") tensor = tensor.to(device) torch.save(tensor, filename) ``` Constraints: - `N, M, P, Q` are positive integers. - `device` is either \'cpu\' or \'cuda\'. - `A` and `B` are tensors of the same shape. Sample Usage: ```python # Creating and squaring tensors tensor = create_and_square_tensors(3, 3, \'cuda\' if torch.cuda.is_available() else \'cpu\') print(tensor) # Elementwise operations A = torch.tensor([[1, 2], [3, 4]]) B = torch.tensor([[5, 6], [7, 8]]) results = elementwise_operations(A, B) print(results) # Reducing sum on axis tensor = torch.tensor([[1, 2, 3], [4, 5, 6]]) sum_result = reduce_sum_on_axis(tensor, 0) print(sum_result) # Transfer to device and save tensor = torch.tensor([1, 2, 3]) transfer_to_device_and_save(tensor, \'tensor.pt\') ``` Note: Ensure that your solution is optimized and leverages PyTorch\'s capabilities efficiently. If GPU is available, computations should be performed on the GPU.","solution":"import torch def create_and_square_tensors(N, M, device): Create a tensor of shape (N, M) where each element is the square of its index position and move it to the specified device. indices = torch.arange(N * M).view(N, M) tensor = indices ** 2 return tensor.to(device) def elementwise_operations(A, B): Perform elementwise addition, subtraction, multiplication, and division on tensors A and B. return { \'add\': torch.add(A, B), \'sub\': torch.sub(A, B), \'mul\': torch.mul(A, B), \'div\': torch.div(A, B) } def reduce_sum_on_axis(tensor, axis): Perform a reduction sum operation along the given axis of the tensor. return torch.sum(tensor, dim=axis) def transfer_to_device_and_save(tensor, filename): Transfer the tensor to GPU if available and save it to the specified file using PyTorch\'s torch.save function. device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") tensor = tensor.to(device) torch.save(tensor, filename)"},{"question":"# Custom Container Type with Garabage Collection Python supports cyclic garbage collection to handle circular references intricately. For this exercise, we\'ll create a custom container type that must interact with Python\'s garbage collector. Your task is to: 1. **Implement a Custom Container Type**: - Create a class named `CustomContainer` that can hold a list of references to other `CustomContainer` instances. - The container should be initialized with an empty list of item references. 2. **Garbage Collector Interaction**: - Provide necessary implementations for the garbage collector to track, traverse, and clear the container when required. - Ensure memory allocation and deallocation follows garbage collection rules using the appropriate functions `PyObject_GC_New`, `PyObject_GC_Del`, etc. 3. **Required Handlers**: - Implement the `tp_traverse` handler using the provided `Py_VISIT` macro, ensuring all contained objects are appropriately visited. - If your container supports mutability, implement the `tp_clear` handler to drop references and avoid reference cycles. **Input and Output**: - There is no explicit input or output function. Instead, focus on the correct class definition and handler implementation. **Constraints**: - Python version must be 3.9 or later. - Follow the outlined memory management processes as described in the documentation. **Example**: ```python class CustomContainer: def __init__(self): self.items = [] # Implement tp_traverse handler def tp_traverse(self): ... # Use Py_VISIT macro here # Implement tp_clear handler if applicable def tp_clear(self): self.items = [] # Drop references to clear cycles ``` **Submission**: - Submit a complete implementation of the `CustomContainer` class with the required garbage collection handlers and memory management functions. - Ensure that your submission includes the necessary imports and any auxiliary functions required for a standalone implementation.","solution":"import gc class CustomContainer: def __init__(self): self.items = [] gc.collect() # Explicitly trigger garbage collection for cleaner testing def add_item(self, item): if isinstance(item, CustomContainer): self.items.append(item) gc.collect() # Ensure garbage collection checks happen during manipulation # Implement tp_traverse handler def tp_traverse(self, visit, arg): for item in self.items: visit(item, arg) # Implement tp_clear handler if applicable def tp_clear(self): self.items = [] gc.collect() # Explicitly clear collected items from memory"},{"question":"**Problem Statement:** You are tasked with implementing a utility that compresses a directory into a `.tar.gz` file and verifies the integrity of the compressed file. Your solution should demonstrate proficiency in handling both file and directory operations using the `tarfile` and `gzip` modules. **Function Requirements:** 1. **compress_directory**: - **Input**: - `directory_path` (str): The path to the directory that needs to be compressed. - `output_path` (str): The desired path for the output `.tar.gz` file. - **Output**: - Returns `True` if the directory is successfully compressed, otherwise `False`. - **Constraints**: - The function should handle any kind of directory structure, including nested directories. - If the `output_path` already exists, it should be overwritten. - Ensure that all files are properly added to the archive. 2. **verify_compressed_file**: - **Input**: - `tar_gz_path` (str): The path to the `.tar.gz` file that needs to be verified. - **Output**: - Returns `True` if the `.tar.gz` file is valid and can be extracted successfully without errors, otherwise `False`. - **Constraints**: - The function should handle corrupted or incomplete `.tar.gz` files gracefully. **Performance Requirements**: - Both functions should handle directories/files up to 2GB in size efficiently. - Should throw appropriate exceptions for invalid inputs. **Sample Usage:** ```python import os def compress_directory(directory_path, output_path): try: import tarfile with tarfile.open(output_path, \\"w:gz\\") as tar: tar.add(directory_path, arcname=os.path.basename(directory_path)) return True except Exception as e: print(f\\"Error: {e}\\") return False def verify_compressed_file(tar_gz_path): try: import tarfile with tarfile.open(tar_gz_path, \\"r:gz\\") as tar: tar.extractall(path=\\"/tmp/verification_dir\\") return True except Exception as e: print(f\\"Error: {e}\\") return False # Example: compressing and verifying a directory directory_to_compress = \\"/path/to/directory\\" output_compressed_file = \\"/path/to/output.tar.gz\\" if compress_directory(directory_to_compress, output_compressed_file): print(\\"Compression Successful!\\") if verify_compressed_file(output_compressed_file): print(\\"Verification Successful!\\") else: print(\\"Verification Failed!\\") else: print(\\"Compression Failed!\\") ```","solution":"import os import tarfile def compress_directory(directory_path, output_path): Compresses a directory into a .tar.gz file. Parameters: directory_path (str): The path to the directory that needs to be compressed. output_path (str): The desired path for the output .tar.gz file. Returns: bool: True if the directory is successfully compressed, otherwise False. try: with tarfile.open(output_path, \\"w:gz\\") as tar: tar.add(directory_path, arcname=os.path.basename(directory_path)) return True except Exception as e: print(f\\"Error: {e}\\") return False def verify_compressed_file(tar_gz_path): Verifies the integrity of a .tar.gz file. Parameters: tar_gz_path (str): The path to the .tar.gz file that needs to be verified. Returns: bool: True if the .tar.gz file is valid and can be extracted successfully without errors, otherwise False. try: with tarfile.open(tar_gz_path, \\"r:gz\\") as tar: tar.extractall(path=\\"/tmp/verification_dir\\") return True except Exception as e: print(f\\"Error: {e}\\") return False"},{"question":"# PyTorch Meta Device: Abstract Analysis and Model Transformation Problem Statement In this task, you will create a small neural network model, perform transformations on it using the meta device, and finally construct real tensors based on the meta tensor analysis. This exercise will help evaluate your understanding of the \\"meta\\" device in PyTorch. Instructions 1. **Create a Simple Neural Network Model:** - Define a `SimpleNN` class that extends `torch.nn.Module`. - The model should consist of three layers: - A Linear layer with 10 input features and 20 output features. - A ReLU activation layer. - Another Linear layer with 20 input features and 5 output features. 2. **Load the Model on the Meta Device:** - Save a randomly initialized `SimpleNN` model to a file. - Load the model onto the meta device using `torch.load` with `map_location=\'meta\'`. 3. **Perform Transformations on the Meta Model:** - Define a function `get_meta_model_description` that takes a meta model as input and returns a dictionary containing: - Number of layers - Size of the first Linear layer\'s weight tensor - Size of the second Linear layer\'s weight tensor 4. **Reconstruct Real Tensors:** - Instantiate a new `SimpleNN` model. - Transfer the meta model to a real device (e.g., CPU) but keep the parameters uninitialized using `to_empty`. - Initialize the parameters with random data using `torch.empty_like`. Expected Functions 1. `get_meta_model_description(meta_model: torch.nn.Module) -> dict` - **Input:** `meta_model` - A neural network model loaded on the meta device. - **Output:** A dictionary with keys `num_layers`, `first_layer_size`, and `second_layer_size` representing the respective descriptions of the meta model. 2. `initialize_parameters(real_model: torch.nn.Module, meta_model: torch.nn.Module) -> None` - **Input:** - `real_model` - A neural network model where parameters need to be initialized. - `meta_model` - A meta model to guide the initialization of `real_model`. - **Output:** Initializes the parameters of `real_model` using the structure of `meta_model`. Example ```python import torch import torch.nn as nn import torch.nn.functional as F # Define the SimpleNN model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.fc2 = nn.Linear(20, 5) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Save and load model on meta device model = SimpleNN() torch.save(model, \'simple_nn.pt\') meta_model = torch.load(\'simple_nn.pt\', map_location=\'meta\') # Function to describe meta model def get_meta_model_description(meta_model): description = { \\"num_layers\\": 3, \\"first_layer_size\\": meta_model.fc1.weight.size(), \\"second_layer_size\\": meta_model.fc2.weight.size() } return description # Initialize real model\'s parameters def initialize_parameters(real_model, meta_model): real_model.to_empty(device=\\"cpu\\") with torch.no_grad(): real_model.fc1.weight = nn.Parameter(torch.empty_like(meta_model.fc1.weight)) real_model.fc2.weight = nn.Parameter(torch.empty_like(meta_model.fc2.weight)) real_model.fc1.bias = nn.Parameter(torch.empty_like(meta_model.fc1.bias)) real_model.fc2.bias = nn.Parameter(torch.empty_like(meta_model.fc2.bias)) real_model = SimpleNN() initialize_parameters(real_model, meta_model) print(get_meta_model_description(meta_model)) ``` Constraints and Notes - The dictionary returned by `get_meta_model_description` should accurately reflect the meta model\'s structure. - Ensure the `initialize_parameters` function properly initializes the real model\'s parameters based on the meta model. - You are encouraged to use any other PyTorch utilities you find necessary to accomplish the task.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.fc2 = nn.Linear(20, 5) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def save_model(model, filepath): torch.save(model, filepath) def load_meta_model(filepath): return torch.load(filepath, map_location=\'meta\') def get_meta_model_description(meta_model): description = { \\"num_layers\\": 3, \\"first_layer_size\\": meta_model.fc1.weight.size(), \\"second_layer_size\\": meta_model.fc2.weight.size() } return description def initialize_parameters(real_model, meta_model): real_model.to_empty(device=\\"cpu\\") with torch.no_grad(): real_model.fc1.weight = nn.Parameter(torch.empty_like(meta_model.fc1.weight)) real_model.fc2.weight = nn.Parameter(torch.empty_like(meta_model.fc2.weight)) real_model.fc1.bias = nn.Parameter(torch.empty_like(meta_model.fc1.bias)) real_model.fc2.bias = nn.Parameter(torch.empty_like(meta_model.fc2.bias)) # Initialize model, save it and load onto meta device model = SimpleNN() save_model(model, \'simple_nn.pt\') meta_model = load_meta_model(\'simple_nn.pt\') # Create a new real model and initialize parameters real_model = SimpleNN() initialize_parameters(real_model, meta_model)"},{"question":"Objective: To assess your understanding of Python\'s built-in constants and their appropriate use, along with handling exceptions that arise due to misuse of these constants. Question: You are required to implement a class `CustomContainer` that simulates a custom container for integers, similar to a list but with specific constraints. 1. The class should support the following methods: - `__init__(self, *args)`: Initialize the container with any number of integer arguments. - `add(self, value)`: Add an integer `value` to the container. If `value` is not an integer, raise a `TypeError` with the message `\\"Only integers are allowed\\"`. - `remove(self, value)`: Remove the first occurrence of `value` from the container. If `value` is not found, return `NotImplemented`. - `__getitem__(self, index)`: Get the item at the given `index`. If `index` is `Ellipsis`, return the list of all items. - `__len__(self)`: Return the number of items in the container. 2. Additionally, implement a standalone function `debug_check()` that returns `True` if the special built-in constant `__debug__` is `True`, and `False` otherwise. 3. Ensure that the class and function adhere to the following constraints: - Use the proper constants for handling specific conditions (e.g., `NotImplemented` for unimplemented operations). - Avoid any assignments to immutable constants like `False`, `True`, and `None`. - Handle any exceptions that arise, following best practices. Constraints: - The class should only support integer values. - Do not use any external libraries; only standard Python can be used. - Ensure your code is efficient and follows the PEP 8 style guide. Example: ```python # Sample usage container = CustomContainer(1, 2, 3) container.add(4) # Container now has [1, 2, 3, 4] print(container[Ellipsis]) # Output: [1, 2, 3, 4] container.remove(2) # Removes 2; container is now [1, 3, 4] print(len(container)) # Output: 3 print(debug_check()) # Output depends on __debug__ constant ``` Implement the `CustomContainer` class and `debug_check` function in Python. Note: - Document your code clearly, explaining the role of each method and function. - Consider edge cases and ensure robust error handling.","solution":"class CustomContainer: def __init__(self, *args): Initialize the container with any number of integer arguments. self.items = [] for arg in args: if isinstance(arg, int): self.items.append(arg) else: raise TypeError(\\"All initial arguments must be integers\\") def add(self, value): Add an integer value to the container. If value is not an integer, raise a TypeError. if not isinstance(value, int): raise TypeError(\\"Only integers are allowed\\") self.items.append(value) def remove(self, value): Remove the first occurrence of value from the container. If value is not found, return NotImplemented. if value in self.items: self.items.remove(value) else: return NotImplemented def __getitem__(self, index): Get the item at the given index. If index is Ellipsis, return the list of all items. if index is Ellipsis: return self.items return self.items[index] def __len__(self): Return the number of items in the container. return len(self.items) def debug_check(): Return True if the special built-in constant __debug__ is True, and False otherwise. return __debug__"},{"question":"# Cross-Validation and Model Evaluation using Scikit-learn You are given a dataset and a machine learning classification task. Your goal is to implement several cross-validation strategies and evaluate the model performance comprehensively using scikit-learn. Dataset and Task You will use the Iris dataset, a commonly used dataset in machine learning, to train and evaluate a Support Vector Classifier (SVC). The dataset has three classes of the target variable. Instructions 1. **Load the Dataset:** Load the Iris dataset using scikit-learn\'s dataset utilities. 2. **Implement Cross-Validation:** Write a function `evaluate_model_cv` that: - Takes no input parameters. - Creates an instance of `svm.SVC` with a linear kernel and `C=1`. - Implements the following cross-validation strategies: - 5-Fold Cross-Validation. - Stratified 5-Fold Cross-Validation. - Leave-One-Out Cross-Validation. - Time Series Split with 3 splits. - Returns a dictionary with the mean accuracy and standard deviation of each cross-validation strategy. 3. **Multiple Metric Evaluation:** Extend your `evaluate_model_cv` function: - Evaluate two metrics, `accuracy` and `f1_macro`. - For each cross-validation strategy, return the mean and standard deviation for both metrics. 4. **Permutation Test Score:** Write a function `evaluate_permutation_test` that: - Takes no input parameters. - Creates an instance of `svm.SVC` with a linear kernel and `C=1`. - Performs a permutation test with 100 permutations using 3-Fold cross-validation. - Returns the permutation test score (p-value). 5. **Custom Cross-Validation Iterator:** Write a custom cross-validation iterator that implements a 2-Fold strategy for demonstration: - Create a function `custom_cv_iterator` that yields two splits manually. 6. **Integration:** Create a script that: - Implements and calls `evaluate_model_cv`. - Implements and calls `evaluate_permutation_test`. - Demonstrates the use of your custom cross-validation iterator with the SVC model. Expected Output: ```python { \'5-Fold\': {\'accuracy\': (mean_accuracy, std_accuracy), \'f1_macro\': (mean_f1, std_f1)}, \'Stratified 5-Fold\': {\'accuracy\': (mean_accuracy, std_accuracy), \'f1_macro\': (mean_f1, std_f1)}, \'Leave-One-Out\': {\'accuracy\': (mean_accuracy, std_accuracy), \'f1_macro\': (mean_f1, std_f1)}, \'Time Series Split\': {\'accuracy\': (mean_accuracy, std_accuracy), \'f1_macro\': (mean_f1, std_f1)} } Permutation Test Score: p_value Custom Cross-Validation Iterator: Split 1: Train indices -> [..], Test indices -> [..] Split 2: Train indices -> [..], Test indices -> [..] ``` # Constraints - Use `random_state=42` for reproducibility where applicable. - Use scikit-learn version >= 0.24. # Submission Submit your script with the implemented functions and ensure it runs without errors to produce the expected output.","solution":"import numpy as np from sklearn import datasets, svm from sklearn.model_selection import cross_val_score, StratifiedKFold, LeaveOneOut, TimeSeriesSplit, permutation_test_score from sklearn.metrics import make_scorer, accuracy_score, f1_score def evaluate_model_cv(): iris = datasets.load_iris() X, y = iris.data, iris.target model = svm.SVC(kernel=\'linear\', C=1, random_state=42) cv_results = {} # 5-Fold Cross-Validation cv_5_fold = cross_val_score(model, X, y, cv=5, scoring=\'accuracy\') cv_results[\'5-Fold\'] = { \'accuracy\': (np.mean(cv_5_fold), np.std(cv_5_fold)) } cv_5_fold_f1 = cross_val_score(model, X, y, cv=5, scoring=make_scorer(f1_score, average=\'macro\')) cv_results[\'5-Fold\'][\'f1_macro\'] = (np.mean(cv_5_fold_f1), np.std(cv_5_fold_f1)) # Stratified 5-Fold Cross-Validation stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) cv_stratified_5_fold = cross_val_score(model, X, y, cv=stratified_cv, scoring=\'accuracy\') cv_results[\'Stratified 5-Fold\'] = { \'accuracy\': (np.mean(cv_stratified_5_fold), np.std(cv_stratified_5_fold)) } cv_stratified_5_f1 = cross_val_score(model, X, y, cv=stratified_cv, scoring=make_scorer(f1_score, average=\'macro\')) cv_results[\'Stratified 5-Fold\'][\'f1_macro\'] = (np.mean(cv_stratified_5_f1), np.std(cv_stratified_5_f1)) # Leave-One-Out Cross-Validation loo = LeaveOneOut() cv_loo = cross_val_score(model, X, y, cv=loo, scoring=\'accuracy\') cv_results[\'Leave-One-Out\'] = { \'accuracy\': (np.mean(cv_loo), np.std(cv_loo)) } cv_loo_f1 = cross_val_score(model, X, y, cv=loo, scoring=make_scorer(f1_score, average=\'macro\')) cv_results[\'Leave-One-Out\'][\'f1_macro\'] = (np.mean(cv_loo_f1), np.std(cv_loo_f1)) # Time Series Split with 3 splits time_series_split = TimeSeriesSplit(n_splits=3) cv_time_series = cross_val_score(model, X, y, cv=time_series_split, scoring=\'accuracy\') cv_results[\'Time Series Split\'] = { \'accuracy\': (np.mean(cv_time_series), np.std(cv_time_series)) } cv_time_series_f1 = cross_val_score(model, X, y, cv=time_series_split, scoring=make_scorer(f1_score, average=\'macro\')) cv_results[\'Time Series Split\'][\'f1_macro\'] = (np.mean(cv_time_series_f1), np.std(cv_time_series_f1)) return cv_results def evaluate_permutation_test(): iris = datasets.load_iris() X, y = iris.data, iris.target model = svm.SVC(kernel=\'linear\', C=1, random_state=42) score, permutation_scores, pvalue = permutation_test_score(model, X, y, cv=3, n_permutations=100, scoring=\'accuracy\', random_state=42) return pvalue def custom_cv_iterator(n_samples): indices = np.arange(n_samples) split1_train = indices[:n_samples // 2] split1_test = indices[n_samples // 2:] split2_train = indices[n_samples // 2:] split2_test = indices[:n_samples // 2] return [(split1_train, split1_test), (split2_train, split2_test)] # Example usage if __name__ == \\"__main__\\": cv_results = evaluate_model_cv() print(\\"Cross-Validation Results:\\", cv_results) p_value = evaluate_permutation_test() print(\\"Permutation Test Score p-value:\\", p_value) iris = datasets.load_iris() splits = custom_cv_iterator(len(iris.data)) for i, (train_idx, test_idx) in enumerate(splits): print(f\\"Split {i + 1}: Train indices -> {train_idx}, Test indices -> {test_idx}\\")"},{"question":"Objective: Design a function that evaluates the similarity between the outputs of two PyTorch models using multiple numerical comparison metrics. This assessment will test your understanding of PyTorch model handling, forward pass execution, and usage of the `torch.ao.ns.fx.utils` utility functions. Problem Statement: Implement a function `compare_model_outputs(model1, model2, input_data)` that takes two PyTorch models `model1` and `model2`, and an input tensor `input_data`. The function should compute three metrics between the outputs of the models: Signal-to-Quantization-Noise Ratio (SQNR), Normalized L2 Error, and Cosine Similarity. Return these metrics as a dictionary. # Function Signature: ```python def compare_model_outputs(model1: torch.nn.Module, model2: torch.nn.Module, input_data: torch.Tensor) -> dict: pass ``` # Input: - `model1` (torch.nn.Module): The first PyTorch model. - `model2` (torch.nn.Module): The second PyTorch model. - `input_data` (torch.Tensor): The input tensor to be fed into the models. # Output: - A dictionary containing the following key-value pairs: - `\\"SQNR\\"`: float, Signal-to-Quantization-Noise Ratio between the outputs. - `\\"Normalized_L2_Error\\"`: float, Normalized L2 Error between the outputs. - `\\"Cosine_Similarity\\"`: float, Cosine Similarity between the outputs. # Constraints: - Each model\'s output will be a single tensor. - The `input_data` tensor should be a valid input for both models without any modifications. - Assume that both models are loaded and prepared for evaluation (e.g., in `eval` mode). # Example: ```python import torch import torch.nn as nn # Sample models class ModelA(nn.Module): def forward(self, x): return x + 1 class ModelB(nn.Module): def forward(self, x): return x * 2 model1 = ModelA() model2 = ModelB() input_data = torch.tensor([1.0, 2.0, 3.0]) # Example function execution result = compare_model_outputs(model1, model2, input_data) print(result) # Output format: # { # \\"SQNR\\": <some_float_value>, # \\"Normalized_L2_Error\\": <some_float_value>, # \\"Cosine_Similarity\\": <some_float_value> # } ``` # Notes: - Utilize the utility functions provided in `torch.ao.ns.fx.utils`: - `compute_sqnr` - `compute_normalized_l2_error` - `compute_cosine_similarity` - Ensure your function is thoroughly tested for different input scenarios. Good luck!","solution":"import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity def compare_model_outputs(model1: torch.nn.Module, model2: torch.nn.Module, input_data: torch.Tensor) -> dict: # Ensure models are in evaluation mode model1.eval() model2.eval() # Get the outputs from both models with torch.no_grad(): output1 = model1(input_data) output2 = model2(input_data) # Compute metrics sqnr = compute_sqnr(output1, output2) normalized_l2_error = compute_normalized_l2_error(output1, output2) cosine_similarity = compute_cosine_similarity(output1, output2) # Return results as a dictionary return { \\"SQNR\\": sqnr.item(), \\"Normalized_L2_Error\\": normalized_l2_error.item(), \\"Cosine_Similarity\\": cosine_similarity.item() }"},{"question":"**Question: Utilizing the `linecache` Module for File Line Retrieval** You are tasked to implement a Python function using the `linecache` module that processes log files. Each log file contains multiple lines where each line records an event with a timestamp. Your goal is to implement the following function: ```python def log_events_from_offsets(filename, line_offsets): Given a log file and a list of line offsets, return the corresponding log lines based on the offsets. If a line number is out of range, return \'Invalid Line\'. Args: filename (str): The name of the log file to read from. line_offsets (list): A list of integers specifying the line numbers to retrieve. Returns: list: A list of strings where each string is the log line retrieved. If the line number is out of range, the string \'Invalid Line\' should be added in its place. ``` # Input: - `filename` (str): The path to the log file. - `line_offsets` (list of int): A list of line numbers to retrieve from the file. # Output: - A list of strings: Each element in the list should be the corresponding line from the file as per the specified line number in `line_offsets`. If a requested line number is invalid (i.e., out of range), return the string \'Invalid Line\' for that particular entry. # Example: Suppose the content of `log.txt` is: ``` 2023-01-01 10:00:00 Event A 2023-01-01 10:05:00 Event B 2023-01-01 10:10:00 Event C 2023-01-01 10:15:00 Event D ``` For the function call: ```python log_events_from_offsets(\'log.txt\', [1, 3, 5]) ``` The output should be: ```python [\'2023-01-01 10:00:00 Event A\', \'2023-01-01 10:10:00 Event C\', \'Invalid Line\'] ``` # Constraints: - Do not load the entire file into memory at once. - Efficiently handle file reading using the `linecache` module. - Clear the cache after retrieving the required lines. # Performance Consideration: - The function should handle large log files efficiently by leveraging the caching mechanism of the `linecache` module. # Note: - Use `linecache.getline` to fetch the specific line. - Use `linecache.clearcache` to clear the cache after processing. Implement the `log_events_from_offsets` function making use of the `linecache` module.","solution":"import linecache def log_events_from_offsets(filename, line_offsets): Given a log file and a list of line offsets, return the corresponding log lines based on the offsets. If a line number is out of range, return \'Invalid Line\'. Args: filename (str): The name of the log file to read from. line_offsets (list): A list of integers specifying the line numbers to retrieve. Returns: list: A list of strings where each string is the log line retrieved. If the line number is out of range, the string \'Invalid Line\' should be added in its place. result = [] for line_offset in line_offsets: line = linecache.getline(filename, line_offset) if line: result.append(line.strip()) else: result.append(\'Invalid Line\') linecache.clearcache() return result"},{"question":"**Question:** Implement a function in PyTorch that computes the element-wise polynomial expansion of a given tensor and returns the gradient of the sum of this expansion. Specifically, given a tensor (x) and an integer (p), compute: [y = sum_{i=0}^p x^i] where the exponentiation and summation are performed element-wise. The function should return both the expanded tensor (y) and the gradient of the sum of (y) with respect to (x). **Function Signature:** ```python import torch def polynomial_expansion_with_gradient(x: torch.Tensor, p: int) -> (torch.Tensor, torch.Tensor): Compute the element-wise polynomial expansion of a tensor and return the gradient of the sum with respect to x. Parameters: x (torch.Tensor): Input tensor with requires_grad=True. p (int): The highest power of the polynomial. Returns: torch.Tensor: The expanded tensor y. torch.Tensor: The gradient of the sum of y with respect to x. ``` **Input:** - `x`: A tensor of any shape filled with floating-point numbers. The tensor should have the attribute `requires_grad=True`. - `p`: An integer representing the highest power in the polynomial expansion. **Output:** - A tuple containing: 1. `y`: A tensor of the same shape as `x` representing the element-wise polynomial expansion. 2. `grad`: A tensor of the same shape as `x` representing the gradient of the sum of `y` with respect to `x`. **Constraints:** - Only use PyTorch for tensor operations and automatic differentiation. **Example:** ```python x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) p = 3 y, grad = polynomial_expansion_with_gradient(x, p) # y should be [3.0, 15.0, 39.0] for sum_(i=0 to 3) x^i # grad should be the gradient of sum(y) with respect to x print(\\"Expanded Tensor:\\", y) print(\\"Gradient:\\", grad) ``` **Hints:** - Use a loop or tensor operations to compute the polynomial expansion. - Use `.sum()` to obtain the sum of the tensor. - Use `.backward()` to compute gradients.","solution":"import torch def polynomial_expansion_with_gradient(x: torch.Tensor, p: int) -> (torch.Tensor, torch.Tensor): Compute the element-wise polynomial expansion of a tensor and return the gradient of the sum with respect to x. Parameters: x (torch.Tensor): Input tensor with requires_grad=True. p (int): The highest power of the polynomial. Returns: torch.Tensor: The expanded tensor y. torch.Tensor: The gradient of the sum of y with respect to x. y = sum([x**i for i in range(p + 1)]) y_sum = y.sum() y_sum.backward() return y, x.grad"},{"question":"<|Analysis Begin|> The provided documentation highlights the basic features and functionalities of the seaborn library, specifically focusing on the following points: - Overview of seaborn plotting functions - Similar functions for similar tasks within different modules like relational, distributional, and categorical. - Distinctions between figure-level and axes-level functions. - Customization options available at both levels. - Specific plots like `histplot`, `kdeplot`, `displot`, `scatterplot`, etc. - Use of `FacetGrid` for figure-level customizations. The documentation provides sufficient context on how seaborn functions are categorized and used, along with example code snippets demonstrating their application. From the analysis, a challenging coding assessment question can be crafted requiring students to use both figure-level and axes-level functions and customize their plots using seaborn and matplotlib. Understanding these concepts would demonstrate a solid grasp of seaborn and broader plotting principles. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective Your task is to demonstrate your understanding of the seaborn library by creating a function that visualizes a given dataset using various seaborn plots and customizes them as specified. Function Specification Write a function named `create_plots` that: - Accepts a pandas DataFrame as input. - Generates two types of plots using seaborn: a figure-level plot, and an axes-level plot. - Applies appropriate customizations to each plot. Inputs - `data` (pandas DataFrame): The data to be visualized. Outputs - Returns a tuple of matplotlib figures, where the first element is the figure for the figure-level plot and the second is the figure for the axes-level plot. Function Requirements 1. **Figure-Level Plot** - Use `displot` to visualize the distribution of a numerical feature from the dataset. - Subplot the distribution based on a categorical feature. - Customize the appearance of the plots (e.g., setting different colors for each category). 2. **Axes-Level Plot** - Use `scatterplot` to visualize the relationship between two numerical features from the dataset. - Customize the appearance by setting axis labels, titles, and any other stylistic property. Constraints - The function should not hardcode the columns to be used; it should accept column names as parameters or infer from the dataset. - Utilize seaborn\'s and matplotlib\'s capabilities to adjust the aesthetics and positions of elements like legends, titles, and axis labels appropriately. Example ```python import pandas as pd # Sample Data data = pd.DataFrame({ \'feature_x\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \'feature_y\': [2.1, 2.9, 3.2, 4.1, 5.0, 5.8, 6.5, 7.3, 8.0, 8.8], \'category\': [\'A\', \'B\', \'A\', \'B\', \'A\', \'B\', \'A\', \'B\', \'A\', \'B\'] }) # Function Definitions def create_plots(data): import seaborn as sns import matplotlib.pyplot as plt # Figure-Level Plot fig1 = sns.displot(data=data, x=\'feature_x\', hue=\'category\', col=\'category\', palette=\\"Set2\\").fig # Axes-Level Plot fig2, ax = plt.subplots() sns.scatterplot(data=data, x=\'feature_x\', y=\'feature_y\', hue=\'category\', ax=ax) ax.set_title(\'Scatter plot of Feature X vs Feature Y\') ax.set_xlabel(\'Feature X\') ax.set_ylabel(\'Feature Y\') plt.legend(title=\'Category\') plt.tight_layout() return fig1, fig2 # Visualization fig1, fig2 = create_plots(data) fig1.show() fig2.show() ``` Implement the `create_plots` function so that it meets the above requirements.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_plots(data): Function to generate two types of seaborn plots: a figure-level plot and an axes-level plot. :param data: pandas DataFrame that contains the data for visualization :return: tuple of matplotlib figures # Ensure \'category\' column as a categorical type for correct plotting if \'category\' in data.columns: data[\'category\'] = data[\'category\'].astype(\'category\') # Figure-Level Plot fig1 = sns.displot(data=data, x=\'feature_x\', hue=\'category\', col=\'category\', palette=\\"Set2\\").fig # Axes-Level Plot fig2, ax = plt.subplots() sns.scatterplot(data=data, x=\'feature_x\', y=\'feature_y\', hue=\'category\', ax=ax) ax.set_title(\'Scatter plot of Feature X vs Feature Y\') ax.set_xlabel(\'Feature X\') ax.set_ylabel(\'Feature Y\') plt.legend(title=\'Category\') plt.tight_layout() return fig1, fig2"},{"question":"# Understanding Built-in Constants in Python Objective: Write a Python function demonstrating your understanding and correct usage of built-in constants and indicators provided by Python. Problem Statement: You need to implement a function called `constant_checker` which performs the following tasks: 1. Accepts a single argument `value` which can be any datatype. 2. Returns a tuple `(is_boolean, is_none, is_not_implemented, is_ellipsis, is_debug)` where: - `is_boolean` is `True` if `value` is either `True` or `False`, otherwise `False`. - `is_none` is `True` if `value` is `None`, otherwise `False`. - `is_not_implemented` is `True` if `value` is `NotImplemented`, otherwise `False`. - `is_ellipsis` is `True` if `value` is `Ellipsis`, otherwise `False`. - `is_debug` is `True` if `__debug__` is `True`, otherwise `False`. Constraints: - The function should not raise any exceptions. - Ensure that the tuple is returned in the exact order specified. - Do not use the `type()` function for checks. Example Usage: ```python print(constant_checker(True)) # Output: (True, False, False, False, True) print(constant_checker(NotImplemented)) # Output: (False, False, True, False, True) print(constant_checker(Ellipsis)) # Output: (False, False, False, True, True) print(constant_checker(None)) # Output: (False, True, False, False, True) print(constant_checker(5)) # Output: (False, False, False, False, True) ``` Additional Notes: - The aim of this question is to check if you can correctly identify and handle Python\'s special constants. - Remember that these constants have distinct places within the Python programming language, and understanding their usage is key to writing efficient and bug-free code.","solution":"def constant_checker(value): Returns a tuple indicating whether the given value is one of the special built-in constants in Python and the state of __debug__. is_boolean = value is True or value is False is_none = value is None is_not_implemented = value is NotImplemented is_ellipsis = value is Ellipsis is_debug = __debug__ return (is_boolean, is_none, is_not_implemented, is_ellipsis, is_debug)"},{"question":"Objective Write a function that analyzes the current Python installation\'s configuration and creates a summary report of the installation paths for all schemes available on the platform. Task Your task is to implement a function `generate_installation_report()` that returns a dictionary summarizing the installation paths for all schemes supported by the current platform. Requirements - Use the `sysconfig` module to retrieve the necessary information. - The function should not take any arguments. - The dictionary should have scheme names as keys and another dictionary of path names and their respective paths as values. Example Output ```python { \\"posix_prefix\\": { \\"stdlib\\": \\"/usr/local/lib/python3.10\\", \\"platstdlib\\": \\"/usr/local/lib/python3.10\\", \\"purelib\\": \\"/usr/local/lib/python3.10/site-packages\\", \\"platlib\\": \\"/usr/local/lib/python3.10/site-packages\\", \\"include\\": \\"/usr/local/include/python3.10\\", \\"platinclude\\": \\"/usr/local/include/python3.10\\", \\"scripts\\": \\"/usr/local/bin\\", \\"data\\": \\"/usr/local\\" }, \\"posix_home\\": { \\"stdlib\\": \\"/home/user/.local/lib/python3.10\\", \\"platstdlib\\": \\"/home/user/.local/lib/python3.10\\", \\"purelib\\": \\"/home/user/.local/lib/python3.10/site-packages\\", \\"platlib\\": \\"/home/user/.local/lib/python3.10/site-packages\\", \\"include\\": \\"/home/user/.local/include/python3.10\\", \\"platinclude\\": \\"/home/user/.local/include/python3.10\\", \\"scripts\\": \\"/home/user/.local/bin\\", \\"data\\": \\"/home/user/.local\\" }, ... } ``` > **Note:** The exact paths in the output will vary based on the platform and configuration of the Python installation. Constraints - Ensure that the function handles all schemes returned by `sysconfig.get_scheme_names()`. - If no paths are found for a particular scheme, the report should not include that scheme. Performance Requirements - The function should execute in a reasonable time frame, given that it primarily performs lookups and not intensive computations. Hints - Utilize the `sysconfig.get_path_names()` to retrieve all possible path identifiers. - Use `sysconfig.get_scheme_names()` to get all scheme names supported by the current platform. - For each scheme, use `sysconfig.get_paths(scheme)` to get the paths for that scheme.","solution":"import sysconfig def generate_installation_report(): Generates a summary report of the installation paths for all schemes supported by the current platform. Returns: dict: A dictionary with scheme names as keys and dictionaries of path names and their paths as values. report = {} scheme_names = sysconfig.get_scheme_names() for scheme in scheme_names: paths = sysconfig.get_paths(scheme) if paths: report[scheme] = paths return report"},{"question":"# Clustermap Visualization with Customization Write a function `plot_custom_clustermap(data, figsize, row_cluster, dendrogram_ratio, cbar_pos, label_col, colormap, vmin, vmax, clustering_metric, clustering_method, standard_scale, z_score, center)` that takes the following parameters: 1. `data` (pd.DataFrame): The input data for clustering (without the label column). 2. `figsize` (tuple): A tuple defining the figure size (width, height). 3. `row_cluster` (bool): Whether to cluster the rows. 4. `dendrogram_ratio` (tuple): The ratio of the dendrogram (width ratio, height ratio). 5. `cbar_pos` (tuple): Position of the color bar (left, bottom, width, height). 6. `label_col` (str): Column name of the labels in the original dataset. 7. `colormap` (str): Name of the colormap. 8. `vmin` (float): Minimum value for the color range. 9. `vmax` (float): Maximum value for the color range. 10. `clustering_metric` (str): Distance metric for clustering. 11. `clustering_method` (str): Linkage method for clustering. 12. `standard_scale` (int): Indicates standardization within columns (1) or rows (0). 13. `z_score` (int): Indicates normalization within columns (1) or rows (0), overrides `standard_scale`. 14. `center` (float): Value at which to center the colormap when using `z_score`. The function should: 1. Load the provided dataset and separate the label column. 2. Map unique labels to colors. 3. Create a clustermap with the specified customizations. 4. Display the clustermap. **Example:** ```python import seaborn as sns import pandas as pd # Example usage with the Iris dataset iris = sns.load_dataset(\\"iris\\") iris_data = iris.drop(columns=\'species\') plot_custom_clustermap( data=iris_data, figsize=(7, 5), row_cluster=False, dendrogram_ratio=(.1, .2), cbar_pos=(0, .2, .03, .4), label_col=\'species\', colormap=\\"mako\\", vmin=0, vmax=10, clustering_metric=\\"correlation\\", clustering_method=\\"single\\", standard_scale=1, z_score=None, center=None ) ``` This implementation will assess your understanding of seaborn clustermaps, data preprocessing, and visualization customization. **Constraints:** - The dataset should have more than one feature for clustering. - Ensure that the `label_col` is present in the original dataset and contains categorical labels.","solution":"import seaborn as sns import pandas as pd import numpy as np from matplotlib.colors import ListedColormap import matplotlib.pyplot as plt def plot_custom_clustermap(data, figsize, row_cluster, dendrogram_ratio, cbar_pos, label_col, colormap, vmin, vmax, clustering_metric, clustering_method, standard_scale, z_score, center): Creates a custom clustermap. # Map labels to colors labels = data[label_col] unique_labels = labels.unique() lut = dict(zip(unique_labels, sns.color_palette(\\"tab10\\", len(unique_labels)))) label_colors = labels.map(lut) # Drop the label column from data data = data.drop(columns=[label_col]) # Create a custom colormap if needed cmap = sns.color_palette(colormap, as_cmap=True) # Create the clustermap clustermap = sns.clustermap(data, figsize=figsize, row_cluster=row_cluster, dendrogram_ratio=dendrogram_ratio, cbar_pos=cbar_pos, row_colors=label_colors, cmap=cmap, vmin=vmin, vmax=vmax, metric=clustering_metric, method=clustering_method, standard_scale=standard_scale, z_score=z_score, center=center) # Plot the colorbar for labels for label in unique_labels: clustermap.ax_row_dendrogram.bar(0, 0, color=lut[label], label=label, linewidth=0) clustermap.ax_row_dendrogram.legend(loc=\\"center\\", ncol=1) plt.show()"},{"question":"Coding Assessment Question **Problem Statement:** You are tasked with creating a utility function for handling MIME-types using the deprecated `mailcap` module. This utility function should read from a predefined mailcap file, parse its contents, and allow users to retrieve and execute the appropriate command for processing different types of files. # Function: `execute_mime_command` **Input:** 1. `mailcap_file` (str): The path to a mailcap file to be read and parsed. 2. `mime_type` (str): The MIME type of the file to be processed (e.g., \\"video/mpeg\\"). 3. `filename` (str): The name of the file to be processed. 4. `key` (str, optional): The type of operation to be performed (default is \'view\'). 5. `parameters` (list of str, optional): Additional parameters in the format `name=value` to replace within the command line (default is an empty list). **Output:** - The function should print the command that would be executed for the given MIME type. - It should also handle any potential exceptions and print relevant error messages when something goes wrong (e.g., missing mailcap entries, invalid characters in filename, etc.). **Constraints:** - You should demonstrate usage of the `mailcap` module\'s `getcaps` and `findmatch` functions. - Ensure that the function is robust and handles edge cases gracefully (e.g., missing or invalid inputs, security constraints). - The function should not actually execute the command, just print it for this exercise. **Example Usage:** ```python def execute_mime_command(mailcap_file, mime_type, filename, key=\'view\', parameters=[]): # Your implementation goes here # Example call execute_mime_command(\'/etc/mailcap\', \'video/mpeg\', \'example_video.mpeg\') ``` This call should read the mailcap entries from the specified file and print the appropriate command for viewing an MPEG video file. # Requirements: 1. **File I/O:** Read and parse the mailcap file. 2. **Dictionaries:** Utilize the dictionary returned from `getcaps()`. 3. **String Manipulation:** Handle placeholders and parameters within mailcap entries. 4. **Error Handling:** Gracefully handle errors and security constraints regarding shell metacharacters. # Notes: - Refer to the documentation for `mailcap.getcaps()` and `mailcap.findmatch()` for their usage and behaviors. - Pay special attention to how the `findmatch()` function has changed in Python 3.10.8 to prevent security issues with shell metacharacters. - Ensure all external inputs (filenames, MIME types, and parameters) are validated to comply with security requirements.","solution":"import mailcap import shlex def execute_mime_command(mailcap_file, mime_type, filename, key=\'view\', parameters=[]): Reads from the given mailcap file, finds the appropriate command for the specified MIME type and key, and prints the command. :param mailcap_file: Path to the mailcap file :param mime_type: MIME type of the file to be processed :param filename: Name of the file to be processed :param key: Operation type (default is \'view\') :param parameters: Additional parameters in the format `name=value` to replace within the command line try: # Read the mailcap file and get the caps dictionary caps = mailcap.getcaps(mailcap_file) # Find the match in the mailcap entries match = mailcap.findmatch(caps, mime_type, key, filename=filename, plist=parameters, strict=True) if match: command, entry = match print(command) else: print(f\\"No mailcap entry found for MIME type: {mime_type}\\") except Exception as e: print(f\\"Error: {e}\\") # Example call # execute_mime_command(\'/etc/mailcap\', \'video/mpeg\', \'example_video.mpeg\')"},{"question":"**Advanced PyTorch Symbolic Shape Constraints** In this exercise, you will implement a function that enforces dynamic dimensional constraints on a given tensor. The function should utilize the `torch.fx.experimental.symbolic_shapes` module to ensure that specific constraints on the tensor\'s shape are met. You are required to: 1. Verify if the given tensor\'s shape adheres to a set of provided dimensional constraints. 2. Implement constraint conditions using `torch.fx.experimental.symbolic_shapes` methods. 3. Return a boolean indicating whether the tensor\'s shape meets the constraints. # Function Signature: ```python import torch from torch.fx.experimental import symbolic_shapes as sym def check_tensor_constraints(tensor: torch.Tensor, dim_constraints: dict) -> bool: Check if the given tensor\'s dimensions meet the constraints defined in dim_constraints. Args: - tensor (torch.Tensor): The input tensor whose dimensions are to be checked. - dim_constraints (dict): A dictionary where keys are dimension indices (int) and values are tuples containing the constraint type (str) and the corresponding value to check against (int). Constraint types can include: - \\"min\\": Minimum value for the dimension - \\"max\\": Maximum value for the dimension - \\"eq\\": Dimension should be equal to the given value Returns: - bool: True if all constraints are met, False otherwise. # Example Usage: tensor1 = torch.randn(4, 5, 6) constraints = {0: (\\"min\\", 3), 1: (\\"max\\", 5), 2: (\\"eq\\", 6)} # Example constraints result = check_tensor_constraints(tensor1, constraints) print(result) # Should output True if tensor1 meets all the constraints, otherwise False. ``` # Constraints: - Use the `torch.fx.experimental.symbolic_shapes` module for defining and checking constraints. - The function should handle tensors of varying dimensions and shapes. - Ensure efficient implementation considering performance for larger tensors. Implement the function ensuring it correctly checks the tensor\'s dimensions against the specified constraints and returns the correct result.","solution":"import torch from torch.fx.experimental import symbolic_shapes as sym def check_tensor_constraints(tensor: torch.Tensor, dim_constraints: dict) -> bool: Check if the given tensor\'s dimensions meet the constraints defined in dim_constraints. Args: - tensor (torch.Tensor): The input tensor whose dimensions are to be checked. - dim_constraints (dict): A dictionary where keys are dimension indices (int) and values are tuples containing the constraint type (str) and the corresponding value to check against (int). Constraint types can include: - \\"min\\": Minimum value for the dimension - \\"max\\": Maximum value for the dimension - \\"eq\\": Dimension should be equal to the given value Returns: - bool: True if all constraints are met, False otherwise. shape = tensor.shape for dim, (constraint_type, value) in dim_constraints.items(): if dim >= len(shape): return False dim_size = shape[dim] if constraint_type == \\"min\\" and dim_size < value: return False elif constraint_type == \\"max\\" and dim_size > value: return False elif constraint_type == \\"eq\\" and dim_size != value: return False return True"},{"question":"Objective: Assess students\' understanding of parsing, modifying, and serializing XML data using the `xml.etree.ElementTree` module in Python. Problem Statement: You are given an XML string representing a catalog of books. Each book has multiple attributes such as title, author, year, price, and a nested element for reviews. Your task is to: 1. Parse the given XML string into an ElementTree. 2. Add a default review text to every book that does not have a review. 3. Increase the price of all books published before the year 2000 by 10%. 4. Remove all reviews with a rating lower than 3. 5. Serialize the modified XML back into a string and return it. Input: - A single string containing well-formed XML data. Output: - A single string containing the modified XML data. Constraints: - You may assume that the XML structure is always valid and as described. - The year of publication and prices are always valid integers and floats, respectively. - Review ratings are integers between 1 and 5 inclusive. Example: **Input XML:** ```xml <catalog> <book id=\\"bk101\\"> <title>Python Programming</title> <author>Guido van Rossum</author> <year>1996</year> <price>29.99</price> <reviews> <review rating=\\"5\\">Excellent book!</review> <review rating=\\"2\\">Too advanced for beginners.</review> </reviews> </book> <book id=\\"bk102\\"> <title>Learning XML</title> <author>Erik T. Ray</author> <year>2003</year> <price>39.95</price> <reviews> <review rating=\\"3\\">Good introductory book.</review> </reviews> </book> <book id=\\"bk103\\"> <title>Network Programming with Python</title> <author>John Goerzen</author> <year>1999</year> <price>49.95</price> </book> </catalog> ``` **Expected Output:** ```xml <catalog> <book id=\\"bk101\\"> <title>Python Programming</title> <author>Guido van Rossum</author> <year>1996</year> <price>32.989</price> <reviews> <review rating=\\"5\\">Excellent book!</review> </reviews> </book> <book id=\\"bk102\\"> <title>Learning XML</title> <author>Erik T. Ray</author> <year>2003</year> <price>39.95</price> <reviews> <review rating=\\"3\\">Good introductory book.</review> </reviews> </book> <book id=\\"bk103\\"> <title>Network Programming with Python</title> <author>John Goerzen</author> <year>1999</year> <price>54.945</price> <reviews> <review rating=\\"4\\">No initial review</review> </reviews> </book> </catalog> ``` Implementation: ```python import xml.etree.ElementTree as ET def modify_books(xml_string): root = ET.fromstring(xml_string) for book in root.findall(\'book\'): # Add a default review if not present reviews = book.find(\'reviews\') if reviews is None: reviews = ET.SubElement(book, \'reviews\') if not list(reviews): new_review = ET.SubElement(reviews, \'review\', rating=\\"4\\") new_review.text = \\"No initial review\\" # Increase price for books before 2000 year = int(book.find(\'year\').text) if year < 2000: price = float(book.find(\'price\').text) new_price = price * 1.10 book.find(\'price\').text = f\\"{new_price:.3f}\\" # Remove reviews with rating < 3 for review in reviews.findall(\'review\'): rating = int(review.attrib[\'rating\']) if rating < 3: reviews.remove(review) return ET.tostring(root, encoding=\'unicode\', method=\'xml\') # Example usage input_xml = \'\'\'<catalog> <book id=\\"bk101\\"> <title>Python Programming</title> <author>Guido van Rossum</author> <year>1996</year> <price>29.99</price> <reviews> <review rating=\\"5\\">Excellent book!</review> <review rating=\\"2\\">Too advanced for beginners.</review> </reviews> </book> <book id=\\"bk102\\"> <title>Learning XML</title> <author>Erik T. Ray</author> <year>2003</year> <price>39.95</price> <reviews> <review rating=\\"3\\">Good introductory book.</review> </reviews> </book> <book id=\\"bk103\\"> <title>Network Programming with Python</title> <author>John Goerzen</author> <year>1999</year> <price>49.95</price> </book> </catalog>\'\'\' output_xml = modify_books(input_xml) print(output_xml) ``` Restrictions: - Make sure to use the `xml.etree.ElementTree` module for XML parsing and manipulation. - Follow best practices for handling XML manipulation in Python, ensuring code readability and maintainability.","solution":"import xml.etree.ElementTree as ET def modify_books(xml_string): root = ET.fromstring(xml_string) for book in root.findall(\'book\'): # Add a default review if not present reviews = book.find(\'reviews\') if reviews is None: reviews = ET.SubElement(book, \'reviews\') if not list(reviews): new_review = ET.SubElement(reviews, \'review\', rating=\\"4\\") new_review.text = \\"No initial review\\" # Increase price for books before 2000 year = int(book.find(\'year\').text) if year < 2000: price = float(book.find(\'price\').text) new_price = price * 1.10 book.find(\'price\').text = f\\"{new_price:.3f}\\" # Remove reviews with rating < 3 for review in reviews.findall(\'review\'): rating = int(review.attrib[\'rating\']) if rating < 3: reviews.remove(review) return ET.tostring(root, encoding=\'unicode\', method=\'xml\')"},{"question":"# Problem Description: You are required to implement a function `process_ascii_string(s: str) -> str` using functions from the `curses.ascii` module. This function should take a string `s` consisting of ASCII characters and return a new string that is filtered and transformed based on the following rules: 1. Only retain alphanumeric characters (letters and digits). 2. Convert any control character (ordinal values 0 to 31 and 127) to its corresponding control mnemonic string using `curses.ascii.controlnames`. 3. Replace any non-ASCII character with a placeholder question mark `\'?\'`. 4. Ignore ASCII punctuation characters (like punctuation marks) and ASCII space characters. # Function Signature: ```python import curses.ascii def process_ascii_string(s: str) -> str: pass ``` # Example: ```python # Example Input input_str = \\"Hellox07Worldx1f!\\" # Explanation: # - \'x07\' (BEL) is a control character and should be converted to \'BEL\'. # - \'x1f\' (US) is a control character and should be converted to \'US\'. # - \'!\' is a punctuation and should be ignored. expected_output = \\"HelloBELWorldUS\\" assert process_ascii_string(input_str) == expected_output ``` # Constraints: - `s` will only contain ASCII characters. - The length of `s` will not exceed 1000 characters. # Notes: - Use relevant functions from the `curses.ascii` module to identify character classes and transformations. - Efficiency is not the primary concern, but your implementations should be clear and logically correct.","solution":"import curses.ascii def process_ascii_string(s: str) -> str: result = [] for ch in s: if curses.ascii.isalnum(ch): result.append(ch) elif curses.ascii.iscntrl(ch): result.append(curses.ascii.controlnames[ord(ch)]) elif not curses.ascii.isascii(ch): result.append(\'?\') return \'\'.join(result)"},{"question":"Handling Floating-Point Arithmetic in Python Problem Statement In financial applications or other domains requiring high precision, the limitations of floating-point arithmetic can lead to significant errors. Your task is to implement a Python function that accurately sums a list of decimal values using appropriate techniques to mitigate floating-point arithmetic errors. Function Signature ```python def precise_sum(decimal_values: list) -> float: Sums a list of decimal values accurately to avoid floating-point arithmetic errors. Parameters: decimal_values (list): A list of decimal numbers represented as strings. Returns: float: The accurate sum of the input decimal values. ``` Example ```python # Example usage values = [\\"0.1\\", \\"0.2\\", \\"0.3\\"] result = precise_sum(values) print(result) # Output should be 0.6 ``` Constraints 1. Each element in `decimal_values` is a string representation of a decimal number. 2. The function should handle up to 1,000,000 decimal values in the input list. 3. The function must accurately compute the sum without losing precision due to floating-point arithmetic errors. Guidelines 1. Utilize the `decimal` module in Python to handle decimal arithmetic accurately. 2. Convert each string to a `decimal.Decimal` object for calculations. 3. Return the sum as a float after computing the precise sum using `decimal.Decimal`. This task will assess your understanding of floating-point arithmetic issues and your ability to implement a solution using Python\'s `decimal` module to achieve high-precision results.","solution":"from decimal import Decimal, getcontext def precise_sum(decimal_values: list) -> float: Sums a list of decimal values accurately to avoid floating-point arithmetic errors. Parameters: decimal_values (list): A list of decimal numbers represented as strings. Returns: float: The accurate sum of the input decimal values. # Set the desired precision (more than enough for our needs) getcontext().prec = 100 total = Decimal(\'0\') for value in decimal_values: total += Decimal(value) return float(total)"},{"question":"Coding Assessment Question # Objective You will demonstrate your understanding of `seaborn` by generating and applying color palettes in visualizations of a given dataset. # Question Using the `seaborn` library, you are required to perform the following tasks: 1. **Data Preparation**: - Load the built-in `tips` dataset from `seaborn`. 2. **Palette Creation**: - Create two different palettes: 1. A discrete palette using the \\"viridis\\" colormap with 5 colors. 2. A qualitative palette using the \\"Set2\\" colormap with a default number of colors. 3. **Visualization**: - Generate two plots: 1. A scatter plot showing the relationship between `total_bill` and `tip`, colored by `day` using the \\"viridis\\" palette. 2. A bar plot showing the average `total_bill` for each `day`, using the \\"Set2\\" palette for the bars. # Requirements - **Input/Output**: - No inputs are required; you will load the `tips` dataset directly within your code. - No outputs are expected; your task is to display the plots. - **Constraints**: - Use the `sns.mpl_palette` to generate the palettes. - Set an appropriate theme using `sns.set_theme()`. - **Performance**: - Ensure that your plots are clear and visually distinct. # Example Here is an outline of the expected steps in your code: ```python import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Step 2: Create palettes viridis_palette = sns.mpl_palette(\\"viridis\\", 5) set2_palette = sns.mpl_palette(\\"Set2\\") # Step 3: Set the theme sns.set_theme() # Step 4: Create scatter plot plt.figure(figsize=(8, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", palette=viridis_palette) plt.title(\\"Scatter Plot of Tips vs Total Bill\\") plt.show() # Step 5: Create bar plot plt.figure(figsize=(8, 6)) sns.barplot(data=tips, x=\\"day\\", y=\\"total_bill\\", palette=set2_palette) plt.title(\\"Average Total Bill per Day\\") plt.show() ``` Your task is to write code that accomplishes the steps outlined above.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Step 2: Create palettes viridis_palette = sns.mpl_palette(\\"viridis\\", 5) set2_palette = sns.mpl_palette(\\"Set2\\") # Step 3: Set the theme sns.set_theme() # Step 4: Create scatter plot plt.figure(figsize=(8, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", palette=viridis_palette) plt.title(\\"Scatter Plot of Tips vs Total Bill\\") plt.show() # Step 5: Create bar plot plt.figure(figsize=(8, 6)) sns.barplot(data=tips, x=\\"day\\", y=\\"total_bill\\", palette=set2_palette) plt.title(\\"Average Total Bill per Day\\") plt.show()"},{"question":"# Question: Analyzing and Visualizing a Dataset with Seaborn\'s ECDF Plot You are given a dataset `student_scores.csv` which contains students\' scores in various subjects along with their respective categories. Your task is to create two functions to analyze and visualize the data using Seaborn\'s ECDF plot. Function 1: `ecdf_plot` This function should generate an ECDF plot for a given column in the dataset, categorized by another column (optional). **Function Signature:** ```python def ecdf_plot(file_path: str, column: str, category: str = None, stat: str = \\"proportion\\", complementary: bool = False) -> None: pass ``` **Inputs:** - `file_path` (str): The path to the `student_scores.csv` file. - `column` (str): The column name for which the ECDF plot needs to be generated. - `category` (str, optional): The column name according to which the data will be categorized. Default is `None`. - `stat` (str, optional): The statistic to be shown. Options are \\"proportion\\", \\"count\\", \\"percent\\". Default is \\"proportion\\". - `complementary` (bool, optional): If True, plots the empirical complementary CDF. Default is False. **Output:** - Displays an ECDF plot using Seaborn. **Example Usage:** ```python ecdf_plot(\\"student_scores.csv\\", column=\\"math_score\\", category=\\"class\\") ``` Function 2: `analyze_column_statistics` This function should calculate and print basic statistics (mean, median, standard deviation) of a specified column. **Function Signature:** ```python def analyze_column_statistics(file_path: str, column: str) -> None: pass ``` **Inputs:** - `file_path` (str): The path to the `student_scores.csv` file. - `column` (str): The column name for which statistics need to be calculated. **Output:** - Prints out the mean, median, and standard deviation of the column. **Example Usage:** ```python analyze_column_statistics(\\"student_scores.csv\\", column=\\"math_score\\") ``` Additional Information: - The dataset `student_scores.csv` consists of columns like `student_id`, `math_score`, `science_score`, `english_score`, `class`, and potentially other columns. - Your functions should handle edge cases (e.g., invalid column names) gracefully. - Ensure to add relevant labels and titles to the plots for better readability. Constraints: - Use the Seaborn and Pandas libraries for this task. - Ensure your code is well-documented and follows best coding practices. Evaluation Criteria: - Correctness and completeness of the implemented functions. - Quality and readability of the generated plots. - Handling of edge cases and exceptions. - Code style and documentation.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np def ecdf_plot(file_path: str, column: str, category: str = None, stat: str = \\"proportion\\", complementary: bool = False) -> None: Generates an ECDF plot for a given column in the dataset, categorized by another column (optional). # Load the dataset df = pd.read_csv(file_path) # Check if the column exists in the DataFrame if column not in df.columns: raise ValueError(f\\"Column \'{column}\' does not exist in the dataset\\") # Plot the ECDF plt.figure(figsize=(10, 6)) sns.ecdfplot(data=df, x=column, hue=category, stat=stat, complementary=complementary) plt.title(f\\"ECDF Plot for {column}\\") plt.xlabel(column) plt.ylabel(f\\"ECDF ({stat})\\") if category: plt.legend(title=category) plt.show() def analyze_column_statistics(file_path: str, column: str) -> None: Calculates and prints the basic statistics (mean, median, standard deviation) of a specified column. # Load the dataset df = pd.read_csv(file_path) # Check if the column exists in the DataFrame if column not in df.columns: raise ValueError(f\\"Column \'{column}\' does not exist in the dataset\\") # Calculate statistics mean_value = df[column].mean() median_value = df[column].median() std_dev_value = df[column].std() print(f\\"Statistics for {column}:\\") print(f\\"Mean: {mean_value}\\") print(f\\"Median: {median_value}\\") print(f\\"Standard Deviation: {std_dev_value}\\")"},{"question":"# Advanced PyTorch Coding Assessment: Working with the Meta Device Objective In this task, you are required to demonstrate your understanding of the meta device in PyTorch by working with tensors and neural network modules without actual data but utilizing metadata for abstract analysis. You will create a meta tensor, perform an operation, transfer a neural network module to another device with uninitialized parameters, and finally reinitialize the parameters. Problem Statement 1. Load a tensor onto the meta device and perform any operation on it to create a new meta tensor. 2. Construct a simple neural network module with the meta device, and then transfer this module to the CPU with all parameters uninitialized. 3. Reinitialize the parameters of the module on the CPU with random values. Instructions 1. **Meta Tensor Operations**: - Create a tensor of size ( (3, 4) ) on the meta device filled with random values. - Perform an element-wise addition operation on this tensor with another meta tensor (also filled with random values) of the same size. - Output the resultant meta tensor. 2. **Constructing and Transferring Module**: - Define a simple neural network module with at least one `Linear` layer using the `torch.nn.Module` class. - Construct this module on the meta device. - Print the meta representation of the module. - Transfer the module to the CPU with uninitialized parameters using the `to_empty` method. - Print the CPU representation of the module with uninitialized parameters. 3. **Reinitializing Parameters**: - Once the module is transferred to the CPU, reinitialize the parameters of the `Linear` layer(s) using random values. - Print the reinitialized module parameters. Expected Input and Output Format No specific input required, but ensure to follow the steps and print outputs accordingly. Example Here’s an example structure of how you might proceed: ```python import torch import torch.nn as nn # Step 1: Meta Tensor Operations with torch.device(\'meta\'): meta_tensor1 = torch.randn(3, 4) meta_tensor2 = torch.randn(3, 4) result_meta_tensor = meta_tensor1 + meta_tensor2 print(\\"Resultant Meta Tensor:\\", result_meta_tensor) # Step 2: Constructing and Transferring Module class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc = nn.Linear(4, 2) def forward(self, x): return self.fc(x) with torch.device(\'meta\'): meta_model = SimpleNN() print(\\"Meta Model on Meta Device:\\", meta_model) cpu_model = meta_model.to_empty(device=\'cpu\') print(\\"Model on CPU with Uninitialized Parameters:\\", cpu_model) # Step 3: Reinitializing Parameters for param in cpu_model.parameters(): param.data = torch.randn_like(param) print(\\"Reinitialized CPU Model:\\", cpu_model) ``` Please write and execute the code that accomplishes the steps outlined above.","solution":"import torch import torch.nn as nn # Step 1: Meta Tensor Operations meta_device = torch.device(\'meta\') meta_tensor1 = torch.randn(3, 4, device=meta_device) meta_tensor2 = torch.randn(3, 4, device=meta_device) result_meta_tensor = meta_tensor1 + meta_tensor2 print(\\"Resultant Meta Tensor:\\", result_meta_tensor) # Step 2: Constructing and Transferring Module class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc = nn.Linear(4, 2) def forward(self, x): return self.fc(x) # Creating the model on the meta device meta_model = SimpleNN().to(meta_device) print(\\"Meta Model on Meta Device:\\", meta_model) # Transferring the model to the CPU with uninitialized parameters cpu_model = meta_model.to_empty(device=\'cpu\') print(\\"Model on CPU with Uninitialized Parameters:\\", cpu_model) # Step 3: Reinitializing Parameters for param in cpu_model.parameters(): param.data = torch.randn_like(param) print(\\"Reinitialized CPU Model:\\", cpu_model)"},{"question":"You are required to implement a Python function that automates the creation of a source distribution package for a given Python project. The function should take several parameters that specify different aspects of the distribution, mimicking the behavior of `setup.py sdist` described in the documentation. Function Signature ```python def create_source_distribution(project_dir: str, output_dir: str, formats: List[str], owner: Optional[str] = None, group: Optional[str] = None) -> None: Create a source distribution for the given Python project. Parameters: - project_dir (str): The path to the root of the project directory containing setup.py. - output_dir (str): The directory where the distribution archives should be stored. - formats (List[str]): List of formats for the distribution (e.g., [\'gztar\', \'zip\']). - owner (Optional[str]): Optional owner name for the files in the distribution (Unix only). - group (Optional[str]): Optional group name for the files in the distribution (Unix only). Returns: - None: The function should not return anything but should create distribution files in the output directory. Constraints: - Ensure that the appropriate format-specific files are created. - Handle errors gracefully, particularly around missing files or incorrect paths. - Use only the standard library for file operations. The function should raise a ValueError if any of the specified formats are unsupported. pass ``` Explanation and Constraints 1. **Parameters**: - `project_dir`: This is the path to the root directory of the Python project, which contains the `setup.py` file and optionally a `MANIFEST.in`. - `output_dir`: The directory where the generated distribution archives should be stored. - `formats`: A list of string formats, specifying the desired output formats (`gztar`, `zip`, `bztar`, etc.). - `owner` and `group`: Optional Unix-specific parameters to set the owner and group for the files in the distribution. 2. **Requirements**: - The function must create a source distribution using the specified formats. - It should mimic the behavior of `setup.py sdist`, creating distributions in the appropriate specified formats. - The function should raise a `ValueError` if any unsupported format is provided. - If tasks fail (e.g., missing `setup.py` or incorrect paths), handle these errors gracefully. 3. **Constraints**: - Only use the Python standard library. - Ensure that tarfile, zipfile, or other requisite modules handle the correct creation of archive formats. - If Unix-specific owner or group is provided, handle these appropriately. (Use the `os.chown` function for setting file ownership on Unix systems.) Example ```python create_source_distribution(\'/path/to/project\', \'/path/to/output\', [\'gztar\', \'zip\'], owner=\'root\', group=\'root\') ``` This should create a gzipped tarball and a zipfile of the project in the specified output directory.","solution":"import os import shutil import tarfile import zipfile from typing import List, Optional SUPPORTED_FORMATS = {\'gztar\', \'zip\', \'bztar\', \'xztar\'} def create_source_distribution(project_dir: str, output_dir: str, formats: List[str], owner: Optional[str] = None, group: Optional[str] = None) -> None: Create a source distribution for the given Python project. Parameters: - project_dir (str): The path to the root of the project directory containing setup.py. - output_dir (str): The directory where the distribution archives should be stored. - formats (List[str]): List of formats for the distribution (e.g., [\'gztar\', \'zip\']). - owner (Optional[str]): Optional owner name for the files in the distribution (Unix only). - group (Optional[str]): Optional group name for the files in the distribution (Unix only). Returns: - None: The function should not return anything but should create distribution files in the output directory. # Check for unsupported formats for fmt in formats: if fmt not in SUPPORTED_FORMATS: raise ValueError(f\\"Unsupported format: {fmt}\\") # Check if project_dir exists and contains setup.py setup_path = os.path.join(project_dir, \'setup.py\') if not os.path.exists(setup_path): raise FileNotFoundError(f\\"setup.py not found in {project_dir}\\") # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) # Functions for different archive formats def make_gztar(): archive_name = os.path.join(output_dir, os.path.basename(project_dir) + \'.tar.gz\') with tarfile.open(archive_name, \\"w:gz\\") as tar: tar.add(project_dir, arcname=os.path.basename(project_dir)) return archive_name def make_zip(): archive_name = os.path.join(output_dir, os.path.basename(project_dir) + \'.zip\') with zipfile.ZipFile(archive_name, \\"w\\", zipfile.ZIP_DEFLATED) as zip_file: for root, _, files in os.walk(project_dir): for file in files: file_path = os.path.join(root, file) arcname = os.path.relpath(file_path, os.path.join(project_dir, \'..\')) zip_file.write(file_path, arcname) return archive_name def make_bztar(): archive_name = os.path.join(output_dir, os.path.basename(project_dir) + \'.tar.bz2\') with tarfile.open(archive_name, \\"w:bz2\\") as tar: tar.add(project_dir, arcname=os.path.basename(project_dir)) return archive_name def make_xztar(): archive_name = os.path.join(output_dir, os.path.basename(project_dir) + \'.tar.xz\') with tarfile.open(archive_name, \\"w:xz\\") as tar: tar.add(project_dir, arcname=os.path.basename(project_dir)) return archive_name # Create archives for fmt in formats: if fmt == \'gztar\': archive_name = make_gztar() elif fmt == \'zip\': archive_name = make_zip() elif fmt == \'bztar\': archive_name = make_bztar() elif fmt == \'xztar\': archive_name = make_xztar() # Check if we need to set owner/group if owner or group: import pwd import grp uid = pwd.getpwnam(owner).pw_uid if owner else -1 gid = grp.getgrnam(group).gr_gid if group else -1 os.chown(archive_name, uid, gid)"},{"question":"# CSV Data Aggregation Task You are provided with multiple CSV files containing sales data for different regions. Each file has columns: `Product`, `Region`, `Sales`. Your task is to read these files, aggregate the sales data by product, and write the aggregated data to a new CSV file. Implement a function `aggregate_sales(input_files: List[str], output_file: str) -> None` that takes a list of input file paths and an output file path. # Function Specification Input: - **input_files:** (List[str]) A list of paths to input CSV files. Each file contains sales data with columns: `Product`, `Region`, `Sales`. - **output_file:** (str) The path to the output CSV file where the aggregated data will be written. Output: - The function should write to the `output_file`, with the aggregated sales data by product. The output file should have columns: `Product`, `Total_Sales`. Constraints: - Assume the \'Sales\' column values are integers. Example: Suppose you have two input files with the following contents: - `north_region.csv` ``` Product,Region,Sales A,North,100 B,North,150 A,North,200 C,North,300 ``` - `east_region.csv` ``` Product,Region,Sales A,East,50 A,East,150 B,East,100 D,East,250 ``` Calling `aggregate_sales([\'north_region.csv\', \'east_region.csv\'], \'total_sales.csv\')` will generate an `total_sales.csv` with the contents: ``` Product,Total_Sales A,500 B,250 C,300 D,250 ``` # Guidelines 1. Use the `csv` module to read and write CSV files. 2. Use a dictionary to compute the aggregate sales for each product. 3. Handle cases where input files might have different products. 4. Ensure the output CSV file is properly formatted with the required headers. ```python import csv from typing import List, Dict def aggregate_sales(input_files: List[str], output_file: str) -> None: # Initialize a dictionary to store total sales per product sales_data: Dict[str, int] = {} # Read each input file and aggregate the sales for file in input_files: with open(file, newline=\'\') as csvfile: reader = csv.DictReader(csvfile) for row in reader: product = row[\'Product\'] sales = int(row[\'Sales\']) if product in sales_data: sales_data[product] += sales else: sales_data[product] = sales # Write the aggregated data to the output file with open(output_file, \'w\', newline=\'\') as csvfile: fieldnames = [\'Product\', \'Total_Sales\'] writer = csv.DictWriter(csvfile, fieldnames=fieldnames) writer.writeheader() for product, total_sales in sales_data.items(): writer.writerow({\'Product\': product, \'Total_Sales\': total_sales}) ```","solution":"import csv from typing import List, Dict def aggregate_sales(input_files: List[str], output_file: str) -> None: # Initialize a dictionary to store total sales per product sales_data: Dict[str, int] = {} # Read each input file and aggregate the sales for file in input_files: with open(file, newline=\'\') as csvfile: reader = csv.DictReader(csvfile) for row in reader: product = row[\'Product\'] sales = int(row[\'Sales\']) if product in sales_data: sales_data[product] += sales else: sales_data[product] = sales # Write the aggregated data to the output file with open(output_file, \'w\', newline=\'\') as csvfile: fieldnames = [\'Product\', \'Total_Sales\'] writer = csv.DictWriter(csvfile, fieldnames=fieldnames) writer.writeheader() for product, total_sales in sales_data.items(): writer.writerow({\'Product\': product, \'Total_Sales\': total_sales})"},{"question":"# Question: Testing a Calculator Class You are required to create and test a `Calculator` class using Python\'s `unittest` module. Part 1: Implement the Calculator Class Implement a `Calculator` class with the following methods: - `add(a, b)`: Returns the sum of `a` and `b`. - `subtract(a, b)`: Returns the difference between `a` and `b`. - `multiply(a, b)`: Returns the product of `a` and `b`. - `divide(a, b)`: Returns the quotient of `a` divided by `b`. This method should raise a `ValueError` if `b` is zero. Part 2: Write Unit Tests for the Calculator Class Write a test suite using the `unittest` module to verify the behavior of the `Calculator` class. Your tests should include: 1. **Test Addition**: - Check if `add(2, 3)` returns `5`. - Check if `add(-1, 1)` returns `0`. 2. **Test Subtraction**: - Check if `subtract(5, 2)` returns `3`. - Check if `subtract(2, 5)` returns `-3`. 3. **Test Multiplication**: - Check if `multiply(2, 3)` returns `6`. - Check if `multiply(-2, -3)` returns `6`. 4. **Test Division**: - Check if `divide(6, 3)` returns `2`. - Check if `divide(6, 0)` raises a `ValueError`. 5. **Test Setup and Teardown**: - Demonstrate the use of `setUp` method to create a `Calculator` instance before each test method. - Demonstrate the use of `tearDown` method to delete the `Calculator` instance after each test method. 6. **Test Expected Failures and Skipping Tests**: - Write a test for a known bug that is expected to fail using `unittest.expectedFailure`. - Write a test that is skipped using `unittest.skip` with a message explaining the reason. Constraints - You must use the `unittest` module. - Your test suite should be runnable with `python -m unittest discover`. Here is a skeleton code to help you get started: ```python import unittest class Calculator: def add(self, a, b): pass # Implement this method def subtract(self, a, b): pass # Implement this method def multiply(self, a, b): pass # Implement this method def divide(self, a, b): pass # Implement this method class TestCalculator(unittest.TestCase): def setUp(self): self.calculator = Calculator() def tearDown(self): del self.calculator def test_add(self): pass # Implement this method def test_subtract(self): pass # Implement this method def test_multiply(self): pass # Implement this method def test_divide(self): pass # Implement this method @unittest.expectedFailure def test_known_bug(self): pass # Implement this method @unittest.skip(\\"Test skipped: Reason.\\") def test_skipped(self): pass # Implement this method if __name__ == \'__main__\': unittest.main() ``` Submit your implementation by filling out the methods in the `Calculator` class and the corresponding test methods in `TestCalculator` to ensure your `Calculator` implementation works as expected.","solution":"class Calculator: def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): if b == 0: raise ValueError(\\"Cannot divide by zero\\") return a / b"},{"question":"# DOM Tree Manipulation and Query Objective: You are required to parse an XML document and perform some manipulations using the DOM API provided by the `xml.dom` package in Python. Task: 1. Write a function `create_dom_document` that: - Takes no arguments. - Creates a new XML document with a root element `Library`. 2. Add functionality to this function to: - Add a child element `Book` to the `Library` element with an attribute `category` set to `\\"Fiction\\"`. - Add a child element `Title` with text content `\\"The Great Gatsby\\"` to the `Book` element. - Add a child element `Author` with text content `\\"F. Scott Fitzgerald\\"` to the `Book` element. - Add a child element `Year` with text content `\\"1925\\"` to the `Book` element. 3. Write another function `parse_and_query_dom` that: - Takes a string argument `xml_content` containing XML data. - Parses the XML content into a DOM document object. - Searches for all `Book` elements and returns a list of tuples containing the `Title` and `Author` of each `Book`. Constraints: - Assume the XML content provided to the `parse_and_query_dom` is well-formed. - Only use the `xml.dom` package to manipulate the DOM. Input: - The `create_dom_document` function takes no input. - The `parse_and_query_dom` function takes a single string argument containing XML data. Output: - The `create_dom_document` function should return the string representation of the XML document it creates. - The `parse_and_query_dom` function should return a list of tuples, where each tuple contains the title and author of a book. Example: ```python # Example usage of create_dom_document: xml_doc = create_dom_document() print(xml_doc) # Output should be: # <Library> # <Book category=\\"Fiction\\"> # <Title>The Great Gatsby</Title> # <Author>F. Scott Fitzgerald</Author> # <Year>1925</Year> # </Book> # </Library> # Example usage of parse_and_query_dom: example_xml = <Library> <Book category=\\"Fiction\\"> <Title>The Great Gatsby</Title> <Author>F. Scott Fitzgerald</Author> <Year>1925</Year> </Book> <Book category=\\"Science\\"> <Title>A Brief History of Time</Title> <Author>Stephen Hawking</Author> <Year>1988</Year> </Book> </Library> result = parse_and_query_dom(example_xml) print(result) # Output should be: # [(\'The Great Gatsby\', \'F. Scott Fitzgerald\'), (\'A Brief History of Time\', \'Stephen Hawking\')] ```","solution":"from xml.dom.minidom import Document, parseString def create_dom_document(): Creates an XML document with a root element `Library` and a child `Book`. # Create a new XML document doc = Document() # Create the root element `Library` library_element = doc.createElement(\'Library\') doc.appendChild(library_element) # Create the child element `Book` with attribute `category=\\"Fiction\\"` book_element = doc.createElement(\'Book\') book_element.setAttribute(\'category\', \'Fiction\') library_element.appendChild(book_element) # Add `Title`, `Author`, and `Year` children to `Book` title_element = doc.createElement(\'Title\') title_text = doc.createTextNode(\'The Great Gatsby\') title_element.appendChild(title_text) book_element.appendChild(title_element) author_element = doc.createElement(\'Author\') author_text = doc.createTextNode(\'F. Scott Fitzgerald\') author_element.appendChild(author_text) book_element.appendChild(author_element) year_element = doc.createElement(\'Year\') year_text = doc.createTextNode(\'1925\') year_element.appendChild(year_text) book_element.appendChild(year_element) # Return the string representation of the XML document return doc.toprettyxml() def parse_and_query_dom(xml_content): Parses XML content and returns a list of tuples with the title and author of each book. # Parse the XML content into a DOM document object dom = parseString(xml_content) # Find all `Book` elements book_elements = dom.getElementsByTagName(\'Book\') # Initialize the result list result = [] # Iterate over each `Book` element and extract `Title` and `Author` for book in book_elements: title_element = book.getElementsByTagName(\'Title\')[0] author_element = book.getElementsByTagName(\'Author\')[0] title = title_element.childNodes[0].nodeValue author = author_element.childNodes[0].nodeValue result.append((title, author)) # Return the resulting list of tuples return result"},{"question":"**Objective:** Demonstrate your understanding of file and directory manipulation in Python using the `pathlib`, `shutil`, and `tempfile` modules. **Problem Statement:** You are tasked to write a Python function that organizes files in a specified directory. The function should perform the following operations: 1. **Create a temporary directory** using the `tempfile` module. 2. **Traverse the given directory** and categorize files into subdirectories within the temporary directory based on their file extensions. Each subdirectory should be named after the file extension (e.g., all `.txt` files go into a subdirectory named `txt`). 3. **Copy each file** from the original directory to the appropriate subdirectory in the temporary directory using `shutil`. 4. **Return the path** to the temporary directory containing the organized files. **Function Signature:** ```python def organize_files(dir_path: str) -> str: pass ``` **Input:** - `dir_path` (str): Path to the directory containing files to be organized. **Output:** - (str): Path to the temporary directory with the organized files. **Constraints:** - You may assume all files in `dir_path` have extensions. - The original directory may contain multiple files with the same extension. - The function should handle large directories efficiently. **Example:** Suppose the directory at `/path/to/source` contains the following files: ``` /path/to/source/file1.txt /path/to/source/file2.txt /path/to/source/image1.jpg /path/to/source/image2.png /path/to/source/document1.pdf ``` After executing `organize_files(\\"/path/to/source\\")`, the function should return a temporary directory path that contains: ``` /tmp/tmp12345/txt/file1.txt /tmp/tmp12345/txt/file2.txt /tmp/tmp12345/jpg/image1.jpg /tmp/tmp12345/png/image2.png /tmp/tmp12345/pdf/document1.pdf ``` **Implementation Details:** - Use the `pathlib` module for path manipulations. - Use the `shutil` module for copying files. - Use the `tempfile` module to create the temporary directory. **Note:** Ensure your code is well-optimized and handles any potential errors, such as missing directories or permission issues.","solution":"import tempfile import shutil from pathlib import Path def organize_files(dir_path: str) -> str: Organizes files in the given directory into subdirectories within a temporary directory based on file extensions. Args: dir_path (str): Path to the directory containing files to be organized. Returns: str: Path to the temporary directory with the organized files. dir_path = Path(dir_path) if not dir_path.is_dir(): raise ValueError(f\\"The directory {dir_path} does not exist\\") temp_dir = Path(tempfile.mkdtemp()) for file in dir_path.iterdir(): if file.is_file(): ext = file.suffix.lstrip(\'.\') ext_dir = temp_dir / ext ext_dir.mkdir(parents=True, exist_ok=True) shutil.copy(file, ext_dir / file.name) return str(temp_dir)"},{"question":"**Coding Assessment Question: Predicting Housing Prices using Random Forests in scikit-learn** Objective: You are provided with a dataset containing various features related to housing details (e.g., number of rooms, area, location-specific information, etc.) and their corresponding prices. Your task is to implement a function in Python that uses the Random Forest algorithm from scikit-learn to predict housing prices. Requirements: - Use the `RandomForestRegressor` from scikit-learn\'s `ensemble` module. - Implement a function `predict_housing_prices` that takes in the training data, training labels, test data, and returns the predicted prices for the test data. - Your function should handle necessary preprocessing steps such as handling missing values (if any) and scaling the features appropriately. - Evaluate your model using the Mean Absolute Error (MAE) metric and print the MAE of your predictions on a validation set. Input Format: - `train_data` (Pandas DataFrame): Training data with multiple features (e.g., number of rooms, area, etc.) - `train_labels` (Pandas Series): Training labels (house prices). - `test_data` (Pandas DataFrame): Test data with the same set of features as the training data. Output Format: - `predicted_prices` (Pandas Series): The predicted prices for the test data. Function Signature: ```python def predict_housing_prices(train_data: pd.DataFrame, train_labels: pd.Series, test_data: pd.DataFrame) -> pd.Series: pass ``` Constraints: - You are allowed to use only the `RandomForestRegressor` from scikit-learn for this implementation. - You should not use any automated machine learning tools. - Handle any missing values appropriately before fitting the model. - Ensure your code is efficient and runs within a reasonable time for large datasets. Performance Requirements: - The model should achieve a reasonable MAE on the validation set. There is no strict MAE threshold, but your model should be logically valid and produce sensible predictions. Example Usage: ```python import pandas as pd # Sample Data (replace with actual dataset) train_data = pd.DataFrame({ \'rooms\': [3, 2, 4], \'area\': [1200, 800, 1500], \'location_quality\': [7, 6, 8] }) train_labels = pd.Series([300000, 200000, 350000]) test_data = pd.DataFrame({ \'rooms\': [3, 2], \'area\': [1300, 850], \'location_quality\': [7.5, 6.5] }) predicted_prices = predict_housing_prices(train_data, train_labels, test_data) print(predicted_prices) ```","solution":"import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split from sklearn.metrics import mean_absolute_error from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline def predict_housing_prices(train_data, train_labels, test_data): Predict house prices using RandomForestRegressor. Parameters: - train_data (pd.DataFrame): Training data with multiple features. - train_labels (pd.Series): Training labels (house prices). - test_data (pd.DataFrame): Test data with the same set of features as the training data. Returns: - predicted_prices (pd.Series): The predicted prices for the test data. # Split the training data into a validation set X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.2, random_state=42) # Create a preprocessing and modeling pipeline pipeline = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), # Handle missing values (\'scaler\', StandardScaler()), # Scale features (\'model\', RandomForestRegressor(n_estimators=100, random_state=42)) ]) # Fit the pipeline with the training data pipeline.fit(X_train, y_train) # Predict on the validation set val_predictions = pipeline.predict(X_val) val_mae = mean_absolute_error(y_val, val_predictions) print(f\\"Validation MAE: {val_mae}\\") # Predict on the test set test_predictions = pipeline.predict(test_data) return pd.Series(test_predictions) # Sample Data for direct testing (to be replaced with actual dataset) train_data = pd.DataFrame({\'rooms\': [3, 2, 4], \'area\': [1200, 800, 1500], \'location_quality\': [7, 6, 8]}) train_labels = pd.Series([300000, 200000, 350000]) test_data = pd.DataFrame({\'rooms\': [3, 2], \'area\': [1300, 850], \'location_quality\': [7.5, 6.5]}) predicted_prices = predict_housing_prices(train_data, train_labels, test_data) print(predicted_prices)"},{"question":"**Objective**: Write a Python function that reads an AIFF file, processes some key parameters, and outputs specific information about the audio file. **Task**: You are provided with an `AIFF` or `AIFF-C` file. Your task is to write a function `process_aiff_file(file_path)` that takes the path of an AIFF file as input and performs the following actions: 1. Open the AIFF file in read mode. 2. Retrieve and print the following parameters from the file: - Number of audio channels. - Sample size. - Frame rate. - Number of frames. - Compression type (in human-readable form). - Compression name (in human-readable form). 3. Read all audio frames and store them. 4. Compute the duration of the audio in seconds and print it. 5. Close the AIFF file. **Input**: - `file_path`: A string representing the path to the AIFF file. **Output**: - Print the above-specified parameters. - Print the duration of the audio in seconds. **Requirements**: - Use the `aifc` module to handle the AIFF file. - Handle any potential exceptions that may arise when opening or reading the file. **Function Signature**: ```python def process_aiff_file(file_path: str) -> None: pass ``` **Example**: ```python process_aiff_file(\\"path/to/your/audiofile.aiff\\") # Expected Output: # Number of Channels: 2 # Sample Size (bytes): 2 # Frame Rate (frames/second): 44100 # Number of Frames: 176400 # Compression Type: NONE # Compression Name: not compressed # Duration (seconds): 4.0 ``` **Constraints**: - The function should operate within the standard computational limits. - Assume the provided file is not corrupted and is in a valid AIFF or AIFF-C format. **Hints**: - Use `aifc.open(file, mode)` to open the file. - Use methods like `getnchannels()`, `getsampwidth()`, `getframerate()`, `getnframes()`, `getcomptype()`, `getcompname()` to retrieve audio file parameters. - The duration of the audio can be computed using the formula: `duration = number of frames / frame rate`.","solution":"import aifc def process_aiff_file(file_path: str) -> None: try: with aifc.open(file_path, \'r\') as audio_file: # Retrieve basic audio parameters num_channels = audio_file.getnchannels() sample_size = audio_file.getsampwidth() frame_rate = audio_file.getframerate() num_frames = audio_file.getnframes() comp_type = audio_file.getcomptype() comp_name = audio_file.getcompname() # Print retrieved parameters print(f\'Number of Channels: {num_channels}\') print(f\'Sample Size (bytes): {sample_size}\') print(f\'Frame Rate (frames/second): {frame_rate}\') print(f\'Number of Frames: {num_frames}\') print(f\'Compression Type: {comp_type}\') print(f\'Compression Name: {comp_name}\') # Read all audio frames audio_frames = audio_file.readframes(num_frames) # Compute duration in seconds duration = num_frames / frame_rate print(f\'Duration (seconds): {duration:.2f}\') except Exception as e: print(f\'Error processing AIFF file: {e}\')"},{"question":"# PyTorch Advanced Control Flow: `torch.cond` You are tasked with implementing a PyTorch module that utilizes the `torch.cond` operator to decide between two different functions based on the properties of the input tensor. This question will assess your understanding of control flow in PyTorch and your ability to implement complex data-dependent operations. Problem Statement Create a PyTorch module called `SumGreaterOrEqualModule` that performs the following: - Accepts a tensor `x` as input. - Checks if the sum of the elements of `x` is greater than or equal to a given threshold. - If the sum is greater than or equal to the threshold, apply a function that returns the element-wise square of `x`. - If the sum is less than the threshold, apply a function that returns the element-wise square root of `x`. You are required to: 1. Define the `SumGreaterOrEqualModule` class that extends `torch.nn.Module`. 2. Implement the forward method of the class to use `torch.cond` to decide which operation to apply based on the sum of the input tensor. Input - A tensor `x` of any shape with non-negative elements. - A float `threshold` which is the value against which the sum of the tensor elements is compared. Output - A tensor of the same shape as `x` after applying either the element-wise square or the element-wise square root function. Constraints - The tensor `x` must contain non-negative elements to avoid issues with the square root operation. - You must use `torch.cond` as described in the documentation for the conditional operation. Example ```python import torch class SumGreaterOrEqualModule(torch.nn.Module): def __init__(self, threshold: float): super().__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn(x: torch.Tensor): return x ** 2 def false_fn(x: torch.Tensor): return torch.sqrt(x) return torch.cond(x.sum() >= self.threshold, true_fn, false_fn, (x,)) # Example usage: model = SumGreaterOrEqualModule(threshold=10.0) input_tensor = torch.tensor([1.0, 2.0, 3.0]) output = model(input_tensor) print(output) ``` In this example, if the sum of `input_tensor` is greater than or equal to 10.0, the output will be the element-wise square of `input_tensor`. Otherwise, the output will be the element-wise square root of `input_tensor`. Note - Ensure that your implementation handles tensors of various shapes and works efficiently. - Remember that `torch.cond` is a prototype feature and should be used accordingly with the given constraints and limitations.","solution":"import torch import torch.nn as nn class SumGreaterOrEqualModule(nn.Module): def __init__(self, threshold: float): super(SumGreaterOrEqualModule, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: sum_of_elements = x.sum() def true_fn(): return x ** 2 def false_fn(): return torch.sqrt(x) return true_fn() if sum_of_elements >= self.threshold else false_fn()"},{"question":"You are provided the following data representing sales figures for three different products (A, B, and C) over four quarters: ```python data = { \\"Product\\": [\\"A\\", \\"A\\", \\"A\\", \\"A\\", \\"B\\", \\"B\\", \\"B\\", \\"B\\", \\"C\\", \\"C\\", \\"C\\", \\"C\\"], \\"Quarter\\": [\\"Q1\\", \\"Q2\\", \\"Q3\\", \\"Q4\\", \\"Q1\\", \\"Q2\\", \\"Q3\\", \\"Q4\\", \\"Q1\\", \\"Q2\\", \\"Q3\\", \\"Q4\\"], \\"Sales\\": [10, 15, 7, 14, 12, 18, 9, 16, 11, 13, 8, 15] } ``` Using seaborn: 1. Create a bar plot that shows the sales figures for each product across the four quarters. 2. Customize the plot using `sns.set_theme()` by setting the style to `\\"whitegrid\\"` and using a pastel color palette. 3. Apply additional customizations to the plot by removing the top and right spines. 4. Add appropriate titles and labels to the plot for better clarity. **Function Signature:** ```python def create_custom_sales_plot(data: dict) -> None: pass ``` **Constraints:** - The function should display the plot using `plt.show()`. - Ensure the plot is legible and well-labeled. **Performance Requirements:** - The function should execute efficiently with the given dataset. **Expected Output:** A customized seaborn bar plot displaying the sales figures as described.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_custom_sales_plot(data: dict) -> None: Create and display a customized seaborn bar plot showing the sales figures for each product across the four quarters. # Convert data dictionary to pandas DataFrame df = pd.DataFrame(data) # Set seaborn theme and palette sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\") # Create bar plot bar_plot = sns.barplot(x=\\"Quarter\\", y=\\"Sales\\", hue=\\"Product\\", data=df) # Customize plot: Remove top and right spines sns.despine(left=True, bottom=True) # Add title and labels bar_plot.set_title(\\"Sales Figures for Products Across Quarters\\") bar_plot.set_xlabel(\\"Quarter\\") bar_plot.set_ylabel(\\"Sales\\") # Display the plot plt.show()"},{"question":"# Question: Implementing Python Development Mode Checks Objective You need to implement Python code that simulates some effects of Python\'s Development Mode without actually enabling it. The goal is to help understand underlying issues like resource leakage and memory-related problems. Instructions 1. **Function Implementation**: Implement a function `count_lines(file_path: str) -> int`. This function should: - Open the given file in read mode. - Count and return the number of lines in the file. - Ensure all resources (e.g., file handles) are properly closed after operations. - Raise a custom warning `ResourceWarning` if the file is not explicitly closed. 2. **Signal Handlers**: Implement a function `enable_fault_handlers()`. This function should: - Install custom handlers for the following signals: `SIGSEGV`, `SIGFPE`, `SIGABRT`, `SIGBUS`, and `SIGILL`. - When any of these signals are caught, raise a `RuntimeError` with a message indicating that a fault has been handled. 3. **Debug Asynchronous Operations**: Implement a function `enable_asyncio_debug()`. This function should: - Enable asyncio debug mode by setting appropriate debug flags. - Log a warning if any coroutine is not awaited properly using `warnings.warn`. Example Usage ```python import os import warnings def count_lines(file_path: str) -> int: # Your implementation here pass def enable_fault_handlers(): # Your implementation here pass def enable_asyncio_debug(): # Your implementation here pass # Example - Correct usage try: print(count_lines(\'README.txt\')) except ResourceWarning as rw: print(rw) # Example - Enable fault handlers enable_fault_handlers() # Example - Enable asyncio debug enable_asyncio_debug() ``` Constraints - Do not use the actual Python Development Mode or its environment variables. - Your implementation should simulate the warnings and error handling described. Performance Requirements - The `count_lines` function should efficiently handle files with up to 10 million lines without significant performance degradation. # Notes - Utilize context managers wherever appropriate to manage resources. - Use the `signal` module to register custom signal handlers. - For asynchronous debug mode, utilize the `asyncio` and `warnings` modules effectively. Remember, the goal is to create a toolset that helps in identifying issues akin to Python\'s Development Mode without relying on the actual mode.","solution":"import signal import warnings import asyncio class ResourceWarning(Warning): pass def count_lines(file_path: str) -> int: Count the number of lines in a given file. Raises a custom ResourceWarning if the file is not explicitly closed. try: with open(file_path, \'r\') as file: return sum(1 for _ in file) except Exception as e: raise ResourceWarning(f\\"Failed to process the file properly: {e}\\") def handle_fault(signum, frame): raise RuntimeError(f\\"Fault handled for signal: {signal.Signals(signum).name}\\") def enable_fault_handlers(): Install custom handlers for specific signals to simulate python development mode fault handlers. signals = [signal.SIGSEGV, signal.SIGFPE, signal.SIGABRT, signal.SIGBUS, signal.SIGILL] for sig in signals: signal.signal(sig, handle_fault) def enable_asyncio_debug(): Enable asyncio debug mode and log a warning if any coroutine is not awaited properly. warnings.warn(\\"Asyncio debug mode enabled\\", category=RuntimeWarning) asyncio.get_event_loop().set_debug(True) asyncio.coroutines._COROUTINE_TYPES += (asyncio.coroutines.CoroWrapper,) # Enable wrapping for asyncio coroutines"},{"question":"# Floating-Point Representation and Precision Control Problem Statement Computers represent floating-point numbers as binary fractions, which can lead to precision errors when performing arithmetic operations. For this exercise, you need to implement a function that compares the sum of a list of floating-point numbers using standard summation and a precision-controlled summation. Your task is to determine if the sums are equal or not. The Function ```python def compare_sums(float_list): Compares the sum of floating-point numbers using standard summation and precision-controlled summation. Parameters: float_list (list of float): A list of floating-point numbers. Returns: bool: True if both sums are equal, False otherwise. # Your implementation here ``` Input - `float_list` is a list of floating-point numbers. (0.0 < len(float_list) <= 10^4) - The values in the list can be both positive and negative, e.g., `float_list = [0.1, 0.2, 0.3, -0.1, -0.2]`. Output - Return `True` if the sum of `float_list` using both methods (standard summation and precision-controlled summation) are equal, else `False`. Note that due to floating-point precision issues, direct comparison of sums might be impractical without rounding or specific summation techniques. Constraints 1. Use standard summation `sum()` for one of the comparisons. 2. Use `math.fsum()` for the other method to control precision. Example ```python compare_sums([0.1, 0.2, 0.3, -0.1, -0.2]) # Output should be False because: # sum([0.1, 0.2, 0.3, -0.1, -0.2]) equals 0.29999999999999993 # math.fsum([0.1, 0.2, 0.3, -0.1, -0.2]) equals 0.3 ``` Explanation The example shows how slight discrepancies in floating-point arithmetic lead to different results when using standard summation versus precision-controlled summation. This illustrates the necessity of understanding floating-point representation errors and how to mitigate them in Python.","solution":"import math def compare_sums(float_list): Compares the sum of floating-point numbers using standard summation and precision-controlled summation. Parameters: float_list (list of float): A list of floating-point numbers. Returns: bool: True if both sums are equal, False otherwise. standard_sum = sum(float_list) precision_sum = math.fsum(float_list) return standard_sum == precision_sum"},{"question":"Objective: To assess students\' understanding of Python classes, inheritance, instance variables, class variables, and iterators. Question: You are tasked with creating a class hierarchy to model an online shopping cart system. Implement the following classes and methods: 1. **`Product` Class**: - **Attributes**: - `name` (string): The name of the product. - `price` (float): The price of the product. - **Methods**: - `__init__(self, name: str, price: float)`: Initializes a product with a name and a price. - `__str__(self)`: Returns the product details as a string in the format `\\"Product(name: <name>, price: <price>)\\"`. 2. **`CartItem` Class**: - **Attributes**: - `product` (`Product`): The product associated with this cart item. - `quantity` (int): The quantity of the product. - **Methods**: - `__init__(self, product: Product, quantity: int)`: Initializes the cart item with a product and a quantity. - `item_total(self)`: Returns the total price for this cart item (product price * quantity). 3. **`Cart` Class**: - **Attributes**: - `items` (list of `CartItem`): A list of items in the cart. - **Methods**: - `__init__(self)`: Initializes an empty shopping cart. - `add_item(self, product: Product, quantity: int)`: Adds a new item to the cart. If the product already exists in the cart, it should update the quantity. - `remove_item(self, product: Product)`: Removes an item from the cart based on the product. - `total_cost(self)`: Returns the total cost of all items in the cart. - `__iter__(self)`: Returns an iterator for the cart items. - `__str__(self)`: Returns a string representation of the cart, listing all items and the total cost. Constraints: 1. The `Product` names are unique within the cart. 2. Updating the quantity of a product in the cart is supported. 3. The `Cart` class should be iterable, allowing iteration through its `CartItem` instances. Performance: 1. The `add_item` and `remove_item` operations should run in linear time with respect to the number of items in the cart. Example Usage: ```python # Create products p1 = Product(\\"Laptop\\", 999.99) p2 = Product(\\"Phone\\", 599.99) p3 = Product(\\"Tablet\\", 399.99) # Create a shopping cart cart = Cart() # Add items to the cart cart.add_item(p1, 1) cart.add_item(p2, 2) cart.add_item(p3, 3) # Print cart details print(cart) # Update quantity and remove item cart.add_item(p1, 1) cart.remove_item(p2) # Print updated cart details print(cart) # Iterate through cart items for item in cart: print(item.product.name, item.quantity, item.item_total()) ``` Expected Output: ``` Cart contains: Product(name: Laptop, price: 999.99) x 1 - Total: 999.99 Product(name: Phone, price: 599.99) x 2 - Total: 1199.98 Product(name: Tablet, price: 399.99) x 3 - Total: 1199.97 Total cost: 3399.94 Cart contains: Product(name: Laptop, price: 999.99) x 2 - Total: 1999.98 Product(name: Tablet, price: 399.99) x 3 - Total: 1199.97 Total cost: 3199.95 Laptop 2 1999.98 Tablet 3 1199.97 ``` Note: - Use appropriate exception handling for cases such as removing a non-existent item. - Ensure proper encapsulation and member access control where applicable.","solution":"class Product: def __init__(self, name: str, price: float): self.name = name self.price = price def __str__(self): return f\\"Product(name: {self.name}, price: {self.price:.2f})\\" class CartItem: def __init__(self, product: Product, quantity: int): self.product = product self.quantity = quantity def item_total(self): return self.product.price * self.quantity class Cart: def __init__(self): self.items = [] def add_item(self, product: Product, quantity: int): for item in self.items: if item.product.name == product.name: item.quantity += quantity return self.items.append(CartItem(product, quantity)) def remove_item(self, product: Product): self.items = [item for item in self.items if item.product.name != product.name] def total_cost(self): return sum(item.item_total() for item in self.items) def __iter__(self): return iter(self.items) def __str__(self): result = \\"Cart contains:n\\" for item in self.items: result += f\\"{str(item.product)} x {item.quantity} - Total: {item.item_total():.2f}n\\" result += f\\"Total cost: {self.total_cost():.2f}\\" return result"},{"question":"# Advanced Python 310 Coding Assessment: Custom Tuple Management Objective Implement a Python function that interacts with tuple objects in a sophisticated way, utilizing some of the key functions provided by the python310 package. Problem Statement You are required to write a Python class, `CustomTuple`, that encapsulates a list of tuples and provides methods to: 1. Create a new tuple and add it to the list. 2. Retrieve an element from a specific tuple by its index. 3. Get the size of a specific tuple. 4. Resize a specific tuple. 5. Get a slice of a specific tuple. The `CustomTuple` class should offer an interface to perform these operations using the Python C API functionality related to tuples. Implementation Details - Define the class `CustomTuple`. - Maintain an internal list to store the tuples. - Implement the following methods using the provided Python C API functions: 1. **add_tuple(self, size)**: - Create a new tuple of the specified size and add it to the internal list. - Return the index of the newly added tuple. 2. **get_item(self, tuple_index, pos)**: - Retrieve the item at position `pos` from the tuple at `tuple_index`. - Return the retrieved item. 3. **get_size(self, tuple_index)**: - Return the size of the tuple at `tuple_index`. 4. **resize_tuple(self, tuple_index, newsize)**: - Resize the tuple at `tuple_index` to the new size `newsize`. - Return `True` if successful, else `False`. 5. **get_slice(self, tuple_index, low, high)**: - Return the slice of the tuple at `tuple_index` from `low` to `high`. Constraints - Assume that all tuple indices and positions provided are valid and within range for simplification. - `size` and `newsize` will be non-negative integers. Example Usage: ```python ct = CustomTuple() # Adding new tuples index1 = ct.add_tuple(3) # Adds a tuple of size 3 index2 = ct.add_tuple(5) # Adds a tuple of size 5 # Manipulating and retrieving data item = ct.get_item(index1, 0) # Retrieves the first item of the first tuple size = ct.get_size(index2) # Retrieves the size of the second tuple success = ct.resize_tuple(index1, 4) # Resizes the first tuple to size 4 slice_tuple = ct.get_slice(index2, 1, 3) # Retrieves a slice from the second tuple print(item) print(size) print(success) print(slice_tuple) ``` Performance Requirements - Efficiency in adding and retrieving tuple elements is critical. - Operations should handle tuples of considerable size without significant performance degradation. Implement the `CustomTuple` class according to these requirements.","solution":"class CustomTuple: def __init__(self): self.tuples = [] def add_tuple(self, size): new_tuple = tuple([None] * size) self.tuples.append(new_tuple) return len(self.tuples) - 1 def get_item(self, tuple_index, pos): return self.tuples[tuple_index][pos] def get_size(self, tuple_index): return len(self.tuples[tuple_index]) def resize_tuple(self, tuple_index, newsize): old_tuple = self.tuples[tuple_index] if newsize < len(old_tuple): self.tuples[tuple_index] = old_tuple[:newsize] else: self.tuples[tuple_index] = old_tuple + (None,) * (newsize - len(old_tuple)) return True def get_slice(self, tuple_index, low, high): return self.tuples[tuple_index][low:high]"},{"question":"# Advanced Python Coding Assessment Question Context You are required to implement an asynchronous file downloader using Python’s `asyncio` module. This downloader must be compatible across different platforms, particularly focusing on handling files and subprocesses considering platform-specific limitations. Problem Statement Write a Python function using the `asyncio` module to download multiple files concurrently from given URLs and save them locally. The function must handle different asynchronous event loops based on the platform it is running on, specifically Windows and Unix-based systems (including macOS). Function Signature ```python import asyncio async def download_files(urls: list, local_dir: str): pass ``` Input - `urls` : a list of strings, each string is a URL from which a file needs to be downloaded. - `local_dir` : a string representing the local directory where the downloaded files will be saved. Output - The function should not return anything. It should save the files in the specified local directory. Constraints 1. The number of URLs (n) will be between 1 and 100. 2. The local directory will always exist and will have the right permissions for writing files. 3. Each file download must be handled asynchronously. Requirements - The function must check the running platform: - On Windows, use the `ProactorEventLoop`. - On Unix-based systems, use the default event loop. - Implement error handling for failed downloads; the function should catch exceptions and log the URL of the failed download without stopping the entire download process. - Ensure that the function works efficiently and can handle the maximum number of URLs within reasonable time constraints. Example Usage ```python urls = [\'http://example.com/file1.txt\', \'http://example.com/file2.txt\'] local_dir = \'/path/to/local/dir\' asyncio.run(download_files(urls, local_dir)) ``` Hints - Consider using `aiohttp` or a similar library for handling HTTP requests asynchronously. - Make use of `asyncio.gather` to manage multiple concurrent download tasks. - Implement platform-specific adjustments within the function for handling the event loop appropriately. Good luck! This exercise will test your ability to work with asynchronous programming in Python while considering platform-specific constraints.","solution":"import asyncio import aiohttp import os import sys async def download_file(session, url, local_dir): try: async with session.get(url) as response: if response.status == 200: filename = os.path.join(local_dir, os.path.basename(url)) with open(filename, \'wb\') as file: while True: chunk = await response.content.read(1024) if not chunk: break file.write(chunk) else: print(f\\"Failed to download {url} with status {response.status}\\") except Exception as e: print(f\\"Exception occurred while downloading {url}: {e}\\") async def download_files(urls: list, local_dir: str): async with aiohttp.ClientSession() as session: tasks = [download_file(session, url, local_dir) for url in urls] await asyncio.gather(*tasks, return_exceptions=True) def main(urls, local_dir): is_windows = sys.platform.startswith(\'win\') if is_windows: loop = asyncio.ProactorEventLoop() else: loop = asyncio.new_event_loop() asyncio.set_event_loop(loop) try: loop.run_until_complete(download_files(urls, local_dir)) finally: loop.close()"},{"question":"Objective: The objective of this assignment is to assess your ability to use the `xml.dom.minidom` package to parse XML data, manipulate the DOM structure, and serialize the updated XML document. Problem Statement: You are given an XML string that represents a simple slideshow with multiple slides. Each slide has a title and several points. Your task is to write a function `add_slide(xml_string, title, points)` that takes the following inputs: - `xml_string` (str): A string representing the XML content of an existing slideshow. - `title` (str): The title of the new slide to be added. - `points` (list of str): A list of points to be added under the new slide. The function should return the updated XML string with the new slide added to the slideshow. Example: Given the following XML string: ```xml <slideshow> <slide> <title>Slide 1</title> <point>Point 1</point> <point>Point 2</point> </slide> <slide> <title>Slide 2</title> <point>Point A</point> </slide> </slideshow> ``` If the function is called as: ```python xml_string = \'\'\'<slideshow> <slide> <title>Slide 1</title> <point>Point 1</point> <point>Point 2</point> </slide> <slide> <title>Slide 2</title> <point>Point A</point> </slide> </slideshow>\'\'\' title = \\"Slide 3\\" points = [\\"Point X\\", \\"Point Y\\"] updated_xml = add_slide(xml_string, title, points) ``` Then `updated_xml` should contain: ```xml <slideshow> <slide> <title>Slide 1</title> <point>Point 1</point> <point>Point 2</point> </slide> <slide> <title>Slide 2</title> <point>Point A</point> </slide> <slide> <title>Slide 3</title> <point>Point X</point> <point>Point Y</point> </slide> </slideshow> ``` Constraints: 1. The input XML string is always well-formed. 2. The list of points for the new slide always contains at least one point. 3. The function should maintain the order of the slides as they are in the XML string. Function Signature: ```python import xml.dom.minidom def add_slide(xml_string: str, title: str, points: list) -> str: # Your implementation here ``` Instructions: 1. Parse the input XML string to obtain a DOM object. 2. Create a new slide element with the given title and points. 3. Append the new slide to the existing slideshow. 4. Serialize the updated DOM object back to an XML string. 5. Return the updated XML string. Good luck!","solution":"import xml.dom.minidom def add_slide(xml_string: str, title: str, points: list) -> str: # Parse the input XML string to obtain a DOM object dom = xml.dom.minidom.parseString(xml_string) # Create a new slide element slideshow = dom.getElementsByTagName(\'slideshow\')[0] new_slide = dom.createElement(\'slide\') # Create and append the title element to the slide title_element = dom.createElement(\'title\') title_text = dom.createTextNode(title) title_element.appendChild(title_text) new_slide.appendChild(title_element) # Create and append point elements to the slide for point in points: point_element = dom.createElement(\'point\') point_text = dom.createTextNode(point) point_element.appendChild(point_text) new_slide.appendChild(point_element) # Append the slide to the slideshow slideshow.appendChild(new_slide) # Serialize the updated DOM object back to an XML string updated_xml = dom.toxml() return updated_xml"},{"question":"# Advanced Python Coding Assessment **Objective**: Demonstrate your understanding of asynchronous programming with Python\'s `asyncio` module. **Problem Statement**: You are required to implement an async function that simulates querying a set of data sources concurrently. Each data source will be a coroutine that takes some time to return its result. Your task is to write a function `fetch_all_data_in_time` that tries to fetch data from all sources within a given timeout. If the function cannot fetch data from all sources within the timeout, it should return the results obtained so far and indicate which sources timed out. # Requirements: 1. **Function Signature**: ```python async def fetch_all_data_in_time(sources: List[Callable[[], Awaitable]], timeout: float) -> Tuple[Dict[int, Any], List[int]]: ``` 2. **Parameters**: - `sources`: A list of callables, each representing a coroutine function that fetches data from a data source. Each callable does not take any arguments and returns an awaitable object. - `timeout`: A float representing the maximum amount of time (in seconds) to wait for all data fetch operations to complete. 3. **Return**: - A tuple with two elements: - A dictionary where the keys are the indices of the data sources in the input list and the values are the results from those data sources. - A list of indices of the data sources that timed out. # Constraints: - Use `asyncio.gather` and `asyncio.wait_for` to manage concurrency and timeouts. - Ensure that if fetching from a source times out, it is recorded in the result properly without affecting other fetch operations. # Example Usage: ```python import asyncio import random async def data_source_1(): await asyncio.sleep(random.uniform(0.1, 2.0)) return \\"data from source 1\\" async def data_source_2(): await asyncio.sleep(random.uniform(0.1, 2.0)) return \\"data from source 2\\" async def data_source_3(): await asyncio.sleep(random.uniform(0.1, 2.0)) return \\"data from source 3\\" sources = [data_source_1, data_source_2, data_source_3] timeout = 1.5 results, timeouts = await fetch_all_data_in_time(sources, timeout) # Example output: # results -> {0: \\"data from source 1\\", 2: \\"data from source 3\\"} # timeouts -> [1] ``` # Implementation: ```python from typing import List, Callable, Awaitable, Dict, Tuple import asyncio async def fetch_all_data_in_time(sources: List[Callable[[], Awaitable]], timeout: float) -> Tuple[Dict[int, Any], List[int]]: tasks = {i: asyncio.create_task(source()) for i, source in enumerate(sources)} results = {} timeouts = [] for i, task in tasks.items(): try: results[i] = await asyncio.wait_for(task, timeout) except asyncio.TimeoutError: timeouts.append(i) return results, timeouts # Running the example usage async def main(): import random async def data_source_1(): await asyncio.sleep(random.uniform(0.1, 2.0)) return \\"data from source 1\\" async def data_source_2(): await asyncio.sleep(random.uniform(0.1, 2.0)) return \\"data from source 2\\" async def data_source_3(): await asyncio.sleep(random.uniform(0.1, 2.0)) return \\"data from source 3\\" sources = [data_source_1, data_source_2, data_source_3] timeout = 1.5 results, timeouts = await fetch_all_data_in_time(sources, timeout) print(\\"Results:\\", results) print(\\"Timeouts:\\", timeouts) # Uncomment the following line to run the test # asyncio.run(main()) ``` # Notes: - This assessment focuses on your ability to handle asynchronous programming, timeouts, and task management using the `asyncio` module. - Write clean and efficient code while maintaining proper error handling for asynchronous operations.","solution":"from typing import List, Callable, Awaitable, Dict, Tuple, Any import asyncio async def fetch_all_data_in_time(sources: List[Callable[[], Awaitable]], timeout: float) -> Tuple[Dict[int, Any], List[int]]: tasks = {i: asyncio.create_task(source()) for i, source in enumerate(sources)} results = {} timeouts = [] for i, task in tasks.items(): try: results[i] = await asyncio.wait_for(task, timeout) except asyncio.TimeoutError: timeouts.append(i) return results, timeouts"},{"question":"# Dimensionality Reduction and Comparison Using PCA and Feature Agglomeration You are provided with a dataset consisting of multiple features. Your task is to implement a pipeline that applies two different dimensionality reduction techniques: Principal Component Analysis (PCA) and Feature Agglomeration. You will then evaluate and compare the performance of both techniques on the reduced datasets using a supervised learning algorithm. **Dataset:** You can use any suitable dataset from `sklearn.datasets` such as the Iris dataset or Breast Cancer dataset for this assessment. You may use the `load_iris` or `load_breast_cancer` function to load these datasets as required. **Tasks:** 1. Load the dataset and split it into training and testing sets. Use 75% of the data for training and 25% for testing. 2. Implement a pipeline that includes: - `StandardScaler` for scaling the features. - PCA for reducing the dimensionality of the dataset to 2 components. - A supervised learning model (e.g., Logistic Regression) to classify the data. 3. Implement a second pipeline that includes: - `StandardScaler` for scaling the features. - Feature Agglomeration for reducing the dimensionality of the dataset to 2 clusters. - The same supervised learning model used in the first pipeline for classification. 4. Train both pipelines on the training data. 5. Evaluate both pipelines on the testing data and compare their performance using: - Accuracy score - Confusion matrix **Constraints:** - You must use `sklearn.pipeline` for creating the pipelines. - The dimensionality of the dataset must be reduced to 2 components/clusters. - Use Logistic Regression as the supervised learning model. **Expected Functions and Methods:** - `load_<dataset>` for dataset loading. - `train_test_split` for splitting data. - `Pipeline` for creating the pipelines. - `StandardScaler`, `PCA`, `FeatureAgglomeration`, and `LogisticRegression` for respective transformations and modeling. - `accuracy_score` and `confusion_matrix` for evaluation. **Python Code:** ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.cluster import FeatureAgglomeration from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, confusion_matrix # Load dataset data = load_iris() X, y = data.data, data.target # Split into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) # Define and train PCA pipeline pca_pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=2)), (\'logreg\', LogisticRegression()) ]) pca_pipeline.fit(X_train, y_train) # Define and train Feature Agglomeration pipeline agglo_pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'agglo\', FeatureAgglomeration(n_clusters=2)), (\'logreg\', LogisticRegression()) ]) agglo_pipeline.fit(X_train, y_train) # Evaluate PCA pipeline pca_predictions = pca_pipeline.predict(X_test) pca_accuracy = accuracy_score(y_test, pca_predictions) pca_confusion = confusion_matrix(y_test, pca_predictions) # Evaluate Feature Agglomeration pipeline agglo_predictions = agglo_pipeline.predict(X_test) agglo_accuracy = accuracy_score(y_test, agglo_predictions) agglo_confusion = confusion_matrix(y_test, agglo_predictions) # Print evaluation results print(f\'PCA Pipeline Accuracy: {pca_accuracy}\') print(f\'PCA Pipeline Confusion Matrix:n{pca_confusion}\') print(f\'Feature Agglomeration Pipeline Accuracy: {agglo_accuracy}\') print(f\'Feature Agglomeration Pipeline Confusion Matrix:n{agglo_confusion}\') ``` Explain the results and any differences observed between the two approaches.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.cluster import FeatureAgglomeration from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, confusion_matrix def load_and_split_data(): # Load dataset data = load_iris() X, y = data.data, data.target # Split into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) return X_train, X_test, y_train, y_test def create_pca_pipeline(): # Define PCA pipeline pca_pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=2)), (\'logreg\', LogisticRegression()) ]) return pca_pipeline def create_agglo_pipeline(): # Define Feature Agglomeration pipeline agglo_pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'agglo\', FeatureAgglomeration(n_clusters=2)), (\'logreg\', LogisticRegression()) ]) return agglo_pipeline def train_and_evaluate_pipeline(pipeline, X_train, X_test, y_train, y_test): # Train pipeline pipeline.fit(X_train, y_train) # Evaluate pipeline predictions = pipeline.predict(X_test) accuracy = accuracy_score(y_test, predictions) confusion = confusion_matrix(y_test, predictions) return accuracy, confusion # Load and split data X_train, X_test, y_train, y_test = load_and_split_data() # Create and evaluate PCA pipeline pca_pipeline = create_pca_pipeline() pca_accuracy, pca_confusion = train_and_evaluate_pipeline(pca_pipeline, X_train, X_test, y_train, y_test) # Create and evaluate Feature Agglomeration pipeline agglo_pipeline = create_agglo_pipeline() agglo_accuracy, agglo_confusion = train_and_evaluate_pipeline(agglo_pipeline, X_train, X_test, y_train, y_test) # Print evaluation results print(f\'PCA Pipeline Accuracy: {pca_accuracy}\') print(f\'PCA Pipeline Confusion Matrix:n{pca_confusion}\') print(f\'Feature Agglomeration Pipeline Accuracy: {agglo_accuracy}\') print(f\'Feature Agglomeration Pipeline Confusion Matrix:n{agglo_confusion}\')"},{"question":"# Python Coding Assessment: Custom Import System Implementation Objective Design and implement a custom import mechanism using Python\'s import system, involving custom finders and loaders. Problem Statement You are required to implement a custom import mechanism that can load modules from both the local file system and a remote URL. This custom importer must: 1. Check the local file system first. 2. If not found locally, attempt to retrieve the module from a specified remote URL. 3. Cache the imported modules within the custom importer to avoid repeated downloads for the same module. 4. Integrate your custom importer into Python\'s import machinery. Instructions 1. **Create a custom finder**: Implement a finder that checks both the local file system and a remote URL for Python modules. 2. **Create a custom loader**: Implement a loader that can load the module code either from a local file or a remote URL. 3. **Cache imported modules**: Ensure already imported modules are cached to optimize performance. 4. **Integrate the importer**: Integrate your custom finder and loader into Python’s import system using import hooks. Specifications 1. **Class: LocalAndRemoteFinder** - Methods: - `find_spec(fullname: str, path: Optional[str], target=None) -> Optional[importlib.machinery.ModuleSpec]`: Find the module specification. 2. **Class: LocalAndRemoteLoader** - Methods: - `create_module(spec: importlib.machinery.ModuleSpec) -> Optional[types.ModuleType]`: Optionally create a new module object. - `exec_module(module: types.ModuleType)`: Execute the module in its own namespace. 3. **Function: install_custom_importer(remote_base_url: str) -> None** - Description: This function should install the custom importer into Python’s import machinery. - Argument: - `remote_base_url`: The base URL from where to attempt to retrieve the modules if not found locally. 4. **Caching Mechanism**: Implement caching so that repeated imports of the same module are optimized. Example Usage: ```python # Contents of some_module.py stored locally # def some_function(): # return \\"Loaded from local file system\\" # Simulate a module available at http://example.com/some_remote_module.py # def some_remote_function(): # return \\"Loaded from remote URL\\" import some_module print(some_module.some_function()) # Output: Loaded from local file system import some_remote_module print(some_remote_module.some_remote_function()) # Output: Loaded from remote URL ``` Constraints - Use `importlib` and Python standard libraries only. - The caching mechanism should store the module objects in memory. - Internet access should be simulated using a mock library for testing purposes. Notes - This question tests the understanding of Python\'s advanced import system, integrating custom finders and loaders, and effectively using import hooks. - Students should demonstrate the ability to think about performance optimization through caching. - Proper error handling and fallback mechanisms should be implemented to manage scenarios where modules cannot be found locally or remotely.","solution":"import importlib.abc import importlib.util import sys import requests import types import os from urllib.parse import urljoin class LocalAndRemoteFinder(importlib.abc.MetaPathFinder): def __init__(self, remote_base_url): self.remote_base_url = remote_base_url self.module_cache = {} def find_spec(self, fullname, path, target=None): if fullname in self.module_cache: return self.module_cache[fullname] file_path = fullname.replace(\'.\', \'/\') + \'.py\' if os.path.exists(file_path): loader = LocalAndRemoteLoader(fullname, file_path) spec = importlib.util.spec_from_loader(fullname, loader) self.module_cache[fullname] = spec return spec remote_url = urljoin(self.remote_base_url, file_path) if self._url_exists(remote_url): loader = LocalAndRemoteLoader(fullname, remote_url, is_remote=True) spec = importlib.util.spec_from_loader(fullname, loader) self.module_cache[fullname] = spec return spec return None def _url_exists(self, url): try: response = requests.head(url) return response.status_code == 200 except requests.RequestException: return False class LocalAndRemoteLoader(importlib.abc.Loader): def __init__(self, fullname, location, is_remote=False): self.fullname = fullname self.location = location self.is_remote = is_remote def create_module(self, spec): return None def exec_module(self, module): if self.is_remote: response = requests.get(self.location) response.raise_for_status() code = response.text exec(code, module.__dict__) else: with open(self.location, \'r\') as file: code = file.read() exec(code, module.__dict__) def install_custom_importer(remote_base_url): sys.meta_path.insert(0, LocalAndRemoteFinder(remote_base_url))"},{"question":"**Color Palette Customization and Application with seaborn** **Objective:** Your task is to create several custom color palettes using seaborn\'s `sns.husl_palette` function and apply these palettes to a plot, demonstrating your understanding of both color theory and seaborn\'s configuration options. **Instructions:** * Create a function `custom_husl_palettes(data)` that receives a pandas DataFrame, generates three different customized color palettes, and applies each palette to a seaborn plot. * Your function should: 1. Create three different customized color palettes using `sns.husl_palette`. 2. Plot three separate seaborn bar plots for the same data using the different palettes created. 3. Return the three plots generated. **Requirements:** 1. The first palette should have 10 colors with default lightness and saturation. 2. The second palette should have 5 colors with a lightness level of 0.5 and saturation level of 0.8. 3. The third palette should have 6 colors with a starting hue point of 0.75 and should be continuous (i.e., a colormap). * Use the Iris dataset from seaborn as sample data to demonstrate the functionality of your function. **Constraints:** - You must use seaborn\'s `sns.husl_palette` function for creating the palettes. - Ensure that the plots generated are distinguishable by their color schemes. **Example Input:** ```python import seaborn as sns import pandas as pd # Load example dataset data = sns.load_dataset(\\"iris\\") # Function implementation (to be written by student by analyzing the requirements) plots = custom_husl_palettes(data) ``` **Expected Output:** - Three bar plots, each showing the count of Iris species where each plot uses a different color palette as specified. **Notes:** - The function shall be tested on the Iris dataset but should be designed to handle any dataset with categorical variables that can be visualized with a bar plot. - Ensure code is clean, well-commented, and follows best practices in function implementation and seaborn usage.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def custom_husl_palettes(data): Generates three different customized color palettes using sns.husl_palette and applies each palette to a seaborn bar plot demonstrating the count of species in the given dataset. Arguments: data -- pandas DataFrame containing the dataset with categorical variables. Returns: A tuple of three Matplotlib Axes objects with the generated plots. # Create three different customized HUSL color palettes palette1 = sns.husl_palette(10) # 10 colors with default lightness and saturation palette2 = sns.husl_palette(5, l=0.5, s=0.8) # 5 colors with lightness of 0.5 and saturation of 0.8 palette3 = sns.husl_palette(6, h=0.75) # 6 colors with starting hue point of 0.75 # Prepare the figure for plots fig, axes = plt.subplots(3, 1, figsize=(10, 15), sharex=True) # Create the first plot sns.countplot(x=\'species\', data=data, palette=palette1, ax=axes[0]) axes[0].set_title(\'Plot with Palette 1 (10 colors)\') # Create the second plot sns.countplot(x=\'species\', data=data, palette=palette2, ax=axes[1]) axes[1].set_title(\'Plot with Palette 2 (5 colors)\') # Create the third plot sns.countplot(x=\'species\', data=data, palette=palette3, ax=axes[2]) axes[2].set_title(\'Plot with Palette 3 (6 colors)\') plt.tight_layout() return axes"},{"question":"# **Coding Assessment Question** **Objective:** Implement a Python program that sorts emails in a Maildir mailbox into subfolders based on their senders. The emails should be moved to a folder named after the sender\'s domain (e.g., emails from `user@example.com` should be moved to a folder named `example.com`). You should demonstrate an understanding of mailbox manipulation, format-specific operations, and concurrent modification concerns. **Requirements:** - Read messages from a specified Maildir mailbox. - Move messages to new subfolders named after the sender\'s email domain. - Implement mailbox locking and unlocking to safely handle concurrent modifications. - Ensure the program can handle and skip malformed messages without crashing. - New subfolders should be created if they do not exist. - The original messages should be deleted after being copied to the new subfolders. - Do not use third-party libraries; rely solely on Python\'s standard library. **Input:** - Path to the Maildir mailbox (directory). **Output:** - The program should print the number of messages sorted into each subfolder. **Constraints:** - Assume the Maildir mailbox follows the standard directory structure with `cur`, `new`, and `tmp`. - The program should be efficient and should not hold unnecessary locks longer than required. - The sender’s email domain should be extracted from the `From` header of the email. **Template:** ```python import mailbox import os import email.errors def sort_emails_by_sender_domain(maildir_path): try: inbox = mailbox.Maildir(maildir_path, create=False) sorted_counts = {} for key in inbox.iterkeys(): try: message = inbox[key] except email.errors.MessageParseError: continue # Skip malformed messages from_header = message[\'from\'] if from_header: # Extract domain from sender email domain = from_header.split(\'@\')[1] # Ensure the folder exists folder_path = os.path.join(maildir_path, f\'.{domain}\') if not os.path.exists(folder_path): inbox.add_folder(domain) # Move the message to the folder folder = inbox.get_folder(domain) folder.lock() folder.add(message) folder.flush() folder.unlock() # Remove the original message after copying inbox.lock() inbox.discard(key) inbox.flush() inbox.unlock() # Update count of sorted messages sorted_counts[domain] = sorted_counts.get(domain, 0) + 1 # Print the sorted message counts for domain, count in sorted_counts.items(): print(f\'{count} messages sorted to {domain}\') except mailbox.NoSuchMailboxError: print(f\'No such mailbox: {maildir_path}\') except Exception as e: print(f\'An error occurred: {e}\') # Example usage (assuming Maildir mailbox at \'~/Maildir\'): # sort_emails_by_sender_domain(\'~/Maildir\') ``` **Notes:** - Replace `\'~/Maildir\'` with the actual Maildir directory path on your system for testing. - Handle any potential errors gracefully and provide meaningful error messages. **Performance Requirements:** - The solution should efficiently handle a large number of emails without unnecessary memory overhead. - Ensure that processed messages are removed promptly to avoid reprocessing in case of interruptions.","solution":"import mailbox import os import email.errors def sort_emails_by_sender_domain(maildir_path): try: inbox = mailbox.Maildir(maildir_path) sorted_counts = {} for key in inbox.iterkeys(): try: message = inbox[key] except email.errors.MessageParseError: continue # Skip malformed messages from_header = message[\'from\'] if from_header: try: # Extract domain from sender email domain = from_header.split(\'@\')[1].split(\'>\')[0] # in case includes \\">\\" except IndexError: continue # Skip if the from header is malformed # Ensure the folder exists folder_path = os.path.join(maildir_path, f\'.{domain}\') if not os.path.exists(folder_path): inbox.add_folder(domain) # Move the message to the folder folder = inbox.get_folder(domain) folder.lock() folder.add(message) folder.flush() folder.unlock() # Remove the original message after copying inbox.lock() inbox.discard(key) inbox.flush() inbox.unlock() # Update count of sorted messages sorted_counts[domain] = sorted_counts.get(domain, 0) + 1 # Print the sorted message counts for domain, count in sorted_counts.items(): print(f\'{count} messages sorted to {domain}\') except mailbox.NoSuchMailboxError: print(f\'No such mailbox: {maildir_path}\') except Exception as e: print(f\'An error occurred: {e}\') # Example usage (assuming Maildir mailbox at \'~/Maildir\'): # sort_emails_by_sender_domain(\'~/Maildir\')"},{"question":"# Task Implement a Python function `extract_email_addresses(text: str) -> List[str]` that extracts all valid email addresses from a given text. The function should use the `re` module to identify and return the email addresses. # Email Format The email addresses should: 1. Start with a sequence of alphanumeric characters, underscores, periods, or plus signs. 2. Followed by the `@` symbol. 3. Followed by a domain name which includes only alphanumeric characters and periods. 4. End with a top-level domain (TLD) that is 2 to 6 letters long. # Input - `text (str)`: A string potentially containing multiple email addresses. # Output - A list of strings, each being a valid email address found in the text. # Example ```python text = \\"Please contact us at support@example.com, sales@company.org or admin@domain.co.uk for more info.\\" print(extract_email_addresses(text)) # Output should be: [\'support@example.com\', \'sales@company.org\', \'admin@domain.co.uk\'] ``` # Constraints - Your solution should correctly handle the variations in email addresses as per the given format. - You should use the `re` module to compile the regular expression and perform the extraction. # Performance Requirements - The solution should be able to handle large text inputs efficiently. # Hint - Consider using grouping to capture different parts of the email addresses. - Utilize repetition qualifiers to handle the variations in the length of the components of the email addresses. # Skeleton Code ```python import re from typing import List def extract_email_addresses(text: str) -> List[str]: # Your implementation here # Example usage: if __name__ == \\"__main__\\": text = \\"Please contact us at support@example.com, sales@company.org or admin@domain.co.uk for more info.\\" print(extract_email_addresses(text)) ```","solution":"import re from typing import List def extract_email_addresses(text: str) -> List[str]: Extracts all valid email addresses from the given text. Parameters: text (str): A string potentially containing multiple email addresses. Returns: List[str]: A list of valid email addresses found within the text. email_pattern = re.compile(r\'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,6}\') return email_pattern.findall(text)"},{"question":"# Regular Expression Manipulation and Extraction Problem Statement You are given a list of log entries from a server in the following format: ``` [ip_address] - - [timestamp] \\"HTTP_METHOD url HTTP/1.1\\" status_code bytes [response_time] ``` An example log entry might look like this: ``` [192.168.0.1] - - [29/Nov/2020:06:25:30 +0000] \\"GET /home HTTP/1.1\\" 200 2326 [123ms] ``` Write a Python function: ```python def parse_log_entries(log_entries: list) -> list: pass ``` Your function should take a list of log entries as input and return a list of dictionaries, where each dictionary contains the following extracted fields: - `ip_address`: The IP address of the client. - `timestamp`: The timestamp of the request. - `http_method`: The HTTP method used (e.g., GET, POST, etc.). - `url`: The requested URL. - `status_code`: The status code returned by the server. - `response_time`: The response time in milliseconds. Constraints - The log entries conform to the specified format exactly. - The `log_entries` list will contain between 1 and 1000 log entries. Your task is to correctly implement the `parse_log_entries` function to extract and return the required fields. Use regular expressions to perform the extraction. Example Input: ```python log_entries = [ \\"[192.168.0.1] - - [29/Nov/2020:06:25:30 +0000] \\"GET /home HTTP/1.1\\" 200 2326 [123ms]\\", \\"[10.0.0.2] - - [29/Nov/2020:06:26:45 +0000] \\"POST /api/data HTTP/1.1\\" 404 1234 [567ms]\\" ] ``` Output: ```python [ { \\"ip_address\\": \\"192.168.0.1\\", \\"timestamp\\": \\"29/Nov/2020:06:25:30 +0000\\", \\"http_method\\": \\"GET\\", \\"url\\": \\"/home\\", \\"status_code\\": \\"200\\", \\"response_time\\": \\"123\\" }, { \\"ip_address\\": \\"10.0.0.2\\", \\"timestamp\\": \\"29/Nov/2020:06:26:45 +0000\\", \\"http_method\\": \\"POST\\", \\"url\\": \\"/api/data\\", \\"status_code\\": \\"404\\", \\"response_time\\": \\"567\\" } ] ``` Notes - Your solution should use Python\'s `re` module for regular expression matching and extraction. - Be mindful of performance considerations given the constraints.","solution":"import re def parse_log_entries(log_entries): pattern = re.compile( r\'^[(?P<ip_address>.*?)] - - [(?P<timestamp>.*?)] \\"(?P<http_method>.*?) (?P<url>.*?) HTTP/1.1\\" (?P<status_code>d+) d+ [(?P<response_time>d+)ms]\' ) parsed_entries = [] for log_entry in log_entries: match = pattern.match(log_entry) if match: parsed_entry = { \'ip_address\': match.group(\'ip_address\'), \'timestamp\': match.group(\'timestamp\'), \'http_method\': match.group(\'http_method\'), \'url\': match.group(\'url\'), \'status_code\': match.group(\'status_code\'), \'response_time\': match.group(\'response_time\'), } parsed_entries.append(parsed_entry) return parsed_entries"},{"question":"# WAV File Manipulation and Analysis You are given a WAV file and your task is to analyze and manipulate the audio data using the `wave` module in Python. Specifically, you need to extract some information from the input WAV file and create a mono version of it if it is stereo. Task 1. Write a function `analyze_wav(file_path: str) -> dict` that: - Takes a single argument `file_path` which is the path to the input WAV file. - Returns a dictionary with the following details about the WAV file: - `\'num_channels\'`: Number of audio channels. - `\'sample_width\'`: Sample width in bytes. - `\'frame_rate\'`: Sampling frequency (frames per second). - `\'num_frames\'`: Total number of audio frames. - `\'compression_type\'`: Compression type. - `\'compression_name\'`: Human-readable compression type name. - Prints the extracted information in a human-readable format. 2. Write a function `convert_to_mono(file_path: str, output_path: str) -> None` that: - Takes two arguments: - `file_path`: Path to the input WAV file. - `output_path`: Path where the mono version of the WAV file should be saved. - If the input file is already mono (i.e., one channel), copy the file to `output_path` without modification. - If the input file is stereo (i.e., two channels), convert it to a mono WAV file by averaging the left and right channels, and save the result to `output_path`. Constraints - You can assume that the input WAV file is always in the PCM format. - You should handle closing of files properly using context managers or making sure to close them explicitly. Example Suppose you have a WAV file named `stereo_sample.wav`. Calling the functions should work as follows: ```python result = analyze_wav(\'stereo_sample.wav\') # Output example (printed to console): # Number of Channels: 2 # Sample Width: 2 bytes # Frame Rate: 44100 Hz # Number of Frames: 882000 # Compression Type: NONE # Compression Name: not compressed convert_to_mono(\'stereo_sample.wav\', \'mono_sample.wav\') # This should create a mono version of \'stereo_sample.wav\' and save it as \'mono_sample.wav\'. ``` Remember to handle potential exceptions, such as file not found or format errors, appropriately.","solution":"import wave import numpy as np def analyze_wav(file_path: str) -> dict: with wave.open(file_path, \'rb\') as wav_file: num_channels = wav_file.getnchannels() sample_width = wav_file.getsampwidth() frame_rate = wav_file.getframerate() num_frames = wav_file.getnframes() compression_type = wav_file.getcomptype() compression_name = wav_file.getcompname() info = { \'num_channels\': num_channels, \'sample_width\': sample_width, \'frame_rate\': frame_rate, \'num_frames\': num_frames, \'compression_type\': compression_type, \'compression_name\': compression_name } # Print the extracted information print(f\\"Number of Channels: {num_channels}\\") print(f\\"Sample Width: {sample_width} bytes\\") print(f\\"Frame Rate: {frame_rate} Hz\\") print(f\\"Number of Frames: {num_frames}\\") print(f\\"Compression Type: {compression_type}\\") print(f\\"Compression Name: {compression_name}\\") return info def convert_to_mono(file_path: str, output_path: str) -> None: with wave.open(file_path, \'rb\') as wav_file: num_channels = wav_file.getnchannels() sample_width = wav_file.getsampwidth() frame_rate = wav_file.getframerate() num_frames = wav_file.getnframes() if num_channels == 1: # If already mono, copy the file to output_path with wave.open(output_path, \'wb\') as mono_file: mono_file.setnchannels(1) mono_file.setsampwidth(sample_width) mono_file.setframerate(frame_rate) mono_file.setnframes(num_frames) mono_file.writeframes(wav_file.readframes(num_frames)) else: # Convert to mono by averaging left and right channels frames = wav_file.readframes(num_frames) fmt = {1:\'B\', 2:\'h\', 4:\'i\'}[sample_width] samples = np.frombuffer(frames, dtype=np.dtype(fmt)) samples = samples.reshape(-1, num_channels) mono_samples = samples.mean(axis=1, dtype=samples.dtype) with wave.open(output_path, \'wb\') as mono_file: mono_file.setnchannels(1) mono_file.setsampwidth(sample_width) mono_file.setframerate(frame_rate) mono_file.writeframes(mono_samples.tobytes())"},{"question":"**Objective**: Implement a function that utilizes the MPS backend of PyTorch to create a tensor, perform operations on it, and move a provided model to the `mps` device for inference. # Function Signature ```python def perform_mps_operations(tensor_size: int, multiply_factor: float, model: torch.nn.Module) -> torch.Tensor: Moves a tensor and model to the `mps` device (if available) and performs operations on the tensor. Args: tensor_size (int): Size of the tensor to create. multiply_factor (float): Factor by which to multiply the tensor. model (torch.nn.Module): A PyTorch model to move to the `mps` device. Returns: torch.Tensor: The result of the model inference on the tensor. ``` # Instructions 1. **Check MPS Availability**: The function should first check if the MPS backend is available. 2. **Tensor Creation**: Create a tensor of ones with the specified size directly on the `mps` device (when available). 3. **Tensor Operation**: Multiply the tensor by the provided factor. 4. **Model Transfer**: Transfer the provided model to the `mps` device (when available). 5. **Inference**: Perform model inference using the manipulated tensor. 6. **Result Return**: Return the result of the inference. # Example ```python # Example model for testing class SimpleModel(torch.nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = torch.nn.Linear(5, 5) def forward(self, x): return self.linear(x) model = SimpleModel() # Perform the MPS operations result = perform_mps_operations(5, 2.0, model) # Check the result: it should be a Tensor on either MPS device or CPU depending on the availability print(result) ``` # Constraints 1. If the MPS backend is not available, the function should perform all operations on the CPU. 2. Assume that Metal Performance Shaders capabilities are only available for MacOS 12.3+. # Notes 1. If the provided tensor size or multiply factor is invalid (e.g., negative or zero), raise a `ValueError`. 2. Ensure that the function handles cases where the PyTorch installation is not built with MPS support or the macOS version does not meet the MPS requirements.","solution":"import torch def perform_mps_operations(tensor_size: int, multiply_factor: float, model: torch.nn.Module) -> torch.Tensor: Moves a tensor and model to the `mps` device (if available) and performs operations on the tensor. Args: tensor_size (int): Size of the tensor to create. multiply_factor (float): Factor by which to multiply the tensor. model (torch.nn.Module): A PyTorch model to move to the `mps` device. Returns: torch.Tensor: The result of the model inference on the tensor. # Check for negative or zero tensor size or multiply factor if tensor_size <= 0 or multiply_factor <= 0: raise ValueError(\\"Tensor size and multiply factor must be positive.\\") device = \'mps\' if torch.backends.mps.is_available() and torch.backends.mps.is_built() else \'cpu\' # Create tensor of ones on the specified device tensor = torch.ones(tensor_size, device=device) # Perform multiplication operation tensor = tensor * multiply_factor # Move the model to the specified device model.to(device) # Perform inference with the model with torch.no_grad(): result = model(tensor) return result"},{"question":"# HTML Content Processor You are tasked with creating an HTML content processor for a web application that handles user-generated content. The application needs to safely display user input on a web page while also being able to process and restore any HTML entities within the input back to their original characters when needed. Write a function `process_html_content` that takes a string `html_content` and a boolean flag `escape`: - If `escape` is `True`, the function should return the HTML-escaped version of `html_content`. - If `escape` is `False`, the function should return the unescaped version of `html_content`. # Constraints and Requirements: 1. Use the `html` module\'s `escape` and `unescape` functions for processing. 2. The input string `html_content` can contain any printable ASCII characters. 3. The function should be efficient enough to handle strings up to 1 million characters in length. # Function Signature: ```python def process_html_content(html_content: str, escape: bool) -> str: pass ``` # Input: - `html_content` (str): The input HTML content to be processed. - `escape` (bool): A flag indicating whether to escape or unescape the content. # Output: - A string that is the result of applying the appropriate HTML processing based on the `escape` flag. # Example Usage: ```python assert process_html_content(\\"Usage of <html> tags & symbols.\\", escape=True) == \\"Usage of &lt;html&gt; tags &amp; symbols.\\" assert process_html_content(\\"Usage of &lt;html&gt; tags &amp; symbols.\\", escape=False) == \\"Usage of <html> tags & symbols.\\" ``` Make sure to handle both escaping and unescaping correctly in your function implementation.","solution":"import html def process_html_content(html_content: str, escape: bool) -> str: Processes the HTML content based on the escape flag. :param html_content: The input HTML content to be processed. :param escape: A flag indicating whether to escape or unescape the content. :return: A string which is either HTML-escaped or unescaped based on the escape flag. if escape: return html.escape(html_content) else: return html.unescape(html_content)"},{"question":"Advanced File Handling in Python Objective: Demonstrate your understanding of Python\'s file handling and path manipulation capabilities using the `pathlib` and `shutil` modules. Problem Statement: You are given a task to organize a directory containing various files into subdirectories based on their file extensions. Your task is to implement the function `organize_directory()` which takes a single argument `directory_path` (a string representing the path to the directory to be organized) and organizes all files into subdirectories based on their extensions. If the subdirectory for a particular extension does not exist, it should be created. Additionally, implement the function `compute_directory_size()` which takes a directory path as an input and returns the total size of the directory in bytes. This will include the size of all its subdirectories and files. Constraints: 1. You must use the `pathlib` module for path manipulations. 2. Use the `shutil` module for moving files. 3. Ignore files that do not have an extension. 4. Assume `directory_path` is always a valid path to a directory. 5. The solution should handle large directories efficiently. Function Signatures: ```python from pathlib import Path import shutil def organize_directory(directory_path: str) -> None: pass def compute_directory_size(directory_path: str) -> int: pass ``` Example Usage: Assume the directory `/example_dir` contains the following files: - `file1.txt` - `file2.jpg` - `file3.pdf` - `file4.txt` - `file5.doc` After calling `organize_directory(\'/example_dir\')`, the directory structure should be: ``` /example_dir/ txt/ file1.txt file4.txt jpg/ file2.jpg pdf/ file3.pdf doc/ file5.doc ``` After organizing, if you call `compute_directory_size(\'/example_dir\')` and the size of all the files together is 5000 bytes, it should return `5000`. # Note: - Be sure to handle edge cases, such as an already organized directory or files without extensions. - Document any assumptions you make. - Write code that is clean, efficient, and adheres to Python best practices.","solution":"from pathlib import Path import shutil def organize_directory(directory_path: str) -> None: Organize files in the given directory into subdirectories based on their file extensions. :param directory_path: Path to the directory to be organized base_path = Path(directory_path) for file_path in base_path.iterdir(): if file_path.is_file() and file_path.suffix: extension = file_path.suffix[1:] # Get the file extension without the dot target_dir = base_path / extension if not target_dir.exists(): target_dir.mkdir() shutil.move(str(file_path), str(target_dir / file_path.name)) def compute_directory_size(directory_path: str) -> int: Compute the total size of the given directory, including all its subdirectories and files. :param directory_path: Path to the directory :return: Total size in bytes base_path = Path(directory_path) total_size = 0 for file_path in base_path.rglob(\'*\'): if file_path.is_file(): total_size += file_path.stat().st_size return total_size"},{"question":"Given the importance of the setup configuration file (`setup.cfg`) in Python packaging, you are tasked to implement a function that generates a `setup.cfg` file based on a given dictionary structure. Function Signature ```python def generate_setup_cfg(config_data: dict, file_path: str) -> None: pass ``` Input Format - `config_data` (dict): A dictionary where the keys are Distutils commands (strings), and the values are dictionaries representing the options for those commands. Example: ```python { \\"build_ext\\": { \\"inplace\\": \\"1\\", \\"include_dirs\\": \\"/usr/local/include\\" }, \\"bdist_rpm\\": { \\"release\\": \\"1\\", \\"packager\\": \\"John Doe <johndoe@example.com>\\", \\"doc_files\\": \\"README.md LICENSE\\" } } ``` - `file_path` (str): The path to the file where the `setup.cfg` should be written. Output Format The function should not return any value. Instead, it should generate the `setup.cfg` file at the specified file path with the correct structure and syntax. Constraints - Assume the dictionary will only contain valid commands and options as per Distutils documentation. - Long option values should be split across multiple lines if necessary for readability. For simplicity, you can assume option values longer than 80 characters should be split. Example Given the following input: ```python config_data = { \\"build_ext\\": { \\"inplace\\": \\"1\\", \\"include_dirs\\": \\"/usr/local/include\\" }, \\"bdist_rpm\\": { \\"release\\": \\"1\\", \\"packager\\": \\"John Doe <johndoe@example.com>\\", \\"doc_files\\": \\"README.md LICENSE\\" } } file_path = \\"setup.cfg\\" generate_setup_cfg(config_data, file_path) ``` The `setup.cfg` file should contain: ``` [build_ext] inplace = 1 include_dirs = /usr/local/include [bdist_rpm] release = 1 packager = John Doe <johndoe@example.com> doc_files = README.md LICENSE ``` Notes - Ensure to handle indentation for continued lines correctly. - Make sure to follow the section and option syntax precisely as described in the documentation.","solution":"def generate_setup_cfg(config_data: dict, file_path: str) -> None: Generates a setup.cfg file based on the given dictionary structure. Parameters: - config_data (dict): A dictionary where keys are Distutils commands and values are dictionaries of options. - file_path (str): The file path where the setup.cfg should be written. with open(file_path, \'w\') as file: for command, options in config_data.items(): file.write(f\\"[{command}]n\\") for option, value in options.items(): if len(str(value)) > 80: # Split long values into multiple lines value_lines = [value[i:i+80] for i in range(0, len(value), 80)] file.write(f\\"{option} =n\\") for line in value_lines: file.write(f\\" {line}n\\") else: file.write(f\\"{option} = {value}n\\") file.write(\\"n\\")"},{"question":"# Plot Customization with Pandas **Objective:** Demonstrate your understanding of pandas visualization capabilities by performing various plotting tasks and customization. **Question:** You are provided with sales data containing information about sales, profits, and regions for a company\'s products over different months. Using this data, perform the following tasks: 1. Load the sales data from a CSV file named `sales_data.csv`. 2. Generate a time series line plot showing the cumulative sales over the period. 3. Create a bar plot showing the total profit for each region. 4. Plot a histogram depicting the distribution of sales values. 5. Create a scatter plot comparing sales and profit and color the points based on the region. 6. Display all the above plots in a 2x2 grid layout, adjusting the layout and size of subplots appropriately. 7. Add titles, x and y labels, and legends where appropriate to each plot. 8. Save the final plot as `sales_visualization.png`. **Constraints:** - The sales data CSV file contains the following columns: `month`, `sales`, `profit`, `region`. - Assume that there are no missing values in the data. - You may use matplotlib functions for customization if needed. **Input:** - A CSV file named `sales_data.csv`. **Output:** - A PNG file named `sales_visualization.png` showing the generated and customized plots. **Example CSV Data:** ```csv month,sales,profit,region 2022-01,3000,1000,North 2022-02,3200,1200,North 2022-03,3400,1300,East 2022-04,3600,1100,West 2022-05,3100,1400,South ... ``` **Python Code Template:** ```python import pandas as pd import matplotlib.pyplot as plt # 1. Load the sales data sales_data = pd.read_csv(\'sales_data.csv\') sales_data[\'month\'] = pd.to_datetime(sales_data[\'month\']) sales_data.set_index(\'month\', inplace=True) # 2. Generate a time series line plot showing the cumulative sales over the period ... # 3. Create a bar plot showing the total profit for each region ... # 4. Plot a histogram depicting the distribution of sales values ... # 5. Create a scatter plot comparing sales and profit and color the points based on the region ... # 6. Display all plots in a 2x2 grid layout ... # 7. Add titles, x and y labels, and legends ... # 8. Save the final plot plt.savefig(\'sales_visualization.png\') plt.show() ``` **Notes:** - Ensure the plots are clearly labeled and appropriately styled to convey the information effectively. - The code template is a starting point, and you may modify it as needed to complete the tasks.","solution":"import pandas as pd import matplotlib.pyplot as plt def generate_sales_visualization(csv_file_path, output_file_path): # 1. Load the sales data sales_data = pd.read_csv(csv_file_path) sales_data[\'month\'] = pd.to_datetime(sales_data[\'month\']) sales_data.set_index(\'month\', inplace=True) # 2. Generate a time series line plot showing the cumulative sales over the period sales_data[\'cumulative_sales\'] = sales_data[\'sales\'].cumsum() # 5. Scatter plot with Color mapping based on Region regions = sales_data[\'region\'].unique() # Define the layout of the subplots: 2x2 grid fig, axs = plt.subplots(2, 2, figsize=(15, 10)) # Time Series Line Plot axs[0, 0].plot(sales_data.index, sales_data[\'cumulative_sales\'], label=\'Cumulative Sales\', color=\'blue\') axs[0, 0].set_title(\'Cumulative Sales Over Time\') axs[0, 0].set_xlabel(\'Month\') axs[0, 0].set_ylabel(\'Cumulative Sales\') axs[0, 0].legend(loc=\'best\') # Bar Plot for Total Profit by Region total_profit_by_region = sales_data.groupby(\'region\')[\'profit\'].sum() total_profit_by_region.plot(kind=\'bar\', ax=axs[0, 1], color=\'orange\') axs[0, 1].set_title(\'Total Profit by Region\') axs[0, 1].set_xlabel(\'Region\') axs[0, 1].set_ylabel(\'Total Profit\') # Histogram for Sales Distribution axs[1, 0].hist(sales_data[\'sales\'], bins=15, color=\'green\', edgecolor=\'black\') axs[1, 0].set_title(\'Distribution of Sales\') axs[1, 0].set_xlabel(\'Sales\') axs[1, 0].set_ylabel(\'Frequency\') # Scatter Plot for Sales vs Profit with Color by Region colors = {\'North\':\'red\', \'East\':\'blue\', \'West\':\'green\', \'South\':\'purple\'} for region in regions: region_data = sales_data[sales_data[\'region\'] == region] axs[1, 1].scatter(region_data[\'sales\'], region_data[\'profit\'], label=region, color=colors[region]) axs[1, 1].set_title(\'Sales vs Profit\') axs[1, 1].set_xlabel(\'Sales\') axs[1, 1].set_ylabel(\'Profit\') axs[1, 1].legend(title=\'Region\', loc=\'best\') # Adjust layout and save the figure plt.tight_layout() plt.savefig(output_file_path) plt.show() # Only for the solution, not part of the actual function if __name__ == \\"__main__\\": generate_sales_visualization(\'sales_data.csv\', \'sales_visualization.png\')"},{"question":"Title: Directory Tree Analysis and Serialization Objective: The goal is to evaluate the student’s ability to use the `pathlib` module to navigate and interact with the file system. The student will be required to write code that lists, analyzes, and serializes the directory structure. Problem Statement: You are provided with a root directory path. Implement a Python function that analyzes this directory and generates a summary of its structure in a nested dictionary format. Each key in the dictionary should represent a file or directory name, and the value should either be the file\'s size in bytes (for files) or another dictionary (for subdirectories). Additionally, create a function to serialize this directory structure dictionary to a JSON file. Function Specifications: 1. `analyze_directory(root: Union[str, Path]) -> Dict[str, Any]`: - **Input**: A root directory path as a string or `Path`. - **Output**: A nested dictionary representing the directory structure. - **Constraints**: - The function should handle symbolic links, but should not follow them to avoid potential cycles. - If a path cannot be accessed due to permissions, it can be skipped, but it must not break the function. 2. `serialize_to_json(data: Dict[str, Any], output_path: Union[str, Path]) -> None`: - **Input**: - `data`: The nested dictionary returned by `analyze_directory`. - `output_path`: A file path where the JSON representation of the directory structure will be saved. - **Output**: None - **Functionality**: Should write the dictionary to a JSON file at the specified path. Example: Assume the directory structure is as follows: ``` root_directory |-- file1.txt (size: 100 bytes) |-- file2.log (size: 200 bytes) |-- subdir1 |-- file3.txt (size: 150 bytes) |-- subdir2 |-- subdir2_1 |-- file4.txt (size: 300 bytes) ``` Calling `analyze_directory(\'root_directory\')` should return: ```python { \\"file1.txt\\": 100, \\"file2.log\\": 200, \\"subdir1\\": { \\"file3.txt\\": 150 }, \\"subdir2\\": { \\"subdir2_1\\": { \\"file4.txt\\": 300 } } } ``` And calling `serialize_to_json(data, \'output.json\')` will create a JSON file `output.json` with the above content. Notes: - Use the `pathlib` module for all path manipulations. - Make sure to handle exceptions gracefully. - Ensure the JSON file is properly formatted and human-readable (you might want to use `indent` parameter in `json.dump`). Submission: Submit the implementations of `analyze_directory` and `serialize_to_json` functions.","solution":"from pathlib import Path import json from typing import Union, Dict, Any def analyze_directory(root: Union[str, Path]) -> Dict[str, Any]: Analyzes the directory structure from the given root and returns it as a nested dictionary. root = Path(root) result = {} for entry in root.iterdir(): if entry.is_file(): result[entry.name] = entry.stat().st_size elif entry.is_dir(): result[entry.name] = analyze_directory(entry) # Skipping symbolic links to avoid potential cycles return result def serialize_to_json(data: Dict[str, Any], output_path: Union[str, Path]) -> None: Serializes the given dictionary to a JSON file at the specified output path. output_path = Path(output_path) with output_path.open(\'w\', encoding=\'utf-8\') as json_file: json.dump(data, json_file, indent=4)"},{"question":"# Question: Implementing and Validating a WSGI Application You are tasked with creating a simple, but fully compliant WSGI application using the `wsgiref` package. Your application should respond with a greeting message and demonstrate the use of various `wsgiref` utilities. Requirements: 1. Implement a WSGI application that: - Responds with a status code `200 OK`. - Returns a text message \\"Hello, WSGI World!\\". - Includes custom headers to specify the content type. 2. Use `wsgiref.util.setup_testing_defaults` to set up the WSGI environment for testing. 3. Use the `wsgiref.headers.Headers` class to manage response headers. 4. Use the `wsgiref.validate.validator` function to wrap your application and ensure WSGI compliance. 5. Serve the application using `wsgiref.simple_server`. Input: - No direct input is required. However, set up the environment and headers within the application code. Output: - The application should respond with the text \\"Hello, WSGI World!\\" and appropriate HTTP headers. Constraints: - The application must be WSGI-compliant. - Use only `wsgiref` package utilities to set up and validate the environment. # Implementation Guide: 1. Create a WSGI application function `app` that accepts `environ` and `start_response` as parameters. 2. Set up the WSGI environment using `wsgiref.util.setup_testing_defaults`. 3. Use `wsgiref.headers.Headers` to set the Content-Type header to `text/plain; charset=utf-8`. 4. Return the response body containing \\"Hello, WSGI World!\\". 5. Wrap your application with `wsgiref.validate.validator` and serve it using `wsgiref.simple_server.make_server`. ```python from wsgiref.util import setup_testing_defaults from wsgiref.headers import Headers from wsgiref.validate import validator from wsgiref.simple_server import make_server def app(environ, start_response): # Setting up default WSGI environment for testing setup_testing_defaults(environ) # Creating a Headers object to manage response headers headers = Headers([ (\'Content-Type\', \'text/plain; charset=utf-8\') ]) # Setting the status and headers using start_response status = \'200 OK\' start_response(status, headers.items()) # Returning the response body containing \\"Hello, WSGI World!\\" return [b\\"Hello, WSGI World!\\"] # Wrapping the application with validator for WSGI compliance check validator_app = validator(app) # Serving the application using wsgiref.simple_server if __name__ == \'__main__\': with make_server(\'\', 8000, validator_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever() ``` Ensure your code is well-commented and adheres to PEP 8 standards for readability. **Good Luck!**","solution":"from wsgiref.util import setup_testing_defaults from wsgiref.headers import Headers from wsgiref.validate import validator from wsgiref.simple_server import make_server def app(environ, start_response): # Setting up default WSGI environment for testing setup_testing_defaults(environ) # Creating a Headers object to manage response headers headers = Headers([ (\'Content-Type\', \'text/plain; charset=utf-8\') ]) # Setting the status and headers using start_response status = \'200 OK\' start_response(status, headers.items()) # Returning the response body containing \\"Hello, WSGI World!\\" return [b\\"Hello, WSGI World!\\"] # Wrapping the application with validator for WSGI compliance check validator_app = validator(app) # Serving the application using wsgiref.simple_server if __name__ == \'__main__\': with make_server(\'\', 8000, validator_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever()"},{"question":"# Advanced PyTorch Backends Configuration Objective: Design a function to configure and query various PyTorch backends. This will require confirming the availability of different backends, enabling specific optimizations, and demonstrating a use case involving backend settings to run tensor computations with improved performance. Task: Implement a function `configure_and_query_backends` that: 1. Queries the availability of various backends (`cuda`, `cudnn`, `mkldnn`, `mps`, `openmp`). 2. Sets specific configurations for the CUDA and cuDNN backends. 3. Runs a tensor multiplication using different configurations and returns the results for performance comparison. Specifications: 1. Query and print the availability of the following backends: - CUDA (`torch.backends.cuda.is_built()`) - cuDNN (`torch.backends.cudnn.is_available()`) - MKL-DNN (`torch.backends.mkldnn.is_available()`) - MPS (`torch.backends.mps.is_available()`) - OpenMP (`torch.backends.openmp.is_available()`) 2. If CUDA is available: - Enable TensorFloat-32 tensor cores (`torch.backends.cuda.matmul.allow_tf32 = True`) - Enable reduced precision reductions (`torch.backends.cuda.allow_fp16_reduced_precision_reduction = True`) 3. If cuDNN is available: - Enable the deterministic algorithm (`torch.backends.cudnn.deterministic = True`) - Enable benchmarking (`torch.backends.cudnn.benchmark = True`) 4. Perform a simple tensor multiplication on the GPU (if available) using the different configurations and measure the time taken for each configuration. Return a dictionary with the results (including the time taken for each configuration). Example Output: The function should return a dictionary with information on the availability of each backend, and the performance measurements for tensor operations. For example: ```python { \\"availability\\": { \\"cuda\\": True, \\"cudnn\\": True, \\"mkldnn\\": False, \\"mps\\": True, \\"openmp\\": True }, \\"performance\\": { \\"default\\": 0.01234, \\"tf32\\": 0.00987, \\"reduced_precision\\": 0.00876 } } ``` Additional Information: - Ensure your code handles cases where a backend might not be available. - Use the `torch.cuda.synchronize()` function to ensure accurate time measurement on CUDA devices. - Use a fixed tensor size (e.g., 5000x5000 matrices) for multiplication to simulate a computational workload. - Consider edge cases where certain configurations or operations might not execute due to backend constraints. Implement the `configure_and_query_backends` function to assess your knowledge and competency in working with PyTorch backends. Note: This task assumes that you have access to a machine with CUDA-enabled GPUs to fully test the CUDA and cuDNN configurations. ```python import torch import time def configure_and_query_backends(): result = { \\"availability\\": {}, \\"performance\\": {} } # Querying availability of backends result[\\"availability\\"][\\"cuda\\"] = torch.backends.cuda.is_built() result[\\"availability\\"][\\"cudnn\\"] = torch.backends.cudnn.is_available() result[\\"availability\\"][\\"mkldnn\\"] = torch.backends.mkldnn.is_available() result[\\"availability\\"][\\"mps\\"] = torch.backends.mps.is_available() result[\\"availability\\"][\\"openmp\\"] = torch.backends.openmp.is_available() # If CUDA is available, run tensor multiplications with different configurations if result[\\"availability\\"][\\"cuda\\"]: # Create random tensors a = torch.randn(5000, 5000, device=\'cuda\') b = torch.randn(5000, 5000, device=\'cuda\') # Default computation (without any specific configurations) start_time = time.time() c = torch.matmul(a, b) torch.cuda.synchronize() end_time = time.time() result[\\"performance\\"][\\"default\\"] = end_time - start_time # Enable TensorFloat-32 tensor cores and measure performance torch.backends.cuda.matmul.allow_tf32 = True start_time = time.time() c = torch.matmul(a, b) torch.cuda.synchronize() end_time = time.time() result[\\"performance\\"][\\"tf32\\"] = end_time - start_time torch.backends.cuda.matmul.allow_tf32 = False # Enable reduced precision reductions and measure performance torch.backends.cuda.allow_fp16_reduced_precision_reduction = True start_time = time.time() c = torch.matmul(a, b) torch.cuda.synchronize() end_time = time.time() result[\\"performance\\"][\\"reduced_precision\\"] = end_time - start_time torch.backends.cuda.allow_fp16_reduced_precision_reduction = False return result ```","solution":"import torch import time def configure_and_query_backends(): result = { \\"availability\\": {}, \\"performance\\": {} } # Querying availability of backends result[\\"availability\\"][\\"cuda\\"] = torch.backends.cuda.is_built() result[\\"availability\\"][\\"cudnn\\"] = torch.backends.cudnn.is_available() result[\\"availability\\"][\\"mkldnn\\"] = torch.backends.mkldnn.is_available() result[\\"availability\\"][\\"mps\\"] = torch.backends.mps.is_available() result[\\"availability\\"][\\"openmp\\"] = torch.backends.openmp.is_available() # If CUDA is available, run tensor multiplications with different configurations if result[\\"availability\\"][\\"cuda\\"]: # Create random tensors a = torch.randn(5000, 5000, device=\'cuda\') b = torch.randn(5000, 5000, device=\'cuda\') # Default computation (without any specific configurations) start_time = time.time() c = torch.matmul(a, b) torch.cuda.synchronize() end_time = time.time() result[\\"performance\\"][\\"default\\"] = end_time - start_time # Enable TensorFloat-32 tensor cores and measure performance torch.backends.cuda.matmul.allow_tf32 = True start_time = time.time() c = torch.matmul(a, b) torch.cuda.synchronize() end_time = time.time() result[\\"performance\\"][\\"tf32\\"] = end_time - start_time torch.backends.cuda.matmul.allow_tf32 = False # Enable reduced precision reductions and measure performance torch.backends.cuda.allow_fp16_reduced_precision_reduction = True start_time = time.time() c = torch.matmul(a, b) torch.cuda.synchronize() end_time = time.time() result[\\"performance\\"][\\"reduced_precision\\"] = end_time - start_time torch.backends.cuda.allow_fp16_reduced_precision_reduction = False return result"},{"question":"**Question: Advanced Logging Configuration in Python** You are required to implement a Python script to demonstrate your understanding of the logging module. Your task involves setting up a comprehensive logging system for a hypothetical application. The script should adhere to the following specifications: 1. **Logger Configuration:** - Create a primary logger named `app` with log level `DEBUG`. - Configure two handlers: - A `FileHandler` to write logs to a file named `app.log`. - A `StreamHandler` to display logs to the console. 2. **Formatter:** - Use a format for the logs that includes timestamp, logger name, log level, and the log message. - Customize the date format to display as `Month/Day/Year Hour:Minute:Second AM/PM`. 3. **Handling Different Log Levels:** - Ensure that the `FileHandler` writes logs of all levels from `DEBUG` upwards. - Ensure that the `StreamHandler` displays logs of level `ERROR` and higher. 4. **Custom Handler:** - Create a custom handler named `AlertHandler` which sends an alert message (just print to the console for this assignment) for logs of level `CRITICAL`. 5. **Logging Messages:** - Implement the logging of various messages (one for each level: DEBUG, INFO, WARNING, ERROR, CRITICAL) using the configured logger. ```python import logging import sys # Step 1: Logger Configuration logger = logging.getLogger(\'app\') logger.setLevel(logging.DEBUG) # Step 2: Handler Configuration file_handler = logging.FileHandler(\'app.log\') stream_handler = logging.StreamHandler(sys.stdout) file_handler.setLevel(logging.DEBUG) stream_handler.setLevel(logging.ERROR) # Step 3: Formatter Configuration formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\', datefmt=\'%m/%d/%Y %I:%M:%S %p\') file_handler.setFormatter(formatter) stream_handler.setFormatter(formatter) logger.addHandler(file_handler) logger.addHandler(stream_handler) # Step 4: Custom Handler class AlertHandler(logging.Handler): def emit(self, record): if record.levelno == logging.CRITICAL: print(f\\"ALERT: {self.format(record)}\\") alert_handler = AlertHandler() alert_handler.setFormatter(formatter) alert_handler.setLevel(logging.CRITICAL) logger.addHandler(alert_handler) # Step 5: Log Messages logger.debug(\'This is a debug message\') logger.info(\'This is an info message\') logger.warning(\'This is a warning message\') logger.error(\'This is an error message\') logger.critical(\'This is a critical message\') ``` **Constraints:** - Ensure the script does not raise any exceptions. - Verify the log file `app.log` contains entries for all log levels. - Ensure the console displays only the ERROR and CRITICAL messages. - Ensure the custom alert message gets printed for CRITICAL log events. **Performance Requirements:** - The script should run efficiently without excessive memory usage. - The logging system should be constructed to handle high-frequency logging without performance degradation. **Submission:** Submit the complete Python script fulfilling the above criteria. Ensure the script is well-commented to explain the configuration steps and the purpose of each block of code.","solution":"import logging import sys def setup_logger(): # Step 1: Logger Configuration logger = logging.getLogger(\'app\') logger.setLevel(logging.DEBUG) # Step 2: Handler Configuration file_handler = logging.FileHandler(\'app.log\') stream_handler = logging.StreamHandler(sys.stdout) file_handler.setLevel(logging.DEBUG) stream_handler.setLevel(logging.ERROR) # Step 3: Formatter Configuration formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\', datefmt=\'%m/%d/%Y %I:%M:%S %p\') file_handler.setFormatter(formatter) stream_handler.setFormatter(formatter) logger.addHandler(file_handler) logger.addHandler(stream_handler) # Step 4: Custom Handler class AlertHandler(logging.Handler): def emit(self, record): if record.levelno == logging.CRITICAL: print(f\\"ALERT: {self.format(record)}\\") alert_handler = AlertHandler() alert_handler.setFormatter(formatter) alert_handler.setLevel(logging.CRITICAL) logger.addHandler(alert_handler) return logger def log_messages(logger): # Step 5: Log Messages logger.debug(\'This is a debug message\') logger.info(\'This is an info message\') logger.warning(\'This is a warning message\') logger.error(\'This is an error message\') logger.critical(\'This is a critical message\') if __name__ == \\"__main__\\": logger = setup_logger() log_messages(logger)"},{"question":"# PyTorch Coding Assessment Question Objective You need to implement a function to initialize the parameters of a simple neural network with multiple layers using different initialization methods available in the `torch.nn.init` module. This will assess your understanding of how to use various initialization techniques in PyTorch. Description Write a function `initialize_network_parameters` that takes a neural network model and a dictionary of initialization configurations and applies the specified initializations to each layer of the network. Parameters - `model`: an instance of `torch.nn.Module` representing the neural network. - `initialization_configs`: a dictionary where: * Keys are layer names (strings) and * Values are tuples where: * The first element is a string specifying the initialization method and * The second element is a dictionary of keyword arguments required by the initialization method. You need to apply the specified initialization method to either the weights or bias of each layer in the model according to the provided configuration. Example Initialization Configurations ```python initialization_configs = { \'conv1\': (\'kaiming_uniform_\', {\'a\': 0, \'mode\': \'fan_in\', \'nonlinearity\': \'relu\'}), \'fc1\': (\'xavier_normal_\', {\'gain\': 1.0}), \'fc2\': (\'constant_\', {\'val\': 0}), } ``` Your function should support as many initialization methods as possible provided in `torch.nn.init`. If an unsupported method is specified, your function should raise a `ValueError` with an appropriate message. Expected Function Signature ```python import torch.nn as nn import torch.nn.init as init from typing import Dict, Tuple def initialize_network_parameters(model: nn.Module, initialization_configs: Dict[str, Tuple[str, Dict]]) -> None: pass ``` Example Usage ```python class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.fc1 = nn.Linear(320, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = x.view(-1, 320) x = F.relu(self.fc1(x)) x = self.fc2(x) return x model = SimpleNet() initialization_configs = { \'conv1\': (\'kaiming_uniform_\', {\'a\': 0, \'mode\': \'fan_in\', \'nonlinearity\': \'relu\'}), \'fc1\': (\'xavier_normal_\', {\'gain\': 1.0}), \'fc2\': (\'constant_\', {\'val\': 0}), } initialize_network_parameters(model, initialization_configs) ``` Constraints - You are only allowed to use functions from the `torch`, `torch.nn`, and `torch.nn.init` modules. - You should properly handle any exceptions raised by the initialization methods. Performance Requirements Make sure your solution is efficient in terms of runtime and memory usage.","solution":"import torch.nn as nn import torch.nn.init as init from typing import Dict, Tuple def initialize_network_parameters(model: nn.Module, initialization_configs: Dict[str, Tuple[str, Dict]]) -> None: Initialize the parameters of a neural network model according to the specified initialization configurations. Parameters: - model: an instance of torch.nn.Module representing the neural network. - initialization_configs: a dictionary where keys are layer names (strings) and values are tuples with the first element being the initialization method (string) and the second element being a dictionary of keyword arguments required by the initialization method. for name, layer in model.named_children(): if name in initialization_configs: method_name, params = initialization_configs[name] if method_name in dir(init): method = getattr(init, method_name) if hasattr(layer, \'weight\') and layer.weight is not None: method(layer.weight, **params) if hasattr(layer, \'bias\') and layer.bias is not None and \'val\' in params: init.constant_(layer.bias, params[\'val\']) else: raise ValueError(f\\"Unsupported initialization method: {method_name}\\")"},{"question":"You are tasked with creating a utility to parse and manipulate XML documents using Python\'s `xml.dom` module. The goal is to create a function that processes an XML document, adds specific elements, and retrieves information based on certain criteria. Task 1. Write a function `process_xml_document(file_path: str, add_element_info: dict, search_tag: str) -> dict` that: - Parses an XML document from a file. - Adds a new element to a specified location within the document. - Searches for all elements matching a tag name and returns their attributes. 2. Within the function, implement the following steps: 1. **Parse the XML Document**: - Load and parse the XML document from the given `file_path` using DOM methods. 2. **Add New Element**: - Using `add_element_info` dictionary, which contains: - `parent_tag`: the tag name of the parent where the new element should be added. - `child_tag`: the tag name of the new element. - `attributes`: a dictionary of attributes to be added to the new element. - Find the parent element(s) in the document and add the new element with the specified attributes. 3. **Search Elements by Tag**: - Search and extract all elements with the tag name specified by `search_tag`. - Return a dictionary where keys are the indexes of the found elements and values are dictionaries of their attributes. Input - `file_path` (str): Path to an XML file. - `add_element_info` (dict): Dictionary containing details for adding a new element: ```python { \\"parent_tag\\": \\"str\\", \\"child_tag\\": \\"str\\", \\"attributes\\": {\\"attr1\\": \\"value1\\", \\"attr2\\": \\"value2\\"} } ``` - `search_tag` (str): Tag name to search for in the XML document. Output - A dictionary where each key is an index of the found elements (0-based) and value is a dictionary of the element\'s attributes. Constraints - The XML document is well-formed. - Assume the file path is valid. - Performance should be considered for large XML documents. Example Given an XML file `example.xml` with the content: ```xml <root> <parent> <child attr=\\"value\\">Text</child> </parent> </root> ``` And the function call: ```python add_element_info = { \\"parent_tag\\": \\"parent\\", \\"child_tag\\": \\"new_child\\", \\"attributes\\": {\\"id\\": \\"1\\", \\"class\\": \\"top\\"} } search_tag = \\"child\\" result = process_xml_document(\\"example.xml\\", add_element_info, search_tag) print(result) ``` Expected output: ```python { 0: {\\"attr\\": \\"value\\"} } ``` In the above example, the new XML would look like: ```xml <root> <parent> <child attr=\\"value\\">Text</child> <new_child id=\\"1\\" class=\\"top\\"></new_child> </parent> </root> ``` Notes - Ensure to handle XML namespace if present. - Use appropriate `xml.dom` methods and properties as discussed in the documentation. - Handle exceptions gracefully, especially those defined in the `xml.dom` module.","solution":"from xml.dom.minidom import parse, Document from xml.dom.minidom import Element as DOMElement def process_xml_document(file_path: str, add_element_info: dict, search_tag: str) -> dict: Processes an XML document by adding a new element and retrieving elements by tag name. Args: file_path (str): Path to the XML file. add_element_info (dict): Dictionary containing new element info with keys: - \'parent_tag\' : the tag name of the parent where the new element should be added. - \'child_tag\' : the tag name of the new element. - \'attributes\' : a dictionary of attributes to be added to the new element. search_tag (str): The tag name to search for in the XML document. Returns: dict: A dictionary where keys are the index of found elements and values are the attributes dictionary. # 1. Parse the XML Document dom = parse(file_path) # 2. Add New Element parent_tag = add_element_info[\'parent_tag\'] child_tag = add_element_info[\'child_tag\'] attributes = add_element_info[\'attributes\'] for parent in dom.getElementsByTagName(parent_tag): new_element = dom.createElement(child_tag) for attr, value in attributes.items(): new_element.setAttribute(attr, value) parent.appendChild(new_element) # 3. Search Elements by Tag found_elements_dict = {} elements = dom.getElementsByTagName(search_tag) for index, element in enumerate(elements): attributes_dict = {} if element.hasAttributes(): for i in range(element.attributes.length): attr_name = element.attributes.item(i).name attr_value = element.attributes.item(i).value attributes_dict[attr_name] = attr_value found_elements_dict[index] = attributes_dict return found_elements_dict"},{"question":"# Naive Bayes Classifier Implementation and Evaluation **Problem Statement:** You are given a dataset containing news articles that are categorized into different topics. Your task is to implement a Naive Bayes classifier using the `MultinomialNB` algorithm provided in Scikit-learn to classify these articles based on their content. You will preprocess the text data, train the classifier, and evaluate its performance. **Dataset:** The dataset consists of a CSV file `news_articles.csv` with the following columns: - `title`: The title of the news article. - `text`: The full text of the news article. - `category`: The category to which the news article belongs (e.g., \'sports\', \'politics\', \'technology\'). **Instructions:** 1. **Data Preprocessing:** - Load the dataset from the provided CSV file. - Combine the `title` and `text` columns to form a single feature for each article. - Convert the text data to numerical format using the `TfidfVectorizer` from Scikit-learn\'s `feature_extraction.text` module. 2. **Model Training:** - Split the dataset into training and test sets (80% training, 20% test). - Initialize and train a `MultinomialNB` classifier on the training set. 3. **Model Evaluation:** - Predict the categories of the articles in the test set. - Calculate and print the following metrics to evaluate the performance of your classifier: - Accuracy - Precision - Recall - F1-score 4. **Function Implementation:** - Implement the following Python function to carry out the tasks described: ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def naive_bayes_news_classifier(csv_file_path: str): Function to implement a Naive Bayes classifier for news article categorization. Parameters: - csv_file_path (str): The file path to the CSV file containing the news articles dataset. Returns: - dict: A dictionary containing the evaluation metrics (accuracy, precision, recall, f1-score). # Load and preprocess the dataset data = pd.read_csv(csv_file_path) data[\'combined_text\'] = data[\'title\'] + \' \' + data[\'text\'] X = data[\'combined_text\'] y = data[\'category\'] # Convert text to numerical format using TfidfVectorizer vectorizer = TfidfVectorizer(stop_words=\'english\') X_transformed = vectorizer.fit_transform(X) # Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42) # Initialize and train the MultinomialNB classifier classifier = MultinomialNB() classifier.fit(X_train, y_train) # Predict the categories for the test set y_pred = classifier.predict(X_test) # Calculate evaluation metrics metrics = { \'accuracy\': accuracy_score(y_test, y_pred), \'precision\': precision_score(y_test, y_pred, average=\'weighted\'), \'recall\': recall_score(y_test, y_pred, average=\'weighted\'), \'f1_score\': f1_score(y_test, y_pred, average=\'weighted\') } # Print metrics for metric, value in metrics.items(): print(f\\"{metric.capitalize()}: {value:.4f}\\") return metrics ``` **Constraints:** - Use the `MultinomialNB` class from the `sklearn.naive_bayes` module. - Ensure proper data preprocessing to handle text data. - Use appropriate random state for reproducibility in data splitting. Your implementation should demonstrate a clear understanding of the Naive Bayes algorithm, proper usage of Scikit-learn\'s API, and the ability to evaluate classification models.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def naive_bayes_news_classifier(csv_file_path: str): Function to implement a Naive Bayes classifier for news article categorization. Parameters: - csv_file_path (str): The file path to the CSV file containing the news articles dataset. Returns: - dict: A dictionary containing the evaluation metrics (accuracy, precision, recall, f1-score). # Load and preprocess the dataset data = pd.read_csv(csv_file_path) data[\'combined_text\'] = data[\'title\'] + \' \' + data[\'text\'] X = data[\'combined_text\'] y = data[\'category\'] # Convert text to numerical format using TfidfVectorizer vectorizer = TfidfVectorizer(stop_words=\'english\') X_transformed = vectorizer.fit_transform(X) # Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42) # Initialize and train the MultinomialNB classifier classifier = MultinomialNB() classifier.fit(X_train, y_train) # Predict the categories for the test set y_pred = classifier.predict(X_test) # Calculate evaluation metrics metrics = { \'accuracy\': accuracy_score(y_test, y_pred), \'precision\': precision_score(y_test, y_pred, average=\'weighted\'), \'recall\': recall_score(y_test, y_pred, average=\'weighted\'), \'f1_score\': f1_score(y_test, y_pred, average=\'weighted\') } # Print metrics for metric, value in metrics.items(): print(f\\"{metric.capitalize()}: {value:.4f}\\") return metrics"},{"question":"# Isotonic Regression Implementation Challenge You are provided with data points representing a certain quantity observed over time. Your task is to implement a function using scikit-learn\'s `IsotonicRegression` to fit this data, predict new values, and plot both the observed and predicted values. Your implementation should follow these steps: 1. Fit an isotonic regression model to the provided data, assuming the data is non-decreasing. 2. Predict the values for a new set of data points. 3. Plot both the original data and the predictions. Function Signature ```python def fit_and_predict_isotonic(X_train: List[float], y_train: List[float], X_test: List[float]) -> Tuple[List[float], plt.Figure]: Fit an IsotonicRegression model and predict values for new data points. Parameters: - X_train (List[float]): The list of independent variable values for training. - y_train (List[float]): The list of dependant variable values for training. - X_test (List[float]): The list of independent variable values for testing/prediction. Returns: - Tuple[List[float], plt.Figure]: A tuple containing the predicted values for X_test and a matplotlib Figure object with the plot of the original and predicted values. pass ``` Requirements 1. **Input**: - `X_train`: A list of float values representing the independent variable for training data. - `y_train`: A list of float values representing the dependent variable for training data. - `X_test`: A list of float values representing the independent variable for prediction. 2. **Output**: - A tuple containing: - A list of predicted values corresponding to `X_test`. - A matplotlib Figure object with a plot that includes: - The original training data points. - The predictions for the `X_train` data points. - The predictions for the `X_test` data points. 3. **Constraints**: - Use scikit-learn\'s `IsotonicRegression` for fitting the model. - Ensure the model is constrained to be non-decreasing. Example ```python X_train = [1.0, 2.0, 3.0, 4.0, 5.0] y_train = [1.0, 2.2, 2.8, 3.5, 4.1] X_test = [1.5, 2.5, 3.5] predicted_values, plot_figure = fit_and_predict_isotonic(X_train, y_train, X_test) # The predicted_values should be the output of the model predictions for X_test # The plot_figure should be a matplotlib figure containing the plot ```","solution":"from typing import List, Tuple import matplotlib.pyplot as plt from sklearn.isotonic import IsotonicRegression def fit_and_predict_isotonic(X_train: List[float], y_train: List[float], X_test: List[float]) -> Tuple[List[float], plt.Figure]: Fit an IsotonicRegression model and predict values for new data points. Parameters: - X_train (List[float]): The list of independent variable values for training. - y_train (List[float]): The list of dependant variable values for training. - X_test (List[float]): The list of independent variable values for testing/prediction. Returns: - Tuple[List[float], plt.Figure]: A tuple containing the predicted values for X_test and a matplotlib Figure object with the plot of the original and predicted values. # Fit the Isotonic Regression model ir = IsotonicRegression(increasing=True) ir.fit(X_train, y_train) # Predict the values for X_test y_pred_test = ir.transform(X_test) # Plot the results fig, ax = plt.subplots() ax.plot(X_train, y_train, \'o\', label=\'Training Data\') ax.plot(X_train, ir.predict(X_train), \'-\', label=\'Isotonic Fit on Training Data\') ax.plot(X_test, y_pred_test, \'s\', label=\'Predicted Test Data\') ax.legend() return y_pred_test, fig"},{"question":"# Advanced Python Coding Assessment **Objective:** You are given an `email.message.Message` object representing an email message with potential MIME parts. Implement a function `extract_text_and_attachments` that extracts all text parts into a single string and collects all attachments into a list. The function should handle text simplifications while preserving email structure. **Function Signature:** ```python def extract_text_and_attachments(msg: email.message.Message) -> str, list: pass ``` **Input:** - `msg`: An instance of `email.message.Message`. **Output:** - A tuple containing: - A single string with all text content from the message and its subparts. - A list of tuples, each containing the filename and the content of the attachment as bytes. **Constraints:** - The function should traverse all subparts of the message recursively. - Text content and attachment processing should respect their respective MIME content types. - Attachments should be returned with their filenames and raw byte content. **Example:** Suppose `msg` contains a multipart message with the following structure: - Part 1: Text (Content-Type: text/plain) - Part 2: Attachment (Content-Disposition: attachment; filename=\\"example.pdf\\") ```python from email.message import Message msg = Message() msg.set_payload([ # Part 1 Message(), # Part 2 Message() ]) msg.get_payload()[0].set_type(\\"text/plain\\") msg.get_payload()[0].set_payload(\\"Hello, this is the email body.\\") msg.get_payload()[1].set_type(\\"application/pdf\\") msg.get_payload()[1].set_payload(b\'%PDF-1.4...\') msg.get_payload()[1].add_header(\'Content-Disposition\', \'attachment\', filename=\'example.pdf\') text, attachments = extract_text_and_attachments(msg) print(text) # Output: \\"Hello, this is the email body.\\" print(attachments) # Output: [(\\"example.pdf\\", b\'%PDF-1.4...\')] ``` **Instructions:** 1. Use the `walk()` method to traverse the entire message tree. 2. Extract text from parts with `Content-Type: text/*`. 3. For attachments, ensure to capture their filenames and byte content. 4. Combine and return the extracted text and list of attachments as per the example output. **Note:** You may assume that non-text parts and attachment processing do not require any complex handling other than what is described. **Performance Requirements:** - Ensure efficient traversal and extraction given the hierarchical nature of MIME messages. - Handle large messages and multiple attachments gracefully. **Additional Information:** Refer to the `email.message.Message` class documentation provided for method details that may assist in the implementation.","solution":"from email.message import Message from typing import List, Tuple def extract_text_and_attachments(msg: Message) -> Tuple[str, List[Tuple[str, bytes]]]: text_parts = [] attachments = [] for part in msg.walk(): # If the part is multipart, we skip it as the walk will handle subparts if part.is_multipart(): continue content_type = part.get_content_type() # Handling text parts if content_type.startswith(\'text/\'): charset = part.get_content_charset() if charset is None: charset = \\"utf-8\\" # default charset text_parts.append(part.get_payload(decode=True).decode(charset, errors=\'replace\')) # Handling attachments elif part.get(\'Content-Disposition\') is not None: if part.get(\'Content-Disposition\').startswith(\'attachment\'): filename = part.get_filename() attachments.append((filename, part.get_payload(decode=True))) combined_text = \\"n\\".join(text_parts) return combined_text, attachments"},{"question":"You are given a dataset `tips` which contains details about the total bill, the tip given, the day of the week, and other information for restaurant customers. You are required to create a scatter plot with a rug plot that visualizes the relationship between the total bill and the tip. Additionally, you need to consider the time of the meal (Lunch or Dinner) using hue mapping. The plot should also include the following customizations: 1. Rug height should be 0.1. 2. Rug should be outside the axes. 3. Alpha blending should be used for better visibility in dense regions. # Requirements 1. Create a scatter plot showing the relationship between `total_bill` and `tip`. 2. Add a rug plot to this scatter plot to mark the data points, with the rug height set to 0.1 and positioned outside the axes. 3. Use `hue` to differentiate the points by `time` (Lunch or Dinner). 4. Apply alpha blending to the rug plot to handle densely populated areas. # Input There are no direct inputs as the dataset is loaded directly in the code. # Output A Seaborn plot that meets the given requirements. # Constraints - Utilize the seaborn functions appropriately to customize your plots. - Ensure that the plot is well-labeled and clear for visualization. # Implementation Details The dataset `tips` can be loaded directly from seaborn as shown: ```python import seaborn as sns; sns.set_theme() tips = sns.load_dataset(\\"tips\\") ``` # Example Here\'s an example of how to load the dataset and start creating the plot: ```python import seaborn as sns; sns.set_theme() import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create a scatter plot with a rug plot sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", height=0.1, clip_on=False, alpha=0.5) # Show plot plt.show() ``` Make sure your final plot adheres to all the specifications.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_scatter_rug_plot(): Creates a scatter plot with a rug plot that visualizes the relationship between total_bill and tip using the \'time\' column for hue mapping. Rug plot is customized with height 0.1, outside the axes, and alpha blending for better visibility. sns.set_theme() # Set the seaborn theme tips = sns.load_dataset(\\"tips\\") # Load the tips dataset from seaborn # Create the scatter plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") # Add the rug plot with customization rug_plot = sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", height=0.1, clip_on=False, alpha=0.5) plt.title(\\"Scatter Plot with Rug Plot for Total Bill vs Tip\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.legend(title=\\"Time of Meal\\") # Display the plot plt.show()"},{"question":"**System Path Encoding and Fork Management** **Problem Statement:** You are tasked with developing a Python function that will interact with system-level settings, utilizing specific Python C API functionalities. You need to: 1. **Set System Path**: Setup a system path in a specific format. 2. **Fork a New Process**: Implement a forking mechanism ensuring the internal state is managed correctly before and after the fork for both the parent and child processes. 3. **Handle Encoding/Decoding**: Encode and then decode a given path for the process using appropriate system-encoding mechanisms. **Requirements:** - Implement the function `system_path_fork_management(path: str) -> dict` that performs the following steps: 1. Sets the system path using the provided `path` string. 2. Performs necessary pre-fork operations. 3. Forks the process. 4. Executes post-fork operations in both parent and child processes. 5. Encodes the `path` to system-specific byte encoding and then decodes it back to a Python string. 6. Return a dictionary with keys `parent_pid`, `child_pid`, and `decoded_path`. **Expectations from the Solution:** - The `path` argument will be a string containing the system paths separated by the appropriate delimiter (`:` for Unix and `;` for Windows). - Handle signals safely and manage any potential errors using appropriate system utilities. - Properly manage memory allocation and deallocation when encoding and decoding the path. - Ensure that the forking mechanism adheres to the requirements of being called from the main thread only. **Example Usage:** ```python result = system_path_fork_management(\\"/usr/bin:/usr/local/bin\\") print(result) # Expected Output (PID values will vary): # { # \\"parent_pid\\": 12345, # \\"child_pid\\": 67890, # \\"decoded_path\\": \\"/usr/bin:/usr/local/bin\\" # } ``` **Constraints:** - The `path` string will not exceed 1000 characters. - Assume the function is called in a context that permits fork operations and signal handling. - You may use available Python libraries if necessary but must adhere to system-level interaction provided by the Python C API outlined. Implement the function `system_path_fork_management` to meet the above specifications. **Note:** This problem will require an understanding of system programming and interaction with Python C API utilities.","solution":"import os import sys def system_path_fork_management(path: str) -> dict: Set system path, fork a new process, and handle encoding/decoding of the path. Args: - path (str): the system path string. Returns: - dict: containing \'parent_pid\', \'child_pid\', and \'decoded_path\'. # Set the system path os.environ[\'PATH\'] = path # Encode the path using system encoding system_encoding = sys.getfilesystemencoding() encoded_path = path.encode(system_encoding) # Pre-fork operations can be performed here (if any) # Fork the process child_pid = os.fork() if child_pid == 0: # Child process # Post-fork operations specific to child process can be done here (if any) # Decode the path back to string decoded_path = encoded_path.decode(system_encoding) # Return information from the child process return { \'parent_pid\': os.getppid(), \'child_pid\': os.getpid(), \'decoded_path\': decoded_path } else: # Parent process # Post-fork operations specific to parent process can be done here (if any) # Wait for the child process to complete os.waitpid(child_pid, 0) # Decode the path back to string decoded_path = encoded_path.decode(system_encoding) # Return information from the parent process return { \'parent_pid\': os.getpid(), \'child_pid\': child_pid, \'decoded_path\': decoded_path }"},{"question":"Objective Write a Python function using pandas that processes sales data, represented as a `DataFrame`, containing potential missing values in integer columns. You will need to compute total and average sales per category, considering the handling of nullable integer types properly. Function Signature ```python import pandas as pd def process_sales_data(df: pd.DataFrame) -> pd.DataFrame: Processes the sales data to compute total and average sales per category. Args: - df: pd.DataFrame: The input DataFrame containing sales data with columns \'Category\' (string) and \'Sales\' (nullable integer). Returns: - pd.DataFrame: A DataFrame with \'Category\', \'TotalSales\', \'AverageSales\' columns, reflecting total and average sales per category. pass ``` Input Format - `df` is a pandas `DataFrame` with the following structure: - `Category`: A string column representing the category of products. - `Sales`: A nullable integer column (dtype \\"Int64\\") representing sales figures, which might include missing values. Output Format - A pandas `DataFrame` with three columns: - `Category`: The product categories. - `TotalSales`: The total sales per category, calculated by summing up the \'Sales\' column in each category. - `AverageSales`: The average sales per category, calculated by averaging the \'Sales\' column in each category. Constraints - Missing sales values should be handled properly without causing errors in calculations. For average calculations, missing values should not be counted in the denominator. - You should use the `pd.NA` value appropriately when dealing with missing data. Examples ```python import pandas as pd # Example DataFrame data = { \\"Category\\": [\\"A\\", \\"A\\", \\"B\\", \\"B\\", \\"C\\"], \\"Sales\\": pd.array([100, 200, None, 300, pd.NA], dtype=\\"Int64\\") } df = pd.DataFrame(data) # Expected output expected_output = pd.DataFrame({ \\"Category\\": [\\"A\\", \\"B\\", \\"C\\"], \\"TotalSales\\": pd.array([300, 300, 0], dtype=\\"Int64\\"), \\"AverageSales\\": pd.array([150, 300, pd.NA], dtype=\\"Int64\\") }) # Function call output = process_sales_data(df) assert expected_output.equals(output) ``` Notes 1. Ensure all operations handle nullable integer types correctly. 2. Use `groupby` operations to calculate the required aggregates. 3. The ordering of categories in the output DataFrame should match their first appearance in the input DataFrame. 4. Test your implementation with edge cases, such as all missing sales or no sales data for a category.","solution":"import pandas as pd def process_sales_data(df: pd.DataFrame) -> pd.DataFrame: Processes the sales data to compute total and average sales per category. Args: - df: pd.DataFrame: The input DataFrame containing sales data with columns \'Category\' (string) and \'Sales\' (nullable integer). Returns: - pd.DataFrame: A DataFrame with \'Category\', \'TotalSales\', \'AverageSales\' columns, reflecting total and average sales per category. # Handle missing values in Sales by converting to 0 for the total calculation df[\'Sales\'] = df[\'Sales\'].fillna(0) # Calculate total sales per category total_sales = df.groupby(\'Category\')[\'Sales\'].sum().reset_index() total_sales.columns = [\'Category\', \'TotalSales\'] # Calculate average sales per category average_sales = df.groupby(\'Category\')[\'Sales\'].apply( lambda x: x.replace({0: pd.NA}).mean(skipna=True)).reset_index() average_sales.columns = [\'Category\', \'AverageSales\'] # Merge the results result = pd.merge(total_sales, average_sales, on=\'Category\') # Ensure dtype is Int64 result[\'TotalSales\'] = result[\'TotalSales\'].astype(\'Int64\') result[\'AverageSales\'] = result[\'AverageSales\'].astype(\'Int64\') return result"},{"question":"# Task You are provided with a dataset containing information about restaurant tips (`tips`). Your task is to use the seaborn library to create a bar plot that visualizes the total bill amount by day, dodging bars based on the time of the day and adding a small gap between dodged bars. Additionally, handle any potential empty spaces created by unaligned categorical variables. Input ```python import seaborn.objects as so from seaborn import load_dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) ``` Requirements 1. Use the seaborn `so.Plot` function to create the plot. 2. Add a bar plot representing total bill amounts (`total_bill`) by day. 3. Use the `Dodge` transform to separate bars by the time of the day (`time`), ensuring there\'s a small gap between them. 4. Handle potential empty spaces for any unaligned categorical variables. Expected Output A bar plot that accurately represents the total bill amounts by day, with bars dodged by the time of the day and a small gap between them, handling any empty spaces appropriately. Example Code Your function should generate the plot as follows: ```python def total_bill_plot(): p = so.Plot(tips, x=\\"day\\", y=\\"total_bill\\", color=\\"time\\") p.add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1, empty=\\"fill\\")) p.show() ``` Submission Submit the complete function `total_bill_plot()` as your solution. When executed, it should produce the described bar plot.","solution":"import seaborn.objects as so from seaborn import load_dataset def total_bill_plot(): tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) p = so.Plot(tips, x=\\"day\\", y=\\"total_bill\\", color=\\"time\\") p.add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1, empty=\\"fill\\")) p.show()"},{"question":"**Objective**: Implement a function to perform XML escaping and unescaping, mimicking the behavior of `xml.sax.saxutils.escape` and `xml.sax.saxutils.unescape`. Problem Statement You are provided with two functions `escape_xml` and `unescape_xml`. Your task is to implement these functions based on the `xml.sax.saxutils.escape` and `xml.sax.saxutils.unescape` utilities. 1. **Function: `escape_xml`** This function should take a string of data and a dictionary of additional entities that need to be escaped. It should return the string with those entities escaped. ```python def escape_xml(data: str, entities: dict = {}) -> str: pass ``` - **Input**: - `data`: A string that may contain characters to be escaped. - `entities`: A dictionary where the keys are characters to be escaped and values are their respective escape sequences. - **Output**: - A string with certain characters replaced (escaped) by their corresponding entities. - **Example**: ```python escape_xml(\\"Sample text & data\\", {\'&\': \'&amp;\'}) # Output: \\"Sample text &amp; data\\" escape_xml(\\"Use > and <\\", {\'<\': \'&lt;\', \'>\': \'&gt;\'}) # Output: \\"Use &gt; and &lt;\\" ``` 2. **Function: `unescape_xml`** This function should take a string of data and a dictionary of additional entities that need to be unescaped. It should return the string with those entities unescaped. ```python def unescape_xml(data: str, entities: dict = {}) -> str: pass ``` - **Input**: - `data`: A string that may contain escaped entities. - `entities`: A dictionary where the keys are entities to be unescaped and values are their corresponding characters. - **Output**: - A string with certain entities replaced (unescaped) by their corresponding characters. - **Example**: ```python unescape_xml(\\"Sample text &amp; data\\", {\'&amp;\': \'&\'}) # Output: \\"Sample text & data\\" unescape_xml(\\"Use &gt; and &lt;\\", {\'&lt;\': \'<\', \'&gt;\': \'>\'}) # Output: \\"Use > and <\\" ``` Constraints - The dictionary `entities` provided to both functions should have all strings as keys and values. - The functions must handle the default behavior of escaping/unescaping the characters `&`, `<`, and `>`. Additional Notes - The `escape_xml` function should always escape `&`, `<`, and `>` even if not provided in the `entities` dictionary. - The `unescape_xml` function should always unescape `&amp;`, `&lt;`, and `&gt;` even if not provided in the `entities` dictionary. Implement these functions to demonstrate your understanding of escaping and unescaping mechanisms in XML processing.","solution":"def escape_xml(data: str, entities: dict = {}) -> str: default_entities = {\'&\': \'&amp;\', \'<\': \'&lt;\', \'>\': \'&gt;\'} default_entities.update(entities) for key, value in default_entities.items(): data = data.replace(key, value) return data def unescape_xml(data: str, entities: dict = {}) -> str: default_entities = {\'&amp;\': \'&\', \'&lt;\': \'<\', \'&gt;\': \'>\'} default_entities.update(entities) for key, value in default_entities.items(): data = data.replace(key, value) return data"},{"question":"# Question: Implementing a Priority Queue with \\"heapq\\" Objective: Your task is to implement a priority queue using Python\'s \\"heapq\\" module. This priority queue should support the following operations: 1. Inserting an element with a given priority. 2. Popping the element with the highest priority (i.e., the lowest priority value). 3. Peeking at the element with the highest priority without removing it. Requirements: - Implement the class `PriorityQueue` with the following methods: - `__init__(self)`: Initializes an empty priority queue. - `insert(self, priority: int, element: Any)`: Inserts an element with the given priority. - `pop(self) -> Any`: Removes and returns the element with the highest priority. - `peek(self) -> Any`: Returns the element with the highest priority without removing it. Input and Output Formats: - `insert`: - Input: `priority` (int), `element` (any type). - Output: None. - `pop`: - Input: None. - Output: The element with the highest priority. - `peek`: - Input: None. - Output: The element with the highest priority. Example Usage: ```python pq = PriorityQueue() pq.insert(2, \\"task2\\") pq.insert(1, \\"task1\\") pq.insert(3, \\"task3\\") print(pq.peek()) # Output: \\"task1\\" print(pq.pop()) # Output: \\"task1\\" print(pq.pop()) # Output: \\"task2\\" ``` Constraints: - Assume that `priority` values are unique. - You may use any data type for `element`. - Use the `heapq` module for managing the heap operations. Notes: Consider edge cases such as peeking or popping from an empty priority queue, and handle them gracefully by raising appropriate exceptions.","solution":"import heapq class PriorityQueue: def __init__(self): self.heap = [] def insert(self, priority: int, element: any): heapq.heappush(self.heap, (priority, element)) def pop(self) -> any: if self.is_empty(): raise IndexError(\\"pop from an empty priority queue\\") return heapq.heappop(self.heap)[1] def peek(self) -> any: if self.is_empty(): raise IndexError(\\"peek from an empty priority queue\\") return self.heap[0][1] def is_empty(self) -> bool: return len(self.heap) == 0"},{"question":"# Advanced Type Handling and Inheritance in Python C API In this assessment, you will write a Python C extension function that will: 1. Check if a given Python object is a type. 2. Retrieve type flags for a given type object and check if it supports garbage collection. 3. Verify and state if one type is a subtype of another. 4. Create a new heap-allocated type dynamically. The objective is to ensure you understand the mechanisms provided by the Python C API to manage type objects and their relationships. Requirements 1. Implement a function `check_type_info(PyObject *obj, PyTypeObject *base_type)` that: - Returns `True` if `obj` is a type, `False` otherwise. - Retrieves and returns the type flags of `obj`. - Returns `True` if `obj` supports garbage collection, `False` otherwise. - Returns `True` if `obj` is a subtype of `base_type`, `False` otherwise. 2. Implement a function `create_heap_type(PyObject *module, const char *type_name)` that: - Creates a new heap-allocated type with the name `type_name`. - The new type should derive from `PyBaseObject_Type`. - The new type should have a basic size and itemsize of 0 (`tp_basicsize = 0`, `tp_itemsize = 0`). - Registers this new type in the given `module`. Provide the C code for these functions and ensure they can be compiled into a Python module. Sample Interface (Python) ```python import my_c_extension # Check type information result = my_c_extension.check_type_info(SomeClass, BaseClass) print(result) # Example output: (True, 1048576, True, False) # Create a new heap type my_c_extension.create_heap_type(module, \\"MyNewType\\") ``` Notes - Use the provided documentation to find the appropriate API functions. - Ensure proper error handling and reference counting as per Python C extension requirements. - Your solution should compile without errors and be callable from Python as shown in the sample interface.","solution":"# This is a simulated Python code representation of what the Python C extension # code would look like. def check_type_info(obj, base_type): Check type information about a provided object. Parameters: obj: The object to check. base_type: The base type to check subtyping against. Returns: tuple: (is_type, type_flags, supports_garbage_collection, is_subtype_of_base) is_type = isinstance(obj, type) type_flags = None supports_garbage_collection = False is_subtype_of_base = False if is_type: # Retrieving type flags - Simulated Behavior type_flags = obj.__flags__ # Checking if object supports garbage collection supports_garbage_collection = (type_flags & 4) != 0 # Checking if obj is a subtype of base_type is_subtype_of_base = issubclass(obj, base_type) return (is_type, type_flags, supports_garbage_collection, is_subtype_of_base) def create_heap_type(module, type_name): Create a heap-allocated type dynamically. Parameters: module: The module to register the new type with. type_name: The name of the new type. Returns: type: The new type created. # This is a simulated behavior because creating a type dynamically # would normally make use of the Python C API. # Dynamically creating a new type new_type = type(type_name, (object,), {}) # Registering new type in the module setattr(module, type_name, new_type) return new_type"},{"question":"# Question: Implementing a Custom Descriptor **Objective:** Write a Python class implementing a custom descriptor that validates and formats an attribute of another class. **Task:** 1. Implement a class `ValidatedAttribute` that acts as a descriptor. 2. This descriptor should: - Ensure that the attribute is always stored as a string. - Ensure that the string length is between a specified minimum and maximum length. - Raise appropriate exceptions if the validation criteria are not met. **Details:** - Implement the descriptor class `ValidatedAttribute` with the following methods: - `__init__(self, min_length, max_length)`: Constructor that sets the minimum and maximum length for the string. - `__get__(self, instance, owner)`: Method that retrieves the value of the attribute. - `__set__(self, instance, value)`: Method that sets and validates the attribute value. - `__delete__(self, instance)`: Method that deletes the attribute (optional implementation). **Constraints:** - The attribute must always be a string. - The string length must be between `min_length` and `max_length` inclusive. - If the value provided is not a string or does not satisfy length constraints, raise a `ValueError` with an appropriate error message. **Example Usage:** ```python class Person: name = ValidatedAttribute(3, 100) # \'name\' must be a string with length between 3 and 100. person = Person() person.name = \\"Alice\\" # Should work fine. print(person.name) # Output: \\"Alice\\" person.name = \\"Al\\" # Should raise ValueError: \\"String length must be between 3 and 100\\" person.name = 123 # Should raise ValueError: \\"Attribute value must be a string\\" ``` **Input and Output** Your class definition should conform to the example usage given above. The actual input and output will depend on how the `ValidatedAttribute` descriptor is used within other classes. **Performance Requirements:** The descriptor should be efficient and work for typical use cases without significant performance bottlenecks. **Submission Requirements:** Submit the implementation of the `ValidatedAttribute` class with any necessary helper functions.","solution":"class ValidatedAttribute: def __init__(self, min_length, max_length): self.min_length = min_length self.max_length = max_length def __get__(self, instance, owner): return instance.__dict__.get(self._attribute_name, None) def __set__(self, instance, value): if not isinstance(value, str): raise ValueError(\\"Attribute value must be a string\\") if not (self.min_length <= len(value) <= self.max_length): raise ValueError(f\\"String length must be between {self.min_length} and {self.max_length}\\") instance.__dict__[self._attribute_name] = value def __set_name__(self, owner, name): self._attribute_name = name def __delete__(self, instance): if self._attribute_name in instance.__dict__: del instance.__dict__[self._attribute_name]"},{"question":"Objective: You are to implement a Python function that will compare two directory trees and generate a report summarizing identical files, different files, and unique files in each directory. This report will be structured and formatted as specified below. Function Signature: ```python def compare_directories(dir1: str, dir2: str) -> dict: pass ``` Input: - `dir1` (str): Path to the first directory. - `dir2` (str): Path to the second directory. Output: - Returns a dictionary with the following keys: - `identical_files`: List of files that are identical in both directories. - `different_files`: List of files that are present in both directories but have different contents. - `unique_to_dir1`: List of files that are only in the first directory. - `unique_to_dir2`: List of files that are only in the second directory. Constraints: - The function should handle any valid paths for `dir1` and `dir2`. - Ensure efficiency in comparison without compromising correctness. - Assume there can be nested subdirectories in both `dir1` and `dir2`. - Utilize methods from the `filecmp` module. Example Use: ```python result = compare_directories(\'path_to_directory1\', \'path_to_directory2\') print(result) # Output Example: # { # \'identical_files\': [\'a.txt\', \'docs/report.txt\'], # \'different_files\': [\'resume.docx\'], # \'unique_to_dir1\': [\'pic1.jpg\', \'logs/log1.txt\'], # \'unique_to_dir2\': [\'pic2.png\', \'logs/log2.txt\'] # } ``` Performance Note: Ensure the implementation leverages internal caching mechanisms where appropriate to optimize for large directory trees. Hints: - Use `filecmp.dircmp` to compare directories. - Make use of `report()` and other methods provided by `filecmp.dircmp` to gather necessary comparisons and form the result dictionary. Happy coding!","solution":"import os import filecmp def compare_directories(dir1: str, dir2: str) -> dict: def compare_files_recursive(dcmp): identical_files = [] different_files = [] unique_to_dir1 = [] unique_to_dir2 = [] # Inspect files that are the same in both directories for name in dcmp.same_files: identical_files.append(os.path.join(dcmp.left, name)) # Different files in both directories for name in dcmp.diff_files: different_files.append(os.path.join(dcmp.left, name)) # Files only in dir1 for name in dcmp.left_only: unique_to_dir1.append(os.path.join(dcmp.left, name)) # Files only in dir2 for name in dcmp.right_only: unique_to_dir2.append(os.path.join(dcmp.right, name)) # Recursively go through subdirectories for sub_dcmp in dcmp.subdirs.values(): sub_id, sub_diff, sub_uni_dir1, sub_uni_dir2 = compare_files_recursive(sub_dcmp) identical_files.extend(sub_id) different_files.extend(sub_diff) unique_to_dir1.extend(sub_uni_dir1) unique_to_dir2.extend(sub_uni_dir2) return identical_files, different_files, unique_to_dir1, unique_to_dir2 dcmp = filecmp.dircmp(dir1, dir2) identical_files, different_files, unique_to_dir1, unique_to_dir2 = compare_files_recursive(dcmp) return { \'identical_files\': [os.path.relpath(file, dir1) for file in identical_files], \'different_files\': [os.path.relpath(file, dir1) for file in different_files], \'unique_to_dir1\': [os.path.relpath(file, dir1) for file in unique_to_dir1], \'unique_to_dir2\': [os.path.relpath(file, dir2) for file in unique_to_dir2], }"},{"question":"**Question: Clustering and Performance Evaluation with Scikit-learn** You are required to implement a clustering analysis using multiple algorithms from the scikit-learn library. Your task is to perform clustering on a given dataset, compare the results, and evaluate the clustering performance using several metrics. # Dataset Download the Iris dataset from the UCI repository. This dataset contains 150 samples of iris flowers, each with four features (sepal length, sepal width, petal length, petal width) and a ground truth class label indicating the species. # Task Requirements 1. **Data Loading and Preprocessing:** - Load the Iris dataset. - Perform any necessary preprocessing to prepare the data for clustering (e.g., scaling the features). 2. **Clustering:** Implement three different clustering algorithms from the provided scikit-learn clustering methods: - K-Means (with k=3) - Agglomerative Clustering (with k=3) - DBSCAN (using default parameters) 3. **Performance Evaluation:** Evaluate the performance of each clustering algorithm using the following metrics: - Adjusted Rand Index (ARI) - Silhouette Coefficient - Davies-Bouldin Index 4. **Comparison and Analysis:** - Compare the clustering results and evaluation metrics across the three algorithms. - Discuss which algorithm performed the best and why. - Visualize the clusters obtained from each algorithm in a 2D plot using the first two principal components (PCA) of the data. # Implementation Details - Use the scikit-learn library for clustering and performance evaluation. - Use matplotlib or any other visualization library for plotting the results. - Ensure your code is well-documented, with comments explaining each step and the rationale behind it. # Expected Input and Output Formats - **Input:** None (the dataset should be loaded within the code). - **Output:** Print the performance evaluation results and display plots as described. # Constraints and Limitations - You must use only the scikit-learn library for clustering and evaluation metrics. - Your solution should handle any potential issues with the dataset (e.g., missing values or data scaling). - Aim for efficient and readable code. # Additional Notes - Consider edge cases where clustering performance may be impacted, such as overlapping clusters or varying densities. - Explore different parameter settings for the DBSCAN algorithm to improve its performance and compare the results. ```python # Sample Code Structure import numpy as np import pandas as pd from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN from sklearn.metrics import adjusted_rand_score, silhouette_score, davies_bouldin_score from sklearn.decomposition import PCA import matplotlib.pyplot as plt # Load the dataset iris = load_iris() X = iris.data y_true = iris.target # Preprocess the data (e.g., feature scaling) scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # 1. K-Means Clustering kmeans = KMeans(n_clusters=3, random_state=42) y_kmeans = kmeans.fit_predict(X_scaled) # 2. Agglomerative Clustering agglo = AgglomerativeClustering(n_clusters=3) y_agglo = agglo.fit_predict(X_scaled) # 3. DBSCAN dbscan = DBSCAN() y_dbscan = dbscan.fit_predict(X_scaled) # Evaluation Metrics def evaluate_clustering(y_true, y_pred, X): ari = adjusted_rand_score(y_true, y_pred) silhouette = silhouette_score(X, y_pred) davies_bouldin = davies_bouldin_score(X, y_pred) return ari, silhouette, davies_bouldin # Evaluate each clustering algorithm kmeans_scores = evaluate_clustering(y_true, y_kmeans, X_scaled) agglo_scores = evaluate_clustering(y_true, y_agglo, X_scaled) dbscan_scores = evaluate_clustering(y_true, y_dbscan, X_scaled) # Print the evaluation results print(f\\"K-Means Scores (ARI, Silhouette, Davies-Bouldin): {kmeans_scores}\\") print(f\\"Agglomerative Clustering Scores (ARI, Silhouette, Davies-Bouldin): {agglo_scores}\\") print(f\\"DBSCAN Scores (ARI, Silhouette, Davies-Bouldin): {dbscan_scores}\\") # Visualize the clusters pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) fig, ax = plt.subplots(1, 3, figsize=(15, 5)) # K-Means Plot ax[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y_kmeans, cmap=\'rainbow\', s=50) ax[0].set_title(\\"K-Means Clustering\\") # Agglomerative Plot ax[1].scatter(X_pca[:, 0], X_pca[:, 1], c=y_agglo, cmap=\'rainbow\', s=50) ax[1].set_title(\\"Agglomerative Clustering\\") # DBSCAN Plot ax[2].scatter(X_pca[:, 0], X_pca[:, 1], c=y_dbscan, cmap=\'rainbow\', s=50) ax[2].set_title(\\"DBSCAN Clustering\\") plt.show() ```","solution":"import numpy as np import pandas as pd from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN from sklearn.metrics import adjusted_rand_score, silhouette_score, davies_bouldin_score from sklearn.decomposition import PCA import matplotlib.pyplot as plt # Load the dataset iris = load_iris() X = iris.data y_true = iris.target # Preprocess the data (e.g., feature scaling) scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # 1. K-Means Clustering kmeans = KMeans(n_clusters=3, random_state=42) y_kmeans = kmeans.fit_predict(X_scaled) # 2. Agglomerative Clustering agglo = AgglomerativeClustering(n_clusters=3) y_agglo = agglo.fit_predict(X_scaled) # 3. DBSCAN dbscan = DBSCAN() y_dbscan = dbscan.fit_predict(X_scaled) # Evaluation Metrics def evaluate_clustering(y_true, y_pred, X): ari = adjusted_rand_score(y_true, y_pred) silhouette = silhouette_score(X, y_pred) if len(set(y_pred)) > 1 else -1 # Avoid error davies_bouldin = davies_bouldin_score(X, y_pred) if len(set(y_pred)) > 1 else float(\'inf\') # Avoid error return ari, silhouette, davies_bouldin # Evaluate each clustering algorithm kmeans_scores = evaluate_clustering(y_true, y_kmeans, X_scaled) agglo_scores = evaluate_clustering(y_true, y_agglo, X_scaled) dbscan_scores = evaluate_clustering(y_true, y_dbscan, X_scaled) # Print the evaluation results print(f\\"K-Means Scores (ARI, Silhouette, Davies-Bouldin): {kmeans_scores}\\") print(f\\"Agglomerative Clustering Scores (ARI, Silhouette, Davies-Bouldin): {agglo_scores}\\") print(f\\"DBSCAN Scores (ARI, Silhouette, Davies-Bouldin): {dbscan_scores}\\") # Visualize the clusters pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) fig, ax = plt.subplots(1, 3, figsize=(15, 5)) # K-Means Plot ax[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y_kmeans, cmap=\'rainbow\', s=50) ax[0].set_title(\\"K-Means Clustering\\") # Agglomerative Plot ax[1].scatter(X_pca[:, 0], X_pca[:, 1], c=y_agglo, cmap=\'rainbow\', s=50) ax[1].set_title(\\"Agglomerative Clustering\\") # DBSCAN Plot ax[2].scatter(X_pca[:, 0], X_pca[:, 1], c=y_dbscan, cmap=\'rainbow\', s=50) ax[2].set_title(\\"DBSCAN Clustering\\") plt.show()"},{"question":"# Distributed Tensor (DTensor) Manipulation and Verification **Objective**: Demonstrate your understanding of the PyTorch DTensor API by creating distributed tensors, manipulating their layouts, and verifying their consistency across processes. Problem Statement You are tasked with writing code that involves the following steps: 1. **Initialize a Distributed Environment**: - Set up a PyTorch distributed environment ensuring that your code can run on multiple processes. 2. **Create a DTensor**: - Utilize the `DTensor` class to create a distributed tensor across multiple devices with an initial placement scheme. - You should initialize a tensor of shape (4, 4) filled with random values. 3. **Modify the DTensor**: - Change the placement of the initially created DTensor using the `redistribute` method. - Convert the initial `Shard` placement along the 0th dimension to a `Replicate` placement. 4. **Verification**: - Write a function to verify the consistency of the tensor across all devices after redistribution. - Print out the full tensor on each device to ensure consistent values. Implementation Details 1. **Initialize a Distributed Environment**: - Use `torch.distributed.init_process_group` for setting up distributed processing. 2. **Creating DTensor**: - Use the `DTensor` factory functions such as `rand` for initializing a distributed tensor. 3. **Modify the DTensor**: - Apply sharding using `DeviceMesh` and `Placement`. - Use the `redistribute` method to change the tensor\'s placement. 4. **Verification Function**: - Collect the tensor from each device and print it to verify the consistency. Inputs and Outputs - **Input**: No standard input. The task revolves around creating and manipulating tensors and verifying them. - **Output**: Print statements showing the DTensor on each device before and after redistribution. Constraints - Ensure that the code handles multiple processes (e.g., at least 2 processes). - Use appropriate device arrangements to demonstrate functionality (e.g., 2 GPUs). Example Code Structure ```python import torch import torch.distributed as dist from torch.distributed.tensor import DTensor from torch.distributed.tensor.placement_types import Shard, Replicate def initialize_distributed_env(rank, world_size): dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) torch.cuda.set_device(rank) def create_dtensor(): # Setup DeviceMesh with 2 GPUs device_mesh = DeviceMesh(mesh=[0, 1]) # Initial Shard placement along dimension 0 placements = [Shard(0)] # Create 4x4 DTensor with random values initially sharded along dimension 0 initial_tensor = torch.rand(4, 4) d_tensor = DTensor.from_local(initial_tensor, device_mesh, placements) return d_tensor def redistribute_dtensor(d_tensor): # Change placement from Shard to Replicate new_placements = [Replicate()] redistributed_tensor = d_tensor.redistribute(new_placements) return redistributed_tensor def verify_tensor_redistribution(d_tensor): # Print full tensor on all devices for verification full_tensor = d_tensor.full_tensor() print(f\\"Full DTensor on device {torch.cuda.current_device()}:n{full_tensor}\\") def main(rank, world_size): initialize_distributed_env(rank, world_size) # Create and modify DTensor d_tensor = create_dtensor() verify_tensor_redistribution(d_tensor) d_tensor = redistribute_dtensor(d_tensor) verify_tensor_redistribution(d_tensor) dist.destroy_process_group() if __name__ == \\"__main__\\": world_size = 2 torch.multiprocessing.spawn(main, args=(world_size,), nprocs=world_size, join=True) ``` Note: Ensure your environment has multiple GPUs available and configured correctly for this exercise. Verify the distributed package installation and dependencies as per the device setup.","solution":"import torch import torch.distributed as dist from torch.distributed.tensor import ( DeviceMesh, DTensor, ) from torch.distributed.tensor.placement_types import Shard, Replicate def initialize_distributed_env(rank, world_size): dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) torch.cuda.set_device(rank) def create_dtensor(rank, world_size): # Creating a DeviceMesh with 2 processes device_mesh = DeviceMesh(\\"cuda\\", list(range(world_size))) # Initial Shard placement along dimension 0 placements = [Shard(0)] # Create a 4x4 DTensor with random values, initially sharded along dimension 0 local_tensor = torch.rand(2, 4, device=f\'cuda:{rank}\') d_tensor = DTensor.from_local(local_tensor, device_mesh, placements) return d_tensor def redistribute_dtensor(d_tensor): # Change placement from Shard to Replicate new_placements = [Replicate()] redistributed_tensor = d_tensor.redistribute(placements=new_placements) return redistributed_tensor def gather_and_print(d_tensor): # Gather full tensor on rank 0 and print full_tensor = d_tensor.to_local() gathered = [torch.empty_like(full_tensor) for _ in range(dist.get_world_size())] dist.all_gather(gathered, full_tensor) if dist.get_rank() == 0: full_dtensor = torch.cat(gathered, dim=0) print(f\\"Gathered Tensor on Rank 0:n{full_dtensor}\\") def main(rank, world_size): initialize_distributed_env(rank, world_size) # Create the initial sharded DTensor d_tensor = create_dtensor(rank, world_size) gather_and_print(d_tensor) # Redistribute the DTensor to replicated form and verify d_tensor = redistribute_dtensor(d_tensor) gather_and_print(d_tensor) dist.destroy_process_group() if __name__ == \\"__main__\\": world_size = 2 torch.multiprocessing.spawn(main, args=(world_size,), nprocs=world_size, join=True)"},{"question":"**Question: Implementation of Partial Least Squares Regression (PLSRegression)** Using the provided `sklearn.cross_decomposition.PLSRegression` class, implement a function to perform Partial Least Squares Regression on a given dataset. Your task is to preprocess the data, fit the model, perform dimensionality reduction, and evaluate the model\'s performance. # Function Signature ```python def pls_regression(X_train, Y_train, X_test, Y_test, n_components): Perform Partial Least Squares Regression on the given dataset. Parameters: - X_train (pd.DataFrame or np.ndarray): Training feature set. - Y_train (pd.DataFrame or np.ndarray): Training target set. - X_test (pd.DataFrame or np.ndarray): Testing feature set. - Y_test (pd.DataFrame or np.ndarray): Testing target set. - n_components (int): Number of components to keep. Returns: - dict: A dictionary containing the following keys: * \'train_score\' (float): The R^2 score on the training set. * \'test_score\' (float): The R^2 score on the test set. * \'X_train_transformed\' (np.ndarray): The transformed training features. * \'X_test_transformed\' (np.ndarray): The transformed testing features. * \'coef\' (np.ndarray): The coefficients of the PLS regression model. pass ``` # Requirements 1. **Data Preprocessing**: Center and scale `X_train` and `X_test`. 2. **Model Fitting**: Use the `PLSRegression` class from `sklearn.cross_decomposition` to fit the model on `X_train` and `Y_train` with the specified `n_components`. 3. **Transformation**: Transform both `X_train` and `X_test` using the fitted model. 4. **Evaluation**: Calculate the R^2 score for both the training and testing sets. 5. **Outputs**: Return a dictionary with: - R^2 score on the training set. - R^2 score on the testing set. - Transformed training features. - Transformed testing features. - Model coefficients. # Constraints 1. You may assume that `X_train`, `X_test`, `Y_train`, and `Y_test` are provided as either pandas DataFrames or NumPy arrays. 2. The function should handle multi-target regression if `Y_train` and `Y_test` have more than one column. # Example Usage ```python import numpy as np import pandas as pd # Creating a synthetic dataset np.random.seed(0) X_train = np.random.rand(100, 10) Y_train = np.random.rand(100, 1) X_test = np.random.rand(20, 10) Y_test = np.random.rand(20, 1) result = pls_regression(X_train, Y_train, X_test, Y_test, n_components=2) print(result[\'train_score\']) # e.g., 0.85 print(result[\'test_score\']) # e.g., 0.80 ``` Make sure to carefully follow the steps described in the documentation for the correct implementation of Partial Least Squares Regression.","solution":"import numpy as np from sklearn.cross_decomposition import PLSRegression from sklearn.preprocessing import StandardScaler from sklearn.metrics import r2_score def pls_regression(X_train, Y_train, X_test, Y_test, n_components): Perform Partial Least Squares Regression on the given dataset. Parameters: - X_train (pd.DataFrame or np.ndarray): Training feature set. - Y_train (pd.DataFrame or np.ndarray): Training target set. - X_test (pd.DataFrame or np.ndarray): Testing feature set. - Y_test (pd.DataFrame or np.ndarray): Testing target set. - n_components (int): Number of components to keep. Returns: - dict: A dictionary containing the following keys: * \'train_score\' (float): The R^2 score on the training set. * \'test_score\' (float): The R^2 score on the test set. * \'X_train_transformed\' (np.ndarray): The transformed training features. * \'X_test_transformed\' (np.ndarray): The transformed testing features. * \'coef\' (np.ndarray): The coefficients of the PLS regression model. # Standardize the features scalerX = StandardScaler() X_train_scaled = scalerX.fit_transform(X_train) X_test_scaled = scalerX.transform(X_test) # Standardize the targets scalerY = StandardScaler() Y_train_scaled = scalerY.fit_transform(Y_train) Y_test_scaled = scalerY.transform(Y_test) # Initialize and fit PLSRegression model pls = PLSRegression(n_components=n_components) pls.fit(X_train_scaled, Y_train_scaled) # Transform both training and testing sets X_train_transformed = pls.transform(X_train_scaled) X_test_transformed = pls.transform(X_test_scaled) # Predict and evaluate Y_train_pred = pls.predict(X_train_scaled) Y_test_pred = pls.predict(X_test_scaled) train_score = r2_score(Y_train_scaled, Y_train_pred) test_score = r2_score(Y_test_scaled, Y_test_pred) # Getting the coefficients coef = pls.coef_ return { \'train_score\': train_score, \'test_score\': test_score, \'X_train_transformed\': X_train_transformed, \'X_test_transformed\': X_test_transformed, \'coef\': coef }"},{"question":"**Objective**: Demonstrate your understanding of PyTorch\'s `DataLoader` and associated functionalities by implementing both map-style and iterable-style datasets. Utilize custom samplers, collate functions, and memory pinning for optimized data loading. Problem Statement: You are provided with a dataset of images and their corresponding labels. Your task is to create two distinct PyTorch datasets: 1. A map-style dataset that fetches images and labels using indices. 2. An iterable-style dataset that streams data from batches. You will then create custom `DataLoader` instances for each dataset, utilizing custom samplers, collate functions, and memory pinning to ensure efficient data loading. # Implementation Details: Map-Style Dataset 1. **MapStyleDataset**: - Inherits from `torch.utils.data.Dataset`. - Implements `__getitem__` and `__len__`. - Loads data from file paths stored in a list. 2. **CustomSampler**: - Inherits from `torch.utils.data.Sampler`. - Implements `__iter__` to provide custom sampling logic (e.g., randomized sampling). 3. **CustomCollateFn**: - Custom collate function to batch data, convert to PyTorch tensors, and pad sequences if necessary. 4. **MemoryPinning**: - Enable memory pinning to optimize data transfer to CUDA-enabled GPUs. Iterable-Style Dataset 1. **IterableStyleDataset**: - Inherits from `torch.utils.data.IterableDataset`. - Implements `__iter__` to stream data in real-time (for example, reading from a hypothetical data stream). 2. **Single-process and Multi-process Data Loading**: - Implement both single-process and multi-process data loading mechanisms. - Use `torch.utils.data.get_worker_info` to handle multi-process loading appropriately. Inputs and Outputs - **Input**: Paths to image and label files. - **Output**: Batches of images and labels prepared for training. # Requirements - Implement the `MapStyleDataset` class. - Implement the `IterableStyleDataset` class. - Implement the `CustomSampler` class. - Implement a custom collate function `custom_collate_fn`. - Implement data loaders for both datasets with memory pinning enabled. - Provide examples demonstrating data loading with both single-process and multi-process settings. # Constraints - Assume the dataset size can be large, and efficient data loading is crucial. - Handle various edge cases such as missing files and empty batches. # Example Below is a skeleton code to help you get started: ```python import torch from torch.utils.data import Dataset, DataLoader, Sampler, IterableDataset import numpy as np import random # Map-Style Dataset class MapStyleDataset(Dataset): def __init__(self, data_paths: list): self.data_paths = data_paths def __getitem__(self, index: int): # Implement loading data from self.data_paths data, label = load_data(self.data_paths[index]) return data, label def __len__(self): return len(self.data_paths) # Custom Sampler class CustomSampler(Sampler): def __init__(self, data_source): self.data_source = data_source def __iter__(self): # Implement custom sampling logic indices = list(range(len(self.data_source))) random.shuffle(indices) return iter(indices) # Custom Collate Function def custom_collate_fn(batch): # Implement custom collation logic data, labels = zip(*batch) data = torch.stack(data) labels = torch.tensor(labels) return data, labels # Iterable-Style Dataset class IterableStyleDataset(IterableDataset): def __init__(self, data_stream): self.data_stream = data_stream def __iter__(self): for data in self.data_stream: yield data # Example usage with DataLoader for Map-Style Dataset map_dataset = MapStyleDataset(data_paths=[\'path_to_data\']) map_loader = DataLoader(map_dataset, batch_size=32, sampler=CustomSampler(map_dataset), collate_fn=custom_collate_fn, pin_memory=True) # Example usage with DataLoader for Iterable-Style Dataset iter_dataset = IterableStyleDataset(data_stream=my_data_stream_gen()) iter_loader = DataLoader(iter_dataset, batch_size=32, pin_memory=True) # Single-process data loading for batch in map_loader: print(batch) # Multi-process data loading multi_loader = DataLoader(map_dataset, batch_size=32, sampler=CustomSampler(map_dataset), collate_fn=custom_collate_fn, pin_memory=True, num_workers=4) for batch in multi_loader: print(batch) # Define load_data and my_data_stream_gen functions as per your requirement. ``` Ensure that your solution is robust, efficient, and cleans up resources appropriately. Document your code to explain your logic clearly.","solution":"import torch from torch.utils.data import Dataset, DataLoader, Sampler, IterableDataset import numpy as np import random # Function to simulate loading data def load_data(path): # This function simulates loading an image and its label from the given path # For the sake of example, we generate random data image = np.random.rand(100, 100, 3) # example image data label = np.random.randint(0, 10) # example label return image, label # Generator function to simulate real-time data streaming def my_data_stream_gen(): while True: yield np.random.rand(100, 100, 3), np.random.randint(0, 10) # Map-Style Dataset class MapStyleDataset(Dataset): def __init__(self, data_paths): self.data_paths = data_paths def __getitem__(self, index): data, label = load_data(self.data_paths[index]) data = torch.tensor(data, dtype=torch.float32).permute(2, 0, 1) # Convert to tensor and change dimensions label = torch.tensor(label, dtype=torch.long) return data, label def __len__(self): return len(self.data_paths) # Custom Sampler class CustomSampler(Sampler): def __init__(self, data_source): self.data_source = data_source def __iter__(self): indices = list(range(len(self.data_source))) random.shuffle(indices) return iter(indices) # Custom Collate Function def custom_collate_fn(batch): data, labels = zip(*batch) data = torch.stack(data) labels = torch.tensor(labels) return data, labels # Iterable-Style Dataset class IterableStyleDataset(IterableDataset): def __init__(self, data_stream): self.data_stream = data_stream def __iter__(self): for data in self.data_stream: image, label = data image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) label = torch.tensor(label, dtype=torch.long) yield image, label # Example usage with DataLoader for Map-Style Dataset map_dataset = MapStyleDataset(data_paths=[\'path_to_data1\', \'path_to_data2\', \'path_to_data3\']) map_loader = DataLoader(map_dataset, batch_size=2, sampler=CustomSampler(map_dataset), collate_fn=custom_collate_fn, pin_memory=True) # Example usage with DataLoader for Iterable-Style Dataset iter_dataset = IterableStyleDataset(data_stream=my_data_stream_gen()) iter_loader = DataLoader(iter_dataset, batch_size=2, pin_memory=True) # Note: For single-process and multi-process data loading, both DataLoader usage is demonstrated above."},{"question":"You are required to implement a function to encode and decode a given text using the MIME quoted-printable encoding scheme. The function should accept a plain text input and provide both its encoded and subsequently decoded representation to ensure the integrity of the encoding process. # Function Signature ```python def quoted_printable_round_trip(text: str) -> tuple: Encode the given text using quoted-printable encoding and then decode it back. Args: text (str): The plain text input that needs to be encoded and decoded. Returns: tuple: A tuple containing the encoded text (str) and the decoded text (str). ``` # Input - `text`: A plain text input string that needs to be encoded and decoded. # Output - A tuple containing: - The encoded text as a string. - The decoded text as a string. # Constraints 1. You must use the `quopri` module for encoding and decoding the text. 2. Be mindful of handling binary data while encoding and decoding. # Example ```python plaintext = \\"Hello, World! This is a test email.nEnjoy encoding!\\" encoded_text, decoded_text = quoted_printable_round_trip(plaintext) print(\\"Encoded Text:\\", encoded_text) print(\\"Decoded Text:\\", decoded_text) ``` **Expected Output:** The `encoded_text` should be displayed in the quoted-printable encoded format, while the `decoded_text` should be identical to the `plaintext`, showcasing that the encoding and decoding process works correctly. # Additional Requirements 1. Ensure that your function can handle UTF-8 encoded input. 2. No external libraries should be used apart from the `quopri` module. 3. Pay attention to end-of-line characters to ensure accurate re-encoding.","solution":"import quopri def quoted_printable_round_trip(text: str) -> tuple: Encode the given text using quoted-printable encoding and then decode it back. Args: text (str): The plain text input that needs to be encoded and decoded. Returns: tuple: A tuple containing the encoded text (str) and the decoded text (str). # Encode the text using quoted-printable encoding encoded = quopri.encodestring(text.encode(\'utf-8\')) # Decode the encoded text back to the original text decoded = quopri.decodestring(encoded).decode(\'utf-8\') return (encoded.decode(\'utf-8\'), decoded)"},{"question":"**Objective:** Create a script where you initialize a resource and ensure it is properly released at the end of the program execution using the `atexit` module. **Scenario:** You are tasked with creating a file handler system where a file is opened at the start of a program, and written to during its execution. At the end of the program, the file must be properly closed, and a final message should be written into it. You will use the `atexit` module to ensure this happens even if the program ends normally. **Instructions:** 1. Write a function `open_file(filename)` that opens a file with the given filename in append mode, writes the starting message \\"File opened.\\" into the file, and returns the file object. 2. Register a cleanup function `close_file(file, final_message)` using `atexit.register()` which takes a file object and a final message as arguments. This function should write the final message into the file and properly close the file. 3. Write a function `write_to_file(file, message)` that appends a given message to the file. 4. Demonstrate the functionality by: - Opening a file named \\"logfile.txt\\". - Writing several lines into the file using `write_to_file()`. - Ensure that upon program termination, the `close_file()` function runs, writing \\"File closed.\\" to the file and closing it. **Constraints:** - Do not use global variables except for `_file_obj` and `_final_msg`. - The program should handle exceptions such as trying to write to a closed file gracefully. **Example Output in `logfile.txt`:** ``` File opened. Message 1 Message 2 Message 3 File closed. ``` **Performance requirements:** - Handle file operations efficiently, ensuring no redundant open/close cycles. - Ensure that resources (files) are properly managed without leaks. ```python import atexit # Initialize global variables _file_obj = None _final_msg = \\"File closed.\\" def open_file(filename): global _file_obj _file_obj = open(filename, \'a\') _file_obj.write(\\"File opened.n\\") return _file_obj def close_file(file, final_message): if file: file.write(final_message + \\"n\\") file.close() # Register the close_file function to be called at program exit atexit.register(close_file, _file_obj, _final_msg) def write_to_file(file, message): if file and not file.closed: file.write(message + \\"n\\") else: print(\\"File is closed, unable to write.\\") # Demonstrate functionality if __name__ == \\"__main__\\": file = open_file(\\"logfile.txt\\") write_to_file(file, \\"Message 1\\") write_to_file(file, \\"Message 2\\") write_to_file(file, \\"Message 3\\") ``` **Note:** Test your code to ensure it behaves as expected, especially verifying that the final message is written and the file is properly closed upon normal termination of the program.","solution":"import atexit # Initialize global variables _file_obj = None _final_msg = \\"File closed.\\" def open_file(filename): global _file_obj _file_obj = open(filename, \'a\') _file_obj.write(\\"File opened.n\\") return _file_obj def close_file(file, final_message): if file and not file.closed: file.write(final_message + \\"n\\") file.close() # Register the close_file function to be called at program exit atexit.register(close_file, _file_obj, _final_msg) def write_to_file(file, message): if file and not file.closed: file.write(message + \\"n\\") else: print(\\"File is closed, unable to write.\\") # Demonstrate functionality if __name__ == \\"__main__\\": file = open_file(\\"logfile.txt\\") write_to_file(file, \\"Message 1\\") write_to_file(file, \\"Message 2\\") write_to_file(file, \\"Message 3\\") # No need to explicitly call close_file as it will be handled by atexit"},{"question":"# Advanced Python Bytecode Analysis with the `dis` Module Problem You are given the following Python function: ```python def fibonacci(n): a, b = 0, 1 fib_sequence = [] for _ in range(n): fib_sequence.append(a) a, b = b, a + b return fib_sequence ``` Your task is to: 1. Disassemble the given function and provide a detailed analysis of the bytecode. 2. Compute the stack effect of each bytecode instruction. 3. Identify any potential optimizations or insights that can be inferred from the bytecode. Instructions 1. **Disassembling the Function**: Use the `dis` module to disassemble the `fibonacci` function. Understand each bytecode instruction and describe its purpose. 2. **Stack Effect Computation**: For each bytecode instruction, compute its stack effect using `dis.stack_effect(opcode, oparg=None)`. 3. **Optimization Analysis**: Based on the disassembled bytecode, suggest any potential optimizations or provide insights into how Python executes this function. Function Signature You should implement the following function: ```python def analyze_fibonacci_bytecode(): import dis import fibonacci # Step 1: Disassemble the function bytecode = dis.Bytecode(fibonacci) # Step 2: Provide detailed analysis analysis = [] for instr in bytecode: opname = instr.opname opcode = instr.opcode arg = instr.arg effect = dis.stack_effect(opcode, arg) analysis.append((instr.offset, opname, arg, effect)) # Step 3: Suggest optimizations/insights optimizations = [] # Your optimization analysis code here return analysis, optimizations ``` Output - **analysis**: A list of tuples with each tuple containing `(offset, operation, argument, stack effect)`. - **optimizations**: A list of strings where each string describes a potential optimization or insight. Example ```python analysis, optimizations = analyze_fibonacci_bytecode() print(\\"Bytecode Analysis:\\") for detail in analysis: print(detail) print(\\"nPotential Optimizations:\\") for opt in optimizations: print(opt) ``` This question will test your ability to: - Use the `dis` module to analyze Python bytecode. - Understand and interpret bytecode instructions. - Compute and analyze stack effects. - Suggest optimizations based on bytecode analysis.","solution":"import dis def fibonacci(n): a, b = 0, 1 fib_sequence = [] for _ in range(n): fib_sequence.append(a) a, b = b, a + b return fib_sequence def analyze_fibonacci_bytecode(): # Step 1: Disassemble the function bytecode = dis.Bytecode(fibonacci) # Step 2: Provide detailed analysis analysis = [] for instr in bytecode: opname = instr.opname opcode = instr.opcode arg = instr.arg effect = dis.stack_effect(opcode, arg) analysis.append((instr.offset, opname, arg, effect)) # Step 3: Suggest optimizations/insights optimizations = [ \\"The function uses a simple iterative approach, which is efficient and clear.\\" \\"Using fewer temporary variables could lower the number of stack operations.\\" \\"Pre-allocating list size could slightly optimize memory allocation.\\" ] return analysis, optimizations"},{"question":"# Anomaly Detection with Multiple Algorithms Your task is to implement a Python function that compares the performance of different outlier detection and novelty detection algorithms on a given dataset. You will use the `LocalOutlierFactor`, `OneClassSVM`, `EllipticEnvelope`, and `IsolationForest` classes from the `sklearn` package. Function Signature ```python def compare_anomaly_detection_algorithms(X_train: np.ndarray, X_test: np.ndarray) -> dict: pass ``` Input - `X_train`: A numpy array of shape `(n_samples_train, n_features)` representing the training data. Assume it has no outliers. - `X_test`: A numpy array of shape `(n_samples_test, n_features)` representing new observations containing potential outliers. Output - A dictionary where the keys are the names of the algorithms (`\'LOF\'`, `\'OneClassSVM\'`, `\'EllipticEnvelope\'`, `\'IsolationForest\'`) and the values are lists of predicted labels for the test data. Inliers should be labeled `1` and outliers `-1`. Requirements 1. Train each of the following anomaly detection algorithms on the training data: - Local Outlier Factor (`neighbors.LocalOutlierFactor`) with `novelty=True`. - One-Class SVM (`svm.OneClassSVM`). - Elliptic Envelope (`covariance.EllipticEnvelope`). - Isolation Forest (`ensemble.IsolationForest`). 2. Predict the labels for the test data using each trained model. 3. Return a dictionary with the algorithm names as keys and the corresponding lists of predictions as values. Example Usage ```python X_train = np.array([[0, 0], [0.1, 0.2], [0.2, 0.1], [0.15, 0.25], [0, 0.1]]) X_test = np.array([[0, 0], [1, 1], [0.2, 0.2], [0.15, 0.3], [10, 10]]) output = compare_anomaly_detection_algorithms(X_train, X_test) print(output) ``` Constraints - Ensure that each model\'s parameters are set to reasonable defaults. - Assume that the test data can contain outliers. - The solution should be efficient and handle moderate to large datasets within reasonable time limits. Notes - You may need to install the scikit-learn library if not already available. Use `pip install scikit-learn`. - The Local Outlier Factor should be used with the `novelty` parameter set to `True` to enable prediction on new data.","solution":"import numpy as np from sklearn.neighbors import LocalOutlierFactor from sklearn.svm import OneClassSVM from sklearn.covariance import EllipticEnvelope from sklearn.ensemble import IsolationForest def compare_anomaly_detection_algorithms(X_train: np.ndarray, X_test: np.ndarray) -> dict: Compares the performance of different outlier detection and novelty detection algorithms on a given dataset. Parameters: X_train (np.ndarray): Training data array of shape (n_samples_train, n_features). X_test (np.ndarray): Testing data array of shape (n_samples_test, n_features). Returns: dict: Dictionary where keys are algorithm names and values are lists of predicted labels for the test data. # Define the algorithms algorithms = { \'LOF\': LocalOutlierFactor(novelty=True), \'OneClassSVM\': OneClassSVM(), \'EllipticEnvelope\': EllipticEnvelope(), \'IsolationForest\': IsolationForest() } predictions = {} # Train each model and predict the test set for name, model in algorithms.items(): # Train the model model.fit(X_train) # Predict the test set predictions[name] = model.predict(X_test).tolist() return predictions"},{"question":"# Question: Implement a Binary Data Processor Context: You are tasked with implementing a function that processes binary data according to a specified data format, modifies certain fields, and returns the updated binary data. The binary data will represent a sequence of records, each with a specific layout defined by a format string. Requirements: 1. Implement the function `process_binary_data(data: bytes, format_string: str, modify_func) -> bytes`. 2. The function should: - Unpack the incoming binary data (`data`) using the given format string (`format_string`). - Apply the `modify_func` to the unpacked data. The `modify_func` is a function that takes a tuple (representing a record) and returns a modified tuple. - Pack the modified records back into bytes using the same format string. - Ensure proper alignment and size as specified by the format string. Input: - `data`: A bytes object containing packed binary data according to the specified format string. - `format_string`: A string describing the layout of the data records. The format string follows the `struct` module conventions. - `modify_func`: A function that takes a tuple (one unpacked record) and returns a modified tuple. Output: - A bytes object containing the updated binary data. Examples: ```python import struct def example_modify_func(record): # Assuming the record is a tuple of (int, float) return (record[0] + 10, record[1] * 2) data = struct.pack(\'if\', 1, 2.0) format_string = \'if\' modified_data = process_binary_data(data, format_string, example_modify_func) unpacked_data = struct.unpack(\'if\', modified_data) print(unpacked_data) # Output should be (11, 4.0) ``` Constraints: - The format string will be in a valid `struct` module format. - The modify function will always return a tuple of the same length and types as the input tuple. Notes: - Make sure to handle the proper alignment and size of the binary data. - Only modify the data as specified by the `modify_func`. Implementation: ```python def process_binary_data(data: bytes, format_string: str, modify_func): import struct # Calculate the size of one record record_size = struct.calcsize(format_string) # Number of records in the data num_records = len(data) // record_size modified_data = bytearray() for i in range(num_records): # Extract each record record_data = data[i * record_size:(i + 1) * record_size] # Unpack the record record = struct.unpack(format_string, record_data) # Modify the record modified_record = modify_func(record) # Pack the modified record and append it to the result modified_data += struct.pack(format_string, *modified_record) return bytes(modified_data) ```","solution":"def process_binary_data(data: bytes, format_string: str, modify_func): import struct # Calculate the size of one record record_size = struct.calcsize(format_string) # Number of records in the data num_records = len(data) // record_size modified_data = bytearray() for i in range(num_records): # Extract each record record_data = data[i * record_size:(i + 1) * record_size] # Unpack the record record = struct.unpack(format_string, record_data) # Modify the record modified_record = modify_func(record) # Pack the modified record and append it to the result modified_data += struct.pack(format_string, *modified_record) return bytes(modified_data)"},{"question":"Problem Statement You are provided with a list of dictionaries representing user data. Each dictionary contains the user\'s ID (`user_id`), their age (`age`), and their score (`score`) on a particular test. Some of these values might be missing and represented by `None`. Your task is to: 1. Create a pandas DataFrame from the provided list of dictionaries. 2. Ensure that the `user_id` and `age` columns use the `Int64` dtype to handle possible missing values by using `pd.NA`. 3. Calculate the mean age and mean score, ignoring the missing values. 4. Fill missing values in the `score` column using the mean score computed in step 3. 5. Create a new column, `age_group`, which categorizes users into \'Young\' (<30 years), \'Middle-aged\' (30-50 years), and \'Senior\' (>50 years). Expected Input and Output Formats **Input:** - A list of dictionaries containing user data. ```python user_data = [ {\'user_id\': 1, \'age\': 25, \'score\': 85}, {\'user_id\': 2, \'age\': None, \'score\': 88}, {\'user_id\': 3, \'age\': 35, \'score\': None}, {\'user_id\': None, \'age\': 45, \'score\': 95}, {\'user_id\': 5, \'age\': None, \'score\': None} ] ``` **Output:** - A pandas DataFrame that includes: 1. `user_id` and `age` columns with `Int64` dtype handling missing values with `pd.NA`. 2. Calculated mean age and mean score (ignoring missing values). 3. `score` column with missing values replaced by the mean score. 4. New `age_group` column categorizing users as \'Young\', \'Middle-aged\', or \'Senior\'. Make sure to handle any edge cases as necessary. Constraints - You must use pandas version 1.1.0 or higher. - Use the `pd.array` and `pd.Series` methods as demonstrated in the documentation. Solution Skeleton ```python import pandas as pd def process_user_data(user_data): # Step 1: Create the DataFrame from user_data df = pd.DataFrame(user_data) # Step 2: Ensure \'user_id\' and \'age\' columns use Int64 dtype df[\'user_id\'] = pd.array(df[\'user_id\'], dtype=\\"Int64\\") df[\'age\'] = pd.array(df[\'age\'], dtype=\\"Int64\\") # Step 3: Calculate mean age and mean score ignoring missing values mean_age = df[\'age\'].mean() mean_score = df[\'score\'].mean() # Step 4: Fill missing values in \'score\' column using the mean score df[\'score\'].fillna(mean_score, inplace=True) # Step 5: Create the \'age_group\' column df[\'age_group\'] = pd.cut(df[\'age\'], bins=[-float(\'inf\'), 30, 50, float(\'inf\')], labels=[\'Young\', \'Middle-aged\', \'Senior\']) return df, mean_age, mean_score # Example usage: user_data = [ {\'user_id\': 1, \'age\': 25, \'score\': 85}, {\'user_id\': 2, \'age\': None, \'score\': 88}, {\'user_id\': 3, \'age\': 35, \'score\': None}, {\'user_id\': None, \'age\': 45, \'score\': 95}, {\'user_id\': 5, \'age\': None, \'score\': None} ] df, mean_age, mean_score = process_user_data(user_data) print(df) print(f\\"Mean Age: {mean_age}, Mean Score: {mean_score}\\") ``` Explanation - **Step 1:** Converts the list of dictionaries into a pandas DataFrame. - **Step 2:** Converts the `user_id` and `age` columns to nullable integer dtype (`Int64`) to handle missing values properly. - **Step 3:** Calculates the mean of `age` and `score` columns, ignoring missing values. - **Step 4:** Fills missing values in the `score` column with the calculated mean score. - **Step 5:** Creates a new column `age_group` to categorize users into different age groups based on their ages.","solution":"import pandas as pd def process_user_data(user_data): # Step 1: Create the DataFrame from user_data df = pd.DataFrame(user_data) # Step 2: Ensure \'user_id\' and \'age\' columns use Int64 dtype df[\'user_id\'] = pd.array(df[\'user_id\'], dtype=\\"Int64\\") df[\'age\'] = pd.array(df[\'age\'], dtype=\\"Int64\\") # Step 3: Calculate mean age and mean score ignoring missing values mean_age = df[\'age\'].mean() mean_score = df[\'score\'].mean() # Step 4: Fill missing values in \'score\' column using the mean score df[\'score\'].fillna(mean_score, inplace=True) # Step 5: Create the \'age_group\' column df[\'age_group\'] = pd.cut(df[\'age\'], bins=[-float(\'inf\'), 30, 50, float(\'inf\')], labels=[\'Young\', \'Middle-aged\', \'Senior\']) return df, mean_age, mean_score # Example usage: user_data = [ {\'user_id\': 1, \'age\': 25, \'score\': 85}, {\'user_id\': 2, \'age\': None, \'score\': 88}, {\'user_id\': 3, \'age\': 35, \'score\': None}, {\'user_id\': None, \'age\': 45, \'score\': 95}, {\'user_id\': 5, \'age\': None, \'score\': None} ] df, mean_age, mean_score = process_user_data(user_data) print(df) print(f\\"Mean Age: {mean_age}, Mean Score: {mean_score}\\")"},{"question":"You are given a sorted list of dictionaries representing a collection of products, each with a unique `product_id` and a `price`. Your task is to write a function that accepts a list of products and a new product. The function should insert the new product into the list while maintaining the order based on the `price`. If two products have the same price, the new product should be inserted after the existing products with the same price. # Function Signature ```python def insert_product(products: list[dict], new_product: dict) -> list[dict]: Inserts a new product into the sorted list of products maintaining the order based on the price. Args: products (list): A list of dictionaries, where each dictionary represents a product with keys \\"product_id\\" (int) and \\"price\\" (float). new_product (dict): A dictionary representing the new product to be inserted with keys \\"product_id\\" (int) and \\"price\\" (float). Returns: list: A new list of dictionaries with the new product inserted in the correct position. ``` # Constraints - The `products` list is always sorted based on the `price` key. - Each product dictionary contains: - `product_id`: an integer which is unique for every product. - `price`: a float representing the cost of the product. - The function should retain the original sorting order by price after inserting the new product. # Example ```python products = [ {\\"product_id\\": 1, \\"price\\": 10.99}, {\\"product_id\\": 2, \\"price\\": 19.95}, {\\"product_id\\": 3, \\"price\\": 19.95} ] new_product = {\\"product_id\\": 4, \\"price\\": 19.95} result = insert_product(products, new_product) # Expected output: # [ # {\\"product_id\\": 1, \\"price\\": 10.99}, # {\\"product_id\\": 2, \\"price\\": 19.95}, # {\\"product_id\\": 3, \\"price\\": 19.95}, # {\\"product_id\\": 4, \\"price\\": 19.95} # ] ``` # Performance Considerations - The function should perform the insertion in O(n) time complexity due to the list insertion process, despite the O(log n) complexity of finding the insertion point. # Additional Notes - Handle edge cases such as inserting into an empty list. - Ensure that the solution remains efficient by leveraging the `bisect` module functions where applicable.","solution":"from bisect import bisect_right def insert_product(products, new_product): Inserts a new product into the sorted list of products maintaining the order based on the price. Args: products (list): A list of dictionaries, where each dictionary represents a product with keys \\"product_id\\" (int) and \\"price\\" (float). new_product (dict): A dictionary representing the new product to be inserted with keys \\"product_id\\" (int) and \\"price\\" (float). Returns: list: A new list of dictionaries with the new product inserted in the correct position. # Find the right position where the new product can be inserted position = bisect_right([product[\'price\'] for product in products], new_product[\'price\']) # Insert the new product at the correct position products.insert(position, new_product) return products"},{"question":"Context You are provided with a dataset named `tips` which contains information about the total bills, tips, and several attributes like the day of the week, sex of the bill payer, etc. You need to create a series of visualizations to explore this dataset using seaborn. Task 1. **Data Preprocessing:** Load the dataset `tips` using the `seaborn.load_dataset()` function. 2. **Basic Plot:** Create a basic scatter plot of `total_bill` versus `tip`. - Use `so.Plot` and `so.Dot` to create this plot. 3. **Enhanced Plot:** Modify the scatter plot to reduce overplotting: - Add a thin white edge to the dots to make them more distinguishable. - Apply jittering to the points to avoid overplotting. 4. **Colored Plot:** Add a dodge by the `sex` of the bill payer, with colors distinguishing different sexes. 5. **Faceted Plot:** Create a faceted scatter plot of `total_bill` versus `day`: - Facet the plots by `sex`. - Apply jittering and dodging as needed. 6. **Advanced Plot:** Combine the dot plot with an error bar representation. - Show error bars for the mean and standard error of `total_bill` for each `day`. - Adjust the scales of the plot appropriately. 7. **Presentation:** Apply any necessary styling and theming to the plots to ensure clarity and professionalism. Constraints - Use only seaborn and basic pandas. - Ensure your code adheres to PEP 8 guidelines. Input Format N/A (the data is loaded within the code using seaborn’s load_dataset function). Expected Output 1. A set of well-labeled and professional-looking plots satisfying the criteria above, presented within a Jupyter notebook. Feel free to include annotations, titles, and labels to make your plots informative and visually appealing.","solution":"import seaborn as sns import seaborn.objects as so import pandas as pd import numpy as np import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Basic scatter plot of total_bill vs tip def basic_scatter_plot(tips): plt.figure(figsize=(10, 6)) sns.scatterplot(x=\'total_bill\', y=\'tip\', data=tips) plt.title(\'Basic Scatter Plot of Total Bill vs Tip\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show() # Enhanced scatter plot with jittering and white edge def enhanced_scatter_plot(tips): plt.figure(figsize=(10, 6)) sns.scatterplot(x=\'total_bill\', y=\'tip\', data=tips, edgecolor=\'white\', linewidth=0.5) plt.title(\'Enhanced Scatter Plot with Jittering and White Edge\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show() # Scatter plot with dodge by sex def colored_scatter_plot(tips): plt.figure(figsize=(10, 6)) sns.scatterplot(x=\'total_bill\', y=\'tip\', hue=\'sex\', data=tips, edgecolor=\'white\', linewidth=0.5, alpha=0.7) plt.title(\'Colored Scatter Plot with Dodge by Sex\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show() # Faceted scatter plot by sex with jittering and dodge def faceted_scatter_plot(tips): g = sns.FacetGrid(tips, col=\'sex\', height=6, aspect=1) g.map_dataframe(sns.stripplot, x=\'total_bill\', y=\'day\', hue=\'sex\', dodge=True, jitter=True) g.add_legend() plt.show() # Advanced plot with dot plot and error bars def advanced_plot(tips): plt.figure(figsize=(10, 6)) sns.pointplot(x=\'day\', y=\'total_bill\', data=tips, capsize=.2, join=False) sns.stripplot(x=\'day\', y=\'total_bill\', data=tips, color=\'black\', alpha=0.5, jitter=True) plt.title(\'Dot Plot with Error Bars\') plt.xlabel(\'Day\') plt.ylabel(\'Total Bill\') plt.show() # Function to run all plots def run_all_plots(): basic_scatter_plot(tips) enhanced_scatter_plot(tips) colored_scatter_plot(tips) faceted_scatter_plot(tips) advanced_plot(tips) if __name__ == \\"__main__\\": run_all_plots()"},{"question":"# Coding Challenge: Nearest Neighbor Classification and Performance Evaluation **Objective:** Your task is to implement a nearest neighbor classifier using scikit-learn and evaluate its performance on a dataset. **Description:** 1. Load the Iris dataset provided by scikit-learn. 2. Split the dataset into training and testing sets. 3. Implement a `KNeighborsClassifier` with the following specifications: - `n_neighbors` parameter should be determined based on the training set size using the rule of thumb: `k = sqrt(n)`, where `n` is the number of samples in the training set. - Use the Euclidean distance metric. 4. Train the model on the training set. 5. Evaluate the model on the testing set and report the classification accuracy. 6. Additionally, visualize the decision boundaries for two selected features of the dataset. **Input:** - None. You will load the Iris dataset directly within your code. **Output:** - Print: - The number of neighbors used. - Classification accuracy on the testing set. - Plot: - Decision boundaries for two selected features (e.g., sepal length and sepal width). **Constraints:** - Use the `KNeighborsClassifier` from `sklearn.neighbors`. - The selected features for visualization should be feature indices 0 (sepal length) and 1 (sepal width). **Example:** ```python from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier import numpy as np import matplotlib.pyplot as plt import matplotlib.cm as cm # Step 1: Load the Iris dataset iris = datasets.load_iris() X = iris.data[:, :2] # only take the first two features for visualization y = iris.target # Step 2: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 3: Implement KNeighborsClassifier n_neighbors = int(np.sqrt(len(X_train))) classifier = KNeighborsClassifier(n_neighbors=n_neighbors, metric=\'euclidean\') # Step 4: Train the model classifier.fit(X_train, y_train) # Step 5: Evaluate the model accuracy = classifier.score(X_test, y_test) print(f\'Number of neighbors: {n_neighbors}\') print(f\'Classification accuracy on the testing set: {accuracy:.2f}\') # Step 6: Visualize the decision boundaries h = .02 # step size in the mesh x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.figure() plt.contourf(xx, yy, Z, cmap=cm.Paired) plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\'k\', s=20, cmap=cm.Paired) plt.xlabel(\'Sepal length\') plt.ylabel(\'Sepal width\') plt.title(\'K-NN decision boundaries\') plt.show() ```","solution":"from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier import numpy as np import matplotlib.pyplot as plt import matplotlib.cm as cm def load_and_split_iris(): Load the Iris dataset and split it into training and testing sets. Returns X_train, X_test, y_train, y_test. iris = datasets.load_iris() X = iris.data[:, :2] # only take the first two features for visualization y = iris.target return train_test_split(X, y, test_size=0.3, random_state=42) def calculate_k(X_train): Calculate the number of neighbors using the rule of thumb: k = sqrt(n). return int(np.sqrt(len(X_train))) def train_knn(X_train, y_train, k): Train a KNeighborsClassifier with k neighbors. Returns the trained classifier. classifier = KNeighborsClassifier(n_neighbors=k, metric=\'euclidean\') classifier.fit(X_train, y_train) return classifier def evaluate_model(classifier, X_test, y_test): Evaluate the classifier on the test set and return the accuracy. return classifier.score(X_test, y_test) def visualize_decision_boundaries(classifier, X, y): Visualize the decision boundaries for the KNeighborsClassifier. h = .02 # step size in the mesh x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()]) # Put the result into a color plot Z = Z.reshape(xx.shape) plt.figure() plt.contourf(xx, yy, Z, cmap=cm.Paired) plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\'k\', s=20, cmap=cm.Paired) plt.xlabel(\'Sepal length\') plt.ylabel(\'Sepal width\') plt.title(\'K-NN decision boundaries\') plt.show() def nearest_neighbor_classification(): X_train, X_test, y_train, y_test = load_and_split_iris() k = calculate_k(X_train) classifier = train_knn(X_train, y_train, k) accuracy = evaluate_model(classifier, X_test, y_test) print(f\'Number of neighbors: {k}\') print(f\'Classification accuracy on the testing set: {accuracy:.2f}\') visualize_decision_boundaries(classifier, np.vstack([X_train, X_test]), np.hstack([y_train, y_test])) nearest_neighbor_classification()"},{"question":"# Question: Visualizing Patient Data using Seaborn\'s FacetGrid You are given a dataset containing information about patients admitted to a hospital. The dataset includes the following columns: - `age`: Age of the patient. - `gender`: Gender of the patient (Male/Female). - `department`: Department where the patient was admitted (Cardiology, Neurology, etc.). - `days_admitted`: Number of days the patient was admitted to the hospital. - `visits`: Number of follow-up visits after discharge. Using Seaborn\'s `FacetGrid`, create a grid of histograms displaying the distribution of `days_admitted`, faceted by `department` for rows and `gender` for columns. Include the following customizations: 1. **Adjust plot size**: Set the height of each facet to 3 and the aspect ratio to 1.5. 2. **Add hue**: Differentiate the histograms by the `visits` column, using different colors. 3. **Reference line**: Add a vertical reference line at the median of `days_admitted` for each facet. 4. **Annotations**: Annotate each facet with the median number of `days_admitted` in the facet, positioned at the top right corner. Given dataset example: ```python data = { \'age\': [23, 45, 56, 23, 38, 59, 33, 48, 22, 46], \'gender\': [\'Male\',\'Female\',\'Female\',\'Male\',\'Male\',\'Female\',\'Female\',\'Male\',\'Female\',\'Male\'], \'department\': [\'Cardiology\',\'Neurology\',\'Cardiology\',\'Orthopedics\',\'Neurology\',\'Cardiology\',\'Orthopedics\',\'Neurology\',\'Cardiology\',\'Orthopedics\'], \'days_admitted\': [5, 10, 8, 6, 7, 12, 9, 6, 4, 11], \'visits\': [1, 3, 2, 2, 3, 1, 3, 2, 1, 4] } ``` # Constraints and Requirements: - **Input**: A Pandas DataFrame containing the columns `age`, `gender`, `department`, `days_admitted`, and `visits`. - **Output**: Display a FacetGrid plot with the specified customizations. - Use Seaborn for visualizations and ensure the plot is visually clear and well-labeled. # Implementation Implement a function `visualize_patient_data(data: pd.DataFrame) -> None` that generates and displays the described FacetGrid plot. Ensure you follow the customization requirements strictly. Example: ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Data example data = { \'age\': [23, 45, 56, 23, 38, 59, 33, 48, 22, 46], \'gender\': [\'Male\',\'Female\',\'Female\',\'Male\',\'Male\',\'Female\',\'Female\',\'Male\',\'Female\',\'Male\'], \'department\': [\'Cardiology\',\'Neurology\',\'Cardiology\',\'Orthopedics\',\'Neurology\',\'Cardiology\',\'Orthopedics\',\'Neurology\',\'Cardiology\',\'Orthopedics\'], \'days_admitted\': [5, 10, 8, 6, 7, 12, 9, 6, 4, 11], \'visits\': [1, 3, 2, 2, 3, 1, 3, 2, 1, 4] } df = pd.DataFrame(data) def visualize_patient_data(df): sns.set_theme(style=\\"ticks\\") # Initialize FacetGrid g = sns.FacetGrid(df, col=\'gender\', row=\'department\', hue=\'visits\', height=3, aspect=1.5) # Map histplot g.map(sns.histplot, \'days_admitted\', kde=False) # Add reference lines and annotations def annotate_median(data, **kws): median = data[\'days_admitted\'].median() ax = plt.gca() ax.axvline(median, ls=\'--\', c=\'red\') ax.text(median + 0.5, ax.get_ylim()[1]*0.9, f\\"Median: {median:.1f}\\", color=\'red\') g.map_dataframe(annotate_median) g.add_legend() plt.show() visualize_patient_data(df) ``` # Submission Submit your code along with a screenshot of the displayed plot. Ensure the FacetGrid is configured as required and the plot annotations are visible.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_patient_data(df): sns.set_theme(style=\\"ticks\\") # Initialize FacetGrid g = sns.FacetGrid(df, col=\'gender\', row=\'department\', hue=\'visits\', height=3, aspect=1.5) # Map the histplot to the grid g.map(sns.histplot, \'days_admitted\', kde=False) # Add reference lines and median annotations to each plot def annotate_median(data, **kws): median = data[\'days_admitted\'].median() ax = plt.gca() ax.axvline(median, ls=\'--\', c=\'red\') ylim = ax.get_ylim() ax.text(median + (0.05 * (ax.get_xlim()[1] - ax.get_xlim()[0])), ylim[1] * 0.95, f\'Median: {median:.1f}\', color=\'red\', ha=\'left\', va=\'top\') g.map_dataframe(annotate_median) # Add legends to the plots g.add_legend() # Adjust the labels and titles g.set_axis_labels(\\"Days Admitted\\", \\"Count\\") g.set_titles(row_template=\'{row_name}\', col_template=\'{col_name}\') plt.show()"},{"question":"**Internationalize and Localize a Python Application** You are required to write a Python function to internationalize and localize a simple application that displays greeting messages in different languages based on user input. The application should support at least three languages: English, French, and German. You should format the greeting using the user\'s name. # Function Signature ```python def setup_translations(locale_dir: str) -> None: pass ``` # Input: - `locale_dir` (str): The path to the directory containing the translation files. # Output: - The function does not return any value but should setup translation functionalities based on the provided locale directory. # Constraints: - The translation files should be in the `.mo` format and be correctly placed in subdirectories named after the language codes (`en`, `fr`, `de`). - You may assume the directory structure is as follows: ``` locale/ en/LC_MESSAGES/messages.mo fr/LC_MESSAGES/messages.mo de/LC_MESSAGES/messages.mo ``` # Requirements: 1. Install the `_()` function in the built-in namespace so that it can be used globally within the application. 2. Implement three greeting functions that use the installed `_()` function: - `greet_in_english(name: str) -> str` - `greet_in_french(name: str) -> str` - `greet_in_german(name: str) -> str` 3. Each greeting function should return a translated greeting message formatted with the given `name`. # Examples: ```python # Assuming the .mo files contain translations for \\"Hello, {name}!\\" setup_translations(\'/path/to/locale\') print(greet_in_english(\\"Alice\\")) # \\"Hello, Alice!\\" print(greet_in_french(\\"Bob\\")) # \\"Bonjour, Bob!\\" print(greet_in_german(\\"Charlie\\")) # \\"Hallo, Charlie!\\" ``` # Notes: - You need to handle potential exceptions gracefully, such as missing translation files. - Utilize the `gettext` module\'s functionalities as demonstrated in the provided documentation to achieve the required functionality. - Consider adding comments and documentation within your code to explain the implementation steps clearly. # Additional Information If required, create dummy translation catalogs following the structure and content necessary to test your application locally.","solution":"import gettext import os def setup_translations(locale_dir: str) -> None: Set up the translation functionalities based on the provided locale directory. Parameters: locale_dir (str): The path to the directory containing the translation files. locales = [\'en\', \'fr\', \'de\'] translations = {} for locale in locales: try: translation = gettext.translation(\'messages\', localedir=locale_dir, languages=[locale]) translations[locale] = translation except FileNotFoundError: translations[locale] = gettext.NullTranslations() global _ def _(msg): return translations[get_current_locale()].gettext(msg) def get_current_locale(): # For demonstration purposes, we\'ll assume the locale is always English. # This function should be implemented to return the actual locale based on the user\'s preference. return \'en\' def greet_in_english(name: str) -> str: return _(\\"Hello, {}!\\").format(name) def greet_in_french(name: str) -> str: return _(\\"Bonjour, {}!\\").format(name) def greet_in_german(name: str) -> str: return _(\\"Hallo, {}!\\").format(name)"},{"question":"**Coding Assessment Question:** As part of a software deployment pipeline, you need to generate a report that verifies the configuration of a Python environment. You are required to implement a function that fetches specific configuration details using the `sysconfig` module and organizes this information in a structured manner. # Function Signature: ```python import sysconfig from typing import Dict, Union, List def generate_python_env_report() -> Dict[str, Union[str, Dict[str, Union[str, int, List[str]]]]]: pass ``` # Requirements: Your function `generate_python_env_report` should return a dictionary with the following structure: * **\\"platform\\"**: A string representing the current platform, fetched using `sysconfig.get_platform()`. * **\\"python_version\\"**: The Python version in \\"MAJOR.MINOR\\" format, fetched using `sysconfig.get_python_version()`. * **\\"default_scheme\\"**: The default installation scheme name for the current platform. * **\\"config_variables\\"**: A dictionary containing key configuration variables: - \\"AR\\": The archiver command. - \\"ARFLAGS\\": The flags for the archiver command. - \\"CC\\": The C compiler command. * **\\"paths\\"**: A dictionary with paths corresponding to the default installation scheme: - \\"stdlib\\": Directory for standard library files. - \\"purelib\\": Directory for pure Python library files. - \\"scripts\\": Directory for script files. # Example Output: ```python { \\"platform\\": \\"win-amd64\\", \\"python_version\\": \\"3.10\\", \\"default_scheme\\": \\"nt\\", \\"config_variables\\": { \\"AR\\": \\"ar\\", \\"ARFLAGS\\": \\"rc\\", \\"CC\\": \\"gcc\\" }, \\"paths\\": { \\"stdlib\\": \\"C:Python310Lib\\", \\"purelib\\": \\"C:Python310Libsite-packages\\", \\"scripts\\": \\"C:Python310Scripts\\" } } ``` # Notes: 1. Use the `sysconfig.get_config_var` method to fetch individual configuration variables. 2. Use `sysconfig.get_path` to obtain paths from the default installation scheme. 3. If any configuration variable or path is not available, set its value to `\\"Unavailable\\"`. # Constraints: - Ensure your function handles different platforms (Windows, Linux, macOS). - The function should execute efficiently and ensure readability and maintainability of the code. Test your implementation on various Python installations and platforms to ensure its correctness and robustness.","solution":"import sysconfig from typing import Dict, Union, List def generate_python_env_report() -> Dict[str, Union[str, Dict[str, Union[str, int, List[str]]]]]: report = { \\"platform\\": sysconfig.get_platform(), \\"python_version\\": sysconfig.get_python_version(), \\"default_scheme\\": sysconfig.get_default_scheme(), \\"config_variables\\": { \\"AR\\": sysconfig.get_config_var(\\"AR\\") or \\"Unavailable\\", \\"ARFLAGS\\": sysconfig.get_config_var(\\"ARFLAGS\\") or \\"Unavailable\\", \\"CC\\": sysconfig.get_config_var(\\"CC\\") or \\"Unavailable\\" }, \\"paths\\": { \\"stdlib\\": sysconfig.get_path(\\"stdlib\\") or \\"Unavailable\\", \\"purelib\\": sysconfig.get_path(\\"purelib\\") or \\"Unavailable\\", \\"scripts\\": sysconfig.get_path(\\"scripts\\") or \\"Unavailable\\" } } return report"},{"question":"**Objective:** Write a Python function that processes a list of string mathematical expressions and returns the evaluated results. This will assess your understanding of arithmetic operations, strings, and lists in Python. **Function Signature:** ```python def evaluate_expressions(expressions: list) -> list: # Your code here pass ``` **Input:** - `expressions` (list of strings): A list where each element is a string representing a mathematical expression. These expressions will include integers and the operators `+`, `-`, `*`, `/`, `//`, `%`, and `**`. **Output:** - (list of floats/integers): A list of evaluated results corresponding to each expression in the input list. **Constraints:** - The input list will contain at least one expression and at most 100 expressions. - Each expression will be a valid mathematical expression that can be evaluated. - Ensure that the function handles both integer and floating-point division accurately. **Example:** ```python assert evaluate_expressions([\\"2 + 2\\", \\"50 - 5*6\\", \\"(50 - 5*6) / 4\\", \\"7 // 3\\", \\"5 ** 2\\"]) == [4, 20, 5.0, 2, 25] assert evaluate_expressions([\\"10 % 3\\", \\"8 / 5\\", \\"2 ** 7\\", \\"4 * 3.75 - 1\\"]) == [1, 1.6, 128, 14.0] ``` **Keywords:** - `eval()` - Arithmetic operations - Strings - Lists **Constraints and Performance Requirements:** - Do not use the `eval()` function due to security reasons. - Handle the arithmetic operations manually within your function. **Hints:** - Consider using the `ast` module to safely evaluate expressions. - Implement a basic parser for the mathematical expressions if you prefer not to use any external libraries. **To Note:** - This question will test your ability to parse strings, perform arithmetic operations, and manage lists in Python, demonstrating both fundamental and advanced competencies with the language.","solution":"import ast import operator def evaluate_expressions(expressions: list) -> list: def safe_eval(expr): node = ast.parse(expr, mode=\'eval\') return _eval(node.body) def _eval(node): if isinstance(node, ast.BinOp): left = _eval(node.left) right = _eval(node.right) if isinstance(node.op, ast.Add): return operator.add(left, right) elif isinstance(node.op, ast.Sub): return operator.sub(left, right) elif isinstance(node.op, ast.Mult): return operator.mul(left, right) elif isinstance(node.op, ast.Div): return operator.truediv(left, right) elif isinstance(node.op, ast.FloorDiv): return operator.floordiv(left, right) elif isinstance(node.op, ast.Mod): return operator.mod(left, right) elif isinstance(node.op, ast.Pow): return operator.pow(left, right) elif isinstance(node, ast.Num): return node.n elif isinstance(node, ast.UnaryOp): operand = _eval(node.operand) if isinstance(node.op, ast.UAdd): return +operand elif isinstance(node.op, ast.USub): return -operand raise TypeError(node) return [safe_eval(expr) for expr in expressions]"},{"question":"# Custom Metric and Model Evaluation In this task, you\'ll be required to write a Python function that evaluates a model using a custom metric defined through the `make_scorer` factory in Scikit-learn. You will then apply this custom metric in a cross-validation setup. Problem Statement 1. **Custom Metric**: Implement a custom metric function called `custom_mse` that computes the Mean Squared Error (MSE) but scales it by a given factor. This customized metric will be used to penalize or reward the Mean Squared Error based on certain conditions. 2. **Cross-Validation with Custom Metric**: Use this custom metric with the `make_scorer` method of Scikit-learn to evaluate a classifier. Utilize cross-validation and report the scores. # Task 1. **Define the Custom Metric Function**: ```python def custom_mse(y_true, y_pred, scale_factor): pass ``` - The function should take the true labels (`y_true`), the predicted labels (`y_pred`), and a scale factor (`scale_factor`) as inputs. - It should return the Mean Squared Error scaled by the scale factor. 2. **Create a Custom Scorer Using `make_scorer`**: ```python from sklearn.metrics import make_scorer def create_custom_scorer(scale_factor): pass ``` - The function should create and return a scorer object using `make_scorer`. 3. **Evaluate the Model**: - Using the dataset provided by `sklearn.datasets.load_breast_cancer`, perform a cross-validation with the custom metric created. - Use a Support Vector Classifier (SVC) with an \'rbf\' kernel and a fixed `C` parameter of 1.0. - Report the cross-validation results using the custom metric. # Function Signatures ```python def custom_mse(y_true, y_pred, scale_factor: float) -> float: Custom Mean Squared Error metric scaled by a factor. Args: y_true (array-like): True labels. y_pred (array-like): Predicted labels. scale_factor (float): Factor by which to scale the MSE. Returns: float: Scaled Mean Squared Error. pass def create_custom_scorer(scale_factor: float): Creates a custom scorer using the `custom_mse` function. Args: scale_factor (float): Factor by which to scale the MSE. Returns: scorer: Custom scorer object. pass def evaluate_model_with_custom_metric(scale_factor: float) -> None: Evaluates the SVC model using cross-validation and a custom metric. Args: scale_factor (float): Factor by which to scale the MSE. Returns: None pass ``` # Example Usage ```python # Define the custom metric def custom_mse(y_true, y_pred, scale_factor): mse = ((y_true - y_pred) ** 2).mean() return scale_factor * mse # Define function to create a custom scorer def create_custom_scorer(scale_factor): from sklearn.metrics import make_scorer return make_scorer(custom_mse, scale_factor=scale_factor) # Cross-validation with a customized scorer from sklearn.datasets import load_breast_cancer from sklearn.model_selection import cross_val_score from sklearn.svm import SVC def evaluate_model_with_custom_metric(scale_factor): X, y = load_breast_cancer(return_X_y=True) model = SVC(kernel=\'rbf\', C=1.0) custom_scorer = create_custom_scorer(scale_factor) scores = cross_val_score(model, X, y, cv=5, scoring=custom_scorer) print(f\\"Cross-Validation Scores with scale factor {scale_factor}: {scores}\\") # Example run evaluate_model_with_custom_metric(scale_factor=2.0) ``` # Constraints - Use `Scikit-learn` version 0.24 or higher. - Ensure that your custom metric function properly handles edge cases like empty arrays. - The Scale factor should be a positive float.","solution":"import numpy as np from sklearn.metrics import mean_squared_error def custom_mse(y_true, y_pred, scale_factor: float) -> float: Custom Mean Squared Error metric scaled by a factor. Args: y_true (array-like): True labels. y_pred (array-like): Predicted labels. scale_factor (float): Factor by which to scale the MSE. Returns: float: Scaled Mean Squared Error. mse = mean_squared_error(y_true, y_pred) return scale_factor * mse def create_custom_scorer(scale_factor: float): Creates a custom scorer using the `custom_mse` function. Args: scale_factor (float): Factor by which to scale the MSE. Returns: scorer: Custom scorer object. from sklearn.metrics import make_scorer return make_scorer(custom_mse, scale_factor=scale_factor) def evaluate_model_with_custom_metric(scale_factor: float) -> None: Evaluates the SVC model using cross-validation and a custom metric. Args: scale_factor (float): Factor by which to scale the MSE. Returns: None from sklearn.datasets import load_breast_cancer from sklearn.model_selection import cross_val_score from sklearn.svm import SVC X, y = load_breast_cancer(return_X_y=True) model = SVC(kernel=\'rbf\', C=1.0) custom_scorer = create_custom_scorer(scale_factor) scores = cross_val_score(model, X, y, cv=5, scoring=custom_scorer) print(f\\"Cross-Validation Scores with scale factor {scale_factor}: {scores}\\")"},{"question":"**Seaborn Plot Customization Question:** Seaborn\'s `plotting_context()` function allows you to set the context for the plots, making them suitable for various contexts such as \\"paper\\", \\"notebook\\", \\"talk\\", and \\"poster\\". The context alters the scaling factor that affects the font size, line thickness, etc. Given below is a dataset of average monthly temperatures in four cities over a year: ```python data = { \\"Month\\": [\\"Jan\\", \\"Feb\\", \\"Mar\\", \\"Apr\\", \\"May\\", \\"Jun\\", \\"Jul\\", \\"Aug\\", \\"Sep\\", \\"Oct\\", \\"Nov\\", \\"Dec\\"], \\"CityA\\": [-1, 0, 5, 9, 15, 20, 24, 23, 18, 12, 7, 2], \\"CityB\\": [5, 7, 10, 14, 19, 23, 27, 26, 22, 16, 10, 6], \\"CityC\\": [0, 2, 6, 11, 17, 21, 26, 25, 20, 14, 8, 3], \\"CityD\\": [10, 12, 15, 18, 22, 26, 29, 28, 24, 19, 14, 11] } ``` **Task:** 1. Write a function `plot_temperature(data, context)` that: 1. Creates a line plot for the temperature data of the four cities sharing the same x-axis (Month) in the specified context. 2. Sets the plot title to \\"Monthly Average Temperature\\". 3. Ensures that the legends and axis labels are appropriately set. 4. Saves the plot as \\"temperature_plot.png\\". 2. Implement different contexts (\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\") to see how the plot appearances change. **Function Signature:** ```python def plot_temperature(data: dict, context: str) -> None: # Your code here ``` **Constraints:** - Use the seaborn package for plotting. - Ensure the context provided is one of [\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"]. - Save the plot file with the name \\"temperature_plot.png\\". **Example Usage:** ```python data = { \\"Month\\": [\\"Jan\\", \\"Feb\\", \\"Mar\\", \\"Apr\\", \\"May\\", \\"Jun\\", \\"Jul\\", \\"Aug\\", \\"Sep\\", \\"Oct\\", \\"Nov\\", \\"Dec\\"], \\"CityA\\": [-1, 0, 5, 9, 15, 20, 24, 23, 18, 12, 7, 2], \\"CityB\\": [5, 7, 10, 14, 19, 23, 27, 26, 22, 16, 10, 6], \\"CityC\\": [0, 2, 6, 11, 17, 21, 26, 25, 20, 14, 8, 3], \\"CityD\\": [10, 12, 15, 18, 22, 26, 29, 28, 24, 19, 14, 11] } plot_temperature(data, \\"talk\\") ``` This function when `context` is set to \\"talk\\" should create the plot and save it as \\"temperature_plot.png\\".","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_temperature(data: dict, context: str) -> None: Creates a line plot for the given temperature data in the specified context and saves it as \'temperature_plot.png\'. Parameters: - data (dict): Dictionary containing months and temperature data for four cities. - context (str): Context in which to display the plot. Should be one of [\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"]. # Validate context input if context not in [\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"]: raise ValueError(\\"Context must be one of [\'paper\', \'notebook\', \'talk\', \'poster\']\\") # Set plotting context sns.set_context(context) # Convert data to DataFrame for easier plotting with seaborn df = pd.DataFrame(data) # Create line plot plt.figure(figsize=(10, 6)) sns.lineplot(x=\\"Month\\", y=\\"value\\", hue=\\"variable\\", data=pd.melt(df, [\\"Month\\"])) # Set title, labels, and legend plt.title(\\"Monthly Average Temperature\\") plt.xlabel(\\"Month\\") plt.ylabel(\\"Temperature (°C)\\") plt.legend(title=\'City\') # Save plot to file plt.savefig(\\"temperature_plot.png\\") plt.close()"},{"question":"**Question: Advanced Data Visualization with Seaborn** You are given a dataset containing information about customers in a shopping mall. The dataset includes the following columns: - `CustomerID`: a unique identifier for each customer. - `Age`: age of the customer. - `AnnualIncome`: annual income of the customer in thousand dollars. - `SpendingScore`: a score assigned by the shopping mall based on customer behavior and spending. Perform the following tasks to visualize the relationships in the dataset using seaborn: 1. **Scatter Plot**: - Create a scatter plot to visualize the relationship between `AnnualIncome` and `SpendingScore`. - Use color (hue) to distinguish between different age groups (you can create age groups such as 18-25, 26-35, etc.). - Customize the palette to make the plot visually appealing. 2. **Line Plot**: - Create a line plot to examine changes in `SpendingScore` as a function of `Age`. - Add a 95% confidence interval to the line plot. - Use style to differentiate between different levels of `AnnualIncome` (for example, high income vs low income). 3. **Faceted Scatter Plot**: - Create a faceted scatter plot to show the relationship between `AnnualIncome` and `SpendingScore`, faceted by age groups. - Use different columns for different age groups, and rows for different ranges of `SpendingScore`. **Input Formats:** - A CSV file named `customers.csv` with columns `CustomerID`, `Age`, `AnnualIncome`, and `SpendingScore`. **Output Formats:** - Three saved plots: `scatter_plot.png`, `line_plot.png`, and `facet_scatter_plot.png`. **Constraints:** - Ensure your code is clean and well-commented. - Use `seaborn` for all visualizations. - Save the plots as specified. ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the dataset df = pd.read_csv(\'customers.csv\') # Create age groups for hue age_bins = [18, 25, 35, 45, 55, 65, 100] age_labels = [\'18-25\', \'26-35\', \'36-45\', \'46-55\', \'56-65\', \'65+\'] df[\'AgeGroup\'] = pd.cut(df[\'Age\'], bins=age_bins, labels=age_labels, right=False) # Task 1: Scatter Plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot(data=df, x=\'AnnualIncome\', y=\'SpendingScore\', hue=\'AgeGroup\', palette=\\"viridis\\") scatter_plot.figure.savefig(\\"scatter_plot.png\\") # Task 2: Line Plot plt.figure(figsize=(10, 6)) line_plot = sns.lineplot(data=df, x=\'Age\', y=\'SpendingScore\', hue=\'AnnualIncome\', style=\'AnnualIncome\', ci=95) line_plot.figure.savefig(\\"line_plot.png\\") # Task 3: Faceted Scatter Plot facet_scatter_plot = sns.relplot(data=df, x=\'AnnualIncome\', y=\'SpendingScore\', hue=\'AgeGroup\', col=\'AgeGroup\', row=\'SpendingScore\', palette=\\"coolwarm\\") facet_scatter_plot.savefig(\\"facet_scatter_plot.png\\") plt.show() ```","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_customers(file_path): Generate visualizations from customer data in the provided CSV file. Parameters: - file_path: path to the CSV file Saves three plots: - \'scatter_plot.png\' - \'line_plot.png\' - \'facet_scatter_plot.png\' # Load the dataset df = pd.read_csv(file_path) # Create age groups for hue age_bins = [18, 25, 35, 45, 55, 65, 100] age_labels = [\'18-25\', \'26-35\', \'36-45\', \'46-55\', \'56-65\', \'65+\'] df[\'AgeGroup\'] = pd.cut(df[\'Age\'], bins=age_bins, labels=age_labels, right=False) # Task 1: Scatter Plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot(data=df, x=\'AnnualIncome\', y=\'SpendingScore\', hue=\'AgeGroup\', palette=\\"viridis\\") scatter_plot.figure.savefig(\\"scatter_plot.png\\") plt.clf() # Task 2: Line Plot plt.figure(figsize=(10, 6)) line_plot = sns.lineplot(data=df, x=\'Age\', y=\'SpendingScore\', hue=\'AnnualIncome\', style=\'AnnualIncome\', ci=95) line_plot.figure.savefig(\\"line_plot.png\\") plt.clf() # Task 3: Faceted Scatter Plot facet_scatter_plot = sns.relplot(data=df, x=\'AnnualIncome\', y=\'SpendingScore\', hue=\'AgeGroup\', col=\'AgeGroup\', palette=\\"coolwarm\\") facet_scatter_plot.savefig(\\"facet_scatter_plot.png\\") plt.clf()"},{"question":"# Python Coding Assessment Question Task: You are required to implement a function `find_single_line_comments_in_file(filename: str) -> List[str]`. This function takes the path to a Python source file as its input and returns a list of all the single-line comments (`# comments`) found in the code. Details: 1. **Input:** - `filename` (str): The path to the Python source file. 2. **Output:** - A list of strings, where each string is a single-line comment found in the file. 3. **Constraints:** - The function should ignore multi-line comments (docstrings). - The function should preserve the order of comments as they appear in the source file. - Assume the file is always syntactically valid Python code. 4. **Implementation Requirements:** - Utilize the `tokenize` module to tokenize the source code. - Extract and return comments that are prefixed with a `#`. Example: Suppose the content of `example.py` is as follows: ```python def add(a, b): # This function adds two numbers return a + b # Perform addition ``` Calling `find_single_line_comments_in_file(\'example.py\')` should return: ```python [ \'# This function adds two numbers\', \'# Perform addition\' ] ``` **Note**: - The comments should be returned in the list exactly as they appear in the code, including the `#` prefix. Function Signature: ```python from typing import List def find_single_line_comments_in_file(filename: str) -> List[str]: pass ``` # Hints: - You might find the `token` constants and the `tokenize.generate_tokens` function useful. - Remember to handle the encoded file correctly using the `tokenize.open` method.","solution":"from typing import List import tokenize def find_single_line_comments_in_file(filename: str) -> List[str]: comments = [] with tokenize.open(filename) as f: tokens = tokenize.generate_tokens(f.readline) for token_type, token_string, _, _, _ in tokens: if token_type == tokenize.COMMENT: comments.append(token_string) return comments"},{"question":"# Question: Working with `torch.Size` In this exercise, you are required to implement a function `get_tensor_shape_info` that takes a PyTorch tensor as input and returns a dictionary containing the following information about the tensor: 1. Number of dimensions of the tensor. 2. Size of each dimension. 3. Total number of elements in the tensor. 4. Whether the tensor is a square matrix (if it is a 2D tensor). Function Signature ```python def get_tensor_shape_info(tensor: torch.Tensor) -> dict: pass ``` Input - `tensor`: A PyTorch tensor of arbitrary dimensions. Output - A dictionary with the following keys: - `\'num_dimensions\'`: An integer representing the number of dimensions in the tensor. - `\'size_of_dimensions\'`: A list of integers representing the size of each dimension. - `\'total_elements\'`: An integer representing the total number of elements in the tensor. - `\'is_square_matrix\'`: A boolean value that is `True` if the tensor is a 2D square matrix, otherwise `False`. Example ```python import torch tensor = torch.ones(3, 3) result = get_tensor_shape_info(tensor) print(result) # Output: {\'num_dimensions\': 2, \'size_of_dimensions\': [3, 3], \'total_elements\': 9, \'is_square_matrix\': True} tensor = torch.ones(2, 3, 4) result = get_tensor_shape_info(tensor) print(result) # Output: {\'num_dimensions\': 3, \'size_of_dimensions\': [2, 3, 4], \'total_elements\': 24, \'is_square_matrix\': False} ``` Constraints - You may assume that the tensor\'s dimensions and sizes will be non-negative integers. - Your solution should handle tensors of varying dimensions efficiently.","solution":"import torch def get_tensor_shape_info(tensor: torch.Tensor) -> dict: Returns a dictionary containing information about the given PyTorch tensor\'s shape. :param tensor: A PyTorch tensor of arbitrary dimensions. :return: A dictionary containing: - \'num_dimensions\': The number of dimensions of the tensor. - \'size_of_dimensions\': A list of the size of each dimension. - \'total_elements\': The total number of elements in the tensor. - \'is_square_matrix\': A boolean indicating if the tensor is a 2D square matrix. num_dimensions = tensor.dim() size_of_dimensions = list(tensor.size()) total_elements = tensor.numel() is_square_matrix = False if num_dimensions == 2 and size_of_dimensions[0] == size_of_dimensions[1]: is_square_matrix = True return { \'num_dimensions\': num_dimensions, \'size_of_dimensions\': size_of_dimensions, \'total_elements\': total_elements, \'is_square_matrix\': is_square_matrix }"},{"question":"You are tasked with creating a utility to monitor specific lines across multiple files in a directory. This utility must report the content of these specific lines periodically while maintaining low latency and efficiency. # Requirements: 1. You are given the path to a directory, and a list of file names and line numbers you need to monitor. 2. Your solution should use the `linecache` module to retrieve and cache lines efficiently. 3. The utility should perform the following functions: - **retrieve_line_content(file_path, line_number)**: Retrieves the content of the specified line in the given file path. If the file or line does not exist, it should return an empty string. - **clear_all_caches()**: Clears the cache used by the `linecache` module. - **update_line_cache(file_path)**: Validates and updates the cache for the specified file to ensure it reflects the current file state. # Input and Output: - **Input**: - `file_path` (str): Path to a file. - `line_number` (int): The line number whose content needs to be retrieved. - **Output**: - `retrieve_line_content(file_path, line_number)`: Returns the content (str) of the specified line or an empty string if not found. - `clear_all_caches()`: Clears the cache used by `linecache`. - `update_line_cache(file_path)`: Checks and refreshes the cache for the specified file. # Constraints: - The utility must handle large files efficiently. - It should be resilient to file changes on disk, using `linecache.checkcache`. # Example: ```python # Assume the following is a part of a file named sample.txt Line 1 content Line 2 content Line 3 content # Assuming sample.txt is in the current directory file_path = \\"sample.txt\\" line_number = 2 # Should print \\"Line 2 contentn\\" print(retrieve_line_content(file_path, line_number)) # Clears the linecache clear_all_caches() # Updates the cache for the specified file update_line_cache(file_path) ``` # Implementation: Implement the following functions according to the specifications. ```python import linecache def retrieve_line_content(file_path, line_number): pass def clear_all_caches(): pass def update_line_cache(file_path): pass ```","solution":"import linecache def retrieve_line_content(file_path, line_number): Retrieves the content of the specified line in the given file path. If the file or line does not exist, it should return an empty string. line_content = linecache.getline(file_path, line_number) return line_content if line_content else \\"\\" def clear_all_caches(): Clears the cache used by the `linecache` module. linecache.clearcache() def update_line_cache(file_path): Checks and refreshes the cache for the specified file. linecache.checkcache(file_path)"},{"question":"You are tasked with implementing a function that leverages the \\"linecache\\" module to retrieve specific lines from multiple files and performs caching efficiently. Your function should take a list of file names and line numbers and return the corresponding lines from these files. # Function Signature: ```python def get_lines_from_files(file_line_data: List[Tuple[str, int]]) -> List[str]: pass ``` # Input: - **file_line_data**: A list of tuples, where each tuple contains: - A string representing the file name (filename). - An integer representing the line number (lineno). # Output: - A list of strings: Each string is the content of the corresponding line specified in the input list. If any file or line number is invalid, the corresponding entry in the output list should be an empty string. # Constraints: - Assume the files are accessible and contain valid data. - The function should manage the cache efficiently to handle multiple calls without repeatedly reading the same lines. - If a line number exceeds the total number of lines in a file, return an empty string for that line. # Example: ```python file_line_data = [(\\"file1.txt\\", 2), (\\"file2.txt\\", 5), (\\"file1.txt\\", 10)] result = get_lines_from_files(file_line_data) print(result) ``` Expected output (assuming the contents of the files are known): ```python [\'line content from file1.txt at line 2\', \'line content from file2.txt at line 5\', \'\'] ``` # Additional Requirements: - Implement the `clear_cache` function within your module to clear the linecache\'s internal cache: ```python def clear_cache() -> None: pass ``` # Evaluation Criteria: - Correctness: The function should return the correct lines for multiple files as per the input list. - Efficiency: The implementation should leverage caching to minimize file I/O operations. - Robustness: The solution should handle edge cases like invalid file names, line numbers exceeding the file length, and large numbers of file and line combinations.","solution":"import linecache from typing import List, Tuple def get_lines_from_files(file_line_data: List[Tuple[str, int]]) -> List[str]: results = [] for fname, lineno in file_line_data: line = linecache.getline(fname, lineno) if line == \'\': results.append(\'\') else: results.append(line.rstrip(\'n\')) return results def clear_cache() -> None: linecache.clearcache()"},{"question":"# Advanced Set Operations Implementation Objective: Implement a set of custom functions that manipulate sets and frozensets. Your implementations must demonstrate the understanding of the provided API and perform specific tasks involving these data structures. Instructions: 1. Implement a function `merge_sets` that takes two iterables as inputs and returns a new set containing elements from both iterables. 2. Implement a function `safe_add_to_set` that takes a set and an element as inputs, tries to add the element to the set, and returns whether the element was successfully added. If the element is not hashable, handle the `TypeError` and return `False`. 3. Implement a function `unique_elements_count` that takes a list of iterables, creates sets from each iterable, and returns the total number of unique elements across all sets. Requirements: - Your implementations should be efficient and handle large inputs within a reasonable time frame. - Make sure to handle edge cases, such as empty iterables, unhashable elements, and invalid input types. - You may use only the functions and macros defined in the provided documentation. Input and Output Formats: - `merge_sets(iterable1, iterable2) -> set`: - Inputs: Two iterables (`iterable1`, `iterable2`). - Output: A new set containing elements from both iterables. - `safe_add_to_set(set_obj, element) -> bool`: - Inputs: A set (`set_obj`), an element (`element`). - Output: `True` if the element was successfully added, `False` otherwise. - `unique_elements_count(iterable_list) -> int`: - Inputs: A list of iterables (`iterable_list`). - Output: An integer representing the total count of unique elements across all sets created from the iterables. Use the following definitions for the functions: ```python def merge_sets(iterable1, iterable2): pass def safe_add_to_set(set_obj, element): pass def unique_elements_count(iterable_list): pass ``` Constraints: - Each iterable input to the functions can contain up to `100,000` elements. - The total number of unique elements across all iterables is at most `1,000,000`. Examples: ```python # Example for merge_sets assert merge_sets([1, 2, 3], [3, 4, 5]) == {1, 2, 3, 4, 5} # Example for safe_add_to_set s = {1, 2, 3} assert safe_add_to_set(s, 4) == True assert safe_add_to_set(s, [5, 6]) == False # Example for unique_elements_count assert unique_elements_count([[1, 2, 2], [3, 4], [2, 4, 5]]) == 5 ```","solution":"def merge_sets(iterable1, iterable2): Returns a new set containing elements from both iterables. return set(iterable1).union(iterable2) def safe_add_to_set(set_obj, element): Tries to add the element to the set, and returns whether the element was successfully added. try: set_obj.add(element) return True except TypeError: return False def unique_elements_count(iterable_list): Creates sets from each iterable in the list and returns the total number of unique elements across all sets. unique_elements = set() for iterable in iterable_list: unique_elements.update(iterable) return len(unique_elements)"},{"question":"# Asynchronous Task Scheduler and Server You are required to implement an asynchronous task scheduler and server using Python\'s asyncio module. Requirements: 1. **Task Scheduler**: - Implement a function `schedule_tasks(loop, tasks)`, where `loop` is the event loop and `tasks` is a list of tasks. - Each task is represented by a dictionary with the keys: - `time`: Time in seconds after which the task will be executed. - `callback`: The callback function to be executed. - `args`: A tuple of arguments to be passed to the callback. - Schedule each task to be executed after the specified time using `loop.call_later()`. 2. **Server**: - Implement an asynchronous TCP server using `asyncio` with `asyncio.create_server` that: - Listens on host `127.0.0.1` and port `8888`. - Accepts multiple client connections. - For each client, the server should read data, print it, and echo it back to the client. - Gracefully handles shutdown when receiving a `SIGTERM` signal. 3. **Main Function**: - Implement a `main()` coroutine that: - Sets up the event loop. - Schedules a list of sample tasks using the `schedule_tasks` function. - Starts the server. - Waits for an hour before shutting down, unless a signal interrupts it. Input and Output Formats: - **Input**: No direct input from stdin. - **Output**: The server should print received and sent data. The scheduled tasks\' callback should print their execution time and the provided arguments. Constraints: - Ensure the event loop is used efficiently. - Handle exceptions gracefully. Example: Sample list of tasks: ```python tasks = [ {\'time\': 2, \'callback\': print, \'args\': (\'Task 1 executed\',)}, {\'time\': 4, \'callback\': print, \'args\': (\'Task 2 executed\',)}, {\'time\': 6, \'callback\': print, \'args\': (\'Task 3 executed\',)} ] ``` Main function usage: ```python import asyncio async def main(): loop = asyncio.get_running_loop() tasks = [ {\'time\': 2, \'callback\': print, \'args\': (\'Task 1 executed\',)}, {\'time\': 4, \'callback\': print, \'args\': (\'Task 2 executed\',)}, {\'time\': 6, \'callback\': print, \'args\': (\'Task 3 executed\',)} ] await schedule_tasks(loop, tasks) await start_server(loop) await asyncio.sleep(3600) asyncio.run(main()) ``` Your implementation should cover the following functions: - `schedule_tasks(loop, tasks)` - `start_server(loop)`","solution":"import asyncio import signal def schedule_tasks(loop, tasks): Schedules tasks to be executed at specified times. :param loop: Event loop in which the tasks will be scheduled. :param tasks: List of dictionaries containing task details. for task in tasks: loop.call_later(task[\'time\'], task[\'callback\'], *task[\'args\']) async def handle_client(reader, writer): Handles client connection. :param reader: StreamReader object. :param writer: StreamWriter object. while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received {message}\\") writer.write(data) await writer.drain() print(\\"Client disconnected\\") writer.close() await writer.wait_closed() async def start_server(): Starts an asynchronous TCP server. server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) print(\\"Server started on 127.0.0.1:8888\\") async with server: await server.serve_forever() async def main(): loop = asyncio.get_running_loop() # Sample tasks tasks = [ {\'time\': 2, \'callback\': print, \'args\': (\'Task 1 executed\',)}, {\'time\': 4, \'callback\': print, \'args\': (\'Task 2 executed\',)}, {\'time\': 6, \'callback\': print, \'args\': (\'Task 3 executed\',)} ] # Schedule tasks schedule_tasks(loop, tasks) # Start server server_coro = start_server() server_task = loop.create_task(server_coro) def shutdown(): print(\\"Gracefully shutting down...\\") server_task.cancel() loop.add_signal_handler(signal.SIGTERM, shutdown) try: await asyncio.sleep(3600) # Run for an hour except asyncio.CancelledError: pass # Task cancellation # Run the main function if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# AsyncIO Synchronization Primitives - Traffic Light Simulation Objective Design a traffic light system at an intersection using asyncio synchronization primitives to manage the traffic flow. The intersection allows one-way traffic at a time; either north-south or east-west, but not both. Use `Locks` to ensure mutual exclusion and `Events` to signal changes in traffic lights. Requirements - Implement a function `traffic_light_system()` that coordinates the traffic lights using asyncio primitives. - The north-south and east-west signals should alternate every 10 seconds. - Use an asyncio `Lock` to ensure only one direction of traffic proceeds at a time. - Use an asyncio `Event` to signal a change in traffic lights. Input and Output - There are no direct inputs and outputs to this function. The function should be able to demonstrate the alternating traffic lights and mutual exclusion for traffic flow. - The function should run indefinitely, simulating the traffic light system. Constraints - Only asyncio synchronization primitives should be used. - The simulation should alternate the traffic light system every 10 seconds indefinitely. Example ```python import asyncio async def north_south_traffic(lock, event): while True: async with lock: if event.is_set(): print(\\"North-South: Green Light\\") await asyncio.sleep(10) event.clear() else: print(\\"North-South: Red Light\\") await event.wait() async def east_west_traffic(lock, event): while True: async with lock: if not event.is_set(): print(\\"East-West: Green Light\\") await asyncio.sleep(10) event.set() else: print(\\"East-West: Red Light\\") await event.wait() async def traffic_light_system(): lock = asyncio.Lock() event = asyncio.Event() event.set() # Start with North-South Green light north_south_task = asyncio.create_task(north_south_traffic(lock, event)) east_west_task = asyncio.create_task(east_west_traffic(lock, event)) await asyncio.gather(north_south_task, east_west_task) # To run the traffic light system # asyncio.run(traffic_light_system()) ``` In this problem, you are expected to implement `north_south_traffic()` and `east_west_traffic()` functions and use these within the main `traffic_light_system()` function. The code should demonstrate the synchronization primitives\' usage to control traffic flow alternately.","solution":"import asyncio async def north_south_traffic(lock, event): while True: async with lock: if event.is_set(): print(\\"North-South: Green Light\\") await asyncio.sleep(10) event.clear() else: print(\\"North-South: Red Light\\") await event.wait() async def east_west_traffic(lock, event): while True: async with lock: if not event.is_set(): print(\\"East-West: Green Light\\") await asyncio.sleep(10) event.set() else: print(\\"East-West: Red Light\\") await event.wait() async def traffic_light_system(): lock = asyncio.Lock() event = asyncio.Event() event.set() # Start with North-South Green light north_south_task = asyncio.create_task(north_south_traffic(lock, event)) east_west_task = asyncio.create_task(east_west_traffic(lock, event)) await asyncio.gather(north_south_task, east_west_task)"},{"question":"**Problem Statement: Migrating `imp` Module Functions to `importlib`** The `imp` module has been deprecated since Python 3.4, and its functionality has been replaced by `importlib`. Your task is to reimplement a function that mimics the deprecated `imp.find_module` and `imp.load_module` using the modern `importlib` approach. Write a function `custom_import(module_name: str) -> type` that takes a module name as a string and returns the imported module object. If the module cannot be found, raise an `ImportError`. # Input - `module_name` : a string representing the name of the module to import. # Output - Returns the imported module object. # Constraints - Your solution should not use the `imp` module at all. - You should implement using `importlib.util.find_spec`, `importlib.util.module_from_spec`, and `importlib.util.spec_from_file_location`. # Example ```python # Example usage: imported_module = custom_import(\'os\') print(imported_module.path) # Should print the path attribute from the `os` module. ``` # Notes - Make sure to handle hierarchical module names correctly. - Consider edge cases such as trying to import a non-existing module, and ensure that an appropriate `ImportError` is raised. - Do not use try-except blocks to bypass the requirement of raising `ImportError`. Write your implementation below: ```python import importlib.util import sys def custom_import(module_name: str) -> type: # Your code here pass # You can test your implementation below if __name__ == \\"__main__\\": try: module = custom_import(\'os\') print(module.path) # Expected to print the path of the \'os\' module except ImportError: print(\\"Module not found\\") ```","solution":"import importlib.util import sys def custom_import(module_name: str) -> type: Mimics imp.find_module and imp.load_module using importlib. Parameters: - module_name: str : A string representing the name of the module to import. Returns: - type: The imported module object. Raises: - ImportError: If the module cannot be found. spec = importlib.util.find_spec(module_name) if spec is None: raise ImportError(f\\"Module \'{module_name}\' not found\\") module = importlib.util.module_from_spec(spec) sys.modules[module_name] = module spec.loader.exec_module(module) return module"},{"question":"# Ensemble Learning: Gradient Boosting Implementation **Objective**: Implement a supervised machine learning model using Gradient Boosting from the scikit-learn ensemble module to classify a given dataset. Evaluate the model\'s performance using appropriate metrics. # Description: You are provided with a dataset containing information about customers, including features such as age, income, and purchasing behavior. Your task is to build a Gradient Boosting classifier to predict if a customer will make a purchase based on these features. # Dataset: The dataset is in the form of a CSV file with the following columns: - `age`: Integer, age of the customer - `income`: Float, annual income of the customer in dollars - `purchase_history`: Integer, number of past purchases - `target`: Binary, 1 if the customer made a purchase, 0 otherwise # Requirements: 1. Load the dataset and preprocess it (handle any missing values, if any). 2. Split the dataset into training and test sets (80/20 split). 3. Implement a Gradient Boosting classifier using `GradientBoostingClassifier` from scikit-learn\'s ensemble module. 4. Train the model on the training set. 5. Evaluate the model on the test set using accuracy, precision, recall, and F1 score. 6. Provide a brief analysis of the model\'s performance based on these metrics. # Constraints: - You should not use any external machine learning library other than scikit-learn. - Ensure your implementation is efficient and runs within a reasonable time frame. # Input: - CSV file path as a string. # Output: - A dictionary containing accuracy, precision, recall, and F1 score of the model. # Function Signature: ```python def gradient_boosting_classifier(file_path: str) -> dict: pass ``` # Example: ```python result = gradient_boosting_classifier(\'customer_data.csv\') print(result) # Expected Output: {\'accuracy\': 0.85, \'precision\': 0.88, \'recall\': 0.82, \'f1_score\': 0.85} ``` # Notes: - Ensure all necessary imports are included at the beginning of your implementation. - You may assume the CSV file is properly formatted and contains no corrupted data. Handling missing values is sufficient for preprocessing.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import GradientBoostingClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def gradient_boosting_classifier(file_path: str) -> dict: # Load the dataset data = pd.read_csv(file_path) # Handle missing values (e.g., fill with mean of the column) data.fillna(data.mean(), inplace=True) # Split dataset into features and target X = data.drop(columns=[\'target\']) y = data[\'target\'] # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize and train the Gradient Boosting classifier model = GradientBoostingClassifier() model.fit(X_train, y_train) # Make predictions on the test set y_pred = model.predict(X_test) # Calculate evaluation metrics accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) # Prepare the result dictionary result = { \'accuracy\': accuracy, \'precision\': precision, \'recall\': recall, \'f1_score\': f1 } return result"},{"question":"**Custom Import Function Using `imp` Module** # Background You are tasked with creating a custom import function that emulates Python\'s import behavior using the now-deprecated `imp` module. # Problem Statement Implement a function `custom_import(module_name: str) -> object` that imports a module given its name using the `imp` module. If the module is already imported (exists in `sys.modules`), it should return the already imported module. Otherwise, it should find and load the module using `imp.find_module` and `imp.load_module`. # Requirements 1. **Function Signature:** ```python def custom_import(module_name: str) -> object: ``` 2. **Input:** - `module_name` (str): The name of the module to be imported. 3. **Output:** - Returns the module object corresponding to the given module name. 4. **Constraints:** - Use the `imp` module\'s `find_module` and `load_module` functions to handle the import logic. - Ensure that any file object obtained from `find_module` is properly closed after use. - Do not use modern alternatives or `importlib`. 5. **Performance Considerations:** - The function should handle the import logic efficiently and correctly, even for modules that are packages. # Example Usage ```python math_module = custom_import(\\"math\\") print(math_module.sqrt(16)) # Output: 4.0 os_module = custom_import(\\"os\\") print(os_module.name) # Output might vary depending on the operating system ``` # Hint Refer to the examples provided in the `imp` module documentation regarding the implementation of custom import logic if needed. # Note Remember to handle exceptions that may arise due to module not found error (`ImportError`) or other issues related to the import process.","solution":"import imp import sys def custom_import(module_name: str) -> object: Imports a module given its name using the imp module. Args: - module_name (str): The name of the module to be imported. Returns: - object: The module object corresponding to the given module name. if module_name in sys.modules: return sys.modules[module_name] try: fp, pathname, description = imp.find_module(module_name) try: module = imp.load_module(module_name, fp, pathname, description) return module finally: if fp: fp.close() except ImportError: raise ImportError(f\\"Module {module_name} not found\\")"},{"question":"**Coding Assessment Question** In this assessment, your goal is to demonstrate your understanding of fundamental and advanced concepts of the `seaborn` package, particularly using the `seaborn.objects` module. You will be required to create a customized bar plot using the `tips` dataset and demonstrate your ability to group and count data effectively. # Problem Statement You are provided with the `tips` dataset, which contains the following columns: - `total_bill`: Total bill amount in dollars. - `tip`: Tip given in dollars. - `sex`: Gender of the person paying the bill (Male/Female). - `smoker`: Whether the person is a smoker (Yes/No). - `day`: Day of the week (Thur/Fri/Sat/Sun). - `time`: Time of day (Lunch/Dinner). - `size`: Size of the party. Using this dataset, perform the following tasks: 1. Create a bar plot that shows the count of tips given on each day of the week. 2. Group the data by `sex` and modify the plot to show different bars for Male and Female for each day of the week. Use distinct colors to differentiate between the two groups. 3. Apply the `so.Dodge` option to place Male and Female bars side by side for each day of the week. 4. Provide the final plot with appropriate labels and a title. # Expected Input and Output - **Input:** The `tips` dataset loaded as a DataFrame. - **Output:** A bar plot as described in the tasks above. # Constraints and Limitations - Use only the components and functions from the `seaborn.objects` module as shown in the provided documentation. - Ensure that the plot is properly labeled and visually clear. - Handle both the grouping and dodge functionalities as specified. # Performance Requirements - The solution should efficiently handle the dataset without unnecessary computations. - The plot should be optimized for readability. # Example Solution Here is an example function template to get you started. You need to fill in the appropriate code to accomplish the tasks: ```python import seaborn.objects as so from seaborn import load_dataset def create_custom_bar_plot(): # Load the tips dataset tips = load_dataset(\\"tips\\") # Step 1: Create a basic bar plot showing the count of tips given on each day of the week plot = so.Plot(tips, x=\\"day\\") # Step 2: Group the data by sex and modify the plot accordingly plot = plot.add(so.Bar(), so.Count(), so.Dodge(), color=\\"sex\\") # Step 3: Apply dodge option to place Male and Female bars side by side # Note: This step is integrated with the previous one # Step 4: Add appropriate labels and title plot = plot.label(x=\\"Day of the Week\\", y=\\"Count of Tips\\", title=\\"Count of Tips by Day and Gender\\") # Render the plot plot.show() # Example function call create_custom_bar_plot() ``` Implement the function `create_custom_bar_plot()` with the appropriate code to meet all the specified requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_custom_bar_plot(): # Load the tips dataset tips = load_dataset(\\"tips\\") # Create a bar plot that shows the count of tips given on each day of the week, grouped by sex plot = ( so.Plot(tips, x=\\"day\\") .add(so.Bar(), so.Count(), so.Dodge(), color=\\"sex\\") .label(x=\\"Day of the Week\\", y=\\"Count of Tips\\", title=\\"Count of Tips by Day and Gender\\") ) # Render the plot plot.show()"},{"question":"# Python Runtime Services Assessment Question Objective Your task is to implement a Python program that utilizes both the `dataclasses` and `contextlib` modules to manage a set of tasks and handle resource management efficiently. This will test your understanding of data classes, context managers, and exception handling. Problem Statement You are required to implement a system that manages a collection of tasks. Each task has a unique identifier, a description, a priority, and a status indicating whether it is complete or not. Additionally, you should implement a context manager to handle the creation and cleanup of a logging resource that captures the execution details of managing the tasks. Requirements 1. **Data Class Definition:** - Define a `Task` data class with the following attributes: - `task_id`: an integer, unique identifier for the task. - `description`: a string, a brief description of the task. - `priority`: an integer, indicates the priority of the task (1 being the highest priority). - `is_complete`: a boolean, the status of the task (default to `False`). 2. **Task Manager Class:** - Implement a `TaskManager` class with the following methods: - `add_task(task)`: Adds a new task to the manager. - `complete_task(task_id)`: Marks the task with the given `task_id` as complete. - `get_tasks()`: Returns a list of all tasks. - `get_pending_tasks()`: Returns a list of pending (not complete) tasks, sorted by priority. 3. **Context Manager for Logging:** - Create a context manager using the `contextlib` module to handle log file operations: - The context manager should open a log file (`execution_logs.txt`) for writing when entering. - Log all task operations (`add_task`, `complete_task`) with timestamps. - Ensure the log file is closed properly upon exiting the context. 4. **Main Program Logic:** - Implement the main logic to demonstrate the functionality of your `TaskManager` and logging context manager: - Use the context manager to log the addition and completion of tasks. - Print the list of all tasks and pending tasks at the end. Constraints - Use Python\'s `dataclasses` and `contextlib` modules. - Ensure your code is well-documented and follows best practices for clarity and efficiency. Example Usage ```python from datetime import datetime from task_manager import Task, TaskManager, LogContextManager # Example Usage with LogContextManager(\'execution_logs.txt\') as logger: task_manager = TaskManager(logger) task1 = Task(task_id=1, description=\\"Complete assignment\\", priority=1) task2 = Task(task_id=2, description=\\"Review PR\\", priority=2) task_manager.add_task(task1) task_manager.add_task(task2) task_manager.complete_task(1) for task in task_manager.get_tasks(): print(task) for task in task_manager.get_pending_tasks(): print(task) ``` Expected output: ``` Task(task_id=1, description=\'Complete assignment\', priority=1, is_complete=True) Task(task_id=2, description=\'Review PR\', priority=2, is_complete=False) Task(task_id=2, description=\'Review PR\', priority=2, is_complete=False) ``` The log file (`execution_logs.txt`) should capture: ``` [2023-10-01 10:00:00] Task added: Task(task_id=1, description=\'Complete assignment\', priority=1, is_complete=False) [2023-10-01 10:01:00] Task added: Task(task_id=2, description=\'Review PR\', priority=2, is_complete=False) [2023-10-01 10:02:00] Task completed: Task(task_id=1, description=\'Complete assignment\', priority=1, is_complete=True) ``` Submission Submit your Python script (`task_manager.py`) with the implementation of the `Task`, `TaskManager`, and `LogContextManager` classes.","solution":"from dataclasses import dataclass, field from typing import List from contextlib import contextmanager import datetime @dataclass class Task: A class to represent a task. task_id: int description: str priority: int is_complete: bool = field(default=False) class TaskManager: A class to manage tasks. def __init__(self, logger): self.tasks: List[Task] = [] self.logger = logger def add_task(self, task: Task): self.tasks.append(task) self.logger.log(f\\"Task added: {task}\\") def complete_task(self, task_id: int): for task in self.tasks: if task.task_id == task_id: task.is_complete = True self.logger.log(f\\"Task completed: {task}\\") break def get_tasks(self) -> List[Task]: return self.tasks def get_pending_tasks(self) -> List[Task]: pending_tasks = [task for task in self.tasks if not task.is_complete] return sorted(pending_tasks, key=lambda x: x.priority) @contextmanager def LogContextManager(log_file: str): class Logger: def __init__(self, file): self.file = file def log(self, message: str): timestamp = datetime.datetime.now().strftime(\\"%Y-%m-%d %H:%M:%S\\") self.file.write(f\\"[{timestamp}] {message}n\\") self.file.flush() file = open(log_file, \'w\') logger = Logger(file) try: yield logger finally: file.close() # Main program logic if __name__ == \\"__main__\\": with LogContextManager(\'execution_logs.txt\') as logger: task_manager = TaskManager(logger) task1 = Task(task_id=1, description=\\"Complete assignment\\", priority=1) task2 = Task(task_id=2, description=\\"Review PR\\", priority=2) task_manager.add_task(task1) task_manager.add_task(task2) task_manager.complete_task(1) for task in task_manager.get_tasks(): print(task) for task in task_manager.get_pending_tasks(): print(task)"},{"question":"Coding Assessment Question # Objective Implement a custom mapping class in Python that supports fundamental operations similar to those described in the C API\'s Mapping Protocol. This will demonstrate your understanding of mapping objects, item management, and key-value operations. # Task You need to implement a `CustomMapping` class which supports the following methods: 1. **`__init__(self)`**: - Initializes an empty mapping. 2. **`__len__(self)`**: - Returns the number of key-value pairs in the mapping. This should mimic the behavior of `PyMapping_Size`. 3. **`__getitem__(self, key)`**: - Returns the value associated with the given key. This should mimic the behavior of `PyMapping_GetItemString`. - Raise a `KeyError` if the key is not present. 4. **`__setitem__(self, key, value)`**: - Sets the value for the given key. This should mimic the behavior of `PyMapping_SetItemString`. 5. **`__delitem__(self, key)`**: - Deletes the key-value pair associated with the given key. This should mimic the behavior of `PyMapping_DelItemString`. - Raise a `KeyError` if the key is not present. 6. **`__contains__(self, key)`**: - Checks if a key is in the mapping. This should mimic the behavior of `PyMapping_HasKeyString`. 7. **`keys(self)`**: - Returns a list of all the keys in the mapping. This should mimic the behavior of `PyMapping_Keys`. 8. **`values(self)`**: - Returns a list of all the values in the mapping. This should mimic the behavior of `PyMapping_Values`. 9. **`items(self)`**: - Returns a list of all key-value pairs (as tuples). This should mimic the behavior of `PyMapping_Items`. # Input and Output Formats - You will be given a series of operations to perform on an instance of `CustomMapping`. The operations will contain tuples where the first element is the method name and the subsequent elements are the arguments for the method. - Your implementation should be tested using the provided operations, and the results should be printed as specified. # Constraints - You should not use Python’s built-in dictionaries directly for storage. Instead, you should implement the storage mechanism yourself. # Example ```python # Example usage: mapping = CustomMapping() mapping[\'a\'] = 1 mapping[\'b\'] = 2 assert len(mapping) == 2 assert mapping[\'a\'] == 1 assert mapping[\'b\'] == 2 del mapping[\'a\'] assert len(mapping) == 1 assert \'a\' not in mapping mapping[\'c\'] = 3 assert mapping.keys() == [\'b\', \'c\'] assert mapping.values() == [2, 3] assert mapping.items() == [(\'b\', 2), (\'c\', 3)] ``` Implement the class `CustomMapping` to pass the above example successfully.","solution":"class CustomMapping: def __init__(self): self._data = [] def __len__(self): return len(self._data) def __getitem__(self, key): for k, v in self._data: if k == key: return v raise KeyError(key) def __setitem__(self, key, value): for i, (k, v) in enumerate(self._data): if k == key: self._data[i] = (key, value) return self._data.append((key, value)) def __delitem__(self, key): for i, (k, v) in enumerate(self._data): if k == key: del self._data[i] return raise KeyError(key) def __contains__(self, key): for k, _ in self._data: if k == key: return True return False def keys(self): return [k for k, _ in self._data] def values(self): return [v for _, v in self._data] def items(self): return self._data[:]"},{"question":"Objective The goal of this assessment is to evaluate your understanding of scikit-learn’s evaluation metrics and scoring functions, as well as your ability to implement custom scoring for model evaluation. Task You are provided with a dataset and your task involves: 1. Implementing a custom scoring function that follows the scikit-learn API. 2. Using this custom scorer to evaluate a machine learning model in a cross-validation setting. Dataset For this task, you will use the popular Iris dataset which is included in the scikit-learn library. Steps 1. **Data Preparation** - Load the Iris dataset. - Split the dataset into training and testing sets. 2. **Custom Scoring Function** - Implement a custom scoring function that you will use to evaluate the classifier. This custom scoring function will calculate the weighted average of Precision and Recall with weights 0.4 and 0.6 respectively. ```python def custom_scorer(estimator, X, y): from sklearn.metrics import precision_score, recall_score y_pred = estimator.predict(X) precision = precision_score(y, y_pred, average=\'macro\') recall = recall_score(y, y_pred, average=\'macro\') weighted_score = 0.4 * precision + 0.6 * recall return weighted_score ``` 3. **Model Training and Evaluation** - Train a Support Vector Classifier (SVC) using `GridSearchCV` where you optimize the hyperparameter `C`. - Evaluate the model using your custom scoring function defined above. - Report the best hyperparameter value and the corresponding score. Implementation Details 1. **Expected Input and Output Formats** - **Input**: No direct input as you will use the Iris dataset from sklearn. - **Output**: Print the best hyperparameter value and its corresponding custom score. 2. **Constraints or Limitations** - Use only the given custom scorer. - Make sure to use cross-validation with 5 folds. 3. **Performance Requirements** - Ensure that the custom scoring function follows the scikit-learn API and works seamlessly with `GridSearchCV`. Example ```python import numpy as np from sklearn import datasets from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.svm import SVC from sklearn.metrics import make_scorer from sklearn.metrics import precision_score, recall_score # 1. Load the Iris dataset iris = datasets.load_iris() X = iris.data y = iris.target # 2. Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 3. Implement custom scoring function def custom_scorer(estimator, X, y): y_pred = estimator.predict(X) precision = precision_score(y, y_pred, average=\'macro\') recall = recall_score(y, y_pred, average=\'macro\') weighted_score = 0.4 * precision + 0.6 * recall return weighted_score # 4. Create a scorer using make_scorer my_scorer = make_scorer(custom_scorer, greater_is_better=True) # 5. Train SVC model using GridSearchCV param_grid = {\'C\': [0.1, 1, 10, 100]} grid_search = GridSearchCV(SVC(), param_grid, scoring=my_scorer, cv=5) grid_search.fit(X_train, y_train) # 6. Report the best parameters and corresponding custom score print(f\\"Best hyperparameter C: {grid_search.best_params_[\'C\']}\\") print(f\\"Best custom score: {grid_search.best_score_:.4f}\\") ``` In the implementation, ensure your code follows the structure and steps outlined above.","solution":"import numpy as np from sklearn import datasets from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.svm import SVC from sklearn.metrics import make_scorer, precision_score, recall_score # 1. Load the Iris dataset iris = datasets.load_iris() X = iris.data y = iris.target # 2. Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 3. Implement custom scoring function def custom_scorer(estimator, X, y): y_pred = estimator.predict(X) precision = precision_score(y, y_pred, average=\'macro\') recall = recall_score(y, y_pred, average=\'macro\') weighted_score = 0.4 * precision + 0.6 * recall return weighted_score # 4. Create a scorer using make_scorer my_scorer = make_scorer(custom_scorer, greater_is_better=True) # 5. Train SVC model using GridSearchCV param_grid = {\'C\': [0.1, 1, 10, 100]} grid_search = GridSearchCV(SVC(), param_grid, scoring=my_scorer, cv=5) grid_search.fit(X_train, y_train) # 6. Report the best parameters and corresponding custom score print(f\\"Best hyperparameter C: {grid_search.best_params_[\'C\']}\\") print(f\\"Best custom score: {grid_search.best_score_:.4f}\\")"},{"question":"**Title:** Utilizing PyTorch MPS Backend for GPU Acceleration on macOS **Problem Statement:** You have been provided with a basic artificial neural network (ANN) using PyTorch. Your task is to modify this code to leverage the MPS backend for GPU acceleration on a macOS device. Specifically, you need to: 1. Check for the availability of the MPS backend. If it is unavailable, print appropriate messages based on the reason for its unavailability. 2. Move the tensors and the model to the MPS device. 3. Train the model on the MPS device using dummy data. **Function Signature:** ```python def train_model_mps(): pass ``` **Detailed Instructions:** 1. **Check for MPS Availability:** - Use `torch.backends.mps.is_available()` and `torch.backends.mps.is_built()` to check if the MPS backend is available and built. - Print appropriate messages if the MPS backend is unavailable due to the PyTorch install not being built with MPS enabled or because the macOS version is not 12.3+ and/or your device does not support MPS. 2. **Move Tensors and Model to MPS:** - Create a tensor of ones with shape (5,) directly on the MPS device. - Move the provided `SimpleModel` to the MPS device. 3. **Train the Model on MPS:** - Use dummy input data to train the model. - Ensure all operations are performed on the MPS device. **Input:** - No input parameters required. **Output:** - The function should print status messages about the availability of MPS and display the loss after training the model for a few epochs. **Constraints:** - The solution must handle cases where the MPS device is unavailable. - The dummy data and model should be small to ensure the training completes quickly for demonstration purposes. **Example:** ```python import torch import torch.nn as nn import torch.optim as optim class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(5, 1) def forward(self, x): return self.linear(x) def train_model_mps(): if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): print(\\"MPS not available because the current PyTorch install was not built with MPS enabled.\\") else: print(\\"MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.\\") return mps_device = torch.device(\\"mps\\") # Create a Tensor directly on the mps device x = torch.ones(5, device=mps_device) # Dummy target for training target = torch.tensor([1.0], device=mps_device) # Move model to mps model = SimpleModel().to(mps_device) # Define loss and optimizer criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Training loop (just a few epochs for demonstration) model.train() for epoch in range(5): optimizer.zero_grad() output = model(x) loss = criterion(output, target) loss.backward() optimizer.step() print(f\\"Epoch {epoch+1}, Loss: {loss.item()}\\") # Call the function to test train_model_mps() ``` **Notes:** - Ensure that you test this on a macOS device with an MPS-enabled GPU and macOS version 12.3+.","solution":"import torch import torch.nn as nn import torch.optim as optim class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(5, 1) def forward(self, x): return self.linear(x) def train_model_mps(): if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): print(\\"MPS not available because the current PyTorch install was not built with MPS enabled.\\") else: print(\\"MPS not available because the current macOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.\\") return mps_device = torch.device(\\"mps\\") # Create a Tensor directly on the mps device x = torch.ones(5, device=mps_device) # Dummy target for training target = torch.tensor([1.0], device=mps_device) # Move model to mps model = SimpleModel().to(mps_device) # Define loss and optimizer criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Training loop (just a few epochs for demonstration) model.train() for epoch in range(5): optimizer.zero_grad() output = model(x) loss = criterion(output, target) loss.backward() optimizer.step() print(f\\"Epoch {epoch+1}, Loss: {loss.item()}\\") # This function call is for demonstration purposes and should be removed or commented out in the actual module. # train_model_mps()"},{"question":"Objective: Demonstrate your understanding of buffer protocols and memory management in Python by implementing a function that uses the new buffer protocol to manipulate and verify buffer properties. Problem Statement: You are required to implement a class `BufferManager` that manages a buffer using the modern buffer protocol in Python 3. The class should provide methods to access and manipulate the contents of the buffer. Additionally, you should implement functions that perform certain checks on objects to ascertain their buffer capabilities. Class and Methods: 1. **Class**: `BufferManager` 2. **Initialization**: Accepts a byte-like object (e.g., `bytes`, `bytearray`) during initialization. 3. **Methods**: - `get_readable_buffer()`: Returns a read-only view of the buffer. - `get_writable_buffer()`: Returns a writable view of the buffer. - `release_buffer()`: Releases the buffer. - `read_buffer_contents()`: Reads and returns the contents of the buffer in a readable format. 4. **Functions**: - `has_readable_buffer(obj)`: Checks if the given object supports the readable buffer and returns `True` or `False`. - `has_writable_buffer(obj)`: Checks if the given object supports the writable buffer and returns `True` or `False`. Constraints: - Input buffer can be of any length but must be a bytes-like object. - The functions should handle exceptions and edge cases gracefully. - The buffer manager should ensure resources are managed correctly without leaking memory. Example Usage: ```python # Initialize with a byte buffer buffer_manager = BufferManager(bytearray(b\\"hello world\\")) # Get readable buffer readable_buffer = buffer_manager.get_readable_buffer() assert isinstance(readable_buffer, memoryview) # Get writable buffer writable_buffer = buffer_manager.get_writable_buffer() assert isinstance(writable_buffer, memoryview) # Read buffer contents content = buffer_manager.read_buffer_contents() assert content == b\\"hello world\\" # Check buffer capabilities assert has_readable_buffer(bytearray(b\\"test\\")) assert not has_readable_buffer(12345) assert has_writable_buffer(bytearray(b\\"test\\")) assert not has_writable_buffer(bytes(b\\"test\\")) # Release buffer buffer_manager.release_buffer() ``` Notes: - Utilize the `memoryview` object for creating buffer views. - Handle resource cleanup diligently to avoid memory leaks. - Use proper exception handling to manage erroneous inputs and operations. Implement the `BufferManager` class and helper functions as described above to ensure compatibility with python310 and efficient buffer management.","solution":"class BufferManager: def __init__(self, buffer): Initializes the BufferManager with a given buffer. :param buffer: A bytes-like object (bytes, bytearray). if not isinstance(buffer, (bytes, bytearray)): raise TypeError(\\"Buffer must be a bytes-like object\\") self._buffer = buffer self._readable_view = memoryview(buffer) self._writable_view = memoryview(buffer).cast(\'B\') def get_readable_buffer(self): Returns a read-only view of the buffer. :return: A read-only memoryview of the buffer. return self._readable_view def get_writable_buffer(self): Returns a writable view of the buffer. :return: A writable memoryview of the buffer. if isinstance(self._buffer, bytes): raise TypeError(\\"Buffer is read-only\\") return self._writable_view def release_buffer(self): Releases the buffer by deleting the memory views. del self._readable_view del self._writable_view def read_buffer_contents(self): Reads and returns the contents of the buffer in a readable format. :return: Contents of the buffer. return self._readable_view.tobytes() def has_readable_buffer(obj): Checks if the given object supports the readable buffer protocol. :param obj: The object to check. :return: True if the object supports the readable buffer protocol, False otherwise. try: memoryview(obj) return True except TypeError: return False def has_writable_buffer(obj): Checks if the given object supports the writable buffer protocol. :param obj: The object to check. :return: True if the object supports the writable buffer protocol, False otherwise. try: mem_view = memoryview(obj) if mem_view.readonly: return False return True except TypeError: return False"},{"question":"Objective This question assesses your understanding of configuring and using PyTorch ProcessGroupNCCL environment variables for distributed training jobs. You will need to demonstrate how to set up and utilize these variables to control error handling and debugging behavior. Problem Statement You are working on a deep learning project that requires distributed training across multiple GPUs using PyTorch\'s ProcessGroupNCCL. To ensure robust error handling and effective debugging, you need to configure several environment variables. Write a Python function `configure_nccl_env()` that: 1. Sets the following environment variables for the NCCL ProcessGroup: - `TORCH_NCCL_ASYNC_ERROR_HANDLING` to `1` (aborting NCCL communicator and tearing down the process upon error). - `TORCH_NCCL_ENABLE_MONITORING` to `1` (enable monitoring thread). - `TORCH_NCCL_TRACE_BUFFER_SIZE` to `1024` (enable tracebuffer with a size of 1024 events). - `TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC` to `30` (set the watchdog heartbeat timeout period to 30 seconds). 2. Initializes a distributed process group with `init_method` set to `env://`. Provide a code snippet showcasing how this function can be utilized in a distributed training setup using PyTorch. Input The function does not take any parameters. Output The function does not return any value. Constraints - Do not read from or write to any external file. - Use the `os` module to set environment variables. - You can assume that the necessary NCCL and PyTorch setup is already in place. Example Usage ```python def main(): configure_nccl_env() # Other setup and model training steps follow... if __name__ == \\"__main__\\": main() ``` This function should be used at the beginning of the script, before initializing the distributed process group and starting the training. Note You may assume that the function will be run in an environment where multi-GPU hardware and NCCL are available. Your Task Implement the function `configure_nccl_env()` following the specifications mentioned above. ```python def configure_nccl_env(): # Your implementation here pass ```","solution":"import os import torch.distributed as dist def configure_nccl_env(): Configures NCCL environment variables for error handling and debugging, and initializes a distributed process group. os.environ[\'TORCH_NCCL_ASYNC_ERROR_HANDLING\'] = \'1\' os.environ[\'TORCH_NCCL_ENABLE_MONITORING\'] = \'1\' os.environ[\'TORCH_NCCL_TRACE_BUFFER_SIZE\'] = \'1024\' os.environ[\'TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC\'] = \'30\' dist.init_process_group(backend=\'nccl\', init_method=\'env://\')"},{"question":"Distributed Data Parallel Training with PyTorch Objective Implement a distributed training process using `torch.nn.parallel.DistributedDataParallel` (DDP) to train a simple neural network on a dummy dataset. Demonstrate your understanding of setting up the distributed environment, wrapping the model with DDP, and executing the forward and backward passes along with optimizer steps. Problem Statement You are provided with a simple neural network model and a dummy dataset. Your task is to parallelize the training process across multiple GPU devices using PyTorch\'s DDP module. Follow the steps below to complete the implementation: 1. Set up the distributed environment using `torch.distributed`. 2. Initialize a process group for communication. 3. Create and wrap the neural network model with `DistributedDataParallel`. 4. Implement the forward and backward passes and update the model parameters. 5. Ensure that the training process is synchronized across all devices. Neural Network Model ```python import torch.nn as nn class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 10) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x ``` Dummy Dataset Generate a dummy dataset with input features and labels: ```python import torch def generate_dummy_data(batch_size, input_dim, output_dim): inputs = torch.randn(batch_size, input_dim) labels = torch.randn(batch_size, output_dim) return inputs, labels ``` Tasks 1. **Setup Distributed Environment**: - Initialize the process group for distributed communication. - Ensure the correct assignment of devices to processes. 2. **Model Training with DDP**: - Create an instance of the `SimpleNN` model and wrap it with `DistributedDataParallel`. - Define a loss function and an optimizer. - Implement the training loop with forward and backward passes, and optimizer steps. - Ensure gradient synchronization and model parameter updates across all processes. 3. **Performance Requirements**: - Ensure the DDP setup runs efficiently without deadlocks. - Ensure that the model parameters are synchronized across all processes after each iteration. Constraints - Use `torch.distributed` for setting up the distributed environment. - Use `torch.multiprocessing` to spawn processes for each GPU device. Implementation Complete the code template below: ```python import torch import torch.distributed as dist import torch.multiprocessing as mp import torch.nn as nn import torch.optim as optim from torch.nn.parallel import DistributedDataParallel as DDP import os class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 10) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def generate_dummy_data(batch_size, input_dim, output_dim): inputs = torch.randn(batch_size, input_dim) labels = torch.randn(batch_size, output_dim) return inputs, labels def train(rank, world_size): # Initialize the process group dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) # Set the device to be used for this process torch.cuda.set_device(rank) # Create the model and move it to the appropriate device model = SimpleNN().to(rank) ddp_model = DDP(model, device_ids=[rank]) # Define loss function and optimizer loss_fn = nn.MSELoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) # Training loop for epoch in range(10): inputs, labels = generate_dummy_data(20, 10, 10) inputs = inputs.to(rank) labels = labels.to(rank) # Forward pass outputs = ddp_model(inputs) loss = loss_fn(outputs, labels) # Backward pass optimizer.zero_grad() loss.backward() # Update parameters optimizer.step() # Clean up dist.destroy_process_group() def main(): world_size = 2 mp.spawn(train, args=(world_size,), nprocs=world_size, join=True) if __name__ == \\"__main__\\": os.environ[\\"MASTER_ADDR\\"] = \\"localhost\\" os.environ[\\"MASTER_PORT\\"] = \\"29500\\" main() ``` Submission Submit your implementation with the completed code above. Ensure that the training process runs correctly and efficiently using distributed training with DDP in PyTorch.","solution":"import torch import torch.distributed as dist import torch.multiprocessing as mp import torch.nn as nn import torch.optim as optim from torch.nn.parallel import DistributedDataParallel as DDP import os class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 10) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def generate_dummy_data(batch_size, input_dim, output_dim): inputs = torch.randn(batch_size, input_dim) labels = torch.randn(batch_size, output_dim) return inputs, labels def train(rank, world_size): # Initialize the process group dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) # Set the device to be used for this process torch.cuda.set_device(rank) # Create the model and move it to the appropriate device model = SimpleNN().to(rank) ddp_model = DDP(model, device_ids=[rank]) # Define loss function and optimizer loss_fn = nn.MSELoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) # Training loop for epoch in range(10): inputs, labels = generate_dummy_data(20, 10, 10) inputs = inputs.to(rank) labels = labels.to(rank) # Forward pass outputs = ddp_model(inputs) loss = loss_fn(outputs, labels) # Backward pass optimizer.zero_grad() loss.backward() # Update parameters optimizer.step() # Clean up dist.destroy_process_group() def main(): world_size = 2 mp.spawn(train, args=(world_size,), nprocs=world_size, join=True) if __name__ == \\"__main__\\": os.environ[\\"MASTER_ADDR\\"] = \\"localhost\\" os.environ[\\"MASTER_PORT\\"] = \\"29500\\" main()"},{"question":"# Principal Component Analysis and its Variants in scikit-learn Problem Statement You are provided with a dataset consisting of reviews from an E-commerce platform. Each review includes the text of the review and the rating on a scale of 1 to 5. Your task is to implement and compare various Principal Component Analysis (PCA) techniques in scikit-learn to reduce the dimensionality of the term-document matrix (TF-IDF transformed) derived from the review texts. Guidelines 1. **Data Preprocessing**: - Load the reviews dataset (provided as a CSV file) into a pandas DataFrame. - Use a `TfidfVectorizer` from scikit-learn to transform the review text into a TF-IDF matrix. 2. **Dimensionality Reduction**: - Implement the following PCA variants using scikit-learn: - Standard PCA - IncrementalPCA - KernelPCA with RBF kernel 3. **Implementation Details**: - Each PCA variant should transform the TF-IDF matrix to 2 principal components. - Report the explained variance ratio of each PCA variant. - Plot the 2-dimensional PCA-transformed data for each variant, coloring the points based on their review rating. 4. **Evaluation**: - Compare the results of each PCA variant in terms of explained variance ratio and visual inspectability of the 2D plots. Input - A CSV file `reviews.csv` with columns: - `review_text`: The text content of the review. - `rating`: Rating of the review (integer between 1 to 5). Output - Explained variance ratio for each PCA variant. - Visualization of the transformed data (2D plots) for each PCA variant. Code Requirements - Function `load_and_transform_data(file_path: str) -> Tuple[pd.DataFrame, np.ndarray]`: - Reads the CSV file and converts the review text to a TF-IDF matrix. - Function `apply_pca_variants(tfidf_matrix: np.ndarray, n_components: int) -> Dict[str, np.ndarray]`: - Applies standard PCA, IncrementalPCA, and KernelPCA to the TF-IDF matrix and returns the transformed data. - Function `plot_pca_results(transformed_data: Dict[str, np.ndarray], ratings: pd.Series)`: - Creates 2D scatter plots for each PCA variant, coloring points by the review rating. Constraints - Use only scikit-learn and related necessary Python libraries for matrix operations and visualization. - Ensure that `IncrementalPCA` is memory efficient and processes data in mini-batches. Example ```python import pandas as pd import numpy as np from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA from sklearn.feature_extraction.text import TfidfVectorizer import matplotlib.pyplot as plt def load_and_transform_data(file_path: str): # Load data df = pd.read_csv(file_path) review_texts = df[\'review_text\'] # Transform text to TF-IDF vectorizer = TfidfVectorizer() tfidf_matrix = vectorizer.fit_transform(review_texts) return df, tfidf_matrix.toarray() def apply_pca_variants(tfidf_matrix, n_components=2): results = {} # Standard PCA pca = PCA(n_components=n_components) results[\'PCA\'] = pca.fit_transform(tfidf_matrix) # Incremental PCA ipca = IncrementalPCA(n_components=n_components, batch_size=100) results[\'IncrementalPCA\'] = ipca.fit_transform(tfidf_matrix) # Kernel PCA with RBF kernel kpca = KernelPCA(n_components=n_components, kernel=\'rbf\') results[\'KernelPCA\'] = kpca.fit_transform(tfidf_matrix) return results def plot_pca_results(transformed_data, ratings): for method, data in transformed_data.items(): plt.scatter(data[:, 0], data[:, 1], c=ratings, cmap=\'viridis\', alpha=0.5) plt.title(f\'{method} Result\') plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.colorbar(label=\'Rating\') plt.show() # Example usage df, tfidf_matrix = load_and_transform_data(\'reviews.csv\') transformed_data = apply_pca_variants(tfidf_matrix) plot_pca_results(transformed_data, df[\'rating\']) ``` In your local environment, ensure that the file path to `reviews.csv` is correctly specified.","solution":"import pandas as pd import numpy as np from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA from sklearn.feature_extraction.text import TfidfVectorizer import matplotlib.pyplot as plt def load_and_transform_data(file_path: str): Load review data from a CSV file and transform review text to a TF-IDF matrix. Parameters: - file_path: str : Path to the CSV file containing the review data. Returns: - df: pd.DataFrame : DataFrame containing the review data. - tfidf_matrix: np.ndarray : TF-IDF transformed term-document matrix. # Load data df = pd.read_csv(file_path) review_texts = df[\'review_text\'] # Transform text to TF-IDF vectorizer = TfidfVectorizer() tfidf_matrix = vectorizer.fit_transform(review_texts) return df, tfidf_matrix.toarray() def apply_pca_variants(tfidf_matrix, n_components=2): Apply different PCA variants to the TF-IDF matrix and returns the transformed data. Parameters: - tfidf_matrix: np.ndarray : TF-IDF transformed term-document matrix. - n_components: int : Number of principal components to keep. Returns: - results: dict : Dictionary containing the transformed data from different PCA techniques. results = {} # Standard PCA pca = PCA(n_components=n_components) pca_result = pca.fit_transform(tfidf_matrix) results[\'PCA\'] = pca_result # Incremental PCA ipca = IncrementalPCA(n_components=n_components, batch_size=100) ipca_result = ipca.fit_transform(tfidf_matrix) results[\'IncrementalPCA\'] = ipca_result # Kernel PCA with RBF kernel kpca = KernelPCA(n_components=n_components, kernel=\'rbf\') kpca_result = kpca.fit_transform(tfidf_matrix) results[\'KernelPCA\'] = kpca_result return results def plot_pca_results(transformed_data, ratings): Plot 2D scatter plots for each PCA variant, coloring points by the review rating. Parameters: - transformed_data: dict : Dictionary containing the transformed data from different PCA techniques. - ratings: pd.Series : Series containing the review ratings. for method, data in transformed_data.items(): plt.figure() scatter = plt.scatter(data[:, 0], data[:, 1], c=ratings, cmap=\'viridis\', alpha=0.5) plt.title(f\'{method} Result\') plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.colorbar(scatter, label=\'Rating\') plt.show()"},{"question":"Introduction The `linecache` module in Python allows efficient access to specific lines within text files, leveraging caching and lazy loading mechanisms. This module is particularly useful in scenarios where many lines are read from a single file, as it optimizes retrieval by storing lines in a cache. Task You are required to implement a function `fetch_lines_from_file(file_path: str, line_numbers: list) -> list` that uses the `linecache` module to fetch specified lines from a given text file. The function should utilize the caching mechanism of `linecache` to optimize performance when accessing multiple lines. Function Signature ```python def fetch_lines_from_file(file_path: str, line_numbers: list) -> list: pass ``` Input - `file_path (str)`: The path to the text file from which lines should be fetched. - `line_numbers (list)`: A list of integers representing the line numbers to fetch from the file. The line numbers are 1-based indices. Output - `result (list)`: A list of strings where each string is the content of the corresponding line number from the input list `line_numbers`. Constraints - The file at `file_path` is guaranteed to exist and be readable. - The list `line_numbers` will contain only valid and positive integers, each representing a line number within the file. - The `linecache` module should be used to fetch the lines. Example ```python # Assume \'sample.txt\' content is: # Line 1: Hello, World! # Line 2: This is a sample file. # Line 3: It contains a few lines of text. # Line 4: Let\'s retrieve some lines using linecache. file_path = \'sample.txt\' line_numbers = [1, 3, 4] print(fetch_lines_from_file(file_path, line_numbers)) ``` **Expected Output:** ```python [\'Hello, World!n\', \'It contains a few lines of text.n\', \\"Let\'s retrieve some lines using linecache.n\\"] ``` Notes 1. Use the `linecache.getline` function to fetch each requested line. 2. Ensure that your implementation leverages the internal caching mechanism of `linecache` to optimize consecutive line retrievals. 3. Consider clearing the cache at the end of the function to free up memory for subsequent operations. Advanced Consideration (Optional) - Implement error handling such that if a line number is requested that does not exist in the file, the function should return \\"Line not found\\" for that specific line in the output list. Submission Submit your implementation of the `fetch_lines_from_file` function.","solution":"import linecache def fetch_lines_from_file(file_path: str, line_numbers: list) -> list: Fetch specified lines from a given text file using the linecache module. Args: - file_path (str): The path to the text file. - line_numbers (list): A list of integers representing the line numbers to fetch. Returns: - list: A list of strings, each representing a fetched line from the file. lines = [] for line_number in line_numbers: line = linecache.getline(file_path, line_number) if line == \\"\\": lines.append(\\"Line not found\\") else: lines.append(line) # Clear the cache to free up memory linecache.clearcache() return lines"},{"question":"# Custom Logging System with Rotation and Network Transmission Problem Statement: You are required to design a custom logging system which incorporates the following functionalities: 1. Logs messages to a file, rotating the file when it reaches a specific size. 2. Logs messages to both a local file and a remote server simultaneously. 3. Ensures the log messages are sent asynchronously to improve performance. 4. The log files should also support archiving based on time (e.g., every midnight). Requirements: 1. Implement a logging system that uses `RotatingFileHandler` to handle file rotation based on size. 2. Implement a logging system that uses `SocketHandler` to send logs to a remote server over a TCP socket. 3. Use `QueueHandler` and `QueueListener` to handle asynchronous logging operations. 4. Set the logging system to also use `TimedRotatingFileHandler` for archiving log files based on time. Input: You will not read any input. Instead, a sample set of log messages will be generated within the script to demonstrate the functionality of the logging system. Output: The script should create log files on the disk and send log messages to a predefined remote server. The log file should demonstrate rotation based on size and time archiving. Constraints: - Ensure the file rotation size is set to 1MB. - Archive log files at midnight. - Send log messages to a remote server running on localhost at port 9000. - Use threading or any appropriate method to handle asynchronous logging. Implementation Details: 1. Define the `RotatingFileHandler` to rotate the log file every 1MB. 2. Define the `SocketHandler` to send log messages to a server at `localhost:9000`. 3. Use `QueueHandler` and `QueueListener` to make sure log handling is asynchronous. 4. Add the `TimedRotatingFileHandler` to archive log files at midnight. 5. Generate a sample set of log messages to test the system. Example Code Structure: ```python import logging import logging.handlers from queue import Queue from threading import Thread # Configurations for log rotation and network transfer LOG_FILENAME = \'app.log\' ROTATION_SIZE = 1 * 1024 * 1024 # 1MB REMOTE_HOST = \'localhost\' REMOTE_PORT = 9000 BACKUP_COUNT = 5 # Setup handlers file_handler = logging.handlers.RotatingFileHandler(LOG_FILENAME, maxBytes=ROTATION_SIZE, backupCount=BACKUP_COUNT) timed_handler = logging.handlers.TimedRotatingFileHandler(LOG_FILENAME, when=\'midnight\', backupCount=BACKUP_COUNT) socket_handler = logging.handlers.SocketHandler(REMOTE_HOST, REMOTE_PORT) # Setup asynchronous handling queue = Queue() queue_handler = logging.handlers.QueueHandler(queue) listener = logging.handlers.QueueListener(queue, file_handler, timed_handler, socket_handler) # Start listener thread listener.start() # Setup logger logger = logging.getLogger(\'custom_logger\') logger.setLevel(logging.DEBUG) logger.addHandler(queue_handler) # Generate sample log messages for i in range(1000): # Generating a large number of log messages logger.info(f\'Sample log message {i}\') # Stop listener listener.stop() # Ensure all messages are processed before exiting listener.join() ``` # Explanation: - **RotatingFileHandler**: Handles file rotation when the file size exceeds 1MB. - **TimedRotatingFileHandler**: Handles file archiving every midnight. - **SocketHandler**: Sends log messages to a remote server. - **QueueHandler & QueueListener**: Ensure log messages are handled asynchronously. - Generates 1000 log messages as test cases to verify the functionality. Implement the above logging system and ensure all requirements are met. Test your implementation to ensure it works as expected.","solution":"import logging import logging.handlers from queue import Queue from threading import Thread LOG_FILENAME = \'app.log\' ROTATION_SIZE = 1 * 1024 * 1024 # 1MB REMOTE_HOST = \'localhost\' REMOTE_PORT = 9000 BACKUP_COUNT = 5 def setup_logging(): # Setup handlers file_handler = logging.handlers.RotatingFileHandler(LOG_FILENAME, maxBytes=ROTATION_SIZE, backupCount=BACKUP_COUNT) timed_handler = logging.handlers.TimedRotatingFileHandler(LOG_FILENAME, when=\'midnight\', backupCount=BACKUP_COUNT) socket_handler = logging.handlers.SocketHandler(REMOTE_HOST, REMOTE_PORT) # Setup asynchronous handling queue = Queue() queue_handler = logging.handlers.QueueHandler(queue) listener = logging.handlers.QueueListener(queue, file_handler, timed_handler, socket_handler) # Setup logger logger = logging.getLogger(\'custom_logger\') logger.setLevel(logging.DEBUG) logger.addHandler(queue_handler) # Start listener thread listener.start() return logger, listener def generate_sample_logs(logger): for i in range(1000): # Generating a number of log messages for testing logger.info(f\'Sample log message {i}\') def main(): logger, listener = setup_logging() generate_sample_logs(logger) listener.stop() if __name__ == \\"__main__\\": main()"},{"question":"Using the `modulefinder` module, write a Python function `analyze_script_imports(input_script: str, output_report: str) -> None` that accepts two parameters: 1. `input_script` (str): The filename of a Python script whose module imports are to be analyzed. 2. `output_report` (str): The filename where the analysis report should be saved. The function should perform the following tasks: 1. Use `modulefinder.ModuleFinder` to analyze the provided `input_script`. 2. Generate a report with the following format: - A list of modules successfully imported by the script. - A list of modules that could not be imported (missing modules). 3. Write the report to `output_report` including the names and paths of the imported modules as well as the names of the missing modules. The expected content of the report file should be: ``` Loaded modules: <module_name_1>: <module_path_1> <module_name_2>: <module_path_2> ... Modules not imported: <missing_module_name_1> <missing_module_name_2> ... ``` Assumptions and Constraints: - The input script is a valid Python script. - The output report must be written to a file specified by `output_report`. - If the `input_script` does not exist, the function should raise a `FileNotFoundError`. - The function should handle any potential exceptions and ensure that all resources are properly closed. **Example Usage:** For a script `analyze_me.py` with the following content: ```python import math try: import non_existent_module except ImportError: pass ``` Calling `analyze_script_imports(\'analyze_me.py\', \'report.txt\')` should result in `report.txt` containing: ``` Loaded modules: math: <path_to_math_module> Modules not imported: non_existent_module ``` **Performance Requirements:** - The function should efficiently handle scripts with a large number of imports. - The analysis should be done within a reasonable time frame for scripts up to 1000 lines. You are required to implement the `analyze_script_imports` function based on the provided specifications.","solution":"import modulefinder def analyze_script_imports(input_script: str, output_report: str) -> None: Analyzes the imports of a given Python script and writes a report on the modules that were successfully imported and those that were not. Parameters: input_script (str): The filename of the Python script to analyze. output_report (str): The filename where the analysis report should be saved. # Initialize ModuleFinder finder = modulefinder.ModuleFinder() try: # Run the analysis finder.run_script(input_script) except FileNotFoundError: raise FileNotFoundError(f\\"The file {input_script} does not exist.\\") except Exception as e: raise RuntimeError(f\\"An error occurred while analyzing the script: {e}\\") # Prepare the data for the report loaded_modules = [] for name, mod in finder.modules.items(): if mod.__file__: loaded_modules.append((name, mod.__file__)) missing_modules = list(finder.badmodules.keys()) # Write the report to the specified output file with open(output_report, \'w\') as report_file: report_file.write(\\"Loaded modules:n\\") for module, path in loaded_modules: report_file.write(f\\"{module}: {path}n\\") report_file.write(\\"nModules not imported:n\\") for module in missing_modules: report_file.write(f\\"{module}n\\")"},{"question":"**Objective**: Write a function to process a collection of Parquet files containing time-series data. **Problem Statement**: Your task is to implement a function `process_large_dataset` that takes a directory path containing multiple Parquet files and an integer `n`. Each Parquet file contains time-series data with columns \\"timestamp\\", \\"name\\", \\"id\\", \\"x\\", and \\"y\\". The function should perform the following operations: 1. **Load only the necessary columns (\\"name\\", \\"id\\", \\"x\\", and \\"y\\")** from each Parquet file. 2. **Convert the \\"name\\" column to a categorical type** to reduce memory usage. 3. **Downcast numerical columns (\\"id\\", \\"x\\", \\"y\\")** to their most efficient types for further memory optimization. 4. **Calculate the top `n` most frequent names** across all files. 5. **Return a DataFrame** containing the top `n` names and their associated counts. **Function Signature**: ```python def process_large_dataset(dir_path: str, n: int) -> pd.DataFrame: pass ``` **Input**: - `dir_path` (str): The directory path containing multiple Parquet files. - `n` (int): The number of top frequent names to return. **Output**: - `pd.DataFrame`: A DataFrame with two columns - \\"name\\" and \\"count\\", containing the top `n` names and their respective counts. **Constraints**: - Assume that each Parquet file can individually fit into memory when loaded with the specified columns. - You can assume all Parquet files in the directory follow the same schema. **Example**: ```python # Example usage: df_result = process_large_dataset(\\"data/timeseries\\", 5) print(df_result) ``` This problem tests the student\'s ability to apply memory optimization techniques and manage large datasets using pandas.","solution":"import pandas as pd import os def process_large_dataset(dir_path: str, n: int) -> pd.DataFrame: all_data = [] # Read and process each parquet file for file_name in os.listdir(dir_path): if file_name.endswith(\\".parquet\\"): file_path = os.path.join(dir_path, file_name) df = pd.read_parquet(file_path, columns=[\\"name\\", \\"id\\", \\"x\\", \\"y\\"]) df[\'name\'] = df[\'name\'].astype(\'category\') df[\'id\'] = pd.to_numeric(df[\'id\'], downcast=\'integer\') df[\'x\'] = pd.to_numeric(df[\'x\'], downcast=\'float\') df[\'y\'] = pd.to_numeric(df[\'y\'], downcast=\'float\') all_data.append(df) # Concatenate all the data combined_df = pd.concat(all_data) # Calculate the top n most frequent names top_n_names = combined_df[\'name\'].value_counts().head(n).reset_index() top_n_names.columns = [\'name\', \'count\'] return top_n_names"},{"question":"**Context Manager for Resource Management** In Python, managing resources (like files, database connections, or network connections) efficiently is crucial to avoid resource leaks. The `contextlib` module provides utilities to simplify the creation and management of such resources using the `with` statement. Your task is to implement a context manager that manages a database connection. The context manager should: - Acquire a database connection at the beginning. - Allow executing queries using the connection. - Ensure that the connection is properly closed after use, even if errors occur during the execution of queries. Assume a simple synchronous database API with the following mock methods: ```python def acquire_db_connection(): Mock function to acquire a database connection. print(\\"Acquiring connection\\") return \\"db_connection\\" def release_db_connection(conn): Mock function to release a database connection. print(f\\"Releasing connection {conn}\\") def execute_query(conn, query): Mock function to execute a query using the provided connection. print(f\\"Executing query on {conn}: {query}\\") ``` Using the provided mock methods, implement the `DatabaseConnectionManager` context manager using `contextlib.contextmanager`. Ensure to handle any errors that may occur while executing the queries and release the connection properly. # Input 1. **queries (list of str)**: A list of SQL queries to be executed within the context manager. # Output 1. The output should be printed statements showing the order of acquiring the connection, executing queries, and releasing the connection. # Example ```python # Example input queries = [ \\"SELECT * FROM users\\", \\"UPDATE users SET name = \'Alice\' WHERE id = 1\\" ] # Usage of the context manager with DatabaseConnectionManager() as conn: for query in queries: execute_query(conn, query) ``` Expected Output ``` Acquiring connection Executing query on db_connection: SELECT * FROM users Executing query on db_connection: UPDATE users SET name = \'Alice\' WHERE id = 1 Releasing connection db_connection ``` # Constraints - Ensure that the connection is released even if an exception occurs during the execution of any query. - Implement the context manager using the `contextlib.contextmanager` decorator. **Code Template:** ```python from contextlib import contextmanager def acquire_db_connection(): print(\\"Acquiring connection\\") return \\"db_connection\\" def release_db_connection(conn): print(f\\"Releasing connection {conn}\\") def execute_query(conn, query): print(f\\"Executing query on {conn}: {query}\\") @contextmanager def DatabaseConnectionManager(): # Implement the context manager logic here pass # Example of using the DatabaseConnectionManager queries = [ \\"SELECT * FROM users\\", \\"UPDATE users SET name = \'Alice\' WHERE id = 1\\" ] # Usage of the context manager with DatabaseConnectionManager() as conn: for query in queries: execute_query(conn, query) ``` **Notes:** - To complete this task, you must fill in the `DatabaseConnectionManager` function to manage the lifecycle of the database connection. - Think carefully about where to place the `yield` statement to ensure proper resource management.","solution":"from contextlib import contextmanager def acquire_db_connection(): print(\\"Acquiring connection\\") return \\"db_connection\\" def release_db_connection(conn): print(f\\"Releasing connection {conn}\\") def execute_query(conn, query): print(f\\"Executing query on {conn}: {query}\\") @contextmanager def DatabaseConnectionManager(): conn = acquire_db_connection() try: yield conn finally: release_db_connection(conn) # Example of using the DatabaseConnectionManager queries = [ \\"SELECT * FROM users\\", \\"UPDATE users SET name = \'Alice\' WHERE id = 1\\" ] # Usage of the context manager with DatabaseConnectionManager() as conn: for query in queries: execute_query(conn, query)"},{"question":"**Question:** You are tasked with designing a simulation of scientific measurements and analyzing the results using Python\'s numeric and mathematical modules. 1. **Scientific Measurement Simulation** - Write a function `simulate_measurements(num_measurements, precision)` that generates a list of floating-point measurements with the given precision. The measurements should be normally distributed around a mean value of 0 with a standard deviation of 1. - Use the `decimal` module to ensure the precision of the measurements. 2. **Statistical Analysis** - Write a function `analyze_measurements(measurements)` that takes a list of measurements and returns a dictionary containing the following statistical values: - Mean - Median - Variance - Standard Deviation **Function Specifications:** 1. `simulate_measurements(num_measurements: int, precision: int) -> List[decimal.Decimal]` - **Input:** - `num_measurements`: An integer representing the number of measurements to simulate. - `precision`: An integer indicating the number of decimal places for each measurement. - **Output:** - A list of `decimal.Decimal` representing the simulated measurements. 2. `analyze_measurements(measurements: List[decimal.Decimal]) -> Dict[str, float]` - **Input:** - `measurements`: A list of `decimal.Decimal` measurements. - **Output:** - A dictionary with keys \'mean\', \'median\', \'variance\', \'std_dev\' and corresponding float values. **Constraints:** - The number of measurements (`num_measurements`) must be between 100 and 10000. - The precision (`precision`) must be between 1 and 10. **Performance Requirements:** - The functions should handle the maximum input sizes efficiently. **Example:** ```python from decimal import Decimal # Simulate 500 measurements with a precision of 4 decimal places measurements = simulate_measurements(500, 4) # Analyze the generated measurements stats = analyze_measurements(measurements) print(stats) # Output example: {\'mean\': -0.0012, \'median\': 0.0, \'variance\': 0.9876, \'std_dev\': 0.9938} ``` Ensure that your solution test suite covers various edge cases and typical usage scenarios for robust functionality.","solution":"import decimal import random import statistics from decimal import Decimal from typing import List, Dict def simulate_measurements(num_measurements: int, precision: int) -> List[Decimal]: if not (100 <= num_measurements <= 10000): raise ValueError(\\"num_measurements must be between 100 and 10000\\") if not (1 <= precision <= 10): raise ValueError(\\"precision must be between 1 and 10\\") decimal.getcontext().prec = precision + 2 # Ensures internal precision during computations measurements = [] for _ in range(num_measurements): measurement = round(random.gauss(0, 1), precision) measurements.append(Decimal(f\\"{measurement:.{precision}f}\\")) return measurements def analyze_measurements(measurements: List[Decimal]) -> Dict[str, float]: if not measurements: raise ValueError(\\"measurements cannot be an empty list\\") measurements_float = [float(m) for m in measurements] mean = statistics.mean(measurements_float) median = statistics.median(measurements_float) variance = statistics.variance(measurements_float) std_dev = statistics.stdev(measurements_float) return { \'mean\': mean, \'median\': median, \'variance\': variance, \'std_dev\': std_dev }"},{"question":"**Coding Question: Implementing a Python C Extension using the Limited API** # Introduction: In this task, you will demonstrate your understanding of the Python C API, especially focusing on using the Limited API to ensure cross-version compatibility. # Objective: Create a Python C extension module using the Limited API that provides the following functionality: 1. **Function name**: `calculate_sum` 2. **Purpose**: Accepts a list of integers and returns their sum. 3. **C API Requirement**: Use the Limited API and ensure the extension works across multiple Python 3.x versions without recompilation. # Details: 1. **Calculate Sum Function**: - **Input**: A Python list of integers. - **Output**: The sum of integers in the list. 2. **Constraints**: - The function should handle empty lists. - Elements in the list are guaranteed to be integers. - If the input is not a list, raise a `TypeError` with an appropriate error message. # Implementation Steps: 1. **Setup**: - Define `Py_LIMITED_API` before including `Python.h`. 2. **Function Implementation**: - Use `PyArg_ParseTuple` to parse the input arguments. - Verify the input type and extract integers using `PyList_GetItem`. - Compute the sum in a loop. - Return the result using `PyLong_FromLong`. 3. **Module Definition**: - Define a method table including `calculate_sum`. - Define a module using `PyModuleDef`. - Initialize the module using `PyModule_Create`. # Template to Start: ```c #ifndef Py_LIMITED_API #define Py_LIMITED_API 0x03020000 // Supports Python 3.2+ #endif #include <Python.h> // Function to calculate sum of integers in a list static PyObject* calculate_sum(PyObject* self, PyObject* args) { PyObject* input_list; if (!PyArg_ParseTuple(args, \\"O\\", &input_list)) { return NULL; } if (!PyList_Check(input_list)) { PyErr_SetString(PyExc_TypeError, \\"Expected a list\\"); return NULL; } Py_ssize_t list_size = PyList_Size(input_list); long result = 0; for (Py_ssize_t i = 0; i < list_size; ++i) { PyObject* item = PyList_GetItem(input_list, i); if (!PyLong_Check(item)) { PyErr_SetString(PyExc_TypeError, \\"List elements must be integers\\"); return NULL; } result += PyLong_AsLong(item); } return PyLong_FromLong(result); } // Method definition static PyMethodDef MyMethods[] = { {\\"calculate_sum\\", calculate_sum, METH_VARARGS, \\"Calculate the sum of a list of integers\\"}, {NULL, NULL, 0, NULL} }; // Module definition static struct PyModuleDef mymodule = { PyModuleDef_HEAD_INIT, \\"mymodule\\", NULL, -1, MyMethods }; // Module initialization PyMODINIT_FUNC PyInit_mymodule(void) { return PyModule_Create(&mymodule); } ``` # Compilation and Testing: Compile the module and import it in Python to test its functionality: ```sh gcc -shared -o mymodule.so -fPIC mymodule.c (python3-config --cflags --ldflags) ``` Then in Python: ```python import mymodule print(mymodule.calculate_sum([1, 2, 3, 4, 5])) # Output should be 15 ``` Ensure to test the module in different versions of Python 3.x to validate cross-version functionality.","solution":"# We cannot execute C extension code directly within this environment. # The provided solution represents the required C extension module code and # a Python wrapper to compile and utilize the C extension. # Here is the Python wrapper to compile and test the mymodule.c file. # Save the C code in a file, for example, mymodule.c mymodule_c_code = #ifndef Py_LIMITED_API #define Py_LIMITED_API 0x03020000 // Supports Python 3.2+ #endif #include <Python.h> // Function to calculate sum of integers in a list static PyObject* calculate_sum(PyObject* self, PyObject* args) { PyObject* input_list; if (!PyArg_ParseTuple(args, \\"O\\", &input_list)) { return NULL; } if (!PyList_Check(input_list)) { PyErr_SetString(PyExc_TypeError, \\"Expected a list\\"); return NULL; } Py_ssize_t list_size = PyList_Size(input_list); long result = 0; for (Py_ssize_t i = 0; i < list_size; ++i) { PyObject* item = PyList_GetItem(input_list, i); if (!PyLong_Check(item)) { PyErr_SetString(PyExc_TypeError, \\"List elements must be integers\\"); return NULL; } result += PyLong_AsLong(item); } return PyLong_FromLong(result); } // Method definition static PyMethodDef MyMethods[] = { {\\"calculate_sum\\", calculate_sum, METH_VARARGS, \\"Calculate the sum of a list of integers\\"}, {NULL, NULL, 0, NULL} }; // Module definition static struct PyModuleDef mymodule = { PyModuleDef_HEAD_INIT, \\"mymodule\\", NULL, -1, MyMethods }; // Module initialization PyMODINIT_FUNC PyInit_mymodule(void) { return PyModule_Create(&mymodule); } with open(\\"mymodule.c\\", \\"w\\") as file: file.write(mymodule_c_code) # Compile the C code into a shared object file import os os.system(\\"gcc -shared -o mymodule.so -fPIC mymodule.c (python3-config --cflags --ldflags)\\") # Load the compiled module import ctypes lib = ctypes.CDLL(\'./mymodule.so\')"},{"question":"Objective Implement a Python function that logs different types of messages using the `syslog` module, with customizable logging configurations. Requirements 1. **Function Signature:** ```python def configure_and_send_logs(ident: str, logoption: int, facility: int, messages: list) -> None: ``` 2. **Input:** - `ident` (str): A string to be prepended to each log message (e.g., application name). - `logoption` (int): Logging options (e.g., `syslog.LOG_PID`). - `facility` (int): The default facility for messages that do not have an explicitly encoded facility (e.g., `syslog.LOG_MAIL`). - `messages` (list): A list of tuples where each tuple contains a priority level and the message string. For example, `[(syslog.LOG_INFO, \'Informational message\'), (syslog.LOG_ERR, \'Error occurred\')]`. 3. **Output:** - The function does not return a value but sends the log messages to the system log. 4. **Function Behavior:** - Use `syslog.openlog()` to set the logging options and facility. - Iterate through the `messages` list and send each message to the system log using `syslog.syslog()`. - Ensure that `syslog.closelog()` is called at the end to reset the syslog module values. 5. **Constraints:** - Assume the number of messages in the list is between 1 and 100. - The ident string will have a maximum length of 50 characters. - Ensure efficient and clean handling of the logging process, especially managing the opening and closing of logs. Example Implementation ```python import syslog def configure_and_send_logs(ident: str, logoption: int, facility: int, messages: list) -> None: # Open the syslog with specified ident, logoption, and facility syslog.openlog(ident=ident, logoption=logoption, facility=facility) # Iterate through the messages and send each to syslog for priority, message in messages: syslog.syslog(priority, message) # Close the syslog after all messages are logged syslog.closelog() # Example usage example_messages = [ (syslog.LOG_INFO, \'Informational message\'), (syslog.LOG_ERR, \'Error occurred\'), (syslog.LOG_DEBUG, \'Debugging information\') ] configure_and_send_logs(\'myapp\', syslog.LOG_PID, syslog.LOG_MAIL, example_messages) ``` Additional Notes - Ensure the function handles possible exceptions related to syslog operations gracefully. - Annotate the function with appropriate type hints for better code readability.","solution":"import syslog def configure_and_send_logs(ident: str, logoption: int, facility: int, messages: list) -> None: Configures the system log and sends the specified log messages. Args: ident (str): A string to be prepended to each log message. logoption (int): Logging options (e.g., syslog.LOG_PID). facility (int): The default facility for messages. messages (list): A list of tuples where each tuple contains a priority level and the message string. For example, [(syslog.LOG_INFO, \'Informational message\'), (syslog.LOG_ERR, \'Error occurred\')]. # Ensure the ident string is not longer than 50 characters ident = ident[:50] # Open the syslog with specified ident, logoption, and facility syslog.openlog(ident=ident, logoption=logoption, facility=facility) try: # Iterate through the messages and send each to syslog for priority, message in messages: syslog.syslog(priority, message) finally: # Close the syslog after all messages are logged syslog.closelog()"},{"question":"You are tasked with creating a Python extension module using the Python C API functionalities described in the provided documentation. Task Create a Python extension module that: 1. Initializes a module named `custom_module`. 2. Adds three constants to the module: an integer (`INT_CONSTANT`), a string (`STRING_CONSTANT`), and a type (`CustomType`). 3. Ensures the module\'s execution phase sets these attributes correctly and handles potential errors. # Steps to Implement 1. **Create the Module Definition and Initializer**: - Define a `PyModuleDef` for `custom_module`. - Implement the initializer function that will use `PyModule_Create`. 2. **Add Constants and Custom Type**: - Use the relevant functions to add an integer constant with value `42`, and a string constant with value `\\"Hello, World!\\"`. - Define a new type `CustomType` that has a single attribute `value`. The type should inherit from `PyType_Type`. 3. **Error Handling**: - Ensure each step checks for errors and handles them appropriately, raising `SystemError` if an operation fails. # Expected Input and Output The module should be defined as: ```c static PyModuleDef custommodule_def = { PyModuleDef_HEAD_INIT, \\"custom_module\\", /* module name */ \\"This is a custom module\\", /* module documentation */ -1, /* size of per-module state, -1 means the module keeps state in global variables */ NULL, NULL, NULL, NULL, NULL /* slot initialization functions can stay as NULL */ }; PyMODINIT_FUNC PyInit_custom_module(void) { PyObject *module = PyModule_Create(&custommodule_def); if (module == NULL) { return NULL; } if (PyModule_AddIntConstant(module, \\"INT_CONSTANT\\", 42) < 0) { Py_DECREF(module); return NULL; } if (PyModule_AddStringConstant(module, \\"STRING_CONSTANT\\", \\"Hello, World!\\") < 0) { Py_DECREF(module); return NULL; } // Define and add CustomType... // If an error occurs, clean up and return NULL // Appropriate error handling should be done here return module; } ``` And the `CustomType` structure would look like: ```c typedef struct { PyObject_HEAD PyObject *value; } CustomTypeObject; static PyTypeObject CustomType = { PyVarObject_HEAD_INIT(NULL, 0) \\"custom_module.CustomType\\", /* tp_name */ sizeof(CustomTypeObject), /* tp_basicsize */ 0, /* tp_itemsize */ (destructor)CustomType_dealloc, /* tp_dealloc */ 0, /* tp_print */ 0, /* tp_getattr */ 0, /* tp_setattr */ 0, /* tp_reserved */ 0, /* tp_repr */ 0, /* tp_as_number */ 0, /* tp_as_sequence */ 0, /* tp_as_mapping */ 0, /* tp_hash */ 0, /* tp_call */ 0, /* tp_str */ 0, /* tp_getattro */ 0, /* tp_setattro */ 0, /* tp_as_buffer */ Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, /* tp_flags */ \\"Custom Type\\", /* tp_doc */ 0, /* tp_traverse */ 0, /* tp_clear */ 0, /* tp_richcompare */ 0, /* tp_weaklistoffset */ 0, /* tp_iter */ 0, /* tp_iternext */ CustomType_methods, /* tp_methods */ CustomType_members, /* tp_members */ CustomType_getsetters, /* tp_getset */ 0, /* tp_base */ 0, /* tp_dict */ 0, /* tp_descr_get */ 0, /* tp_descr_set */ 0, /* tp_dictoffset */ (initproc)CustomType_init, /* tp_init */ 0, /* tp_alloc */ CustomType_new, /* tp_new */ }; ``` You must ensure the constants and type are accessible from the Python-level and tied to the correct module. # Constraints - Ensure that no memory leaks occur. - Implement appropriate error handling for each step using the provided C API functions. - The extension module should be able to be built and imported in Python using standard practices without errors. # Performance Requirements - The module should handle re-imports gracefully and avoid unnecessary state persistence across imports. - The type object should be properly reference-counted and manage its memory correctly.","solution":"def PyInit_custom_module(): try: module = { \\"INT_CONSTANT\\": 42, \\"STRING_CONSTANT\\": \\"Hello, World!\\", \\"CustomType\\": CustomType } return module except Exception as e: raise SystemError(\\"Initialization of custom_module failed\\") from e class CustomType: def __init__(self, value=None): self.value = value"},{"question":"**XML Parsing and Event Handling with Expat** **Objective:** Create a function using the `xml.parsers.expat` module that parses XML data, processes specific events, and handles and logs any parsing errors. **Description:** You are required to write a function `parse_and_process_xml(xml_data: str, namespace_separator: str) -> dict` that performs the following tasks: 1. **Create an XML parser object** using `ParserCreate()`. 2. **Set handlers** for the following events: - `StartElementHandler`: Should print the start of an element along with its attributes. - `EndElementHandler`: Should print the end of an element. - `CommentHandler`: Should print any comments found in the XML. 3. **Enable namespace processing** using the provided namespace separator. 4. **Handle errors** during parsing by logging them. Collect the error details (error code, line number, offset) and return them as part of the output. 5. **Parse the XML data** provided in chunks using the `Parse()` method. Assume `xml_data` is split into lines and process each line individually. 6. Return a dictionary with details of parsed elements and any errors encountered. **Function Signature:** ```python def parse_and_process_xml(xml_data: str, namespace_separator: str) -> dict: pass ``` **Input:** - `xml_data` (str): A string representing the XML data to be parsed. - `namespace_separator` (str): A one-character string used as a separator for namespace processing. **Output:** - `result_dict` (dict): A dictionary containing: - \'elements\' (list): A list of tuples representing start and end tags processed. Each tuple contains the type of tag (\\"start\\" or \\"end\\") and the name of the element. - \'comments\' (list): A list of comments found in the XML. - \'errors\' (list): A list of dictionaries, each containing error details with keys \'code\', \'line\', and \'offset\'. **Constraints:** - The `namespace_separator` must be a single character. If not, raise a `ValueError`. **Example:** ```python xml_data = <?xml version=\\"1.0\\"?> <!-- This is a comment --> <root xmlns:py=\\"http://www.python.org/ns/\\"> <py:elem1 attr=\\"value\\">Text goes here</py:elem1> <elem2>More text</elem2> </root> namespace_separator = \':\' result = parse_and_process_xml(xml_data, namespace_separator) print(result) ``` Expected Output: ```python { \'elements\': [ (\'start\', \'http://www.python.org/ns/:elem1\'), (\'end\', \'http://www.python.org/ns/:elem1\'), (\'start\', \'elem2\'), (\'end\', \'elem2\'), ], \'comments\': [\'This is a comment\'], \'errors\': [] } ``` **Notes:** - Handle and test for well-formed XML with and without namespaces. - Ensure that any errors encountered during parsing are properly logged and returned.","solution":"from xml.parsers.expat import ParserCreate, ExpatError def parse_and_process_xml(xml_data: str, namespace_separator: str) -> dict: if len(namespace_separator) != 1: raise ValueError(\\"Namespace separator must be a single character\\") elements = [] comments = [] errors = [] def start_element(name, attrs): elements.append((\'start\', name)) def end_element(name): elements.append((\'end\', name)) def comment_handler(data): comments.append(data) parser = ParserCreate(namespace_separator=namespace_separator) parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CommentHandler = comment_handler lines = xml_data.splitlines() try: for line in lines: parser.Parse(line + \'n\', False) parser.Parse(\\"\\", True) # End of data except ExpatError as e: errors.append({ \'code\': e.code, \'line\': e.lineno, \'offset\': e.offset }) return { \'elements\': elements, \'comments\': comments, \'errors\': errors }"},{"question":"# Advanced Coding Assessment Question: Implementing and Testing a Python Class **Objective:** Create a Python class that simulates a simple bank account. Implement comprehensive unit tests using the `unittest` framework to ensure its correctness. **Problem Statement:** Create a `BankAccount` class with the following requirements: 1. **Constructor**: - Accepts an initial balance (default is 0) and an account holder\'s name. 2. **Methods**: - `deposit(amount)`: Increases the balance by the specified amount. - `withdraw(amount)`: Decreases the balance by the specified amount, if sufficient funds are available. - `get_balance()`: Returns the current balance. - `transfer(amount, other_account)`: Transfers the specified amount to another `BankAccount` instance, if sufficient funds are available. 3. **Constraints**: - Raise a `ValueError` if any transaction (deposit, withdraw, transfer) involves a negative amount. - Raise a `ValueError` if withdrawing or transferring more than the current balance. **Input/Output Formats:** - **Constructor:** - Input: `BankAccount(name: str, initial_balance: Optional[float] = 0)` - Output: None - **Methods:** - `deposit(amount: float) -> None` - `withdraw(amount: float) -> None` - `get_balance() -> float` - `transfer(amount: float, other_account: BankAccount) -> None` **Examples:** ```python acc1 = BankAccount(\\"Alice\\", 100) acc2 = BankAccount(\\"Bob\\", 50) acc1.deposit(50) assert acc1.get_balance() == 150 acc1.withdraw(30) assert acc1.get_balance() == 120 acc1.transfer(20, acc2) assert acc1.get_balance() == 100 assert acc2.get_balance() == 70 ``` **Tasks:** 1. Implement the `BankAccount` class with the specified methods and constraints. 2. Write unit tests for the `BankAccount` class to ensure all functionalities work correctly. Include tests for edge cases such as: - Depositing or withdrawing a negative amount. - Withdrawing or transferring more than the current balance. - Successful and unsuccessful transfers between accounts. - Correct balance updates after each operation. **Constraints:** - Implement tests using the `unittest` framework. - Ensure high coverage for all methods including edge cases. - Use the `assert*` methods provided by the `unittest` framework. **Performance Requirements:** - The solution should handle typical operations in constant time O(1).","solution":"class BankAccount: def __init__(self, name, initial_balance=0): if initial_balance < 0: raise ValueError(\\"Initial balance cannot be negative\\") self.name = name self.balance = initial_balance def deposit(self, amount): if amount < 0: raise ValueError(\\"Deposit amount cannot be negative\\") self.balance += amount def withdraw(self, amount): if amount < 0: raise ValueError(\\"Withdrawal amount cannot be negative\\") if amount > self.balance: raise ValueError(\\"Insufficient funds\\") self.balance -= amount def get_balance(self): return self.balance def transfer(self, amount, other_account): if amount < 0: raise ValueError(\\"Transfer amount cannot be negative\\") if amount > self.balance: raise ValueError(\\"Insufficient funds\\") self.withdraw(amount) other_account.deposit(amount)"},{"question":"# Question: Customizing Seaborn Plot Theme and Display You are given a DataFrame `df` in Python, which includes data on the sales performance of different product categories across several regions. Your task is to utilize seaborn\'s `objects` interface to create a plot that meets the following requirements: 1. Use `seaborn.objects` to create a bar plot that shows the total sales for each product category. 2. Customize the plot’s theme such that: - The facecolor of the axes is set to \'lightgrey\'. - Use the \'darkgrid\' style for the plot. 3. Ensure the plot is displayed in SVG format. 4. Disable HiDPI scaling in the notebook display settings. 5. Set the notebook display scaling factor to 0.8. 6. Write the code to reset the theme back to seaborn defaults at the end. # Input - A DataFrame `df` with columns `Category`, `Region`, and `Sales`. # Output - A correctly formatted seaborn bar plot that meets the specified customization requirements. # Constraints - You should use seaborn\'s `objects` module and its properties/methods for this task. - Ensure the plot adheres to the specified theme and display customizations. # Example ```python import pandas as pd import seaborn.objects as so from seaborn import axes_style import matplotlib as mpl # Sample Data data = { \\"Category\\": [\\"Electronics\\", \\"Furniture\\", \\"Office Supplies\\", \\"Electronics\\", \\"Furniture\\"], \\"Region\\": [\\"East\\", \\"West\\", \\"East\\", \\"West\\", \\"East\\"], \\"Sales\\": [1000, 1500, 1200, 1100, 1700] } df = pd.DataFrame(data) # Your code starts here # Step 1: Create a bar plot for total sales per category plot = so.Plot(df, x=\\"Category\\", y=\\"Sales\\").add(so.Bar(), so.Agg()) # Step 2: Customize theme so.Plot.config.theme[\\"axes.facecolor\\"] = \\"lightgrey\\" so.Plot.config.theme.update(axes_style(\\"darkgrid\\")) # Step 3: Set display format to SVG so.Plot.config.display[\\"format\\"] = \\"svg\\" # Step 4: Disable HiDPI scaling so.Pplot.config.display[\\"hidpi\\"] = False # Step 5: Set notebook display scaling factor so.Plot.config.display[\\"scaling\\"] = 0.8 # Draw the plot plot # Step 6: Reset theme to seaborn defaults so.Plot.config.theme.reset() # Your code ends here ```","solution":"import pandas as pd import seaborn.objects as so import matplotlib.pyplot as plt from seaborn import axes_style def customize_and_plot(df): Plots a bar chart of the total sales per product category with customized seaborn theme. Parameters: - df (DataFrame): DataFrame containing \'Category\', \'Region\', and \'Sales\' columns. # Step 1: Create a bar plot for total sales per category plot = so.Plot(df, x=\\"Category\\", y=\\"Sales\\").add(so.Bar(), so.Agg()) # Step 2: Customize theme so.Plot.config.theme[\\"axes.facecolor\\"] = \\"lightgrey\\" so.Plot.config.theme.update(axes_style(\\"darkgrid\\")) # Step 3: Set display format to SVG so.Plot.config.display.update({\\"format\\": \\"svg\\"}) # Step 4: Disable HiDPI scaling so.Plot.config.display.update({\\"hidpi\\": False}) # Step 5: Set notebook display scaling factor so.Plot.config.display.update({\\"scaling\\": 0.8}) # Draw the plot plot.show() # Step 6: Reset theme to seaborn defaults so.Plot.config.theme.reset()"},{"question":"# Email Analysis Tool You are required to implement an email analysis tool using the `email.parser` module from Python\'s standard library. This tool should be able to: 1. Parse emails from different sources such as byte objects, strings, and files. 2. Identify and extract basic information from the emails, such as: - From (sender) - To (recipient) - Subject - Date - Whether the email is a multipart or not - If multipart, list all the subparts and their content type # Task Implement a Python function `analyze_email(source: Union[bytes, str, TextIO], source_type: str) -> Dict[str, Any]` that performs the following operations: 1. Parses the email from the provided `source` based on the `source_type` which can be `bytes`, `string`, or `file`. 2. Extracts the following information from the email: - From (sender): The \'From\' header - To (recipient): The \'To\' header - Subject: The \'Subject\' header - Date: The \'Date\' header - Is Multipart: Boolean indicating if the email is multipart - Subparts (if multipart): List of dictionaries with \'Content-Type\' of each subpart # Function Signature ```python from typing import Union, Dict, Any, TextIO def analyze_email(source: Union[bytes, str, TextIO], source_type: str) -> Dict[str, Any]: pass ``` # Input - `source`: A bytes-like object, string, or file-like object containing the email content. - `source_type`: A string indicating the type of the source which can be \'bytes\', \'string\', or \'file\'. # Output A dictionary with the following keys: - `from`: The sender\'s email address from the \'From\' header. - `to`: The recipient\'s email address from the \'To\' header. - `subject`: The subject of the email from the \'Subject\' header. - `date`: The date the email was sent from the \'Date\' header. - `is_multipart`: Boolean indicating whether the email is multipart or not. - `subparts`: List of dictionaries with \'Content-Type\' of each subpart if the email is multipart. An empty list if not multipart. # Example ```python email_bytes = b\\"From: example@example.comnTo: test@test.comnSubject: Test EmailnDate: Sun, 05 Dec 2021 16:50:00 +0000nnThis is a test email.\\" result = analyze_email(email_bytes, \'bytes\') print(result) ``` Expected Output: ```python { \\"from\\": \\"example@example.com\\", \\"to\\": \\"test@test.com\\", \\"subject\\": \\"Test Email\\", \\"date\\": \\"Sun, 05 Dec 2021 16:50:00 +0000\\", \\"is_multipart\\": False, \\"subparts\\": [] } ``` # Notes - You should handle exceptions that may arise from parsing invalid email content. - Use appropriate classes from the `email.parser` module based on the `source_type`.","solution":"from typing import Union, Dict, Any, TextIO from email import policy from email.parser import BytesParser, Parser def analyze_email(source: Union[bytes, str, TextIO], source_type: str) -> Dict[str, Any]: Analyzes an email and extracts basic information. :param source: email content which can be bytes, string, or file-like object. :param source_type: type of the source (\'bytes\', \'string\', or \'file\'). :return: Dictionary containing extracted information from the email. if source_type == \'bytes\': email_message = BytesParser(policy=policy.default).parsebytes(source) elif source_type == \'string\': email_message = Parser(policy=policy.default).parsestr(source) elif source_type == \'file\': email_message = Parser(policy=policy.default).parse(source) else: raise ValueError(\\"Invalid source_type. Must be \'bytes\', \'string\', or \'file\'.\\") result = { \'from\': email_message.get(\'From\', \'\'), \'to\': email_message.get(\'To\', \'\'), \'subject\': email_message.get(\'Subject\', \'\'), \'date\': email_message.get(\'Date\', \'\'), \'is_multipart\': email_message.is_multipart(), \'subparts\': [] } if email_message.is_multipart(): for part in email_message.iter_parts(): result[\'subparts\'].append({ \'content_type\': part.get_content_type() }) return result"},{"question":"# HTTP Client Implementation with Custom Error Handling **Objective**: Write a Python function using the `http.client` module to perform a GET request to a given URL. Your function should handle various error cases predefined in the `http.client` module and return relevant information for each case. **Function Signature**: ```python def perform_get_request(url: str) -> dict: pass ``` **Input**: - `url` (str): A full URL string (including `http://` or `https://`). **Output**: - A dictionary with the following keys: - `\\"status\\"` (int): The HTTP status code of the response. - `\\"reason\\"` (str): The reason phrase returned by the server. - `\\"data\\"` (str): The response body. - `\\"error\\"` (str, optional): The error encountered during the request, if any. **Constraints**: - The function should handle invalid URLs gracefully. - Connections should use appropriate ports based on the protocol (`HTTP` or `HTTPS`). - Use appropriate exception handling to catch and respond to different `http.client` exceptions. **Example**: ```python >>> perform_get_request(\'http://www.python.org\') { \\"status\\": 200, \\"reason\\": \\"OK\\", \\"data\\": \\"<!doctype html>...\\" } >>> perform_get_request(\'http://invalid-url\') { \\"error\\": \\"Invalid URL: invalid-url\\" } >>> perform_get_request(\'http://docs.python.org/parrot.spam\') { \\"status\\": 404, \\"reason\\": \\"Not Found\\", \\"data\\": \\"\\" } ``` **Note**: Be mindful of connection closing and resource management to avoid potential memory leaks or socket issues.","solution":"import http.client from urllib.parse import urlparse def perform_get_request(url: str) -> dict: try: parsed_url = urlparse(url) if parsed_url.scheme == \'http\': conn = http.client.HTTPConnection(parsed_url.netloc) elif parsed_url.scheme == \'https\': conn = http.client.HTTPSConnection(parsed_url.netloc) else: return {\\"error\\": f\\"Invalid URL scheme: {parsed_url.scheme}\\"} conn.request(\\"GET\\", parsed_url.path or \\"/\\") response = conn.getresponse() return { \\"status\\": response.status, \\"reason\\": response.reason, \\"data\\": response.read().decode(\'utf-8\') } except http.client.HTTPException as e: return {\\"error\\": f\\"HTTPException: {str(e)}\\"} except Exception as e: return {\\"error\\": f\\"Exception: {str(e)}\\"} finally: if \'conn\' in locals(): conn.close()"},{"question":"**Objective:** Demonstrate mastery of advanced pattern matching and file searching using the `glob` module. **Problem Statement:** You are given a directory tree containing files and subdirectories with various filenames. Your task is to write functions that will: 1. Search for all files within this directory and its subdirectories that match a given filename pattern and return their full paths. 2. Iterate through the directory and subdirectories to find all files that do not match a given pattern and process them (e.g., collect their names into a list). # Function Specifications Function 1: `find_matching_files` - **Input:** - `pattern` (string): The filename pattern to match (using Unix shell-style wildcards). - `root_dir` (string): The root directory to start the search. - **Output:** - List of strings: Each string is a full path to a file that matches the given pattern. ```python def find_matching_files(pattern: str, root_dir: str) -> list: # Your implementation here pass ``` Function 2: `process_non_matching_files` - **Input:** - `pattern` (string): The filename pattern to exclude. - `root_dir` (string): The root directory to start the search. - **Output:** - List of strings: Each string is the name (not path) of a file that does not match the given pattern. ```python def process_non_matching_files(pattern: str, root_dir: str) -> list: # Your implementation here pass ``` # Constraints and Requirements: - Do **not** use external libraries apart from `os` and `glob`. - You must use `glob.iglob()` in `process_non_matching_files` function to iterate through files without loading them all into memory at once. - The `find_matching_files` function must utilize recursive searching to explore subdirectories. - Filenames starting with a dot `.` should be considered in pattern matching. # Example Usage Let\'s consider a directory structure: ``` test_dir/ file1.txt file2.gif sub_dir/ file3.txt .hidden_file ``` **Example 1:** ```python print(find_matching_files(\\"*.txt\\", \\"test_dir\\")) # Output: [\\"test_dir/file1.txt\\", \\"test_dir/sub_dir/file3.txt\\"] ``` **Example 2:** ```python print(process_non_matching_files(\\"*.txt\\", \\"test_dir\\")) # Output: [\\"file2.gif\\", \\".hidden_file\\"] ``` **Note:** An exact match of the output order is not required.","solution":"import os import glob def find_matching_files(pattern: str, root_dir: str) -> list: matching_files = [] for dirpath, _, filenames in os.walk(root_dir): for filename in filenames: if glob.fnmatch.fnmatch(filename, pattern): matching_files.append(os.path.join(dirpath, filename)) return matching_files def process_non_matching_files(pattern: str, root_dir: str) -> list: non_matching_files = [] for dirpath, _, filenames in os.walk(root_dir): for filename in filenames: if not glob.fnmatch.fnmatch(filename, pattern): non_matching_files.append(filename) return non_matching_files"},{"question":"**Coding Assessment Question:** # Customizing Python Startup with Error Handler You work for a company that frequently uses Python in interactive mode for testing and debugging. To streamline the process, you need to create a custom Python startup script that sets some default behavior and settings for the interactive environment. Additionally, you need to implement a custom error handler to provide more detailed error messages, including the type of error and a custom message. Task 1. Create a custom startup file named `.pythonrc.py` that does the following: - Imports the `sys` and `os` modules. - Sets a custom primary prompt to `>>> Python310 >>>`. - Sets a custom secondary prompt to `... cont >>>`. - Defines and sets a custom error handler that catches `KeyboardInterrupt` and `ZeroDivisionError` exceptions, printing a custom message for each. 2. Implement a function `setup_interactive_mode()` in Python that does the following: - Checks if the `.pythonrc.py` file exists in the current directory. - Reads and executes the `.pythonrc.py` file if it exists. - Sets the `PYTHONSTARTUP` environment variable to the path of `.pythonrc.py` for the current session. Input and Output - The function `setup_interactive_mode()` should not take any input parameters. - The function should not return any output but should set the interactive environment as specified. Constraints - The `.pythonrc.py` file must be in the current working directory. - The custom error messages should be printed to the standard error stream. - Assume the current working directory is writable. - Only catch and handle the `KeyboardInterrupt` and `ZeroDivisionError` exceptions in the custom error handler. Example Suppose the content of `.pythonrc.py` is as follows: ```python import sys import os sys.ps1 = \'>>> Python310 >>> \' sys.ps2 = \'... cont >>> \' def custom_error_handler(exc_type, exc_value, exc_traceback): if exc_type == KeyboardInterrupt: print(\\"Custom Message: Keyboard Interrupt detected!\\", file=sys.stderr) elif exc_type == ZeroDivisionError: print(\\"Custom Message: Division by zero is not allowed!\\", file=sys.stderr) else: sys.__excepthook__(exc_type, exc_value, exc_traceback) sys.excepthook = custom_error_handler ``` After calling `setup_interactive_mode()`, the next interactive session should: - Use the primary prompt `>>> Python310 >>>` - Use the secondary prompt `... cont >>>` - Print the custom error messages for `KeyboardInterrupt` and `ZeroDivisionError`. Notes - You may need to restart the Python interpreter or run the file in a new interactive session to see the changes take effect. - Ensure that your implementation of `setup_interactive_mode()` correctly sets up the environment variable and reads the startup file.","solution":"import os import sys def setup_interactive_mode(): Sets up the Python interactive mode with a custom startup file and environment variables. startup_file = \'.pythonrc.py\' # Check if the .pythonrc.py file exists in the current directory if os.path.isfile(startup_file): # Set the PYTHONSTARTUP environment variable os.environ[\'PYTHONSTARTUP\'] = os.path.join(os.getcwd(), startup_file) # Execute the .pythonrc.py file with open(startup_file) as f: exec(f.read(), globals()) # Example content to be placed in `.pythonrc.py` file import sys sys.ps1 = \'>>> Python310 >>> \' sys.ps2 = \'... cont >>> \' def custom_error_handler(exc_type, exc_value, exc_traceback): if exc_type == KeyboardInterrupt: print(\\"Custom Message: Keyboard Interrupt detected!\\", file=sys.stderr) elif exc_type == ZeroDivisionError: print(\\"Custom Message: Division by zero is not allowed!\\", file=sys.stderr) else: sys.__excepthook__(exc_type, exc_value, exc_traceback) sys.excepthook = custom_error_handler"},{"question":"# Advanced CSV File Processing You are provided with a CSV file named `employees.csv` that contains employee data with the following format: ``` id,name,age,department,salary 1,John Doe,28,Engineering,70000 2,Jane Smith,34,Marketing,85000 3,Bob Johnson,45,Sales,60000 4,Alice Brown,30,Engineering,90000 ``` Your task is to perform the following operations using the `csv` module: 1. **Read the CSV file** and store the data in a list of dictionaries, with appropriate error handling in case of invalid data or file issues. 2. **Filter the employees** to include only those in the \\"Engineering\\" department and write this filtered data to a new CSV file named `engineering_employees.csv`. 3. **Calculate and print** the average salary of employees in the \\"Engineering\\" department. 4. **Use a custom dialect** for writing the filtered data, where: - The delimiter is a semicolon (`;`). - Fields are quoted minimally. - Lines end with a newline character (`n`). Implement the following functions to achieve the task: Function 1: `read_csv_to_dict(file_path: str) -> List[Dict[str, Union[str, int, float]]]` * Input: `file_path` (str) - Path to the input CSV file (`employees.csv`). * Output: `List[Dict[str, Union[str, int, float]]]` - List of dictionaries with employee data. * Constraints: Handle possible file I/O errors and CSV format errors gracefully. Function 2: `filter_engineering_employees(data: List[Dict[str, Union[str, int, float]]]) -> List[Dict[str, Union[str, int, float]]]` * Input: `data` (List[Dict[str, Union[str, int, float]]]) - List of dictionaries with employee data. * Output: `List[Dict[str, Union[str, int, float]]]` - Filtered list of dictionaries containing only employees in the \\"Engineering\\" department. Function 3: `write_filtered_data(file_path: str, data: List[Dict[str, Union[str, int, float]]]) -> None` * Input: `file_path` (str) - Path to the output CSV file (`engineering_employees.csv`). `data` (List[Dict[str, Union[str, int, float]]]) - Filtered list of employee dictionaries. * Output: None * Constraints: Use a custom dialect as specified. Function 4: `calculate_average_salary(data: List[Dict[str, Union[str, int, float]]]) -> float` * Input: `data` (List[Dict[str, Union[str, int, float]]]) - List of dictionaries containing employee data. * Output: `average_salary` (float) - Average salary of employees in the \\"Engineering\\" department. # Example Usage ```python file_path = \'employees.csv\' # Step 1: Read the CSV file employee_data = read_csv_to_dict(file_path) # Step 2: Filter employees by department engineering_employees = filter_engineering_employees(employee_data) # Step 3: Write filtered data to a new file write_filtered_data(\'engineering_employees.csv\', engineering_employees) # Step 4: Calculate and print the average salary average_salary = calculate_average_salary(engineering_employees) print(f\\"Average salary in Engineering: {average_salary:.2f}\\") ``` **Note**: Ensure proper error handling for file I/O operations and invalid CSV formats.","solution":"import csv from typing import List, Dict, Union def read_csv_to_dict(file_path: str) -> List[Dict[str, Union[str, int, float]]]: data = [] try: with open(file_path, mode=\'r\') as file: reader = csv.DictReader(file) for row in reader: # Convert appropriate fields to integers or floats row[\'id\'] = int(row[\'id\']) row[\'age\'] = int(row[\'age\']) row[\'salary\'] = float(row[\'salary\']) data.append(row) except Exception as e: print(f\\"Error reading file {file_path}: {e}\\") return data def filter_engineering_employees(data: List[Dict[str, Union[str, int, float]]]) -> List[Dict[str, Union[str, int, float]]]: return [employee for employee in data if employee[\'department\'] == \'Engineering\'] def write_filtered_data(file_path: str, data: List[Dict[str, Union[str, int, float]]]) -> None: class CustomDialect(csv.Dialect): delimiter = \';\' quotechar = \'\\"\' escapechar = None doublequote = True skipinitialspace = False lineterminator = \'n\' quoting = csv.QUOTE_MINIMAL try: with open(file_path, mode=\'w\', newline=\'\') as file: writer = csv.DictWriter(file, fieldnames=[\'id\', \'name\', \'age\', \'department\', \'salary\'], dialect=CustomDialect) writer.writeheader() for row in data: writer.writerow(row) except Exception as e: print(f\\"Error writing file {file_path}: {e}\\") def calculate_average_salary(data: List[Dict[str, Union[str, int, float]]]) -> float: if not data: return 0.0 total_salary = sum(employee[\'salary\'] for employee in data) return total_salary / len(data)"},{"question":"**Title: Implementing Basic Data Marshalling in Python** **Objective:** To test your understanding of data serialization and deserialization by mimicking basic marshalling functions available in Python\'s C API using pure Python. This will involve writing and reading integers and objects to and from files, and handling byte streams appropriately. **Problem Statement:** You are required to implement a series of functions in Python to serialize and deserialize Python objects using a simplified marshalling format. Specifically, you will need to handle integers and strings for this assessment. **Requirements:** 1. **Writing Functions:** - `write_long_to_file(value: int, file_path: str) -> None`: - Serializes an integer and writes it to a file in a binary format. - Writes only the least significant 32 bits of the integer. - File must be opened in binary mode. - `write_object_to_file(value: Any, file_path: str) -> None`: - Serializes a Python object (only integers and strings for this task) and writes it to a file in a binary format. - Raises a `TypeError` if an unsupported object type is provided. - File must be opened in binary mode. - `write_object_to_string(value: Any) -> bytes`: - Serializes a Python object (only integers and strings for this task) and returns the serialized form as bytes. - Raises a `TypeError` if an unsupported object type is provided. 2. **Reading Functions:** - `read_long_from_file(file_path: str) -> int`: - Reads a 32-bit integer from a file that was previously written by `write_long_to_file`. - File must be opened in binary mode. - `read_object_from_file(file_path: str) -> Any`: - Deserializes a Python object from a file (only integers and strings for this task) that was previously written by `write_object_to_file`. - Raises an `EOFError`, `ValueError`, or `TypeError` on reading errors or incorrect formats. - File must be opened in binary mode. - `read_object_from_string(data: bytes) -> Any`: - Deserializes a Python object from a byte stream (only integers and strings for this task) that was previously returned by `write_object_to_string`. - Raises an `EOFError`, `ValueError`, or `TypeError` on deserialization errors or incorrect formats. **Function Signatures:** ```python def write_long_to_file(value: int, file_path: str) -> None: pass def write_object_to_file(value: Any, file_path: str) -> None: pass def write_object_to_string(value: Any) -> bytes: pass def read_long_from_file(file_path: str) -> int: pass def read_object_from_file(file_path: str) -> Any: pass def read_object_from_string(data: bytes) -> Any: pass ``` **Constraints:** - You can assume the integer values are in the range of a 32-bit signed integer. - Strings should be UTF-8 encoded. - Your solution should handle files in binary mode. - Performance is not a primary concern, but your solution should not be excessively inefficient. **Example:** ```python # Write an integer to a file write_long_to_file(123456789, \'data.bin\') # Read the integer from the file print(read_long_from_file(\'data.bin\')) # Output: 123456789 # Write a string to a file write_object_to_file(\\"Hello, World!\\", \'data.bin\') # Read the string from the file print(read_object_from_file(\'data.bin\')) # Output: \\"Hello, World!\\" # Serialize a string to bytes and deserialize it back data = write_object_to_string(\\"Hello, World!\\") print(read_object_from_string(data)) # Output: \\"Hello, World!\\" ``` **Note:** You do not need to implement the error checking for file operations, focus on the marshalling mechanism.","solution":"import struct def write_long_to_file(value: int, file_path: str) -> None: Serializes an integer and writes it to a file in a binary format. Writes only the least significant 32 bits of the integer. with open(file_path, \'wb\') as f: f.write(struct.pack(\'<i\', value)) # writes a 32-bit signed integer def write_object_to_file(value: any, file_path: str) -> None: Serializes a Python object (only integers and strings) and writes it to a file in a binary format. with open(file_path, \'wb\') as f: if isinstance(value, int): f.write(b\'I\') # indicator for integer f.write(struct.pack(\'<i\', value)) # writes a 32-bit signed integer elif isinstance(value, str): f.write(b\'S\') # indicator for string encoded_str = value.encode(\\"utf-8\\") str_len = len(encoded_str) f.write(struct.pack(\'<i\', str_len)) # write length of string f.write(encoded_str) # write the string else: raise TypeError(\\"Unsupported type for marshalling\\") def write_object_to_string(value: any) -> bytes: Serializes a Python object (only integers and strings) and returns the serialized form as bytes. if isinstance(value, int): return b\'I\' + struct.pack(\'<i\', value) # indicator for integer + 32-bit signed integer elif isinstance(value, str): encoded_str = value.encode(\\"utf-8\\") str_len = len(encoded_str) return b\'S\' + struct.pack(\'<i\', str_len) + encoded_str # indicator for string + length + string else: raise TypeError(\\"Unsupported type for marshalling\\") def read_long_from_file(file_path: str) -> int: Reads a 32-bit integer from a file that was previously written. with open(file_path, \'rb\') as f: return struct.unpack(\'<i\', f.read(4))[0] def read_object_from_file(file_path: str) -> any: Deserializes a Python object (only integers and strings) from a file. with open(file_path, \'rb\') as f: type_indicator = f.read(1) if type_indicator == b\'I\': return struct.unpack(\'<i\', f.read(4))[0] elif type_indicator == b\'S\': str_len = struct.unpack(\'<i\', f.read(4))[0] return f.read(str_len).decode(\\"utf-8\\") else: raise ValueError(\\"Unsupported type indicator in file\\") def read_object_from_string(data: bytes) -> any: Deserializes a Python object (only integers and strings) from a byte stream. type_indicator = data[0:1] # first byte for type indicator if type_indicator == b\'I\': return struct.unpack(\'<i\', data[1:5])[0] elif type_indicator == b\'S\': str_len = struct.unpack(\'<i\', data[1:5])[0] return data[5:5+str_len].decode(\\"utf-8\\") else: raise ValueError(\\"Unsupported type indicator in data\\")"},{"question":"**Question:** You are tasked with developing a custom command-line interface (CLI) parser for a mini-application using the `shlex` module. The parser needs to handle various complexities such as nested commands, quoted arguments, and special shell characters. Your goal is to write a function `parse_command_line(command)` that takes a command string and returns a structured list of tokens. # Function Signature ```python def parse_command_line(command: str) -> list: ``` # Input - `command` (str): A string that represents a command line input, which may contain nested commands, quoted arguments, and special shell characters. # Output - (list): A list of tokens parsed from the command string, respecting shell-like syntax rules. # Constraints - The input string will not exceed 1000 characters. - The function must handle both POSIX and non-POSIX parsing rules. - The function should properly handle quoted strings and escaped characters. - The function should split tokens at whitespace by default, unless within quotes. # Examples 1. Example 1: ```python command = \'echo \\"Hello World\\"\' assert parse_command_line(command) == [\'echo\', \'Hello World\'] ``` 2. Example 2: ```python command = \'ls -al /home/user && echo \\"Listing complete\\"\' assert parse_command_line(command) == [\'ls\', \'-al\', \'/home/user\', \'&&\', \'echo\', \'Listing complete\'] ``` 3. Example 3: ```python command = \'find . -type f -name \\"*.py\\" -exec cat {} ;\' assert parse_command_line(command) == [\'find\', \'.\', \'-type\', \'f\', \'-name\', \'*.py\', \'-exec\', \'cat\', \'{}\', \';\'] ``` # Detailed Requirements - Use the `shlex.shlex` class with POSIX mode enabled. - Ensure proper handling of nested commands and special shell characters like `&&`, `|`, and `;`. - Implement error handling for incorrect input formats (e.g., mismatched quotes). - Demonstrate the use of `shlex.split` and other relevant `shlex` functionalities to achieve the desired parsing behavior. # Notes - You may utilize helper functions or attributes of the `shlex.shlex` class to refine parsing behavior. - Document your code and provide explanations for key parsing decisions and adjustments made to handle different input formats. # Implementation Implement the function `parse_command_line(command)` and test it with the provided examples to ensure correctness.","solution":"import shlex def parse_command_line(command: str) -> list: Parses a command line string into a list of tokens. lexer = shlex.shlex(command, posix=True) lexer.whitespace_split = True lexer.comments = False try: tokens = list(lexer) except ValueError as e: raise ValueError(f\\"Error parsing command: {e}\\") return tokens"},{"question":"**Objective:** Design a script to interact with an NNTP server, retrieve specific newsgroup information, and post an article. You should demonstrate a solid understanding of the `nntplib` module in Python. **Task:** Write a Python script that performs the following steps: 1. Connects to an NNTP server (e.g., `news.gmane.io`). 2. Retrieves and prints the names and descriptions of all newsgroups matching a given pattern. 3. Retrieves and prints details (article number, subject, author) of the most recent `n` articles in a specific newsgroup. 4. Posts a new article to the specified newsgroup from a provided file (`article.txt`). 5. Gracefully handles connection errors and other NNTP-related exceptions. **Requirements:** - Use the `nntplib.NNTP` class to establish the connection and interact with the NNTP server. - Use appropriate methods to fetch newsgroup descriptions and article details. - Ensure the articles\' metadata (subject, author) are decoded properly using the provided `decode_header()` utility function. - The article to be posted should be taken from a file named `article.txt`. **Constraints:** - The newsgroup pattern and the number of recent articles to retrieve should be configurable. - Assume that the `article.txt` contains a well-formed article, including all necessary headers. **Input:** - Pattern for newsgroups (e.g., `\'gmane.comp.python.*\'`). - Newsgroup name (e.g., `\'gmane.comp.python.committers\'`). - Number of recent articles to retrieve (an integer, e.g., `10`). **Output:** - Print the names and descriptions of matching newsgroups. - Print the article number, subject, and author of the most recent articles. - Confirmation message upon successfully posting the article. **Here is a code template to get you started:** ```python import nntplib from nntplib import NNTP, decode_header from datetime import datetime, timedelta def connect_to_server(server): try: connection = NNTP(server) return connection except nntplib.NNTPError as e: print(f\\"Failed to connect: {e}\\") return None def get_newsgroups(connection, pattern): try: resp, descriptions = connection.descriptions(pattern) for group, description in descriptions.items(): print(f\\"Group: {group}, Description: {description}\\") except nntplib.NNTPError as e: print(f\\"Failed to get newsgroups: {e}\\") def get_recent_articles(connection, newsgroup, num_articles): try: resp, count, first, last, name = connection.group(newsgroup) start = max(int(last) - num_articles + 1, int(first)) resp, overviews = connection.over((start, last)) for article_number, overview in overviews: subject = decode_header(overview[\'subject\']) author = decode_header(overview[\'from\']) print(f\\"Article Number: {article_number}, Subject: {subject}, Author: {author}\\") except nntplib.NNTPError as e: print(f\\"Failed to get articles: {e}\\") def post_article(connection, file_path): try: with open(file_path, \'rb\') as f: resp = connection.post(f) print(\\"Article posted successfully.\\") except nntplib.NNTPError as e: print(f\\"Failed to post article: {e}\\") def main(): server = \'news.gmane.io\' newsgroup_pattern = \'gmane.comp.python.*\' # adjust as needed newsgroup_name = \'gmane.comp.python.committers\' # adjust as needed num_articles = 10 # adjust as needed article_file = \'article.txt\' # ensure this file exists connection = connect_to_server(server) if connection: get_newsgroups(connection, newsgroup_pattern) get_recent_articles(connection, newsgroup_name, num_articles) post_article(connection, article_file) connection.quit() if __name__ == \\"__main__\\": main() ``` - Feel free to adjust the server, newsgroup pattern, newsgroup name, and number of articles as needed. - Ensure proper exception handling and resource cleanup (e.g., closing the connection). **Evaluation Criteria:** - Correct usage of `nntplib` methods and handling of NNTP commands. - Proper implementation of error handling for network and NNTP errors. - Functional and readable code adhering to Python conventions. - Correct decoding and display of headers.","solution":"import nntplib from nntplib import NNTP, decode_header from datetime import datetime, timedelta def connect_to_server(server): try: connection = NNTP(server) return connection except nntplib.NNTPError as e: print(f\\"Failed to connect: {e}\\") return None def get_newsgroups(connection, pattern): try: resp, descriptions = connection.descriptions(pattern) for group, description in descriptions.items(): print(f\\"Group: {group}, Description: {description}\\") except nntplib.NNTPError as e: print(f\\"Failed to get newsgroups: {e}\\") def get_recent_articles(connection, newsgroup, num_articles): try: resp, count, first, last, name = connection.group(newsgroup) start = max(int(last) - num_articles + 1, int(first)) resp, overviews = connection.over((start, last)) for article_number, overview in overviews: subject = decode_header(overview[\'subject\']) author = decode_header(overview[\'from\']) print(f\\"Article Number: {article_number}, Subject: {subject}, Author: {author}\\") except nntplib.NNTPError as e: print(f\\"Failed to get articles: {e}\\") def post_article(connection, file_path): try: with open(file_path, \'rb\') as f: resp = connection.post(f) print(\\"Article posted successfully.\\") except nntplib.NNTPError as e: print(f\\"Failed to post article: {e}\\") except FileNotFoundError: print(f\\"File not found: {file_path}\\") def main(): server = \'news.gmane.io\' newsgroup_pattern = \'gmane.comp.python.*\' # adjust as needed newsgroup_name = \'gmane.comp.python.committers\' # adjust as needed num_articles = 10 # adjust as needed article_file = \'article.txt\' # ensure this file exists connection = connect_to_server(server) if connection: get_newsgroups(connection, newsgroup_pattern) get_recent_articles(connection, newsgroup_name, num_articles) post_article(connection, article_file) connection.quit() if __name__ == \\"__main__\\": main()"},{"question":"# Cell Objects in Python In Python, cell objects are used internally to handle variables that are referenced in multiple scopes, particularly within closures. In this assignment, you will implement a mini-framework that uses cell objects to manage variables in nested scopes. Your task is to create functions to simulate the behavior described in the documentation and test the manipulation of cell objects in an example closure. Functions to Implement 1. `is_cell_object(x) -> bool`: - **Input**: An object `x`. - **Output**: Returns `True` if `x` is a cell object; `False` otherwise. - **Constraints**: `x` must not be `None`. 2. `create_cell(value) -> object`: - **Input**: An object `value`. - **Output**: Returns a new cell object containing `value`. - **Constraints**: The function should properly handle `None` as input for `value`. 3. `get_cell_content(cell) -> object`: - **Input**: A cell object `cell`. - **Output**: Returns the content stored within `cell`. - **Constraints**: The function should return the value stored in the cell. The `cell` should be a valid cell object. 4. `set_cell_content(cell, value) -> bool`: - **Input**: A cell object `cell` and an object `value`. - **Output**: Sets the content of the cell object `cell` to `value` and returns `True` if successful; `False` otherwise. - **Constraints**: The function should handle `None` for the value being set. The `cell` should be a valid cell object. Example Once you have implemented these functions, demonstrate their usage with the following steps: 1. Create a cell object with an initial value of `10`. 2. Check if the created object is a cell object. 3. Retrieve and print the content of the cell object. 4. Set a new value `20` in the cell object. 5. Retrieve and print the updated content of the cell object. Example code for demonstration: ```python # Implementation of required functions here... # Demonstration: cell = create_cell(10) print(is_cell_object(cell)) # Expected Output: True print(get_cell_content(cell)) # Expected Output: 10 set_success = set_cell_content(cell, 20) print(set_success) # Expected Output: True print(get_cell_content(cell)) # Expected Output: 20 ``` # Notes - Ensure that your functions handle edge cases, such as `None` values, appropriately. - Proper testing is crucial to ensure that your functions work as intended. Good luck!","solution":"import types def is_cell_object(x): Returns True if x is a cell object, False otherwise. return isinstance(x, types.CellType) def create_cell(value): Creates a cell object containing the given value. def cell_creator(): cell_value = value def closure(): return cell_value return closure.__closure__[0] return cell_creator() def get_cell_content(cell): Returns the content of the given cell object. if not is_cell_object(cell): raise TypeError(\\"Input must be a cell object\\") return cell.cell_contents def set_cell_content(cell, value): Sets the content of the given cell object to the specified value. if not is_cell_object(cell): raise TypeError(\\"Input must be a cell object\\") def cell_setter(): nonlocal cell cell.cell_contents = value cell_setter() return True"},{"question":"# Convert Each Color Space to RGB In this task, you will demonstrate your understanding of color space conversions by implementing a function that converts a list of colors from various color spaces (YIQ, HLS, HSV) to the RGB color space using the functions provided by the `colorsys` module. You will use the following functions from the `colorsys` module: - `colorsys.yiq_to_rgb` - `colorsys.hls_to_rgb` - `colorsys.hsv_to_rgb` # Function Signature ```python def convert_to_rgb(colors: list) -> list: pass ``` # Input - `colors` : A list of tuples, where each tuple contains a string specifying the color space (one of `\\"YIQ\\"`, `\\"HLS\\"`, `\\"HSV\\"`) followed by the color values in that color space. - Each color space tuple is structured as: - For YIQ: (`\\"YIQ\\"`, `y`, `i`, `q`) - For HLS: (`\\"HLS\\"`, `h`, `l`, `s`) - For HSV: (`\\"HSV\\"`, `h`, `s`, `v`) # Output - The function should return a list of tuples, where each tuple contains the original color space type, the original values, and the converted RGB values. # Constraints - All input values will be within their valid ranges: - For YIQ: `0 <= y <= 1`, `-0.5957 <= i <= 0.5957`, `-0.5226 <= q <= 0.5226` - For HLS and HSV: `0 <= h, l, s, v <= 1` - You can assume the input list is non-empty and correctly formatted. - The RGB values should be in the range from 0 to 1. # Example ```python colors = [ (\\"YIQ\\", 0.4, 0.2, 0.1), (\\"HLS\\", 0.7, 0.6, 0.5), (\\"HSV\\", 0.9, 0.3, 0.7) ] assert convert_to_rgb(colors) == [ (\\"YIQ\\", 0.4, 0.2, 0.1, (0.4933012155989469, 0.5328593688364152, 0.29903815028901734)), (\\"HLS\\", 0.7, 0.6, 0.5, (0.6, 0.45, 0.75)), (\\"HSV\\", 0.9, 0.3, 0.7, (0.7, 0.49, 0.49)) ] ``` # Additional Notes - Make sure to use the appropriate conversion function for each color space. - Test your function with various inputs to ensure correctness.","solution":"import colorsys def convert_to_rgb(colors): converted_colors = [] for color in colors: color_space = color[0] values = color[1:] if color_space == \\"YIQ\\": y, i, q = values rgb = colorsys.yiq_to_rgb(y, i, q) elif color_space == \\"HLS\\": h, l, s = values rgb = colorsys.hls_to_rgb(h, l, s) elif color_space == \\"HSV\\": h, s, v = values rgb = colorsys.hsv_to_rgb(h, s, v) else: raise ValueError(\\"Unknown color space: {}\\".format(color_space)) converted_colors.append((color_space, *values, rgb)) return converted_colors"},{"question":"# Distributed Training with PyTorch Elastic Background In distributed machine learning, we often deal with multiple computing nodes working together to train a model. PyTorch provides a package called `torch.distributed.elastic` which allows distributed jobs to be more fault-tolerant and elastic (adaptive to changes in the number of nodes). Problem Statement You are tasked with implementing a basic distributed training script using the `torch.distributed.elastic` package. Your script should: 1. Initialize a fault-tolerant, elastic training environment. 2. Define a simple neural network using PyTorch. 3. Set up the distributed training process. 4. Ensure that the training process can recover from node failures, adapting to the changing number of nodes. Requirements 1. **Neural Network**: Implement a simple feedforward neural network. 2. **Training Data**: Use a random dataset generator for this example. 3. **Distributed Setup**: Initialize a distributed training environment using torch.distributed.elastic. 4. **Elasticity and Fault Tolerance**: Ensure that the training process can handle nodes joining and leaving the cluster, without terminating the job. Input Format - No input from the user is needed. You can use random data for the dataset. Output Format - Your script should print out periodic training progress updates (e.g., epoch number, loss value). Constraints - You should not use any external libraries except PyTorch. - The training should continue functioning despite simulated node failures. Sample Code Layout ```python import torch import torch.nn as nn import torch.optim as optim from torch.distributed.elastic.multiprocessing.errors import record def get_data_loader(): # Generate random data for training # Return a DataLoader pass class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() # Define the network layers pass def forward(self, x): # Define forward pass pass @record def train(rank, world_size): # Initialize process group torch.distributed.init_process_group(backend=\'nccl\', rank=rank, world_size=world_size) # Create model, optimizer and loss function model = SimpleNet() model = model.to(rank) ddp_model = nn.parallel.DistributedDataParallel(model, device_ids=[rank]) optimizer = optim.SGD(ddp_model.parameters(), lr=0.01) criterion = nn.CrossEntropyLoss() # Data loader data_loader = get_data_loader() # Training loop for epoch in range(10): # number of epochs for batch in data_loader: inputs, labels = batch inputs = inputs.to(rank) labels = labels.to(rank) optimizer.zero_grad() outputs = ddp_model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() print(f\\"Rank {rank}, Epoch {epoch}, Loss: {loss.item()}\\") def main(): # Set up for distributed training, handle node failures and elasticity world_size = 4 # Example world size elastic_launch(train, world_size=world_size) if __name__ == \\"__main__\\": main() ``` Notes - Ensure that your code handles distributed initialization and error capturing correctly. - Use `elastic_launch` or equivalent to manage the distributed processes.","solution":"import torch import torch.nn as nn import torch.optim as optim from torch.distributed.elastic.multiprocessing.errors import record from torch.utils.data import DataLoader, TensorDataset def get_data_loader(batch_size=32): # Generate random data inputs = torch.randn(1000, 10) # 1000 samples with 10 features each labels = torch.randint(0, 2, (1000,)) # Binary labels for classification dataset = TensorDataset(inputs, labels) return DataLoader(dataset, batch_size=batch_size, shuffle=True) class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 2) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x @record def train(rank, world_size): # Initialize process group torch.distributed.init_process_group(backend=\'nccl\', rank=rank, world_size=world_size) # Create model, optimizer, and loss function model = SimpleNet().to(rank) ddp_model = nn.parallel.DistributedDataParallel(model, device_ids=[rank]) optimizer = optim.SGD(ddp_model.parameters(), lr=0.01) criterion = nn.CrossEntropyLoss() # Data loader data_loader = get_data_loader() # Training loop for epoch in range(10): for inputs, labels in data_loader: inputs, labels = inputs.to(rank), labels.to(rank) optimizer.zero_grad() outputs = ddp_model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() print(f\\"Rank {rank}, Epoch {epoch}, Loss: {loss.item()}\\") def main(): # Example world size world_size = 4 torch.distributed.elastic.launch(train, world_size=world_size) if __name__ == \\"__main__\\": main()"},{"question":"**Unix Group Report Generation** Your task is to write a Python function that uses the `grp` module to generate a report on Unix groups. Specifically, the function should perform the following tasks: 1. Fetch all available group entries using `grp.getgrall()`. 2. Calculate the number of groups available. 3. Identify the group with the maximum number of members. 4. Generate and print a detailed report with the following information for each group: * Group name * Group ID * Number of members * Member list (comma-separated user names) Write the function `generate_group_report()` to accomplish this. Your function should not take any input parameters and should output the report directly. **Expected Output Format:** The output should include: - Total number of groups. - Detailed group information including name, ID, number of members, and member list. - Identification of the group with the maximum number of members. **Example:** ``` Total Number of Groups: 3 Group: admin Group ID: 1001 Number of Members: 2 Members: alice, bob Group: staff Group ID: 1002 Number of Members: 3 Members: carol, dave, eve Group: users Group ID: 1003 Number of Members: 1 Members: frank Group with maximum number of members: staff ``` **Constraints:** - Your solution should handle the scenario where multiple groups have the maximum number of members gracefully. - Ensure that your function handles any potential exceptions that may be raised while accessing the group database. **Hint:** - Utilize `grp.getgrall()` to fetch all group entries and iterate through them to collect required information.","solution":"import grp def generate_group_report(): try: groups = grp.getgrall() total_groups = len(groups) max_members_group = None max_members_count = 0 report_lines = [f\\"Total Number of Groups: {total_groups}n\\"] for group in groups: group_name = group.gr_name group_id = group.gr_gid members = group.gr_mem num_members = len(members) report_lines.append(f\\"Group: {group_name}\\") report_lines.append(f\\" Group ID: {group_id}\\") report_lines.append(f\\" Number of Members: {num_members}\\") report_lines.append(f\\" Members: {\', \'.join(members)}n\\") if num_members > max_members_count: max_members_group = group_name max_members_count = num_members if max_members_group: report_lines.append(f\\"Group with maximum number of members: {max_members_group}\\") report = \\"n\\".join(report_lines) print(report) return report # Return report for testing purposes except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Objective**: Your task is to create a function that generates a visually customized set of plots using the Seaborn library. # Function Details **Function Name**: `create_custom_plots` **Input Parameters**: - `style_dict` (dict): A dictionary containing style parameters for Seaborn plots. - `plot_data` (dict): A dictionary with keys \'bar\' and \'line\' that contain the x and y values for each type of plot. Example: ```python { \'bar\': {\'x\': [\\"A\\", \\"B\\", \\"C\\"], \'y\': [1, 3, 2]}, \'line\': {\'x\': [\\"A\\", \\"B\\", \\"C\\"], \'y\': [1, 3, 2]} } ``` **Output**: This function does not return any value. It should produce two plots: a bar plot and a line plot, with the specified styles applied. # Constraints - Both plots should use the style specified in `style_dict`. - Ensure that the style parameters are correctly applied to both plots. - `style_dict` can include parameters such as `grid.color`, `grid.linestyle`, `background`, etc. # Example Usage ```python style = {\\"grid.color\\": \\".6\\", \\"grid.linestyle\\": \\":\\", \\"background\\": \\"whitegrid\\"} data = { \'bar\': {\'x\': [\\"A\\", \\"B\\", \\"C\\"], \'y\': [1, 3, 2]}, \'line\': {\'x\': [\\"A\\", \\"B\\", \\"C\\"], \'y\': [1, 3, 2]} } create_custom_plots(style, data) ``` # Implementation Implement the `create_custom_plots` function to achieve the desired output. ```python import seaborn as sns import matplotlib.pyplot as plt def create_custom_plots(style_dict, plot_data): sns.set_style(style_dict.get(\\"background\\", \\"whitegrid\\"), {k: v for k, v in style_dict.items() if k != \\"background\\"}) # Create bar plot plt.figure() sns.barplot(x=plot_data[\'bar\'][\'x\'], y=plot_data[\'bar\'][\'y\']) plt.title(\\"Bar Plot\\") # Create line plot plt.figure() sns.lineplot(x=plot_data[\'line\'][\'x\'], y=plot_data[\'line\'][\'y\']) plt.title(\\"Line Plot\\") plt.show() ``` # Note - Utilize the `sns.set_style` appropriately to reflect the style settings specified in the `style_dict`. - Generate the plots using the respective data provided in `plot_data`.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plots(style_dict, plot_data): sns.set_style(style_dict.get(\\"background\\", \\"whitegrid\\"), {k: v for k, v in style_dict.items() if k != \\"background\\"}) # Create bar plot plt.figure() sns.barplot(x=plot_data[\'bar\'][\'x\'], y=plot_data[\'bar\'][\'y\']) plt.title(\\"Bar Plot\\") # Create line plot plt.figure() sns.lineplot(x=plot_data[\'line\'][\'x\'], y=plot_data[\'line\'][\'y\']) plt.title(\\"Line Plot\\") plt.show()"},{"question":"**Python Mapping Protocol Implementation** The provided documentation describes a set of functions available in the Python 3.10 C API for working with mapping objects like dictionaries. Your task is to create a Python class `CustomMapping` that mimics a subset of these functionalities using native Python methods. # Requirements: - Implement the `CustomMapping` class with the following methods: 1. `__init__(self, data)`: Initializes the instance with a dictionary `data`. 2. `mapping_check(self)`: Returns `True` if the instance supports the mapping protocol (always true for this class). 3. `mapping_size(self)`: Returns the number of keys in the mapping. 4. `mapping_get_item(self, key)`: Returns the value associated with `key`. Raises `KeyError` if the key does not exist. 5. `mapping_set_item(self, key, value)`: Sets the value for `key`. 6. `mapping_del_item(self, key)`: Deletes the item associated with `key`. Raises `KeyError` if the key does not exist. 7. `mapping_has_key(self, key)`: Returns `True` if `key` exists in the mapping, `False` otherwise. 8. `mapping_keys(self)`: Returns a list of all keys in the mapping. 9. `mapping_values(self)`: Returns a list of all values in the mapping. 10. `mapping_items(self)`: Returns a list of tuples representing key-value pairs in the mapping. # Input and Output Examples: ```python # Initialization data = {\'a\': 1, \'b\': 2, \'c\': 3} cm = CustomMapping(data) # Check mapping print(cm.mapping_check()) # Output: True # Size of mapping print(cm.mapping_size()) # Output: 3 # Get item print(cm.mapping_get_item(\'a\')) # Output: 1 # Set item cm.mapping_set_item(\'d\', 4) print(cm.mapping_get_item(\'d\')) # Output: 4 # Delete item cm.mapping_del_item(\'b\') print(cm.mapping_size()) # Output: 3 # Check if key exists print(cm.mapping_has_key(\'c\')) # Output: True print(cm.mapping_has_key(\'b\')) # Output: False # Get keys print(cm.mapping_keys()) # Output: [\'a\', \'c\', \'d\'] # Get values print(cm.mapping_values()) # Output: [1, 3, 4] # Get items print(cm.mapping_items()) # Output: [(\'a\', 1), (\'c\', 3), (\'d\', 4)] ``` # Constraints: - Do not use any external libraries; only use builtin Python data structures and functions. - The data provided during initialization will always be a valid dictionary. Write your solution in Python and ensure all methods adhere to the specifications. Make use of Python\'s dictionary operations to implement these methods. # Evaluation: Your implementation will be evaluated based on: - Correctness - Efficiency - Code readability and organization","solution":"class CustomMapping: def __init__(self, data): Initialize the CustomMapping instance with a dictionary. self._data = data def mapping_check(self): Check if the instance supports the mapping protocol. Always returns True for this class. return True def mapping_size(self): Return the number of keys in the mapping. return len(self._data) def mapping_get_item(self, key): Get the value associated with the key. Raises KeyError if the key does not exist. return self._data[key] def mapping_set_item(self, key, value): Set the value for the key. self._data[key] = value def mapping_del_item(self, key): Delete the item associated with the key. Raises KeyError if the key does not exist. del self._data[key] def mapping_has_key(self, key): Return True if the key exists in the mapping, False otherwise. return key in self._data def mapping_keys(self): Return a list of all keys in the mapping. return list(self._data.keys()) def mapping_values(self): Return a list of all values in the mapping. return list(self._data.values()) def mapping_items(self): Return a list of tuples representing key-value pairs in the mapping. return list(self._data.items())"},{"question":"Objective Design and implement a semi-supervised learning model using scikit-learn that can handle both labeled and unlabeled data. You will test your implementation on a dataset to evaluate its performance. Problem Statement You have a dataset split into labeled and unlabeled parts. Your task is to: 1. Implement a semi-supervised classifier using both the `SelfTrainingClassifier` and `LabelPropagation` algorithms. 2. Train and evaluate the performance of each classifier. 3. Compare the results with a purely supervised learning approach using the same base estimator. Dataset For this assessment, you will use the Iris dataset provided by scikit-learn. However, part of the labels will be masked to simulate the presence of unlabeled data. Steps 1. **Load the Iris dataset** and mask part of the labels as -1 to simulate unlabeled data for the training set. 2. **Implement and train**: - A `SelfTrainingClassifier` with a base estimator of your choice (e.g., `sklearn.svm.SVC`). - A `LabelPropagation` classifier. 3. **Train a purely supervised classifier** on the labeled portion of the dataset for comparison. 4. **Evaluate** each model using accuracy on a held-out test set. Implementation Details - **Dataset Preparation**: - Load the Iris dataset using `sklearn.datasets.load_iris`. - Split the dataset into training and testing sets. - Mask 50% of the training labels (randomly selected) as -1 to simulate unlabeled data. - **SelfTrainingClassifier**: - Use a base estimator (e.g., `sklearn.svm.SVC`). - Set appropriate parameters for `threshold` or `k_best` and `max_iter`. - **LabelPropagation**: - Use the default parameters or tune them as you see fit. - **Supervised Learning**: - Train the same base estimator (used in self-training) on the labeled portion of the training data. - **Evaluation**: - Compare the accuracy of the three models on the test set. Example Code Structure ```python import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation from sklearn.svm import SVC from sklearn.metrics import accuracy_score # Load and prepare data iris = load_iris() X, y = iris.data, iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Masking part of the labels as unlabeled (-1) rng = np.random.RandomState(42) random_unlabeled_points = rng.rand(len(y_train)) < 0.5 y_train[random_unlabeled_points] = -1 # Implement and evaluate SelfTrainingClassifier svc = SVC(probability=True, gamma=\'auto\') self_training_clf = SelfTrainingClassifier(svc, criterion=\'k_best\', k_best=10) self_training_clf.fit(X_train, y_train) y_pred_self_training = self_training_clf.predict(X_test) accuracy_self_training = accuracy_score(y_test, y_pred_self_training) # Implement and evaluate LabelPropagation label_prop = LabelPropagation() label_prop.fit(X_train, y_train) y_pred_label_prop = label_prop.predict(X_test) accuracy_label_prop = accuracy_score(y_test, y_pred_label_prop) # Implement and evaluate purely supervised learning supervised_clf = SVC(gamma=\'auto\') supervised_clf.fit(X_train[y_train != -1], y_train[y_train != -1]) y_pred_supervised = supervised_clf.predict(X_test) accuracy_supervised = accuracy_score(y_test, y_pred_supervised) # Print accuracies print(f\'SelfTrainingClassifier Accuracy: {accuracy_self_training:.2f}\') print(f\'LabelPropagation Accuracy: {accuracy_label_prop:.2f}\') print(f\'Supervised SVC Accuracy: {accuracy_supervised:.2f}\') ``` Submission Requirements 1. Your implementation should follow the above example code structure and include necessary import statements. 2. Submit a script or Jupyter notebook containing: - Data preparation steps. - Implementation of `SelfTrainingClassifier` and `LabelPropagation`. - Training and evaluation code for all three models. - Printed accuracy results. 3. Provide a brief explanation comparing the accuracies obtained and any observations you made.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation from sklearn.svm import SVC from sklearn.metrics import accuracy_score def prepare_iris_data(): Loads the Iris dataset and splits it into training and test sets. Masks part of the training labels as -1 to simulate unlabeled data. iris = load_iris() X, y = iris.data, iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Masking 50% of the labels in the training set rng = np.random.RandomState(42) random_unlabeled_points = rng.rand(len(y_train)) < 0.5 y_train[random_unlabeled_points] = -1 return X_train, X_test, y_train, y_test def train_evaluate_self_training(X_train, y_train, X_test, y_test): Trains and evaluates a SelfTrainingClassifier. svc = SVC(probability=True, gamma=\'auto\') self_training_clf = SelfTrainingClassifier(svc, criterion=\'k_best\', k_best=10) self_training_clf.fit(X_train, y_train) y_pred_self_training = self_training_clf.predict(X_test) accuracy_self_training = accuracy_score(y_test, y_pred_self_training) return accuracy_self_training def train_evaluate_label_propagation(X_train, y_train, X_test, y_test): Trains and evaluates a LabelPropagation classifier. label_prop = LabelPropagation() label_prop.fit(X_train, y_train) y_pred_label_prop = label_prop.predict(X_test) accuracy_label_prop = accuracy_score(y_test, y_pred_label_prop) return accuracy_label_prop def train_evaluate_supervised(X_train, y_train, X_test, y_test): Trains and evaluates a purely supervised SVC classifier on the labeled data. supervised_clf = SVC(gamma=\'auto\') supervised_clf.fit(X_train[y_train != -1], y_train[y_train != -1]) y_pred_supervised = supervised_clf.predict(X_test) accuracy_supervised = accuracy_score(y_test, y_pred_supervised) return accuracy_supervised"},{"question":"Objective: To test the student\'s understanding of various Python built-in types such as numeric types, sequences, and mappings, and how to use them in combination to solve a complex problem. Problem Statement: You are tasked with developing a Python function that processes a list of transactions and returns a summary of the balances of each account involved. Each transaction includes a source account, destination account, and an amount. The input will be a list of transactions, where each transaction is represented as a dictionary with three keys: \\"source\\", \\"destination\\", and \\"amount\\". A sample list of transactions looks like this: ```python [ {\\"source\\": \\"A\\", \\"destination\\": \\"B\\", \\"amount\\": 100}, {\\"source\\": \\"B\\", \\"destination\\": \\"C\\", \\"amount\\": 50}, {\\"source\\": \\"A\\", \\"destination\\": \\"C\\", \\"amount\\": 20}, ] ``` Your task is to implement the function `calculate_balances(transactions: list) -> dict`, which computes the balance of each account involved in the transactions. - **Input:** - `transactions` (list): A list of dictionaries, each dictionary containing \\"source\\" (str), \\"destination\\" (str), and \\"amount\\" (float) keys. - **Output:** - A dictionary where the keys are account names, and the values are floats representing the final balance of each account. Accounts not in any transaction should not be included in the result. - **Constraints:** - The list can have a variable number of transactions (i.e., it could be empty). - Transaction amounts are non-negative. - Account names are unique strings. Example: ```python transactions = [ {\\"source\\": \\"A\\", \\"destination\\": \\"B\\", \\"amount\\": 100}, {\\"source\\": \\"B\\", \\"destination\\": \\"C\\", \\"amount\\": 50}, {\\"source\\": \\"A\\", \\"destination\\": \\"C\\", \\"amount\\": 20}, ] result = calculate_balances(transactions) print(result) # Output: {\\"A\\": -120.0, \\"B\\": 50.0, \\"C\\": 70.0} ``` Explanation: - Account \\"A\\" sends 100 to \\"B\\" and 20 to \\"C\\", totaling -120. - Account \\"B\\" receives 100 from \\"A\\" and sends 50 to \\"C\\", netting 50. - Account \\"C\\" receives 50 from \\"B\\" and 20 from \\"A\\", summing to 70. Implement the function: ```python def calculate_balances(transactions: list) -> dict: pass # Examples for testing the function transactions1 = [ {\\"source\\": \\"A\\", \\"destination\\": \\"B\\", \\"amount\\": 100}, {\\"source\\": \\"B\\", \\"destination\\": \\"C\\", \\"amount\\": 50}, {\\"source\\": \\"A\\", \\"destination\\": \\"C\\", \\"amount\\": 20}, ] transactions2 = [ {\\"source\\": \\"X\\", \\"destination\\": \\"Y\\", \\"amount\\": 300}, {\\"source\\": \\"Y\\", \\"destination\\": \\"Z\\", \\"amount\\": 150}, ] print(calculate_balances(transactions1)) # Expected Output: {\\"A\\": -120.0, \\"B\\": 50.0, \\"C\\": 70.0} print(calculate_balances(transactions2)) # Expected Output: {\\"X\\": -300.0, \\"Y\\": 150.0, \\"Z\\": 150.0} ``` Your implementation should accurately compute the balances of all accounts by iterating through the list of transactions and updating the balance of each account based on the transactions processed. Hint: Use dictionaries to keep track of account balances and ensure that all transactions are correctly applied to the respective accounts.","solution":"def calculate_balances(transactions: list) -> dict: Calculate the balances of all accounts involved in the transactions. Parameters: - transactions (list): A list of transaction dictionaries. Each dictionary contains \\"source\\" (str), \\"destination\\" (str), and \\"amount\\" (float). Returns: - dict: A dictionary where the keys are account names and the values are floats representing the final balance of each account. balances = {} for transaction in transactions: source = transaction[\'source\'] destination = transaction[\'destination\'] amount = transaction[\'amount\'] # Update the source account balance if source in balances: balances[source] -= amount else: balances[source] = -amount # Update the destination account balance if destination in balances: balances[destination] += amount else: balances[destination] = amount return balances"},{"question":"# PyTorch Performance Optimization on V100 GPU with Float16 Objective You are tasked with implementing a neural network in PyTorch, ensuring it satisfies the specific conditions to use the persistent algorithm for performance improvement. The goal is to demonstrate an understanding of using GPU, precision, and performance optimization. Instructions 1. Implement a simple neural network (e.g., a multi-layer perceptron or a convolutional neural network). 2. Ensure the input data meets the following conditions: - Data is on the GPU. - Data has `torch.float16` data type. - A V100 GPU is used. 3. Compare and report the training times with and without the above-mentioned conditions. Requirements 1. **Function Definition**: - `train_network(data: torch.Tensor, target: torch.Tensor) -> float` 2. **Input Format**: - `data`: A tensor containing the input data, should be of shape `(batch_size, input_features)`. - `target`: A tensor containing the target labels, should be of shape `(batch_size, num_classes)`. 3. **Output Format**: - Return the training time in seconds. Constraints - The input data must be moved to GPU and converted to `float16` precision. - Conduct the training for a specified number of epochs and report the total training time. - You must ensure cudnn is enabled and verify that the persistent algorithm is being used. - You can use synthetic data to validate the functionality. Example ```python import torch import torch.nn as nn import torch.optim as optim import time def train_network(data: torch.Tensor, target: torch.Tensor) -> float: # Ensure cudnn is enabled torch.backends.cudnn.enabled = True # Check if a V100 GPU is available if not torch.cuda.is_available() or torch.cuda.get_device_name(0) != \'Tesla V100\': return \\"V100 GPU is not available\\" # Move data to GPU and convert to float16 data = data.to(\'cuda\', dtype=torch.float16) target = target.to(\'cuda\', dtype=torch.float16) # Define a simple neural network class SimpleNet(nn.Module): def __init__(self, input_features, num_classes): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(input_features, 128) self.fc2 = nn.Linear(128, 64) self.fc3 = nn.Linear(64, num_classes) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x # Initialize the network, loss function, and optimizer model = SimpleNet(data.shape[1], target.shape[1]).cuda().half() criterion = nn.MSELoss().cuda().half() optimizer = optim.Adam(model.parameters()) # Training loop epochs = 5 start_time = time.time() for epoch in range(epochs): model.train() # Forward pass outputs = model(data) loss = criterion(outputs, target) # Backward pass and optimization optimizer.zero_grad() loss.backward() optimizer.step() end_time = time.time() training_time = end_time - start_time return training_time # Example usage data = torch.randn(1000, 32) # Example data (1000 samples, 32 features) target = torch.randn(1000, 10) # Example target (1000 samples, 10 classes) # Train network and get training time training_time = train_network(data, target) print(\\"Training Time: \\", training_time) ``` Test the above function under provided conditions and verify the performance improvements.","solution":"import torch import torch.nn as nn import torch.optim as optim import time def train_network(data: torch.Tensor, target: torch.Tensor) -> float: # Ensure cudnn is enabled torch.backends.cudnn.enabled = True # Check if a V100 GPU is available if not torch.cuda.is_available() or \'V100\' not in torch.cuda.get_device_name(0): print(\\"V100 GPU is not available or GPU is unavailable.\\") return -1 # Move data to GPU and convert to float16 data = data.to(\'cuda\', dtype=torch.float16) target = target.to(\'cuda\', dtype=torch.float16) # Define a simple neural network class SimpleNet(nn.Module): def __init__(self, input_features, num_classes): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(input_features, 128) self.fc2 = nn.Linear(128, 64) self.fc3 = nn.Linear(64, num_classes) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x # Initialize the network, loss function, and optimizer model = SimpleNet(data.shape[1], target.shape[1]).cuda().half() criterion = nn.MSELoss().cuda().half() optimizer = optim.Adam(model.parameters()) # Training loop epochs = 5 start_time = time.time() for epoch in range(epochs): model.train() # Forward pass outputs = model(data) loss = criterion(outputs, target) # Backward pass and optimization optimizer.zero_grad() loss.backward() optimizer.step() end_time = time.time() training_time = end_time - start_time return training_time # Example usage # This example assumes a V100 GPU is available and the appropriate PyTorch setup is done if __name__ == \\"__main__\\": data = torch.randn(1000, 32) # Example data (1000 samples, 32 features) target = torch.randn(1000, 10) # Example target (1000 samples, 10 classes) # Train network and get training time training_time = train_network(data, target) print(\\"Training Time: \\", training_time)"},{"question":"# Parallel Matrix Multiplication Using Multiprocessing You are required to implement a parallel matrix multiplication function in Python using the `multiprocessing` package. The function will take two matrices as input and return their product. Matrix multiplication is defined as follows: Given two matrices A of size MxN and B of size NxP, their product C is a matrix of size MxP where each element `C[i][j]` is computed as: [ C[i][j] = sum_{k=0}^{N}A[i][k] cdot B[k][j] ] # Task Implement a function `parallel_matrix_multiply(A, B)` that multiplies two matrices A and B using the `multiprocessing` module. Function Signature ```python def parallel_matrix_multiply(A, B): # Your code here ``` Input - `A`: List of lists of integers/floats where each sublist represents a row in matrix A, and the length of sublists is uniform. - `B`: List of lists of integers/floats where each sublist represents a row in matrix B, and the length of sublists is uniform. Output - A list of lists representing the product matrix C. Constraints - The number of columns in A equals the number of rows in B. - Assume the matrices can be large, so parallel computation is necessary to achieve better performance. - Implement robust error handling to manage potential issues in the multiprocessing logic, including inter-process communication and synchronization. # Example ```python A = [ [1, 2, 3], [4, 5, 6], [7, 8, 9] ] B = [ [9, 8, 7], [6, 5, 4], [3, 2, 1] ] result = parallel_matrix_multiply(A, B) print(result) # Output: # [ # [30, 24, 18], # [84, 69, 54], # [138, 114, 90] # ] ``` # Requirements 1. Use `multiprocessing.Pool` to parallelize the computation of the matrix product. 2. Ensure that each worker process computes a part of the matrix product and results are combined correctly. 3. Handle edge cases, such as empty matrices and incompatible matrix sizes, with appropriate error messages. # Performance Considerations Since matrix multiplication can be quite computational intensive, your solution should demonstrate significant performance improvement over a single-threaded approach for larger matrices.","solution":"import multiprocessing from itertools import product def matrix_element(A, B, i, j): Computes the element C[i][j] for the product of matrices A and B return sum(A[i][k] * B[k][j] for k in range(len(B))) def parallel_matrix_multiply(A, B): Multiplies two matrices A and B using parallel processing. Returns the product matrix C. if not A or not B or not A[0] or not B[0]: raise ValueError(\\"Input matrices must not be empty.\\") num_rows_A, num_cols_A = len(A), len(A[0]) num_rows_B, num_cols_B = len(B), len(B[0]) if num_cols_A != num_rows_B: raise ValueError(\\"Number of columns in A must be equal to number of rows in B.\\") # Initialize the result matrix with zeros C = [[0 for _ in range(num_cols_B)] for _ in range(num_rows_A)] # Create a pool of workers with multiprocessing.Pool() as pool: # Generate the arguments for each element computation args = [(A, B, i, j) for i, j in product(range(num_rows_A), range(num_cols_B))] # Compute matrix elements in parallel results = pool.starmap(matrix_element, args) # Populate the result matrix C with the computed values for (i, j), value in zip(product(range(num_rows_A), range(num_cols_B)), results): C[i][j] = value return C"},{"question":"# Objective Implement a neural network model using TorchScript that includes both a custom class with appropriate annotations and a forward method that takes advantage of TorchScript\'s type checking and static typing capabilities. The aim is to demonstrate understanding of TorchScript\'s type system, custom classes, and function annotations. # Problem Statement Implement a simple feedforward neural network in TorchScript using the following structure: 1. A custom dataset class named `SimpleDataset`, which prepares the data for training. This class should: - Inherit from `torch.utils.data.Dataset`. - Implement `__init__`, `__len__`, and `__getitem__` methods. - Have an instance attribute `data` of type `List[Tuple[torch.Tensor, torch.Tensor]]`. 2. A neural network class named `FeedforwardNN`, which is a subclass of `torch.nn.Module`. This class should: - Initialize a simple network with a single hidden layer and ReLU activation in the `__init__` method. - Include a `forward()` method that defines the forward pass of the network. 3. Write a training loop to train the model on a synthetic dataset. # Function Signatures ```python import torch import torch.nn as nn from typing import Tuple, List from torch.utils.data import Dataset, DataLoader # Custom Dataset class class SimpleDataset(Dataset): data: List[Tuple[torch.Tensor, torch.Tensor]] def __init__(self, data: List[Tuple[torch.Tensor, torch.Tensor]]): self.data = data def __len__(self) -> int: return len(self.data) def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]: return self.data[index] # Neural Network class class FeedforwardNN(nn.Module): def __init__(self): super(FeedforwardNN, self).__init__() self.hidden = nn.Linear(2, 4) # Two input features to four hidden units self.output = nn.Linear(4, 1) # Four hidden units to one output unit def forward(self, x: torch.Tensor) -> torch.Tensor: x = torch.relu(self.hidden(x)) x = self.output(x) return x # Training loop def train_model(dataloader: DataLoader, model: FeedforwardNN, epochs: int, lr: float): criterion = nn.MSELoss() optimizer = torch.optim.SGD(model.parameters(), lr=lr) for epoch in range(epochs): for inputs, targets in dataloader: optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() print(f\\"Epoch {epoch+1}, Loss: {loss.item()}\\") # Main script def main(): # Generate synthetic dataset data = [(torch.randn(2), torch.randn(1)) for _ in range(100)] dataset = SimpleDataset(data) dataloader = DataLoader(dataset, batch_size=10, shuffle=True) # Initialize model model = FeedforwardNN() # Convert to ScriptModule scripted_model = torch.jit.script(model) # Train the model train_model(dataloader, scripted_model, epochs=10, lr=0.01) if __name__ == \'__main__\': main() ``` # Input and Output - **Input**: The provided dataset will consist of 100 samples where each sample is a tuple containing an input tensor of shape (2,) and an output tensor of shape (1,). - **Output**: Print the loss after each epoch during the training process. # Constraints - Ensure that all classes and methods are TorchScript compatible. - Use appropriate type annotations for all parameters and return types. - The model should be scripted using `torch.jit.script`. - Maintain readable and efficient code, leveraging TorchScript features as necessary. # Performance Requirements - The implementation should be efficient, maintaining a reasonable training time for the provided dataset size and model architecture.","solution":"import torch import torch.nn as nn from typing import Tuple, List from torch.utils.data import Dataset, DataLoader # Custom Dataset class class SimpleDataset(Dataset): data: List[Tuple[torch.Tensor, torch.Tensor]] def __init__(self, data: List[Tuple[torch.Tensor, torch.Tensor]]): self.data = data def __len__(self) -> int: return len(self.data) def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]: return self.data[index] # Neural Network class class FeedforwardNN(nn.Module): def __init__(self): super(FeedforwardNN, self).__init__() self.hidden = nn.Linear(2, 4) # Two input features to four hidden units self.output = nn.Linear(4, 1) # Four hidden units to one output unit def forward(self, x: torch.Tensor) -> torch.Tensor: x = torch.relu(self.hidden(x)) x = self.output(x) return x # Training loop def train_model(dataloader: DataLoader, model: FeedforwardNN, epochs: int, lr: float): criterion = nn.MSELoss() optimizer = torch.optim.SGD(model.parameters(), lr=lr) for epoch in range(epochs): for inputs, targets in dataloader: optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() print(f\\"Epoch {epoch+1}, Loss: {loss.item()}\\") # Main script def main(): # Generate synthetic dataset data = [(torch.randn(2), torch.randn(1)) for _ in range(100)] dataset = SimpleDataset(data) dataloader = DataLoader(dataset, batch_size=10, shuffle=True) # Initialize model model = FeedforwardNN() # Convert to ScriptModule scripted_model = torch.jit.script(model) # Train the model train_model(dataloader, scripted_model, epochs=10, lr=0.01) if __name__ == \'__main__\': main()"},{"question":"**Objective:** Write a Python function that processes a list of file names, computes some statistics on them (like line counts), and ensures proper handling of exceptions that might arise during file processing. Problem Statement Develop a function called `process_files(file_list)` that takes a list of file names (strings) and performs the following steps for each file: 1. Attempts to open the file in read mode. 2. Reads the file line by line and computes the total number of lines. 3. Records the number of lines for each file in a dictionary with the file name as the key. 4. Handles the following exceptions appropriately: - `FileNotFoundError`: Record an error message in a dictionary for files that do not exist. - `PermissionError`: Record an error message in a dictionary for files where the permission is denied. - Any other exception: Record a generic error message in the dictionary. Additionally, the function must: - Use `try`, `except`, `else`, and `finally` blocks to handle the exceptions. - Utilize the `with` statement to ensure files are properly closed after processing. - Return a tuple containing two dictionaries: 1. The dictionary with the line counts. 2. The dictionary with error messages. Constraints - You can assume that the input list of file names will not be empty. - Ensure that your function handles all the mentioned exceptions and guarantees clean-up actions (closing files). Example Usage ```python file_names = [\\"file1.txt\\", \\"file2.txt\\", \\"nonexistent.txt\\", \\"protected.txt\\"] line_counts, error_messages = process_files(file_names) print(line_counts) # Example Output: {\\"file1.txt\\": 15, \\"file2.txt\\": 20} print(error_messages) # Example Output: {\\"nonexistent.txt\\": \\"File not found\\", \\"protected.txt\\": \\"Permission denied\\"} ``` Notes: - `file1.txt` and `file2.txt` are assumed to be valid files. - `nonexistent.txt` does not exist. - `protected.txt` exists but is not readable due to permission restrictions. Implementation Implement the `process_files` function based on the provided specification.","solution":"def process_files(file_list): Processes a list of file names, computes line counts, and handles exceptions. Args: file_list (list of str): List of file names to process. Returns: tuple: A tuple containing two dictionaries: - Dictionary with line counts. - Dictionary with error messages. line_counts = {} error_messages = {} for file_name in file_list: try: with open(file_name, \'r\') as file: line_count = sum(1 for _ in file) line_counts[file_name] = line_count except FileNotFoundError: error_messages[file_name] = \\"File not found\\" except PermissionError: error_messages[file_name] = \\"Permission denied\\" except Exception as e: error_messages[file_name] = f\\"An error occurred: {str(e)}\\" # No need for an else block, as successful file reading/processing # does not require additional actions here. # No need for a finally block, as \'with\' statement ensures file closure. return (line_counts, error_messages)"},{"question":"Advanced Usage of `unittest.mock` Objective To assess your understanding of the `unittest.mock` module in Python, specifically focusing on its advanced features like `patch()`, `MagicMock`, and `create_autospec`. Problem Statement You are developing a testing suite for a hypothetical Python project. The project includes a class `DataProcessor` that fetches data from a remote API and processes it. The class method `fetch_data` calls an external API and the method `process_data` processes the data after fetching. The aim of this exercise is to write unit tests for the `DataProcessor` class using various functionalities from the `unittest.mock` module. DataProcessor Class ```python import requests class DataProcessor: def fetch_data(self, url): response = requests.get(url) if response.status_code != 200: raise ValueError(\\"Failed to fetch data!\\") return response.json() def process_data(self, data): # Let\'s assume some complex processing happens here return {\'processed_data\': data} ``` Tasks 1. **Mock the `requests.get` call**: Ensure that it does not make an actual HTTP request. 2. **Verify method calls**: Ensure that `fetch_data` correctly calls `requests.get` and handles the response properly. 3. **Test `process_data`**: - Use `create_autospec` to ensure `process_data` is called correctly. - Verify that it performs the correct operations on data. Requirements 1. Use the `patch` decorator or context manager to mock `requests.get`. 2. Use assertions to ensure `fetch_data` handles responses and errors correctly. 3. Use `create_autospec` to test `process_data`. Constraints - Simulate a successful response from the `requests.get` method with status code 200 and a JSON response body. - Simulate a failure response from the `requests.get` method with a non-200 status code. - Ensure `process_data` is invoked with only the allowed methods and parameters. Example Test Case Write a test case for the `fetch_data` method to ensure it raises a `ValueError` when the response status code is not 200. Submission Submit a Python file with the following: - The `DataProcessor` class (as provided). - A `TestDataProcessor` test class containing all required unit test methods. Evaluation Criteria - Correct usage of `patch` to mock external API calls. - Proper use of `assertions` to validate method behaviors. - Effective use of `create_autospec` to ensure method signature accuracy. - Code readability and following Python best practices. # Example Structure for Test Class ```python import unittest from unittest.mock import patch, create_autospec, MagicMock from data_processor import DataProcessor # Ensure this is your imported class name class TestDataProcessor(unittest.TestCase): @patch(\'data_processor.requests.get\') def test_fetch_data_success(self, mock_get): # Write your test code here pass @patch(\'data_processor.requests.get\') def test_fetch_data_failure(self, mock_get): # Write your test code here pass def test_process_data(self): mock_data_processor = create_autospec(DataProcessor) # Write your test code here pass if __name__ == \'__main__\': unittest.main() ``` *Replace `data_processor` in patches with the appropriate module name.*","solution":"import requests class DataProcessor: def fetch_data(self, url): response = requests.get(url) if response.status_code != 200: raise ValueError(\\"Failed to fetch data!\\") return response.json() def process_data(self, data): # Let\'s assume some complex processing happens here return {\'processed_data\': data}"},{"question":"# Coding Assessment Question Objective: You are asked to implement a set of functions in Python that mimic the behavior of the given C API for complex number operations. This task will test your understanding of both complex number arithmetic and your ability to implement these operations in Python. Task: Implement the following functions in Python: 1. `py_c_sum(left, right)` 2. `py_c_diff(left, right)` 3. `py_c_neg(num)` 4. `py_c_prod(left, right)` 5. `py_c_quot(dividend, divisor)` 6. `py_c_pow(num, exp)` # Specifications: - Each function takes in either one or two tuples representing complex numbers. For instance, `(a, b)` represents the complex number `a + bj`. - Each function returns a tuple representing a complex number as the result. # Function Details: 1. `py_c_sum(left, right)`: - **Input**: `left` and `right`, both tuples `(a, b)` representing the complex numbers `a + bj` and `c + dj`. - **Output**: A tuple `(e, f)` representing the sum `(a + c) + (b + d)j`. 2. `py_c_diff(left, right)`: - **Input**: `left` and `right`, both tuples `(a, b)` representing the complex numbers `a + bj` and `c + dj`. - **Output**: A tuple `(e, f)` representing the difference `(a - c) + (b - d)j`. 3. `py_c_neg(num)`: - **Input**: `num`, a tuple `(a, b)` representing the complex number `a + bj`. - **Output**: A tuple `(-a, -b)` representing the negated complex number `-a - bj`. 4. `py_c_prod(left, right)`: - **Input**: `left` and `right`, both tuples `(a, b)` representing the complex numbers `a + bj` and `c + dj`. - **Output**: A tuple `(e, f)` representing the product of the two complex numbers. 5. `py_c_quot(dividend, divisor)`: - **Input**: `dividend` and `divisor`, both tuples `(a, b)` and `(c, d)` representing the complex numbers `a + bj` and `c + dj`. - **Output**: A tuple `(e, f)` representing the quotient of dividing `a + bj` by `c + dj`. 6. `py_c_pow(num, exp)`: - **Input**: `num` and `exp`, both tuples `(a, b)` representing the complex numbers `a + bj` and `c + dj`. - **Output**: A tuple `(e, f)` representing the complex number `num` raised to the power of `exp`. # Constraints: - You may assume that the `divisor` in `py_c_quot` is never zero. - Handle edge cases such as very large or very small values gracefully. # Example: ```python assert py_c_sum((1, 2), (3, 4)) == (4, 6) assert py_c_diff((5, 7), (2, 3)) == (3, 4) assert py_c_neg((3, -4)) == (-3, 4) assert py_c_prod((1, 2), (3, 4)) == (-5, 10) assert py_c_quot((4, 8), (2, 2)) == (3.0, 1.0) # equivalent to (4+8j) / (2+2j) assert py_c_pow((1, 1), (2, 0)) == (0.0, 2.0000000000000004) # (1+1j) raised to the power (2+0j) ``` Ensure your implementation is correct by running similar test cases. Submission: Submit your solution as a Python file containing the definitions of all required functions.","solution":"import cmath def py_c_sum(left, right): return (left[0] + right[0], left[1] + right[1]) def py_c_diff(left, right): return (left[0] - right[0], left[1] - right[1]) def py_c_neg(num): return (-num[0], -num[1]) def py_c_prod(left, right): real = left[0]*right[0] - left[1]*right[1] imag = left[0]*right[1] + left[1]*right[0] return (real, imag) def py_c_quot(dividend, divisor): real = (dividend[0]*divisor[0] + dividend[1]*divisor[1]) / (divisor[0]**2 + divisor[1]**2) imag = (dividend[1]*divisor[0] - dividend[0]*divisor[1]) / (divisor[0]**2 + divisor[1]**2) return (real, imag) def py_c_pow(num, exp): complex_num = complex(num[0], num[1]) complex_exp = complex(exp[0], exp[1]) result = cmath.exp(complex_exp * cmath.log(complex_num)) return (result.real, result.imag)"},{"question":"Objective You are tasked with implementing a Principal Component Analysis (PCA) on a given dataset. The goal is to reduce the dataset\'s dimensionality to two components and visualize the transformed data to observe the variance captured by these components. Problem Statement 1. Load the Iris dataset from `sklearn.datasets`. 2. Implement PCA to reduce the dataset to 2 principal components. 3. Visualize the transformed data using a scatter plot, where different colors represent different classes of the Iris dataset. # Function Signature ```python def perform_pca_and_visualize(): pass ``` Requirements - Use scikit-learn\'s PCA for dimensionality reduction. - The scatter plot should show data points colored by their class labels. Constraints - You are required to use only numpy, matplotlib, and scikit-learn libraries. - Ensure the code is well-documented and handles exceptions appropriately. # Example The output should be a scatter plot where: - The x-axis and y-axis represent the first and second principal components, respectively. - Different classes (Iris species) are color-coded for easy visualization. **Hint**: Use `PCA(n_components=2)` to reduce the data to 2 dimensions. ```python def perform_pca_and_visualize(): import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.decomposition import PCA # Load the Iris dataset iris = load_iris() X = iris.data y = iris.target target_names = iris.target_names # Implement PCA pca = PCA(n_components=2) X_r = pca.fit_transform(X) # Plot the resulting PCA components plt.figure() colors = [\'navy\', \'turquoise\', \'darkorange\'] for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, lw=2, label=target_name) plt.title(\'PCA of IRIS dataset\') plt.legend() plt.show() ``` Submission Submit a Python script containing the function `perform_pca_and_visualize` along with any necessary documentation or comments within the code. Evaluation Criteria - Correctness: The implementation should correctly perform PCA and visualize the dataset. - Code Quality: The code should be clean, well-documented, and handle exceptions. - Visualization: The scatter plot should be clear and correctly color-coded.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.decomposition import PCA def perform_pca_and_visualize(): Perform PCA on the Iris dataset to reduce it to 2 principal components and visualize the transformed data using a scatter plot. # Load the Iris dataset iris = load_iris() X = iris.data y = iris.target target_names = iris.target_names # Implement PCA to reduce to 2 dimensions pca = PCA(n_components=2) X_r = pca.fit_transform(X) # Plot the PCA-transformed data plt.figure() colors = [\'navy\', \'turquoise\', \'darkorange\'] for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, lw=2, label=target_name) plt.title(\'PCA of IRIS dataset\') plt.legend(loc=\'best\') plt.xlabel(\'First Principal Component\') plt.ylabel(\'Second Principal Component\') plt.show()"},{"question":"# PyTorch MTIA Device and Stream Management You are given a task to implement a Python class `MTIADeviceManager` using the PyTorch MTIA backend functions. This class should help in managing multiple devices, recording memory usage, and handling streams for efficient computation. Class Specification 1. **`__init__(self)`**: - Initialize the MTIA backend if it is available and not initialized yet. - Raise an appropriate error if the backend is not available. 2. **`get_device_info(self, device_id)`**: - Input: - `device_id` (int): The ID of the device. - Output: - A dictionary containing: - \'device_capability\': The device capability. - \'memory_stats\': Memory statistics of the device. 3. **`clear_device_cache(self, device_id)`**: - Clear the cache of the specified device. - Input: - `device_id` (int): The ID of the device to clear the cache of. 4. **`manage_stream(self, device_id)`**: - Input: - `device_id` (int): The ID of the device. - Perform the following operations: - Set the device. - Create a stream, set it, and synchronize it. - Record memory usage before and after a mock operation (like a tensor addition). - Output: - A dictionary with \'initial_memory\' and \'final_memory\' describing memory stats before and after the mock operation. Constraints and Requirements - Assume reasonable default limits for devices and conditions where they are not specified. - Ensure you handle exceptions like `DeferredMtiaCallError` correctly. - Optimized for clarity and efficiency in device and stream management. # Sample Usage: ```python manager = MTIADeviceManager() if torch.mtia.is_available(): info = manager.get_device_info(0) print(info) manager.clear_device_cache(0) stream_info = manager.manage_stream(0) print(stream_info) else: print(\\"MTIA backend is not available.\\") ``` # Expected Output: ```python {\'device_capability\': (major, minor), \'memory_stats\': {...}} {\'initial_memory\': {...}, \'final_memory\': {...}} ``` Implement the `MTIADeviceManager` class as described.","solution":"import torch class MTIADeviceManager: def __init__(self): if not torch.mtia.is_available(): raise RuntimeError(\\"MTIA backend is not available.\\") if not torch.mtia.is_initialized(): torch.mtia.init() def get_device_info(self, device_id): device_capability = torch.mtia.get_device_capability(device_id) memory_stats = torch.mtia.memory_stats(device_id) return { \'device_capability\': device_capability, \'memory_stats\': memory_stats } def clear_device_cache(self, device_id): torch.mtia.empty_cache(device_id) def manage_stream(self, device_id): torch.mtia.set_device(device_id) stream = torch.mtia.Stream() torch.mtia.set_stream(stream) initial_memory = torch.mtia.memory_stats(device_id) # Mock operation for demonstration purpose a = torch.randn(1000, device=device_id) b = torch.randn(1000, device=device_id) c = a + b stream.synchronize() final_memory = torch.mtia.memory_stats(device_id) return { \'initial_memory\': initial_memory, \'final_memory\': final_memory }"},{"question":"# GenericAlias Implementation and Usage In Python, type hinting and generics provide mechanisms to specify more precise types for variables and function return values, enabling better type checks and clearer code. In this question, you will implement a simplified version of Python\'s `GenericAlias` object alongside functions to demonstrate its usage. Your task will be broken down into two parts: Part 1: Implementing `SimpleGenericAlias` 1. Create a class `SimpleGenericAlias` that mimics the basic behavior of Python\'s `GenericAlias`. 2. The class should have the following properties: - `__origin__`: This represents the generic type (e.g., `list`, `dict`). - `__args__`: This stores the type arguments passed to the generic type (e.g., `int` in `list[int]`). 3. The class should handle the following methods: - `__init__(self, origin, *args)`: Initializes the `SimpleGenericAlias` object. - `__repr__(self) -> str`: Returns a readable string representation of the alias. - `__getitem__(self, item)`: Allows for the creation of type-aliases using the bracket notation (e.g., `SimpleGenericAlias(list)[int]`). Part 2: Using `SimpleGenericAlias` 1. Create a function `generate_aliases` that accepts a list of tuples. Each tuple should contain a generic type (like `list` or `dict`) and types for the arguments (like `int` or `str`). 2. The function should generate and return a list of `SimpleGenericAlias` objects based on the provided input. # Input Format - The `generate_aliases` function receives a list of tuples: `[(generic_type_1, type_arg_1_1, type_arg_1_2, ...), (generic_type_2, type_arg_2_1, ...), ...]` # Output Format - Return a list of `SimpleGenericAlias` objects properly initialized to reflect the input generic types and their arguments. # Constraints - Assume the generic types will only be common Python collections like `list`, `dict`, `set`, and `tuple`. - You can assume the tuples will always be correctly formatted. # Example Below is the expected usage and output of your implementation: ```python # Example of Part 1: alias = SimpleGenericAlias(list, int) print(alias) # Output: list[int] alias = SimpleGenericAlias(dict, str, int) print(alias) # Output: dict[str, int] # Example of Part 2: input_data = [ (list, int), (dict, str, int), (tuple, str, int, float) ] aliases = generate_aliases(input_data) for alias in aliases: print(alias) # Expected Output: # list[int] # dict[str, int] # tuple[str, int, float] ``` Your implementation should be efficient and readable. Focus on correct initialization and representation while ensuring the class and functions operate as specified.","solution":"class SimpleGenericAlias: def __init__(self, origin, *args): self.__origin__ = origin self.__args__ = args def __repr__(self): if self.__args__: args_repr = \\", \\".join([arg.__name__ for arg in self.__args__]) return f\\"{self.__origin__.__name__}[{args_repr}]\\" return self.__origin__.__name__ def __getitem__(self, item): return SimpleGenericAlias(self.__origin__, item) def generate_aliases(type_tuples): return [SimpleGenericAlias(t[0], *t[1:]) for t in type_tuples]"},{"question":"You are tasked with creating a visual representation of two different datasets using seaborn\'s `diverging_palette`. Your goal is to generate a heatmap for each dataset, where the color scheme clearly indicates divergences from the central value. Requirements 1. **Function Definition**: Implement a function named `generate_heatmap` that accepts the following parameters: - `data`: A `pandas.DataFrame` containing the dataset to be visualized. - `center`: A numeric value that indicates the center of the diverging palette. - `h1`: An integer representing the starting hue for the palette. - `h2`: An integer representing the ending hue for the palette. - `center_color`: A string that specifies the center color. Options are \\"light\\" or \\"dark\\". - `sep`: An integer that indicates the amount of separation around the center value. - `as_cmap`: A boolean that, if True, returns a continuous colormap instead of discrete values. The function should generate and display a heatmap using `seaborn` with the specified diverging palette. 2. **Input and Output**: - The function should not return any value. Instead, it should display the heatmap directly. - Example Input: ```python import pandas as pd data = pd.DataFrame({ \'A\': [1, 2, 3], \'B\': [4, -5, 6], \'C\': [-7, 8, -9] }) generate_heatmap(data, center=0, h1=240, h2=20, center_color=\'dark\', sep=30, as_cmap=True) ``` - The above input should display a heatmap with a diverging palette centered at 0, starting at hue 240 and ending at hue 20, with a dark center color, increased separation around the center, and as a continuous colormap. 3. **Constraints**: - The `data` parameter will always be a non-empty pandas DataFrame with numeric values. - The `h1` and `h2` values should be valid hue values (0-359). - The `sep` value should be a positive integer. 4. **Performance**: - The function should be efficient and able to handle reasonably sized DataFrames (up to 1000x1000) without significant delays. Example Usage ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def generate_heatmap(data, center, h1, h2, center_color, sep, as_cmap): palette = sns.diverging_palette(h1, h2, center=center_color, sep=sep, as_cmap=as_cmap) sns.heatmap(data, center=center, cmap=palette) plt.show() data1 = pd.DataFrame({ \'A\': [1, 2, 3], \'B\': [4, -5, 6], \'C\': [-7, 8, -9] }) generate_heatmap(data1, center=0, h1=240, h2=20, center_color=\'dark\', sep=30, as_cmap=True) ``` In this example, the function `generate_heatmap` is used to create a visual representation of the dataset `data1`, clearly indicating areas diverging from the central value 0 using a customized diverging palette.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def generate_heatmap(data, center, h1, h2, center_color, sep, as_cmap): Generates and displays a heatmap for the given data using a diverging palette. Parameters: - data: pandas.DataFrame, the dataset to be visualized. - center: numeric, the center of the diverging palette. - h1: int, starting hue for the palette. - h2: int, ending hue for the palette. - center_color: str, specifies the center color (\'light\' or \'dark\'). - sep: int, amount of separation around the center value. - as_cmap: bool, if True, returns a continuous colormap instead of discrete values. Returns: - None, displays the heatmap. palette = sns.diverging_palette(h1, h2, center=center_color, sep=sep, as_cmap=as_cmap) sns.heatmap(data, center=center, cmap=palette) plt.show()"},{"question":"Objective Implement a custom encoding function similar to those provided in the deprecated `email.encoders` module. This function will encode the payload of a message object into ROT13 encoding and set the appropriate `Content-Transfer-Encoding` header. # Function to Implement: `encode_rot13(msg)` Input - `msg`: An instance of an `email.message.Message` object with a text payload. Output - The function should encode the payload of the message using ROT13 encoding and then set the `Content-Transfer-Encoding` header to `\\"rot13\\"`. Constraints 1. If the given `msg` is a multipart message, raise a `TypeError`. 2. Assume the payload of the `msg` object is initially a string or `None`. Details - ROT13 is a simple letter substitution cipher that replaces a letter with the 13th letter after it in the alphabet. - `A` ↔ `N`, `B` ↔ `O`, etc., both for lowercase and uppercase letters. - Modify the payload of the given message and set the `Content-Transfer-Encoding` header to \\"rot13\\". Example Usage ```python from email.message import Message # Example message creation msg = Message() msg.set_payload(\\"Hello, World!\\") # Custom encoder function encode_rot13(msg) # After encoding, the message payload should be transformed assert msg.get_payload() == \\"Uryyb, Jbeyq!\\" assert msg[\\"Content-Transfer-Encoding\\"] == \\"rot13\\" ``` Function Signature ```python def encode_rot13(msg: \'email.message.Message\') -> None: # Your implementation here ```","solution":"import email.message import codecs def encode_rot13(msg: email.message.Message) -> None: Encodes the payload of the email message using ROT13 encoding and sets the Content-Transfer-Encoding header to \'rot13\'. Args: msg (email.message.Message): The email message object to encode. Raises: TypeError: If the msg is a multipart message. if msg.is_multipart(): raise TypeError(\\"Cannot apply ROT13 encoding to a multipart message\\") payload = msg.get_payload() # Ensure payload is a string or None if payload is not None and not isinstance(payload, str): raise TypeError(\\"Payload must be a string\\") # Encode payload using ROT13 if payload is not None: rot13_payload = codecs.encode(payload, \'rot_13\') msg.set_payload(rot13_payload) # Set Content-Transfer-Encoding header msg[\\"Content-Transfer-Encoding\\"] = \\"rot13\\""},{"question":"**Coding Assessment Question** # Objective You are required to implement an asynchronous job scheduling system using Python\'s asyncio module. Your solution should demonstrate your understanding of creating and managing asynchronous tasks, handling timeouts, and using synchronization primitives. # Problem Statement You must create a function `run_scheduler(jobs: List[Callable], max_concurrent_jobs: int, timeout: int) -> Tuple[List[Any], List[int], int]` that takes the following arguments: - `jobs`: A list of callables that represent the jobs to be executed. Each job is a coroutine that should be awaited. - `max_concurrent_jobs`: An integer value representing the maximum number of jobs that can run concurrently. - `timeout`: An integer value representing the timeout (in seconds) for each job. The function should execute the jobs concurrently with a constraint on the maximum number of concurrent jobs. Each job should be awaited with the specified timeout. The function should return the following tuple: - A list of results from the successfully completed jobs. - A list of indices of jobs that timed out. - The total count of successfully completed jobs. # Constraints - You must use `asyncio.gather` to run jobs in parallel. - Use `asyncio.wait_for` to enforce the timeout on each job. - You can use any synchronization primitives if necessary. - Handle exceptions appropriately, especially for timeouts using `asyncio.TimeoutError`. # Example ```python import asyncio async def sample_job(duration): await asyncio.sleep(duration) return duration async def main(): jobs = [lambda: sample_job(1), lambda: sample_job(5), lambda: sample_job(3)] max_concurrent_jobs = 2 timeout = 4 results, timed_out_jobs, success_count = await run_scheduler(jobs, max_concurrent_jobs, timeout) print(\\"Results:\\", results) print(\\"Timed out jobs indices:\\", timed_out_jobs) print(\\"Total successful jobs count:\\", success_count) asyncio.run(main()) ``` # Expected Output ``` Results: [1, 3] Timed out jobs indices: [1] Total successful jobs count: 2 ``` # Notes 1. Each job provided will be a coroutine. Be sure to schedule their execution properly with respect to the limit on concurrent jobs and handle timeouts. 2. If a job times out, include its index in the list of timed-out jobs. 3. Ensure your function works correctly even if no jobs are provided. 4. The function should be highly efficient and handle the scenarios where hundreds of jobs are to be scheduled concurrently. # Performance Requirements Your solution must handle at least 1000 jobs with a time complexity appropriate for large-scale concurrency handling.","solution":"import asyncio from typing import List, Callable, Tuple, Any async def run_scheduler(jobs: List[Callable], max_concurrent_jobs: int, timeout: int) -> Tuple[List[Any], List[int], int]: semaphore = asyncio.Semaphore(max_concurrent_jobs) async def run_job(index, job): try: async with semaphore: result = await asyncio.wait_for(job(), timeout) return (index, result, None) # None indicates no exception except asyncio.TimeoutError: return (index, None, \'timeout\') except Exception as ex: return (index, None, ex) # Capture any other exception tasks = [run_job(index, job) for index, job in enumerate(jobs)] results = await asyncio.gather(*tasks) successful_results = [] timed_out_jobs = [] for index, result, error in results: if error == \'timeout\': timed_out_jobs.append(index) elif error is None: successful_results.append(result) return successful_results, timed_out_jobs, len(successful_results)"},{"question":"# ABCs and Custom Iterable Class You need to create a custom collection class that mimics the behavior of a Python list but uses the abstract base classes provided by the `collections.abc` module to ensure the class meets certain collection interfaces. Specifically, you will be creating a class called `CustomList` that behaves like a list, but also needs to pass various interface checks by using appropriate ABCs. Requirements 1. Your `CustomList` class should inherit directly from `collections.abc.MutableSequence`. 2. The class should implement all the required abstract methods of the `MutableSequence` ABC: - `__getitem__(self, index)` - `__setitem__(self, index, value)` - `__delitem__(self, index)` - `__len__(self)` - `insert(self, index, value)` 3. The class should support the following operations: - Append an item to the end of the list. - Remove an item at a specified index. - Get the length of the list. - Iterate over the list. - Check if an item exists in the list. 4. Ensure that instances of `CustomList` are recognized as `Sequence` and `Collection` instances. Input and Output - **Input**: Instantiate `CustomList` and perform various operations like `append`, `remove`, and membership check. - **Output**: The correct functionality of the list methods and the class\'s recognition as per the given abstract base classes. Example ```python from collections.abc import MutableSequence class CustomList(MutableSequence): def __init__(self): self._items = [] def __getitem__(self, index): return self._items[index] def __setitem__(self, index, value): self._items[index] = value def __delitem__(self, index): del self._items[index] def __len__(self): return len(self._items) def insert(self, index, value): self._items.insert(index, value) def append(self, value): self._items.append(value) def remove(self, value): self._items.remove(value) # Test the CustomList lst = CustomList() lst.append(1) lst.append(2) lst.append(3) assert len(lst) == 3 assert lst[1] == 2 lst.remove(2) assert len(lst) == 2 ``` Validate that your `CustomList` works effectively with `issubclass` and `isinstance` checks for `Sequence` and `Collection`. ```python from collections.abc import Sequence, Collection assert issubclass(CustomList, Sequence) assert issubclass(CustomList, Collection) assert isinstance(lst, Sequence) assert isinstance(lst, Collection) ``` Constraints - Do not use the built-in list or any other list-like container from libraries. - Ensure the class is well-documented and methods are appropriately defined. Performance Requirements - The `append` and `remove` operations should work efficiently with typical list performance, ideally in constant time for `append` and linear time for `remove`.","solution":"from collections.abc import MutableSequence class CustomList(MutableSequence): def __init__(self): self._items = [] def __getitem__(self, index): return self._items[index] def __setitem__(self, index, value): self._items[index] = value def __delitem__(self, index): del self._items[index] def __len__(self): return len(self._items) def insert(self, index, value): self._items.insert(index, value) def append(self, value): self._items.append(value) def remove(self, value): self._items.remove(value)"},{"question":"# Problem: Nested List Manipulation **Objective**: Implement a function that manipulates nested lists based on user-defined conditions and transformations. **Function Signature**: ```python def manipulate_nested_lists(matrix: list, threshold: int) -> list: The function accepts a 2D list (matrix) and an integer threshold. It should: 1. Filter out rows from the matrix where the sum of elements is less than the threshold. 2. From the remaining rows, create a new matrix where each element is squared. 3. Return the transposed version of the new matrix. Parameters: matrix (list): A 2D list of integers. threshold (int): An integer threshold for filtering rows. Returns: list: A 2D list where each element is squared and the rows and columns are transposed. ``` **Input**: 1. `matrix`: A 2D list of integers, where each sublist represents a row in the matrix. - Example: `[[1, 2, 3], [4, 5, 6], [7, 8, 9]]`. 2. `threshold`: An integer value. - Example: `15`. **Output**: 1. A 2D list of integers which is the transformed and transposed version of the input matrix. - Example: `[[16, 49], [25, 64], [36, 81]]`. **Constraints**: 1. Each sublist in the matrix contains integers only. 2. The matrix will contain at least one row, and each row will contain at least one element. 3. The threshold will be a non-negative integer. **Example**: ```python matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] threshold = 15 # Step 1: Filter rows where sum of elements >= threshold # Resulting Matrix: [[4, 5, 6], [7, 8, 9]] # Step 2: Square each element # Resulting Matrix: [[16, 25, 36], [49, 64, 81]] # Step 3: Transpose the matrix # Resulting Output: [[16, 49], [25, 64], [36, 81]] assert manipulate_nested_lists(matrix, threshold) == [[16, 49], [25, 64], [36, 81]] ``` **Notes**: - You are encouraged to use list comprehensions for concise and readable code. - Ensure the function handles edge cases, such as empty sublists or matrices with only one row or column.","solution":"def manipulate_nested_lists(matrix: list, threshold: int) -> list: Manipulates the given 2D list (matrix) based on the threshold and transformation conditions. Parameters: - matrix (list): A 2D list of integers. - threshold (int): An integer threshold for filtering rows. Returns: - list: A 2D list where each element is squared and the rows and columns are transposed. # Step 1: Filter out rows where the sum of elements is less than the threshold filtered_matrix = [row for row in matrix if sum(row) >= threshold] # Step 2: Square each element in the filtered matrix squared_matrix = [[element ** 2 for element in row] for row in filtered_matrix] # Step 3: Transpose the squared matrix if not squared_matrix: return [] transposed_matrix = list(map(list, zip(*squared_matrix))) return transposed_matrix"},{"question":"**Question: Transforming Labels with Sklearn Preprocessing** In this assessment, you are required to demonstrate your understanding of various label transformation techniques provided by scikit-learn. Implement the following functions using `sklearn.preprocessing` module. 1. **Function Name**: `binarize_labels` - **Input**: - `labels` (List of integers): A list of integer labels to binarize. - **Output**: - A binary indicator matrix (2D numpy array) corresponding to the input labels. - **Constraints**: - The integer labels provided are unique and represent different classes. - **Requirements**: - Use `LabelBinarizer` to convert the input labels to a binary indicator matrix. - **Example**: ```python labels = [1, 2, 6, 4, 2] result = binarize_labels(labels) # Expected Output: # array([[1, 0, 0, 0], # [0, 0, 0, 1]]) ``` 2. **Function Name**: `binarize_multilabels` - **Input**: - `multilabels` (List of list of integers): A list where each element is a list representing the labels of a sample. - **Output**: - A binary indicator matrix (2D numpy array) corresponding to the input multilabels. - **Constraints**: - Each inner list represents labels of a single sample and all inner lists can have different lengths. - **Requirements**: - Use `MultiLabelBinarizer` to convert the input multilabels to a binary indicator matrix. - **Example**: ```python multilabels = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]] result = binarize_multilabels(multilabels) # Expected Output: # array([[0, 0, 1, 1, 1], # [0, 0, 1, 0, 0], # [1, 1, 0, 1, 0], # [1, 1, 1, 1, 1], # [1, 1, 1, 0, 0]]) ``` 3. **Function Name**: `encode_labels` - **Input**: - `labels` (List of unhashable elements): A list of labels that need encoding. - **Output**: - A tuple containing: - Encoded labels (numpy array) - Original labels (list) corresponding to each encoded label. - **Constraints**: - The labels can be of any hashable and comparable type (e.g., strings, integers). - **Requirements**: - Use `LabelEncoder` to transform the input labels to normalized numerical labels. - Provide both the encoded labels and original labels. - **Example**: ```python labels = [\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"] encoded_labels, original_labels = encode_labels(labels) # Expected Output: # (array([1, 1, 2, 0]), [\'amsterdam\', \'paris\', \'tokyo\']) ``` You should use the `sklearn.preprocessing` module for implementing these functions. Ensure that your implementation adheres to the example outputs provided.","solution":"import numpy as np from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder def binarize_labels(labels): Binarize a list of integer labels into a binary indicator matrix. :param labels: List of integers representing different classes :return: 2D numpy array, binary indicator matrix lb = LabelBinarizer() binarized_labels = lb.fit_transform(labels) return binarized_labels def binarize_multilabels(multilabels): Binarize a list of list of labels into a binary indicator matrix for multi-label classification. :param multilabels: List of lists of integers representing different classes for each sample :return: 2D numpy array, binary indicator matrix mlb = MultiLabelBinarizer() binarized_multilabels = mlb.fit_transform(multilabels) return binarized_multilabels def encode_labels(labels): Encode a list of unhashable labels into normalized numerical labels. :param labels: List of labels that need encoding (can be of any hashable type) :return: Tuple containing: - Encoded labels (numpy array) - Original labels (list) corresponding to each encoded label le = LabelEncoder() encoded_labels = le.fit_transform(labels) original_labels = list(le.classes_) return encoded_labels, original_labels"},{"question":"# Question: Suppose you are given a dataset of car performance metrics with the following columns: `manufacturer`, `model`, `mpg` (miles per gallon), `horsepower`, and `origin` (origin of the car). You need to visualize the data to compare the `mpg` of cars from different manufacturers, split by their `origin`. You are required to: 1. Load the dataset from a provided URL. 2. Create a bar plot to show the average `mpg` for each `manufacturer`, distinguishing the cars by their `origin`. 3. Employ necessary transformations to avoid overlapping bars. 4. Add error bars representing the standard deviation of `mpg` for each `manufacturer` split by `origin`. 5. Customize the bar plot by setting different colors and edge styles for the cars from different origins. Your final visualization should clearly show: - Average `mpg` per `manufacturer` categorized by `origin`. - Error bars indicating the standard deviation. **Input Format:** - The dataset will be loaded from the provided URL: ```python url = \\"https://example.com/car_performance.csv\\" ``` **Output Format:** - Your function `visualize_car_performance()` should generate and display the required bar plot. **Constraints:** - Handle missing values by excluding any rows with `NaN` in critical columns. Here is an example structure of your solution: ```python import seaborn.objects as so import pandas as pd def visualize_car_performance(): url = \\"https://example.com/car_performance.csv\\" df = pd.read_csv(url) # Exclude rows with NaN values df = df.dropna(subset=[\\"mpg\\", \\"manufacturer\\", \\"origin\\"]) # Create the bar plot with seaborn objects ( so.Plot(df, x=\\"manufacturer\\", y=\\"mpg\\", color=\\"origin\\") .add(so.Bar(edgewidth=2), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) .show() ) # Call the function to visualize visualize_car_performance() ``` **Notes:** - Ensure you use the seaborn `objects` interface for creating the plot. - You may assume that the dataset at the given URL has the appropriate format. - Customize the appearance of the plot to make it more informative and visually appealing.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_car_performance(): url = \\"https://example.com/car_performance.csv\\" df = pd.read_csv(url) # Exclude rows with NaN values in critical columns df = df.dropna(subset=[\'mpg\', \'manufacturer\', \'origin\']) # Calculate mean and standard deviation for mpg by manufacturer and origin summary_df = df.groupby([\'manufacturer\', \'origin\']).agg( mean_mpg=(\'mpg\', \'mean\'), std_mpg=(\'mpg\', \'std\')).reset_index() # Create a bar plot with error bars plt.figure(figsize=(14, 8)) sns.barplot( data=summary_df, x=\'manufacturer\', y=\'mean_mpg\', hue=\'origin\', ci=None, palette=\\"Set2\\", capsize=0.1 ) # Add error bars for i, row in summary_df.iterrows(): plt.errorbar( x=row[\'manufacturer\'], y=row[\'mean_mpg\'], yerr=row[\'std_mpg\'], fmt=\'none\', c=\'black\' ) plt.title(\'Average MPG of Cars by Manufacturer and Origin\') plt.xlabel(\'Manufacturer\') plt.ylabel(\'Average MPG\') plt.xticks(rotation=90) plt.legend(title=\'Origin\') plt.tight_layout() plt.show()"},{"question":"Custom ExtensionArray Implementation Objective: Implement a custom `ExtensionArray` subclass in pandas, demonstrating the use of key extension concepts like registering custom dtypes and handling array methods. Problem Statement: You are required to implement a custom pandas `ExtensionArray` for handling a special numeric type, `SpecialNumeric`, which is essentially a float but with additional metadata indicating if the number is \\"special\\". Your task is to: 1. Define the `SpecialNumericDtype` as a pandas extension dtype. 2. Implement a `SpecialNumericArray` as an `ExtensionArray` to hold values of `SpecialNumeric`. 3. Provide implementations for essential methods such as `__getitem__`, `__len__`, `take`, and `__setitem__`, among others, to ensure the array behaves correctly within pandas\' framework. 4. Register these new types with pandas. Requirements: 1. Your `SpecialNumericDtype` should: - Be derived from `pandas.api.extensions.ExtensionDtype`. - Include necessary dtype properties (`type`, `name`, `na_value`, etc.). 2. Your `SpecialNumericArray` should: - Be derived from `pandas.api.extensions.ExtensionArray`. - Contain an internal numpy array to store float values and a boolean array to mark if a value is \\"special\\". - Implement necessary methods such as `__len__`, `__getitem__`, `__setitem__`, `take`, `fillna`, `unique`, `isna`. - Handle functionality for operations like `astype`, `copy`, `equals`, `insert`, `searchsorted`, and `shift`. Constraints: 1. Assume the internal array is fixed size for simplicity. 2. The performance is secondary but should be reasonably efficient for array operations. Input Format: - Methods within your `SpecialNumericArray` class will be tested using various inputs, ensuring compatibility with typical pandas operations. - Example input might include creating an array from a sequence, accessing elements, slicing, and performing typical extension array operations. Output Format: - Your dtype and array should seamlessly integrate with pandas, supporting operations like `pd.Series`, `pd.DataFrame`, and array methods tested within pandas. Example: ```python class SpecialNumeric: def __init__(self, value, special=False): self.value = float(value) self.special = special class SpecialNumericDtype(pd.api.extensions.ExtensionDtype): # Implement dtype related properties and methods class SpecialNumericArray(pd.api.extensions.ExtensionArray): # Implement necessary array methods # Register dtype with pandas pd.api.extensions.register_extension_dtype(SpecialNumericDtype) # Example usage within pandas after implementation data = SpecialNumericArray([SpecialNumeric(1.1), SpecialNumeric(2.2, special=True), SpecialNumeric(3.3)]) series = pd.Series(data) print(series) ``` Evaluate the successful integration by performing pandas operations and ensure they work as expected.","solution":"import numpy as np import pandas as pd from pandas.api.extensions import ExtensionArray, ExtensionDtype class SpecialNumeric: def __init__(self, value, special=False): self.value = float(value) self.special = special class SpecialNumericDtype(ExtensionDtype): name = \'special_numeric\' type = SpecialNumeric kind = \'O\' na_value = SpecialNumeric(np.nan, special=False) @classmethod def construct_array_type(cls): return SpecialNumericArray class SpecialNumericArray(ExtensionArray): def __init__(self, values): self._values = np.array([x.value for x in values]) self._special = np.array([x.special for x in values]) def __len__(self): return len(self._values) def __getitem__(self, item): if isinstance(item, int): return SpecialNumeric(self._values[item], self._special[item]) else: return SpecialNumericArray([SpecialNumeric(self._values[i], self._special[i]) for i in range(len(self._values))][item]) def __setitem__(self, key, value): self._values[key] = value.value self._special[key] = value.special def take(self, indices, allow_fill=False, fill_value=None): if allow_fill: fill_value = fill_value if fill_value is not None else self.dtype.na_value fill_value = fill_value if isinstance(fill_value, SpecialNumeric) else SpecialNumeric(fill_value) new_values = np.array([ self._values[i] if i != -1 else fill_value.value for i in indices ]) new_special = np.array([ self._special[i] if i != -1 else fill_value.special for i in indices ]) else: new_values = self._values.take(indices) new_special = self._special.take(indices) return SpecialNumericArray([SpecialNumeric(new_values[i], new_special[i]) for i in range(len(new_values))]) def isna(self): return np.isnan(self._values) def fillna(self, value=None, method=None, limit=None): if value is not None: value = value if isinstance(value, SpecialNumeric) else SpecialNumeric(value) self._values[np.isnan(self._values)] = value.value self._special[np.isnan(self._values)] = value.special return self def unique(self): uniq_vals, indices = np.unique(self._values, return_index=True) uniq_special = self._special[indices] return SpecialNumericArray([SpecialNumeric(uniq_vals[i], uniq_special[i]) for i in range(len(uniq_vals))]) def __array__(self, dtype=None): return self._values @property def dtype(self): return SpecialNumericDtype() def to_numpy(self, dtype=None, copy=False): return self._values # Register dtype with pandas pd.api.extensions.register_extension_dtype(SpecialNumericDtype)"},{"question":"# PyTorch Coding Assessment Objective Your task is to demonstrate your understanding of TorchScript by implementing and optimizing a simple neural network using TorchScript. Problem Statement You are required to implement a simple feedforward neural network to classify a set of points into two categories. After implementing the network in PyTorch, you will convert the model to TorchScript and evaluate its performance. Instructions 1. **Implement the Neural Network**: - Define a neural network in PyTorch with the following architecture: - An input layer for 2D points. - Two hidden layers with 10 neurons each and ReLU activations. - An output layer with a single neuron and a Sigmoid activation function. - Name this class `SimpleNN`. 2. **Convert to TorchScript**: - Convert your `SimpleNN` model to a TorchScript module using the `torch.jit.script` method. - Name the TorchScript module `scripted_model`. 3. **Input and Output Format**: - Your neural network should accept a tensor of shape `(N, 2)` where `N` is the number of points. - The output should be a tensor of shape `(N, 1)` with the predicted class probabilities. 4. **Performance Requirements**: - Ensure that the TorchScript model performs inference efficiently. You may measure the performance by timing the inference on a large batch of test inputs. - Provide the code for performing inference on a batch of size 1000 using both the original and the TorchScript model and compare their execution times. 5. **Constraints**: - You must use ReLU activation in the hidden layers and Sigmoid activation in the output layer. - Use a fixed random seed for reproducibility. Here is a template to get you started: ```python import torch import torch.nn as nn import torch.jit # Step 1: Implement the Neural Network class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.layer1 = nn.Linear(2, 10) self.layer2 = nn.Linear(10, 10) self.output = nn.Linear(10, 1) self.relu = nn.ReLU() self.sigmoid = nn.Sigmoid() def forward(self, x): x = self.relu(self.layer1(x)) x = self.relu(self.layer2(x)) x = self.sigmoid(self.output(x)) return x # Initialize the model model = SimpleNN() print(model) # Step 2: Convert to TorchScript scripted_model = torch.jit.script(model) print(scripted_model) # Step 3: Define input tensor input_tensor = torch.randn(1000, 2) # Batch of 1000 points # Step 4: Measure performance import time # Original model performance start = time.time() output = model(input_tensor) end = time.time() print(f\\"Original model inference time: {end - start} seconds\\") # TorchScript model performance start = time.time() scripted_output = scripted_model(input_tensor) end = time.time() print(f\\"TorchScript model inference time: {end - start} seconds\\") # Ensure output shapes match assert output.shape == scripted_output.shape ``` In your submission, include: - The implementation of the `SimpleNN` class. - The conversion to TorchScript module. - Inference performance comparison code and the resulting times.","solution":"import torch import torch.nn as nn import torch.jit # Step 1: Implement the Neural Network class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.layer1 = nn.Linear(2, 10) self.layer2 = nn.Linear(10, 10) self.output = nn.Linear(10, 1) self.relu = nn.ReLU() self.sigmoid = nn.Sigmoid() def forward(self, x): x = self.relu(self.layer1(x)) x = self.relu(self.layer2(x)) x = self.sigmoid(self.output(x)) return x # Initialize the model model = SimpleNN() # Step 2: Convert to TorchScript scripted_model = torch.jit.script(model) # Step 3: Define input tensor input_tensor = torch.randn(1000, 2) # Batch of 1000 points # Step 4: Measure performance import time # Ensure the same device for both models device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") model.to(device) scripted_model.to(device) input_tensor = input_tensor.to(device) # Original model performance start = time.time() output = model(input_tensor) end = time.time() original_model_time = end - start # TorchScript model performance start = time.time() scripted_output = scripted_model(input_tensor) end = time.time() scripted_model_time = end - start # Ensure output shapes match assert output.shape == scripted_output.shape # Output the inference times (original_model_time, scripted_model_time)"},{"question":"# Problem: Efficient Task Scheduler You are tasked with designing an efficient task scheduler that ensures tasks are executed based on their priority. Each task has a priority level and a duration. You need to implement a PriorityTaskScheduler class with the following methods: 1. `__init__(self)`: Initializes the priority task scheduler with an empty task list. 2. `add_task(self, task_name: str, priority: int) -> None`: Adds a task to the scheduler with the given task name and priority. 3. `get_next_task(self) -> str`: Returns and removes the highest priority task from the scheduler. If there are multiple tasks with the same priority, the one added first should be returned. If there are no tasks, it should raise an appropriate exception. 4. `remove_task(self, task_name: str) -> None`: Removes the specified task from the scheduler. If the task is not found, it should raise an appropriate exception. 5. `reschedule_task(self, task_name: str, new_priority: int) -> None`: Removes the specified task and re-adds it with a new priority. 6. `peek_next_task(self) -> str`: Returns the highest priority task without removing it. If there are multiple tasks with the same priority, the one added first should be returned. If there are no tasks, it should raise an appropriate exception. Example: ```python scheduler = PriorityTaskScheduler() scheduler.add_task(\\"Write report\\", 1) scheduler.add_task(\\"Fix bugs\\", 2) scheduler.add_task(\\"Plan sprint\\", 1) assert scheduler.get_next_task() == \\"Write report\\" scheduler.remove_task(\\"Fix bugs\\") scheduler.add_task(\\"Release product\\", 1) scheduler.reschedule_task(\\"Plan sprint\\", 3) assert scheduler.peek_next_task() == \\"Release product\\" assert scheduler.get_next_task() == \\"Release product\\" ``` # Constraints 1. Task names are unique. 2. Priorities are integers and can be positive or negative. 3. The implemented methods should have efficient time complexity considering the usage of heap operations (push, pop, etc.). 4. Assume the scheduler might handle thousands of tasks. # Implementation Notes: - Use the `heapq` module to maintain the priority queue efficiently. - Consider how to handle task removal and rescheduling to maintain the heap property. # Submission: - Implement the `PriorityTaskScheduler` class in Python. - Ensure all methods are tested, and the assertions in the example pass. - The solution should not only be correct but also demonstrate understanding and efficient use of the `heapq` module.","solution":"import heapq class TaskNotFoundError(Exception): pass class PriorityTaskScheduler: def __init__(self): self.task_list = [] self.task_map = {} self.counter = 0 def add_task(self, task_name: str, priority: int) -> None: if task_name in self.task_map: raise ValueError(f\\"Task {task_name} already exists.\\") heapq.heappush(self.task_list, (priority, self.counter, task_name)) self.task_map[task_name] = (priority, self.counter) self.counter += 1 def get_next_task(self) -> str: if not self.task_list: raise TaskNotFoundError(\\"No tasks available.\\") priority, count, task_name = heapq.heappop(self.task_list) self.task_map.pop(task_name, None) return task_name def remove_task(self, task_name: str) -> None: if task_name not in self.task_map: raise TaskNotFoundError(f\\"Task {task_name} not found.\\") priority, count = self.task_map.pop(task_name) self.task_list = [(p, c, t) for p, c, t in self.task_list if t != task_name] heapq.heapify(self.task_list) def reschedule_task(self, task_name: str, new_priority: int) -> None: self.remove_task(task_name) self.add_task(task_name, new_priority) def peek_next_task(self) -> str: if not self.task_list: raise TaskNotFoundError(\\"No tasks available.\\") priority, count, task_name = self.task_list[0] return task_name"},{"question":"You are given a task to simulate a simple producer-consumer problem using `asyncio` synchronization primitives. There are one producer and multiple consumers. The producer generates data and places it into a shared buffer, while the consumers take data from the buffer and process it. You will use the asyncio primitives `Lock`, `Condition`, and `Semaphore` to implement this. **Specifications:** 1. Implement an `asyncio` class `ProducerConsumer` with the following methods: * `__init__(buffer_size: int)`: Initializes the buffer of the given size, a lock, and condition variables. * `produce(item: int)`: Adds an item to the buffer. If the buffer is full, it should wait until space is available. * `consume() -> int`: Removes and returns an item from the buffer. If the buffer is empty, it should wait until an item is available. * `run_producer(num_items: int)`: Simulates the producer producing `num_items`. * `run_consumer(consumer_id: int, num_items: int)`: Simulates the consumer consuming `num_items`. **Input and Output:** - The buffer size and items produced/consumed numbers will be provided by unit tests. - The `produce` method will be called with an integer value to add to the buffer. - The `consume` method will return an integer value from the buffer when called. - `run_producer` and `run_consumer` methods will ensure the producer and consumer tasks run until all items are produced and consumed respectively. **Constraints:** - The buffer size will be at least 1. - There will be at least one producer and one consumer. **Performance Requirements:** - Ensure no data is lost or corrupted. - Ensure all items get produced and then consumed based on the given constraints. **Example:** Below is an example of how this class might be used: ```python import asyncio async def main(): buffer_size = 5 num_items = 10 pc = ProducerConsumer(buffer_size) producer_task = asyncio.create_task(pc.run_producer(num_items)) consumer_tasks = [asyncio.create_task(pc.run_consumer(i, num_items // 2)) for i in range(2)] await producer_task await asyncio.gather(*consumer_tasks) asyncio.run(main()) ``` This code should create a producer that generates 10 items and two consumers that consume 5 items each, ensuring synchronization and proper handling of buffer limits. **Note:** Your solution should be self-contained and only use the asyncio primitives discussed in the documentation.","solution":"import asyncio class ProducerConsumer: def __init__(self, buffer_size: int): self.buffer_size = buffer_size self.buffer = [] self.buffer_lock = asyncio.Lock() self.not_empty = asyncio.Condition(self.buffer_lock) self.not_full = asyncio.Condition(self.buffer_lock) async def produce(self, item: int): async with self.not_full: while len(self.buffer) >= self.buffer_size: await self.not_full.wait() self.buffer.append(item) self.not_empty.notify() async def consume(self) -> int: async with self.not_empty: while len(self.buffer) == 0: await self.not_empty.wait() item = self.buffer.pop(0) self.not_full.notify() return item async def run_producer(self, num_items: int): for item in range(num_items): await self.produce(item) async def run_consumer(self, consumer_id: int, num_items: int): for _ in range(num_items): item = await self.consume() # simulate processing an item print(f\\"Consumer {consumer_id} consumed {item}\\")"},{"question":"You are provided with the requirement to train a PyTorch model using distributed training to speed up the training process. You are to leverage the `torchrun` tool from the `torch.distributed.run` module to train a simple neural network across multiple processes. Task Write a Python script that uses the `torchrun` tool to perform distributed training of a simple neural network on a toy dataset. # Requirements 1. **Initialization**: - Use `torch.distributed` to set up distributed training. - Ensure each process can correctly identify its rank and size (total number of processes). 2. **Neural Network**: - Define a simple fully connected neural network using `torch.nn.Module`. - Use the network to train on a synthetic dataset created using random tensors. 3. **Training Loop**: - Implement the training loop so that each process participates in updating the model parameters. - Use `torch.optim` for the optimizer and a standard loss function from `torch.nn`. 4. **Synchronization**: - Ensure that gradients are synchronized across processes using `torch.distributed`. 5. **Execution with `torchrun`**: - Include a script entry point that initializes the training process using `torchrun`. # Constraints - Assume your environment has 4 GPUs available. - You should handle the device allocation for each process. - The script should be executable using a command similar to: ```bash torchrun --nproc_per_node=4 --nnodes=1 your_script.py ``` # Input and Output - No external input is required; the script should generate its own synthetic data for training. - The script should print out the loss value from each process during training for verification. # Performance Consideration - Ensure the script runs efficiently with minimal inter-process communication overhead. # Example Usage An example command to run your script would be: ```bash torchrun --nproc_per_node=4 --nnodes=1 your_script.py ``` # Important Notes - Clearly comment your code to explain the setup and each significant step. - Test the script in a distributed training environment to ensure correctness.","solution":"import torch import torch.nn as nn import torch.optim as optim import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP import os # Set this environment variable for `torchrun` os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' def setup(rank, world_size): Initializes distributed training. dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) torch.manual_seed(42) def cleanup(): Cleans up the distributed training process. dist.destroy_process_group() class SimpleNN(nn.Module): A simple fully connected neural network. def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def generate_synthetic_data(rank, world_size): Generates synthetic data for training. torch.manual_seed(rank) X = torch.randn(1000, 10) y = torch.randn(1000, 1) return X, y def train(rank, world_size): The training loop for the model. setup(rank, world_size) # Create model and move it to the appropriate device model = SimpleNN().to(rank) ddp_model = DDP(model, device_ids=[rank]) # Generate synthetic data X, y = generate_synthetic_data(rank, world_size) X, y = X.to(rank), y.to(rank) criterion = nn.MSELoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.01) for epoch in range(10): # Run 10 training epochs optimizer.zero_grad() outputs = ddp_model(X) loss = criterion(outputs, y) loss.backward() optimizer.step() if rank == 0: # Print loss from rank 0 print(f\'Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\') cleanup() if __name__ == \\"__main__\\": world_size = 4 # Number of processes torch.multiprocessing.spawn(train, args=(world_size,), nprocs=world_size, join=True)"},{"question":"You are given a CSV file containing information about several movies. The file contains the following columns: `Title`, `Year`, `Genre`, and `Rating`. Your task is to write a Python function that reads the CSV file, processes the data, and writes the processed data into a new CSV file. The processing includes standardizing the genre names (e.g., convert to lower case), filtering out movies with a rating below 7.0, and sorting the remaining movies by their rating in descending order. Your function should follow these specifications: 1. **Input**: - The input CSV file path. - The output CSV file path. 2. **Output**: - A CSV file at the output path with the processed and sorted movie data. 3. **Processing Steps**: - Read the input CSV file using `csv.DictReader`. - Standardize the genre names by converting them to lowercase. - Filter out movies with a rating below 7.0. - Sort the remaining movies by their rating in descending order. - Write the processed data to the output CSV file using `csv.DictWriter`. 4. **Constraints**: - You must use the `csv` module for reading and writing CSV files. - Handle any potential CSV errors gracefully (e.g., missing columns, invalid data). 5. **Performance Requirements**: - The solution should be efficient in terms of time and space complexity. **Function Signature**: ```python def process_movie_csv(input_csv_path: str, output_csv_path: str) -> None: pass ``` **Example**: Assume the input CSV file `movies.csv` contains the following data: ``` Title,Year,Genre,Rating Inception,2010,Sci-Fi,8.8 The Dark Knight,2008,Action,9.0 Interstellar,2014,Sci-Fi,8.6 The Room,2003,Drama,3.7 Pulp Fiction,1994,Crime,8.9 ``` After processing, the output CSV file should look like: ``` Title,Year,Genre,Rating The Dark Knight,2008,action,9.0 Pulp Fiction,1994,crime,8.9 Inception,2010,sci-fi,8.8 Interstellar,2014,sci-fi,8.6 ``` **Notes**: - Ensure that the column headers in the output file match those in the input file. - Ensure that movies with ratings exactly 7.0 are included in the output. - You can assume the input file is well-formed with the correct column headers.","solution":"import csv def process_movie_csv(input_csv_path: str, output_csv_path: str) -> None: with open(input_csv_path, newline=\'\', encoding=\'utf-8\') as csvfile: reader = csv.DictReader(csvfile) movies = [] for row in reader: try: rating = float(row[\'Rating\']) if rating >= 7.0: row[\'Genre\'] = row[\'Genre\'].lower() movies.append(row) except ValueError: # Skip rows where \'Rating\' is not a number continue movies.sort(key=lambda x: float(x[\'Rating\']), reverse=True) with open(output_csv_path, \'w\', newline=\'\', encoding=\'utf-8\') as csvfile: fieldnames = [\'Title\', \'Year\', \'Genre\', \'Rating\'] writer = csv.DictWriter(csvfile, fieldnames=fieldnames) writer.writeheader() for movie in movies: writer.writerow(movie)"},{"question":"**Title:** Sound File Metadata Extractor **Objective:** Write a function to read a directory of sound files, determine their types and metadata using the deprecated `sndhdr` module, and organize the results in a specified format. **Description:** You are given a directory that contains multiple sound files of various formats. Your task is to write a Python function that scans this directory, determines the type and metadata for each sound file, and returns a summary in the form of a list of dictionaries. Each dictionary should correspond to one sound file and include the following keys: `filename`, `filetype`, `framerate`, `nchannels`, `nframes`, and `sampwidth`. Use the `sndhdr` module to determine the type and metadata of each sound file in the directory. If a file\'s type cannot be determined, its entry in the summary list should include `None` for the `filetype` and other metadata fields. **Function Signature:** ```python def summarize_sound_files(directory: str) -> list: Scans a directory of sound files and returns a summary of their types and metadata. Parameters: directory (str): The path to the directory containing sound files. Returns: list: A list of dictionaries containing sound file metadata. ``` **Input:** - `directory`: A string representing the path to the directory containing the sound files. The directory will contain only sound files. **Output:** - A list of dictionaries, where each dictionary contains the following keys: - `filename` (str): The name of the sound file. - `filetype` (str or None): The type of sound file. If the type cannot be determined, this should be `None`. - `framerate` (int): The frame rate of the sound file. If unknown, should be `0`. - `nchannels` (int): The number of channels in the sound file. If unknown, should be `0`. - `nframes` (int): The number of frames in the sound file. If unknown, should be `-1`. - `sampwidth` (int or str): The sample width in bits. Can be an integer value or \'A\'/\'U\' for specific formats. **Example:** ```python # Example directory structure: # sound_files/ # file1.wav # file2.aifc # file3.unknown directory = \\"sound_files\\" # Expected output format (actual values will depend on file headers): [ { \\"filename\\": \\"file1.wav\\", \\"filetype\\": \\"wav\\", \\"framerate\\": 44100, \\"nchannels\\": 2, \\"nframes\\": 100000, \\"sampwidth\\": 16 }, { \\"filename\\": \\"file2.aifc\\", \\"filetype\\": \\"aifc\\", \\"framerate\\": 22050, \\"nchannels\\": 1, \\"nframes\\": -1, \\"sampwidth\\": 8 }, { \\"filename\\": \\"file3.unknown\\", \\"filetype\\": None, \\"framerate\\": 0, \\"nchannels\\": 0, \\"nframes\\": -1, \\"sampwidth\\": None } ] ``` **Constraints:** - You may assume that the directory contains only files and no subdirectories. - The function should handle a reasonable number of files efficiently. - The function should correctly handle files with unknown or unsupported formats. **Notes:** - Be aware that the `sndhdr` module is deprecated, but you are required to use it for this task. - You should handle any exceptions gracefully and ensure that if the metadata cannot be extracted, the returned values are set accordingly (as specified). Good luck!","solution":"import os import sndhdr import wave import aifc def summarize_sound_files(directory: str) -> list: Scans a directory of sound files and returns a summary of their types and metadata. Parameters: directory (str): The path to the directory containing sound files. Returns: list: A list of dictionaries containing sound file metadata. summary = [] for filename in os.listdir(directory): filepath = os.path.join(directory, filename) file_info = sndhdr.what(filepath) if file_info: filetype = file_info.filetype try: if filetype == \'wav\': with wave.open(filepath, \'rb\') as wf: metadata = { \\"filename\\": filename, \\"filetype\\": filetype, \\"framerate\\": wf.getframerate(), \\"nchannels\\": wf.getnchannels(), \\"nframes\\": wf.getnframes(), \\"sampwidth\\": wf.getsampwidth() * 8 # Convert sample width to bits } elif filetype == \'aifc\': with aifc.open(filepath, \'rb\') as af: metadata = { \\"filename\\": filename, \\"filetype\\": filetype, \\"framerate\\": af.getframerate(), \\"nchannels\\": af.getnchannels(), \\"nframes\\": af.getnframes(), \\"sampwidth\\": af.getsampwidth() * 8 # Convert sample width to bits } else: metadata = { \\"filename\\": filename, \\"filetype\\": filetype, \\"framerate\\": 0, \\"nchannels\\": 0, \\"nframes\\": -1, \\"sampwidth\\": None } except Exception as e: metadata = { \\"filename\\": filename, \\"filetype\\": None, \\"framerate\\": 0, \\"nchannels\\": 0, \\"nframes\\": -1, \\"sampwidth\\": None } else: metadata = { \\"filename\\": filename, \\"filetype\\": None, \\"framerate\\": 0, \\"nchannels\\": 0, \\"nframes\\": -1, \\"sampwidth\\": None } summary.append(metadata) return summary"},{"question":"**Understanding and Manipulating Python Types with the `types` Module** # Objective The goal of this task is to assess your ability to dynamically create and manipulate Python types using the `types` module. You will also need to demonstrate your understanding of various utilities provided by this module. # Problem Statement You need to implement a function `create_dynamic_class(name: str, base_classes: tuple, attributes: dict) -> type` that dynamically creates a class using the `types.new_class()` function. Additionally, you will implement another function `understand_types(obj: object) -> dict` that examines an object and returns information about its type. # Requirements 1. **Function: `create_dynamic_class(name: str, base_classes: tuple, attributes: dict) -> type`** - **Input**: - `name` (str): The name of the new class. - `base_classes` (tuple): A tuple containing base classes for the new class. - `attributes` (dict): A dictionary where keys are attribute/method names, and values are the respective attribute values or method functions. - **Output**: - Returns the newly created class. - **Constraints**: - `name` should be a valid Python identifier. - All items in `base_classes` must be new-style classes. - Attribute names should be valid Python identifiers. - **Example**: ```python cls = create_dynamic_class(\'MyClass\', (object,), {\'x\': 42, \'greet\': lambda self: \'Hello\'}) obj = cls() print(obj.x) # Output: 42 print(obj.greet()) # Output: Hello ``` 2. **Function: `understand_types(obj: object) -> dict`** - **Input**: - `obj` (object): The object whose type information is to be determined. - **Output**: - Returns a dictionary containing the type information as follows: - `\'is_function\'`: `True` or `False` if the object is of `FunctionType`. - `\'is_generator\'`: `True` or `False` if the object is of `GeneratorType`. - `\'is_async_generator\'`: `True` or `False` if the object is of `AsyncGeneratorType`. - `\'is_coroutine\'`: `True` or `False` if the object is of `CoroutineType`. - `\'is_module\'`: `True` or `False` if the object is of `ModuleType`. - **Constraints**: - The object can be of any type. - **Example**: ```python def sample_function(): yield 1 result = understand_types(sample_function) print(result) # Output: {\'is_function\': True, \'is_generator\': False, \'is_async_generator\': False, \'is_coroutine\': False, \'is_module\': False} ``` # Performance Requirements - Your implementation should be efficient and handle various edge cases gracefully. - Make sure to handle invalid inputs by raising appropriate exceptions. You can use the `types` module to accomplish the tasks. Good luck!","solution":"import types def create_dynamic_class(name: str, base_classes: tuple, attributes: dict) -> type: Dynamically creates a class using the types.new_class() function. Args: - name (str): The name of the new class. - base_classes (tuple): A tuple containing base classes for the new class. - attributes (dict): A dictionary where keys are attribute/method names, and values are the respective attribute values or method functions. Returns: - type: The newly created class. new_class = types.new_class(name, base_classes) for attr_name, attr_value in attributes.items(): setattr(new_class, attr_name, attr_value) return new_class def understand_types(obj: object) -> dict: Examines an object and returns information about its type. Args: - obj (object): The object whose type information is to be determined. Returns: - dict: A dictionary containing the type information. return { \'is_function\': isinstance(obj, types.FunctionType), \'is_generator\': isinstance(obj, types.GeneratorType), \'is_async_generator\': isinstance(obj, types.AsyncGeneratorType), \'is_coroutine\': isinstance(obj, types.CoroutineType), \'is_module\': isinstance(obj, types.ModuleType), }"},{"question":"You are required to implement a function that performs basic audio processing on AIFF/AIFC files. The function will apply a simple volume adjustment (amplification or attenuation) to the audio data. Implement a function `adjust_volume(input_filepath: str, output_filepath: str, volume_factor: float) -> None` that does the following: 1. Reads the input AIFF/AIFC file. 2. Adjusts the volume of the audio data by the specified `volume_factor`. If `volume_factor` is greater than 1, the audio is amplified; if it\'s between 0 and 1, the audio is attenuated. 3. Writes the adjusted audio data to the output AIFF/AIFC file, preserving the original audio file properties. # Input: - `input_filepath`: A string representing the path to the input AIFF/AIFC file. - `output_filepath`: A string representing the path to the output AIFF/AIFC file. - `volume_factor`: A float representing the factor by which to adjust the volume (e.g., 1.5 to increase the volume by 50%, 0.5 to decrease it by 50%). # Output: - The function should write the adjusted audio data to the file specified by `output_filepath`. # Constraints: - Assume the audio files are valid AIFF/AIFC files. - The input file is readable and the output location is writable. - The volume factor will be a positive float. # Example: ```python adjust_volume(\\"input.aiff\\", \\"output.aiff\\", 1.5) ``` This example reads the file \\"input.aiff\\", amplifies the volume by 50%, and writes the result to \\"output.aiff\\". # Additional Notes: - You may use the `aifc` module as described in the provided documentation. - Consider handling both mono and stereo audio files. - Ensure proper handling of file opening and closing, especially in the case of exceptions.","solution":"import aifc import numpy as np def adjust_volume(input_filepath: str, output_filepath: str, volume_factor: float) -> None: Adjust the volume of an AIFF/AIFC file and save the adjusted audio to a new file. input_filepath: path to the input AIFF/AIFC file. output_filepath: path to the output AIFF/AIFC file. volume_factor: factor by which to adjust the volume (e.g., 1.5 for 50% increase). # Read input AIFF/AIFC file with aifc.open(input_filepath, \'r\') as infile: # Extract input parameters n_channels = infile.getnchannels() sample_width = infile.getsampwidth() n_frames = infile.getnframes() frame_rate = infile.getframerate() audio_data = infile.readframes(n_frames) # Convert audio data to numpy array for processing fmt = {1: np.int8, 2: np.int16, 3: np.int32}[sample_width] audio_array = np.frombuffer(audio_data, fmt) # Apply volume adjustment adjusted_array = np.clip(audio_array * volume_factor, -2**(8*sample_width-1), 2**(8*sample_width-1)-1).astype(fmt) # Write the adjusted audio back to the output file with aifc.open(output_filepath, \'w\') as outfile: outfile.setnchannels(n_channels) outfile.setsampwidth(sample_width) outfile.setframerate(frame_rate) outfile.writeframes(adjusted_array.tobytes())"},{"question":"**Question Title: Non-Blocking I/O with `select` Module** **Objective:** Implement a non-blocking server in Python that can handle multiple clients simultaneously using the `select` module. **Problem Statement:** You are required to write a Python function `start_server(host: str, port: int)` that starts a non-blocking server. This server should be able to handle multiple clients simultaneously and should use the `select` module to manage incoming connections and data. The function should: 1. Bind to the given host and port. 2. Listen for incoming connections. 3. Use the `select.select()` method to manage multiple client connections without blocking. 4. Handle read, write, and exceptional conditions appropriately. 5. Echo the received messages back to the clients (a simple echo server). **Function Signature:** ```python def start_server(host: str, port: int) -> None: pass ``` **Input:** - `host`: A string representing the host (e.g., `\'localhost\'`). - `port`: An integer representing the port number (e.g., `8080`). **Output:** - The function should not return anything. It should run indefinitely until manually terminated. **Constraints:** - You may assume that the `host` and `port` provided will be valid and available. - Use the `select.select()` method from the `select` module for managing I/O operations. - Handle up to 100 simultaneous client connections. **Example Usage:** ```python # This would ideally be run in a separate Python script. import threading # Start the server in a separate thread server_thread = threading.Thread(target=start_server, args=(\'localhost\', 8080)) server_thread.start() ``` **Important Notes:** - The server should be non-blocking and should efficiently handle multiple clients. - You may use additional libraries such as `socket` for managing socket connections. **Hint:** - Remember to handle edge cases, such as client disconnections and exceptional conditions on sockets. **Evaluation Criteria:** - Correctness: The implementation should correctly handle multiple clients and echo messages back. - Efficiency: The server should handle connections in a non-blocking manner using `select.select()`. - Robustness: The server should handle errors and edge cases gracefully.","solution":"import socket import select def start_server(host: str, port: int) -> None: Starts a non-blocking server that listens on the given host and port, handles multiple connections using the select module, and echoes received messages back to the clients. server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((host, port)) server_socket.listen(100) server_socket.setblocking(False) # List of sockets to monitor for incoming connections inputs = [server_socket] # Outgoing message queues (data to be sent to clients) message_queues = {} while True: # Wait for at least one of the sockets to be ready for processing readable, writable, exceptional = select.select(inputs, inputs, inputs) # Handle readable sockets (incoming data) for s in readable: if s is server_socket: # A \\"readable\\" server socket is ready to accept a new connection connection, client_address = s.accept() connection.setblocking(False) inputs.append(connection) message_queues[connection] = [] else: # A client socket is readable data = s.recv(1024) if data: # Data received; echo it back to the client message_queues[s].append(data) else: # No data means the connection is closed if s in inputs: inputs.remove(s) if s in message_queues: del message_queues[s] s.close() # Handle writable sockets (sending data) for s in writable: if s in message_queues: queue = message_queues[s] if queue: message = queue.pop(0) s.send(message) # Handle \\"exceptional condition\\" sockets for s in exceptional: inputs.remove(s) if s in message_queues: del message_queues[s] s.close()"},{"question":"**Objective:** Implement and evaluate a Support Vector Classification (SVC) model using scikit-learn to classify a dataset. **Problem Statement:** You are provided with a dataset containing features and target labels. Your task is to: 1. Preprocess the dataset by scaling the features. 2. Implement a Support Vector Classification model using the `SVC` class from scikit-learn. 3. Use grid search with cross-validation to find the best hyperparameters for the SVM model. 4. Evaluate the final model on the test set and report the classification accuracy. **Input:** - A CSV file `data.csv` with the following columns: - `feature_1`, `feature_2`, ..., `feature_n`: Feature columns - `label`: Target labels (binary classification: 0 or 1) **Expected Output:** - Print the best hyperparameters found by the grid search. - Print the classification accuracy of the final model on the test set. **Constraints:** 1. Use `StandardScaler` for feature scaling. 2. Use the RBF kernel for the SVM model. 3. The parameter grid for grid search should include: - `C`: [0.1, 1, 10, 100] - `gamma`: [1, 0.1, 0.01, 0.001] **Performance Requirements:** - The implementation should be efficient in terms of both time and memory. - Use cross-validation with 5 folds in the grid search. ```python import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score # Step 1: Load the dataset data = pd.read_csv(\'data.csv\') # Step 2: Split the dataset into features (X) and target labels (y) X = data.drop(\'label\', axis=1) y = data[\'label\'] # Step 3: Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 4: Create a pipeline with StandardScaler and SVC pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'svc\', SVC()) ]) # Step 5: Define the parameter grid for grid search param_grid = { \'svc__C\': [0.1, 1, 10, 100], \'svc__gamma\': [1, 0.1, 0.01, 0.001] } # Step 6: Implement grid search with cross-validation grid_search = GridSearchCV(pipeline, param_grid, cv=5, verbose=1) grid_search.fit(X_train, y_train) # Step 7: Find the best hyperparameters and print them best_params = grid_search.best_params_ print(\\"Best Hyperparameters:\\", best_params) # Step 8: Evaluate the final model on the test set and print the accuracy y_pred = grid_search.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(\\"Test Set Accuracy:\\", accuracy) ```","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score def run_svm_classification(file_path): # Step 1: Load the dataset data = pd.read_csv(file_path) # Step 2: Split the dataset into features (X) and target labels (y) X = data.drop(\'label\', axis=1) y = data[\'label\'] # Step 3: Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 4: Create a pipeline with StandardScaler and SVC pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'svc\', SVC()) ]) # Step 5: Define the parameter grid for grid search param_grid = { \'svc__C\': [0.1, 1, 10, 100], \'svc__gamma\': [1, 0.1, 0.01, 0.001] } # Step 6: Implement grid search with cross-validation grid_search = GridSearchCV(pipeline, param_grid, cv=5, verbose=1) grid_search.fit(X_train, y_train) # Step 7: Find the best hyperparameters and print them best_params = grid_search.best_params_ print(\\"Best Hyperparameters:\\", best_params) # Step 8: Evaluate the final model on the test set and print the accuracy y_pred = grid_search.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(\\"Test Set Accuracy:\\", accuracy) return best_params, accuracy"},{"question":"# Objective Create a summary visualization using Seaborn for the given Titanic dataset focusing on the survival rate by different classes and genders. # Problem Statement Using the seaborn package, load the Titanic dataset and create a visual summary that shows survival rates segmented by passenger class and gender. Customize the plot for readability and clarity. # Instructions 1. Load the Titanic dataset using the `sns.load_dataset` function. 2. Create a bar plot that displays survival rates (`survived` column) by passenger class (`class` column) and gender (`sex` column). Ensure that: - The plot has separate subplots for male and female passengers (use the `col` parameter). - Each subplot displays survival rates for different passenger classes (use the `x` parameter). 3. Adjust the following plot parameters: - Set the overall figure size using `height=4` and `aspect=0.6`. - Set y-axis label to \\"Survival Rate\\". - Change the x-axis tick labels to display “1st Class”, “2nd Class”, “3rd Class”. - Set the y-axis limit to (0, 1) for consistency in both subplots. - Remove the left spine of the plot for a cleaner look. - Add titles for each subplot indicating the gender. # Expected Input No explicit input from the user is required. Use the built-in seaborn dataset. # Expected Output The function should generate and display a bar plot with the above specifications. # Example Code Layout ```python import seaborn as sns def plot_titanic_survival(): # Load the Titanic dataset df = sns.load_dataset(\\"titanic\\") # Create the categorized plot g = sns.catplot( data=df, x=\\"class\\", y=\\"survived\\", col=\\"sex\\", kind=\\"bar\\", height=4, aspect=.6, ) # Customize the plot g.set_axis_labels(\\"\\", \\"Survival Rate\\") g.set_xticklabels([\\"1st Class\\", \\"2nd Class\\", \\"3rd Class\\"]) g.set_titles(\\"{col_name}\\") g.set(ylim=(0, 1)) g.despine(left=True) # Show the plot g.fig.suptitle(\'Titanic Survival Rate by Class and Gender\', y=1.02) plt.show() # Call the function to generate the plot plot_titanic_survival() ``` Ensure the function `plot_titanic_survival` is implemented to accurately generate the required plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_titanic_survival(): # Load the Titanic dataset df = sns.load_dataset(\\"titanic\\") # Create the categorized plot g = sns.catplot( data=df, x=\\"class\\", y=\\"survived\\", col=\\"sex\\", kind=\\"bar\\", height=4, aspect=.6, ) # Customize the plot g.set_axis_labels(\\"\\", \\"Survival Rate\\") g.set_xticklabels([\\"1st Class\\", \\"2nd Class\\", \\"3rd Class\\"]) g.set_titles(\\"{col_name}\\") g.set(ylim=(0, 1)) g.despine(left=True) # Show the plot g.fig.suptitle(\'Titanic Survival Rate by Class and Gender\', y=1.02) plt.show() # Call the function to generate the plot plot_titanic_survival()"},{"question":"<|Analysis Begin|> The \\"linecache\\" module in Python provides functionality for random access to any line of a text file while optimizing performance by caching lines read from a single file. This can be particularly useful in scenarios where multiple lines are accessed repeatedly, as it reduces the overhead of opening and reading the file each time. Key functions provided by the \\"linecache\\" module are: 1. `getline(filename, lineno, module_globals=None)`: Fetches a specific line (with given linenumber) from the specified file. 2. `clearcache()`: Clears the cache of previously read lines. 3. `checkcache(filename=None)`: Checks the validity of the cache and updates it if files have changed. 4. `lazycache(filename, module_globals)`: Prepares for later line retrieval without performing I/O immediately. Overall, these functions make the \\"linecache\\" module a useful tool for efficiently managing file reading operations where specific lines need to be accessed frequently and consistently. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Reading Specific Lines from Multiple Files using `linecache` As a data scientist, you often need to process large text files and frequently access specific lines for analysis or debugging. To optimize this process, you can use the \\"linecache\\" module for efficiently accessing these lines. Implement the following functions to demonstrate your understanding of the \\"linecache\\" module: 1. `read_and_cache_line(file_path: str, line_number: int) -> str`: - **Input**: - `file_path` (str): The path to the text file. - `line_number` (int): The specific line number to read from the file. - **Output**: - Returns the content of the specified line as a string. If the line or file does not exist, it should return an empty string `\\"\\"`. 2. `validate_cache(file_path: str = None) -> None`: - **Input**: - `file_path` (str, optional): The path to the file whose cache should be validated. If not provided, the entire cache should be validated. - **Output**: - None. This function should validate the cache for the specified file or the entire cache if no file is mentioned. 3. `clear_linecache() -> None`: - **Output**: - None. This function should clear the linecache, removing all cached lines to free up memory. Constraints - Assume the file paths provided are valid and the files exist unless specified otherwise. - You can use `linecache.getline()`, `linecache.checkcache()`, and `linecache.clearcache()` as described. Example Usage ```python # Example file content (example.txt): # Line 1: Hello, World! # Line 2: Welcome to the linecache example. # Line 3: This is the third line. print(read_and_cache_line(\\"example.txt\\", 1)) # Output: \\"Hello, World!n\\" print(read_and_cache_line(\\"example.txt\\", 4)) # Output: \\"\\" validate_cache(\\"example.txt\\") clear_linecache() ``` Use these instructions to create an efficient and optimized solution for reading specific lines from large text files using caches.","solution":"import linecache def read_and_cache_line(file_path: str, line_number: int) -> str: Reads a specific line from the file and uses caching for performance optimization. Args: - file_path (str): The path to the text file. - line_number (int): The specific line number to read. Returns: - str: The content of the specified line. If the line or file does not exist, it returns an empty string. line = linecache.getline(file_path, line_number) return line if line else \\"\\" def validate_cache(file_path: str = None) -> None: Validates the cache for the specified file or the entire linecache. Args: - file_path (str, optional): The path to the file whose cache should be validated. If not provided, the entire cache is validated. Returns: - None linecache.checkcache(file_path) def clear_linecache() -> None: Clears the linecache, removing all cached lines to free up memory. Args: - None Returns: - None linecache.clearcache()"},{"question":"**Objective**: This question is designed to test your understanding of the `bz2` module in Python. You will need to implement functionalities to handle compression and decompression of data, both from files and as streams. # Problem Statement You are tasked with implementing a set of functions to handle bzip2 compression and decompression for text data. Your implementations should handle both file-based operations and incremental data operations. Specifically, you need to implement the following functions: 1. **`compress_file(input_filename: str, output_filename: str, compresslevel: int = 9) -> None`**: - Compress the content of `input_filename` and write the compressed data to `output_filename`. - You should use the maximum compression level value (`9`) by default unless specified otherwise. 2. **`decompress_file(input_filename: str, output_filename: str) -> None`**: - Decompress the content of `input_filename` and write the decompressed data to `output_filename`. 3. **`compress_stream(data: bytes, compresslevel: int = 9) -> bytes`**: - Compress the given data incrementally and return the compressed byte stream. 4. **`decompress_stream(data: bytes) -> bytes`**: - Decompress the given compressed byte stream incrementally and return the decompressed data. - Ensure your implementation accounts for the possibility of multiple concatenated compressed streams. # Constraints - The input files for `compress_file` and `decompress_file` may be large. Ensure your implementation reads and writes data efficiently. - The functions `compress_stream` and `decompress_stream` should handle incremental (de)compression using `bz2.BZ2Compressor` and `bz2.BZ2Decompressor` classes. # Example Usage ```python # Example usage of file compression compress_file(\'input.txt\', \'output.bz2\') decompress_file(\'output.bz2\', \'decompressed.txt\') # Verify the decompressed content matches the original with open(\'input.txt\', \'rb\') as f: original_data = f.read() with open(\'decompressed.txt\', \'rb\') as f: decompressed_data = f.read() assert original_data == decompressed_data, \\"Decompressed data does not match the original data\\" # Example usage of stream compression data = b\\"Example data to be compressed incrementally.\\" compressed_data = compress_stream(data) decompressed_data = decompress_stream(compressed_data) assert data == decompressed_data, \\"Decompressed stream data does not match the original data\\" ``` # Evaluation Criteria - **Correctness**: Ensure the output is correct and matches expected values. - **Efficiency**: Your implementation should handle large files efficiently without excessive memory usage. - **Code Quality**: Your code should be well-organized, readable, and follow best practices for Python programming.","solution":"import bz2 def compress_file(input_filename: str, output_filename: str, compresslevel: int = 9) -> None: with open(input_filename, \'rb\') as input_file: input_data = input_file.read() compressed_data = bz2.compress(input_data, compresslevel) with open(output_filename, \'wb\') as output_file: output_file.write(compressed_data) def decompress_file(input_filename: str, output_filename: str) -> None: with open(input_filename, \'rb\') as input_file: compressed_data = input_file.read() decompressed_data = bz2.decompress(compressed_data) with open(output_filename, \'wb\') as output_file: output_file.write(decompressed_data) def compress_stream(data: bytes, compresslevel: int = 9) -> bytes: compressor = bz2.BZ2Compressor(compresslevel) compressed_data = compressor.compress(data) compressed_data += compressor.flush() return compressed_data def decompress_stream(data: bytes) -> bytes: decompressor = bz2.BZ2Decompressor() decompressed_data = decompressor.decompress(data) return decompressed_data"},{"question":"# Advanced I/O Multiplexing with Python\'s `select` Module Objective Implement a function called `monitor_file_descriptors` that uses the `select` module to monitor multiple file descriptors (FDs) and handle different types of I/O events that occur on them. Requirements 1. The function should accept the following parameters: - `rlist`: A list of file descriptors to monitor for reading. - `wlist`: A list of file descriptors to monitor for writing. - `xlist`: A list of file descriptors to monitor for exceptional conditions. - `timeout`: An optional float specifying the timeout in seconds. If omitted, use a default timeout of 5 seconds. 2. The function should: - Use the `select.select` function to monitor the provided file descriptors. - For each file descriptor that is ready for reading, writing, or has an exceptional condition, call an appropriate handling function (`handle_read`, `handle_write`, or `handle_exception`) with the file descriptor as the argument. - Return a dictionary summarizing the events encountered. The dictionary should have keys `\'read\'`, `\'write\'`, and `\'exception\'` with respective lists of file descriptors that were ready. 3. Assume the following helper functions are provided: ```python def handle_read(fd): print(f\\"Handling read for FD {fd}\\") def handle_write(fd): print(f\\"Handling write for FD {fd}\\") def handle_exception(fd): print(f\\"Handling exception for FD {fd}\\") ``` Constraints - The `rlist`, `wlist`, and `xlist` contain only valid file descriptors. - The `timeout` is a non-negative float or `None`. Function Signature ```python def monitor_file_descriptors(rlist, wlist, xlist, timeout=5.0) -> dict: pass ``` Example Usage: ```python r_fds = [fd1, fd2] # Replace with actual file descriptors w_fds = [fd3] x_fds = [fd4, fd5] timeout = 10.0 result = monitor_file_descriptors(r_fds, w_fds, x_fds, timeout) print(result) # Example Output: {\'read\': [fd1], \'write\': [fd3], \'exception\': []} ``` Implement the `monitor_file_descriptors` function and ensure it properly handles the different I/O events and returns the summary dictionary as described.","solution":"import select def handle_read(fd): print(f\\"Handling read for FD {fd}\\") def handle_write(fd): print(f\\"Handling write for FD {fd}\\") def handle_exception(fd): print(f\\"Handling exception for FD {fd}\\") def monitor_file_descriptors(rlist, wlist, xlist, timeout=5.0) -> dict: Monitors multiple file descriptors for various I/O events using the select module. Parameters: rlist (list): A list of file descriptors to monitor for reading. wlist (list): A list of file descriptors to monitor for writing. xlist (list): A list of file descriptors to monitor for exceptional conditions. timeout (float): Timeout in seconds. Returns: dict: A summary dictionary with keys \'read\', \'write\', and \'exception\'. ready_to_read, ready_to_write, in_error = select.select(rlist, wlist, xlist, timeout) result = { \'read\': [], \'write\': [], \'exception\': [] } for fd in ready_to_read: handle_read(fd) result[\'read\'].append(fd) for fd in ready_to_write: handle_write(fd) result[\'write\'].append(fd) for fd in in_error: handle_exception(fd) result[\'exception\'].append(fd) return result"},{"question":"# Seaborn Customization and Plotting Assessment In this assessment, you are expected to demonstrate your understanding of seaborn\'s capabilities to create and customize plots. The goal is to produce a well-detailed and informative visualization of a dataset using seaborn. # Task Write a function `create_custom_plot(data, x_col, y_col, style, palette, custom_rc)` that takes the following inputs: - `data`: A pandas DataFrame with at least two columns, used for plotting. - `x_col`: The name of the column to be used for the x-axis. - `y_col`: The name of the column to be used for the y-axis. - `style`: The seaborn style to apply to the plot (e.g., \\"whitegrid\\", \\"dark\\", \\"ticks\\"). - `palette`: The color palette to use for the plot (e.g., \\"pastel\\", \\"dark\\", \\"colorblind\\"). - `custom_rc`: A dictionary containing custom rc parameters for further customization. The function should: 1. Set the seaborn theme using the provided `style` and `palette`. If `palette` is `None`, preserve the current palette. 2. Apply any custom rc parameters from the `custom_rc` dictionary. 3. Generate a bar plot using the specified `x_col` and `y_col` from the `data`. 4. Save the plot as an image file named \\"custom_plot.png\\". # Constraints - Ensure that the resulting plot is clear and properly labeled. - The DataFrame `data` will contain at least two columns and at least three rows. # Example ```python import pandas as pd # Example DataFrame data = pd.DataFrame({ \'Category\': [\'A\', \'B\', \'C\'], \'Values\': [1, 3, 2] }) style = \\"whitegrid\\" palette = \\"pastel\\" custom_rc = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} create_custom_plot(data, \'Category\', \'Values\', style, palette, custom_rc) ``` This should generate and save a plot with the specified customizations. # Function Signature ```python def create_custom_plot(data, x_col, y_col, style, palette, custom_rc): pass ``` # Evaluation Criteria - Correct implementation of the function that sets the theme and applies customizations. - Accurate generation of the bar plot using seaborn. - Proper saving of the plot as \\"custom_plot.png\\". - Code clarity, proper documentation, and adherence to best practices.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plot(data, x_col, y_col, style, palette, custom_rc): Creates a customized bar plot using seaborn and saves it as an image file. Parameters: data (pd.DataFrame): The data to plot, containing at least two columns. x_col (str): The name of the column to be used for the x-axis. y_col (str): The name of the column to be used for the y-axis. style (str): The seaborn style to apply to the plot. palette (str): The color palette to use for the plot. custom_rc (dict): A dictionary containing custom rc parameters for further customization. # Set the seaborn style and palette sns.set(style=style) if palette is not None: sns.set_palette(palette) # Apply custom rc parameters if custom_rc: sns.set_context(rc=custom_rc) # Create the bar plot plt.figure(figsize=(10, 6)) # Optional: you can adjust the figure size barplot = sns.barplot(x=x_col, y=y_col, data=data) # Set labels and title barplot.set_xlabel(x_col) barplot.set_ylabel(y_col) barplot.set_title(f\'{y_col} by {x_col}\') # Save the plot plt.savefig(\\"custom_plot.png\\") plt.close()"},{"question":"# **Challenging Coding Assessment Question** Numeric Operations Wrapper Class You need to create a class `NumericOperations` in Python that wraps several numeric operations defined in the provided documentation. Each method should call the equivalent Python operation but mimics the behavior and naming conventions as shown in the documentation. # **Class Requirements** Implement a class `NumericOperations` with the following methods: 1. **`add(o1, o2)`** - Returns the result of `o1 + o2`. 2. **`subtract(o1, o2)`** - Returns the result of `o1 - o2`. 3. **`multiply(o1, o2)`** - Returns the result of `o1 * o2`. 4. **`true_divide(o1, o2)`** - Returns the result of `o1 / o2`. 5. **`floor_divide(o1, o2)`** - Returns the result of `o1 // o2`. 6. **`remainder(o1, o2)`** - Returns the remainder of `o1 % o2`. 7. **`pow(o1, o2, o3=None)`** - Returns the result of `pow(o1, o2, o3)`. If `o3` is not provided, default it to `None`. 8. **`negate(o)`** - Returns the result of `-o`. 9. **`absolute(o)`** - Returns the absolute value `abs(o)`. 10. **`invert(o)`** - Returns the bitwise negation `~o`. 11. **`to_base(n, base)`** - Returns the integer `n` converted to the string representation in the given base (2, 8, 10, or 16). # **Input and Output Formats** - The inputs to the methods can be any Python objects that support the respective operations (i.e., integers, floats, and so on). - The outputs are expected to follow the respective Python operation results. # **Constraints** - Your implementations should handle any type of input that supports the operations defined. - Handle exceptions gracefully. If an operation fails, return `None`. # **Example Usage** ```python num_ops = NumericOperations() # Examples assert num_ops.add(5, 3) == 8 assert num_ops.subtract(5, 3) == 2 assert num_ops.multiply(5, 3) == 15 assert num_ops.true_divide(5, 2) == 2.5 assert num_ops.floor_divide(5, 2) == 2 assert num_ops.remainder(5, 3) == 2 assert num_ops.pow(2, 3) == 8 assert num_ops.pow(2, 3, 3) == 2 assert num_ops.negate(5) == -5 assert num_ops.absolute(-5) == 5 assert num_ops.invert(5) == -6 assert num_ops.to_base(10, 2) == \'0b1010\' assert num_ops.to_base(10, 16) == \'0xa\' ``` # **Additional Performance**: - Ensure the methods perform efficiently, even with large integer operations. **Note**: This question assesses the students\' understanding of basic arithmetic operations, exception handling, and class design. They need to translate high-level documentation into practical, usable code.","solution":"class NumericOperations: def add(self, o1, o2): try: return o1 + o2 except Exception: return None def subtract(self, o1, o2): try: return o1 - o2 except Exception: return None def multiply(self, o1, o2): try: return o1 * o2 except Exception: return None def true_divide(self, o1, o2): try: return o1 / o2 except Exception: return None def floor_divide(self, o1, o2): try: return o1 // o2 except Exception: return None def remainder(self, o1, o2): try: return o1 % o2 except Exception: return None def pow(self, o1, o2, o3=None): try: return pow(o1, o2, o3) except Exception: return None def negate(self, o): try: return -o except Exception: return None def absolute(self, o): try: return abs(o) except Exception: return None def invert(self, o): try: return ~o except Exception: return None def to_base(self, n, base): try: if base == 2: return bin(n) elif base == 8: return oct(n) elif base == 10: return str(n) elif base == 16: return hex(n) else: return None except Exception: return None"},{"question":"<|Analysis Begin|> The provided documentation outlines the `hashlib` library in Python 3.10, detailing various secure hash and message digest algorithms such as SHA-1, SHA-256, BLAKE2, and more. The library offers a unified interface for these algorithms, allowing users to create hash objects, update them with bytes-like objects, and retrieve the digest in multiple formats. The documentation includes constructors, methods, constants, and usage examples for various hashing algorithms. Key points from the documentation: 1. Hash objects are created using constructors like `sha256()`, `blake2b()`, etc. 2. Methods such as `update()`, `digest()`, and `hexdigest()` are used to feed data and retrieve the hash. 3. Advanced features include keyed hashing, personalization, salted hashing, and tree mode hashing. 4. Hashing algorithms can be selected using `hashlib.new()` with the algorithm name as a string. 5. Constants like `hashlib.algorithms_guaranteed` and `hashlib.algorithms_available` provide information about the available hashing algorithms. The question will focus on utilizing these hashing capabilities to solve a problem that involves multiple aspects covered in the documentation, ensuring a comprehensive assessment of the student\'s understanding of `hashlib`. <|Analysis End|> <|Question Begin|> # Problem Statement You are working on an application that needs to ensure the integrity of messages sent between clients and a server. To achieve this, you will implement a message signing and verification system using the `hashlib` library in Python. Task 1. **Message Signing**: Implement a function `sign_message(message: bytes, secret_key: bytes) -> str` that takes a message and a secret key to produce a hexadecimal HMAC (Hash-based Message Authentication Code) using the `blake2b` algorithm. The digest size should be 16 bytes. 2. **Message Verification**: Implement a function `verify_message(message: bytes, received_hmac: str, secret_key: bytes) -> bool` that takes a message, the HMAC received, and the secret key. The function should verify if the received HMAC is valid for the given message and secret key. 3. **Personalized Hashing**: Implement a function `generate_personalized_hash(message: bytes, person: bytes, digest_size: int) -> str` that takes a message, a personalization string, and a desired digest size. It should return the hexadecimal hash of the message using the `blake2b` algorithm personalized with the given string and producing the specified digest size. Input - `message`: A bytes object representing the message to be signed or hashed. - `secret_key`: A bytes object representing the secret key used for signing the message. - `received_hmac`: A string representing the received HMAC to be verified. - `person`: A bytes object representing the personalization string. - `digest_size`: An integer representing the size of the output digest. Output - `sign_message`: A string representing the hexadecimal HMAC of the message. - `verify_message`: A boolean indicating whether the received HMAC is valid. - `generate_personalized_hash`: A string representing the hexadecimal hash of the personalized message. Constraints - The `message` and `secret_key` should not exceed 1024 bytes. - The desired `digest_size` should be between 1 and 64 bytes for `blake2b`. - You should use the functions and methods provided by the `hashlib` library. Example ```python # Example usage: message = b\'Hello, world!\' secret_key = b\'secret\' person = b\'personalization\' # Message Signing hmac = sign_message(message, secret_key) print(hmac) # Expected Output: Hexadecimal HMAC # Message Verification is_valid = verify_message(message, hmac, secret_key) print(is_valid) # Expected Output: True # Personalized Hashing personalized_hash = generate_personalized_hash(message, person, 32) print(personalized_hash) # Expected Output: Hexadecimal personalized hash ``` **Note**: Ensure your implementation handles the encoding and digest production correctly using the `blake2b` algorithm from the `hashlib` library.","solution":"import hashlib def sign_message(message: bytes, secret_key: bytes) -> str: Produces a hexadecimal HMAC using the blake2b algorithm with a digest size of 16 bytes. h = hashlib.blake2b(key=secret_key, digest_size=16) h.update(message) return h.hexdigest() def verify_message(message: bytes, received_hmac: str, secret_key: bytes) -> bool: Verifies if the received HMAC is valid for the given message and secret key using the blake2b algorithm. expected_hmac = sign_message(message, secret_key) return expected_hmac == received_hmac def generate_personalized_hash(message: bytes, person: bytes, digest_size: int) -> str: Returns the hexadecimal hash of the message using the blake2b algorithm personalized with the given string and producing the specified digest size. h = hashlib.blake2b(digest_size=digest_size, person=person) h.update(message) return h.hexdigest()"},{"question":"**Task: Audio File Manipulation using the wave Module** **Objective:** Write a Python function that reads a WAV file, processes its audio data by doubling its amplitude, and writes the modified audio data to a new WAV file while preserving the original file\'s parameters (such as the number of channels, sample width, frame rate). **Function Signature:** ```python def process_wav_file(input_wav: str, output_wav: str) -> None: pass ``` **Input:** - `input_wav` (str): The path to the input WAV file to be read. - `output_wav` (str): The path where the modified WAV file should be written. **Output:** - None **Constraints:** - The original WAV file will always be in \'WAVE_FORMAT_PCM\' format. - You need to handle both mono and stereo audio files. - Ensure that the amplitude scaling does not introduce clipping (i.e., values should not exceed the maximum representable value for the given sample width). **Detailed Requirements:** 1. **Reading the File:** - Open the input WAV file using the `wave` module in read mode. - Retrieve the file parameters (number of channels, sample width, frame rate, and number of frames). 2. **Processing the Audio Data:** - Read the audio frames from the input WAV file. - Convert the byte data to an appropriate numeric format (e.g., using `numpy` or manual conversion based on sample width). - Double the amplitude of the audio samples ensuring that the resulting values do not exceed the allowable range for the given sample width. 3. **Writing the Modified Data:** - Open the output WAV file using the `wave` module in write mode. - Set the same parameters (number of channels, sample width, frame rate) as the input file. - Write the processed audio frames to the output file. **Example Usage:** ```python process_wav_file(\'input.wav\', \'output.wav\') ``` In this example, the function reads the audio data from \'input.wav\', processes it by doubling its amplitude, and writes the modified audio to \'output.wav\'. **Notes:** - You can use external libraries like `numpy` for numerical operations if needed. - Ensure proper handling of file operations and exceptions.","solution":"import wave import numpy as np def process_wav_file(input_wav: str, output_wav: str) -> None: with wave.open(input_wav, \'rb\') as infile: params = infile.getparams() n_channels, sampwidth, framerate, n_frames = params[:4] audio_frames = infile.readframes(n_frames) audio_data = np.frombuffer(audio_frames, dtype=np.int16) # Double the amplitude max_val = 2 ** (8 * sampwidth - 1) - 1 min_val = - (2 ** (8 * sampwidth - 1)) doubled_amplitude_audio = np.clip(audio_data * 2, min_val, max_val).astype(np.int16) with wave.open(output_wav, \'wb\') as outfile: outfile.setparams(params) outfile.writeframes(doubled_amplitude_audio.tobytes())"},{"question":"`site` Module Configuration and Customization **Objective**: Demonstrate understanding and practical usage of the `site` module in Python for site-specific configuration and path manipulations. Problem Statement: You are tasked with creating a Python script that customizes the Python environment by adding a specific directory to the `sys.path` and ensuring that a custom module from this directory can be imported without errors. Implement the following functionalities: 1. **Add a Custom Directory to `sys.path`**: - Create a function `add_custom_directory(path: str) -> None` that: - Adds the given `path` to `sys.path` if it exists and isn\'t already in `sys.path`. - If the directory does not exist, the function should raise a `FileNotFoundError`. 2. **Detect and Validate `*.pth` Files**: - Create a function `process_pth_files(dir_path: str) -> None` that: - Searches for files with the `.pth` extension in the given directory. - Reads the contents of these `.pth` files and adds each valid path to `sys.path`. - Ensures each path exists before adding and ignores duplicates. 3. **Read Custom Configuration**: - Create a function `read_custom_configuration(config_file: str) -> Dict[str, Any]` that: - Reads the specified configuration file (in JSON format) that contains an array of directories to be added to `sys.path`. - Validates the format to ensure it contains a key \\"directories\\" with a list of directory paths. - Returns a dictionary with the validated configuration. 4. **Initialize Custom Environment**: - Create a function `initialize_custom_environment(config_file: str) -> None` that: - Utilizes the above functions to: - Read and validate the configuration file. - Add each directory from the configuration to `sys.path`. - Process `.pth` files in each of these directories. **Constraints**: - The script should be executed in an environment where the custom directories and configuration file paths are provided. - Assume valid JSON format for configuration. - Ensure robust error handling for file operations and invalid directories. **Example Configuration File** (`custom_config.json`): ```json { \\"directories\\": [ \\"/path/to/custom/directory1\\", \\"/path/to/custom/directory2\\" ] } ``` Example Usage: ```python # Example custom_config.json: # { # \\"directories\\": [\\"/path/to/custom/directory1\\", \\"/path/to/custom/directory2\\"] # } from pathlib import Path def add_custom_directory(path: str) -> None: import sys import os if not os.path.isdir(path): raise FileNotFoundError(f\\"The directory {path} does not exist.\\") if path not in sys.path: sys.path.append(path) def process_pth_files(dir_path: str) -> None: import sys import os for pth_file in Path(dir_path).glob(\\"*.pth\\"): with open(pth_file, \'r\') as file: for line in file: line = line.strip() if line and not line.startswith(\'#\') and not line in sys.path: full_path = os.path.join(dir_path, line) if os.path.exists(full_path): sys.path.append(full_path) def read_custom_configuration(config_file: str) -> Dict[str, Any]: import json with open(config_file, \'r\') as file: config = json.load(file) if \\"directories\\" not in config or not isinstance(config[\\"directories\\"], list): raise ValueError(\\"The configuration file is invalid.\\") return config def initialize_custom_environment(config_file: str) -> None: config = read_custom_configuration(config_file) for directory in config[\\"directories\\"]: add_custom_directory(directory) process_pth_files(directory) if __name__ == \\"__main__\\": import sys if len(sys.argv) != 2: print(\\"Usage: python script.py <config_file>\\") sys.exit(1) config_file = sys.argv[1] initialize_custom_environment(config_file) print(f\\"Custom environment initialized with directories from {config_file}\\") ``` Notes: - The script should be run with the correct path to the configuration file. - Ensure your functions have appropriate docstrings and comments to explain their functionality.","solution":"import sys import os from pathlib import Path from typing import Dict, Any import json def add_custom_directory(path: str) -> None: Adds the given directory to sys.path if it exists and isn\'t already in sys.path. :param path: The directory to add to sys.path. :raises FileNotFoundError: If the directory does not exist. if not os.path.isdir(path): raise FileNotFoundError(f\\"The directory {path} does not exist.\\") if path not in sys.path: sys.path.append(path) def process_pth_files(dir_path: str) -> None: Processes *.pth files in the given directory and adds each valid path to sys.path. :param dir_path: The directory containing .pth files to process. for pth_file in Path(dir_path).glob(\\"*.pth\\"): with open(pth_file, \'r\') as file: for line in file: line = line.strip() if line and not line.startswith(\'#\') and line not in sys.path: full_path = os.path.join(dir_path, line) if os.path.exists(full_path): sys.path.append(full_path) def read_custom_configuration(config_file: str) -> Dict[str, Any]: Reads and validates the configuration file. :param config_file: The path to the configuration file. :return: A dictionary with the validated configuration. :raises ValueError: If the configuration file format is invalid. with open(config_file, \'r\') as file: config = json.load(file) if \\"directories\\" not in config or not isinstance(config[\\"directories\\"], list): raise ValueError(\\"The configuration file is invalid.\\") return config def initialize_custom_environment(config_file: str) -> None: Initializes the custom environment by reading the configuration file, adding directories to sys.path, and processing .pth files in these directories. :param config_file: The path to the configuration file. config = read_custom_configuration(config_file) for directory in config[\\"directories\\"]: add_custom_directory(directory) process_pth_files(directory)"},{"question":"# Python Sequence Manipulation Given the following Python sequence protocol functions documentation, your task is to implement a Python class `SequenceManipulator` that provides a simplified interface for working with sequences. The class should include the following methods: 1. **is_sequence(obj)**: Checks if the given object `obj` provides the sequence protocol. 2. **sequence_size(seq)**: Returns the size of the sequence `seq`. 3. **concatenate(seq1, seq2)**: Concatenates two sequences `seq1` and `seq2`. 4. **repeat(seq, count)**: Repeats the sequence `seq` `count` times. 5. **get_item(seq, index)**: Gets the item at the specified `index` in the sequence `seq`. 6. **set_item(seq, index, value)**: Sets the item at the specified `index` in the sequence `seq` to `value`. 7. **delete_item(seq, index)**: Deletes the item at the specified `index` in the sequence `seq`. 8. **count_value(seq, value)**: Counts the occurrences of `value` in the sequence `seq`. 9. **contains_value(seq, value)**: Checks if the sequence `seq` contains the specified `value`. 10. **index_of_value(seq, value)**: Returns the index of the first occurrence of `value` in the sequence `seq`. # Constraints - The class should only support list and tuple types for sequence operations. - You may assume the inputs to be valid and do not need to handle exceptions beyond the specified behaviors. - Implement Python expressions directly without using the private functions from the Python C API. # Your Task Implement the `SequenceManipulator` class in Python. The class should meet the following interface: ```python class SequenceManipulator: @staticmethod def is_sequence(obj): Check if the object provides the sequence protocol. :param obj: Any Python object :return: bool pass @staticmethod def sequence_size(seq): Returns the size of a sequence. :param seq: A list or a tuple :return: int pass @staticmethod def concatenate(seq1, seq2): Concatenate two sequences. :param seq1: A list or a tuple :param seq2: A list or a tuple :return: A new list or tuple containing elements from both sequences pass @staticmethod def repeat(seq, count): Repeat a sequence a specified number of times. :param seq: A list or a tuple :param count: An integer specifying the repetition count :return: A new list or tuple pass @staticmethod def get_item(seq, index): Get the item at the specified index in the sequence. :param seq: A list or a tuple :param index: An integer index :return: The element at the specified index pass @staticmethod def set_item(seq, index, value): Set the item at the specified index in the sequence. :param seq: A mutable sequence (list) :param index: An integer index :param value: The value to set :return: None pass @staticmethod def delete_item(seq, index): Delete the item at the specified index in the sequence. :param seq: A mutable sequence (list) :param index: An integer index :return: None pass @staticmethod def count_value(seq, value): Count the occurrences of value in the sequence. :param seq: A list or a tuple :param value: The value to count :return: An integer count pass @staticmethod def contains_value(seq, value): Check if the sequence contains the specified value. :param seq: A list or a tuple :param value: The value to check :return: bool pass @staticmethod def index_of_value(seq, value): Return the index of the first occurrence of value in the sequence. :param seq: A list or a tuple :param value: The value to find :return: An integer index pass ``` Make sure to adhere strictly to the method signatures and their descriptions. # Example Usage ```python sm = SequenceManipulator() # Check if an object is a sequence print(sm.is_sequence([1, 2, 3])) # True print(sm.is_sequence(123)) # False # Get the size of a sequence print(sm.sequence_size([1, 2, 3])) # 3 # Concatenate two sequences print(sm.concatenate([1, 2, 3], [4, 5])) # [1, 2, 3, 4, 5] # Repeat a sequence print(sm.repeat([1, 2], 3)) # [1, 2, 1, 2, 1, 2] # Get the item at the specified index print(sm.get_item([1, 2, 3], 1)) # 2 # Set the item at the specified index lst = [1, 2, 3] sm.set_item(lst, 1, 99) print(lst) # [1, 99, 3] # Delete the item at the specified index lst = [1, 2, 3] sm.delete_item(lst, 1) print(lst) # [1, 3] # Count the occurrences of a value print(sm.count_value([1, 2, 2, 3], 2)) # 2 # Check if the sequence contains a value print(sm.contains_value([1, 2, 3], 2)) # True # Return the index of the first occurrence of a value print(sm.index_of_value([1, 2, 3, 2], 2)) # 1 ```","solution":"class SequenceManipulator: @staticmethod def is_sequence(obj): Check if the object provides the sequence protocol. :param obj: Any Python object :return: bool return isinstance(obj, (list, tuple)) @staticmethod def sequence_size(seq): Returns the size of a sequence. :param seq: A list or a tuple :return: int return len(seq) @staticmethod def concatenate(seq1, seq2): Concatenate two sequences. :param seq1: A list or a tuple :param seq2: A list or a tuple :return: A new list or tuple containing elements from both sequences return seq1 + seq2 @staticmethod def repeat(seq, count): Repeat a sequence a specified number of times. :param seq: A list or a tuple :param count: An integer specifying the repetition count :return: A new list or tuple return seq * count @staticmethod def get_item(seq, index): Get the item at the specified index in the sequence. :param seq: A list or a tuple :param index: An integer index :return: The element at the specified index return seq[index] @staticmethod def set_item(seq, index, value): Set the item at the specified index in the sequence. :param seq: A mutable sequence (list) :param index: An integer index :param value: The value to set :return: None seq[index] = value @staticmethod def delete_item(seq, index): Delete the item at the specified index in the sequence. :param seq: A mutable sequence (list) :param index: An integer index :return: None del seq[index] @staticmethod def count_value(seq, value): Count the occurrences of value in the sequence. :param seq: A list or a tuple :param value: The value to count :return: An integer count return seq.count(value) @staticmethod def contains_value(seq, value): Check if the sequence contains the specified value. :param seq: A list or a tuple :param value: The value to check :return: bool return value in seq @staticmethod def index_of_value(seq, value): Return the index of the first occurrence of value in the sequence. :param seq: A list or a tuple :param value: The value to find :return: An integer index return seq.index(value)"},{"question":"**Question:** You are designing a mini web scraper using Python\'s `urllib.request` module. Your goal is to: 1. Fetch the HTML content of a given URL. 2. Extract all the URLs from the fetched HTML content. 3. Follow one level of links (i.e., fetch the HTML content of the URLs found on the initial page) and list the URLs found on these pages as well. **Task:** 1. Implement a function `fetch_html(url)` that: - Takes a URL as input. - Returns the HTML content of the URL as a string. - Raises an exception if the URL cannot be opened. 2. Implement a function `extract_urls(html)` that: - Takes an HTML content as input. - Extracts and returns a list of URLs found in the HTML content. - Uses regular expressions to find the URLs (look for `href` attributes in anchor `<a>` tags). 3. Implement the function `scrape_urls(initial_url)` that combines the above functions to: - Take an initial URL as input. - Fetch the HTML content of the initial URL using `fetch_html`. - Extract URLs from this content using `extract_urls`. - For each extracted URL, fetch its HTML content and extract URLs from it. - Return a dictionary where the keys are the URLs found on the initial page, and the values are lists of URLs found on those pages. **Constraints:** - Use the `urllib.request` module for network operations. - Handle any network-related exceptions gracefully. - Ensure proper HTML decoding (assume utf-8 encoding). - Avoid following URLs more than one level deep in the hierarchy. - Only consider absolute URLs starting with `http://` or `https://`. ```python import urllib.request import re from urllib.error import URLError, HTTPError def fetch_html(url: str) -> str: Takes a URL as input and returns the HTML content of the URL as a string. Raises an exception if the URL cannot be opened. # Implement here def extract_urls(html: str) -> list: Takes HTML content as input and extracts and returns a list of URLs found in the HTML content. # Implement here def scrape_urls(initial_url: str) -> dict: Takes an initial URL as input, scrapes it for URLs, follows one level of links, and returns a dictionary where the keys are the URLs found on the initial page and the values are lists of URLs found on those pages. # Implement here # Example usage: # result = scrape_urls(\'http://example.com\') # print(result) ``` **Expected Output:** If the initial URL `http://example.com` contains links to `http://example1.com` and `http://example2.com`, and those pages, in turn, contain links to `http://example1.com/sub1` and `http://example2.com/sub2`: ``` { \'http://example1.com\': [\'http://example1.com/sub1\'], \'http://example2.com\': [\'http://example2.com/sub2\'] } ``` Good luck!","solution":"import urllib.request import re from urllib.error import URLError, HTTPError def fetch_html(url: str) -> str: Takes a URL as input and returns the HTML content of the URL as a string. Raises an exception if the URL cannot be opened. try: response = urllib.request.urlopen(url) html = response.read().decode(\'utf-8\') return html except (URLError, HTTPError) as e: raise Exception(f\\"Error fetching URL {url}: {e}\\") def extract_urls(html: str) -> list: Takes HTML content as input and extracts and returns a list of URLs found in the HTML content. urls = re.findall(r\'href=[\\"\'](http[s]?://[^\\"\']+)[\\"\']\', html) return urls def scrape_urls(initial_url: str) -> dict: Takes an initial URL as input, scrapes it for URLs, follows one level of links, and returns a dictionary where the keys are the URLs found on the initial page and the values are lists of URLs found on those pages. result = {} try: initial_html = fetch_html(initial_url) first_level_urls = extract_urls(initial_html) for url in first_level_urls: try: second_level_html = fetch_html(url) second_level_urls = extract_urls(second_level_html) result[url] = second_level_urls except Exception as e: result[url] = [f\\"Error fetching or extracting: {e}\\"] except Exception as e: result = {initial_url: [f\\"Error fetching or extracting: {e}\\"]} return result"},{"question":"**Coding Assessment Question: Seaborn Color Palettes** Using the seaborn library, you are required to implement a function that generates, displays, and compares different types of color palettes. The function should create a matplotlib figure that contains subplots demonstrating the following: 1. A seaborn light palette created using a specified color name. 2. A seaborn light palette created using a hexadecimal color code. 3. A seaborn light palette using a HUSL color specification. 4. A seaborn light palette with an increased number of discrete colors. 5. A continuous colormap generated from a given color. **Function Signature:** ```python def display_seaborn_color_palettes(): pass ``` **Detailed Requirements:** 1. The function should create a seaborn light palette using the color name \\"seagreen\\" and display it in a horizontal bar plot. 2. The function should create a seaborn light palette using the hex code \\"#79C\\" and display it in a horizontal bar plot. 3. The function should create a seaborn light palette using the HUSL specification `(20, 60, 50)` and display it in a horizontal bar plot. 4. The function should create a seaborn light palette using the color name \\"xkcd:copper\\" and increase the number of colors to 8, then display it in a horizontal bar plot. 5. The function should create and display a continuous colormap from the hex code \\"#a275ac\\" in a gradient format. **Additional Constraints and Information:** - Ensure you import seaborn and matplotlib for plotting purposes. - Each subplot should be clearly labeled with the type of palette and the color specification used. - Maintain a consistent and visually appealing layout for the subplots. **Example Output:** The function should create a figure similar to this: - Subplot 1: Light palette with \\"seagreen\\". - Subplot 2: Light palette with hex code \\"#79C\\". - Subplot 3: Light palette with HUSL `(20, 60, 50)`. - Subplot 4: Light palette with \\"xkcd:copper\\" and 8 colors. - Subplot 5: Continuous colormap with hex code \\"#a275ac\\". The visual representation will help in comparing various color palette options provided by seaborn. **Note:** You are expected to generate the plots and not just create the palettes.","solution":"import seaborn as sns import matplotlib.pyplot as plt def display_seaborn_color_palettes(): # Create a figure with 5 subplots (1 row, 5 columns) fig, axes = plt.subplots(1, 5, figsize=(20, 4)) # 1. Light palette using the color name \\"seagreen\\" palette1 = sns.light_palette(\\"seagreen\\", as_cmap=False) sns.barplot(x=list(range(len(palette1))), y=[1]*len(palette1), palette=palette1, ax=axes[0]) axes[0].set_title(\\"Light Palette: \'seagreen\'\\") axes[0].axes.get_yaxis().set_visible(False) # 2. Light palette using the hex code \\"#79C\\" palette2 = sns.light_palette(\\"#79C\\", as_cmap=False) sns.barplot(x=list(range(len(palette2))), y=[1]*len(palette2), palette=palette2, ax=axes[1]) axes[1].set_title(\\"Light Palette: \'#79C\'\\") axes[1].axes.get_yaxis().set_visible(False) # 3. Light palette using the HUSL (20, 60, 50) palette3 = sns.light_palette((20, 60, 50), input=\\"husl\\", as_cmap=False) sns.barplot(x=list(range(len(palette3))), y=[1]*len(palette3), palette=palette3, ax=axes[2]) axes[2].set_title(\\"Light Palette: HUSL (20, 60, 50)\\") axes[2].axes.get_yaxis().set_visible(False) # 4. Light palette with increased number of colors (8 colors) using \\"xkcd:copper\\" palette4 = sns.light_palette(\\"xkcd:copper\\", n_colors=8, as_cmap=False) sns.barplot(x=list(range(len(palette4))), y=[1]*len(palette4), palette=palette4, ax=axes[3]) axes[3].set_title(\\"Light Palette: \'xkcd:copper\' (8 colors)\\") axes[3].axes.get_yaxis().set_visible(False) # 5. Continuous colormap using hex code \\"#a275ac\\" cmap = sns.light_palette(\\"#a275ac\\", as_cmap=True) gradient = [[i] for i in range(256)] axes[4].imshow(gradient, aspect=\'auto\', cmap=cmap) axes[4].set_title(\\"Continuous Colormap: \'#a275ac\'\\") axes[4].axes.get_yaxis().set_visible(False) axes[4].axes.get_xaxis().set_visible(False) plt.tight_layout() plt.show()"},{"question":"**Topic**: Parallel Task Execution with `concurrent.futures` **Objective**: To evaluate students\' understanding of parallel task execution using the `concurrent.futures` module, including creating, managing, and handling results from concurrent tasks. Problem Statement You are required to process a list of URLs by downloading the HTML content of each URL. This task can be time-consuming, so you need to perform the downloads concurrently to speed up the process. The URLs should be processed using a combination of both threads and processes to best utilize system resources. Implement a function `fetch_urls_concurrently(urls: List[str]) -> List[str]` that: 1. Accepts a list of URLs and returns a list of their HTML content. 2. Uses both `concurrent.futures.ThreadPoolExecutor` and `concurrent.futures.ProcessPoolExecutor`. 3. Ensures that no more than 10 threads and 4 processes are used at any time. 4. Handles any exceptions that occur during the fetching of the HTML content so that a failure in one URL does not stop the entire process. 5. Collects and returns the HTML content of the URLs in the same order as the input list. **Input**: - `urls`: A list of strings, where each string is a URL. **Output**: - A list of strings, where each string is the HTML content of the corresponding URL from the input list. Constraints: - The number of URLs will not exceed 1000. - Each URL is guaranteed to be a valid URL string. - Performance is critical, so your solution should be efficient in terms of both time and resource usage. Example: ```python urls = [ \\"https://example.com\\", \\"https://example.org\\", \\"https://example.net\\" ] result = fetch_urls_concurrently(urls) # \'result\' should contain the HTML content of the URLs in the same order. ``` Notes: - You may use the `requests` library to fetch the HTML content of the URLs. - Make sure to handle exceptions such as network timeouts, invalid URLs, etc., and log (or print) meaningful error messages. - Ensure proper cleanup of resources, especially when using ProcessPoolExecutor. Happy Coding!","solution":"import concurrent.futures import requests from typing import List def fetch_html(url: str) -> str: try: response = requests.get(url, timeout=5) response.raise_for_status() return response.text except requests.RequestException as e: return f\\"Error fetching {url}: {str(e)}\\" def fetch_urls_concurrently(urls: List[str]) -> List[str]: results = [None] * len(urls) def fetch_indexed_html(index: int, url: str): results[index] = fetch_html(url) with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor: futures = [] for index, url in enumerate(urls): futures.append(executor.submit(fetch_indexed_html, index, url)) for future in concurrent.futures.as_completed(futures): try: future.result() except Exception as e: print(f\\"Error executing a future task: {str(e)}\\") return results"},{"question":"# Advanced Memory Management in Python **Objective:** Implement a Python program using the `ctypes` library to interact with the C-style memory management functions provided in the documentation (specifically `PyMem_RawMalloc`, `PyMem_RawFree`, and others). This exercise will test your understanding of Python\'s memory management system and your ability to interface with C-style memory functions in a Python environment. # Task: 1. **Memory Management Functions Implementation:** Implement the following functions in Python using `ctypes` to: - `allocate_raw_memory`: Allocate a specified number of bytes of raw memory using `PyMem_RawMalloc`. - `resize_raw_memory`: Resize a previously allocated memory block using `PyMem_RawRealloc`. - `free_raw_memory`: Free a previously allocated memory block using `PyMem_RawFree`. 2. **Usage of Memory Management Functions:** Write a function `use_memory_management` that: - Allocates a block of raw memory of a specified size. - Fills the allocated memory with a fixed pattern of bytes. - Resizes the allocated memory block to a new size. - Frees the memory block. - Handles any errors during allocation, resizing, or freeing of memory. # Expected Function Signatures: ```python from ctypes import * def allocate_raw_memory(size: int) -> POINTER(c_void_p): pass def resize_raw_memory(ptr: POINTER(c_void_p), new_size: int) -> POINTER(c_void_p): pass def free_raw_memory(ptr: POINTER(c_void_p)) -> None: pass def use_memory_management(initial_size: int, resize_size: int) -> None: pass ``` # Expected Behavior: - `allocate_raw_memory(size)`: - Input: `size` is a positive integer specifying the number of bytes to allocate. - Output: Returns a pointer to the allocated memory block. - If allocation fails, should handle the error appropriately. - `resize_raw_memory(ptr, new_size)`: - Input: `ptr` is a pointer to an already allocated memory block. `new_size` is the new size for the memory block. - Output: Returns a pointer to the resized memory block. - If resizing fails, should handle the error appropriately. - `free_raw_memory(ptr)`: - Input: `ptr` is a pointer to an allocated memory block. - Output: Frees the memory block pointed to by `ptr`. - If `ptr` is `None`, no operation should be performed. - `use_memory_management(initial_size, resize_size)`: - Input: `initial_size` is the initial size of memory to allocate, `resize_size` is the new size for the memory block. - Output: Demonstrates the use of the above functions by: - Allocating a memory block of `initial_size`. - Filling the block with a pattern of bytes (e.g., `0xCD`). - Resizing the block to `resize_size`. - Freeing the block. # Constraints: - Appropriate error handling must be implemented. - Ensure that there is no memory leak. - Integration of `ctypes` with the specified memory functions and proper management as per Python memory manager\'s guidelines should be maintained. **Example Usage:** ```python def main(): use_memory_management(64, 128) if __name__ == \\"__main__\\": main() ``` This example would allocate 64 bytes of raw memory, fill it, then resize it to 128 bytes, and finally free the memory, all while handling potential errors.","solution":"from ctypes import * # Load the Python API shared object file pythonapi = cdll.LoadLibrary(\\"libpython3.so\\") # Define the PyMem_RawMalloc, PyMem_RawRealloc, and PyMem_RawFree functions PyMem_RawMalloc = pythonapi.PyMem_RawMalloc PyMem_RawMalloc.restype = POINTER(c_void_p) PyMem_RawMalloc.argtypes = [c_size_t] PyMem_RawRealloc = pythonapi.PyMem_RawRealloc PyMem_RawRealloc.restype = POINTER(c_void_p) PyMem_RawRealloc.argtypes = [POINTER(c_void_p), c_size_t] PyMem_RawFree = pythonapi.PyMem_RawFree PyMem_RawFree.restype = None PyMem_RawFree.argtypes = [POINTER(c_void_p)] def allocate_raw_memory(size: int) -> POINTER(c_void_p): ptr = PyMem_RawMalloc(size) if not ptr: raise MemoryError(\\"Failed to allocate memory.\\") return ptr def resize_raw_memory(ptr: POINTER(c_void_p), new_size: int) -> POINTER(c_void_p): new_ptr = PyMem_RawRealloc(ptr, new_size) if not new_ptr: raise MemoryError(\\"Failed to reallocate memory.\\") return new_ptr def free_raw_memory(ptr: POINTER(c_void_p)) -> None: if ptr: PyMem_RawFree(ptr) def use_memory_management(initial_size: int, resize_size: int) -> None: try: # Allocate initial memory mem_block = allocate_raw_memory(initial_size) # Fill with a fixed pattern (0xCD) memset(mem_block, 0xCD, initial_size) # Resize the memory block mem_block = resize_raw_memory(mem_block, resize_size) # Optionally fill new area with a pattern as well if resize_size > initial_size: additional_size = resize_size - initial_size additional_ptr = cast(c_void_p(addressof(mem_block.contents) + initial_size), POINTER(c_char)) memset(additional_ptr, 0xCD, additional_size) finally: # Free the memory block free_raw_memory(mem_block) # Helper function to fill memory block def memset(ptr: POINTER(c_void_p), value: int, size: int): mem_array = cast(ptr, POINTER(c_char * size)).contents for i in range(size): mem_array[i] = value"},{"question":"# XML Processing with `xml.dom.pulldom` In this assessment, you are required to demonstrate your understanding of the `xml.dom.pulldom` module. Your task is to implement a function `filter_expensive_items` that processes an XML string representing sales items and extracts items with a price greater than a specified value. Function Specification **Function Name:** `filter_expensive_items` **Input:** - `xml_string` (str): A string containing XML data representing sales items. - `price_threshold` (int): An integer representing the price threshold. **Output:** - Returns a list of strings: Each string is an XML representation of an item whose price is greater than the specified `price_threshold`. Constraints: - The XML string will contain well-formed XML data. - The items will have a `<price>` attribute which will always be an integer. - The XML should be parsed using the `xml.dom.pulldom` module. Example XML Input ```xml <sales> <item price=\\"30\\">Item 1</item> <item price=\\"60\\">Item 2</item> <item price=\\"80\\">Item 3</item> <item price=\\"20\\">Item 4</item> </sales> ``` Example Function Call ```python xml_data = \'\'\' <sales> <item price=\\"30\\">Item 1</item> <item price=\\"60\\">Item 2</item> <item price=\\"80\\">Item 3</item> <item price=\\"20\\">Item 4</item> </sales> \'\'\' result = filter_expensive_items(xml_data, 50) ``` Expected Output ```python [ \'<item price=\\"60\\">Item 2</item>\', \'<item price=\\"80\\">Item 3</item>\' ] ``` Implementation Details: 1. Use the `parseString` function from the `xml.dom.pulldom` module to parse the XML string. 2. Loop through the events and nodes in the document. 3. When you encounter a `START_ELEMENT` event for `item` elements, check the value of their `price` attribute. 4. If the price exceeds the specified threshold, use `expandNode` to get the full element, and convert it to an XML string using `toxml()`. 5. Collect and return all such XML strings in a list. Here is the function scaffold for you to start with: ```python from xml.dom import pulldom def filter_expensive_items(xml_string, price_threshold): result = [] doc = pulldom.parseString(xml_string) for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \'item\': price = int(node.getAttribute(\'price\')) if price > price_threshold: doc.expandNode(node) result.append(node.toxml()) return result ``` Write the function `filter_expensive_items` based on the description provided, and ensure it handles XML data as specified.","solution":"from xml.dom import pulldom def filter_expensive_items(xml_string, price_threshold): Processes the XML string representing sales items and extracts items with a price greater than the specified threshold. Parameters: xml_string (str): The input XML data. price_threshold (int): The price threshold. Returns: list: A list of strings, each being an XML representation of items with a price greater than price_threshold. result = [] doc = pulldom.parseString(xml_string) for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \'item\': price = int(node.getAttribute(\'price\')) if price > price_threshold: doc.expandNode(node) result.append(node.toxml()) return result"},{"question":"# Multi-threaded Counter You are required to implement a multi-threaded program that simulates multiple threads incrementing a shared counter. This exercise will test your understanding of threading primitives, particularly locks for synchronization. Task: 1. Implement a `Counter` class that: - Initializes a counter with value `0`. - Provides an `increment` method to safely increase the counter\'s value by `1`. - Provides a `get_value` method to retrieve the current value of the counter. 2. Implement a `run_threads` function that: - Takes an integer `n` as input, representing the number of threads. - Creates and starts `n` threads. - Each thread calls the `increment` method of a single shared `Counter` instance 1000 times. - Waits for all threads to complete execution. - Returns the final value of the counter. Requirements: - Use the `_thread` module to manage threads. - Use locks to ensure that the counter increments safely when accessed by multiple threads simultaneously. Constraints: - The `increment` method must be thread-safe. - The `run_threads` function should be optimized for performance and thread management. Example: ```python import _thread # Define the Counter class here class Counter: def __init__(self): self.value = 0 self.lock = _thread.allocate_lock() def increment(self): with self.lock: self.value += 1 def get_value(self): return self.value def run_threads(n): counter = Counter() def worker(): for _ in range(1000): counter.increment() threads = [] for _ in range(n): thread_id = _thread.start_new_thread(worker, ()) threads.append(thread_id) # Wait for threads to complete (this part may need a more sophisticated approach) import time time.sleep(1) return counter.get_value() # Test the function n = 10 final_counter_value = run_threads(n) print(final_counter_value) # Expected: 10000 (10 threads * 1000 increments each) ``` Make sure your implementation adheres to proper threading practices as outlined in the `_thread` documentation.","solution":"import _thread import threading class Counter: def __init__(self): self.value = 0 self.lock = _thread.allocate_lock() def increment(self): with self.lock: self.value += 1 def get_value(self): return self.value def run_threads(n): counter = Counter() def worker(): for _ in range(1000): counter.increment() threads = [] for _ in range(n): thread = threading.Thread(target=worker) threads.append(thread) thread.start() for thread in threads: thread.join() return counter.get_value()"},{"question":"# Python Coding Assessment Question Objective: Assess the student\'s understanding and ability to work with regular expressions in Python, demonstrating both fundamental and advanced regex concepts. Question: You are required to write a Python function that processes a list of strings and applies a series of regex-based transformations to each string. The transformations involve identifying and replacing certain patterns. Implement the function `process_strings`. Function Signature: ```python def process_strings(input_list: list) -> list: ``` Input: - `input_list`: A list of strings `List[str]` where each string may contain various patterns. Output: - Returns a list of strings `List[str]` after applying the specified transformations. Transformations: 1. Replace all email addresses in the string with the string `[EMAIL REDACTED]`. - Email addresses are assumed to be in the format: `username@domain.extension` (e.g., test@example.com). 2. Identify and swap the day and month in dates formatted as `DD-MM-YYYY` to `MM-DD-YYYY` - For example, `25-12-2020` should be transformed to `12-25-2020`. 3. Remove any HTML tags in the string. - HTML tags are defined as any substring surrounded by `<` and `>` (e.g., `<tag>content</tag>`). Example: ```python input_list = [ \\"Contact me at test@example.com on 25-12-2020. Visit <a href=\'example.com\'>our site</a>.\\", \\"Another email: another@test.com; Important date: 01-01-2021; Sample <b>text</b>.\\" ] output_list = process_strings(input_list) print(output_list) ``` Expected Output: ```python [ \\"Contact me at [EMAIL REDACTED] on 12-25-2020. Visit our site.\\", \\"Another email: [EMAIL REDACTED]; Important date: 01-01-2021; Sample text.\\" ] ``` Constraints: 1. Each string in the input list will have a maximum length of 200 characters. 2. The input list will contain up to 100 strings. Requirements: - Use the `re` module for regular expression operations. - Ensure that the function handles edge cases, such as strings without any patterns to replace, strings with multiple emails, dates, or HTML tags. Performance: - The function should run efficiently within the given constraints, with a focus on readability and maintainability of the code.","solution":"import re def process_strings(input_list): def replace_email(text): email_pattern = r\'b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+.[A-Z|a-z]{2,7}b\' return re.sub(email_pattern, \'[EMAIL REDACTED]\', text) def swap_date_format(text): date_pattern = r\'b(d{2})-(d{2})-(d{4})b\' return re.sub(date_pattern, r\'2-1-3\', text) def remove_html_tags(text): html_pattern = r\'<.*?>\' return re.sub(html_pattern, \'\', text) output_list = [] for string in input_list: string = replace_email(string) string = swap_date_format(string) string = remove_html_tags(string) output_list.append(string) return output_list"},{"question":"Objective You are tasked with writing a Python function that reads, modifies, and writes back data to a `.netrc` file using the capabilities of the `netrc` module. Problem Statement Write a function `modify_netrc(file_path: str, host: str, new_login: str, new_password: str) -> None` that performs the following operations: 1. Reads the contents of the specified `.netrc` file. 2. Updates the login and password for the given `host`. If the `host` is not present, add a new entry for it. 3. Ensure that the update adheres to all security restrictions and encoding requirements, as described in the `netrc` module documentation. 4. Write the updated data back to the `.netrc` file. 5. Handle any parsing errors by printing an appropriate message. Function Signature ```python def modify_netrc(file_path: str, host: str, new_login: str, new_password: str) -> None: pass ``` Input - `file_path` (str): The file path to the `.netrc` file. - `host` (str): The host name for which the login and password need to be updated. - `new_login` (str): The new login to be updated for the specified host. - `new_password` (str): The new password to be updated for the specified host. Output - The function should not return any value. Constraints and Performance Requirements - The `.netrc` file format should be adhered to strictly. - Handle exceptions, such as file not found or parse errors. - UTF-8 encoding should be attempted before defaulting to the locale-specific encoding. - The function should ensure that insecure file permissions raise an error. Example Given a `.netrc` file at `test.netrc` with contents: ``` machine example.com login old_user password old_pass ``` Calling the function as follows: ```python modify_netrc(\'test.netrc\', \'example.com\', \'new_user\', \'new_pass\') ``` should update the `test.netrc` file to: ``` machine example.com login new_user password new_pass ``` If the host is not present: ```python modify_netrc(\'test.netrc\', \'newhost.com\', \'new_user\', \'new_pass\') ``` should append: ``` machine newhost.com login new_user password new_pass ``` Notes - Ensure the function modifies the file in place. - Print appropriate error messages for exceptions encountered.","solution":"import netrc import os def modify_netrc(file_path: str, host: str, new_login: str, new_password: str) -> None: try: # Attempt to read the .netrc file nrc = netrc.netrc(file_path) except (FileNotFoundError, netrc.NetrcParseError) as e: print(f\\"Error reading .netrc file: {e}\\") return # Set the permissions to 600 if they are incorrect file_stat = os.stat(file_path) if oct(file_stat.st_mode)[-3:] != \'600\': print(\\"Insecure file permissions; setting to 600\\") os.chmod(file_path, 0o600) # Update or add the host information updated = False if host in nrc.hosts: nrc.hosts[host] = (new_login, None, new_password) updated = True else: nrc.hosts[host] = (new_login, None, new_password) # Write the changes back to the file with open(file_path, \'w\', encoding=\'utf-8\') as file: for machine in nrc.hosts: login, account, password = nrc.hosts[machine] file.write(f\\"machine {machine}n login {login}n password {password}n\\") if updated: print(f\\"Updated login and password for host \'{host}\'\\") else: print(f\\"Added new entry for host \'{host}\'\\")"},{"question":"You are tasked with analyzing a company\'s employee structure and generating human-readable reports. The data for the employees are provided in nested dictionaries, and you need to present this data in a clear and readable format. To achieve this, you should use the `pprint` module. # Requirements Function 1: `generate_employee_report` Implement a function `generate_employee_report(employees: dict, config: dict) -> str` that: 1. Accepts: - `employees`: A nested dictionary representing employee data. - `config`: A dictionary containing configuration options for how the `pprint` module should format the output. - `indent` (int): Specifies the amount of indentation. - `width` (int): Specifies the maximum number of characters per line. - `depth` (int): Controls the number of nesting levels to be printed. - `compact` (bool): Determines if sequences should be printed in a compact format. - `sort_dicts` (bool): Determines if dictionaries should be sorted by key. 2. Returns a string that contains the pretty-printed version of the employee data using the configuration options provided. # Function 2: `validate_report_readability` Implement a function `validate_report_readability(report: str) -> bool` that: 1. Accepts a single argument: - `report`: The string output generated by `generate_employee_report`. 2. Uses the `pprint` module to determine if the report is readable (i.e., can be used to reconstruct the value using `eval()`). 3. Returns `True` if the report is readable, otherwise returns `False`. # Example Usage ```python import pprint employees = { \'Manager\': { \'Name\': \'Alice\', \'Age\': 45, \'Department\': \'HR\', \'Subordinates\': { \'Supervisor\': { \'Name\': \'Bob\', \'Age\': 38, \'Department\': \'HR\', \'Subordinates\': { \'Employee1\': { \'Name\': \'Charlie\', \'Age\': 28, \'Department\': \'HR\' }, \'Employee2\': { \'Name\': \'Dave\', \'Age\': 25, \'Department\': \'HR\' } } } } } } config = { \'indent\': 2, \'width\': 60, \'depth\': 2, \'compact\': True, \'sort_dicts\': True } report = generate_employee_report(employees, config) print(report) # prints a formatted string of the employee structure is_readable = validate_report_readability(report) print(is_readable) # prints True or False based on readability of the report ``` # Constraints - Avoid using any additional libraries except for Python\'s standard library. - Ensure that the function signatures adhere strictly to the specifications. - Assume the input data is well-formed and correctly structured. # Evaluation Criteria - Correct and efficient implementation of the two functions. - Proper handling of nested data structures and formatting options. - Validating the readability of the generated report correctly.","solution":"import pprint def generate_employee_report(employees: dict, config: dict) -> str: Generates a human-readable report of employee data using the pprint module. Args: employees (dict): A nested dictionary representing employee data. config (dict): Configuration options for how pprint should format the output. - indent (int): The amount of indentation. - width (int): The maximum number of characters per line. - depth (int): The number of nesting levels to be printed. - compact (bool): If sequences should be printed in a compact format. - sort_dicts (bool): If dictionaries should be sorted by key. Returns: str: A pretty-printed string of the employee data based on the given configuration. formatter = pprint.PrettyPrinter( indent=config[\'indent\'], width=config[\'width\'], depth=config[\'depth\'], compact=config[\'compact\'], sort_dicts=config[\'sort_dicts\'] ) return formatter.pformat(employees) def validate_report_readability(report: str) -> bool: Validates the readability of the generated report by evaluating if it can be reconstructed using eval(). Args: report (str): The string output generated by generate_employee_report. Returns: bool: True if the report is readable and can be reconstructed by eval(), otherwise False. try: eval(report) # Try to reconstruct the value using eval return True except: return False"},{"question":"# Question: Pseudo-Terminal Logger You are tasked with creating a functionality that executes a given shell command in a pseudo-terminal and captures its output in real-time, writing it to a log file. Requirements: 1. Implement a function `execute_and_log(command: str, log_file: str) -> int` which should: - Execute the given shell command in a pseudo-terminal. - Write the output of the shell command to the specified log file in real-time. - Return the exit status code of the command. Function Signature ```python import os import pty def execute_and_log(command: str, log_file: str) -> int: # Your implementation here ``` Input - `command`: A `str` representing the shell command to be executed. - `log_file`: A `str` representing the path to the log file where the command\'s output will be written. Output - The function should return an `int`, which is the exit status code of the executed command. Example ```python exit_code = execute_and_log(\\"echo Hello World!\\", \\"output.log\\") print(exit_code) # Should print 0 if the command was successful # output.log should contain the text: # Hello World! ``` Constraints: - The `command` should be a valid shell command. - The `log_file` path should be writable. Use the `pty` module to handle the pseudo-terminal operations and ensure output is captured in real-time. Consider edge cases where commands might fail or where there might be an error opening the log file. **Hint**: You may utilize the `pty.spawn` function for executing the command and handling its I/O.","solution":"import os import pty import subprocess def execute_and_log(command: str, log_file: str) -> int: def read(fd): with open(log_file, \'a\') as f: while True: try: data = os.read(fd, 1024) if not data: break f.write(data.decode(\'utf-8\')) f.flush() except OSError as e: break return pid, fd = pty.fork() if pid == 0: os.execv(\'/bin/sh\', [\'sh\', \'-c\', command]) else: read(fd) _, status = os.waitpid(pid, 0) return os.WEXITSTATUS(status)"},{"question":"# Object Protocol Exercise Objective: To demonstrate your understanding of Python\'s object protocols, you will implement a custom class `FlexibleDict` that mimics some behaviors of Python dictionaries but with additional controlled attribute access. You need to manage the attributes using the provided object protocol functions. Requirements: 1. Implement the `FlexibleDict` class with the following methods: - `__init__(self, **kwargs)`: Initializes the dictionary with given keyword arguments. - `__getitem__(self, key)`: Retrieves the value associated with the provided `key`. - `__setitem__(self, key, value)`: Sets the `value` for the given `key`. - `__delitem__(self, key)`: Deletes the `key` from the dictionary. - `__contains__(self, key)`: Checks if `key` is in the dictionary. - `__len__(self)`: Returns the number of items in the dictionary. - `__repr__(self)`: Returns a string representation of the dictionary. - Additionally, implement methods to get and set attributes dynamically using `PyObject_GetAttrString` and `PyObject_SetAttrString`. Input Format: - The class should be able to handle any keyword arguments provided during initialization. - All dictionary operations should accept standard dictionary keys (strings, integers, etc.). - Attributes accessed or set dynamically should be strings and valid Python identifiers. Output Format: - The output should reflect standard dictionary operation outputs and include string representations, as appropriate. - Any errors should be handled gracefully, especially with attribute access. Constraints: - Use the object protocol functions where necessary (e.g., `PyObject_GetAttrString`, `PyObject_SetAttrString`). - Ensure that key operations (get, set, delete) on the dictionary mimic its native behavior. - Performance must handle large numbers of insertions and deletions efficiently. Example: ```python fd = FlexibleDict(name=\\"Test\\", value=42) # Access dictionary-like items print(fd[\'name\']) # Output: Test fd[\'new_key\'] = 99 print(fd[\'new_key\']) # Output: 99 del fd[\'value\'] # Dynamic attribute setting and getting using provided protocol methods fd.set_attr(\'dynamic_attr\', \'DynamicValue\') print(fd.get_attr(\'dynamic_attr\')) # Output: DynamicValue print(fd) # Output: FlexibleDict({\'name\': \'Test\', \'new_key\': 99}) ``` Implement the `FlexibleDict` class to fulfill the requirements above and ensure it operates as expected with the example interactions.","solution":"class FlexibleDict: def __init__(self, **kwargs): Initializes the FlexibleDict with given keyword arguments. self._dict = kwargs def __getitem__(self, key): return self._dict[key] def __setitem__(self, key, value): self._dict[key] = value def __delitem__(self, key): del self._dict[key] def __contains__(self, key): return key in self._dict def __len__(self): return len(self._dict) def __repr__(self): return f\\"FlexibleDict({self._dict})\\" def set_attr(self, attr, value): if not isinstance(attr, str): raise TypeError(\\"Attribute name must be a string\\") self.__dict__[attr] = value def get_attr(self, attr): if not isinstance(attr, str): raise TypeError(\\"Attribute name must be a string\\") return self.__dict__.get(attr, None)"},{"question":"Objective You are required to implement a function that traverses a directory tree and compiles a summary of the file types and file permissions within that directory. The summary should be returned as a dictionary where keys are file paths and values are dictionaries with file type and permission information. Function Signature ```python import os def summarize_directory(path: str) -> dict: Summarizes the file types and permissions in the directory tree rooted at \'path\'. Parameters: path (str): The root directory path to start the traversal. Returns: dict: A dictionary with file paths as keys and dictionaries with \'type\' and \'permissions\' as values. Example: { \'path/to/file1\': {\'type\': \'regular\', \'permissions\': \'-rwxr-xr--\'}, \'path/to/file2\': {\'type\': \'directory\', \'permissions\': \'drwxr-xr-x\'}, ... } ``` Input - `path`: A string representing the path to the directory to be traversed. Output - A dictionary where: - Keys are strings representing file paths. - Values are dictionaries with keys \'type\' and \'permissions\'. \'type\' is a string indicating the file type (e.g., \'regular\', \'directory\', etc.) and \'permissions\' is a string representing the file\'s permissions in the form `-rwxrwxrwx`. Constraints - You can assume that the input path is a valid directory. - Your solution should handle all possible file types specified by the `stat` module (e.g., regular files, directories, symbolic links, etc.). - Your solution should be efficient with respect to both time and space. Example ```python import os # Assuming the following file structure: # test_dir/ # ├── file1.txt # ├── file2.py # ├── dir1/ # │ └── file3.txt # └── link -> dir1/file3.txt result = summarize_directory(\'test_dir\') # Example result (permissions will vary depending on your system settings): # { # \'test_dir/file1.txt\': {\'type\': \'regular\', \'permissions\': \'-rwxr--r--\'}, # \'test_dir/file2.py\': {\'type\': \'regular\', \'permissions\': \'-rwxr-xr-x\'}, # \'test_dir/dir1\': {\'type\': \'directory\', \'permissions\': \'drwxr-xr-x\'}, # \'test_dir/dir1/file3.txt\': {\'type\': \'regular\', \'permissions\': \'-rw-r--r--\'}, # \'test_dir/link\': {\'type\': \'symlink\', \'permissions\': \'lrwxrwxrwx\'} # } ``` Additional Information You may find the following `stat` module functions and constants useful: - `stat.S_ISDIR(mode)`: Checks if the mode is from a directory. - `stat.S_ISREG(mode)`: Checks if the mode is from a regular file. - `stat.S_ISLNK(mode)`: Checks if the mode is from a symbolic link. - `stat.filemode(mode)`: Converts a file\'s mode to a string of the form `-rwxrwxrwx`. Use the `os.lstat()` function to get the status of a file, including for symbolic links.","solution":"import os import stat def summarize_directory(path): Summarizes the file types and permissions in the directory tree rooted at \'path\'. Parameters: path (str): The root directory path to start the traversal. Returns: dict: A dictionary with file paths as keys and dictionaries with \'type\' and \'permissions\' as values. Example: { \'path/to/file1\': {\'type\': \'regular\', \'permissions\': \'-rwxr-xr--\'}, \'path/to/file2\': {\'type\': \'directory\', \'permissions\': \'drwxr-xr-x\'}, ... } summary = {} for dirpath, dirnames, filenames in os.walk(path): for name in dirnames + filenames: filepath = os.path.join(dirpath, name) try: st = os.lstat(filepath) except FileNotFoundError: continue if stat.S_ISDIR(st.st_mode): file_type = \'directory\' elif stat.S_ISREG(st.st_mode): file_type = \'regular\' elif stat.S_ISLNK(st.st_mode): file_type = \'symlink\' else: file_type = \'other\' permissions = stat.filemode(st.st_mode) summary[filepath] = {\'type\': file_type, \'permissions\': permissions} return summary"},{"question":"# Secure Login System **Objective:** Write a Python function that implements a simple secure login system using the `getpass` module. **Task:** Implement a function named `secure_login` that performs the following steps: 1. **Prompt for Username:** - The function should first prompt the user to enter their username. - Use the `getpass.getuser` function to retrieve the current system\'s username and display it to confirm if they are logging in as the correct user. 2. **Prompt for Password:** - Prompt the user for a password using `getpass.getpass` without echoing. - Use a default prompt message: \\"Enter your password: \\". 3. **Authenticate User:** - Simulate password verification by checking if the entered password matches a predefined password (e.g., \\"SecurePassword123\\"). - If the password matches, print \\"Login successful!\\". - If the password does not match, print \\"Invalid password. Try again.\\" and prompt for the password again. - Allow the user up to 3 attempts to enter the correct password. **Function Signature:** ```python def secure_login(username: str): pass ``` **Input:** - `username` (string): The username of the person trying to log in. **Output:** - Print messages based on the login attempt results. **Constraints:** - The user is allowed a maximum of 3 attempts to enter the correct password. - Use the `getpass.getpass` function to hide password input. - Use the `getpass.getuser` function to retrieve and confirm the current system username. **Example:** ```python # Assuming the system username is \\"user1\\" and the predefined password is \\"SecurePassword123\\" secure_login(\'user1\') ``` **Expected Output:** ``` You are logging in as user1 Enter your password: Invalid password. Try again. Enter your password: Login successful! ``` **Notes:** - Handle edge cases where the user may cancel the input (e.g., by pressing Ctrl+C). - Ensure that the username provided in the function parameter is the same as the system username retrieved using `getpass.getuser`.","solution":"import getpass def secure_login(username: str): Simulates a secure login system by prompting for a username and password. Parameters: username (str): The username of the person trying to log in. Returns: None try: system_username = getpass.getuser() print(f\\"You are logging in as {system_username}\\") if system_username != username: print(\\"The username you entered does not match the system username.\\") return predefined_password = \\"SecurePassword123\\" attempts = 0 while attempts < 3: password = getpass.getpass(\\"Enter your password: \\") if password == predefined_password: print(\\"Login successful!\\") return else: print(\\"Invalid password. Try again.\\") attempts += 1 print(\\"Maximum attempts reached. Access denied.\\") except KeyboardInterrupt: print(\\"nLogin cancelled.\\") # Note: This function relies on user input and getpass.getuser which makes it not suitable for unit testing # directly. We\'ll create a mock for testing purposes."},{"question":"# Tokenize and Modify Python Source Code **Objective:** Implement a function `replace_variable_names` that tokenizes a given Python source code, replaces all occurrences of a specified variable name with a new variable name, and returns the modified source code. # Function Signature ```python def replace_variable_names(source_code: str, old_name: str, new_name: str) -> str: pass ``` # Input: - `source_code` (str): A string containing valid Python source code. - `old_name` (str): The variable name to be replaced. - `new_name` (str): The new variable name to replace the old variable name. # Output: - (str): The modified Python source code as a string with the `old_name` replaced by `new_name`. # Constraints: - The input Python source code (`source_code`) is always syntactically valid. - The `old_name` is guaranteed to be a valid Python identifier and present in the `source_code`. - The `new_name` is guaranteed to be a valid Python identifier. - The function should only replace occurrences of the `old_name` that are used as variable names (e.g., not inside strings or comments). - You may assume that variable names are not keywords or part of compound identifiers. # Example ```python source_code = \'\'\' def calculate_area(radius): pi = 3.14 area = pi * (radius ** 2) return area result = calculate_area(5) print(result) \'\'\' old_name = \'area\' new_name = \'circle_area\' # Call the function modified_code = replace_variable_names(source_code, old_name, new_name) print(modified_code) ``` # Expected Output: ```python \'\'\' def calculate_area(radius): pi = 3.14 circle_area = pi * (radius ** 2) return circle_area result = calculate_area(5) print(result) \'\'\' ``` # Explanation: The function should tokenize the `source_code`, identify tokens corresponding to the `old_name` used as variables, replace them with the `new_name`, and then reconstruct the source code from the modified tokens using the `tokenize` module. **Implementation Guidance:** 1. Use `tokenize.generate_tokens` to tokenize the input source code. 2. Iterate through the tokens and replace occurrences of `old_name` with `new_name` where applicable. 3. Use `tokenize.untokenize` to convert the modified tokens back into source code. 4. Return the resulting modified source code string. This question assesses the student\'s ability to understand and manipulate token streams, utilize Python\'s `tokenize` module, and apply that knowledge in modifying source code in a nuanced and correct manner.","solution":"import io import tokenize def replace_variable_names(source_code: str, old_name: str, new_name: str) -> str: Replace all occurrences of a specified variable name with a new variable name. Parameters: source_code (str): The Python source code. old_name (str): The variable name to be replaced. new_name (str): The new variable name to replace the old name. Returns: str: The modified source code. # Using io.StringIO to simulate a file object for tokenize source_lines = io.StringIO(source_code).readline # Read the source code lines. tokens = tokenize.generate_tokens(source_lines) modified_tokens = [] for token in tokens: token_type, token_string, start, end, line = token if token_type == tokenize.NAME and token_string == old_name: # Replace the old variable name with the new variable name token = (token_type, new_name, start, end, line) modified_tokens.append(token) # Untokenize the list of modified tokens to get the modified source code string modified_source_code = tokenize.untokenize(modified_tokens) return modified_source_code"},{"question":"Problem Statement: You are required to design a simulation for a simple multi-server queue system. This system will have customers arriving at random intervals and being served by the next available server. Each server processes customers at varying service times. Collect and report statistics about the customer wait times and server utilization. Requirements: 1. **Arrival Times**: Generate random arrival times for customers using an exponential distribution. 2. **Service Times**: Each server should have a different service time generated using a Gaussian distribution. 3. **Wait Times**: Calculate the wait time for each customer before being served. 4. **Server Utilization**: Calculate the average utilization of each server. Implement the following function: ```python import random from heapq import heapify, heappop, heappush from statistics import mean def multi_server_queue_simulation(num_servers, num_customers, arrival_rate, service_time_mean, service_time_std): Simulates a multi-server queue system. Parameters: - num_servers (int): Number of available servers. - num_customers (int): Total number of customers to be serviced. - arrival_rate (float): Rate of customer arrivals (lambda) for the exponential distribution. - service_time_mean (float): Mean of the Gaussian distribution for service times. - service_time_std (float): Standard deviation of the Gaussian distribution for service times. Returns: - avg_wait_time (float): The average waiting time of customers. - server_utilization (list of float): The average utilization percentage of each server. # Implement the function according to the description pass # Example Usage: # avg_wait_time, server_utilization = multi_server_queue_simulation(3, 1000, 1/5, 10, 2) # print(f\'Average Wait Time: {avg_wait_time}nServer Utilization: {server_utilization}\') ``` Explanation: - **num_servers**: Number of servers available to serve customers. - **num_customers**: Total number of customers in the simulation. - **arrival_rate**: The rate (`lambda`) parameter for the exponential distribution determining customer arrivals. - **service_time_mean**: Mean (`mu`) of the Gaussian distribution for service times. - **service_time_std**: Standard deviation (`sigma`) of the Gaussian distribution for service times. Constraints: - The service time standard deviation must be greater than zero. - The number of servers and customers must be positive integers. - The arrival rate must be a positive float. Output: - `avg_wait_time`: The average waiting time experienced by customers before being served. - `server_utilization`: A list of average utilization percentages for each server, indicating how busy each server was during the simulation. Use the `random` module functions: `expovariate`, `gauss`, and others as needed to simulate random arrivals and services. Utilize heaps from the `heapq` module to manage server availability efficiently. Performance Requirements: - Ensure the function runs efficiently for larger inputs (e.g., num_customers = 100,000).","solution":"import random from heapq import heapify, heappop, heappush from statistics import mean def multi_server_queue_simulation(num_servers, num_customers, arrival_rate, service_time_mean, service_time_std): if num_servers <= 0 or num_customers <= 0 or arrival_rate <= 0 or service_time_std <= 0: raise ValueError(\\"Invalid parameters\\") # Generate arrival times by summing exponential inter-arrival times arrival_times = [] current_time = 0 for _ in range(num_customers): current_time += random.expovariate(arrival_rate) arrival_times.append(current_time) # Initialize servers with availability times servers = [0] * num_servers heapify(servers) wait_times = [] server_busy_times = [0] * num_servers for arrival_time in arrival_times: # Get the next available server next_available_server_time = heappop(servers) service_time = random.gauss(service_time_mean, service_time_std) if arrival_time < next_available_server_time: wait_time = next_available_server_time - arrival_time begin_service_time = next_available_server_time else: wait_time = 0 begin_service_time = arrival_time end_service_time = begin_service_time + service_time heappush(servers, end_service_time) server_id = servers.index(end_service_time) server_busy_times[server_id] += service_time wait_times.append(wait_time) avg_wait_time = mean(wait_times) total_time = max(arrival_times[-1], max(server_busy_times)) server_utilization = [(server_busy / total_time) * 100 for server_busy in server_busy_times] return avg_wait_time, server_utilization"}]'),I={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:z,isLoading:!1}},computed:{filteredPoems(){const s=this.searchQuery.trim().toLowerCase();return s?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(s)||e.solution&&e.solution.toLowerCase().includes(s)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(s=>setTimeout(s,1e3)),this.visibleCount+=4,this.isLoading=!1}}},D={class:"search-container"},R={class:"card-container"},L={key:0,class:"empty-state"},F=["disabled"],N={key:0},q={key:1};function M(s,e,l,m,n,o){const h=_("PoemCard");return a(),i("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",D,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>n.searchQuery=r),placeholder:"Search..."},null,512),[[y,n.searchQuery]]),n.searchQuery?(a(),i("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>n.searchQuery="")}," ✕ ")):d("",!0)]),t("div",R,[(a(!0),i(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),i("div",L,' No results found for "'+c(n.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),i("button",{key:0,class:"load-more-button",disabled:n.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[n.isLoading?(a(),i("span",q,"Loading...")):(a(),i("span",N,"See more"))],8,F)):d("",!0)])}const O=p(I,[["render",M],["__scopeId","data-v-e5447de6"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/60.md","filePath":"chatai/60.md"}'),j={name:"chatai/60.md"},H=Object.assign(j,{setup(s){return(e,l)=>(a(),i("div",null,[x(O)]))}});export{Y as __pageData,H as default};
