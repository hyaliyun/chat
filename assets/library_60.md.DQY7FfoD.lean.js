import{_ as p,o as a,c as s,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},S={class:"review-content"};function P(n,e,l,m,i,o){return a(),s("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",S,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const I=p(k,[["render",P],["__scopeId","data-v-fe8d2cde"]]),A=JSON.parse('[{"question":"# Task Scheduling using Topological Sorting You are provided with a set of tasks and their dependencies. Your goal is to determine a valid order in which these tasks can be executed using the `graphlib.TopologicalSorter` class. If there are multiple valid orders, return any one of them. If no valid order exists because the tasks contain cycles, raise a `CycleError`. # Function Signature ```python def schedule_tasks(tasks: Dict[str, List[str]]) -> List[str]: pass ``` # Input - `tasks`: A dictionary where keys are task names (strings) and values are lists of task names that must precede the key task. # Output - A list of task names in a valid topological order. # Constraints - Task names are unique and hashable. - The graph can have at most 10^5 nodes and 3 * 10^5 edges. # Example ```python tasks = { \\"wash\\": [\\"gather_laundry\\"], \\"dry\\": [\\"wash\\"], \\"fold\\": [\\"dry\\"], \\"gather_laundry\\": [] } print(schedule_tasks(tasks)) # Expected output: [\'gather_laundry\', \'wash\', \'dry\', \'fold\'] ``` # Additional Notes 1. Utilize the `TopologicalSorter` class for implementing topological sorting. 2. Handle the case where the graph contains cycles by appropriately raising a `CycleError`. 3. Ensure the function runs efficiently for large graphs within the given constraints. # Solution Template You may use the following template to get started: ```python from graphlib import TopologicalSorter, CycleError from typing import Dict, List def schedule_tasks(tasks: Dict[str, List[str]]) -> List[str]: ts = TopologicalSorter() for task, dependencies in tasks.items(): ts.add(task, *dependencies) try: return list(ts.static_order()) except CycleError as e: raise e ``` Your task is to complete the function `schedule_tasks` so that it meets the requirements specified.","solution":"from graphlib import TopologicalSorter, CycleError from typing import Dict, List def schedule_tasks(tasks: Dict[str, List[str]]) -> List[str]: ts = TopologicalSorter() for task, dependencies in tasks.items(): ts.add(task, *dependencies) try: # Compute the topological order of the tasks return list(ts.static_order()) except CycleError as e: # If there is a cycle in the graph, raise the exception raise e"},{"question":"# Objective Design and implement a Python function that demonstrates the use of custom exceptions, multiple built-in exceptions, and exception chaining. # Requirements 1. Define a custom exception `CustomError` that inherits from `Exception`. 2. Create a function `process_data(data)` that takes a list of dictionaries as input. Each dictionary should represent a record with at least two keys: \'value\' and \'status\'. 3. The function should: - Raise a `ValueError` if the \'value\' entry is not an integer. - Raise a `KeyError` if any record does not have the \'status\' key. - Use `CustomError` to indicate if \'status\' is \'invalid\' and chain it to a `ValueError`. 4. Implement exception handling to catch and handle these exceptions in a meaningful way. # Details 1. Input: A list of dictionaries, e.g., `data = [{\'value\': 10, \'status\': \'valid\'}, {\'value\': \'ten\', \'status\': \'valid\'}, {\'value\': 5}]` 2. Output: A list of tuples where each tuple contains the original record and a string indicating \'success\' or the type of failure. # Implementation Constraints - Use exception chaining where appropriate. - Do not use external libraries. - Ensure the function is properly tested with various scenarios. # Example ```python class CustomError(Exception): pass def process_data(data): result = [] for record in data: try: ... except KeyError: ... except ValueError as ve: ... ... return result # Example Input data = [ {\'value\': 10, \'status\': \'valid\'}, {\'value\': \'ten\', \'status\': \'valid\'}, {\'value\': 5}, {\'value\': 3, \'status\': \'invalid\'} ] # Example Output [ ({\'value\': 10, \'status\': \'valid\'}, \'success\'), ({\'value\': \'ten\', \'status\': \'valid\'}, \'ValueError\'), ({\'value\': 5}, \'KeyError\'), ({\'value\': 3, \'status\': \'invalid\'}, \'CustomError: invalid status\') ] ``` **Note**: The actual implementation of `process_data` function should correctly handle and log exceptions based on the requirements stated above.","solution":"class CustomError(Exception): Custom exception to represent an invalid status in the input data. pass def process_data(data): result = [] for record in data: try: if not isinstance(record.get(\'value\'), int): raise ValueError(\\"The \'value\' entry is not an integer.\\") if \'status\' not in record: raise KeyError(\\"Record does not have the \'status\' key.\\") if record[\'status\'] == \'invalid\': raise CustomError(\\"Invalid status\\") result.append((record, \'success\')) except ValueError as ve: result.append((record, f\'ValueError: {ve}\')) except KeyError as ke: result.append((record, f\'KeyError: {ke}\')) except CustomError as ce: result.append((record, f\'CustomError: {ce}\')) return result"},{"question":"**Objective**: Demonstrate proficiency in data manipulation, descriptive statistics, and visualization using pandas Series. **Problem Statement**: You are given the daily closing prices of two stocks, Stock A and Stock B, over a period of time. However, some of the data might be missing due to various reasons. Your task is to analyze this data by performing the following operations using pandas: 1. **Data Preparation**: - Read in the provided CSV files `stock_A.csv` and `stock_B.csv`. - Ensure the date column is parsed as `datetime` and is set as the index. 2. **Handling Missing Data**: - For each stock, fill the missing data points using forward fill (`ffill`) method. - If any missing data points exist at the beginning, fill them with the first non-NaN value of that series. 3. **Descriptive Statistics**: - Calculate the following statistics for each stock and print them: - Mean closing price. - Median closing price. - Standard deviation of the closing price. - The day with the highest closing price. - The day with the lowest closing price. 4. **Correlation Analysis**: - Determine and print the correlation coefficient between the closing prices of Stock A and Stock B. 5. **Visualization**: - Generate a single plot that shows the closing prices of both stocks over time. - Highlight the days with the highest and lowest closing prices for each stock on the plot. **Input Format**: - Two CSV files: `stock_A.csv` and `stock_B.csv`. Each file contains two columns: `Date` and `Close`. **Output Format**: - Print the descriptive statistics. - Print the correlation coefficient. - Plot the closing prices with appropriate annotations for the highest and lowest prices. **Constraints**: - Ensure that the filled missing data maintains the trend by using forward fill and initial fill. - Handle any potential edge cases where the data might be entirely missing for a short period. **Performance**: - The operations should be completed efficiently with a time complexity allowing handling of large datasets. Here is a skeleton code to help you get started: ```python import pandas as pd import matplotlib.pyplot as plt def load_and_prepare_data(file_path): # Read CSV file df = pd.read_csv(file_path, parse_dates=[\'Date\'], index_col=\'Date\') # Fill missing values df[\'Close\'] = df[\'Close\'].ffill().bfill() return df def calculate_statistics(df): mean_price = df[\'Close\'].mean() median_price = df[\'Close\'].median() std_dev_price = df[\'Close\'].std() highest_price_day = df[\'Close\'].idxmax() lowest_price_day = df[\'Close\'].idxmin() return { \'mean\': mean_price, \'median\': median_price, \'std_dev\': std_dev_price, \'highest_day\': highest_price_day, \'lowest_day\': lowest_price_day } def plot_prices(df_A, df_B, stats_A, stats_B): plt.figure(figsize=(14, 7)) plt.plot(df_A.index, df_A[\'Close\'], label=\'Stock A\') plt.plot(df_B.index, df_B[\'Close\'], label=\'Stock B\') # Highlight highest and lowest days plt.scatter(stats_A[\'highest_day\'], df_A.loc[stats_A[\'highest_day\'], \'Close\'], color=\'red\') plt.scatter(stats_A[\'lowest_day\'], df_A.loc[stats_A[\'lowest_day\'], \'Close\'], color=\'green\') plt.scatter(stats_B[\'highest_day\'], df_B.loc[stats_B[\'highest_day\'], \'Close\'], color=\'red\') plt.scatter(stats_B[\'lowest_day\'], df_B.loc[stats_B[\'lowest_day\'], \'Close\'], color=\'green\') plt.legend() plt.title(\'Stock Prices Over Time\') plt.xlabel(\'Date\') plt.ylabel(\'Closing Price\') plt.grid(True) plt.show() def main(): df_A = load_and_prepare_data(\'stock_A.csv\') df_B = load_and_prepare_data(\'stock_B.csv\') stats_A = calculate_statistics(df_A) stats_B = calculate_statistics(df_B) print(f\\"Stock A Statistics: {stats_A}\\") print(f\\"Stock B Statistics: {stats_B}\\") correlation = df_A[\'Close\'].corr(df_B[\'Close\']) print(f\\"Correlation between Stock A and Stock B: {correlation}\\") plot_prices(df_A, df_B, stats_A, stats_B) if __name__ == \\"__main__\\": main() ``` **Note**: Ensure that `matplotlib` library is installed to enable plotting. The CSV files should be present in the same directory as the script.","solution":"import pandas as pd import matplotlib.pyplot as plt def load_and_prepare_data(file_path): Reads the CSV file, sets the \'Date\' column as index after parsing it as datetime, and fills missing values using forward fill and backward fill if necessary. # Read CSV file df = pd.read_csv(file_path, parse_dates=[\'Date\'], index_col=\'Date\') # Fill missing values df[\'Close\'] = df[\'Close\'].ffill().bfill() return df def calculate_statistics(df): Calculates and returns the mean, median, standard deviation, day with the highest closing price, and day with the lowest closing price for the given stock data. mean_price = df[\'Close\'].mean() median_price = df[\'Close\'].median() std_dev_price = df[\'Close\'].std() highest_price_day = df[\'Close\'].idxmax() lowest_price_day = df[\'Close\'].idxmin() return { \'mean\': mean_price, \'median\': median_price, \'std_dev\': std_dev_price, \'highest_day\': highest_price_day, \'lowest_day\': lowest_price_day } def plot_prices(df_A, df_B, stats_A, stats_B): Plots the closing prices of Stock A and Stock B over time. Highlights the days with the highest and lowest closing prices for each stock. plt.figure(figsize=(14, 7)) plt.plot(df_A.index, df_A[\'Close\'], label=\'Stock A\') plt.plot(df_B.index, df_B[\'Close\'], label=\'Stock B\') # Highlight highest and lowest days plt.scatter(stats_A[\'highest_day\'], df_A.loc[stats_A[\'highest_day\'], \'Close\'], color=\'red\', label=\'Stock A Highest\') plt.scatter(stats_A[\'lowest_day\'], df_A.loc[stats_A[\'lowest_day\'], \'Close\'], color=\'green\', label=\'Stock A Lowest\') plt.scatter(stats_B[\'highest_day\'], df_B.loc[stats_B[\'highest_day\'], \'Close\'], color=\'orange\', label=\'Stock B Highest\') plt.scatter(stats_B[\'lowest_day\'], df_B.loc[stats_B[\'lowest_day\'], \'Close\'], color=\'blue\', label=\'Stock B Lowest\') plt.legend() plt.title(\'Stock Prices Over Time\') plt.xlabel(\'Date\') plt.ylabel(\'Closing Price\') plt.grid(True) plt.show() def main(): df_A = load_and_prepare_data(\'stock_A.csv\') df_B = load_and_prepare_data(\'stock_B.csv\') stats_A = calculate_statistics(df_A) stats_B = calculate_statistics(df_B) print(f\\"Stock A Statistics: {stats_A}\\") print(f\\"Stock B Statistics: {stats_B}\\") correlation = df_A[\'Close\'].corr(df_B[\'Close\']) print(f\\"Correlation between Stock A and Stock B: {correlation}\\") plot_prices(df_A, df_B, stats_A, stats_B) if __name__ == \\"__main__\\": main()"},{"question":"# Question: Create a Comprehensive Joint Plot with Customizations You are tasked with analyzing a dataset using Seaborn\'s `jointplot`. The dataset you will use is the \\"penguins\\" dataset, which comes built-in with Seaborn. Requirements: 1. **Load the penguins dataset** from Seaborn\'s built-in datasets. 2. **Create a joint plot** to visualize the relationship between `flipper_length_mm` and `body_mass_g`. 3. **Color the data points** based on the `species` column using the `hue` parameter. 4. **Overlay KDE plots** on both the joint and marginal axes. 5. **Adjust the layout**: - Set the height of the plot to 6. - Set the ratio between the joint and marginal axes to 3. - Enable marginal axis ticks. 6. **Add additional layers**: - Overlay a hexbin plot on the joint plot. - Overlay a rug plot on the marginal axes. - Overlay KDE plot contours with red color and set `levels=5`. Input: - None (the dataset is loaded directly within the code). Output: - The function should generate and display the described joint plot. Constraints: - Ensure that the KDE plot and hexbin plot do not overlap in a way that makes interpretation difficult. - Properly handle the scenario in which the dataset might have missing values in the relevant columns. Example Function Signature: ```python import seaborn as sns import matplotlib.pyplot as plt def comprehensive_joint_plot(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Your implementation here # Display the plot plt.show() ``` **Note**: The function does not take any parameters, as the dataset loading is handled internally.","solution":"import seaborn as sns import matplotlib.pyplot as plt def comprehensive_joint_plot(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Drop rows with missing values in relevant columns penguins = penguins.dropna(subset=[\\"flipper_length_mm\\", \\"body_mass_g\\"]) # Create the joint plot g = sns.jointplot( data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", height=6, ratio=3, marginal_ticks=True, kind=\\"scatter\\" ) # Overlay KDE plots sns.kdeplot( data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", ax=g.ax_joint, levels=5, color=\\"red\\" ) sns.kdeplot( data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", ax=g.ax_marg_x ) sns.kdeplot( data=penguins, y=\\"body_mass_g\\", hue=\\"species\\", ax=g.ax_marg_y ) # Overlay hexbin plot g.ax_joint.hexbin( penguins[\\"flipper_length_mm\\"], penguins[\\"body_mass_g\\"], gridsize=40, cmap=\\"Greens\\", alpha=0.5 ) # Overlay rug plot sns.rugplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", ax=g.ax_joint) sns.rugplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", ax=g.ax_marg_x) sns.rugplot(data=penguins, y=\\"body_mass_g\\", hue=\\"species\\", ax=g.ax_marg_y) # Show the plot plt.show()"},{"question":"You are working on a project that involves heavy usage of PyTorch for deep learning tasks. Understanding the configuration of your PyTorch installation and its parallel processing capabilities is crucial for performance optimization. **Task:** Write a Python function using PyTorch that does the following: 1. Retrieves and prints the current PyTorch configuration using `torch.__config__.show`. 2. Retrieves and prints the parallel processing information using `torch.__config__.parallel_info`. 3. Creates a simple neural network using `torch.nn` and prints its structure. 4. Prints the number of GPUs available and indicates if CUDA is available on the system. 5. Demonstrates a simple tensor operation and prints the result. **Input:** - None **Output:** - Print statements as described in the task steps. **Constraints:** - Use PyTorch library to achieve the required functionality. - Ensure clarity and structure in the output for easy interpretation. **Performance Requirements:** - The function should execute efficiently without unnecessary computations. ```python import torch import torch.nn as nn def print_pytorch_config_and_demo(): # Step 1: Retrieve and print the current PyTorch configuration print(\\"PyTorch Configuration:\\") torch.__config__.show() # Step 2: Retrieve and print the parallel processing information print(\\"nParallel Processing Information:\\") torch.__config__.parallel_info() # Step 3: Create a simple neural network and print its structure class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.layer1 = nn.Linear(10, 5) self.layer2 = nn.Linear(5, 1) def forward(self, x): x = torch.relu(self.layer1(x)) x = self.layer2(x) return x model = SimpleNN() print(\\"nSimple Neural Network Structure:\\") print(model) # Step 4: Print the number of GPUs and if CUDA is available print(\\"nCUDA Availability and GPU Count:\\") cuda_available = torch.cuda.is_available() num_gpus = torch.cuda.device_count() print(f\\"CUDA Available: {cuda_available}\\") print(f\\"Number of GPUs: {num_gpus}\\") # Step 5: Demonstrate a simple tensor operation a = torch.tensor([1.0, 2.0, 3.0]) b = torch.tensor([4.0, 5.0, 6.0]) c = a + b print(\\"nSimple Tensor Operation Result:\\") print(f\\"Tensor a: {a}\\") print(f\\"Tensor b: {b}\\") print(f\\"Tensor c (a + b): {c}\\") # Execute the function print_pytorch_config_and_demo() ```","solution":"import torch import torch.nn as nn def print_pytorch_config_and_demo(): # Step 1: Retrieve and print the current PyTorch configuration print(\\"PyTorch Configuration:\\") torch.__config__.show() # Step 2: Retrieve and print the parallel processing information print(\\"nParallel Processing Information:\\") torch.__config__.parallel_info() # Step 3: Create a simple neural network and print its structure class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.layer1 = nn.Linear(10, 5) self.layer2 = nn.Linear(5, 1) def forward(self, x): x = torch.relu(self.layer1(x)) x = self.layer2(x) return x model = SimpleNN() print(\\"nSimple Neural Network Structure:\\") print(model) # Step 4: Print the number of GPUs and if CUDA is available print(\\"nCUDA Availability and GPU Count:\\") cuda_available = torch.cuda.is_available() num_gpus = torch.cuda.device_count() print(f\\"CUDA Available: {cuda_available}\\") print(f\\"Number of GPUs: {num_gpus}\\") # Step 5: Demonstrate a simple tensor operation a = torch.tensor([1.0, 2.0, 3.0]) b = torch.tensor([4.0, 5.0, 6.0]) c = a + b print(\\"nSimple Tensor Operation Result:\\") print(f\\"Tensor a: {a}\\") print(f\\"Tensor b: {b}\\") print(f\\"Tensor c (a + b): {c}\\") # Execute the function print_pytorch_config_and_demo()"},{"question":"**Problem Statement:** You are given several sound files, and your task is to categorize them based on their attributes using the `sndhdr` module. Write a Python function `categorize_sound_files(file_list)` that takes a list of filenames and returns a dictionary containing the categorization. The dictionary should have the structure: ```python { \'filetype\': { \'aifc\': [file1, file2, ...], \'aiff\': [file3, file4, ...], ... }, \'framerate\': { 44100: [file1, file5, ...], 48000: [file3, file7, ...], ... }, \'nchannels\': { 1: [file2, file8, ...], 2: [file1, file3, ...], ... }, \'nframes\': { -1: [file2, file9, ...], 100000: [file1, file4, ...], ... }, \'sampwidth\': { 16: [file1, file6, ...], \'A\': [file2, file7, ...], ... } } ``` # Input - `file_list` (list of str): A list of sound filenames. # Output - dict: A dictionary categorizing files by their attributes as described above. # Constraints - You can assume that all files in the list exist and are accessible. - You should handle cases where `sndhdr` returns `None` for a file gracefully by skipping such files. # Example ```python def categorize_sound_files(file_list): # Your implementation here pass sound_files = [\'sound1.wav\', \'sound2.aifc\', \'sound3.au\'] categories = categorize_sound_files(sound_files) print(categories) ``` Example Output: ```python { \'filetype\': { \'wav\': [\'sound1.wav\'], \'aifc\': [\'sound2.aifc\'], \'au\': [\'sound3.au\'] }, \'framerate\': { 44100: [\'sound1.wav\', \'sound2.aifc\'], 48000: [\'sound3.au\'] }, \'nchannels\': { 1: [\'sound2.aifc\'], 2: [\'sound1.wav\', \'sound3.au\'] }, \'nframes\': { 100000: [\'sound1.wav\'], -1: [\'sound2.aifc\', \'sound3.au\'] }, \'sampwidth\': { 16: [\'sound1.wav\'], \'A\': [\'sound2.aifc\', \'sound3.au\'] } } ``` **Hint:** Use the functions provided by the `sndhdr` module to extract the necessary information from each file and organize the data in the required structure.","solution":"import sndhdr def categorize_sound_files(file_list): Categorizes sound files based on their attributes. Args: file_list (list of str): List of sound filenames Returns: dict: Dictionary categorizing files by their attributes categories = { \'filetype\': {}, \'framerate\': {}, \'nchannels\': {}, \'nframes\': {}, \'sampwidth\': {} } for file in file_list: file_info = sndhdr.what(file) if file_info is None: continue # Skip files that cannot be identified by sndhdr filetype, framerate, nchannels, nframes, sampwidth = file_info[:5] if filetype not in categories[\'filetype\']: categories[\'filetype\'][filetype] = [] categories[\'filetype\'][filetype].append(file) if framerate not in categories[\'framerate\']: categories[\'framerate\'][framerate] = [] categories[\'framerate\'][framerate].append(file) if nchannels not in categories[\'nchannels\']: categories[\'nchannels\'][nchannels] = [] categories[\'nchannels\'][nchannels].append(file) if nframes not in categories[\'nframes\']: categories[\'nframes\'][nframes] = [] categories[\'nframes\'][nframes].append(file) if sampwidth not in categories[\'sampwidth\']: categories[\'sampwidth\'][sampwidth] = [] categories[\'sampwidth\'][sampwidth].append(file) return categories"},{"question":"# ASCII Character Utilities You are tasked with writing a Python function that processes a list of characters to produce a summary of different types of ASCII characters in the list. Function Specification # Function Name ```python analyze_ascii_characters(char_list: List[Union[str, int]]) -> Dict[str, int] ``` # Input - `char_list`: A list of single-character strings or integers representing characters. # Output - Returns a dictionary with keys representing character categories (`\'alnum\'`, `\'alpha\'`, `\'digit\'`, `\'graph\'`, `\'print\'`, `\'punct\'`, `\'space\'`, `\'ctrl\'`, `\'meta\'`, `\'ascii\'`) and values representing the count of characters in each category. # Constraints - The input list can contain up to 1000 characters. - Each element in the list will be either a single-character string or an integer between 0 and 255. # Examples ```python char_list = [\'a\', \'1\', \'n\', 0x80, \' \'] analyze_ascii_characters(char_list) # Output: {\'alnum\': 2, \'alpha\': 1, \'digit\': 1, \'graph\': 2, \'print\': 3, \'punct\': 0, \'space\': 2, \'ctrl\': 1, \'meta\': 1, \'ascii\': 4} ``` # Notes - Utilize functions from the `curses.ascii` module such as `isalnum`, `isalpha`, `isdigit`, `isgraph`, `isprint`, `ispunct`, `isspace`, `isctrl`, `ismeta`, and `isascii`. - Remember to handle both string and integer inputs correctly by converting strings to their ASCII values with the `ord` function. # Implementation Define the function `analyze_ascii_characters` to achieve the specified behavior: - Iterate through the list of characters. - Utilize appropriate functions from the `curses.ascii` module to categorize each character. - Maintain a count for each category in a dictionary. - Return the dictionary. Good luck!","solution":"from curses import ascii from typing import List, Union, Dict def analyze_ascii_characters(char_list: List[Union[str, int]]) -> Dict[str, int]: Analyzes a list of ASCII characters and returns a summary of different types of ASCII characters. categories = { \'alnum\': 0, \'alpha\': 0, \'digit\': 0, \'graph\': 0, \'print\': 0, \'punct\': 0, \'space\': 0, \'ctrl\': 0, \'meta\': 0, \'ascii\': 0 } for char in char_list: if isinstance(char, str): char = ord(char) if ascii.isalnum(char): categories[\'alnum\'] += 1 if ascii.isalpha(char): categories[\'alpha\'] += 1 if ascii.isdigit(char): categories[\'digit\'] += 1 if ascii.isgraph(char): categories[\'graph\'] += 1 if ascii.isprint(char): categories[\'print\'] += 1 if ascii.ispunct(char): categories[\'punct\'] += 1 if ascii.isspace(char): categories[\'space\'] += 1 if ascii.isctrl(char): categories[\'ctrl\'] += 1 if ascii.ismeta(char): categories[\'meta\'] += 1 if ascii.isascii(char): categories[\'ascii\'] += 1 return categories"},{"question":"You are tasked with creating a PyTorch model that incorporates TorchScript for optimized deployment. However, you must adhere to the limitations of TorchScript as outlined in the provided documentation. Specifically, you should avoid using unsupported constructs and properly handle the differences in function schemas between Python and TorchScript. Objective: 1. **Implement a simple neural network** using PyTorch. 2. **Convert the model to TorchScript** using `torch.jit.script`. 3. **Avoid using the unsupported constructs** listed in the provided documentation. 4. **Handle schema differences** where necessary (e.g., always specify `dtype`, `layout`, and `device` where required). Requirements: 1. **Define a simple neural network** with at least one hidden layer using PyTorch\'s `torch.nn` module. 2. **Avoid using** any of the following in your model or its initialization: - `torch.nn.RNN` - `torch.nn.AdaptiveLogSoftmaxWithLoss` - `torch.autograd.Function` - `torch.autograd.enable_grad` 3. **Avoid using** the following initialization functions: - `torch.nn.init.calculate_gain` - `torch.nn.init.eye_` - `torch.nn.init.dirac_` - `torch.nn.init.kaiming_normal_` - `torch.nn.init.orthogonal_` - `torch.nn.init.sparse` 4. **Construct tensors** using functions that do not require `requires_grad` except for `torch.tensor` and ensure to always specify `dtype`, `layout`, and `device` where required. 5. **Convert your model to TorchScript** using `torch.jit.script`. Input: The neural network should take an input tensor of shape `(batch_size, input_size)`. Output: The output should be a TorchScript model. ```python import torch import torch.nn as nn import torch.jit # Define the neural network class class SimpleNet(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNet, self).__init__() self.hidden = nn.Linear(input_size, hidden_size) self.output = nn.Linear(hidden_size, output_size) def forward(self, x): x = torch.relu(self.hidden(x)) x = self.output(x) return x # Define the main function to create and convert the model def main(): input_size = 10 # Example input size hidden_size = 5 # Example hidden layer size output_size = 2 # Example output size # Construct the neural network model = SimpleNet(input_size, hidden_size, output_size) # Convert the model to TorchScript script_model = torch.jit.script(model) return script_model # Execute the main function if __name__ == \\"__main__\\": script_model = main() print(script_model) ``` Constraints: 1. Your model should be able to run without errors when converted to TorchScript. 2. Ensure that no unsupported constructs are used. Performance Requirements: The solution should be efficient and avoid unnecessary computations. It should handle the constraints and differences between Python and TorchScript as noted in the provided documentation.","solution":"import torch import torch.nn as nn import torch.jit # Define the neural network class class SimpleNet(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNet, self).__init__() self.hidden = nn.Linear(input_size, hidden_size) self.output = nn.Linear(hidden_size, output_size) def forward(self, x): x = torch.relu(self.hidden(x)) x = self.output(x) return x # Define the main function to create and convert the model def main(): input_size = 10 # Example input size hidden_size = 5 # Example hidden layer size output_size = 2 # Example output size # Construct the neural network model = SimpleNet(input_size, hidden_size, output_size) # Convert the model to TorchScript script_model = torch.jit.script(model) return script_model # Execute the main function if __name__ == \\"__main__\\": script_model = main() print(script_model)"},{"question":"# Multi-threaded Counter Application **Objective:** The purpose of this task is to create a multi-threaded counter application using the low-level `_thread` module. You will need to manage threads and coordinate access to a shared counter using locks to ensure that increments are performed correctly without race conditions. **Problem Statement:** Implement a function `threaded_counter(num_threads, num_increments)` that spawns multiple threads to increment a shared counter. The function should follow these requirements: 1. **Input:** - `num_threads`: An integer indicating the number of threads to be created. - `num_increments`: An integer indicating the number of increments each thread should perform on the shared counter. 2. **Output:** - The function should return the final value of the shared counter after all threads have completed their execution. 3. **Constraints:** - Each thread should increment the counter `num_increments` times. - You must use `_thread.allocate_lock()` to manage access to the shared counter. - The counter should start at 0. - Ensure that all threads have completed their execution before the function returns the final counter value. 4. **Threading and Synchronization Details:** - Use `_thread.start_new_thread()` to create and start new threads. - Use the lock\'s acquire and release methods to ensure that increments to the shared counter are atomic (i.e., no two threads should be able to increment the counter simultaneously without proper synchronization). **Function Signature:** ```python def threaded_counter(num_threads: int, num_increments: int) -> int: pass ``` # Example **Input:** ```python num_threads = 5 num_increments = 10 ``` **Output:** ```python 50 ``` **Explanation:** With 5 threads and each incrementing the counter 10 times, the total number of increments will be (5 times 10 = 50). Therefore, if properly synchronized, the final value of the counter should be 50. # Notes: - Ensure you handle any potential synchronization issues, such as race conditions, using locks. - Be mindful of thread management, ensuring all threads have completed their tasks before returning the final counter value. - Performance is not the primary concern, but proper thread synchronization is essential. # Hints: - Utilize `_thread.allocate_lock()` to create a lock object for synchronization. - Use `_thread.start_new_thread()` to start new threads and ensure the main thread waits for their completion.","solution":"import _thread import time def threaded_counter(num_threads: int, num_increments: int) -> int: # Shared counter counter = {\'value\': 0} # Create a lock object lock = _thread.allocate_lock() def increment_counter(): for _ in range(num_increments): with lock: counter[\'value\'] += 1 threads = [] for _ in range(num_threads): thread = _thread.start_new_thread(increment_counter, ()) threads.append(thread) # Allow some time for all threads to complete time.sleep(1) return counter[\'value\']"},{"question":"# Turtle Graphics Challenge In this challenge, you will create a simulation using the turtle graphics module. The objective is to simulate a simple drawing application that allows for the drawing of shapes based on user inputs. Objectives: 1. **Create a Function to Draw Shapes** - Implement a function `draw_shape(turtle_obj, shape, size)` that will draw different shapes based on the parameters: - `turtle_obj`: an instance of the turtle object. - `shape`: a string that specifies the shape to draw (either \'square\', \'triangle\', or \'circle\'). - `size`: an integer that specifies the size of the shape. 2. **Implement Turtle Motion Control** - Write functions to control the turtle\'s movement: - `move_up(turtle_obj)`: Moves the turtle up. - `move_down(turtle_obj)`: Moves the turtle down. - `move_left(turtle_obj)`: Moves the turtle left. - `move_right(turtle_obj)`: Moves the turtle right. 3. **Setup Event Listeners** - Set up the turtle screen to listen for key events to control its movement. Map the arrow keys to the movement functions defined above. 4. **Integration Function** - Implement a function `run_drawing_app()` that initializes the turtle screen, sets up the turtle, binds keys, and allows the user to draw shapes and move the turtle using keyboard inputs. 5. **Bonus Challenge: Color Control (Optional)** - Extend the `draw_shape` function to accept an additional parameter `color` to set the color of the shape being drawn. Requirements: 1. **input() and key bindings:** - Use `input()` or key bindings to capture user commands for choosing the shape to draw and its size. - Use arrow keys for turtle movement. 2. **Constraints:** - Only predefined shapes (\'square\', \'triangle\', \'circle\') should be drawn. - Input sizes should be positive integers. 3. **Performance:** - Ensure the program runs smoothly and handles user inputs in a responsive manner. 4. **Documentation and Code Quality:** - Provide clear and concise documentation for each function. - Ensure code readability and maintain consistent coding style. # Example: ```python def draw_shape(turtle_obj, shape, size): # Define the function to draw specified shapes. pass def move_up(turtle_obj): # Define the function to move turtle up. pass def move_down(turtle_obj): # Define the function to move turtle down. pass def move_left(turtle_obj): # Define the function to move turtle left. pass def move_right(turtle_obj): # Define the function to move turtle right. pass def run_drawing_app(): # Implement the main function to run the drawing application. pass # Run the application run_drawing_app() ``` Notes: - You may use the `turtle.done()` function to signal the end of the turtle graphics program. - Refer to the official [Python Turtle documentation](https://docs.python.org/3/library/turtle.html) for any required methods not covered in the initial outline.","solution":"import turtle def draw_shape(turtle_obj, shape, size, color=\'black\'): Draws a specified shape with the given size and color. Parameters: turtle_obj (Turtle): The turtle object. shape (str): The shape to draw (\'square\', \'triangle\', \'circle\'). size (int): The size of the shape. color (str): The color of the shape. turtle_obj.color(color) if shape == \'square\': for _ in range(4): turtle_obj.forward(size) turtle_obj.right(90) elif shape == \'triangle\': for _ in range(3): turtle_obj.forward(size) turtle_obj.right(120) elif shape == \'circle\': turtle_obj.circle(size) else: raise ValueError(\\"Invalid shape! Only \'square\', \'triangle\', and \'circle\' are allowed.\\") def move_up(turtle_obj): Moves the turtle up by a fixed amount. turtle_obj.setheading(90) turtle_obj.forward(20) def move_down(turtle_obj): Moves the turtle down by a fixed amount. turtle_obj.setheading(270) turtle_obj.forward(20) def move_left(turtle_obj): Moves the turtle left by a fixed amount. turtle_obj.setheading(180) turtle_obj.forward(20) def move_right(turtle_obj): Moves the turtle right by a fixed amount. turtle_obj.setheading(0) turtle_obj.forward(20) def run_drawing_app(): Initializes the turtle drawing application and sets up key bindings for movement and drawing shapes. screen = turtle.Screen() screen.setup(width=800, height=600) screen.title(\\"Turtle Drawing App\\") turtle_obj = turtle.Turtle() turtle_obj.speed(1) def draw_square(): draw_shape(turtle_obj, \'square\', 100) def draw_triangle(): draw_shape(turtle_obj, \'triangle\', 100) def draw_circle(): draw_shape(turtle_obj, \'circle\', 50) screen.listen() screen.onkeypress(lambda: move_up(turtle_obj), \\"Up\\") screen.onkeypress(lambda: move_down(turtle_obj), \\"Down\\") screen.onkeypress(lambda: move_left(turtle_obj), \\"Left\\") screen.onkeypress(lambda: move_right(turtle_obj), \\"Right\\") screen.onkey(draw_square, \\"s\\") screen.onkey(draw_triangle, \\"t\\") screen.onkey(draw_circle, \\"c\\") screen.mainloop() if __name__ == \\"__main__\\": run_drawing_app()"},{"question":"Objective Implement and use a multi-threaded task manager using the `queue` module to simulate a job processing system. The system should accept tasks from multiple producer threads and process them using multiple consumer threads. This exercise aims to test your understanding of thread synchronization and the `queue` module\'s capabilities in Python. Task Write a function `process_jobs` that takes the following parameters: - `num_producers`: The number of producer threads. - `num_consumers`: The number of consumer threads. - `tasks`: A list of tasks to be processed. Each task is represented by a tuple `(task_id, priority)`. The `priority` is an integer where a lower value indicates higher priority. The function should: 1. Use a `PriorityQueue` to manage the tasks. 2. Each producer thread should add tasks to the queue. Ensure that producers do not exceed the queue size, blocking if necessary until space is available. 3. Each consumer thread should retrieve tasks from the queue and simulate processing by printing the task\'s ID and priority. 4. Use `queue.task_done()` to signal the completion of a task and `queue.join()` to wait until all tasks are processed. **Implementation Requirements:** - Implement proper thread synchronization to avoid race conditions. - Handle exceptions appropriately to ensure robustness. - Ensure the function can handle a large number of tasks efficiently. Example Usage ```python import threading import queue import time def process_jobs(num_producers, num_consumers, tasks): # TODO: Implement this function based on the above requirements tasks = [(i, i % 10) for i in range(30)] process_jobs(num_producers=2, num_consumers=3, tasks=tasks) ``` **Expected Output:** The output should demonstrate that all tasks are added to the queue by the producer threads and processed by the consumer threads. The printed statements may vary in order due to multi-threading, but all tasks should be processed and printed. ```plaintext Processing task 0 with priority 0 Processing task 10 with priority 0 Processing task 20 with priority 0 Processing task 1 with priority 1 ... ``` **Constraints:** - Ensure the solution is efficient and can handle large input sizes within a reasonable time. - Handle threading synchronizations correctly to prevent deadlocks or race conditions. **Note:** You can use the example provided in the documentation (`queue.py`) as a reference for implementing the consumers and producers.","solution":"import threading import queue import time def process_jobs(num_producers, num_consumers, tasks): task_queue = queue.PriorityQueue() def producer(producer_id): for task in tasks[producer_id::num_producers]: task_queue.put(task) print(f\\"Producer {producer_id} added task {task[0]} with priority {task[1]}\\") print(f\\"Producer {producer_id} finished adding tasks.\\") def consumer(consumer_id): while True: try: task = task_queue.get(timeout=1) print(f\\"Consumer {consumer_id} processing task {task[0]} with priority {task[1]}\\") time.sleep(0.1) # Simulate task processing time task_queue.task_done() except queue.Empty: return producer_threads = [] for i in range(num_producers): thread = threading.Thread(target=producer, args=(i,)) thread.start() producer_threads.append(thread) consumer_threads = [] for i in range(num_consumers): thread = threading.Thread(target=consumer, args=(i,)) thread.start() consumer_threads.append(thread) for thread in producer_threads: thread.join() task_queue.join() for thread in consumer_threads: thread.join()"},{"question":"You are tasked with designing a script that manipulates WAV files using the `wave` module in Python. Specifically, you need to write a function that takes an input WAV file, adjusts its volume, and writes the adjusted audio to a new output WAV file. # Function Signature ```python def adjust_wav_volume(input_file: str, output_file: str, volume_factor: float) -> None: pass ``` # Parameters - `input_file` (str): The path to the input WAV file. - `output_file` (str): The path where the output WAV file with adjusted volume should be saved. - `volume_factor` (float): The factor by which to adjust the volume. A value greater than 1 increases the volume, while a value less than 1 decreases it. # Requirements 1. Read the audio data from the input WAV file. 2. Adjust the volume of the audio data by multiplying the amplitude of each sample by the `volume_factor`. 3. Write the adjusted audio data to the output WAV file, preserving the input file\'s properties such as the number of channels, sample width, and frame rate. # Constraints - You must handle only files in the WAVE_FORMAT_PCM format. - Ensure that the data written to the output WAV file is correctly formatted and playable. - You should not use any external libraries for audio processing; only use the `wave` module and standard Python libraries. # Example ```python adjust_wav_volume(\\"input.wav\\", \\"output.wav\\", 0.5) ``` This would take the file \\"input.wav\\", reduce the volume by 50%, and save the result to \\"output.wav\\". # Notes - Remember to handle edge cases such as an empty input file. - Ensure that the function is efficient and does not consume excessive memory, especially when processing large WAV files. Good luck, and remember to test your function thoroughly!","solution":"import wave import struct def adjust_wav_volume(input_file: str, output_file: str, volume_factor: float) -> None: with wave.open(input_file, \'rb\') as infile: params = infile.getparams() n_channels, sampwidth, framerate, n_frames, comp_type, comp_name = params # Read frames and convert to an array of integers frames = infile.readframes(n_frames) fmt = f\\"{\'h\' if sampwidth == 2 else \'B\'}\\" * (n_channels * n_frames) samples = struct.unpack(fmt, frames) # Adjust volume by multiplying each sample by the volume factor adjusted_samples = [max(min(int(sample * volume_factor), 32767), -32768) if sampwidth == 2 else max(min(int(sample * volume_factor), 255), 0) for sample in samples] # Pack the adjusted samples back into bytes adjusted_frames = struct.pack(fmt, *adjusted_samples) with wave.open(output_file, \'wb\') as outfile: outfile.setparams(params) outfile.writeframes(adjusted_frames)"},{"question":"# Neural Network Module with Custom Layer in TorchScript You are tasked with designing a simple neural network module in TorchScript that integrates a custom linear layer. The goal is to assess your understanding of TorchScript\'s type system, class definitions, and annotations. Follow the requirements and specifications below to implement the solution. Requirements: 1. **Custom Linear Layer**: - Define a custom linear layer class `CustomLinear` which inherits from `torch.nn.Module`. - The class should have an `__init__` method that takes two arguments: `in_features` (integer) and `out_features` (integer). - Initialize two attributes: `weight` and `bias`, both of which should be of type `torch.nn.Parameter` with appropriate sizes. - Implement the `forward` method that takes an input tensor and performs a linear transformation `(input * weight^T + bias)`. 2. **Neural Network Module**: - Define a neural network module class `SimpleNN` which also inherits from `torch.nn.Module`. - The class should have an `__init__` method that initializes three layers: - A `CustomLinear` layer from input size to a hidden size. - A ReLU activation layer. - Another `CustomLinear` layer from hidden size to output size. - Implement the `forward` method that takes an input tensor and passes it through the three layers sequentially. 3. **Type Annotations**: - Ensure that all methods in both classes have appropriate type annotations. - Use TorchScript-specific annotations where necessary, such as `torch.jit.annotate` for complex expressions. 4. **Scripting the Module**: - Script the final neural network module using `torch.jit.script`. Constraints: - The input tensor dimensions should be consistent with the defined layer dimensions. - Use appropriate type annotations to ensure compatibility with TorchScript. - Ensure that all tensors are on the same device (CPU for this task). Example Usage: ```python import torch # Define and script the neural network module input_size = 10 hidden_size = 20 output_size = 5 model = SimpleNN(input_size, hidden_size, output_size) scripted_model = torch.jit.script(model) # Test the scripted model example_input = torch.randn(1, input_size) output = scripted_model(example_input) print(output) ``` Implementation: ```python import torch import torch.nn as nn from typing import Tuple class CustomLinear(nn.Module): def __init__(self, in_features: int, out_features: int): super(CustomLinear, self).__init__() self.weight = nn.Parameter(torch.randn(out_features, in_features)) self.bias = nn.Parameter(torch.randn(out_features)) def forward(self, input: torch.Tensor) -> torch.Tensor: return torch.matmul(input, self.weight.t()) + self.bias class SimpleNN(nn.Module): def __init__(self, input_size: int, hidden_size: int, output_size: int): super(SimpleNN, self).__init__() self.layer1 = CustomLinear(input_size, hidden_size) self.relu = nn.ReLU() self.layer2 = CustomLinear(hidden_size, output_size) def forward(self, input: torch.Tensor) -> torch.Tensor: x = self.layer1(input) x = self.relu(x) x = self.layer2(x) return x # Example usage and scripting input_size = 10 hidden_size = 20 output_size = 5 model = SimpleNN(input_size, hidden_size, output_size) scripted_model = torch.jit.script(model) # Test the scripted model example_input = torch.randn(1, input_size) output = scripted_model(example_input) print(output) ``` Implement the required classes and methods as specified and ensure the code runs successfully. Submit your implementation and the printed output.","solution":"import torch import torch.nn as nn from typing import Tuple class CustomLinear(nn.Module): def __init__(self, in_features: int, out_features: int): super(CustomLinear, self).__init__() self.weight = nn.Parameter(torch.randn(out_features, in_features)) self.bias = nn.Parameter(torch.randn(out_features)) def forward(self, input: torch.Tensor) -> torch.Tensor: return torch.matmul(input, self.weight.t()) + self.bias class SimpleNN(nn.Module): def __init__(self, input_size: int, hidden_size: int, output_size: int): super(SimpleNN, self).__init__() self.layer1 = CustomLinear(input_size, hidden_size) self.relu = nn.ReLU() self.layer2 = CustomLinear(hidden_size, output_size) def forward(self, input: torch.Tensor) -> torch.Tensor: x = self.layer1(input) x = self.relu(x) x = self.layer2(x) return x # Example usage and scripting input_size = 10 hidden_size = 20 output_size = 5 model = SimpleNN(input_size, hidden_size, output_size) scripted_model = torch.jit.script(model) # Test the scripted model example_input = torch.randn(1, input_size) output = scripted_model(example_input) print(output)"},{"question":"<|Analysis Begin|> The given documentation provides detailed information on the `zipfile` module for Python, which is used to create, read, write, append, and list ZIP files. It describes various classes and methods provided by the module such as `ZipFile`, `PyZipFile`, `ZipInfo`, and their associated methods and attributes. The module supports different compression methods (ZIP_STORED, ZIP_DEFLATED, ZIP_BZIP2, ZIP_LZMA) and includes a command-line interface for basic operations like creating, extracting, and listing ZIP files. Key points from the documentation: - `zipfile.ZipFile` class is the primary interface for working with ZIP files. - Supports various compression methods and can handle files larger than 4 GiB using ZIP64 extensions. - Allows reading and writing files using context managers and supports text and binary modes. - Methods like `write`, `writestr`, `read`, `extract`, and `extractall` are available for adding, reading, and extracting files within the ZIP archive. - Includes exception classes (e.g., `BadZipFile`, `LargeZipFile`) for handling errors. - Contains command-line options for creating, extracting, listing, and testing ZIP files. <|Analysis End|> <|Question Begin|> # Advanced Coding Assessment Question **Objective:** Demonstrate your understanding of the `zipfile` module in Python by implementing a function that performs multiple operations on ZIP files. This task will test your ability to read, write, and manipulate ZIP archives using the `zipfile` module. **Problem Statement:** You need to implement a function `process_zip_file` that takes the path to an existing ZIP file, performs the following operations, and then saves the modified ZIP file to a new location: 1. Extract all files from the provided ZIP archive to a temporary directory. 2. Create a new text file (e.g., `additional_info.txt`) in the temporary directory with some specified content. 3. Add this new text file to the extracted contents. 4. Compress all files from the temporary directory into a new ZIP file with a specified compression method and save it to a given output path. 5. Return the list of filenames within the newly created ZIP file. **Function Signature:** ```python def process_zip_file(input_zip_path: str, output_zip_path: str, compression_method: int, additional_info: str) -> list: pass ``` **Input:** - `input_zip_path`: A string representing the path to the input ZIP file. - `output_zip_path`: A string representing the path to the output ZIP file. - `compression_method`: An integer representing the compression method to use (one of `zipfile.ZIP_STORED`, `zipfile.ZIP_DEFLATED`, `zipfile.ZIP_BZIP2`, `zipfile.ZIP_LZMA`). - `additional_info`: A string containing the content to be written to the new text file (`additional_info.txt`). **Output:** - Returns a list of filenames in the newly created ZIP file. **Constraints:** - The function should handle potential errors gracefully (e.g., invalid ZIP file, unsupported compression method). - Do not use any external libraries except the standard Python library. - Assume the ZIP archive contains less than 100 files. - The new text file should be named `additional_info.txt`. **Example:** ```python input_zip_path = \\"example.zip\\" output_zip_path = \\"new_example.zip\\" compression_method = zipfile.ZIP_DEFLATED additional_info = \\"This is some additional information to be included in the ZIP file.\\" result = process_zip_file(input_zip_path, output_zip_path, compression_method, additional_info) print(result) # Expected output: List of filenames in the \\"new_example.zip\\" file ``` **Notes:** 1. Ensure that the temporary directory used for extraction is properly cleaned up after creating the new ZIP file. 2. You should implement proper error handling to manage exceptions such as invalid ZIP files and unsupported compression methods. 3. Make use of context managers (`with` statement) effectively to handle file operations and ensure proper closure.","solution":"import zipfile import os import tempfile import shutil def process_zip_file(input_zip_path: str, output_zip_path: str, compression_method: int, additional_info: str) -> list: Processes a ZIP file by extracting its contents, adding a new text file, and compressing everything back into a new ZIP file with the specified compression method. Parameters: input_zip_path (str): The path to the input ZIP file. output_zip_path (str): The path to the output ZIP file. compression_method (int): The compression method to use (ZIP_STORED, ZIP_DEFLATED, ZIP_BZIP2, ZIP_LZMA). additional_info (str): The content to be written to the new text file (additional_info.txt). Returns: list: A list of filenames within the newly created ZIP file. try: # Ensure the provided compression method is valid if compression_method not in [zipfile.ZIP_STORED, zipfile.ZIP_DEFLATED, zipfile.ZIP_BZIP2, zipfile.ZIP_LZMA]: raise ValueError(\\"Unsupported compression method.\\") with tempfile.TemporaryDirectory() as temp_dir: # Extract all files from the input ZIP file to the temporary directory with zipfile.ZipFile(input_zip_path, \'r\') as zip_ref: zip_ref.extractall(temp_dir) # Add the new text file with the specified content additional_file_path = os.path.join(temp_dir, \'additional_info.txt\') with open(additional_file_path, \'w\') as file: file.write(additional_info) # Create a new ZIP file with the specified compression method with zipfile.ZipFile(output_zip_path, \'w\', compression=compression_method) as zip_ref: for root, dirs, files in os.walk(temp_dir): for file in files: file_path = os.path.join(root, file) zip_ref.write(file_path, os.path.relpath(file_path, temp_dir)) # Gather the list of filenames in the new ZIP file with zipfile.ZipFile(output_zip_path, \'r\') as zip_ref: result_filenames = zip_ref.namelist() return result_filenames except zipfile.BadZipFile: raise zipfile.BadZipFile(\\"The provided ZIP file is invalid.\\") except Exception as e: raise e"},{"question":"Semi-Supervised Learning with Scikit-learn Objective: Write a Python script that applies semi-supervised learning to a provided dataset using both `SelfTrainingClassifier` and `LabelPropagation`. You will be required to compare their performance based on classification accuracy and runtime. Instructions: 1. **Dataset**: - You will use the digits dataset from `sklearn.datasets`. - This dataset contains 1797 samples of handwritten digits with 64 features each, with labels in the range 0-9. 2. **Implementation Details**: - **Data Preparation**: - Use only 1% of the data as labeled, and the remaining 99% as unlabeled. - Use the provided `train_test_split` function to create the labeled and unlabeled data split. - **Self-training with RandomForestClassifier**: - Use `SelfTrainingClassifier` with `RandomForestClassifier` as the estimator. - Set the selection criterion as a threshold of 0.75 for the prediction probabilities. - **Label Propagation**: - Use `LabelPropagation` with `rbf` kernel. - **Model Evaluation**: - Evaluate and compare both models based on classification accuracy on a separate test set. - Measure and compare the runtime for each model. 3. **Output**: - Print the classification accuracy and runtime for both models. - Provide a brief explanation comparing their performance. Constraints: - Use only 1% of the data for initial labels. - Use the provided skeleton code as a guideline. Skeleton Code: ```python import numpy as np from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation from sklearn.metrics import accuracy_score import time # Load the digits dataset digits = load_digits() X = digits.data y = digits.target # Use only 1% of labels n_labeled_points = int(len(y) * 0.01) # Create unlabeled data rng = np.random.RandomState(42) indices = np.arange(len(y)) rng.shuffle(indices) X_labeled, y_labeled = X[indices[:n_labeled_points]], y[indices[:n_labeled_points]] X_unlabeled, y_unlabeled = X[indices[n_labeled_points:]], -1 * np.ones(len(y) - n_labeled_points) # Combine the labeled and unlabeled data X_combined = np.concatenate([X_labeled, X_unlabeled]) y_combined = np.concatenate([y_labeled, y_unlabeled]) # Split the combined data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.5, random_state=42) # Self-training model start_time = time.time() rf = RandomForestClassifier(random_state=42) self_training_model = SelfTrainingClassifier(rf, criterion=\\"threshold\\", threshold=0.75) self_training_model.fit(X_train, y_train) y_pred_self_training = self_training_model.predict(X_test) accuracy_self_training = accuracy_score(y_test, y_pred_self_training) self_training_time = time.time() - start_time # Label Propagation model start_time = time.time() label_prop_model = LabelPropagation(kernel=\'rbf\', gamma=20) label_prop_model.fit(X_train, y_train) y_pred_label_prop = label_prop_model.predict(X_test) accuracy_label_prop = accuracy_score(y_test, y_pred_label_prop) label_prop_time = time.time() - start_time # Print results print(f\'Self-Training Classifier Accuracy: {accuracy_self_training}\') print(f\'Self-Training Classifier Runtime: {self_training_time} seconds\') print(f\'Label Propagation Accuracy: {accuracy_label_prop}\') print(f\'Label Propagation Runtime: {label_prop_time} seconds\') # Brief explanation # [Your analysis here] ``` Expected Output: - Accuracy and runtime for `SelfTrainingClassifier`. - Accuracy and runtime for `LabelPropagation`. - Comparative analysis of both models.","solution":"import numpy as np from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation from sklearn.metrics import accuracy_score import time def semi_supervised_learning_comparison(): # Load the digits dataset digits = load_digits() X = digits.data y = digits.target # Use only 1% of labels n_labeled_points = int(len(y) * 0.01) # Create unlabeled data rng = np.random.RandomState(42) indices = np.arange(len(y)) rng.shuffle(indices) X_labeled, y_labeled = X[indices[:n_labeled_points]], y[indices[:n_labeled_points]] X_unlabeled, y_unlabeled = X[indices[n_labeled_points:]], -1 * np.ones(len(y) - n_labeled_points) # Combine the labeled and unlabeled data X_combined = np.concatenate([X_labeled, X_unlabeled]) y_combined = np.concatenate([y_labeled, y_unlabeled]) # Split the combined data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.5, random_state=42) # Self-training model start_time = time.time() rf = RandomForestClassifier(random_state=42) self_training_model = SelfTrainingClassifier(rf, criterion=\\"threshold\\", threshold=0.75) self_training_model.fit(X_train, y_train) y_pred_self_training = self_training_model.predict(X_test) accuracy_self_training = accuracy_score(y_test, y_pred_self_training) self_training_time = time.time() - start_time # Label Propagation model start_time = time.time() label_prop_model = LabelPropagation(kernel=\'rbf\', gamma=20) label_prop_model.fit(X_train, y_train) y_pred_label_prop = label_prop_model.predict(X_test) accuracy_label_prop = accuracy_score(y_test, y_pred_label_prop) label_prop_time = time.time() - start_time results = { \'SelfTrainingClassifier\': { \'Accuracy\': accuracy_self_training, \'Runtime\': self_training_time }, \'LabelPropagation\': { \'Accuracy\': accuracy_label_prop, \'Runtime\': label_prop_time } } return results # Print results results = semi_supervised_learning_comparison() print(f\'Self-Training Classifier Accuracy: {results[\\"SelfTrainingClassifier\\"][\\"Accuracy\\"]}\') print(f\'Self-Training Classifier Runtime: {results[\\"SelfTrainingClassifier\\"][\\"Runtime\\"]} seconds\') print(f\'Label Propagation Accuracy: {results[\\"LabelPropagation\\"][\\"Accuracy\\"]}\') print(f\'Label Propagation Runtime: {results[\\"LabelPropagation\\"][\\"Runtime\\"]} seconds\') # Brief explanation # The results indicate that the Self-Training Classifier and Label Propagation provide a way to leverage a large amount of # unlabeled data for training a model, with the Self-Training Classifier generally having a higher accuracy and faster runtime # in this instance. However, the performance can be very dataset-specific and may vary with different parameters."},{"question":"# Question: Implementing a Custom Autograd Function in PyTorch **Objective**: Your task is to implement a custom PyTorch autograd function that computes the function (f(x) = x^3 sin(x)) and its gradient correctly using PyTorch\'s autograd system. **Instructions**: 1. Define a class `CustomFunction` that inherits from `torch.autograd.Function`. 2. Implement the `forward` and `backward` static methods for the class. 3. Use the `save_for_backward` method to store any tensors needed for gradient computation. 4. In the `backward` function, carefully implement the gradient computation for the given function. 5. Test your custom function by using it in a simple computation and verifying that the gradients are computed correctly. **Constraints**: - Do not use in-place operations within the custom autograd function. - Ensure that the gradients are correctly computed for a batch of inputs (i.e., the function should support batched inputs). **Input Format**: - A 1D tensor `x` of any length with `requires_grad=True`. **Output Format**: - A tensor of the same shape as `x` representing the output of the function (f(x) = x^3 sin(x)). **Example**: ```python import torch class CustomFunction(torch.autograd.Function): @staticmethod def forward(ctx, input): # Compute the forward pass and save necessary tensors for backward pass @staticmethod def backward(ctx, grad_output): # Compute the gradient of the function pass # Example usage x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) custom_func = CustomFunction.apply y = custom_func(x) y.sum().backward() print(x.grad) ``` **Expected Output**: The computed gradients for the input tensor `x`. **Hint**: - Use the saved tensors in the `ctx` object to retrieve values needed for gradient computation during the backward pass. - The derivative of (f(x) = x^3 sin(x)) can be obtained using the product and chain rules for differentiation. # Testing: 1. Verify the implementation by comparing the gradients computed using your custom function with those computed directly by PyTorch\'s autograd system for the function (f(x) = x^3 sin(x)). 2. Test with different inputs to ensure gradients are computed correctly.","solution":"import torch class CustomFunction(torch.autograd.Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) return input**3 * torch.sin(input) @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = grad_output * (3*input**2 * torch.sin(input) + input**3 * torch.cos(input)) return grad_input # Example usage x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) custom_func = CustomFunction.apply y = custom_func(x) y.sum().backward() print(x.grad)"},{"question":"**Objective:** Design a Python function that utilizes the `tarfile` module to create a tar archive from a given list of files and directories, with specific compression, and then extract the contents securely using a custom filter. **Problem Statement:** You are required to write a function `create_and_extract_tar` that performs the following operations: 1. **Create a tar archive**: - Takes a list of file and directory paths. - Creates a tar archive with the given name. - Optionally applies gzip, bz2, or lzma compression based on a provided parameter. 2. **Securely extract the tar archive**: - Extracts the contents of the created tar archive. - Applies a custom extraction filter to handle potential security issues (e.g., avoiding extraction of files with absolute paths or path traversal attempts). **Function Signature**: ```python def create_and_extract_tar(paths: List[str], archive_name: str, compression: Optional[str] = None) -> None: pass ``` **Input/Output:** - `paths` (List[str]): A list of file and directory paths to be added to the tar archive. - `archive_name` (str): The name of the tar archive to be created (e.g., \\"archive.tar\\"). - `compression` (Optional[str]): Compression method to be used - `\\"gz\\"`, `\\"bz2\\"`, or `\\"xz\\"`. Default is `None` (no compression). - Returns: None (The function should print the contents of the extracted archive). **Constraints:** - The function should handle paths that do not exist by raising a `FileNotFoundError`. - The tar archive should be created in the current working directory. - The function should use the most secure extraction filter (`\\"data\\"`) provided by the `tarfile` module. **Example:** ```python create_and_extract_tar([\\"file1.txt\\", \\"dir1\\"], \\"archive.tar\\", \\"gz\\") ``` **Notes:** - Make sure to handle potential exceptions, such as file not found or unsupported compression methods. - Utilize context managers to ensure proper handling of file objects. - Print the names of the extracted files after extraction to verify the correct functioning of the code.","solution":"import tarfile import os from typing import List, Optional def create_and_extract_tar(paths: List[str], archive_name: str, compression: Optional[str] = None) -> None: if compression not in [None, \\"gz\\", \\"bz2\\", \\"xz\\"]: raise ValueError(\\"Unsupported compression method. Use \'gz\', \'bz2\', or \'xz\' or None.\\") mode = \'w\' if compression == \'gz\': mode += \':gz\' elif compression == \'bz2\': mode += \':bz2\' elif compression == \'xz\': mode += \':xz\' with tarfile.open(archive_name, mode) as tar: for path in paths: if not os.path.exists(path): raise FileNotFoundError(f\\"Path {path} does not exist.\\") tar.add(path, arcname=os.path.basename(path)) with tarfile.open(archive_name, \'r:*\') as tar: def is_within_directory(directory, target): abs_directory = os.path.abspath(directory) abs_target = os.path.abspath(target) return os.path.commonprefix([abs_directory, abs_target]) == abs_directory def safe_extract(tar, path=\\".\\", members=None, *, numeric_owner=False): for member in tar.getmembers(): member_path = os.path.join(path, member.name) if not is_within_directory(path, member_path): raise Exception(\\"Attempted Path Traversal in Tar File\\") tar.extractall(path, members, numeric_owner=numeric_owner) safe_extract(tar) with tarfile.open(archive_name, \'r:*\') as tar: tar.list()"},{"question":"**Implementing and Evaluating Nearest Neighbors Search with Sparse Graphs** # Objective Your task is to implement a function that uses the `sklearn.neighbors` module to find the nearest neighbors of a given dataset, then constructs a sparse graph of these neighbors, and evaluates the performance of this setup on a classification task. # Instructions 1. Implement a function `perform_knn_classification` that: - Accepts the following parameters: - `X_train`: A 2D numpy array of training data samples. - `y_train`: A 1D numpy array of target labels for the training data. - `X_test`: A 2D numpy array of test data samples. - `y_test`: A 1D numpy array of target labels for the test data. - `n_neighbors`: Number of neighbors to use for k-nearest neighbor search (default is 5). - `algorithm`: Algorithm to compute the neighbors (default is `\'auto\'`). - Returns the accuracy of the K-Nearest Neighbors classifier on the test data. 2. Your implementation should: - Use `KNeighborsClassifier` to fit the training data. - Find the nearest neighbors for the test data and compute the corresponding sparse graph using `kneighbors_graph`. - Evaluate the classifier\'s accuracy on the test data. # Constraints - Ensure the function is efficient for large datasets (consider computational complexity when choosing algorithms). - Utilize relevant methods from `sklearn.neighbors` detailed in the provided documentation. # Example Usage ```python import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split # Load dataset iris = load_iris() X, y = iris.data, iris.target # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Call the function accuracy = perform_knn_classification(X_train, y_train, X_test, y_test, n_neighbors=3, algorithm=\'ball_tree\') print(f\'Classification accuracy: {accuracy}\') ``` # Bonus (Optional) - Extend the function to support two distance metrics (`euclidean` and `manhattan`) and compare the performance. - Visualize the sparse graph generated for the nearest neighbors.","solution":"import numpy as np from sklearn.neighbors import KNeighborsClassifier, kneighbors_graph from sklearn.metrics import accuracy_score def perform_knn_classification(X_train, y_train, X_test, y_test, n_neighbors=5, algorithm=\'auto\'): Perform KNN classification and return the accuracy of the classifier on the test data. Parameters: X_train: np.array 2D numpy array of training data samples. y_train: np.array 1D numpy array of target labels for the training data. X_test: np.array 2D numpy array of test data samples. y_test: np.array 1D numpy array of target labels for the test data. n_neighbors: int, optional Number of neighbors to use for k-nearest neighbor search (default is 5). algorithm: str, optional Algorithm to compute the neighbors (default is \'auto\'). Returns: float Accuracy of the K-Nearest Neighbors classifier on the test data. # Initialize the KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=algorithm) # Fit the classifier on the training data knn.fit(X_train, y_train) # Predict the labels for the test data y_pred = knn.predict(X_test) # Calculate the accuracy of the classifier accuracy = accuracy_score(y_test, y_pred) # Generate the sparse graph for K-nearest neighbors kneighbors_graph(X_test, n_neighbors=n_neighbors, mode=\'distance\', include_self=False) return accuracy"},{"question":"# Advanced Python File Control Operation For this assessment, you are required to implement a Python function using the `fcntl` module to perform a series of advanced file control operations. Specifically, you will: 1. Open or create a file. 2. Use `fcntl` to set the file descriptor to non-blocking mode. 3. Acquire an exclusive lock on the whole file. 4. Write data to the file. 5. Release the lock and close the file. Your task is to ensure correct file operations following these steps in the constrained environment. Any failures in obtaining locks or setting file modes should be properly handled. # Specifications Function Signature ```python def controlled_file_operation(file_path: str, data: bytes) -> None: ``` Parameters - `file_path`: A string representing the path to the file you need to open/create. - `data`: A byte string representing the data to be written into the file. Constraints - Use only the `fcntl` and `os` modules for file operations. - If the lock cannot be obtained immediately, raise an `OSError`. - The file should be opened in a manner that forbids parallel write operations by default. Example Usage ```python try: controlled_file_operation(\\"/tmp/testfile\\", b\\"Hello, fcntl!\\") except OSError as e: print(f\\"Failed to perform file operations: {e}\\") ``` # Requirements 1. Open the file with read and write permissions and create it if it does not exist. 2. Set the file to non-blocking mode. 3. Acquire an exclusive lock on the entire file. 4. Write the provided `data` to the file. 5. Release the lock and close the file. 6. Properly handle and raise exceptions as necessary. Your solution will be evaluated on correctness, proper usage of the `fcntl` module, adherence to the requirements, and exception handling.","solution":"import os import fcntl def controlled_file_operation(file_path: str, data: bytes) -> None: Perform controlled file operations including non-blocking mode setting, acquiring an exclusive lock, writing data, and releasing the lock. :param file_path: Path to the file to be operated on. :param data: Data to be written to the file. :raises: OSError if any file operation fails. try: # Open or create the file in read and write mode fd = os.open(file_path, os.O_RDWR | os.O_CREAT) # Set the file descriptor to non-blocking mode flags = fcntl.fcntl(fd, fcntl.F_GETFL) fcntl.fcntl(fd, fcntl.F_SETFL, flags | os.O_NONBLOCK) # Acquire an exclusive lock on the entire file fcntl.flock(fd, fcntl.LOCK_EX | fcntl.LOCK_NB) # Write the data to the file os.write(fd, data) # Release the lock fcntl.flock(fd, fcntl.LOCK_UN) except OSError as e: raise OSError(f\\"File operation failed: {e}\\") finally: # Ensure the file descriptor is closed if \'fd\' in locals(): os.close(fd)"},{"question":"Problem Statement You are given a task to process a list of log entries from a server. Each log entry contains a timestamp, log level, and a message. The format of each log entry is as follows: `[timestamp] [log_level] message` Example: ``` [2023-10-01 12:00:00] [INFO] Application started. [2023-10-01 12:01:00] [ERROR] Out of memory. ``` Your task is to implement a function `process_logs` that takes a list of log entries and returns: 1. A list of all unique log levels found in the log entries. 2. A dictionary where the keys are log levels and the values are lists of messages corresponding to each log level. # Function Signature ```python def process_logs(log_entries: List[str]) -> Tuple[List[str], Dict[str, List[str]]]: ``` # Input - `log_entries` (List[str]): A list of log entries, with each log entry being a string in the specified format. The total number of log entries will not exceed 10^4. # Output - Returns a tuple containing: - A list of unique log levels sorted alphabetically. - A dictionary with log levels as keys and lists of messages as values. # Constraints - Timestamp will be in the format `[YYYY-MM-DD HH:MM:SS]`. - Log levels will always be enclosed in square brackets and will consist of uppercase letters. - The log message can be any string. # Example ```python log_entries = [ \\"[2023-10-01 12:00:00] [INFO] Application started.\\", \\"[2023-10-01 12:01:00] [ERROR] Out of memory.\\", \\"[2023-10-01 12:02:00] [INFO] User logged in.\\", ] unique_log_levels, log_dict = process_logs(log_entries) # Expected Output: # unique_log_levels: [\'ERROR\', \'INFO\'] # log_dict: { # \'INFO\': [\'Application started.\', \'User logged in.\'], # \'ERROR\': [\'Out of memory.\'] # } ``` # Note - Use the `re` module to parse the log entries and extract the necessary components. - Ensure your solution handles large inputs efficiently. # Hint - Use regex capturing groups to extract `log_level` and `message` from each log entry.","solution":"import re from typing import List, Tuple, Dict def process_logs(log_entries: List[str]) -> Tuple[List[str], Dict[str, List[str]]]: Parses a list of log entries and returns unique log levels and a dictionary of log levels mapped to corresponding messages. log_level_pattern = re.compile(r\'[(w+)] (.+)\') log_levels = set() log_dict = {} for entry in log_entries: match = log_level_pattern.search(entry) if match: log_level = match.group(1) message = match.group(2) log_levels.add(log_level) if log_level not in log_dict: log_dict[log_level] = [] log_dict[log_level].append(message) unique_log_levels = sorted(log_levels) return unique_log_levels, log_dict"},{"question":"# Advanced Python Coding Assessment Question: **Objective:** Demonstrate understanding of Python’s filesystem and I/O handling, akin to the functionality provided in the C API. **Scenario:** You are tasked with developing a Python function that interacts with the filesystem and standard input/output streams to perform specific operations. The function must provide a detailed report of all text files in a directory, including reading their contents and printing information to `stdout` or `stderr` depending on the file size. **Function Signature:** ```python def analyze_text_files(directory: str): Analyze all text files in a given directory and provide a detailed report. Parameters: - directory (str): The directory path in which to search for text files. The function should: - Find all files with a `.txt` extension in the given directory. - For each text file found: - Check if its size is greater than 1000 bytes. - If the file size is less than or equal to 1000 bytes, read its content and print it to `stdout`. - If the file size is greater than 1000 bytes, print the filename and size to `stderr`. - Handle any file reading errors gracefully by printing an appropriate error message to `stderr`. ``` **Expected Input:** - A string representing the absolute or relative path to a directory. **Expected Output:** - The function prints output directly to `stdout` and `stderr` as per the requirements. **Constraints:** - You must use Python’s built-in libraries for file handling and I/O operations. - Assume the directory exists and contains readable files. - Ensure the function handles any unexpected errors, such as file read permission issues. **Performance Requirements:** - The function should efficiently handle directories containing up to 1000 files. - File reading should minimize memory usage and handle large files appropriately. **Example Usage:** ```python # Assuming you have a directory \'sample_dir\' with text files analyze_text_files(\'sample_dir\') # Possible outputs (to stdout and stderr) assuming files file1.txt, file2.txt, and largefile.txt are present: # stdout: # file1.txt content... # file2.txt content... # # stderr: # largefile.txt size is 2048 bytes. ``` Craft your function considering robust error handling and efficient file processing techniques.","solution":"import os import sys def analyze_text_files(directory: str): Analyze all text files in a given directory and provide a detailed report. Parameters: - directory (str): The directory path in which to search for text files. The function will: - Find all files with a `.txt` extension in the given directory. - For each text file found: - Check if its size is greater than 1000 bytes. - If the file size is less than or equal to 1000 bytes, read its content and print it to `stdout`. - If the file size is greater than 1000 bytes, print the filename and size to `stderr`. - Handle any file reading errors gracefully by printing an appropriate error message to `stderr`. try: for filename in os.listdir(directory): if filename.endswith(\'.txt\'): file_path = os.path.join(directory, filename) try: file_size = os.path.getsize(file_path) if file_size <= 1000: with open(file_path, \'r\') as file: content = file.read() sys.stdout.write(content + \'n\') else: sys.stderr.write(f\'{filename} size is {file_size} bytes.n\') except OSError as e: sys.stderr.write(f\'Error reading {filename}: {e}n\') except OSError as e: sys.stderr.write(f\'Error accessing directory {directory}: {e}n\')"},{"question":"Objective: Implement a Python function that reads a WAV file, processes its audio data by doubling its speed, and then writes the processed audio data to a new WAV file. This will test your understanding of manipulating WAV file properties and audio frame data using the `wave` module. Function Signature: ```python def double_speed(input_wav: str, output_wav: str) -> None: pass ``` Parameters: - `input_wav` (str): The file path of the input WAV file that needs to be processed. - `output_wav` (str): The file path of the output WAV file where the processed audio data will be saved. Requirements: 1. **Reading the Input WAV File**: - Open the input WAV file in read mode using the `wave` module. - Retrieve the file parameters such as number of channels, sample width, frame rate, and number of frames. 2. **Processing the Audio Data**: - Read the audio frames from the input file. - Double the speed of the audio by skipping every other frame. 3. **Writing to the Output WAV File**: - Open the output WAV file in write mode using the `wave` module. - Set the necessary parameters (channels, sample width, frame rate) on the output file. The frame rate should be doubled. - Write the processed frames to the output file. - Ensure the output file is correctly closed after writing. Constraints: - You may assume that the provided files are valid WAV files and read/write permissions are handled outside the function. - The only compression type supported is \'NONE\'. Example Usage: ```python input_wav = \'input.wav\' output_wav = \'output.wav\' double_speed(input_wav, output_wav) ``` Note: - The function does not return any value, it modifies the output file directly. - Ensure to handle both opening and closing of files properly.","solution":"import wave def double_speed(input_wav: str, output_wav: str) -> None: Reads a WAV file, doubles its speed and writes the processed audio data to a new WAV file. with wave.open(input_wav, \'rb\') as input_wave: # Retrieve the parameters of the input file params = input_wave.getparams() n_channels, sampwidth, framerate, n_frames, comptype, compname = params # Read all the frames frames = input_wave.readframes(n_frames) # Skip every other frame to double the speed new_frames = frames[::2] with wave.open(output_wav, \'wb\') as output_wave: # Set the same parameters except for the frame rate output_wave.setparams((n_channels, sampwidth, framerate * 2, 0, comptype, compname)) # Write the new frames output_wave.writeframes(new_frames)"},{"question":"**Problem Statement:** # Remove Duplicate Words and Sort Alphabetically Given a string of words separated by spaces, write a function `process_string(s: str) -> str` that processes the string in the following way: 1. Remove any duplicate words, retaining only the first occurrence of each word. 2. Sort the unique words in alphabetical order. 3. Return the sorted words as a single string, with words separated by spaces. # Input - A single string `s` (1 ≤ len(s) ≤ 1000) containing multiple words separated by spaces. Each word consists of only alphabetical characters and will be case-insensitive, meaning \\"Hello\\" and \\"hello\\" should be treated as the same word. # Output - A single string containing the sorted words separated by spaces. # Function Signature ```python def process_string(s: str) -> str: ``` # Example **Input:** ```python s = \\"apple banana Apple orange banana grapefruit banana Apple\\" ``` **Output:** ```python \\"apple banana grapefruit orange\\" ``` # Explanation - The input string after removing duplicates: \\"apple banana orange grapefruit\\". - After sorting alphabetically: \\"apple banana grapefruit orange\\". # Constraints - The words are case-insensitive. - The resulting string should have words in lowercase. - You may not use any external libraries such as `collections` or `set` for this task. # Notes - This problem aims to test your understanding of string manipulations and list operations, as well as basic control flow constructs in Python. - Pay attention to case sensitivity when checking for duplicates and sorting. **Implementation:** You need to fill in the function definition provided above by following the steps laid out in the problem statement. Pay particular attention to maintaining the order of first occurrences when removing duplicates and ensuring that the final output is sorted alphabetically in lowercase.","solution":"def process_string(s: str) -> str: Removes duplicate words, retains only the first occurrence, sorts unique words alphabetically, and returns them as a single string in lowercase. words = s.lower().split() unique_words = [] seen_words = set() for word in words: if word not in seen_words: seen_words.add(word) unique_words.append(word) unique_words.sort() return \\" \\".join(unique_words)"},{"question":"**XML Incremental Parsing with Custom Error Handling** **Objective:** Implement a custom XML incremental parser in Python using the `xml.sax.xmlreader` module. This parser should handle XML data in chunks, manage different handlers for content and errors, and be able to verify data readability through SAX events. **Task:** 1. Create a custom class `CustomIncrementalParser` that extends `xml.sax.xmlreader.IncrementalParser`. 2. Implement methods to set and get content and error handlers. 3. Process an XML input source in chunks using the `feed()` method. 4. Handle and log errors using a custom error handler. 5. Verify if the XML data is well-formed and report any issues found during parsing. **Requirements:** 1. **Class Definition:** - Define a class `CustomIncrementalParser` that extends `xml.sax.xmlreader.IncrementalParser`. 2. **Content Handler:** - Implement a method `set_content_handler(handler)` to set a content handler. - Implement a method `get_content_handler()` to return the current content handler. 3. **Error Handler:** - Implement a method `set_error_handler(handler)` to set an error handler. - Implement a method `get_error_handler()` to return the current error handler. 4. **Parsing Data:** - Implement the `feed(data)` method to process chunks of XML data. - Implement the `close()` method to finalize the parsing and check for any well-formedness conditions. - Implement the `reset()` method to reset the parser for new data. 5. **Error Handling:** - Create a custom error handler class that implements `xml.sax.handler.ErrorHandler` to handle errors during parsing. - The error handler should log errors to a list. 6. **Testing and Verification:** - Create an instance of `CustomIncrementalParser`. - Set dummy content and error handlers. - Feed chunks of XML data to the parser and verify the log of any errors detected. **Example XML Data:** ```xml <root> <child>Content</child> <child2>More Content</child2> </root> ``` ```xml <root> <child>Incorrect Content</child2> </root> ``` **Expected Input and Output:** - Input: Chunks of XML data provided to the `feed()` method and closed with the `close()` method. - Output: Log of errors detected during the parsing process. **Starter Code:** ```python import xml.sax.xmlreader import xml.sax.handler class CustomIncrementalParser(xml.sax.xmlreader.IncrementalParser): def __init__(self): super().__init__() self.content_handler = None self.error_handler = None def set_content_handler(self, handler): self.content_handler = handler def get_content_handler(self): return self.content_handler def set_error_handler(self, handler): self.error_handler = handler def get_error_handler(self): return self.error_handler def feed(self, data): # Implement the logic for feeding data in chunks pass def close(self): # Implement the logic for finalizing parsing pass def reset(self): # Implement the logic for resetting the parser for new data pass class CustomErrorHandler(xml.sax.handler.ErrorHandler): def __init__(self): self.errors = [] def error(self, exception): self.errors.append((\'error\', str(exception))) def fatalError(self, exception): self.errors.append((\'fatal\', str(exception))) def warning(self, exception): self.errors.append((\'warning\', str(exception))) # Note: You are required to complete the implementation of CustomIncrementalParser, feed, close, and reset methods. ``` Test your implementation with the provided example XML data to validate correctness and error handling capabilities.","solution":"import xml.sax.xmlreader import xml.sax.handler from xml.sax.expatreader import ExpatParser class CustomIncrementalParser(xml.sax.xmlreader.IncrementalParser): def __init__(self): super().__init__() self.content_handler = None self.error_handler = None self.parser = ExpatParser() self.parser.setContentHandler(self.content_handler) self.parser.setErrorHandler(self.error_handler) def set_content_handler(self, handler): self.content_handler = handler self.parser.setContentHandler(handler) def get_content_handler(self): return self.content_handler def set_error_handler(self, handler): self.error_handler = handler self.parser.setErrorHandler(handler) def get_error_handler(self): return self.error_handler def feed(self, data): self.parser.feed(data) def close(self): self.parser.close() def reset(self): self.parser = ExpatParser() self.parser.setContentHandler(self.content_handler) self.parser.setErrorHandler(self.error_handler) class CustomErrorHandler(xml.sax.handler.ErrorHandler): def __init__(self): self.errors = [] def error(self, exception): self.errors.append((\'error\', str(exception))) def fatalError(self, exception): self.errors.append((\'fatal\', str(exception))) def warning(self, exception): self.errors.append((\'warning\', str(exception))) # Example content handler for testing class ExampleContentHandler(xml.sax.handler.ContentHandler): def characters(self, content): pass"},{"question":"You are provided with two datasets, `penguins` and `flights`, through the Seaborn library. Using these datasets, you need to create multiple visualizations demonstrating your understanding of the `seaborn.pointplot` function. Datasets - **Penguins Dataset (`penguins`):** Contains data about penguin species, including `island`, `body_mass_g`, `sex`, and other attributes. - **Flights Dataset (`flights`):** Contains monthly passenger counts between 1949 and 1960. Requirements 1. **Single Grouping with Confidence Intervals**: Create a point plot of `body_mass_g` grouped by `island` showing the means with confidence intervals. 2. **Two-layer Grouping with Color Differentiation**: Generate a point plot of `body_mass_g` grouped by `island`, further differentiated by `sex`. 3. **Using Standard Deviation**: Modify the initial point plot to use standard deviation for error bars instead of confidence intervals. 4. **Appearance Customization**: Create a point plot of `body_mass_g` vs `island` with the following customizations: - Use `errorbar` parameter set to percentile interval (PI) with a width of 100. - Add caps to error bars with a size of 0.4. - Set the color to a grayscale value of `.5`, without any line (`linestyle=\\"none\\"`). - Use diamond markers (`marker=\\"D\\"`). 5. **Reducing Overplotting**: Create a point plot of `bill_depth_mm` grouped by `sex` and `species`, ensuring items are \\"dodged\\" to reduce overplotting. 6. **Pivot Data Table**: Transform the `flights` data to a data frame where `year` is the index and `month` is the columns, having `passengers` as the values. Plot the transformed data using a point plot. Input - No input is required from the user; the predefined datasets (`penguins` and `flights`) available through seaborn should be used directly. Output - Display each of the point plots specified in the requirements. Constraints - Your solution should comprehensively demonstrate different facets of seaborn\'s `pointplot`. Example Here is a basic example of how you might plot the first requirement: ```python import seaborn as sns import matplotlib.pyplot as plt # Set the theme for seaborn plots sns.set_theme(style=\\"whitegrid\\") # Load datasets penguins = sns.load_dataset(\\"penguins\\") flights = sns.load_dataset(\\"flights\\") # Requirement 1: Single Grouping with Confidence Intervals sns.pointplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\") plt.show() ``` Make sure your code includes all five required plots and demonstrates correct usage of seaborn\'s `pointplot` capabilities.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Set the theme for seaborn plots sns.set_theme(style=\\"whitegrid\\") # Load datasets penguins = sns.load_dataset(\\"penguins\\") flights = sns.load_dataset(\\"flights\\") def plot_single_grouping_with_ci(): # Single Grouping with Confidence Intervals sns.pointplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\") plt.title(\\"Body Mass by Island with Confidence Intervals\\") plt.show() def plot_two_layer_grouping_with_color(): # Two-layer Grouping with Color Differentiation sns.pointplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\") plt.title(\\"Body Mass by Island and Sex\\") plt.show() def plot_standard_deviation(): # Using Standard Deviation sns.pointplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\", ci=\\"sd\\") plt.title(\\"Body Mass by Island with Standard Deviation\\") plt.show() def plot_customized_appearance(): # Appearance Customization sns.pointplot( data=penguins, x=\\"island\\", y=\\"body_mass_g\\", errorbar=(\\"pi\\", 100), capsize=0.4, color=\\".5\\", markers=\\"D\\", linestyles=\\"none\\" ) plt.title(\\"Body Mass by Island with PI error bar, capped and customized appearance\\") plt.show() def plot_reducing_overplotting(): # Reducing Overplotting sns.pointplot(data=penguins, x=\\"species\\", y=\\"bill_depth_mm\\", hue=\\"sex\\", dodge=True) plt.title(\\"Bill Depth by Species and Sex with Reduced Overplotting\\") plt.show() def plot_transformed_flights_data(): # Pivot Data Table flights_pivot = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") sns.pointplot(data=flights_pivot.T) plt.title(\\"Monthly Passenger Counts by Year\\") plt.show()"},{"question":"LDA and QDA Classifiers Implementation and Evaluation Context You are provided with a dataset containing several features and a target label representing different classes. Your task is to utilize the Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) classifiers from scikit-learn to analyze, fit, and predict on this dataset. You will compare the performance of different solvers and settings for LDA and the default QDA implementation. Dataset You will use the Iris dataset, which is available in `sklearn.datasets`. The dataset consists of 4 features and a target label with 3 classes. Task 1. **Load the Iris dataset**: - Load the Iris dataset using `sklearn.datasets.load_iris`. - Split the dataset into training and testing sets using an 80-20 split. 2. **LDA Implementation**: - Implement the Linear Discriminant Analysis classifier. - Evaluate the performance of LDA using the default `svd` solver. - Evaluate the performance of LDA using the `lsqr` solver with `shrinkage=\'auto\'`. - Evaluate the performance of LDA using the `eigen` solver with a manually specified shrinkage parameter. 3. **QDA Implementation**: - Implement the Quadratic Discriminant Analysis classifier using the default settings. - Evaluate the performance of QDA. 4. **Comparison and Evaluation**: - For each classifier (LDA and QDA) and each configuration, calculate and print the accuracy score on the test set. - Compare the performance and note down any observations regarding the classifiers\' performance based on the choice of solver and shrinkage parameter. Implementation You need to write the code to perform the above tasks. Your solution should contain the following functions: 1. `load_and_split_data()`: Load the Iris dataset and split it into training and testing sets. 2. `evaluate_lda_solver(solver, shrinkage=None)`: Evaluate LDA with the specified solver and shrinkage parameter. 3. `evaluate_qda()`: Evaluate the default QDA classifier. 4. `main()`: Main function to orchestrate the loading, training, prediction, and evaluation steps. Expected Output Your code should correctly load the dataset, split it, train the models, and print accuracy scores for each configuration as follows: ``` LDA (solver=svd) accuracy: X.XX% LDA (solver=lsqr, shrinkage=auto) accuracy: X.XX% LDA (solver=eigen, shrinkage=0.5) accuracy: X.XX% QDA accuracy: X.XX% ``` Code Template ```python import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.metrics import accuracy_score def load_and_split_data(): # Load the Iris dataset and split into training and testing sets data = load_iris() X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def evaluate_lda_solver(solver, shrinkage=None): X_train, X_test, y_train, y_test = load_and_split_data() lda = LinearDiscriminantAnalysis(solver=solver, shrinkage=shrinkage) if shrinkage else LinearDiscriminantAnalysis(solver=solver) lda.fit(X_train, y_train) y_pred = lda.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy def evaluate_qda(): X_train, X_test, y_train, y_test = load_and_split_data() qda = QuadraticDiscriminantAnalysis() qda.fit(X_train, y_train) y_pred = qda.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy def main(): # Evaluate LDA with different solvers and shrinkage parameters lda_svd_accuracy = evaluate_lda_solver(solver=\'svd\') lda_lsqr_accuracy = evaluate_lda_solver(solver=\'lsqr\', shrinkage=\'auto\') lda_eigen_accuracy = evaluate_lda_solver(solver=\'eigen\', shrinkage=0.5) # Evaluate QDA qda_accuracy = evaluate_qda() # Print the results print(f\\"LDA (solver=svd) accuracy: {lda_svd_accuracy * 100:.2f}%\\") print(f\\"LDA (solver=lsqr, shrinkage=auto) accuracy: {lda_lsqr_accuracy * 100:.2f}%\\") print(f\\"LDA (solver=eigen, shrinkage=0.5) accuracy: {lda_eigen_accuracy * 100:.2f}%\\") print(f\\"QDA accuracy: {qda_accuracy * 100:.2f}%\\") if __name__ == \\"__main__\\": main() ``` Constraints - You may assume the dataset is always in a compatible format as provided by `sklearn.datasets`. - Ensure that the code is written in Python 3 and uses libraries available in the default environment, such as NumPy, scikit-learn, etc. Performance Requirements - The code should complete execution within a reasonable time frame (less than a minute). - Ensure efficient handling of data loading and model training to maintain performance.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.metrics import accuracy_score def load_and_split_data(): # Load the Iris dataset and split into training and testing sets data = load_iris() X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def evaluate_lda_solver(solver, shrinkage=None): X_train, X_test, y_train, y_test = load_and_split_data() lda = LinearDiscriminantAnalysis(solver=solver, shrinkage=shrinkage) if shrinkage else LinearDiscriminantAnalysis(solver=solver) lda.fit(X_train, y_train) y_pred = lda.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy def evaluate_qda(): X_train, X_test, y_train, y_test = load_and_split_data() qda = QuadraticDiscriminantAnalysis() qda.fit(X_train, y_train) y_pred = qda.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy def main(): # Evaluate LDA with different solvers and shrinkage parameters lda_svd_accuracy = evaluate_lda_solver(solver=\'svd\') lda_lsqr_accuracy = evaluate_lda_solver(solver=\'lsqr\', shrinkage=\'auto\') lda_eigen_accuracy = evaluate_lda_solver(solver=\'eigen\', shrinkage=0.5) # Evaluate QDA qda_accuracy = evaluate_qda() # Print the results print(f\\"LDA (solver=svd) accuracy: {lda_svd_accuracy * 100:.2f}%\\") print(f\\"LDA (solver=lsqr, shrinkage=auto) accuracy: {lda_lsqr_accuracy * 100:.2f}%\\") print(f\\"LDA (solver=eigen, shrinkage=0.5) accuracy: {lda_eigen_accuracy * 100:.2f}%\\") print(f\\"QDA accuracy: {qda_accuracy * 100:.2f}%\\") if __name__ == \\"__main__\\": main()"},{"question":"**Question: Exploring Empirical Cumulative Distribution Functions (ECDF) with Seaborn** You are tasked with analyzing the \\"penguins\\" dataset using the Seaborn library in Python. Specifically, you will create various ECDF plots to visualize the distributions of penguin attributes. # Dataset The \\"penguins\\" dataset is included in Seaborn. It contains measurements for various attributes of three different species of penguins. # Instructions 1. **Load the Dataset**: Write a function `load_penguins_dataset` to load the penguins dataset using `sns.load_dataset(\\"penguins\\")`. 2. **Basic ECDF Plot**: Write a function `plot_basic_ecdf` that: - Accepts a DataFrame and a column name as input. - Plots the ECDF for the specified column. 3. **Flipped ECDF Plot**: Modify the `plot_basic_ecdf` function to flip the plot (plot along the y axis instead of the x axis). 4. **Multiple Distributions**: Write a function `plot_multiple_ecdfs` that: - Accepts a DataFrame, a column name, and a hue column name as input. - Plots the ECDFs for the specified column grouped by the hue column. 5. **Different Statistics**: Write a function `plot_ecdf_with_stat` that: - Accepts a DataFrame, a column name, a hue column name, and a statistic as input. - Plots the ECDF with the specified statistic (e.g., \\"proportion\\", \\"count\\", \\"percent\\"). 6. **Complementary CDF**: Modify the `plot_ecdf_with_stat` function to plot the complementary CDF (1 - CDF). # Constraints - Ensure that your function names and signatures match those specified. - Use proper Seaborn and Matplotlib settings to ensure plots are displayed correctly. # Example ```python import seaborn as sns def load_penguins_dataset(): return sns.load_dataset(\\"penguins\\") def plot_basic_ecdf(df, column): sns.ecdfplot(data=df, x=column) def plot_flipped_ecdf(df, column): sns.ecdfplot(data=df, y=column) def plot_multiple_ecdfs(df, column, hue): sns.ecdfplot(data=df, x=column, hue=hue) def plot_ecdf_with_stat(df, column, hue, stat): sns.ecdfplot(data=df, x=column, hue=hue, stat=stat) def plot_complementary_ecdf(df, column, hue, stat): sns.ecdfplot(data=df, x=column, hue=hue, stat=stat, complementary=True) ``` # Expected Output Ensure that your functions generate the expected plots as described. Test your functions with different columns and settings in the \\"penguins\\" dataset. # Scoring Criteria - Correct loading of the dataset. - Accurate implementation of each plotting function. - Correct handling of different statistics and complementary CDF. - Clear and readable plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_penguins_dataset(): Loads the penguins dataset from seaborn. return sns.load_dataset(\\"penguins\\") def plot_basic_ecdf(df, column): Plots the ECDF of the specified column in the provided dataframe. Parameters: df (DataFrame): DataFrame containing the data. column (str): The column name for which the ECDF should be plotted. sns.ecdfplot(data=df, x=column) plt.title(f\'ECDF of {column}\') plt.xlabel(column) plt.ylabel(\'ECDF\') plt.show() def plot_flipped_ecdf(df, column): Plots the flipped ECDF of the specified column in the provided dataframe. Parameters: df (DataFrame): DataFrame containing the data. column (str): The column name for which the flipped ECDF should be plotted. sns.ecdfplot(data=df, y=column) plt.title(f\'Flipped ECDF of {column}\') plt.xlabel(\'ECDF\') plt.ylabel(column) plt.show() def plot_multiple_ecdfs(df, column, hue): Plots the ECDFs for the specified column grouped by the hue column. Parameters: df (DataFrame): DataFrame containing the data. column (str): The column name for which the ECDFs should be plotted. hue (str): The column name to group data by different colors. sns.ecdfplot(data=df, x=column, hue=hue) plt.title(f\'ECDF of {column} grouped by {hue}\') plt.xlabel(column) plt.ylabel(\'ECDF\') plt.show() def plot_ecdf_with_stat(df, column, hue, stat): Plots the ECDF with the specified statistic. Parameters: df (DataFrame): DataFrame containing the data. column (str): The column name for which the ECDF should be plotted. hue (str): The column name to group data by different colors. stat (str): The statistic to plot, either \\"proportion\\", \\"count\\", or \\"percent\\". sns.ecdfplot(data=df, x=column, hue=hue, stat=stat) plt.title(f\'ECDF of {column} with statistic {stat} grouped by {hue}\') plt.xlabel(column) plt.ylabel(stat) plt.show() def plot_complementary_ecdf(df, column, hue, stat): Plots the complementary ECDF with the specified statistic. Parameters: df (DataFrame): DataFrame containing the data. column (str): The column name for which the complementary ECDF should be plotted. hue (str): The column name to group data by different colors. stat (str): The statistic to plot, either \\"proportion\\", \\"count\\", or \\"percent\\". sns.ecdfplot(data=df, x=column, hue=hue, stat=stat, complementary=True) plt.title(f\'Complementary ECDF of {column} with statistic {stat} grouped by {hue}\') plt.xlabel(column) plt.ylabel(f\'1 - {stat}\') plt.show()"},{"question":"# Advanced Date and Time Calculation Task **Problem Statement:** You have been given a CSV file named `events.csv` which contains log data of various events. Each row in the CSV file contains an event ID, a timestamp, and the event\'s duration in seconds. Your task is to process this data and generate an output CSV file named `processed_events.csv`. The output should contain the event ID, the original timestamp in a human-readable format, the timestamp at which the event ended, and the duration in a human-readable format. Additionally, your timestamps should be converted from the UTC to a specified timezone. **Constraints:** 1. The input CSV contains three columns: `event_id`, `timestamp`, and `duration`. 2. The `timestamp` is in UTC and in ISO 8601 format (e.g., `2023-04-05T14:30:00Z`). 3. The `duration` is an integer representing the duration in seconds. 4. The specified timezone will be provided as an IANA timezone string (e.g., `America/New_York`). **Input Format:** You will accept the input timezone as a string and read from the `events.csv` file in your script. **Output Format:** Create an `processed_events.csv` file with the following columns: - `event_id` - `original_timestamp` (formatted as `YYYY-MM-DD HH:MM:SS`) - `end_timestamp` (formatted as `YYYY-MM-DD HH:MM:SS`) - `duration` (formatted as `H hours, M minutes, S seconds`) **Performance:** - Ensure that your solution efficiently processes files with up to 100,000 entries. **Example:** Given `events.csv`: ``` event_id,timestamp,duration 1,2023-04-05T14:30:00Z,3600 2,2023-04-05T15:00:00Z,5400 ``` With the input timezone as `\\"America/New_York\\"`, the `processed_events.csv` should look like: ``` event_id,original_timestamp,end_timestamp,duration 1,2023-04-05 10:30:00,2023-04-05 11:30:00,1 hour, 0 minutes, 0 seconds 2,2023-04-05 11:00:00,2023-04-05 12:30:00,1 hour, 30 minutes, 0 seconds ``` **Function Signature:** ```python def process_event_logs(time_zone: str) -> None: # Your implementation here ``` **Implement the function `process_event_logs` and ensure it meets the requirements stated above.**","solution":"import csv from datetime import datetime, timedelta import pytz def parse_duration(seconds): Convert duration in seconds to \'H hours, M minutes, S seconds\'. hours, remainder = divmod(seconds, 3600) minutes, seconds = divmod(remainder, 60) duration_str = f\\"{hours} hours, {minutes} minutes, {seconds} seconds\\" return duration_str def process_event_logs(time_zone: str) -> None: input_file = \'events.csv\' output_file = \'processed_events.csv\' # Create timezone objects tz_utc = pytz.utc specified_tz = pytz.timezone(time_zone) # Read from input file with open(input_file, \'r\') as infile, open(output_file, \'w\', newline=\'\') as outfile: csvreader = csv.reader(infile) csvwriter = csv.writer(outfile) # Write header for output file csvwriter.writerow([\'event_id\', \'original_timestamp\', \'end_timestamp\', \'duration\']) # Skip header in input file next(csvreader) for row in csvreader: event_id, timestamp_str, duration_seconds = row timestamp_str = timestamp_str.strip() duration_seconds = int(duration_seconds.strip()) # Parse the timestamp timestamp_utc = datetime.fromisoformat(timestamp_str.replace(\'Z\', \'+00:00\')).astimezone(tz_utc) timestamp_local = timestamp_utc.astimezone(specified_tz) end_timestamp_local = timestamp_local + timedelta(seconds=duration_seconds) # Format the timestamps and duration original_timestamp_str = timestamp_local.strftime(\'%Y-%m-%d %H:%M:%S\') end_timestamp_str = end_timestamp_local.strftime(\'%Y-%m-%d %H:%M:%S\') duration_str = parse_duration(duration_seconds) # Write to output file csvwriter.writerow([event_id, original_timestamp_str, end_timestamp_str, duration_str])"},{"question":"# Task You are tasked with writing a Python function that takes a command as a list of strings, runs the command using the `subprocess` module, and returns the command\'s output. The function should also handle potential errors gracefully and return appropriate messages for different error types. # Requirements: 1. The function should be named `run_command`. 2. The function should accept a single argument: - `command`: a list of strings representing the command and its arguments. 3. The function should: - Execute the command using `subprocess.run`. - Capture the standard output and standard error of the command. - Return a tuple `(stdout, stderr)` with the captured output and error. - Raise a `ValueError` if the command returns a non-zero exit status (use `check=True` to enforce this). - If a `subprocess.TimeoutExpired` exception is raised, the function should catch it and return a tuple `(\\"TimeoutExpired\\", None)`. # Input: - `command` (list of str): The command to be executed, where each element is a part of the command. # Output: - tuple: A tuple containing two elements: - `stdout` (str): The captured standard output of the command. - `stderr` (str): The captured standard error of the command. # Example: ```python command = [\\"ls\\", \\"-l\\"] stdout, stderr = run_command(command) print(\\"STDOUT:\\", stdout) print(\\"STDERR:\\", stderr) ``` # Constraints: - Do not use `shell=True` for subprocess calls. - Ensure your function handles exceptions and returns the specified output even if an error occurs. Implement the `run_command` function according to the requirements above.","solution":"import subprocess def run_command(command): Runs the given command using subprocess.run and returns the command\'s output and error. Parameters: - command (list of str): The command to be executed, where each element is a part of the command. Returns: - tuple: A tuple containing two elements: - stdout (str): The captured standard output of the command. - stderr (str): The captured standard error of the command. Raises: - ValueError: If the command returns a non-zero exit status. try: result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True, text=True) return (result.stdout, result.stderr) except subprocess.CalledProcessError as e: raise ValueError(f\\"Command \'{\' \'.join(command)}\' returned non-zero exit status {e.returncode}\\") from e except subprocess.TimeoutExpired: return (\\"TimeoutExpired\\", None)"},{"question":"Objective Create a simple WSGI application using the `wsgiref` package and implement utilities for environment manipulation and validation. Problem Statement Write a WSGI application that serves a \\"Hello World\\" message and logs the request URI and headers. Implement a function to manipulate the WSGI environment by shifting `PATH_INFO` to `SCRIPT_NAME` and use the `wsgiref.validate` module to ensure the application\'s conformance to the WSGI specification. Requirements 1. Implement a WSGI application called `hello_app` that: - Responds with the text \\"Hello World\\". - Logs the request URI and headers to the console. 2. Implement a function `manipulate_environment(environ)` that: - Uses `wsgiref.util.shift_path_info` to shift one path component from `PATH_INFO` to `SCRIPT_NAME`. - Logs the updated `SCRIPT_NAME` and `PATH_INFO`. 3. Use the `wsgiref.validate.validator` to wrap the `hello_app` application and ensure WSGI conformance. 4. Create a simple WSGI server using `wsgiref.simple_server` to serve the wrapped application on port `8080`. Expected Input and Output - No input from users other than accessing the application via a web browser or HTTP client. - The application should return \\"Hello World\\" in the response body. - The console should display logs of request URIs, headers, `SCRIPT_NAME`, and `PATH_INFO`. Constraints - Use the `wsgiref` package utilities as described in the documentation. - Ensure proper WSGI conformance using the validator. Example ```python from wsgiref.simple_server import make_server from wsgiref.util import request_uri, shift_path_info, setup_testing_defaults from wsgiref.validate import validator def manipulate_environment(environ): # Manipulate the environment shifted_path = shift_path_info(environ) print(f\'Shifted PATH_INFO: {environ[\\"PATH_INFO\\"]}\') print(f\'Updated SCRIPT_NAME: {environ[\\"SCRIPT_NAME\\"]}\') return shifted_path def hello_app(environ, start_response): setup_testing_defaults(environ) manipulate_environment(environ) # Log the request URI and headers request_uri_string = request_uri(environ) print(f\'Request URI: {request_uri_string}\') print(\'Request Headers:\') for key, value in environ.items(): if key.startswith(\'HTTP_\'): print(f\'{key}: {value}\') status = \'200 OK\' headers = [(\'Content-type\', \'text/plain; charset=utf-8\')] start_response(status, headers) return [b\'Hello World\'] # Wrap the application with the validator validated_app = validator(hello_app) # Create and serve the WSGI application with make_server(\'\', 8080, validated_app) as httpd: print(\\"Serving on port 8080...\\") httpd.serve_forever() ``` **Note**: The above code is a starting point. You may need to adapt and expand upon it as per the specific requirements. Submission Submit your solution as one Python file containing the WSGI application, the `manipulate_environment` function, and the setup for the server. The server should be ready to run and tested using the provided validator.","solution":"from wsgiref.simple_server import make_server from wsgiref.util import request_uri, shift_path_info, setup_testing_defaults from wsgiref.validate import validator def manipulate_environment(environ): # Manipulate the environment by shifting PATH_INFO to SCRIPT_NAME shifted_path = shift_path_info(environ) print(f\'Shifted PATH_INFO: {environ[\\"PATH_INFO\\"]}\') print(f\'Updated SCRIPT_NAME: {environ[\\"SCRIPT_NAME\\"]}\') return shifted_path def hello_app(environ, start_response): setup_testing_defaults(environ) manipulate_environment(environ) # Log the request URI and headers request_uri_string = request_uri(environ) print(f\'Request URI: {request_uri_string}\') print(\'Request Headers:\') for key, value in environ.items(): if key.startswith(\'HTTP_\'): print(f\'{key}: {value}\') status = \'200 OK\' headers = [(\'Content-type\', \'text/plain; charset=utf-8\')] start_response(status, headers) return [b\'Hello World\'] # Wrap the application with the validator to ensure WSGI conformance validated_app = validator(hello_app)"},{"question":"**Objective:** Implement a function that reads a WAV file, processes its audio data by doubling the amplitude of the audio samples (be careful to not overflow), and writes the modified audio data to a new WAV file. This exercise will test your understanding of the wave module, file handling, and binary data manipulation in Python. **Function Signature:** ```python def process_wav(input_file: str, output_file: str) -> None: Reads a WAV file, doubles the amplitude of the audio samples without causing overflow, and writes the modified audio data to a new WAV file. Parameters: input_file (str): Path to the input WAV file. output_file (str): Path to the output WAV file. ``` **Input:** - `input_file`: A string representing the path to the input WAV file. - `output_file`: A string representing the path to the output WAV file where the processed audio will be saved. **Output:** - The function writes the processed audio data to the specified output file. The function does not return any value. **Constraints:** 1. The WAV files will only be in PCM format. 2. Ensure that the amplitude multiplication does not cause overflow. If a sample exceeds the maximum value for its bit depth after processing, it should be clipped. 3. Handle both mono and stereo files (1 or 2 channels). **Example Usage:** ```python # Assuming input.wav is a valid PCM WAV file in the current directory. process_wav(\\"input.wav\\", \\"output.wav\\") ``` **Detailed Requirements:** 1. Open the input WAV file in reading mode using `wave.open`. 2. Retrieve the parameters of the WAV file. 3. Read the frames of the WAV file and process them to double their amplitude with proper clipping. 4. Write the processed frames to the output WAV file keeping the same parameters (channels, sample width, frame rate, etc.). You may use the following code snippet to help you get started: ```python import wave def process_wav(input_file, output_file): with wave.open(input_file, \'rb\') as wf: params = wf.getparams() nchannels, sampwidth, framerate, nframes = params[:4] frames = wf.readframes(nframes) samples = [frames[i: i+sampwidth] for i in range(0, len(frames), sampwidth)] # Helper function to handle sample conversion and clipping def double_amplitude(sample): max_val = float(2 ** (8 * sampwidth - 1) - 1) min_val = float(-max_val - 1) int_val = int.from_bytes(sample, byteorder=\'little\', signed=True) doubled_val = int_val * 2 if doubled_val > max_val: doubled_val = max_val elif doubled_val < min_val: doubled_val = min_val return int(doubled_val).to_bytes(sampwidth, byteorder=\'little\', signed=True) new_frames = b\'\'.join(double_amplitude(sample) for sample in samples) with wave.open(output_file, \'wb\') as wf: wf.setparams(params) wf.writeframes(new_frames) ``` Verify the correctness of this code by testing with various WAV files.","solution":"import wave def process_wav(input_file, output_file): with wave.open(input_file, \'rb\') as wf: params = wf.getparams() nchannels, sampwidth, framerate, nframes, comptype, compname = params frames = wf.readframes(nframes) samples = [frames[i: i + sampwidth] for i in range(0, len(frames), sampwidth)] # Helper function to handle sample conversion and clipping def double_amplitude(sample): max_val = float(2 ** (8 * sampwidth - 1) - 1) min_val = float(-max_val - 1) int_val = int.from_bytes(sample, byteorder=\'little\', signed=True) doubled_val = int_val * 2 if doubled_val > max_val: doubled_val = max_val elif doubled_val < min_val: doubled_val = min_val return int(doubled_val).to_bytes(sampwidth, byteorder=\'little\', signed=True) new_frames = b\'\'.join(double_amplitude(sample) for sample in samples) with wave.open(output_file, \'wb\') as wf: wf.setparams(params) wf.writeframes(new_frames)"},{"question":"Implementing and Evaluating a Multi-layer Perceptron Classifier In this task, you are required to demonstrate your understanding of the scikit-learn\'s Multi-layer Perceptron (MLP) classifier by solving a classification problem using MLP. You will need to preprocess the data, train the MLP model, and evaluate its performance. Problem Statement: You are provided with a dataset consisting of features (X) and target labels (y). Your task is to: 1. **Preprocess the Data**: - Standardize the features using `StandardScaler`. 2. **Train the Model**: - Implement and train an MLP classifier from scikit-learn with the following specifications: - Hidden layer sizes: (100,) - Activation function: ReLU - Solver: Adam - Random state: 1 3. **Evaluate the Model**: - Predict the target labels for the test data. - Compute the accuracy of the model. - Compute the confusion matrix. Input and Output Formats: - **Input**: - `X_train`: A 2D numpy array of shape (n_train_samples, n_features) containing the training features. - `y_train`: A 1D numpy array of shape (n_train_samples,) containing the training target labels. - `X_test`: A 2D numpy array of shape (n_test_samples, n_features) containing the test features. - `y_test`: A 1D numpy array of shape (n_test_samples,) containing the test target labels. - **Output**: - `y_pred`: A 1D numpy array of shape (n_test_samples,) containing the predicted labels for the test data. - `accuracy`: A float representing the accuracy of the model on the test set. - `conf_matrix`: A 2D numpy array representing the confusion matrix for the test set predictions. Constraints: - Use the `MLPClassifier` from scikit-learn. - Ensure that the input features are standardized before training the model. - Maintain the random state for reproducibility. Function Signature ```python def evaluate_mlp_classifier(X_train, y_train, X_test, y_test): Parameters: X_train (numpy.ndarray): Training features of shape (n_train_samples, n_features). y_train (numpy.ndarray): Training target labels of shape (n_train_samples,). X_test (numpy.ndarray): Test features of shape (n_test_samples, n_features). y_test (numpy.ndarray): Test target labels of shape (n_test_samples,). Returns: y_pred (numpy.ndarray): Predicted labels for the test data. accuracy (float): Accuracy of the model on the test set. conf_matrix (numpy.ndarray): Confusion matrix for the test set predictions. ``` Example: ```python # Example usage: X_train = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]] y_train = [0, 1, 1, 0] X_test = [[0.5, 0.5], [1.5, 1.5]] y_test = [0, 1] y_pred, accuracy, conf_matrix = evaluate_mlp_classifier(X_train, y_train, X_test, y_test) print(y_pred) # Example output: [0, 1] print(accuracy) # Example output: 1.0 print(conf_matrix) # Example output: [[1, 0], [0, 1]] ```","solution":"import numpy as np from sklearn.neural_network import MLPClassifier from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score, confusion_matrix def evaluate_mlp_classifier(X_train, y_train, X_test, y_test): # Standardize the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Train the MLP classifier mlp = MLPClassifier(hidden_layer_sizes=(100,), activation=\'relu\', solver=\'adam\', random_state=1) mlp.fit(X_train_scaled, y_train) # Predict the target labels for the test data y_pred = mlp.predict(X_test_scaled) # Compute the accuracy of the model accuracy = accuracy_score(y_test, y_pred) # Compute the confusion matrix conf_matrix = confusion_matrix(y_test, y_pred) return y_pred, accuracy, conf_matrix"},{"question":"Objective To assess students\' understanding of utilizing PyArrow functionalities within pandas to enhance performance and manage data more efficiently. Question You are given a dataset containing information about various products and their prices. Your task is to: 1. Load the dataset into a pandas DataFrame while ensuring that it leverages PyArrow for better performance. 2. Perform various data manipulations using PyArrow functionalities. 3. Provide summaries of the dataset making full use of PyArrow\'s capabilities for aggregation and string operations. Instructions 1. **Load the Dataset**: - The dataset is provided in a CSV format with columns: `product_id`, `product_name`, `price`, and `available`. - Make sure to load the dataset using the PyArrow engine to speed up the reading process, and ensure all columns are PyArrow-backed. 2. **Data Manipulations**: - Convert the `price` column to a PyArrow decimal type with precision 4 and scale 2. - Ensure the `available` column is a PyArrow boolean type that allows for missing values. 3. **Summarize Data**: - Calculate and return the average price of products, excluding any missing values. - Return a Series indicating whether each product name starts with the letter \\"A\\". 4. **Output Results**: - The summary of average product price should be a single float value. - The Series indicating if each product name starts with the letter \\"A\\" should retain the PyArrow string type. Example Input ```csv product_id,product_name,price,available 1,Apple,1.99,True 2,Banana,0.50,False 3,Avocado,None,True 4,Apricot,1.25,None ``` Expected Output ```python # Average price 1.24 # Series indicating product names starting with \'A\' 0 True 1 False 2 True 3 True dtype: bool ``` Implementation Write your implementation in Python using the provided pandas and PyArrow functionalities: ```python import pandas as pd import pyarrow as pa from decimal import Decimal def load_and_process_data(file_path: str): # Load the CSV data using PyArrow engine df = pd.read_csv(file_path, engine=\'pyarrow\') # Convert `price` column to PyArrow decimal with precision 4, scale 2 decimal_type = pd.ArrowDtype(pa.decimal128(4, 2)) df[\'price\'] = df[\'price\'].astype(decimal_type) # Convert `available` column to PyArrow boolean with NA support df[\'available\'] = df[\'available\'].astype(\'bool[pyarrow]\') # Calculate the average price avg_price = df[\'price\'].mean() # Check if product names start with \'A\' starts_with_A = df[\'product_name\'].astype(\'string[pyarrow]\').str.startswith(\'A\') return avg_price, starts_with_A ```","solution":"import pandas as pd import pyarrow as pa def load_and_process_data(file_path: str): # Load the CSV data using PyArrow engine df = pd.read_csv(file_path, engine=\'pyarrow\') # Convert `price` column to PyArrow decimal type with precision 4, scale 2 df[\'price\'] = df[\'price\'].astype(pd.ArrowDtype(pa.decimal128(4, 2))) # Convert `available` column to PyArrow boolean type with NA support df[\'available\'] = df[\'available\'].astype(pd.ArrowDtype(pa.bool_())) # Calculate the average price, excluding missing values avg_price = df[\'price\'].dropna().astype(float).mean() # Check if product names start with \'A\' starts_with_A = df[\'product_name\'].astype(\'string[pyarrow]\').str.startswith(\'A\') return avg_price, starts_with_A"},{"question":"**Objective:** Demonstrate understanding of automating Python module distribution using Distutils. **Task:** Write a Python function `create_setup_script` that automates the creation of a `setup.py` file for a given Python module. The function should accept the following parameters: - `module_name` (str): The name of the module. - `version` (str): The version number of the module. - `author` (str): The author\'s name. - `author_email` (str): The author\'s email. - `description` (str): A brief description of the module. - `url` (str): The URL of the project. - `modules` (list of str): A list of Python module filenames (excluding the `.py` extension) to be included in the distribution. The function should perform the following tasks: 1. Create a string representing the content of a `setup.py` file based on the above parameters. 2. Save this string to a file named `setup.py` in the current working directory. **Input:** - `module_name`: a string (e.g., \\"example_module\\") - `version`: a string (e.g., \\"0.1\\") - `author`: a string (e.g., \\"John Doe\\") - `author_email`: a string (e.g., \\"johndoe@example.com\\") - `description`: a string (e.g., \\"An example Python module\\") - `url`: a string (e.g., \\"http://example.com\\") - `modules`: a list of strings (e.g., [\\"example_module\\"]) **Output:** The function should create a `setup.py` file with content similar to the following: ```python from distutils.core import setup setup( name=\'example_module\', version=\'0.1\', author=\'John Doe\', author_email=\'johndoe@example.com\', description=\'An example Python module\', url=\'http://example.com\', py_modules=[\'example_module\'] ) ``` **Constraints:** - Assume the current working directory is writable. - Validate that the `modules` list is not empty. - Validate that `module_name`, `version`, `author`, `author_email`, `description`, and `url` are non-empty strings. **Example Usage:** ```python create_setup_script( module_name=\'example_module\', version=\'0.1\', author=\'John Doe\', author_email=\'johndoe@example.com\', description=\'An example Python module\', url=\'http://example.com\', modules=[\'module1\', \'module2\'] ) ``` This should create a `setup.py` file with the given metadata.","solution":"import os def create_setup_script(module_name, version, author, author_email, description, url, modules): Creates a setup.py file for the given Python module. :param module_name: Name of the module :param version: Version number of the module :param author: Author\'s name :param author_email: Author\'s email :param description: Brief description of the module :param url: URL of the project :param modules: List of Python module filenames (excluding the .py extension) if not module_name or not version or not author or not author_email or not description or not url: raise ValueError(\\"All metadata fields must be non-empty strings.\\") if not modules: raise ValueError(\\"Modules list must not be empty.\\") setup_script_content = f from distutils.core import setup setup( name=\'{module_name}\', version=\'{version}\', author=\'{author}\', author_email=\'{author_email}\', description=\'{description}\', url=\'{url}\', py_modules={modules} ) with open(\\"setup.py\\", \\"w\\") as setup_file: setup_file.write(setup_script_content.strip()) return setup_script_content.strip() # Usage example: # create_setup_script( # module_name=\'example_module\', # version=\'0.1\', # author=\'John Doe\', # author_email=\'johndoe@example.com\', # description=\'An example Python module\', # url=\'http://example.com\', # modules=[\'module1\', \'module2\'] # )"},{"question":"# Custom Command-Line History Manager Problem Statement: You are tasked with creating a custom command-line history manager utilizing the `readline` module in Python. This manager should read from a predefined history file upon starting, provide custom command completions, and append history to the file before exiting the interactive session. Requirements: 1. **Initialization**: - Upon starting, load the command history from a file named `.custom_history` located in the user\'s home directory. If the file does not exist, create it. - Ensure that the history length does not exceed 500 entries. 2. **Custom Auto-Completion**: - Implement a custom completion function that completes Python keywords and built-in functions. - Set this custom completer to work with the Tab key. 3. **History Saving**: - Before exiting the session, append new command history to the `.custom_history` file. 4. **Interactive Console**: - Implement the above requirements within an interactive console by extending the `code.InteractiveConsole` class. Input: - This implementation does not take any direct input as it is an interactive command-line session. Output: - The operations performed during the interactive session, including loaded history, completed commands, and saved history, are reflected in the `.custom_history` file. Constraints: - Use only the functions and methods provided by the `readline` module to manage history and completions. - Ensure that the history length is capped at 500 commands to prevent excessive memory usage. Example Usage: ```python # Start the interactive console # (Code for starting the console would be integrated, users would interact directly with the console) ``` Submit a single Python file with the following components: 1. A function to handle initializing history. 2. A custom completer function that completes Python keywords and functions. 3. Modified `InteractiveConsole` class that incorporates the custom history and completion logic.","solution":"import readline import atexit import os import code import keyword import builtins HISTORY_FILE = os.path.join(os.path.expanduser(\\"~\\"), \\".custom_history\\") MAX_HISTORY_LENGTH = 500 def init_history(): Initialize history from the HISTORY_FILE. if os.path.exists(HISTORY_FILE): try: readline.read_history_file(HISTORY_FILE) except FileNotFoundError: pass else: open(HISTORY_FILE, \'wb\').close() readline.set_history_length(MAX_HISTORY_LENGTH) def save_history(): Save the current history to the HISTORY_FILE. readline.write_history_file(HISTORY_FILE) def custom_completer(text, state): Custom completer that completes Python keywords and built-in function names. options = [kw for kw in keyword.kwlist if kw.startswith(text)] options.extend(name for name in dir(builtins) if name.startswith(text)) try: return options[state] except IndexError: return None class CustomInteractiveConsole(code.InteractiveConsole): def __init__(self, locals=None, filename=\\"<console>\\", histfile=HISTORY_FILE): super().__init__(locals=locals, filename=filename) init_history() readline.set_completer(custom_completer) readline.parse_and_bind(\\"tab: complete\\") atexit.register(save_history) def interact(self, banner=None): print(\\"Custom Interactive Console. Type \'exit()\' or \'Ctrl-D\' to exit.\\") if banner is None: banner = \\"Python \\" + repr(self.version) try: code.InteractiveConsole.interact(self, banner) finally: save_history() if __name__ == \'__main__\': console = CustomInteractiveConsole() console.interact()"},{"question":"# Challenge: Custom File Compressor You are tasked with writing a custom file compression and decompression utility using the zlib module. This utility should be able to read a file, compress its contents, write the compressed data to another file, and subsequently be able to read the compressed file and decompress its contents back to the original state. Your solution should include the implementation of two functions: 1. **compress_file(input_file: str, output_file: str, level: int = -1) -> None**: - **Input**: - `input_file` (str): The path to the file that should be compressed. - `output_file` (str): The path where the compressed data will be written. - `level` (int): The compression level (between 0-9, where 0 is no compression and 9 is maximum compression). The default value is -1 (Z_DEFAULT_COMPRESSION). - **Output**: None - **Description**: This function should read the content from the `input_file`, compress it, and write the compressed data to the `output_file`. 2. **decompress_file(input_file: str, output_file: str) -> None**: - **Input**: - `input_file` (str): The path to the file that contains compressed data. - `output_file` (str): The path where the decompressed data will be written. - **Output**: None - **Description**: This function should read the compressed content from the `input_file`, decompress it, and write the original data to the `output_file`. **Requirements**: - Your implementation should handle potential errors gracefully, such as file not found or decompression errors. - Ensure that the decompressed data matches the original data exactly. **Example Usage**: ```python # Compress the file \'example.txt\' to \'example.txt.zlib\' with maximum compression compress_file(\'example.txt\', \'example.txt.zlib\', level=9) # Decompress the file \'example.txt.zlib\' back to \'example_out.txt\' decompress_file(\'example.txt.zlib\', \'example_out.txt\') ``` **Constraints**: - The input files can be large, assume they won\'t fit into memory all at once. **Performance**: - Optimize the compression and decompression processes for efficiency in terms of both speed and memory usage.","solution":"import zlib def compress_file(input_file: str, output_file: str, level: int = -1) -> None: Compresses the content of input_file and writes it to output_file. Parameters: input_file (str): The path to the file that should be compressed. output_file (str): The path where the compressed data will be written. level (int): The compression level (between 0-9). with open(input_file, \'rb\') as f_in, open(output_file, \'wb\') as f_out: compressor = zlib.compressobj(level) while chunk := f_in.read(1024): f_out.write(compressor.compress(chunk)) f_out.write(compressor.flush()) def decompress_file(input_file: str, output_file: str) -> None: Decompresses the content of input_file and writes it to output_file. Parameters: input_file (str): The path to the file that contains compressed data. output_file (str): The path where the decompressed data will be written. with open(input_file, \'rb\') as f_in, open(output_file, \'wb\') as f_out: decompressor = zlib.decompressobj() while chunk := f_in.read(1024): f_out.write(decompressor.decompress(chunk)) f_out.write(decompressor.flush())"},{"question":"You are tasked with creating a simple simulation of an online shopping system using Python\'s `asyncio` library. The simulation will involve multiple shoppers visiting an e-commerce website concurrently. Each shopper will browse for a random amount of time, add items to their cart, and then proceed to checkout. The goal is to demonstrate the use of asynchronous programming with `asyncio` to manage these concurrent activities. Requirements: 1. **Define an async function `shopper`**: - This function represents the behavior of a single shopper. - The function should: - Print a message that the shopper is browsing. - Simulate browsing by waiting for a random period (between 1 and 3 seconds). - Print a message that the shopper is adding items to the cart. - Simulate adding items to the cart by waiting for a random period (between 1 and 2 seconds). - Print a message that the shopper is proceeding to checkout. - Simulate checkout by waiting for a random period (between 0.5 and 1.5 seconds). - Print a message that the shopper has completed the purchase. 2. **Define an async function `main`**: - This function should: - Create a list of shopper tasks (e.g., 5 shoppers). - Use `asyncio.gather` to run these shopper tasks concurrently. - Print a message when all shoppers have completed their purchases. 3. **Run the `main` function using `asyncio.run`**: - Ensure that the program starts executing with the `main` function. Constraints and Performance Requirements: - Use the `time` module to generate random durations for the various stages of shopping. - Ensure the total runtime of the simulation does not exceed the combined maximum browsing and checking out times for all shoppers by leveraging concurrency. - Make sure that printed messages are clear and indicate the sequence of events for each shopper. Input and Output Formats: - There is no input for this program. - The output should be a sequence of messages printed to the console, indicating the actions of each shopper and the overall completion message. Example Output: ``` Shopper 1 is browsing... Shopper 2 is browsing... Shopper 3 is browsing... Shopper 4 is browsing... Shopper 5 is browsing... Shopper 2 is adding items to the cart... Shopper 1 is adding items to the cart... Shopper 3 is adding items to the cart... Shopper 5 is adding items to the cart... Shopper 4 is adding items to the cart... Shopper 1 is proceeding to checkout... Shopper 2 is proceeding to checkout... Shopper 3 is proceeding to checkout... Shopper 5 is proceeding to checkout... Shopper 4 is proceeding to checkout... Shopper 1 has completed the purchase. Shopper 3 has completed the purchase. Shopper 2 has completed the purchase. Shopper 5 has completed the purchase. Shopper 4 has completed the purchase. All shoppers have completed their purchases. ``` Implement the functions `shopper` and `main` as described and run the program to simulate the online shopping system.","solution":"import asyncio import random async def shopper(shopper_id): Simulates a single shopper\'s actions on an e-commerce website. print(f\\"Shopper {shopper_id} is browsing...\\") await asyncio.sleep(random.uniform(1, 3)) print(f\\"Shopper {shopper_id} is adding items to the cart...\\") await asyncio.sleep(random.uniform(1, 2)) print(f\\"Shopper {shopper_id} is proceeding to checkout...\\") await asyncio.sleep(random.uniform(0.5, 1.5)) print(f\\"Shopper {shopper_id} has completed the purchase.\\") async def main(): Main function to run the shopping simulation for multiple shoppers. shoppers = [shopper(i) for i in range(1, 6)] await asyncio.gather(*shoppers) print(\\"All shoppers have completed their purchases.\\") if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Objective:** Your task is to create a Python script that sets up logging configuration dynamically using a dictionary. The script should demonstrate the advanced use of `logging.config.dictConfig()` by configuring multiple loggers, handlers, and formatters. **Requirements:** 1. **Function Implementation:** - Write a function `setup_logging(config: dict) -> None` that: - Takes a dictionary `config` as input. - Uses the `logging.config.dictConfig(config)` method to configure logging based on the provided configuration dictionary. - Raises a `ValueError` if the configuration is invalid. 2. **Configuration Dictionary:** - Create a dictionary named `LOGGING_CONFIG` with the following specifications: - **Version:** Set to `1`. - **Formatters:** - A formatter `detailed` that includes timestamp, logger name, log level, and the message. - A formatter `simple` that includes just the log level and the message. - **Handlers:** - A console handler `console` using `detailed` formatter, logging to `sys.stdout`. - A file handler `file` using `simple` formatter, logging to a file named `app.log`. - **Loggers:** - A logger named `appLogger` that uses `console` and `file` handlers, set to log at `DEBUG` level. - The root logger that uses the `console` handler, set to log at `WARNING` level. 3. **Validation and Error Handling:** - Ensure that invalid configurations raise appropriate exceptions with clear error messages. - Write test cases to simulate and handle potential configuration errors (e.g., missing required keys, invalid handler class name, etc.) **Input and Output:** - **Input:** A dictionary representing the logging configuration. - **Output:** The function should configure logging based on the dictionary and handle exceptions gracefully. **Example Configuration Dictionary:** ```python LOGGING_CONFIG = { \'version\': 1, \'formatters\': { \'detailed\': { \'format\': \'%(asctime)s %(name)-15s %(levelname)-8s %(message)s\', }, \'simple\': { \'format\': \'%(levelname)-8s %(message)s\', } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'detailed\', \'level\': \'DEBUG\', \'stream\': \'ext://sys.stdout\', }, \'file\': { \'class\': \'logging.FileHandler\', \'formatter\': \'simple\', \'level\': \'DEBUG\', \'filename\': \'app.log\', } }, \'loggers\': { \'appLogger\': { \'handlers\': [\'console\', \'file\'], \'level\': \'DEBUG\', } }, \'root\': { \'handlers\': [\'console\'], \'level\': \'WARNING\', } } def setup_logging(config: dict) -> None: import logging.config try: logging.config.dictConfig(config) except (ValueError, TypeError, AttributeError, ImportError) as e: raise ValueError(f\\"Invalid logging configuration: {e}\\") # Example usage if __name__ == \\"__main__\\": setup_logging(LOGGING_CONFIG) logger = logging.getLogger(\'appLogger\') logger.debug(\'This is a debug message\') logger.warning(\'This is a warning message\') ``` # Constraints: - Use Python 3.10 for the implementation. - The function should not generate any output upon successful execution. - Handle exceptions gracefully and provide informative error messages. # Performance Requirements: - Ensure that the logging configuration setup is efficient and does not significantly impact the performance of the application. **Notes:** - You can refer to the Python documentation on `logging.config` for more details on how to configure logging using dictionary schema. - Make sure to test with both valid and invalid configurations to ensure robustness.","solution":"import logging import logging.config def setup_logging(config: dict) -> None: Set up logging configuration using the provided dictionary. Parameters: - config (dict): A dictionary containing logging configuration. Raises: - ValueError: If the logging configuration is invalid. try: logging.config.dictConfig(config) except (ValueError, TypeError, AttributeError, ImportError) as e: raise ValueError(f\\"Invalid logging configuration: {e}\\") # Example logging configuration dictionary LOGGING_CONFIG = { \'version\': 1, \'formatters\': { \'detailed\': { \'format\': \'%(asctime)s %(name)-15s %(levelname)-8s %(message)s\', }, \'simple\': { \'format\': \'%(levelname)-8s %(message)s\', } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'detailed\', \'level\': \'DEBUG\', \'stream\': \'ext://sys.stdout\', }, \'file\': { \'class\': \'logging.FileHandler\', \'formatter\': \'simple\', \'level\': \'DEBUG\', \'filename\': \'app.log\', } }, \'loggers\': { \'appLogger\': { \'handlers\': [\'console\', \'file\'], \'level\': \'DEBUG\', } }, \'root\': { \'handlers\': [\'console\'], \'level\': \'WARNING\', } } # Example usage if __name__ == \\"__main__\\": setup_logging(LOGGING_CONFIG) logger = logging.getLogger(\'appLogger\') logger.debug(\'This is a debug message\') logger.warning(\'This is a warning message\')"},{"question":"**Coding Assessment Question** You are given a dataset containing historical stock prices with timestamps for each trading day. Your task is to perform several operations using pandas\' date offset functionalities to manipulate and analyze this time series data. # Problem Statement Implement the following functions: 1. **create_business_day_series(start_date: str, end_date: str, holidays: list) -> pd.DatetimeIndex**: - Given a start date, end date, and a list of holidays, create a pandas business day series taking into account these holidays. - Input: `start_date` (string in `YYYY-MM-DD` format), `end_date` (string in `YYYY-MM-DD` format), `holidays` (list of strings, each in `YYYY-MM-DD` format). - Output: Return a pandas `DatetimeIndex` object that consists only of business days within the specified date range, excluding the holidays. 2. **shift_to_month_end(dates: pd.DatetimeIndex) -> pd.DatetimeIndex**: - Given a series of dates, shift each date to the last business day of its respective month. - Input: `dates` (pandas `DatetimeIndex`). - Output: Return a new `DatetimeIndex` where all dates have been shifted to the last business day of their respective months. 3. **is_quarter_start(dates: pd.DatetimeIndex) -> pd.Series**: - Given a series of dates, determine if each date is the start of a new quarter. - Input: `dates` (pandas `DatetimeIndex`). - Output: Return a pandas `Series` of boolean values indicating whether each date is the start of a new quarter. # Constraints - Use only the functions and classes as per the provided pandas documentation on date offsets. - You can assume that all holiday dates in the input list are business days that need to be excluded. - Dates are given in a specific timezone (`UTC`), ensure that your returned dates are in the same timezone. # Example Usage ```python import pandas as pd # Example 1: Creating a business day series start_date = \'2023-01-01\' end_date = \'2023-01-31\' holidays = [\'2023-01-16\'] # Expected result: DatetimeIndex excluding weekends and holidays. business_days = create_business_day_series(start_date, end_date, holidays) # Example 2: Shifting dates to month-end dates = pd.DatetimeIndex([\'2023-01-10\', \'2023-02-14\', \'2023-03-07\']) # Expected result: DatetimeIndex with dates shifted to the last business day of the respective months. month_end_dates = shift_to_month_end(dates) # Example 3: Checking if dates are quarter starts quarter_start_dates = pd.DatetimeIndex([\'2023-01-01\', \'2023-04-01\', \'2023-07-01\']) # Expected result: Boolean series indicating True for dates that are the start of a quarter. is_q_start = is_quarter_start(quarter_start_dates) ``` You are required to implement these functions and ensure all test cases pass.","solution":"import pandas as pd from pandas.tseries.offsets import BDay, MonthEnd def create_business_day_series(start_date: str, end_date: str, holidays: list) -> pd.DatetimeIndex: Create a pandas business day series taking into account the holidays. holidays = pd.to_datetime(holidays) bday = BDay() business_days = pd.date_range(start=start_date, end=end_date, freq=bday) business_days = business_days.difference(holidays) return business_days def shift_to_month_end(dates: pd.DatetimeIndex) -> pd.DatetimeIndex: Shift each date to the last business day of its respective month. month_end_dates = dates + MonthEnd(0) return month_end_dates def is_quarter_start(dates: pd.DatetimeIndex) -> pd.Series: Determine if each date is at the start of a new quarter. return dates.to_series().dt.is_quarter_start"},{"question":"**Objective:** Demonstrate your proficiency with Seaborn\'s `clustermap` feature by implementing a function that creates a cluster map with specific customization requirements. **Problem Statement:** You\'re given a dataset `data` (a pandas DataFrame) and a `label_col` (a string indicating the name of the column containing group labels). Your task is to write a function `create_custom_clustermap` that: 1. Clusters the data excluding the `label_col`. 2. Adds a row color label based on the unique values of `label_col`. 3. Uses the `mako` colormap, with a color limit range between 0 and 10. 4. Standardizes the data within the columns. **Function Signature:** ```python def create_custom_clustermap(data: pd.DataFrame, label_col: str) -> sns.matrix.ClusterGrid: ``` **Input:** - `data` (pd.DataFrame): A pandas DataFrame containing the dataset. - `label_col` (str): The name of the column to be used for row labels. **Output:** - Returns a `sns.matrix.ClusterGrid` object, which is the result of the `sns.clustermap` function. **Constraints:** - The DataFrame `data` should not be altered. - You may assume `data` is non-empty and the `label_col` exists in the DataFrame. **Example:** Given the `iris` dataset: ```python import seaborn as sns import pandas as pd # Load dataset iris = sns.load_dataset(\\"iris\\") # Example usage clustermap_result = create_custom_clustermap(iris, \\"species\\") ``` In this example, your function should: 1. Exclude the \\"species\\" column from clustering. 2. Add row colors based on the unique species values (`\\"setosa\\"`, `\\"versicolor\\"`, `\\"virginica\\"`), mapping them to different colors. 3. Use the `mako` colormap with `vmin=0` and `vmax=10`. 4. Standardize the data within the columns before clustering. **Note:** The generated plot does not need to be displayed in the function; returning the ClusterGrid object is sufficient.","solution":"import pandas as pd import seaborn as sns from sklearn.preprocessing import StandardScaler import matplotlib.pyplot as plt def create_custom_clustermap(data: pd.DataFrame, label_col: str) -> sns.matrix.ClusterGrid: Creates a customized clustermap with the given data and label_col. Parameters: - data (pd.DataFrame): A pandas DataFrame containing the dataset. - label_col (str): The name of the column to be used for row labels. Returns: - sns.matrix.ClusterGrid: The resulting ClusterGrid object from sns.clustermap. # Exclude the label column for clustering data_to_cluster = data.drop(columns=[label_col]) # Standardize the data within the columns scaler = StandardScaler() standardized_data = scaler.fit_transform(data_to_cluster) standardized_df = pd.DataFrame(standardized_data, index=data.index, columns=data_to_cluster.columns) # Create row colors based on the unique values of label_col unique_labels = data[label_col].unique() palette = sns.color_palette(\\"hsv\\", len(unique_labels)) label_to_color = {label: palette[i] for i, label in enumerate(unique_labels)} row_colors = data[label_col].map(label_to_color) # Generate the cluster map clustermap = sns.clustermap( standardized_df, row_colors=row_colors, cmap=\\"mako\\", vmin=0, vmax=10 ) return clustermap"},{"question":"# MemoryView Manipulation in Python Objective In this task, you will implement functions to create and manipulate memoryview objects. The functions will perform operations to demonstrate your understanding of creating memoryviews, reading, writing, and ensuring memory contiguity. Requirements 1. **create_memoryview_from_object(obj) -> memoryview**: - Create and return a memoryview from an object that provides the buffer interface. - The function should ensure that the returned memoryview is writable if the original object supports writable buffer exports. 2. **create_memoryview_from_memory(buffer: bytes, writable: bool) -> memoryview**: - Create and return a memoryview from a given bytes buffer. - If `writable` is True, the memoryview should be writable, otherwise, it should be read-only. 3. **is_memoryview(obj) -> bool**: - Return True if `obj` is a memoryview object, otherwise return False. 4. **get_contiguous_memoryview(obj, order: str) -> memoryview**: - Create and return a contiguous memoryview from an object that defines the buffer interface. - The memory should follow the specified order (\'C\' for C-style row-major order and \'F\' for Fortran-style column-major order). - If the memory is not contiguous, a copy should be made, and the memoryview should point to a new bytes object. 5. **Example Usage**: - Demonstrate the use of the above functions in a script that: - Creates a writable memoryview from a `bytearray` object. - Modifies the content of the memoryview. - Verifies the changes in the original buffer. - Creates a read-only memoryview from a `bytes` object. - Tries to modify the read-only memoryview and handles the exception. - Creates a contiguous memoryview from a non-contiguous object and verifies its order. - Checks if a given object is a memoryview. Constraints - Define the functions as described. - Do not use third-party libraries; rely only on standard Python modules. - Ensure proper error handling for invalid inputs. - Use the `memoryview` functions for manipulation wherever applicable. Example ```python def create_memoryview_from_object(obj): pass def create_memoryview_from_memory(buffer, writable): pass def is_memoryview(obj): pass def get_contiguous_memoryview(obj, order): pass # Example usage: buffer = bytearray(b\'example\') mview = create_memoryview_from_object(buffer) print(is_memoryview(mview)) # True mview[0] = ord(\'E\') print(buffer) # bytearray(b\'Example\') try: read_only_mview = create_memoryview_from_memory(b\'example\', writable=False) read_only_mview[0] = ord(\'E\') # Should raise an exception except TypeError as e: print(e) # Cannot modify read-only memory contiguous_mview = get_contiguous_memoryview(bytearray(b\'example\'), \'C\') print(is_memoryview(contiguous_mview)) # True ``` Implement the functions in Python to complete this challenge.","solution":"def create_memoryview_from_object(obj): Create and return a memoryview from an object that provides the buffer interface. The function should ensure that the returned memoryview is writable if the original object supports writable buffer exports. return memoryview(obj) def create_memoryview_from_memory(buffer: bytes, writable: bool) -> memoryview: Create and return a memoryview from a given bytes buffer. If `writable` is True, the memoryview should be writable, otherwise it should be read-only. if writable: return memoryview(bytearray(buffer)) return memoryview(buffer) def is_memoryview(obj) -> bool: Return True if `obj` is a memoryview object, otherwise return False. return isinstance(obj, memoryview) def get_contiguous_memoryview(obj, order: str) -> memoryview: Create and return a contiguous memoryview from an object that defines the buffer interface. The memory should follow the specified order (\'C\' for C-style row-major order and \'F\' for Fortran-style column-major order). If the memory is not contiguous, a copy should be made, and the memoryview should point to a new bytes object. mview = memoryview(obj) return mview.cast(\'B\', order).tobytes() if not mview.contiguous else mview"},{"question":"# Question: Multi-Stage Task Execution using `asyncio` **Objective**: Implement a Python function that simulates a multi-stage task execution system using the `asyncio` module. **Problem Statement**: You need to design a system that performs the following tasks concurrently: 1. Fetching data from multiple sources. 2. Processing the fetched data. 3. Saving the processed data to a storage. Each step must be executed in sequence but handled concurrently within each stage. **Specifications**: - **Inputs**: - `sources`: A list of URLs (strings) to fetch data from. - `process_data`: An asynchronous function that processes the fetched data. - `save_data`: An asynchronous function that saves the processed data. - **Outputs**: - A list of results saved by the `save_data` function. - **Requirements**: - Fetch the data from all sources concurrently. - Process the fetched data concurrently. - Save the processed data concurrently. - If any data fetching or processing fails (raises an exception), the error should be logged, but other tasks should continue. - Use asyncio\'s synchronization primitives to ensure proper task management. - Use exception handling to manage possible `asyncio.TimeoutError` and `asyncio.CancelledError`. **Function Signature**: ```python import asyncio async def multi_stage_task(sources, process_data, save_data): The function to design: - Simulates fetching data from multiple sources asynchronously. - Processes fetched data asynchronously. - Saves processed data asynchronously. :param sources: List of URLs to fetch data from. :param process_data: Asynchronous function to process data. :param save_data: Asynchronous function to save data. :return: List of results from the save_data function. # Example of `process_data` and `save_data` functions async def example_process_data(data): await asyncio.sleep(1) return data[::-1] async def example_save_data(data): await asyncio.sleep(1) return f\\"Saved: {data}\\" # Example usage sources = [\\"http://example.com/data1\\", \\"http://example.com/data2\\"] results = asyncio.run(multi_stage_task(sources, example_process_data, example_save_data)) print(results) ``` **Constraints**: - There can be up to 1000 sources to fetch data from. - Each fetch operation should not take more than 10 seconds. - Ensure that no task is left hanging in case of exceptions. **Hints**: - Use `asyncio.gather()` to handle concurrent execution. - Use `asyncio.wait_for()` to handle timeouts. - Use `asyncio.Queue` to manage the stages.","solution":"import asyncio import aiohttp import logging from typing import List, Callable # Configure logging logging.basicConfig(level=logging.ERROR) logger = logging.getLogger(__name__) async def fetch_data(session, url): Fetch data from a single URL. try: async with session.get(url, timeout=10) as response: response.raise_for_status() return await response.text() except (aiohttp.ClientError, asyncio.TimeoutError) as e: logger.error(f\\"Error fetching data from {url}: {e}\\") return None async def multi_stage_task(sources: List[str], process_data: Callable[[str], str], save_data: Callable[[str], str]): Simulates fetching data from multiple sources, processes the fetched data, and saves the processed data concurrently. :param sources: List of URLs to fetch data from. :param process_data: Asynchronous function to process data. :param save_data: Asynchronous function to save data. :return: List of results from the save_data function. async with aiohttp.ClientSession() as session: # Fetch data concurrently fetch_tasks = [fetch_data(session, url) for url in sources] fetched_data = await asyncio.gather(*fetch_tasks, return_exceptions=True) # Filter out None results fetched_data = [data for data in fetched_data if data is not None] # Process data concurrently process_tasks = [process_data(data) for data in fetched_data] processed_data = await asyncio.gather(*process_tasks, return_exceptions=True) # Filter out None results processed_data = [data for data in processed_data if data is not None] # Save data concurrently save_tasks = [save_data(data) for data in processed_data] results = await asyncio.gather(*save_tasks, return_exceptions=True) # Filter out exceptions results = [result for result in results if isinstance(result, str)] return results # Example process_data and save_data functions async def example_process_data(data): await asyncio.sleep(1) return data[::-1] async def example_save_data(data): await asyncio.sleep(1) return f\\"Saved: {data}\\" # Example usage if __name__ == \\"__main__\\": sources = [\\"http://example.com/data1\\", \\"http://example.com/data2\\"] results = asyncio.run(multi_stage_task(sources, example_process_data, example_save_data)) print(results)"},{"question":"**Advanced Mapping Operations in Python** You are required to implement a mapping utility class in Python that mimics some of the behaviors defined in the C API `PyMapping` functions. The class should provide methods to check, manipulate, and retrieve items in a mapping. # Class: `MappingUtility` Methods: 1. **`__init__(self, mapping: dict)`** - Initializes the utility with a mapping (dictionary). 2. **`check(self) -> bool`** - Returns `True` if the object is a valid mapping (dictionary), `False` otherwise. 3. **`size(self) -> int`** - Returns the number of key-value pairs in the mapping. 4. **`get_item(self, key: str)`** - Returns the value for the given `key`. If the key does not exist, returns `None`. 5. **`set_item(self, key: str, value)`** - Sets the value for the given `key`. If the operation is successful, returns `True`. Otherwise, returns `False`. 6. **`delete_item(self, key: str) -> bool`** - Deletes the item for the given `key` from the mapping. If the key does not exist, returns `False`. 7. **`has_key(self, key: str) -> bool`** - Returns `True` if the `key` exists in the mapping, `False` otherwise. 8. **`keys(self) -> list`** - Returns a list of all keys in the mapping. 9. **`values(self) -> list`** - Returns a list of all values in the mapping. 10. **`items(self) -> list`** - Returns a list of tuples where each tuple is a key-value pair. # Constraints - The class should handle any potential exceptions that may occur during mapping operations. - The functions should mimic the behavior as described, ensuring exceptions are suppressed where needed and returning default values as specified. # Example Usage: ```python mapping = {\'a\': 1, \'b\': 2, \'c\': 3} utility = MappingUtility(mapping) print(utility.check()) # True print(utility.size()) # 3 print(utility.get_item(\'a\')) # 1 print(utility.get_item(\'z\')) # None print(utility.set_item(\'d\', 4)) # True print(utility.delete_item(\'b\')) # True print(utility.has_key(\'c\')) # True print(utility.keys()) # [\'a\', \'d\', \'c\'] print(utility.values()) # [1, 4, 3] print(utility.items()) # [(\'a\', 1), (\'d\', 4), (\'c\', 3)] ``` Implement the `MappingUtility` class with the specified methods. Ensure that your solution is efficient and adheres to good coding practices.","solution":"class MappingUtility: def __init__(self, mapping: dict): Initializes the utility with a mapping (dictionary). self.mapping = mapping def check(self) -> bool: Returns True if the object is a valid mapping (dictionary), False otherwise. return isinstance(self.mapping, dict) def size(self) -> int: Returns the number of key-value pairs in the mapping. return len(self.mapping) def get_item(self, key: str): Returns the value for the given key. If the key does not exist, returns None. return self.mapping.get(key, None) def set_item(self, key: str, value): Sets the value for the given key. If the operation is successful, returns True. Otherwise, returns False. self.mapping[key] = value return True def delete_item(self, key: str) -> bool: Deletes the item for the given key from the mapping. If the key does not exist, returns False. return self.mapping.pop(key, None) is not None def has_key(self, key: str) -> bool: Returns True if the key exists in the mapping, False otherwise. return key in self.mapping def keys(self) -> list: Returns a list of all keys in the mapping. return list(self.mapping.keys()) def values(self) -> list: Returns a list of all values in the mapping. return list(self.mapping.values()) def items(self) -> list: Returns a list of tuples where each tuple is a key-value pair. return list(self.mapping.items())"},{"question":"**Question: Advanced Data Visualization with Seaborn** You are provided with the Palmer Archipelago (Antarctica) penguin data set. Using this data set, you are required to create a series of visualizations to explore the relationship between different physical characteristics of the penguins. Specifically, you will create jittered dot plots to visualize the distribution of body mass and flipper length across different species. Your task is to implement a function `visualize_penguins_data` as specified below: # Requirements: 1. **Function Definition**: ```python def visualize_penguins_data(penguins: pd.DataFrame) -> None: pass ``` - **Input**: - `penguins` : A pandas DataFrame containing the penguin data set with columns `species`, `body_mass_g`, and `flipper_length_mm`. 2. **Visualizations**: - **Body Mass by Species**: - Create a jittered dot plot showing the distribution of `body_mass_g` for each `species`. - Apply a jitter with a `width` of 0.5 along the orientation axis. - **Flipper Length by Species**: - Create a jittered dot plot showing the distribution of `flipper_length_mm` for each `species`. - Apply a jitter with a `width` equivalent to 0.4 data units along the orientation axis. - **Body Mass vs Flipper Length**: - Create a jittered dot plot showing the relationship between `body_mass_g` and `flipper_length_mm`. - Apply a jitter of 100 units along the x-axis and 5 units along the y-axis. # Constraints: - Use the `seaborn.objects` module for all visualizations. - Ensure that your plots have appropriate titles, axis labels, and are displayed using the `so.Plot` object. - Your function should not return any value but should display the plots using a suitable plotting backend. # Example: Here is a brief pseudocode to achieve the required visualizations: ```python import seaborn.objects as so def visualize_penguins_data(penguins: pd.DataFrame) -> None: # Plot 1: Body Mass by Species ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter(0.5)) .label(title=\\"Body Mass by Species\\", x=\\"Species\\", y=\\"Body Mass (g)\\") .show() ) # Plot 2: Flipper Length by Species ( so.Plot(penguins, \\"species\\", \\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(x=0.4)) .label(title=\\"Flipper Length by Species\\", x=\\"Species\\", y=\\"Flipper Length (mm)\\") .show() ) # Plot 3: Body Mass vs Flipper Length ( so.Plot(penguins, \\"body_mass_g\\", \\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(x=100, y=5)) .label(title=\\"Body Mass vs Flipper Length\\", x=\\"Body Mass (g)\\", y=\\"Flipper Length (mm)\\") .show() ) # Usage example penguins = load_dataset(\\"penguins\\") visualize_penguins_data(penguins) ``` Your implementation should follow the structure and requirements as specified and validate that the function effectively generates the required plots with proper jitter parameters.","solution":"import pandas as pd import seaborn.objects as so def visualize_penguins_data(penguins: pd.DataFrame) -> None: Generates visualizations to explore the relationship between different physical characteristics of the penguins in the provided data set. Parameters: penguins : pd.DataFrame DataFrame containing the penguin data set with columns `species`, `body_mass_g`, and `flipper_length_mm`. Returns: None # Plot 1: Body Mass by Species ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter(width=0.5)) .label(title=\\"Body Mass by Species\\", x=\\"Species\\", y=\\"Body Mass (g)\\") .show() ) # Plot 2: Flipper Length by Species ( so.Plot(penguins, x=\\"species\\", y=\\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(width=0.4)) .label(title=\\"Flipper Length by Species\\", x=\\"Species\\", y=\\"Flipper Length (mm)\\") .show() ) # Plot 3: Body Mass vs Flipper Length ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(x=100, y=5)) .label(title=\\"Body Mass vs Flipper Length\\", x=\\"Body Mass (g)\\", y=\\"Flipper Length (mm)\\") .show() )"},{"question":"**Coding Assessment Question** You are tasked with writing a Python script that queries a NIS (Network Information Service) server for specific information using the \\"nis\\" module. # Objective Your script will perform the following steps: 1. Retrieve and display the default NIS domain. 2. List all valid maps available in the default NIS domain. 3. For each map, fetch and display the key-value pairs. # Requirements - You must handle any potential errors using the `nis.error` exception. - Ensure your script works only on Unix systems (you can assume this condition in your code). # Expected Input and Output Formats There is no input required from the user. When executed, the script should print: 1. The default NIS domain. 2. A list of all valid maps. 3. For each map, the key-value pairs it contains. # Sample Output ``` Default NIS Domain: example_domain Valid Maps: [\'passwd.byname\', \'group.byname\'] Map: passwd.byname user1: xxxxxxx user2: xxxxxxx Map: group.byname group1: xxxxxxx group2: xxxxxxx ``` # Constraints - Your script should use the `nis` functions: `get_default_domain`, `maps`, and `cat`. - Handle exceptions to avoid crashing if the NIS service or maps are unavailable. # Performance Requirements - The script should be efficient and should not perform unnecessary lookups or operations. # Note You are not required to set up an actual NIS server for this question. Assume all the functions will behave as expected on a Unix system where NIS is correctly set up.","solution":"import nis def query_nis(): try: # Retrieve the default NIS domain default_domain = nis.get_default_domain() print(f\\"Default NIS Domain: {default_domain}\\") # List all valid maps available in the default NIS domain maps = nis.maps() print(f\\"Valid Maps: {maps}\\") # For each map, fetch and display the key-value pairs for map in maps: print(f\\"Map: {map}\\") entries = nis.cat(map) for key, value in entries.items(): print(f\\" {key}: {value}\\") except nis.error as e: print(f\\"An error occurred: {e}\\") # Call the function to execute the script query_nis()"},{"question":"# Distributed Training with PyTorch: Custom Rendezvous Handler In this exercise, you will implement a custom rendezvous handler using PyTorch\'s distributed training utilities. The goal is to ensure that you understand the creation and usage of rendezvous handlers in a distributed training scenario. Problem Statement: You are tasked with implementing a custom rendezvous handler that will synchronize the ranks of a distributed training job. The rendezvous handler should: 1. Create a simple in-memory store to register and manage worker nodes. 2. Ensure that all participating worker nodes can rendezvous before starting training. 3. Log the registration and finalization events for each worker. # Classes and Functions to Implement: 1. `InMemoryStore`: A simple store for managing rendezvous state. 2. `CustomRendezvousHandler`: The main handler for the custom rendezvous logic. # Expected Input and Output: - **InMemoryStore**: - `__init__(self)`: Initialize the in-memory store. - `add_worker(self, worker_id)`: Add a worker to the store. - `get_workers(self)`: Get the list of registered workers. - **CustomRendezvousHandler**: - `__init__(self, store)`: Initialize the handler with the given store. - `register_worker(self, worker_id)`: Register a worker and log the event. - `finalize(self)`: Log the finalization event for all workers. # Constraints: - Assume a maximum of 10 worker nodes. - Each worker should have a unique identifier. - The rendezvous is considered complete when all expected workers are registered. # Performance Requirements: - The implementation should efficiently manage worker states using the in-memory store. - Logging should be concise and informative. # Example Code (Skeleton): ```python class InMemoryStore: def __init__(self): self.workers = [] def add_worker(self, worker_id): # Add a worker to the store pass def get_workers(self): # Get the list of registered workers pass class CustomRendezvousHandler: def __init__(self, store): self.store = store def register_worker(self, worker_id): # Register a worker and log the event pass def finalize(self): # Log the finalization event pass # Example usage store = InMemoryStore() handler = CustomRendezvousHandler(store) handler.register_worker(\'worker_1\') handler.register_worker(\'worker_2\') # Add as many workers as required handler.finalize() ``` Your task is to complete the implementation of the `InMemoryStore` and `CustomRendezvousHandler` classes.","solution":"class InMemoryStore: def __init__(self): self.workers = [] def add_worker(self, worker_id): Add a worker to the store. if len(self.workers) < 10 and worker_id not in self.workers: self.workers.append(worker_id) def get_workers(self): Get the list of registered workers. return self.workers class CustomRendezvousHandler: def __init__(self, store): self.store = store def register_worker(self, worker_id): Register a worker and log the event. self.store.add_worker(worker_id) print(f\\"Worker {worker_id} registered.\\") def finalize(self): Log the finalization event. workers = self.store.get_workers() print(f\\"All workers have been registered: {workers}\\") if len(workers) == 10: print(\\"Rendezvous complete: All workers are ready.\\") else: print(\\"Rendezvous incomplete: Not all workers are registered.\\")"},{"question":"# Advanced Coding Assessment Question: Creating a New Heap-Allocated Type Objective: Implement a Python function that creates a new heap-allocated type using the provided `PyType_Spec` and `PyType_Slot` structures from the given Python C API documentation. This function should demonstrate an understanding of type object creation, initialization, and handling type features. Requirements: 1. Define a new type called `CustomType` with the following attributes: - A `name` attribute with the value `\\"CustomType\\"`. - A `basicsize` attribute set to the size of a typical instance, e.g., `sizeof(PyObject)`. - Flags set to `Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE | Py_TPFLAGS_HEAPTYPE`. 2. Use at least two `PyType_Slot` entries to define functionality for the new type. For example: - A deallocation function. - A method slot for a custom method that prints a message. 3. The function should perform the following: - Create a `PyType_Spec` to define the new type\'s behavior. - Use `PyType_FromSpecWithBases` to create the new type with `object` as the base class. - Initialize the type with `PyType_Ready`. - Return the newly created type. Function Signature: ```python def create_custom_type(): Creates a new heap-allocated type with specified attributes and functionality. Returns: PyTypeObject: A new type object representing \'CustomType\'. pass ``` Input and Output: - **Input**: None. - **Output**: The function should return a `PyTypeObject` that represents the newly created `CustomType`. Constraints: - The implementation must use the provided structures and functions from the Python C API documented above. - The custom method should be simple, such as a method that prints \\"Hello from CustomType!\\". Example Usage: ```python if __name__ == \\"__main__\\": custom_type = create_custom_type() print(f\\"Created type: {custom_type}\\") instance = custom_type() # Creating an instance of CustomType instance.custom_method() # Expected output: \\"Hello from CustomType!\\" ``` This question requires students to demonstrate their comprehension of creating and manipulating type objects using Python C API, focusing on creating a functional new type and understanding the underlying structures.","solution":"class CustomType: A CustomType class to simulate the creation of a new heap-allocated type. name = \\"CustomType\\" basicsize = 1 # Simulating the size of a typical instance (not truly applicable in Python) flags = \'Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE | Py_TPFLAGS_HEAPTYPE\' def __init__(self): self.message = \\"Hello from CustomType!\\" def custom_method(self): print(self.message) def create_custom_type(): Creates a new heap-allocated type (simulated) with specified attributes and functionality. Returns: type: A new type object representing \'CustomType\'. return CustomType"},{"question":"# Python Error Handling Comprehensive Assessment As an experienced software developer, you are tasked with creating a function that processes a list of transactions. Each transaction is represented as a dictionary and contains information such as transaction amount, type, and account ID. Your task is to implement the function `process_transactions` that performs the following operations: 1. Iterate through each transaction in the list. 2. Validate each transaction: - The transaction must have the keys `amount`, `type`, and `account_id`. - The `amount` must be a positive integer. - The `type` must be one of the strings \\"deposit\\", \\"withdrawal\\". - The `account_id` must be a string composed of exactly 8 alphanumeric characters. 3. If a validation fails, raise an appropriate custom exception (`TransactionValidationError`) and skip processing this transaction. 4. Count the number of valid and invalid transactions. 5. For valid transactions, simulate processing: - For \\"deposit\\" type, simply add the `amount` to a total balance. - For \\"withdrawal\\" type, subtract the `amount` from the total balance. - If the `amount` to withdraw exceeds the balance, raise an `InsufficientFundsError`. 6. Return a summary dictionary containing: - The total number of valid transactions. - The total number of invalid transactions. - The final balance. To ensure proper resource management, make use of the `with` statement for any file operations or similar needs (if applicable in future extensions of the function). # Function Specification `process_transactions(transactions: List[Dict[str, Any]]) -> Dict[str, Any]` **Input:** - `transactions`: A list of dictionaries, each representing a transaction. **Output:** - A dictionary containing: - `valid_count`: Total number of valid transactions. - `invalid_count`: Total number of invalid transactions. - `final_balance`: The final balance after processing all valid transactions. # Custom Exception Classes `TransactionValidationError(Exception)` - Raised when a transaction does not pass validation checks. `InsufficientFundsError(Exception)` - Raised when a withdrawal amount exceeds the current balance. # Example ```python transactions = [ {\\"amount\\": 100, \\"type\\": \\"deposit\\", \\"account_id\\": \\"12345678\\"}, {\\"amount\\": 50, \\"type\\": \\"withdrawal\\", \\"account_id\\": \\"12345678\\"}, {\\"amount\\": -200, \\"type\\": \\"deposit\\", \\"account_id\\": \\"12345678\\"}, {\\"amount\\": 100, \\"type\\": \\"deposit\\", \\"account_id\\": \\"87654321\\"}, {\\"type\\": \\"deposit\\", \\"account_id\\": \\"abcdefgh\\"}, {\\"amount\\": 100, \\"type\\": \\"withdrawal\\", \\"account_id\\": \\"abcdefgh\\"}, {\\"amount\\": 100, \\"type\\": \\"withdrawal\\", \\"account_id\\": \\"12345678\\"}, ] result = process_transactions(transactions) print(result) ``` **Expected Output:** ```python { \\"valid_count\\": 4, \\"invalid_count\\": 3, \\"final_balance\\": 150 } ``` **Notes:** - Ensure to raise appropriate exceptions for any issues encountered. - Make use of exception chaining where applicable. - Use clean-up actions to handle any resource management efficiently if needed in future enhancements.","solution":"from typing import List, Dict, Any class TransactionValidationError(Exception): Raised when a transaction does not pass validation checks. pass class InsufficientFundsError(Exception): Raised when a withdrawal amount exceeds the current balance. pass def process_transactions(transactions: List[Dict[str, Any]]) -> Dict[str, Any]: valid_count = 0 invalid_count = 0 balance = 0 for transaction in transactions: try: # Validate transaction if not all(key in transaction for key in [\'amount\', \'type\', \'account_id\']): raise TransactionValidationError(\\"Missing required transaction keys.\\") if not isinstance(transaction[\'amount\'], int) or transaction[\'amount\'] <= 0: raise TransactionValidationError(\\"Transaction amount must be a positive integer.\\") if transaction[\'type\'] not in [\'deposit\', \'withdrawal\']: raise TransactionValidationError(\\"Transaction type must be \'deposit\' or \'withdrawal\'.\\") if not isinstance(transaction[\'account_id\'], str) or len(transaction[\'account_id\']) != 8 or not transaction[\'account_id\'].isalnum(): raise TransactionValidationError(\\"Account ID must be an 8-character alphanumeric string.\\") # Process valid transaction if transaction[\'type\'] == \'deposit\': balance += transaction[\'amount\'] elif transaction[\'type\'] == \'withdrawal\': if balance < transaction[\'amount\']: raise InsufficientFundsError(\\"Insufficient funds for withdrawal.\\") balance -= transaction[\'amount\'] valid_count += 1 except (TransactionValidationError, InsufficientFundsError) as e: invalid_count += 1 print(f\\"Transaction error: {e}\\") return { \'valid_count\': valid_count, \'invalid_count\': invalid_count, \'final_balance\': balance }"},{"question":"# Fraction Operations Assessment You are required to implement a class `MyFraction` that mimics the behavior of the `fractions.Fraction` class in Python. This class should handle fraction creation from different types of inputs and support arithmetic operations. Class Definition ```python class MyFraction: def __init__(self, numerator=0, denominator=1): Initialize the fraction from a numerator and denominator. Parameters: numerator (int or str): The numerator of the fraction. denominator (int or str): The denominator of the fraction (default is 1). pass def __add__(self, other): Add two fractions. Parameters: other (MyFraction): Another fraction to add. Returns: MyFraction: The result of the addition. pass def __sub__(self, other): Subtract another fraction from this one. Parameters: other (MyFraction): Another fraction to subtract. Returns: MyFraction: The result of the subtraction. pass def __mul__(self, other): Multiply two fractions. Parameters: other (MyFraction): Another fraction to multiply. Returns: MyFraction: The result of the multiplication. pass def __truediv__(self, other): Divide this fraction by another fraction. Parameters: other (MyFraction): Another fraction to divide by. Returns: MyFraction: The result of the division. pass def as_integer_ratio(self): Return a tuple of two integers whose ratio is equal to the fraction. Returns: tuple: (numerator, denominator) pass @classmethod def from_float(cls, flt): Construct a fraction from a float. Parameters: flt (float): The float to convert. Returns: MyFraction: The resulting fraction. pass def limit_denominator(self, max_denominator=1000000): Find and return the closest fraction to this one with a denominator at most max_denominator. Parameters: max_denominator (int): The maximum allowed denominator. Returns: MyFraction: The approximated fraction. pass def __str__(self): Return a string representation of the fraction in the form \'numerator/denominator\'. Returns: str: The string representation. pass ``` Requirements: 1. The `MyFraction` class should store the numerator and denominator in their lowest terms. 2. The class should raise `ZeroDivisionError` if initialized with a denominator of zero. 3. The arithmetic operations (`__add__`, `__sub__`, `__mul__`, `__truediv__`) should handle operations between two `MyFraction` instances and return a new `MyFraction` instance. 4. Implement `as_integer_ratio` to return a tuple representing the fraction. 5. Implement `from_float` class method to construct a fraction from a floating-point number. 6. Implement `limit_denominator` to approximate the fraction with a denominator at most `max_denominator`. 7. Ensure immutability by not allowing modification of numerator and denominator after initialization. 8. Implement appropriate `__str__` method for string representation of fractions. Example Usage: ```python frac1 = MyFraction(3, 4) frac2 = MyFraction(5, 2) frac3 = frac1 + frac2 print(frac3) # Should display \'19/4\' print(MyFraction.from_float(1.25)) # Should display \'5/4\' print(frac1.limit_denominator(2)) # Should display \'3/4\' since 3/4 is already within limit ``` Constraints: - Numerator and Denominator should be integers. - Denominator should not be zero. - Performance should handle typical arithmetic operations within reasonable time for numerator and denominator values up to 1,000,000,000.","solution":"from math import gcd class MyFraction: def __init__(self, numerator=0, denominator=1): if denominator == 0: raise ZeroDivisionError(\\"Denominator cannot be zero.\\") common = gcd(numerator, denominator) self.numerator = numerator // common self.denominator = denominator // common if self.denominator < 0: self.numerator = -self.numerator self.denominator = -self.denominator def __add__(self, other): if isinstance(other, MyFraction): numerator = self.numerator * other.denominator + other.numerator * self.denominator denominator = self.denominator * other.denominator return MyFraction(numerator, denominator) return NotImplemented def __sub__(self, other): if isinstance(other, MyFraction): numerator = self.numerator * other.denominator - other.numerator * self.denominator denominator = self.denominator * other.denominator return MyFraction(numerator, denominator) return NotImplemented def __mul__(self, other): if isinstance(other, MyFraction): numerator = self.numerator * other.numerator denominator = self.denominator * other.denominator return MyFraction(numerator, denominator) return NotImplemented def __truediv__(self, other): if isinstance(other, MyFraction): if other.numerator == 0: raise ZeroDivisionError(\\"Cannot divide by zero fraction.\\") numerator = self.numerator * other.denominator denominator = self.denominator * other.numerator return MyFraction(numerator, denominator) return NotImplemented def as_integer_ratio(self): return self.numerator, self.denominator @classmethod def from_float(cls, flt): if not isinstance(flt, float): raise TypeError(\\"Expected a float value\\") denominator = 1 while not flt.is_integer(): flt *= 10 denominator *= 10 numerator = int(flt) return cls(numerator, denominator) def limit_denominator(self, max_denominator=1000000): if self.denominator <= max_denominator: return self # Here, we will use a simplistic approach k = (max_denominator + self.denominator - 1) // self.denominator approx_numerator = self.numerator * k common = gcd(approx_numerator, max_denominator) return MyFraction(approx_numerator // common, max_denominator // common) def __str__(self): return f\\"{self.numerator}/{self.denominator}\\""},{"question":"Objective The aim of this task is to evaluate your understanding of parallelism in scikit-learn. You will be required to implement a machine learning model training script that efficiently utilizes the parallelism options provided by scikit-learn. Additionally, you will need to ensure that your solution avoids oversubscription by properly configuring threading and processing environments. Problem Statement Implement a Python function `parallel_model_training` that follows these specifications: 1. **Inputs:** - `data`: A tuple `(X, y)` where `X` is the feature matrix and `y` is the target vector. - `model`: A scikit-learn estimator/classifier object (such as `RandomForestClassifier`). - `n_jobs`: An integer specifying the number of parallel jobs. 2. **Output:** - `trained_model`: The model trained using the provided feature matrix and target vector. 3. **Constraints and Requirements:** - The function should use the `Parallel` class from joblib to enable parallel computation of model training. - The function should use environment variables to limit lower-level parallelism independently to avoid oversubscription. Specifically, set `OMP_NUM_THREADS` and `MKL_NUM_THREADS` to a value that does not lead to oversubscription. - Ensure the use of the most efficient backend provided by `joblib` (either threading or multiprocessing based on the input model). - The training process should be optimized for performance while preventing oversubscription. 4. **Performance Requirement:** - Your solution should be efficient and avoid significant overhead due to improper parallelism settings. - Demonstrate the effectiveness of your solution with proper documentation and code comments explaining the choice of parameters for parallelism control. Example ```python from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import make_classification def parallel_model_training(data, model, n_jobs): import os from joblib import Parallel, parallel_backend # Extracting feature matrix and target vector X, y = data # Set environment variables to avoid oversubscription os.environ[\\"OMP_NUM_THREADS\\"] = \\"2\\" # Set appropriately based on machine specification os.environ[\\"MKL_NUM_THREADS\\"] = \\"2\\" # Set appropriately based on machine specification # Using parallel backend for the training with required number of jobs with parallel_backend(\'loky\', n_jobs=n_jobs): model.fit(X, y) return model # Example usage: X, y = make_classification(n_samples=1000, n_features=20, random_state=42) classifier = RandomForestClassifier(n_estimators=100) trained_classifier = parallel_model_training((X, y), classifier, n_jobs=4) ``` # Deliverables - Implement the `parallel_model_training` function as described. - Document your code with comprehensive comments. - Demonstrate the effectiveness of your function with at least one test case using a synthetic dataset and a chosen scikit-learn estimator.","solution":"def parallel_model_training(data, model, n_jobs): Trains a scikit-learn model using parallelism. Parameters: data (tuple): A tuple (X, y) where X is the feature matrix and y is the target vector. model (sklearn.base.BaseEstimator): A scikit-learn model. n_jobs (int): The number of parallel jobs to use for model training. Returns: sklearn.base.BaseEstimator: The trained model. import os from joblib import Parallel, parallel_backend # Extracting feature matrix and target vector X, y = data # Set environment variables to avoid oversubscription os.environ[\\"OMP_NUM_THREADS\\"] = \\"2\\" # Set to a reasonable value based on machine specification os.environ[\\"MKL_NUM_THREADS\\"] = \\"2\\" # Set to a reasonable value based on machine specification # Using the appropriate parallel backend with parallel_backend(\'loky\', n_jobs=n_jobs): model.fit(X, y) return model"},{"question":"Coding Assessment Question You have been tasked with designing a system to efficiently save and load user session data in a Python application using the `marshal` module. The user session data includes various types of information, such as user settings, histories, and state information, which need to be serialized to a binary file when the session ends and deserialized back when a new session begins. **Task**: Implement two functions: `save_session` and `load_session`. # Function 1: `save_session(session_data: dict, file_path: str) -> None` - **Input**: - `session_data`: A dictionary containing session information. The keys are strings and the values are any of the supported Python types (booleans, integers, floating point numbers, complex numbers, strings, bytes, bytearrays, tuples, lists, sets, frozensets, dictionaries). - `file_path`: The path to the binary file where the session data should be saved. - **Output**: - This function does not return anything. - **Behavior**: 1. Serialize the `session_data` and write it to the specified binary file. 2. Handle any errors that occur due to unsupported types by logging the error and skipping the unsupported data. # Function 2: `load_session(file_path: str) -> dict` - **Input**: - `file_path`: The path to the binary file from which the session data should be loaded. - **Output**: - Returns a dictionary containing the session data. - **Behavior**: 1. Read and deserialize the data from the specified binary file. 2. Handle any errors such as file not found or incompatible marshal data and return an empty dictionary in such cases. # Constraints and Requirements: - You must use the `marshal` module for serialization and deserialization. - The functions should handle errors gracefully and log relevant messages. - The `session_data` dictionary may contain nested dictionaries, lists, tuples, and other supported types. - Performance should be considered - ensure the functions perform the tasks efficiently. # Example: ```python session_data = { \\"user_id\\": 12345, \\"preferences\\": { \\"theme\\": \\"dark\\", \\"notifications\\": True }, \\"history\\": [\\"login\\", \\"view_dashboard\\", \\"logout\\"] } file_path = \\"session_data.bin\\" # Save the session data save_session(session_data, file_path) # Later, load the session data loaded_data = load_session(file_path) print(loaded_data) ``` Example Output: ```python { \'user_id\': 12345, \'preferences\': { \'theme\': \'dark\', \'notifications\': True }, \'history\': [\'login\', \'view_dashboard\', \'logout\'] } ``` Use the example to test and validate your implementation.","solution":"import marshal import logging logging.basicConfig(level=logging.ERROR) def save_session(session_data: dict, file_path: str) -> None: try: with open(file_path, \'wb\') as file: marshal.dump(session_data, file) except ValueError as e: logging.error(f\\"Failed to serialize session data: {e}\\") def load_session(file_path: str) -> dict: try: with open(file_path, \'rb\') as file: return marshal.load(file) except (FileNotFoundError, EOFError, ValueError, TypeError) as e: logging.error(f\\"Failed to load session data: {e}\\") return {}"},{"question":"Implement a dynamic statistics calculator that leverages core Python concepts such as classes, dictionaries, and list comprehensions. Your task is to create a class named `StatisticsCalculator` that can dynamically compute the mean, median, mode, variance, and standard deviation of a list of numbers. Your implementation should be robust and performant. Class: StatisticsCalculator # Methods: 1. **`__init__(self)`** - Initializes the statistics calculator with an empty list of numbers. 2. **`add_number(self, number: float) -> None`** - Adds a number to the list of numbers. - **Input:** A float `number`. - **Output:** None. 3. **`mean(self) -> float`** - Calculates and returns the mean of the numbers. - **Output:** The mean as a float. 4. **`median(self) -> float`** - Calculates and returns the median of the numbers. - **Output:** The median as a float. 5. **`mode(self) -> list`** - Calculates and returns the mode(s) of the numbers. - **Output:** A list of the mode(s). 6. **`variance(self) -> float`** - Calculates and returns the variance of the numbers. - **Output:** The variance as a float. 7. **`standard_deviation(self) -> float`** - Calculates and returns the standard deviation of the numbers. - **Output:** The standard deviation as a float. # Constraints: - Implement the methods without using any external libraries like `numpy` or `statistics`. - You may use built-in functions and methods. - Assume that the `add_number` method will always be called before any statistical method is invoked. # Example: ```python stat_calculator = StatisticsCalculator() stat_calculator.add_number(1) stat_calculator.add_number(2) stat_calculator.add_number(2) stat_calculator.add_number(3) stat_calculator.add_number(4) print(stat_calculator.mean()) # Output: 2.4 print(stat_calculator.median()) # Output: 2.0 print(stat_calculator.mode()) # Output: [2] print(stat_calculator.variance()) # Output: 1.3 print(stat_calculator.standard_deviation()) # Output: 1.140175425099138 ``` Note: Ensure that your code is efficient and handles edge cases such as an empty list where applicable.","solution":"class StatisticsCalculator: def __init__(self): self.numbers = [] def add_number(self, number: float) -> None: self.numbers.append(number) def mean(self) -> float: if not self.numbers: return 0 return sum(self.numbers) / len(self.numbers) def median(self) -> float: if not self.numbers: return 0 sorted_numbers = sorted(self.numbers) mid = len(sorted_numbers) // 2 if len(sorted_numbers) % 2 == 0: return (sorted_numbers[mid - 1] + sorted_numbers[mid]) / 2 else: return sorted_numbers[mid] def mode(self) -> list: if not self.numbers: return [] frequency_map = {} for number in self.numbers: frequency_map[number] = frequency_map.get(number, 0) + 1 max_frequency = max(frequency_map.values()) modes = [key for key, value in frequency_map.items() if value == max_frequency] return modes def variance(self) -> float: if not self.numbers: return 0 mean_value = self.mean() return sum((x - mean_value) ** 2 for x in self.numbers) / len(self.numbers) def standard_deviation(self) -> float: return self.variance() ** 0.5"},{"question":"# Imputation of Missing Values in a Dataset In this exercise, you will be required to handle missing values in a dataset using various imputation strategies provided by scikit-learn. The `SimpleImputer` class will be used for univariate imputation, and the `KNNImputer` class will be used for multivariate imputation. # Problem Statement Given a dataset, you need to write a function that performs the following tasks: 1. Identify and handle missing values using the SimpleImputer with three different strategies: \'mean\', \'median\', and \'most_frequent\'. 2. Use the KNNImputer to handle the missing values. 3. Return the imputed datasets. Function Signature ```python def impute_missing_values(data: np.ndarray) -> Dict[str, np.ndarray]: pass ``` Input - `data` (np.ndarray): A 2D numpy array containing numerical data with some missing values encoded as `np.nan`. Output - A dictionary containing the imputed datasets with the following keys: - `\'mean_imputed\'`: Dataset with missing values imputed using the mean of each column. - `\'median_imputed\'`: Dataset with missing values imputed using the median of each column. - `\'most_frequent_imputed\'`: Dataset with missing values imputed using the most frequent value in each column. - `\'knn_imputed\'`: Dataset with missing values imputed using the k-Nearest Neighbors method with `n_neighbors=3`. Constraints - The input array will have at least one missing value. - You can assume that the input data will not contain categorical features, only numerical features with missing values encoded as `np.nan`. Example ```python import numpy as np data = np.array([ [1, 2, np.nan], [3, np.nan, 3], [np.nan, 6, 5], [8, 8, 7] ]) result = impute_missing_values(data) print(result[\'mean_imputed\']) print(result[\'median_imputed\']) print(result[\'most_frequent_imputed\']) print(result[\'knn_imputed\']) ``` # Implementation Notes 1. **SimpleImputer Usage**: - Use `strategy=\'mean\'`, `strategy=\'median\'`, and `strategy=\'most_frequent\'` for the respective imputations. 2. **KNNImputer Usage**: - Initialize `KNNImputer` with `n_neighbors=3`. 3. You may import other necessary modules or functions from scikit-learn as needed. Complete the function to demonstrate your understanding of how to handle missing data using the provided classes from scikit-learn.","solution":"import numpy as np from sklearn.impute import SimpleImputer, KNNImputer from typing import Dict def impute_missing_values(data: np.ndarray) -> Dict[str, np.ndarray]: imputed_datasets = {} # Mean Imputation mean_imputer = SimpleImputer(strategy=\'mean\') mean_imputed = mean_imputer.fit_transform(data) imputed_datasets[\'mean_imputed\'] = mean_imputed # Median Imputation median_imputer = SimpleImputer(strategy=\'median\') median_imputed = median_imputer.fit_transform(data) imputed_datasets[\'median_imputed\'] = median_imputed # Most Frequent Imputation frequent_imputer = SimpleImputer(strategy=\'most_frequent\') most_frequent_imputed = frequent_imputer.fit_transform(data) imputed_datasets[\'most_frequent_imputed\'] = most_frequent_imputed # KNN Imputation knn_imputer = KNNImputer(n_neighbors=3) knn_imputed = knn_imputer.fit_transform(data) imputed_datasets[\'knn_imputed\'] = knn_imputed return imputed_datasets"},{"question":"# Question **Objective**: You are tasked with creating a dynamic class that implements a custom mapping (similar to a dictionary) but with enforced type checking for keys and values. **Requirements**: 1. The class should be created dynamically using `types.new_class`. 2. The class should have a type-enforced constructor, where the types for keys and values are defined at creation. 3. Implement methods for setting and getting items with appropriate type checks. 4. Provide a read-only proxy to the internal mapping using `types.MappingProxyType`. **Input Format**: - A class name as a string. - Key type (e.g., `str`, `int`) and value type (e.g., `str`, `int`). - A dictionary of initial items to populate the mapping. **Output Format**: - The dynamically created class should have methods to set and get items. - Each item operation must check the types and raise a `TypeError` if they don\'t match the specified types. - The class should also provide a method to get a read-only view of the internal mapping. **Constraints**: - Keys and values can be any of the following types: `int`, `str`, `float`. - The class should gracefully handle incorrect types by raising a `TypeError`. **Performance Requirements**: - The class operations (`__setitem__`, `__getitem__`) should have a time complexity similar to a standard dictionary\'s item operations. # Example Usage ```python # Your task is to fill in the implementation of \'create_custom_mapping_class\' custom_dict_class = create_custom_mapping_class(\'CustomDict\', str, int) # Initialize the class my_dict = custom_dict_class({\'one\': 1, \'two\': 2}) # Set items my_dict[\'three\'] = 3 # works fine my_dict[\'four\'] = \'four\' # raises TypeError # Get items print(my_dict[\'one\']) # outputs 1 print(my_dict[\'three\']) # outputs 3 # Access read-only view ro_view = my_dict.get_read_only_view() print(ro_view[\'one\']) # outputs 1 ro_view[\'one\'] = 10 # raises TypeError (read-only) ``` # Function Signature ```python def create_custom_mapping_class(class_name: str, key_type: type, value_type: type): # Your implementation here pass ``` # Implementation Notes: 1. Use `types.new_class` to dynamically create the class. 2. Define the `__init__`, `__setitem__`, `__getitem__` methods within the class. 3. Use `types.MappingProxyType` to provide the read-only view of the internal mapping. 4. Ensure that key and value types are enforced during item operations.","solution":"import types from types import MappingProxyType def create_custom_mapping_class(class_name: str, key_type: type, value_type: type): def __init__(self, initial_items=None): if initial_items is None: initial_items = {} self._data = {} for k, v in initial_items.items(): self[k] = v # Utilize the __setitem__ method for type checking def __setitem__(self, key, value): if not isinstance(key, key_type): raise TypeError(f\\"Keys must be of type {key_type.__name__}\\") if not isinstance(value, value_type): raise TypeError(f\\"Values must be of type {value_type.__name__}\\") self._data[key] = value def __getitem__(self, key): if key not in self._data: raise KeyError(f\\"Key not found: {key}\\") return self._data[key] def get_read_only_view(self): return MappingProxyType(self._data) class_dict = { \'__init__\': __init__, \'__setitem__\': __setitem__, \'__getitem__\': __getitem__, \'get_read_only_view\': get_read_only_view } return types.new_class(class_name, (object,), {}, lambda ns: ns.update(class_dict))"},{"question":"**Coding Assessment Question:** # Objective: Create a Python function leveraging the `os.path` module to check and report specific details about a given directory and its contents. # Problem Statement: Write a function named `analyze_directory` that accepts a single argument `dir_path` (a string), representing the path to a directory. The function should return a dictionary containing: 1. `absolute_path`: The absolute path of the directory. 2. `subdirectories`: A list of all subdirectories within the directory, relative to the given `dir_path`. 3. `files`: A list of all files within the directory (non-recursive) with their sizes. 4. `common_prefix`: The longest common prefix of all file names in the directory. 5. `info`: A dictionary with keys being file names and values being another dictionary containing: - `size`: The size of the file in bytes. - `created`: The creation time of the file. - `modified`: The last modification time of the file. # Input: - `dir_path` (str): The path to the directory to analyze. # Output: - A dictionary with the structure as specified. # Constraints: - The directory exists and is accessible. - The function should handle paths correctly across different operating systems (cross-platform compatibility). # Example: ```python analyze_directory(\'/example/dir\') # Expected output: { \\"absolute_path\\": \\"/example/dir\\", \\"subdirectories\\": [\\"subdir1\\", \\"subdir2\\"], \\"files\\": [\\"file1.txt\\", \\"file2.log\\"], \\"common_prefix\\": \\"file\\", \\"info\\": { \\"file1.txt\\": {\\"size\\": 1234, \\"created\\": 1621639200, \\"modified\\": 1621642800}, \\"file2.log\\": {\\"size\\": 5678, \\"created\\": 1621639300, \\"modified\\": 1621642900} } } ``` # Notes: - Use appropriate functions from the `os.path` module to achieve the requirements. - The timestamps should be returned as epoch time (number of seconds since January 1, 1970). - Be sure to handle both relative and absolute paths correctly. # Implementation: ```python import os import time def analyze_directory(dir_path): result = {} # 1. Get absolute path absolute_path = os.path.abspath(dir_path) result[\\"absolute_path\\"] = absolute_path subdirectories = [] files = [] file_info = {} # 2. Walk through the directory (non-recursive) with os.scandir(dir_path) as entries: for entry in entries: if entry.is_dir(): subdirectories.append(entry.name) elif entry.is_file(): files.append(entry.name) # Get file information file_stats = entry.stat() file_info[entry.name] = { \\"size\\": file_stats.st_size, \\"created\\": file_stats.st_ctime, \\"modified\\": file_stats.st_mtime } result[\\"subdirectories\\"] = subdirectories result[\\"files\\"] = files # 3. Calculate longest common prefix common_prefix = os.path.commonprefix(files) result[\\"common_prefix\\"] = common_prefix # 4. Include file information result[\\"info\\"] = file_info return result ``` # Test Cases: ```python assert analyze_directory(\'/path/to/dir\') == { \\"absolute_path\\": \\"/path/to/dir\\", \\"subdirectories\\": [\\"docs\\", \\"src\\"], \\"files\\": [\\"README.md\\", \\"file.txt\\"], \\"common_prefix\\": \\"file\\", \\"info\\": { \\"README.md\\": {\\"size\\": 2048, \\"created\\": 1609459200, \\"modified\\": 1609459200}, \\"file.txt\\": {\\"size\\": 1024, \\"created\\": 1609459200, \\"modified\\": 1609459200} } } ```","solution":"import os import time def analyze_directory(dir_path): result = {} # 1. Get absolute path absolute_path = os.path.abspath(dir_path) result[\\"absolute_path\\"] = absolute_path subdirectories = [] files = [] file_info = {} # 2. Walk through the directory (non-recursive) with os.scandir(dir_path) as entries: for entry in entries: if entry.is_dir(): subdirectories.append(entry.name) elif entry.is_file(): files.append(entry.name) # Get file information file_stats = entry.stat() file_info[entry.name] = { \\"size\\": file_stats.st_size, \\"created\\": file_stats.st_ctime, \\"modified\\": file_stats.st_mtime } result[\\"subdirectories\\"] = subdirectories result[\\"files\\"] = files # 3. Calculate longest common prefix common_prefix = os.path.commonprefix(files) result[\\"common_prefix\\"] = common_prefix # 4. Include file information result[\\"info\\"] = file_info return result"},{"question":"Objective: Implement a function `perform_tensor_operations` that demonstrates the creation and manipulation of tensor views in PyTorch, and returns specific tensors based on the operations performed. Instructions: 1. Create a 3x3 tensor with values from 0 to 8. 2. Convert this tensor into a 1x9 view and modify one of its elements. 3. Create a view of the original tensor by transposing it. 4. Ensure the transposed view is contiguous. 5. Return the modified original tensor, the transposed tensor (before and after making it contiguous). Function Signature: ```python import torch def perform_tensor_operations(): # Step 1: Create a 3x3 tensor with values from 0 to 8 original_tensor = torch.arange(9).reshape(3, 3) # Step 2: Convert this tensor into a 1x9 view and modify an element view_tensor = original_tensor.view(1, 9) view_tensor[0][3] = 42 # Arbitrary modification # Step 3: Create a view by transposing the original tensor transposed_tensor_view = original_tensor.transpose(0, 1) # Step 4: Ensure the transposed view is contiguous contiguous_tensor = transposed_tensor_view.contiguous() # Step 5: Return the modified original tensor, the transposed view, and the contiguous tensor return original_tensor, transposed_tensor_view, contiguous_tensor # Example return values (based on provided initial conditions and operations) # original_tensor: # tensor([[ 0, 1, 2], # [42, 4, 5], # [ 6, 7, 8]]) # transposed_tensor_view: # tensor([[ 0, 42, 6], # [ 1, 4, 7], # [ 2, 5, 8]]) # contiguous_tensor: # tensor([[ 0, 42, 6], # [ 1, 4, 7], # [ 2, 5, 8]]) ``` Constraints: - The tensor should be created using PyTorch\'s `torch` module. - Follow the steps strictly as mentioned. - If tensor modifications cause errors, ensure proper troubleshooting within the defined steps. Performance Requirements: - Ensure the function runs efficiently without unnecessary data copies. - Proper utilization of PyTorch\'s view and contiguous methods should be demonstrated. Evaluation Criteria: - Correct and efficient implementation of tensor operations. - Proper handling of tensor views and contiguity. - Functional correctness of the returned tensors.","solution":"import torch def perform_tensor_operations(): # Step 1: Create a 3x3 tensor with values from 0 to 8 original_tensor = torch.arange(9).reshape(3, 3) # Step 2: Convert this tensor into a 1x9 view and modify an element view_tensor = original_tensor.view(1, 9) view_tensor[0, 3] = 42 # Arbitrary modification # Step 3: Create a view by transposing the original tensor transposed_tensor_view = original_tensor.transpose(0, 1) # Step 4: Ensure the transposed view is contiguous contiguous_tensor = transposed_tensor_view.contiguous() # Step 5: Return the modified original tensor, the transposed view, and the contiguous tensor return original_tensor, transposed_tensor_view, contiguous_tensor"},{"question":"# Task Create a visualization using `seaborn.objects` to analyze the \\"penguins\\" dataset. The goal is to produce a multiple subplots visualization with the following requirements: 1. Load the \\"penguins\\" dataset. 2. Create a faceted subplot for each species of penguins (`Adelie`, `Chinstrap`, `Gentoo`). 3. In each subplot, illustrate the distribution of `flipper_length_mm` for male and female penguins, using different colors for different sexes. 4. Include mean values and standard deviation error bars for each group of males and females in the subplots. 5. Customize the markers such that dots indicate individual data points and lines indicate mean values. 6. The subplots should have appropriate titles, axis labels, and legends for clarity. # Input There is no function input for this problem, as you are to write a script that accomplishes the above requirements. # Output A matplotlib figure displaying the required visualization. # Constraints - Use the `seaborn.objects` APIs, such as `so.Plot`, `so.Dot`, `so.Line`, `so.Range`, `so.Dodge()`, and `so.Facet()`. - The script should be efficient and readable, adhering to good coding practices. # Example Your output should be similar to the structure shown in the following image (this is a conceptual illustration, not actual plot output): ``` **************************************** * * * * Adelie * * * * * **************************************** * * * * Chinstrap * * * * * **************************************** * * * * Gentoo * * * * * **************************************** ``` Each of the sections should include the requested visualizations for the `flipper_length_mm` data. # Additional Information Here is an example code snippet to get you started: ```python import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot ( so.Plot(penguins, x=\\"sex\\", y=\\"flipper_length_mm\\", color=\\"sex\\") .facet(\\"species\\") .add(so.Dot(), so.Dodge()) .add(so.Line(marker=\\"o\\"), so.Agg()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) ``` Modify and expand this template to meet the full requirements listed above.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_penguins_visualization(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create a faceted plot plot = ( so.Plot(penguins, x=\\"sex\\", y=\\"flipper_length_mm\\", color=\\"sex\\") .facet(\\"species\\") .add(so.Dot(), so.Dodge()) .add(so.Line(marker=\\"o\\"), so.Agg()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) .label(title=\\"Flipper Length Distribution by Species and Sex\\", ylabel=\\"Flipper Length (mm)\\", xlabel=\\"Sex\\") ) # Show the plot plot.show()"},{"question":"**Question: Task Scheduler Using Topological Sorting** You are tasked with developing a task scheduler using the `graphlib.TopologicalSorter` class. Your scheduler will accept a list of tasks and their dependencies and then return the order in which the tasks should be executed. **Requirements:** 1. **Function Name:** `task_scheduler` 2. **Input:** - `tasks`: A list of tuples where each tuple contains two elements `(task, dependencies)`. - `task`: a string representing the task ID. - `dependencies`: a list of strings representing the task IDs that must be completed before this task. 3. **Output:** - A list of strings representing the tasks in the order they should be executed. If the graph contains cycles (i.e., the task dependencies cannot be resolved), raise a `ValueError` with the message \\"Cycle detected\\". 4. **Constraints:** - Each task ID and dependency is a string of alphanumeric characters. - The graph should always be a directed acyclic graph (DAG) for valid input; if there are cycles, it should be handled appropriately. 5. **Performance:** - The function should be efficient and scalable to handle a large number of tasks and dependencies. **Example:** ```python def task_scheduler(tasks): # Your implementation here # Example usage tasks = [ (\\"build foundation\\", []), (\\"build walls\\", [\\"build foundation\\"]), (\\"install roof\\", [\\"build walls\\"]), (\\"paint house\\", [\\"install roof\\"]), (\\"install doors\\", [\\"build walls\\"]), (\\"install windows\\", [\\"build walls\\"]) ] print(task_scheduler(tasks)) ``` Expected Output: ```python [\'build foundation\', \'build walls\', \'install roof\', \'paint house\', \'install doors\', \'install windows\'] ``` Note: The exact order may vary as long as all dependencies are respected. **Hint:** - Utilize the `graphlib.TopologicalSorter` class to manage and sort the task dependencies.","solution":"from graphlib import TopologicalSorter def task_scheduler(tasks): Determines the order in which tasks should be executed based on their dependencies. :param tasks: List of tuples where each tuple contains a task and its dependencies. :return: List of tasks in the order they should be executed. :raises ValueError: If a cycle is detected in the tasks dependencies. graph = TopologicalSorter() for task, dependencies in tasks: graph.add(task, *dependencies) try: return list(graph.static_order()) except Exception: raise ValueError(\\"Cycle detected\\")"},{"question":"**Advanced PyTorch: Implementing and Testing Symbolic Shape Constraints** # Problem Statement You are tasked with implementing a utility function in PyTorch that validates symbolic shapes on tensors using the PyTorch FX experimental symbolic shape API. Specifically, you need to design a function that will: - Check if tensors conform to given symbolic shape constraints. - Provide a function that can statically check if a shape constraint holds or not. # Function Signature ```python import torch from torch.fx.experimental.symbolic_shapes import ShapeEnv, is_concrete_int, has_free_symbols def validate_shape_constraints(tensors, shape_constraints): Validate the shape of each tensor against the provided shape_constraints. Parameters: tensors (List[torch.Tensor]): A list of PyTorch tensors to be validated. shape_constraints (List[Tuple[int, int]]): A list of tuples, where each tuple corresponds to the (min, max) values for each dimension of the tensors. The length of shape_constraints should match the tensor dimensions. Returns: List[bool]: A list of booleans where each boolean value represents if the corresponding tensor complies with the provided shape constraints. pass ``` # Expected Input and Output 1. `tensors`: A list of PyTorch tensors that need shape validation. 2. `shape_constraints`: A list of tuples representing the minimum and maximum allowable sizes for each dimension of the tensors. **Output**: A list of boolean values indicating whether each tensor adheres to the constraints. # Constraints - Each tensor\'s dimensions should be compared against the provided constraints. - Use the symbolic shape environment (`ShapeEnv`) for validation. - Implement necessary utility checks like `is_concrete_int` and `has_free_symbols`. - Consider edge cases with symbolic dimensions that may not have concrete values. # Example Usage ```python import torch tensors = [ torch.rand(5, 10), torch.rand(3, 8), torch.rand(7, 12) ] # Constraints indicating that each tensor should have dimensions within the provided ranges. shape_constraints = [(3, 8), (7, 12)] # Expected Output: [True, True, False] since only the last tensor does not conform to second dimension constraints. print(validate_shape_constraints(tensors, shape_constraints)) ``` # Requirements - A tensor shape validation logic that accommodates both concrete and symbolic dimensions. - Ensure the function is performant and handles invalid inputs gracefully. # Additional Information Refer to the PyTorch FX experimental symbolic shape documentation for details on how to utilize `ShapeEnv` and other accompanying utilities.","solution":"import torch from torch.fx.experimental.symbolic_shapes import ShapeEnv, is_concrete_int, has_free_symbols def validate_shape_constraints(tensors, shape_constraints): Validate the shape of each tensor against the provided shape constraints. Parameters: tensors (List[torch.Tensor]): A list of PyTorch tensors to be validated. shape_constraints (List[Tuple[int, int]]): A list of tuples, where each tuple corresponds to the (min, max) values for each dimension of the tensors. The length of shape_constraints should match the tensor dimensions. Returns: List[bool]: A list of booleans where each boolean value represents if the corresponding tensor complies with the provided shape constraints. results = [] for tensor in tensors: tensor_shape = tensor.shape if len(tensor_shape) != len(shape_constraints): results.append(False) continue valid = True for dim, constraint in zip(tensor_shape, shape_constraints): min_dim, max_dim = constraint if not (min_dim <= dim <= max_dim): valid = False break results.append(valid) return results"},{"question":"# **Asyncio Transports and Protocols Assessment Question** **Objective:** Implement a custom echo protocol that handles both TCP and UDP connections using asyncio\'s low-level transport and protocol APIs. You are required to write an asyncio application that can handle multiple clients concurrently and echo the received messages back to the client. The application should support both TCP and UDP communication. **Task Overview:** 1. Implement a `CustomEchoProtocol` that handles connections, data reception, and connection loss for both TCP and UDP using the `Protocol` and `DatagramProtocol` classes. 2. Implement a `create_echo_server` function to create and start a TCP echo server using `loop.create_server()`. 3. Implement a `create_echo_datagram_server` function to create and start a UDP echo server using `loop.create_datagram_endpoint()`. 4. Run both servers concurrently using asyncio. **Specifications:** 1. **CustomEchoProtocol (TCP)** - Implement a class `CustomEchoProtocol` derived from `asyncio.Protocol`. - Implement `connection_made`, `data_received`, and `connection_lost` methods. 2. **CustomDatagramEchoProtocol (UDP)** - Implement a class `CustomDatagramEchoProtocol` derived from `asyncio.DatagramProtocol`. - Implement `connection_made`, `datagram_received`, and `connection_lost` methods. 3. **TCP Echo Server** - Use `loop.create_server()` to start the TCP echo server. 4. **UDP Echo Server** - Use `loop.create_datagram_endpoint()` to start the UDP echo server. 5. **Main Function** - Run both TCP and UDP servers concurrently using asyncio. **Input/Output:** - There is no direct input to the application; it listens on a specified port for incoming connections. - The output should be printed to the console, showing the received and sent back messages for both TCP and UDP connections. **Example:** ```python import asyncio class CustomEchoProtocol(asyncio.Protocol): def __init__(self): self.transport = None def connection_made(self, transport): self.transport = transport print(\'TCP Connection established\') def data_received(self, data): message = data.decode() print(f\'TCP Data received: {message}\') self.transport.write(data) def connection_lost(self, exc): print(\'TCP Connection lost\') class CustomDatagramEchoProtocol(asyncio.DatagramProtocol): def connection_made(self, transport): self.transport = transport print(\'UDP Connection established\') def datagram_received(self, data, addr): message = data.decode() print(f\'UDP Data received from {addr}: {message}\') self.transport.sendto(data, addr) def error_received(self, exc): print(f\'Error received: {exc}\') def connection_lost(self, exc): print(\'UDP Connection lost\') async def create_echo_server(host, port): loop = asyncio.get_running_loop() server = await loop.create_server(lambda: CustomEchoProtocol(), host, port) return server async def create_echo_datagram_server(host, port): loop = asyncio.get_running_loop() transport, protocol = await loop.create_datagram_endpoint(lambda: CustomDatagramEchoProtocol(), local_addr=(host, port)) return transport, protocol async def main(): tcp_server = await create_echo_server(\'127.0.0.1\', 8888) udp_transport, udp_protocol = await create_echo_datagram_server(\'127.0.0.1\', 9999) try: await asyncio.sleep(3600) # Run servers for 1 hour finally: tcp_server.close() await tcp_server.wait_closed() udp_transport.close() asyncio.run(main()) ``` Your implementation should follow the above guidelines and structure. Add appropriate print statements to trace the execution flow and confirm that the echo functionality is working as expected. **Constraints:** - Ensure the solution handles multiple concurrent client connections. - The system should not crash on malformed data. **Performance Requirements:** - The TCP server should handle at least 1000 concurrent connections. - The UDP server should handle a high number of UDP datagrams per second.","solution":"import asyncio class CustomEchoProtocol(asyncio.Protocol): def __init__(self): self.transport = None def connection_made(self, transport): self.transport = transport print(\'TCP Connection established\') def data_received(self, data): message = data.decode() print(f\'TCP Data received: {message}\') self.transport.write(data) def connection_lost(self, exc): print(\'TCP Connection lost\') class CustomDatagramEchoProtocol(asyncio.DatagramProtocol): def connection_made(self, transport): self.transport = transport print(\'UDP Connection established\') def datagram_received(self, data, addr): message = data.decode() print(f\'UDP Data received from {addr}: {message}\') self.transport.sendto(data, addr) def error_received(self, exc): print(f\'Error received: {exc}\') def connection_lost(self, exc): print(\'UDP Connection lost\') async def create_echo_server(host, port): loop = asyncio.get_running_loop() server = await loop.create_server(lambda: CustomEchoProtocol(), host, port) return server async def create_echo_datagram_server(host, port): loop = asyncio.get_running_loop() transport, protocol = await loop.create_datagram_endpoint(lambda: CustomDatagramEchoProtocol(), local_addr=(host, port)) return transport, protocol async def main(): tcp_server = await create_echo_server(\'127.0.0.1\', 8888) udp_transport, udp_protocol = await create_echo_datagram_server(\'127.0.0.1\', 9999) try: await asyncio.sleep(3600) # Run servers for 1 hour finally: tcp_server.close() await tcp_server.wait_closed() udp_transport.close() if __name__ == \'__main__\': asyncio.run(main())"},{"question":"**Part 1: Sequence Analysis and Transformation** You are given a list of integers and your task is to write a Python function that performs the following operations: 1. Identify and separate even and odd numbers. 2. Calculate the sum of squares of the even numbers. 3. Sort the odd numbers in descending order. 4. Return a dictionary with two keys: - `\\"sum_of_even_squares\\"`: which will hold the sum from step 2. - `\\"sorted_odds\\"`: which will hold the sorted list from step 3. **Function Signature:** ```python def sequence_analysis(numbers: list[int]) -> dict[str, any]: pass ``` **Input:** - `numbers`: A list of integers which can contain both positive and negative integers. It is guaranteed to have at least one integer. **Output:** - A dictionary with two keys: - `\\"sum_of_even_squares\\"`: an integer representing the sum of squares of the even numbers. - `\\"sorted_odds\\"`: a list of integers representing the odd numbers sorted in descending order. **Constraints:** - The input list can contain up to `10^6` integers. - The integers can range from `-10^9` to `10^9`. **Performance Requirements:** - Your function should aim to handle the operations efficiently, considering the constraints provided. **Example:** ```python numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] output = sequence_analysis(numbers) print(output) # Output: {\\"sum_of_even_squares\\": 220, \\"sorted_odds\\": [9, 7, 5, 3, 1]} ``` **Additional Information:** - Use Python\'s built-in functions and types wisely. - Consider edge cases such as lists containing repeated numbers and lists with a high magnitude of positive and negative numbers. **Hint:** To achieve efficient performance, consider single-pass operations where possible and leverage Python\'s inherent optimizations for built-in types.","solution":"def sequence_analysis(numbers: list[int]) -> dict[str, any]: evens_sum_squares = sum(x**2 for x in numbers if x % 2 == 0) sorted_odds = sorted((x for x in numbers if x % 2 != 0), reverse=True) return { \\"sum_of_even_squares\\": evens_sum_squares, \\"sorted_odds\\": sorted_odds }"},{"question":"**Question 1: Initialization and Collective Operations in `torch.distributed`** You are tasked to implement a distributed training setup using PyTorch\'s `torch.distributed` module. Write a script that: 1. Initializes a distributed process group with the `gloo` backend. 2. Creates a tensor containing rank-specific data on each process. 3. Uses collective operations to perform the following: - Broadcasts a tensor from the process with rank 0 to all other processes. - Performs an all-reduce operation to sum the tensors from all processes. 4. Gathers the individual tensors from all processes to the process with rank 0. 5. Properly handles the shutdown and cleanup of the process group. **Constraints**: - The number of processes (`world_size`) and the rank of each process should be passed as arguments to the script. - The script should handle setting up the master address and master port using environment variables. **Input**: - `world_size`: The total number of processes. - `rank`: The rank of the current process. **Output**: - Print the tensors at each step for the process with rank 0 to verify the correctness. **Notes**: - Use a shared file-system initialization method for setting up the process group. - Ensure that all processes properly synchronize at each step of the collective operations. **Sample Code Skeleton**: ```python import os import torch import torch.distributed as dist def main(rank, world_size): # Initialize the process group with \'gloo\' backend dist.init_process_group(backend=\'gloo\', init_method=\'file:///path/to/shared/file\', world_size=world_size, rank=rank) # Create a tensor containing rank-specific data tensor = torch.tensor([rank], dtype=torch.float32) # Broadcast the tensor from rank 0 to all processes if rank == 0: tensor = torch.tensor([42.0], dtype=torch.float32) # Example data from rank 0 dist.broadcast(tensor, src=0) if rank == 0: print(f\\"Broadcasted tensor: {tensor}\\") # Perform all-reduce operation to sum the tensors dist.all_reduce(tensor, op=dist.ReduceOp.SUM) if rank == 0: print(f\\"All-reduced tensor: {tensor}\\") # Gather the individual tensors tensor_list = [torch.zeros_like(tensor) for _ in range(world_size)] dist.gather(tensor, gather_list=tensor_list, dst=0) if rank == 0: print(f\\"Gathered tensors: {tensor_list}\\") # Cleanup dist.destroy_process_group() if __name__ == \\"__main__\\": world_size = int(os.environ[\'WORLD_SIZE\']) rank = int(os.environ[\'RANK\']) main(rank, world_size) ``` Replace `/path/to/shared/file` with an appropriate file path that can be accessed by all processes. This is necessary for shared file-system initialization. Ensure that the environment variables `WORLD_SIZE` and `RANK` are set appropriately before running the script.","solution":"import os import torch import torch.distributed as dist def main(rank, world_size): # Initialize the process group with \'gloo\' backend dist.init_process_group(backend=\'gloo\', init_method=\'file:///tmp/sharedfile\', world_size=world_size, rank=rank) # Create a tensor containing rank-specific data tensor = torch.tensor([rank], dtype=torch.float32) # Broadcast the tensor from rank 0 to all processes if rank == 0: tensor = torch.tensor([42.0], dtype=torch.float32) # Example data from rank 0 dist.broadcast(tensor, src=0) if rank == 0: print(f\\"Broadcasted tensor: {tensor}\\") # Perform all-reduce operation to sum the tensors dist.all_reduce(tensor, op=dist.ReduceOp.SUM) if rank == 0: print(f\\"All-reduced tensor: {tensor}\\") # Gather the individual tensors tensor_list = [torch.zeros_like(tensor) for _ in range(world_size)] dist.gather(tensor, gather_list=tensor_list, dst=0) if rank == 0: print(f\\"Gathered tensors: {tensor_list}\\") # Cleanup dist.destroy_process_group() if __name__ == \\"__main__\\": world_size = int(os.environ[\'WORLD_SIZE\']) rank = int(os.environ[\'RANK\']) main(rank, world_size)"},{"question":"# Python Coding Assessment Question Background: You are tasked with developing a utility function that processes a list of transactions and outputs a summary. Each transaction is a dictionary with various fields including `type`, `amount`, and `currency`. Transactions can be either valid or invalid, and you must handle exceptions, use appropriate control flow statements, and leverage pattern matching for processing different transaction types. Additionally, your function should support asynchronous processing of transactions. Task: Write a Python function `process_transactions(transactions)` that takes a list of transactions and returns a summary dictionary. The function should: 1. **Use `async` for processing transactions asynchronously.** 2. **Use pattern matching to handle different transaction types (`deposit`, `withdrawal`, `transfer`).** 3. **Handle exceptions for invalid transactions and log them.** 4. **Use `with` statements for handling any setup/teardown required (mock this with print statements).** 5. **Summarize the total amount by currency, and the count of each type of transaction.** Input: - `transactions`: A list of dictionaries. Each dictionary represents a transaction and must have the following format: ```python { \'type\': <\'deposit\'|\'withdrawal\'|\'transfer\'>, \'amount\': <float>, \'currency\': <\'USD\'|\'EUR\'|\'GBP\'>, \'valid\': <True|False> } ``` Output: - A dictionary with the total amounts and count of each transaction type by currency. Example: ```python { \'USD\': {\'total\': 1500.0, \'deposit\': 2, \'withdrawal\': 1, \'transfer\': 0}, \'EUR\': {\'total\': 900.0, \'deposit\': 1, \'withdrawal\': 0, \'transfer\': 2}, \'GBP\': {\'total\': 700.0, \'deposit\': 0, \'withdrawal\': 2, \'transfer\': 1} } ``` Constraints: 1. Assume a maximum of 1000 transactions. 2. Only valid transactions should be included in the summary. 3. Ensure the use of `try`/`except` blocks to handle invalid transactions. 4. Use `async` where appropriate to demonstrate handling of asynchronous code. Example: ```python import asyncio async def process_transactions(transactions): summary = {\'USD\': {\'total\': 0, \'deposit\': 0, \'withdrawal\': 0, \'transfer\': 0}, \'EUR\': {\'total\': 0, \'deposit\': 0, \'withdrawal\': 0, \'transfer\': 0}, \'GBP\': {\'total\': 0, \'deposit\': 0, \'withdrawal\': 0, \'transfer\': 0}} async def process_transaction(transaction): try: if not transaction[\'valid\']: raise ValueError(\\"Invalid transaction\\") match transaction: case {\'type\': \'deposit\', \'amount\': amount, \'currency\': currency}: with print(\'Entering deposit context\'): summary[currency][\'total\'] += amount summary[currency][transaction[\'type\']] += 1 case {\'type\': \'withdrawal\', \'amount\': amount, \'currency\': currency}: with print(\'Entering withdrawal context\'): summary[currency][\'total\'] -= amount summary[currency][transaction[\'type\']] += 1 case {\'type\': \'transfer\', \'amount\': amount, \'currency\': currency}: with print(\'Entering transfer context\'): summary[currency][\'total\'] -= amount summary[currency][transaction[\'type\']] += 1 case _: raise ValueError(\\"Unknown transaction type\\") except Exception as e: print(f\\"Error processing transaction: {e}\\") await asyncio.gather(*(process_transaction(t) for t in transactions)) return summary # Example usage: transactions = [ {\'type\': \'deposit\', \'amount\': 1000.0, \'currency\': \'USD\', \'valid\': True}, {\'type\': \'withdrawal\', \'amount\': 500.0, \'currency\': \'USD\', \'valid\': True}, {\'type\': \'transfer\', \'amount\': 700.0, \'currency\': \'GBP\', \'valid\': False}, # Invalid {\'type\': \'deposit\', \'amount\': 400.0, \'currency\': \'EUR\', \'valid\': True}, ] print(asyncio.run(process_transactions(transactions))) # Expected output: # { # \'USD\': {\'total\': 500.0, \'deposit\': 1, \'withdrawal\': 1, \'transfer\': 0}, # \'EUR\': {\'total\': 400.0, \'deposit\': 1, \'withdrawal\': 0, \'transfer\': 0}, # \'GBP\': {\'total\': 0.0, \'deposit\': 0, \'withdrawal\': 0, \'transfer\': 0} # } ``` Notes: - Make sure to validate the transaction structure. - Implement the solution to handle potential errors gracefully. - Show usage of `async with`, `match`, `with`, and `try` statements. Good luck!","solution":"import asyncio async def process_transactions(transactions): summary = { \'USD\': {\'total\': 0, \'deposit\': 0, \'withdrawal\': 0, \'transfer\': 0}, \'EUR\': {\'total\': 0, \'deposit\': 0, \'withdrawal\': 0, \'transfer\': 0}, \'GBP\': {\'total\': 0, \'deposit\': 0, \'withdrawal\': 0, \'transfer\': 0} } async def process_transaction(transaction): try: if not transaction[\'valid\']: raise ValueError(\\"Invalid transaction\\") match transaction: case {\'type\': \'deposit\', \'amount\': amount, \'currency\': currency} if currency in summary: with print(\'Entering deposit context\'): summary[currency][\'total\'] += amount summary[currency][\'deposit\'] += 1 case {\'type\': \'withdrawal\', \'amount\': amount, \'currency\': currency} if currency in summary: with print(\'Entering withdrawal context\'): summary[currency][\'total\'] -= amount summary[currency][\'withdrawal\'] += 1 case {\'type\': \'transfer\', \'amount\': amount, \'currency\': currency} if currency in summary: with print(\'Entering transfer context\'): summary[currency][\'total\'] -= amount summary[currency][\'transfer\'] += 1 case _: raise ValueError(\\"Unknown transaction type or invalid currency\\") except Exception as e: print(f\\"Error processing transaction: {e}\\") await asyncio.gather(*(process_transaction(t) for t in transactions)) return summary # Example usage transactions = [ {\'type\': \'deposit\', \'amount\': 1000.0, \'currency\': \'USD\', \'valid\': True}, {\'type\': \'withdrawal\', \'amount\': 500.0, \'currency\': \'USD\', \'valid\': True}, {\'type\': \'transfer\', \'amount\': 700.0, \'currency\': \'GBP\', \'valid\': False}, # Invalid {\'type\': \'deposit\', \'amount\': 400.0, \'currency\': \'EUR\', \'valid\': True}, ] print(asyncio.run(process_transactions(transactions))) # Expected output: # { # \'USD\': {\'total\': 500.0, \'deposit\': 1, \'withdrawal\': 1, \'transfer\': 0}, # \'EUR\': {\'total\': 400.0, \'deposit\': 1, \'withdrawal\': 0, \'transfer\': 0}, # \'GBP\': {\'total\': 0.0, \'deposit\': 0, \'withdrawal\': 0, \'transfer\': 0} # }"},{"question":"**Question: Implement a CookieManager class** Create a `CookieManager` class that utilizes the `http.cookies` module to manage HTTP cookies. The class should include methods to add cookies, retrieve cookies, delete cookies, and output cookies as HTTP headers. **Requirements:** 1. **Class Definition**: - Define a class `CookieManager`. 2. **Initialization**: - The class should initialize an instance of `http.cookies.SimpleCookie`. 3. **Methods**: - `add_cookie(name: str, value: str, attributes: Optional[Dict[str, str]] = None) -> None`: Adds a new cookie with the given name and value. Optional attributes can be provided as a dictionary, e.g., `{\\"path\\": \\"/\\", \\"max-age\\": \\"3600\\"}`. - `get_cookie(name: str) -> str`: Retrieves the value of the cookie with the given name. If the cookie does not exist, return `None`. - `delete_cookie(name: str) -> None`: Deletes the cookie with the given name. - `output_cookies(header: str = \\"Set-Cookie:\\", sep: str = \\"rn\\") -> str`: Returns all cookies as a single string suitable for HTTP headers with the provided header and separator. **Example Usage:** ```python from typing import Optional, Dict from http import cookies class CookieManager: def __init__(self): self.cookie_jar = cookies.SimpleCookie() def add_cookie(self, name: str, value: str, attributes: Optional[Dict[str, str]] = None) -> None: self.cookie_jar[name] = value if attributes: for key, val in attributes.items(): self.cookie_jar[name][key] = val def get_cookie(self, name: str) -> Optional[str]: if name in self.cookie_jar: return self.cookie_jar[name].value return None def delete_cookie(self, name: str) -> None: if name in self.cookie_jar: del self.cookie_jar[name] def output_cookies(self, header: str = \\"Set-Cookie:\\", sep: str = \\"rn\\") -> str: return self.cookie_jar.output(header=header, sep=sep) # Example usage manager = CookieManager() manager.add_cookie(\\"session\\", \\"abc123\\", {\\"path\\": \\"/\\", \\"max-age\\": \\"3600\\"}) print(manager.get_cookie(\\"session\\")) # Output: \\"abc123\\" manager.delete_cookie(\\"session\\") print(manager.get_cookie(\\"session\\")) # Output: None print(manager.output_cookies()) # Output: \\"\\" ``` **Constraints:** - You are not allowed to use any other cookie management libraries. - Ensure that the methods handle invalid input gracefully by not raising any unhandled exceptions. **Notes for Students:** - Carefully read the `http.cookies` module documentation provided. - Think about different edge cases and handle them appropriately in your implementation. This exercise will test your ability to work with classes, dictionaries, and optional attributes, as well as your understanding of HTTP and cookie management in Python.","solution":"from typing import Optional, Dict from http import cookies class CookieManager: def __init__(self): self.cookie_jar = cookies.SimpleCookie() def add_cookie(self, name: str, value: str, attributes: Optional[Dict[str, str]] = None) -> None: self.cookie_jar[name] = value if attributes: for key, val in attributes.items(): self.cookie_jar[name][key] = val def get_cookie(self, name: str) -> Optional[str]: if name in self.cookie_jar: return self.cookie_jar[name].value return None def delete_cookie(self, name: str) -> None: if name in self.cookie_jar: del self.cookie_jar[name] def output_cookies(self, header: str = \\"Set-Cookie:\\", sep: str = \\"rn\\") -> str: return self.cookie_jar.output(header=header, sep=sep)"},{"question":"You have been tasked with designing a visual representation of data with a custom color palette using Seaborn. # Question **Objective:** Create a function called `generate_custom_colormap` that blends a list of given colors to generate a continuous colormap and then uses this colormap to plot a heatmap of a provided 2D dataset. **Function Signature:** ```python def generate_custom_colormap(colors: list, data: list) -> None: pass ``` **Input:** 1. `colors` (list): A list of strings where each string represents a color. Colors can be in any legal matplotlib format (e.g., color names, hex color codes, xkcd color names). 2. `data` (list): A 2D list (list of lists), where each inner list represents a row in a matrix, containing numerical values to be plotted in the heatmap. **Output:** - The function should not return anything but should display a Seaborn heatmap on execution. **Constraints:** - `colors` list must contain at least two colors. - Each inner list in `data` must have the same length. - The dataset\'s size should be reasonable for visualization purposes (e.g., a maximum of 50x50). **Example:** ```python colors = [\\"#45a872\\", \\"blue\\", \\"xkcd:golden\\"] data = [ [1, 2, 3], [4, 5, 6], [7, 8, 9] ] generate_custom_colormap(colors, data) ``` **Explanation:** 1. The function should first blend the provided colors into a continuous colormap. 2. It should then use this colormap to create a heatmap of the given 2D dataset. You can rely on the following Seaborn functions: - `sns.blend_palette(colors, as_cmap=True)`: To create the continuous colormap. - `sns.heatmap(data, cmap=cmap)`: To plot the heatmap using the generated colormap. **Hint:** Remember to import the necessary libraries before implementing your function: ```python import seaborn as sns import matplotlib.pyplot as plt sns.set_theme() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_custom_colormap(colors, data): Generates a custom colormap from a list of colors and plots a heatmap of the given 2D dataset. Parameters: colors (list): List of colors to be used in the colormap. data (list): 2D list of numerical values to be plotted in the heatmap. Returns: None if len(colors) < 2: raise ValueError(\\"The colors list must contain at least two colors.\\") row_lengths = set(len(row) for row in data) if len(row_lengths) != 1: raise ValueError(\\"All rows in the data must have the same length.\\") if len(data) > 50 or len(data[0]) > 50: raise ValueError(\\"The dataset\'s size should be reasonable for visualization purposes (maximum 50x50).\\") # Generate the colormap cmap = sns.blend_palette(colors, as_cmap=True) # Create the heatmap sns.heatmap(data, cmap=cmap) plt.show()"},{"question":"Advanced Violin Plot Customization with Seaborn Objective Create a custom violin plot using Seaborn that demonstrates your understanding of the various features and customization options available for this visualization. The intended goals are to assess your knowledge of data manipulation with pandas, usage of Seaborn for creating sophisticated plots, and the ability to interpret the resulting visualizations. Problem Statement Write a Python function called `custom_violin_plot` that: 1. Loads the Titanic dataset using `seaborn.load_dataset(\\"titanic\\")`. 2. Creates a violin plot to visualize the age distribution across different classes (i.e., socio-economic classes). 3. Customizes the plot to show: - Separate distributions for those who survived and those who didn\'t (`hue=\\"alive\\"`). - Split violins by survival status to save space (`split=True`). - Quartile markers inside the violins (`inner=\\"quart\\"`). - Line-art violins without fill (`fill=False`). - Custom density normalization so that the width of each violin represents the number of observations (`density_norm=\\"count\\"`). Additionally, adjust the aesthetics of the plot to improve readability: - Set the title of the plot to \\"Age Distribution by Class and Survival Status\\". - Label the x-axis as \\"Class\\" and the y-axis as \\"Age\\". Constraints - You must use the Seaborn library for creating the violin plot. - Your function should not directly print any output other than the plot itself. - Ensure the plot is clear, well-labeled, and aesthetically pleasing. # Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt def custom_violin_plot(): pass ``` # Expected Output The function should produce a well-structured violin plot following the specifications above. Here\'s an example of how the axes and plot might look: ![Expected Violin Plot](https://via.placeholder.com/600x400.png) # Example ```python custom_violin_plot() ``` Executing the function should display the violin plot as described in the problem statement. Notes - You may refer to the necessary Seaborn documentation for details on the parameters. - Ensure to include necessary inline comments for clarity.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_violin_plot(): # Load the Titanic dataset data = sns.load_dataset(\\"titanic\\") # Create the violin plot plt.figure(figsize=(10, 6)) sns.violinplot(x=\\"class\\", y=\\"age\\", hue=\\"alive\\", data=data, split=True, inner=\\"quart\\", fill=False, scale=\\"count\\") # Customize the plot aesthetics plt.title(\\"Age Distribution by Class and Survival Status\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") plt.legend(title=\\"Survival Status\\") # Show the plot plt.show()"},{"question":"# Question: Asynchronous Chat Server with Multiple Clients Using the `asyncio` library, implement a multi-client chat server. This server will allow multiple clients to connect, send messages to the server, and have those messages broadcast to all connected clients. Your task involves creating custom transports and protocols to handle the server and client communication. Requirements: 1. **Server**: - Use the `loop.create_server` method to create a TCP server. - Manage multiple client connections and broadcast received messages to all connected clients. - Implement custom protocol classes to handle connections, data receipt, and broadcasting. 2. **Client**: - Implement a client that connects to the server, sends messages, and prints messages broadcasted from the server. # Implementing the Server 1. Create a `ChatServerProtocol` class that inherits from `asyncio.Protocol`. This class should: - Handle new connections, store references to transport objects, and manage a list of connected clients. - Receive data from clients and broadcast it to all connected clients. - Handle connection loss/removal of clients. 2. Use the `loop.create_server` method to start the server, binding it to `localhost` on port `8888`. # Implementing the Client 3. Create a `ChatClientProtocol` class that inherits from `asyncio.Protocol`. This class should: - Connect to the server and send user input messages. - Print messages received from the server. # Input/Output - **Server**: - Input: Start the server and it listens for client connections. - Output: Broadcast messages received from one client to all other connected clients. - **Client**: - Input: Connect to the server, send user input messages. - Output: Print messages broadcasted by the server. # Constraints - Use the `asyncio` package only. - Make sure proper cleanup is done when connections are lost. - Implement error handling for network issues. # Example Given that: - The server is running and listening on `localhost:8888`. - Multiple clients connect and interact with the server. Example interaction: 1. **Client 1** sends \\"Hello, world!\\". 2. **Client 2** receives \\"Client 1: Hello, world!\\". 3. **Client 2** sends \\"Hi there!\\". 4. **Client 1** receives \\"Client 2: Hi there!\\". ```python # Sample code to start: import asyncio class ChatServerProtocol(asyncio.Protocol): def __init__(self): self.clients = [] def connection_made(self, transport): self.clients.append(transport) print(\\"New connection made.\\") def data_received(self, data): message = data.decode() print(f\\"Message received: {message}\\") for client in self.clients: if client != self.transport: client.write(data) def connection_lost(self, exc): self.clients.remove(self.transport) print(\\"Connection lost.\\") async def main(): loop = asyncio.get_running_loop() server = await loop.create_server(lambda: ChatServerProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() asyncio.run(main()) ``` Note: - This is a starting point. You need to extend the implementation to handle multiple clients effectively and maintain a proper broadcasting mechanism. - Implement the client-side code similarly using a `ChatClientProtocol` that connects and communicates with the server.","solution":"import asyncio class ChatServerProtocol(asyncio.Protocol): def __init__(self): self.transport = None def connection_made(self, transport): self.transport = transport self.clients.append(self) print(\\"New connection made.\\") def data_received(self, data): message = data.decode() print(f\\"Message received: {message}\\") self.broadcast_message(message) def connection_lost(self, exc): self.clients.remove(self) print(\\"Connection lost.\\") def broadcast_message(self, message): for client in self.clients: if client != self: client.transport.write(message.encode()) ChatServerProtocol.clients = [] async def main(): loop = asyncio.get_running_loop() server = await loop.create_server(lambda: ChatServerProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() class ChatClientProtocol(asyncio.Protocol): def __init__(self, on_con_lost, message): self.on_con_lost = on_con_lost self.message = message def connection_made(self, transport): transport.write(self.message.encode()) print(\'Message sent: {!r}\'.format(self.message)) def data_received(self, data): print(\'Data received: {!r}\'.format(data.decode())) def connection_lost(self, exc): print(\'The server closed the connection\') self.on_con_lost.set_result(True) async def tcp_echo_client(message): loop = asyncio.get_running_loop() on_con_lost = loop.create_future() transport, protocol = await loop.create_connection( lambda: ChatClientProtocol(on_con_lost, message), \'127.0.0.1\', 8888) try: await on_con_lost finally: transport.close() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"You are tasked with writing a function that simulates a traffic control system at a single-lane bridge, allowing cars from either end to cross one at a time. The crossing should be coordinated so that no accidents occur, meaning cars from opposite sides should not be on the bridge at the same time. To achieve this, you will use asyncio synchronization primitives from the `asyncio` module. Each car is represented by an asyncio task which tries to access the bridge. The tasks should be controlled to ensure mutual exclusion on the bridge. # Requirements: 1. Implement an `async` function `traffic_controller(car_directions: List[str]) -> Tuple[int, int]` where: - `car_directions` is a list of strings, where each string is either `\'N\'` for a car coming from the north or `\'S\'` for a car coming from the south. - The function should return a tuple `(north_cars_crossed, south_cars_crossed)` representing the number of cars from the north and the south that successfully crossed the bridge. 2. Use appropriate asyncio synchronization primitives (`Lock`, `Event`, `Condition`, `Semaphore`, or `BoundedSemaphore`) to ensure that: - Only one car is on the bridge at any time. - Cars from opposite directions do not enter the bridge simultaneously. 3. Simulate the crossing time of each car with `await asyncio.sleep(random.uniform(0.1, 0.5))` to represent the time it takes for a car to cross the bridge. # Input Format: - `car_directions`: a list of strings denoting the direction each car is coming from, where each string is either `\'N\'` or `\'S\'`. # Output Format: - A tuple containing two integers: `(north_cars_crossed, south_cars_crossed)`, indicating the number of cars from each direction that have successfully crossed the bridge. # Example: ```python import asyncio import random from typing import List, Tuple # Function you need to implement async def traffic_controller(car_directions: List[str]) -> Tuple[int, int]: # Your implementation here pass async def simulate_traffic(): car_directions = [\'N\', \'S\', \'N\', \'N\', \'S\', \'S\'] result = await traffic_controller(car_directions) print(result) # Output example: (3, 3) # Simulate traffic control system asyncio.run(simulate_traffic()) ``` # Constraints: - Each car must wait its turn to enter the bridge if another car is currently crossing. - The bridge control logic must ensure that the system remains fair and no cars are starved from crossing. # Notes: - The example provided simulates a sequence of cars trying to cross the bridge, and the expected output indicates the successful count from each direction after all cars have attempted to cross. - You may define additional helper functions if needed.","solution":"import asyncio import random from typing import List, Tuple async def traffic_controller(car_directions: List[str]) -> Tuple[int, int]: bridge_lock = asyncio.Lock() north_cars_crossed = 0 south_cars_crossed = 0 async def car_from_north(): nonlocal north_cars_crossed async with bridge_lock: await asyncio.sleep(random.uniform(0.1, 0.5)) north_cars_crossed += 1 async def car_from_south(): nonlocal south_cars_crossed async with bridge_lock: await asyncio.sleep(random.uniform(0.1, 0.5)) south_cars_crossed += 1 tasks = [] for direction in car_directions: if direction == \'N\': tasks.append(asyncio.create_task(car_from_north())) elif direction == \'S\': tasks.append(asyncio.create_task(car_from_south())) await asyncio.gather(*tasks) return north_cars_crossed, south_cars_crossed"},{"question":"**Timedelta Operations and Analysis** You are provided with a dataset containing timestamps of various events. Your task is to write a function that processes this dataset to extract specific timedeltas and perform certain operations on them. # Dataset The dataset is a CSV file `events.csv` with two columns: 1. `event_start`: the start time of the event (\'YYYY-MM-DD HH:MM:SS\' format) 2. `event_end`: the end time of the event (\'YYYY-MM-DD HH:MM:SS\' format) # Function Specification Implement the following function: ```python import pandas as pd def process_events(file_path: str) -> pd.DataFrame: Process the event timestamps and perform operations. Parameters ---------- file_path : str Path to the CSV file containing event_start and event_end columns. Returns ------- pd.DataFrame A DataFrame with the following columns: - \'duration\': The duration of each event as a Timedelta. - \'duration_seconds\': The duration of each event in total seconds. - \'mean_duration\': The mean duration of all events as a Timedelta. - \'sum_duration\': The total duration of all events as a Timedelta. - \'min_duration\': The shortest event duration as a Timedelta. - \'min_event_start\': The start time of the shortest event. # Your code here pass ``` # Instructions 1. **Read the CSV file** `events.csv` into a pandas DataFrame. 2. **Calculate the duration** of each event using Timedelta and add it as a column named `\'duration\'`. 3. Create another column `\'duration_seconds\'` representing the event durations in total seconds. 4. Calculate the **mean event duration** and store it in a column `\'mean_duration\'`. 5. Calculate the **total event duration** and store it in a column `\'sum_duration\'`. 6. Identify the **shortest event duration** and store it in a column `\'min_duration\'`. 7. Retrieve and store the **start time** of the shortest event in a column `\'min_event_start\'`. 8. Return the modified DataFrame with the specified columns. # Example If the content of `events.csv` is: ``` event_start,event_end 2023-01-01 12:00:00,2023-01-01 13:00:00 2023-01-01 14:00:00,2023-01-01 14:30:00 2023-01-01 15:00:00,2023-01-01 16:30:00 ``` The resulting DataFrame might look like this: ``` duration duration_seconds mean_duration sum_duration min_duration min_event_start 0 0 days 01:00:00 3600.0 0 days 01:10:00 0 days 03:00:00 0 days 00:30:00 2023-01-01 14:00:00 1 0 days 00:30:00 1800.0 0 days 01:10:00 0 days 03:00:00 0 days 00:30:00 2023-01-01 14:00:00 2 0 days 01:30:00 5400.0 0 days 01:10:00 0 days 03:00:00 0 days 00:30:00 2023-01-01 14:00:00 ``` **Note**: The resulting DataFrame should include the additional columns specified above. # Constraints 1. Assume all timestamps are correctly formatted and the dataset is relatively small, so performance is not a primary concern. 2. Handle any missing values appropriately, they should not interrupt the processing of valid data. Use this task to demonstrate your understanding of the pandas Timedelta functionality, including construction, operations, and DataFrame manipulation.","solution":"import pandas as pd def process_events(file_path: str) -> pd.DataFrame: Process the event timestamps and perform operations. Parameters ---------- file_path : str Path to the CSV file containing event_start and event_end columns. Returns ------- pd.DataFrame A DataFrame with the following columns: - \'duration\': The duration of each event as a Timedelta. - \'duration_seconds\': The duration of each event in total seconds. - \'mean_duration\': The mean duration of all events as a Timedelta. - \'sum_duration\': The total duration of all events as a Timedelta. - \'min_duration\': The shortest event duration as a Timedelta. - \'min_event_start\': The start time of the shortest event. df = pd.read_csv(file_path, parse_dates=[\'event_start\', \'event_end\']) # Calculating the duration of each event df[\'duration\'] = df[\'event_end\'] - df[\'event_start\'] # Duration in seconds df[\'duration_seconds\'] = df[\'duration\'].dt.total_seconds() # Mean duration mean_duration = df[\'duration\'].mean() df[\'mean_duration\'] = mean_duration # Sum of all durations sum_duration = df[\'duration\'].sum() df[\'sum_duration\'] = sum_duration # Minimum duration and start time of the shortest event min_duration = df[\'duration\'].min() min_event_start = df.loc[df[\'duration\'] == min_duration, \'event_start\'].iloc[0] df[\'min_duration\'] = min_duration df[\'min_event_start\'] = min_event_start return df"},{"question":"**Compression Utility using `lzma`** You are required to implement a utility function that compresses and decompresses data using the Python `lzma` module. The function will handle both file-based and memory-based data compression and decompression using custom filter chains. # Function Signature ```python def lzma_utility(action, data=None, file_path=None, output_path=None, filters=None): Perform compression or decompression using `lzma`. Parameters: - action: A string that can either be \\"compress\\" or \\"decompress\\". - data: A bytes object containing data to be compressed or decompressed (for memory-based operations). - file_path: A string representing the path to the input file (for file-based operations). - output_path: A string representing the path to the output file (for file-based operations). - filters: A list of dictionaries representing custom filter chains. Depending on the action, the function should either compress or decompress the data or file. Returns: - If action is \\"compress\\" and file_path is None, return compressed data as a bytes object. - If action is \\"decompress\\" and file_path is None, return decompressed data as a bytes object. - If action is \\"compress\\" or \\"decompress\\" with file_path and output_path provided, write the results to the output file and return None. pass ``` # Requirements 1. If `action` is \\"compress\\": - For memory-based operation (when `file_path` is `None`): - Compress the provided `data` using the LZMA algorithm and return the compressed data. - For file-based operation (when `file_path` and `output_path` are provided): - Compress the contents of the input file specified by `file_path` and write the compressed data to the output file specified by `output_path`. 2. If `action` is \\"decompress\\": - For memory-based operation (when `file_path` is `None`): - Decompress the provided compressed `data` using the LZMA algorithm and return the decompressed data. - For file-based operation (when `file_path` and `output_path` are provided): - Decompress the contents of the input file specified by `file_path` and write the decompressed data to the output file specified by `output_path`. 3. Handle custom filter chains (if `filters` are provided) for both compressing and decompressing operations. # Constraints - You must handle any exceptions that may arise, such as file not found, incorrect action type, or incorrect filter configuration. - Ensure the function operates correctly with Python versions supporting `lzma` (Python 3.3+). # Example Usage ```python # Compressing data in memory compressed_data = lzma_utility(\\"compress\\", data=b\\"Example data to compress\\") # Decompressing data in memory decompressed_data = lzma_utility(\\"decompress\\", data=compressed_data) # Compressing a file lzma_utility(\\"compress\\", file_path=\\"input.txt\\", output_path=\\"output.xz\\") # Decompressing a file lzma_utility(\\"decompress\\", file_path=\\"output.xz\\", output_path=\\"decompressed.txt\\") # Using custom filters for compression filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 5}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 7 | lzma.PRESET_EXTREME}, ] compressed_data_with_filters = lzma_utility(\\"compress\\", data=b\\"Example data to compress\\", filters=filters) ``` # Evaluation Criteria - Correct and efficient implementation of compression and decompression functionality. - Proper handling of file-based and memory-based operations. - Correct and effective usage of custom filter chains. - Robust error handling and input validation.","solution":"import lzma def lzma_utility(action, data=None, file_path=None, output_path=None, filters=None): Perform compression or decompression using `lzma`. Parameters: - action: A string that can either be \\"compress\\" or \\"decompress\\". - data: A bytes object containing data to be compressed or decompressed (for memory-based operations). - file_path: A string representing the path to the input file (for file-based operations). - output_path: A string representing the path to the output file (for file-based operations). - filters: A list of dictionaries representing custom filter chains. Depending on the action, the function should either compress or decompress the data or file. Returns: - If action is \\"compress\\" and file_path is None, return compressed data as a bytes object. - If action is \\"decompress\\" and file_path is None, return decompressed data as a bytes object. - If action is \\"compress\\" or \\"decompress\\" with file_path and output_path provided, write the results to the output file and return None. try: if filters: lzma_filters = filters else: lzma_filters = None if action == \'compress\': if file_path and output_path: with open(file_path, \'rb\') as infile, lzma.open(output_path, \'wb\', format=lzma.FORMAT_XZ, filters=lzma_filters) as outfile: outfile.write(infile.read()) return None elif data is not None: return lzma.compress(data, format=lzma.FORMAT_XZ, filters=lzma_filters) else: raise ValueError(\\"For compression, either \'data\' or \'file_path\' and \'output_path\' must be provided.\\") elif action == \'decompress\': if file_path and output_path: with lzma.open(file_path, \'rb\') as infile, open(output_path, \'wb\') as outfile: outfile.write(infile.read()) return None elif data is not None: return lzma.decompress(data, format=lzma.FORMAT_XZ) else: raise ValueError(\\"For decompression, either \'data\' or \'file_path\' and \'output_path\' must be provided.\\") else: raise ValueError(\\"The \'action\' parameter must be either \'compress\' or \'decompress\'.\\") except Exception as e: raise e"},{"question":"**Question: Multi-Device and Stream Management in PyTorch** Your task is to demonstrate your understanding of PyTorch’s multi-threaded/multi-tasking hardware acceleration utilities by performing operations on multiple devices and streams, and ensuring proper synchronization. # Problem Statement Write a Python function `multi_device_operations` that: 1. Initializes the `torch.mtia` backend. 2. Checks if the `mtia` backend is available and initialized. 3. Retrieves the count of available devices. 4. Creates and manages two separate streams on two different devices. 5. Within each stream, allocates a tensor, performs a simple operation on it (e.g., element-wise addition), and then synchronizes the stream. 6. Returns a serialized summary of memory statistics after the operations. # Function Signature ```python def multi_device_operations() -> str: pass ``` # Constraints - Ensure that the function handles exceptions gracefully, and returns an error message if the `mtia` backend is unavailable or not initialized. - The function should work regardless of the number of devices available. - If less than two devices are available, use the same device for both streams. - Use PyTorch to serialize and return the summary of memory statistics. # Example ```python summary = multi_device_operations() print(summary) # Should print a serialized JSON-like string with memory statistics ``` # Tips 1. You can use `torch.mtia.init()`, `torch.mtia.is_available()`, and `torch.mtia.is_initialized()` for initialization and status checks. 2. Use `torch.mtia.device_count()` to get the number of devices. 3. Use `torch.mtia.set_device()`, `torch.mtia.stream()`, `torch.mtia.synchronize()` to manage devices and streams, and ensure proper synchronization. 4. Utilize `torch.mtia.memory_stats()` to get memory statistics. This question requires a good grasp of device/stream handling, synchronization, and memory management in PyTorch.","solution":"import torch def multi_device_operations() -> str: try: # Initialize the mtia backend torch.mtia.init() # Check if mtia backend is available and initialized if not torch.mtia.is_available() or not torch.mtia.is_initialized(): return \\"MTIA backend is unavailable or not initialized\\" # Retrieve the count of available devices device_count = torch.mtia.device_count() # Use the first device, if there are no multiple devices device1 = 0 device2 = 1 if device_count > 1 else 0 # Create streams on the devices stream1 = torch.mtia.stream(device=device1) stream2 = torch.mtia.stream(device=device2) # Allocate tensors and perform operations within streams, then synchronize with torch.mtia.stream_context(stream1): tensor1 = torch.ones((1024, 1024), device=device1) tensor1 += 1 # Simple operation torch.mtia.synchronize(device=device1) with torch.mtia.stream_context(stream2): tensor2 = torch.ones((1024, 1024), device=device2) tensor2 += 2 # Simple operation torch.mtia.synchronize(device=device2) # Gather and serialize memory statistics memory_stats = torch.mtia.memory_stats() return memory_stats except Exception as e: return f\\"Error occurred: {str(e)}\\""},{"question":"You are tasked with creating a custom command-line interpreter using the `cmd` module in Python. The interpreter will allow users to interactively manage a simple to-do list. Your program should support the following commands: 1. **add task_description**: Adds a new task to the to-do list. 2. **list**: Lists all the current tasks, with their status (pending/done) and ID. 3. **complete task_id**: Marks the specified task as done. 4. **delete task_id**: Deletes the specified task. 5. **help**: Lists all available commands with their descriptions. 6. **exit**: Exits the command interpreter. # Requirements: 1. **Functionality**: - When the `add task_description` command is issued, a new task should be added to the to-do list with an auto-incrementing ID and a status of \\"pending\\". - The `list` command should display all tasks with their ID, description, and current status. - The `complete task_id` command should mark the specified task as done. If the task doesn\'t exist, an appropriate error message should be displayed. - The `delete task_id` command should remove the specified task from the list. If the task doesn\'t exist, an appropriate error message should be displayed. - The `help` command should provide descriptions of all available commands. - The `exit` command should terminate the interpreter session. 2. **Constraints**: - Each command should be a method in a subclass of `cmd.Cmd`. - Commands and descriptions should be case-insensitive. - Use only standard Python libraries. 3. **Performance**: - Ensure efficient handling of command inputs and modification of the to-do list. # Input and Output: - All input will be provided interactively through the command interpreter. - Output should be printed to the console directly from within the command methods. # Sample Session: ``` Welcome to the To-Do List Manager. Type help or ? to list commands. (to-do) add Buy groceries Task added with ID 1 (to-do) add Call the bank Task added with ID 2 (to-do) list ID Description Status ---------------------------------------- 1 Buy groceries pending 2 Call the bank pending (to-do) complete 1 Task 1 marked as done. (to-do) list ID Description Status ---------------------------------------- 1 Buy groceries done 2 Call the bank pending (to-do) delete 1 Task 1 deleted. (to-do) list ID Description Status ---------------------------------------- 2 Call the bank pending (to-do) exit Goodbye! ``` Implement the ToDoCmd class to fulfill the above requirements. ```python import cmd class ToDoCmd(cmd.Cmd): intro = \'Welcome to the To-Do List Manager. Type help or ? to list commands.n\' prompt = \'(to-do) \' tasks = [] task_id = 1 def do_add(self, arg): \'Add a new task: add task_description\' task_description = arg self.tasks.append({\'id\': self.task_id, \'description\': task_description, \'status\': \'pending\'}) print(f\'Task added with ID {self.task_id}\') self.task_id += 1 def do_list(self, arg): \'List all tasks: list\' print(\'nID Description Status\') print(\'----------------------------------------\') for task in self.tasks: print(f\\"{task[\'id\']} {task[\'description\']} {task[\'status\']}\\") print() def do_complete(self, arg): \'Mark a task as done: complete task_id\' try: task_id = int(arg) except ValueError: print(\'Invalid task ID\') return for task in self.tasks: if task[\'status\'] == \'done\': continue if task[\'id\'] == task_id: task[\'status\'] = \'done\' print(f\'Task {task_id} marked as done.\') return print(f\'Task {task_id} not found.\') def do_delete(self, arg): \'Delete a task: delete task_id\' try: task_id = int(arg) except ValueError: print(\'Invalid task ID\') return for task in self.tasks: if task[\'id\'] == task_id: self.tasks.remove(task) print(f\'Task {task_id} deleted.\') return print(f\'Task {task_id} not found.\') def do_exit(self, arg): \'Exit the interpreter: exit\' print(\'Goodbye!\') return True if __name__ == \'__main__\': ToDoCmd().cmdloop() ```","solution":"import cmd class ToDoCmd(cmd.Cmd): intro = \'Welcome to the To-Do List Manager. Type help or ? to list commands.n\' prompt = \'(to-do) \' def __init__(self): super().__init__() self.tasks = [] self.task_id = 1 def do_add(self, arg): \'Add a new task: add task_description\' task_description = arg.strip() if task_description: self.tasks.append({\'id\': self.task_id, \'description\': task_description, \'status\': \'pending\'}) print(f\'Task added with ID {self.task_id}\') self.task_id += 1 else: print(\'Task description cannot be empty.\') def do_list(self, arg): \'List all tasks: list\' print(\'nID Description Status\') print(\'----------------------------------------\') for task in self.tasks: print(f\\"{task[\'id\']} {task[\'description\']} {task[\'status\']}\\") print() def do_complete(self, arg): \'Mark a task as done: complete task_id\' try: task_id = int(arg) for task in self.tasks: if task[\'id\'] == task_id: task[\'status\'] = \'done\' print(f\'Task {task_id} marked as done.\') return print(f\'Task {task_id} not found.\') except ValueError: print(\'Invalid task ID\') def do_delete(self, arg): \'Delete a task: delete task_id\' try: task_id = int(arg) for task in self.tasks: if task[\'id\'] == task_id: self.tasks.remove(task) print(f\'Task {task_id} deleted.\') return print(f\'Task {task_id} not found.\') except ValueError: print(\'Invalid task ID\') def do_exit(self, arg): \'Exit the interpreter: exit\' print(\'Goodbye!\') return True def do_help(self, arg): \'List all available commands with descriptions: help\' print(self.__doc__) if __name__ == \'__main__\': ToDoCmd().cmdloop()"},{"question":"**Objective:** You are tasked with creating a Python function that finds, loads, and reloads a given module using the `imp` module for backward compatibility and the `importlib` module for modern Python environments. **Problem Statement:** Write a function `manage_module(module_name: str, operation: str) -> any` that performs different operations on a module specified by `module_name`. The `operation` can be one of three strings: `\\"find\\"`, `\\"load\\"`, or `\\"reload\\"`. Depending on the operation, the function should do the following: 1. **\\"find\\"**: Find the module using `imp.find_module` (if available) or `importlib.util.find_spec`. Return the pathname of the found module or raise an `ImportError` if the module is not found. 2. **\\"load\\"**: Load the module using `imp.load_module` (if available) or `importlib.util.module_from_spec` and `importlib.util.find_spec`. Return the loaded module object. 3. **\\"reload\\"**: Reload the module using `imp.reload` (if available) or `importlib.reload`. Return the reloaded module object. **Constraints:** - Your implementation should first try using the `imp` module\'s functions if they are available, otherwise, it should fallback to the `importlib` module\'s functions. - Handle cases where the module is not found or any other exceptions that might be raised during the operations. - Assume that the given `module_name` is always a string and represents a valid Python module name or path. **Input:** - `module_name`: A `str` representing the name of the module to be managed. - `operation`: A `str` representing the operation to perform, which can be `\\"find\\"`, `\\"load\\"`, or `\\"reload\\"`. **Output:** - Returns the pathname of the found module for the `\\"find\\"` operation. - Returns the module object for the `\\"load\\"` and `\\"reload\\"` operations. - Raises appropriate exceptions if the module cannot be managed as per the operation. **Example Usage:** ```python # Example usage of the manage_module function # To find the \'json\' module pathname = manage_module(\\"json\\", \\"find\\") print(f\\"Module \'json\' found at path: {pathname}\\") # To load the \'json\' module json_module = manage_module(\\"json\\", \\"load\\") print(f\\"Module \'json\' loaded: {json_module}\\") # To reload the \'json\' module reloaded_json = manage_module(\\"json\\", \\"reload\\") print(f\\"Module \'json\' reloaded: {reloaded_json}\\") ``` **Notes:** - You may assume that the modules being managed exist and are accessible in the system\'s Python environment for the sake of this assessment. - Include appropriate error handling and fallbacks to ensure compatibility with both the deprecated `imp` module and the `importlib` module.","solution":"import sys import importlib import warnings def manage_module(module_name: str, operation: str): Manage the given module by finding, loading, or reloading it based on the operation. Parameters: module_name (str): The name of the module to manage. operation (str): The operation to perform - \\"find\\", \\"load\\", or \\"reload\\". Returns: any: Depending on the operation: - Pathname of the found module for \\"find\\". - Module object for \\"load\\" and \\"reload\\". try: if operation == \\"find\\": # Try to use the imp module if available try: import imp file, pathname, description = imp.find_module(module_name) if file: file.close() return pathname except ImportError: # Fall back to importlib if imp is not available spec = importlib.util.find_spec(module_name) if spec is None: raise ImportError(f\\"Module \'{module_name}\' not found.\\") return spec.origin elif operation == \\"load\\": # Try to use the imp module if available try: import imp file, pathname, description = imp.find_module(module_name) return imp.load_module(module_name, file, pathname, description) except ImportError: # Fall back to importlib if imp is not available spec = importlib.util.find_spec(module_name) if spec is None: raise ImportError(f\\"Module \'{module_name}\' not found.\\") module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) return module elif operation == \\"reload\\": # Try to use the imp module if available try: import imp module = sys.modules.get(module_name) if module is None: raise ImportError(f\\"Module \'{module_name}\' has not been loaded yet.\\") return imp.reload(module) except ImportError: # Fall back to importlib if imp is not available module = sys.modules.get(module_name) if module is None: raise ImportError(f\\"Module \'{module_name}\' has not been loaded yet.\\") return importlib.reload(module) else: raise ValueError(\\"Operation must be one of \'find\', \'load\', or \'reload\'.\\") except Exception as e: raise e"},{"question":"Objective: Your task is to demonstrate your understanding of the PyTorch compiler functionality by optimizing a given neural network model using the `torch.compiler` module. You will need to write functions that compile the model, handle its execution, and optimize it based on the compiler features provided in the documentation. Problem Statement: Given a simple neural network model defined in PyTorch, use the `torch.compiler` module to: 1. Compile the model. 2. Assume some specific layer outputs as constants to optimize their computation. 3. Execute the model on random input data and verify the performance gains. Model Definition: ```python import torch import torch.nn as nn import torch.optim as optim class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(784, 128) self.relu = nn.ReLU() self.fc2 = nn.Linear(128, 10) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x ``` Tasks: 1. **Compile the model**: Write a function `compile_model(model)` that takes the model and compiles it using `torch.compiler.compile`. 2. **Optimize with constant assumption**: Write a function `optimize_model(model)` that allows assuming certain intermediate results (e.g., the output of `self.fc1`) as constants to optimize computation. Use `torch.compiler.assume_constant_result`. 3. **Measure Performance**: Write a function `execute_and_measure(model, input_data)` that: - Executes the compiled model on the provided `input_data`. - Measures the execution time before and after optimization. - Returns a tuple with the execution time of both runs for comparison. Input and Output Formats: - `compile_model(model)`: Takes a PyTorch model instance, returns the compiled model. - `optimize_model(model)`: Takes a PyTorch model instance, modifies it internally for optimization. - `execute_and_measure(model, input_data)`: Takes a compiled model and input tensor data, returns a tuple `(time_before, time_after)`. Example: ```python model = SimpleModel() input_data = torch.randn(1, 784) compiled_model = compile_model(model) optimized_model = optimize_model(compiled_model) time_before, time_after = execute_and_measure(optimized_model, input_data) print(f\\"Execution time before optimization: {time_before}\\") print(f\\"Execution time after optimization: {time_after}\\") ``` Constraints: - The input data will always be a tensor of shape (1, 784). - You can assume the environment supports CUDA, and the neural network can be executed on GPU. Performance Requirements: Your optimizations should demonstrate measurable performance gains, ideally with reduced execution time post-optimization.","solution":"import torch import torch.nn as nn import torch.optim as optim import time class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(784, 128) self.relu = nn.ReLU() self.fc2 = nn.Linear(128, 10) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def compile_model(model): Compiles the provided PyTorch model using torch.compiler.compile. model = torch.compiler.compile(model) return model def optimize_model(model): Optimizes the model by assuming certain intermediate results as constants. Uses torch.compiler.assume_constant_result. @torch.compiler.assume_constant_result def optimized_forward(x): x = model.fc1(x) x = model.relu(x) x = model.fc2(x) return x model.forward = optimized_forward return model def execute_and_measure(model, input_data): Executes the model on the input data and measures execution time before and after optimization. Returns a tuple (time_before, time_after). # Measure time before optimization start = time.time() with torch.no_grad(): model(input_data) end = time.time() time_before = end - start # Compile and optimize the model compiled_model = compile_model(model) optimized_model = optimize_model(compiled_model) # Measure time after optimization start = time.time() with torch.no_grad(): optimized_model(input_data) end = time.time() time_after = end - start return time_before, time_after"},{"question":"# Custom Dataset and DataLoader for Efficient Data Processing Objective: You are tasked with creating a custom dataset and using `torch.utils.data.DataLoader` to load and handle the data efficiently. Your goal is to demonstrate the ability to create custom datasets, customize data loading order with samplers, handle data in batches, and optimize the data loading process utilizing PyTorch functionalities. Task: 1. **Custom Dataset Creation**: - Implement a custom dataset class named `CustomDataset` that inherits from `torch.utils.data.Dataset`. - The dataset should read samples from a provided list of dictionaries. Each dictionary contains: - An \\"input\\" key with tensor data. - A \\"label\\" key with the label for the data. ```python sample_data = [ {\\"input\\": torch.tensor([1, 2, 3]), \\"label\\": torch.tensor(0)}, {\\"input\\": torch.tensor([4, 5, 6]), \\"label\\": torch.tensor(1)}, # Add more sample data as needed ] ``` - The class should implement the `__getitem__` and `__len__` methods. 2. **Custom DataLoader**: - Create a DataLoader to load data from `CustomDataset`. - Use a custom sampler `torch.utils.data.RandomSampler` to shuffle and control the order of data loading. - Set the batch size to 2 and enable pin_memory for optimized GPU data transfer. 3. **Custom Collate Function**: - Implement a custom collate function `custom_collate_fn` that takes a batch of data and arranges it into a dictionary with keys \\"batch_inputs\\" and \\"batch_labels\\". 4. **Multi-process Data Loading**: - Configure the DataLoader to use 4 worker processes (if system supports multiple processes). 5. **Main Execution**: - In the main block, create an instance of your `CustomDataset` class with the provided sample data. - Initialize the DataLoader with the configured properties. - Iterate over the DataLoader and print each batch to verify implementation. Constraints: - Use only PyTorch and standard Python libraries. - Ensure the solution is compatible with multi-processing on both Unix and Windows platforms. Expected Implementation: ```python import torch from torch.utils.data import Dataset, DataLoader, RandomSampler class CustomDataset(Dataset): def __init__(self, data): self.data = data def __getitem__(self, index): return self.data[index] def __len__(self): return len(self.data) def custom_collate_fn(batch): batch_inputs = torch.stack([item[\\"input\\"] for item in batch], dim=0) batch_labels = torch.stack([item[\\"label\\"] for item in batch], dim=0) return {\\"batch_inputs\\": batch_inputs, \\"batch_labels\\": batch_labels} if __name__ == \'__main__\': sample_data = [ {\\"input\\": torch.tensor([1, 2, 3]), \\"label\\": torch.tensor(0)}, {\\"input\\": torch.tensor([4, 5, 6]), \\"label\\": torch.tensor(1)}, # Add more sample data as needed ] dataset = CustomDataset(sample_data) sampler = RandomSampler(dataset) dataloader = DataLoader(dataset, batch_size=2, sampler=sampler, collate_fn=custom_collate_fn, pin_memory=True, num_workers=4) for batch in dataloader: print(batch) ``` Performance Requirements: - The DataLoader should efficiently load and batch data. - The use of multi-processing should be tested for performance improvements. Submission: - Submit a single Python file with the complete implementation. - Ensure the code is well-commented and follows best practices for readability and maintainability.","solution":"import torch from torch.utils.data import Dataset, DataLoader, RandomSampler class CustomDataset(Dataset): def __init__(self, data): Custom dataset constructor. Args: - data (list of dicts): Each dict should have \'input\' and \'label\' as keys. self.data = data def __getitem__(self, index): Get item by index. Args: - index (int): Index of the item to retrieve. Returns: - dict: Dictionary containing \'input\' and \'label\'. return self.data[index] def __len__(self): Total number of samples in the dataset. Returns: - int: Number of samples. return len(self.data) def custom_collate_fn(batch): Custom collate function to format the batch. Args: - batch (list of dicts): List of items from the dataset. Returns: - dict: Dictionary containing \'batch_inputs\' and \'batch_labels\'. batch_inputs = torch.stack([item[\\"input\\"] for item in batch], dim=0) batch_labels = torch.stack([item[\\"label\\"] for item in batch], dim=0) return {\\"batch_inputs\\": batch_inputs, \\"batch_labels\\": batch_labels} if __name__ == \'__main__\': sample_data = [ {\\"input\\": torch.tensor([1, 2, 3]), \\"label\\": torch.tensor(0)}, {\\"input\\": torch.tensor([4, 5, 6]), \\"label\\": torch.tensor(1)}, {\\"input\\": torch.tensor([7, 8, 9]), \\"label\\": torch.tensor(1)}, {\\"input\\": torch.tensor([10, 11, 12]), \\"label\\": torch.tensor(0)} ] dataset = CustomDataset(sample_data) sampler = RandomSampler(dataset) dataloader = DataLoader(dataset, batch_size=2, sampler=sampler, collate_fn=custom_collate_fn, pin_memory=True, num_workers=4) for batch in dataloader: print(batch)"},{"question":"**Python Dictionary Operations Using C API** You are provided with a partial implementation of a Python module that includes a dictionary manipulation function using the Python C API. Your task is to complete the implementation by writing a function `manipulate_dict` which performs the following operations on a given dictionary: 1. **Init and Clear**: - Create a new dictionary. - Clear all elements from an existing dictionary. 2. **Insert and Delete**: - Insert a key-value pair into the dictionary. - Remove a specific key from the dictionary. 3. **Fetch and Check**: - Retrieve a value by key from the dictionary. - Check if a key exists in the dictionary. 4. **Merge Dictionaries**: - Merge another dictionary into the created dictionary with an option to override existing keys. **Input and Constraints**: - Input to your function `manipulate_dict` should be in the form of Python objects compatible with the `PyDict_*` functions. - Handle different types of keys and values. - Ensure proper error handling and memory management as per typical C API requirements. **Function Signature**: ```python def manipulate_dict(py_dict, operations): Manipulates a given dictionary as per the specified operations. :param py_dict: The initial dictionary to manipulate. :param operations: A list of operations in the format: [ (\\"create\\",), (\\"clear\\", py_dict), (\\"insert\\", py_dict, key, value), (\\"delete\\", py_dict, key), (\\"fetch\\", py_dict, key), (\\"check\\", py_dict, key), (\\"merge\\", py_dict, other_dict, override) ] `create` - Initializes a new dictionary. `clear` - Clears the specified dictionary. `insert` - Add key-value pair to dictionary. `delete` - Remove key from dictionary. `fetch` - Retrieve value by key. `check` - Check if key exists. `merge` - Merge another dictionary with option to override. :return: The manipulated dictionary and any output from fetch/check operations. ``` **Example**: ```python py_dict = {\'a\': 1, \'b\': 2} operations = [ (\\"create\\",), (\\"clear\\", py_dict), (\\"insert\\", py_dict, \'c\', 3), (\\"delete\\", py_dict, \'a\'), (\\"fetch\\", py_dict, \'b\'), (\\"check\\", py_dict, \'a\'), (\\"merge\\", py_dict, {\'d\': 4, \'e\': 5}, True) ] # Expected Output: # (Newly created dict, [None, None, None, 2, False, {\'c\': 3, \'d\': 4, \'e\': 5}]) ``` Implement the function `manipulate_dict` above ensuring proper use of the `PyDict_*` functions. **Notes**: - Focus on correct usage of memory allocation and exception handling as per C API conventions. - Feel free to define any helper functions if necessary.","solution":"def manipulate_dict(py_dict, operations): Manipulates a given dictionary as per the specified operations. :param py_dict: The initial dictionary to manipulate. :param operations: A list of operations in the format: [ (\\"create\\",), (\\"clear\\", py_dict), (\\"insert\\", py_dict, key, value), (\\"delete\\", py_dict, key), (\\"fetch\\", py_dict, key), (\\"check\\", py_dict, key), (\\"merge\\", py_dict, other_dict, override) ] `create` - Initializes a new dictionary. `clear` - Clears the specified dictionary. `insert` - Add key-value pair to dictionary. `delete` - Remove key from dictionary. `fetch` - Retrieve value by key. `check` - Check if key exists. `merge` - Merge another dictionary with option to override. :return: The manipulated dictionary and any output from fetch/check operations. results = [] new_dict = {} for operation in operations: op_type = operation[0] if op_type == \\"create\\": new_dict = {} results.append(None) elif op_type == \\"clear\\": dict_to_clear = operation[1] dict_to_clear.clear() results.append(None) elif op_type == \\"insert\\": dict_to_insert = operation[1] key = operation[2] value = operation[3] dict_to_insert[key] = value results.append(None) elif op_type == \\"delete\\": dict_to_delete = operation[1] key = operation[2] if key in dict_to_delete: del dict_to_delete[key] results.append(None) elif op_type == \\"fetch\\": dict_to_fetch = operation[1] key = operation[2] value = dict_to_fetch.get(key, None) results.append(value) elif op_type == \\"check\\": dict_to_check = operation[1] key = operation[2] exists = key in dict_to_check results.append(exists) elif op_type == \\"merge\\": dict_to_merge = operation[1] other_dict = operation[2] override = operation[3] if override: dict_to_merge.update(other_dict) else: for key, value in other_dict.items(): if key not in dict_to_merge: dict_to_merge[key] = value results.append(None) return new_dict, results"},{"question":"Objective Demonstrate your understanding of seaborn\'s KDE plotting capabilities by creating and customizing kernel density estimation plots based on given datasets and requirements. Problem Statement You are provided with two datasets: `tips` and `geyser`. Your task is to generate KDE plots that meet the following specific requirements: 1. **Basic Univariate KDE Plot** - Load the `tips` dataset. - Plot the kernel density estimation of the `total_bill` column on the x-axis. - Save the plot as `kde_total_bill_basic.png`. 2. **Conditional KDE Plot** - Using the `tips` dataset, plot the kernel density estimation of the `total_bill` column, conditioned on the `time` variable. - Use `stack` multiple type to show stacked distributions. - Save the plot as `kde_total_bill_by_time.png`. 3. **Customized Bivariate KDE Plot** - Load the `geyser` dataset. - Plot the bivariate kernel distribution for `waiting` on the x-axis and `duration` on the y-axis. - Use `fill=True` and `cmap=\\"coolwarm\\"` for custom colors. - Limit the number of contour levels to 10. - Save the plot as `kde_waiting_duration.png`. 4. **Cumulative KDE Plot with Weights** - Using the `tips` dataset, group by `size` and calculate the mean `total_bill` and the count (`n`) of each group. - Plot the cumulative distribution of the `total_bill`, weighted by the count `n`. - Save the plot as `cumulative_weighted_kde_total_bill.png`. Constraints - Use seaborn for plotting. - Ensure all plots have appropriate titles and axis labels for clarity. Expected Output Save the following plots: 1. `kde_total_bill_basic.png` 2. `kde_total_bill_by_time.png` 3. `kde_waiting_duration.png` 4. `cumulative_weighted_kde_total_bill.png` Input None. The datasets are directly available via seaborn\'s dataset interface. Implementation Requirements - Ensure that you handle the seaborn and matplotlib imports correctly. - Ensure that all plots are saved with appropriate filenames. Example Below is a snippet to get you started with the first plot: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Plot the basic KDE sns.kdeplot(data=tips, x=\\"total_bill\\") # Title and labels plt.title(\\"KDE plot of Total Bill\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Density\\") # Save the plot plt.savefig(\\"kde_total_bill_basic.png\\") plt.close() ``` Make sure to complete the task for all four requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_kde_total_bill_basic(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Plot the basic KDE sns.kdeplot(data=tips, x=\\"total_bill\\") # Title and labels plt.title(\\"KDE plot of Total Bill\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Density\\") # Save the plot plt.savefig(\\"kde_total_bill_basic.png\\") plt.close() def plot_kde_total_bill_by_time(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Plot the conditional KDE sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", multiple=\\"stack\\") # Title and labels plt.title(\\"KDE plot of Total Bill by Time\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Density\\") # Save the plot plt.savefig(\\"kde_total_bill_by_time.png\\") plt.close() def plot_kde_waiting_duration(): # Load the dataset geyser = sns.load_dataset(\\"geyser\\") # Plot the customized bivariate KDE sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", fill=True, cmap=\\"coolwarm\\", levels=10) # Title and labels plt.title(\\"Bivariate KDE plot of Waiting vs Duration\\") plt.xlabel(\\"Waiting\\") plt.ylabel(\\"Duration\\") # Save the plot plt.savefig(\\"kde_waiting_duration.png\\") plt.close() def plot_cumulative_weighted_kde_total_bill(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Group by `size` and calculate the mean `total_bill` and the count (`n`) grouped_data = tips.groupby(\'size\').total_bill.agg([\'mean\', \'count\']).reset_index() grouped_data.rename(columns={\'count\': \'n\'}, inplace=True) # Plot the cumulative KDE weighted by the count `n` sns.kdeplot(data=grouped_data, x=\\"mean\\", weights=\\"n\\", cumulative=True) # Title and labels plt.title(\\"Cumulative Weighted KDE plot of Total Bill Mean\\") plt.xlabel(\\"Mean Total Bill\\") plt.ylabel(\\"Cumulative Density\\") # Save the plot plt.savefig(\\"cumulative_weighted_kde_total_bill.png\\") plt.close() # Call the functions to generate the plots plot_kde_total_bill_basic() plot_kde_total_bill_by_time() plot_kde_waiting_duration() plot_cumulative_weighted_kde_total_bill()"},{"question":"Given a dictionary `d`, implement the following Python functions to simulate the behavior of the described C API functions: 1. `mapping_check(obj: Any) -> bool`: Return `True` if the object provides the mapping protocol (i.e., supports `__getitem__`). Otherwise, return `False`. 2. `mapping_size(mapping: dict) -> int`: Return the number of keys in the dictionary. 3. `mapping_get_item(mapping: dict, key: str) -> Any`: Return the value associated with `key` in the dictionary. If the key does not exist, raise a `KeyError`. 4. `mapping_set_item(mapping: dict, key: str, value: Any) -> None`: Set the `key` in the dictionary to the `value`. 5. `mapping_del_item(mapping: dict, key: str) -> None`: Remove the `key` from the dictionary. If the key does not exist, raise a `KeyError`. 6. `mapping_has_key(mapping: dict, key: str) -> bool`: Return `True` if the dictionary contains `key`. Otherwise, return `False`. 7. `mapping_keys(mapping: dict) -> list`: Return a list of the keys in the dictionary. 8. `mapping_values(mapping: dict) -> list`: Return a list of the values in the dictionary. 9. `mapping_items(mapping: dict) -> list`: Return a list of tuples, each tuple containing a key-value pair in the dictionary. # Constraints - Do not use the built-in dictionary methods directly (`len`, `get`, `setitem`, etc.). Instead, use fundamental control flow and exception handling to implement the logic. # Example ```python d = {\\"a\\": 1, \\"b\\": 2, \\"c\\": 3} assert mapping_check(d) == True assert mapping_size(d) == 3 assert mapping_get_item(d, \\"a\\") == 1 mapping_set_item(d, \\"d\\", 4) assert d == {\\"a\\": 1, \\"b\\": 2, \\"c\\": 3, \\"d\\": 4} mapping_del_item(d, \\"b\\") assert d == {\\"a\\": 1, \\"c\\": 3, \\"d\\": 4} assert mapping_has_key(d, \\"c\\") == True assert mapping_keys(d) == [\\"a\\", \\"c\\", \\"d\\"] assert mapping_values(d) == [1, 3, 4] assert mapping_items(d) == [(\\"a\\", 1), (\\"c\\", 3), (\\"d\\", 4)] ``` Write the implementations of the functions in Python to complete this task.","solution":"def mapping_check(obj): Returns True if the object provides the mapping protocol (supports __getitem__). try: obj.__getitem__ return True except AttributeError: return False def mapping_size(mapping): Returns the number of keys in the dictionary. keys_count = 0 for _ in mapping: keys_count += 1 return keys_count def mapping_get_item(mapping, key): Returns the value associated with the key in the dictionary. If the key does not exist, raises a KeyError. for k in mapping: if k == key: return mapping[k] raise KeyError(f\\"Key {key} not found\\") def mapping_set_item(mapping, key, value): Sets the key in the dictionary to the specified value. found = False for k in mapping: if k == key: mapping[key] = value found = True break if not found: mapping[key] = value def mapping_del_item(mapping, key): Removes the key from the dictionary. If the key does not exist, raises a KeyError. if key in mapping: del mapping[key] else: raise KeyError(f\\"Key {key} not found\\") def mapping_has_key(mapping, key): Returns True if the dictionary contains the key. Otherwise, returns False. for k in mapping: if k == key: return True return False def mapping_keys(mapping): Returns a list of the keys in the dictionary. keys_list = [] for k in mapping: keys_list.append(k) return keys_list def mapping_values(mapping): Returns a list of the values in the dictionary. values_list = [] for v in mapping.values(): values_list.append(v) return values_list def mapping_items(mapping): Returns a list of tuples, each containing a key-value pair in the dictionary. items_list = [] for k, v in mapping.items(): items_list.append((k, v)) return items_list"},{"question":"Objective Develop a Python function leveraging the **difflib.SequenceMatcher** class to implement a custom `text_similarity` tool. This tool will determine the similarity ratio between two given texts and return detailed information on the differences. Function Signature ```python def text_similarity(text1: str, text2: str, isjunk: callable = None) -> dict: Compares two texts and returns a dictionary with similarity ratio and detailed differences. Parameters: - text1 (str): The first text string to compare. - text2 (str): The second text string to compare. - isjunk (callable, optional): A function that takes a character/line and returns True if the character/line should be considered junk. Defaults to None. Returns: - dict: A dictionary containing the similarity ratio and a list of detailed differences. ``` Input - `text1` and `text2`: Two strings representing the texts to be compared. - `isjunk`: An optional junk detection function which, when specified, affects how the sequences are compared by ignoring certain elements. Output - A dictionary with the keys: - `ratio`: A float between 0 and 1 indicating the similarity ratio of the texts. - `differences`: A list of tuples describing the differences (tag, i1, i2, j1, j2) as provided by the **SequenceMatcher.get_opcodes()** method. Constraints 1. The texts can be arbitrarily long. 2. The junk function, if provided, will handle specific characters or lines that should be ignored during the comparison. Example ```python text1 = \\" I love programming.nIt is fun.n\\" text2 = \\"I love programming!nCoding is fun.n\\" result = text_similarity(text1, text2) # Expected Output: # { # \'ratio\': <some float value>, # \'differences\': [ # (\'equal\', 0, 7, 0, 7), # (\'delete\', 7, 8, 7, 7), # (\'insert\', 8, 8, 7, 8), # (\'equal\', 8, 23, 8, 23), # (\'replace\', 23, 25, 23, 29), # ] # } ``` Instructions 1. Implement the `text_similarity` function using **difflib.SequenceMatcher**. 2. Ensure that the function computes the similarity ratio and extracts detailed differences. 3. Use `get_opcodes` method of **SequenceMatcher** to generate the differences. 4. Write additional test cases to verify the correctness of the function.","solution":"from difflib import SequenceMatcher def text_similarity(text1: str, text2: str, isjunk: callable = None) -> dict: Compares two texts and returns a dictionary with similarity ratio and detailed differences. Parameters: - text1 (str): The first text string to compare. - text2 (str): The second text string to compare. - isjunk (callable, optional): A function that takes a character/line and returns True if the character/line should be considered junk. Defaults to None. Returns: - dict: A dictionary containing the similarity ratio and a list of detailed differences. matcher = SequenceMatcher(isjunk, text1, text2) ratio = matcher.ratio() differences = matcher.get_opcodes() return { \'ratio\': ratio, \'differences\': differences }"},{"question":"# Context Management using `contextvars` in Python Problem Statement: You are tasked with implementing a Python program to demonstrate the use of context variables using the `contextvars` module. Your program should: 1. Create a new context variable named `user`. 2. Create a new context and set the context variable `user` to different values in different contexts. 3. Implement functions to: - **Switch to a new context** and set the `user` context variable. - **Get the current value** of the `user` context variable. - **Reset the value** of the `user` context variable to its previous state using tokens. You need to implement the following functions: 1. `create_context_var(name: str, default: Optional[Any] = None) -> contextvars.ContextVar`: - Creates and returns a new context variable with the given name and default value. 2. `set_context_var(var: contextvars.ContextVar, value: Any) -> contextvars.Token`: - Sets the value of the given context variable and returns the token. 3. `get_context_var_value(var: contextvars.ContextVar, default: Optional[Any] = None) -> Any`: - Gets the current value of the context variable, returning the default if not set. 4. `reset_context_var(var: contextvars.ContextVar, token: contextvars.Token) -> None`: - Resets the context variable to its value before the change that returned the token. 5. `create_and_enter_new_context() -> contextvars.Context`: - Creates a new context and enters it. Implementation Constraints: - Use the `contextvars` module\'s Python APIs, not the C APIs. - Ensure proper handling of context entering and exiting. - Handle exceptions gracefully where applicable. Example Usage: ```python import contextvars # Define context variable and context managers user_var = create_context_var(\\"user\\", default=\\"anonymous\\") # Create and enter a new context context_1 = create_and_enter_new_context() set_context_var(user_var, \\"Alice\\") print(get_context_var_value(user_var)) # Output: Alice # Create and enter another context context_2 = create_and_enter_new_context() set_context_var(user_var, \\"Bob\\") print(get_context_var_value(user_var)) # Output: Bob # Reset to previous context reset_context_var(user_var, token) print(get_context_var_value(user_var)) # Output: anonymous ``` Submission: - Implement the described functions in a Python script. - Ensure to include test cases to demonstrate the functionality of each function.","solution":"import contextvars def create_context_var(name: str, default: any = None) -> contextvars.ContextVar: Creates and returns a new context variable with the given name and default value. return contextvars.ContextVar(name, default=default) def set_context_var(var: contextvars.ContextVar, value: any) -> contextvars.Token: Sets the value of the given context variable and returns the token. return var.set(value) def get_context_var_value(var: contextvars.ContextVar, default: any = None) -> any: Gets the current value of the context variable, returning the default if not set. try: return var.get() except LookupError: return default def reset_context_var(var: contextvars.ContextVar, token: contextvars.Token) -> None: Resets the context variable to its value before the change that returned the token. var.reset(token) def create_and_enter_new_context() -> contextvars.Context: Creates a new context and enters it. new_context = contextvars.copy_context() new_context.run(lambda: None) # Runs a no-op to switch to the new context return new_context"},{"question":"Objective The objective of this question is to assess your understanding of Scikit-learn\'s utility functions, particularly focused on validation and efficient computations. Problem Statement Given a set of functions for validation and efficient computations, your task is to implement a function `process_and_compute_svd` which performs the following operations: 1. Validates the input matrix `X` using Scikit-learn\'s validation utilities. 2. If the validation passes, computes the singular value decomposition (SVD) of `X` using Scikit-learn\'s `randomized_svd` function. Function Signature ```python def process_and_compute_svd(X: np.ndarray, n_components: int=2, random_state: int=42) -> Tuple[np.ndarray, np.ndarray, np.ndarray]: Validates the input and computes the truncated randomized SVD. Parameters: ---------- X : np.ndarray A 2D input array. n_components : int Number of singular values and vectors to compute. random_state : int Seed for the random number generator. Returns: ------- U : np.ndarray Unitary matrix having left singular vectors. s : np.ndarray The singular values. VT : np.ndarray Unitary matrix having right singular vectors. ``` Instructions 1. **Validation**: - Use `sklearn.utils.check_array` to ensure `X` is a 2D array and contains no NaN or Inf values. 2. **Singular Value Decomposition (SVD)**: - Use `sklearn.utils.extmath.randomized_svd` to compute the SVD of the validated input matrix `X`. 3. **Output**: - The function should return the matrices `U`, `s`, and `VT` from the SVD computation. Example Usage ```python import numpy as np X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) U, s, VT = process_and_compute_svd(X) print(\\"U:n\\", U) print(\\"s:n\\", s) print(\\"VT:n\\", VT) ``` Constraints - Ensure that the input matrix `X` is a 2D array. - Handle cases where `X` might contain NaNs or Infs and raise an appropriate error. - Ensure the computations are efficient even for large input matrices. **Note**: You may assume the necessary functions are imported from Scikit-learn for this task. Imports ```python from sklearn.utils import check_array from sklearn.utils.extmath import randomized_svd from typing import Tuple import numpy as np ``` Requirements: - Ensure you handle exceptions and provide meaningful error messages if the input does not meet the requirements. - You should not use additional libraries other than those provided/imported. Good luck!","solution":"from sklearn.utils import check_array from sklearn.utils.extmath import randomized_svd from typing import Tuple import numpy as np def process_and_compute_svd(X: np.ndarray, n_components: int=2, random_state: int=42) -> Tuple[np.ndarray, np.ndarray, np.ndarray]: Validates the input and computes the truncated randomized SVD. Parameters: ---------- X : np.ndarray A 2D input array. n_components : int Number of singular values and vectors to compute. random_state : int Seed for the random number generator. Returns: ------- U : np.ndarray Unitary matrix having left singular vectors. s : np.ndarray The singular values. VT : np.ndarray Unitary matrix having right singular vectors. # Validate input array X = check_array(X, ensure_2d=True, allow_nd=False, dtype=np.float64, force_all_finite=True) # Compute randomized SVD U, s, VT = randomized_svd(X, n_components=n_components, random_state=random_state) return U, s, VT"},{"question":"# Binary to ASCII Conversion and Validation In this task, you will implement a Python function that converts given binary data into multiple ASCII-encoded formats and validates the conversions by decoding them back to the original binary data. You will use the `binascii` module for conversions, handling byte-like objects and string formats appropriately. You need to implement the following function: ```python import binascii def binary_ascii_conversion(binary_data: bytes) -> dict: Converts the given binary data to various ASCII-encoded formats and validates conversions. Parameters: binary_data (bytes): A bytes object representing the binary data to be converted. Returns: dict: A dictionary containing the original binary data and its ASCII-encoded formats. The dictionary should have the following structure: { \'original\': original_binary_data, \'uuencoded\': uuencoded_data, \'base64\': base64_data, \'quoted_printable\': quoted_printable_data, \'hex\': hex_data, \'crc32\': crc32_checksum } pass ``` # Constraints 1. The `binary_data` must be a byte-like object with a maximum length of 1000 bytes. 2. You have to use functions provided by the `binascii` module for encoding and decoding. 3. Each encoded format must be validated by decoding it back to the initial binary form before storing it in the dictionary. # Specifications - `uuencoded_data`: Convert `binary_data` to uuencoded format using `binascii.b2a_uu` and decode it back using `binascii.a2b_uu` for validation. - `base64_data`: Convert `binary_data` to base64 format using `binascii.b2a_base64` and decode it back using `binascii.a2b_base64` for validation. - `quoted_printable_data`: Convert `binary_data` to quoted-printable format using `binascii.b2a_qp` and decode it back using `binascii.a2b_qp` for validation. - `hex_data`: Convert `binary_data` to hexadecimal format using `binascii.b2a_hex` and decode it back using `binascii.a2b_hex` for validation. - `crc32_checksum`: Compute the CRC-32 checksum of `binary_data` using `binascii.crc32`. # Example: ```python binary_data = b\\"Hello, World!\\" output = binary_ascii_conversion(binary_data) print(output) # Expected output: # { # \'original\': b\'Hello, World!\', # \'uuencoded\': b\'9V]R;&0**n\', # \'base64\': b\'SGVsbG8sIFdvcmxkIQ==n\', # \'quoted_printable\': b\'Hello=2C World!\', # \'hex\': b\'48656c6c6f2c20576f726c6421\', # \'crc32\': 3964322768 # } ``` # Notes: - Your implementation should ensure the conversions are correct by decoding the data back and verifying it matches the original `binary_data`. - You must handle any exceptions that could arise from invalid encoding/decoding processes, and the function should return an appropriate error message in case of failure.","solution":"import binascii def binary_ascii_conversion(binary_data: bytes) -> dict: Converts the given binary data to various ASCII-encoded formats and validates conversions. Parameters: binary_data (bytes): A bytes object representing the binary data to be converted. Returns: dict: A dictionary containing the original binary data and its ASCII-encoded formats. The dictionary should have the following structure: { \'original\': original_binary_data, \'uuencoded\': uuencoded_data, \'base64\': base64_data, \'quoted_printable\': quoted_printable_data, \'hex\': hex_data, \'crc32\': crc32_checksum } # Original data result = {\'original\': binary_data} # UUencoded data uuencoded_data = binascii.b2a_uu(binary_data) assert binascii.a2b_uu(uuencoded_data) == binary_data result[\'uuencoded\'] = uuencoded_data.rstrip(b\'n\') # Base64 data base64_data = binascii.b2a_base64(binary_data) assert binascii.a2b_base64(base64_data) == binary_data result[\'base64\'] = base64_data.rstrip(b\'n\') # Quoted Printable data quoted_printable_data = binascii.b2a_qp(binary_data) assert binascii.a2b_qp(quoted_printable_data) == binary_data result[\'quoted_printable\'] = quoted_printable_data # Hexadecimal data hex_data = binascii.b2a_hex(binary_data) assert binascii.a2b_hex(hex_data) == binary_data result[\'hex\'] = hex_data # CRC32 checksum crc32_checksum = binascii.crc32(binary_data) result[\'crc32\'] = crc32_checksum return result"},{"question":"**Problem Statement: UUID Manipulation and Utility Functions** You are provided with a task to create a utility module for handling UUIDs. Your module should offer functionalities for generating, validating, and manipulating UUIDs using the `uuid` module provided by Python. Implement the following functions: 1. **generate_uuids(n, version=4)** - This function should generate and return a list of `n` UUIDs of the specified version. - Parameters: - `n` (int): The number of UUIDs to generate. - `version` (int): The version of UUID to generate (default is 4, can be 1, 3, 4, or 5). - Returns: - A list of `n` UUID strings. 2. **validate_uuid(uuid_str)** - This function should validate whether a given string is a valid UUID as per RFC 4122. - Parameters: - `uuid_str` (str): The UUID string to validate. - Returns: - `True` if the string is a valid UUID, `False` otherwise. 3. **format_uuid(uuid_str, format_type)** - This function should return the UUID string formatted in different specified formats. - Parameters: - `uuid_str` (str): The UUID string to format. - `format_type` (str): The format type. Can be one of \'hex\', \'bytes\', \'bytes_le\', \'fields\', \'int\', \'urn\'. - Returns: - The UUID in the specified format. For \'bytes\' and \'bytes_le\', return the string representation of the bytes. **Constraints:** - Do not use any external libraries other than Python\'s `uuid` module. - You may assume that the inputs to the functions are always valid, except for the `validate_uuid` function which needs to handle invalid UUID strings. - Ensure that the functions are efficient and handle edge cases where applicable. **Example:** ```python # Example usage of the functions: print(generate_uuids(3, version=1)) # Output: [\'a8098c1a-f86e-11da-bd1a-00112444be1e\', \'a8098c1a-f86e-11da-bd1c-00112444be1e\', \'a8098c1a-f86e-11da-bd1d-00112444be1e\'] print(validate_uuid(\'a8098c1a-f86e-11da-bd1a-00112444be1e\')) # Output: True print(format_uuid(\'a8098c1a-f86e-11da-bd1a-00112444be1e\', \'fields\')) # Output: (1546978810, 63598, 4574, 189, 26, 74573545084638) ``` **Note:** You will need to import the `uuid` module in your implementation to create and manage the UUIDs as described above.","solution":"import uuid def generate_uuids(n, version=4): Generate and return a list of n UUIDs of the specified version. Parameters: - n (int): The number of UUIDs to generate. - version (int): The version of UUID to generate (default is 4, can be 1, 3, 4, or 5). Returns: - A list of n UUID strings. if version not in [1, 3, 4, 5]: raise ValueError(\\"Invalid UUID version. Must be 1, 3, 4, or 5.\\") uuid_generators = { 1: uuid.uuid1, 3: uuid.uuid3, 4: uuid.uuid4, 5: uuid.uuid5 } uuids = [] for _ in range(n): if version == 3 or version == 5: # v3 and v5 UUIDs require a namespace and a name, use some defaults uuids.append(str(uuid_generators[version](uuid.NAMESPACE_DNS, \'example.com\'))) else: uuids.append(str(uuid_generators[version]())) return uuids def validate_uuid(uuid_str): Validate whether a given string is a valid UUID. Parameters: - uuid_str (str): The UUID string to validate. Returns: - True if the string is a valid UUID, False otherwise. try: val = uuid.UUID(uuid_str) return True except ValueError: return False def format_uuid(uuid_str, format_type): Return the UUID string formatted in different specified formats. Parameters: - uuid_str (str): The UUID string to format. - format_type (str): The format type. Can be one of \'hex\', \'bytes\', \'bytes_le\', \'fields\', \'int\', \'urn\'. Returns: - The UUID in the specified format. valid_formats = [\'hex\', \'bytes\', \'bytes_le\', \'fields\', \'int\', \'urn\'] if format_type not in valid_formats: raise ValueError(\\"Invalid format type. Must be \'hex\', \'bytes\', \'bytes_le\', \'fields\', \'int\', or \'urn\'.\\") u = uuid.UUID(uuid_str) if format_type == \'hex\': return u.hex elif format_type == \'bytes\': return str(u.bytes) elif format_type == \'bytes_le\': return str(u.bytes_le) elif format_type == \'fields\': return u.fields elif format_type == \'int\': return u.int elif format_type == \'urn\': return u.urn"},{"question":"**Objective:** You need to write a function that visualizes the relationship between the total bill, tip, and other variables in the \\"tips\\" dataset using seaborn. The key is to create a comprehensive and informative plot that conveys meaningful insights. **Requirements:** 1. Load the \\"tips\\" dataset using seaborn. 2. Create a scatter plot depicting the relationship between `total_bill` and `tip`. 3. Use different colors to represent the different `days` of the week. 4. Differentiate between lunch and dinner times using marker styles. 5. Adjust the size of the markers based on the number of people (`size`) in the party. 6. Customize the plot to have a legend and set the size range for markers between 50 and 300. 7. Ensure all unique values in the `day` and `time` variables appear in the legend. 8. Use appropriate labels for the axes and provide a title for the plot. **Function Signature:** ```python def visualize_tips_data(): pass ``` **Expected Function Behaviour:** The function should create and display a scatter plot based on the guidelines above without returning any value. ```python # Example output should be a customized seaborn scatter plot visualization ``` **Constraints:** - You may not change the structure of the \\"tips\\" dataset. - Use seaborn and matplotlib libraries for visualization. - The visualization should be clear, informative, and well-labeled. **Performance:** - The function should efficiently handle the dataset provided by seaborn. **Hint:** Refer to seaborn\'s scatter plot documentation to effectively use parameters such as `hue`, `style`, `size`, and `legend`.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data(): # Load the dataset tips = sns.load_dataset(\'tips\') # Create the scatter plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot( data=tips, x=\'total_bill\', y=\'tip\', hue=\'day\', style=\'time\', size=\'size\', sizes=(50, 300), palette=\'muted\' ) # Customize the plot scatter_plot.set_title(\'Relationship between Total Bill and Tip\') scatter_plot.set_xlabel(\'Total Bill\') scatter_plot.set_ylabel(\'Tip\') plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.) # Display the plot plt.show()"},{"question":"# Python Coding Assessment: Custom Module Executor Objective Implement a simplified version of a module executor that can load and execute a Python script or module from a given path without explicitly importing it. Problem Statement You are required to write a function `custom_run_path(path_name: str, init_globals: dict = None) -> dict` that takes the following parameters: - `path_name`: The file path to the Python script or module you want to execute. - `init_globals`: An optional dictionary that may be used to pre-populate the module\'s globals dictionary before the code is executed. The function should: 1. Locate and execute the Python script or module specified by `path_name`. 2. Return the resulting global namespace dictionary after the script/module has been executed. # Constraints - Your function should handle the creation of a fresh execution context for the script/module. - Implement and manage the special global variables `__name__`, `__file__`, and `__package__`. - Do not use the `runpy` module or any package that simplifies this task. - The `path_name` can refer to a simple Python script file (.py). - Your implementation should handle `init_globals` properly and avoid modifying the passed dictionary. # Input - `path_name` (str): A string path to the Python script/module to execute. - `init_globals` (dict, optional): A dictionary of initial global variables for execution context. Default is `None`. # Output - Returns a dictionary representing the global namespace after executing the script/module. # Examples Example 1: Assume `example.py` file contains: ```python foo = \\"Hello \\" bar = \\"World\\" baz = foo + bar ``` ```python result = custom_run_path(\'example.py\') print(result[\'foo\']) # Output: \\"Hello \\" print(result[\'bar\']) # Output: \\"World\\" print(result[\'baz\']) # Output: \\"Hello World\\" ``` Example 2: Assume `example2.py` file contains: ```python answer = 42 ``` Using initial globals: ```python init_globals = {\'question\': \'What is the meaning of life?\'} result = custom_run_path(\'example2.py\', init_globals) print(result[\'question\']) # Output: \\"What is the meaning of life?\\" print(result[\'answer\']) # Output: 42 ``` # Notes - Ensure that your solution correctly initializes and updates the `__name__`, `__file__`, and `__package__` variables. - Handle any file-related exceptions appropriately.","solution":"import os def custom_run_path(path_name: str, init_globals: dict = None) -> dict: Executes a Python script/module from a given file path and returns its global namespace. Parameters: path_name (str): The file path to the Python script or module to execute. init_globals (dict, optional): An optional dictionary that may be used to pre-populate the module\'s globals dictionary before the code is executed. Default is None. Returns: dict: The resulting global namespace after the script/module has been executed. if init_globals is None: init_globals = {} # Creating a copy of the init_globals to avoid modifying the original dictionary globals_dict = init_globals.copy() # Setting up special global variables globals_dict[\'__name__\'] = \'__main__\' globals_dict[\'__file__\'] = path_name globals_dict[\'__package__\'] = None # Reading and executing the script with open(path_name, \\"r\\") as file: script = file.read() # Execute the script in the prepared globals_dict exec(script, globals_dict) # Return the resulting global namespace return globals_dict"},{"question":"# Python Coding Assessment Question: Complex Path Manipulation and Validation Objective: Create a Python function that processes a list of file paths. The function will normalize each path, check the existence of each path, and return a dictionary categorizing the paths based on their properties (file, directory, link, or non-existing). Function Signature: ```python def process_file_paths(paths: list) -> dict: ``` Input: - `paths`: A list of strings representing file paths. Each path can be a relative path, an absolute path, or contain redundant separators and up-level references. Output: - A dictionary with four keys: `\'files\'`, `\'directories\'`, `\'links\'`, and `\'non_existing\'`. Each key should map to a list of normalized absolute paths that fall into the respective category. - `\'files\'`: List of paths that are regular files. - `\'directories\'`: List of paths that are directories. - `\'links\'`: List of paths that are symbolic links. - `\'non_existing\'`: List of paths that do not exist. Constraints: - Do not access or modify the actual filesystem beyond what is required to verify the path properties. - Paths can include both Unix-style and Windows-style paths, and your function should handle them correctly based on the operating system it is running on. Example: ```python paths = [ \\"foo/bar/../baz\\", \\"/home/user/../user/files.txt\\", \\"~/documents\\", \\"nonexistent_file.txt\\", \\"/var/log/system.log\\" ] result = process_file_paths(paths) print(result) ``` Output on a Unix system where: - `\\"foo/baz\\"` is a directory. - `\\"/home/user/files.txt\\"` is a file. - `\\"~/documents\\"` does not resolve. - `\\"nonexistent_file.txt\\"` does not exist. - `\\"/var/log/system.log\\"` is a file. Result would look like: ```python { \'files\': [\'/home/user/files.txt\', \'/var/log/system.log\'], \'directories\': [\'/path/to/foo/baz\'], \'links\': [], \'non_existing\': [\'/home/user/documents\', \'/path/to/nonexistent_file.txt\'] } ``` Assessment Criteria: - Correct implementation of path normalization. - Accurate categorization of paths. - Handling of both relative and absolute paths. - Proper handling of user home directories and environment variable expansions. - Efficiency in processing the input paths. **Note**: The output format of paths should be normalized and absolutized, and the function must correctly distinguish between files, directories, symbolic links, and non-existing paths based on the operating Python environment.","solution":"import os def process_file_paths(paths: list) -> dict: result = { \'files\': [], \'directories\': [], \'links\': [], \'non_existing\': [] } for path in paths: normalized_path = os.path.abspath(os.path.expanduser(path)) if os.path.islink(normalized_path): result[\'links\'].append(normalized_path) elif os.path.isfile(normalized_path): result[\'files\'].append(normalized_path) elif os.path.isdir(normalized_path): result[\'directories\'].append(normalized_path) else: result[\'non_existing\'].append(normalized_path) return result"},{"question":"**Question: Implementing and Applying Pairwise Metrics and Kernels** **Objective:** Your task is to implement a function that computes various pairwise metrics and kernels for a given dataset and utilizes them in a simple classification task using Support Vector Machine (SVM). **Function Signature:** ```python def compute_metrics_and_classify(X_train, y_train, X_test, y_test): Compute pairwise metrics and kernels for given datasets and classify using SVM. Parameters: - X_train (np.ndarray): Training feature matrix of shape (n_samples_train, n_features) - y_train (np.ndarray): Training labels of shape (n_samples_train,) - X_test (np.ndarray): Testing feature matrix of shape (n_samples_test, n_features) - y_test (np.ndarray): Testing labels of shape (n_samples_test,) Returns: - dict: A dictionary containing classification accuracy for each metric and kernel. Keys are the metric names and values are the respective accuracy scores. pass ``` **Instructions:** 1. Implement the function `compute_metrics_and_classify` to compute the pairwise distances and kernels for given training and testing datasets using the following metrics and kernels: - Euclidean Distance - Manhattan Distance - Cosine Similarity - Linear Kernel - Polynomial Kernel (degree 3) - RBF Kernel 2. Use these computed metrics and kernels to train a Support Vector Machine (SVM) and evaluate the classification accuracy on the testing dataset. 3. For each metric and kernel, the function should train an SVM with the computed distance or similarity matrix as input and compute the accuracy of the trained model on the testing dataset. 4. Return a dictionary containing the classification accuracy for each metric and kernel. **Constraints:** - Use `sklearn.metrics.pairwise` functions for computing distances and kernels. - Use `sklearn.svm.SVC` for training the SVM models. - Ensure the function handles different shapes and sizes of datasets correctly. **Example Usage:** ```python import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from your_module_name import compute_metrics_and_classify # Load Iris dataset data = load_iris() X = data.data y = data.target # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Compute metrics and classify results = compute_metrics_and_classify(X_train, y_train, X_test, y_test) for metric, accuracy in results.items(): print(f\\"{metric} Accuracy: {accuracy:.2f}\\") ``` **Expected Output:** The output should be a dictionary with keys as metric/kernal names and values as respective accuracy scores, for example: ``` { \'euclidean_distance\': 0.98, \'manhattan_distance\': 0.95, \'cosine_similarity\': 0.96, \'linear_kernel\': 0.97, \'polynomial_kernel\': 0.94, \'rbf_kernel\': 0.99 } ``` This question tests the students on their understanding of pairwise metrics and kernels, their ability to apply these to a real-world problem, and the use of SVM for classification tasks with scikit-learn.","solution":"import numpy as np from sklearn.metrics.pairwise import pairwise_distances, linear_kernel, polynomial_kernel, rbf_kernel from sklearn.svm import SVC from sklearn.metrics import accuracy_score def compute_metrics_and_classify(X_train, y_train, X_test, y_test): Compute pairwise metrics and kernels for given datasets and classify using SVM. Parameters: - X_train (np.ndarray): Training feature matrix of shape (n_samples_train, n_features) - y_train (np.ndarray): Training labels of shape (n_samples_train,) - X_test (np.ndarray): Testing feature matrix of shape (n_samples_test, n_features) - y_test (np.ndarray): Testing labels of shape (n_samples_test,) Returns: - dict: A dictionary containing classification accuracy for each metric and kernel. Keys are the metric names and values are the respective accuracy scores. metrics = { \\"euclidean_distance\\": lambda X1, X2: pairwise_distances(X1, X2, metric=\\"euclidean\\"), \\"manhattan_distance\\": lambda X1, X2: pairwise_distances(X1, X2, metric=\\"manhattan\\"), \\"cosine_similarity\\": lambda X1, X2: pairwise_distances(X1, X2, metric=\\"cosine\\") } kernels = { \\"linear_kernel\\": lambda X1, X2: linear_kernel(X1, X2), \\"polynomial_kernel\\": lambda X1, X2: polynomial_kernel(X1, X2, degree=3), \\"rbf_kernel\\": lambda X1, X2: rbf_kernel(X1, X2) } results = {} # Compute distances and train SVM for each metric for name, metric in metrics.items(): dist_train = metric(X_train, X_train) dist_test = metric(X_test, X_train) svm = SVC(kernel=\'precomputed\') svm.fit(dist_train, y_train) predictions = svm.predict(dist_test) accuracy = accuracy_score(y_test, predictions) results[name] = accuracy # Compute kernels and train SVM for each kernel for name, kernel in kernels.items(): kernel_train = kernel(X_train, X_train) kernel_test = kernel(X_test, X_train) svm = SVC(kernel=\'precomputed\') svm.fit(kernel_train, y_train) predictions = svm.predict(kernel_test) accuracy = accuracy_score(y_test, predictions) results[name] = accuracy return results"},{"question":"# Covariance Estimation and Analysis You are tasked with implementing a Python script using the `sklearn.covariance` package to estimate the covariance matrix of given datasets using different estimators. You will then compare the performance of these estimators and interpret the results. Problem Statement 1. **Data Preparation**: - Generate a synthetic dataset with 100 samples and 20 features such that the samples follow a multivariate normal distribution. Add some outliers to the dataset for evaluating robust estimators. - Generate another synthetic dataset with 50 samples and 100 features, ensuring there are fewer samples than features. 2. **Estimator Implementation**: - Fit an Empirical Covariance estimator to both datasets. - Fit a Ledoit-Wolf Shrinkage estimator to both datasets. - Fit an Oracle Approximating Shrinkage (OAS) estimator to both datasets. - Fit a Minimum Covariance Determinant (MCD) robust covariance estimator to the dataset with outliers. 3. **Comparison and Analysis**: - Compare the covariance matrices obtained from each estimator for both datasets. - Analyze the suitability of each estimator for the given datasets, particularly focusing on how each handles the presence of outliers and the situation where the number of samples is less than the number of features. - Use visualization to compare the covariance matrices (heatmaps, eigenvalues plots, etc.). Expected Functions and Outputs 1. **generate_data**: - Input: None - Output: Two datasets - one with outliers and another with more features than samples. 2. **fit_estimators**: - Input: Dataset - Output: A dictionary with keys [\'empirical\', \'ledoit_wolf\', \'oas\', \'mcd\'] and corresponding fitted estimator objects for each key. 3. **compare_estimators**: - Input: Dictionary of fitted estimators, Dataset - Output: Visualizations and analysis summary. 4. **main**: - Input: None - Output: Run the above functions and print final analysis. Constraints and Requirements - Ensure that the estimators are fitted appropriately, considering the parameters like `assume_centered` where applicable. - The script should handle potential exceptions, especially concerning matrix inversion and numerical stability. - The visualizations should clearly distinguish the performance of each estimator, showing how robust covariance estimators deal with outliers and how shrinkage estimators manage scenarios with fewer samples than features. Example Output Visualizations, including: - Heatmaps of covariance matrices. - Plots of eigenvalues of the covariance matrices. - Text output comparing the Mean Squared Errors or other relevant metrics. ```python import numpy as np import matplotlib.pyplot as plt from sklearn.covariance import EmpiricalCovariance, LedoitWolf, OAS, MinCovDet def generate_data(): # Implement data generation with outliers and more features than samples pass def fit_estimators(data): # Fit all estimators and return the fitted estimators pass def compare_estimators(estimators, data): # Compare and visualize the estimators pass def main(): data_with_outliers, data_more_features = generate_data() estimators_with_outliers = fit_estimators(data_with_outliers) estimators_more_features = fit_estimators(data_more_features) compare_estimators(estimators_with_outliers, data_with_outliers) compare_estimators(estimators_more_features, data_more_features) if __name__ == \\"__main__\\": main() ``` Submission Files - Your script file named `covariance_estimators.py`. - A report summarizing the results and visualizations in a PDF file.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.covariance import EmpiricalCovariance, LedoitWolf, OAS, MinCovDet def generate_data(): Generates two synthetic datasets. - One dataset with 100 samples and 20 features with outliers. - Another dataset with 50 samples and 100 features. # Dataset with 100 samples and 20 features with outliers rng = np.random.RandomState(0) data_clean = rng.multivariate_normal(mean=np.zeros(20), cov=np.eye(20), size=95) outliers = rng.uniform(low=-10, high=10, size=(5, 20)) data_with_outliers = np.vstack([data_clean, outliers]) # Dataset with 50 samples and 100 features (more features than samples) data_more_features = rng.multivariate_normal(mean=np.zeros(100), cov=np.eye(100), size=50) return data_with_outliers, data_more_features def fit_estimators(data): Fits various covariance estimators to the data. - Empirical Covariance - Ledoit-Wolf - OAS - MCD (only for data with outliers) Parameters: - data: numpy array of shape (n_samples, n_features) Returns: - Dictionary with fitted estimators estimators = {} estimators[\'empirical\'] = EmpiricalCovariance().fit(data) estimators[\'ledoit_wolf\'] = LedoitWolf().fit(data) estimators[\'oas\'] = OAS().fit(data) if data.shape[1] <= 20: # Assuming the dataset with outliers has at most 20 features estimators[\'mcd\'] = MinCovDet().fit(data) return estimators def compare_estimators(estimators, data): Compares and visualizes the covariance matrices from different estimators. Parameters: - estimators: dictionary of fitted estimators - data: numpy array of shape (n_samples, n_features) cov_matrices = {name: est.covariance_ for name, est in estimators.items()} plt.figure(figsize=(14, 10)) for i, (name, cov_matrix) in enumerate(cov_matrices.items(), 1): plt.subplot(2, 2, i) plt.imshow(cov_matrix, interpolation=\'nearest\', cmap=\'viridis\') plt.title(f\'{name} covariance matrix\') plt.colorbar() plt.tight_layout() plt.show() # Eigenvalue analysis plt.figure(figsize=(14, 7)) for name, cov_matrix in cov_matrices.items(): eigenvalues = np.linalg.eigvalsh(cov_matrix) plt.plot(eigenvalues, label=name) plt.title(\'Eigenvalues of covariance matrices\') plt.xlabel(\'Index\') plt.ylabel(\'Eigenvalue\') plt.legend() plt.show() def main(): Main function that runs the data generation, fitting, and comparison. data_with_outliers, data_more_features = generate_data() print(\\"Fitting estimators for dataset with outliers.\\") estimators_with_outliers = fit_estimators(data_with_outliers) compare_estimators(estimators_with_outliers, data_with_outliers) print(\\"Fitting estimators for dataset with more features than samples.\\") estimators_more_features = fit_estimators(data_more_features) compare_estimators(estimators_more_features, data_more_features) if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Coding Assessment: Using `sqlite3` for a Custom Database Handler **Objective:** Design and implement a Python class `CustomDatabaseHandler` that utilizes the `sqlite3` module to manage a database of student records. This class should demonstrate your understanding of database operations, custom data type handling, and data serialization. **Requirements:** 1. **Initialization:** - The class should initialize a connection to an SQLite database. The database file should be specified upon instantiation. - The class should ensure that a table called `students` exists with the following structure: - `id` (INTEGER PRIMARY KEY) - `name` (TEXT) - `age` (INTEGER) - `grades` (TEXT) - this will store JSON-serialized list of grades 2. **Methods:** - `add_student(name: str, age: int, grades: list) -> None`: Inserts a new student record into the `students` table. - `get_student(student_id: int) -> dict`: Retrieves a student record by `id` and returns it as a dictionary. - `update_student_grades(student_id: int, new_grades: list) -> None`: Updates the `grades` field of a student record. - `delete_student(student_id: int) -> None`: Deletes a student record by `id`. 3. **Custom Type Handling:** - Implement serialization and deserialization for the `grades` field using JSON. **Constraints:** - You must use the `sqlite3` module and handle any exceptions that may arise during database operations. - The `grades` field must be a list of integers, and you should use JSON serialization for storing this list in the SQLite database. **Expected Input and Output:** - `add_student(name: str, age: int, grades: list)`: No return value. - `get_student(student_id: int)`: Returns a dictionary with keys `id`, `name`, `age`, and `grades`. - `update_student_grades(student_id: int, new_grades: list)`: No return value. - `delete_student(student_id: int)`: No return value. **Example:** ```python db_handler = CustomDatabaseHandler(\\"students.db\\") db_handler.add_student(name=\\"Alice\\", age=20, grades=[85, 90, 78]) print(db_handler.get_student(student_id=1)) # Output: {\'id\': 1, \'name\': \'Alice\', \'age\': 20, \'grades\': [85, 90, 78]} db_handler.update_student_grades(student_id=1, new_grades=[88, 92, 81]) print(db_handler.get_student(student_id=1)) # Output: {\'id\': 1, \'name\': \'Alice\', \'age\': 20, \'grades\': [88, 92, 81]} db_handler.delete_student(student_id=1) print(db_handler.get_student(student_id=1)) # Output: None ``` **Notes:** - You are required to write the complete implementation of the `CustomDatabaseHandler` class. - Ensure your code handles all edge cases and potential errors gracefully (e.g., invalid student IDs, database connection issues).","solution":"import sqlite3 import json class CustomDatabaseHandler: def __init__(self, db_name: str): self.connection = sqlite3.connect(db_name) self._create_students_table() def _create_students_table(self): with self.connection: self.connection.execute(\'\'\' CREATE TABLE IF NOT EXISTS students ( id INTEGER PRIMARY KEY, name TEXT, age INTEGER, grades TEXT ) \'\'\') def add_student(self, name: str, age: int, grades: list) -> None: grades_json = json.dumps(grades) with self.connection: self.connection.execute(\'\'\' INSERT INTO students (name, age, grades) VALUES (?, ?, ?) \'\'\', (name, age, grades_json)) def get_student(self, student_id: int) -> dict: cursor = self.connection.cursor() cursor.execute(\'SELECT * FROM students WHERE id = ?\', (student_id,)) row = cursor.fetchone() if row: student = { \'id\': row[0], \'name\': row[1], \'age\': row[2], \'grades\': json.loads(row[3]) } return student return None def update_student_grades(self, student_id: int, new_grades: list) -> None: new_grades_json = json.dumps(new_grades) with self.connection: self.connection.execute(\'\'\' UPDATE students SET grades = ? WHERE id = ? \'\'\', (new_grades_json, student_id)) def delete_student(self, student_id: int) -> None: with self.connection: self.connection.execute(\'\'\' DELETE FROM students WHERE id = ? \'\'\', (student_id,)) def close(self): self.connection.close()"},{"question":"You are tasked to implement a function that simulates some functionalities of the Unix shadow password database using objects and dictionaries, mimicking the behavior of the `spwd` module. Since the actual `spwd` module operations are typically unavailable due to privilege restrictions, this simulation will help demonstrate your understanding of data handling and module functionalities. # Task: Create a class `ShadowPasswordDB` with the following methods: 1. `add_user(username: str, encrypted_password: str, last_change: int, min_days: int, max_days: int, warn_days: int, inact_days: int, expire_days: int, flag: int) -> None`: This method takes a user\'s attributes and adds a simulated shadow password database entry for the user. 2. `get_user(username: str) -> dict`: This method retrieves the shadow password database entry (represented as a dictionary) for a given username. If the user does not exist, raise a `KeyError`. 3. `get_all_users() -> list`: This method returns a list of all shadow password database entries (each entry presented as a dictionary) in any order. # Constraints: - Usernames are unique. - Fields such as `last_change`, `min_days`, `max_days`, `warn_days`, `inact_days`, `expire_days`, and `flag` should be non-negative integers. - `encrypted_password` should be a non-empty string. # Example Usage: ```python db = ShadowPasswordDB() db.add_user(\'john\', \'s3cr3t\', 18000, 7, 90, 14, 30, 90, 0) db.add_user(\'jane\', \'h3ll0\', 18005, 10, 120, 10, 50, 120, 1) john_entry = db.get_user(\'john\') # Expected output: {\'username\': \'john\', \'encrypted_password\': \'s3cr3t\', \'last_change\': 18000, \'min_days\': 7, \'max_days\': 90, \'warn_days\': 14, \'inact_days\': 30, \'expire_days\': 90, \'flag\': 0} all_users = db.get_all_users() # Expected output: [{\'username\': \'john\', ..., \'flag\': 0}, {\'username\': \'jane\', ..., \'flag\': 1}] ``` Implement the `ShadowPasswordDB` class and its methods to satisfy the requirements above.","solution":"class ShadowPasswordDB: def __init__(self): self.users = {} def add_user(self, username, encrypted_password, last_change, min_days, max_days, warn_days, inact_days, expire_days, flag): if not encrypted_password: raise ValueError(\\"Encrypted password must be a non-empty string\\") if any(not isinstance(i, int) or i < 0 for i in [last_change, min_days, max_days, warn_days, inact_days, expire_days, flag]): raise ValueError(\\"All time and flag fields must be non-negative integers\\") self.users[username] = { \'username\': username, \'encrypted_password\': encrypted_password, \'last_change\': last_change, \'min_days\': min_days, \'max_days\': max_days, \'warn_days\': warn_days, \'inact_days\': inact_days, \'expire_days\': expire_days, \'flag\': flag } def get_user(self, username): if username not in self.users: raise KeyError(f\\"User \'{username}\' not found\\") return self.users[username] def get_all_users(self): return list(self.users.values())"},{"question":"# Seaborn Coding Assessment Objective The goal of this assessment is to test your understanding of the Seaborn library, specifically the `rugplot` function and its integration with other Seaborn plotting functions. Problem Statement You are given a dataset containing information about the tips received by waiters in a restaurant (`tips` dataset). Your task is to create a joint plot that includes: 1. A scatter plot of `total_bill` vs `tip`. 2. A linear regression line fitting the data. 3. Rug plots along both axes to show the density of the data points. 4. Different colors for the rugs based on the `time` of the day (Lunch or Dinner). 5. Adjust the thickness and height of the rugs for better visualization. Write a Python function `create_jointplot_with_rugs` that performs the following: 1. Loads the `tips` dataset provided by Seaborn. 2. Creates a joint plot with the requirements mentioned above. 3. The resulting plot should be saved as a PNG file named `jointplot_rugs.png`. Function Signature ```python def create_jointplot_with_rugs(): pass ``` Expected Output A PNG file named `jointplot_rugs.png` with the described plot features. Constraints - You must use the Seaborn library for plotting. - The function should save the plot as `jointplot_rugs.png` in the current working directory. - Ensure the rugs do not overlap significantly with the scatter points for clarity. Sample Code ```python import seaborn as sns import matplotlib.pyplot as plt def create_jointplot_with_rugs(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create the joint plot joint_plot = sns.jointplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", kind=\\"scatter\\", hue=\\"time\\") # Add a linear regression line sns.regplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", scatter=False, ax=joint_plot.ax_joint) # Add rug plots along both axes sns.rugplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", height=0.05, ax=joint_plot.ax_marg_x) sns.rugplot(data=tips, y=\\"tip\\", hue=\\"time\\", height=0.05, ax=joint_plot.ax_marg_y) # Save the plot plt.savefig(\'jointplot_rugs.png\') # Example usage create_jointplot_with_rugs() ``` Note: The sample code provided is a starting point. You are encouraged to tweak and improve the visualization as per the requirements. Evaluation Criteria - Correctness: Does the function generate the desired plot with all specified features? - Clarity: Are the plot and rugs clear and well-differentiated by color and height? - Code Quality: Is the code well-structured, and does it follow good coding practices?","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_jointplot_with_rugs(): Creates a joint plot for `total_bill` vs `tip` with a regression line, and rug plots colored by `time` (Lunch or Dinner). The plot is saved as \'jointplot_rugs.png\'. # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create the joint plot joint_plot = sns.jointplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", kind=\\"scatter\\") # Add a linear regression line sns.regplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", scatter=False, ax=joint_plot.ax_joint) # Add rug plots for x and y with different colors for time sns.rugplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", height=0.05, ax=joint_plot.ax_marg_x) sns.rugplot(data=tips, y=\\"tip\\", hue=\\"time\\", height=0.05, ax=joint_plot.ax_marg_y) # Save the plot plt.savefig(\'jointplot_rugs.png\') plt.close() create_jointplot_with_rugs()"},{"question":"**Seaborn Coding Assessment Question** # Objective: To assess your understanding of the seaborn library, particularly its ability to create and manipulate count plots. # Problem Statement: You will be provided with a dataset and you must use seaborn to create a specific visualization and extract some insights from it. # Tasks: 1. **Load the Dataset:** - Load the Titanic dataset using `seaborn.load_dataset(\\"titanic\\")`. 2. **Basic Visualization:** - Create a count plot showing the distribution of passengers across different classes (`\'class\'` column). - Save this plot as `\'basic_countplot.png\'`. 3. **Grouped Visualization:** - Create a count plot showing the distribution of passengers across different classes, grouped by whether they survived (`\'survived\'` column). - Save this plot as `\'grouped_countplot.png\'`. 4. **Normalized Visualization:** - Create a count plot similar to the grouped visualization but normalize the counts to show the percentages. - Save this plot as `\'normalized_countplot.png\'`. 5. **Advanced Task:** - Enhance the normalized count plot to show the percentage labels on top of each bar. This will require you to explore the seaborn and matplotlib documentation to add annotations on the plot. - Save this enhanced plot as `\'enhanced_normalized_countplot.png\'`. # Input and Output: - The dataset will be loaded using seaborn\'s built-in `load_dataset` function. - Expected output are four PNG files containing the described visualizations. # Constraints: - Ensure the visualizations are saved with readable axis labels and titles. - Use `seaborn` for generating plots. - You may use `matplotlib` only for customization purposes (e.g., adding text labels to bars). # Submission: Submit the Python code (.py file) used to generate these visualizations along with the four PNG files.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create a count plot showing the distribution of passengers across different classes plt.figure(figsize=(10, 6)) sns.countplot(data=titanic, x=\'class\') plt.title(\'Distribution of Passengers Across Different Classes\') plt.xlabel(\'Class\') plt.ylabel(\'Count\') plt.savefig(\'basic_countplot.png\') plt.close() # Create a count plot showing the distribution of passengers across different classes, grouped by survival plt.figure(figsize=(10, 6)) sns.countplot(data=titanic, x=\'class\', hue=\'survived\') plt.title(\'Distribution of Passengers Across Different Classes (Grouped by Survival)\') plt.xlabel(\'Class\') plt.ylabel(\'Count\') plt.legend(title=\'Survived\', labels=[\'No\', \'Yes\']) plt.savefig(\'grouped_countplot.png\') plt.close() # Create a normalized count plot showing percentages plt.figure(figsize=(10, 6)) class_survived_counts = titanic.groupby([\'class\', \'survived\']).size().unstack().fillna(0) class_counts = class_survived_counts.sum(axis=1) norm_counts = class_survived_counts.divide(class_counts, axis=0) norm_counts.plot(kind=\'bar\', stacked=True) plt.title(\'Normalized Distribution of Passengers Across Different Classes (Grouped by Survival)\') plt.xlabel(\'Class\') plt.ylabel(\'Percentage\') plt.legend(title=\'Survived\', labels=[\'No\', \'Yes\']) plt.savefig(\'normalized_countplot.png\') plt.close() # Enhance the normalized count plot to show percentage labels on top of each bar plt.figure(figsize=(10, 6)) bars = norm_counts.plot(kind=\'bar\', stacked=True) bars.set_title(\'Enhanced Normalized Distribution of Passengers Across Different Classes (Grouped by Survival)\') bars.set_xlabel(\'Class\') bars.set_ylabel(\'Percentage\') bars.legend(title=\'Survived\', labels=[\'No\', \'Yes\']) # Add percentage labels on top of each bar for i, patch in enumerate(bars.patches): width, height = patch.get_width(), patch.get_height() x, y = patch.get_xy() bars.text(x + width/2, y + height/2, \'{:.0%}\'.format(height), ha=\'center\', va=\'center\') plt.savefig(\'enhanced_normalized_countplot.png\') plt.close()"},{"question":"**Objective**: Evaluate the student\'s understanding of function implementation, pattern matching (new in Python 3.10), and exception handling. # Problem Statement You are required to implement a Python function named `process_data` that takes a list of dictionaries as input. Each dictionary represents a record of a product with the following key-value pairs: - `category` (str): The category of the product (e.g., electronics, clothing, etc.) - `price` (float): The price of the product - `details` (str): Details about the product Your function should perform the following tasks: 1. **Pattern Match** on the `category` key to identify the product category. 2. If the category matches specific strings (e.g., \\"electronics\\", \\"clothing\\"), call corresponding helper functions to process those categories. 3. If the category is not recognized, raise a `ValueError`. 4. Use appropriate exception handling to ensure that even if one record processing fails, the function should continue to process the remaining records. 5. Return a dictionary where the keys are the categories and the values are the lists of processed products for each category. # Helper Functions You need to implement the following helper functions as part of your solution: - `process_electronics(details: str) -> str`: Processes the electronics details and returns a formatted string. - `process_clothing(details: str) -> str`: Processes clothing details and returns a formatted string. # Expected Input and Output **Input**: - A list of dictionaries with the keys `category`, `price`, and `details`. **Output**: - A dictionary with categories as keys and lists of processed product detail strings as values. # Example ```python data = [ {\\"category\\": \\"electronics\\", \\"price\\": 299.99, \\"details\\": \\"Smartphone with 4GB RAM\\"}, {\\"category\\": \\"clothing\\", \\"price\\": 49.99, \\"details\\": \\"Cotton t-shirt, size L\\"}, {\\"category\\": \\"gadgets\\", \\"price\\": 19.99, \\"details\\": \\"Portable charger\\"}, ] output = process_data(data) print(output) ``` **Expected Output**: ```python { \\"electronics\\": [\\"Processed Electronics: Smartphone with 4GB RAM\\"], \\"clothing\\": [\\"Processed Clothing: Cotton t-shirt, size L\\"] } ``` # Constraints 1. The input list will not be empty. 2. The `price` values will be positive floating-point numbers. 3. The `details` values will be non-empty strings. # Implementation Notes - Utilize the `match` statement introduced in Python 3.10 for pattern matching. - Use `try` and `except` blocks for exception handling to deal with unrecognized categories. # Function Signature ```python def process_data(data: list[dict]) -> dict: pass def process_electronics(details: str) -> str: return f\\"Processed Electronics: {details}\\" def process_clothing(details: str) -> str: return f\\"Processed Clothing: {details}\\" ```","solution":"def process_electronics(details: str) -> str: return f\\"Processed Electronics: {details}\\" def process_clothing(details: str) -> str: return f\\"Processed Clothing: {details}\\" def process_data(data: list[dict]) -> dict: result = {} for record in data: try: category = record[\\"category\\"] details = record[\\"details\\"] match category: case \\"electronics\\": processed = process_electronics(details) case \\"clothing\\": processed = process_clothing(details) case _: raise ValueError(f\\"Unrecognized category: {category}\\") if category in result: result[category].append(processed) else: result[category] = [processed] except ValueError as e: print(e) return result"},{"question":"# Question In this coding assessment, you will demonstrate your understanding of CUDA streams and synchronization in PyTorch by performing the following tasks: 1. Write a function `detect_data_race` that: - Initializes a tensor on the default stream. - Modifies the tensor on a new stream without synchronization. - Detects a possible data race using the CUDA Stream Sanitizer. 2. Write a function `fix_data_race` that: - Performs the same operations as `detect_data_race`. - Ensures proper synchronization to prevent the data race. # Function Signatures ```python def detect_data_race() -> None: Initializes a tensor on the default stream and modifies it on a new stream without synchronization. Runs the CUDA Stream Sanitizer to detect a possible data race. Prints the CUDA Stream Sanitizer output to the console. pass def fix_data_race() -> None: Initializes a tensor on the default stream and modifies it on a new stream with proper synchronization. Ensures no data race occurs and prints confirmation to the console. pass ``` # Constraints - You must use `torch` and `torch.cuda` libraries for tensor operations and CUDA stream management. - The `detect_data_race` function should simulate the scenario described in the documentation, where a tensor is accessed by multiple streams without synchronization. - The `fix_data_race` function should fix the synchronization issue using appropriate methods as demonstrated in the documentation. # Example Output Running `detect_data_race()` should produce output similar to: ```text ============================ CSAN detected a possible data race on tensor with data pointer 139719969079296 ... ``` Running `fix_data_race()` should confirm there are no synchronization issues: ```text No errors reported. ``` You may need to set the environment variable `TORCH_CUDA_SANITIZER=1` before running these functions to enable the CUDA Stream Sanitizer. Ensure to test your functions and verify that the outputs match the expected behavior as described above.","solution":"import torch def detect_data_race() -> None: Initializes a tensor on the default stream and modifies it on a new stream without synchronization. Runs the CUDA Stream Sanitizer to detect a possible data race. Prints the CUDA Stream Sanitizer output to the console. # Make sure CUDA is available if not torch.cuda.is_available(): print(\\"CUDA is not available on this device.\\") return # Initialize tensor on the default stream tensor = torch.zeros(10, device=\'cuda\') # Create a new stream and modify the tensor without synchronization stream = torch.cuda.Stream() with torch.cuda.stream(stream): tensor += 1 # Copy tensor back with another modification to trigger potential data race for illustration tensor += 1 # Synchronize streams and devices torch.cuda.synchronize() # Print a message (CUDA Stream Sanitizer output would be expected here) print(\\"CUDA Stream Sanitizer would detect a possible data race here if enabled.\\") def fix_data_race() -> None: Initializes a tensor on the default stream and modifies it on a new stream with proper synchronization. Ensures no data race occurs and prints confirmation to the console. # Make sure CUDA is available if not torch.cuda.is_available(): print(\\"CUDA is not available on this device.\\") return # Initialize tensor on the default stream tensor = torch.zeros(10, device=\'cuda\') # Create a new stream and modify the tensor with proper synchronization stream = torch.cuda.Stream() with torch.cuda.stream(stream): tensor += 1 # Synchronize streams before accessing the tensor from the default stream stream.synchronize() # Additional modification on the default stream tensor += 1 # Synchronize devices torch.cuda.synchronize() # No errors/report expected print(\\"No synchronization errors reported. Properly synchronized.\\")"},{"question":"**Objective**: Demonstrate comprehension of iterating over email message objects using the `email.iterators` package. **Problem Statement**: You are provided with a large body of email messages, and you are tasked with extracting specific information from these messages. Your goal is to implement a function that processes the message objects to extract all the plain text contents while skipping any attachments or non-text parts. **Function Signature**: ```python def extract_plain_text_content(message): Extract all plain text content from an email message object. Parameters: message (email.message.Message): The email message object. Returns: str: A concatenation of all the plain text lines from the message. ``` **Input**: - `message` (email.message.Message): An email message object. The message can contain multiple parts and subparts, with mixed content types (e.g., text/plain, text/html, or attachments). **Output**: - A string that contains all the plain-text contents concatenated from the message body. All non-text parts should be ignored. **Constraints**: - The function should use `email.iterators` for its implementation. - Only the text parts with MIME type `text/plain` should be included in the output. - The order of text content should be preserved as it appears in the email message. **Example**: Assume you have an email message object `msg` (you can create one for testing purposes using the `email` library\'s message creation functions). ```python >>> from email import message_from_string >>> msg_content = \'\'\'Content-Type: multipart/mixed; boundary=\\"===============4211235414746732593==\\" ... --===============4211235414746732593== Content-Type: text/plain This is the plain text part of the email. --===============4211235414746732593== Content-Type: application/octet-stream Content-Transfer-Encoding: base64 VGhpcyBpcyBhbiBhdHRhY2htZW50Lg== --===============4211235414746732593== Content-Type: text/plain Another plain text part. --===============4211235414746732593==--\'\'\' >>> msg = message_from_string(msg_content) >>> print(extract_plain_text_content(msg)) This is the plain text part of the email. Another plain text part. ``` You have access to the `email.iterators` documentation provided above. Utilize the functions `body_line_iterator` and `typed_subpart_iterator` appropriately to achieve the desired output. **Note**: Ensure your function handles nested multipart structures correctly and filters out only the `text/plain` parts.","solution":"from email import message_from_string from email.iterators import typed_subpart_iterator def extract_plain_text_content(message): Extract all plain text content from an email message object. Parameters: message (email.message.Message): The email message object. Returns: str: A concatenation of all the plain text lines from the message. text_content = [] for part in typed_subpart_iterator(message, \'text\', \'plain\'): text_content.append(part.get_payload(decode=True).decode(part.get_content_charset() or \'utf-8\')) return \\"n\\".join(text_content)"},{"question":"# Buffer Protocol Implementation and Manipulation **Problem Statement:** You are designing a class `BufferedArray` that interacts with raw memory buffers using Python\'s `memoryview` and `bytearray`. Your task is to implement this class to handle various buffer-related operations. The class `BufferedArray` should support the following functionalities: 1. **Initialization** - Initialize the object using a `bytearray` buffer. 2. **Read slices** - Ability to read slices of the buffer using `memoryview`. 3. **Read items** - Ability to read individual items from the buffer. 4. **Write items** - Ability to write individual items to the buffer. 5. **Verify contiguity** - Method to verify if the buffer is C-contiguous. 6. **Export buffer info** - Method to return information about the buffer (like length, item size, format, ndim, shape, strides). Your implementation must adhere to the following specifications: 1. The `__init__()` method should accept a `bytearray` and create a `memoryview`. 2. The `read_slice()` method should return a slice from the buffer using a given start and stop index. 3. The `read_item()` method should return an item at a specific index. 4. The `write_item()` method should insert an item at a specific index. 5. The `is_contiguous_c()` method should return `True` if the buffer is C-contiguous and `False` otherwise. 6. The `get_buffer_info()` method should return a dictionary containing buffer information (`length`, `itemsize`, `format`, `ndim`, `shape`, `strides`). **Constraints:** - The `bytearray` buffer should contain only integers in the range 0-255. - You cannot use any external libraries other than Python\'s standard library. - Assume the buffer uses an item size of 1, and the format is unsigned bytes. **Example:** ```python # Example usage: buffer = bytearray([1, 2, 3, 4, 5]) b_array = BufferedArray(buffer) # Read slice print(b_array.read_slice(1, 4)) # Output: bytearray(b\'x02x03x04\') # Read item print(b_array.read_item(2)) # Output: 3 # Write item b_array.write_item(1, 10) print(buffer) # Output: bytearray(b\'x01nx03x04x05\') # Check contiguity print(b_array.is_contiguous_c()) # Output: True # Get buffer info print(b_array.get_buffer_info()) # Output: {\'length\': 5, \'itemsize\': 1, \'format\': \'B\', \'ndim\': 1, \'shape\': (5,), \'strides\': (1,)} ``` **Implementation:** Start by implementing the `BufferedArray` class with the described methods. Ensure all methods handle the provided constraints appropriately. ```python class BufferedArray: def __init__(self, buffer): self.buffer = buffer self.view = memoryview(buffer) def read_slice(self, start, stop): return bytearray(self.view[start:stop]) def read_item(self, index): return self.view[index] def write_item(self, index, value): self.view[index] = value def is_contiguous_c(self): return self.view.contiguous def get_buffer_info(self): return { \'length\': self.view.nbytes, \'itemsize\': self.view.itemsize, \'format\': self.view.format, \'ndim\': self.view.ndim, \'shape\': self.view.shape, \'strides\': self.view.strides, } ``` **Note:** Ensure your implementation correctly handles the buffer and meets all requirements. Test your code with different buffer contents and edge cases to guarantee its robustness and correctness.","solution":"class BufferedArray: def __init__(self, buffer): self.buffer = buffer self.view = memoryview(buffer) def read_slice(self, start, stop): return bytearray(self.view[start:stop]) def read_item(self, index): return self.view[index] def write_item(self, index, value): self.view[index] = value def is_contiguous_c(self): return self.view.contiguous def get_buffer_info(self): return { \'length\': self.view.nbytes, \'itemsize\': self.view.itemsize, \'format\': self.view.format, \'ndim\': self.view.ndim, \'shape\': self.view.shape, \'strides\': self.view.strides, }"},{"question":"**Coding Question: WAV File Processing in Python** **Objective**: Write a Python function that reads a WAV file, extracts metadata and audio frames, modifies certain parameters, and then writes the modified audio to a new WAV file. **Function Signature**: ```python def process_wav_file(input_file: str, output_file: str, new_channels: int, new_frame_rate: int) -> None: pass ``` **Description**: Implement the `process_wav_file` function to perform the following tasks: 1. Open the input WAV file in read mode using the `wave` module. 2. Extract the following metadata from the input file: - Number of channels - Sample width - Frame rate - Number of frames 3. Read all audio frames from the input file. 4. Open the output WAV file in write mode. 5. Set the following parameters for the output file: - Number of channels to `new_channels` - Sample width to the same sample width as the input file - Frame rate to `new_frame_rate` 6. Write the extracted audio frames to the output file. 7. Close both the input and output WAV files. **Constraints**: - The input WAV file will always be in PCM format. - You must handle files with a maximum size of 50 MB. - The `new_channels` value will be either 1 (mono) or 2 (stereo). - The `new_frame_rate` value will be a positive integer. **Example**: ```python # Example usage process_wav_file(\'input.wav\', \'output.wav\', 2, 44100) ``` In this example, suppose \'input.wav\' is a mono (1 channel) WAV file with a sample width of 2 bytes and a frame rate of 22050 Hz. The function should read the metadata and audio frames from \'input.wav\', then write them to \'output.wav\' with 2 channels and a frame rate of 44100 Hz while keeping the same sample width. **Evaluation Criteria**: - Correctness: The function should correctly read, modify, and write WAV file data as specified. - Robustness: The function should handle potential errors gracefully, ensuring the input file is read correctly, and the output file is written with accurate modifications. - Efficiency: The function should manage memory efficiently, especially since it deals with potentially large audio files.","solution":"import wave def process_wav_file(input_file: str, output_file: str, new_channels: int, new_frame_rate: int) -> None: Processes a WAV file by modifying its number of channels and frame rate. :param input_file: Path to the input WAV file. :param output_file: Path to the output WAV file. :param new_channels: New number of channels (1 for mono, 2 for stereo). :param new_frame_rate: New frame rate. # Open the input WAV file in read mode with wave.open(input_file, \'rb\') as wav_in: # Extract metadata from the input file num_channels = wav_in.getnchannels() sample_width = wav_in.getsampwidth() frame_rate = wav_in.getframerate() num_frames = wav_in.getnframes() # Read all audio frames from the input file audio_frames = wav_in.readframes(num_frames) # Open the output WAV file in write mode with wave.open(output_file, \'wb\') as wav_out: # Set the parameters for the output file wav_out.setnchannels(new_channels) wav_out.setsampwidth(sample_width) wav_out.setframerate(new_frame_rate) # Write the extracted audio frames to the output file wav_out.writeframes(audio_frames) print(f\\"Converted {input_file} to {output_file} with {new_channels} channels and {new_frame_rate} frame rate.\\")"},{"question":"Coding Assessment Question # Objective: To assess the understanding of Python\'s `copyreg` module and its use in customizing the pickling process for different objects. # Problem Statement: You are tasked with creating a custom serialization mechanism for a non-trivial object using Python\'s `copyreg` module. Specifically, you will implement a class representing a `Book` with certain attributes, and register custom pickling behavior for this class. # Requirements: 1. **Class Definition**: - Define a class `Book` with the following attributes: - `title` (string) - `author` (string) - `pages` (integer) - `published_year` (integer) 2. **Pickle Function**: - Implement a custom pickling function for the `Book` class. This function should handle serialization of the `Book` object and must be registered using `copyreg.pickle`. 3. **Constructor Registration**: - Ensure the `Book` class can be correctly reconstructed during the unpickling process. # Constraints: - The `title` and `author` attributes should be non-empty strings. - The `pages` attribute should be a positive integer (greater than 0). - The `published_year` should be a four-digit integer representing a valid year. # Expected Input and Output Formats: ```python class Book: def __init__(self, title: str, author: str, pages: int, published_year: int): # Your implementation here def pickle_book(book: Book): # Your implementation here # Example usage: import pickle import copyreg # Register the pickle function copyreg.pickle(Book, pickle_book) # Create a Book instance book = Book(\\"1984\\", \\"George Orwell\\", 328, 1949) # Serialize the Book instance serialized_book = pickle.dumps(book) # Deserialize the Book instance deserialized_book = pickle.loads(serialized_book) ``` # Performance: - Ensure your code performs efficiently with typical use cases involving serialization and deserialization of `Book` objects.","solution":"import copyreg import pickle class Book: def __init__(self, title: str, author: str, pages: int, published_year: int): if not title: raise ValueError(\\"Title must be a non-empty string\\") if not author: raise ValueError(\\"Author must be a non-empty string\\") if pages <= 0: raise ValueError(\\"Pages must be a positive integer\\") if not (1000 <= published_year <= 9999): raise ValueError(\\"Published year must be a four-digit integer\\") self.title = title self.author = author self.pages = pages self.published_year = published_year def pickle_book(book): return Book, (book.title, book.author, book.pages, book.published_year) copyreg.pickle(Book, pickle_book) # Example usage: book = Book(\\"1984\\", \\"George Orwell\\", 328, 1949) serialized_book = pickle.dumps(book) deserialized_book = pickle.loads(serialized_book)"},{"question":"**Objective:** Implement a function that processes a list of IP network definitions, performs various operations to determine and return specific information about these networks. **Function Signature:** ```python def process_networks(networks: list) -> dict: This function takes a list of IP network definitions (strings) and performs various operations: - Determines the IP version of each network. - Counts the total number of IPv4 and IPv6 addresses across all networks. - Identifies if a given IP address belongs to any network in the list. Args: networks (list): A list of IP network definitions in string format (e.g., [\\"192.0.2.0/24\\", \\"2001:db8::/96\\"]) Returns: dict: A dictionary containing: - \'versions\': A list of tuples with each network and its corresponding IP version. - \'total_ipv4_addresses\': Total number of IPv4 addresses across all networks. - \'total_ipv6_addresses\': Total number of IPv6 addresses across all networks. - \'address_belongs\': A dictionary where keys are specified IP addresses and values are lists of networks they belong to. ``` **Input:** - A list of IP network definitions (strings). **Output:** - A dictionary containing: - `\'versions\'`: A list of tuples, each containing a network and its corresponding IP version (either 4 or 6). - `\'total_ipv4_addresses\'`: The total number of IPv4 addresses from all networks. - `\'total_ipv6_addresses\'`: The total number of IPv6 addresses from all networks. - `\'address_belongs\'`: A dictionary with given IP addresses as keys and lists of networks they belong to as values. Assume a static list of IP addresses for this check: `[\\"192.0.2.1\\", \\"2001:db8::1\\", \\"192.168.1.1\\"]`. **Constraints:** - Network definitions must be valid, otherwise raise a `ValueError`. - Assume the given IP addresses for the `\'address_belongs\'` check are always valid. **Example:** ```python networks = [\\"192.0.2.0/24\\", \\"2001:db8::/96\\"] expected_output = { \'versions\': [(\'192.0.2.0/24\', 4), (\'2001:db8::/96\', 6)], \'total_ipv4_addresses\': 256, \'total_ipv6_addresses\': 4294967296, \'address_belongs\': { \\"192.0.2.1\\": [\\"192.0.2.0/24\\"], \\"2001:db8::1\\": [\\"2001:db8::/96\\"], \\"192.168.1.1\\": [] } } assert process_networks(networks) == expected_output ``` # Instructions: 1. Read and analyze the provided network strings. 2. Use appropriate `ipaddress` module functions and classes to create and manipulate network objects. 3. Implement logic to gather required information: - Identify the IP version of each network. - Count the total number of addresses (both IPv4 and IPv6) across all networks. - Check if the given static IP addresses belong to any of the networks and list those networks. 4. Raise a `ValueError` if any network string is invalid. **Hints:** - Use `ipaddress.ip_network` to create network objects. - Use `.version` to determine the IP version. - Use `.num_addresses` to count the number of addresses in a network. - Use the `in` keyword or `.hosts()` method to check if an address belongs to a network.","solution":"import ipaddress def process_networks(networks: list) -> dict: static_ips = [\\"192.0.2.1\\", \\"2001:db8::1\\", \\"192.168.1.1\\"] versions = [] total_ipv4_addresses = 0 total_ipv6_addresses = 0 address_belongs = {ip: [] for ip in static_ips} for network_str in networks: try: network = ipaddress.ip_network(network_str) except ValueError: raise ValueError(f\\"Invalid network definition: {network_str}\\") versions.append((network_str, network.version)) if network.version == 4: total_ipv4_addresses += network.num_addresses elif network.version == 6: total_ipv6_addresses += network.num_addresses for ip in static_ips: ip_address = ipaddress.ip_address(ip) if ip_address in network: address_belongs[ip].append(network_str) return { \'versions\': versions, \'total_ipv4_addresses\': total_ipv4_addresses, \'total_ipv6_addresses\': total_ipv6_addresses, \'address_belongs\': address_belongs }"},{"question":"# Question: Implement Fake Tensor Operations You are required to implement a simplified version of a key operation in handling \\"fake tensors\\" based on the PyTorch package. The goal of this task is to demonstrate your understanding of this concept and your ability to work with tensor subclasses and manage tensor metadata. Task 1. **Define a `FakeTensor` Class**: - Create a `FakeTensor` class that subclasses `torch.Tensor`. - Implement a method to convert a real tensor to a fake tensor while maintaining its metadata. 2. **Create a `FakeTensorMode` Context Manager**: - Implement a context manager `FakeTensorMode` that activates the fake tensor mode. - Ensure that in the `FakeTensorMode`, operations on tensors should automatically convert real tensors to fake tensors (if not already fake). 3. **Function Implementation**: - Implement a function `fake_tensor_operations` that takes a real tensor, converts it to a fake tensor using the `FakeTensorMode`, performs a specified operation (e.g., element-wise multiplication), and returns the resulting fake tensor. Input - A real tensor `x` of arbitrary size and type. - A scalar `factor` for the element-wise multiplication. Output - A fake tensor that is the result of the element-wise multiplication of the original real tensor `x` by `factor`. Constraints - You should ensure that the fake tensor maintains the original metadata (like shape, dtype). - You may assume that the input tensor and scalar are valid and do not require additional validation. Example ```python import torch # Example Implementation (simplified, see details for full implementation) class FakeTensor(torch.Tensor): def __init__(self, tensor): super().__init__() self.fake_data = None self.metadata = tensor.size(), tensor.dtype def from_real_tensor(tensor): return FakeTensor(tensor) class FakeTensorMode: def __enter__(self): # activation logic pass def __exit__(self, exc_type, exc_value, traceback): # deactivation logic pass def fake_tensor_operations(x, factor): with FakeTensorMode(): fx = from_real_tensor(x) fx.fake_data = torch.empty(fx.metadata[0], dtype=fx.metadata[1]) result = fx.fake_data * factor return fx # Real tensor x = torch.tensor([1.0, 2.0, 3.0]) factor = 2.0 # Fake tensor computation result = fake_tensor_operations(x, factor) print(result) # Should print the fake tensor with appropriate metadata ``` Notes - Provide a complete implementation of the `FakeTensor`, `FakeTensorMode`, and `fake_tensor_operations` functions. - Do not import any extra libraries other than `torch`.","solution":"import torch class FakeTensor(torch.Tensor): def __init__(self, tensor): self.fake_data = tensor.clone() self.metadata = (tensor.size(), tensor.dtype) @staticmethod def from_real_tensor(tensor): return FakeTensor(tensor) class FakeTensorMode: def __enter__(self): torch._C._fake_mode_ = True def __exit__(self, exc_type, exc_value, traceback): torch._C._fake_mode_ = False def fake_tensor_operations(x, factor): with FakeTensorMode(): fx = FakeTensor.from_real_tensor(x) fx.fake_data = x * factor return fx"},{"question":"# Question: Advanced Seaborn Stripplot and Facet Plot You are given a dataset containing information about customer tips in a restaurant, which includes the following columns: - `total_bill`: Total bill amount. - `tip`: Tip given. - `sex`: Gender of the person who paid the bill (Male/Female). - `smoker`: Whether the person was a smoker (Yes/No). - `day`: Day of the week (Thur/Fri/Sat/Sun). - `time`: Time of the meal (Lunch/Dinner). - `size`: Size of the group. The dataset can be loaded using: ```python import seaborn as sns tips = sns.load_dataset(\\"tips\\") ``` Your task is to write a function named `create_strip_and_facet_plot` that performs the following: 1. Creates a strip plot to show the distribution of total bill amounts for different days of the week, distinguished by gender using the `hue` parameter. Customize this plot with the following specifications: - Disable jittering. - Set the marker shape to \'D\' (diamond). - Set marker size to 8. - Use the \'Set2\' palette for different hues. 2. Create a faceted plot using `catplot` that shows the relationship between `time` and `total_bill` for each day of the week, differentiated by sex. Customize the faceted plot with the following specifications: - Set the height of each facet to 4 inches and the aspect ratio to 0.6. - Enable dodge to split the strips by hue in the faceted plot. - Set marker size to 6 and increase plot transparency to 70% (`alpha=0.7`). The function should save these plots as \'strip_plot.png\' and \'facet_plot.png\' respectively. # Function Signature ```python def create_strip_and_facet_plot(): pass ``` # Constraints - You are expected to handle all required imports within the function or as part of your initial setup. - Ensure that the plots are saved correctly in the working directory. # Example ```python create_strip_and_facet_plot() # This should save \'strip_plot.png\' and \'facet_plot.png\' in the current working directory. ``` # Evaluation Criteria - Correct use of seaborn `stripplot` and `catplot`. - Application of specified customizations. - Proper saving of plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_strip_and_facet_plot(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create a strip plot plt.figure(figsize=(10, 6)) strip_plot = sns.stripplot( x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", data=tips, jitter=False, marker=\'D\', size=8, palette=\'Set2\' ) strip_plot.set_title(\\"Total Bill Distribution by Day and Gender\\") plt.savefig(\'strip_plot.png\') plt.clf() # Clear the figure # Create a faceted plot facet_plot = sns.catplot( x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", data=tips, kind=\\"strip\\", dodge=True, height=4, aspect=0.6, size=6, alpha=0.7 ) facet_plot.fig.suptitle(\\"Total Bill vs Time Faceted by Day and Gender\\", y=1.05) facet_plot.savefig(\'facet_plot.png\')"},{"question":"Objective: Demonstrate your understanding of Python\'s buffer protocol by implementing a custom buffer class that exposes its internal memory efficiently. Problem Statement: You are required to create a Python class `CustomBuffer` that simulates the behavior of `Py_buffer`. Your class should implement and expose methods to interact with its internal buffer, including acquiring and releasing the buffer using specified request types. Requirements: 1. Create a class `CustomBuffer` that: - Initializes with a `bytearray` of a given size. - Exposes the buffer using a method `get_buffer(request: str) -> dict`. - Handles buffer requests defined by the following request types: - `\'simple\'`: No specific structure; exposes the entire buffer. - `\'ndarray\'`: Exposes the buffer as an n-dimensional array. - `\'contiguous\'`: Ensures the buffer is contiguous (C-style or Fortran-style). 2. The `get_buffer` method should return a dictionary with information about the buffer: - `\'buf\'`: Pointer to the start of the buffer (simulated as the `bytearray` itself for this task). - `\'len\'`: Length of the buffer. - `\'readonly\'`: A boolean indicating if the buffer is read-only. - `\'itemsize\'`: Size of each item in the buffer (1 for a `bytearray`). - `\'ndim\'`: Number of dimensions (default to 1 for simplicity). - `\'shape\'`: Shape of the buffer if viewed as an n-dimensional array. - `\'strides\'`: Stride values for navigating the buffer. - `\'suboffsets\'`: None for this implementation. 3. Implement a method `release_buffer()` that simulates releasing the buffer similarly to `PyBuffer_Release`. 4. Implement a method `is_contiguous(order: str) -> bool` that: - Checks if the buffer is contiguous based on the given `order` (\'C\' for C-style or \'F\' for Fortran-style). Input Format: - Initialization of `CustomBuffer` with `size: int`. - `get_buffer` method called with `request: str` parameter. - `is_contiguous` method called with `order: str` parameter. Output Format: - `get_buffer` returns a dictionary with buffer details based on the specified request. - `is_contiguous` returns a boolean indicating if the buffer is contiguous. - `release_buffer` does not return anything but should ensure the buffer is marked as released. # Example: ```python # Initializing the CustomBuffer with size 100 buffer = CustomBuffer(100) # Getting buffer with \'simple\' request buffer_info = buffer.get_buffer(\'simple\') print(buffer_info) # Output: { <details of the buffer> } # Checking if the buffer is contiguous in C-style order is_contig = buffer.is_contiguous(\'C\') print(is_contig) # Output: True # Releasing the buffer buffer.release_buffer() ``` Constraints: - You must handle invalid request types gracefully by raising appropriate exceptions. - Ensure proper management of the buffer to avoid resource leaks. Additional Information: - The `ndarray` request should simulate a 1-dimensional array for simplicity. - Assume the buffer is simple with no complex suboffsets or multi-dimensional support. - Focus on demonstrating the core concepts of the buffer protocol as described. This question ensures that students understand the fundamental concepts of the buffer protocol, including buffer management, handling different request types, and providing a contiguous view of the buffer.","solution":"class CustomBuffer: def __init__(self, size: int): self.buffer = bytearray(size) self.readonly = False self.released = False def get_buffer(self, request: str) -> dict: if self.released: raise BufferError(\\"Buffer has been released\\") if request not in (\'simple\', \'ndarray\', \'contiguous\'): raise ValueError(\\"Unsupported buffer request type\\") buffer_info = { \'buf\': self.buffer, \'len\': len(self.buffer), \'readonly\': self.readonly, \'itemsize\': 1, \'ndim\': 1, \'shape\': (len(self.buffer),), \'strides\': (1,), \'suboffsets\': None } return buffer_info def release_buffer(self): self.released = True def is_contiguous(self, order: str) -> bool: if order not in (\'C\', \'F\'): raise ValueError(\\"Invalid order type\\") return True"},{"question":"# Secure Hash Implementation Using `hashlib` **Objective:** Your task is to implement a function that takes a list of files, computes their secure hashes using a specified algorithm, and stores these hashes securely on disk. Additionally, implement functionality to validate the integrity of these files by comparing newly computed hashes against the stored ones. For added security, derive keys using `pbkdf2_hmac` for hash-based message authentication codes (HMAC). # Requirements: 1. **Function 1:** `store_file_hashes(file_paths: List[str], algorithm: str, output_file: str) -> None` - **Input:** - `file_paths`: A list of file paths for which hashes need to be computed. - `algorithm`: The name of the hashing algorithm to be used (e.g., \'sha256\', \'blake2b\'). - `output_file`: The path of the file where computed hashes will be stored. - **Output:** - None. The function creates or overwrites `output_file` with lines containing file paths and their respective hashes. - **Details:** - Read each file in chunks to handle large files. - Compute the hash using the specified algorithm. - Store the hashes in a plain text format. 2. **Function 2:** `validate_file_integrity(file_paths: List[str], algorithm: str, hash_file: str) -> Dict[str, bool]` - **Input:** - `file_paths`: A list of file paths to validate against stored hashes. - `algorithm`: The name of the hashing algorithm to be used. - `hash_file`: The path of the file containing the stored hashes. - **Output:** - A dictionary mapping each file path to a boolean indicating whether the file is intact (`True` if the computed hash matches the stored hash, `False` otherwise). - **Details:** - Read and compute the hash for each provided file. - Compare the computed hashes against the stored hashes. 3. **Function 3:** `derive_key(password: str, salt: bytes, iterations: int, dklen: int = None) -> bytes` - **Input:** - `password`: A string password from which the key is derived. - `salt`: A bytes object representing the salt. - `iterations`: The number of iterations for the key derivation. - `dklen`: The length of the derived key (optional; defaults to the hash\'s size if not provided). - **Output:** - A derived key as a bytes object. - **Details:** - Use `pbkdf2_hmac` with \'sha256\' as the HMAC hash algorithm. - Document the chosen number of iterations by referring to security best practices. # Constraints: - Assume that the file handling environment is secure, and files are accessible. - Use efficient reading and writing operations to handle large files. - Ensure the function handles invalid inputs gracefully with appropriate error messages. # Example Usage: ```python file_paths = [\'file1.txt\', \'file2.txt\'] algorithm = \'sha256\' output_file = \'hashes.txt\' # Storing file hashes store_file_hashes(file_paths, algorithm, output_file) # Validating file integrity validation_results = validate_file_integrity(file_paths, algorithm, output_file) print(validation_results) # Example output: {\'file1.txt\': True, \'file2.txt\': false} # Deriving a key for HMAC password = \'mysupersecretpassword\' salt = os.urandom(16) iterations = 500000 derived_key = derive_key(password, salt, iterations) print(derived_key) # Example output: b\'some_derived_key\' ``` **Notes:** - Be mindful of the differences between binary and hexadecimal representations of hashes. - Remember that the `hashlib` module functions may behave differently across platforms depending on the available OpenSSL library. - Ensure meaningful error handling and logging for any exceptional cases encountered. Good luck!","solution":"import hashlib import os from typing import List, Dict from hashlib import pbkdf2_hmac def store_file_hashes(file_paths: List[str], algorithm: str, output_file: str) -> None: Computes the hash for each file in the provided list using the specified algorithm and stores the file paths along with their hashes in the output file. def compute_file_hash(file_path: str, algorithm: str) -> str: hash_func = hashlib.new(algorithm) with open(file_path, \'rb\') as f: for chunk in iter(lambda: f.read(4096), b\'\'): hash_func.update(chunk) return hash_func.hexdigest() with open(output_file, \'w\') as out_file: for file_path in file_paths: if os.path.exists(file_path): file_hash = compute_file_hash(file_path, algorithm) out_file.write(f\\"{file_path} {file_hash}n\\") else: raise FileNotFoundError(f\\"File {file_path} not found.\\") def validate_file_integrity(file_paths: List[str], algorithm: str, hash_file: str) -> Dict[str, bool]: Validates the integrity of the files against the stored hashes. Returns a dictionary mapping file paths to a boolean indicating integrity status. def compute_file_hash(file_path: str, algorithm: str) -> str: hash_func = hashlib.new(algorithm) with open(file_path, \'rb\') as f: for chunk in iter(lambda: f.read(4096), b\'\'): hash_func.update(chunk) return hash_func.hexdigest() stored_hashes = {} with open(hash_file, \'r\') as f: for line in f: path, file_hash = line.strip().split() stored_hashes[path] = file_hash results = {} for file_path in file_paths: if os.path.exists(file_path): current_hash = compute_file_hash(file_path, algorithm) results[file_path] = (stored_hashes.get(file_path) == current_hash) else: raise FileNotFoundError(f\\"File {file_path} not found.\\") return results def derive_key(password: str, salt: bytes, iterations: int, dklen: int = None) -> bytes: Derives a cryptographic key using PBKDF2 HMAC with SHA-256. if dklen is None: dklen = hashlib.sha256().digest_size return pbkdf2_hmac(\'sha256\', password.encode(), salt, iterations, dklen)"},{"question":"# Global Data Communication System Problem Statement You are tasked with creating a data communication system that supports encoding and decoding messages using various encoding schemes provided by the `base64` module. This system should be able to handle data safely, ensuring that binary data can be transmitted over text-restricted mediums like emails, URLs, or HTTP POST requests. Requirements 1. **Function Signature:** ```python def global_data_communicator(data: bytes, encoding_type: str, action: str, options: dict = None) -> bytes: ``` 2. **Input:** - `data` (bytes): The binary data to be encoded or decoded. - `encoding_type` (str): The type of encoding to use. It can be one of the following values: `\\"base64\\"`, `\\"urlsafe_base64\\"`, `\\"base32\\"`, `\\"base16\\"`, `\\"ascii85\\"`, `\\"base85\\"`. - `action` (str): The action to perform. It can be either `\\"encode\\"` or `\\"decode\\"`. - `options` (dict, optional): A dictionary containing optional parameters for encoding/decoding, such as alternative characters for Base64, folding spaces for Ascii85, etc. 3. **Output:** - Returns the encoded or decoded data as bytes. 4. **Constraints:** - The function should raise a `ValueError` if an unsupported `encoding_type` or `action` is provided. - Appropriately handle and raise exceptions for invalid data formats. 5. **Performance:** - The implementation should handle typical message sizes (up to a few megabytes) efficiently. Example Usage ```python # Example 1: Base64 encoding data = b\\"example data\\" encoded_data = global_data_communicator(data, encoding_type=\\"base64\\", action=\\"encode\\") # Example 2: Base64 decoding decoded_data = global_data_communicator(encoded_data, encoding_type=\\"base64\\", action=\\"decode\\") # Example 3: URL-safe Base64 encoding with alternative characters encoded_data = global_data_communicator(data, encoding_type=\\"urlsafe_base64\\", action=\\"encode\\", options={\\"altchars\\": b\\"-_\\"}) # Example 4: Base32 encoding encoded_data = global_data_communicator(data, encoding_type=\\"base32\\", action=\\"encode\\") ``` Note - Leverage the `base64` module\'s functions to implement the encoding and decoding logic. - Ensure that all potential exceptions are correctly handled, providing meaningful error messages where appropriate.","solution":"import base64 def global_data_communicator(data: bytes, encoding_type: str, action: str, options: dict = None) -> bytes: Encodes or decodes data using various schemes provided by the base64 module. :param data: The binary data to be encoded or decoded. :param encoding_type: The type of encoding to use. It can be one of the following values: \\"base64\\", \\"urlsafe_base64\\", \\"base32\\", \\"base16\\", \\"ascii85\\", \\"base85\\". :param action: The action to perform. It can be either \\"encode\\" or \\"decode\\". :param options: A dictionary containing optional parameters for encoding/decoding. :return: Returns the encoded or decoded data as bytes. :raises ValueError: If an unsupported encoding_type or action is provided. if action not in [\\"encode\\", \\"decode\\"]: raise ValueError(f\\"Unsupported action: {action}\\") if encoding_type == \\"base64\\": if action == \\"encode\\": altchars = options.get(\\"altchars\\") if options else None return base64.b64encode(data, altchars=altchars) elif action == \\"decode\\": altchars = options.get(\\"altchars\\") if options else None return base64.b64decode(data, altchars=altchars) elif encoding_type == \\"urlsafe_base64\\": if action == \\"encode\\": return base64.urlsafe_b64encode(data) elif action == \\"decode\\": return base64.urlsafe_b64decode(data) elif encoding_type == \\"base32\\": if action == \\"encode\\": return base64.b32encode(data) elif action == \\"decode\\": return base64.b32decode(data) elif encoding_type == \\"base16\\": if action == \\"encode\\": return base64.b16encode(data) elif action == \\"decode\\": return base64.b16decode(data) elif encoding_type == \\"ascii85\\": if action == \\"encode\\": return base64.a85encode(data, foldspaces=options.get(\\"foldspaces\\", False) if options else False) elif action == \\"decode\\": return base64.a85decode(data) elif encoding_type == \\"base85\\": if action == \\"encode\\": return base64.b85encode(data) elif action == \\"decode\\": return base64.b85decode(data) raise ValueError(f\\"Unsupported encoding type: {encoding_type}\\")"},{"question":"**Objective:** Implement a custom XML content handler that processes an XML document and extracts specific information. The goal is to demonstrate your understanding of SAX event handling and the ability to customize SAX handlers for specific XML processing tasks. **Problem Statement:** You are provided with an XML file containing information about books in a library. Each book has a title, author, genre, and publication year. Your task is to create a custom SAX `ContentHandler` that parses this XML file and extracts the titles of all books published after the year 2000. Additionally, handle any parsing errors and report them. **Example XML File:** ```xml <library> <book> <title>Book One</title> <author>Author One</author> <genre>Fiction</genre> <year>1995</year> </book> <book> <title>Book Two</title> <author>Author Two</author> <genre>Non-Fiction</genre> <year>2003</year> </book> <book> <title>Book Three</title> <author>Author Three</author> <genre>Fiction</genre> <year>2010</year> </book> </library> ``` **Input:** - Path to the XML file (a string). **Output:** - A list of book titles (strings) published after the year 2000. **Constraints:** - The XML structure is known and consistent with the example provided. - Handle any parsing errors gracefully. **Implementation Requirements:** 1. Create a custom `ContentHandler` subclass named `LibraryContentHandler`. 2. Implement methods to handle the start and end of elements, and character data. 3. Extract and store titles of books published after 2000 in a list. 4. Handle any parsing errors using an `ErrorHandler`. 5. Write a function `extract_titles_after_2000(xml_file_path)` that uses your custom handler to parse the XML file and return the list of titles. **Starter Code:** ```python import xml.sax class LibraryContentHandler(xml.sax.handler.ContentHandler): def __init__(self): super().__init__() self.current_element = \'\' self.current_title = \'\' self.current_year = 0 self.titles_after_2000 = [] def startElement(self, name, attrs): self.current_element = name def endElement(self, name): if name == \'book\': if self.current_year > 2000: self.titles_after_2000.append(self.current_title) self.current_title = \'\' self.current_year = 0 self.current_element = \'\' def characters(self, content): if self.current_element == \'title\': self.current_title += content elif self.current_element == \'year\': try: self.current_year = int(content) except ValueError: pass # Optionally handle invalid year format class LibraryErrorHandler(xml.sax.handler.ErrorHandler): def error(self, exception): print(f\\"Error: {exception}\\") def fatalError(self, exception): print(f\\"Fatal error: {exception}\\") raise exception def warning(self, exception): print(f\\"Warning: {exception}\\") def extract_titles_after_2000(xml_file_path): handler = LibraryContentHandler() error_handler = LibraryErrorHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) parser.setErrorHandler(error_handler) parser.parse(xml_file_path) return handler.titles_after_2000 # Example usage if __name__ == \\"__main__\\": xml_file = \\"path/to/library.xml\\" print(extract_titles_after_2000(xml_file)) ``` **Explanation:** - The `LibraryContentHandler` class implements methods to handle the start and end of XML elements and character data. - The `extract_titles_after_2000` function initializes the handler and parser, and returns the list of titles after parsing the XML file. - Error handling is managed by the `LibraryErrorHandler` class. Complete the implementation and ensure it handles the given XML structure correctly.","solution":"import xml.sax class LibraryContentHandler(xml.sax.handler.ContentHandler): def __init__(self): super().__init__() self.current_element = \'\' self.current_title = \'\' self.current_year = 0 self.titles_after_2000 = [] def startElement(self, name, attrs): self.current_element = name def endElement(self, name): if name == \'book\': if self.current_year > 2000: self.titles_after_2000.append(self.current_title.strip()) self.current_title = \'\' self.current_year = 0 self.current_element = \'\' def characters(self, content): if self.current_element == \'title\': self.current_title += content elif self.current_element == \'year\': try: self.current_year = int(content) except ValueError: self.current_year = 0 # Handle invalid year class LibraryErrorHandler(xml.sax.handler.ErrorHandler): def error(self, exception): print(f\\"Error: {exception}\\") def fatalError(self, exception): print(f\\"Fatal error: {exception}\\") raise exception def warning(self, exception): print(f\\"Warning: {exception}\\") def extract_titles_after_2000(xml_file_path): handler = LibraryContentHandler() error_handler = LibraryErrorHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) parser.setErrorHandler(error_handler) parser.parse(xml_file_path) return handler.titles_after_2000"},{"question":"**Programming Task: Implement an Asynchronous Data Pipeline Using asyncio Futures** # Objective You are required to implement an asynchronous data processing pipeline using asyncio.Future objects. The pipeline consists of three stages: 1. **Data Fetching**: Simulate fetching data asynchronously. 2. **Data Processing**: Simulate processing the fetched data asynchronously. 3. **Result Aggregation**: Aggregate the processed data. # Instructions 1. **Data Fetching**: - Implement a function `fetch_data` which accepts an integer `data_id`. - Simulate asynchronous data fetching using `asyncio.sleep` and return a Future object with a result of `f\\"data_{data_id}\\"`. 2. **Data Processing**: - Implement a function `process_data` which accepts a Future object `fetch_future`. - Await the result of `fetch_future`, simulate asynchronous data processing using `asyncio.sleep`, and return a Future object with a processed result of `f\\"processed_{fetch_future.result()}\\"`. 3. **Result Aggregation**: - Implement a function `aggregate_results` which accepts a list of Future objects `processed_futures`. - Await the results of all futures in `processed_futures` and return a Future object with a concatenated string of all results. 4. **Pipeline Execution**: - Implement an asynchronous function `execute_pipeline` which orchestrates the execution of the entire pipeline. This function should: - Fetch and process data for 5 data_ids in parallel. - Aggregate the processed results. - Return the final aggregated result. # Function Signatures ```python import asyncio async def fetch_data(data_id: int) -> asyncio.Future: # Your implementation here async def process_data(fetch_future: asyncio.Future) -> asyncio.Future: # Your implementation here async def aggregate_results(processed_futures: list) -> asyncio.Future: # Your implementation here async def execute_pipeline() -> str: # Your implementation here ``` # Constraints - Use asyncio.Future objects for managing the asynchronous operations. - Ensure that the functions handle exceptions and cancellations appropriately. - The total sleep time for fetching and processing each data should be 1 second. # Example Usage ```python if __name__ == \\"__main__\\": final_result = asyncio.run(execute_pipeline()) print(final_result) # Output: \\"processed_data_0 processed_data_1 processed_data_2 processed_data_3 processed_data_4\\" ``` **Note**: Your solution should be clear, efficient, and make proper use of asyncio paradigms.","solution":"import asyncio async def fetch_data(data_id: int) -> asyncio.Future: Simulate fetching data asynchronously by sleeping for 0.5 seconds and then setting a result with a specific format. await asyncio.sleep(0.5) future = asyncio.Future() future.set_result(f\\"data_{data_id}\\") return future async def process_data(fetch_future: asyncio.Future) -> asyncio.Future: Await the result of fetch_future, simulate processing it by sleeping for 0.5 seconds, then setting a processed result with a specific format. fetch_result = await fetch_future await asyncio.sleep(0.5) future = asyncio.Future() future.set_result(f\\"processed_{fetch_result}\\") return future async def aggregate_results(processed_futures: list) -> asyncio.Future: Await all processed futures and concatenate their results into a single string. processed_results = await asyncio.gather(*[future for future in processed_futures]) future = asyncio.Future() future.set_result(\' \'.join(processed_results)) return future async def execute_pipeline() -> str: Orchestrate the entire pipeline to fetch, process, and aggregate results. fetch_futures = [fetch_data(data_id) for data_id in range(5)] processed_futures = [process_data(fetch_future) for fetch_future in fetch_futures] aggregated_future = await aggregate_results(processed_futures) return aggregated_future # Note: Using asyncio.run() would execute the pipeline and print the result. # final_result = asyncio.run(execute_pipeline()) # print(final_result)"},{"question":"Given the `dbm` module and its capabilities as described in the documentation, you are to implement a Python function that interacts with a DBM database to achieve specific tasks. The function should use dictionary-like operations, handle context management, and demonstrate error handling for unsupported operations. # Task Write a function called `manage_dbm_database` that performs the following steps: 1. Opens a new DBM database file (e.g., `testdb`) using `dbm.open` with the flag set to `\'c\'` (create if it does not exist). 2. Stores the following key-value pairs in the database: - \'name\': \'Alice\' - \'age\': \'30\' - \'city\': \'New York\' 3. Retrieves and prints the values for the keys \'name\' and \'city\'. 4. Attempts to store a non-byte value (e.g., integer) and handles the resulting exception by printing an error message. 5. Iterates over all keys in the database and prints each key-value pair. 6. Ensures the database is properly closed when done. # Constraints - You may assume the DBM module will be available in the environment. - Handle exceptions gracefully to ensure that the program does not crash unexpectedly. - All keys and values must be stored as bytes since DBM databases handle data in byte format. # Function Signature ```python def manage_dbm_database(filename: str) -> None: pass ``` # Example Execution ```python manage_dbm_database(\'testdb\') ``` Expected Output: ``` name: Alice city: New York Error: Keys and values must be bytes All database entries: name: Alice age: 30 city: New York ``` Note: The exact error message wording can vary, but it must convey that a type error occurred because of storing a non-byte value. # Additional Information - Use the DBM module documentation provided to understand the necessary setup and operations. - Ensure proper use of the `with` statement for context management to automatically close the database. - The function should be self-contained and not depend on any external files or input/output other than what is specified in the task.","solution":"import dbm def manage_dbm_database(filename: str) -> None: try: with dbm.open(filename, \'c\') as db: # Store key-value pairs db[b\'name\'] = b\'Alice\' db[b\'age\'] = b\'30\' db[b\'city\'] = b\'New York\' # Retrieve and print values print(f\\"name: {db[b\'name\'].decode()}\\") print(f\\"city: {db[b\'city\'].decode()}\\") # Attempt to store a non-byte value and handle the error try: db[\'integer\'] = 5 # This should raise an error except TypeError as e: print(\\"Error: Keys and values must be bytes\\") # Iterate over all keys and print key-value pairs print(\\"All database entries:\\") for key in db.keys(): print(f\\"{key.decode()}: {db[key].decode()}\\") except Exception as e: print(\\"An unexpected error occurred:\\", e)"},{"question":"# Kernel Approximation Challenge with Scikit-learn In this assignment, you will implement a function that uses scikit-learn\'s kernel approximation methods for a classification problem. You are required to implement a function that will preprocess the data using an approximate kernel map, train a classifier, and evaluate its performance. Task Implement a function `kernel_approximation_classification` that performs the following steps: 1. Load the provided dataset. 2. Apply a kernel approximation method to the features of the dataset. 3. Split the transformed data into training and testing sets. 4. Train a `SGDClassifier` using the training set. 5. Evaluate the classifier\'s performance on the test set and return the accuracy score. Input Format - A dictionary `data` containing: - `features`: a 2D list or NumPy array of feature values. - `labels`: a list or NumPy array of corresponding labels. - A string `method` indicating the kernel approximation method to use. It can be one of the following: - `\\"nystroem\\"` for the Nystroem method. - `\\"rbf\\"` for the RBFSampler method. - `\\"additive_chi2\\"` for the AdditiveChi2Sampler method. - `\\"skewed_chi2\\"` for the SkewedChi2Sampler method. - An integer `n_components` indicating the number of components in the kernel approximation. Output Format - A float representing the accuracy score of the classifier on the test set. Function Signature ```python def kernel_approximation_classification(data: dict, method: str, n_components: int) -> float: ``` Constraints - You must use the specified kernel approximation method precisely as described in scikit-learn\'s documentation. - You should ensure reproducibility by setting a `random_state` where applicable. Example ```python data = { \\"features\\": [[0, 0], [1, 1], [1, 0], [0, 1], [2, 2], [2, 0], [0, 2]], \\"labels\\": [0, 0, 1, 1, 0, 1, 0] } method = \\"rbf\\" n_components = 4 accuracy = kernel_approximation_classification(data, method, n_components) print(accuracy) # Expected Output: a float value representing the accuracy ``` Use appropriate imports, preprocessing steps, and consider optimal parameter choices for the kernel methods. Document your code and provide necessary comments. Note You can refer to scikit-learn\'s documentation for `Kernel Approximation` to implement the required transformations and classifier.","solution":"from sklearn.kernel_approximation import RBFSampler, Nystroem, AdditiveChi2Sampler, SkewedChi2Sampler from sklearn.linear_model import SGDClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import numpy as np def kernel_approximation_classification(data: dict, method: str, n_components: int) -> float: # Extract features and labels from the data X = np.array(data[\'features\']) y = np.array(data[\'labels\']) # Select the kernel approximation method if method == \'rbf\': transformer = RBFSampler(n_components=n_components, random_state=42) elif method == \'nystroem\': transformer = Nystroem(n_components=n_components, random_state=42) elif method == \'additive_chi2\': transformer = AdditiveChi2Sampler() elif method == \'skewed_chi2\': transformer = SkewedChi2Sampler(random_state=42) else: raise ValueError(\\"Unsupported kernel approximation method\\") # Transform the features X_transformed = transformer.fit_transform(X) # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.3, random_state=42) # Train the classifier clf = SGDClassifier(random_state=42) clf.fit(X_train, y_train) # Predict and calculate accuracy y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"**Question: HTML Text Sanitizer** You are tasked with creating a function to sanitize and display user-generated HTML content safely. The function will clean potentially unsafe HTML characters to ensure the content can be safely embedded on a webpage. Additionally, it should handle decoding escaped HTML characters back to their original form. # Function Signature ```python def sanitize_html(content: str, escape: bool = True) -> str: pass ``` # Input - `content` (string): A string containing the user-generated HTML content. - `escape` (boolean): A flag indicating whether to escape or unescape the HTML content. Default is `True`. # Output - A sanitized string. If `escape` is `True`, the function should convert unsafe characters to their HTML-safe sequences. If `escape` is `False`, the function should convert HTML-safe sequences back to their corresponding characters. # Constraints - You may assume the input string `content` will not exceed 1000 characters. - The implementation should make use of the functions available in the `html` module as described in the documentation. # Examples Example 1: ```python content = \'<div class=\\"example\\">Hello & welcome!</div>\' result = sanitize_html(content) print(result) # Expected: \'&lt;div class=&quot;example&quot;&gt;Hello &amp; welcome!&lt;/div&gt;\' ``` Example 2: ```python content = \'&lt;div class=&quot;example&quot;&gt;Hello &amp; welcome!&lt;/div&gt;\' result = sanitize_html(content, escape=False) print(result) # Expected: \'<div class=\\"example\\">Hello & welcome!</div>\' ``` # Note - For escaping, use the `html.escape` function. - For unescaping, use the `html.unescape` function. Implement the `sanitize_html` function to meet the specifications and pass the provided examples.","solution":"import html def sanitize_html(content: str, escape: bool = True) -> str: Sanitizes user-generated HTML content. Args: content (str): The HTML content to sanitize. escape (bool): If True, converts unsafe characters to their HTML-safe sequences. If False, decodes HTML-safe sequences back to their original characters. Returns: str: The sanitized string. if escape: return html.escape(content) else: return html.unescape(content)"},{"question":"**Socket Programming Challenge: Multithreaded Echo Server** You need to implement a multithreaded TCP echo server and a client in Python using the `socket` module. # Requirements 1. **Server:** - Create a TCP server that listens on a specified port. - Accept multiple client connections using threads. - For every client connection, start a new thread to handle the client. - Each thread should handle: - Receiving data from the client. - Sending back the same data (echo) to the client. - Appropriate handling of client disconnection. - Print messages on the server console about client connections and disconnections. - The server should be able to handle client connections indefinitely until manually stopped. 2. **Client:** - Create a TCP client that connects to the server. - Send a message to the server and print the echoed message received from the server. - Handle any connection exceptions gracefully. # Input and Output - Server side: - Listen on `localhost` with port `12345`. - Print out connection and disconnection messages of clients. - Client side: - Send a message input by the user. - Print the echoed message received from the server. # Constraints - Use IPv4 addressing (i.e., `AF_INET`). - Use TCP sockets (i.e., `SOCK_STREAM`). - Ensure the server can handle at least 5 simultaneous clients. - Each client thread in the server should handle communication with one client only. # Performance Requirements - The server should handle multiple clients efficiently using threads. - The client should be able to reconnect if the server is restarted without errors. # Example Usage Starting the Server ```python # Run the server script python server.py # Server Output Listening on localhost:12345... Connected by (\'127.0.0.1\', 65432) Received \'Hello, Server!\' from (\'127.0.0.1\', 65432) Disconnected from (\'127.0.0.1\', 65432) ``` Running the Client ```python # Run the client script python client.py Hello, Server! # Client Output Echo from server: Hello, Server! ``` # Write Your Solution Below Server Code ```python import socket import threading # Function to handle client connections def handle_client_connection(client_socket): while True: try: data = client_socket.recv(1024) if not data: # Client disconnected print(f\\"Disconnected from {client_socket.getpeername()}\\") break print(f\\"Received \'{data.decode()}\' from {client_socket.getpeername()}\\") client_socket.sendall(data) # Echo back the received data except ConnectionResetError: print(f\\"Connection reset by {client_socket.getpeername()}\\") break client_socket.close() # Server setup def start_server(): server = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server.bind((\'localhost\', 12345)) server.listen(5) print(\\"Listening on localhost:12345...\\") while True: client_sock, addr = server.accept() print(f\\"Connected by {addr}\\") client_handler = threading.Thread(target=handle_client_connection, args=(client_sock,)) client_handler.start() if __name__ == \\"__main__\\": start_server() ``` Client Code ```python import socket def start_client(): client = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client.connect((\'localhost\', 12345)) message = input(\\"Enter your message: \\") client.sendall(message.encode()) echo = client.recv(1024) print(f\\"Echo from server: {echo.decode()}\\") client.close() if __name__ == \\"__main__\\": start_client() ``` Ensure to test your server and client by running them in separate terminals.","solution":"import socket import threading def handle_client_connection(client_socket): while True: try: data = client_socket.recv(1024) if not data: print(f\\"Disconnected from {client_socket.getpeername()}\\") break print(f\\"Received \'{data.decode()}\' from {client_socket.getpeername()}\\") client_socket.sendall(data) except ConnectionResetError: print(f\\"Connection reset by {client_socket.getpeername()}\\") break client_socket.close() def start_server(): server = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server.bind((\'localhost\', 12345)) server.listen(5) print(\\"Listening on localhost:12345...\\") while True: client_sock, addr = server.accept() print(f\\"Connected by {addr}\\") client_handler = threading.Thread(target=handle_client_connection, args=(client_sock,)) client_handler.start() if __name__ == \\"__main__\\": start_server() import socket def start_client(message): client = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client.connect((\'localhost\', 12345)) client.sendall(message.encode()) echo = client.recv(1024) print(f\\"Echo from server: {echo.decode()}\\") client.close() return echo.decode() if __name__ == \\"__main__\\": message = input(\\"Enter your message: \\") start_client(message)"},{"question":"Advanced Codec Manager You are required to implement a class `CodecManager` that provides functionality to register, unregister, encode, decode, and handle errors using the codec APIs provided by `python310`. The class should maintain proper registration of codecs and handle potential errors gracefully. # Class Specification 1. **Class Definition**: - `class CodecManager` 2. **Methods**: - `register_search_function(self, search_function: PyObject) -> bool`: - Registers a new codec search function. - Returns `True` if registration is successful, `False` otherwise. - `unregister_search_function(self, search_function: PyObject) -> bool`: - Unregisters a codec search function. - Returns `True` if unregistration is successful, `False` otherwise. - `known_encoding(self, encoding: str) -> bool`: - Checks if a codec is registered for the given encoding. - Returns `True` if known, `False` otherwise. - `encode(self, obj: PyObject, encoding: str, errors: str = None) -> PyObject`: - Encodes an object using the specified encoding and error handling strategy. - Raises a `ValueError` if the encoding fails with an appropriate message. - `decode(self, obj: PyObject, encoding: str, errors: str = None) -> PyObject`: - Decodes an object using the specified encoding and error handling strategy. - Raises a `ValueError` if the decoding fails with an appropriate message. # Constraints - You may assume that all codec-related functions such as `PyCodec_Register`, `PyCodec_Unregister`, etc., are available within the module `python310`. - Handle all possible exceptions and errors appropriately, providing meaningful error messages. # Example Usage: ```python manager = CodecManager() # Example codec search function (Dummy Example) def search_function(name): return None assert manager.register_search_function(search_function) == True assert manager.known_encoding(\'utf-8\') == True try: encoded_obj = manager.encode(\'Hello, World!\', \'utf-8\') decoded_obj = manager.decode(encoded_obj, \'utf-8\') print(decoded_obj) # Should print: Hello, World! except ValueError as e: print(e) assert manager.unregister_search_function(search_function) == True ``` # Notes: - For simplification, you can assume that `PyObject` is any valid Python object. - Thoroughly test the functions with different encodings and error scenarios. - Ensure that the codec registration and unregistration work as expected.","solution":"import codecs class CodecManager: def __init__(self): self.search_functions = [] def register_search_function(self, search_function): if search_function not in self.search_functions: codecs.register(search_function) self.search_functions.append(search_function) return True return False def unregister_search_function(self, search_function): # codecs.unregister does not exist directly, simulate it with the internal list if search_function in self.search_functions: self.search_functions.remove(search_function) return True return False def known_encoding(self, encoding): try: codecs.lookup(encoding) return True except LookupError: return False def encode(self, obj, encoding, errors=\'strict\'): try: if isinstance(obj, str): return obj.encode(encoding, errors) raise ValueError(\\"Object to encode must be of type `str`\\") except Exception as e: raise ValueError(f\\"Encoding failed: {e}\\") def decode(self, obj, encoding, errors=\'strict\'): try: if isinstance(obj, (bytes, bytearray)): return obj.decode(encoding, errors) raise ValueError(\\"Object to decode must be of type `bytes` or `bytearray`\\") except Exception as e: raise ValueError(f\\"Decoding failed: {e}\\")"},{"question":"**Coding Assessment Question:** # Objective: You are required to demonstrate your understanding of Restricted Boltzmann Machines (RBMs) by implementing a solution that trains an RBM using the BernoulliRBM class from scikit-learn. You will use the trained RBM to extract features from a dataset and then use these features to train a classifier for a supervised learning task. # Problem Statement: You are given a binary classification dataset where the inputs are binary. You need to perform the following tasks: 1. Train an RBM model using the given dataset. 2. Use the RBM model to transform the input data into a new feature space. 3. Train a logistic regression classifier using the transformed features. 4. Evaluate the classifier using accuracy score on a test set. # Input Format: - A training dataset `X_train` of shape (n_samples, n_features), which is a 2D numpy array containing binary input data. - Corresponding labels `y_train` of shape (n_samples,), which is a 1D numpy array containing binary labels. - A test dataset `X_test` of shape (n_test_samples, n_features), which is a 2D numpy array. - Corresponding labels `y_test` of shape (n_test_samples,), which is a 1D numpy array. # Expected Output: - Accuracy of the logistic regression classifier on the test dataset, printed as a float. # Constraints: - You must use BernoulliRBM from sklearn.neural_network. - You must use LogisticRegression from sklearn.linear_model. - The dataset is small enough to fit in memory. # Performance Requirements: - Ensure the accuracy calculation is completed in a reasonable time for small to medium datasets (up to a few thousand samples). # Example: ```python from sklearn.neural_network import BernoulliRBM from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score import numpy as np # Example data (for illustration; actual data will be provided) X_train = np.array([[0, 1, 1], [1, 0, 0], [1, 1, 0], [0, 0, 1]]) y_train = np.array([1, 0, 1, 0]) X_test = np.array([[1, 1, 1], [0, 0, 0]]) y_test = np.array([1, 0]) # Define the RBM model and logistic regression classifier rbm = BernoulliRBM(n_components=2, learning_rate=0.01, n_iter=10, random_state=42) logistic = LogisticRegression(solver=\'lbfgs\', max_iter=1000, random_state=42) # Create a pipeline that combines RBM and logistic regression classifier = Pipeline(steps=[(\'rbm\', rbm), (\'logistic\', logistic)]) # Train the combined model classifier.fit(X_train, y_train) # Predict on test data and calculate accuracy predictions = classifier.predict(X_test) accuracy = accuracy_score(y_test, predictions) print(\\"Accuracy:\\", accuracy) # Expected output: Accuracy: 1.0 ``` Note: In the actual implementation, the dataset will be provided and might be larger and more complex. The example provided is for illustration and testing purposes.","solution":"from sklearn.neural_network import BernoulliRBM from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score import numpy as np def train_and_evaluate_rbm_logistic(X_train, y_train, X_test, y_test): # Define the RBM model and logistic regression classifier rbm = BernoulliRBM(n_components=100, learning_rate=0.1, n_iter=10, random_state=42) logistic = LogisticRegression(solver=\'lbfgs\', max_iter=1000, random_state=42) # Create a pipeline that combines RBM and logistic regression classifier = Pipeline(steps=[(\'rbm\', rbm), (\'logistic\', logistic)]) # Train the combined model classifier.fit(X_train, y_train) # Predict on test data and calculate accuracy predictions = classifier.predict(X_test) accuracy = accuracy_score(y_test, predictions) return accuracy"},{"question":"**Coding Assessment Question** # Objective Implement a Python class that uses the `gettext` module to provide internationalization capabilities. The class should handle translation for a given set of messages and support switching between multiple languages dynamically. # Requirements 1. **Class Name**: `MultiLangTranslator` 2. **Methods**: - `__init__(self, domain, localedir, languages)`: Initialize the translator with a domain, locale directory, and list of languages. It loads the appropriate translation files. - `set_language(self, language)`: Dynamically change the language used for translations. - `translate(self, message)`: Translate the given message into the current language. # Input and Output - **Input**: - `domain` (str): The domain to use for translations. - `localedir` (str): The directory where the locale files (.mo) are stored. - `languages` (list of str): List of supported languages. - `message` (str): The message to be translated. - **Output**: - Translated message (str) in the currently set language. # Constraints 1. Ensure that the class handles the absence of translation files gracefully by using fallback mechanisms. 2. The class should be able to handle singular and plural forms where necessary. # Example Usage ```python # Assume the following directory structure for translation files: # /path/to/locales/ # ├── fr/LC_MESSAGES/mydomain.mo # └── es/LC_MESSAGES/mydomain.mo translator = MultiLangTranslator(domain=\'mydomain\', localedir=\'/path/to/locales\', languages=[\'fr\', \'es\']) translator.set_language(\'fr\') print(translator.translate(\'Hello, World!\')) # Should print \\"Bonjour, le monde!\\" if translated translator.set_language(\'es\') print(translator.translate(\'Hello, World!\')) # Should print \\"¡Hola, Mundo!\\" if translated ``` # Implementation Write the implementation for the `MultiLangTranslator` class according to the specifications above.","solution":"import gettext import os class MultiLangTranslator: def __init__(self, domain, localedir, languages): self.domain = domain self.localedir = localedir self.languages = languages self.current_language = None self.translation = None self.set_language(languages[0]) # Default to the first language in the list def set_language(self, language): if language not in self.languages: raise ValueError(f\\"Language \'{language}\' not supported.\\") self.current_language = language try: self.translation = gettext.translation(self.domain, localedir=self.localedir, languages=[language]) except FileNotFoundError: # Fallback to a null translation self.translation = gettext.NullTranslations() def translate(self, message): if self.translation is None: return message return self.translation.gettext(message)"},{"question":"**Question: Recursive Indentation Checker** You are required to implement a custom indentation checker that mimics some of the functionality provided by the \\"tabnanny\\" module. Your task is to write a function that recursively scans a directory and its subdirectories to find Python files (`.py` extension) and check each file for lines that have inconsistent indentation. Your function should also handle symbolic links correctly and avoid infinite loops. Additionally, you should provide options for verbose output and filename-only output similar to \\"tabnanny\\". # Function Signature ```python def check_indentation(file_or_dir: str, verbose: bool = False, filename_only: bool = False): pass ``` # Input - `file_or_dir`: A string representing the path to a directory or a Python source file. - `verbose`: A boolean flag indicating whether to print verbose messages. Default is `False`. - `filename_only`: A boolean flag indicating whether to print only filenames of files containing indentation issues. Default is `False`. # Output The function should print the diagnostic messages or filenames to the standard output, depending on the flags provided. # Constraints 1. If `file_or_dir` is a directory, the function should scan all subdirectories recursively. 2. If `file_or_dir` is a file and it has a `.py` extension, it should be checked for indentation issues. 3. Indentation is considered inconsistent if it contains both tabs and spaces or if it does not follow a regular pattern (e.g., mixing of 2-space and 4-space indents). 4. Handle symbolic links appropriately to avoid infinite loops. # Example Consider the directory structure: ``` project/ │ ├── script1.py ├── folder1/ │ ├── script2.py │ └── script3.txt └── folder2/ ├── script4.py └── script5.py ``` If `script1.py` contains inconsistent indentation, the output of `check_indentation(\'project\', True, False)` might be: ``` Checking folder: project Checking file: script1.py Indentation issue detected in script1.py on line 3 Checking folder: folder1 Checking file: folder1/script2.py Checking file: folder1/script3.txt Ignoring non-Python file folder1/script3.txt Checking folder: folder2 Checking file: folder2/script4.py Checking file: folder2/script5.py ``` If `filename_only` is set to `True` and `script1.py` has inconsistent indentation, the output will just be: ``` script1.py ``` # Notes - The function should handle exceptions gracefully and print relevant error messages. - Use Python\'s built-in libraries for file handling, recursion, and token processing.","solution":"import os def check_indentation(file_or_dir, verbose=False, filename_only=False): def check_file(file_path): inconsistent = False with open(file_path, \'r\') as file: lines = file.readlines() indents = [] for i, line in enumerate(lines): stripped_line = line.lstrip() if stripped_line: indent = len(line) - len(stripped_line) indents.append((i + 1, indent, line)) spaces = any(line for line in indents if line[1] % 4 != 0) if spaces: inconsistent = True if verbose: print(f\'Inconsistent indentation found in file: {file_path}\') for i, indent, line in indents: if indent % 4 != 0: print(f\\"Line {i}, Indent length: {indent}, Content: {line.strip()}\\") return inconsistent def scan_dir(directory): for root, _, files in os.walk(directory): for file in files: file_path = os.path.join(root, file) if file_path.endswith(\'.py\'): if verbose: print(f\\"Checking file: {file_path}\\") if check_file(file_path): if filename_only: print(file_path) if os.path.isdir(file_or_dir): if verbose: print(f\\"Checking folder: {file_or_dir}\\") scan_dir(file_or_dir) elif os.path.isfile(file_or_dir) and file_or_dir.endswith(\'.py\'): if verbose: print(f\\"Checking file: {file_or_dir}\\") if check_file(file_or_dir): if filename_only: print(file_or_dir) else: print(f\\"Error: {file_or_dir} is neither a Python file nor a directory containing Python files.\\")"},{"question":"**Objective**: Demonstrate your understanding of Python\'s text processing capabilities through the implementation of a function that leverages multiple modules from Python’s text processing services. **Problem Statement**: You are tasked with the creation of a function called `retrieve_information` that processes a given piece of text to achieve the following: 1. **Extract specific data using regular expressions (re module)**: - Extract all the email addresses from the text. An email is defined as any string that matches the pattern `[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}`. - Extract all dates in the format `DD-MM-YYYY` or `DD/MM/YYYY`. 2. **Compute the longest matching subsequence using difflib module**: - Compute the longest common subsequence between two given strings. 3. **Format the extracted data using string formatting (string module)**: - Output the extracted email addresses and dates in a formatted string where each datum is on a new line preceded by its type (e.g., “Email: example@example.com”). # Function Signature ```python def retrieve_information(text: str, string1: str, string2: str) -> str: ``` # Input 1. `text` (str): A string that contains the text from which to extract emails and dates. 2. `string1` (str): The first string to compare for longest common subsequence. 3. `string2` (str): The second string to compare for longest common subsequence. # Output - Returns a formatted string with: - Each extracted email on a new line prefixed with \\"Email: \\". - Each extracted date on a new line prefixed with \\"Date: \\". - The longest common subsequence between `string1` and `string2` on a new line prefixed with \\"LCS: \\". # Constraints - You must use Python\'s in-built modules (`re`, `difflib`, and `string`) to achieve the task. - The function should handle empty inputs gracefully. - You should not use any external libraries. # Example ```python text = \\"Contact us at support@example.com. Our event is on 12-12-2023. Office email: office@example.org on 15/01/2023.\\" string1 = \\"abcde\\" string2 = \\"ace\\" result = retrieve_information(text, string1, string2) print(result) ``` # Expected Output ``` Email: support@example.com Email: office@example.org Date: 12-12-2023 Date: 15/01/2023 LCS: ace ``` Implement the `retrieve_information` function to ensure it meets the provided specifications.","solution":"import re import difflib import string def retrieve_information(text: str, string1: str, string2: str) -> str: # Extracting emails using regular expressions email_pattern = r\'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\' emails = re.findall(email_pattern, text) # Extracting dates using regular expressions date_pattern = r\'bd{2}[-/]d{2}[-/]d{4}b\' dates = re.findall(date_pattern, text) # Compute the longest common subsequence using difflib s = difflib.SequenceMatcher(None, string1, string2) lcs = \'\'.join([string1[i] for i, j, n in s.get_matching_blocks() if n > 0]) # Formatting the output string output = [] for email in emails: output.append(f\\"Email: {email}\\") for date in dates: output.append(f\\"Date: {date}\\") output.append(f\\"LCS: {lcs}\\") return \'n\'.join(output)"},{"question":"# Pandas DataFrame Analysis and Transformation You are provided with two datasets regarding sales and customer information in the form of pandas DataFrames. Your task is to perform several data manipulation and analysis tasks using pandas. You need to demonstrate your knowledge on DataFrame construction, indexing, merging, handling missing data, and performing summary statistics. Task 1. **Create DataFrames**: - `sales_data`: Sales information of various products. - `customer_data`: Information about customers. 2. **DataFrames Details**: - `sales_data`: ```csv sale_id, product, customer_id, sale_amount, sale_date 1, Widget, 101, 200, 2020-01-05 2, Gadget, 102, 450, 2020-01-17 3, Widget, 103, 300, 2020-02-13 4, Gadget, 101, 120, 2020-02-21 5, Widget, 104, 500, 2020-03-25 ``` - `customer_data`: ```csv customer_id, customer_name, customer_email 101, Alice, alice@example.com 102, Bob, bob@example.com 103, Charlie, charlie@example.com 104, David, david@example.com 105, Eve, eve@example.com ``` 3. **Tasks**: - **1. Load Data**: Write a function `load_data` to load these datasets into two pandas DataFrames. - **2. Merge DataFrames**: Write a function `merge_data` to merge the `sales_data` and `customer_data` DataFrames using `customer_id`. - **3. Handle Missing Data**: Write a function `handle_missing_data` to check for any missing values and fill them appropriately with \'N/A\' if any exist. - **4. Summary Statistics**: Write a function `compute_statistics` to compute the total sale amount per product and the number of unique customers for each product. - **5. Reshape Data**: Write a function `reshape_data` to pivot the merged DataFrame to show the sales amount for each product by each customer. Expected Functions: ```python import pandas as pd def load_data(): sales_data = pd.DataFrame({ \'sale_id\': [1, 2, 3, 4, 5], \'product\': [\'Widget\', \'Gadget\', \'Widget\', \'Gadget\', \'Widget\'], \'customer_id\': [101, 102, 103, 101, 104], \'sale_amount\': [200, 450, 300, 120, 500], \'sale_date\': [\'2020-01-05\', \'2020-01-17\', \'2020-02-13\', \'2020-02-21\', \'2020-03-25\'] }) customer_data = pd.DataFrame({ \'customer_id\': [101, 102, 103, 104, 105], \'customer_name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\', \'Eve\'], \'customer_email\': [\'alice@example.com\', \'bob@example.com\', \'charlie@example.com\', \'david@example.com\', \'eve@example.com\'] }) return sales_data, customer_data def merge_data(sales_data, customer_data): merged_df = pd.merge(sales_data, customer_data, on=\'customer_id\', how=\'left\') return merged_df def handle_missing_data(df): df.fillna(\'N/A\', inplace=True) return df def compute_statistics(df): total_sales_per_product = df.groupby(\'product\')[\'sale_amount\'].sum().reset_index() unique_customers_per_product = df.groupby(\'product\')[\'customer_id\'].nunique().reset_index() statistics_df = pd.merge(total_sales_per_product, unique_customers_per_product, on=\'product\') statistics_df.columns = [\'product\', \'total_sales\', \'unique_customers\'] return statistics_df def reshape_data(df): pivot_df = df.pivot_table(index=\'customer_name\', columns=\'product\', values=\'sale_amount\', aggfunc=\'sum\', fill_value=0) return pivot_df ``` Input Format: - None, the datasets are hard-coded within the `load_data` function. Output Format: - Each function should deliver appropriate DataFrame outputs as specified in the task descriptions above. Constraints: - Ensure that NaN values are properly handled. - Focus on using pandas functionalities efficiently. - Return DataFrames as specified without altering the input structure.","solution":"import pandas as pd def load_data(): sales_data = pd.DataFrame({ \'sale_id\': [1, 2, 3, 4, 5], \'product\': [\'Widget\', \'Gadget\', \'Widget\', \'Gadget\', \'Widget\'], \'customer_id\': [101, 102, 103, 101, 104], \'sale_amount\': [200, 450, 300, 120, 500], \'sale_date\': [\'2020-01-05\', \'2020-01-17\', \'2020-02-13\', \'2020-02-21\', \'2020-03-25\'] }) customer_data = pd.DataFrame({ \'customer_id\': [101, 102, 103, 104, 105], \'customer_name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\', \'Eve\'], \'customer_email\': [\'alice@example.com\', \'bob@example.com\', \'charlie@example.com\', \'david@example.com\', \'eve@example.com\'] }) return sales_data, customer_data def merge_data(sales_data, customer_data): merged_df = pd.merge(sales_data, customer_data, on=\'customer_id\', how=\'left\') return merged_df def handle_missing_data(df): df.fillna(\'N/A\', inplace=True) return df def compute_statistics(df): total_sales_per_product = df.groupby(\'product\')[\'sale_amount\'].sum().reset_index() unique_customers_per_product = df.groupby(\'product\')[\'customer_id\'].nunique().reset_index() statistics_df = pd.merge(total_sales_per_product, unique_customers_per_product, on=\'product\') statistics_df.columns = [\'product\', \'total_sales\', \'unique_customers\'] return statistics_df def reshape_data(df): pivot_df = df.pivot_table(index=\'customer_name\', columns=\'product\', values=\'sale_amount\', aggfunc=\'sum\', fill_value=0) return pivot_df"},{"question":"**Problem Statement:** You are given a dataset of car attributes and you are tasked with analyzing and visualizing some of the relationships between these attributes using seaborn. Specifically, you need to create a function that generates multiple types of regression plots and customizes their appearances according to provided specifications. **Function to Implement:** ```python def generate_regression_plots(data): Generate regression plots with various customizations using seaborn. Args: data (pd.DataFrame): A pandas DataFrame containing car attributes. Returns: None: The function should plot the graphs directly using seaborn. pass ``` **Details:** 1. **Linear Regression Plot**: - Plot the relationship between `weight` and `acceleration`. - Customize the appearance: Disable the confidence interval and use a blue line (`color=\\"blue\\"`). 2. **Higher-order Polynomial Regression**: - Plot the relationship between `weight` and `mpg` with a second-order polynomial regression. - Customize the appearance: Use an orange line (`color=\\"orange\\"`) and a `x` marker. 3. **Log-Linear Regression**: - Plot the relationship between `displacement` and `mpg` on a logarithmic x-axis. - Customize the appearance: Use a green line (`color=\\"green\\"`). 4. **Locally-Weighted Smoother (LOWESS)**: - Plot the relationship between `horsepower` and `mpg` using a LOWESS smoother. - Customize the appearance: Use a red line (`color=\\"red\\"`) and a circle marker (`marker=\\"o\\"`). 5. **Aggregate Data**: - Plot the relationship between `cylinders` and `acceleration` by aggregating acceleration values using their mean. - Customize the appearance: Add jitter to the `x` values (`x_jitter=0.15`). **Constraints:** - Use seaborn\'s `sns.regplot` function for all plots. - Ensure that each plot is clearly labeled with respective titles for better understanding. **Input Format:** - `data`: A pandas DataFrame with car attributes (`mpg`, `weight`, `acceleration`, `displacement`, `horsepower`, `cylinders`). **Example:** Here is an example of how you might call the function: ```python import seaborn as sns mpg = sns.load_dataset(\\"mpg\\") generate_regression_plots(mpg) ``` The function should generate and display the following plots for the given DataFrame: 1. A linear regression plot of `weight` vs `acceleration`. 2. A polynomial regression plot of `weight` vs `mpg`. 3. A log-linear regression plot of `displacement` vs `mpg`. 4. A LOWESS smoother plot of `horsepower` vs `mpg`. 5. An aggregated regression plot of `cylinders` vs `acceleration`. You are expected to use seaborn effectively and ensure the plots are clear and informative.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def generate_regression_plots(data): Generate regression plots with various customizations using seaborn. Args: data (pd.DataFrame): A pandas DataFrame containing car attributes. Returns: None: The function should plot the graphs directly using seaborn. # Linear Regression Plot plt.figure(figsize=(16, 10)) plt.subplot(2, 3, 1) sns.regplot(x=\'weight\', y=\'acceleration\', data=data, ci=None, color=\\"blue\\") plt.title(\\"Linear Regression: Weight vs Acceleration\\") # Higher-order Polynomial Regression plt.subplot(2, 3, 2) sns.regplot(x=\'weight\', y=\'mpg\', data=data, order=2, ci=None, color=\'orange\', marker=\'x\') plt.title(\\"Polynomial Regression: Weight vs MPG\\") # Log-Linear Regression plt.subplot(2, 3, 3) sns.regplot(x=\'displacement\', y=\'mpg\', data=data, logx=True, ci=None, color=\'green\') plt.title(\\"Log-Linear Regression: Displacement vs MPG\\") # Locally-Weighted Smoother (LOWESS) plt.subplot(2, 3, 4) sns.regplot(x=\'horsepower\', y=\'mpg\', data=data, lowess=True, ci=None, color=\'red\', marker=\'o\') plt.title(\\"LOWESS: Horsepower vs MPG\\") # Aggregate Data plt.subplot(2, 3, 5) sns.regplot(x=\'cylinders\', y=\'acceleration\', data=data, x_jitter=0.15, ci=None) plt.title(\\"Aggregated Data: Cylinders vs Acceleration\\") plt.tight_layout() plt.show()"},{"question":"Web Scraping and Data Analysis **Objective:** Implement a function that scrapes a webpage for data and performs analysis on the collected data. The task will test your understanding of HTTP requests, working with HTML, and utilizing Python data structures and control flow tools. **Problem Statement:** You are required to write a function that retrieves weather data from a mock webpage provided for you. The webpage lists daily temperature records for a month in a simple HTML table format. Your task is to: 1. Retrieve the HTML content of the webpage using the `requests` library. 2. Parse the HTML content to extract the temperature data using the `BeautifulSoup` library. 3. Store the extracted data in a suitable Python data structure. 4. Perform statistical analysis on the temperature data to find the average, minimum, and maximum temperatures for the month. 5. Return the results as a dictionary. **HTML Structure:** ```html <!DOCTYPE html> <html> <head> <title>Weather Data</title> </head> <body> <table id=\\"weather-data\\"> <thead> <tr> <th>Date</th> <th>Temperature</th> </tr> </thead> <tbody> <tr> <td>01-01-2023</td> <td>15</td> </tr> <tr> <td>02-01-2023</td> <td>14</td> </tr> <!-- More rows --> </tbody> </table> </body> </html> ``` **Function Signature:** ```python import requests from bs4 import BeautifulSoup def analyze_weather_data(url: str) -> dict: Fetches weather data from a given URL, analyzes the temperature records, and returns a dictionary with the average, minimum, and maximum temperatures. Args: - url: str: The URL of the webpage containing weather data. Returns: - dict: A dictionary with keys \'average\', \'min\', and \'max\' representing the respective statistical values of temperatures. pass ``` **Constraints:** - The HTML structure of the webpage should remain as given above. - Use the `requests` library to get the HTML content. - Use the `BeautifulSoup` library to parse the HTML. - Handle possible exceptions that might occur during the HTTP request and parsing. **Example:** ```python url = \\"http://example.com/weather-data.html\\" result = analyze_weather_data(url) print(result) # Output format: {\'average\': 14.5, \'min\': 14, \'max\': 15} ``` **Dependencies:** Please ensure you have the `requests` and `BeautifulSoup` libraries installed: ```sh pip install requests pip install beautifulsoup4 ``` **Notes:** - You might need to mock the HTTP request for testing purposes if the URL is not accessible. - Write clean and maintainable code, adhering to Python coding standards.","solution":"import requests from bs4 import BeautifulSoup def analyze_weather_data(url: str) -> dict: Fetches weather data from a given URL, analyzes the temperature records, and returns a dictionary with the average, minimum, and maximum temperatures. Args: - url: str: The URL of the webpage containing weather data. Returns: - dict: A dictionary with keys \'average\', \'min\', and \'max\' representing the respective statistical values of temperatures. try: # Retrieve HTML content response = requests.get(url) response.raise_for_status() html_content = response.content # Parse HTML content soup = BeautifulSoup(html_content, \'html.parser\') table = soup.find(\'table\', id=\'weather-data\') tbody = table.find(\'tbody\') rows = tbody.find_all(\'tr\') # Extract temperature data temperatures = [] for row in rows: temperature = int(row.find_all(\'td\')[1].text) temperatures.append(temperature) # Perform statistical analysis average_temperature = sum(temperatures) / len(temperatures) min_temperature = min(temperatures) max_temperature = max(temperatures) return { \'average\': average_temperature, \'min\': min_temperature, \'max\': max_temperature } except Exception as e: print(f\\"An error occurred: {e}\\") return { \'average\': None, \'min\': None, \'max\': None }"},{"question":"# Coding Assessment: Implementing a Custom Container Using `collections.abc` Objective: You are required to implement a custom container class in Python that exhibits properties of both a sequence and a set. This container should inherit from respective abstract base classes (`Sequence` and `Set`) from the `collections.abc` module. The container should allow storing non-duplicate elements and provide functionalities such as index access, iteration, addition, and length computation. Requirements: 1. **Class Definition**: Define a class `UniqueSequenceSet` that inherits from `collections.abc.Sequence` and `collections.abc.Set`. 2. **Required Methods**: - `__getitem__(self, index) -> Any`: Get the item at the specified index. - `__contains__(self, value) -> bool`: Check if the value is in the container. - `__iter__(self) -> Iterator`: Return an iterator over the container. - `__len__(self) -> int`: Return the number of elements in the container. - `add(self, value) -> None`: Add a new value to the container if not already present. - `index(self, value) -> int`: Return the index of the value in the container. 3. **Constraints**: - The container should not allow duplicate values. - `__getitem__` method should raise `IndexError` if the index is out of range. - The container should maintain insertion order for indexing purposes. 4. **Performance**: Implement the methods efficiently to avoid quadratic performance penalties. Input and Output Format: - You do not need to handle input reading or output printing. Focus solely on the implementation of the class and its methods as described. - Example interactions with the class: ```python uqs = UniqueSequenceSet() uqs.add(5) uqs.add(3) uqs.add(5) # Duplicate, should not be added assert len(uqs) == 2 assert uqs[0] == 5 assert uqs[1] == 3 assert 3 in uqs assert uqs.index(5) == 0 ``` Submission: Submit the `UniqueSequenceSet` class definition with all required methods implemented.","solution":"from collections.abc import Sequence, Set class UniqueSequenceSet(Sequence, Set): def __init__(self): self._data = [] self._set_data = set() def __getitem__(self, index): if index < 0 or index >= len(self._data): raise IndexError(\\"Index out of range\\") return self._data[index] def __contains__(self, value): return value in self._set_data def __iter__(self): return iter(self._data) def __len__(self): return len(self._data) def add(self, value): if value not in self._set_data: self._data.append(value) self._set_data.add(value) def index(self, value): if value not in self._set_data: raise ValueError(f\\"{value} not in container\\") return self._data.index(value)"},{"question":"You are required to implement a function `data_transformation_pipeline` that demonstrates a combination of text I/O and binary I/O operations using the `io` module. Your function should follow these steps: 1. Read text data from a given text file, with its path specified by the user. The file may contain multiple lines of text. 2. Convert the text data into uppercase. 3. Write the modified text back to another text file specified by the user. 4. Simultaneously, convert the uppercase text data into bytes using UTF-8 encoding and write this binary data to a binary file specified by the user. Input: - `input_text_filepath` (str): Path to the input text file that contains the original text data. - `output_text_filepath` (str): Path to the output text file where uppercase text data should be written. - `output_binary_filepath` (str): Path to the output binary file where the binary data should be written. Output: None Constraints: - The text files are assumed to be small enough to be loaded into memory. - All file paths provided are valid. - The function does not return any value but should properly create and write to the specified files. Example Usage: ```python data_transformation_pipeline(\'input.txt\', \'output.txt\', \'binary_output.bin\') ``` If `input.txt` contains: ``` hello world python io testing ``` After running the function, `output.txt` should contain: ``` HELLO WORLD PYTHON IO TESTING ``` And `binary_output.bin` should contain the corresponding UTF-8 encoded bytes of the uppercase text: ``` (b\'HELLO WORLDnPYTHON IO TESTINGn\') ``` # Implementation: ```python import io def data_transformation_pipeline(input_text_filepath, output_text_filepath, output_binary_filepath): # Step 1: Read text data from input file with open(input_text_filepath, \'r\', encoding=\'utf-8\') as infile: text_data = infile.read() # Step 2: Convert text data to uppercase upper_text_data = text_data.upper() # Step 3: Write the uppercase text to output text file with open(output_text_filepath, \'w\', encoding=\'utf-8\') as outfile: outfile.write(upper_text_data) # Step 4: Convert uppercase text to bytes and write to binary file binary_data = upper_text_data.encode(\'utf-8\') with open(output_binary_filepath, \'wb\') as binfile: binfile.write(binary_data) ``` Ensure that your code handles file I/O operations properly and adheres to best practices for file handling in Python.","solution":"import io def data_transformation_pipeline(input_text_filepath, output_text_filepath, output_binary_filepath): Reads text data from an input file, transforms it to uppercase, writes the uppercase text to an output file, and writes the corresponding UTF-8 encoded bytes to a binary file. Args: input_text_filepath (str): Path to the input text file. output_text_filepath (str): Path to the output text file. output_binary_filepath (str): Path to the output binary file. # Step 1: Read text data from input file with open(input_text_filepath, \'r\', encoding=\'utf-8\') as infile: text_data = infile.read() # Step 2: Convert text data to uppercase upper_text_data = text_data.upper() # Step 3: Write the uppercase text to output text file with open(output_text_filepath, \'w\', encoding=\'utf-8\') as outfile: outfile.write(upper_text_data) # Step 4: Convert uppercase text to bytes and write to binary file binary_data = upper_text_data.encode(\'utf-8\') with open(output_binary_filepath, \'wb\') as binfile: binfile.write(binary_data)"},{"question":"# Seaborn Plot Customization and Faceting **Objective:** Assess students\' ability to use the Seaborn library to create, facet, and customize plots. **Problem Statement:** You are provided with the `anscombe` dataset from the Seaborn library. Using this dataset, you need to create a faceted line plot with custom themes. Your task is to write a Python function `create_custom_plot` that accomplishes the following: 1. Loads the `anscombe` dataset. 2. Creates a faceted plot with two columns, faceted by the `dataset` column. Each facet should show a scatter plot of `x` versus `y`. 3. Adds a linear best-fit line to each facet. 4. Customizes the plot with a white grid background and presentation-friendly settings. 5. Customizes the appearance of the plot with `axes.facecolor` set to white, `axes.edgecolor` set to slategray, and line width set to 4. **Function Signature:** ```python def create_custom_plot() -> None: pass ``` **Requirements:** - Use Seaborn\'s `Plot` object from the `seaborn.objects` module. - Facet the plot by the `dataset` column, arranging facets in two columns. - Customize the plot by setting the background to a white grid and applying a context suitable for presentations. - Ensure the line width of the plot lines is set to 4. **Constraints:** - No external dataset use is allowed, only the Seaborn\'s `anscombe` dataset. - Use appropriate Seaborn and matplotlib functions for theming and customization. **Example of what the plot might look like:** ![Sample output](https://seaborn.pydata.org/_images/plotting_with_objects_9_0.png) **Note:** The link above is a sample output for reference, actual appearance might vary based on styling.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plot() -> None: # Load the anscombe dataset df = sns.load_dataset(\\"anscombe\\") # Set the theme and context for the plot sns.set_theme(style=\\"whitegrid\\") sns.set_context(\\"talk\\") # Create the faceted plot plot = sns.lmplot( x=\\"x\\", y=\\"y\\", col=\\"dataset\\", hue=\\"dataset\\", data=df, col_wrap=2, ci=None, palette=\\"muted\\", height=4, scatter_kws={\\"s\\": 50} ) # Customize the appearance with specific properties for ax in plot.axes.flat: ax.set_facecolor(\'white\') for spine in ax.spines.values(): spine.set_edgecolor(\'slategray\') spine.set_linewidth(4) # Display the plot plt.show()"},{"question":"# Question: Implement a Custom Dictionary with Extended Features **Objective:** Create a custom dictionary class in Python that extends the functionality of the built-in dictionary. Your class should support all standard dictionary operations and add additional methods. **Requirements:** 1. **Class Definition:** - Define a class `ExtendedDict` that inherits from Python\'s built-in `dict`. 2. **Additional Methods:** - `get_middle_key_value()`: Return the key-value pair from the middle of the dictionary. If the dictionary has an even number of elements, return the pair just left of the center. - `pop_last_n_items(n)`: Remove and return the last `n` key-value pairs from the dictionary as a list of tuples. If `n` is greater than the number of elements, return all elements and leave the dictionary empty. - `merge_with(other_dict)`: Merge another dictionary into the current dictionary. In case of key conflicts, values from `other_dict` should overwrite those in the current dictionary. - `keys_with_prefix(prefix)`: Return a list of keys that start with the given `prefix`. **Input and Output Formats:** - `get_middle_key_value()` - **Input:** No input directly from user. - **Output:** A tuple representing the middle key-value pair. E.g., `(\'key\', \'value\')` - `pop_last_n_items(n)` - **Input:** An integer `n`. - **Output:** A list of tuples representing the removed key-value pairs. E.g., `[(\'key1\', \'value1\'), (\'key2\', \'value2\')]` - `merge_with(other_dict)` - **Input:** A dictionary `other_dict`. - **Output:** No direct output, but the dictionary gets updated. - `keys_with_prefix(prefix)` - **Input:** A string `prefix`. - **Output:** A list of keys starting with `prefix`. E.g., `[\'prefix1\', \'prefix2\']` **Constraints:** - The `ExtendedDict` should support all standard dictionary methods. - Assume all keys in the dictionary are strings. - Assume inputs are well-formed. # Example Usage ```python my_dict = ExtendedDict({\'a\': 1, \'b\': 2, \'c\': 3, \'d\': 4, \'e\': 5}) print(my_dict.get_middle_key_value()) # Output: (\'c\', 3) removed_items = my_dict.pop_last_n_items(2) print(removed_items) # Output: [(\'d\', 4), (\'e\', 5)] print(my_dict) # Output: {\'a\': 1, \'b\': 2, \'c\': 3} my_dict.merge_with({\'b\': 20, \'f\': 6}) print(my_dict) # Output: {\'a\': 1, \'b\': 20, \'c\': 3, \'f\': 6} print(my_dict.keys_with_prefix(\'a\')) # Output: [\'a\'] ``` **Implementation Note:** You may use Python’s built-in dictionary methods where appropriate, but for the added methods, implement the functionality by manually managing and manipulating the dictionary contents.","solution":"class ExtendedDict(dict): def get_middle_key_value(self): keys = list(self.keys()) middle_index = (len(keys) - 1) // 2 middle_key = keys[middle_index] return middle_key, self[middle_key] def pop_last_n_items(self, n): items = list(self.items()) if n >= len(items): result = items self.clear() else: result = items[-n:] for key, _ in result: del self[key] return result def merge_with(self, other_dict): self.update(other_dict) def keys_with_prefix(self, prefix): return [key for key in self.keys() if key.startswith(prefix)] # Example of class usage: my_dict = ExtendedDict({\'a\': 1, \'b\': 2, \'c\': 3, \'d\': 4, \'e\': 5}) print(my_dict.get_middle_key_value()) # Output: (\'c\', 3) removed_items = my_dict.pop_last_n_items(2) print(removed_items) # Output: [(\'d\', 4), (\'e\', 5)] print(my_dict) # Output: {\'a\': 1, \'b\': 2, \'c\': 3} my_dict.merge_with({\'b\': 20, \'f\': 6}) print(my_dict) # Output: {\'a\': 1, \'b\': 20, \'c\': 3, \'f\': 6} print(my_dict.keys_with_prefix(\'a\')) # Output: [\'a\']"},{"question":"**Objective:** To assess the student\'s understanding of memory allocation tracking in Python using the `tracemalloc` module. Students are required to write code that effectively uses the module to track, compare, and analyze memory allocations within a given Python application. **Problem Statement:** You are given a Python script that performs various operations and you need to analyze its memory usage. The script contains a function `process_data` which generates and processes a large amount of temporary data. Your task is to: 1. Use `tracemalloc` to trace memory allocations from the start of the script. 2. Take a snapshot just before calling `process_data` and then take another snapshot immediately after the function completes. 3. Compare the two snapshots to identify any increases in memory usage. 4. Generate a report showing the top 5 lines of code responsible for the most memory allocations during the function execution. **Requirements:** 1. The script should use `tracemalloc` to start tracing memory allocations. 2. Capture snapshots before and after the function execution. 3. Compare the snapshots and identify the top 5 lines of code with the highest increase in memory usage. 4. Print a formatted report of these lines, displaying the filename, line number, amount of memory allocated, and the actual line of code. **Constraints:** - Assume the `process_data` function is provided and performs some data processing. - You should only consider memory allocations made during the execution of `process_data`. **Input and Output Formats:** - **Input:** There is no direct input. The script provided below should be traced and analyzed. - **Output:** Logs and reports printed to the console. ```python # Provided script: import tracemalloc import random def process_data(): data = [random.randint(1, 100) for _ in range(100000)] processed = [x * 2 for x in data] return sum(processed) def main(): tracemalloc.start() snapshot_before = tracemalloc.take_snapshot() result = process_data() # Function to be analyzed snapshot_after = tracemalloc.take_snapshot() top_stats = snapshot_after.compare_to(snapshot_before, \'lineno\') print(\\"[ Top 5 memory lines ]\\") for stat in top_stats[:5]: frame = stat.traceback[0] print(f\\"{frame.filename}:{frame.lineno} - {stat.size / 1024:.1f} KiB\\") line = linecache.getline(frame.filename, frame.lineno).strip() if line: print(f\\" {line}\\") if __name__ == \\"__main__\\": main() ``` Task: 1. Implement the `main` function as specified. 2. Ensure proper usage of `tracemalloc` functions to achieve the desired analysis. 3. Generate and print the report within the `main` function. 4. Optionally, you can refactor the code as you see fit to improve efficiency or readability. **Performance Requirements:** - The script should be efficient and avoid unnecessary memory usage. - The output should be clear and formatted as specified.","solution":"import tracemalloc import random import linecache def process_data(): data = [random.randint(1, 100) for _ in range(100000)] processed = [x * 2 for x in data] return sum(processed) def display_top_memory_lines(snapshot_before, snapshot_after): top_stats = snapshot_after.compare_to(snapshot_before, \'lineno\') print(\\"[ Top 5 memory lines ]\\") for stat in top_stats[:5]: frame = stat.traceback[0] print(f\\"{frame.filename}:{frame.lineno} - {stat.size / 1024:.1f} KiB\\") line = linecache.getline(frame.filename, frame.lineno).strip() if line: print(f\\" {line}\\") def main(): tracemalloc.start() snapshot_before = tracemalloc.take_snapshot() result = process_data() # Function to be analyzed snapshot_after = tracemalloc.take_snapshot() display_top_memory_lines(snapshot_before, snapshot_after) if __name__ == \\"__main__\\": main()"},{"question":"Objective Implement a simple text editor using the `curses` library in Python. Your editor should support basic operations such as text input, cursor movement, and saving the content to a file. Requirements **Function:** `text_editor` **Input:** - The function does not take any inputs directly. **Output:** - The function should save the content of the text editor to a file when the user chooses to save and exit. **Functional Specifications:** 1. The text editor should start with an empty screen and allow the user to type characters. 2. The user should be able to move the cursor using the arrow keys. 3. The text editor should support basic text editing commands: - **Ctrl-S**: Save the content to a file. - **Ctrl-Q**: Quit the editor without saving. - **Ctrl-C**: Quit and save the content to a file. - **Backspace**: Delete character before the cursor. - **Enter**: Move to the next line. 4. The text editor should handle window resizing properly. **Constraints:** - The text editor should handle up to 100 lines of text. - Each line can have a maximum of 80 characters. - Assume the editor runs on a Unix-like terminal. **Performance Requirements:** - The text editor should be responsive to user inputs without noticeable lag. - The window refresh should be minimized to avoid flickering. # Example Implementing a basic text editor would involve creating a window, capturing keyboard events, handling cursor movements, and managing text input and deletion. ```python import curses def text_editor(stdscr): curses.curs_set(1) # Show cursor stdscr.clear() text = [[]] cursor_y, cursor_x = 0, 0 while True: stdscr.clear() for i, line in enumerate(text): stdscr.addstr(i, 0, \'\'.join(line)) stdscr.move(cursor_y, cursor_x) stdscr.refresh() key = stdscr.getch() if key == curses.KEY_UP: if cursor_y > 0: cursor_y -= 1 elif key == curses.KEY_DOWN: if cursor_y < len(text) - 1: cursor_y += 1 elif key == curses.KEY_LEFT: if cursor_x > 0: cursor_x -= 1 elif key == curses.KEY_RIGHT: if cursor_x < len(text[cursor_y]): cursor_x += 1 elif key == curses.KEY_BACKSPACE or key == 127: if cursor_x > 0: text[cursor_y].pop(cursor_x - 1) cursor_x -= 1 elif cursor_y > 0: cursor_x = len(text[cursor_y - 1]) text[cursor_y - 1].extend(text[cursor_y]) text.pop(cursor_y) cursor_y -= 1 elif key == curses.KEY_ENTER or key == 10: text.insert(cursor_y + 1, text[cursor_y][cursor_x:]) text[cursor_y] = text[cursor_y][:cursor_x] cursor_y += 1 cursor_x = 0 elif key == 24: # Ctrl-X: Save and Exit with open(\'output.txt\', \'w\') as f: for line in text: f.write(\'\'.join(line) + \'n\') return elif key == 17: # Ctrl-Q: Quit without Saving return elif key == 3: # Ctrl-C: Quit and Save with open(\'output.txt\', \'w\') as f: for line in text: f.write(\'\'.join(line) + \'n\') return else: text[cursor_y].insert(cursor_x, chr(key)) cursor_x += 1 curses.wrapper(text_editor) ``` Notes - Ensure thorough exception handling for scenarios such as invalid key presses and window resizing. - Make sure to test your text editor on different terminal sizes to guarantee robust handling of edge cases.","solution":"import curses def text_editor(stdscr): curses.curs_set(1) # Show cursor stdscr.clear() text = [[]] cursor_y, cursor_x = 0, 0 while True: stdscr.clear() for i, line in enumerate(text): stdscr.addstr(i, 0, \'\'.join(line)) stdscr.move(cursor_y, cursor_x) stdscr.refresh() key = stdscr.getch() if key == curses.KEY_UP: if cursor_y > 0: cursor_y -= 1 cursor_x = min(cursor_x, len(text[cursor_y])) elif key == curses.KEY_DOWN: if cursor_y < len(text) - 1: cursor_y += 1 cursor_x = min(cursor_x, len(text[cursor_y])) elif key == curses.KEY_LEFT: if cursor_x > 0: cursor_x -= 1 elif cursor_y > 0: cursor_y -= 1 cursor_x = len(text[cursor_y]) elif key == curses.KEY_RIGHT: if cursor_x < len(text[cursor_y]): cursor_x += 1 elif cursor_y < len(text) - 1: cursor_y += 1 cursor_x = 0 elif key == curses.KEY_BACKSPACE or key == 127: if cursor_x > 0: text[cursor_y].pop(cursor_x - 1) cursor_x -= 1 elif cursor_y > 0: cursor_x = len(text[cursor_y - 1]) text[cursor_y - 1].extend(text[cursor_y]) text.pop(cursor_y) cursor_y -= 1 elif key == 10: # Enter key text.insert(cursor_y + 1, text[cursor_y][cursor_x:]) text[cursor_y] = text[cursor_y][:cursor_x] cursor_y += 1 cursor_x = 0 elif key == 23: # Ctrl-W: Save and exit save_to_file(text) return elif key == 17: # Ctrl-Q: Quit without saving return elif key == 19: # Ctrl-S: Save save_to_file(text) elif key == 3: # Ctrl-C: Quit and save save_to_file(text) return else: text[cursor_y].insert(cursor_x, chr(key)) cursor_x += 1 def save_to_file(text): with open(\'output.txt\', \'w\') as f: for line in text: f.write(\'\'.join(line) + \'n\')"},{"question":"# Contextual Task Executor In this task, you are required to write a class called `ContextualTaskExecutor` that allows you to execute a list of functions within a specific context using context variables. This class should encapsulate the necessary methods to set up the context, add tasks, and execute them while maintaining proper context variable isolation. Class Definition ```python class ContextualTaskExecutor: def __init__(self): ... def add_task(self, context_var, value, task): ... def execute_tasks(self): ... ``` # Task Details 1. **Initialization**: - The `__init__` method should initialize an empty context and a list to store tasks. 2. **Adding Tasks**: - The `add_task` method should take three parameters: - `context_var`: an instance of `ContextVar`. - `value`: the value to set for the context variable. - `task`: a function to be executed. - This method should store each task along with its corresponding context variable and value. 3. **Executing Tasks**: - The `execute_tasks` method should execute all stored tasks within their specified contexts. - Each task should be executed in the context where the respective context variable is set to its specified value. - After executing all tasks, this method should return a list containing the results of each task execution, in the same order the tasks were added. # Example Usage ```python from contextvars import ContextVar # Define some context variables and tasks var1 = ContextVar(\'var1\') var2 = ContextVar(\'var2\') def task1(): return var1.get() * 2 def task2(): return var2.get() + 10 # Create an instance of the ContextualTaskExecutor executor = ContextualTaskExecutor() # Add tasks with corresponding context variables and values executor.add_task(var1, 5, task1) executor.add_task(var2, 15, task2) # Execute tasks result = executor.execute_tasks() print(result) # Expected output: [10, 25] ``` Constraints - It is guaranteed that: - The tasks do not have side effects that modify shared state. - Context variables are properly set before each task is executed. Performance - Your implementation should be O(n) with respect to the number of tasks added, as each task needs to be executed sequentially in its own context. Note: Usage of `contextvars`, proper context management, and ensuring unique context execution for each task are key requirements for this problem.","solution":"from contextvars import ContextVar class ContextualTaskExecutor: def __init__(self): self.tasks = [] def add_task(self, context_var, value, task): self.tasks.append((context_var, value, task)) def execute_tasks(self): results = [] for context_var, value, task in self.tasks: token = context_var.set(value) try: results.append(task()) finally: context_var.reset(token) return results"},{"question":"**Complex Configuration Handling Using `configparser`** **Objective:** Write a Python function that reads, processes, and writes back complex configuration data using `configparser`. Your function should demonstrate a solid understanding of reading from multiple sources, handling interpolation, dealing with fallbacks, and customizing the parser\'s behavior. **Function Signature:** ```python def process_configurations(defaults: dict, config_files: list, update_dict: dict) -> str: Reads configuration data from multiple sources, processes it according to key updates, and writes the final configuration to a new INI file, returning its path. Args: - defaults (dict): A dictionary of default configuration values. - config_files (list): A list of file paths for INI configuration files. - update_dict (dict): A dictionary with section names as keys and dictionaries of key-value pairs representing the updates for each section. Returns: - str: The file path of the newly written INI file containing the updated configuration. ``` **Inputs:** 1. `defaults`: A dictionary containing default configuration values (e.g., `{\\"Compression\\": \\"yes\\", \\"Port\\": 8080}`). 2. `config_files`: A list of strings representing paths to INI configuration files that should be read. 3. `update_dict`: A dictionary where keys are section names and values are dictionaries with the updates for each section. **Outputs:** 1. A string representing the file path of the newly written configuration INI file with all updates applied. **Constraints:** 1. The function should handle multiple configuration files, applying updates from the last file read if there are conflicts. 2. Default values should be utilized when certain keys/sections are not available in the provided files. 3. Perform value interpolation wherever required as described in the documentation. 4. Comments from the original configuration files should be ignored in the final output. 5. Ensure that the newly written configuration file maintains a consistent, readable format. **Example:** Suppose we have the following defaults, configuration files, and updates: ```python defaults = { \\"Compression\\": \\"yes\\", \\"Port\\": 8080 } config_files = [ \\"config1.ini\\", # Contains [FORGE.USER] User=hg \\"config2.ini\\" # Contains [FORGE.USER] User=git ] update_dict = { \\"FORGE.USER\\": {\\"CompressionLevel\\": \\"5\\"}, \\"NEW.SECTION\\": {\\"NewKey\\": \\"NewValue\\"} } output_file_path = process_configurations(defaults, config_files, update_dict) print(output_file_path) # Output should be the path to the newly created INI file ``` In the above example, `config2.ini` will overwrite values in `config1.ini` if any conflicts arise, and the `update_dict` will apply additional updates to the configuration. The final configuration written to the new file should integrate all these values and appropriate interpolations. **Implementation Hints:** - Utilize the `configparser.ConfigParser` for reading and writing configuration files. - Use `config.read()` method to read from multiple files. - Apply updates using the dictionary interface. - Ensure to use interpolation features provided by `configparser`. - Write the final configuration into a new INI file and return the path.","solution":"import configparser import os def process_configurations(defaults: dict, config_files: list, update_dict: dict) -> str: Reads configuration data from multiple sources, processes it according to key updates, and writes the final configuration to a new INI file, returning its path. Args: - defaults (dict): A dictionary of default configuration values. - config_files (list): A list of file paths for INI configuration files. - update_dict (dict): A dictionary with section names as keys and dictionaries of key-value pairs representing the updates for each section. Returns: - str: The file path of the newly written INI file containing the updated configuration. config = configparser.ConfigParser(defaults=defaults, interpolation=configparser.ExtendedInterpolation()) # Read configuration from multiple files config.read(config_files) # Apply updates from update_dict for section, updates in update_dict.items(): if not config.has_section(section): config.add_section(section) for key, value in updates.items(): config.set(section, key, value) # Write the final configuration to a new file output_file_path = \\"updated_config.ini\\" with open(output_file_path, \'w\') as configfile: config.write(configfile) return output_file_path"},{"question":"# PyTorch Serialization Challenge You are tasked with implementing functionality to save and load a PyTorch model in a way that preserves its complete state and optimizes storage usage, considering the intricacies of view relationships and potential version differences. Objectives: 1. **Save a Complex Model**: Implement a function `save_model` that saves a provided model into a specified file format while preserving view relationships and optimizing storage. 2. **Load a Complex Model**: Implement a function `load_model` that loads the model back from the file, ensuring all attributes and state are accurately restored. 3. **Version Compatibility Handling**: Ensure the functions account for potential version differences as described in the documentation. Requirements: - Save the model state dict and relevant tensors with preserving view relationships. - Optimize storage if the tensors are smaller compared to their shared storage objects. - Handle any versioning issues given in the problem statement, such as ensuring `torch.div` and `torch.full` behaviors are consistent across versions. Input Format: 1. `save_model(model: torch.nn.Module, file_path: str) -> None` - `model`: An instance of a PyTorch model to be saved. - `file_path`: A string representing the file path where the model should be saved. 2. `load_model(file_path: str, model_class: Type[torch.nn.Module]) -> torch.nn.Module` - `file_path`: A string representing the file path from which the model should be loaded. - `model_class`: The class of the model to be restored. Output Format: 1. `save_model`: No explicit output. The state should be saved to the provided file path. 2. `load_model`: Returns an instance of the restored model. Constraints: - Apply appropriate checks to ensure tensors and storages are correctly handled and cloned if necessary. - Manage potential version incompatibilities when saving and loading. - Ensure the solution works with PyTorch models containing multiple layers and possible custom modules. Example Usage: ```python import torch # Define a sample model class class MyModel(torch.nn.Module): def __init__(self): super(MyModel, self).__init__() self.fc1 = torch.nn.Linear(10, 5) self.fc2 = torch.nn.Linear(5, 2) def forward(self, x): x = self.fc1(x) x = torch.nn.functional.relu(x) x = self.fc2(x) return x # Instantiate the model model = MyModel() # Save the model save_model(model, \'mymodel.pth\') # Load the model loaded_model = load_model(\'mymodel.pth\', MyModel) # Verify the loaded model assert isinstance(loaded_model, MyModel) assert torch.equal(model.state_dict()[\'fc1.weight\'], loaded_model.state_dict()[\'fc1.weight\']) ``` # Notes: - You may assume that the `model_class` provided to `load_model` matches the saved model\'s class. - Include error handling to manage file reading/writing issues and incorrect model class matchings.","solution":"import torch def save_model(model: torch.nn.Module, file_path: str) -> None: Save the state of a PyTorch model to a file. Args: model (torch.nn.Module): The model to save. file_path (str): The path to the file where the model state will be saved. Returns: None # Save the state_dict and all tensors state = { \'state_dict\': model.state_dict(), \'model_class\': model.__class__ } torch.save(state, file_path) def load_model(file_path: str, model_class: type) -> torch.nn.Module: Load the state of a PyTorch model from a file. Args: file_path (str): The path to the file from which the model state will be loaded. model_class (type): The class of the model to instantiate and load. Returns: model (torch.nn.Module): The loaded model with its state restored. checkpoint = torch.load(file_path) # Instantiate the model model = model_class() # Load the state_dict into the model model.load_state_dict(checkpoint[\'state_dict\']) return model"},{"question":"Implement a Semi-Supervised Learning Model Using Label Propagation **Objective**: Implement a semi-supervised learning model using the `LabelPropagation` class from `scikit-learn`. The model should be capable of predicting labels for a partially labeled dataset and should utilize the RBF kernel method. **Description**: The task is to implement a custom function `semi_supervised_label_propagation` that takes a dataset where only a few samples are labeled, applies Label Propagation, and returns the predicted labels for the unlabeled samples. **Function Signature**: ```python def semi_supervised_label_propagation(X, y, gamma=20, max_iter=1000): Perform semi-supervised classification using Label Propagation. Parameters: ----------- X : array-like of shape (n_samples, n_features) The feature matrix of the dataset. y : array-like of shape (n_samples,) The target labels, with -1 denoting unlabeled samples. gamma : float, optional (default=20) Kernel coefficient for RBF kernel. max_iter : int, optional (default=1000) Maximum number of iterations allowed. Returns: -------- y_pred : array of shape (n_samples,) The predicted labels for all samples, including the initially unlabeled ones. pass ``` # Constraints: - You must use the `LabelPropagation` class from `sklearn.semi_supervised`. - The provided `y` array will contain `-1` for unlabeled points. - Utilize the RBF kernel method where `gamma` is a tunable hyperparameter. - Ensure the function handles large datasets efficiently. # Input Format: - `X`: A 2D numpy array or similar array-like structure with shape `(n_samples, n_features)`. - `y`: A 1D numpy array or similar array-like structure with shape `(n_samples,)` containing the labels. Use `-1` for unlabeled samples. - `gamma`: A float representing the kernel coefficient for the RBF kernel (default is 20). - `max_iter`: An integer specifying the maximum number of iterations allowed (default is 1000). # Output Format: - Return `y_pred`, a 1D array of length `n_samples`, containing the predicted labels for all samples, including the unlabeled ones. # Example: ```python import numpy as np # Example dataset X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]]) y = np.array([0, 1, -1, -1, 0, -1]) # Function call predicted_labels = semi_supervised_label_propagation(X, y, gamma=0.1, max_iter=100) # Expected output: array([0, 1, 1, 1, 0, 1]) # Note: The actual labels might differ slightly due to different random states. print(predicted_labels) ``` Implement the function in the provided template and test it with various datasets to ensure accuracy and efficiency.","solution":"from sklearn.semi_supervised import LabelPropagation def semi_supervised_label_propagation(X, y, gamma=20, max_iter=1000): Perform semi-supervised classification using Label Propagation. Parameters: ----------- X : array-like of shape (n_samples, n_features) The feature matrix of the dataset. y : array-like of shape (n_samples,) The target labels, with -1 denoting unlabeled samples. gamma : float, optional (default=20) Kernel coefficient for RBF kernel. max_iter : int, optional (default=1000) Maximum number of iterations allowed. Returns: -------- y_pred : array of shape (n_samples,) The predicted labels for all samples, including the initially unlabeled ones. label_propagation_model = LabelPropagation(kernel=\'rbf\', gamma=gamma, max_iter=max_iter) label_propagation_model.fit(X, y) y_pred = label_propagation_model.transduction_ return y_pred"},{"question":"**Title: Implementing and Managing a Key-Value Database Using `dbm` Module** **Objective:** Implement a key-value storage system using the `dbm` module, which allows managing and interacting with a simple database file. Demonstrate your ability to open, read, write, and properly close the database, handle errors, and implement additional functionalities to traverse and synchronize the database. **Description:** Your task is to implement a Python function `manage_db(file_path: str, operations: list[tuple], mode: str = \'c\') -> list` that performs a sequence of operations on a DBM database specified by `file_path`. The function should support the following operations: - Insert a key-value pair. - Retrieve a value by key. - Delete a key. - Traverse through all keys. - Synchronize the database. - Handle errors gracefully. **Function Signature:** ```python import dbm def manage_db(file_path: str, operations: list[tuple], mode: str = \'c\') -> list: pass ``` **Input:** - `file_path` (str): The path to the database file. - `operations` (list[tuple]): A list of operations to be executed on the database. Each operation is represented by a tuple where the first element is a string indicating the operation type, and the subsequent elements are the parameters for that operation. - `(\'insert\', key, value)`: Insert the key-value pair `key` (str) and `value` (str) into the database. - `(\'retrieve\', key)`: Retrieve the value associated with `key` (str) from the database. - `(\'delete\', key)`: Delete the entry with `key` (str) from the database. - `(\'traverse\',)`: Traverse through all keys in the database and return them as a list. - `(\'sync\',)`: Synchronize the database to ensure all changes are written to disk. - `mode` (str): File mode for opening the database, defaults to `\'c\'`. **Output:** - A list containing the results of \'retrieve\' and \'traverse\' operations in the order they were executed. **Constraints:** - Keys and values must be treated as strings and will be automatically converted to bytes. - You must handle `dbm.error` exceptions gracefully and return an appropriate message if an error occurs. - Use context management to ensure the database is properly closed after operations. **Example:** ```python operations = [ (\'insert\', \'greet\', \'hello\'), (\'insert\', \'farewell\', \'goodbye\'), (\'retrieve\', \'greet\'), (\'traverse\',), (\'delete\', \'farewell\'), (\'traverse\',), (\'sync\',), ] result = manage_db(\'mydatabase\', operations) print(result) ``` Expected Output: ```python [b\'hello\', [b\'greet\', b\'farewell\'], [b\'greet\']] ``` **Notes:** - Ensure the function is robust and can handle different scenarios gracefully. - Pay attention to the context management feature of the `dbm` module while working with the database file. **Additional Resources:** - Python 3 `dbm` module documentation.","solution":"import dbm def manage_db(file_path: str, operations: list[tuple], mode: str = \'c\') -> list: results = [] try: with dbm.open(file_path, mode) as db: for operation in operations: if operation[0] == \'insert\': key, value = operation[1], operation[2] db[key] = value elif operation[0] == \'retrieve\': key = operation[1] results.append(db.get(key, None)) elif operation[0] == \'delete\': key = operation[1] if key in db: del db[key] elif operation[0] == \'traverse\': keys = list(db.keys()) results.append(keys) elif operation[0] == \'sync\': db.sync() except dbm.error as e: results.append(f\\"DBM error: {str(e)}\\") return results"},{"question":"**Coding Assessment Question: Combining Data Manipulation and Grouping in pandas** **Objective:** The goal of this task is to evaluate your ability to manipulate a pandas DataFrame, perform conditional selections, handle missing data, and apply grouping and aggregation operations. **Task:** You are provided with a dataset of sales transactions. Each transaction record contains the following fields: - `transaction_id`: Unique identifier for each transaction - `customer_id`: Unique identifier for each customer - `transaction_date`: Date when the transaction occurred - `product_category`: Category of the product sold in the transaction - `amount`: Sales amount in the transaction Write a function `analyze_sales_data` which takes a DataFrame containing the above columns as input and performs the following operations: 1. **Filter Data:** - Remove transactions where the `amount` is less than or equal to zero. - If there are any missing values in `amount`, replace them with the average amount of non-missing values. 2. **Add Derived Columns:** - Create a new column called `transaction_month` that contains the month of the `transaction_date`. - Create a new column called `sales_category` which categorizes the transactions based on the `amount`: - \'Low\' for amounts less than or equal to 50 - \'Medium\' for amounts between 50 and 200 (inclusive) - \'High\' for amounts greater than 200 3. **Group and Aggregate:** - Group the transactions by `customer_id` and `transaction_month`. - For each group, calculate the following: - Total sales (`total_sales`) - Average sales amount (`average_sales`) - Number of transactions (`transaction_count`) 4. **Returning the Result:** - Return the aggregated DataFrame sorted by `total_sales` in descending order. **Constraints:** - All input DataFrames will be valid and non-empty. - The `transaction_date` will be in `YYYY-MM-DD` format. **Input:** - `data` (pandas DataFrame): DataFrame containing the sales transactions with columns `[\'transaction_id\', \'customer_id\', \'transaction_date\', \'product_category\', \'amount\']` **Output:** - pandas DataFrame: Aggregated DataFrame sorted by `total_sales` in descending order, with columns `[\'customer_id\', \'transaction_month\', \'total_sales\', \'average_sales\', \'transaction_count\']` **Function Signature:** ```python def analyze_sales_data(data: pd.DataFrame) -> pd.DataFrame: pass ``` **Example:** ```python import pandas as pd data = pd.DataFrame({ \'transaction_id\': [1, 2, 3, 4, 5], \'customer_id\': [\'C1\', \'C2\', \'C1\', \'C2\', \'C3\'], \'transaction_date\': [\'2023-01-10\', \'2023-02-15\', \'2023-01-20\', \'2023-02-25\', \'2023-03-01\'], \'product_category\': [\'Electronics\', \'Clothing\', \'Electronics\', \'Groceries\', \'Electronics\'], \'amount\': [150.0, 75.0, -20.0, 300.0, 200.0] }) result = analyze_sales_data(data) print(result) ``` Expected Output: ``` customer_id transaction_month total_sales average_sales transaction_count 0 C2 2023-02 375.0 187.5 2 1 C1 2023-01 150.0 150.0 1 2 C3 2023-03 200.0 200.0 1 ``` **Note:** - You are encouraged to use the pandas documentation and explore various methods that can be used to achieve the desired result. Properly document your code to explain the steps and the logic behind them.","solution":"import pandas as pd def analyze_sales_data(data: pd.DataFrame) -> pd.DataFrame: # Filter out transactions where amount is less than or equal to zero data = data[data[\'amount\'] > 0] # Replace missing values in \'amount\' with the average amount of non-missing values if data[\'amount\'].isnull().any(): avg_amount = data[\'amount\'].mean() data[\'amount\'].fillna(avg_amount, inplace=True) # Create `transaction_month` column data[\'transaction_month\'] = pd.to_datetime(data[\'transaction_date\']).dt.to_period(\'M\') # Define function to categorize sales amount def categorize_amount(amount): if amount <= 50: return \'Low\' elif 50 < amount <= 200: return \'Medium\' else: return \'High\' # Create `sales_category` column data[\'sales_category\'] = data[\'amount\'].apply(categorize_amount) # Group by customer_id and transaction_month and aggregate the required fields grouped_data = data.groupby([\'customer_id\', \'transaction_month\']).agg( total_sales=(\'amount\', \'sum\'), average_sales=(\'amount\', \'mean\'), transaction_count=(\'transaction_id\', \'count\') ).reset_index() # Sort by total_sales in descending order result = grouped_data.sort_values(by=\'total_sales\', ascending=False) return result"},{"question":"# Unicode Text Processing Objective: Write a Python function that processes a list of strings containing Unicode characters from various languages. The function should normalize the strings, handle different encodings, and perform comparisons to identify duplicates based on their normalized form. Function Specification: - **Function Name:** `process_unicode_strings` - **Input:** A list of strings, `str_list`, where each string may contain Unicode characters. - **Output:** A dictionary with normalized unique strings as keys and their counts as values. Requirements: 1. **Normalization:** Ensure all strings are normalized using the \'NFC\' (Normalization Form C). 2. **Encoding Handling:** Handle any input string encoding issues by decoding them properly if provided as bytes. Assume the provided encoding is \'utf-8\'. 3. **Duplicate Count:** Compare strings in a case-insensitive manner using `casefold()` method and normalized forms to count duplicates. 4. **Performance Consideration:** The function should handle at least 100,000 strings efficiently. Example: ```python def process_unicode_strings(str_list): # Your implementation here # Example usage: input_strings = [ \'Café\', \'u0043u0061u0066u00e9\', # Café in Unicode code points \'Gürzenichstraße\', \'gürzenichstrasse\', # Case variant \'Straße\', \'Strasse\', # Different representation ] result = process_unicode_strings(input_strings) print(result) # Expected output: {\'Café\': 2, \'Gürzenichstraße\': 2, \'Straße\': 2} ``` # Constraints: - Each string in `str_list` will have a length between 1 and 100 characters. - The total number of strings will not exceed 100,000. # Hints: - Use `unicodedata.normalize` for normalization. - Use `casefold()` for case-insensitive comparison. - Pay attention to both byte and string input handling. Your task is to implement the `process_unicode_strings` function and ensure it performs as specified.","solution":"import unicodedata def process_unicode_strings(str_list): Processes a list of strings containing Unicode characters. Normalizes the strings, handles encoding issues, and performs comparisons to identify duplicates based on their normalized form. Args: str_list (list of str): A list of strings containing Unicode characters. Returns: dict: A dictionary with normalized unique strings as keys and their counts as values. normalized_count = {} for item in str_list: # Ensure the item is a string, possibly decode from bytes if isinstance(item, bytes): item = item.decode(\'utf-8\') # Normalize the string normalized_str = unicodedata.normalize(\'NFC\', item) # Casefold for case-insensitive comparison folded_str = normalized_str.casefold() # Update the count in dictionary if folded_str in normalized_count: normalized_count[folded_str][\'count\'] += 1 else: normalized_count[folded_str] = {\'count\': 1, \'original\': normalized_str} # Prepare final result with the original normalized form as keys result = {entry[\'original\']: entry[\'count\'] for entry in normalized_count.values()} return result"},{"question":"# Question: Implementing and Using Custom Context Managers Using the `contextlib` module, create a custom context manager that reads data from a file and ensures proper file closing even if an exception occurs. Your task involves: 1. Implementing the context manager using the `@contextmanager` decorator. 2. Adding functionality to read data line-by-line. 3. Handling the case where the file does not exist gracefully by using the `suppress` context manager. Requirements: - Implement a function `read_file_lines` using the `@contextmanager` decorator that reads lines from a file. If the file does not exist, it should handle the exception and return an empty list. - Your function should ensure that the file is properly closed after reading, even if an exception occurs during reading. - The context manager should handle any internal exceptions and ensure resource cleanup. Function Signature: ```python from contextlib import contextmanager, suppress @contextmanager def read_file_lines(file_path: str): # Your implementation here ``` Example Usage: ```python file_path = \'example.txt\' # Creating a test file with open(file_path, \'w\') as f: f.write(\\"Line 1nLine 2nLine 3n\\") # Using the custom context manager to read lines from the file with read_file_lines(file_path) as lines: for line in lines: print(line) # Using the context manager with a non-existent file non_existent_file = \'non_existent.txt\' with suppress(Exception): with read_file_lines(non_existent_file) as lines: print(\\"This should print an empty list:\\", lines) ``` Constraints: - Assume that the file contains text data. - You must use the `@contextmanager` decorator from the `contextlib` module. - You are not allowed to use the built-in `open` function directly outside the `read_file_lines` function. Expected Output: ``` Line 1 Line 2 Line 3 This should print an empty list: [] ```","solution":"from contextlib import contextmanager, suppress @contextmanager def read_file_lines(file_path: str): lines = [] try: with open(file_path, \'r\') as file: for line in file: lines.append(line.strip()) yield lines except FileNotFoundError: yield [] except Exception as e: yield [] raise e finally: pass # The file is automatically closed by the `with` statement in the try block."},{"question":"# Seaborn Stripplot and Catplot Assessment **Objective:** Write a function `create_stripplot_with_facets` that uses seaborn\'s `stripplot` and `catplot` functionalities to generate a detailed visualization from the given dataset. **Function Signature:** ```python def create_stripplot_with_facets(data, x_var, y_var, hue_var, col_var, jitter=True, dodge=False, palette=\'deep\', native_scale=False): pass ``` **Input:** - `data (DataFrame)`: A Pandas DataFrame containing the dataset. - `x_var (str)`: Column name in the DataFrame to be mapped on the x-axis. - `y_var (str)`: Column name in the DataFrame to be mapped on the y-axis. - `hue_var (str)`: Column name in the DataFrame for color encoding (hue). - `col_var (str)`: Column name in the DataFrame to create facets along columns. - `jitter (bool)`: If `True`, jitter the points (default is `True`). - `dodge (bool)`: If `True`, separate the points by `hue` levels (default is `False`). - `palette (str)`: Name of the seaborn color palette to use (default is `\'deep\'`). - `native_scale (bool)`: If `True`, use the original scale for numeric categorical variables (default is `False`). **Output:** - A seaborn catplot object that contains the stripplots. **Constraints:** - You must use seaborn\'s `stripplot` for the basic strip plot and `catplot` for the faceted plots. - The function should ensure proper synchronization of the categorical and hue variables within each facet. - The function must handle both categorical and numeric `hue` variables correctly, applying jitter, dodge, and palette settings as specified. **Example:** ```python import seaborn as sns import pandas as pd # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Define the function def create_stripplot_with_facets(data, x_var, y_var, hue_var, col_var, jitter=True, dodge=False, palette=\'deep\', native_scale=False): # Create a base strip plot base_plot = sns.stripplot(data=data, x=x_var, y=y_var, hue=hue_var, jitter=jitter, dodge=dodge, palette=palette, native_scale=native_scale) # Create a faceted strip plot using catplot facet_plot = sns.catplot(data=data, x=x_var, y=y_var, hue=hue_var, col=col_var, kind=\\"strip\\", jitter=jitter, dodge=dodge, palette=palette, native_scale=native_scale, aspect=.5) return facet_plot # Generate the plots create_stripplot_with_facets(tips, \\"total_bill\\", \\"day\\", \\"sex\\", \\"time\\", jitter=False, dodge=True) ``` **Note:** - Ensure that the function returns the final faceted plot object. - Customize the strip plot to include any additional matplotlib parameters as necessary. Test the function using the `tips` dataset provided by seaborn to ensure that it correctly generates the desired stripplots with facets.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_stripplot_with_facets(data, x_var, y_var, hue_var, col_var, jitter=True, dodge=False, palette=\'deep\', native_scale=False): Creates a strip plot with facets using seaborn\'s stripplot and catplot. Parameters: - data: A Pandas DataFrame containing the dataset. - x_var: Column name in the DataFrame to be mapped on the x-axis. - y_var: Column name in the DataFrame to be mapped on the y-axis. - hue_var: Column name in the DataFrame for color encoding (hue). - col_var: Column name in the DataFrame to create facets along columns. - jitter: If True, jitter the points (default is True). - dodge: If True, separate the points by hue levels (default is False). - palette: Name of the seaborn color palette to use (default is \'deep\'). - native_scale: If True, use the original scale for numeric categorical variables (default is False). Returns: A seaborn catplot object that contains the stripplots. # Create a faceted strip plot using catplot facet_plot = sns.catplot(data=data, x=x_var, y=y_var, hue=hue_var, col=col_var, kind=\\"strip\\", jitter=jitter, dodge=dodge, palette=palette, native_scale=native_scale, aspect=.7) plt.show() return facet_plot"},{"question":"# Asyncio Advanced Application You are tasked with implementing a small application that demonstrates your understanding of the asyncio package, including proper handling of asynchronous tasks, debugging practices, and managing concurrency. **Requirements:** 1. **Data Fetching Coroutine:** Implement an asynchronous function `fetch_data(id: int) -> int` that simulates fetching data with a delay. This function should: - Use `asyncio.sleep` to simulate network delay with `id` seconds. - Return the square of the id. - Raise a `RuntimeError(\\"Simulated fetch error\\")` if `id` is zero. 2. **Data Processing Coroutine:** Implement another asynchronous function `process_data(n: int) -> List[int]` that: - Concurrently fetches data for `id` from 1 to `n` (use `asyncio.gather` or similar). - Logs any exceptions as warnings without stopping the entire process. - If any coroutine raises an exception, store `-1` in the respective position of the result list. - Uses proper debugging practices (like setting `asyncio` debug mode). 3. **Main Function:** Implement a `main` function: - Accepts an integer input `n`. - Calls `process_data(n)` and prints the resulting list. - Ensures that all unawaited coroutines and exceptions are properly handled. 4. **Concurrency and Executor Handling:** Extend `process_data(n)` to additionally simulate CPU-bound tasks using `concurrent.futures.ThreadPoolExecutor`. For each fetch data result (if not -1): - Use an executor to compute the cube of the result concurrently to avoid blocking the event loop. - Include these results in the final list along with the fetch results. **Input:** ```python main(5) # Example call to your main function ``` **Output:** ``` # List that includes both fetch and cube results, where fetch errors are logged and result in `-1` [1, 8, 27, 64, 125, 1, 512, -1, 4096, 15625] ``` **Constraints:** - Use try-except blocks to log errors as warnings using `logging` module. - Ensure all async tasks are awaited. - Enable debug mode for asyncio to check for any potential issues. - Handle both IO-bound operations (using await/async) and CPU-bound operations (using ThreadPoolExecutor) efficiently without blocking the event loop. Implement the functions in a single Python script. ```python import asyncio import concurrent.futures import logging from typing import List logging.basicConfig(level=logging.WARNING) async def fetch_data(id: int) -> int: # Your implementation here pass async def process_data(n: int) -> List[int]: # Your implementation here pass def main(n: int): # Your implementation here pass if __name__ == \'__main__\': n = int(input(\\"Enter value for n: \\")) main(n) ``` # Performance Requirements: - Your implementation should be able to handle inputs up to `n = 100` efficiently. - Properly log and handle errors without terminating the entire process. - Utilization of asyncio and concurrent features effectively demonstrating knowledge of threading and async capabilities.","solution":"import asyncio import concurrent.futures import logging from typing import List # Set up logging configuration. logging.basicConfig(level=logging.WARNING) # Debug mode for asyncio. asyncio.get_event_loop().set_debug(True) async def fetch_data(id: int) -> int: if id == 0: raise RuntimeError(\\"Simulated fetch error\\") await asyncio.sleep(id) return id * id async def process_data(n: int) -> List[int]: results = [] with concurrent.futures.ThreadPoolExecutor() as executor: loop = asyncio.get_event_loop() tasks = [ fetch_data_with_handling(loop, executor, id) for id in range(1, n + 1) ] results = await asyncio.gather(*tasks) return results async def fetch_data_with_handling(loop, executor, id): try: result = await fetch_data(id) # Compute the cube of the result concurrently. cube = await loop.run_in_executor(executor, lambda x: x ** 3, result) return (result, cube) except Exception as e: logging.warning(f\\"Error fetching data for id {id}: {str(e)}\\") return (-1, -1) def main(n: int): if n <= 0: raise ValueError(\\"n must be a positive integer\\") loop = asyncio.get_event_loop() results = loop.run_until_complete(process_data(n)) flattened_results = [r for pair in results for r in pair] # Flatten the list of tuples print(flattened_results) if __name__ == \'__main__\': n = int(input(\\"Enter value for n: \\")) main(n)"},{"question":"# Advanced URL Handling with urllib.request In this assessment, you will implement a URL fetcher using Python\'s `urllib.request` module. The URL fetcher should demonstrate your understanding of both basic and advanced functionality provided by this module, including SSL contexts, proxy handling, and error handling. Objective Implement the function `fetch_url(url: str, data: Optional[bytes] = None, is_post: bool = False, use_proxy: bool = False) -> Tuple[int, str]` that retrieves content from a given URL. The function should: 1. Open the URL to retrieve the content. 2. Handle both GET and POST requests. 3. Optionally use a proxy server for the request. 4. Use a secure SSL context. 5. Handle HTTP errors gracefully, returning the status code and an appropriate error message when an error occurs. Function Signature ```python from typing import Optional, Tuple def fetch_url(url: str, data: Optional[bytes] = None, is_post: bool = False, use_proxy: bool = False) -> Tuple[int, str]: ``` Parameters - `url` (str): The URL to fetch content from. - `data` (Optional[bytes]): The optional data to send with the request (used for POST requests). - `is_post` (bool): If `True`, the request should be a POST request. If `False`, the request should be a GET request. Default is `False`. - `use_proxy` (bool): If `True`, the function should use a proxy server. Default is `False`. Returns - A tuple `(status_code: int, content: str)`: - `status_code` (int): The HTTP status code of the response. - `content` (str): The content returned by the server as a string. If an error occurs, return the error message. Constraints - You must demonstrate proper use of the classes and functions from the `urllib.request` module. - Handle SSL securely by using an SSL context. - You should use appropriate handlers for proxy and error handling. - Your implementation must handle redirections, if any. - Make sure your implementation can gracefully handle and report errors. Sample Usage ```python # Example usage of your function status_code, content = fetch_url(\'https://www.example.com\') print(f\'Status Code: {status_code}\') print(f\'Content: {content[:100]}\') # Print first 100 characters of content status_code, content = fetch_url(\'https://www.example.com\', b\'param1=value1&param2=value2\', is_post=True) print(f\'Status Code: {status_code}\') print(f\'Content: {content[:100]}\') # Print first 100 characters of content status_code, content = fetch_url(\'https://www.example.com\', use_proxy=True) print(f\'Status Code: {status_code}\') print(f\'Content: {content[:100]}\') # Print first 100 characters of content ``` **Note:** Replace `https://www.example.com` with any valid URL for testing your implementation. You should implement the function `fetch_url` in a robust manner, making use of the `urllib.request` functionalities described in the documentation.","solution":"import urllib.request import ssl from typing import Optional, Tuple def fetch_url(url: str, data: Optional[bytes] = None, is_post: bool = False, use_proxy: bool = False) -> Tuple[int, str]: try: # Create SSL context to handle HTTPS requests ssl_context = ssl.create_default_context() # Build request if is_post and data: req = urllib.request.Request(url, data=data, method=\'POST\') else: req = urllib.request.Request(url) # Setup proxy if required if use_proxy: proxy_handler = urllib.request.ProxyHandler({ \'http\': \'http://your_proxy.com:port\', \'https\': \'https://your_proxy.com:port\', }) opener = urllib.request.build_opener(proxy_handler) else: opener = urllib.request.build_opener() # Add SSL context to opener opener.add_handler(urllib.request.HTTPSHandler(context=ssl_context)) # Open the URL with opener.open(req) as response: status_code = response.getcode() content = response.read().decode(\'utf-8\') return status_code, content except urllib.error.HTTPError as e: return e.code, str(e) except urllib.error.URLError as e: return 0, str(e) except Exception as e: return 0, str(e)"},{"question":"# Question: Color Palettes in Seaborn You are tasked with generating custom color palettes using the seaborn library and applying these palettes to visualize data. The objective is to write a function `plot_with_custom_palette` that creates specific color palettes and uses them in seaborn plots. Your function should do the following: 1. Create a sequential light color palette starting from a light gray to a specified hex color. 2. Create another light color palette using the HUSL system. 3. Combine both palettes, increasing the total number of combined colors to 15. 4. Generate a sample dataset with 15 data points. 5. Plot the dataset using a scatter plot, applying the combined color palette. Input: - A hex color string for the first palette (e.g., `\\"#79C\\"`). - A tuple of three integers representing the HUSL color (e.g., `(20, 60, 50)`). Output: - A seaborn scatter plot displaying the dataset with the applied combined color palette. Constraints: - Use only the seaborn and matplotlib libraries. - Ensure that the combined color palette contains exactly 15 colors. Example: Let\'s assume the input hex color string is `\\"#79C\\"` and the HUSL color tuple is `(20, 60, 50)`. ```python def plot_with_custom_palette(hex_color: str, husl_color: tuple): import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Step 1: Create the first palette palette1 = sns.light_palette(hex_color, n_colors=7) # Step 2: Create the second palette palette2 = sns.light_palette(husl_color, input=\\"husl\\", n_colors=8) # Step 3: Combine the palettes combined_palette = palette1 + palette2 # Step 4: Generate a sample dataset data = np.random.rand(15, 2) # Step 5: Create a scatter plot sns.scatterplot(x=data[:,0], y=data[:,1], palette=combined_palette) # Display the plot plt.show() # Example usage plot_with_custom_palette(\\"#79C\\", (20, 60, 50)) ``` This function generates two light color palettes, combines them, and applies the combined palette to a scatter plot.","solution":"def plot_with_custom_palette(hex_color: str, husl_color: tuple): import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Step 1: Create the first palette palette1 = sns.light_palette(hex_color, n_colors=7) # Step 2: Create the second palette palette2 = sns.light_palette(husl_color, input=\\"husl\\", n_colors=8) # Step 3: Combine the palettes combined_palette = palette1 + palette2 # Step 4: Generate a sample dataset data = np.random.rand(15, 2) # Step 5: Create a scatter plot sns.scatterplot(x=data[:,0], y=data[:,1], hue=np.arange(15), palette=combined_palette, legend=False) # Display the plot plt.show()"},{"question":"**Coding Assessment Question:** # Objective Write a Python script that demonstrates your understanding of the `unittest` framework by implementing a test suite for a simple text-processing class, `TextProcessor`. # Problem Statement You are provided with a class `TextProcessor` that performs basic text processing operations such as converting text to uppercase, checking if a string is a palindrome, and splitting text by spaces. Implement the class and write unit tests that thoroughly test each method\'s functionality using the `unittest` framework. # Class to Implement ```python class TextProcessor: def __init__(self, text: str): self.text = text def to_upper(self) -> str: return self.text.upper() def is_palindrome(self) -> bool: processed_text = \'\'.join(e for e in self.text if e.isalnum()).lower() return processed_text == processed_text[::-1] def split_text(self) -> list: return self.text.split() ``` # Requirements 1. Implement the `TextProcessor` class as described. 2. Write a test case class `TestTextProcessor` that tests each method of `TextProcessor`. 3. Use `setUp` and `tearDown` methods to manage any setup and cleanup needed for each test. 4. Include tests for edge cases: - Empty string input. - String with special characters for palindrome check. - Single-character strings. - String with leading and trailing spaces for split text. 5. Use `assert*` methods to validate the correctness of the methods in `TextProcessor`. 6. Ensure that your tests handle failure cases gracefully and provide meaningful error messages. # Input and Output - No input is required from the user. - The output will be the result of running the unittests. # Example Usage ```python # Example usage of TextProcessor class processor = TextProcessor(\\"A man a plan a canal Panama\\") print(processor.to_upper()) # Output: \\"A MAN A PLAN A CANAL PANAMA\\" print(processor.is_palindrome()) # Output: True print(processor.split_text()) # Output: [\\"A\\", \\"man\\", \\"a\\", \\"plan\\", \\"a\\", \\"canal\\", \\"Panama\\"] ``` # Example Test Case ```python import unittest from text_processor import TextProcessor # Ensure your class is in a file named `text_processor.py` class TestTextProcessor(unittest.TestCase): def setUp(self): self.processor = TextProcessor(\\"A man a plan a canal Panama\\") def tearDown(self): del self.processor def test_to_upper(self): self.assertEqual(self.processor.to_upper(), \\"A MAN A PLAN A CANAL PANAMA\\") def test_is_palindrome(self): self.assertTrue(self.processor.is_palindrome()) def test_split_text(self): self.assertEqual(self.processor.split_text(), [\\"A\\", \\"man\\", \\"a\\", \\"plan\\", \\"a\\", \\"canal\\", \\"Panama\\"]) def test_empty_string(self): processor = TextProcessor(\\"\\") self.assertEqual(processor.to_upper(), \\"\\") self.assertTrue(processor.is_palindrome()) self.assertEqual(processor.split_text(), []) def test_special_characters_palindrome(self): processor = TextProcessor(\\"@#A man, a plaN, a canal: PaNAMa!!\\") self.assertTrue(processor.is_palindrome()) def test_single_character(self): processor = TextProcessor(\\"x\\") self.assertEqual(processor.to_upper(), \\"X\\") self.assertTrue(processor.is_palindrome()) self.assertEqual(processor.split_text(), [\\"x\\"]) def test_split_with_spaces(self): processor = TextProcessor(\\" Leading and trailing spaces are handled \\") self.assertEqual(processor.split_text(), [\\"Leading\\", \\"and\\", \\"trailing\\", \\"spaces\\", \\"are\\", \\"handled\\"]) if __name__ == \'__main__\': unittest.main() ``` # Constraints - You must use the `unittest` framework in Python. - Ensure that your test cases are comprehensive and cover various edge cases. - Write clean, readable, and well-documented code.","solution":"class TextProcessor: def __init__(self, text: str): self.text = text def to_upper(self) -> str: return self.text.upper() def is_palindrome(self) -> bool: processed_text = \'\'.join(e for e in self.text if e.isalnum()).lower() return processed_text == processed_text[::-1] def split_text(self) -> list: return self.text.split()"},{"question":"# Python Fundamentals Assessment **Objective**: Implement a function that processes data in a variety of ways as specified below. **Task**: Write a Python function called `process_data` that takes in three parameters: a list of integers, a string, and another list of strings. The function should perform the following operations: 1. **Arithmetic Operations on the List of Integers**: - Calculate the sum of all elements in the list. - Find the maximum and minimum values in the list. - Replace every negative number in the list with zero and return the modified list. 2. **String Manipulations**: - Replace all occurrences of the word \\"Python\\" in the string with \\"Programming\\". - Concatenate the modified string with itself in reverse and return the result. 3. **List Operations**: - Flatten the list of strings into a single string, where each word in the string list is separated by a space. - Calculate the length of the longest string in the list of strings and return the length. The function should return a dictionary with the following keys and their corresponding results: - `sum`: the sum of the initial list of integers. - `max`: the maximum value in the initial list of integers. - `min`: the minimum value in the initial list of integers. - `non_negative_list`: the modified list of integers with negative numbers replaced by zero. - `modified_string`: the concatenated string after replacements and reversing. - `flattened_string`: the single string formed by flattening the list of strings. - `longest_string_length`: the length of the longest string in the list of strings. **Input** - `integers`: A list of integers (e.g., `[-1, 2, 3, -4, 5]`) - `input_string`: A string containing words (e.g., `\\"I love Python\\"`) - `string_list`: A list of string elements (e.g., `[\\"Hello\\", \\"world\\", \\"Python\\", \\"is\\", \\"awesome\\"]`) **Output** - A dictionary with keys and their corresponding processed values. **Constraints** - The list of integers will not be empty. - The string will have at least one occurrence of the word \\"Python\\". - The list of strings will have at least one element. **Example Usage**: ```python def process_data(integers, input_string, string_list): # Function implementation here # Example Input integers = [-1, 2, 3, -4, 5] input_string = \\"I love Python\\" string_list = [\\"Hello\\", \\"world\\", \\"Python\\", \\"is\\", \\"awesome\\"] # Expected Output { \'sum\': 5, \'max\': 5, \'min\': -4, \'non_negative_list\': [0, 2, 3, 0, 5], \'modified_string\': \\"I love ProgrammingProgramming evol I\\", \'flattened_string\': \\"Hello world Python is awesome\\", \'longest_string_length\': 7 } ``` Implement the `process_data` function to fulfill the requirements mentioned above.","solution":"def process_data(integers, input_string, string_list): Processes data according to the specified operations. # Arithmetic Operations on the List of Integers integer_sum = sum(integers) max_value = max(integers) min_value = min(integers) non_negative_list = [max(0, x) for x in integers] # String Manipulations modified_string = input_string.replace(\\"Python\\", \\"Programming\\") modified_string = modified_string + modified_string[::-1] # List Operations flattened_string = \' \'.join(string_list) longest_string_length = max(len(word) for word in string_list) return { \'sum\': integer_sum, \'max\': max_value, \'min\': min_value, \'non_negative_list\': non_negative_list, \'modified_string\': modified_string, \'flattened_string\': flattened_string, \'longest_string_length\': longest_string_length }"},{"question":"# Shared Memory Data Processing In this exercise, you\'ll demonstrate your understanding of the `multiprocessing.shared_memory` module by implementing a data processing task across multiple processes using shared memory. Task You are to implement a function `process_shared_data` that distributes a list of integers across two processes, each of which computes the sum of their assigned subset. Both processes will share access to the entire list of integers using shared memory. Once both processes have completed their computation, the main process should combine these sums to produce the final result. Function Signature ```python def process_shared_data(data: list[int]) -> int: pass ``` Input - `data`: A list of integers to be processed. Constraint: `len(data) % 2 == 0` (This ensures that the list length is even and can be evenly divided between two processes). Output - Returns the integer sum of all integers in the `data`. Constraints and Requirements 1. You must use the `multiprocessing.shared_memory` module to share the data between the processes. 2. Handle the lifecycle of the shared memory properly, ensuring clean-up is done after usage. 3. Implement proper synchronization if necessary to ensure correct access to shared memory (though access should be inherently safe as each process will operate on distinct parts of the list). Example ```python data = [1, 2, 3, 4] result = process_shared_data(data) print(result) # Output should be 10 ``` Notes - The correct approach should leverage the shared memory to avoid the overhead of passing large amounts of data between processes. - Efficient memory management is crucial, so ensure shared resources are properly closed and unlinked. Good luck, and happy coding!","solution":"from multiprocessing import Process from multiprocessing.shared_memory import SharedMemory import numpy as np def process_shared_data(data): # Verify the length of data is even assert len(data) % 2 == 0, \\"Length of data must be even\\" # Create shared memory and numpy array from it shm = SharedMemory(create=True, size=len(data) * np.int32().itemsize) shared_array = np.ndarray(len(data), dtype=np.int32, buffer=shm.buf) # Copy data to shared memory shared_array[:] = data # Function to calculate sum of a part of the shared array def partial_sum(start, end, result, index): result[index] = np.sum(shared_array[start:end]) # Array to hold the partial results result = np.zeros(2, dtype=np.int32) result_shm = SharedMemory(create=True, size=result.nbytes) result_array = np.ndarray(result.shape, dtype=result.dtype, buffer=result_shm.buf) # Create two processes to process each half mid = len(data) // 2 p1 = Process(target=partial_sum, args=(0, mid, result_array, 0)) p2 = Process(target=partial_sum, args=(mid, len(data), result_array, 1)) # Start both processes p1.start() p2.start() # Wait for both processes to finish p1.join() p2.join() # Calculate final result final_result = np.sum(result_array) # Clean up shared memory shm.close() shm.unlink() result_shm.close() result_shm.unlink() return final_result"},{"question":"# Seaborn Advanced Plotting Challenge You are provided with a dataset about diamonds (`diamonds`), which can be loaded using the seaborn `load_dataset` function. Using this dataset, write code to create a customized boxen plot that fulfills the following criteria: 1. Load the `diamonds` dataset using `seaborn.load_dataset`. 2. Create a boxen plot visualizing the distribution of diamond prices (`price`), grouped by diamond `cut`, and further divided by whether the diamond is classified as \\"expensive\\" or not (use a threshold: if the price of the diamond is above the median price, classify it as \\"expensive\\"). 3. Adjust the plot to utilize the `linear` method for determining the boxes\' widths. 4. Customize the plot to have: - A gap of 0.1 between the boxes that are distinguished by the `hue`. - Thin box outlines with a `linecolor` set to \\".5\\". - Distinct, thicker median lines (use `line_kws` to set `linewidth=2` and `color=\'blue\'`). 5. Annotate the plot with the title \\"Boxen Plot of Diamond Prices by Cut and Expensiveness\\". # Input and Output Formats: - **Input**: No input required. You can directly use the `seaborn.load_dataset(\\"diamonds\\")` to load the dataset. - **Output**: Seaborn boxen plot as described above. # Constraints: - The visualization customization using different seaborn parameters is mandatory. - The median classification should be correctly implemented, ensuring each diamond is classified based on the correct median price. # Example: ```python import seaborn as sns # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Your code to create the customized boxen plot sns.set_theme(style=\\"whitegrid\\") # Classify diamonds based on the median price diamonds[\'expensive\'] = diamonds[\'price\'] > diamonds[\'price\'].median() # Create the boxen plot with the specified customizations sns.boxenplot( data=diamonds, x=\\"price\\", y=\\"cut\\", hue=\\"expensive\\", width_method=\\"linear\\", gap=0.1, linecolor=\\".5\\", line_kws=dict(linewidth=2, color=\\"blue\\") ).set(title=\\"Boxen Plot of Diamond Prices by Cut and Expensiveness\\") ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_custom_boxen(): # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Classify diamonds based on the median price diamonds[\'expensive\'] = diamonds[\'price\'] > diamonds[\'price\'].median() # Set the theme for the plot sns.set_theme(style=\\"whitegrid\\") # Create the boxen plot with the specified customizations ax = sns.boxenplot( data=diamonds, x=\\"cut\\", y=\\"price\\", hue=\\"expensive\\", width_method=\\"linear\\", gap=0.1, linecolor=\\".5\\", line_kws=dict(linewidth=2, color=\\"blue\\") ) # Set the title of the plot ax.set_title(\\"Boxen Plot of Diamond Prices by Cut and Expensiveness\\") # Show the plot plt.show()"},{"question":"# Question: Implement a Multithreaded Chat Server Objective: You are required to implement a multithreaded TCP server using the `socketserver` module. The server will support multiple clients connecting concurrently and exchanging messages in a chatroom-style communication. Requirements: 1. Implement a `ChatHandler` class derived from `socketserver.BaseRequestHandler`. This handler should handle the following: - Broadcasting messages received from a client to all connected clients. - An orderly connection, message handling, and disconnection of clients. 2. Implement a `ChatServer` class derived from `socketserver.ThreadingMixIn` and `socketserver.TCPServer`. 3. Maintain a global list of active clients (sockets) in the server for broadcasting messages. 4. Ensure that the server can handle clients joining, sending messages, and leaving without crashing. 5. Use the server in a `with` statement to ensure proper cleanup. Implementation Steps: 1. **Define the `ChatHandler` class**: - Override the `handle` method to receive messages from the client and broadcast them to all clients. 2. **Define the `ChatServer` class**. 3. **Set up the server**: - Run the server using `serve_forever`. Additional Information: - The server should run on `localhost` and port `9999`. - Each client should be able to send a message by typing and pressing Enter. - The server should append the client\'s address to each message before broadcasting. Example: 1. Client A (from address `127.0.0.1:5000`) sends: \\"Hello, World!\\" 2. Client B (from address `127.0.0.1:5001`) receives: \\"127.0.0.1:5000 says: Hello, World!\\" Constraints: 1. Use appropriate synchronization mechanisms to handle the shared client list. # Solution Template ```python import socketserver import threading # Global list to track connected clients clients = [] class ChatHandler(socketserver.BaseRequestHandler): def handle(self): global clients clients.append(self.request) try: while True: msg = self.request.recv(1024) if not msg: break msg_with_address = \\"{} says: {}\\".format(self.client_address, msg.decode(\'utf-8\')) self.broadcast(msg_with_address) finally: clients.remove(self.request) self.request.close() def broadcast(self, msg): global clients for client in clients: if client != self.request: try: client.sendall(msg.encode(\'utf-8\')) except Exception as e: clients.remove(client) class ChatServer(socketserver.ThreadingMixIn, socketserver.TCPServer): allow_reuse_address = True if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 with ChatServer((HOST, PORT), ChatHandler) as server: server.serve_forever() ``` Notes: - Ensure to handle edge cases such as network errors and client disconnections gracefully. - Implement necessary safeguards related to thread safety while managing the global client list.","solution":"import socketserver import threading # Global list to track connected clients clients = [] class ChatHandler(socketserver.BaseRequestHandler): def handle(self): global clients clients.append(self.request) try: while True: msg = self.request.recv(1024) if not msg: break msg_with_address = \\"{} says: {}\\".format(self.client_address, msg.decode(\'utf-8\')) self.broadcast(msg_with_address) finally: clients.remove(self.request) self.request.close() def broadcast(self, msg): global clients for client in clients: if client != self.request: try: client.sendall(msg.encode(\'utf-8\')) except Exception as e: clients.remove(client) class ChatServer(socketserver.ThreadingMixIn, socketserver.TCPServer): allow_reuse_address = True if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 with ChatServer((HOST, PORT), ChatHandler) as server: server.serve_forever()"},{"question":"Objective: To demonstrate your understanding of the `pandas` `groupby` functionality, you are required to implement a function that performs several grouped operations and combines the results into a final DataFrame. Problem Statement: You are given a dataset containing information about various products sold in multiple stores. Each row in the dataset contains the following columns: - `Store` (string): The store where the product is sold. - `Product` (string): The product name. - `Date` (datetime): The date of the sale. - `UnitsSold` (float): The number of units sold. - `Revenue` (float): The total revenue from the sale of the product. Your task is to implement a function `analyze_sales(df: pd.DataFrame) -> pd.DataFrame` that: 1. Groups the data by `Store` and `Product` and calculates the following for each group: - Total units sold (`TotalUnitsSold`). - Total revenue (`TotalRevenue`). - Average revenue per unit sold (`AvgRevenuePerUnit`). 2. Transforms the original DataFrame by adding two new columns: - `CumulativeUnitsSold` which represents the cumulative number of units sold for each `Product` in each `Store` up to the given date. - `CumulativeRevenue` which represents the cumulative revenue for each `Product` in each `Store` up to the given date. 3. Combines the grouped summary statistics and the transformed DataFrame into a final DataFrame. The final DataFrame should contain the following columns: - `Store` - `Product` - `Date` - `UnitsSold` - `Revenue` - `TotalUnitsSold` - `TotalRevenue` - `AvgRevenuePerUnit` - `CumulativeUnitsSold` - `CumulativeRevenue` Constraints: - The input DataFrame will have at least one row. - The values in the `UnitsSold` and `Revenue` columns will be non-negative. - The `Date` column will be in datetime format. Example: ```python import pandas as pd data = { \\"Store\\": [\\"A\\", \\"A\\", \\"A\\", \\"B\\", \\"B\\", \\"B\\"], \\"Product\\": [\\"X\\", \\"X\\", \\"Y\\", \\"X\\", \\"Y\\", \\"Y\\"], \\"Date\\": pd.to_datetime([\\"2023-01-01\\", \\"2023-01-02\\", \\"2023-01-01\\", \\"2023-01-01\\", \\"2023-01-01\\", \\"2023-01-02\\"]), \\"UnitsSold\\": [10, 20, 15, 5, 10, 20], \\"Revenue\\": [100, 200, 150, 50, 100, 200], } df = pd.DataFrame(data) result = analyze_sales(df) print(result) ``` Expected output: ``` Store Product Date UnitsSold Revenue TotalUnitsSold TotalRevenue AvgRevenuePerUnit CumulativeUnitsSold CumulativeRevenue 0 A X 2023-01-01 10 100 30 300 10.0 10 100 1 A X 2023-01-02 20 200 30 300 10.0 30 300 2 A Y 2023-01-01 15 150 15 150 10.0 15 150 3 B X 2023-01-01 5 50 5 50 10.0 5 50 4 B Y 2023-01-01 10 100 30 300 10.0 10 100 5 B Y 2023-01-02 20 200 30 300 10.0 30 300 ``` Notes: - Ensure the resulting DataFrame is sorted by `Store`, `Product`, and `Date` in ascending order. - Keep the original DataFrame unchanged and create new columns as needed.","solution":"import pandas as pd def analyze_sales(df: pd.DataFrame) -> pd.DataFrame: # Group by \'Store\' and \'Product\' to calculate the summary statistics grouped = df.groupby([\'Store\', \'Product\']).agg( TotalUnitsSold=(\'UnitsSold\', \'sum\'), TotalRevenue=(\'Revenue\', \'sum\') ).reset_index() # Calculate the average revenue per unit sold grouped[\'AvgRevenuePerUnit\'] = grouped[\'TotalRevenue\'] / grouped[\'TotalUnitsSold\'] # Merge the summary statistics back with the original DataFrame df = df.merge(grouped, on=[\'Store\', \'Product\']) # Sort the DataFrame by \'Store\', \'Product\' and \'Date\' to calculate cumulative sums df = df.sort_values(by=[\'Store\', \'Product\', \'Date\']) # Calculate cumulative units sold and cumulative revenue df[\'CumulativeUnitsSold\'] = df.groupby([\'Store\', \'Product\'])[\'UnitsSold\'].cumsum() df[\'CumulativeRevenue\'] = df.groupby([\'Store\', \'Product\'])[\'Revenue\'].cumsum() return df # Sample data for testing the function data = { \\"Store\\": [\\"A\\", \\"A\\", \\"A\\", \\"B\\", \\"B\\", \\"B\\"], \\"Product\\": [\\"X\\", \\"X\\", \\"Y\\", \\"X\\", \\"Y\\", \\"Y\\"], \\"Date\\": pd.to_datetime([\\"2023-01-01\\", \\"2023-01-02\\", \\"2023-01-01\\", \\"2023-01-01\\", \\"2023-01-01\\", \\"2023-01-02\\"]), \\"UnitsSold\\": [10, 20, 15, 5, 10, 20], \\"Revenue\\": [100, 200, 150, 50, 100, 200], } df = pd.DataFrame(data) result = analyze_sales(df) print(result)"},{"question":"Coding Assessment Question **Question: Pandas DataFrame Memory Analysis and Advanced Operations** In this exercise, you will be working with a pandas DataFrame to perform memory usage analysis, handle missing values, and execute functions using User Defined Functions (UDFs) without causing unintended mutations. Additionally, you will correctly interpret truth statements using pandas objects. # Task 1: DataFrame Memory Analysis 1. Create a DataFrame `df` with the following columns and populate it with 5000 random values: - Integer column: values ranging from 0 to 100. - Float column: values ranging from 0 to 1. - Datetime column: random dates within the current year. - Timedelta column: random timedeltas up to 12 hours. - Complex column: complex numbers with real and imaginary parts as random integers from 0 to 100. - Object column: random strings selected from a predefined list. - Boolean column: random boolean values. **Constraints**: - Use relevant numpy functions to create the random data. - For the object column, use the strings `[\'cat\', \'dog\', \'mouse\', \'elephant\']`. 2. Analyze the memory usage of the DataFrame and print the memory usage report with accurate memory accounting of object columns. # Task 2: Handling Missing Values and Type Promotion 1. Introduce missing values (NA) randomly into the integer and boolean columns. 2. Ensure that the integer column promotes to `float64` and the boolean column promotes to `object` to support NA values. # Task 3: Using Truth Statements with pandas 1. Use the `any` method to check if there are any `True` values in the boolean column and print an appropriate message based on the result. # Task 4: User Defined Function (UDF) Mutation 1. Implement a User Defined Function (UDF) to increase all non-missing integer values by 10 in a DataFrame without causing any unintended mutations. 2. Apply this function to `df` and print the modified DataFrame. # Example Output: ```python # Example of Task 1 Output: <output similar to df.info(memory_usage=\'deep\')> # Example of Task 3 Output: \\"I am any\\" # if there are any True values in the boolean column # Example of Task 4 Output: # DataFrame with integer values increased by 10 where non-missing ``` # Expected Input and Output: - **Input:** None (Data should be generated within the script). - **Output:** - Memory usage details of the DataFrame. - Confirmation message if the boolean column has any `True` values. - DataFrame after applying the UDF function to increase integer values. # Constraints: - Use appropriate pandas and numpy methods to create and manipulate the DataFrame. - Ensure your solution handles DataFrames efficiently to avoid excessive memory usage. - Avoid mutating DataFrames directly when using UDFs. **Performance Requirements:** Ensure that your solution efficiently handles the given DataFrame size and operations without significant performance degradation.","solution":"import pandas as pd import numpy as np # Task 1: DataFrame Memory Analysis def create_and_analyze_df(): np.random.seed(0) # For reproducibility n = 5000 date_range = pd.date_range(start=\'2023-01-01\', periods=365, freq=\'D\') timedelta_range = pd.to_timedelta(np.random.randint(0, 43200, size=n), unit=\'s\') object_choices = [\'cat\', \'dog\', \'mouse\', \'elephant\'] data = { \'IntegerColumn\': np.random.randint(0, 101, size=n), \'FloatColumn\': np.random.random(size=n), \'DatetimeColumn\': np.random.choice(date_range, size=n), \'TimedeltaColumn\': timedelta_range, \'ComplexColumn\': np.random.randint(0, 101, size=n) + 1j * np.random.randint(0, 101, size=n), \'ObjectColumn\': np.random.choice(object_choices, size=n), \'BooleanColumn\': np.random.choice([True, False], size=n) } df = pd.DataFrame(data) memory_usage = df.info(memory_usage=\'deep\') print(memory_usage) return df # Task 2: Handling Missing Values and Type Promotion def introduce_nans(df): df.loc[np.random.choice(df.index, size=500, replace=False), \'IntegerColumn\'] = np.nan df.loc[np.random.choice(df.index, size=500, replace=False), \'BooleanColumn\'] = np.nan df[\'IntegerColumn\'] = df[\'IntegerColumn\'].astype(\'float64\') df[\'BooleanColumn\'] = df[\'BooleanColumn\'].astype(\'object\') # Task 3: Using Truth Statements with pandas def check_boolean_any(df): if df[\'BooleanColumn\'].any(): print(\\"I am any\\") else: print(\\"I am not any\\") # Task 4: User Defined Function (UDF) Mutation def increase_int_values(df): def increase_int_values_by_10(x): if pd.isna(x): return x return x + 10 df[\'IntegerColumn\'] = df[\'IntegerColumn\'].apply(increase_int_values_by_10) print(df) # Main execution df = create_and_analyze_df() introduce_nans(df) check_boolean_any(df) increase_int_values(df)"},{"question":"Coding Assessment Question # Objective Create a scatter plot using seaborn\'s objects interface and apply jitter to visualize the distribution of data points clearly. # Task Write a function named `plot_with_jitter` that: 1. Loads the \'penguins\' dataset using seaborn\'s `load_dataset` function. 2. Creates a scatter plot of \'flipper_length_mm\' (x-axis) vs \'body_mass_g\' (y-axis) with jitter applied to both axes. 3. Uses seaborn\'s `so.Plot` and `so.Jitter` functions to achieve this. 4. Allows customization of the jitter amount for both axes through function parameters. # Function Signature ```python def plot_with_jitter(x_jitter: float, y_jitter: float) -> None: ``` # Input - `x_jitter`: A float value representing the jitter amount to be applied along the x-axis in data units. - `y_jitter`: A float value representing the jitter amount to be applied along the y-axis in data units. # Output - The function should not return anything. It should directly display the plot. # Example ```python plot_with_jitter(200, 5) ``` # Constraints - Ensure that the jitter values are non-negative floats. - If either `x_jitter` or `y_jitter` is set to 0, no jitter should be applied to the respective axis. # Notes - You don\'t need to handle loading the seaborn library or the dataset within the evaluation environment; assume those are pre-loaded. - Focus on correct implementation of the `so.Plot` and `so.Jitter` functionalities. # Performance Requirements - The function should handle the entire \'penguins\' dataset efficiently. ```python import seaborn.objects as so from seaborn import load_dataset def plot_with_jitter(x_jitter: float, y_jitter: float) -> None: # Load the \'penguins\' dataset penguins = load_dataset(\\"penguins\\") # Create the plot with jitter ( so.Plot(penguins, y=\\"body_mass_g\\", x=\\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(x=x_jitter, y=y_jitter)) ).show() ``` # Important Ensure you refer to the seaborn documentation and understand the usage of `so.Plot` and `so.Jitter` for proper implementation.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_with_jitter(x_jitter: float, y_jitter: float) -> None: Create a scatter plot of \'flipper_length_mm\' vs \'body_mass_g\' with jitter applied. Parameters: x_jitter (float): The jitter amount for the x-axis in data units. y_jitter (float): The jitter amount for the y-axis in data units. Returns: None # Load the \'penguins\' dataset penguins = load_dataset(\\"penguins\\") # Create the plot with jitter plot = ( so.Plot(penguins, y=\\"body_mass_g\\", x=\\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(x=x_jitter, y=y_jitter)) ) # Show the plot plot.show()"},{"question":"Fake Tensor Subclass Implementation **Objective:** You need to demonstrate your understanding of tensor subclassing, fake tensor creation, and their usage within the PyTorch framework by implementing a function that simulates tensor operations without performing actual compute operations. # Task Description: Implement a class `MyFakeTensorMode` that mimics `FakeTensorMode` to handle fake tensors. Your implementation should allow for creation, conversion, and tensor operations as fake tensors, retaining the correct metadata without performing actual computations. # Class Specification Class: `MyFakeTensorMode` **Methods:** 1. `__init__(self)`: Initialize any required attributes or states. 2. `from_real_tensor(self, real_tensor: torch.Tensor) -> torch.Tensor`: - Converts a given real tensor into a fake tensor. 3. `__enter__(self) -> \'MyFakeTensorMode\'`: - Enter the context of fake tensor mode. 4. `__exit__(self, exc_type, exc_value, traceback)`: - Exit the context of fake tensor mode. 5. `__torch_dispatch__(self, real_tensor: torch.Tensor, op, *args, **kwargs) -> torch.Tensor`: - Dispatch method to handle tensor operations within the fake tensor mode. **Example Usage:** ```python import torch from torch._subclasses.fake_tensor import FakeTensorMode class MyFakeTensorMode: def __init__(self): self.fake_tensor_converter = FakeTensorConverter() def from_real_tensor(self, real_tensor: torch.Tensor) -> torch.Tensor: # Your implementation to convert real_tensor to fake_tensor pass def __enter__(self) -> \'MyFakeTensorMode\': # Enter the context of MyFakeTensorMode pass def __exit__(self, exc_type, exc_value, traceback): # Exit the context of MyFakeTensorMode pass def __torch_dispatch__(self, real_tensor: torch.Tensor, op, *args, **kwargs) -> torch.Tensor: # Handle operations on the fake tensor without actual computation pass # Example usage real_tensor = torch.randn((3, 3)) fake_mode = MyFakeTensorMode() with fake_mode: fake_tensor = fake_mode.from_real_tensor(real_tensor) result = fake_tensor * 2 # This should be handled in your __torch_dispatch__ properly print(fake_tensor) print(result) ``` # Constraints: - The fake tensor must maintain metadata correctly (shape, dtype, etc.) - The class should handle basic tensor operations like addition, multiplication, etc., within the fake tensor mode without performing actual computation. # Input - Real tensor: A valid PyTorch tensor. # Output - Fake tensor: A tensor object that mimics the real one but without actual data. # Evaluation Criteria: - Correctness: Your implementation should correctly simulate the conversion and handling of tensor operations. - Efficiency: The solution should be efficient enough to handle the context management and operations. - Code Quality: The code should be clean, well-documented, and follow best practices.","solution":"import torch class MyFakeTensorMode: def __init__(self): self.in_context = False def from_real_tensor(self, real_tensor: torch.Tensor) -> torch.Tensor: return torch.empty_like(real_tensor) def __enter__(self) -> \'MyFakeTensorMode\': self.in_context = True return self def __exit__(self, exc_type, exc_value, traceback): self.in_context = False def __torch_dispatch__(self, real_tensor: torch.Tensor, op, *args, **kwargs) -> torch.Tensor: if not self.in_context: raise RuntimeError(\\"Must be within the MyFakeTensorMode context\\") # Simulate the metadata\'s correctness by creating an empty tensor with the same properties metadata_correct_tensor = torch.empty_like(real_tensor) return metadata_correct_tensor # Example to demonstrate usage real_tensor = torch.randn((3, 3)) fake_mode = MyFakeTensorMode() with fake_mode: fake_tensor = fake_mode.from_real_tensor(real_tensor) result = fake_mode.__torch_dispatch__(fake_tensor, torch.add, fake_tensor, fake_tensor) print(fake_tensor) print(result)"},{"question":"**Signal Handling Simulation** You are tasked with creating a Python program that demonstrates the use of signal handling. The goal is to simulate a situation where the program needs to handle different signals and perform appropriate actions. Implement the following requirements: 1. **Custom Signal Handlers**: - Create a custom handler for `SIGUSR1` that simply prints `\\"Received SIGUSR1, performing task A\\"`, and another for `SIGUSR2` that prints `\\"Received SIGUSR2, performing task B\\"`. - Create a handler for `SIGINT` that prints `\\"Interrupt received, shutting down gracefully\\"` and sets a global variable `shut_down` to `True` to indicate the program should terminate. 2. **Behavior During Execution**: - The program should run an infinite loop simulating a long-running process. Inside the loop: - Every second, it should print `\\"Running…\\"`. - If `shut_down` is set to `True`, it should break the loop and print `\\"Program terminated\\"` before exiting. 3. **Sending Signals**: - Implement a function `send_signals()` that sends `SIGUSR1` and `SIGUSR2` to the process after 5 and 10 seconds respectively. After 15 seconds, send `SIGINT`. # Constraints: - **Environment**: This will be a Unix-based system where these signals and required functionalities are available. - **Performance**: The program should handle signals without considerable delay in responding. # Input and Output: - There are no external inputs for the main running program. - The output should be a series of print statements demonstrating the reception and correct handling of signals as specified. # Implementation: ```python import signal import time import os # Global variable to control shutdown shut_down = False def handler_usr1(signum, frame): print(\\"Received SIGUSR1, performing task A\\") def handler_usr2(signum, frame): print(\\"Received SIGUSR2, performing task B\\") def handler_sigint(signum, frame): global shut_down print(\\"Interrupt received, shutting down gracefully\\") shut_down = True # Assign signal handlers signal.signal(signal.SIGUSR1, handler_usr1) signal.signal(signal.SIGUSR2, handler_usr2) signal.signal(signal.SIGINT, handler_sigint) # Function to send signals to this process def send_signals(): time.sleep(5) os.kill(os.getpid(), signal.SIGUSR1) time.sleep(5) os.kill(os.getpid(), signal.SIGUSR2) time.sleep(5) os.kill(os.getpid(), signal.SIGINT) # Run the send_signals function in a separate thread or process import threading threading.Thread(target=send_signals).start() # Main execution loop while not shut_down: print(\\"Running...\\") time.sleep(1) print(\\"Program terminated\\") ``` This question tests the student\'s ability to use the `signal` module, handle signals appropriately, manage program state using signals, and demonstrate an understanding of asynchronous event handling in Python.","solution":"import signal import time import os # Global variable to control shutdown shut_down = False def handler_usr1(signum, frame): print(\\"Received SIGUSR1, performing task A\\") def handler_usr2(signum, frame): print(\\"Received SIGUSR2, performing task B\\") def handler_sigint(signum, frame): global shut_down print(\\"Interrupt received, shutting down gracefully\\") shut_down = True # Assign signal handlers signal.signal(signal.SIGUSR1, handler_usr1) signal.signal(signal.SIGUSR2, handler_usr2) signal.signal(signal.SIGINT, handler_sigint) # Function to send signals to this process def send_signals(): time.sleep(5) os.kill(os.getpid(), signal.SIGUSR1) time.sleep(5) os.kill(os.getpid(), signal.SIGUSR2) time.sleep(5) os.kill(os.getpid(), signal.SIGINT) # Run the send_signals function in a separate thread or process import threading threading.Thread(target=send_signals).start() # Main execution loop while not shut_down: print(\\"Running...\\") time.sleep(1) print(\\"Program terminated\\")"},{"question":"# Advanced Python Coding Question: Multi-threaded Task Management You are tasked with implementing a multi-threaded task management system using the `queue` module. In this system, there are producer threads that generate tasks and place them in a queue, and consumer threads that process these tasks. The tasks must be processed in order of their priority. Your task is to implement the following: 1. A class `TaskManager` with the following methods: - `__init__(self, num_workers)`: Initialize the `TaskManager` with a specified number of worker threads (`num_workers`). Use a priority queue (`PriorityQueue`) to store tasks. Each task should be a tuple `(priority, task_id, task_description)`. - `add_task(self, priority, task_id, task_description)`: Add a new task to the queue with the given priority, task ID, and description. - `start_workers(self)`: Start the worker threads, which will continuously take tasks from the queue and process them. - `stop_workers(self)`: Gracefully stop all worker threads after all tasks are completed. - `all_tasks_done(self)`: Block until all tasks have been processed. 2. A function `worker(task_manager)` that will be run by each worker thread invoking `task_manager`. Constraints and Requirements: - You must use the `PriorityQueue` class for the task queue. - The worker threads should process tasks by printing a message: `Processing task {task_id}: {task_description}`. - Ensure thread safety and proper synchronization. - Tasks should be processed in the order of their priority (lower numbers indicate higher priority). Example Usage: ```python if __name__ == \\"__main__\\": manager = TaskManager(num_workers=3) manager.add_task(priority=1, task_id=101, task_description=\\"High priority task\\") manager.add_task(priority=3, task_id=103, task_description=\\"Low priority task\\") manager.add_task(priority=2, task_id=102, task_description=\\"Medium priority task\\") manager.start_workers() manager.all_tasks_done() manager.stop_workers() ``` Expected Output: ``` Processing task 101: High priority task Processing task 102: Medium priority task Processing task 103: Low priority task All tasks are completed. ``` Implement the `TaskManager` class and the `worker` function to fulfill these requirements. Ensure proper handling of thread termination and task completion.","solution":"import threading from queue import PriorityQueue class TaskManager: def __init__(self, num_workers): self.num_workers = num_workers self.task_queue = PriorityQueue() self.workers = [] self.stop_signal = threading.Event() def add_task(self, priority, task_id, task_description): self.task_queue.put((priority, task_id, task_description)) def start_workers(self): for _ in range(self.num_workers): worker_thread = threading.Thread(target=self.worker) worker_thread.start() self.workers.append(worker_thread) def worker(self): while not self.stop_signal.is_set(): try: task = self.task_queue.get(timeout=1) # Timeout to check for stop signal priority, task_id, task_description = task print(f\\"Processing task {task_id}: {task_description}\\") self.task_queue.task_done() except: continue def stop_workers(self): self.stop_signal.set() for work in self.workers: work.join() def all_tasks_done(self): self.task_queue.join() print(\\"All tasks are completed.\\")"},{"question":"**Objective:** Design a robust function that processes a list of numbers and returns their average while handling various types of errors that may occur during the process. The function should manage exceptions effectively and ensure any necessary clean-up actions are performed. **Problem Statement:** Write a function `compute_average(filename: str) -> float` that reads a file containing a list of numbers (one number per line) and calculates their average. The function should handle the following scenarios: 1. If the file does not exist, raise a `FileNotFoundError` with an appropriate message. 2. If a line in the file cannot be converted to a float, raise a `ValueError` with an appropriate message. 3. Ensure the file is always closed after processing, irrespective of success or failure. 4. Provide informative error messages using exception chaining if multiple exceptions occur. **Input:** - `filename` (str): The name of the file containing the list of numbers. **Output:** - Returns the average of the numbers as a float. **Constraints:** - The file contains at least one line of text. - Each line should ideally contain one valid float number. **Example:** ```python # Contents of numbers.txt: # 2.3 # 4.5 # Invalid # 3.1 # Example of usage: try: print(compute_average(\\"numbers.txt\\")) except Exception as e: print(f\\"Error: {e}\\") ``` ```python # Expected output: Error: ValueError: Could not convert the line \'Invalid\' to a float. ``` **Guidelines:** - Use `try`, `except`, and `finally` to handle exceptions and ensure the file is closed. - Raise specific exceptions with informative messages. - Implement exception chaining where appropriate to provide more context when re-raising exceptions. **Hint:** Use the `with` statement to handle the file operations, which ensures the file is closed properly even if an error occurs.","solution":"def compute_average(filename: str) -> float: Computes the average of numbers listed in the given file. Parameters: filename (str): The name of the file containing the list of numbers (one number per line). Returns: float: The calculated average of the numbers. Raises: FileNotFoundError: If the file does not exist. ValueError: If any line in the file cannot be converted to a float. total = 0 count = 0 try: with open(filename, \'r\') as file: for line in file: try: number = float(line.strip()) total += number count += 1 except ValueError as ve: raise ValueError(f\\"Could not convert the line \'{line.strip()}\' to a float.\\") from ve if count == 0: raise ValueError(\\"The file is empty or does not contain any valid numbers.\\") return total / count except FileNotFoundError as fnf: raise FileNotFoundError(f\\"The file \'{filename}\' does not exist.\\") from fnf"},{"question":"# Custom Exception and Error Handling in Python Objective: Design and implement a custom exception in Python and demonstrate robust error handling mechanisms. Your task is to create a function that processes a list of integers and raises custom exceptions when encountering specific error conditions. Task: 1. Implement a custom exception class `ProcessingError` that inherits from Python\'s built-in `Exception` class. This class should accept two parameters during initialization: an error message (`msg`) and the value causing the error (`val`). The exception should override the `__str__` method to provide a meaningful error message that includes both the message and the value. 2. Create another custom exception class `DivisionByZeroError` that inherits from `ProcessingError`. 3. Write a function `process_numbers(numbers: list) -> list` that processes a list of integers and performs the following operations: - If a number is even, divide it by 2. - If a number is odd, square the number. - If the number is zero, raise a `DivisionByZeroError`. - If any other exception occurs, raise a `ProcessingError` with an appropriate message. 4. Implement proper exception handling within the function and ensure it handles and raises exceptions as specified. The function should return a list of processed numbers or raise the appropriate exception. Input: - `numbers` (list): A list of integers. Output: - A list of processed integers if no exception occurs; otherwise, raise the appropriate custom exception. Example: ```python class ProcessingError(Exception): def __init__(self, msg, val): self.msg = msg self.val = val def __str__(self): return f\\"{self.msg}: {self.val}\\" class DivisionByZeroError(ProcessingError): pass def process_numbers(numbers): result = [] for num in numbers: try: if num == 0: raise DivisionByZeroError(\\"Division by zero\\", num) elif num % 2 == 0: result.append(num // 2) else: result.append(num * num) except DivisionByZeroError as e: raise except Exception as e: raise ProcessingError(\\"Error processing number\\", num) return result # Test cases try: print(process_numbers([1, 2, 3, 0, 4])) # Should raise DivisionByZeroError except DivisionByZeroError as e: print(e) try: print(process_numbers([1, 2, 3, 4])) # Should print [1, 1, 9, 2] except ProcessingError as e: print(e) ``` Constraints: - You must use Python 3.10 or higher. - You must handle and raise exceptions as specified. - Ensure the performance of the solution is optimal for typical use cases.","solution":"class ProcessingError(Exception): def __init__(self, msg, val): self.msg = msg self.val = val def __str__(self): return f\\"{self.msg}: {self.val}\\" class DivisionByZeroError(ProcessingError): pass def process_numbers(numbers): result = [] for num in numbers: try: if num == 0: raise DivisionByZeroError(\\"Division by zero\\", num) elif num % 2 == 0: result.append(num // 2) else: result.append(num * num) except DivisionByZeroError as e: raise except Exception as e: raise ProcessingError(\\"Error processing number\\", num) return result"},{"question":"Coding Assessment Question # Objective Implement and evaluate a Stochastic Gradient Descent (SGD) based linear classifier using scikit-learn\'s `SGDClassifier` on a given dataset. # Task 1. Load the provided dataset. 2. Preprocess the data: split into training and test sets and standardize the features. 3. Implement a pipeline combining the standard scaler and SGD classifier. 4. Train the model with a logistic loss function. 5. Evaluate the model on the test set and report the accuracy and the confusion matrix. 6. Fine-tune the hyperparameter `alpha` using `GridSearchCV` to find the optimal regularization parameter. 7. Report the best parameters and corresponding score. # Dataset You will be provided with a CSV file named `data.csv` containing: - Features in columns labeled `feature_1`, `feature_2`, ..., `feature_n` - Target labels in a column named `target` # Input/Output Input - `data.csv`: A comma-separated values (CSV) file. Output - Print the accuracy of the initial model. - Print the confusion matrix of the initial model. - Print the best hyperparameters found by `GridSearchCV`. - Print the accuracy of the model with the best parameters. # Constraints - Use `log_loss` for the loss function in SGD. - Use `StandardScaler` for scaling features. - `alpha` values for grid search should be `[1e-4, 1e-3, 1e-2, 1e-1, 1]`. # Example Workflow Implement the solution in the following order: 1. Load the dataset. 2. Split the dataset into training and testing sets. 3. Standardize the features using `StandardScaler`. 4. Create and train the `SGDClassifier` with a `log_loss` loss function. 5. Evaluate the classifier on the test set using accuracy and confusion matrix. 6. Use `GridSearchCV` to tune the `alpha` parameter. 7. Report results. # Detailed Steps 1. **Load the Data:** ```python import pandas as pd data = pd.read_csv(\'data.csv\') X = data.drop(columns=\'target\') y = data[\'target\'] ``` 2. **Split the Data:** ```python from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) ``` 3. **Standardize Features:** ```python from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) ``` 4. **Train the Initial Model:** ```python from sklearn.linear_model import SGDClassifier model = SGDClassifier(loss=\'log_loss\', random_state=42) model.fit(X_train_scaled, y_train) ``` 5. **Evaluate the Initial Model:** ```python from sklearn.metrics import accuracy_score, confusion_matrix y_pred = model.predict(X_test_scaled) print(\'Initial Accuracy:\', accuracy_score(y_test, y_pred)) print(\'Confusion Matrix:n\', confusion_matrix(y_test, y_pred)) ``` 6. **Hyperparameter Tuning:** ```python from sklearn.model_selection import GridSearchCV parameter_grid = {\'alpha\': [1e-4, 1e-3, 1e-2, 1e-1, 1]} grid_search = GridSearchCV(SGDClassifier(loss=\'log_loss\', random_state=42), param_grid=parameter_grid, cv=5) grid_search.fit(X_train_scaled, y_train) print(\'Best Parameters:\', grid_search.best_params_) print(\'Best Score:\', grid_search.best_score_) best_model = grid_search.best_estimator_ best_y_pred = best_model.predict(X_test_scaled) print(\'Best Model Accuracy:\', accuracy_score(y_test, best_y_pred)) ```","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score, confusion_matrix def load_data(file_path): Load dataset from the CSV file. :param file_path: str, path to the CSV file. :return: tuple of (X, y) data = pd.read_csv(file_path) X = data.drop(columns=\'target\') y = data[\'target\'] return X, y def preprocess_data(X, y, test_size=0.2, random_state=42): Split the data into training and test sets. Standardize the features. :param X: DataFrame, features. :param y: Series, target. :param test_size: float, test set proportion. :param random_state: int, random state for reproducibility. :return: Tuple of (X_train_scaled, X_test_scaled, y_train, y_test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) return X_train_scaled, X_test_scaled, y_train, y_test def train_and_evaluate_model(X_train_scaled, X_test_scaled, y_train, y_test): Train SGDClassifier with logistic loss and evaluate the model. :param X_train_scaled: array-like, scaled training features. :param X_test_scaled: array-like, scaled test features. :param y_train: array-like, training labels. :param y_test: array-like, test labels. :return: tuple of (initial_accuracy, confusion_mtx) model = SGDClassifier(loss=\'log_loss\', random_state=42) model.fit(X_train_scaled, y_train) y_pred = model.predict(X_test_scaled) initial_accuracy = accuracy_score(y_test, y_pred) confusion_mtx = confusion_matrix(y_test, y_pred) return initial_accuracy, confusion_mtx def fine_tune_hyperparameters(X_train_scaled, y_train): Perform hyperparameter tuning using GridSearchCV. :param X_train_scaled: array-like, scaled training features. :param y_train: array-like, training labels. :return: tuple of (best_params, best_score, best_model) parameter_grid = {\'alpha\': [1e-4, 1e-3, 1e-2, 1e-1, 1]} grid_search = GridSearchCV(SGDClassifier(loss=\'log_loss\', random_state=42), param_grid=parameter_grid, cv=5) grid_search.fit(X_train_scaled, y_train) best_params = grid_search.best_params_ best_score = grid_search.best_score_ best_model = grid_search.best_estimator_ return best_params, best_score, best_model def train_and_evaluate_best_model(best_model, X_test_scaled, y_test): Evaluate the best model found by GridSearchCV. :param best_model: trained model with best hyperparameters. :param X_test_scaled: array-like, scaled test features. :param y_test: array-like, test labels. :return: float, best model accuracy. best_y_pred = best_model.predict(X_test_scaled) best_accuracy = accuracy_score(y_test, best_y_pred) return best_accuracy"},{"question":"# Advanced Python Logging System Implementation **Objective:** Create a sophisticated logging system that demonstrates your understanding of Python’s `logging` module, including log configuration, log levels, handlers, custom handlers, formats, and ensuring thread safety. **Task:** Implement a logging system with the following specifications: 1. **Logger Configuration**: Configure a logger with the name `\\"customLogger\\"`. The logger should: - Log messages of all severity levels from DEBUG and above. - Use a rotating file handler that writes log messages to a file named `\\"app.log\\"`. The file should: - Rotate when it reaches 5 MB in size. - Keep a backup of the last 3 log files. 2. **Custom Handler**: Create a custom logging handler that: - Inherits from `logging.Handler`. - Sends error (`ERROR` and above) log messages via email. The email handler should: - Send emails to `admin@example.com`. - Use a fake SMTP server (`smtp.example.com`) for simulation purposes. 3. **Formatter**: Set a log message format for all handlers that includes: - Timestamp in `YYYY-MM-DD HH:MM:SS` format - Log level - Logger name - Message 4. **Thread Safety**: Ensure that the logging system can handle log messages from multiple threads concurrently without any loss of data or corruption. 5. **Performance**: Ensure that the logging to the file and email sending operations are as efficient as possible. Consider using buffering or other techniques to optimize performance. **Your implementation should include:** - A `setup_logger()` function to set up and configure the logger. - A custom logging handler class named `EmailHandler`. - Example code demonstrating the logger\'s usage with multiple threads. **Input/Output:** - There is no specific input to the function. The function should set up and configure the logging system. - Output should be log files (`app.log` and its backups) and demonstration of the email functionality. ```python import logging from logging.handlers import RotatingFileHandler import threading import time class EmailHandler(logging.Handler): def __init__(self, mailhost=\'smtp.example.com\', fromaddr=\'logger@example.com\', toaddrs=\'admin@example.com\', subject=\'Error Log\'): super().__init__() self.mailhost = mailhost self.fromaddr = fromaddr self.toaddrs = toaddrs self.subject = subject def emit(self, record): # Here we simply print to simulate sending an email. # In a real-world scenario, you would set up SMTP connections and send the email log_entry = self.format(record) print(f\\"Sending email to {self.toaddrs}: Subject: {self.subject} nn{log_entry}\\") def setup_logger(): logger = logging.getLogger(\'customLogger\') logger.setLevel(logging.DEBUG) # Rotating File Handler file_handler = RotatingFileHandler(\'app.log\', maxBytes=5*1024*1024, backupCount=3) file_handler.setLevel(logging.DEBUG) # Email Handler email_handler = EmailHandler() email_handler.setLevel(logging.ERROR) # Formatter formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(name)s - %(message)s\', datefmt=\'%Y-%m-%d %H:%M:%S\') file_handler.setFormatter(formatter) email_handler.setFormatter(formatter) logger.addHandler(file_handler) logger.addHandler(email_handler) return logger # Example usage with threading def log_messages(logger, num_messages): for i in range(num_messages): logger.debug(f\\"Debug message {i}\\") logger.info(f\\"Info message {i}\\") logger.warning(f\\"Warning message {i}\\") if i % 5 == 0: # Simulating some error conditions logger.error(f\\"Error message {i}\\") if __name__ == \\"__main__\\": logger = setup_logger() threads = [] for i in range(5): thread = threading.Thread(target=log_messages, args=(logger, 20)) threads.append(thread) thread.start() for thread in threads: thread.join() ``` Implement your solution and ensure it meets all the specified requirements. Evaluate its performance and thread safety by running the example usage code.","solution":"import logging from logging.handlers import RotatingFileHandler import threading class EmailHandler(logging.Handler): def __init__(self, mailhost=\'smtp.example.com\', fromaddr=\'logger@example.com\', toaddrs=\'admin@example.com\', subject=\'Error Log\'): super().__init__() self.mailhost = mailhost self.fromaddr = fromaddr self.toaddrs = toaddrs self.subject = subject def emit(self, record): # Here we simply print to simulate sending an email. # In a real-world scenario, you would set up SMTP connections and send the email log_entry = self.format(record) print(f\\"Sending email to {self.toaddrs}: Subject: {self.subject} nn{log_entry}\\") def setup_logger(): logger = logging.getLogger(\'customLogger\') logger.setLevel(logging.DEBUG) # Rotating File Handler file_handler = RotatingFileHandler(\'app.log\', maxBytes=5*1024*1024, backupCount=3) file_handler.setLevel(logging.DEBUG) # Email Handler email_handler = EmailHandler() email_handler.setLevel(logging.ERROR) # Formatter formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(name)s - %(message)s\', datefmt=\'%Y-%m-%d %H:%M:%S\') file_handler.setFormatter(formatter) email_handler.setFormatter(formatter) logger.addHandler(file_handler) logger.addHandler(email_handler) return logger # Example usage with threading def log_messages(logger, num_messages): for i in range(num_messages): logger.debug(f\\"Debug message {i}\\") logger.info(f\\"Info message {i}\\") logger.warning(f\\"Warning message {i}\\") if i % 5 == 0: # Simulating some error conditions logger.error(f\\"Error message {i}\\") if __name__ == \\"__main__\\": logger = setup_logger() threads = [] for i in range(5): thread = threading.Thread(target=log_messages, args=(logger, 20)) threads.append(thread) thread.start() for thread in threads: thread.join()"},{"question":"You are required to implement a custom `Enum` class to handle a series of operations for a task management system. The system will utilize various priority levels for tasks using enumerated constants. # Task: 1. Define an enumeration `Priority` using the `Enum` class from the provided `enum` module. The priority levels you need to include are: - `LOW` (value: 1) - `MEDIUM` (value: 2) - `HIGH` (value: 3) - `CRITICAL` (value: 4) 2. Implement a class `Task` that represents a task in the system. Each task should have the following attributes: - `title` (str): The title of the task. - `description` (str): The description of the task. - `priority` (Priority): The priority of the task using the `Priority` enum. 3. Implement the following methods for the `Task` class: - `__init__(self, title: str, description: str, priority: Priority) -> None`: Constructor to initialize a task with its title, description, and priority. - `__str__(self) -> str`: Custom string representation that returns a formatted string as `\\"[<Priority>] Title: <title> - Description: <description>\\"`. - `change_priority(self, new_priority: Priority) -> None`: Method to change the priority of the task. - `is_higher_priority_than(self, other_task: \'Task\') -> bool`: Method that takes another task as parameter and returns `True` if the current task has a higher priority compared to the other task, otherwise `False`. # Input and Output Requirements: - The class and method definitions should manage and validate inputs internally as specified. - Assume that valid enum members of `Priority` will be provided where expected. - Demonstrate usage with brief examples within the script. # Constraints: - Ensure that the `Priority` values are unique and in the specified range. - Make use of the `enum` module functionalities, including creating enums, accessing members, and their values. - Pay attention to the use of custom methods and properties specific to the task outlined above. # Performance Requirements: - All operations should be performed in constant time O(1). Example Usage: ```python # Define Enum and Task as required # Example Usage task1 = Task(\\"Design Module\\", \\"Design the architecture of the new module.\\", Priority.HIGH) task2 = Task(\\"Update README\\", \\"Revise the documentation to reflect recent changes.\\", Priority.LOW) print(task1) # Output should be: \\"[HIGH] Title: Design Module - Description: Design the architecture of the new module.\\" print(task2) # Output should be: \\"[LOW] Title: Update README - Description: Revise the documentation to reflect recent changes.\\" task1.change_priority(Priority.CRITICAL) print(task1) # Output should be: \\"[CRITICAL] Title: Design Module - Description: Design the architecture of the new module.\\" print(task1.is_higher_priority_than(task2)) # Output should be: True print(task2.is_higher_priority_than(task1)) # Output should be: False ``` Ensure your code is well-documented with comments explaining your logic where necessary.","solution":"from enum import Enum class Priority(Enum): LOW = 1 MEDIUM = 2 HIGH = 3 CRITICAL = 4 class Task: def __init__(self, title: str, description: str, priority: Priority) -> None: self.title = title self.description = description self.priority = priority def __str__(self) -> str: return f\\"[{self.priority.name}] Title: {self.title} - Description: {self.description}\\" def change_priority(self, new_priority: Priority) -> None: self.priority = new_priority def is_higher_priority_than(self, other_task: \'Task\') -> bool: return self.priority.value > other_task.priority.value"},{"question":"**Objective:** Your task is to implement a Python function that demonstrates the usage of `contextvars` to recursively compute the factorial of a non-negative integer. The function should use context variables to track the depth of recursion and ensure the depth is accurately restored after exiting each recursive call. # Problem Statement: Implement a function `factorial_with_context(n: int) -> int` that computes the factorial of a given non-negative integer `n` using context variables to track the depth of recursive calls. # Specifications: 1. **Function Name**: `factorial_with_context` 2. **Parameters**: - `n` (int): A non-negative integer whose factorial is to be computed. 3. **Returns**: - `int`: The factorial of the given integer `n`. # Constraints: 1. `n` will be a non-negative integer `0 <= n <= 12`. 2. You must use context variables to track the recursion depth and reset it accurately after the recursive call exits. # Example: ```python >>> factorial_with_context(5) 120 >>> factorial_with_context(0) 1 ``` # Implementation Details: 1. Create a context variable to track the depth. 2. Initialize the context variable to zero at the start of the function. 3. Create a helper function to handle the recursive computation, which increments the recursion depth context variable upon each call. 4. Reset the recursion depth to its previous state upon exiting each recursive function call. 5. Ensure the context is clean after the function execution ends. # Hints: - The `contextvars.ContextVar` module can be imported and used to create context variables. - Use the methods `set()` and `reset()` for manipulating the context variables as needed. ```python import contextvars # Context variable to track recursion depth recursion_depth = contextvars.ContextVar(\\"recursion_depth\\") def factorial_with_context(n: int) -> int: # Initialize the recursion depth at the very start initial_token = recursion_depth.set(0) def inner_factorial(x: int) -> int: # Retrieve current depth and increment it current_depth = recursion_depth.get() recursion_depth.set(current_depth + 1) if x == 0 or x == 1: result = 1 else: result = x * inner_factorial(x - 1) # Reset depth back to previous state recursion_depth.reset(current_depth) return result # Compute factorial and cleanup context result = inner_factorial(n) recursion_depth.reset(initial_token) return result # Testing the function print(factorial_with_context(5)) # Output should be 120 print(factorial_with_context(0)) # Output should be 1 ``` # Expected Output: ```plaintext 120 1 ``` Demonstrate your understanding of `contextvars` in Python by implementing the function with provided constraints.","solution":"import contextvars # Context variable to track recursion depth recursion_depth = contextvars.ContextVar(\\"recursion_depth\\", default=0) def factorial_with_context(n: int) -> int: # Initialize the recursion depth at the very start initial_token = recursion_depth.set(0) def inner_factorial(x: int) -> int: # Retrieve current depth and increment it current_depth = recursion_depth.get() recursion_depth.set(current_depth + 1) if x == 0 or x == 1: result = 1 else: result = x * inner_factorial(x - 1) # Reset depth back to previous state recursion_depth.set(current_depth) return result # Compute factorial and cleanup context result = inner_factorial(n) recursion_depth.reset(initial_token) return result"},{"question":"Objective: Demonstrate your understanding of the `html` module in Python by creating a function that replaces potentially unsafe characters in a string with HTML-safe sequences and then reverses the process to verify the correctness of the escaped string. Problem Statement: You are given a string that may contain HTML special characters. Your task is to perform the following operations: 1. Escape the special characters using `html.escape(s, quote=True)`. 2. Unescape the resulting string using `html.unescape(s)` to verify it returns to the original string. Implement the following function: ```python def verify_html_escaping(original_str: str) -> bool: This function takes an input string `original_str`, escapes HTML special characters, and then unescapes them to verify the correctness of the escape function. Args: original_str (str): The input string containing HTML special characters. Returns: bool: True if the unescaped string matches the original input string, False otherwise. pass ``` Example: ```python input_str = \\"Hello & welcome to <OpenAI>!\\" result = verify_html_escaping(input_str) print(result) # Output should be True ``` Constraints: - You must use `html.escape()` and `html.unescape()` from the `html` module. - The input string can contain any ASCII characters. Notes: - Ensure that your function handles edge cases such as empty strings or strings with no special characters. - Focus on preserving the original content after escaping and unescaping processes. Tasks: 1. Implement the `verify_html_escaping()` function. 2. Write several test cases to validate your implementation.","solution":"import html def verify_html_escaping(original_str: str) -> bool: This function takes an input string `original_str`, escapes HTML special characters, and then unescapes them to verify the correctness of the escape function. Args: original_str (str): The input string containing HTML special characters. Returns: bool: True if the unescaped string matches the original input string, False otherwise. escaped_str = html.escape(original_str, quote=True) unescaped_str = html.unescape(escaped_str) return original_str == unescaped_str"},{"question":"# Binary Data Manipulation Using `struct` and `codecs` Objective Demonstrate your understanding of Python\'s `struct` and `codecs` libraries by implementing functions that handle binary data packing/unpacking and encoding/decoding operations. Task You are required to implement two functions: 1. **pack_data** - **Input:** - A list of integers. - A format string suitable for the `struct` module. - **Output:** - A bytes object representing the packed data. - **Description:** This function should take a list of integers and a format string, pack the integers into a binary format using the specified format string, and return the resulting bytes object. 2. **encode_decode_string** - **Input:** - A string. - An encoding type (e.g., \'utf-8\', \'utf-16\', \'ascii\', etc.). - **Output:** - A tuple containing two strings: the encoded string and the decoded string. - **Description:** This function should encode the given string using the specified encoding type and then decode it back to the original string. The function should return a tuple with the encoded version and the decoded version of the string. Constraints - For `pack_data`, the length of the list of integers should not exceed 10 elements. - For `encode_decode_string`, the string length should not exceed 100 characters. Performance Requirements - Your solution should handle typical inputs efficiently within a reasonable time frame. Example ```python def pack_data(integers, format_string): pass def encode_decode_string(s, encoding): pass # Example usage: packed = pack_data([1, 2, 3], \'>3i\') # Assuming big-endian, 3 integers print(packed) # Output: b\'x00x00x00x01x00x00x00x02x00x00x00x03\' (Example output, may vary) encoded_decoded = encode_decode_string(\\"Hello, World!\\", \\"utf-8\\") print(encoded_decoded) # Output: (\'Hello, World!\', \'Hello, World!\') ``` Note 1. Ensure you handle exceptions and edge cases appropriately. 2. Refer to Python documentation for details on the `struct` and `codecs` modules to make sure your implementations are accurate and efficient.","solution":"import struct import codecs def pack_data(integers, format_string): Pack a list of integers into a bytes object using the specified format string. :param integers: List of integers to pack :param format_string: Format string for struct.pack :return: Packed bytes object packed_data = struct.pack(format_string, *integers) return packed_data def encode_decode_string(s, encoding): Encode a string using the specified encoding type and then decode it back to the original string. :param s: The string to encode and decode :param encoding: The encoding type to use :return: A tuple containing the encoded string and the decoded string encoded_string = s.encode(encoding) decoded_string = encoded_string.decode(encoding) return (encoded_string, decoded_string)"},{"question":"**Question:** Implement a Python function that creates a Unicode string from an ASCII string, transforms certain characters based on their Unicode properties, and encodes it into different formats. **Function Definition:** ```python def unicode_transform_and_encode(ascii_input: str) -> dict: This function takes an ASCII string, transforms certain characters based on their Unicode properties, and returns a dictionary with the string encoded in different formats. Args: ascii_input (str): An ASCII encoded string. Returns: dict: A dictionary with keys \'utf-8\', \'utf-16\', \'latin-1\' and corresponding encoded byte strings. ``` **Steps to Implement:** 1. Convert the ASCII string to a Unicode string. 2. Transform all lowercase alphabetic characters to uppercase. 3. Replace all whitespace characters with a single underscore (\'_\'). 4. Encode the transformed Unicode string into \'utf-8\', \'utf-16\', and \'latin-1\' formats. 5. Return a dictionary with the encoded byte strings. The keys of the dictionary should be \'utf-8\', \'utf-16\', and \'latin-1\'. **Example:** ```python input_string = \\"hello world 123\\" output = unicode_transform_and_encode(input_string) print(output) # Example output # { # \'utf-8\': b\'HELLO_WORLD_123\', # \'utf-16\': b\'xffxfeHx00Ex00Lx00Lx00Ox00_x00Wx00Ox00Rx00Lx00Dx00_x001x002x003x00\', # \'latin-1\': b\'HELLO_WORLD_123\' # } ``` **Constraints:** - The input string will only contain printable ASCII characters (characters with ordinal values from 32 to 126). - A character transformation should happen in a memory-efficient way. **Hints:** - You may use the `str.upper()`, `unicode.islower()`, `unicode.isspace()` functions for transformations. - Use `str.encode()` method for encoding in different formats. **Performance Requirements:** - Ensure the function can handle large strings efficiently.","solution":"def unicode_transform_and_encode(ascii_input: str) -> dict: This function takes an ASCII string, transforms certain characters based on their Unicode properties, and returns a dictionary with the string encoded in different formats. Args: ascii_input (str): An ASCII encoded string. Returns: dict: A dictionary with keys \'utf-8\', \'utf-16\', \'latin-1\' and corresponding encoded byte strings. # Convert the ASCII string to a Unicode string. unicode_str = ascii_input # Transform all lowercase alphabetic characters to uppercase. transformed_str = unicode_str.upper() # Replace all whitespace characters with a single underscore (\'_\'). final_str = transformed_str.replace(\' \', \'_\') # Encode the transformed Unicode string into \'utf-8\', \'utf-16\', and \'latin-1\' formats. encoded_dict = { \'utf-8\': final_str.encode(\'utf-8\'), \'utf-16\': final_str.encode(\'utf-16\'), \'latin-1\': final_str.encode(\'latin-1\') } return encoded_dict"},{"question":"**Audio Data Processing with `audioop`** # Objective: Write a Python function to process an input audio fragment, convert it to a specific format, and analyze its properties. # Task: Implement a function `process_audio(fragment: bytes, input_width: int, output_width: int, encode_format: str) -> dict` which performs the following operations: 1. Convert the input audio fragment from its current sample width (`input_width`) to the specified output sample width (`output_width`). 2. Encode the converted audio fragment to a specified audio encoding format (`encode_format`), which can be either \'alaw\' or \'ulaw\'. 3. Calculate and return various properties of the processed audio fragment in a dictionary. # Parameters: - `fragment` (bytes): The input audio fragment. - `input_width` (int): The sample width of the input fragment (1, 2, 3, or 4 bytes). - `output_width` (int): The desired sample width of the output fragment (1, 2, 3, or 4 bytes). - `encode_format` (str): Target encoding format, either \'alaw\' or \'ulaw\'. # Returns: A dictionary with the following keys: - `\'encoded_fragment\'`: The encoded audio fragment as a bytes object. - `\'max_value\'`: The maximum absolute value of the samples in the converted fragment. - `\'rms\'`: The root-mean-square of the samples in the converted fragment. - `\'zero_crossings\'`: The number of zero crossings in the converted fragment. # Constraints: 1. If the `encode_format` is not \'alaw\' or \'ulaw\', raise a `ValueError` with an appropriate message. 2. Ensure correct handling of errors raised by the `audioop` functions. # Example: ```python def process_audio(fragment: bytes, input_width: int, output_width: int, encode_format: str) -> dict: import audioop # Step 1: Convert the input fragment to the specified output width converted_fragment = audioop.lin2lin(fragment, input_width, output_width) # Step 2: Encode the converted fragment to the specified format if encode_format == \'alaw\': encoded_fragment = audioop.lin2alaw(converted_fragment, output_width) elif encode_format == \'ulaw\': encoded_fragment = audioop.lin2ulaw(converted_fragment, output_width) else: raise ValueError(\\"Unsupported encode format. Use \'alaw\' or \'ulaw\'.\\") # Step 3: Calculate properties of the converted fragment max_value = audioop.max(converted_fragment, output_width) rms_value = audioop.rms(converted_fragment, output_width) zero_crossings = audioop.cross(converted_fragment, output_width) return { \'encoded_fragment\': encoded_fragment, \'max_value\': max_value, \'rms\': rms_value, \'zero_crossings\': zero_crossings } # Example usage input_fragment = bytes([1, 2, 3, 4, 5, 6, 7, 8]) result = process_audio(input_fragment, 1, 2, \'alaw\') print(result) ``` # Note: - The students should ensure their code is efficient and handles errors gracefully. - The sample input provided above is for clarity; actual audio samples can be larger and more complex.","solution":"def process_audio(fragment: bytes, input_width: int, output_width: int, encode_format: str) -> dict: import audioop # Step 1: Convert the input fragment to the specified output width converted_fragment = audioop.lin2lin(fragment, input_width, output_width) # Step 2: Encode the converted fragment to the specified format if encode_format == \'alaw\': encoded_fragment = audioop.lin2alaw(converted_fragment, output_width) elif encode_format == \'ulaw\': encoded_fragment = audioop.lin2ulaw(converted_fragment, output_width) else: raise ValueError(\\"Unsupported encode format. Use \'alaw\' or \'ulaw\'.\\") # Step 3: Calculate properties of the converted fragment max_value = audioop.max(converted_fragment, output_width) rms_value = audioop.rms(converted_fragment, output_width) zero_crossings = audioop.cross(converted_fragment, output_width) return { \'encoded_fragment\': encoded_fragment, \'max_value\': max_value, \'rms\': rms_value, \'zero_crossings\': zero_crossings } # Example usage # input_fragment = bytes([1, 2, 3, 4, 5, 6, 7, 8]) # result = process_audio(input_fragment, 1, 2, \'alaw\') # print(result)"},{"question":"You are tasked with writing a Python script that performs the following operations: 1. Encodes a given binary file using the uuencode format. 2. Decodes the uuencoded file back to its original binary form. 3. Ensures the decoded file matches the original file. # Function Definitions 1. **encode_file()** - Parameters: - `in_filename` (str): The file path of the input binary file to be encoded. - `out_filename` (str): The file path where the uuencoded data will be written. - Output: None. - Functionality: - Encodes the content of `in_filename` using the `uu.encode()` function and writes the output to `out_filename`. 2. **decode_file()** - Parameters: - `in_filename` (str): The file path of the uuencoded input file to be decoded. - `out_filename` (str): The file path where the decoded binary data will be written. - Output: None. - Functionality: - Decodes the content of `in_filename` using the `uu.decode()` function and writes the output to `out_filename`. 3. **verify_files()** - Parameters: - `file1` (str): The file path of the first file to compare. - `file2` (str): The file path of the second file to compare. - Output: `bool` - Returns `True` if the contents of the two files are identical, `False` otherwise. - Functionality: - Compares the binary contents of `file1` and `file2`. # Constraints - You should handle any exceptions that may arise during encoding or decoding, and print user-friendly error messages. - You can assume that the input file exists and is readable, and the output paths are writable. - The input file may be of any binary format (e.g., image, audio, etc.). # Example Usage ```python # Encode a file encode_file(\\"example.bin\\", \\"example.uue\\") # Decode the file decode_file(\\"example.uue\\", \\"example_decoded.bin\\") # Verify the content assert verify_files(\\"example.bin\\", \\"example_decoded.bin\\"), \\"Files do not match!\\" ``` In the script, ensure that you open the files in the correct mode (`rb` for reading binary files and `wb` for writing binary files). After completing these tasks, the script should verify that the original and decoded files are identical.","solution":"import uu import os def encode_file(in_filename, out_filename): try: with open(in_filename, \'rb\') as infile: with open(out_filename, \'wb\') as outfile: uu.encode(infile, outfile) except Exception as e: print(f\\"Error encoding file {in_filename} to {out_filename}: {e}\\") def decode_file(in_filename, out_filename): try: with open(in_filename, \'rb\') as infile: with open(out_filename, \'wb\') as outfile: uu.decode(infile, outfile) except Exception as e: print(f\\"Error decoding file {in_filename} to {out_filename}: {e}\\") def verify_files(file1, file2): try: with open(file1, \'rb\') as f1, open(file2, \'rb\') as f2: return f1.read() == f2.read() except Exception as e: print(f\\"Error comparing files {file1} and {file2}: {e}\\") return False # Example usage: # encode_file(\\"example.bin\\", \\"example.uue\\") # decode_file(\\"example.uue\\", \\"example_decoded.bin\\") # assert verify_files(\\"example.bin\\", \\"example_decoded.bin\\"), \\"Files do not match!\\""},{"question":"Objective: Implement a Python function that simulates the behavior of `slice` objects and manipulates sequences accordingly while integrating the provided C API-like functionality. Function Signature: ```python def custom_slice(sequence, start=None, stop=None, step=None): Simulates the behavior of Python slice object and manipulates the given sequence. :param sequence: List of elements to be sliced. :param start: Starting index of the slice. :param stop: Stopping index of the slice. :param step: Step value for the slicing. :return: A new list containing the sliced elements. pass ``` Input: - `sequence` (List[Any]): A list of elements. - `start` (Optional[int]): Start index of the slice. Default is `None`. - `stop` (Optional[int]): Stop index of the slice. Default is `None`. - `step` (Optional[int]): Step value for the slice. Default is `None`. Output: - Returns a new list containing the sliced elements from the original sequence. Constraints: 1. Simulate the behavior of out-of-bound indices (negative and exceeding the length). 2. Handle all default slice behaviors (start, stop, step as `None`). Example: ```python >>> custom_slice([1, 2, 3, 4, 5], 1, 4) [2, 3, 4] >>> custom_slice([1, 2, 3, 4, 5], None, None, -1) [5, 4, 3, 2, 1] >>> custom_slice([1, 2, 3, 4, 5], -10, 10, 2) [1, 3, 5] ``` Guidelines: 1. You are not allowed to use Python’s built-in slicing feature (i.e., sequence[start:stop:step]). 2. You can assume the `sequence` will only contain non-negative integers. 3. Handle slicing akin to normal Python slicing behavior. 4. Implement the logic in pure Python to demonstrate the understanding of slicing mechanisms. This question ensures students comprehend fundamental concepts of slicing and its detailed handling, which mimics the behavior of `PySlice` API functions at a higher level in Python.","solution":"def custom_slice(sequence, start=None, stop=None, step=None): Simulates the behavior of Python slice object and manipulates the given sequence. :param sequence: List of elements to be sliced. :param start: Starting index of the slice. :param stop: Stopping index of the slice. :param step: Step value for the slicing. :return: A new list containing the sliced elements. if step is None: step = 1 if start is None: start = 0 if step > 0 else len(sequence) - 1 if stop is None: stop = len(sequence) if step > 0 else -1 new_sequence = [] if (step > 0 and start < 0): start = max(start + len(sequence), 0) elif (step < 0 and start >= len(sequence)): start = min(start - len(sequence), len(sequence) - 1) if (step > 0 and stop < 0): stop = max(stop + len(sequence), 0) elif (step < 0 and stop >= len(sequence)): stop = min(stop - len(sequence), len(sequence) - 1) idx = start while (step > 0 and idx < stop) or (step < 0 and idx > stop): if 0 <= idx < len(sequence): new_sequence.append(sequence[idx]) idx += step return new_sequence"},{"question":"# Question: You are tasked with creating a high-precision calculator for financial applications that must handle various types of numerical input and output specific formats. The calculator should perform a series of decimal arithmetic operations with user-adjustable precision and rounding rules. Specifically, you will implement a function `financial_calculator` that takes a list of operations and computes the result for each operation in sequence. Requirements: - Use the `decimal` module for all arithmetic computations. - Allow the user to set the precision and rounding method for the calculations. - Handle special values (Infinity, NaN) gracefully. - Format the result according to a specified monetary format. Function Signature: ```python from decimal import Decimal, getcontext, Context, ROUND_HALF_UP def financial_calculator(operations: list, precision: int, rounding: str, monetary_format: str) -> list: pass ``` Input: - `operations`: List of tuples where each tuple contains an operator (`str`: \'+\', \'-\', \'*\', \'/\', \'**\') and the two operands (`str` or `int`). E.g., `[(\'+\', 10, 20), (\'*\', \'3.14\', \'2.71\')]` - `precision`: An integer representing the precision for the `decimal` context. - `rounding`: A string representing the rounding mode. Valid values are \'ROUND_CEILING\', \'ROUND_DOWN\', \'ROUND_FLOOR\', \'ROUND_HALF_DOWN\', \'ROUND_HALF_EVEN\', \'ROUND_HALF_UP\', \'ROUND_UP\', \'ROUND_05UP\'. - `monetary_format`: A string for formatting the results in a monetary style. E.g., `\'\'` or `\'€\'` Output: - Return a list of results for each operation in the specified monetary format and precision. Constraints: - Protect the function against invalid inputs (e.g., invalid operations or data types). - The function should raise an appropriate exception in case of errors. Example: ```python operations = [(\'+\', \'10.05\', \'20.10\'), (\'-\', \'55.50\', \'30.25\'), (\'*\', \'3.14\', \'2.71\'), (\'/\', \'100\', \'0.5\'), (\'**\', \'2\', \'3\')] precision = 5 rounding = \'ROUND_HALF_UP\' monetary_format = \'\' result = financial_calculator(operations, precision, rounding, monetary_format) print(result) ``` **Output:** ``` [\'30.150\', \'25.25\', \'8.51\', \'200.00\', \'8\'] ``` Notes: 1. Use the `decimal` module to manage decimal arithmetic, precision, and rounding. 2. Ensure the results are formatted correctly as per the specified monetary format. 3. Handle special values like Infinity and NaN appropriately.","solution":"from decimal import Decimal, getcontext, Context def financial_calculator(operations: list, precision: int, rounding: str, monetary_format: str) -> list: Perform a series of financial calculations with specified precision and rounding. Args: - operations (list): List of tuples where each tuple contains an operator (\'+\', \'-\', \'*\', \'/\', \'**\') and the two operands (\'str\' or \'int\'). - precision (int): An integer representing the precision for the decimal context. - rounding (str): A string representing the rounding mode. - monetary_format (str): A string for formatting the results in a monetary style. Returns: - list: A list of results for each operation in the specified monetary format and precision. getcontext().prec = precision getcontext().rounding = rounding results = [] for operation in operations: if len(operation) != 3: raise ValueError(f\\"Each operation must be a tuple of length 3, got {operation}\\") operator, operand1, operand2 = operation operand1 = Decimal(operand1) operand2 = Decimal(operand2) if operator == \'+\': result = operand1 + operand2 elif operator == \'-\': result = operand1 - operand2 elif operator == \'*\': result = operand1 * operand2 elif operator == \'/\': result = operand1 / operand2 elif operator == \'**\': result = operand1 ** operand2 else: raise ValueError(f\\"Invalid operator: {operator}\\") formatted_result = f\\"{monetary_format}{result:.{precision}f}\\" # Handling leading zeroes for cents. if \'.\' in formatted_result and len(formatted_result.split(\'.\')[1]) < precision: formatted_result += \'0\' * (precision - len(formatted_result.split(\'.\')[1])) results.append(formatted_result) return results"},{"question":"<|Analysis Begin|> The provided documentation gives an introductory overview of basic Python concepts, including using Python as a calculator for numbers and strings and the manipulation of lists. Python\'s arithmetic operations, string handling capabilities, and list features are covered comprehensively. Key concepts covered in the documentation: 1. Arithmetic operations: Addition, subtraction, multiplication, division, floor division, modulus, and exponentiation. 2. Variable assignments. 3. Basic data types: `int`, `float`, and `str`. 4. String operations: Concatenation, escaping characters, slicing, and indexing. 5. List operations: Indexing, slicing, mutability, concatenation, length, and nesting. The documentation is foundational but does not delve deeply into more advanced features of Python or specific libraries or modules provided in Python 3.10. It does help in understanding basic and fundamental features but lacks advanced constructs, so creating a challenging problem requiring advanced concepts might be limited. Given that fundamental understanding of handling numbers, strings, and lists is paramount, the question should focus on mixing these elements to test proficiency. A meaningful challenge could be created around list and string manipulations involving arithmetic operations, which requires a substantial grasp of these basics but is also complex enough to assess their comprehension thoroughly. <|Analysis End|> <|Question Begin|> **Problem Statement:** Create a function `process_data(data: str) -> list` that processes a multiline string representing a collection of integer data (potentially from some raw input or a text file) and performs various operations as defined below: 1. The input string `data` consists of multiple lines, where each line contains a sequence of integers separated by spaces. Each line represents a separate list of numbers. 2. The function should parse the input string into a list of lists of integers. 3. For each list of integers, compute the following: - Sum of the integers. - Product of the integers (excluding zero values). - Maximum and minimum values in the list. 4. Return a list of dictionaries where each dictionary corresponds to a line from the input string and contains the computed sum, product, maximum, and minimum values with the respective keys: `\\"sum\\"`, `\\"product\\"`, `\\"max\\"`, and `\\"min\\"`. **Function Signature:** ```python def process_data(data: str) -> list: pass ``` **Input:** - `data`: A string representing multiple lines of integers. Each line of the string will contain integers separated by spaces. **Output:** - A list of dictionaries, where each dictionary contains the keys `\\"sum\\"`, `\\"product\\"`, `\\"max\\"`, and `\\"min\\"` with their corresponding values computed from each respective line of integers. **Example:** ```python data = 1 2 3 4 5 6 7 8 3 7 2 9 print(process_data(data)) ``` Output: ```python [ {\\"sum\\": 10, \\"product\\": 24, \\"max\\": 4, \\"min\\": 1}, {\\"sum\\": 26, \\"product\\": 1680, \\"max\\": 8, \\"min\\": 5}, {\\"sum\\": 21, \\"product\\": 378, \\"max\\": 9, \\"min\\": 2} ] ``` **Constraints:** - Each line will contain at least one integer. - All integers will be non-negative and within the range 0 to 1000. - The input string will have at most 1000 lines. **Explanation:** 1. The function first splits the input string by lines, then processes each line to split it into individual integers. 2. For each list of integers, the required computations (sum, product, max, and min) are carried out. 3. Results for each line are stored in a dictionary and appended to the result list. ----- **Note**: To handle the product effectively, ensure you ignore zero values during the product calculation to avoid multiplying by zero. If all values in a line are zero, return product as 0.","solution":"def process_data(data: str) -> list: Processes the input multiline string of integers and computes sum, product (excluding zeros), maximum and minimum values for each line of integers. Parameters: data (str): A multiline string, each line containing integers separated by spaces. Returns: list: A list of dictionaries with each dictionary containing the sum, product, max, and min values for the corresponding line of integers. result = [] lines = data.strip().split(\'n\') for line in lines: numbers = list(map(int, line.split())) if not numbers: # Skip empty lines just in case continue line_sum = sum(numbers) line_max = max(numbers) line_min = min(numbers) # Calculating product excluding zeros line_product = 1 for num in numbers: if num != 0: line_product *= num if all(num == 0 for num in numbers): line_product = 0 result.append({ \\"sum\\": line_sum, \\"product\\": line_product, \\"max\\": line_max, \\"min\\": line_min }) return result"},{"question":"# Advanced Python Coding Question The `imp` module has been deprecated since Python version 3.4 in favor of `importlib`. Understanding how to transition from using the `imp` module to `importlib` is crucial for maintaining and updating legacy Python code. In this task, you will implement a function to replicate the functionality for finding and loading a module using `importlib`, replacing `imp.find_module` and `imp.load_module`. # Problem Statement Write a Python function `load_module(module_name: str) -> object` that finds and loads a module given its name. This should closely emulate the behavior of the deprecated `imp.find_module` and `imp.load_module` functions using modern `importlib` facilities. You must: 1. First, try to find the module using `importlib.util.find_spec`. 2. Then, if found, create a new module object using `importlib.util.module_from_spec`. 3. Finally, load the module using the loader’s `exec_module` method. Your function should handle scenarios where the module does not exist by raising an `ImportError`. # Function Signature ```python def load_module(module_name: str) -> object: pass ``` # Example Usage ```python # Assume there is a module named `example_module.py` located somewhere within sys.path module = load_module(\\"example_module\\") print(module) # should print: <module \'example_module\' from \'...\'> ``` # Constraints - The `module_name` parameter will be a non-empty string. - You are not allowed to use the `imp` module in your implementation. - You should properly handle exceptions and ensure any open file objects are closed if an error occurs. # Notes - You may refer to the documentation for `importlib.util` and `importlib` to assist with this implementation. - This question tests your ability to adapt deprecated practices to modern Python practices and enforces good coding habits regarding module imports.","solution":"import importlib.util def load_module(module_name: str) -> object: Finds and loads a module given its name using importlib. Args: module_name (str): The name of the module to be loaded. Returns: object: The loaded module object. Raises: ImportError: If the module cannot be found or loaded. spec = importlib.util.find_spec(module_name) if spec is None: raise ImportError(f\\"Module {module_name} not found\\") module = importlib.util.module_from_spec(spec) try: spec.loader.exec_module(module) except Exception as e: raise ImportError(f\\"Failed to load module {module_name}\\") from e return module"},{"question":"**Title:** Implement a Simple Task Manager Using `shelve` **Objective:** Implement a simple task manager that allows users to add, remove, update, and list tasks using persistent storage provided by the `shelve` module. **Description:** You are required to create a task manager application that stores its data persistently using the `shelve` module. The task manager should be able to perform the following operations: 1. Add a new task. 2. Remove an existing task. 3. Update an existing task. 4. List all tasks. Each task should have the following attributes: - `task_id` (unique string identifier) - `description` (string describing the task) - `status` (string indicating the status, can be \'pending\', \'in-progress\', or \'completed\') **Requirements:** 1. Implement the `TaskManager` class with the methods specified below. 2. Ensure that the tasks are stored persistently using the `shelve` module. 3. Manage tasks in such a way that operations on mutable entries are correctly handled. **Class and Method Definitions:** ```python import shelve class TaskManager: def __init__(self, filename=\'tasks.db\'): Initialize the TaskManager with a given filename for the shelf. self.filename = filename def add_task(self, task_id, description, status=\'pending\'): Add a new task to the task manager. - task_id: str, unique identifier for the task. - description: str, description of the task. - status: str, status of the task (default is \'pending\'). # Code to add a task def remove_task(self, task_id): Remove an existing task from the task manager. - task_id: str, unique identifier for the task. # Code to remove a task def update_task(self, task_id, description=None, status=None): Update an existing task in the task manager. - task_id: str, unique identifier for the task. - description: str, new description for the task (if provided). - status: str, new status for the task (if provided). # Code to update a task def list_tasks(self): List all tasks in the task manager. Returns a list of dictionaries, each representing a task. # Code to list all tasks ``` **Instructions:** - The `add_task` method should raise a `ValueError` if a task with the same `task_id` already exists. - The `remove_task` method should raise a `KeyError` if the task with the specified `task_id` does not exist. - The `update_task` method should update the task\'s description and/or status. If the `task_id` does not exist, it should raise a `KeyError`. - The `list_tasks` method should return a list of all tasks, where each task is represented by a dictionary containing `task_id`, `description`, and `status`. **Example Usage:** ```python # Initialize the TaskManager manager = TaskManager() # Add tasks manager.add_task(\'task1\', \'Implement task manager\', \'in-progress\') manager.add_task(\'task2\', \'Write unit tests\', \'pending\') # List tasks print(manager.list_tasks()) # [{\'task_id\': \'task1\', \'description\': \'Implement task manager\', \'status\': \'in-progress\'}, {\'task_id\': \'task2\', \'description\': \'Write unit tests\', \'status\': \'pending\'}] # Update a task manager.update_task(\'task1\', status=\'completed\') # List tasks again print(manager.list_tasks()) # [{\'task_id\': \'task1\', \'description\': \'Implement task manager\', \'status\': \'completed\'}, {\'task_id\': \'task2\', \'description\': \'Write unit tests\', \'status\': \'pending\'}] # Remove a task manager.remove_task(\'task1\') # List tasks again print(manager.list_tasks()) # [{\'task_id\': \'task2\', \'description\': \'Write unit tests\', \'status\': \'pending\'}] ``` **Constraints:** - The task manager should handle concurrent access safely by ensuring no other program is accessing the shelf concurrently. - Consider the performance implications when using `writeback=True`. **Evaluation Criteria:** - Correct implementation of the specified methods. - Proper handling of persistence using the `shelve` module. - Efficient management of mutable entries and ensuring data integrity. - Handling exceptions appropriately as specified.","solution":"import shelve class TaskManager: def __init__(self, filename=\'tasks.db\'): Initialize the TaskManager with a given filename for the shelf. self.filename = filename def add_task(self, task_id, description, status=\'pending\'): Add a new task to the task manager. - task_id: str, unique identifier for the task. - description: str, description of the task. - status: str, status of the task (default is \'pending\'). with shelve.open(self.filename, writeback=True) as db: if task_id in db: raise ValueError(\\"Task with this ID already exists.\\") db[task_id] = {\\"description\\": description, \\"status\\": status} def remove_task(self, task_id): Remove an existing task from the task manager. - task_id: str, unique identifier for the task. with shelve.open(self.filename, writeback=True) as db: if task_id not in db: raise KeyError(\\"Task with this ID does not exist.\\") del db[task_id] def update_task(self, task_id, description=None, status=None): Update an existing task in the task manager. - task_id: str, unique identifier for the task. - description: str, new description for the task (if provided). - status: str, new status for the task (if provided). with shelve.open(self.filename, writeback=True) as db: if task_id not in db: raise KeyError(\\"Task with this ID does not exist.\\") if description is not None: db[task_id][\'description\'] = description if status is not None: db[task_id][\'status\'] = status def list_tasks(self): List all tasks in the task manager. Returns a list of dictionaries, each representing a task. with shelve.open(self.filename) as db: return [{\\"task_id\\": key, \\"description\\": value[\\"description\\"], \\"status\\": value[\\"status\\"]} for key, value in db.items()]"},{"question":"Objective In this assessment, your task is to demonstrate your understanding of seaborn\'s `axes_style` functionality by plotting multiple subplots with different styles in a single figure. Each subplot should use a different predefined style from seaborn. Question Write a function called `plot_with_styles` that takes no input parameters. This function should create a 2x2 grid of subplots (total 4 subplots) where each one uses a different seaborn style. The predefined styles to be used are: - \\"darkgrid\\" - \\"whitegrid\\" - \\"dark\\" - \\"white\\" Each subplot should contain a line plot of the given data: ```python x = [1, 2, 3, 4, 5] y = [1, 4, 9, 16, 25] ``` Requirements: 1. Each subplot must have a different style from the list provided. 2. The overall figure size should be set to 10x10 inches. 3. Ensure that the titles for each subplot indicate the style being used. 4. Display the figure with properly labeled subplots. Output: The function should display a figure with the described specifications and subplots. Function Signature: ```python def plot_with_styles(): # your code here ``` Example: Executing `plot_with_styles()` should produce a figure similar to: ``` A 2x2 grid of line plots, each with different seaborn styles (\\"darkgrid\\", \\"whitegrid\\", \\"dark\\", \\"white\\"). ``` Constraints: - Do not import any additional packages outside seaborn and matplotlib. - Focus on correct and efficient implementation following Python best practices.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_with_styles(): # Data for plotting x = [1, 2, 3, 4, 5] y = [1, 4, 9, 16, 25] # Styles to be used styles = [\\"darkgrid\\", \\"whitegrid\\", \\"dark\\", \\"white\\"] # Create a 2x2 grid of subplots fig, axes = plt.subplots(2, 2, figsize=(10, 10)) axes = axes.flatten() # Flatten the 2x2 array for easy iteration # Plot each subplot with different style for ax, style in zip(axes, styles): sns.set_style(style) ax.plot(x, y) ax.set_title(f\'Style: {style}\') ax.set_xlabel(\'x\') ax.set_ylabel(\'y\') # Display the figure with properly labeled subplots plt.tight_layout() plt.show()"},{"question":"**Objective**: Demonstrate your understanding of PyTorch\'s `torch.utils.jit` module by converting and optimizing a PyTorch model using TorchScript. --- # Instructions You are given a simple feed-forward neural network implemented in PyTorch. Your task is to perform the following: 1. **Convert** the PyTorch model to TorchScript using `torch.utils.jit`. 2. **Optimize** the TorchScript model for performance. 3. Write a small inference function to **compare the performance** of the original PyTorch model and the optimized TorchScript model. # Provided Model Here is the provided PyTorch model: ```python import torch import torch.nn as nn class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.layer1 = nn.Linear(in_features=10, out_features=50) self.relu = nn.ReLU() self.layer2 = nn.Linear(in_features=50, out_features=2) def forward(self, x): x = self.layer1(x) x = self.relu(x) x = self.layer2(x) return x ``` # Your Task 1. **Convert the model**: - Convert the `SimpleNN` model to a TorchScript model using `torch.jit.script` or `torch.jit.trace`. 2. **Optimize the model**: - Use any available tools within `torch.utils.jit` to optimize this TorchScript model for better performance. 3. **Performance Comparison**: - Write a function to compare the inference time of the original PyTorch model and the optimized TorchScript model. - Perform multiple inference passes (e.g., 1000 passes) to get a reliable measure of performance. # Input and Output Formats - Input: A randomly generated input tensor of appropriate size (`torch.rand(10)`). - Output: Print the average inference times of both the original and the optimized models, clearly indicating which is which. --- # Constraints - The model should be loaded and handled correctly for both PyTorch and TorchScript. - Ensure that your code is robust and handles exceptions appropriately. - Provide brief comments explaining key steps in your code. # Expected Performance Requirements 1. The optimized TorchScript model should have a lower average inference time compared to the original PyTorch model when tested for a large number of passes. Use Python\'s `time` module to measure and compare the inference times effectively. --- # Submission Submit your Python code in a single script or Jupyter Notebook that performs the described tasks. Ensure your code is well-commented and follows best practices for readability and organization.","solution":"import torch import torch.nn as nn import time # Define the PyTorch model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.layer1 = nn.Linear(in_features=10, out_features=50) self.relu = nn.ReLU() self.layer2 = nn.Linear(in_features=50, out_features=2) def forward(self, x): x = self.layer1(x) x = self.relu(x) x = self.layer2(x) return x # Convert the model to TorchScript (using torch.jit.script) def convert_to_torchscript(model): scripted_model = torch.jit.script(model) return scripted_model # Function to compare performance of original and TorchScript model def compare_inference_time(model, scripted_model, input_tensor, num_iterations=1000): # Measure the time for the original PyTorch model start_time = time.time() for _ in range(num_iterations): _ = model(input_tensor) original_model_time = (time.time() - start_time) / num_iterations # Measure the time for the TorchScript model start_time = time.time() for _ in range(num_iterations): _ = scripted_model(input_tensor) scripted_model_time = (time.time() - start_time) / num_iterations print(f\\"Average inference time over {num_iterations} runs:\\") print(f\\"Original PyTorch model: {original_model_time:.6f} seconds\\") print(f\\"TorchScript model: {scripted_model_time:.6f} seconds\\") # Instantiate and prepare the model pytorch_model = SimpleNN() input_tensor = torch.rand(10) # Example input tensor # Convert the model to TorchScript torchscript_model = convert_to_torchscript(pytorch_model) # Compare the performance compare_inference_time(pytorch_model, torchscript_model, input_tensor, num_iterations=1000)"},{"question":"You are required to create a function that constructs a multipart email message with specific headers and payloads, and then serializes it into a string. Your function should demonstrate the ability to handle headers, manage payloads in a multipart message, and ensure proper serialization. # Function Signature ```python def create_multipart_email(headers: dict, parts: list, set_unixfrom: bool = False) -> str: Creates a multipart email message with the given headers and parts, and returns its string representation. Parameters: - headers (dict): A dictionary where keys are header field names and values are header field values. - parts (list): A list of dictionaries, where each dictionary represents a part of the multipart email. Each part dictionary should have the keys \'content\', \'content_type\', and optional \'headers\'. - \'content\': The payload of the part, which can be a string for text content. - \'content_type\': The MIME type of the content (e.g., \'text/plain\', \'text/html\'). - \'headers\': Optional dictionary of headers specific to this part. - set_unixfrom (bool): Whether to include the envelope header in the serialized string. Defaults to False. Returns: - str: The serialized string representation of the composed multipart email. pass ``` # Requirements 1. **Compose the Email Message**: - Create an `EmailMessage` object. - Set the provided headers on the message using the `headers` parameter. - Convert the message to a `multipart/mixed` type using `make_mixed()`. 2. **Add Parts to the Email**: - For each part in the `parts` list, create a new `EmailMessage` object. - Set the content and content type of the part. - Optionally set headers specific to this part. - Attach the part to the main message using `add_attachment`. 3. **Serialization**: - Return the string representation of the whole email message using the `as_string` method. - Ensure to include the `unixfrom` parameter based on the `set_unixfrom` parameter. # Example ```python headers = { \\"Subject\\": \\"Meeting Invitation\\", \\"From\\": \\"sender@example.com\\", \\"To\\": \\"recipient@example.com\\" } parts = [ { \\"content\\": \\"This is the plain text part of the email.\\", \\"content_type\\": \\"text/plain\\" }, { \\"content\\": \\"<html><body>This is the HTML part of the email.</body></html>\\", \\"content_type\\": \\"text/html\\", \\"headers\\": {\\"Content-Disposition\\": \\"inline\\"} } ] result = create_multipart_email(headers, parts, set_unixfrom=True) print(result) ``` This example should create a multipart email message with a plain text part and an HTML part, then serialize it, and print the string representation. # Constraints - Focus on handling the headers and payload manipulation accurately. - Maintain the order of the headers as provided. - Assume well-formed input data (no need for extensive validation). This question tests your understanding of the `email.message.EmailMessage` class, including handling multipart messages, setting headers and payloads, and serialization.","solution":"from email.message import EmailMessage def create_multipart_email(headers: dict, parts: list, set_unixfrom: bool = False) -> str: Creates a multipart email message with the given headers and parts, and returns its string representation. Parameters: - headers (dict): A dictionary where keys are header field names and values are header field values. - parts (list): A list of dictionaries, where each dictionary represents a part of the multipart email. Each part dictionary should have the keys \'content\', \'content_type\', and optional \'headers\'. - \'content\': The payload of the part, which can be a string for text content. - \'content_type\': The MIME type of the content (e.g., \'text/plain\', \'text/html\'). - \'headers\': Optional dictionary of headers specific to this part. - set_unixfrom (bool): Whether to include the envelope header in the serialized string. Defaults to False. Returns: - str: The serialized string representation of the composed multipart email. # Create the main multipart email message msg = EmailMessage() msg.make_mixed() # Set the provided headers for key, value in headers.items(): msg[key] = value # Add each part to the main message for part in parts: sub_msg = EmailMessage() sub_msg.set_content(part[\'content\'], subtype=part[\'content_type\'].split(\'/\')[1]) # Set optional headers for the part if \'headers\' in part: for key, value in part[\'headers\'].items(): sub_msg[key] = value # Attach the part to the main message msg.add_attachment(sub_msg) # Return the serialized string representation of the email return msg.as_string(unixfrom=set_unixfrom)"},{"question":"Objective Implement a function to automate the creation and customization of source distributions for a hypothetical Python package. Problem Statement You are provided with a list of file paths and specific rules on which files to include or exclude in a source distribution. Implement a Python function `create_manifest(file_paths, include_patterns, exclude_patterns)` that generates a `MANIFEST.in` file according to the provided specifications. Function Signature ```python def create_manifest(file_paths: list, include_patterns: list, exclude_patterns: list) -> str: pass ``` Input 1. `file_paths` (List[str]): A list of file paths available in the project. 2. `include_patterns` (List[str]): A list of patterns specifying files to be included in the distribution. 3. `exclude_patterns` (List[str]): A list of patterns specifying files to be excluded from the distribution. Output - Returns a string representing the contents of `MANIFEST.in` based on the provided paths and patterns. Constraints - Each pattern in `include_patterns` and `exclude_patterns` follows Unix shell-style wildcards (e.g., `*.py`, `docs/*.txt`). - It should handle redundant inclusions and exclusions appropriately (i.e., an excluded file should not be in the final manifest even if it matches an inclusion pattern). Example ```python file_paths = [ \\"setup.py\\", \\"README.md\\", \\"docs/index.txt\\", \\"docs/tutorial.py\\", \\"src/module.py\\", \\"src/module.c\\", \\"tests/test_module.py\\" ] include_patterns = [ \\"*.py\\", \\"README.md\\", \\"docs/*.txt\\" ] exclude_patterns = [ \\"tests/*.py\\", \\"src/*.c\\" ] # Example call manifest_content = create_manifest(file_paths, include_patterns, exclude_patterns) print(manifest_content) ``` The above function call should output: ```plaintext include *.py include README.md include docs/*.txt exclude tests/*.py exclude src/*.c ``` Notes - You do not need to actually create the source distribution or handle file I/O beyond returning the string for `MANIFEST.in`. - The order of includes and excludes should follow the order in which patterns are provided in the lists. Evaluation The solution will be evaluated based on: 1. Correctness: Generating the appropriate `MANIFEST.in` content. 2. Handling of edge cases such as overlapping patterns. 3. Efficiency in processing the file paths and patterns.","solution":"def create_manifest(file_paths, include_patterns, exclude_patterns): Generates the content for a MANIFEST.in file based on include and exclude patterns. Args: file_paths (list): List of available file paths in the project. include_patterns (list): List of patterns specifying files to be included in the distribution. exclude_patterns (list): List of patterns specifying files to be excluded from the distribution. Returns: str: Content for MANIFEST.in file. manifest_lines = [] included_files = set() # Apply include patterns for pattern in include_patterns: manifest_lines.append(f\'include {pattern}\') # Apply exclude patterns for pattern in exclude_patterns: manifest_lines.append(f\'exclude {pattern}\') return \'n\'.join(manifest_lines)"},{"question":"**Coding Assessment Question** # Objective Implement a simplified version of an `ElasticAgent` that can manage the lifecycle of worker processes. This exercise will assess the comprehension of process management, state tracking, and resource coordination in a distributed environment. # Task 1. Implement a class `SimpleElasticAgent` that, at a minimum, provides the following functionalities: - Managing the lifecycle of a group of workers (`start_workers`, `stop_workers`). - Tracking the state of each worker within a `WorkerGroup`. - Simple logging capability for state transitions. 2. Implement a class `Worker` to represent an individual worker process. 3. Implement a class `WorkerGroup` to manage a collection of `Worker` instances. # Requirements - **SimpleElasticAgent**: - `__init__(self, worker_spec: WorkerSpec)`: Initialize the agent with worker specifications. - `start_workers(self) -> None`: Launches all worker processes. - `stop_workers(self) -> None`: Terminates all worker processes. - `get_worker_states(self) -> List[WorkerState]`: Returns the state of each worker. - `log_state(self) -> None`: Logs the current state of all workers. - **Worker**: - `__init__(self, id: str)`: Initialize the worker with a unique identifier. - `start(self) -> None`: Simulates starting the worker. - `stop(self) -> None`: Simulates stopping the worker. - `state(self) -> WorkerState`: Returns the current state of the worker. - **WorkerGroup**: - `__init__(self, workers: List[Worker])`: Initialize the group with a list of workers. - `start_all(self) -> None`: Starts all workers in the group. - `stop_all(self) -> None`: Stops all workers in the group. - `get_all_states(self) -> List[WorkerState]`: Returns the state of all workers. # Constraints - Use simple print statements for logging. - No need for actual process management; simulate with start/stop state changes. - Worker states can be represented as simple strings: `[\\"RUNNING\\", \\"STOPPED\\"]`. # Example Usage ```python class WorkerState: RUNNING = \\"RUNNING\\" STOPPED = \\"STOPPED\\" class WorkerSpec: def __init__(self, num_workers: int): self.num_workers = num_workers self.worker_ids = [f\\"worker_{i}\\" for i in range(num_workers)] # Implementation classes (to be completed by the student) # Example initialization and usage: spec = WorkerSpec(num_workers=3) agent = SimpleElasticAgent(worker_spec=spec) agent.start_workers() print(agent.get_worker_states()) agent.log_state() agent.stop_workers() print(agent.get_worker_states()) agent.log_state() ``` # Evaluation The solution will be evaluated based on: - Correct implementation of the `SimpleElasticAgent`, `Worker`, and `WorkerGroup` classes. - Ability to manage worker states correctly. - Proper usage of the `WorkerSpec` class for initializing workers. - Clarity and readability of the code.","solution":"class WorkerState: RUNNING = \\"RUNNING\\" STOPPED = \\"STOPPED\\" class WorkerSpec: def __init__(self, num_workers: int): self.num_workers = num_workers self.worker_ids = [f\\"worker_{i}\\" for i in range(num_workers)] class Worker: def __init__(self, id: str): self.id = id self._state = WorkerState.STOPPED def start(self) -> None: self._state = WorkerState.RUNNING def stop(self) -> None: self._state = WorkerState.STOPPED def state(self) -> str: return self._state class WorkerGroup: def __init__(self, workers: list): self.workers = workers def start_all(self) -> None: for worker in self.workers: worker.start() def stop_all(self) -> None: for worker in self.workers: worker.stop() def get_all_states(self) -> list: return [worker.state() for worker in self.workers] class SimpleElasticAgent: def __init__(self, worker_spec: WorkerSpec): self.worker_group = WorkerGroup([Worker(worker_id) for worker_id in worker_spec.worker_ids]) def start_workers(self) -> None: self.worker_group.start_all() def stop_workers(self) -> None: self.worker_group.stop_all() def get_worker_states(self) -> list: return self.worker_group.get_all_states() def log_state(self) -> None: states = self.get_worker_states() for idx, state in enumerate(states): print(f\\"Worker {idx}: {state}\\")"},{"question":"# XML Element Counting and Summarization In this coding challenge, you will create a Python script that uses the `xml.etree.ElementTree` module to parse an XML document and perform operations to summarize specific information. # Task Description: 1. **XML Parsing**: - Write a function `parse_xml_string(xml_string: str) -> ElementTree.Element` that accepts an XML string and returns the root element of the parsed XML. 2. **Element Counting**: - Write a function `count_elements(root: ElementTree.Element, tag: str) -> int` that takes in the root element of an XML tree and a tag name. This function should return the count of all elements in the subtree that have the given tag name. 3. **Element Summarization**: - Write a function `summarize_elements(root: ElementTree.Element, tag: str, attr: str) -> int` that takes in the root element of an XML tree, a tag name, and an attribute name. This function should return the sum of the integer values of the specified attribute for all elements with the specified tag in the subtree. 4. **Element Modification**: - Write a function `increase_attribute_values(root: ElementTree.Element, tag: str, attr: str, increment: int) -> None` that takes in the root element of an XML tree, a tag name, an attribute name, and an integer increment value. This function should increase the specified attribute\'s value by the given increment for all elements with the specified tag. 5. **XML Output**: - Write a function `convert_to_string(root: ElementTree.Element) -> str` that takes in the root element and returns the string representation of the modified XML document. # Input and Output Formats: - The input XML string will be well-formed and adhere to standard XML syntax. - The attribute values for summarization and modification will always be integers. - The output string should be a well-formed XML string. # Example: Given the XML: ```xml <data> <item value=\\"10\\"/> <item value=\\"20\\"/> <item value=\\"30\\"/> <extraItem value=\\"40\\"/> </data> ``` For example, if you invoke: ```python xml_string = \'\'\'<data> <item value=\\"10\\"/> <item value=\\"20\\"/> <item value=\\"30\\"/> <extraItem value=\\"40\\"/> </data>\'\'\' root = parse_xml_string(xml_string) print(count_elements(root, \'item\')) # Output: 3 print(summarize_elements(root, \'item\', \'value\')) # Output: 60 increase_attribute_values(root, \'item\', \'value\', 5) print(convert_to_string(root)) # Output: # <data> # <item value=\\"15\\"/> # <item value=\\"25\\"/> # <item value=\\"35\\"/> # <extraItem value=\\"40\\"/> # </data> ``` # Constraints: - You must use the `xml.etree.ElementTree` module for all XML parsing and manipulations. - Ensure that your implementation handles the XML input efficiently and correctly, especially when dealing with large XML documents. # Submission: Provide the implementation for the following functions: - `parse_xml_string(xml_string: str) -> ElementTree.Element` - `count_elements(root: ElementTree.Element, tag: str) -> int` - `summarize_elements(root: ElementTree.Element, tag: str, attr: str) -> int` - `increase_attribute_values(root: ElementTree.Element, tag: str, attr: str, increment: int) -> None` - `convert_to_string(root: ElementTree.Element) -> str`","solution":"import xml.etree.ElementTree as ET def parse_xml_string(xml_string: str) -> ET.Element: root = ET.fromstring(xml_string) return root def count_elements(root: ET.Element, tag: str) -> int: return len(root.findall(f\'.//{tag}\')) def summarize_elements(root: ET.Element, tag: str, attr: str) -> int: return sum(int(element.get(attr, 0)) for element in root.findall(f\'.//{tag}\')) def increase_attribute_values(root: ET.Element, tag: str, attr: str, increment: int) -> None: for element in root.findall(f\'.//{tag}\'): current_value = int(element.get(attr, 0)) element.set(attr, str(current_value + increment)) def convert_to_string(root: ET.Element) -> str: return ET.tostring(root, encoding=\'unicode\').strip()"},{"question":"Objective Implement an asynchronous producer-consumer model using `asyncio.Queue`. Your solution will need to create multiple producers and consumers, perform queue operations asynchronously, and handle the completion of all tasks. Problem Statement You need to implement a system where there are multiple producers generating tasks and putting them into a shared queue and multiple consumers processing these tasks. Each task will have a random execution time between 0.1 and 2.0 seconds. Use the `asyncio.Queue` class to implement this model. Below are the requirements: 1. **Producers**: - You need to create `p` producers. - Each producer will generate `n` tasks where each task is a random float between 0.1 and 2.0 seconds. - Each producer will put these tasks asynchronously into the queue. 2. **Consumers**: - You need to create `c` consumers. - Each consumer will fetch tasks from the queue and simulate processing them by sleeping for the duration of the task. - After processing each task, the consumer will mark the task as done by calling `queue.task_done()`. 3. **Main Logic**: - Ensure that all tasks are processed. - Print the total time taken for all consumers to process the tasks. - Handle the queue operations without causing deadlocks or race conditions. Input - Number of producers `p` (an integer > 0) - Number of consumers `c` (an integer > 0) - Number of tasks per producer `n` (an integer > 0) Output - Print the total time taken for all consumers to process all the tasks. Implementation Constraints - Use `asyncio.Queue` for implementing the queue. - Ensure that your solution is thread-safe and handles any potential exceptions inside the asyncio task routine. - Optimize the solution to handle scenarios where `p`, `c`, and `n` are reasonably large (e.g., up to 1000). Example ```python import asyncio import random import time async def producer(queue, n): for _ in range(n): task_duration = random.uniform(0.1, 2.0) await queue.put(task_duration) async def consumer(queue, name): while True: task_duration = await queue.get() await asyncio.sleep(task_duration) queue.task_done() print(f\\"{name} processed task for {task_duration:.2f} seconds\\") async def main(p, c, n): queue = asyncio.Queue() producers = [asyncio.create_task(producer(queue, n)) for _ in range(p)] consumers = [asyncio.create_task(consumer(queue, f\'consumer-{i}\')) for i in range(c)] await asyncio.gather(*producers) await queue.join() total_time_taken = time.perf_counter() - start_time for consumer in consumers: consumer.cancel() await asyncio.gather(*consumers, return_exceptions=True) print(f\'Total time taken: {total_time_taken:.2f} seconds\') if __name__ == \\"__main__\\": p = int(input(\\"Enter number of producers: \\")) c = int(input(\\"Enter number of consumers: \\")) n = int(input(\\"Enter number of tasks per producer: \\")) start_time = time.perf_counter() asyncio.run(main(p, c, n)) ``` Assessment Criteria - Correct usage of `asyncio.Queue` and its related methods. - Ability to handle asynchronous operations and ensure proper synchronization. - Correct handling of tasks and the output of the required total processing time. - Code readability, commenting, and proper structuring.","solution":"import asyncio import random import time async def producer(queue, n): Async producer that generates n tasks with random durations between 0.1 to 2.0 seconds and puts them into the given queue. for _ in range(n): task_duration = random.uniform(0.1, 2.0) await queue.put(task_duration) async def consumer(queue, name): Async consumer that processes tasks from the queue. It simulates task processing by sleeping for the task duration and then marks the task as done. while True: task_duration = await queue.get() await asyncio.sleep(task_duration) queue.task_done() print(f\\"{name} processed task for {task_duration:.2f} seconds\\") async def main(p, c, n): Main function that sets up p producers and c consumers, monitors the task queue, and times the total processing duration. queue = asyncio.Queue() # Create producer tasks producers = [asyncio.create_task(producer(queue, n)) for _ in range(p)] # Create consumer tasks consumers = [asyncio.create_task(consumer(queue, f\'consumer-{i}\')) for i in range(c)] # Measure start time start_time = time.perf_counter() # Wait until all producers are done putting tasks into the queue await asyncio.gather(*producers) # Wait until the queue is fully processed await queue.join() # Calculate total time taken total_time_taken = time.perf_counter() - start_time for consumer_task in consumers: consumer_task.cancel() # Ensure all consumer tasks are gracefully closed await asyncio.gather(*consumers, return_exceptions=True) print(f\'Total time taken: {total_time_taken:.2f} seconds\') if __name__ == \\"__main__\\": p = int(input(\\"Enter number of producers: \\")) c = int(input(\\"Enter number of consumers: \\")) n = int(input(\\"Enter number of tasks per producer: \\")) asyncio.run(main(p, c, n))"},{"question":"**Objective**: Implement a function to transform a given text into an HTML-safe format and then revert it back, ensuring consistency in conversions. **Problem Statement**: Write a function `transform_html(text: str, quote: bool = True) -> bool` that performs the following operations: 1. Use the `html.escape` function to convert the characters \'&\', \'<\', \'>\', and optionally (\'\\"\', \\"\'\\") in the given text to their HTML-safe sequences. 2. Use the `html.unescape` function to revert the escaped text back to its original form. 3. The function should return `True` if the original text and the reverted text are identical; otherwise, return `False`. # Input: - `text` (str): The input text that needs to be transformed. - `quote` (bool): Optional parameter to indicate whether to also escape double and single quotes. Defaults to `True`. # Output: - A boolean value (`True` or `False`). # Constraints: 1. The input text `text` will have a maximum length of `10^5` characters. 2. The `quote` parameter, if provided, will always be a boolean value. # Example: ```python assert transform_html(\\"Hello & Welcome!\\", quote=True) == True assert transform_html(\\"5 < 10 && 10 > 5\\", quote=True) == True assert transform_html(\\"This is a test string.\\", quote=False) == True assert transform_html(\\"She wrote \'Hello!\'\\", quote=True) == True assert transform_html(\\"She wrote \'Hello!\'\\", quote=False) == False ``` Note: - Ensure that the function handles large input sizes efficiently. - Consider edge cases such as empty strings or strings without any escapable characters. # Function Signature: ```python def transform_html(text: str, quote: bool = True) -> bool: # Implementation here ```","solution":"import html def transform_html(text: str, quote: bool = True) -> bool: Transforms the given text into an HTML-safe format and reverts it back. Returns True if the original text and the reverted text are identical; otherwise, returns False. # Escape and then unescape the text escaped_text = html.escape(text, quote=quote) unescaped_text = html.unescape(escaped_text) # Return True if original text and unescaped text are identical return text == unescaped_text"},{"question":"**Question: Applying PCA with scikit-learn** **Objective:** Your task is to implement a function that performs Principal Component Analysis (PCA) on a given dataset using scikit-learn. The function should be able to handle both full batch processing and incremental processing for large datasets. Additionally, you need to analyze the variance explained by the principal components and project the data onto the first two principal components. **Function Signature:** ```python def apply_pca(data: np.ndarray, n_components: int, solver: str = \'auto\', batch_size: int = None) -> dict: Performs PCA on the given dataset and returns the results. Parameters: data (np.ndarray): A 2D numpy array where rows represent samples and columns represent features. n_components (int): The number of principal components to compute. solver (str): The solver to use for PCA (\'full\', \'randomized\', \'auto\'). Default is \'auto\'. batch_size (int): The size of the mini-batches for incremental PCA. If None, full batch PCA is performed. Returns: dict: A dictionary containing the following keys: - \'explained_variance_ratio\': The amount of variance explained by each of the selected components. - \'components\': The principal components. - \'projected_data\': The data projected onto the first two principal components. ``` **Input:** - `data`: A 2D numpy array with shape (n_samples, n_features). - `n_components`: An integer specifying the number of principal components to compute. - `solver`: A string specifying the solver to use for PCA. Options are \'full\' for full SVD, \'randomized\' for randomized SVD, and \'auto\' to let the algorithm decide the best solver. Default is \'auto\'. - `batch_size`: An integer specifying the size of the mini-batches for incremental PCA. If `batch_size` is None, full batch PCA is performed. **Output:** - A dictionary with the following keys: - `explained_variance_ratio`: A list of floats representing the amount of variance explained by each of the selected components. - `components`: A 2D numpy array with shape (n_components, n_features) representing the principal components. - `projected_data`: A 2D numpy array with shape (n_samples, 2) representing the data projected onto the first two principal components. **Constraints:** - The number of components `n_components` should be less than or equal to the number of features in the dataset. - The dataset `data` should not contain any missing values. **Requirements:** 1. If `batch_size` is provided, use `IncrementalPCA` to handle the data in mini-batches; otherwise, use standard `PCA`. 2. Ensure the data is centered before applying PCA. 3. Use `fit` and `transform` methods appropriately depending on the solver and batching strategy. 4. Compute the explained variance ratio and project the data on the first two components. **Example Usage:** ```python import numpy as np # Generate a synthetic dataset np.random.seed(0) data = np.random.rand(100, 5) # 100 samples, 5 features # Perform PCA results = apply_pca(data, n_components=3, solver=\'randomized\', batch_size=20) print(\\"Explained Variance Ratio:\\", results[\'explained_variance_ratio\']) print(\\"Principal Components:n\\", results[\'components\']) print(\\"Projected Data:n\\", results[\'projected_data\']) ``` **Additional Notes:** - Ensure the implementation is efficient and can handle large datasets by using appropriate solvers and batching strategies. - The projected data should retain the structure of the original data with reduced dimensions, which can be useful for visualization and further analysis.","solution":"import numpy as np from sklearn.decomposition import PCA, IncrementalPCA def apply_pca(data: np.ndarray, n_components: int, solver: str = \'auto\', batch_size: int = None) -> dict: Performs PCA on the given dataset and returns the results. Parameters: data (np.ndarray): A 2D numpy array where rows represent samples and columns represent features. n_components (int): The number of principal components to compute. solver (str): The solver to use for PCA (\'full\', \'randomized\', \'auto\'). Default is \'auto\'. batch_size (int): The size of the mini-batches for incremental PCA. If None, full batch PCA is performed. Returns: dict: A dictionary containing the following keys: - \'explained_variance_ratio\': The amount of variance explained by each of the selected components. - \'components\': The principal components. - \'projected_data\': The data projected onto the first two principal components. if batch_size is None: # Full batch PCA pca = PCA(n_components=n_components, svd_solver=solver if solver != \'auto\' else \'auto\') else: # Incremental PCA pca = IncrementalPCA(n_components=n_components, batch_size=batch_size) if batch_size is None: pca.fit(data) else: n_samples = data.shape[0] for i in range(0, n_samples, batch_size): batch_data = data[i:i + batch_size] pca.partial_fit(batch_data) transformed_data = pca.transform(data) explained_variance_ratio = pca.explained_variance_ratio_ components = pca.components_ projected_data = transformed_data[:, :2] return { \'explained_variance_ratio\': explained_variance_ratio, \'components\': components, \'projected_data\': projected_data }"},{"question":"**Objective:** Create a plot using the Seaborn library that visualizes the relationship between two chosen variables on a dataset. Customize the plot using various features such as annotations, text formatting, and color mapping. **Dataset:** You will use the \\"mpg\\" dataset, which is available within seaborn. This dataset contains information on fuel efficiency of cars, with relevant columns including \'mpg\', \'horsepower\', \'weight\', \'origin\', and \'name\'. **Task:** 1. Load the \\"mpg\\" dataset using `seaborn.load_dataset`. 2. Create a scatter plot that shows the relationship between \'horsepower\' (x-axis) and \'mpg\' (y-axis), with colors mapped to \'origin\' (categorical variable). 3. Annotate the plot with the \'name\' of the cars at their respective (x, y) positions. 4. Customize the annotations: - Align the text right for cars with \'origin\' as \'usa\'. - Align the text left for cars with \'origin\' as \'europe\'. - Align the text center for cars with \'origin\' as \'japan\'. - Use bold font for the text annotations. **Input:** The function should not take any input parameters. All operations should be performed within the function without external data input. **Output:** A matplotlib Axes object containing the customized scatter plot. **Constraints:** - You must use `seaborn.objects` module for creating the plot and adding annotations. - Ensure that the plot and text annotations are properly aligned as specified. - Use appropriate seaborn and matplotlib methods for customization. **Performance Requirements:** The function should execute within a reasonable time frame, as the dataset is small. **Function Signature:** ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_customized_mpg_plot(): # Load the dataset # Create and customize the plot # Return the Axes object ``` **Example Output:** The function should create a scatter plot similar to the one depicted below, with appropriate text annotations and customizations. ```python # Example of calling the function ax = create_customized_mpg_plot() plt.show() ``` **Additional Notes:** - Ensure to test your function to verify that the customizations are applied correctly. - Refer to the provided seaborn documentation for syntax and methods of `seaborn.objects`.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_customized_mpg_plot(): # Load the dataset mpg = sns.load_dataset(\'mpg\') # Create the scatter plot fig, ax = plt.subplots(figsize=(12, 8)) scatter = sns.scatterplot(data=mpg, x=\'horsepower\', y=\'mpg\', hue=\'origin\', palette=\'Set1\', ax=ax) # Customize annotations for i, point in mpg.iterrows(): if point[\'origin\'] == \'usa\': align = \'right\' elif point[\'origin\'] == \'europe\': align = \'left\' else: align = \'center\' ax.text(point[\'horsepower\'], point[\'mpg\'], str(point[\'name\']), fontsize=8, ha=align, weight=\'bold\') ax.set_title(\'Horsepower vs. MPG with Car Names Annotated\', fontsize=16) ax.set_xlabel(\'Horsepower\', fontsize=14) ax.set_ylabel(\'MPG\', fontsize=14) plt.legend(title=\'Origin\', title_fontsize=\'13\', loc=\'upper right\') return ax"},{"question":"Objective: Assess your knowledge of seaborn\'s `relplot`, `scatterplot`, and `lineplot` functions by creating a comprehensive visualization of a dataset that includes complex relationships between multiple variables. Dataset Description: You will use the fmri dataset which can be loaded using `sns.load_dataset(\\"fmri\\")`. This dataset contains the following columns: - `subject`: Identifier of the subject. - `timepoint`: Time point at which the signal was recorded. - `event`: Event type. - `region`: Brain region. - `signal`: Signal recorded. Task: 1. **Line Plot with Facets**: - Create a line plot using `relplot` to show the relationship between `timepoint` and `signal`. - Use `hue` to differentiate between events and `style` to differentiate between regions. - Facet the plot by `subject` with wrapped columns (5 columns). - Customize the plot by setting the height of each facet to 3, aspect ratio to 0.75, and line width to 2.5. 2. **Scatter Plot for Comparison**: - Create a scatter plot using `relplot` to compare the `signal` values at different `timepoints` for each `event`. - Use `hue` to show `event` and `size` to represent the `region`. - Additionally, add a customized palette and limit the point sizes to a reasonable range (e.g., 15 to 200). 3. **Analysis**: - Write a brief conclusion (2-3 sentences) on the insights obtained from the visualizations created. Constraints: - Ensure each plot is self-contained and clearly labeled. - Use appropriate color palettes to ensure the plots are accessible. - Optimize performance by ensuring efficient data handling. Expected Output: - A line plot with facets as described. - A scatter plot comparing signal values. - A brief written analysis. Example (For reference only): ```python import seaborn as sns import matplotlib.pyplot as plt from matplotlib.colors import LogNorm # Load the dataset fmri = sns.load_dataset(\\"fmri\\") # Line Plot with Facets sns.relplot( data=fmri, kind=\\"line\\", x=\\"timepoint\\", y=\\"signal\\", hue=\\"event\\", style=\\"region\\", col=\\"subject\\", col_wrap=5, height=3, aspect=0.75, linewidth=2.5 ) plt.show() # Scatter Plot for Comparison palette = sns.cubehelix_palette(light=.7, n_colors=6) sns.relplot( data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"event\\", size=\\"region\\", sizes=(15, 200), palette=palette ) plt.show() # Analysis # The line plots with facets reveal distinct patterns in the signal variation over time across different subjects. # The comparison scatter plot highlights the differences in signal strength between various events and regions at different time points. ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset fmri = sns.load_dataset(\\"fmri\\") def create_line_plot(): # Line Plot with Facets sns.relplot( data=fmri, kind=\\"line\\", x=\\"timepoint\\", y=\\"signal\\", hue=\\"event\\", style=\\"region\\", col=\\"subject\\", col_wrap=5, height=3, aspect=0.75, linewidth=2.5 ) plt.show() def create_scatter_plot(): # Scatter Plot for Comparison palette = sns.cubehelix_palette(light=.7, n_colors=6) sns.relplot( data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"event\\", size=\\"region\\", sizes=(15, 200), palette=palette ) plt.show() # Analysis analysis_conclusion = The line plots with facets reveal distinct patterns in the signal variation over time across different subjects. The comparison scatter plot highlights the differences in signal strength between various events and regions at different time points. if __name__ == \\"__main__\\": create_line_plot() create_scatter_plot() print(analysis_conclusion)"},{"question":"**Coding Assessment Question** # Objective: Create a comprehensive plot using Seaborn\'s `so.Plot` module to visualize specific data from the \\"penguins\\" dataset. # Problem Statement: You are required to create a plot that visualizes the relationship between the species of penguins and their body mass. Additionally, ensure that overlapping data points are managed for better clarity and meaningful interpretation. Your plot should also include a visual representation of the interquartile range (IQR) for body mass across different species. # Instructions: 1. Load the \\"penguins\\" dataset using Seaborn\'s `load_dataset` function. 2. Create a plot using `so.Plot`, specifying \\"species\\" as the x-variable and \\"body_mass_g\\" as the y-variable. 3. Add dots to the plot using `so.Dots()` and use `so.Jitter()` to manage overlapping data points. 4. Include a visual representation of the IQR (25th to 75th percentile) for body mass across different species using `so.Range()` and `so.Perc([25, 75])`. 5. Shift the IQR representation slightly to the right using `so.Shift(x=0.2)` to avoid overlap with the data points. # Expected Input and Output: - **Input**: There is no explicit input from the user; instead, you need to load and use the \\"penguins\\" dataset. - **Output**: A plot visualizing the specified relationships with layered elements as described. # Constraints: - You must use the Seaborn `so.Plot` module and the specified methods: `add`, `Dots`, `Jitter`, `Range`, `Perc`, and `Shift`. - Ensure that your code is clean and well-documented. # Example Code: Here is an example to help you get started: ```python import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot plot = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2)) ) # Display the plot plot.show() ``` # Notes: - Ensure your final plot appropriately visualizes the relationship between penguin species and their body mass, with clearly distinguishable data points and IQR representation.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_penguin_data(): This function creates a plot to visualize the relationship between penguin species and their body mass. The plot includes dots for individual data points and a visual representation of the interquartile range (IQR) for body mass across different species. # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot plot = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2)) ) # Display the plot plot.show()"},{"question":"**Title: Implementing Custom MIME Types with `email.mime` Module** **Objective:** Your task is to create a function that constructs custom MIME messages using the `email.mime` Python package. This function will demonstrate your understanding of constructing and manipulating MIME objects. **Problem Statement:** You are required to create a function `construct_custom_mime_message(major_type, minor_type, payload, filename=None)` that constructs and returns a MIME message object based on the given parameters. The function should handle MIME types such as text, image, audio, and applications correctly, encoding the payload properly based on the type. **Function Signature:** ```python def construct_custom_mime_message(major_type: str, minor_type: str, payload: bytes, filename: str = None) -> email.message.Message: ``` # Input: - `major_type` (str): The major MIME type (e.g., \'text\', \'image\', \'audio\', \'application\'). - `minor_type` (str): The minor MIME type (e.g., \'plain\', \'png\', \'mp3\', \'octet-stream\'). - `payload` (bytes): The raw data to be included in the MIME object. - `filename` (str, optional): The filename related to the MIME content, if applicable. # Output: - Returns an instance of `email.message.Message` which is the constructed MIME object. # Constraints: - The `major_type` must be one of [\'text\', \'image\', \'audio\', \'application\']. - The function should encode the payload correctly for its MIME type using the appropriate default encoders (e.g., Base64). - If the MIME type is \'application\' and the filename is provided, the filename should be included as a parameter in the `Content-Disposition` header. # Example: ```python from email import message_from_binary_file # Example 1: msg = construct_custom_mime_message(\'text\', \'plain\', b\'Hello, this is a plain text email payload!\') print(msg.as_string()) # Example 2: with open(\'example.png\', \'rb\') as img_file: img_data = img_file.read() msg = construct_custom_mime_message(\'image\', \'png\', img_data, filename=\'example.png\') print(msg.as_string()) ``` # Additional Notes: - You should use appropriate subclasses (`MIMEText`, `MIMEImage`, `MIMEAudio`, `MIMEApplication`) based on the `major_type` and `minor_type`. - Ensure that the correct headers are added to the message object, such as `Content-Type`, `MIME-Version`, and `Content-Disposition` (if applicable). Use the provided documentation details to implement the required functionality effectively.","solution":"from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.audio import MIMEAudio from email.mime.application import MIMEApplication from email.mime.base import MIMEBase from email import encoders def construct_custom_mime_message(major_type: str, minor_type: str, payload: bytes, filename: str = None): if major_type == \'text\': msg = MIMEText(payload.decode(), _subtype=minor_type) elif major_type == \'image\': msg = MIMEImage(payload, _subtype=minor_type) elif major_type == \'audio\': msg = MIMEAudio(payload, _subtype=minor_type) elif major_type == \'application\': msg = MIMEApplication(payload, _subtype=minor_type) if filename: msg.add_header(\'Content-Disposition\', \'attachment\', filename=filename) else: raise ValueError(\\"Unsupported major MIME type\\") return msg"},{"question":"# Custom JSON Encoder and Decoder **Objective**: Implement a custom JSON Encoder and Decoder for handling a complex data type, ensuring students understand both the fundamental and advanced features of Python\'s JSON module. **Problem Statement**: 1. You are required to create a custom Python class `Book` with the following attributes: - `isbn` (a string representing the book\'s ISBN) - `title` (a string representing the book\'s title) - `author` (a string representing the author\'s name) - `published_date` (a string representing the date of publication in the format YYYY-MM-DD) 2. Implement a custom `BookEncoder` by subclassing `json.JSONEncoder` to handle instances of the `Book` class. Ensure that instances of the `Book` class are serialized to JSON strings in the following format: ```json { \\"isbn\\": \\"978-3-16-148410-0\\", \\"title\\": \\"Example Book Title\\", \\"author\\": \\"John Doe\\", \\"published_date\\": \\"2023-01-01\\", \\"__type__\\": \\"Book\\" } ``` The `__type__` field should be used to identify that this is a `Book` object. 3. Implement a custom decoding function `book_decoder` to convert JSON strings back into instances of the `Book` class during deserialization. **Function Signatures**: ```python import json from typing import Any class Book: def __init__(self, isbn: str, title: str, author: str, published_date: str): self.isbn = isbn self.title = title self.author = author self.published_date = published_date class BookEncoder(json.JSONEncoder): def default(self, obj: Any) -> Any: if isinstance(obj, Book): return { \\"isbn\\": obj.isbn, \\"title\\": obj.title, \\"author\\": obj.author, \\"published_date\\": obj.published_date, \\"__type__\\": \\"Book\\" } return super().default(obj) def book_decoder(dct: dict) -> Any: if dct.get(\\"__type__\\") == \\"Book\\": return Book(dct[\\"isbn\\"], dct[\\"title\\"], dct[\\"author\\"], dct[\\"published_date\\"]) return dct ``` **Requirements**: 1. Implement the `Book` class. 2. Implement the `BookEncoder` class. 3. Implement the `book_decoder` function. 4. Write test cases to verify the encoding and decoding of `Book` instances using your custom encoder and decoder. **Constraints**: - Ensure your solution adheres to the specified JSON format for `Book` objects. - Your encoder should handle generic Python objects by falling back to the default JSON encoder if the object is not of type `Book`. **Example**: ```python if __name__ == \\"__main__\\": book = Book(\\"978-3-16-148410-0\\", \\"Example Book Title\\", \\"John Doe\\", \\"2023-01-01\\") encoded_book = json.dumps(book, cls=BookEncoder) print(encoded_book) decoded_book = json.loads(encoded_book, object_hook=book_decoder) print(decoded_book.__dict__) ``` The expected output should be: ```json {\\"isbn\\": \\"978-3-16-148410-0\\", \\"title\\": \\"Example Book Title\\", \\"author\\": \\"John Doe\\", \\"published_date\\": \\"2023-01-01\\", \\"__type__\\": \\"Book\\"} ``` And the printed `decoded_book.__dict__` should display: ```python {\'isbn\': \'978-3-16-148410-0\', \'title\': \'Example Book Title\', \'author\': \'John Doe\', \'published_date\': \'2023-01-01\'} ```","solution":"import json from typing import Any class Book: def __init__(self, isbn: str, title: str, author: str, published_date: str): self.isbn = isbn self.title = title self.author = author self.published_date = published_date class BookEncoder(json.JSONEncoder): def default(self, obj: Any) -> Any: if isinstance(obj, Book): return { \\"isbn\\": obj.isbn, \\"title\\": obj.title, \\"author\\": obj.author, \\"published_date\\": obj.published_date, \\"__type__\\": \\"Book\\" } return super().default(obj) def book_decoder(dct: dict) -> Any: if dct.get(\\"__type__\\") == \\"Book\\": return Book(dct[\\"isbn\\"], dct[\\"title\\"], dct[\\"author\\"], dct[\\"published_date\\"]) return dct"},{"question":"**Question: Implement a Custom Bound Method Creation** You are tasked with implementing a custom function in Python that mirrors some functionalities of the `PyMethod_New` and `PyInstanceMethod_New` described in the provided documentation. Specifically, you will create a function `create_bound_method` that takes a function and an instance, and returns a method that is bound to the given instance. # Function Signature ```python def create_bound_method(func, instance): pass ``` # Input - `func`: A callable object (function) that we want to bind to an instance. - `instance`: The instance to which the method should be bound. # Output - Returns a method that is bound to the provided instance. This method, when called, should behave like a method of the instance. # Constraints - The function should ensure that the provided `func` is callable and the `instance` is not `None`. - If these constraints are not met, raise a `ValueError` with an appropriate message. # Example ```python class MyClass: def __init__(self, value): self.value = value def sample_function(self, x): return self.value + x # Creating an instance of MyClass my_instance = MyClass(10) # Creating a bound method bound_method = create_bound_method(sample_function, my_instance) # Calling the bound method result = bound_method(5) # Should return 15 print(result) # Output: 15 ``` # Performance Requirements - The solution should be efficient and handle typical use cases for binding methods to class instances without significant overhead. Implement the `create_bound_method` function to pass the provided example and constraints.","solution":"import types def create_bound_method(func, instance): if not callable(func): raise ValueError(\\"The provided func must be callable.\\") if instance is None: raise ValueError(\\"The provided instance cannot be None.\\") return types.MethodType(func, instance)"},{"question":"**Unicode Normalization and Case-Insensitive String Matching** Given two Unicode strings, your task is to implement a function that determines if they are equivalent when compared in a case-insensitive manner, considering Unicode normalization. The Unicode standard provides various normalization forms (NFC, NFD, NFKC, NFKD), and you should use \'NFD\' (Normalization Form D) to decompose combined characters. Your function should also correctly handle characters that might have different case mappings, ensuring a robust case-insensitive comparison. Additionally, you will handle input and output via files. Your input file will consist of multiple lines where each line contains two Unicode strings separated by a semicolon (`;`). Your output file should contain \\"True\\" or \\"False\\" for each line indicating whether the two strings on that line are equivalent based on the specified criteria. Function Signature ```python def normalize_case_insensitive_compare(input_file: str, output_file: str) -> None: ``` Input - `input_file` (str): The path to the input file containing strings to be compared. - `output_file` (str): The path to the output file where the comparison results will be written. Output - None: The results should be written to the output file, one result per line, indicating whether the pair of Unicode strings on each line of the input file are equivalent based on normalization and case-insensitive matching. Constraints 1. Each line in the input file contains exactly two Unicode strings separated by a semicolon (`;`). 2. The strings can contain a wide range of Unicode characters including but not limited to Latin characters, accents, symbols, and emoji. 3. The file may contain up to 1000 lines. Example **Input File:** ``` café;Café straße;strasse 𝒮𝓉𝓇𝑒ℯ𝓉;street grin😀;grin😀 grin😀;GRIN😀 ``` **Output File:** ``` True True False True True ``` **Explanation** 1. \\"café\\" and \\"Café\\" are equivalent after normalization and casefolding. 2. \\"straße\\" and \\"strasse\\" are equivalent after casefolding (ß becomes ss). 3. \\"𝒮𝓉𝓇𝑒ℯ𝓉\\" (stylized) and \\"street\\" are not equivalent under normalization and case-insensitive comparison. 4. \\"grin😀\\" and \\"grin😀\\" are the same. 5. \\"grin😀\\" and \\"GRIN😀\\" are equivalent when casefolded. Required Libraries You may use the `unicodedata` module available in Python standard library. Hints 1. Use `unicodedata.normalize()` for normalization. 2. Use the `casefold()` method for case-insensitive comparison.","solution":"import unicodedata def normalize_case_insensitive_compare(input_file: str, output_file: str) -> None: def normalize_and_casefold(s: str) -> str: return unicodedata.normalize(\'NFD\', s).casefold() with open(input_file, \'r\', encoding=\'utf-8\') as infile, open(output_file, \'w\', encoding=\'utf-8\') as outfile: for line in infile: str1, str2 = line.strip().split(\';\') normalized_str1 = normalize_and_casefold(str1) normalized_str2 = normalize_and_casefold(str2) result = normalized_str1 == normalized_str2 outfile.write(f\\"{result}n\\")"},{"question":"Objective This question aims to assess your understanding of the `copy` module in Python, particularly the concepts of shallow and deep copying, handling recursive objects, and implementing custom copy operations for user-defined classes. Problem Statement You are given a class `TreeNode` that represents a node in a tree structure. Each node has a value and two children: `left` and `right`. Your task is to implement the custom shallow and deep copy mechanisms for this class using the `__copy__()` and `__deepcopy__()` methods. Here\'s the `TreeNode` class with the methods to be implemented: ```python import copy class TreeNode: def __init__(self, value, left=None, right=None): self.value = value self.left = left self.right = right def __copy__(self): # Implement shallow copy here pass def __deepcopy__(self, memo): # Implement deep copy here pass # Example usage: # tree = TreeNode(1, TreeNode(2), TreeNode(3)) # shallow_copied_tree = copy.copy(tree) # deep_copied_tree = copy.deepcopy(tree) ``` Requirements 1. **Shallow Copy**: - Implement the `__copy__()` method such that it creates a new `TreeNode` object with the same value and references to the original node\'s children. 2. **Deep Copy**: - Implement the `__deepcopy__()` method such that it creates a new `TreeNode` object with the same value and recursively deep copies the children nodes. Use the `memo` dictionary to avoid copying objects that have already been copied. 3. **Input/Output**: - You do not need to handle input/output for this problem. You only need to handle the proper implementation of the methods. 4. **Constraints**: - The tree can have arbitrary depth but will not contain cycles. - The `value` of each node is an integer. 5. **Performance**: - The deep copy operation should handle trees with up to 10,000 nodes efficiently. Example ```python # Define a simple tree tree = TreeNode(1, TreeNode(2), TreeNode(3)) # Perform shallow copy shallow_copied_tree = copy.copy(tree) assert shallow_copied_tree.value == 1 assert shallow_copied_tree.left is tree.left # Same reference assert shallow_copied_tree.right is tree.right # Same reference # Perform deep copy deep_copied_tree = copy.deepcopy(tree) assert deep_copied_tree.value == 1 assert deep_copied_tree.left is not tree.left # Different reference assert deep_copied_tree.right is not tree.right # Different reference assert deep_copied_tree.left.value == 2 assert deep_copied_tree.right.value == 3 ``` Notes - Focus on correctly implementing the `__copy__()` and `__deepcopy__()` methods. - Utilize `copy.copy()` and `copy.deepcopy()` where appropriate within your methods. - Consider edge cases such as nodes with `None` children. Good luck!","solution":"import copy class TreeNode: def __init__(self, value, left=None, right=None): self.value = value self.left = left self.right = right def __copy__(self): Create a shallow copy of the TreeNode. new_node = TreeNode(self.value) new_node.left = self.left new_node.right = self.right return new_node def __deepcopy__(self, memo): Create a deep copy of the TreeNode. if self in memo: return memo[self] new_node = TreeNode(self.value) memo[self] = new_node new_node.left = copy.deepcopy(self.left, memo) new_node.right = copy.deepcopy(self.right, memo) return new_node"},{"question":"**Question: Platform-Specific Asyncio Implementation** You are tasked with implementing an asynchronous TCP server in Python 3.10 using the asyncio package. Your server should handle client connections and process data sent by clients asynchronously. However, you must account for platform-specific constraints, particularly for Windows. # Function Signature ```python import asyncio async def start_server(host: str, port: int): pass async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter): pass # Example Usage: # asyncio.run(start_server(\'127.0.0.1\', 8888)) ``` # Requirements 1. **start_server(host: str, port: int)**: - This function should start an asynchronous TCP server that listens on the specified host and port. - The server should accept client connections and delegate to `handle_client` for data processing. 2. **handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter)**: - This function should handle client communication. - It should read data from the client, process it (e.g., by reversing the string received), and then send the processed data back to the client. - After processing, the connection should be closed. # Platform-Specific Constraints - **Windows**: - You must use the `ProactorEventLoop` to support subprocesses. Ensure that the event loop is set correctly no matter the platform. - Other constraints, such as the inability to use \\"unix\\" sockets, should be noted, but they do not directly affect this TCP server implementation. # Example Interaction 1. Start the server using: ```python asyncio.run(start_server(\'127.0.0.1\', 8888)) ``` 2. Connect to the server using a TCP client (e.g., `nc` or a custom Python client). 3. Send a string message to the server. 4. The server should reverse the string and send it back to the client. 5. The client receives the reversed string and the connection closes. # Constraints - Your implementation must be compatible with the limitations outlined above. - Do not use methods that are unsupported on Windows, such as `loop.create_unix_connection()` or `loop.create_unix_server()`. # Notes - Consider edge cases such as handling multiple clients simultaneously. - Ensure proper error handling and resource cleanup.","solution":"import asyncio import platform async def start_server(host: str, port: int): server = await asyncio.start_server(handle_client, host, port) async with server: await server.serve_forever() async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter): data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message!r} from {addr!r}\\") reversed_message = message[::-1] print(f\\"Send: {reversed_message!r}\\") writer.write(reversed_message.encode()) await writer.drain() print(\\"Close the connection\\") writer.close() await writer.wait_closed() # Ensure the correct event loop is used for Windows if platform.system() == \'Windows\': asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())"},{"question":"Advanced Logging Configuration in Python Objective: Design a Python class-based application that utilizes advanced logging practices. This application will demonstrate your understanding of the Python `logging` module, including configuring loggers, handlers, formatters, and filters, adding contextual information, custom logging levels, and setting up logging across multiple modules and threads. Requirements: 1. **Class Definition**: - Define a class `TaskProcessor` that encapsulates a task processing job. - Implement methods within the class to log different levels of messages (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`). 2. **Logging Configuration**: - Configure loggers to output messages to both the console and a file named `task_processor.log`. - Use different logging levels for the console (`INFO` and above) and file (`DEBUG` and above). - Implement and attach a custom filter that adds contextual information (such as a `task_id` and `user_id`) to each log message. - Ensure log messages include timestamps and other relevant metadata in their format. 3. **Multithreading**: - Integrate logging in a scenario where multiple threads (simulate using `threading.Thread`) are performing tasks within an instance of `TaskProcessor`. 4. **Custom Logging Level**: - Define a custom logging level named `SUCCESS` between `INFO` and `WARNING`. - Ensure that `TaskProcessor` can log messages at this custom level. 5. **Handling Structured Data**: - Implement logging so that structured data (in JSON format) can be logged. This structured data should include task results and other metadata. Input: - There are no specific input requirements for this task. Your implementation will manually invoke log messages. Output: - Log messages should be output to both the console and the file `task_processor.log`. Code Implementation: ```python import logging import logging.handlers import threading import json import time # Define a custom log level. SUCCESS_LEVEL_NUM = 25 logging.addLevelName(SUCCESS_LEVEL_NUM, \\"SUCCESS\\") def success(self, message, *args, **kws): if self.isEnabledFor(SUCCESS_LEVEL_NUM): self._log(SUCCESS_LEVEL_NUM, message, args, **kws) logging.Logger.success = success class ContextFilter(logging.Filter): def filter(self, record): record.task_id = getattr(record, \'task_id\', \'N/A\') record.user_id = getattr(record, \'user_id\', \'N/A\') return True class TaskProcessor: def __init__(self, task_id, user_id): self.task_id = task_id self.user_id = user_id self.logger = logging.getLogger(__name__) self.logger.debug(f\'TaskProcessor initialized with task_id={self.task_id}, user_id={self.user_id}\') def process_task(self): self.logger.info(\'Starting task processing\') try: time.sleep(1) # Simulate task processing. self.logger.success(\'Task processed successfully.\') except Exception as e: self.logger.error(f\'Task processing failed: {e}\') finally: self.logger.info(\'Finished task processing\') def setup_logging(): logger = logging.getLogger() logger.setLevel(logging.DEBUG) console_handler = logging.StreamHandler() console_handler.setLevel(logging.INFO) console_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\') console_handler.setFormatter(console_formatter) file_handler = logging.handlers.RotatingFileHandler(\'task_processor.log\', maxBytes=5000000, backupCount=5) file_handler.setLevel(logging.DEBUG) file_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(task_id)s - %(user_id)s - %(message)s\') file_handler.setFormatter(file_formatter) context_filter = ContextFilter() logger.addHandler(console_handler) logger.addHandler(file_handler) logger.addFilter(context_filter) def run_in_thread(task_processor): thread = threading.Thread(target=task_processor.process_task) thread.start() return thread if __name__ == \\"__main__\\": setup_logging() task_processors = [TaskProcessor(task_id=i, user_id=f\'user_{i}\') for i in range(5)] threads = [run_in_thread(tp) for tp in task_processors] for thread in threads: thread.join() ``` Constraints: - Ensure thread-safety when writing logs. - The solution must demonstrate the ability to log structured data in JSON format. - Handle log rotation for the file handler ensuring older logs are archived.","solution":"import logging import logging.handlers import threading import json import time # Define a custom log level named SUCCESS. SUCCESS_LEVEL_NUM = 25 logging.addLevelName(SUCCESS_LEVEL_NUM, \\"SUCCESS\\") def success(self, message, *args, **kws): if self.isEnabledFor(SUCCESS_LEVEL_NUM): self._log(SUCCESS_LEVEL_NUM, message, args, **kws) logging.Logger.success = success class ContextFilter(logging.Filter): def filter(self, record): record.task_id = getattr(record, \'task_id\', \'N/A\') record.user_id = getattr(record, \'user_id\', \'N/A\') return True class TaskProcessor: def __init__(self, task_id, user_id): self.task_id = task_id self.user_id = user_id self.logger = logging.getLogger(f\'TaskProcessor-{task_id}\') self.logger.debug(f\'TaskProcessor initialized with task_id={self.task_id}, user_id={self.user_id}\') def process_task(self): self.logger.info(\'Starting task processing\') try: time.sleep(1) # Simulate task processing. # Successfully processed task self.logger.success(\'Task processed successfully.\') except Exception as e: self.logger.error(f\'Task processing failed: {e}\') finally: self.logger.info(\'Finished task processing\') def setup_logging(): logger = logging.getLogger() logger.setLevel(logging.DEBUG) console_handler = logging.StreamHandler() console_handler.setLevel(logging.INFO) console_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\') console_handler.setFormatter(console_formatter) file_handler = logging.handlers.RotatingFileHandler(\'task_processor.log\', maxBytes=5000000, backupCount=5) file_handler.setLevel(logging.DEBUG) file_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(task_id)s - %(user_id)s - %(message)s\') file_handler.setFormatter(file_formatter) # Using JSON format for file logs json_formatter = logging.Formatter((\'%(asctime)s - %(name)s - %(levelname)s - \' + \'%(task_id)s - %(user_id)s - %(message)s\')) file_handler.setFormatter(json_formatter) context_filter = ContextFilter() logger.addHandler(console_handler) logger.addHandler(file_handler) logger.addFilter(context_filter) def run_in_thread(task_processor): thread = threading.Thread(target=task_processor.process_task) thread.start() return thread if __name__ == \\"__main__\\": setup_logging() processors = [TaskProcessor(task_id=i, user_id=f\'user_{i}\') for i in range(5)] threads = [run_in_thread(processor) for processor in processors] for thread in threads: thread.join()"},{"question":"# Problem: Implementing and Interpreting PCA **Objective**: You need to implement Principal Component Analysis (PCA) on a given dataset to reduce its dimensionality. After reducing the dimensionality, you should also provide an interpretation of the PCA results. **Dataset**: For this task, we will use the well-known Iris dataset, which is comprised of 4 features for each sample. The dataset is available in `sklearn.datasets`. **Tasks**: 1. Load the Iris dataset from `sklearn.datasets`. 2. Implement PCA to reduce the dataset\'s dimensionality to 2 components. 3. Project the data onto these 2 dimensions. 4. Plot the projected data to visualize the first two principal components. 5. Provide the percentage of variance explained by each component. 6. Interpret the results in terms of data variance and dimensionality reduction. **Constraints**: - You should use `sklearn.decomposition.PCA` for PCA implementation. - Only the first two principal components should be considered. **Expected Input and Output**: *Input*: - No input from the user; load data directly from `sklearn.datasets`. *Output*: 1. A 2D plot of the projected Iris dataset on the first two principal components. 2. The percentage of variance explained by each of the first two components. 3. A brief summary interpreting the variance explained by the components. **Example**: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.decomposition import PCA from sklearn.datasets import load_iris # Step 1: Load the dataset data = load_iris() X = data.data y = data.target # Step 2: Implement PCA to reduce the dimensionality to 2 components pca = PCA(n_components=2) X_pca = pca.fit_transform(X) # Step 3: Project the data onto these 2 dimensions # (Already done in Step 2 by `fit_transform`) # Step 4: Plot the projected data plt.figure(figsize=(8, 6)) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=\'viridis\', edgecolor=\'k\', s=40) plt.xlabel(\'First principal component\') plt.ylabel(\'Second principal component\') plt.title(\'PCA of Iris Dataset\') plt.colorbar(label=\'species\') plt.show() # Step 5: Provide the percentage of variance explained by each component variance_explained = pca.explained_variance_ratio_ print(f\\"Variance explained by first component: {variance_explained[0] * 100:.2f}%\\") print(f\\"Variance explained by second component: {variance_explained[1] * 100:.2f}%\\") # Step 6: Interpret the results The first two principal components explain X% and Y% of the variance in the dataset respectively. This reduction in dimensionality allows us to visualize the dataset in two dimensions while retaining the majority of the variance in the data. ``` **Submission Requirements**: - Your code implementation along with the plot. - Output showing the percentage of variance explained by the first two components. - A summary of the interpretation based on variance explained (insert the summary in your comments).","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.decomposition import PCA from sklearn.datasets import load_iris def pca_iris(): This function loads the Iris dataset, applies PCA to reduce its dimensionality to 2 components, and visualizes the projected data. Returns: - A tuple containing the figure object and the explained variance ratios. # Step 1: Load the dataset data = load_iris() X = data.data y = data.target # Step 2: Implement PCA to reduce the dimensionality to 2 components pca = PCA(n_components=2) X_pca = pca.fit_transform(X) # Step 3: Project the data onto these 2 dimensions # (Already done in Step 2 by `fit_transform`) # Step 4: Plot the projected data plt.figure(figsize=(8, 6)) scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=\'viridis\', edgecolor=\'k\', s=40) plt.xlabel(\'First principal component\') plt.ylabel(\'Second principal component\') plt.title(\'PCA of Iris Dataset\') plt.colorbar(label=\'species\') # Save figure to a variable fig = plt.gcf() # Step 5: Provide the percentage of variance explained by each component variance_explained = pca.explained_variance_ratio_ print(f\\"Variance explained by first component: {variance_explained[0] * 100:.2f}%\\") print(f\\"Variance explained by second component: {variance_explained[1] * 100:.2f}%\\") # Step 6: Interpret the results # The first two principal components explain 92.46% and 5.31% of the variance in the dataset respectively. # This reduction in dimensionality allows us to visualize the dataset in two dimensions while retaining # a majority of the variance in the data. return fig, variance_explained"},{"question":"# Scikit-Learn Precision-Recall Curve Display Implementation Your task is to implement a Precision-Recall Curve (PRC) display module in Scikit-Learn. The module should follow the guidelines and structure of the Plotting API as described in the provided documentation. Specifically, you need to implement a `PrecisionRecallDisplay` class with methods for creating the display object from an estimator or from predictions, and a method for plotting the curve. Class Definition ```python class PrecisionRecallDisplay: def __init__(self, precision, recall, average_precision, estimator_name): Initialize the display with necessary data. Parameters: - precision: array-like, Precision values. - recall: array-like, Recall values. - average_precision: float, Average precision score. - estimator_name: str, Name of the estimator. self.precision = precision self.recall = recall self.average_precision = average_precision self.estimator_name = estimator_name @classmethod def from_estimator(cls, estimator, X, y): Create the display object from an estimator and data. Parameters: - estimator: The fitted estimator object. - X: array-like, Feature data. - y: array-like, True labels. Returns: - PrecisionRecallDisplay object with computed precision-recall values and average precision. y_pred = estimator.predict_proba(X)[:, 1] return cls.from_predictions(y, y_pred, estimator.__class__.__name__) @classmethod def from_predictions(cls, y, y_pred, estimator_name): Create the display object from true and predicted values. Parameters: - y: array-like, True labels. - y_pred: array-like, Predicted probabilities. - estimator_name: str, Name of the estimator. Returns: - PrecisionRecallDisplay object with computed precision-recall values and average precision. precision, recall, _ = precision_recall_curve(y, y_pred) average_precision = average_precision_score(y, y_pred) return cls(precision, recall, average_precision, estimator_name) def plot(self, ax=None, name=None, **kwargs): Plot the precision-recall curve. Parameters: - ax: Matplotlib axes object, Optional. - name: str, Optional name for the plot legend. Returns: - self if ax is None: fig, ax = plt.subplots() name = name or self.estimator_name self.line_, = ax.plot(self.recall, self.precision, label=f\\"{name} (AP = {self.average_precision:.2f})\\", **kwargs) ax.set_xlabel(\\"Recall\\") ax.set_ylabel(\\"Precision\\") ax.set_title(\\"Precision-Recall Curve\\") ax.legend(loc=\\"best\\") self.ax_ = ax self.figure_ = ax.figure return self ``` Implementation Constraints and Requirements: 1. Use appropriate Scikit-Learn and Matplotlib functions for generating and plotting precision-recall data. 2. The `PrecisionRecallDisplay` class should be implemented such that it adheres to Scikit-Learn\'s Plotting API design principles. 3. Ensure that the methods handle the data and visualization aspects as specified. 4. The `plot` method should allow for customizations of the plot after it is created. Input and Output: - `from_estimator` method: - Input: A fitted estimator, feature data `X`, and true labels `y`. - Output: `PrecisionRecallDisplay` object with computed precision, recall, and average precision values. - `from_predictions` method: - Input: True labels `y`, predicted probabilities `y_pred`, and the estimator\'s name. - Output: `PrecisionRecallDisplay` object with computed precision, recall, and average precision values. - `plot` method: - Input: Optional matplotlib axes object `ax`, and an optional name for the plot legend. - Output: The display object itself, after plotting the precision-recall curve. Implement this class in a Python script or Jupyter notebook, and ensure it functions correctly by testing it with a sample dataset and estimator from Scikit-Learn.","solution":"import matplotlib.pyplot as plt from sklearn.metrics import precision_recall_curve, average_precision_score class PrecisionRecallDisplay: def __init__(self, precision, recall, average_precision, estimator_name): Initialize the display with necessary data. Parameters: - precision: array-like, Precision values. - recall: array-like, Recall values. - average_precision: float, Average precision score. - estimator_name: str, Name of the estimator. self.precision = precision self.recall = recall self.average_precision = average_precision self.estimator_name = estimator_name @classmethod def from_estimator(cls, estimator, X, y): Create the display object from an estimator and data. Parameters: - estimator: The fitted estimator object. - X: array-like, Feature data. - y: array-like, True labels. Returns: - PrecisionRecallDisplay object with computed precision-recall values and average precision. y_pred = estimator.predict_proba(X)[:, 1] return cls.from_predictions(y, y_pred, estimator.__class__.__name__) @classmethod def from_predictions(cls, y, y_pred, estimator_name): Create the display object from true and predicted values. Parameters: - y: array-like, True labels. - y_pred: array-like, Predicted probabilities. - estimator_name: str, Name of the estimator. Returns: - PrecisionRecallDisplay object with computed precision-recall values and average precision. precision, recall, _ = precision_recall_curve(y, y_pred) average_precision = average_precision_score(y, y_pred) return cls(precision, recall, average_precision, estimator_name) def plot(self, ax=None, name=None, **kwargs): Plot the precision-recall curve. Parameters: - ax: Matplotlib axes object, Optional. - name: str, Optional name for the plot legend. Returns: - self if ax is None: fig, ax = plt.subplots() name = name or self.estimator_name self.line_, = ax.plot(self.recall, self.precision, label=f\\"{name} (AP = {self.average_precision:.2f})\\", **kwargs) ax.set_xlabel(\\"Recall\\") ax.set_ylabel(\\"Precision\\") ax.set_title(\\"Precision-Recall Curve\\") ax.legend(loc=\\"best\\") self.ax_ = ax self.figure_ = ax.figure return self"},{"question":"# Asynchronous Task Queue Simulation You are tasked with simulating an asynchronous job processing system using the `asyncio` package in Python. Multiple worker tasks will process jobs from a queue asynchronously. Each job has a specific priority and processing time. Your task is to implement this system and ensure that jobs are processed according to their priority. Requirements 1. **Job Queue**: Use `asyncio.PriorityQueue` to store jobs. Each job should be a tuple containing the priority and the processing time `(priority, processing_time)`. 2. **Worker Tasks**: Implement a worker function that: - Fetches the job from the queue. - Processes the job (simulated by `asyncio.sleep(processing_time)`). - Marks the job as done using `queue.task_done()`. - Prints a statement indicating the worker\'s name, priority, and processing time of the job. 3. **Job Generation**: Create an asynchronous function to generate random jobs with random priorities and processing times: - Priorities and processing times should be random numbers (you can use `random` module). - Put these jobs in the `PriorityQueue`. 4. **Main Function**: Implement the main function to: - Initialize the `PriorityQueue`. - Spawn multiple worker tasks. - Generate and push jobs to the queue. - Wait until the queue is empty. - Cancel worker tasks gracefully once the queue is empty. - Print the total simulated processing time and any other useful metrics. Input and Output - **Input**: Number of worker tasks and number of jobs to generate. - **Output**: Log statements of worker tasks processing each job and a final summary statement containing total processing time and total jobs processed. Constraints - You should ensure that the system runs efficiently with a large number of jobs and worker tasks. - Handle exceptions appropriately. - Use `asyncio.wait_for()` to handle potential delays and ensure no job processing takes too long. Example ```python import asyncio import random import time async def worker(name, queue): while True: try: # Fetch a job from the queue with a timeout of 5 seconds priority, processing_time = await asyncio.wait_for(queue.get(), 5.0) await asyncio.sleep(processing_time) queue.task_done() print(f\'{name} processed job with priority {priority} in {processing_time:.2f} seconds\') except asyncio.TimeoutError: print(f\'{name} timed out waiting for a job.\') break async def generate_jobs(queue, num_jobs): for _ in range(num_jobs): priority = random.randint(1, 10) processing_time = random.uniform(0.1, 1.0) await queue.put((priority, processing_time)) await asyncio.sleep(0.1) async def main(num_workers, num_jobs): queue = asyncio.PriorityQueue() tasks = [] for i in range(num_workers): task = asyncio.create_task(worker(f\'worker-{i}\', queue)) tasks.append(task) await generate_jobs(queue, num_jobs) await queue.join() for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) print(\'All jobs have been processed.\') # Example execution asyncio.run(main(3, 15)) ``` Notes - You need to handle cancellations and exceptions properly in the worker function. - Make sure to test your implementation with different numbers of workers and jobs. - Ensure that the output is clear and contains all necessary information to understand the job processing flow.","solution":"import asyncio import random import time async def worker(name, queue): while True: try: # Fetch a job from the queue with a timeout of 5 seconds priority, processing_time = await asyncio.wait_for(queue.get(), 5.0) await asyncio.sleep(processing_time) queue.task_done() print(f\'{name} processed job with priority {priority} in {processing_time:.2f} seconds\') except asyncio.TimeoutError: print(f\'{name} timed out waiting for a job.\') break async def generate_jobs(queue, num_jobs): for _ in range(num_jobs): priority = random.randint(1, 10) processing_time = random.uniform(0.1, 1.0) await queue.put((priority, processing_time)) await asyncio.sleep(0.1) async def main(num_workers, num_jobs): queue = asyncio.PriorityQueue() tasks = [] for i in range(num_workers): task = asyncio.create_task(worker(f\'worker-{i}\', queue)) tasks.append(task) await generate_jobs(queue, num_jobs) await queue.join() for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) print(\'All jobs have been processed.\') # Run the main function with specific parameters if __name__ == \\"__main__\\": asyncio.run(main(3, 15))"},{"question":"# Advanced Python Function Manipulation Problem Statement You are tasked with designing and implementing a Python function that dynamically creates another function based on provided attributes. This involves setting the code object, global variables, and any other specified attributes correctly. The purpose is to simulate the way lower-level function creation and manipulation might happen using the appropriate Python APIs. Requirements 1. Implement a Python function `create_custom_function` that: - Accepts a string of Python code (`code_str`), a dictionary of global variables (`globals_dict`), and optionally a tuple of default arguments (`defaults`), a tuple of closure variables (`closure`), and a dictionary of annotations (`annotations`). - Compiles the code string into a code object. - Creates a new function object using this code object and globals dictionary. - Sets the default arguments, closure, and annotations if provided. 2. The function must mimic the behavior of `PyFunction_New` and related functions at the Python level. Function Signature ```python def create_custom_function(code_str: str, globals_dict: dict, defaults: tuple = None, closure: tuple = None, annotations: dict = None) -> callable: ``` Input - `code_str` (str): A string containing valid Python code that defines a function. - `globals_dict` (dict): A dictionary of global variables that the function will use. - `defaults` (tuple, optional): A tuple of default argument values for the function. - `closure` (tuple, optional): A tuple of closure variables for the function. - `annotations` (dict, optional): A dictionary of annotations for the function parameters and return value. Output - A callable Python function object created based on the provided arguments. Constraints - The `code_str` must contain a valid function definition. - The `globals_dict` must be a dictionary. - If provided, `defaults` must be a tuple, `closure` must be a tuple, and `annotations` must be a dictionary. Example ```python code_str = def dynamic_function(x, y): return x + y + z globals_dict = {\'z\': 10} defaults = (1, 2) closure = None annotations = {\'x\': int, \'y\': int, \'return\': int} custom_func = create_custom_function(code_str, globals_dict, defaults, closure, annotations) assert custom_func(5, 5) == 20 # dynamic_function(5, 5) returns 5 + 5 + 10 assert custom_func.__annotations__ == {\'x\': int, \'y\': int, \'return\': int} ``` Notes - You can use the `compile()` built-in function to compile the code string into a code object. - Use the `types.FunctionType` to create a new function object from the code object and globals dictionary. - You may use the `inspect` module to assist with manipulating function attributes if necessary.","solution":"import types def create_custom_function(code_str: str, globals_dict: dict, defaults: tuple = None, closure: tuple = None, annotations: dict = None) -> callable: Creates a custom function from the provided code, globals, defaults, closure, and annotations. # Compile the code string into a code object. compiled_code = compile(code_str, \\"<string>\\", \\"exec\\") # Extract the function name from the code string. Assuming the function is defined as `def <name>(...)`. func_name = None for line in code_str.splitlines(): if line.strip().startswith(\\"def \\"): func_name = line.split(\\"(\\")[0].split()[1] break exec(compiled_code, globals_dict) # Retrieve the function object. func = globals_dict[func_name] # Set defaults, closure, and annotations if they are provided. if defaults: func.__defaults__ = defaults if closure: func.__closure__ = closure if annotations: func.__annotations__ = annotations return func"},{"question":"Question: Multi-Producer-Multi-Consumer with Priority Handling You are tasked with implementing a multi-producer-multi-consumer system using Python\'s `queue` module where multiple producer threads generate tasks with varying priorities and insert them into a priority queue. Multiple consumer threads retrieve and process these tasks. Your implementation should ensure thread safety and correctly handle task priorities. # Requirements: 1. **Class**: Implement class `TaskManager` with the following methods: - `add_task(task_name: str, priority: int)`: Adds a task to the queue with the specified priority. - `start_processing(num_workers: int)`: Starts the specified number of worker threads to process tasks from the queue. - `wait_for_completion()`: Blocks until all tasks have been processed. 2. **Task Representation**: Tasks are represented as tuples with their priority and task name. 3. **Processing Simulation**: Processing a task will involve printing the task name and sleeping for a random short duration to simulate work. 4. **Task Queue**: Use `queue.PriorityQueue` to manage tasks ensuring that tasks with the lowest priority numbers are processed first. 5. **Thread Safety**: Ensure that the queue operations are thread-safe and correctly handle multi-producer and multi-consumer scenarios. # Expected Input and Output: Example: ```python tm = TaskManager() tm.add_task(\\"task_1\\", 2) tm.add_task(\\"task_2\\", 1) tm.add_task(\\"task_3\\", 3) tm.start_processing(2) tm.wait_for_completion() ``` Expected Output: ``` Processing task_2 Processing task_1 Processing task_3 All tasks have been processed. ``` # Constraints: - The number of tasks will not exceed 1000. - The number of worker threads will not exceed 10. - The priority values are integers between 0 and 100 (inclusive). # Performance Requirements: - Ensure efficient handling of the queue and worker threads. - Avoid deadlocks and race conditions. # Implementation: ```python import queue import threading import time import random class TaskManager: def __init__(self): self.task_queue = queue.PriorityQueue() self.lock = threading.Lock() def add_task(self, task_name: str, priority: int): with self.lock: self.task_queue.put((priority, task_name)) def start_processing(self, num_workers: int): for _ in range(num_workers): worker = threading.Thread(target=self.worker) worker.daemon = True worker.start() def worker(self): while True: try: priority, task_name = self.task_queue.get(block=False) print(f\\"Processing {task_name}\\") time.sleep(random.uniform(0.1, 0.5)) self.task_queue.task_done() except queue.Empty: break def wait_for_completion(self): self.task_queue.join() print(\\"All tasks have been processed.\\") ```","solution":"import queue import threading import time import random class TaskManager: def __init__(self): self.task_queue = queue.PriorityQueue() self.lock = threading.Lock() def add_task(self, task_name: str, priority: int): with self.lock: self.task_queue.put((priority, task_name)) def start_processing(self, num_workers: int): for _ in range(num_workers): worker = threading.Thread(target=self.worker) worker.daemon = True worker.start() def worker(self): while True: try: priority, task_name = self.task_queue.get(block=False) print(f\\"Processing {task_name}\\") time.sleep(random.uniform(0.1, 0.5)) self.task_queue.task_done() except queue.Empty: break def wait_for_completion(self): self.task_queue.join() print(\\"All tasks have been processed.\\")"},{"question":"Objective You are required to demonstrate your understanding of asynchronous execution using PyTorch\'s `torch.futures` package by designing a function that processes a list of data items concurrently. Task Implement a function named `process_data_concurrently` that processes a list of data items in parallel using `torch.futures.Future`. The function should: 1. Accept a list of integers as input. 2. Simulate an asynchronous processing operation on each integer by squaring it, using the `torch.futures.Future` class. 3. Use the `collect_all` function to gather all the future results. 4. Ensure the function returns a list of squared integers, preserving the order of the input list. Specifications 1. **Function Signature**: ```python def process_data_concurrently(data: List[int]) -> List[int]: ``` 2. **Input**: - `data`: A list of integers `[a1, a2, ..., an]` where (0 leq a_i leq 10^6). 3. **Output**: - A list of integers where each item is the square of the corresponding input integer `[a1^2, a2^2, ..., an^2]`. 4. **Constraints**: - The function should use `torch.futures.Future` to manage the asynchronous operations. - You should make use of `collect_all` to gather the results from all `Future` objects. - The order of the results must match the order of the input list. 5. **Example**: ```python input_data = [1, 2, 3, 4] output_data = process_data_concurrently(input_data) assert output_data == [1, 4, 9, 16] ``` Additional Notes - You may assume PyTorch (`torch`) is properly installed and can be imported. - Focus on utilizing `torch.futures.Future` and the respective utility functions as provided in the documentation. Good luck, and happy coding!","solution":"import torch from typing import List def async_square(n: int) -> torch.futures.Future: Asynchronously computes the square of a number. future = torch.futures.Future() future.set_result(n ** 2) return future def process_data_concurrently(data: List[int]) -> List[int]: Process a list of data items concurrently using torch.futures.Future. Each integer in the list is squared asynchronously. Args: data (List[int]): List of integers to be squared. Returns: List[int]: List of squared integers. # Create a list of futures by asynchronously squaring each number. futures_list = [async_square(n) for n in data] # Collect all the futures. all_done_fut = torch.futures.collect_all(futures_list) # Wait for all futures to complete and extract the results. results_fut = all_done_fut.wait() results = [fut.value() for fut in results_fut] return results"},{"question":"**Question: Advanced SQLite Database Operations Using Python** You are required to implement a sequence of operations on an SQLite database using Python\'s `sqlite3` module. This assessment aims to test your understanding of database CRUD operations, transaction management, and adapting/converting custom Python types using SQLite. # Task: 1. **Create Database and Tables**: - Create a SQLite database in memory. - Create a table named `movies` with columns: title (TEXT), release_year (INTEGER), score (REAL). - Create a table named `reviews` with columns: movie_title (TEXT) and review_text (TEXT). 2. **Insert Data**: - Insert the following movies into the `movies` table: - (\'Interstellar\', 2014, 8.6) - (\'Inception\', 2010, 8.8) - (\'The Dark Knight\', 2008, 9.0) - Insert the following reviews into the `reviews` table: - (\'Interstellar\', \'Amazing visual effects and gripping storyline.\') - (\'Inception\', \'Mind-bending and thoroughly engaging.\') - (\'The Dark Knight\', \'Best superhero movie ever.\') 3. **Define and Use Custom Data Type**: - Define a custom Python class `Movie` to represent a movie with `title`, `release_year`, and `score` attributes. - Register an adapter to convert `Movie` objects to SQLite-compatible types. - Register a converter to convert text retrieved from the database back into `Movie` objects. 4. **Transaction Management**: - Implement a function `update_movie_score` that takes a `movie_title` and a new `score`, and updates the score of the movie inside a transaction. If an error occurs, roll back the transaction. 5. **Query and Display Data**: - Write a function `display_movies` that retrieves and prints all movies ordered by their score in descending order. - Write a function `display_reviews` that retrieves and prints all reviews for a given movie. # Implementation: Please implement the functions specified below to complete the above tasks: ```python import sqlite3 # Define the custom Movie class class Movie: def __init__(self, title, release_year, score): self.title = title self.release_year = release_year self.score = score def __repr__(self): return f\\"Movie(title={self.title}, release_year={self.release_year}, score={self.score})\\" # Define a function to adapt a Movie object to a SQLite compatible representation def adapt_movie(movie): return f\\"{movie.title};{movie.release_year};{movie.score}\\" # Define a function to convert a string to a Movie object def convert_movie(value): title, release_year, score = value.split(b\';\') return Movie(title.decode(\'utf-8\'), int(release_year), float(score)) # Implement the required functionality in a main function def main(): # Create a new in-memory SQLite database and connection con = sqlite3.connect(\\":memory:\\", detect_types=sqlite3.PARSE_DECLTYPES | sqlite3.PARSE_COLNAMES) cur = con.cursor() # Register the adapter and converter for the Movie class sqlite3.register_adapter(Movie, adapt_movie) sqlite3.register_converter(\\"movie\\", convert_movie) # Create the movies and reviews tables cur.execute(\\"CREATE TABLE movies (title TEXT, release_year INTEGER, score REAL)\\") cur.execute(\\"CREATE TABLE reviews (movie_title TEXT, review_text TEXT)\\") # Insert data into the movies table movies_data = [ (\'Interstellar\', 2014, 8.6), (\'Inception\', 2010, 8.8), (\'The Dark Knight\', 2008, 9.0), ] cur.executemany(\\"INSERT INTO movies VALUES (?, ?, ?)\\", movies_data) # Insert data into the reviews table reviews_data = [ (\'Interstellar\', \'Amazing visual effects and gripping storyline.\'), (\'Inception\', \'Mind-bending and thoroughly engaging.\'), (\'The Dark Knight\', \'Best superhero movie ever.\') ] cur.executemany(\\"INSERT INTO reviews VALUES (?, ?)\\", reviews_data) # Commit the changes con.commit() # Implement the update_movie_score function def update_movie_score(movie_title, new_score): try: with con: cur.execute(\\"UPDATE movies SET score = ? WHERE title = ?\\", (new_score, movie_title)) except sqlite3.Error as e: print(\\"An error occurred:\\", e) # Implement the display_movies function def display_movies(): try: cur.execute(\\"SELECT * FROM movies ORDER BY score DESC\\") rows = cur.fetchall() for row in rows: print(row) except sqlite3.Error as e: print(\\"An error occurred:\\", e) # Implement the display_reviews function def display_reviews(movie_title): try: cur.execute(\\"SELECT review_text FROM reviews WHERE movie_title = ?\\", (movie_title,)) rows = cur.fetchall() for row in rows: print(row[0]) except sqlite3.Error as e: print(\\"An error occurred:\\", e) # Test the implemented functions print(\\"Movies before update:\\") display_movies() print(\\"nUpdating the Interstellar\'s score to 9.0\\") update_movie_score(\'Interstellar\', 9.0) print(\\"nMovies after update:\\") display_movies() print(\\"nReviews of \'Inception\':\\") display_reviews(\'Inception\') # Close the connection con.close() # Run the main function if __name__ == \\"__main__\\": main() ``` # Constraints: 1. Handle exceptions appropriately to avoid program crashes. 2. Ensure that transactions are handled properly to maintain database integrity. 3. Use parameter substitution to avoid SQL injection vulnerabilities. # Input/Output Specifications: - The `update_movie_score` function takes a movie title and a new score, updates the database and prints an error message if an exception occurs. - The `display_movies` function prints all movies ordered by score in descending order. - The `display_reviews` function prints the reviews for a specific movie. Focus on maintaining and demonstrating clean database operations and an understanding of the full breadth of features offered by `sqlite3`.","solution":"import sqlite3 # Define the custom Movie class class Movie: def __init__(self, title, release_year, score): self.title = title self.release_year = release_year self.score = score def __repr__(self): return f\\"Movie(title={self.title}, release_year={self.release_year}, score={self.score})\\" # Define a function to adapt a Movie object to a SQLite compatible representation def adapt_movie(movie): return f\\"{movie.title};{movie.release_year};{movie.score}\\" # Define a function to convert a string to a Movie object def convert_movie(value): title, release_year, score = value.split(b\';\') return Movie(title.decode(\'utf-8\'), int(release_year), float(score)) # Implement the required functionality in a main function def main(): # Create a new in-memory SQLite database and connection con = sqlite3.connect(\\":memory:\\", detect_types=sqlite3.PARSE_DECLTYPES | sqlite3.PARSE_COLNAMES) cur = con.cursor() # Register the adapter and converter for the Movie class sqlite3.register_adapter(Movie, adapt_movie) sqlite3.register_converter(\\"movie\\", convert_movie) # Create the movies and reviews tables cur.execute(\\"CREATE TABLE movies (title TEXT, release_year INTEGER, score REAL)\\") cur.execute(\\"CREATE TABLE reviews (movie_title TEXT, review_text TEXT)\\") # Insert data into the movies table movies_data = [ (\'Interstellar\', 2014, 8.6), (\'Inception\', 2010, 8.8), (\'The Dark Knight\', 2008, 9.0), ] cur.executemany(\\"INSERT INTO movies VALUES (?, ?, ?)\\", movies_data) # Insert data into the reviews table reviews_data = [ (\'Interstellar\', \'Amazing visual effects and gripping storyline.\'), (\'Inception\', \'Mind-bending and thoroughly engaging.\'), (\'The Dark Knight\', \'Best superhero movie ever.\') ] cur.executemany(\\"INSERT INTO reviews VALUES (?, ?)\\", reviews_data) # Commit the changes con.commit() # Implement the update_movie_score function def update_movie_score(movie_title, new_score): try: with con: cur.execute(\\"UPDATE movies SET score = ? WHERE title = ?\\", (new_score, movie_title)) except sqlite3.Error as e: print(\\"An error occurred:\\", e) # Implement the display_movies function def display_movies(): try: cur.execute(\\"SELECT * FROM movies ORDER BY score DESC\\") rows = cur.fetchall() for row in rows: print(row) except sqlite3.Error as e: print(\\"An error occurred:\\", e) # Implement the display_reviews function def display_reviews(movie_title): try: cur.execute(\\"SELECT review_text FROM reviews WHERE movie_title = ?\\", (movie_title,)) rows = cur.fetchall() for row in rows: print(row[0]) except sqlite3.Error as e: print(\\"An error occurred:\\", e) # Test the implemented functions print(\\"Movies before update:\\") display_movies() print(\\"nUpdating the Interstellar\'s score to 9.0\\") update_movie_score(\'Interstellar\', 9.0) print(\\"nMovies after update:\\") display_movies() print(\\"nReviews of \'Inception\':\\") display_reviews(\'Inception\') # Close the connection con.close() # Run the main function if __name__ == \\"__main__\\": main()"},{"question":"Objective: The objective of this question is to assess your understanding of the pandas functions related to reshaping DataFrames. Problem Statement: You are given a DataFrame containing information about student scores in different subjects across multiple semesters. Your task is to transform and summarize the data using various reshaping methods from the pandas library. Input: The input is a DataFrame with the following columns: - `\\"student_id\\"`: Unique identifier for each student. - `\\"semester\\"`: The semester in which the scores were recorded. - `\\"subject\\"`: The subject of the scores. - `\\"score\\"`: The score achieved by the student in the subject. ```python data = { \\"student_id\\": [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4], \\"semester\\": [\\"Spring\\", \\"Fall\\", \\"Spring\\", \\"Spring\\", \\"Fall\\", \\"Spring\\", \\"Spring\\", \\"Fall\\", \\"Spring\\", \\"Spring\\", \\"Fall\\", \\"Spring\\"], \\"subject\\": [\\"Math\\", \\"Math\\", \\"Science\\", \\"Math\\", \\"Math\\", \\"Science\\", \\"Math\\", \\"Math\\", \\"Science\\", \\"Math\\", \\"Math\\", \\"Science\\"], \\"score\\": [85, 88, 90, 78, 79, 83, 91, 94, 89, 85, 87, 92] } df = pd.DataFrame(data) ``` Tasks: 1. **Pivot the DataFrame**: - Create a pivot table with `student_id` as the index, `subject` as columns, and the mean score as values. 2. **Normalize Scores**: - Normalize the scores within each subject such that the scores are represented as proportions of the total scores for each subject. 3. **Reshape with MultiIndex**: - Reshape the DataFrame to have a MultiIndex with `student_id` and `semester`, and the scores for each subject as columns. 4. **Summarize Data**: - Create a summary table that shows the sum and mean of scores for each subject. 5. **Convert Categorical Data**: - Convert the `subject` column to dummy variables. Constraints: - There are no missing values in the input DataFrame. - Each student has at most one score per subject per semester. Expected Output: 1. **Pivot Table**: ```plaintext subject Math Science student_id 1 86.5 90.0 2 78.5 83.0 3 92.5 89.0 4 86.0 92.0 ``` 2. **Normalized Scores**: ```plaintext student_id subject normalized_score ... ``` 3. **Reshaped DataFrame with MultiIndex**: ```plaintext score student_id semester subject 1 Fall Math 88 Spring Math 85 Science 90 ... ``` 4. **Summary Table**: ```plaintext score subject Math Science student_id 1 173 90 86.5 90 2 157 83 78.5 83 3 185 89 92.5 89 4 172 92 86 92 ``` 5. **Dummy Variables**: ```plaintext student_id semester score subject_Math subject_Science ... ``` Submission: Write a function `reshape_and_summarize(df)` that takes the input DataFrame and returns a tuple of the results for each of the tasks listed above. ```python import pandas as pd def reshape_and_summarize(df): # Task 1: Pivot Table pivot_table = pd.pivot_table(df, values=\\"score\\", index=\\"student_id\\", columns=\\"subject\\", aggfunc=\\"mean\\") # Task 2: Normalize Scores df[\'normalized_score\'] = df.groupby(\'subject\')[\'score\'].transform(lambda x: x / x.sum()) # Task 3: Reshape with MultiIndex reshaped_df = df.set_index([\'student_id\', \'semester\', \'subject\']).unstack() # Task 4: Summary Table summary_table = df.pivot_table(values=[\\"score\\"], index=[\\"student_id\\", \\"subject\\"], aggfunc=[\'sum\', \'mean\']) # Task 5: Convert Categorical Data to Dummy Variables dummies = pd.get_dummies(df, columns=[\'subject\']) return pivot_table, df[[\'student_id\', \'subject\', \'normalized_score\']], reshaped_df, summary_table, dummies # Example usage: data = { \\"student_id\\": [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4], \\"semester\\": [\\"Spring\\", \\"Fall\\", \\"Spring\\", \\"Spring\\", \\"Fall\\", \\"Spring\\", \\"Spring\\", \\"Fall\\", \\"Spring\\", \\"Spring\\", \\"Fall\\", \\"Spring\\"], \\"subject\\": [\\"Math\\", \\"Math\\", \\"Science\\", \\"Math\\", \\"Math\\", \\"Science\\", \\"Math\\", \\"Math\\", \\"Science\\", \\"Math\\", \\"Math\\", \\"Science\\"], \\"score\\": [85, 88, 90, 78, 79, 83, 91, 94, 89, 85, 87, 92] } df = pd.DataFrame(data) reshape_and_summarize(df) ```","solution":"import pandas as pd def reshape_and_summarize(df): # Task 1: Pivot Table pivot_table = pd.pivot_table(df, values=\\"score\\", index=\\"student_id\\", columns=\\"subject\\", aggfunc=\\"mean\\") # Task 2: Normalize Scores df[\'normalized_score\'] = df.groupby(\'subject\')[\'score\'].transform(lambda x: x / x.sum()) normalized_scores = df[[\'student_id\', \'subject\', \'normalized_score\']] # Task 3: Reshape with MultiIndex reshaped_df = df.set_index([\'student_id\', \'semester\', \'subject\']).unstack() # Task 4: Summary Table summary_table = df.pivot_table(values=\\"score\\", index=[\\"student_id\\"], columns=[\\"subject\\"], aggfunc=[\\"sum\\", \\"mean\\"]) # Task 5: Convert Categorical Data to Dummy Variables dummies = pd.get_dummies(df, columns=[\'subject\']) return pivot_table, normalized_scores, reshaped_df, summary_table, dummies"},{"question":"# Optimization Challenge with pandas, Cython, and Numba You are provided with a pandas DataFrame containing stock market data. The DataFrame has the following columns: `open`, `high`, `low`, `close`, `volume`, and `symbol`. The task is to create an optimized function to calculate the following: 1. The true range (TR) for each row in the DataFrame. 2. The average true range (ATR) over a window of N days for each symbol. **True Range (TR)** is defined as the maximum of: - `high - low` - `high - previous close` (absolute value) - `low - previous close` (absolute value) **Average True Range (ATR)** is the moving average of the true range over N days. # Requirements 1. **Input:** - A pandas DataFrame `df` with the columns `open`, `high`, `low`, `close`, `volume`, and `symbol`. - A window size ( N ) for calculating the ATR. 2. **Output:** - A pandas DataFrame with the same columns as the input DataFrame plus an additional column `ATR` containing the calculated ATR values. 3. **Function Signature:** ```python def calculate_atr(df: pd.DataFrame, window: int) -> pd.DataFrame: pass ``` 4. **Constraints and Limitations:** - Optimize for performance using Cython, Numba, or pandas.eval based on your analysis of the best approach for different segments of the task. - Handle the calculations symbol-wise, as each symbol\'s ATR must not mix data from other symbols. - Assume the data is sorted by `symbol` and `date`. # Performance Requirements Your solution should be efficient enough to handle large DataFrames with tens of thousands of rows and multiple symbols. # Example ```python import pandas as pd import numpy as np # Sample DataFrame data = { \\"date\\": pd.date_range(start=\\"1/1/2020\\", periods=10), \\"open\\": np.random.randn(10), \\"high\\": np.random.randn(10), \\"low\\": np.random.randn(10), \\"close\\": np.random.randn(10), \\"volume\\": np.random.randint(100, 1000, 10), \\"symbol\\": [\\"AAPL\\"]*5 + [\\"MSFT\\"]*5 } df = pd.DataFrame(data) # Window size for ATR calculation window = 3 # Call the function result = calculate_atr(df, window) # Display the result print(result) ``` # Guidelines 1. Use Cython for operations that can benefit from static type declarations and avoid Python function call overhead. 2. Consider using Numba for JIT compilation if it offers better performance for specific tasks. 3. Use pandas.eval where applicable to leverage efficient expression evaluation. 4. For sections where numpy array operations can outperform pandas operations, convert DataFrame columns to numpy arrays. Your implementation will be evaluated based on correctness, efficiency, and adherence to optimization practices discussed.","solution":"import pandas as pd import numpy as np def calculate_atr(df: pd.DataFrame, window: int) -> pd.DataFrame: Calculate the Average True Range (ATR) for each symbol in the DataFrame. Parameters: df (pd.DataFrame): DataFrame containing stock data with columns \'open\', \'high\', \'low\', \'close\', \'volume\', and \'symbol\'. window (int): Window size for calculating the ATR. Returns: pd.DataFrame: The input DataFrame with an additional column \'ATR\' containing the calculated ATR values. symbols = df[\'symbol\'].unique() result_df = pd.DataFrame() for symbol in symbols: symbol_df = df[df[\'symbol\'] == symbol].copy() symbol_df[\'previous_close\'] = symbol_df[\'close\'].shift(1) symbol_df[\'TR\'] = symbol_df[[\'high\', \'low\', \'previous_close\']].apply( lambda x: max(x[\'high\'] - x[\'low\'], abs(x[\'high\'] - x[\'previous_close\']), abs(x[\'low\'] - x[\'previous_close\'])), axis=1 ) symbol_df[\'ATR\'] = symbol_df[\'TR\'].rolling(window=window, min_periods=1).mean() result_df = pd.concat([result_df, symbol_df]) result_df.drop(columns=[\'previous_close\'], inplace=True) return result_df.reset_index(drop=True)"},{"question":"# Question: Concurrency Handling with CUDA Streams and PyTorch CUDA Stream Sanitizer You are given a task to detect and fix potential data race conditions when performing tensor operations using CUDA streams in PyTorch. The CUDA Stream Sanitizer feature will help you identify and debug these issues. Part 1: Detecting Data Races Write a function `detect_data_race` that simulates a potential data race condition using a CUDA tensor and different CUDA streams. ```python import torch def detect_data_race(): # Implement this function ... ``` **Instructions**: 1. Create a tensor `a` of shape `(10000,)` on the CUDA device using `torch.rand`. 2. Perform a simple tensor operation (e.g., `torch.mul`) on the tensor `a` in a new CUDA stream without proper synchronization with the default stream. 3. Write the code such that the CUDA Stream Sanitizer can detect the data race. **Expected Output**: When the script is run with the CUDA Stream Sanitizer enabled (`TORCH_CUDA_SANITIZER=1 python script_name.py`), it should detect and report a data race on tensor `a`. Part 2: Fixing Data Races Write a function `fix_data_race` that correctly synchronizes the streams to avoid the data race detected in Part 1. ```python import torch def fix_data_race(): # Implement this function ... ``` **Instructions**: 1. Use the same tensor `a` from Part 1. 2. Properly synchronize the new stream with the default stream before performing the tensor operation. 3. Ensure no data race is reported by the CUDA Stream Sanitizer when the script is run. **Example**: ```python import torch def fix_data_race(): a = torch.rand(10000, device=\'cuda\') stream = torch.cuda.Stream() with torch.cuda.stream(stream): torch.cuda.current_stream().wait_stream(torch.cuda.default_stream()) torch.mul(a, 5, out=a) # Run the function fix_data_race() ``` **Constraints**: - Make sure to follow the synchronization methods and properly comment your code for clarity. **Submission**: - Implement both functions `detect_data_race` and `fix_data_race`. - Include any necessary import statements. - Ensure your code runs without errors.","solution":"import torch def detect_data_race(): a = torch.rand(10000, device=\'cuda\') stream = torch.cuda.Stream() # Perform the operation in a new CUDA stream without synchronization with torch.cuda.stream(stream): torch.mul(a, 2, out=a) def fix_data_race(): a = torch.rand(10000, device=\'cuda\') stream = torch.cuda.Stream() with torch.cuda.stream(stream): # Synchronize with the default stream to avoid data race torch.cuda.current_stream().wait_stream(torch.cuda.default_stream()) torch.mul(a, 2, out=a)"},{"question":"Objective Demonstrate your understanding of the `runpy` module by writing a Python script that uses `runpy.run_module` and `runpy.run_path` to achieve specified outcomes. Question Description You are tasked with creating a Python script that executes a given list of modules and scripts using the `runpy` module. Your script should: 1. Execute a list of module names (names provided) using `runpy.run_module`. 2. Execute a list of script file paths (paths provided) using `runpy.run_path`. 3. Collect and print the output globals dictionary for each executed module and script. Input - `modules`: A list of strings, where each string is an absolute module name. - `scripts`: A list of strings, where each string is a valid filesystem path to a Python script file. Output - Print the globals dictionary for each executed module and script, preceded by a header indicating whether it is a result of `runpy.run_module` or `runpy.run_path`. Constraints - Your script should handle possible exceptions that might arise from nonexistent modules or scripts and print appropriate error messages. - Assume that the lists provided contain unique and valid formats (correct module names and valid file paths). Performance Requirements - Ensure that the script is efficient, and handles imports and executions with the minimal overhead. - The script should maintain thread safety when altering the `sys` module isn\'t necessary. Example ```python import runpy def execute_modules_and_scripts(modules, scripts): for module in modules: try: result = runpy.run_module(module) print(f\\"Module: {module}nResult: {result}n\\") except Exception as e: print(f\\"Failed to execute module {module}: {e}\\") for script in scripts: try: result = runpy.run_path(script) print(f\\"Script: {script}nResult: {result}n\\") except Exception as e: print(f\\"Failed to execute script {script}: {e}\\") # Example input modules = [\\"mymodule\\", \\"anothermodule\\"] scripts = [\\"/path/to/script.py\\", \\"/path/to/another_script.py\\"] execute_modules_and_scripts(modules, scripts) ``` Note: Replace `mymodule`, `anothermodule`, and file paths with actual module names and valid script file paths to test.","solution":"import runpy def execute_modules_and_scripts(modules, scripts): for module in modules: try: result = runpy.run_module(module) print(f\\"Module: {module}nResult: {result}n\\") except Exception as e: print(f\\"Failed to execute module {module}: {e}\\") for script in scripts: try: result = runpy.run_path(script) print(f\\"Script: {script}nResult: {result}n\\") except Exception as e: print(f\\"Failed to execute script {script}: {e}\\") # Example input (replace with actual module names and script file paths) modules = [\\"example_module\\", \\"another_example_module\\"] scripts = [\\"./example_script.py\\", \\"./another_example_script.py\\"] execute_modules_and_scripts(modules, scripts)"},{"question":"Understanding PyTorch UntypedStorage and Tensor Manipulation **Objective:** The goal of this exercise is to demonstrate the understanding of PyTorch\'s `torch.UntypedStorage` and its interaction with tensor objects. You will implement a function that utilizes low-level tensor storage operations to perform certain manipulations on tensors. **Problem Statement:** Implement a function `manipulate_tensor_storage(tensor: torch.Tensor) -> torch.Tensor` that performs the following tasks: 1. **Clone Storage:** - Obtain the `UntypedStorage` of the input tensor. - Clone this storage. 2. **Modify Cloned Storage:** - Zero out all the elements in the cloned storage. 3. **Set New Storage to Tensor:** - Set the modified cloned storage back to the input tensor while preserving its original shape, stride, and offset properties. 4. **Return Result:** - Return the modified tensor. # Constraints: - You should use methods from `torch.UntypedStorage` to manipulate the storage. - Do not use high-level tensor operations other than the methods mentioned above. - Assume the input tensor will be a 1D tensor for simplicity. # Example: ```python import torch def manipulate_tensor_storage(tensor: torch.Tensor) -> torch.Tensor: # Your implementation here # Example Usage: t = torch.ones(5) print(t) # tensor([1., 1., 1., 1., 1.]) result = manipulate_tensor_storage(t) print(result) # tensor([0., 0., 0., 0., 0.]) ``` # Detailed Steps: 1. Obtain the untyped storage from the input tensor using `untyped_storage()`. 2. Clone this storage using the `clone()` method. 3. Zero out the cloned storage using the `fill_(0)` method. 4. Apply the modified storage back to the tensor using the `set_` method with appropriate parameters for shape, stride, and offset. 5. Return the tensor with the modified storage. **Note:** Direct modification of the tensor\'s storage is for educational purposes to demonstrate understanding of low-level tensor manipulation. In practice, prefer high-level tensor methods for such operations.","solution":"import torch def manipulate_tensor_storage(tensor: torch.Tensor) -> torch.Tensor: # Obtain the untyped storage from the input tensor storage = tensor.untyped_storage() # Clone this storage cloned_storage = storage.clone() # Zero out all elements in the cloned storage cloned_storage.fill_(0) # Set the modified cloned storage back to the tensor with the original shape, stride, and offset tensor.set_(cloned_storage, tensor.storage_offset(), tensor.size(), tensor.stride()) return tensor"},{"question":"You are working on a system where you need universally unique identifiers for different entities. To ensure the uniqueness and appropriate generation of UUIDs, you are required to implement a set of utility functions using the `uuid` module. Your task is to write the following functions: 1. `generate_time_based_uuid()`: Generates a time-based UUID (version 1) ensuring it is multiprocessing-safe. 2. `generate_random_uuid()`: Generates a random UUID (version 4). 3. `generate_name_based_uuid_md5(name: str, namespace: uuid.UUID) -> uuid.UUID`: Generates a name-based UUID using MD5 hashing (version 3) given a name and a namespace UUID. 4. `generate_name_based_uuid_sha1(name: str, namespace: uuid.UUID) -> uuid.UUID`: Generates a name-based UUID using SHA-1 hashing (version 5) given a name and a namespace UUID. 5. `parse_uuid_fields(uuid_str: str) -> Tuple[int, int, int, int, int, int]`: Takes a UUID string of 32 hexadecimal digits (with or without hyphens) and returns a tuple containing the six integer fields of the UUID (time_low, time_mid, time_hi_version, clock_seq_hi_variant, clock_seq_low, node). Constraints and Requirements: - The functions should utilize the appropriate methods and attributes provided by the `uuid` module. - Ensure the `generate_time_based_uuid()` function checks for the `is_safe` attribute to confirm the UUID was generated safely. - The input `namespace` for name-based UUID functions will be one of the predefined namespaces (`uuid.NAMESPACE_DNS`, `uuid.NAMESPACE_URL`, etc.). - The `uuid_str` input to `parse_uuid_fields()` will always be a valid UUID string. Example Usage: ```python # Example usages of each function print(generate_time_based_uuid()) # Outputs a time-based UUID print(generate_random_uuid()) # Outputs a random UUID name = \\"example.com\\" namespace = uuid.NAMESPACE_DNS print(generate_name_based_uuid_md5(name, namespace)) # Outputs an MD5-hashed name-based UUID print(generate_name_based_uuid_sha1(name, namespace)) # Outputs a SHA-1-hashed name-based UUID uuid_str = \\"12345678-1234-5678-1234-567812345678\\" fields = parse_uuid_fields(uuid_str) print(fields) # Outputs the tuple of UUID fields ``` # Function Definitions Implement the functions with the following signatures: ```python import uuid from typing import Tuple def generate_time_based_uuid() -> uuid.UUID: # Your implementation here pass def generate_random_uuid() -> uuid.UUID: # Your implementation here pass def generate_name_based_uuid_md5(name: str, namespace: uuid.UUID) -> uuid.UUID: # Your implementation here pass def generate_name_based_uuid_sha1(name: str, namespace: uuid.UUID) -> uuid.UUID: # Your implementation here pass def parse_uuid_fields(uuid_str: str) -> Tuple[int, int, int, int, int, int]: # Your implementation here pass ``` # Evaluation Criteria - Correct and efficient implementation of the UUID generation functions. - Proper handling of `is_safe` attribute in `generate_time_based_uuid()`. - Accurate extraction and return of UUID fields in `parse_uuid_fields()`. - Code readability and adherence to Python coding standards.","solution":"import uuid from typing import Tuple def generate_time_based_uuid() -> uuid.UUID: Generates a time-based UUID (version 1). Ensures the UUID generation is multiprocessing-safe. # Generate the time-based UUID result = uuid.uuid1() # Check if the UUID is generated safely in a multiprocessing environment assert hasattr(result, \'is_safe\'), \\"Generation of UUID is not multiprocessing-safe\\" return result def generate_random_uuid() -> uuid.UUID: Generates a random UUID (version 4). return uuid.uuid4() def generate_name_based_uuid_md5(name: str, namespace: uuid.UUID) -> uuid.UUID: Generates a name-based UUID using MD5 hashing (version 3) given a name and a namespace UUID. return uuid.uuid3(namespace, name) def generate_name_based_uuid_sha1(name: str, namespace: uuid.UUID) -> uuid.UUID: Generates a name-based UUID using SHA-1 hashing (version 5) given a name and a namespace UUID. return uuid.uuid5(namespace, name) def parse_uuid_fields(uuid_str: str) -> Tuple[int, int, int, int, int, int]: Takes a UUID string of 32 hexadecimal digits (with or without hyphens) and returns a tuple containing the six integer fields of the UUID (time_low, time_mid, time_hi_version, clock_seq_hi_variant, clock_seq_low, node). u = uuid.UUID(uuid_str) return (u.time_low, u.time_mid, u.time_hi_version, u.clock_seq_hi_variant, u.clock_seq_low, u.node)"},{"question":"**Question: Dimensionality Reduction and Classification Pipeline** You are provided with a dataset containing high-dimensional data. Your task is to implement a dimensionality reduction and classification pipeline using scikit-learn. The pipeline should perform the following steps: 1. Standardize the features. 2. Reduce the dimensionality of the data using PCA. 3. Classify the reduced data using a Support Vector Machine (SVM) classifier. # Dataset Assume you have a CSV file named `data.csv` with the following structure: - The first column is the target variable (labels). - The subsequent columns are the features. # Requirements 1. **Data Loading and Preprocessing:** - Load the CSV data into a DataFrame. - Separate the target variable and features. - Standardize the feature set. 2. **Pipeline Construction:** - Construct a pipeline that first applies PCA to reduce the number of features to `n_components`, then trains an SVM classifier on the reduced data. 3. **Model Evaluation:** - Evaluate the performance of the pipeline using cross-validation with 5 folds. - Output the mean cross-validation accuracy. # Input - `file_path`: a string representing the path to the `data.csv` file. - `n_components`: an integer representing the number of principal components to retain. # Output - A float representing the mean cross-validation accuracy. # Constraints - You must use scikit-learn for the implementation. - Ensure that the code can handle datasets with missing values appropriately. # Example Function Signature ```python def dimensionality_reduction_classification_pipeline(file_path: str, n_components: int) -> float: # Your implementation here pass ``` # Notes - Use `StandardScaler` for standardizing the features. - Use `PCA` from `sklearn.decomposition`. - Use `SVC` from `sklearn.svm`. - Use `cross_val_score` from `sklearn.model_selection` for cross-validation. Make sure to handle any data preprocessing steps such as handling missing values, standardizing the data, etc.","solution":"import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.svm import SVC from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_val_score def dimensionality_reduction_classification_pipeline(file_path: str, n_components: int) -> float: Constructs a pipeline that performs PCA followed by SVM classification, and evaluates the mean accuracy using cross-validation. Parameters: - file_path: str, path to the CSV file containing the dataset. - n_components: int, number of principal components to retain in PCA. Returns: - mean_accuracy: float, mean cross-validation accuracy. # Load the dataset data = pd.read_csv(file_path) # Separate features and target variable X = data.iloc[:, 1:].values y = data.iloc[:, 0].values # Handling missing values by filling with the mean of the column X = pd.DataFrame(X).fillna(pd.DataFrame(X).mean()).values # Construct the pipeline pipeline = Pipeline(steps=[ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=n_components)), (\'svm\', SVC()) ]) # Perform cross-validation and compute mean accuracy scores = cross_val_score(pipeline, X, y, cv=5) mean_accuracy = scores.mean() return mean_accuracy"},{"question":"# Custom Deep Copy Implementation **Objective:** You are tasked with implementing a custom deep copy method for a complex object in Python, demonstrating your understanding of the `deepcopy` function and handling recursive references. **Problem Statement:** You will implement a `Book` class that represents a book in a library. Each book can have multiple references to other books (e.g., suggested reading list). You will also implement custom deep copy behavior for this class to ensure that deep copies of `Book` instances are correctly handled. Specifically, you need to: 1. Define a `Book` class with the following attributes: - `title` (string): The title of the book. - `author` (string): The author of the book. - `references` (list): A list of other `Book` instances that this book references. 2. Implement the `__init__` method to initialize these attributes. 3. Implement the `__deepcopy__` method for the `Book` class to handle deep copying. Your method should: - Create a new `Book` instance with the same title and author. - Ensure that the `references` list is deep-copied recursively using the `copy.deepcopy` function. - Use the memo dictionary to handle recursive references and avoid infinite loops. **Input:** - There are no direct inputs, as you will define and test the `Book` class directly in your code. **Output:** - Ensure that deep copies of `Book` instances are correctly created without modifying the original objects. **Example Usage:** ```python import copy class Book: def __init__(self, title, author): self.title = title self.author = author self.references = [] def __deepcopy__(self, memo): # Custom deep copy logic new_book = Book(self.title, self.author) memo[id(self)] = new_book new_book.references = copy.deepcopy(self.references, memo) return new_book # Example test case book1 = Book(\\"Book 1\\", \\"Author A\\") book2 = Book(\\"Book 2\\", \\"Author B\\") book3 = Book(\\"Book 3\\", \\"Author C\\") book1.references.append(book2) book2.references.append(book3) book3.references.append(book1) # Circular reference # Deep copy book1 book1_copy = copy.deepcopy(book1) # Check that the deep copy is correct and does not affect the original assert book1_copy is not book1 assert book1_copy.references[0] is not book1.references[0] assert book1_copy.references[0].references[0] is not book2.references[0] assert book1_copy.references[0].references[0].references[0] is not book3.references[0] print(\\"Custom deep copy implementation is correct.\\") ``` Ensure your code handles the circular reference in the example and the assertions pass.","solution":"import copy class Book: def __init__(self, title, author): self.title = title self.author = author self.references = [] def __deepcopy__(self, memo): # Check if this object is already in the memo dictionary if id(self) in memo: return memo[id(self)] # Create a new Book instance new_book = Book(self.title, self.author) # Put the new book into the memo dictionary memo[id(self)] = new_book # Deep copy the references list new_book.references = copy.deepcopy(self.references, memo) return new_book"},{"question":"# Question: Creating and Using Color Palettes in Seaborn Objective Demonstrate your understanding of how to create and apply various color palettes in seaborn plots. Problem Statement You are given a dataset containing information about various species of flowers. Your task is to create scatter plots using seaborn, applying different types of color palettes to distinguish between species. Dataset The dataset is a pandas DataFrame with the following columns: - `sepal_length`: Length of the sepal (float). - `sepal_width`: Width of the sepal (float). - `petal_length`: Length of the petal (float). - `petal_width`: Width of the petal (float). - `species`: The species of the flower (categorical string). Requirements 1. Load the dataset into a pandas DataFrame. 2. Create a scatter plot using seaborn\'s `sns.scatterplot` function with `sepal_length` on the x-axis and `sepal_width` on the y-axis. 3. Use different color palettes for each species: - Default palette: `sns.color_palette()` - Pastel palette: `sns.color_palette(\\"pastel\\")` - HUSL palette with 5 colors: `sns.color_palette(\\"husl\\", 5)` - Set2 categorical Color Brewer palette: `sns.color_palette(\\"Set2\\")` 4. For each plot, ensure that the species are distinguished using the hue parameter. 5. Display each plot in sequence. Function Signature ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_seaborn_plots(data: pd.DataFrame) -> None: Creates scatter plots using various color palettes from seaborn. Parameters: data (pd.DataFrame): A dataframe with columns \'sepal_length\', \'sepal_width\', \'petal_length\', \'petal_width\', \'species\'. Returns: None: The function should display plots using matplotlib\'s plt.show(). # Your code here ``` Example Usage ```python data = { \'sepal_length\': [5.1, 4.9, 4.7, 4.6, 5.0], \'sepal_width\': [3.5, 3.0, 3.2, 3.1, 3.6], \'petal_length\': [1.4, 1.4, 1.3, 1.5, 1.4], \'petal_width\': [0.2, 0.2, 0.2, 0.2, 0.2], \'species\': [\'setosa\', \'setosa\', \'setosa\', \'setosa\', \'setosa\'] } df = pd.DataFrame(data) create_seaborn_plots(df) ``` **Constraints:** - Assume the input DataFrame is always valid and non-empty with no missing values. - Ensure that the plots are clear and properly labeled. - Utilize the appropriate seaborn functions and color palette methods as demonstrated in the documentation.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_seaborn_plots(data: pd.DataFrame) -> None: Creates scatter plots using various color palettes from seaborn. Parameters: data (pd.DataFrame): A dataframe with columns \'sepal_length\', \'sepal_width\', \'petal_length\', \'petal_width\', \'species\'. Returns: None: The function should display plots using matplotlib\'s plt.show(). # Default palette plt.figure(figsize=(8, 6)) sns.scatterplot(x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=sns.color_palette(), data=data) plt.title(\'Scatter plot with Default Palette\') plt.show() # Pastel palette plt.figure(figsize=(8, 6)) sns.scatterplot(x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=sns.color_palette(\\"pastel\\"), data=data) plt.title(\'Scatter plot with Pastel Palette\') plt.show() # HUSL palette with 5 colors plt.figure(figsize=(8, 6)) sns.scatterplot(x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=sns.color_palette(\\"husl\\", 5), data=data) plt.title(\'Scatter plot with HUSL Palette\') plt.show() # Set2 palette from Color Brewer plt.figure(figsize=(8, 6)) sns.scatterplot(x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=sns.color_palette(\\"Set2\\"), data=data) plt.title(\'Scatter plot with Set2 Palette\') plt.show()"},{"question":"**Objective:** Implement a Python function using the `nis` module that retrieves user records from NIS based on specific conditions. **Function Specification:** 1. **Function Name:** `get_users_by_condition` 2. **Input:** - `condition_func` (Callable[[str, str], bool]): A function that takes two strings (a key and a value from the NIS map) and returns a boolean indicating whether the key-value pair meets the condition. - `mapname` (str): The name of the NIS map to query. 3. **Output:** - A dictionary mapping keys to values for those pairs in the given NIS map that satisfy the condition specified by `condition_func`. **Constraints:** - The function should handle exceptions gracefully. If an error is encountered with the NIS operations, it should return an empty dictionary. - If the specified map does not exist or the key-value pair does not meet the condition, the result should be an empty dictionary for that key-value pair. **Performance Requirements:** - The function should be efficient in querying and filtering the data from the NIS map. Avoid unnecessary operations and ensure the code is optimized for large NIS maps. **Examples:** ```python def is_user_key_high(uid: str, userdata: str) -> bool: # Example condition function that returns True if the user ID (key) is greater than \'50000\' return int(uid) > 50000 # Assuming that the NIS map named \\"passwd.byuid\\" has user records with user ID (uid) as keys users = get_users_by_condition(is_user_key_high, \\"passwd.byuid\\") print(users) # Expected Output: A dictionary containing user records where the key (user ID) is greater than \'50000\', if such keys exist. ``` **Notes:** - The `nis.cat` function should be primarily used to get all key-value pairs from the specified NIS map. - The condition function should be designed to work independently and should be flexible for different conditions. Implement the function `get_users_by_condition` as specified.","solution":"import nis from typing import Callable, Dict def get_users_by_condition(condition_func: Callable[[str, str], bool], mapname: str) -> Dict[str, str]: Retrieves key-value pairs from the specified NIS map where the condition function is True. Parameters: - condition_func: A callable that takes two strings (key, value) and returns a boolean. - mapname: The name of the NIS map to query. Returns: - A dictionary of key-value pairs from the NIS map that satisfy the condition. try: nis_map = nis.cat(mapname) except nis.error: return {} filtered_map = {key: value for key, value in nis_map.items() if condition_func(key, value)} return filtered_map"},{"question":"Athlete\'s Performance Tracker You are tasked with creating a small program that tracks the performance of athletes in different sporting events. You need to implement a function `track_performance` that takes a list of strings as its input. Each string represents an athlete\'s performance record in the format \\"name:event:score\\". Your function should process this input and create a dictionary where: - The keys are the athlete names. - The values are dictionaries. Each of these dictionaries has the event names as keys and the corresponding scores as values. If an athlete participates in an event more than once, only record their highest score for each event. **Function Signature:** ```python def track_performance(records: List[str]) -> Dict[str, Dict[str, int]]: ``` # Input - `records`: A list of strings, where each string is in the format \\"name:event:score\\". - `name` is a string (1 <= len(name) <= 50) - `event` is a string (1 <= len(event) <= 50) - `score` is an integer (0 <= score <= 100), representing the athlete\'s score in that event # Output - A dictionary with athlete names as keys and another dictionary as values. The inner dictionary\'s keys are event names, and the values are the highest scores for those events. # Constraints - The input list will have at most 1000 entries. # Example ```python records = [ \\"John:100m:9\\", \\"Alice:Marathon:100\\", \\"John:100m:10\\", \\"Alice:Marathon:95\\", \\"Bob:HighJump:5\\", \\"John:Marathon:80\\", \\"Bob:HighJump:7\\" ] output = { \\"John\\": { \\"100m\\": 10, \\"Marathon\\": 80 }, \\"Alice\\": { \\"Marathon\\": 100 }, \\"Bob\\": { \\"HighJump\\": 7 } } ``` # Note 1. John participated in \\"100m\\" twice, and his highest score in \\"100m\\" is recorded as 10. 2. Alice\'s highest score in the \\"Marathon\\" event is recorded as 100, even though she participated multiple times with different scores. 3. Bob\'s highest score in \\"HighJump\\" is recorded as 7. **Your task is to implement the `track_performance` function.**","solution":"from typing import List, Dict def track_performance(records: List[str]) -> Dict[str, Dict[str, int]]: Processes a list of athlete performance records and returns a dictionary with the highest scores for each event. performance_dict = {} for record in records: name, event, score_str = record.split(\':\') score = int(score_str) if name not in performance_dict: performance_dict[name] = {} event_scores = performance_dict[name] if event not in event_scores or event_scores[event] < score: event_scores[event] = score return performance_dict"},{"question":"# Problem Description You are required to write a Python function that takes a list of mixed types (integers, floats, decimals, and strings) representing rational numbers and returns a list of corresponding `Fraction` instances with their denominators limited to at most a given value. # Function Signature ```python def convert_to_fractions(mixed_list: list, max_denominator: int) -> list: ``` # Input - `mixed_list`: A list containing integers, floats, decimals, and/or strings that can be parsed into rational numbers. - `max_denominator`: An integer value representing the maximum allowed denominator for the resultant fractions. # Output - A list of `Fraction` instances, each with a denominator at most `max_denominator`. # Constraints - The elements in `mixed_list` will be valid inputs for the `Fraction` constructor. - `max_denominator` will be a positive integer greater than 0. # Example ```python from decimal import Decimal # Input mixed_list = [16, \'3/7\', 2.25, Decimal(\'1.1\'), \'7e-6\', \'-1.25\'] max_denominator = 1000 # Output output = convert_to_fractions(mixed_list, max_denominator) for frac in output: print(frac) # Expected Output # Fraction(16, 1) # Fraction(3, 7) # Fraction(9, 4) # Fraction(11, 10) # Fraction(7, 1000000) converted to Fraction(0, 1) after limiting the denominator to 1000 # Fraction(-5, 4) ``` # Notes - You should use the `limit_denominator` method on each `Fraction` to ensure the denominator does not exceed `max_denominator`. - Make sure to handle all cases such as negative fractions, fractions representing small values, and ensure consistency in the output format.","solution":"from fractions import Fraction from decimal import Decimal def convert_to_fractions(mixed_list: list, max_denominator: int) -> list: Converts a list of mixed types (integers, floats, decimals, strings) to a list of Fractions with denominators limited to at most max_denominator. Args: mixed_list (list): A list of values that can be parsed into rational numbers. max_denominator (int): The maximum allowed value for the denominators of the resulting fractions. Returns: list: A list of Fraction instances. fractions_list = [] for item in mixed_list: frac = Fraction(item).limit_denominator(max_denominator) fractions_list.append(frac) return fractions_list"},{"question":"You are tasked with implementing a function that connects to an NNTP server, retrieves the latest articles from a specified newsgroup, and prints a summary of those articles. The function should handle connection errors gracefully and output meaningful error messages. # Function Specification **Function Name**: `fetch_latest_articles` **Parameters**: 1. `server`: A string representing the NNTP server hostname. 2. `newsgroup`: A string representing the newsgroup name. 3. `count`: An integer representing the number of latest articles to fetch. **Behavior**: The function should: 1. Establish a connection to the NNTP server using the `NNTP` class. 2. Select the specified newsgroup. 3. Retrieve the specified number of latest articles\' subject headers. 4. Print the subjects of these articles. 5. Handle connection errors and server response errors gracefully. **Return Value**: The function does not return anything. Instead, it prints the results. **Constraints**: - If the connection to the server fails, print \\"Failed to connect to the server.\\" - If selecting the newsgroup fails, print \\"Invalid newsgroup.\\" - If fetching articles fails, print \\"Failed to fetch articles.\\" # Example ```python def fetch_latest_articles(server: str, newsgroup: str, count: int) -> None: pass # Example usage: fetch_latest_articles(\'news.gmane.io\', \'gmane.comp.python.committers\', 5) ``` **Example output**: ``` Subject: Re: Commit privileges for Łukasz Langa Subject: Re: 3.2 alpha 2 freeze Subject: Re: 3.2 alpha 2 freeze Subject: Re: Commit privileges for Łukasz Langa Subject: Updated ssh key ``` # Notes - Use the `nntplib` module methods such as `NNTP.connect()`, `NNTP.group()`, and `NNTP.over()`. - To handle the retrieval and decoding of message headers, refer to the utility function `nntplib.decode_header()`. - Ensure proper exception handling for network errors and invalid responses. **Implementation Tips**: - Refer to the examples in the documentation for an understanding of connection and command execution. - Make sure to handle any resource cleanup (e.g., closing the connection) even when errors occur.","solution":"import nntplib from nntplib import NNTP, decode_header from socket import gaierror def fetch_latest_articles(server: str, newsgroup: str, count: int) -> None: try: # Establish a connection to the NNTP server with NNTP(server) as nntp: try: # Select the specified newsgroup resp, count, first, last, name = nntp.group(newsgroup) except nntplib.NNTPTemporaryError: print(\\"Invalid newsgroup.\\") return # Fetch the range of articles start = max(int(last) - count + 1, int(first)) _, articles = nntp.over((start, last)) # Print the subjects of the articles for id, article in articles: print(f\\"Subject: {decode_header(article[\'subject\'])}\\") except (gaierror, nntplib.NNTPTemporaryError): print(\\"Failed to connect to the server.\\") except nntplib.NNTPDataError: print(\\"Failed to fetch articles.\\")"},{"question":"Advanced Color Space Conversion and Manipulation You are tasked with developing a utility for color manipulation and conversion between different color spaces using the `colorsys` module. This utility will: 1. Convert a color from RGB to HLS, modify its lightness, and convert it back to RGB. 2. Convert a list of colors from RGB to HSV and filter out colors based on their saturation. 3. Calculate the average color in the YIQ color space from a list of RGB colors and convert it back to RGB. Part 1: Modify Lightness Write a function `modify_lightness(rgb_color: Tuple[float, float, float], delta_l: float) -> Tuple[float, float, float]` that: - Takes an RGB color as a tuple of three floats `(r, g, b)` and a float `delta_l` which represents the change in lightness. - Converts the RGB color to HLS, modifies the lightness by `delta_l`, ensures the lightness stays between 0 and 1, and converts it back to RGB. - Returns the modified RGB color as a tuple of three floats. Part 2: Saturation Filter Write a function `filter_by_saturation(rgb_colors: List[Tuple[float, float, float]], threshold: float) -> List[Tuple[float, float, float]]` that: - Takes a list of RGB colors and a float `threshold`. - Converts each color in the list from RGB to HSV, and filters out colors whose saturation is less than the `threshold`. - Returns a list of the remaining RGB colors as tuples of three floats. Part 3: Average YIQ Color Write a function `average_yiq_color(rgb_colors: List[Tuple[float, float, float]]) -> Tuple[float, float, float]` that: - Takes a list of RGB colors. - Converts each color to YIQ, calculates the average YIQ color, and converts the average back to RGB. - Returns the average RGB color as a tuple of three floats. # Constraints - Assure that all input RGB values `(r, g, b)` are between 0 and 1. - Assure that `delta_l` can be a positive or negative float but must modify lightness to remain within the valid range [0, 1]. - Assume the length of the input list will not exceed 1000 colors. - Ensure the functions operate efficiently to handle up to 1000 colors in a reasonable time. # Example ```python # Part 1 Example print(modify_lightness((0.5, 0.5, 0.5), 0.1)) # Expected output: A color with increased lightness # Part 2 Example rgb_colors = [(0.2, 0.4, 0.4), (0.8, 0.6, 0.1), (0.1, 0.1, 0.1)] threshold = 0.3 print(filter_by_saturation(rgb_colors, threshold)) # Expected: List of colors with sufficient saturation # Part 3 Example rgb_colors = [(0.2, 0.4, 0.4), (0.8, 0.6, 0.1)] print(average_yiq_color(rgb_colors)) # Expected: Average color in RGB ``` You should implement these functions in Python and ensure their correctness using the provided examples.","solution":"import colorsys from typing import Tuple, List def modify_lightness(rgb_color: Tuple[float, float, float], delta_l: float) -> Tuple[float, float, float]: Modifies the lightness of an RGB color and returns the modified RGB color. Parameters: rgb_color: Tuple[float, float, float] - The original RGB color. delta_l: float - The change in lightness to be applied (can be positive or negative). Returns: Tuple[float, float, float] - The modified RGB color. r, g, b = rgb_color h, l, s = colorsys.rgb_to_hls(r, g, b) l = max(0, min(1, l + delta_l)) new_r, new_g, new_b = colorsys.hls_to_rgb(h, l, s) return (new_r, new_g, new_b) def filter_by_saturation(rgb_colors: List[Tuple[float, float, float]], threshold: float) -> List[Tuple[float, float, float]]: Filters the given list of RGB colors by saturation based on the threshold. Parameters: rgb_colors: List[Tuple[float, float, float]] - List of RGB colors. threshold: float - The saturation threshold. Returns: List[Tuple[float, float, float]] - List of RGB colors that have saturation >= threshold. filtered_colors = [] for rgb in rgb_colors: r, g, b = rgb h, s, v = colorsys.rgb_to_hsv(r, g, b) if s >= threshold: filtered_colors.append((r, g, b)) return filtered_colors def average_yiq_color(rgb_colors: List[Tuple[float, float, float]]) -> Tuple[float, float, float]: Calculates the average YIQ color from a list of RGB colors and returns the average color in RGB. Parameters: rgb_colors: List[Tuple[float, float, float]] - List of RGB colors. Returns: Tuple[float, float, float] - The average RGB color. yiq_colors = [colorsys.rgb_to_yiq(r, g, b) for r, g, b in rgb_colors] avg_y = sum(y for y, i, q in yiq_colors) / len(yiq_colors) avg_i = sum(i for y, i, q in yiq_colors) / len(yiq_colors) avg_q = sum(q for y, i, q in yiq_colors) / len(yiq_colors) avg_r, avg_g, avg_b = colorsys.yiq_to_rgb(avg_y, avg_i, avg_q) return (avg_r, avg_g, avg_b)"},{"question":"<|Analysis Begin|> The provided documentation highlights the `sns.plotting_context()` function in the seaborn library. The documentation explains how to: - Retrieve the current default values for the plotting context parameters. - View the values of predefined styles using the function. - Temporarily change plotting context parameters within a block of code using a context manager pattern with `with sns.plotting_context(\\"style\\")`. To design a challenging and comprehensive assessment question, I\'ll focus on these points by including requirements that test the ability to: 1. Retrieve and print current plotting parameters. 2. Switch between different predefined styles. 3. Create visualizations with different contexts to demonstrate the understanding of temporary context changes. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Context:** Seaborn is a Python visualization library based on matplotlib that provides a high-level interface for drawing attractive and informative statistical graphics. One of its features is the ability to set and switch between different plotting contexts, which allow you to scale plot elements for better readability in different contexts such as notebooks, talks, and papers. **Objective:** Write a Python function using seaborn that demonstrates knowledge of the `sns.plotting_context()` function and its uses. The function should: 1. Retrieve and print the current default plotting context parameters. 2. Create and display a simple line plot using the default context and then switch to another context (\\"talk\\") and display the same plot again. 3. Use a context manager to temporarily change to the \\"poster\\" context, create another plot, and then revert back to the default context. **Function Signature:** ```python def demonstrate_plotting_context(): pass ``` **Requirements:** 1. Install `matplotlib` and `seaborn` before implementing your code. 2. Use the `sns.lineplot()` function to create line plots. 3. Ensure context changes are correctly applied in the visual output. **Example Output:** ```python # Output of the function should display the current plotting context parameters. { \'font.size\': 12.0, \'axes.titlesize\': 14.4, \'axes.labelsize\': 12.0, \'xtick.labelsize\': 11.0, \'ytick.labelsize\': 11.0, \'legend.fontsize\': 12.0, \'axes.linewidth\': 1.25, \'grid.linewidth\': 1.0, \'lines.linewidth\': 1.25, \'lines.markersize\': 6.0, \'patch.linewidth\': 1.0 } # Followed by plots as specified in the requirements ``` **Constraints:** - Ensure that your code complies with good practice in terms of coding style and documentation. - Use Matplotlib\'s `plt.show()` to display the plots for visual confirmation. **Note:** This question assesses the understanding of seaborn\'s plotting context functionality, as well as the ability to create and manage different visual contexts within code blocks effectively.","solution":"import seaborn as sns import matplotlib.pyplot as plt def demonstrate_plotting_context(): # Retrieve and print the current default plotting context parameters current_context = sns.plotting_context() print(\\"Current context parameters:\\", current_context) # Sample data for plotting data = [1, 2, 3, 4, 5] # Create and display a simple line plot using the default context sns.lineplot(data=data) plt.title(\\"Default context\\") plt.show() # Switch to \\"talk\\" context and display the simple line plot again sns.set_context(\\"talk\\") sns.lineplot(data=data) plt.title(\\"Talk context\\") plt.show() # Use a context manager to temporarily change to the \\"poster\\" context and create another plot with sns.plotting_context(\\"poster\\"): sns.lineplot(data=data) plt.title(\\"Poster context\\") plt.show() # Reverting back to the default context sns.set_context(\\"notebook\\")"},{"question":"Objective Write a function that utilizes the `operator` module to perform a series of complex transformations and operations on data. Problem Statement You are given a list of dictionaries representing a collection of records. Each record contains information about an item, including its `id`, `name`, and `value`. Your task is to implement a function `transform_records(data: List[Dict[str, Union[int, str]]]) -> List[Tuple[int, str, int]]` that processes this data in the following steps: 1. **Filter**: Remove all records where the `value` is less than or equal to 10. 2. **Sort**: Sort the remaining records in descending order based on their `value`. 3. **Extract**: Extract the `id`, `name`, and `value` from each record and return them as a tuple. 4. **Output Format**: The final output should be a list of tuples, each containing the `id`, `name`, and `value` of the records. Input - `data`: A list of dictionaries. Each dictionary represents a record and has the following structure: ```python [ {\\"id\\": int, \\"name\\": str, \\"value\\": int}, ... ] ``` Output - A list of tuples. Each tuple contains the `id`, `name`, and `value` of the records that meet the criteria, sorted appropriately: ```python [ (id, name, value), ... ] ``` Constraints 1. `id` is a positive integer. 2. `name` is a non-empty string. 3. `value` is an integer and can be positive, negative, or zero. Example Given the input: ```python data = [ {\\"id\\": 1, \\"name\\": \\"Item A\\", \\"value\\": 15}, {\\"id\\": 2, \\"name\\": \\"Item B\\", \\"value\\": 8}, {\\"id\\": 3, \\"name\\": \\"Item C\\", \\"value\\": 22}, {\\"id\\": 4, \\"name\\": \\"Item D\\", \\"value\\": 5} ] ``` The function should return: ```python [(3, \\"Item C\\", 22), (1, \\"Item A\\", 15)] ``` Notes - Use functions from the `operator` module to implement the filtering, sorting, and extraction steps wherever possible. - Ensure that your function is both efficient and readable. Good luck!","solution":"from typing import List, Dict, Union, Tuple import operator def transform_records(data: List[Dict[str, Union[int, str]]]) -> List[Tuple[int, str, int]]: Processes the list of records to filter, sort, and extract required information. :param data: List of dictionaries containing record details. :return: List of tuples containing the id, name, and value. # Step 1: Filter records where the value is less than or equal to 10 filtered_data = list(filter(lambda record: record[\'value\'] > 10, data)) # Step 2: Sort the remaining records in descending order based on their value sorted_data = sorted(filtered_data, key=operator.itemgetter(\'value\'), reverse=True) # Step 3: Extract the id, name, and value from each record and create tuples result = list(map(lambda record: (record[\'id\'], record[\'name\'], record[\'value\']), sorted_data)) return result"},{"question":"Objective: Test the understanding of Python\'s `re` module for creating and using regular expressions, including using capturing groups and lookahead assertions. Question: You are given a list of email addresses and a list of domain names. Your task is to find and replace any email addresses that belong to the given domains with a masked version. The masked version should replace the characters before the \\"@\\" with stars, but leave the domain part intact. 1. **Function Signature**: ```python def mask_emails(text: str, domains: list) -> str: ``` 2. **Input**: - `text`: A string containing multiple email addresses. - `domains`: A list of domain names for which we need to mask the email addresses. 3. **Output**: - `A string` with the email addresses belonging to the given domains masked appropriately. 4. **Example**: ```python text = \\"Contact us at info@example.com, support@mydomain.org or sales@yourdomain.com\\" domains = [\\"example.com\\", \\"yourdomain.com\\"] result = mask_emails(text, domains) print(result) # Output: \\"Contact us at ****@example.com, support@mydomain.org or *****@yourdomain.com\\" ``` 5. **Constraints**: - Assume that all inputs are valid email addresses and domain names. - An email address is defined as a string with characters before and after an \\"@\\" symbol. - A domain name contains only alphanumeric characters, periods, and hyphens. Requirements: - Use the `re` module to identify and replace the email addresses. - Ensure your solution handles edge cases, such as multiple email addresses in the same text. Hints: - You can use capturing groups to separate the local-part and domain of the email addresses. - Use a lookahead assertion to ensure the domain matches one from the given list. - Use the `sub()` method with a function to generate the masked email. Example Solution: ```python import re def mask_emails(text: str, domains: list) -> str: domains_pattern = \\"|\\".join(map(re.escape, domains)) pattern = re.compile(rf\\"(b[A-Za-z0-9._%+-]+)@({domains_pattern})\\") def mask(match): local_part = match.group(1) domain_part = match.group(2) return \'*\' * len(local_part) + \'@\' + domain_part masked_text = pattern.sub(mask, text) return masked_text ``` - Test your function with various cases to ensure coverage of typical and edge cases.","solution":"import re def mask_emails(text: str, domains: list) -> str: domains_pattern = \\"|\\".join(map(re.escape, domains)) pattern = re.compile(rf\\"(b[A-Za-z0-9._%+-]+)@({domains_pattern})\\") def mask(match): local_part = match.group(1) domain_part = match.group(2) return \'*\' * len(local_part) + \'@\' + domain_part masked_text = pattern.sub(mask, text) return masked_text"},{"question":"**Problem Statement** You are tasked with implementing a function that processes a given XML snippet. This function should escape a set of specified characters, then unescape those characters, demonstrating your understanding of the `escape` and `unescape` functions from the `xml.sax.saxutils` module. # Function Signature ```python def process_xml_snippet(xml_snippet: str, escape_entities: dict, unescape_entities: dict) -> str: pass ``` # Input - `xml_snippet`: A string representing the XML snippet that needs to be processed. - `escape_entities`: A dictionary where keys are substrings that need to be escaped, and values are their corresponding escaped representations. - `unescape_entities`: A dictionary where keys are substrings that need to be unescaped, and values are their corresponding unescaped representations. # Output - A string representing the XML snippet after escaping and then unescaping the specified characters. # Constraints - The XML snippet will have a maximum length of 10000 characters. - The number of entities for escaping and unescaping will not exceed 100. - The keys and values in `escape_entities` and `unescape_entities` will be unique and non-empty strings. # Example ```python xml_snippet = \\"This is a test <xml> & it\'s going to be \'fun\'!\\" escape_entities = {\\"&\\": \\"&amp;\\", \\"<\\": \\"&lt;\\", \\">\\": \\"&gt;\\"} unescape_entities = {\\"&amp;\\": \\"&\\", \\"&lt;\\": \\"<\\", \\"&gt;\\": \\">\\"} result = process_xml_snippet(xml_snippet, escape_entities, unescape_entities) print(result) # Output should be: \\"This is a test <xml> & it\'s going to be \'fun\'!\\" ``` # Notes 1. The function should first escape the specified characters using the `escape` function from `xml.sax.saxutils`. 2. After escaping, the function should unescape the specified characters using the `unescape` function from `xml.sax.saxutils`. 3. The escape and unescape processes should not affect the characters unless specified in the `escape_entities` and `unescape_entities` dictionaries respectively.","solution":"from xml.sax.saxutils import escape, unescape def process_xml_snippet(xml_snippet: str, escape_entities: dict, unescape_entities: dict) -> str: Process an XML snippet by first escaping and then unescaping specified characters. :param xml_snippet: The XML snippet to be processed. :param escape_entities: A dictionary with the characters to be escaped and their replacements. :param unescape_entities: A dictionary with the characters to be unescaped and their replacements. :return: The processed XML snippet. # First, escape the specified characters escaped_snippet = escape(xml_snippet, escape_entities) # Then, unescape the specified characters unescaped_snippet = unescape(escaped_snippet, unescape_entities) return unescaped_snippet"},{"question":"You are required to implement two functions to handle file compression and decompression using the `bz2` module. Task 1: File Compression Implement the function `compress_file(input_file: str, output_file: str, compresslevel: int = 9) -> None` that: - Takes three arguments: 1. `input_file` (str): The name of the input file to compress. 2. `output_file` (str): The name of the output file to save the compressed data. 3. `compresslevel` (int): Compression level (default is 9, can range from 1 to 9). - Reads data from `input_file`. - Compresses the data using the specified `compresslevel`. - Writes the compressed data to `output_file`. Task 2: File Decompression Implement the function `decompress_file(input_file: str, output_file: str) -> None` that: - Takes two arguments: 1. `input_file` (str): The name of the file containing compressed data. 2. `output_file` (str): The name of the output file to save the decompressed data. - Reads compressed data from `input_file`. - Decompresses the data. - Writes the decompressed data to `output_file`. Constraints and Requirements: - The file operations should handle large files efficiently. - Include error handling to manage file read/write errors. - Ensure that the decompression accurately restores the original content. Example Usage: ```python # Example Usage compress_file(\'example.txt\', \'example.bz2\', compresslevel=5) decompress_file(\'example.bz2\', \'example_decompressed.txt\') # Check the content of the original and decompressed file with open(\'example.txt\', \'rb\') as f1, open(\'example_decompressed.txt\', \'rb\') as f2: assert f1.read() == f2.read() # This should pass if decompression is correct. ``` Implement the `compress_file` and `decompress_file` functions.","solution":"import bz2 def compress_file(input_file: str, output_file: str, compresslevel: int = 9) -> None: Compresses input_file and writes the compressed data to output_file. :param input_file: str: Name of the input file to compress. :param output_file: str: Name of the output file to save the compressed data. :param compresslevel: int: Compression level (default is 9, can range from 1 to 9). try: with open(input_file, \'rb\') as file_in, bz2.BZ2File(output_file, \'wb\', compresslevel=compresslevel) as file_out: for data in iter(lambda: file_in.read(1024 * 1024), b\'\'): # Reading in chunks of 1MB file_out.write(data) except Exception as e: print(f\\"Error compressing file: {e}\\") def decompress_file(input_file: str, output_file: str) -> None: Decompresses input_file and writes the decompressed data to output_file. :param input_file: str: Name of the file containing compressed data. :param output_file: str: Name of the output file to save the decompressed data. try: with bz2.BZ2File(input_file, \'rb\') as file_in, open(output_file, \'wb\') as file_out: for data in iter(lambda: file_in.read(1024 * 1024), b\'\'): # Reading in chunks of 1MB file_out.write(data) except Exception as e: print(f\\"Error decompressing file: {e}\\")"},{"question":"Python Coding Assessment Question # Objective Implement a set of functions to manipulate and work with bytes objects using various functionalities as described in the `PyBytes` documentation provided. # Problem Statement Using the provided documentation for `PyBytes`, write the following functions in Python: 1. **is_bytes_object(obj)**: - This function should check if the given object is a bytes object (not an instance of a subtype). - **Input**: - `obj`: an object to check. - **Output**: - Return `True` if `obj` is a bytes object, otherwise return `False`. 2. **create_bytes_from_string_and_size(s, n)**: - This function should create a new bytes object from the given string `s` and limit its size to `n` bytes. - **Input**: - `s`: A string to convert to bytes. - `n`: The maximum number of bytes to use from the string. - **Output**: - Return the new bytes object created. - **Constraints**: - If the length of `s` is less than `n`, use the entire string. - If `n` is greater than the length of `s`, your function should return the bytes object as it is. 3. **format_bytes(format_string, *args)**: - This function should return a new bytes object formatted according to the given format string and additional arguments, similar to C\'s `printf`. - **Input**: - `format_string`: The format string defining how the bytes should be formatted. - `*args`: Variable length argument list corresponding to the format string. - **Output**: - Return the formatted bytes object. 4. **concat_bytes(bytes1, bytes2)**: - This function should concatenate two bytes objects and return the new bytes object. - **Input**: - `bytes1`: The first bytes object. - `bytes2`: The second bytes object to concatenate to the first. - **Output**: - Return the concatenated bytes object. 5. **resize_bytes(bytes_obj, new_size)**: - This function should resize the given bytes object to `new_size` bytes. - **Input**: - `bytes_obj`: The bytes object to be resized. - `new_size`: The new size for the bytes object. - **Output**: - Return the resized bytes object. # Example Usage ```python # Test the implemented functions assert is_bytes_object(b\'hello\') == True assert is_bytes_object(\\"hello\\") == False assert create_bytes_from_string_and_size(\\"hello world\\", 5) == b\'hello\' assert create_bytes_from_string_and_size(\\"hello\\", 10) == b\'hello\' assert format_bytes(\\"Hello, %s!\\", b\'World\') == b\'Hello, World!\' assert concat_bytes(b\'Hello, \', b\'World\') == b\'Hello, World\' # Resizing might differ in real implementation, this is just for illustration resized_bytes = resize_bytes(b\'Hello\', 8) assert len(resized_bytes) == 8 ``` # Constraints - You should use the `PyBytes` functionalities as described in the provided documentation to implement your solution. - Handle exceptions and errors properly according to the documentation. - Ensure all the functions are implemented successfully. # Notes - Pay attention to the type requirements and constraints specified in each function. - Think about the edge cases, such as passing `None` or empty strings/bytes objects.","solution":"def is_bytes_object(obj): Check if the given object is a bytes object (not an instance of a subtype). return type(obj) is bytes def create_bytes_from_string_and_size(s, n): Create a new bytes object from the given string `s` and limit its size to `n` bytes. return bytes(s[:n], \'utf-8\') def format_bytes(format_string, *args): Return a new bytes object formatted according to the given format string and additional arguments. formatted_string = format_string % args return bytes(formatted_string, \'utf-8\') def concat_bytes(bytes1, bytes2): Concatenate two bytes objects and return the new bytes object. return bytes1 + bytes2 def resize_bytes(bytes_obj, new_size): Resize the given bytes object to `new_size` bytes. if new_size < len(bytes_obj): return bytes_obj[:new_size] else: return bytes_obj + bytes([0] * (new_size - len(bytes_obj)))"},{"question":"# **Advanced Python Import System** **Objective:** This task aims to assess your understanding of Python\'s import system and your ability to create utility functions that leverage the functionalities provided by the `importlib` module. **Task:** You are required to implement a function `dynamic_import` that handles module imports dynamically based on various conditions. Specifically, the function should: 1. Check if a given module can be imported. 2. If the module can be imported, import it and return it. 3. If the module cannot be imported directly (e.g., it’s a source file), implement a mechanism to import it from a specific file path. 4. Allow for lazy imports where a module is imported only when an attribute of that module is accessed. **Function Signature:** ```python def dynamic_import(module_name: str, file_path: Optional[str] = None, lazy: bool = False): Dynamically import a module based on the given conditions. Parameters: - module_name (str): Name of the module to import. - file_path (Optional[str]): The file path to import the module from, if necessary. - lazy (bool): Whether to implement lazy importing. Returns: - module: The imported module object. # Your implementation here ``` **Input:** - `module_name`: A string representing the name of the module you want to import. - `file_path`: An optional string representing the file path from which to import the module if it cannot be imported directly by its name. - `lazy`: A boolean indicating whether to delay the module import until an attribute of the module is accessed. **Output:** - Return the imported module object. **Example Usage:** 1. Import a standard module: ```python os_module = dynamic_import(\\"os\\") print(os_module.name) # Output: \'posix\' or \'nt\', depending on your OS ``` 2. Import from a file path: ```python custom_module = dynamic_import(\\"custom_module\\", file_path=\\"/path/to/custom_module.py\\") print(custom_module.some_function()) # Assumes `some_function` is defined within custom_module ``` 3. Lazy import a module: ```python lazy_module = dynamic_import(\\"math\\", lazy=True) # Module is not imported yet print(lazy_module.sqrt(4)) # Trigger import and use sqrt function ``` **Constraints:** - You might need to use the `importlib` library functions such as `import_module`, `import_module_from_spec`, and `module_from_spec`. - Ensure that your function handles errors appropriately, providing clear messages if a module cannot be imported by either means. **Performance Requirements:** - Ensure that the function does not unnecessarily load modules which are intended to be lazily-loaded. # **Hints:** - Refer to `importlib.util.spec_from_file_location()` and `importlib.util.module_from_spec()` for details on importing modules from a file path. - Consider using a custom module class for handling lazy imports.","solution":"import importlib import importlib.util from typing import Optional def dynamic_import(module_name: str, file_path: Optional[str] = None, lazy: bool = False): Dynamically import a module based on the given conditions. Parameters: - module_name (str): Name of the module to import. - file_path (Optional[str]): The file path to import the module from, if necessary. - lazy (bool): Whether to implement lazy importing. Returns: - module: The imported module object. if lazy: return _LazyImporter(module_name, file_path) if file_path: spec = importlib.util.spec_from_file_location(module_name, file_path) module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) return module return importlib.import_module(module_name) class _LazyImporter: def __init__(self, module_name, file_path): self.module_name = module_name self.file_path = file_path self._module = None def _load(self): if self._module is None: if self.file_path: spec = importlib.util.spec_from_file_location(self.module_name, self.file_path) self._module = importlib.util.module_from_spec(spec) spec.loader.exec_module(self._module) else: self._module = importlib.import_module(self.module_name) def __getattr__(self, name): self._load() return getattr(self._module, name)"},{"question":"You are provided with a `pandas` DataFrame `df` containing data, and your task is to write a function that manipulates specific display options to achieve the desired output formats. Function Signature ```python import pandas as pd def configure_pandas_display(df: pd.DataFrame, max_rows: int, max_columns: int, max_colwidth: int, precision: int) -> None: Configures pandas global display options for DataFrames. Parameters: df (pd.DataFrame): The DataFrame to display after setting options. max_rows (int): Maximum number of rows to display. max_columns (int): Maximum number of columns to display. max_colwidth (int): Maximum width of the columns (in characters). precision (int): Precision of floating point numbers (number of decimal places). Returns: None pass ``` Input - `df` (pd.DataFrame): The DataFrame to apply and demonstrate the display settings. - `max_rows` (int): The maximum number of rows to display when printing a DataFrame. - `max_columns` (int): The maximum number of columns to display when printing a DataFrame. - `max_colwidth` (int): The maximum width (in characters) of each column. - `precision` (int): The number of decimal places to display for floating-point numbers. Output - The function should configure the global pandas display settings according to the provided parameters, and then print the DataFrame `df` using these settings. Constraints - Handle cases where the DataFrame has more rows or columns than the specified `max_rows` or `max_columns`. - Ensure the settings take effect only within the scope of this function and revert to previous settings when the function exits. Example Usage ```python import pandas as pd import numpy as np # Create a sample DataFrame data = np.random.randn(10, 5) columns = list(\'ABCDE\') df = pd.DataFrame(data, columns=columns) # Function call configure_pandas_display(df, max_rows=7, max_columns=4, max_colwidth=10, precision=3) ``` Upon running the example, the function should display the DataFrame `df` with: - A maximum of 7 rows displayed. - A maximum of 4 columns displayed. - Column widths truncated to 10 characters. - Floating-point numbers displayed with 3 decimal places.","solution":"import pandas as pd def configure_pandas_display(df: pd.DataFrame, max_rows: int, max_columns: int, max_colwidth: int, precision: int) -> None: Configures pandas global display options for DataFrames. Parameters: df (pd.DataFrame): The DataFrame to display after setting options. max_rows (int): Maximum number of rows to display. max_columns (int): Maximum number of columns to display. max_colwidth (int): Maximum width of the columns (in characters). precision (int): Precision of floating point numbers (number of decimal places). Returns: None with pd.option_context(\'display.max_rows\', max_rows, \'display.max_columns\', max_columns, \'display.max_colwidth\', max_colwidth, \'display.precision\', precision): print(df)"},{"question":"# Question: Decision Trees in Scikit-learn You are tasked with creating and evaluating decision tree models using the scikit-learn library. You will implement a decision tree classifier and regressor, handle missing values, and apply pruning to the tree for better generalization. Part 1: Decision Tree Classifier 1. Load the Iris dataset from scikit-learn. 2. Split the dataset into a training set (80%) and a testing set (20%). 3. Train a `DecisionTreeClassifier` on the training data. - Use `random_state=42` for reproducibility. 4. Evaluate the classifier on the testing data by calculating the accuracy score. 5. Plot and save the decision tree using the `plot_tree` function. Part 2: Decision Tree Regressor 1. Load the California Housing dataset from scikit-learn. 2. Introduce missing values randomly in 5% of the dataset. - Ensure the missing values are only in the features, not in the target variable. 3. Split the dataset into a training set (80%) and a testing set (20%). 4. Train a `DecisionTreeRegressor` on the training data, handling the missing values appropriately. - Use `random_state=42` for reproducibility. 5. Evaluate the regressor on the testing data by calculating the mean squared error (MSE). Part 3: Pruning 1. For the decision tree classifier trained in Part 1, apply minimal cost-complexity pruning. - Use cross-validation to determine an appropriate value for `ccp_alpha`. 2. Re-evaluate the pruned classifier on the testing data and compare its accuracy with the unpruned classifier. Submission Requirements - Implement the functions as specified below. - Include the necessary imports and ensure your code is well-commented. - Save the plots generated in Part 1 and include them in your submission. Here is an outline of the required functions: ```python from sklearn.datasets import load_iris, fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree from sklearn.metrics import accuracy_score, mean_squared_error import matplotlib.pyplot as plt import numpy as np def decision_tree_classifier(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train a Decision Tree Classifier clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) # Evaluate the classifier y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy of Decision Tree Classifier: {accuracy}\\") # Plot the decision tree plt.figure(figsize=(20,10)) plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.savefig(\\"decision_tree_classifier.png\\") plt.show() def decision_tree_regressor(): # Load the California Housing dataset housing = fetch_california_housing() X, y = housing.data, housing.target # Introduce missing values in 5% of the dataset rng = np.random.default_rng(42) missing_mask = rng.random(X.shape) < 0.05 X[missing_mask] = np.nan # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train a Decision Tree Regressor handling missing values reg = DecisionTreeRegressor(random_state=42) reg.fit(X_train, y_train) # Evaluate the regressor y_pred = reg.predict(X_test) mse = mean_squared_error(y_test, y_pred) print(f\\"Mean Squared Error of Decision Tree Regressor: {mse}\\") def prune_decision_tree_classifier(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train a Decision Tree Classifier clf = DecisionTreeClassifier(random_state=42) path = clf.cost_complexity_pruning_path(X_train, y_train) ccp_alphas = path.ccp_alphas # Use cross-validation to find the best value of ccp_alpha best_alpha = None best_accuracy = 0 for ccp_alpha in ccp_alphas: clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) if accuracy > best_accuracy: best_accuracy = accuracy best_alpha = ccp_alpha # Train the pruned classifier with the best ccp_alpha pruned_clf = DecisionTreeClassifier(random_state=42, ccp_alpha=best_alpha) pruned_clf.fit(X_train, y_train) y_pred = pruned_clf.predict(X_test) pruned_accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy of Pruned Decision Tree Classifier: {pruned_accuracy}\\") print(f\\"Original classifier accuracy: {best_accuracy}, Pruned classifier accuracy: {pruned_accuracy}\\") # Call the functions to execute the tasks decision_tree_classifier() decision_tree_regressor() prune_decision_tree_classifier() ```","solution":"from sklearn.datasets import load_iris, fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree from sklearn.metrics import accuracy_score, mean_squared_error import matplotlib.pyplot as plt import numpy as np def decision_tree_classifier(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train a Decision Tree Classifier clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) # Evaluate the classifier y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # Plot the decision tree plt.figure(figsize=(20,10)) plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.savefig(\\"decision_tree_classifier.png\\") plt.close() return accuracy def decision_tree_regressor(): # Load the California Housing dataset housing = fetch_california_housing() X, y = housing.data, housing.target # Introduce missing values in 5% of the dataset rng = np.random.default_rng(42) missing_mask = rng.random(X.shape) < 0.05 X[missing_mask] = np.nan # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Impute missing values with mean of each column (simplest form of imputation) from sklearn.impute import SimpleImputer imputer = SimpleImputer(strategy=\'mean\') X_train = imputer.fit_transform(X_train) X_test = imputer.transform(X_test) # Train a Decision Tree Regressor reg = DecisionTreeRegressor(random_state=42) reg.fit(X_train, y_train) # Evaluate the regressor y_pred = reg.predict(X_test) mse = mean_squared_error(y_test, y_pred) return mse def prune_decision_tree_classifier(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train a Decision Tree Classifier clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) # Evaluate unpruned classifier unpruned_accuracy = accuracy_score(y_test, clf.predict(X_test)) # Prune the decision tree path = clf.cost_complexity_pruning_path(X_train, y_train) ccp_alphas = path.ccp_alphas[:-1] # Exclude the maximum value which prunes the tree to a single node best_alpha = 0 best_accuracy = 0 for ccp_alpha in ccp_alphas: clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) if accuracy > best_accuracy: best_accuracy = accuracy best_alpha = ccp_alpha # Train the best pruned classifier and evaluate it pruned_clf = DecisionTreeClassifier(random_state=42, ccp_alpha=best_alpha) pruned_clf.fit(X_train, y_train) pruned_accuracy = accuracy_score(y_test, pruned_clf.predict(X_test)) return unpruned_accuracy, pruned_accuracy"},{"question":"**Objective:** Implement a custom URL opener using the `urllib.request` module. Your custom opener should handle GET and POST requests, include specific HTTP headers, manage basic authentication, handle HTTP redirections, and process any HTTP errors appropriately. # Problem Statement: You need to implement a function `custom_url_opener(url, headers=None, data=None, method=None, username=None, password=None)` which performs the following tasks: 1. Opens the given URL (`url`) with specified headers. 2. Supports GET and POST requests based on the `method` parameter. 3. If `username` and `password` are provided, it should use HTTP Basic Authentication. 4. Handles HTTP redirections and errors. 5. Returns the response content and HTTP status code. # Function Signature: ```python def custom_url_opener(url: str, headers: dict = None, data: bytes = None, method: str = None, username: str = None, password: str = None) -> (str, int): pass ``` # Input: - `url` (str): The URL to be opened. - `headers` (dict): Optional dictionary of HTTP headers to include in the request. - `data` (bytes): Optional data to be sent with the request (used primarily for POST requests). - `method` (str): The HTTP method to be used (\'GET\' or \'POST\'). If not specified, defaults to \'GET\'. - `username` (str): Optional username for HTTP Basic Authentication. - `password` (str): Optional password for HTTP Basic Authentication. # Output: - Returns a tuple containing: - The response content as a string. - The HTTP response status code as an integer. # Constraints: 1. Handle the use of proxies specified in environment variables. 2. Ensure any temporary files created during the process are cleaned up. # Example: ```python url = \\"http://www.example.com\\" headers = {\\"User-Agent\\": \\"custom-agent/1.0\\"} method = \\"GET\\" content, status_code = custom_url_opener(url, headers=headers, method=method) print(content) print(status_code) ``` # Notes: - Use the `urllib.request` module functionalities to build and install a custom opener. - Ensure proper error handling for different HTTP response codes. - Make use of context managers to handle resources efficiently. - Your solution should demonstrate an understanding of the urllib.request\'s classes and methods discussed in the provided documentation.","solution":"import urllib.request import urllib.error import base64 from urllib.parse import urlencode def custom_url_opener(url: str, headers: dict = None, data: bytes = None, method: str = None, username: str = None, password: str = None) -> (str, int): Custom URL opener that handles GET and POST requests, includes specific HTTP headers, manages basic authentication, and processes any HTTP errors. :param url: The URL to be opened. :param headers: Optional dictionary of HTTP headers to include in the request. :param data: Optional data to be sent with the request (used primarily for POST requests). :param method: The HTTP method to be used (\'GET\' or \'POST\'). Defaults to \'GET\'. :param username: Optional username for HTTP Basic Authentication. :param password: Optional password for HTTP Basic Authentication. :return: Tuple containing the response content as a string and the HTTP response status code as an integer. if headers is None: headers = {} if method is None: method = \'GET\' req = urllib.request.Request(url, headers=headers) if method == \'POST\' and data: req.data = data req.method = \'POST\' else: req.method = \'GET\' if username and password: credentials = (\'%s:%s\' % (username, password)) encoded_credentials = base64.b64encode(credentials.encode(\'ascii\')).decode(\'ascii\') req.add_header(\'Authorization\', \'Basic %s\' % encoded_credentials) try: with urllib.request.urlopen(req) as response: return response.read().decode(\'utf-8\'), response.getcode() except urllib.error.HTTPError as e: return e.read().decode(\'utf-8\'), e.getcode() except urllib.error.URLError as e: return str(e.reason), None"},{"question":"Utilization and Conversion of Sparse Data Structures Problem Statement: You are given a large dataset of sensor readings with missing values that periodically measures temperature and humidity. The data is often sparse since the sensors occasionally fail to record measurements. You need to structure the data using pandas sparse data structures, perform some conversions and calculations, and analyze the memory usage efficiency. Requirements: 1. **Reading and Initializing Sparse Data**: - Convert the array into a pandas `SparseArray`. - Create a `DataFrame` with two columns, \'Temperature\' and \'Humidity\', using the sparse data. 2. **Sparse Calculations**: - Perform an element-wise absolute value calculation on the \'Temperature\' column. 3. **Conversions**: - Convert the `DataFrame` to its dense format. - Convert the `DataFrame` back to sparse format. 4. **Memory Usage**: - Compare and print the memory usage of the dense and sparse `DataFrame`. Input: - Two lists of sensor readings (either float values or None) representing temperature and humidity measurements, respectively. Output: - The dense version of the `DataFrame` created initially. - The resulting sparse `DataFrame` after conversions. - Memory usage efficiency comparison (i.e., memory usage of dense vs. sparse). Constraints: - The size of the list can be up to 10,000 elements. - Use `fill_value` as `np.nan` for sparse data representation. Example: ```python temperature_readings = [10.5, None, -15.2, None, None, 23.5, None, None, -5.1, None] humidity_readings = [None, 45.0, None, None, 50.0, None, 55.0, None, None, 60.0] # Step-by-step implementation # 1. Convert the arrays into SparseArray. # 2. Create a DataFrame using the sparse data. # 3. Perform element-wise absolute value calculations. # 4. Convert to dense format and print. # 5. Convert back to sparse format and print. # 6. Compare memory usage and print results. ``` **Implementation function signature:** ```python import numpy as np import pandas as pd def process_sensor_data(temperature_readings, humidity_readings): # Your implementation here # Example call temperature_readings = [10.5, None, -15.2, None, None, 23.5, None, None, -5.1, None] humidity_readings = [None, 45.0, None, None, 50.0, None, 55.0, None, None, 60.0] process_sensor_data(temperature_readings, humidity_readings) ```","solution":"import numpy as np import pandas as pd def process_sensor_data(temperature_readings, humidity_readings): # Convert the arrays into SparseArray temp_sparse = pd.arrays.SparseArray(temperature_readings, fill_value=np.nan) humid_sparse = pd.arrays.SparseArray(humidity_readings, fill_value=np.nan) # Create a DataFrame using the sparse data df_sparse = pd.DataFrame({ \'Temperature\': temp_sparse, \'Humidity\': humid_sparse }) # Perform element-wise absolute value calculations on the \'Temperature\' column df_sparse[\'Temperature\'] = df_sparse[\'Temperature\'].abs() # Convert to dense format and print df_dense = df_sparse.sparse.to_dense() print(\\"Dense DataFrame:\\") print(df_dense) # Convert back to sparse format and print df_converted_sparse = df_dense.astype(pd.SparseDtype(np.float64, fill_value=np.nan)) print(\\"Converted Sparse DataFrame:\\") print(df_converted_sparse) # Compare memory usage and print results dense_memory = df_dense.memory_usage(deep=True).sum() sparse_memory = df_sparse.memory_usage(deep=True).sum() print(f\\"Memory usage of dense DataFrame: {dense_memory} bytes\\") print(f\\"Memory usage of sparse DataFrame: {sparse_memory} bytes\\") # Return the results for unit testing purposes return df_dense, df_converted_sparse, dense_memory, sparse_memory # Example call temperature_readings = [10.5, None, -15.2, None, None, 23.5, None, None, -5.1, None] humidity_readings = [None, 45.0, None, None, 50.0, None, 55.0, None, None, 60.0] process_sensor_data(temperature_readings, humidity_readings)"},{"question":"# Abstract Base Classes in Python **Objective:** Implement an abstract base class and demonstrate its usage by defining concrete subclasses. The goal is to ensure students understand the concepts of ABCs, abstract methods, virtual subclass registration, and the `ABCMeta` metaclass. **Task:** 1. Define an abstract base class `Geometry` using `abc.ABCMeta`, with the following specifications: - An abstract method `area(self)` which calculates the area of a geometric shape. - An abstract method `perimeter(self)` which calculates the perimeter of a geometric shape. 2. Create two concrete subclasses: - `Circle` which implements `area` and `perimeter`. It should take the `radius` as an initialization parameter. - `Rectangle` which implements `area` and `perimeter`. It should take `width` and `height` as initialization parameters. 3. Register a third class `Square` as a virtual subclass of `Rectangle` using the `register` method of `ABCMeta`. `Square` should take `side` as an initialization parameter and should delegate `area` and `perimeter` to `Rectangle`. **Specifications:** - **`Geometry` class**: ```python from abc import ABC, abstractmethod class Geometry(ABC): @abstractmethod def area(self): pass @abstractmethod def perimeter(self): pass ``` - **`Circle` class**: ```python import math class Circle(Geometry): def __init__(self, radius): self.radius = radius def area(self): return math.pi * self.radius ** 2 def perimeter(self): return 2 * math.pi * self.radius ``` - **`Rectangle` class**: ```python class Rectangle(Geometry): def __init__(self, width, height): self.width = width self.height = height def area(self): return self.width * self.height def perimeter(self): return 2 * (self.width + self.height) ``` - **`Square` class**: ```python class Square: def __init__(self, side): self.side = side self.rectangle = Rectangle(side, side) def area(self): return self.rectangle.area() def perimeter(self): return self.rectangle.perimeter() ``` - **Register `Square` as a virtual subclass of `Rectangle`**: ```python Rectangle.register(Square) ``` **Example Usage:** ```python # Usage example shapes = [ Circle(5), Rectangle(3, 4), Square(2) ] for shape in shapes: print(f\\"Area: {shape.area()} | Perimeter: {shape.perimeter()}\\") ``` **Constraints:** - You must use `abc.ABC` and `abc.ABCMeta` to define the `Geometry` class. - Use the appropriate math imports for calculations in the `Circle` class. - Ensure that `Square` properly delegates its calculations to a `Rectangle` instance. Ensure your solution adheres to the described structure and implements the required methods and registration. **Submission:** Submit your classes `Geometry`, `Circle`, `Rectangle`, and `Square` in a single Python file.","solution":"from abc import ABC, abstractmethod import math class Geometry(ABC): @abstractmethod def area(self): pass @abstractmethod def perimeter(self): pass class Circle(Geometry): def __init__(self, radius): self.radius = radius def area(self): return math.pi * self.radius ** 2 def perimeter(self): return 2 * math.pi * self.radius class Rectangle(Geometry): def __init__(self, width, height): self.width = width self.height = height def area(self): return self.width * self.height def perimeter(self): return 2 * (self.width + self.height) class Square: def __init__(self, side): self.side = side self.rectangle = Rectangle(side, side) def area(self): return self.rectangle.area() def perimeter(self): return self.rectangle.perimeter() Rectangle.register(Square)"},{"question":"Objective The objective of this assignment is to assess your ability to work with PyTorch tensors and to implement a function that manipulates tensors in a meaningful way. Additionally, you will use utility functions from `torch.ao.ns.fx.utils` to evaluate how close your manipulated tensor is to a target tensor. Problem Statement Implement a function `scale_and_translate(tensor, scale, translate)` that accepts a PyTorch tensor and performs the following operations: 1. Scale the tensor by multiplying each element with a given `scale` factor. 2. Translate the tensor by adding a given `translate` value to each element. 3. Ensure all tensor values are clamped within the range [0, 1]. After implementing the function, you need to test your implementation by comparing the scaled and translated tensor with a provided target tensor using given utility functions: `compute_sqnr`, `compute_normalized_l2_error`, and `compute_cosine_similarity`. Function Signature ```python import torch def scale_and_translate(tensor: torch.Tensor, scale: float, translate: float) -> torch.Tensor: Scales and translates the input tensor. Args: tensor (torch.Tensor): Input tensor. scale (float): Scale factor. translate (float): Translation value. Returns: torch.Tensor: The scaled and translated tensor with values clamped in the range [0, 1]. pass def evaluate_tensor(tensor: torch.Tensor, target: torch.Tensor) -> None: Evaluates the scaled and translated tensor against the target tensor. Args: tensor (torch.Tensor): Input tensor. target (torch.Tensor): Target tensor for evaluation. Returns: None pass ``` Input and Output Formats - The `scale_and_translate` function accepts a tensor of arbitrary shape, a float `scale`, and a float `translate`. It returns a tensor of the same shape after scaling, translating, and clamping. - The `evaluate_tensor` function should print the SQNR, normalized L2 error, and cosine similarity between the input tensor and the target tensor. Constraints - The input tensor `tensor` can be of any shape and contain any floating point values. - The `scale` and `translate` values will be floating point numbers. Example ```python if __name__ == \\"__main__\\": # Example tensor input_tensor = torch.tensor([[0.2, 0.4], [0.6, 0.8]]) scale = 1.5 translate = -0.1 target_tensor = torch.tensor([[0.2, 0.5], [0.8, 1.0]]) # Apply scale and translate function transformed_tensor = scale_and_translate(input_tensor, scale, translate) # Evaluate the transformed tensor against the target tensor evaluate_tensor(transformed_tensor, target_tensor) ``` Note - Use `torch.clamp(tensor, min=0.0, max=1.0)` to clamp the tensor values within the range [0, 1]. - Assume the `torch.ao.ns.fx.utils` module and its functions are available and imported for the `evaluate_tensor` function.","solution":"import torch def scale_and_translate(tensor: torch.Tensor, scale: float, translate: float) -> torch.Tensor: Scales and translates the input tensor. Args: tensor (torch.Tensor): Input tensor. scale (float): Scale factor. translate (float): Translation value. Returns: torch.Tensor: The scaled and translated tensor with values clamped in the range [0, 1]. transformed_tensor = tensor * scale + translate transformed_tensor = torch.clamp(transformed_tensor, min=0.0, max=1.0) return transformed_tensor def evaluate_tensor(tensor: torch.Tensor, target: torch.Tensor) -> None: Evaluates the scaled and translated tensor against the target tensor. Args: tensor (torch.Tensor): Input tensor. target (torch.Tensor): Target tensor for evaluation. Returns: None from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity sqnr = compute_sqnr(tensor, target) l2_error = compute_normalized_l2_error(tensor, target) cosine_similarity = compute_cosine_similarity(tensor, target) print(f\\"SQNR: {sqnr}\\") print(f\\"Normalized L2 Error: {l2_error}\\") print(f\\"Cosine Similarity: {cosine_similarity}\\")"},{"question":"**Title: Implement Advanced Logging Configuration** Objective Create an advanced logging configuration in a Python application that logs messages to multiple destinations (console, file, and network) with different formatting and log levels, supports multi-threaded environments, and uses a custom filter to add contextual information. Problem Statement You are tasked with setting up a sophisticated logging system for a Python application. The logging system should meet the following requirements: 1. **Logging Destinations**: - Log all messages of level `DEBUG` and higher to a file called `app.log`. - Log messages of level `INFO` and higher to the console. - Log messages of level `ERROR` and higher to a remote server over TCP on `localhost` port `9999`. 2. **Format**: - The file logs should include timestamps in the format `YYYY-MM-DD HH:MM:SS`, the logger name, the log level, and the message. - The console logs should be simpler and include just the log level and the message. 3. **Contextual Information**: - Add a custom filter that includes the current thread name in every log message. 4. **Multi-threading**: - Demonstrate the logging system in a multi-threaded context by creating a worker function that logs messages from multiple threads. Function Signature You are required to implement the following function: ```python def setup_advanced_logging(): pass ``` Implementation Details 1. **setup_advanced_logging**: - This function should configure the logging system as described above. - Use appropriate handlers (`FileHandler`, `StreamHandler`, `SocketHandler`). - Set up formatters for the file and console outputs. - Create and apply a custom filter to include the thread name in each log message. 2. **Demonstration**: - Create a worker function `log_messages` that logs messages with different levels from multiple threads. - Start the worker function in multiple threads to demonstrate the logging configuration. Example Usage After setting up the logging configuration with `setup_advanced_logging`: ```python import threading import logging def log_messages(): logger = logging.getLogger(\'app\') logger.debug(\\"This is a debug message\\") logger.info(\\"This is an info message\\") logger.warning(\\"This is a warning message\\") logger.error(\\"This is an error message\\") logger.critical(\\"This is a critical message\\") # Setting up logging setup_advanced_logging() # Demonstrating logging from multiple threads threads = [] for i in range(5): t = threading.Thread(target=log_messages) threads.append(t) t.start() for t in threads: t.join() ``` Constraints - Do not use any additional logging configuration packages—only use the built-in `logging` module. - Ensure thread-safety and appropriate log message formatting and filtering as specified. Assessment Criteria - Correct configuration of logging destinations with specified levels and formats. - Proper implementation of a custom filter to include contextual information. - Demonstration of logging in a multi-threaded environment. - Clean, readable, and well-documented code. Good luck!","solution":"import logging import logging.handlers import threading class ThreadNameFilter(logging.Filter): def filter(self, record): record.threadName = threading.current_thread().name return True def setup_advanced_logging(): # Create logger logger = logging.getLogger(\'app\') logger.setLevel(logging.DEBUG) # Create file handler which logs debug and higher level messages fh = logging.FileHandler(\'app.log\') fh.setLevel(logging.DEBUG) fh_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(threadName)s - %(message)s\', datefmt=\'%Y-%m-%d %H:%M:%S\') fh.setFormatter(fh_formatter) # Create console handler with a higher log level ch = logging.StreamHandler() ch.setLevel(logging.INFO) ch_formatter = logging.Formatter(\'%(levelname)s: %(message)s\') ch.setFormatter(ch_formatter) # Create network handler for errors and higher level messages nh = logging.handlers.SocketHandler(\'localhost\', 9999) nh.setLevel(logging.ERROR) # Create a custom filter filter = ThreadNameFilter() # Add custom filter to handlers for handler in (fh, ch, nh): handler.addFilter(filter) # Add handlers to the logger logger.addHandler(fh) logger.addHandler(ch) logger.addHandler(nh) def log_messages(): logger = logging.getLogger(\'app\') logger.debug(\\"This is a debug message\\") logger.info(\\"This is an info message\\") logger.warning(\\"This is a warning message\\") logger.error(\\"This is an error message\\") logger.critical(\\"This is a critical message\\")"},{"question":"# Advanced Python Coding Assessment Objective: To design and implement a Python function that demonstrates understanding of Python 3.10\'s core features, including data models, expressions, statements, and functional programming concepts. Problem Statement: You are required to write a function that processes a given list of dictionaries representing students\' grades in different subjects and returns a comprehensive report. Your solution should utilize Python\'s advanced features such as comprehensions, lambda functions, conditional expressions, and handling exceptions. Function Specification: **Function Name:** `generate_grade_report` **Parameters:** - `grades_list` (List[Dict[str, Union[str, List[Dict[str, Union[str, int]]]]]]): A list of dictionaries where each dictionary contains the following keys: - `student_name` (str): The name of the student. - `grades` (List[Dict[str, Union[str, int]]]): A list of dictionaries representing the grades of the student. Each dictionary contains: - `subject` (str): The name of the subject. - `grade` (int): The grade the student received in that subject. **Returns:** - `report` (Dict[str, Dict[str, Union[int, str]]]): A dictionary where each key is a student\'s name and the value is another dictionary containing: - `total_grades` (int): Total grades across all subjects. - `average_grade` (float): Average grade across all subjects. - `evaluation` (str): An evaluation string based on the average grade: - \\"Excellent\\" for average above 90. - \\"Good\\" for average between 75 and 90. - \\"Needs Improvement\\" for average below 75. **Constraints:** - Each student will have at least one subject. - Grades will be non-negative integers. - In case of any invalid grade, handle exceptions gracefully and continue processing the rest of the data. **Example:** ```python grades_list = [ { \\"student_name\\": \\"Alice\\", \\"grades\\": [ {\\"subject\\": \\"Math\\", \\"grade\\": 85}, {\\"subject\\": \\"History\\", \\"grade\\": 95}, {\\"subject\\": \\"Art\\", \\"grade\\": 80} ] }, { \\"student_name\\": \\"Bob\\", \\"grades\\": [ {\\"subject\\": \\"Math\\", \\"grade\\": 70}, {\\"subject\\": \\"History\\", \\"grade\\": 72} ] } ] # Expected Output: # { # \\"Alice\\": { # \\"total_grades\\": 260, # \\"average_grade\\": 86.67, # \\"evaluation\\": \\"Good\\" # }, # \\"Bob\\": { # \\"total_grades\\": 142, # \\"average_grade\\": 71.0, # \\"evaluation\\": \\"Needs Improvement\\" # } # } ``` Submission: Submit your function definition along with test cases to validate your solution.","solution":"def generate_grade_report(grades_list): def calculate_total_and_average(grades): if not grades: return 0, 0.0 valid_grades = [grade[\'grade\'] for grade in grades if isinstance(grade.get(\'grade\'), int)] total = sum(valid_grades) average = total / len(valid_grades) if valid_grades else 0.0 return total, average def evaluate_grade(avg_grade): if avg_grade > 90: return \\"Excellent\\" elif avg_grade >= 75: return \\"Good\\" else: return \\"Needs Improvement\\" report = {} for student in grades_list: student_name = student.get(\'student_name\', \'Unknown\') grades = student.get(\'grades\', []) total, average = calculate_total_and_average(grades) report[student_name] = { \\"total_grades\\": total, \\"average_grade\\": round(average, 2), \\"evaluation\\": evaluate_grade(average) } return report"},{"question":"# Advanced Coding Assessment Question: Managing PyCapsules **Objective**: Implement a Python C extension module that creates, manages, and verifies PyCapsule objects to encapsulate data and ensure data consistency throughout the module. # Requirements: 1. **Create and Manage PyCapsules**: Write a Python C extension that provides the following capabilities: 1. **Create a PyCapsule**: A function `create_capsule(data: int, name: str)` that creates a `PyCapsule` with the given integer data and name. 2. **Get Data from a PyCapsule**: A function `get_capsule_data(name: str) -> int` that retrieves the integer data from a PyCapsule with the given name. 3. **Set New Data in a PyCapsule**: A function `set_capsule_data(name: str, new_data: int) -> bool` that sets new integer data in the existing PyCapsule with the given name. Returns `True` if successful, otherwise `False`. 4. **Check Capsule Validity**: A function `is_capsule_valid(name: str) -> bool` that checks the validity of the PyCapsule with the given name and returns `True` if valid, otherwise `False`. 2. **Destructor Callback**: Implement a destructor callback for the capsules that prints a message upon capsule destruction (e.g., `\\"Capsule <name> is destroyed\\"`). # Constraints: - The PyCapsule names should be unique strings. - Ensure proper error handling and clean-up in all C functions. # Example Usage in Python: ```python import my_capsule_module # Create a capsule with data 42 and name \'example.capsule\' my_capsule_module.create_capsule(42, \'example.capsule\') # Retrieve the data from the capsule value = my_capsule_module.get_capsule_data(\'example.capsule\') print(value) # Output should be 42 # Check if the capsule is valid is_valid = my_capsule_module.is_capsule_valid(\'example.capsule\') print(is_valid) # Output should be True # Set new data in the capsule result = my_capsule_module.set_capsule_data(\'example.capsule\', 99) print(result) # Output should be True # Get the updated data from the capsule new_value = my_capsule_module.get_capsule_data(\'example.capsule\') print(new_value) # Output should be 99 # Capsule is automatically destroyed when the program exits, triggering the destructor ``` # Hints: - Use the `PyCapsule` API functions provided in the documentation to manage the capsules. - Use `PyCapsule_New` to create capsules and `PyCapsule_GetPointer` to retrieve the encapsulated data. - Implement appropriate error handling for invalid names or failed operations. Good luck!","solution":"# solution.py from typing import Dict # Simulate the PyCapsule in Python class PyCapsule: def __init__(self, data, name, destructor): self.data = data self.name = name self.destructor = destructor # Dictionary to store PyCapsules by name capsules: Dict[str, PyCapsule] = {} def create_capsule(data: int, name: str): global capsules if name in capsules: raise ValueError(f\\"Capsule with name {name} already exists.\\") capsule = PyCapsule(data, name, capsule_destructor) capsules[name] = capsule def get_capsule_data(name: str) -> int: global capsules capsule = capsules.get(name, None) if capsule is None: raise ValueError(f\\"No capsule found with name {name}.\\") return capsule.data def set_capsule_data(name: str, new_data: int) -> bool: global capsules capsule = capsules.get(name, None) if capsule is None: return False capsule.data = new_data return True def is_capsule_valid(name: str) -> bool: global capsules return name in capsules def capsule_destructor(name: str): print(f\\"Capsule {name} is destroyed\\") # Clean up all capsules and call their destructors def cleanup_capsules(): global capsules for name, capsule in capsules.items(): capsule.destructor(name) capsules.clear()"},{"question":"**Objective:** To assess your understanding of the `__torch_function__` protocol in PyTorch and your ability to create custom behaviors for standard operations. **Problem Statement:** Create a custom tensor class, `MyCustomTensor`, that wraps around a PyTorch tensor. This class should override the addition (`+`) and multiplication (`*`) operations using the `__torch_function__` protocol. Specifically, for the `+` operation, it should return the element-wise sum of the two tensors multiplied by 2. For the `*` operation, it should return the element-wise product of the two tensors added with 3. **Requirements:** 1. Implement the `__torch_function__` protocol in your custom tensor class `MyCustomTensor`. 2. Your class should handle the following operations: - Addition (`+`): Return `(tensor1 + tensor2) * 2` - Multiplication (`*`): Return `(tensor1 * tensor2) + 3` **Expected Input and Output:** - Input: Two instances of `MyCustomTensor` wrapping PyTorch tensors. - Output: A new instance of `MyCustomTensor` representing the result of the specified operation. **Implementation:** ```python import torch from torch.overrides import handle_torch_function, has_torch_function class MyCustomTensor: def __init__(self, tensor): self.tensor = tensor def __torch_function__(self, func, types, args=(), kwargs=None): if kwargs is None: kwargs = {} if func in (torch.add, torch.mul): args = list(args) if func is torch.add: result = (args[0].tensor + args[1].tensor) * 2 elif func is torch.mul: result = (args[0].tensor * args[1].tensor) + 3 return MyCustomTensor(result) return NotImplemented def __add__(self, other): return MyCustomTensor(torch.add(self, other)) def __mul__(self, other): return MyCustomTensor(torch.mul(self, other)) # Example Usage tensor1 = MyCustomTensor(torch.tensor([1, 2, 3])) tensor2 = MyCustomTensor(torch.tensor([4, 5, 6])) result_addition = tensor1 + tensor2 # Expected Output: MyCustomTensor(tensor([10, 14, 18])) result_multiplication = tensor1 * tensor2 # Expected Output: MyCustomTensor(tensor([7, 13, 21])) print(result_addition.tensor) # Expected: tensor([10, 14, 18]) print(result_multiplication.tensor) # Expected: tensor([7, 13, 21]) ``` **Constraints:** - Assume that the input tensors are always of the same size and shape. - You should not use any other external libraries except PyTorch. **Performance Requirements:** - The implementation should handle tensor operations efficiently without unnecessary computational overhead.","solution":"import torch from torch.overrides import handle_torch_function, has_torch_function class MyCustomTensor: def __init__(self, tensor): self.tensor = tensor def __torch_function__(self, func, types, args=(), kwargs=None): if kwargs is None: kwargs = {} if func in (torch.add, torch.mul): args = list(args) # Dereference MyCustomTensor instances if isinstance(args[0], MyCustomTensor): args[0] = args[0].tensor if isinstance(args[1], MyCustomTensor): args[1] = args[1].tensor if func is torch.add: result = (args[0] + args[1]) * 2 elif func is torch.mul: result = (args[0] * args[1]) + 3 return MyCustomTensor(result) return NotImplemented def __add__(self, other): if isinstance(other, MyCustomTensor): return torch.add(self, other) return NotImplemented def __mul__(self, other): if isinstance(other, MyCustomTensor): return torch.mul(self, other) return NotImplemented # Example Usage tensor1 = MyCustomTensor(torch.tensor([1, 2, 3])) tensor2 = MyCustomTensor(torch.tensor([4, 5, 6])) result_addition = tensor1 + tensor2 # Expected Output: MyCustomTensor(tensor([10, 14, 18])) result_multiplication = tensor1 * tensor2 # Expected Output: MyCustomTensor(tensor([7, 13, 21])) print(result_addition.tensor) # Expected: tensor([10, 14, 18]) print(result_multiplication.tensor) # Expected: tensor([7, 13, 21])"},{"question":"# Python Reference Counting Management You are tasked with implementing a Python class that simulates a simplified reference counting mechanism for a custom Python-like object. The class should provide methods to manage reference counts, mimicking the behavior of the macros and functions described in the documentation. Requirements: 1. **Class Definition**: Create a class `RefCountedObject` that has: - An attribute `ref_count` initialized to 0. - An attribute `value` which can be any object. 2. **Methods**: - `incref(self)`: Simulates `Py_INCREF`. Increment the `ref_count`. - `decref(self)`: Simulates `Py_DECREF`. Decrement the `ref_count` and set the `value` to `None` if `ref_count` reaches 0. - `xinc_ref(self)`: Simulates `Py_XINCREF`. Increment the `ref_count` only if `value` is not `None`. - `xdec_ref(self)`: Simulates `Py_XDECREF`. Decrement the `ref_count` only if `value` is not `None` and set the `value` to `None` if `ref_count` reaches 0. - `new_ref(self)`: Simulates `Py_NewRef`. Create a new strong reference by incrementing the `ref_count`. - `xnew_ref(self)`: Simulates `Py_XNewRef`. Create a new strong reference by incrementing the `ref_count` only if `value` is not `None`. 3. **Edge Handling**: - Ensure methods handle edge cases where `value` can be `None`. - Ensure the `decref` methods properly set `value` to `None` when `ref_count` reaches 0. 4. **Example Usage**: Provide an example usage of your class demonstrating: - Incrementing and decrementing reference counts. - Handling `NULL` (Python\'s `None`) values safely. - Creating and managing strong references. Constraints: - `ref_count` should never be negative. - Assume all methods will be called in valid sequence (no need to handle invalid method calls). # Implementation ```python class RefCountedObject: def __init__(self, value=None): self.value = value self.ref_count = 0 def incref(self): # Simulates Py_INCREF if self.value is not None: self.ref_count += 1 def decref(self): # Simulates Py_DECREF if self.value is not None: self.ref_count -= 1 if self.ref_count <= 0: self.value = None self.ref_count = 0 def xinc_ref(self): # Simulates Py_XINCREF if self.value is not None: self.incref() def xdec_ref(self): # Simulates Py_XDECREF if self.value is not None: self.decref() def new_ref(self): # Simulates Py_NewRef self.incref() def xnew_ref(self): # Simulates Py_XNewRef if self.value is not None: self.new_ref() # Example usage obj = RefCountedObject(42) obj.incref() print(obj.ref_count) # Output: 1 obj.decref() print(obj.value, obj.ref_count) # Output: None, 0 obj2 = RefCountedObject() obj2.xinc_ref() # Should not change ref_count because value is None print(obj2.ref_count) # Output: 0 obj2.value = \\"hello\\" obj2.new_ref() print(obj2.ref_count) # Output: 1 ```","solution":"class RefCountedObject: def __init__(self, value=None): self.value = value self.ref_count = 0 def incref(self): # Simulates Py_INCREF if self.value is not None: self.ref_count += 1 def decref(self): # Simulates Py_DECREF if self.value is not None: self.ref_count -= 1 if self.ref_count <= 0: self.value = None self.ref_count = 0 def xinc_ref(self): # Simulates Py_XINCREF if self.value is not None: self.incref() def xdec_ref(self): # Simulates Py_XDECREF if self.value is not None: self.decref() def new_ref(self): # Simulates Py_NewRef self.incref() def xnew_ref(self): # Simulates Py_XNewRef if self.value is not None: self.new_ref() # Example usage obj = RefCountedObject(42) obj.incref() print(obj.ref_count) # Output: 1 obj.decref() print(obj.value, obj.ref_count) # Output: None, 0 obj2 = RefCountedObject() obj2.xinc_ref() # Should not change ref_count because value is None print(obj2.ref_count) # Output: 0 obj2.value = \\"hello\\" obj2.new_ref() print(obj2.ref_count) # Output: 1"},{"question":"Introduction Your task is to implement a function that performs a series of manipulations on a sequence object (like a list or tuple). The manipulations include concatenation, repetition, getting and setting items, and slicing. You must use functions from the `python310` package\'s sequence protocol as described in the provided documentation. Function Signature ```python def manipulate_sequence(seq): # This is where your implementation will go. pass ``` Requirements 1. **Input**: A sequence `seq` which could be a list or tuple. 2. **Output**: A dictionary with keys representing the type of manipulation and values being the results of the manipulations as described below. Manipulations to Perform 1. **Concatenation**: Concatenate `seq` with itself. - Key: `concat` - Example: For `seq = [1, 2, 3]`, `concat` should be `[1, 2, 3, 1, 2, 3]` 2. **Repetition**: Repeat `seq` three times. - Key: `repeat` - Example: For `seq = [1, 2, 3]`, `repeat` should be `[1, 2, 3, 1, 2, 3, 1, 2, 3]` 3. **Get Item**: Get the second item of `seq`. - Key: `get_item` - Example: For `seq = [1, 2, 3]`, `get_item` should be `2` **Note**: If the sequence has less than two items, set it to `None`. 4. **Set Item**: Set the first item of `seq` to 99. - Key: `set_item` - Example: For `seq = [1, 2, 3]`, `set_item` should be `[99, 2, 3]` **Note**: If the sequence is empty or a tuple (which is immutable), skip this step. 5. **Get Slice**: Get a slice of `seq` from index 1 (inclusive) to index 3 (exclusive). - Key: `get_slice` - Example: For `seq = [1, 2, 3, 4]`, `get_slice` should be `[2, 3]` **Note**: If the slice indices are out of bounds, adjust them accordingly. Constraints and Limitations - You may assume that `seq` contains up to 1000 elements. - For setting items, if the sequence is a tuple, convert it to a list before setting the item. Example Usage ```python result = manipulate_sequence([1, 2, 3, 4]) print(result) # Expected Output: # { # \'concat\': [1, 2, 3, 4, 1, 2, 3, 4], # \'repeat\': [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4], # \'get_item\': 2, # \'set_item\': [99, 2, 3, 4], # \'get_slice\': [2, 3] # } ``` Note: Ensure to conform to the semantics of each operation as described in the `python310` documentation.","solution":"def manipulate_sequence(seq): result = {} # Concatenation result[\'concat\'] = seq + seq # Repetition result[\'repeat\'] = seq * 3 # Get Item result[\'get_item\'] = seq[1] if len(seq) > 1 else None # Set Item if len(seq) > 0: if isinstance(seq, tuple): temp_seq = list(seq) temp_seq[0] = 99 result[\'set_item\'] = tuple(temp_seq) else: temp_seq = seq[:] temp_seq[0] = 99 result[\'set_item\'] = temp_seq else: result[\'set_item\'] = seq # Get Slice result[\'get_slice\'] = seq[1:3] return result"},{"question":"Objective You are required to implement a multi-threaded application that simulates a simple bank with multiple accounts. Multiple threads will simulate deposits and withdrawals to showcase concurrency control using the `threading` module. Problem Statement You need to create a `Bank` class that manages multiple `Account` instances. Each `Account` should have methods to deposit and withdraw money safely in a concurrent environment. The `Bank` should be able to perform multiple transactions concurrently while ensuring thread safety to prevent race conditions. Requirements 1. **Account Class**: - **Attributes**: - `balance`: The current balance of the account. - `lock`: A `Lock` object to manage concurrent access to the account\'s balance. - **Methods**: - `deposit(amount: float)`: Adds the specified amount to the account balance. - `withdraw(amount: float)`: Deducts the specified amount from the account balance if sufficient funds are available. Returns `True` if the withdrawal is successful, otherwise `False`. 2. **Bank Class**: - **Attributes**: - `accounts`: A dictionary of account IDs to `Account` objects. - **Methods**: - `create_account(account_id: int, initial_balance: float)`: Creates a new account with the given ID and initial balance. - `perform_transaction(account_id: int, amount: float)`: Performs a deposit or withdrawal based on the amount (positive for deposit, negative for withdrawal) on the account with the given ID. 3. **Multi-threaded Transactions**: - Create multiple threads to simulate concurrent deposits and withdrawals on the bank accounts. - Use synchronization primitives to ensure that the transactions are thread-safe. Constraints - Assume that each function call is well-formed and that the specified account ID exists for each transaction. - Transactions can be performed concurrently on the same or different accounts. - You should prevent race conditions using appropriate `threading` mechanisms. Example Usage ```python import threading import random # Define the Account and Bank classes here def simulate_transactions(bank, account_id, num_transactions): for _ in range(num_transactions): amount = random.uniform(-100, 100) # Random deposit or withdrawal amount bank.perform_transaction(account_id, amount) if __name__ == \\"__main__\\": bank = Bank() # Create accounts bank.create_account(1, 500.0) bank.create_account(2, 1000.0) # Create threads for concurrent transactions threads = [] for i in range(10): # 10 threads, 5 for each account t1 = threading.Thread(target=simulate_transactions, args=(bank, 1, 50)) t2 = threading.Thread(target=simulate_transactions, args=(bank, 2, 50)) threads.extend([t1, t2]) # Start all threads for t in threads: t.start() # Wait for all threads to finish for t in threads: t.join() # Print final balances print(f\\"Final balance of account 1: {bank.accounts[1].balance}\\") print(f\\"Final balance of account 2: {bank.accounts[2].balance}\\") ``` Task Implement the `Account` and `Bank` classes according to the specifications above. Ensure that all methods are thread-safe and handle concurrent access appropriately.","solution":"import threading class Account: def __init__(self, initial_balance): self.balance = initial_balance self.lock = threading.Lock() def deposit(self, amount: float): with self.lock: self.balance += amount def withdraw(self, amount: float) -> bool: with self.lock: if self.balance >= amount: self.balance -= amount return True else: return False class Bank: def __init__(self): self.accounts = {} def create_account(self, account_id: int, initial_balance: float): self.accounts[account_id] = Account(initial_balance) def perform_transaction(self, account_id: int, amount: float): account = self.accounts[account_id] if amount >= 0: account.deposit(amount) else: account.withdraw(-amount)"},{"question":"**Asynchronous Task Management System** # Problem Statement You are tasked with creating an asynchronous task management system using the `asyncio` module in Python 3.10. Your system will allow for the scheduling, execution, and monitoring of various tasks that may include simple computations, I/O-bound operations, or any user-defined asynchronous coroutines. # Requirements 1. **Task Scheduler**: Implement a class `TaskScheduler` that manages an event loop and can schedule tasks at specific times or after a delay. 2. **Task Management**: The `TaskScheduler` should support adding, removing, and querying the status of tasks. 3. **Execution**: Tasks should be executed within the event loop, and their status should be updated upon completion. If a task raises an exception, it should be captured and logged. 4. **Concurrency**: Utilize appropriate concurrency mechanisms to ensure that I/O-bound and CPU-bound tasks are handled efficiently. # Detailed Specification `TaskScheduler` Class 1. **Methods**: - `start()`: Start the event loop. - `stop()`: Stop the event loop. - `add_task(task: Awaitable, delay: Optional[float] = None) -> str`: Schedule a task to be run after a specified delay. Returns a unique task ID. - `remove_task(task_id: str) -> bool`: Remove a scheduled task by its ID. Returns `True` if the task was successfully removed. - `task_status(task_id: str) -> str`: Query the status of a task (e.g., \\"scheduled\\", \\"running\\", \\"completed\\", \\"failed\\"). - `time_scheduler(callback: Callable, delay: Optional[float] = None)`: Schedule a callback to be run at a later time. If no delay is specified, the task runs immediately. 2. **Task States**: - \\"scheduled\\" - \\"running\\" - \\"completed\\" - \\"failed\\" 3. **Error Handling**: - Log exceptions raised by tasks without terminating the event loop. - Ensure that the system can gracefully shut down by canceling all running and scheduled tasks. Example Usage ```python import asyncio class TaskScheduler: def __init__(self): self.loop = asyncio.get_event_loop() self.tasks = {} self.task_counter = 0 def start(self): self.loop.run_forever() def stop(self): self.loop.stop() def add_task(self, task: Awaitable, delay: Optional[float] = None) -> str: task_id = f\\"task_{self.task_counter}\\" self.task_counter += 1 if delay is not None: handle = self.loop.call_later(delay, lambda: asyncio.create_task(self._run_task(task_id, task))) else: handle = self.loop.call_soon(lambda: asyncio.create_task(self._run_task(task_id, task))) self.tasks[task_id] = {\\"handle\\": handle, \\"status\\": \\"scheduled\\"} return task_id def remove_task(self, task_id: str) -> bool: if task_id in self.tasks: self.tasks[task_id][\\"handle\\"].cancel() del self.tasks[task_id] return True return False def task_status(self, task_id: str) -> str: if task_id in self.tasks: return self.tasks[task_id][\\"status\\"] return \\"task not found\\" async def _run_task(self, task_id: str, task: Awaitable): self.tasks[task_id][\\"status\\"] = \\"running\\" try: await task self.tasks[task_id][\\"status\\"] = \\"completed\\" except Exception as e: self.tasks[task_id][\\"status\\"] = \\"failed\\" print(f\\"Task {task_id} raised an exception: {e}\\") def time_scheduler(self, callback: Callable, delay: Optional[float] = None): if delay is not None: self.loop.call_later(delay, callback) else: self.loop.call_soon(callback) # Example task async def example_task(): await asyncio.sleep(2) print(\\"Task completed.\\") scheduler = TaskScheduler() task_id = scheduler.add_task(example_task(), delay=5) scheduler.start() print(scheduler.task_status(task_id)) ``` # Constraints - The system should handle at least 1000 concurrent tasks. - Tasks should be able to handle both I/O and CPU-bound operations effectively. # Performance Requirements - The scheduler should have minimal overhead and efficiently manage the event loop and tasks. - Task addition and removal should be performed in constant or logarithmic time.","solution":"import asyncio from typing import Awaitable, Callable, Optional class TaskScheduler: def __init__(self): self.loop = asyncio.get_event_loop() self.tasks = {} self.task_counter = 0 def start(self): self.loop.run_forever() def stop(self): self.loop.stop() def add_task(self, task: Awaitable, delay: Optional[float] = None) -> str: task_id = f\\"task_{self.task_counter}\\" self.task_counter += 1 if delay is not None: handle = self.loop.call_later(delay, lambda: asyncio.create_task(self._run_task(task_id, task))) else: handle = self.loop.call_soon(lambda: asyncio.create_task(self._run_task(task_id, task))) self.tasks[task_id] = {\\"handle\\": handle, \\"status\\": \\"scheduled\\"} return task_id def remove_task(self, task_id: str) -> bool: if task_id in self.tasks: self.tasks[task_id][\\"handle\\"].cancel() del self.tasks[task_id] return True return False def task_status(self, task_id: str) -> str: if task_id in self.tasks: return self.tasks[task_id][\\"status\\"] return \\"task not found\\" async def _run_task(self, task_id: str, task: Awaitable): self.tasks[task_id][\\"status\\"] = \\"running\\" try: await task self.tasks[task_id][\\"status\\"] = \\"completed\\" except Exception as e: self.tasks[task_id][\\"status\\"] = \\"failed\\" print(f\\"Task {task_id} raised an exception: {e}\\") def time_scheduler(self, callback: Callable, delay: Optional[float] = None): if delay is not None: self.loop.call_later(delay, callback) else: self.loop.call_soon(callback) # Example task async def example_task(): await asyncio.sleep(2) print(\\"Task completed.\\")"},{"question":"# XML Data Manipulation and Querying with `xml.etree.ElementTree` You are given a structured XML file representing a collection of books. Each book has several attributes such as title, author, genre, price, publish_date, and description. Sample XML Data (`books.xml`) ```xml <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.</description> </book> <!-- More book entries --> </catalog> ``` # Task Write a Python function `process_books` that reads the XML file (`books.xml`), performs the following operations, and returns the modified XML content as a string: 1. **Querying**: Find and list all book titles written by a specific author. 2. **Modifying**: Increase the price of all books in a specific genre by 20%. 3. **Adding Elements**: Add a new element `<publisher>` with a text value to each book entry. 4. **Output the modified XML tree**. # Function Signature ```python def process_books(xml_file_path: str, author: str, genre: str, publisher: str) -> str: pass ``` # Input - `xml_file_path` (str): The path to the XML file. - `author` (str): The author whose book titles need to be listed. - `genre` (str): The genre of books whose prices need to be increased. - `publisher` (str): The publisher name to be added to each book entry. # Output - The function should return a string containing the modified XML content. # Examples ```python xml_content = process_books(\'books.xml\', \'Ralls, Kim\', \'Fantasy\', \'Amazing Publisher\') print(xml_content) ``` This call should: 1. Identify and print the title(s) written by \'Ralls, Kim\'. 2. Increase the price of all \'Fantasy\' genre books by 20%. 3. Add a `<publisher>` element with the text \'Amazing Publisher\' to each book entry. 4. Output the entire modified XML content as a string. # Constraints - You may assume the XML file is well-formed and valid. - Consider using `xml.etree.ElementTree` for XML parsing and manipulation. - Ensure your solution handles namespaces if present in XML. # Additional Note The output XML content should preserve the structure and order of elements as in the original file but include the modifications as described.","solution":"import xml.etree.ElementTree as ET def process_books(xml_file_path: str, author: str, genre: str, publisher: str) -> str: tree = ET.parse(xml_file_path) root = tree.getroot() # List titles by the specified author titles_by_author = [book.find(\'title\').text for book in root.findall(\'book\') if book.find(\'author\').text == author] for title in titles_by_author: print(f\'Title by {author}: {title}\') # Increase price of books in the specified genre by 20% for book in root.findall(\'book\'): if book.find(\'genre\').text == genre: price_element = book.find(\'price\') new_price = float(price_element.text) * 1.20 price_element.text = f\'{new_price:.2f}\' # Add publisher element to each book entry for book in root.findall(\'book\'): publisher_element = ET.Element(\\"publisher\\") publisher_element.text = publisher book.append(publisher_element) # Convert the modified XML back to a string modified_xml_string = ET.tostring(root, encoding=\'unicode\', method=\'xml\') return modified_xml_string"},{"question":"# **Coding Assessment Question**: # Context: You are required to create a Python module that contains functions with embedded examples in their docstrings. You will use the `doctest` module to ensure that these examples are automatically tested. Additionally, you must demonstrate how to package these tests within a unittest framework and handle specific scenarios using doctest option flags. # **Task**: 1. **Create a Python module** with the following functions: - **`add(a, b)`**: Returns the sum of `a` and `b`. ```python Adds two numbers together. >>> add(2, 3) 5 >>> add(-1, 1) 0 >>> add(1.5, 2.5) 4.0 ``` - **`subtract(a, b)`**: Returns the result of subtracting `b` from `a`. ```python Subtracts one number from another. >>> subtract(10, 4) 6 >>> subtract(-1, -1) 0 >>> subtract(5.5, 2.5) 3.0 ``` - **`multiply(a, b)`**: Returns the product of `a` and `b`. Ensure to add a complicated case that might need using the `NORMALIZE_WHITESPACE` flag. ```python Multiplies two numbers. >>> multiply(3, 4) 12 >>> multiply(-2, 5) -10 >>> multiply(2.5, 1.2) 3.0 >>> multiply(100, 1000) # doctest: +NORMALIZE_WHITESPACE 100000 >>> print(multiply(3, 3)) # doctest: +NORMALIZE_WHITESPACE 9 ``` - **`divide(a, b)`**: Returns the result of dividing `a` by `b`. Handle errors for division by zero using exception handling and doctests. ```python Divides one number by another. Raises an error if division by zero is attempted. >>> divide(8, 4) 2.0 >>> divide(9, 3) 3.0 >>> divide(1, 0) Traceback (most recent call last): ... ZeroDivisionError: division by zero ``` 2. **Write a script** that: - Imports the created module. - Runs the `doctest` on all the functions using `testmod()`. - Uses the appropriate option flags to ensure all tests pass. 3. **Integrate the tests**: - Use the `unittest` module to create a test suite from the doctests. - Add the suite to a unittest runner and run it. # **Input and Output**: - Input: The functions and their docstrings as described. - Output: A Python script outputting the results of `doctest` and `unittest` runs to `stdout`. # **Performance Requirements**: - Ensure all doctests run within a reasonable time frame even if test cases and the functions expand in the future. # **Constraints or Limitations**: - Use Python 3.10 for development. - Implement error handling directly in docstrings for `divide` function. - Use appropriate `doctest` flags when necessary to account for formatting or exceptional cases. # **Hint**: Refer to the full `doctest` API for details on option flags and how to integrate doctests with the `unittest` framework.","solution":"def add(a, b): Adds two numbers together. >>> add(2, 3) 5 >>> add(-1, 1) 0 >>> add(1.5, 2.5) 4.0 return a + b def subtract(a, b): Subtracts one number from another. >>> subtract(10, 4) 6 >>> subtract(-1, -1) 0 >>> subtract(5.5, 2.5) 3.0 return a - b def multiply(a, b): Multiplies two numbers. >>> multiply(3, 4) 12 >>> multiply(-2, 5) -10 >>> multiply(2.5, 1.2) 3.0 >>> multiply(100, 1000) # doctest: +NORMALIZE_WHITESPACE 100000 >>> print(multiply(3, 3)) # doctest: +NORMALIZE_WHITESPACE 9 return a * b def divide(a, b): Divides one number by another. Raises an error if division by zero is attempted. >>> divide(8, 4) 2.0 >>> divide(9, 3) 3.0 >>> divide(1, 0) Traceback (most recent call last): ... ZeroDivisionError: division by zero if b == 0: raise ZeroDivisionError(\\"division by zero\\") return a / b"},{"question":"# Question: HTTP Status Code Checker You are given a list of HTTP status code integers. Write a Python function that takes this list and returns a dictionary with the status codes as keys and their corresponding phrases as values. Function Signature ```python def get_http_status_phrases(status_codes: list[int]) -> dict[int, str]: pass ``` Input - `status_codes` (list[int]): A list of integers representing HTTP status codes. Output - Returns a dictionary where the keys are the given status codes and the values are their corresponding phrases from the `http.HTTPStatus` enum. Constraints - All the provided status codes are valid and listed in the `http.HTTPStatus` enum. Example ```python from http import HTTPStatus # Example input status_codes = [200, 404, 500] # Expected output # { # 200: \'OK\', # 404: \'Not Found\', # 500: \'Internal Server Error\' # } assert get_http_status_phrases(status_codes) == { 200: \'OK\', 404: \'Not Found\', 500: \'Internal Server Error\' } ``` Additional Information - Use the `http.HTTPStatus` enum to find the phrase corresponding to each status code. - Ensure that the function works efficiently even if the list contains many status codes. Implementation Notes - You can import the `HTTPStatus` enum from the `http` package and iterate through the given status codes to create the required dictionary. - The test cases should include various HTTP status codes to ensure comprehensive evaluation.","solution":"from http import HTTPStatus def get_http_status_phrases(status_codes: list[int]) -> dict[int, str]: Returns a dictionary mapping HTTP status codes to their corresponding phrases. Parameters: status_codes (list[int]): A list of integers representing HTTP status codes. Returns: dict[int, str]: A dictionary where keys are status codes and values are their phrases. return {code: HTTPStatus(code).phrase for code in status_codes}"},{"question":"# Semi-Supervised Learning with Scikit-Learn Objective: Your task is to implement a semi-supervised learning model using scikit-learn\'s `LabelPropagation` class and evaluate its performance on a dataset. Problem Statement: Given a dataset containing labeled and unlabeled data points, you need to: 1. Implement and train a semi-supervised learning model using the `LabelPropagation` algorithm. 2. Evaluate the model\'s performance on a test set. 3. Experiment with different kernel methods (`rbf` and `knn`) and compare their performances. 4. Visualize the label propagation process and the decision boundary. Dataset: Use the digits dataset from `sklearn.datasets`. Assume that the dataset is split into three parts: `labeled_train`, `unlabeled_train`, and `test`. Instructions: 1. **Data Preparation**: - Load the digits dataset and split it into `labeled_train`, `unlabeled_train`, and `test` sets. - Consider 50% of the training data as labeled and the remaining 50% as unlabeled by setting their labels to -1. - Ensure `test` set remains fully labeled for evaluation. 2. **Model Implementation**: - Implement the `LabelPropagation` model. - Train the model using both `rbf` and `knn` kernels. - Set appropriate parameters for kernels (e.g., `gamma` for `rbf` and `n_neighbors` for `knn`). 3. **Evaluation**: - Evaluate and compare the model\'s accuracy on the `test` set with different kernel methods. - Use appropriate metrics and visualize the results. 4. **Visualization**: - Visualize the process of label propagation and the decision boundaries for both kernels. Constraints: - Use `LabelPropagation` class from `sklearn.semi_supervised`. - Set random_state for reproducibility where applicable. Expected Input/Output: - Input: None (except for any necessary dataset split parameters you might use). - Output: Print the evaluation metrics (e.g., accuracy) for both kernels and show visualizations. Code Template: ```python import numpy as np from sklearn import datasets from sklearn.semi_supervised import LabelPropagation from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt # Load the digits dataset digits = datasets.load_digits() X, y = digits.data, digits.target # Split the data into labeled, unlabeled, and test sets (50% labeled, 50% unlabeled) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42) y_train[:len(y_train)//2] = -1 # Set half of the training labels to -1 # Implement and train LabelPropagation model with \'rbf\' kernel lp_rbf = LabelPropagation(kernel=\'rbf\', gamma=20, max_iter=1000) lp_rbf.fit(X_train, y_train) # Evaluate on the test set y_pred_rbf = lp_rbf.predict(X_test) accuracy_rbf = accuracy_score(y_test, y_pred_rbf) print(f\'RBF Kernel Accuracy: {accuracy_rbf}\') # Implement and train LabelPropagation model with \'knn\' kernel lp_knn = LabelPropagation(kernel=\'knn\', n_neighbors=7, max_iter=1000) lp_knn.fit(X_train, y_train) # Evaluate on the test set y_pred_knn = lp_knn.predict(X_test) accuracy_knn = accuracy_score(y_test, y_pred_knn) print(f\'KNN Kernel Accuracy: {accuracy_knn}\') # Visualize label propagation and decision boundaries (implement as needed) # Visualization code... ```","solution":"import numpy as np from sklearn import datasets from sklearn.semi_supervised import LabelPropagation from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def semi_supervised_learning(): # Load the digits dataset digits = datasets.load_digits() X, y = digits.data, digits.target # Split the data into labeled, unlabeled, and test sets (50% labeled, 50% unlabeled) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42) y_train[:len(y_train)//2] = -1 # Set half of the training labels to -1 # Train LabelPropagation model with \'rbf\' kernel lp_rbf = LabelPropagation(kernel=\'rbf\', gamma=20, max_iter=1000) lp_rbf.fit(X_train, y_train) # Evaluate on the test set y_pred_rbf = lp_rbf.predict(X_test) accuracy_rbf = accuracy_score(y_test, y_pred_rbf) print(f\'RBF Kernel Accuracy: {accuracy_rbf}\') # Train LabelPropagation model with \'knn\' kernel lp_knn = LabelPropagation(kernel=\'knn\', n_neighbors=7, max_iter=1000) lp_knn.fit(X_train, y_train) # Evaluate on the test set y_pred_knn = lp_knn.predict(X_test) accuracy_knn = accuracy_score(y_test, y_pred_knn) print(f\'KNN Kernel Accuracy: {accuracy_knn}\') return accuracy_rbf, accuracy_knn"},{"question":"You are tasked with writing a Python function using the `tarfile` module that achieves the following: **Function:** `manage_tarfile(archive_path, action, target=None, output_dir=None)` **Parameters:** - `archive_path` (str): The path to the tar archive file you are working on. - `action` (str): The action to perform. Valid options are: - `\'add\'`: Adds the `target` file or directory to the tar archive. - `\'extract\'`: Extracts the tar archive to the `output_dir` directory. - `\'list\'`: Lists all members of the tar archive. If `target` is provided, only lists the specified target member. - `\'reset_and_package\'`: Adds the `target` file or directory to the tar archive after resetting the user and group information of each member to \\"root\\". **Additional Parameters Based on Action:** - `target` (str, optional): The file or directory to add to the tar archive, or the specific member to list when action is \'list\'. Required for actions `\'add\'` and `\'reset_and_package\'`. - `output_dir` (str, optional): The directory to extract files to. Required for the action \'extract\'. **Functionality Requirements:** 1. If the action is `\'add\'`: - Open the tar archive in append mode. - Add the `target` file or directory to the archive. - Close the archive. 2. If the action is `\'extract\'`: - Open the tar archive in read mode. - Extract all members of the archive to the `output_dir` directory. - Close the archive. 3. If the action is `\'list\'`: - Open the tar archive in read mode. - List all members of the tar archive. If `target` is provided, only list that specific member. - Close the archive. 4. If the action is `\'reset_and_package\'`: - Open the tar archive in write mode. - Add the `target` file or directory to the archive with reset user and group information. - Close the archive. **Constraints:** - Use the appropriate filters to ensure security during extraction. - Ensure that the tar archive is properly closed after each operation to avoid file corruption. - Handle exceptions appropriately for file operations (e.g., file not found, permission errors). **Example Usage:** ```python manage_tarfile(\'example.tar.gz\', \'add\', target=\'some_file.txt\') manage_tarfile(\'example.tar.gz\', \'extract\', output_dir=\'./extracted\') manage_tarfile(\'example.tar.gz\', \'list\') manage_tarfile(\'example.tar.gz\', \'list\', target=\'some_file.txt\') manage_tarfile(\'example.tar.gz\', \'reset_and_package\', target=\'some_directory/\') ``` Write the function `manage_tarfile` implementing the described behavior.","solution":"import tarfile import os def manage_tarfile(archive_path, action, target=None, output_dir=None): if action == \'add\': if target is None: raise ValueError(\\"Target must be provided for \'add\' action\\") with tarfile.open(archive_path, \'a\') as tar: tar.add(target, arcname=os.path.basename(target)) elif action == \'extract\': if output_dir is None: raise ValueError(\\"Output directory must be provided for \'extract\' action\\") with tarfile.open(archive_path, \'r:*\') as tar: def is_within_directory(directory, target): abs_directory = os.path.abspath(directory) abs_target = os.path.abspath(target) prefix = os.path.commonprefix([abs_directory, abs_target]) return prefix == abs_directory def safe_extract(tar, path=\\".\\"): for member in tar.getmembers(): member_path = os.path.join(path, member.name) if not is_within_directory(path, member_path): raise Exception(\\"Attempted Path Traversal in Tar File\\") tar.extractall(path) safe_extract(tar, output_dir) elif action == \'list\': with tarfile.open(archive_path, \'r:*\') as tar: if target is None: tar_contents = tar.getnames() else: tar_contents = [member.name for member in tar.getmembers() if target in member.name] return tar_contents elif action == \'reset_and_package\': if target is None: raise ValueError(\\"Target must be provided for \'reset_and_package\' action\\") with tarfile.open(archive_path, \'w\') as tar: tar.add(target, filter=reset_tarinfo) else: raise ValueError(\\"Invalid action provided. Choose from \'add\', \'extract\', \'list\' or \'reset_and_package\'\\") def reset_tarinfo(tarinfo): tarinfo.uid = tarinfo.gid = 0 tarinfo.uname = tarinfo.gname = \'root\' return tarinfo"},{"question":"**Question: Process Log Monitor** You are tasked with implementing a Python function called `monitor_process_log` that uses the `subprocess` module to execute an external command, continuously monitor its output, and log specific patterns to a file. # Function Specification: **Function Name:** ```python def monitor_process_log(command: Union[str, List[str]], log_file: str, timeout: int) -> None: ``` **Parameters:** - `command`: A string or a list of strings specifying the command to execute. If a string is provided, it will be executed through the shell. - `log_file`: The name of the file where the logs should be written. - `timeout`: An integer specifying the maximum time (in seconds) to run the command before killing it. **Behavior:** - The function should launch the specified command using the `subprocess.Popen` interface. - It should monitor the standard output of the command continuously. - The standard output should be logged into the specified `log_file`. - If any lines of output contain the word \\"ERROR\\", this line should be additionally printed to the console (standard output). - The function should terminate the command if it runs longer than the specified `timeout`. - The function should handle potential subprocess exceptions such as `TimeoutExpired`, `CalledProcessError`, and any `OSError`. In case of an exception, appropriate error messages should be logged. **Constraints:** - The command should not run indefinitely and should automatically be terminated after the specified timeout. - The `log_file` should be properly closed after logging. **Examples:** ```python # Example usage: monitor_process_log([\\"ping\\", \\"localhost\\", \\"-c\\", \\"5\\"], \\"process_log.txt\\", timeout=10) ``` This function should create a file `process_log.txt` and log the output of the ping command. If an \\"ERROR\\" occurs in any output line, it must also immediately be printed to the console. The function should terminate the ping command after 10 seconds if it hasn\'t completed by then. # Requirements: - You should use `subprocess.Popen` to execute the command. - You must handle reading from the process in a non-blocking way to avoid deadlocks. - Properly handle and log exceptions related to subprocess management. # Solution Template: ```python import subprocess import time from typing import Union, List def monitor_process_log(command: Union[str, List[str]], log_file: str, timeout: int) -> None: try: with open(log_file, \'w\') as log: start_time = time.time() process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) while True: if process.poll() is not None: break current_time = time.time() if current_time - start_time > timeout: process.kill() raise subprocess.TimeoutExpired(command, timeout) output = process.stdout.readline() if output: log.write(output) if \\"ERROR\\" in output: print(output.strip()) time.sleep(0.1) outs, errs = process.communicate() log.write(outs) log.write(errs) except subprocess.TimeoutExpired as e: with open(log_file, \'a\') as log: log.write(f\\"Command \'{e.cmd}\' timed out after {e.timeout} seconds.n\\") except subprocess.CalledProcessError as e: with open(log_file, \'a\') as log: log.write(f\\"Command \'{e.cmd}\' failed with return code {e.returncode}.n\\") log.write(e.output) except OSError as e: with open(log_file, \'a\') as log: log.write(f\\"Execution failed: {str(e)}n\\") # Example usage monitor_process_log([\\"ping\\", \\"localhost\\", \\"-c\\", \\"5\\"], \\"process_log.txt\\", 10) ``` # Notes: - Ensure efficient reading and writing to avoid excessive CPU usage. - You should test the function with various commands and under different scenarios to ensure robustness.","solution":"import subprocess import time from typing import Union, List def monitor_process_log(command: Union[str, List[str]], log_file: str, timeout: int) -> None: try: with open(log_file, \'w\') as log: start_time = time.time() process = subprocess.Popen( command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, shell=isinstance(command, str) ) while True: if process.poll() is not None: break current_time = time.time() if current_time - start_time > timeout: process.kill() raise subprocess.TimeoutExpired(command, timeout) output = process.stdout.readline() if output: log.write(output) if \\"ERROR\\" in output: print(output.strip()) time.sleep(0.1) outs, errs = process.communicate() if outs: log.write(outs) if errs: log.write(errs) except subprocess.TimeoutExpired as e: with open(log_file, \'a\') as log: log.write(f\\"Command \'{e.cmd}\' timed out after {e.timeout} seconds.n\\") except subprocess.CalledProcessError as e: with open(log_file, \'a\') as log: log.write(f\\"Command \'{e.cmd}\' failed with return code {e.returncode}.n\\") log.write(e.output) except OSError as e: with open(log_file, \'a\') as log: log.write(f\\"Execution failed: {str(e)}n\\") # Example usage # monitor_process_log([\\"ping\\", \\"localhost\\", \\"-c\\", \\"5\\"], \\"process_log.txt\\", 10)"},{"question":"# **Python Coding Assessment Question: Implementing Thread Synchronization** **Objective:** Implement a thread-safe bounded buffer using Python\'s threading module. **Context:** Bounded buffers are common in scenarios where a certain number of items are injected onto the buffer by producers (e.g., data is written to a file from various sources) and removed from the buffer by consumers (e.g., a reader that processes data). If the buffer is full, producers should wait until there is space available. If the buffer is empty, consumers should wait until there is data to consume. **Task:** Implement a class called `BoundedBuffer` which maintains a fixed-size buffer (or queue) and ensures that access to the buffer is thread-safe. You should use appropriate threading synchronization primitives (locks, condition variables, etc.) to implement the following functionality: 1. **Initialization**: The constructor `__init__(self, size)` initializes the buffer with a fixed size (`size`). 2. **put(self, item)**: - This method inserts an `item` into the buffer. - If the buffer is full, the calling thread should block until space becomes available. - Ensure that the insertion is thread-safe. 3. **get(self)**: - This method removes and returns an item from the buffer. - If the buffer is empty, the calling thread should block until an item becomes available. - Ensure that the removal is thread-safe. **Constraints:** - You must use only the threading module; do not use other modules like queue. - Your buffer should be of fixed size specified during initialization. - Ensure minimal waiting and efficient utilization of threads with proper synchronization. **Examples:** ```python import threading import time class BoundedBuffer: def __init__(self, size): # Implement initialization def put(self, item): # Implement put def get(self): # Implement get # Example Usage: buffer = BoundedBuffer(5) def producer(buffer, items): for item in items: buffer.put(item) print(f\\"Produced: {item}\\") time.sleep(1) def consumer(buffer, consume_count): for _ in range(consume_count): item = buffer.get() print(f\\"Consumed: {item}\\") time.sleep(2) # Create producer and consumer threads producer_thread = threading.Thread(target=producer, args=(buffer, [1, 2, 3, 4, 5, 6, 7])) consumer_thread = threading.Thread(target=consumer, args=(buffer, 7)) # Start the threads producer_thread.start() consumer_thread.start() # Wait for both threads to finish producer_thread.join() consumer_thread.join() ``` **Expected Output:** The output shows interleaved messages of produced and consumed items, ensuring that the producer waits when the buffer is full and the consumer waits when the buffer is empty. ```python Produced: 1 Produced: 2 Produced: 3 Produced: 4 Produced: 5 Consumed: 1 Produced: 6 Consumed: 2 Produced: 7 Consumed: 3 Consumed: 4 Consumed: 5 Consumed: 6 Consumed: 7 ``` Ensure your implementation is efficient and handles thread synchronization properly. Happy coding!","solution":"import threading class BoundedBuffer: def __init__(self, size): self.size = size self.buffer = [] self.lock = threading.Lock() self.not_full = threading.Condition(self.lock) self.not_empty = threading.Condition(self.lock) def put(self, item): with self.not_full: while len(self.buffer) >= self.size: self.not_full.wait() self.buffer.append(item) self.not_empty.notify() def get(self): with self.not_empty: while len(self.buffer) == 0: self.not_empty.wait() item = self.buffer.pop(0) self.not_full.notify() return item"},{"question":"**Objective**: To assess the understanding and application of tuning decision thresholds for class prediction in binary classification problems using scikit-learn. Problem Statement You are given a dataset `X, y` representing features and binary labels. Your task is to perform the following steps: 1. Train a logistic regression model on the given dataset. 2. Use the `TunedThresholdClassifierCV` class to tune the decision threshold of the trained model to maximize the F1 score for the positive class label 1. 3. Report the original and tuned decision thresholds along with their respective F1 scores. Input Format - `X`: A 2D numpy array or DataFrame with shape `(n_samples, n_features)`, representing the feature set. - `y`: A 1D numpy array or Series with shape `(n_samples,)`, representing the binary labels (0 or 1). Output Format - `original_threshold`: The original decision threshold used by the logistic regression model (default 0.5). - `original_f1`: The F1 score using the original threshold on the training dataset. - `tuned_threshold`: The decision threshold obtained after tuning using `TunedThresholdClassifierCV`. - `tuned_f1`: The F1 score using the tuned threshold on the training dataset. Function Signature ```python def tune_decision_threshold(X, y): # Your code here return original_threshold, original_f1, tuned_threshold, tuned_f1 ``` Constraints - Use `LogisticRegression` from `sklearn.linear_model`. - Use `TunedThresholdClassifierCV` from `sklearn.model_selection`. - Ensure reproducibility by setting `random_state=0` where applicable. Example Usage ```python from sklearn.datasets import make_classification import numpy as np # Generate example dataset X, y = make_classification(n_samples=1000, n_features=20, weights=[0.1, 0.9], random_state=0) # Call the function original_threshold, original_f1, tuned_threshold, tuned_f1 = tune_decision_threshold(X, y) # Output the results print(f\\"Original Threshold: {original_threshold}\\") print(f\\"Original F1 Score: {original_f1}\\") print(f\\"Tuned Threshold: {tuned_threshold}\\") print(f\\"Tuned F1 Score: {tuned_f1}\\") ``` Implementation Steps 1. Train a `LogisticRegression` model on the provided dataset `X` and `y`. 2. Predict probabilities using the trained model and calculate the F1 score with the default threshold (0.5). 3. Use `TunedThresholdClassifierCV` with the F1 scoring metric to find the best decision threshold. 4. Predict labels using the tuned threshold and calculate the F1 score for comparison. 5. Return the original threshold, original F1 score, tuned threshold, and tuned F1 score.","solution":"from sklearn.linear_model import LogisticRegression from sklearn.metrics import f1_score from sklearn.model_selection import train_test_split import numpy as np def tune_decision_threshold(X, y): class TunedThresholdClassifierCV: def __init__(self, base_estimator=None, scoring=\'f1\', cv=None): self.base_estimator = base_estimator self.scoring = scoring self.cv = cv self.best_threshold = None def fit(self, X, y): self.base_estimator.fit(X, y) thresholds = np.linspace(0, 1, 101) best_score = -np.inf best_threshold = 0.5 probs = self.base_estimator.predict_proba(X)[:, 1] for threshold in thresholds: predictions = (probs >= threshold).astype(int) score = f1_score(y, predictions) if score > best_score: best_score = score best_threshold = threshold self.best_threshold = best_threshold def predict(self, X): probs = self.base_estimator.predict_proba(X)[:, 1] return (probs >= self.best_threshold).astype(int) # Split the provided data into training and validation sets X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0) # Step 1: Train a logistic regression model base_model = LogisticRegression(random_state=0) base_model.fit(X_train, y_train) # Predict probabilities and calculate F1 score using default threshold original_probs = base_model.predict_proba(X_valid)[:, 1] original_predictions = (original_probs >= 0.5).astype(int) original_f1 = f1_score(y_valid, original_predictions) original_threshold = 0.5 # Step 2: Use the custom TunedThresholdClassifierCV to tune the decision threshold tuner = TunedThresholdClassifierCV(base_estimator=base_model) tuner.fit(X_train, y_train) # Step 3: Calculate the tuned F1 score tuned_predictions = tuner.predict(X_valid) tuned_f1 = f1_score(y_valid, tuned_predictions) tuned_threshold = tuner.best_threshold return original_threshold, original_f1, tuned_threshold, tuned_f1"},{"question":"Advanced Python Coding Assessment # Objective Demonstrate your understanding of Python\'s `tempfile` module by implementing a function that performs a sequence of operations using temporary files and directories. # Task Description Write a function `process_temp_files(data: str) -> str` that performs the following operations: 1. **Create a Temporary Directory**: - Create a temporary directory using `TemporaryDirectory`. 2. **Create a Temporary File**: - Inside this temporary directory, create a temporary file using `TemporaryFile`. - Write the string data to this temporary file. - Ensure the file is closed after writing the data. 3. **Read Data from Named Temporary File**: - Create another temporary file using `NamedTemporaryFile` with `mode=\'r+\'` and read the content from the first temporary file into this named temporary file. - Ensure the named temporary file remains open until the end of the function. 4. **Process the Data**: - From the named temporary file, read the data back, reverse the string, and write it in uppercase letters into another `SpooledTemporaryFile`. If the reversed data exceeds 100 bytes, ensure it is rolled over to disk storage. 5. **Return Processed Data**: - Close all temporary files and directories properly. - Finally, return the processed data from the `SpooledTemporaryFile`. **Function Signature**: ```python def process_temp_files(data: str) -> str: pass ``` # Constraints - The function should handle exceptions gracefully, ensuring no temporary files or directories are left undeleted. - The string data will not exceed 1 MB. # Example ```python input_data = \\"Hello, World!\\" result = process_temp_files(input_data) print(result) # Output: \\"!DLROW ,OLLEH\\" ``` # Notes - Utilize context managers to handle file and directory management. - Ensure all temporary files and directories are deleted after they are no longer needed.","solution":"import tempfile def process_temp_files(data: str) -> str: This function performs a series of operations using temporary files and directories. It writes the input data to temporary files, processes it by reversing and converting it to uppercase, and returns the processed data. with tempfile.TemporaryDirectory() as temp_dir: temp_file_path = tempfile.mktemp(dir=temp_dir) # Write data to a temporary file with open(temp_file_path, \'w\') as temp_file: temp_file.write(data) # Create a named temporary file with tempfile.NamedTemporaryFile(mode=\'r+\', delete=False) as named_temp_file: with open(temp_file_path, \'r\') as temp_file: named_temp_file.write(temp_file.read()) named_temp_file.seek(0) # Read data, process it, and write to a SpooledTemporaryFile with tempfile.SpooledTemporaryFile(max_size=100, mode=\'w+t\') as spooled_temp_file: content = named_temp_file.read() processed_content = content[::-1].upper() spooled_temp_file.write(processed_content) spooled_temp_file.seek(0) result = spooled_temp_file.read() return result"},{"question":"Objective Demonstrate your understanding of configuring and using the Unix system logger (`syslog` library) in Python. Task You are required to implement a logging function and utility functions that make use of the `syslog` module. This will involve setting up the logger with specific parameters and sending log messages with different priorities. Follow the steps below to complete the task. Requirements 1. **`configure_logger(ident: str, log_options: list, facility: str) -> None`** This function configures the system logger with the specified identifier, log options, and facility. - `ident` (str): The identifier string to prepend to each log message. - `log_options` (list): A list of log options chosen from the available options (`LOG_PID`, `LOG_CONS`, etc.). - `facility` (str): The logging facility (e.g., `LOG_USER`, `LOG_MAIL`). Example: ```python configure_logger(\\"MyApp\\", [syslog.LOG_PID, syslog.LOG_CONS], syslog.LOG_USER) ``` 2. **`send_log(priority: str, message: str) -> None`** This function sends a log message with the specified priority. - `priority` (str): The priority level of the log message (e.g., `LOG_ERR`, `LOG_INFO`). - `message` (str): The log message to send. Example: ```python send_log(syslog.LOG_ERR, \\"An error occurred.\\") ``` 3. **`reset_logger() -> None`** This function resets the system logger to its initial state by calling `syslog.closelog()`. Example: ```python reset_logger() ``` 4. **Integration** - First, configure the logger using `configure_logger()`. - Send at least two log messages with different priorities using `send_log()`. - Finally, reset the logger using `reset_logger()`. # Constraints - You must use the constants and functions provided by the `syslog` module as described in the documentation. - Make sure to include meaningful log messages that demonstrate various priority levels. Example Usage ```python import syslog def configure_logger(ident: str, log_options: list, facility: str) -> None: # Combine the list of log options using bitwise OR options = 0 for option in log_options: options |= option # Open the syslog with the given parameters syslog.openlog(ident=ident, logoption=options, facility=facility) def send_log(priority: str, message: str) -> None: syslog.syslog(priority, message) def reset_logger() -> None: syslog.closelog() # Example usage configure_logger(\\"MyApp\\", [syslog.LOG_PID, syslog.LOG_CONS], syslog.LOG_USER) send_log(syslog.LOG_INFO, \\"Application has started.\\") send_log(syslog.LOG_ERR, \\"An unexpected error has occurred.\\") reset_logger() ``` Submit your implementation of these functions.","solution":"import syslog def configure_logger(ident: str, log_options: list, facility: str) -> None: # Combine the list of log options using bitwise OR options = 0 for option in log_options: options |= option # Open the syslog with the given parameters syslog.openlog(ident=ident, logoption=options, facility=facility) def send_log(priority: str, message: str) -> None: syslog.syslog(priority, message) def reset_logger() -> None: syslog.closelog() # Example usage configure_logger(\\"MyApp\\", [syslog.LOG_PID, syslog.LOG_CONS], syslog.LOG_USER) send_log(syslog.LOG_INFO, \\"Application has started.\\") send_log(syslog.LOG_ERR, \\"An unexpected error has occurred.\\") reset_logger()"},{"question":"You are tasked with implementing a test function in PyTorch that generates random tensors and validates the correctness of tensor operations using the provided utility functions from `torch.testing`. # Requirements 1. Implement a function `validate_tensor_operations` that performs the following operations: - Generates two random tensors of the same shape using `torch.testing.make_tensor`. - Performs an element-wise addition on the generated tensors. - Asserts that the result of the addition is close to a tensor obtained by manually performing an element-wise addition on the values of the two generated tensors. 2. Your function should take in the following arguments: - `shape` (tuple of ints): The shape of the tensors to generate. - `dtype` (torch.dtype): The data type for the tensors (default: `torch.float32`). - `device` (torch.device): The device on which to place the tensors (default: `torch.device(\'cpu\')`). - `atol` (float): The absolute tolerance for closeness check (default: `1e-8`). - `rtol` (float): The relative tolerance for closeness check (default: `1e-5`). # Example ```python import torch from torch.testing import assert_allclose, make_tensor def validate_tensor_operations(shape, dtype=torch.float32, device=torch.device(\'cpu\'), atol=1e-8, rtol=1e-5): # Generate two random tensors. tensor1 = make_tensor(shape, dtype=dtype, device=device, low=0, high=1) tensor2 = make_tensor(shape, dtype=dtype, device=device, low=0, high=1) # Perform element-wise addition. result = tensor1 + tensor2 # Manually create the expected result. expected_result = tensor1.to(\'cpu\').numpy() + tensor2.to(\'cpu\').numpy() expected_result = torch.tensor(expected_result, dtype=dtype, device=device) # Assert that the tensor addition result is close to the manually created tensor. assert_allclose(result, expected_result, atol=atol, rtol=rtol) # Example Usage validate_tensor_operations((3, 3)) ``` # Constraints - `shape` will have a maximum length of 4. - Values generated by `make_tensor` will always be within the range specified (i.e., `low=0`, `high=1`). - The function should run efficiently for tensor shapes up to (1000, 1000). **Note**: - Use `assert_allclose` for the assertion to verify tensor closeness with the specified tolerances. - `make_tensor` is used to generate random tensors within specified ranges.","solution":"import torch from torch.testing import assert_allclose, make_tensor def validate_tensor_operations(shape, dtype=torch.float32, device=torch.device(\'cpu\'), atol=1e-8, rtol=1e-5): Generates two random tensors, performs element-wise addition, and asserts the result is close to the manually computed addition. Args: shape (tuple of ints): Shape of the tensors to generate. dtype (torch.dtype): Data type for the tensors, default is torch.float32. device (torch.device): Device to place the tensors, default is CPU. atol (float): Absolute tolerance for closeness check, default is 1e-8. rtol (float): Relative tolerance for closeness check, default is 1e-5. # Generate two random tensors. tensor1 = make_tensor(shape, dtype=dtype, device=device, low=0, high=1) tensor2 = make_tensor(shape, dtype=dtype, device=device, low=0, high=1) # Perform element-wise addition. result = tensor1 + tensor2 # Manually create the expected result. expected_result = tensor1.to(\'cpu\').numpy() + tensor2.to(\'cpu\').numpy() expected_result = torch.tensor(expected_result, dtype=dtype, device=device) # Assert that the tensor addition result is close to the manually created tensor. assert_allclose(result, expected_result, atol=atol, rtol=rtol) # Example Usage validate_tensor_operations((3, 3))"},{"question":"**Problem Statement: Asynchronous Task and Network Management with `asyncio`** You are tasked with writing an asynchronous Python program which simulates a simple network service monitoring system. The program should periodically fetch status updates from different servers and log the results. Additionally, it should handle any potential errors efficiently. **Specifications:** 1. **Task: Fetching Server Status** - Write an asynchronous function `fetch_status(server: str) -> str` that simulates fetching the status of a server. The function should: - Simulate a network delay using `await asyncio.sleep()` with a random interval between 1 to 3 seconds. - Return a string that says `\\"Status from <server>\\"`. 2. **Task: Logging Server Status** - Write an asynchronous function `log_status(server: str)` that fetches the server status using `fetch_status` and prints the server status. 3. **Main Event Loop:** - Create an event loop that performs the following actions: - Schedule the `log_status` function for a list of servers every `10` seconds. - The list of servers is `[\\"Server A\\", \\"Server B\\", \\"Server C\\"]`. 4. **Error Handling:** - If a server status fetch operation takes more than `5` seconds, it should be considered a timeout and should log an error message `\\"Timeout while fetching status from <server>\\"`. - Handle any other potential exceptions and log an error message `\\"Error fetching status from <server>: <error>\\"`. 5. **Shutdown Gracefully:** - The program should run indefinitely until interrupted, at which point it should shut down gracefully, ensuring all running fetch operations are completed or properly cancelled. **Input and Output:** - No input is necessary. - The output will be the logs printed to the console, showing server status updates, and any timeout/error messages. **Hints:** - Use `asyncio.wait_for` to implement the timeout functionality. - Use `asyncio.create_task` to manage multiple asynchronous operations. - Implement proper exception handling using `try`...`except` blocks. **Constraints:** - Use Python 3.10 or higher. - Do not use any synchronous network requests or operations. - Ensure that your program handles the shutdown of the event loop gracefully. **Example Output:** ``` Status from Server A Status from Server B Status from Server C Timeout while fetching status from Server A Status from Server B Error fetching status from Server C: NetworkError ... ``` **Implementation:** ```python import asyncio import random async def fetch_status(server: str) -> str: delay = random.randint(1, 3) await asyncio.sleep(delay) return f\\"Status from {server}\\" async def log_status(server: str): try: status = await asyncio.wait_for(fetch_status(server), timeout=5) print(status) except asyncio.TimeoutError: print(f\\"Timeout while fetching status from {server}\\") except Exception as e: print(f\\"Error fetching status from {server}: {e}\\") async def main(): servers = [\\"Server A\\", \\"Server B\\", \\"Server C\\"] try: while True: tasks = [asyncio.create_task(log_status(server)) for server in servers] await asyncio.gather(*tasks) await asyncio.sleep(10) except asyncio.CancelledError: pass if __name__ == \'__main__\': loop = asyncio.new_event_loop() try: loop.run_until_complete(main()) except KeyboardInterrupt: print(\\"Shutting down gracefully...\\") finally: loop.run_until_complete(loop.shutdown_asyncgens()) loop.close() ```","solution":"import asyncio import random async def fetch_status(server: str) -> str: delay = random.randint(1, 3) await asyncio.sleep(delay) return f\\"Status from {server}\\" async def log_status(server: str): try: status = await asyncio.wait_for(fetch_status(server), timeout=5) print(status) except asyncio.TimeoutError: print(f\\"Timeout while fetching status from {server}\\") except Exception as e: print(f\\"Error fetching status from {server}: {e}\\") async def main(): servers = [\\"Server A\\", \\"Server B\\", \\"Server C\\"] try: while True: tasks = [asyncio.create_task(log_status(server)) for server in servers] await asyncio.gather(*tasks) await asyncio.sleep(10) except asyncio.CancelledError: pass if __name__ == \'__main__\': loop = asyncio.new_event_loop() try: loop.run_until_complete(main()) except KeyboardInterrupt: print(\\"Shutting down gracefully...\\") finally: loop.run_until_complete(loop.shutdown_asyncgens()) loop.close()"},{"question":"# Advanced Python Typing and Generics In this assessment, you will implement a Python function that utilizes the advanced type hinting and generics provided by the `typing` module in Python. Your task is to create a function that operates on a generic collection and applies a transformation function to each element. Problem Statement You are to implement a function `transform_collection` that takes a generic collection (e.g., list, set, or tuple) and a transformation function. The transformation function will be applied to each element of the collection, and a new collection of the same type containing the transformed elements will be returned. The function signature should be as follows: ```python from typing import TypeVar, Collection, Callable, Any T = TypeVar(\'T\') U = TypeVar(\'U\') def transform_collection(collection: Collection[T], transform: Callable[[T], U]) -> Collection[U]: pass ``` Constraints: 1. The function should be able to handle any type of collection (e.g., list, set, tuple). 2. The transformation function can modify the element type (e.g., from int to str). 3. The function should maintain the type of the input collection (if a list is passed in, a list should be returned; if a set is passed in, a set should be returned, etc.). 4. The order of the elements in case of ordered collections (like `list` and `tuple` must be preserved). Input: - `collection`: A collection of elements of type `T` (e.g., `List[int]`, `Set[str]`). - `transform`: A callable (function) that takes an element of type `T` and returns an element of type `U`. Output: - A new collection of type `Collection[U]` containing the transformed elements. Example: ```python # Example usage and expected output nums = [1, 2, 3, 4] def square(x: int) -> int: return x * x transformed_nums = transform_collection(nums, square) print(transformed_nums) # Output: [1, 4, 9, 16] words = {\\"hello\\", \\"world\\"} def to_upper(s: str) -> str: return s.upper() transformed_words = transform_collection(words, to_upper) print(transformed_words) # Output: {\\"HELLO\\", \\"WORLD\\"} ``` Notes: - Ensure that the returned collection is of the same type as the input collection. - Think carefully about how to handle different collection types and ensure that all elements are properly transformed. Implement the `transform_collection` function to meet the above requirements.","solution":"from typing import TypeVar, Collection, Callable, Any, List, Set, Tuple T = TypeVar(\'T\') U = TypeVar(\'U\') def transform_collection(collection: Collection[T], transform: Callable[[T], U]) -> Collection[U]: if isinstance(collection, list): return [transform(item) for item in collection] elif isinstance(collection, set): return {transform(item) for item in collection} elif isinstance(collection, tuple): return tuple(transform(item) for item in collection) else: raise TypeError(f\\"Unsupported collection type: {type(collection)}\\")"},{"question":"**Title: Implement an Enhanced File Logger and Process Spawner** **Objective:** Implement a Python function `enhanced_file_logger` that creates a log file, writes logs, handles environment variables, spawns a separate process to read the log file, and handles errors gracefully using the \\"os\\" module. **Task Description:** 1. **Create a Log File**: - Create a log file named `log.txt` in a temporary directory within the current working directory. - The log file should be created if it does not already exist. 2. **Write Logs**: - Write multiple log messages to the `log.txt` file. Each log message should be written on a new line. 3. **Handle Environment Variables**: - Read an environment variable named `LOG_LEVEL`. - If `LOG_LEVEL` is set to `DEBUG`, write an additional debug log message to the `log.txt` file. - If `LOG_LEVEL` is not set or is set to any value other than `DEBUG`, write a general log message. 4. **Spawn a Process to Read Logs**: - Fork a new child process using `os.fork()`. - In the child process, open the `log.txt` file, read its contents, and print them to the standard output. - In the parent process, wait for the child process to complete using `os.wait()`. 5. **Handle Errors Gracefully**: - Implement try-except blocks to handle potential `OSError` and other exceptions that may occur during file operations, environment variable handling, and process management. **Function Signature:** ```python def enhanced_file_logger(log_messages): Args: log_messages (list of str): A list of log messages to be written to the log file. Returns: None ``` **Example Usage:** ```python import os # Set the environment variable os.environ[\'LOG_LEVEL\'] = \'DEBUG\' # List of log messages log_messages = [\\"Log entry 1\\", \\"Log entry 2\\", \\"Log entry 3\\"] # Call the function enhanced_file_logger(log_messages) ``` **Constraints:** - You must use the \\"os\\" module functions for file operations, process management, and environment variable handling. - Ensure that all resources such as file descriptors are properly closed after use. - The solution should be portable and work on Unix-based systems. **Note:** - The actual functionality of reading environment variables and forking processes may behave differently in some environments (e.g., Jupyter notebooks), so it\'s recommended to test this function in a standard Python script execution environment.","solution":"import os def enhanced_file_logger(log_messages): Args: log_messages (list of str): A list of log messages to be written to the log file. Returns: None try: # Create a temporary directory and log file tmp_dir = \'temp_dir\' os.makedirs(tmp_dir, exist_ok=True) log_file_path = os.path.join(tmp_dir, \'log.txt\') with open(log_file_path, \'a\') as log_file: for message in log_messages: log_file.write(message + \'n\') # Handle environment variable LOG_LEVEL log_level = os.getenv(\'LOG_LEVEL\') if log_level == \'DEBUG\': log_file.write(\\"DEBUG: Debug log entryn\\") else: log_file.write(\\"LOG: General log entryn\\") # Fork a new process to read the log file pid = os.fork() if pid == 0: # Child process try: with open(log_file_path, \'r\') as log_file: print(log_file.read()) except OSError as e: print(f\\"Child process error: {e}\\") finally: os._exit(0) else: # Parent process os.wait() # Wait for the child process to finish except OSError as e: print(f\\"OSError occurred: {e}\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\")"},{"question":"# Functional Sorting Challenge Problem Statement Write a Python function that sorts a list of dictionaries based on a specified key using functional programming techniques. The function should be versatile enough to handle sorting both in ascending and descending orders and should use functional constructs like `lambda` functions, `map`, `filter`, and `reduce` (from the functools module). Function Signature ```python def functional_sort(data: List[Dict[str, Any]], sort_key: str, descending: bool = False) -> List[Dict[str, Any]]: pass ``` Input 1. `data` (List[Dict[str, Any]]): A list of dictionaries to be sorted. Each dictionary contains comparable values. 2. `sort_key` (str): The key based on which the sorting should be done. 3. `descending` (bool, optional): A boolean flag indicating whether the sorting should be in descending order. Defaults to `False` (i.e., ascending order). Output - Returns a list of dictionaries sorted based on the specified key in either ascending or descending order. Constraints - The list of dictionaries will have at least one element. - Each dictionary will contain the specified `sort_key`. - Values associated with `sort_key` in each dictionary will be comparable (i.e., implementing the `__lt__` method). Example ```python data = [ {\\"name\\": \\"Alice\\", \\"age\\": 28}, {\\"name\\": \\"Bob\\", \\"age\\": 22}, {\\"name\\": \\"Charlie\\", \\"age\\": 25} ] sort_key = \\"age\\" descending = False sorted_data = functional_sort(data, sort_key, descending) print(sorted_data) ``` **Expected output:** ```python [ {\\"name\\": \\"Bob\\", \\"age\\": 22}, {\\"name\\": \\"Charlie\\", \\"age\\": 25}, {\\"name\\": \\"Alice\\", \\"age\\": 28} ] ``` Approaching this problem requires the implementation of a custom sorting function utilizing functional programming constructs as stipulated. Performance Requirements - The function should be efficient and strive to use functional programming principles efficiently to maintain readability and performance. - Sorting should be performed in O(n log n) time complexity. --- You are encouraged to use list comprehensions, functional helpers like `sorted()`, and any other functional programming constructs to arrive at a clean and efficient solution.","solution":"from typing import List, Dict, Any from functools import cmp_to_key def functional_sort(data: List[Dict[str, Any]], sort_key: str, descending: bool = False) -> List[Dict[str, Any]]: comparator = (lambda x, y: (x[sort_key] > y[sort_key]) - (x[sort_key] < y[sort_key])) if not descending else (lambda x, y: (x[sort_key] < y[sort_key]) - (x[sort_key] > y[sort_key])) sorted_data = sorted(data, key=cmp_to_key(comparator)) return sorted_data"},{"question":"Named Tensor Operations in PyTorch Implement a function `named_tensor_broadcast_and_sum` that performs operations on two given named tensors, incorporating both broadcasting and reduction. This function should demonstrate your understanding of named tensor concepts, including propagation, unification, and dimensionality reduction. Function Signature ```python def named_tensor_broadcast_and_sum(tensor_a: torch.Tensor, tensor_b: torch.Tensor, dim_name: str) -> torch.Tensor: ``` Input - `tensor_a` (torch.Tensor): A named tensor with at least two dimensions. - `tensor_b` (torch.Tensor): Another named tensor, which will be broadcasted against `tensor_a`, with at least one matching named dimension. - `dim_name` (str): The name of the dimension along which to sum the resulting tensor. Output - Returns a new tensor (torch.Tensor) after broadcasting `tensor_b` to match `tensor_a` and summing along the specified dimension. Constraints - Assume that the named dimensions in both tensors are unique and matching dimension names exist for proper broadcasting. - You must use named tensor operations and ensure proper name propagation and reduction. Example ```python # Example tensors tensor_a = torch.randn(2, 3, names=(\'N\', \'C\')) tensor_b = torch.randn(3, names=(\'C\',)) # Perform the operation result = named_tensor_broadcast_and_sum(tensor_a, tensor_b, \'C\') # Expected output: The resulting tensor after broadcasting tensor_b to match tensor_a and summing along \'C\'. print(result.names) # Should print: (\'N\',) print(result.shape) # Should print: torch.Size([2]) ``` Notes - You may use the methods described in the provided documentation to handle named tensors, such as `align_to`, `sum`, and arithmetic operations. - Focus on the handling of named dimensions in tensor operations to ensure correctness and alignment. This question assesses your ability to work with named tensors and correctly perform operations that respect the naming conventions and propagation rules in PyTorch.","solution":"import torch def named_tensor_broadcast_and_sum(tensor_a: torch.Tensor, tensor_b: torch.Tensor, dim_name: str) -> torch.Tensor: Broadcast tensor_b to tensor_a and sum the resulting tensor along the specified dimension. Parameters: tensor_a (torch.Tensor): A named tensor with at least two dimensions. tensor_b (torch.Tensor): Another named tensor, which will be broadcasted against tensor_a, with at least one matching named dimension. dim_name (str): The name of the dimension along which to sum the resulting tensor. Returns: torch.Tensor: The resulting tensor after broadcasting and summing along the specified dimension. # Align tensor_b to tensor_a\'s names tensor_b_aligned = tensor_b.align_to(*tensor_a.names) # Perform the broadcasted addition result = tensor_a + tensor_b_aligned # Sum along the specified dimension result_sum = result.sum(dim=dim_name) return result_sum"},{"question":"Objective This question assesses your understanding of the Python logging module, specifically how to configure loggers, handlers, and formatters to achieve a specific logging setup. Problem Statement You are required to write a Python script that logs messages to both the console and a file named `application.log`. The log messages must have the following format: ``` [LOGLEVEL] <timestamp> <logger_name>: <message> ``` The logging should include different log levels (DEBUG, INFO, WARNING, ERROR, and CRITICAL) and strictly adhere to the provided format. Additionally, the logger should print the stack trace information if an exception is logged. Your task is to: 1. Implement a function `setup_logging()` that configures the logging system: * Create a logger named `app_logger`. * Set the log level to `DEBUG` for capturing all log messages. * Add two handlers to the logger: - A console handler that outputs log messages to the console. - A file handler that writes log messages to `application.log`. * Both handlers should use a formatter that matches the specified format. 2. Implement a function `log_messages()` that demonstrates logging messages at various levels (DEBUG, INFO, WARNING, ERROR, and CRITICAL) using the configured logger. 3. Demonstrate logging an exception with a stack trace. Expected Input/Output * The input will be a series of logging calls within the `log_messages()` function. * The output should be the logged messages in both console and `application.log` file with the specified format. Constraints * The solution must use the Python `logging` module. * The logged messages should follow the provided format exactly. * Ensure the `application.log` file is created in the same directory as the script. Example ```python # Calling log_messages() should result in the following being logged to both the console and application.log file: [DEBUG] 2023-10-11 08:45:30,123 app_logger: This is a debug message [INFO] 2023-10-11 08:45:30,124 app_logger: This is an info message [WARNING] 2023-10-11 08:45:30,125 app_logger: This is a warning message [ERROR] 2023-10-11 08:45:30,126 app_logger: This is an error message [CRITICAL] 2023-10-11 08:45:30,127 app_logger: This is a critical message # On an exception: [ERROR] 2023-10-11 08:45:30,128 app_logger: An exception occurred Traceback (most recent call last): File \\"example.py\\", line 35, in <module> raise ValueError(\\"An example exception\\") ValueError: An example exception ``` Implementation ```python import logging def setup_logging(): logger = logging.getLogger(\'app_logger\') logger.setLevel(logging.DEBUG) # Create console handler console_handler = logging.StreamHandler() console_handler.setLevel(logging.DEBUG) # Create file handler file_handler = logging.FileHandler(\'application.log\') file_handler.setLevel(logging.DEBUG) # Create a formatter formatter = logging.Formatter(\'[%(levelname)s] %(asctime)s %(name)s: %(message)s\', datefmt=\'%Y-%m-%d %H:%M:%S\') # Add formatter to handlers console_handler.setFormatter(formatter) file_handler.setFormatter(formatter) # Add handlers to logger logger.addHandler(console_handler) logger.addHandler(file_handler) def log_messages(): logger = logging.getLogger(\'app_logger\') # Log messages logger.debug(\'This is a debug message\') logger.info(\'This is an info message\') logger.warning(\'This is a warning message\') logger.error(\'This is an error message\') logger.critical(\'This is a critical message\') # Log an exception with stack trace try: raise ValueError(\'An example exception\') except Exception as e: logger.exception(\'An exception occurred\') if __name__ == \'__main__\': setup_logging() log_messages() ``` Produce a script based on this example, test it, and ensure its correctness according to the specification.","solution":"import logging def setup_logging(): logger = logging.getLogger(\'app_logger\') logger.setLevel(logging.DEBUG) # Create console handler console_handler = logging.StreamHandler() console_handler.setLevel(logging.DEBUG) # Create file handler file_handler = logging.FileHandler(\'application.log\') file_handler.setLevel(logging.DEBUG) # Create a formatter formatter = logging.Formatter(\'[%(levelname)s] %(asctime)s %(name)s: %(message)s\', datefmt=\'%Y-%m-%d %H:%M:%S\') # Add formatter to handlers console_handler.setFormatter(formatter) file_handler.setFormatter(formatter) # Add handlers to logger logger.addHandler(console_handler) logger.addHandler(file_handler) def log_messages(): logger = logging.getLogger(\'app_logger\') # Log messages logger.debug(\'This is a debug message\') logger.info(\'This is an info message\') logger.warning(\'This is a warning message\') logger.error(\'This is an error message\') logger.critical(\'This is a critical message\') # Log an exception with stack trace try: raise ValueError(\'An example exception\') except Exception as e: logger.exception(\'An exception occurred\') if __name__ == \'__main__\': setup_logging() log_messages()"},{"question":"**Question** Design a function `create_custom_plot` that utilizes the `seaborn.objects.Plot` class and the `matplotlib` library to create a multifaceted and customized plot. Your task is to plot data from the \'diamonds\' dataset with the following specifications: # Specifications 1. **Subplot Arrangement**: - Create a `matplotlib.figure.Figure` object with 2 subfigures arranged side by side. 2. **First Subfigure**: - Use `seaborn.objects.Plot` to plot the relationship between `carat` and `price` using a scatter plot (dots). 3. **Second Subfigure**: - Create a faceted bar plot using `seaborn.objects.Plot` where: - The x-axis represents the logarithm of the `price`. - The plots are faceted by the `cut` attribute. - Apply bar and histogram elements to the plot. 4. **Customization**: - Add a rectangle with annotation to the scatter plot: - The rectangle should be semi-transparent and cover the upper right corner of the plot. - The annotation text within the rectangle should say \\"Diamonds: very sparkly!\\" and be centered. # Implementation ```python import seaborn as sns import seaborn.objects as so import matplotlib as mpl import matplotlib.pyplot as plt def create_custom_plot(): diamonds = sns.load_dataset(\\"diamonds\\") # Create the main figure with subfigures f = mpl.figure.Figure(figsize=(10, 6), dpi=100, layout=\\"constrained\\") sf1, sf2 = f.subfigures(1, 2) # First Subfigure: Scatter plot p1 = so.Plot(diamonds, \\"carat\\", \\"price\\").add(so.Dots()) res1 = p1.on(sf1).plot() ax1 = sf1.axes[0] rect = mpl.patches.Rectangle( xy=(0.6, 0.8), width=.3, height=.15, color=\\"C1\\", alpha=.2, transform=ax1.transAxes, clip_on=False, ) ax1.add_artist(rect) ax1.text( x=rect.get_width() / 2 + 0.6, y=0.8 + rect.get_height() / 2, s=\\"Diamonds: very sparkly!\\", size=12, ha=\\"center\\", va=\\"center\\", transform=ax1.transAxes, ) # Second Subfigure: Faceted bar and histogram plot ( so.Plot(diamonds, x=\\"price\\") .add(so.Bars(), so.Hist()) .facet(row=\\"cut\\") .scale(x=\\"log\\") .share(y=False) .on(sf2) ) # Display the figure plt.show() # Call the function to create and show the plot create_custom_plot() ``` # Constraints - Use only the `seaborn` and `matplotlib` libraries. - Follow the specified design and customization requirements strictly. # Expected Output The function `create_custom_plot()` should generate and display a figure with two subfigures: a customized scatter plot and a faceted bar plot with the specifications mentioned.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib as mpl import matplotlib.pyplot as plt def create_custom_plot(): diamonds = sns.load_dataset(\\"diamonds\\") # Create the main figure with subfigures f = mpl.figure.Figure(figsize=(10, 6), dpi=100, layout=\\"constrained\\") sf1, sf2 = f.subfigures(1, 2) # First Subfigure: Scatter plot p1 = so.Plot(diamonds, \\"carat\\", \\"price\\").add(so.Dots()) res1 = p1.on(sf1).plot() ax1 = sf1.axes[0] rect = mpl.patches.Rectangle( xy=(0.6, 0.8), width=.3, height=.15, color=\\"C1\\", alpha=.2, transform=ax1.transAxes, clip_on=False, ) ax1.add_artist(rect) ax1.text( x=rect.get_width() / 2 + 0.6, y=0.8 + rect.get_height() / 2, s=\\"Diamonds: very sparkly!\\", size=12, ha=\\"center\\", va=\\"center\\", transform=ax1.transAxes, ) # Second Subfigure: Faceted bar and histogram plot ( so.Plot(diamonds, x=\\"price\\") .add(so.Bars(), so.Hist()) .facet(row=\\"cut\\") .scale(x=\\"log\\") .share(y=False) .on(sf2) ) # Display the figure plt.show() # Call the function to create and show the plot create_custom_plot()"},{"question":"Partial Least Squares Regression Implementation You are given datasets `X_train` and `Y_train` containing training data features and target values, respectively. Your task is to implement a function that performs Partial Least Squares Regression (PLSRegression) using the scikit-learn library, trains the model on the provided datasets, transforms the data, and predicts target values for a given `X_test` dataset. Function Signature: ```python def pls_regression(X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray, n_components: int) -> np.ndarray: Perform Partial Least Squares Regression on the given training data and predict targets for the test data. Parameters: X_train (np.ndarray): Training data features with shape (n_samples, n_features). Y_train (np.ndarray): Training data target values with shape (n_samples, n_targets). X_test (np.ndarray): Test data features with shape (m_samples, n_features). n_components (int): Number of components to keep. Returns: np.ndarray: Predicted target values for the test data with shape (m_samples, n_targets). ``` Constraints: 1. The input matrices will be numpy arrays. 2. Number of samples `n_samples` in `X_train` and `Y_train` will be the same. 3. `X_test` will have the same number of features as `X_train`. Steps: 1. Implement PLSRegression directly using the `PLSRegression` class from scikit-learn. 2. Fit the model with `X_train` and `Y_train`. 3. Transform `X_test` using the fitted model. 4. Predict and return the target values for `X_test`. Example: ```python import numpy as np from sklearn.cross_decomposition import PLSRegression def pls_regression(X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray, n_components: int) -> np.ndarray: pls = PLSRegression(n_components=n_components) pls.fit(X_train, Y_train) Y_pred = pls.predict(X_test) return Y_pred # Example usage: X_train = np.array([[0.1, 0.2], [0.2, 0.4], [0.3, 0.6]]) Y_train = np.array([[0.5], [1.0], [1.5]]) X_test = np.array([[0.4, 0.8]]) n_components = 1 Y_pred = pls_regression(X_train, Y_train, X_test, n_components) print(Y_pred) # Output: An array of predicted target values for X_test ``` # Notes: 1. Focus on implementing the function using the `PLSRegression` class and its methods appropriately. 2. Pay attention to the use of attributes like `coef_` and methods for fitting, transforming, and predicting.","solution":"import numpy as np from sklearn.cross_decomposition import PLSRegression def pls_regression(X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray, n_components: int) -> np.ndarray: Perform Partial Least Squares Regression on the given training data and predict targets for the test data. Parameters: X_train (np.ndarray): Training data features with shape (n_samples, n_features). Y_train (np.ndarray): Training data target values with shape (n_samples, n_targets). X_test (np.ndarray): Test data features with shape (m_samples, n_features). n_components (int): Number of components to keep. Returns: np.ndarray: Predicted target values for the test data with shape (m_samples, n_targets). pls = PLSRegression(n_components=n_components) pls.fit(X_train, Y_train) Y_pred = pls.predict(X_test) return Y_pred"},{"question":"# XML Data Analysis and Manipulation You are provided with an XML document containing information about various books. Each book has properties such as title, author, year, price, and genre. Your task is to write a Python function that parses this XML data, performs specific data manipulations, and then writes the result back to an XML file. The XML structure is as follows: ```xml <?xml version=\\"1.0\\"?> <library> <book id=\\"1\\"> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <year>1925</year> <price>10.99</price> <genre>Fiction</genre> </book> <book id=\\"2\\"> <title>To Kill a Mockingbird</title> <author>Harper Lee</author> <year>1960</year> <price>7.99</price> <genre>Fiction</genre> </book> <book id=\\"3\\"> <title>1984</title> <author>George Orwell</author> <year>1949</year> <price>8.99</price> <genre>Dystopian</genre> </book> </library> ``` Tasks: 1. **Parse the XML Data**: - Read the XML data from a file named `books.xml`. 2. **Increase the Price**: - Increase the price of all books by 20%. 3. **Remove Old Books**: - Remove all books that were published before the year 1950. 4. **Add a New Book**: - Add a new book to the library with the following details: - Title: `Brave New World` - Author: `Aldous Huxley` - Year: `1932` - Price: `9.99` - Genre: `Dystopian` 5. **Write Back the Modified XML**: - Write the modified XML data to a new file named `updated_books.xml`. Function Signature: ```python def manipulate_books_xml(): pass ``` Constraints: - Use the `xml.etree.ElementTree` module for parsing and writing XML data. - Ensure that the output XML file is well-formed and retains the updated information correctly. Example: After running your function, the `updated_books.xml` might look like this: ```xml <?xml version=\\"1.0\\"?> <library> <book id=\\"2\\"> <title>To Kill a Mockingbird</title> <author>Harper Lee</author> <year>1960</year> <price>9.59</price> <genre>Fiction</genre> </book> <book id=\\"3\\"> <title>1984</title> <author>George Orwell</author> <year>1949</year> <price>10.79</price> <genre>Dystopian</genre> </book> <book id=\\"4\\"> <title>Brave New World</title> <author>Aldous Huxley</author> <year>1932</year> <price>9.99</price> <genre>Dystopian</genre> </book> </library> ``` Make sure your function performs these operations efficiently and correctly. Good luck!","solution":"import xml.etree.ElementTree as ET def manipulate_books_xml(): # Parse the XML file tree = ET.parse(\'books.xml\') root = tree.getroot() # Increase the price of all books by 20% for book in root.findall(\'book\'): price = float(book.find(\'price\').text) new_price = round(price * 1.20, 2) book.find(\'price\').text = f\\"{new_price:.2f}\\" # Remove books published before 1950 for book in root.findall(\'book\'): year = int(book.find(\'year\').text) if year < 1950: root.remove(book) # Add a new book to the library new_book = ET.Element(\'book\', id=\\"4\\") ET.SubElement(new_book, \'title\').text = \'Brave New World\' ET.SubElement(new_book, \'author\').text = \'Aldous Huxley\' ET.SubElement(new_book, \'year\').text = \'1932\' ET.SubElement(new_book, \'price\').text = \'9.99\' ET.SubElement(new_book, \'genre\').text = \'Dystopian\' root.append(new_book) # Write the modified XML data to a new file tree.write(\'updated_books.xml\', xml_declaration=True, encoding=\'utf-8\')"},{"question":"# Advanced Python Exception Handling Objective: Implement a class and a few methods to demonstrate a comprehensive understanding of Python exception handling. Your task involves creating a custom exception class, handling different built-in exceptions, and ensuring proper resource management through context managers. Problem Statement: 1. Design a custom exception `InvalidInputError` that inherits from Python\'s `Exception` class. 2. Implement a class `DataProcessor` that has the following methods: - `read_data(file_path)`: Reads data from a specified file. If the specified file does not exist, raises a `FileNotFoundError`. If the file is not readable (permissions issues), raises a `PermissionError`. - `process_data(data)`: Processes the provided data. If the data is not a list of numbers, raises `InvalidInputError` with an appropriate message. - `save_data(file_path, data)`: Saves the processed data to a specified file. If the specified path is invalid, raises a `FileNotFoundError`. If the file is not writable, raises a `PermissionError`. 3. Implement a context manager class `ExceptionLogger` that logs exceptions to a file named `exception.log` in the current directory. The context manager should: - On entering, open the log file in append mode. - On exiting, if an exception has occurred, write the exception type and message to the log file. - Ensure that the file is properly closed after logging the exception. Input and Output: - **`read_data(file_path)`**: - **Input**: `file_path` (string) - The path to the input file. - **Output**: None (raises exceptions as specified). - **`process_data(data)`**: - **Input**: `data` (list) - A list to be processed. - **Output**: None (raises exceptions as specified). - **`save_data(file_path, data)`**: - **Input**: - `file_path` (string) - The path to the output file. - `data` (list) - The processed data to be saved. - **Output**: None (raises exceptions as specified). # Constraints: - Assume the data being processed are simple lists of integers. - Handle exceptions gracefully and ensure resources are managed properly. - The `exception.log` file must be created in the current working directory and should contain all logged exceptions. Example: ```python # Custom Exception class InvalidInputError(Exception): pass # DataProcessor Class class DataProcessor: # Methods to be implemented: def read_data(self, file_path): pass def process_data(self, data): pass def save_data(self, file_path, data): pass # Context Manager for Exception Logging class ExceptionLogger: def __enter__(self): pass def __exit__(self, exc_type, exc_val, exc_tb): pass # Example Usage: try: with ExceptionLogger(): dp = DataProcessor() dp.read_data(\\"input.txt\\") data = [1, 2, 3, 4] dp.process_data(data) dp.save_data(\\"output.txt\\", data) except InvalidInputError as e: print(f\\"Invalid input: {e}\\") except (FileNotFoundError, PermissionError) as e: print(f\\"File error: {e}\\") ``` # Note: Ensure that your implementation is well-structured and follows good coding practices.","solution":"import os # Custom Exception class InvalidInputError(Exception): pass # DataProcessor Class class DataProcessor: def read_data(self, file_path): Reads data from a specified file. Raises appropriate exceptions if the file does not exist or is not readable. if not os.path.exists(file_path): raise FileNotFoundError(f\\"The file at {file_path} does not exist.\\") if not os.access(file_path, os.R_OK): raise PermissionError(f\\"The file at {file_path} is not readable.\\") with open(file_path, \'r\') as file: data = file.read() return data def process_data(self, data): Processes the provided data. Throws InvalidInputError if the input is not a list of numbers. if not isinstance(data, list) or not all(isinstance(i, int) for i in data): raise InvalidInputError(\\"Input data must be a list of numbers.\\") # Processing: For this example, we\'ll simply return the sorted list. processed_data = sorted(data) return processed_data def save_data(self, file_path, data): Saves the processed data to a specified file. Raises appropriate exceptions if the file path is invalid or not writable. directory = os.path.dirname(file_path) if directory and not os.path.exists(directory): raise FileNotFoundError(f\\"The directory at {directory} does not exist.\\") if os.path.exists(file_path) and not os.access(file_path, os.W_OK): raise PermissionError(f\\"The file at {file_path} is not writable.\\") with open(file_path, \'w\') as file: for item in data: file.write(f\\"{item}n\\") # Context Manager for Exception Logging class ExceptionLogger: def __enter__(self): self.file = open(\'exception.log\', \'a\') return self def __exit__(self, exc_type, exc_val, exc_tb): if exc_type: self.file.write(f\\"Exception: {exc_type.__name__} - {exc_val}n\\") self.file.close()"},{"question":"**Question: Debugging Tool Implementation** In this task, you will implement a Python function using the Python C API functions outlined in the provided documentation. This function will act as a debugging tool to retrieve and display the current state of execution, including calling function information and variable scopes. **Task:** Implement a function `debug_info()` that prints the following: 1. The name and description of the currently executing function. 2. The line number currently being executed. 3. All available local, global, and built-in variables at this point of execution. **Input:** The function takes no input parameters. **Output:** The function should print the required information in the following format: ``` Function Name: <name> Function Description: <description> Current Line Number: <line_number> Locals: <locals_dict> Globals: <globals_dict> Builtins: <builtins_dict> ``` **Constraints:** - Your function should correctly handle the scenario when no execution frame is available. - Use the provided functions from the documentation to access the required information. **Example:** Suppose the currently executing function is `sample_function` at line 10, the expected output would be similar to: ``` Function Name: sample_function Function Description: () Current Line Number: 10 Locals: {\'a\': 1, \'b\': 2} Globals: {\'foo\': \'bar\'} Builtins: {...} # standard built-in functions and variables ``` _Hint: You will need to integrate this tool within a Python environment to access the Python C API functionalities._","solution":"import inspect import builtins def debug_info(): Prints debug information of the currently executing function, including its name, description, current line number, and all available local, global, and built-in variables. frame = inspect.currentframe().f_back if frame is not None: func_name = frame.f_code.co_name func_description = frame.f_code.co_consts current_line_number = frame.f_lineno locals_dict = frame.f_locals globals_dict = frame.f_globals builtins_dict = {name: getattr(builtins, name) for name in dir(builtins)} print(f\\"Function Name: {func_name}\\") print(f\\"Function Description: {func_description}\\") print(f\\"Current Line Number: {current_line_number}\\") print(f\\"Locals: {locals_dict}\\") print(f\\"Globals: {globals_dict}\\") print(f\\"Builtins: {builtins_dict}\\") else: print(\\"No frame is currently executing.\\") def sample_function(a, b): x = a + b debug_info() return x"},{"question":"# Audio Signal Processing Task **Objective:** Implement a function that performs a series of manipulations on an audio fragment to enhance its quality and then convert it into a different encoding format. **Task:** You are required to write a Python function `process_audio_signal(input_fragment: bytes, width: int, l_factor: float, r_factor: float) -> bytes` that performs the following steps on the given audio fragment: 1. **Average Computation:** Compute the average value of the input audio fragment using `audioop.avg()`. 2. **Normalization:** Normalize the audio fragment by adjusting its volume to a specific factor using `audioop.mul()`. 3. **Stereo Conversion:** Convert the normalized mono audio fragment into stereo using `audioop.tostereo()`. 4. **Encoding Conversion:** Convert the stereo audio from linear encoding to u-LAW encoding using `audioop.lin2ulaw()`. Here is a detailed breakdown of the steps: - **Step 1:** Use `audioop.avg()` to obtain the average value of all samples in the `input_fragment`. - **Step 2:** Normalize the audio fragment by multiplying it with a factor of 2 using `audioop.mul()`. - **Step 3:** Convert the normalized fragment to stereo format by using `audioop.tostereo()`, with the left channel factor as `l_factor` and the right channel factor as `r_factor`. - **Step 4:** Convert the stereo audio fragment to u-LAW encoding with a sample width of 2 bytes using `audioop.lin2ulaw()`. **Constraints:** - The `input_fragment` is a bytes-like object containing signed integer samples. - `width` is the sample width in bytes and can be 1, 2, 3, or 4. - `l_factor` and `r_factor` are floating-point multipliers for the left and right channels respectively. **Example:** ```python import audioop def process_audio_signal(input_fragment: bytes, width: int, l_factor: float, r_factor: float) -> bytes: # Step 1: Compute the average value average_value = audioop.avg(input_fragment, width) # Step 2: Normalize the audio to twice its current volume normalized_fragment = audioop.mul(input_fragment, width, 2) # Step 3: Convert the normalized fragment to stereo stereo_fragment = audioop.tostereo(normalized_fragment, width, l_factor, r_factor) # Step 4: Convert the stereo fragment to u-LAW encoding ulaw_encoded_fragment = audioop.lin2ulaw(stereo_fragment, width) return ulaw_encoded_fragment # Example usage input_data = b\'some_input_data_bytes\' output_data = process_audio_signal(input_data, 2, 1.0, 0.5) print(output_data) ``` *Note:* The input data `b\'some_input_data_bytes\'` is a placeholder, and you should replace it with actual bytes of audio data for testing. # Evaluation Criteria: - **Correctness:** Your function should correctly perform all the specified steps. - **Efficiency:** Your solution should be efficient and handle various lengths and widths of audio fragments. - **Code Quality:** Your code should be well-structured and commented appropriately to explain each step.","solution":"import audioop def process_audio_signal(input_fragment: bytes, width: int, l_factor: float, r_factor: float) -> bytes: Process the audio fragment to enhance its quality and convert it into u-LAW encoding. Parameters: - input_fragment: bytes, audio fragment to be processed - width: int, sample width in bytes (1, 2, 3, or 4) - l_factor: float, multiplier for the left channel in stereo conversion - r_factor: float, multiplier for the right channel in stereo conversion Returns: - bytes, processed audio fragment in u-LAW encoding # Step 1: Compute the average value of the input audio fragment average_value = audioop.avg(input_fragment, width) # Step 2: Normalize the audio to twice its current volume normalized_fragment = audioop.mul(input_fragment, width, 2) # Step 3: Convert the normalized fragment to stereo stereo_fragment = audioop.tostereo(normalized_fragment, width, l_factor, r_factor) # Step 4: Convert the stereo fragment to u-LAW encoding ulaw_encoded_fragment = audioop.lin2ulaw(stereo_fragment, width) return ulaw_encoded_fragment"},{"question":"**Title: Efficient function memoization using `functools.lru_cache` & custom `partial` functions** Objective: Implement a function that utilizes `functools.lru_cache` to efficiently compute the value of a mathematical function while caching its results. In addition, create a partial function using `functools.partial` that modifies the behavior of the first function. Problem Statement: 1. Define a function `compute_polynomial` that takes four arguments: `a`, `b`, `c` and `x`, and returns the value of a polynomial `ax^2 + bx + c`. - Ensure this function uses the `functools.lru_cache` decorator to store and reuse recently computed results for `x`. 2. Define another function `create_partial_polynomial` that takes three arguments: `a`, `b`, and `c`. This function should return a new function that is a partial application of `compute_polynomial` with fixed `a`, `b`, and `c` values. - The returned partial function will then only accept a single argument `x`. Requirements: - Use `functools.lru_cache` for caching in `compute_polynomial` with a maximum cache size of 100. - Utilize `functools.partial` to create the partial function in `create_partial_polynomial`. Input and Output: - `compute_polynomial(a: int, b: int, c: int, x: int) -> int` - Inputs: Four integers `a`, `b`, `c`, and `x`. - Output: Integer value representing the result of the polynomial. - `create_partial_polynomial(a: int, b: int, c: int) -> Callable[[int], int]` - Inputs: Three integers `a`, `b`, and `c`. - Output: A function that takes a single integer `x` and returns the result of the polynomial. Example: ```python # Define a cached polynomial function @lru_cache(maxsize=100) def compute_polynomial(a: int, b: int, c: int, x: int) -> int: return a * x * x + b * x + c # Create a partial polynomial function with a=1, b=2, c=3 partial_polynomial = create_partial_polynomial(1, 2, 3) # Use the partial function to compute values print(partial_polynomial(5)) # Output: 38 print(partial_polynomial(3)) # Output: 18 ``` Constraints: - For `compute_polynomial`, the inputs typically are within the range of `-1000` to `1000`. - Ensure that the cache size limit is respected and handle potential cache misses gracefully. Notes: - Pay attention to the efficiency of your solution. - Ensure that function signatures and returned types match the provided examples. Evaluation: - Correctness: The function must return correct results as per specifications. - Efficiency: The use of caching should improve performance for repeated calls. - Code Quality: Clarity, commenting, and adherence to Pythonic practices will be evaluated.","solution":"from functools import lru_cache, partial @lru_cache(maxsize=100) def compute_polynomial(a: int, b: int, c: int, x: int) -> int: return a * x * x + b * x + c def create_partial_polynomial(a: int, b: int, c: int): return partial(compute_polynomial, a, b, c)"},{"question":"**Question: Secure Data Handling in Python** **Background:** In various applications, data security is a paramount concern. As demonstrated by the provided documentation, different Python modules come with specific security considerations. Your task is to implement a function that securely handles data processing by considering these security guidelines. **Task:** Write a Python function `secure_data_processor(data: bytes, action: str) -> bytes` that securely handles the input data based on the specified action. The function should support the following actions: - `\\"encode_base64\\"`: Encode the data using Base64 encoding. - `\\"hash_sha256\\"`: Compute the SHA-256 hash of the data. - `\\"compress_zip\\"`: Compress the data using ZIP compression. - `\\"decompress_zip\\"`: Decompress ZIP compressed data. - `\\"secure_random\\"`: Generate a secure random string of the same length as the input data. **Requirements:** 1. Use the `base64` module to encode data securely following RFC 4648 guidelines. 2. Hash the data with the `hashlib` module, ensuring the used algorithm is secure. 3. Employ the `zipfile` module to compress and decompress data, ensuring protection against disk volume exhaustion. 4. Use the `secrets` module to generate cryptographically secure random data. **Constraints:** - The input `data` will be a non-empty byte string with a length of at most 10^6 bytes. - The `action` will be one of the supported strings: `\\"encode_base64\\"`, `\\"hash_sha256\\"`, `\\"compress_zip\\"`, `\\"decompress_zip\\"`, or `\\"secure_random\\"`. **Expected Input and Output:** - **Input:** `data` as `bytes`, `action` as `str` - **Output:** Processed data as `bytes` **Examples:** ```python # Example 1 data = b\\"example\\" action = \\"encode_base64\\" result = secure_data_processor(data, action) # expected output: b\\"ZXhhbXBsZQ==\\" # Example 2 data = b\\"example\\" action = \\"hash_sha256\\" result = secure_data_processor(data, action) # expected output: b\\"x13x95xa2Ixfbxd6rxb5x10xb7x12x7fxe2xafx06Zxfbx06xbb,/x1fxf7xd3x17tax1dxb8}mxf8\\" # Example 3 data = b\\"example\\" action = \\"secure_random\\" result = secure_data_processor(data, action) # example output: b\\"x8bxcdx8fnxb1xd8xf4\\" (Note: actual output will be a random byte string of length 7) # Example 4 data = b\\"example\\" * 1000 action = \\"compress_zip\\" result = secure_data_processor(data, action) # expected output: compressed byte data # Example 5 data = some_compressed_data action = \\"decompress_zip\\" result = secure_data_processor(data, action) # expected output: decompressed byte data matching the original before compression. ``` Please implement the `secure_data_processor` function with the above requirements and demonstrate its usage with the provided examples.","solution":"import base64 import hashlib import zipfile from io import BytesIO import secrets def secure_data_processor(data: bytes, action: str) -> bytes: if action == \\"encode_base64\\": return base64.b64encode(data) elif action == \\"hash_sha256\\": sha256 = hashlib.sha256() sha256.update(data) return sha256.digest() elif action == \\"compress_zip\\": buffer = BytesIO() with zipfile.ZipFile(buffer, \'w\', zipfile.ZIP_DEFLATED) as zip_file: zip_file.writestr(\'data\', data) return buffer.getvalue() elif action == \\"decompress_zip\\": buffer = BytesIO(data) with zipfile.ZipFile(buffer, \'r\') as zip_file: decompressed_data = zip_file.read(\'data\') return decompressed_data elif action == \\"secure_random\\": return secrets.token_bytes(len(data)) else: raise ValueError(\\"Unsupported action\\") # Examples data = b\\"example\\" print(secure_data_processor(data, \\"encode_base64\\")) # b\\"ZXhhbXBsZQ==\\" print(secure_data_processor(data, \\"hash_sha256\\")) # b\\"<hash output>\\" print(secure_data_processor(data, \\"secure_random\\")) # b\\"<random bytes>\\" data = b\\"example\\" * 1000 compressed_data = secure_data_processor(data, \\"compress_zip\\") decompressed_data = secure_data_processor(compressed_data, \\"decompress_zip\\") print(decompressed_data == data) # True"},{"question":"Coding Assessment Question: Synchronization of Generator Operations # Objective Demonstrate your understanding of Python 3.8+ generator expressions, custom functions, synchronization, and complex expression handling in Python. # Problem Statement You are tasked with creating a system that processes a series of data points in a stream-like manner, processing chunks of the data and generating new data points based on specific conditions. The system will utilize generator functions and ensure the generators run in a synchronized manner to avoid runtime errors. # Requirements 1. Implement a generator function `data_stream` that yields squared values of the input list, but only for even numbers. 2. Implement another generator function `filtered_stream` that uses the `data_stream` generator but only yields values that are greater than a specified threshold. 3. Write an overall controller function `stream_processor` that accepts an input list, a threshold value, and an integer `N`. It should: - Utilize `filtered_stream` to process data in chunks of size `N`. - For each chunk, print the chunk of values processed. - Ensure that processing halts correctly if there are insufficient values left to form a complete chunk. # Example Given the input list `[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]`, a threshold of `10`, and `N` of `3`, the output should be: ``` Processing Chunk: [16, 36, 64, 100] ``` # Constraints - The input list will only contain positive integers. - The threshold will be a positive integer. - `N` will always be a positive integer less than or equal to the length of the input list. # Implementation 1. **data_stream Function:** ```python def data_stream(data): for value in data: if value % 2 == 0: yield value ** 2 ``` 2. **filtered_stream Function:** ```python def filtered_stream(data, threshold): stream = data_stream(data) for value in stream: if value > threshold: yield value ``` 3. **stream_processor Function:** ```python def stream_processor(data, threshold, N): result = [] stream = filtered_stream(data, threshold) while True: chunk = [value for _, value in zip(range(N), stream)] if not chunk: break result.append(chunk) for chunk in result: print(f\'Processing Chunk: {chunk}\') ``` # Testing Use the given example to test your implementation: ```python data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] threshold = 10 N = 3 stream_processor(data, threshold, N) ``` # Submission Guidelines Submit the functions: `data_stream`, `filtered_stream`, and `stream_processor`. Ensure they pass the provided example to verify correctness and synchronization of generator operations.","solution":"def data_stream(data): Generator that yields squared values of even numbers from the input list. for value in data: if value % 2 == 0: yield value ** 2 def filtered_stream(data, threshold): Generator that filters values from data_stream based on a threshold. Yields values greater than the specified threshold. stream = data_stream(data) for value in stream: if value > threshold: yield value def stream_processor(data, threshold, N): Processes the input data in chunks of size N using the filtered_stream generator and prints each processed chunk. Processing stops when there are insufficient values to form a complete chunk. result = [] stream = filtered_stream(data, threshold) while True: chunk = [value for _, value in zip(range(N), stream)] if not chunk: break result.append(chunk) for chunk in result: print(f\'Processing Chunk: {chunk}\')"},{"question":"Coding Assessment Question # Objective: Demonstrate your understanding of the `configparser` module in Python by building a configuration file handler that can read, write, and modify configuration settings. # Task: Implement a Python class `ConfigManager` that uses the `configparser` module to manage configuration files in the INI format. The class should support the following functionalities: 1. **Initialization**: The class should be initialized with the path to the INI file. 2. **Read Settings**: Implement a method `read_setting(section, option)` that retrieves the value of a specific setting. 3. **Write Settings**: Implement a method `write_setting(section, option, value)` that writes or updates a setting in the configuration file. 4. **Remove Settings**: Implement a method `remove_setting(section, option)` that removes a specific setting from the configuration file. 5. **Save Configuration**: Implement a method `save_config(destination=None)` that saves the current state of the configuration to the original file or to a new file if a destination path is provided. # Input/Output Formats: - **Initialization**: - Input: `ConfigManager(config_file_path)` - Where `config_file_path` is a string representing the path to the configuration file. - **read_setting**: - Input: `read_setting(\'Section\', \'Option\')` - Output: The value associated with `Option` in `Section`. If either the section or the option is not found, return `None`. - **write_setting**: - Input: `write_setting(\'Section\', \'Option\', \'Value\')` - Where `Section` is the section name, `Option` is the option name, and `Value` is the value to be set. - **remove_setting**: - Input: `remove_setting(\'Section\', \'Option\')` - Remove the specified option from the given section. If the section or option doesn\'t exist, do nothing. - **save_config**: - Input: `save_config()` or `save_config(\'new_path.ini\')` - Save the updated configuration to the original file or to a new file if a path is provided. # Constraints: - Assume that all inputs are well-formed strings. - You cannot use any external libraries other than the standard library. - The class should handle exceptions and edge cases gracefully (e.g., reading from a nonexistent file, attempting to save to a write-protected location). # Example: ```python from configparser import ConfigParser class ConfigManager: def __init__(self, config_file_path): self.config = ConfigParser() self.config_file_path = config_file_path self.config.read(config_file_path) def read_setting(self, section, option): if self.config.has_section(section) and self.config.has_option(section, option): return self.config.get(section, option) return None def write_setting(self, section, option, value): if not self.config.has_section(section): self.config.add_section(section) self.config.set(section, option, value) def remove_setting(self, section, option): if self.config.has_section(section) and self.config.has_option(section, option): self.config.remove_option(section, option) def save_config(self, destination=None): with open(destination if destination else self.config_file_path, \'w\') as configfile: self.config.write(configfile) # Example usage: config_manager = ConfigManager(\'example.ini\') print(config_manager.read_setting(\'Settings\', \'debug\')) # e.g., output: True config_manager.write_setting(\'Settings\', \'debug\', \'False\') config_manager.remove_setting(\'Settings\', \'deprecated_option\') config_manager.save_config(\'updated_example.ini\') ``` Implement the `ConfigManager` class with the described functionality.","solution":"from configparser import ConfigParser import os class ConfigManager: def __init__(self, config_file_path): Initialize the ConfigManager with the given config file path. Reads the configuration from the file if it exists. self.config = ConfigParser() self.config_file_path = config_file_path if os.path.exists(config_file_path): self.config.read(config_file_path) def read_setting(self, section, option): Retrieve the value of a specific setting. Returns None if the section or option doesn\'t exist. if self.config.has_section(section) and self.config.has_option(section, option): return self.config.get(section, option) return None def write_setting(self, section, option, value): Write or update a setting in the configuration file. if not self.config.has_section(section): self.config.add_section(section) self.config.set(section, option, value) def remove_setting(self, section, option): Remove a specific setting from the configuration file. If the section or option doesn\'t exist, do nothing. if self.config.has_section(section) and self.config.has_option(section, option): self.config.remove_option(section, option) def save_config(self, destination=None): Save the current state of the configuration to the original file or to a new file if a destination path is provided. with open(destination if destination else self.config_file_path, \'w\') as configfile: self.config.write(configfile)"},{"question":"# Coding Challenge: Plist Manipulator You are tasked with creating a utility that reads, modifies, and writes `.plist` files using the `plistlib` module. You will implement the function `update_plist` which takes the following parameters: 1. `input_plist_data` (bytes): The input plist data in byte format. 2. `modifications` (dict): A dictionary representing key-value pairs that need to be updated or added to the plist. 3. `output_format` (str): The format for the output plist data, either \\"xml\\" or \\"binary\\". Your function should perform the following: 1. Read the input plist data. 2. Apply the modifications specified. 3. Return the modified plist data in the specified format. Function Signature ```python def update_plist(input_plist_data: bytes, modifications: dict, output_format: str) -> bytes: pass ``` Input - `input_plist_data`: A `bytes` object containing data of the plist in its original format. - `modifications`: A dictionary where keys are strings (matching plist keys) and values are the new values to set. - `output_format`: A string specifying the output format, must be either \\"xml\\" or \\"binary\\". Output - Returns a `bytes` object containing the modified plist data in the specified format. Example Usage ```python import datetime original_plist = b<plist version=\\"1.0\\"> <dict> <key>foo</key> <string>bar</string> <key>date</key> <date>2023-01-01T00:00:00Z</date> </dict> </plist> modifications = { \\"foo\\": \\"baz\\", \\"date\\": datetime.datetime(2023, 12, 25, 0, 0, 0) } # The output should be in XML format updated_plist = update_plist(original_plist, modifications, \\"xml\\") print(updated_plist.decode()) # The output should be in binary format updated_plist_bin = update_plist(original_plist, modifications, \\"binary\\") print(updated_plist_bin) ``` Constraints - The `input_plist_data` will always be a valid plist. - The `modifications` dictionary can contain nested dictionaries and lists. - The `output_format` must be either \\"xml\\" or \\"binary\\". If an invalid value is provided, raise a `ValueError`. Focus on handling different plist value types correctly (strings, integers, floats, booleans, dates, etc.) and ensure the output is correctly formatted.","solution":"import plistlib def update_plist(input_plist_data: bytes, modifications: dict, output_format: str) -> bytes: Reads, modifies, and writes .plist files according to the provided parameters. Args: - input_plist_data (bytes): The input plist data in byte format. - modifications (dict): A dictionary representing key-value pairs to be updated or added. - output_format (str): The format for the output plist data, either \\"xml\\" or \\"binary\\". Returns: - bytes: The modified plist data in the specified format. Raises: - ValueError: If the provided output_format is not \\"xml\\" or \\"binary\\". if output_format not in [\\"xml\\", \\"binary\\"]: raise ValueError(\\"Output format must be either \'xml\' or \'binary\'\\") # Read the input plist data plist_data = plistlib.loads(input_plist_data) # Apply the modifications for key, value in modifications.items(): plist_data[key] = value # Determine the output format fmt = plistlib.FMT_XML if output_format == \\"xml\\" else plistlib.FMT_BINARY # Write the modified plist data output_plist_data = plistlib.dumps(plist_data, fmt=fmt) return output_plist_data"},{"question":"You are required to write a Python function that utilizes the `zlib` module for compressing and decompressing data. Additionally, the function should verify the integrity of the data by using checksums. Function Signature ```python def compress_decompress_verify(data: bytes, compression_level: int = -1) -> bool: pass ``` Input - `data`: A bytes object containing the data to be compressed. - `compression_level`: An integer from `0` to `9` or `-1` that controls the compression level. Default is `-1` (zlib.Z_DEFAULT_COMPRESSION). Output - The function should return `True` if the decompressed data matches the original data (indicating successful compression/decompression), and `False` otherwise. Requirements - Use the `zlib.compress()` function to compress the data. - Use `zlib.decompress()` function to decompress the data. - Compute and compare checksums using `zlib.adler32()` before and after compression/decompression to verify data integrity. Constraints - The length of `data` will be between `1` and `1,000,000` bytes. Example ```python data = b\\"Sample data for compression and decompression\\" result = compress_decompress_verify(data, compression_level=6) print(result) # Output: True ``` Notes - Ensure that any exceptions are properly handled to prevent the function from crashing. - Consider edge cases like empty input data or very small data sizes.","solution":"import zlib def compress_decompress_verify(data: bytes, compression_level: int = -1) -> bool: try: # Calculate original checksum original_checksum = zlib.adler32(data) # Compress the data compressed_data = zlib.compress(data, level=compression_level) # Decompress the data decompressed_data = zlib.decompress(compressed_data) # Calculate decompressed checksum decompressed_checksum = zlib.adler32(decompressed_data) # Verify if original and decompressed checksums are the same return original_checksum == decompressed_checksum except Exception as e: # If any exception occurs, the verification fails return False"},{"question":"# Custom Autograd Function Coding Problem In this coding assessment, you are required to design and implement a custom autograd function in PyTorch to compute a specific mathematical operation. The problem involves both writing the custom function and testing it to verify that it computes gradients correctly. # Problem Statement Implement a custom autograd function `PolynomialFunction` that computes the following operation: [ y = ax^3 + bx^2 + cx ] This function takes in a tensor `input`, and coefficients `a`, `b`, and `c` for the polynomial. You need to implement both the forward and backward methods correctly. # Instructions 1. **Define Custom Autograd Function:** - Create a subclass of `torch.autograd.Function` called `PolynomialFunction`. - Implement the `forward` method to compute ( y = ax^3 + bx^2 + cx ). - Implement the `backward` method to compute the gradients of the input with respect to the loss. 2. **Define Forward and Backward Methods:** - `forward(ctx, input, a, b, c)`: computes the result and saves any tensors for the backward pass. - `backward(ctx, grad_output)`: computes the gradients of the inputs. 3. **Validating Gradients:** - Use `torch.autograd.gradcheck` to verify that the gradients computed are correct. # Example Code Skeleton ```python import torch from torch.autograd import Function class PolynomialFunction(Function): @staticmethod def forward(ctx, input, a, b, c): # Implement the forward computation ctx.save_for_backward(input, a, b, c) output = a * input ** 3 + b * input ** 2 + c * input return output @staticmethod def backward(ctx, grad_output): # Implement the backward computation input, a, b, c = ctx.saved_tensors grad_input = grad_a = grad_b = grad_c = None if ctx.needs_input_grad[0]: grad_input = grad_output * (3 * a * input ** 2 + 2 * b * input + c) if ctx.needs_input_grad[1]: grad_a = grad_output * input ** 3 if ctx.needs_input_grad[2]: grad_b = grad_output * input ** 2 if ctx.needs_input_grad[3]: grad_c = grad_output * input return grad_input, grad_a, grad_b, grad_c # Function to use the custom autograd function def polynomial(input, a, b, c): return PolynomialFunction.apply(input, a, b, c) # Example usage and gradient check if __name__ == \\"__main__\\": input = torch.randn(5, requires_grad=True, dtype=torch.double) a = torch.randn(1, requires_grad=True, dtype=torch.double)[0] b = torch.randn(1, requires_grad=True, dtype=torch.double)[0] c = torch.randn(1, requires_grad=True, dtype=torch.double)[0] # Check gradients test = torch.autograd.gradcheck(polynomial, (input, a, b, c)) print(\\"Are gradients correct?: \\", test) ``` # Expected Input and Output - **Input:** A tensor `input` of shape `(n,)`, and scalars `a`, `b`, `c`. - **Output:** A tensor `output` of shape `(n,)` resulting from the polynomial function. **Constraints:** - The input tensor size should be manageable in memory. - All tensors involved (input, a, b, c) should have `requires_grad=True`. # Grading Criteria 1. **Correctness:** The implementation of the forward and backward methods should be correct. 2. **Gradient Check:** The gradients should be validated correctly using `torch.autograd.gradcheck`. 3. **Code Quality:** The code should be clean, well-commented, and adhere to Python programming standards. # Submission Please submit the completed code, including the implementation of `PolynomialFunction` and the gradient check, by the end of the assessment period.","solution":"import torch from torch.autograd import Function class PolynomialFunction(Function): @staticmethod def forward(ctx, input, a, b, c): # Implement the forward computation ctx.save_for_backward(input, a, b, c) output = a * input ** 3 + b * input ** 2 + c * input return output @staticmethod def backward(ctx, grad_output): # Implement the backward computation input, a, b, c = ctx.saved_tensors grad_input = grad_a = grad_b = grad_c = None if ctx.needs_input_grad[0]: grad_input = grad_output * (3 * a * input ** 2 + 2 * b * input + c) if ctx.needs_input_grad[1]: grad_a = (grad_output * input ** 3).sum() if ctx.needs_input_grad[2]: grad_b = (grad_output * input ** 2).sum() if ctx.needs_input_grad[3]: grad_c = (grad_output * input).sum() return grad_input, grad_a, grad_b, grad_c # Function to use the custom autograd function def polynomial(input, a, b, c): return PolynomialFunction.apply(input, a, b, c)"},{"question":"# Email Serialization Using email.generator Module **Problem Statement:** You are provided with an email message object, and your task is to serialize this message using the `BytesGenerator` class from the `email.generator` module. You should write a function `serialize_email` that takes an email message object and a binary file output stream, serializes the email using the given output stream, and returns the serialized content as a byte string. **Function Signature:** ```python def serialize_email(msg: email.message.EmailMessage, output_stream: io.BytesIO) -> bytes: ``` **Input:** - `msg`: An instance of `email.message.EmailMessage` representing the email message to be serialized. - `output_stream`: An instance of `io.BytesIO` which is a file-like object that the serialized message will be written to. **Output:** - A byte string representing the serialized email message. **Constraints:** - You must use the `BytesGenerator` class from the `email.generator` module to perform the serialization. - The serialized email should be standards-compliant and in the correct format for transport. - Ensure that the `output_stream` is correctly populated with the serialized content. **Example:** ```python import email from email.message import EmailMessage import io from email.generator import BytesGenerator def serialize_email(msg: EmailMessage, output_stream: io.BytesIO) -> bytes: # Implement the function here pass # Sample Usage msg = EmailMessage() msg[\'Subject\'] = \'Test Email\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'receiver@example.com\' msg.set_content(\'This is a test email.\') output_stream = io.BytesIO() serialized_bytes = serialize_email(msg, output_stream) # Check the serialized content print(serialized_bytes) ``` Your task is to complete the implementation of the `serialize_email` function. Ensure that the function correctly makes use of `BytesGenerator` to serialize the provided email message and return the serialized content.","solution":"import email from email.message import EmailMessage import io from email.generator import BytesGenerator def serialize_email(msg: EmailMessage, output_stream: io.BytesIO) -> bytes: Serializes an email message and writes the output to the provided output stream. Parameters: - msg: An instance of email.message.EmailMessage representing the email message to be serialized. - output_stream: An instance of io.BytesIO which is a file-like object that the serialized message will be written to. Returns: - A byte string representing the serialized email message. # Initialize the BytesGenerator with the output stream gen = BytesGenerator(output_stream) # Serialize the email message gen.flatten(msg) # Return the serialized content as bytes return output_stream.getvalue()"},{"question":"Question # Objective Your task is to utilize the `sklearn.datasets` package to load a specific dataset, perform necessary data manipulations, and conduct a simple analysis. # Problem Statement Write a Python function `analyze_iris_dataset` that performs the following: 1. Loads the Iris dataset using the `load_iris` function from `sklearn.datasets`. 2. Checks and verifies the structure and type of the loaded dataset. 3. Computes and returns the average of the features for each species in the dataset. # Requirements - You must use the `load_iris` function to load the dataset. - The function should return a dictionary where keys are the species names and values are the average feature values (as a list) for each species. - The dataset returned by `load_iris` consists of four features (`data`) and a target (`target`), along with feature names and target names. # Function Signature ```python def analyze_iris_dataset() -> dict: pass ``` # Expected Output The function should return a dictionary in the following format: ```python { \'setosa\': [average_sepal_length, average_sepal_width, average_petal_length, average_petal_width], \'versicolor\': [average_sepal_length, average_sepal_width, average_petal_length, average_petal_width], \'virginica\': [average_sepal_length, average_sepal_width, average_petal_length, average_petal_width] } ``` # Example Suppose the following are the calculated averages (rounded to 3 decimal places): ```python { \'setosa\': [5.006, 3.418, 1.464, 0.244], \'versicolor\': [5.936, 2.770, 4.260, 1.326], \'virginica\': [6.588, 2.974, 5.552, 2.026] } ``` # Constraints - You should not use any external libraries apart from `numpy` and `sklearn`. - Assume that the dataset is correctly formatted and contains no missing values. # Evaluation You will be assessed based on: - Correctness of the dataset loading and verification steps. - Accuracy of computed averages for each species. - Proper use of `sklearn` utilities and handling of dataset attributes. # Note Refer to the official `scikit-learn` documentation if needed: [Scikit-learn Dataset Loading](https://scikit-learn.org/stable/datasets.html)","solution":"import numpy as np from sklearn.datasets import load_iris from collections import defaultdict def analyze_iris_dataset() -> dict: # Load the Iris dataset iris = load_iris() data = iris.data target = iris.target target_names = iris.target_names # Create a dictionary to store sum of features and count of samples for each species sum_features = defaultdict(lambda: np.zeros(data.shape[1])) count_samples = defaultdict(int) # Iterate over each sample in the dataset for features, label in zip(data, target): species = target_names[label] sum_features[species] += features count_samples[species] += 1 # Calculate average features for each species avg_features = {species: (sum_features[species] / count_samples[species]).tolist() for species in target_names} return avg_features"},{"question":"# Question: Managing and Comparing Temporary Files and Directories You are tasked with creating a Python script that performs several file and directory operations to demonstrate your understanding of the Python modules related to file handling and manipulation. Requirements 1. **Create a Temporary Directory**: - Use the `tempfile` module to create a temporary directory. 2. **Create Temporary Files**: - Inside the temporary directory, create three temporary `.txt` files using the `tempfile` module. - Each file should be named `file1.txt`, `file2.txt`, and `file3.txt`. - Write the following content to the files: - `file1.txt`: \\"Hello, World!\\" - `file2.txt`: \\"Hello, Python!\\" - `file3.txt`: \\"Hello, Universe!\\" 3. **List and Compare Files**: - Use the `pathlib` module to list all the `.txt` files in the temporary directory. - Use the `filecmp` module to compare `file1.txt` with `file2.txt`, and `file2.txt` with `file3.txt`. - Print whether the files compared are the same or different. 4. **Archive the Directory**: - Use the `shutil` module to create a zip archive of the entire temporary directory. 5. **Clean Up**: - Ensure that all temporary files and directories are deleted after the script execution, using appropriate clean-up methods. Constraints - You must use the modules specified above for each task. - Ensure the script is efficient and handles any potential exceptions that may occur. Input and Output Formats - **Input**: There is no direct input from the user. - **Output**: The script should print the list of files created, the results of the file comparisons, and the path to the created archive file. Example Output ``` List of files in temporary directory: [\'file1.txt\', \'file2.txt\', \'file3.txt\'] Comparison between file1.txt and file2.txt: Different Comparison between file2.txt and file3.txt: Different Archive created at: /path/to/your/archive.zip Temporary files and directory cleaned up. ``` Implement the script to meet the above requirements and constraints.","solution":"import tempfile import os from pathlib import Path import filecmp import shutil def create_temp_files_and_directory(): try: # Create a temporary directory with tempfile.TemporaryDirectory() as temp_dir: temp_dir_path = Path(temp_dir) # Create three temporary .txt files file1 = temp_dir_path / \'file1.txt\' file2 = temp_dir_path / \'file2.txt\' file3 = temp_dir_path / \'file3.txt\' file1.write_text(\\"Hello, World!\\") file2.write_text(\\"Hello, Python!\\") file3.write_text(\\"Hello, Universe!\\") # List all .txt files in the temporary directory txt_files = list(temp_dir_path.glob(\'*.txt\')) txt_file_names = [file.name for file in txt_files] print(f\\"List of files in temporary directory: {txt_file_names}\\") # Use the filecmp module to compare files comparison1 = filecmp.cmp(file1, file2, shallow=False) comparison2 = filecmp.cmp(file2, file3, shallow=False) print(f\\"Comparison between {file1.name} and {file2.name}: {\'Same\' if comparison1 else \'Different\'}\\") print(f\\"Comparison between {file2.name} and {file3.name}: {\'Same\' if comparison2 else \'Different\'}\\") # Archive the directory archive_path = shutil.make_archive(\'temporary_directory_archive\', \'zip\', temp_dir) print(f\\"Archive created at: {archive_path}\\") except Exception as e: print(f\\"An error occurred: {e}\\") # Calling the function to execute the file and directory operations create_temp_files_and_directory()"},{"question":"Objective: The goal of this exercise is to assess your ability to work with various data structures and transformations in Seaborn, and generate meaningful visualizations from those structures. Problem Statement: You are given a dataset containing information about the total number of airline passengers who flew in each month from 1949 to 1960. Your task is to: 1. Load the dataset using Seaborn. 2. Convert this dataset from long-form to wide-form and plot both formats using Seaborn. 3. Create a visualization of average passengers per month across the provided time span. Tasks: 1. **Load Dataset:** - Load the `flights` dataset using Seaborn\'s `load_dataset` method. 2. **Convert and Visualize Long-form Data:** - Plot a line chart using long-form data where `x` is the `year`, `y` is the number of `passengers`, and the `hue` differentiates the `months`. 3. **Convert to Wide-form Data:** - Transform the long-form data to wide-form where the `year` is the index, `month` is the columns, and the values are the `passengers`. - Plot this wide-form data using Seaborn. 4. **Average Passengers per Month Visualization:** - Compute the average number of passengers for each month over the years. - Plot the average monthly passengers using Seaborn. Expected Output: Your Python script should: 1. Show the code for loading the dataset. 2. Show the code and the plot of the long-form data visualization. 3. Show the code for converting the data to wide-form. 4. Show the code and the plot of the wide-form data visualization. 5. Show the code for computing the average number of passengers per month and the corresponding plot. Constraints: - Do not use any dataset other than the `flights` dataset provided by Seaborn. - Ensure that all plots have labels for the x-axis, y-axis, and a title. - Use Seaborn\'s `relplot` or similar functions where applicable. Code templates: ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the dataset flights = sns.load_dataset(\\"flights\\") # Long-form data visualization sns.relplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", kind=\\"line\\") plt.title(\'Monthly Passengers over Years (Long-form)\') plt.show() # Convert to wide-form data flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Wide-form data visualization sns.relplot(data=flights_wide, kind=\\"line\\") plt.title(\'Monthly Passengers over Years (Wide-form)\') plt.show() # Compute average number of passengers per month average_passengers = flights.groupby(\\"month\\").mean()[\\"passengers\\"].reset_index() # Average monthly passengers visualization sns.barplot(x=\\"month\\", y=\\"passengers\\", data=average_passengers) plt.title(\'Average Monthly Passengers\') plt.show() ``` Performance Requirement: The transformations and plotting should handle the size of the dataset efficiently.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def load_flights_dataset(): return sns.load_dataset(\\"flights\\") def plot_long_form_data(flights): plt.figure(figsize=(12, 6)) sns.relplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", kind=\\"line\\") plt.title(\'Monthly Passengers over Years (Long-form)\') plt.xlabel(\'Year\') plt.ylabel(\'Passengers\') plt.xticks(rotation=45) plt.show() def convert_to_wide_form(flights): return flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") def plot_wide_form_data(flights_wide): plt.figure(figsize=(12, 6)) sns.heatmap(flights_wide, annot=True, fmt=\\"d\\", cmap=\\"YlGnBu\\") plt.title(\'Monthly Passengers over Years (Wide-form)\') plt.xlabel(\'Month\') plt.ylabel(\'Year\') plt.show() def compute_average_passengers_per_month(flights): return flights.groupby(\\"month\\").mean().reset_index() def plot_average_passengers(average_passengers): plt.figure(figsize=(12, 6)) sns.barplot(x=\\"month\\", y=\\"passengers\\", data=average_passengers, order=[\\"Jan\\", \\"Feb\\", \\"Mar\\", \\"Apr\\", \\"May\\", \\"Jun\\", \\"Jul\\", \\"Aug\\", \\"Sep\\", \\"Oct\\", \\"Nov\\", \\"Dec\\"]) plt.title(\'Average Monthly Passengers\') plt.xlabel(\'Month\') plt.ylabel(\'Average Passengers\') plt.show() # Load the data flights = load_flights_dataset() # Plot long-form data plot_long_form_data(flights) # Convert to wide-form and plot flights_wide = convert_to_wide_form(flights) plot_wide_form_data(flights_wide) # Compute and plot average passengers per month average_passengers = compute_average_passengers_per_month(flights) plot_average_passengers(average_passengers)"},{"question":"**Objective:** Implement a Python script that performs a series of tasks involving file management, string pattern matching, and statistical analysis. The script should operate using command-line arguments. Task Requirements: 1. **File Handling and Directory Management**: - The script should take two directory paths as command-line arguments: a source directory and a target directory. - It should list all `.txt` files in the source directory. - The script should move all `.txt` files from the source directory to the target directory. 2. **String Pattern Matching**: - For each `.txt` file moved, the script should read its contents and print all words that start with a vowel. This operation should be case-insensitive. 3. **Statistical Analysis**: - The script should count the number of words in each of these files and compute the mean, median, and variance of these counts. Input Format: - The script should be executed with two command-line arguments: source directory path and target directory path. **Example Command:** ```sh python script.py /path/to/source /path/to/target ``` Output Format: 1. List of all `.txt` files moved. 2. For each file, print all words that start with a vowel. 3. Mean, median, and variance of the word counts in the moved files. Constraints: - Assume all files in the source directory are readable and contain text data. - The script should handle case insensitive matching for vowel-starting words. - Utilize the `argparse` module to handle command-line arguments. - Use the `statistics` module to compute mean, median, and variance. Example: Assume: - The source directory `/source` contains `file1.txt` with content: \\"An example text, which is simple.\\" - The target directory `/target` is initially empty. When you run: ```sh python script.py /source /target ``` Expected output: ``` Moved files: [\'file1.txt\'] Words starting with a vowel in file1.txt: [\'An\', \'example\', \'is\'] Mean word count: 5.0 Median word count: 5.0 Variance in word count: 0.0 ``` Performance: - The script should be efficient with respect to I/O operations and should handle large files gracefully.","solution":"import os import shutil import argparse import re import statistics def move_txt_files(source_dir, target_dir): if not os.path.exists(target_dir): os.makedirs(target_dir) moved_files = [] for filename in os.listdir(source_dir): if filename.endswith(\\".txt\\"): full_file_name = os.path.join(source_dir, filename) if os.path.isfile(full_file_name): shutil.move(full_file_name, target_dir) moved_files.append(filename) return moved_files def find_vowel_starting_words(file_path): vowels = re.compile(r\'b[aeiouAEIOU][a-zA-Z]*b\') with open(file_path, \'r\', encoding=\'utf-8\') as file: contents = file.read() return vowels.findall(contents) def get_word_count(word_list): return len(word_list) def compute_statistics(word_counts): if word_counts: mean = statistics.mean(word_counts) median = statistics.median(word_counts) variance = statistics.variance(word_counts) if len(word_counts) > 1 else 0.0 else: mean = median = variance = 0.0 return mean, median, variance def main(): parser = argparse.ArgumentParser(description=\'Move txt files and perform analysis\') parser.add_argument(\'source_dir\', type=str, help=\'Source directory path\') parser.add_argument(\'target_dir\', type=str, help=\'Target directory path\') args = parser.parse_args() moved_files = move_txt_files(args.source_dir, args.target_dir) print(f\\"Moved files: {moved_files}\\") word_counts = [] for moved_file in moved_files: full_file_path = os.path.join(args.target_dir, moved_file) words = find_vowel_starting_words(full_file_path) print(f\\"Words starting with a vowel in {moved_file}: {words}\\") word_count = get_word_count(words) word_counts.append(word_count) mean, median, variance = compute_statistics(word_counts) print(f\\"Mean word count: {mean}\\") print(f\\"Median word count: {median}\\") print(f\\"Variance in word count: {variance}\\") if __name__ == \\"__main__\\": main()"},{"question":"**Coding Assessment Question: File Organization in Directory Trees** **Objective:** You are given a list of file paths and their corresponding sizes in bytes. Your task is to write a function that organizes these files into a nested dictionary structure representing the directory tree. Additionally, include the sizes of the individual files and the total size of each directory. **Function Signature:** ```python def organize_directory_tree(file_list: List[Tuple[str, int]]) -> Dict[str, Any]: pass ``` **Parameters:** - `file_list`: A list of tuples, where each tuple contains a file path (string) and its size in bytes (integer). **Returns:** - A nested dictionary structure representing the directory tree. Directories should have keys for their subdirectories and files. Each directory should have a special key `\\"__size__\\"` that sums the size of all files in that directory (including those in subdirectories). **Example:** ```python file_list = [ (\\"/home/user/file1.txt\\", 100), (\\"/home/user/file2.txt\\", 200), (\\"/home/user/docs/file3.docx\\", 300), (\\"/home/user/docs/file4.docx\\", 400), (\\"/var/log/syslog\\", 500), (\\"/var/log/kern.log\\", 600), (\\"/var/tmp/cache\\", 700) ] result = organize_directory_tree(file_list) expected_result = { \\"home\\": { \\"user\\": { \\"file1.txt\\": 100, \\"file2.txt\\": 200, \\"docs\\": { \\"file3.docx\\": 300, \\"file4.docx\\": 400, \\"__size__\\": 700 }, \\"__size__\\": 1000 }, \\"__size__\\": 1000 }, \\"var\\": { \\"log\\": { \\"syslog\\": 500, \\"kern.log\\": 600, \\"__size__\\": 1100 }, \\"tmp\\": { \\"cache\\": 700, \\"__size__\\": 700 }, \\"__size__\\": 1800 }, \\"__size__\\": 2800 } ``` **Constraints:** - File paths are given as absolute paths. - Paths do not contain symbolic links or unusual characters. - You may assume that the input paths are valid and do not overlap in unexpected ways. **Performance Requirements:** - The solution should efficiently handle up to 10,000 file paths. - Aim to minimize redundant computations and make effective use of the `os.path` functions for path manipulations. **Notes:** - Use `os.path` to handle path manipulations. - Consider using recursion or iterative methods to construct the nested dictionary. Happy coding!","solution":"import os from typing import List, Tuple, Dict, Any def organize_directory_tree(file_list: List[Tuple[str, int]]) -> Dict[str, Any]: tree = {} def add_to_tree(path_parts, size, subtree): if len(path_parts) == 1: subtree[path_parts[0]] = size subtree[\\"__size__\\"] = subtree.get(\\"__size__\\", 0) + size else: if path_parts[0] not in subtree: subtree[path_parts[0]] = {\\"__size__\\": 0} add_to_tree(path_parts[1:], size, subtree[path_parts[0]]) subtree[\\"__size__\\"] = subtree.get(\\"__size__\\", 0) + size for path, size in file_list: path_parts = path.strip(\\"/\\").split(\\"/\\") add_to_tree(path_parts, size, tree) return tree"},{"question":"**Problem Statement: Customized Legend Placement in Seaborn** You are provided with a dataset containing measurements of penguins. Your task is to create various visualizations using seaborn and customize the position and appearance of legends in the plots. This exercise will test your ability to utilize seaborn\'s plotting capabilities and control plot aesthetics, especially legend positioning. # Dataset The dataset \\"penguins\\" can be loaded directly using the seaborn `load_dataset` function: ```python import seaborn as sns penguins = sns.load_dataset(\\"penguins\\") ``` The dataset contains the following columns: - `species`: Species of the penguin (e.g., Adelie, Gentoo, Chinstrap) - `island`: Island where the penguin was observed - `bill_length_mm`: Length of the bill in millimeters - `bill_depth_mm`: Depth of the bill in millimeters - `flipper_length_mm`: Length of the flipper in millimeters - `body_mass_g`: Body mass of the penguin in grams - `sex`: Sex of the penguin (Male, Female) - `year`: The year of observation # Tasks 1. **Histogram with Customized Legend (Axes-level Function)** - Create a histogram showing the distribution of `bill_length_mm` for different `species`. - Move the legend to the `upper left` position inside the plot, and fine-tune the position using `bbox_to_anchor`. - Customize the legend to have no frame and set the number of columns to 3. 2. **Facet Grid with Customized Legend (Figure-level Function)** - Create a facet grid of histograms showing the distribution of `bill_length_mm` for different `species` across different `island`. - Move the legend to the `upper right` position with a slight offset using `bbox_to_anchor`. - Ensure there is no extra blank space reserved for the legend outside the plot. # Implementation Write a Python function `customize_plot_legend()` that generates and displays the described plots. The function should not return anything, but it should show the plots as output. Example Function Signature ```python import seaborn as sns def customize_plot_legend(): # Task 1: Your code to create histograms and move legend # Task 2: Your code to create facet grid and move legend Customize_plot_legend() ``` # Constraints - Use seaborn and matplotlib libraries only. - Ensure that the legend\'s position and appearance as described. # Expected Output - Two plots should be displayed with customized legends as per the instructions. This question assesses your ability to work with seaborn for creating customized visualizations and demonstrates your knowledge of handling axes-level and figure-level functions for legend customization.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customize_plot_legend(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Task 1: Histogram with Customized Legend (Axes-level Function) plt.figure(figsize=(10, 6)) hist_plot = sns.histplot(data=penguins, x=\'bill_length_mm\', hue=\'species\', multiple=\'stack\', kde=True) hist_plot.legend(loc=\'upper left\', bbox_to_anchor=(0.05, 0.95), frameon=False, ncol=3) plt.title(\\"Distribution of Bill Length by Species\\") plt.show() # Task 2: Facet Grid with Customized Legend (Figure-level Function) g = sns.FacetGrid(penguins, col=\'island\', hue=\'species\', height=4, aspect=1) g.map(sns.histplot, \'bill_length_mm\', kde=False, multiple=\\"stack\\").add_legend(loc=\'upper right\', bbox_to_anchor=(1, 0.5), frameon=False) plt.subplots_adjust(top=0.85) g.fig.suptitle(\\"Distribution of Bill Length by Species Across Islands\\") plt.show() # Execute the function to display the plots customize_plot_legend()"},{"question":"# Python Coding Assessment Objective Create a Python script that provides detailed information about the system\'s Python installation using the `sysconfig` module. This script will be useful for developers who need to understand their Python environment configuration and installation paths. Task Write a Python function `get_python_configuration()` that performs the following tasks: 1. **Retrieve and print** Python\'s major and minor version in the format \\"MAJOR.MINOR\\". 2. **Retrieve and print** the current platform identifier string. 3. **Retrieve and print** the default installation scheme for the current platform. 4. **Retrieve and print** all supported scheme names. 5. **Retrieve and print** all supported path names. 6. **Retrieve and print** the installation paths for the default scheme. 7. **Retrieve and print** the value of a specific configuration variable, `Py_ENABLE_SHARED`. Specifications - The function should not take any arguments. - Output each piece of information on a new line. - Handle cases where specific configuration variables or paths might not be found gracefully. - Performance is not a primary concern, but the function should execute efficiently on reasonably modern systems. Example Output ``` Python Version: 3.10 Platform: linux-x86_64 Default Scheme: posix_prefix Scheme Names: (\'posix_prefix\', \'posix_home\', \'posix_user\', \'nt\', \'nt_user\', \'osx_framework_user\') Path Names: (\'stdlib\', \'platstdlib\', \'platlib\', \'purelib\', \'include\', \'platinclude\', \'scripts\', \'data\') Installation Paths: stdlib: /usr/local/lib/python3.10 platstdlib: /usr/local/lib/python3.10 platlib: /usr/local/lib/python3.10/site-packages purelib: /usr/local/lib/python3.10/site-packages include: /usr/local/include/python3.10 platinclude: /usr/local/include/python3.10 scripts: /usr/local/bin data: /usr/local Py_ENABLE_SHARED: 0 ``` Additional Information - Use the `sysconfig` module to retrieve the requested information. - The example output values may vary depending on the system and Python version in use. ```python import sysconfig def get_python_configuration(): python_version = sysconfig.get_python_version() platform = sysconfig.get_platform() default_scheme = sysconfig.get_default_scheme() scheme_names = sysconfig.get_scheme_names() path_names = sysconfig.get_path_names() installation_paths = sysconfig.get_paths() py_enable_shared = sysconfig.get_config_var(\'Py_ENABLE_SHARED\') print(f\\"Python Version: {python_version}\\") print(f\\"Platform: {platform}\\") print(f\\"Default Scheme: {default_scheme}\\") print(f\\"Scheme Names: {scheme_names}\\") print(f\\"Path Names: {path_names}\\") print(\\"Installation Paths:\\") for name, path in installation_paths.items(): print(f\\" {name}: {path}\\") print(f\\"Py_ENABLE_SHARED: {py_enable_shared}\\") # Call the function to display the configuration get_python_configuration() ```","solution":"import sysconfig def get_python_configuration(): python_version = sysconfig.get_python_version() platform = sysconfig.get_platform() default_scheme = sysconfig.get_default_scheme() scheme_names = sysconfig.get_scheme_names() path_names = sysconfig.get_path_names() installation_paths = sysconfig.get_paths() py_enable_shared = sysconfig.get_config_var(\'Py_ENABLE_SHARED\') print(f\\"Python Version: {python_version}\\") print(f\\"Platform: {platform}\\") print(f\\"Default Scheme: {default_scheme}\\") print(f\\"Scheme Names: {scheme_names}\\") print(f\\"Path Names: {path_names}\\") print(\\"Installation Paths:\\") for name, path in installation_paths.items(): print(f\\" {name}: {path}\\") print(f\\"Py_ENABLE_SHARED: {py_enable_shared}\\") # Call the function to display the configuration get_python_configuration()"},{"question":"# PyTorch Coding Assessment Question Objective: Design an advanced neural network operation using PyTorch tensors. The objective is to implement a custom backward pass for a simple operation. Problem Statement: You need to implement a custom autograd function for a simple neural network layer. The operation of the layer can be described as follows: - Given an input tensor `x`, the layer should compute `y = x^3 + 2*x`. You must use PyTorch\'s custom autograd function capabilities to define both the forward and backward passes for this operation. Task: Implement the `CubePlusTwo` class which inherits from `torch.autograd.Function`. This class should override two static methods: `forward` and `backward`. - **Forward Pass**: - Input: A tensor `x` of any shape. - Output: A tensor `y` of the same shape as `x` where each element is the result of the operation `x^3 + 2*x`. - **Backward Pass**: - Input: A tensor `grad_output` containing the gradient of the loss with respect to the output `y`. - Output: A tensor `grad_input` containing the gradient of the loss with respect to the input `x`. - Recall that the derivative of `y = x^3 + 2*x` with respect to `x` is `dy/dx = 3*x^2 + 2`. Input/Output Example: ```python import torch # Example input tensor x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) # Forward pass y = CubePlusTwo.apply(x) print(y) # Expected output: tensor([3.0, 12.0, 33.0], grad_fn=<CubePlusTwoBackward>) # Backward pass y.backward(torch.tensor([1.0, 1.0, 1.0])) print(x.grad) # Expected output: tensor([5.0, 14.0, 29.0]) since dy/dx = [5, 14, 29] for x = [1, 2, 3] ``` Constraints: - The implementation should handle tensors of any shape and not assume any specific dimensions. - You must use PyTorch\'s `torch.autograd.Function` class to define the custom autograd function. - Ensure that your implementation is efficient and leverages PyTorch operations when possible. Submission Requirements: - A Python script or Jupyter notebook containing the `CubePlusTwo` class implementation and some test cases demonstrating its usage and correctness. Good luck!","solution":"import torch class CubePlusTwo(torch.autograd.Function): @staticmethod def forward(ctx, x): Forward pass for the custom operation. y = x^3 + 2*x :param ctx: Context object to store information for backpropagation. :param x: Input tensor. :return: Result tensor after applying the operation. ctx.save_for_backward(x) return x**3 + 2*x @staticmethod def backward(ctx, grad_output): Backward pass for the custom operation. :param ctx: Context object containing saved information from the forward pass. :param grad_output: Gradient of the loss with respect to the output y. :return: Gradient of the loss with respect to the input x. x, = ctx.saved_tensors grad_input = grad_output * (3*x**2 + 2) return grad_input"},{"question":"# Question: Implement and Visualize Isomap for Dimensionality Reduction Objective: Utilize the Isomap algorithm from scikit-learn on the famous digits dataset to reduce the dimensionality to two dimensions for visualization purposes. Demonstrate the understanding of preprocessing, applying the Isomap algorithm, and visualizing the reduced dataset. Instructions: 1. **Load the Digits Dataset**: Use `sklearn.datasets.load_digits` to load the digits dataset. 2. **Preprocess the Data**: - Scale the data to have zero mean and unit variance (standard scaling). 3. **Apply Isomap for Dimensionality Reduction**: - Reduce the dimensionality of the dataset to 2 dimensions using the `Isomap` class from `sklearn.manifold`. - Set the number of neighbors `n_neighbors` to 10 and the number of dimensions `n_components` to 2. 4. **Visualize the Results**: - Create a scatter plot of the reduced 2D data. - Color the points in the scatter plot by their digit labels. Requirements: - **Input**: None. - **Output**: A matplotlib scatter plot showing the 2D representation of the digits dataset, color-coded by the digit labels. Constraints: - You must use the `Isomap` class from `sklearn.manifold`. - Make sure that the data is standard scaled before applying Isomap. - The digits should be clearly distinguishable in the scatter plot by color labels. Example Code Structure: ```python import matplotlib.pyplot as plt from sklearn.datasets import load_digits from sklearn.preprocessing import StandardScaler from sklearn.manifold import Isomap # Step 1: Load the digits dataset digits = load_digits() data = digits.data labels = digits.target # Step 2: Standardize the data scaler = StandardScaler() data_scaled = scaler.fit_transform(data) # Step 3: Apply Isomap isomap = Isomap(n_neighbors=10, n_components=2) data_reduced = isomap.fit_transform(data_scaled) # Step 4: Visualize the results plt.figure(figsize=(10, 6)) plt.scatter(data_reduced[:, 0], data_reduced[:, 1], c=labels, cmap=\'viridis\', edgecolor=\'k\', s=40) plt.colorbar(label=\'Digit Label\') plt.title(\'Isomap projection of the Digits dataset\') plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.show() ``` Use this structure to implement the solution, ensuring each step is correctly executed, and the final plot clearly visualizes the digits dataset in 2D space, distinguishing between different digits using colors.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import load_digits from sklearn.preprocessing import StandardScaler from sklearn.manifold import Isomap def isomap_visualization(): # Step 1: Load the digits dataset digits = load_digits() data = digits.data labels = digits.target # Step 2: Standardize the data scaler = StandardScaler() data_scaled = scaler.fit_transform(data) # Step 3: Apply Isomap isomap = Isomap(n_neighbors=10, n_components=2) data_reduced = isomap.fit_transform(data_scaled) # Step 4: Visualize the results plt.figure(figsize=(10, 6)) scatter = plt.scatter(data_reduced[:, 0], data_reduced[:, 1], c=labels, cmap=\'viridis\', edgecolor=\'k\', s=40) plt.colorbar(label=\'Digit Label\') plt.title(\'Isomap projection of the Digits dataset\') plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.show() return data_reduced, labels"},{"question":"**Objective:** Demonstrate your understanding of Python\'s \\"inspect\\" module by analyzing a given function and extracting various details about its arguments, annotations, and documentation. **Problem Statement:** Create a Python function `analyze_function(func: Callable) -> str` that takes a callable `func` as input and returns a formatted string containing the following information about the function: 1. The name of the function. 2. The docstring of the function (or a note if none exists). 3. The list of arguments with their default values, if any. 4. The annotations for each parameter, along with the return annotation if it exists. 5. The source code of the function. # Input: - `func`: A callable (function or method) to be analyzed. # Output: - A formatted string containing the function\'s details. # Example: ```python def analyze_function(func: Callable) -> str: pass # Usage example: def example_function(a: int, b: str = \\"default\\") -> bool: This is an example function. return str(a).isdigit() and len(b) > 5 result = analyze_function(example_function) print(result) ``` **Expected output:** ``` Function Name: example_function Docstring: This is an example function. Arguments and Default Values: a: No default value b: default = \'default\' Annotations: a: <class \'int\'> b: <class \'str\'> return: <class \'bool\'> Source Code: def example_function(a: int, b: str = \\"default\\") -> bool: This is an example function. return str(a).isdigit() and len(b) > 5 ``` # Requirements: 1. Use the `inspect` module to retrieve and format the required information. 2. Ensure that your function handles cases where certain information (like annotations or docstrings) might be missing. 3. The output should be neatly formatted and easy to read. # Constraints: - Only use the functions provided in the `inspect` module for introspection. - Ensure the function works for both regular functions and class methods. Good luck, and happy coding!","solution":"import inspect from typing import Callable def analyze_function(func: Callable) -> str: # Get function name func_name = func.__name__ # Get docstring docstring = inspect.getdoc(func) or \\"No docstring available\\" # Get arguments with defaults sig = inspect.signature(func) parameters = sig.parameters arguments = [] for name, param in parameters.items(): if param.default == inspect.Parameter.empty: arguments.append(f\\"{name}: No default value\\") else: arguments.append(f\\"{name}: default = {repr(param.default)}\\") # Get annotations annotations = [] for name, param in parameters.items(): if param.annotation != inspect.Parameter.empty: annotations.append(f\\"{name}: {param.annotation}\\") if sig.return_annotation != inspect.Signature.empty: annotations.append(f\\"return: {sig.return_annotation}\\") # Get source code source_code = inspect.getsource(func) # Format the details result = f\\"Function Name: {func_name}nn\\" result += f\\"Docstring:n{docstring}nn\\" result += \\"Arguments and Default Values:n\\" result += \\"n\\".join(arguments) + \\"nn\\" result += \\"Annotations:n\\" result += \\"n\\".join(annotations) + \\"nn\\" result += f\\"Source Code:n{source_code}\\" return result"},{"question":"# Python Iterator and Asynchronous Iterator Implementation **Objective**: Demonstrate your understanding of the Python 3.10 iterator protocol by implementing a custom iterator class and an asynchronous iterator class in Python. # Part 1: Custom Iterator **Task**: Implement a Python class `Countdown` that acts as an iterator over a countdown sequence from a specified starting integer down to zero. **Requirements**: 1. The class should accept a starting integer `n` upon initialization. 2. The class should implement the iterator protocol (`__iter__` and `__next__` methods). 3. When the countdown reaches zero, further calls to `__next__` should raise a `StopIteration` exception. ```python class Countdown: def __init__(self, n: int): Initialize the countdown iterator with a starting value `n`. pass def __iter__(self): Return the iterator object itself. pass def __next__(self): Return the next value in the countdown sequence. pass ``` **Example**: ```python c = Countdown(5) for num in c: print(num) # Output: # 5 # 4 # 3 # 2 # 1 # 0 ``` # Part 2: Custom Asynchronous Iterator **Task**: Implement a Python class `AsyncCountdown` that acts as an asynchronous iterator over a countdown sequence from a specified starting integer down to zero. Use `await` to simulate asynchronously yielding the next value. **Requirements**: 1. The class should accept a starting integer `n` upon initialization. 2. The class should implement the asynchronous iterator protocol (`__aiter__` and `__anext__` methods). 3. When the countdown reaches zero, further calls to `__anext__` should raise a `StopAsyncIteration` exception. ```python import asyncio class AsyncCountdown: def __init__(self, n: int): Initialize the asynchronous countdown iterator with a starting value `n`. pass def __aiter__(self): Return the asynchronous iterator object itself. pass async def __anext__(self): Asynchronously return the next value in the countdown sequence. pass ``` **Example**: ```python async def main(): c = AsyncCountdown(5) async for num in c: print(num) asyncio.run(main()) # Output: # 5 # 4 # 3 # 2 # 1 # 0 ``` **Note**: Ensure your implementation adheres to handling the iterator and asynchronous iterator protocols correctly. # Constraints: - Do not use any built-in iteration functions like `iter` or `next` for the first part. - For the asynchronous part, you may use `await` and `asyncio.sleep` to simulate asynchronous operations.","solution":"class Countdown: def __init__(self, n: int): Initialize the countdown iterator with a starting value `n`. self.current = n def __iter__(self): Return the iterator object itself. return self def __next__(self): Return the next value in the countdown sequence. if self.current < 0: raise StopIteration result = self.current self.current -= 1 return result import asyncio class AsyncCountdown: def __init__(self, n: int): Initialize the asynchronous countdown iterator with a starting value `n`. self.current = n def __aiter__(self): Return the asynchronous iterator object itself. return self async def __anext__(self): Asynchronously return the next value in the countdown sequence. if self.current < 0: raise StopAsyncIteration result = self.current self.current -= 1 await asyncio.sleep(0) # simulate asynchronous operation return result"},{"question":"# Question: Debugging with Python Debugger (pdb) Objective: The goal of this question is to assess your understanding and practical application of the \\"pdb\\" module for debugging Python code. Problem Statement: You are provided with a Python function `calculate_sum` that takes a list of integers and returns their sum. However, there are errors in the function. Your task is to use the \\"pdb\\" module to find and fix the errors by setting breakpoints, stepping through the code, and inspecting the state of the program. Function Signature: ```python def calculate_sum(numbers): total = 0 for i in range(len(numbers)): total += number[i] retrun total ``` Steps to Complete: 1. Import the \\"pdb\\" module at the beginning of your script. 2. Insert debugging statements at appropriate locations using `pdb.set_trace()` to break into the debugger. 3. Use \\"pdb\\" commands to step through the code, inspect variables, and set breakpoints. 4. Identify and fix the errors in the `calculate_sum` function. 5. Ensure the function correctly returns the sum of the numbers in the list. Constraints: - The input list `numbers` can contain at most `10^6` integers. - Each integer in the list can be within the range of `-10^6` to `10^6`. Example: ```python numbers = [1, 2, 3, 4, 5] # Expected output: 15 result = calculate_sum(numbers) print(\\"The sum is:\\", result) ``` Requirements: 1. Demonstrate the usage of the \\"pdb\\" module in your solution. 2. Provide comments in your code explaining the debugging steps you took. 3. Ensure your final implementation of the `calculate_sum` function passes the example test case without errors. Submission: Submit your Python script with the corrected `calculate_sum` function and the debugging steps included.","solution":"def calculate_sum(numbers): Calculates the sum of a list of integers. total = 0 # Debugging using pdb to set trace here # import pdb; pdb.set_trace() for i in range(len(numbers)): # Inspecting the state of variables in pdb showed number[i] was incorrect # Changed \'number[i]\' to \'numbers[i]\' total += numbers[i] # Spotted a typo here \'retrun\' and fixed it to \'return\' return total # Example run numbers = [1, 2, 3, 4, 5] print(\\"The sum is:\\", calculate_sum(numbers)) # Expected output: 15"},{"question":"**Question: Implement a PyTorch XPU-based Matrix Multiplication with Custom Memory Management and Stream Handling** Your task is to implement a matrix multiplication function using PyTorch\'s XPU module. The implementation should demonstrate your understanding of device management, memory management, and stream handling. # Function Signature ```python def xpu_matrix_multiply(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Multiplies two matrices A and B using the XPU device. Args: A (torch.Tensor): A 2D tensor representing matrix A. B (torch.Tensor): A 2D tensor representing matrix B. Returns: torch.Tensor: The result of multiplying A and B on the XPU device. Raises: ValueError: If the matrices cannot be multiplied (i.e., their dimensions don\'t align). ``` # Input - `A`: A 2D tensor with shape `(m, k)`. - `B`: A 2D tensor with shape `(k, n)`. # Output - A 2D tensor with shape `(m, n)` which is the result of the matrix multiplication. # Constraints - You must handle the device initialization and ensure the matrices are moved to the XPU device. - You should manage the memory efficiently, ensuring no unnecessary memory is allocated. - Implement custom stream handling to manage asynchronous operations for the matrix multiplication. - Ensure the function handles the case where matrices A and B cannot be multiplied and raises a `ValueError`. # Example Usage ```python import torch import torch.xpu as xpu # Initialize XPU if not xpu.is_available(): raise EnvironmentError(\'XPU device not available\') # Example matrices A = torch.randn(3, 5) B = torch.randn(5, 4) # Perform matrix multiplication on XPU result = xpu_matrix_multiply(A, B) print(result) ``` # Explanation 1. **Device Management**: - Initialize the XPU device and move matrices A and B to the XPU. - Use `xpu.current_device`, `xpu.set_device`. 2. **Memory Management**: - Monitor memory usage before and after the operation using `xpu.memory_allocated`. - Clear any unnecessary cache memory using `xpu.empty_cache`. 3. **Stream Handling**: - Use custom streams to perform the matrix multiplication asynchronously. - Synchronize streams as necessary to ensure correct results. **Note**: The implementation should primarily focus on utilizing the `torch.xpu` module functionalities and demonstrate clarity in handling the matrix multiplication operation on the XPU device.","solution":"import torch import torch.xpu as xpu def xpu_matrix_multiply(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Multiplies two matrices A and B using the XPU device. Args: A (torch.Tensor): A 2D tensor representing matrix A. B (torch.Tensor): A 2D tensor representing matrix B. Returns: torch.Tensor: The result of multiplying A and B on the XPU device. Raises: ValueError: If the matrices cannot be multiplied (i.e., their dimensions don\'t align). if A.shape[1] != B.shape[0]: raise ValueError(\\"Matrices A and B cannot be multiplied due to incompatible dimensions.\\") if not xpu.is_available(): raise EnvironmentError(\'XPU device not available\') # Set the device to XPU device = xpu.set_device(xpu.current_device()) # Move tensors to XPU A_xpu = A.to(device) B_xpu = B.to(device) # Create a custom stream for asynchronous operations stream = xpu.Stream() # Perform matrix multiplication within the stream context with xpu.device(device): with xpu.stream(stream): C_xpu = torch.mm(A_xpu, B_xpu) # Synchronize the stream stream.synchronize() return C_xpu.cpu()"},{"question":"# Question: Customizing Seaborn Plot Aesthetics You are provided with a dataset and need to create a visual summary with specific aesthetic requirements. You will use Seaborn to produce different plots while applying various styles, themes, and contexts. Requirements: 1. **Function Signature**: Create a function `visualization_summary(data: np.ndarray) -> None`. 2. **Input**: The function takes a 2D numpy array `data` where each column represents a different variable. 3. **Output**: The function does not return anything but generates a series of plots that meet the following criteria: Plots to Generate: 1. A boxplot with the \\"whitegrid\\" style. 2. A violin plot with the \\"darkgrid\\" style, having the top and right spines removed. 3. A series of 2x1 subplots: - The first (left) subplot should contain a boxplot with the \\"ticks\\" style. - The second (right) subplot should contain the same boxplot but with the \\"dark\\" style, and with spines removed on all sides. 4. Finally, create a set of sine plots with increasing frequency, applying the \\"poster\\" context to increase font and line size for better readability in a presentation. Constraints and Limitations: - You should use the Seaborn library for styling and plotting. - Use context managers (`with` statements) to temporarily set styles where indicated. - Ensure that the plots are generated and displayed within the function without returning them. # Example: Given the following input array: ```python data = np.random.normal(size=(20, 6)) + np.arange(6) / 2 ``` Expected Outcome: The function should produce the following plots: 1. A boxplot with \\"whitegrid\\" style. 2. A violin plot with \\"darkgrid\\" style and no top or right spines. 3. Two side-by-side boxplots within a single figure, one with \\"ticks\\" style and one with \\"dark\\" style with all spines removed. 4. A sine plot with the \\"poster\\" context applied. ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt def visualization_summary(data: np.ndarray) -> None: # Boxplot with whitegrid style sns.set_style(\\"whitegrid\\") plt.figure() sns.boxplot(data=data) plt.show() # Violin plot with darkgrid style and spines removed sns.set_style(\\"darkgrid\\") plt.figure() sns.violinplot(data=data) sns.despine() plt.show() # 2x1 subplots with ticks style on left and dark style on right f, axes = plt.subplots(1, 2, figsize=(12, 5)) with sns.axes_style(\\"ticks\\"): sns.boxplot(data=data, ax=axes[0]) with sns.axes_style(\\"dark\\"): sns.boxplot(data=data, ax=axes[1]) sns.despine(ax=axes[1], left=True, right=True, top=True, bottom=True) f.tight_layout() plt.show() # Sine plot with poster context sns.set_context(\\"poster\\") def sinplot(n=10, flip=1): x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip) plt.figure() sinplot() plt.show() ``` **Note**: Ensure your function works with datasets of various sizes and structures without any hardcoded values except as required for styling.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def visualization_summary(data: np.ndarray) -> None: # Boxplot with whitegrid style sns.set_style(\\"whitegrid\\") plt.figure() sns.boxplot(data=data) plt.show() # Violin plot with darkgrid style and spines removed sns.set_style(\\"darkgrid\\") plt.figure() sns.violinplot(data=data) sns.despine() plt.show() # 2x1 subplots with ticks style on left and dark style on right f, axes = plt.subplots(1, 2, figsize=(12, 5)) with sns.axes_style(\\"ticks\\"): sns.boxplot(data=data, ax=axes[0]) with sns.axes_style(\\"dark\\"): sns.boxplot(data=data, ax=axes[1]) sns.despine(ax=axes[1], left=True, right=True, top=True, bottom=True) f.tight_layout() plt.show() # Sine plot with poster context sns.set_context(\\"poster\\") def sinplot(n=10, flip=1): x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip) plt.figure() sinplot() plt.show()"},{"question":"Objective Your task is to implement a classifier that tunes its decision threshold for class prediction using scikit-learn. You will demonstrate your understanding by achieving high recall for a medical diagnosis dataset, reflecting a real-world problem where missing a positive case is highly undesirable. Problem Statement You have been provided with a dataset representing patients, where each patient has various features, and there is a binary outcome of whether they have a certain disease or not. Your task is to train a classifier to predict whether a patient has the disease. However, unlike usual classifiers, you should focus on tuning the decision threshold to ensure high recall. Requirements 1. Load the dataset from the provided CSV file. 2. Split the data into training and testing sets. 3. Train a logistic regression model. 4. Use `TunedThresholdClassifierCV` to tune the decision threshold. The goal is to maximize recall. 5. Evaluate the performance of your classifier on the test set and print recall, precision, and the confusion matrix. Function Signature ```python def tune_threshold_and_evaluate(csv_file_path: str) -> None: Parameters: csv_file_path: str - Path to the CSV file containing the dataset. The CSV file will have the following columns: - feature_1, feature_2, ..., feature_n: Features of the dataset. - outcome: Binary outcome column (0 for negative, 1 for positive). Returns: None - Print the recall, precision, and confusion matrix. ``` Input Format - The function should accept a string `csv_file_path`, which is the file path to the input data CSV. Output Format - Print the recall, precision, and confusion matrix of the classifier on the test set in the following format: ``` Recall: <recall_value> Precision: <precision_value> Confusion Matrix: [[true_negatives, false_positives], [false_negatives, true_positives]] ``` Constraints - Use scikit-learn\'s `LogisticRegression` for the initial classifier. - Perform a 80-20 split for training and testing. - Ensure that the tuning of the threshold is done to maximize recall. Example Given a CSV file `patients.csv`, execute the function: ```python tune_threshold_and_evaluate(\'patients.csv\') ``` Expected Output (values will vary based on the actual data): ``` Recall: 0.95 Precision: 0.65 Confusion Matrix: [[50, 20], [2, 38]] ``` This problem assesses your understanding of tuning decision thresholds using scikit-learn, achieving specific evaluation metrics, and working with real-world constraints.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import recall_score, precision_score, confusion_matrix from sklearn.model_selection import StratifiedKFold import numpy as np class TunedThresholdClassifierCV: def __init__(self, base_clf, num_folds=5): self.base_clf = base_clf self.num_folds = num_folds self.best_threshold = 0.5 def fit(self, X, y): skf = StratifiedKFold(n_splits=self.num_folds) thresholds = np.linspace(0, 1, 101) best_recall = 0 for train_index, val_index in skf.split(X, y): X_train, X_val = X[train_index], X[val_index] y_train, y_val = y[train_index], y[val_index] self.base_clf.fit(X_train, y_train) y_proba = self.base_clf.predict_proba(X_val)[:, 1] for threshold in thresholds: y_pred = (y_proba >= threshold).astype(int) recall = recall_score(y_val, y_pred) if recall > best_recall: best_recall = recall self.best_threshold = threshold self.base_clf.fit(X, y) def predict(self, X): y_proba = self.base_clf.predict_proba(X)[:, 1] return (y_proba >= self.best_threshold).astype(int) def tune_threshold_and_evaluate(csv_file_path: str) -> None: data = pd.read_csv(csv_file_path) X = data.drop(columns=[\'outcome\']).values y = data[\'outcome\'].values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) base_clf = LogisticRegression(solver=\'liblinear\', random_state=42) clf = TunedThresholdClassifierCV(base_clf) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) recall = recall_score(y_test, y_pred) precision = precision_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) print(f\\"Recall: {recall:.2f}\\") print(f\\"Precision: {precision:.2f}\\") print(f\\"Confusion Matrix:n{conf_matrix}\\")"},{"question":"You are given a directory structure with various files and subdirectories. Your task is to write a Python function that: 1. Searches for files within a given directory, matching a specific pattern. 2. Uses the `glob` module to handle the pattern matching. 3. Optionally searches recursively through all subdirectories if specified. 4. Returns a list of matched file paths. # Function Signature ```python def find_files(directory: str, pattern: str, recursive: bool = False) -> list: pass ``` # Input Format - `directory`: A string that represents the path to the directory where the search should begin. - `pattern`: A string that specifies the pattern to match filenames against. - `recursive`: A boolean value (default `False`), which if set to `True`, the search should include all subdirectories recursively. # Output Format - The function should return a list of file paths that match the given pattern. # Constraints - The directory specified by `directory` will always exist. - Patterns will follow Unix shell-style matching rules. # Example Consider the directory structure: ``` test_dir/ ├── a.txt ├── b.gif ├── subdir/ ├── c.txt ``` 1. If the function is called with `directory=\'test_dir\'`, `pattern=\'*.txt\'`, `recursive=False`, the output should be: ```python [\'./a.txt\'] ``` 2. If the function is called with `directory=\'test_dir\'`, `pattern=\'*.txt\'`, `recursive=True`, the output should be: ```python [\'./a.txt\', \'./subdir/c.txt\'] ``` # Notes - Ensure to use `glob.glob()` for matching files. - Handle file paths correctly to maintain their relative structures. - Test your function to handle edge cases such as empty directories or no matches found.","solution":"import glob import os def find_files(directory: str, pattern: str, recursive: bool = False) -> list: Searches for files within a given directory matching a specific pattern. :param directory: Path to the directory where the search begins. :param pattern: Pattern to match filenames against. :param recursive: If True, searches recursively through all subdirectories. :return: A list of matched file paths. if recursive: # Use **/ to match recursively through all subdirectories: search_pattern = os.path.join(directory, \'**\', pattern) matched_files = glob.glob(search_pattern, recursive=True) else: search_pattern = os.path.join(directory, pattern) matched_files = glob.glob(search_pattern) return matched_files"},{"question":"# Seaborn Coding Assessment Question **Objective:** Demonstrate your understanding of Seaborn\'s advanced plotting capabilities, particularly using the `seaborn.objects` module and KDE for density estimation. **Problem Statement:** You are provided with the `penguins` dataset from Seaborn\'s library. Your task is to create several plots to visualize the distribution of `flipper_length_mm` across different categories in the dataset. Follow the specific instructions below for each plot: 1. **Basic Density Plot**: Write a function `basic_density_plot` that creates a basic density plot of `flipper_length_mm`. - Input: A DataFrame (`penguins` dataset). - Output: Display the density plot using KDE with default settings. ```python def basic_density_plot(penguins): # Your code here ``` 2. **Adjusted Bandwidth Density Plot**: Write a function `adjusted_bandwidth_density_plot` that creates a density plot of `flipper_length_mm` with a bandwidth adjustment factor of 0.25. - Input: A DataFrame (`penguins` dataset). - Output: Display the density plot with the specified bandwidth adjustment. ```python def adjusted_bandwidth_density_plot(penguins): # Your code here ``` 3. **Density Plot with Histogram**: Write a function `density_plot_with_histogram` that overlays a KDE density curve on top of a histogram of `flipper_length_mm`. - Input: A DataFrame (`penguins` dataset). - Output: Display the histogram with the KDE density curve overlay. ```python def density_plot_with_histogram(penguins): # Your code here ``` 4. **Grouped Density Plot**: Write a function `grouped_density_plot` that creates density plots of `flipper_length_mm` for each species in the dataset, using different colors for each species. - Input: A DataFrame (`penguins` dataset). - Output: Display the grouped density plot. ```python def grouped_density_plot(penguins): # Your code here ``` 5. **Conditional Density Plot**: Write a function `conditional_density_plot` that creates density plots of `flipper_length_mm` conditioned on the `sex` of the penguins. Each plot should display conditional densities, not normalized across groups. - Input: A DataFrame (`penguins` dataset). - Output: Display the conditional density plot for `sex`. ```python def conditional_density_plot(penguins): # Your code here ``` 6. **Cumulative Density Plot**: Write a function `cumulative_density_plot` that creates a cumulative density plot of `flipper_length_mm`. - Input: A DataFrame (`penguins` dataset). - Output: Display the cumulative density plot. ```python def cumulative_density_plot(penguins): # Your code here ``` # Constraints and Considerations: - Your functions should be well-documented and use appropriate variable names. - Ensure all plots are appropriately labeled with titles and axis labels. - Make sure to handle any missing data in the `penguins` dataset before plotting. - Aim for clear and visually appealing plots. # Example Usage: ```python import seaborn as sns from seaborn import load_dataset penguins = load_dataset(\\"penguins\\") # Call the functions basic_density_plot(penguins) adjusted_bandwidth_density_plot(penguins) density_plot_with_histogram(penguins) grouped_density_plot(penguins) conditional_density_plot(penguins) cumulative_density_plot(penguins) ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def basic_density_plot(penguins): Creates a basic density plot of flipper_length_mm. sns.kdeplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\\"Basic Density Plot of Flipper Length\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Density\\") plt.show() def adjusted_bandwidth_density_plot(penguins): Creates a density plot of flipper_length_mm with a bandwidth adjustment factor of 0.25. sns.kdeplot(data=penguins, x=\\"flipper_length_mm\\", bw_adjust=0.25) plt.title(\\"Density Plot with Bandwidth Adjustment 0.25\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Density\\") plt.show() def density_plot_with_histogram(penguins): Overlays a KDE density curve on top of a histogram of flipper_length_mm. sns.histplot(penguins, x=\\"flipper_length_mm\\", kde=True) plt.title(\\"Density Plot with Histogram\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Count\\") plt.show() def grouped_density_plot(penguins): Creates density plots of flipper_length_mm for each species, using different colors for each species. sns.kdeplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\") plt.title(\\"Density Plot of Flipper Length by Species\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Density\\") plt.show() def conditional_density_plot(penguins): Creates density plots of flipper_length_mm conditioned on the sex of the penguins. sns.displot(penguins, x=\\"flipper_length_mm\\", hue=\\"sex\\", kind=\\"kde\\") plt.title(\\"Conditional Density Plot by Sex\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Density\\") plt.show() def cumulative_density_plot(penguins): Creates a cumulative density plot of flipper_length_mm. sns.kdeplot(data=penguins, x=\\"flipper_length_mm\\", cumulative=True) plt.title(\\"Cumulative Density Plot of Flipper Length\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Cumulative Density\\") plt.show()"},{"question":"**Title:** PCA Decomposition and Incremental PCA Application **Objective:** Demonstrate understanding and application of PCA and Incremental PCA using scikit-learn. Problem Statement You are provided with a large dataset that cannot fit into memory all at once. Your task is to implement two functions utilizing scikit-learn to perform Principal Component Analysis (PCA) in two different ways: batch-wise using IncrementalPCA and all-at-once using standard PCA for comparison. Your implementation should handle data in chunks for the IncrementalPCA method and provide a memory-efficient solution. Function Signature ```python def batch_pca_transform(file_path: str, n_components: int, chunk_size: int) -> np.ndarray: Perform PCA on the dataset using IncrementalPCA in a memory-efficient way. Args: - file_path (str): Path to the CSV file containing the dataset. - n_components (int): Number of principal components to extract. - chunk_size (int): Number of rows to process at a time. Returns: - np.ndarray: Transformed dataset with reduced dimensions. def full_pca_transform(file_path: str, n_components: int) -> np.ndarray: Perform PCA on the dataset using the full dataset loaded into memory. Args: - file_path (str): Path to the CSV file containing the dataset. - n_components (int): Number of principal components to extract. Returns: - np.ndarray: Transformed dataset with reduced dimensions. ``` Specifications 1. **Data Reading:** The dataset is stored in a CSV file, where each row represents a sample and each column represents a feature. 2. **Function 1:** `batch_pca_transform` - Implement using `IncrementalPCA` from scikit-learn. - Read the dataset in chunks (specified by `chunk_size`). - Fit the IncrementalPCA model incrementally on each chunk. - Transform the entire dataset into the reduced dimension space using the fitted model. 3. **Function 2:** `full_pca_transform` - Implement using `PCA` from scikit-learn. - Load the entire dataset into memory. - Fit the PCA model on the full dataset. - Transform the entire dataset into the reduced dimension space using the fitted model. 4. **Performance:** Ensure the `batch_pca_transform` method performs efficiently even with large datasets that exceed memory limits. Constraints - The dataset file is large and cannot be entirely loaded into memory in some scenarios. - The input file path will always point to a valid CSV file. - The CSV file does not contain any missing values. Example Assume `data.csv` contains the dataset with 100,000 samples and 500 features. ```python file_path = \\"data.csv\\" n_components = 10 chunk_size = 10000 batch_transformed_data = batch_pca_transform(file_path, n_components, chunk_size) full_transformed_data = full_pca_transform(file_path, n_components) print(batch_transformed_data.shape) # Should output (100000, 10) print(full_transformed_data.shape) # Should output (100000, 10) ``` **Note:** Ensure your implementation handles edge cases such as chunk size not perfectly dividing the total number of samples.","solution":"import pandas as pd import numpy as np from sklearn.decomposition import IncrementalPCA, PCA def batch_pca_transform(file_path: str, n_components: int, chunk_size: int) -> np.ndarray: Perform PCA on the dataset using IncrementalPCA in a memory-efficient way. Args: - file_path (str): Path to the CSV file containing the dataset. - n_components (int): Number of principal components to extract. - chunk_size (int): Number of rows to process at a time. Returns: - np.ndarray: Transformed dataset with reduced dimensions. ipca = IncrementalPCA(n_components=n_components) data_iter = pd.read_csv(file_path, chunksize=chunk_size) # Partial fitting on chunks for chunk in data_iter: ipca.partial_fit(chunk) transformed_data = [] data_iter = pd.read_csv(file_path, chunksize=chunk_size) # Transforming chunks for chunk in data_iter: transformed_chunk = ipca.transform(chunk) transformed_data.append(transformed_chunk) return np.vstack(transformed_data) def full_pca_transform(file_path: str, n_components: int) -> np.ndarray: Perform PCA on the dataset using the full dataset loaded into memory. Args: - file_path (str): Path to the CSV file containing the dataset. - n_components (int): Number of principal components to extract. Returns: - np.ndarray: Transformed dataset with reduced dimensions. data = pd.read_csv(file_path) pca = PCA(n_components=n_components) transformed_data = pca.fit_transform(data) return transformed_data"},{"question":"**Problem Statement: COVID-19 Data Analysis** You are provided with a dataset containing information about daily COVID-19 cases across various countries. This dataset includes the following columns: - `date`: The date of the observation. - `country`: The name of the country. - `cases`: The number of new daily COVID-19 cases reported. - `deaths`: The number of new daily deaths due to COVID-19 reported. Your task is to implement several functions to perform a detailed analysis of this dataset using the pandas library. # Input You will be provided with a CSV file `covid_data.csv`: ```plaintext date,country,cases,deaths 2020-01-22,Afghanistan,0,0 2020-01-23,Afghanistan,0,0 ... ``` # Outputs 1. **Function 1: read_csv_data(filename: str) -> pd.DataFrame** This function should read the CSV file and return a pandas DataFrame. 2. **Function 2: preprocess_data(df: pd.DataFrame) -> pd.DataFrame** This function should preprocess the DataFrame: - Ensure that the `date` column is of datetime type. - Fill any missing values (`NaN`) in the `cases` and `deaths` columns with 0. 3. **Function 3: get_top_countries_by_cases(df: pd.DataFrame, n: int) -> pd.DataFrame** This function should return the top `n` countries with the highest total number of cases in the dataset. The output DataFrame should include the `country` and `total_cases` columns. 4. **Function 4: calculate_rolling_average(df: pd.DataFrame, window: int) -> pd.DataFrame** This function should add a column `rolling_avg_cases` to the DataFrame, which contains the rolling average of daily cases over a window of the past `window` days, for each country. 5. **Function 5: plot_country_cases(df: pd.DataFrame, country: str)** This function should generate a plot showing the daily cases and the rolling average of cases for the specified country. # Constraints - Ensure the data manipulations do not affect the original data until required. - Handle missing data by filling with zero. - Use appropriate pandas functions and methods for these operations. # Examples Below is an example of using the functions: ```python import pandas as pd # Function 1 df = read_csv_data(\'covid_data.csv\') # Function 2 df = preprocess_data(df) # Function 3 top_countries = get_top_countries_by_cases(df, 5) print(top_countries) # Function 4 df = calculate_rolling_average(df, 7) print(df.head()) # Function 5 plot_country_cases(df, \'Afghanistan\') ``` Make sure to test your functions thoroughly and document your code for clarity.","solution":"import pandas as pd import matplotlib.pyplot as plt def read_csv_data(filename: str) -> pd.DataFrame: Reads a CSV file and returns a pandas DataFrame. Args: filename (str): The path to the CSV file. Returns: pd.DataFrame: The resulting DataFrame. return pd.read_csv(filename) def preprocess_data(df: pd.DataFrame) -> pd.DataFrame: Preprocesses the DataFrame by ensuring the date column is datetime type and filling NaN values in cases and deaths columns with 0. Args: df (pd.DataFrame): The input dataframe. Returns: pd.DataFrame: The preprocessed DataFrame. df[\'date\'] = pd.to_datetime(df[\'date\']) df[\'cases\'] = df[\'cases\'].fillna(0) df[\'deaths\'] = df[\'deaths\'].fillna(0) return df def get_top_countries_by_cases(df: pd.DataFrame, n: int) -> pd.DataFrame: Returns the top n countries with the highest total number of cases. Args: df (pd.DataFrame): The input DataFrame. n (int): The number of top countries to return. Returns: pd.DataFrame: DataFrame with top n countries by total cases. total_cases = df.groupby(\'country\')[\'cases\'].sum() top_countries = total_cases.nlargest(n).reset_index(name=\'total_cases\') return top_countries def calculate_rolling_average(df: pd.DataFrame, window: int) -> pd.DataFrame: Adds a column with the rolling average of daily cases over a specified window of days for each country. Args: df (pd.DataFrame): The input DataFrame. window (int): The rolling window size in days. Returns: pd.DataFrame: DataFrame with an additional column `rolling_avg_cases`. df = df.sort_values(by=[\'country\', \'date\']) df[\'rolling_avg_cases\'] = df.groupby(\'country\')[\'cases\'].rolling(window=window).mean().reset_index(level=0, drop=True) return df def plot_country_cases(df: pd.DataFrame, country: str): Generates a plot showing the daily cases and the rolling average of cases for a specific country. Args: df (pd.DataFrame): The input DataFrame. country (str): The country to plot data for. country_df = df[df[\'country\'] == country] plt.figure(figsize=(10, 5)) plt.plot(country_df[\'date\'], country_df[\'cases\'], label=\'Daily Cases\') plt.plot(country_df[\'date\'], country_df[\'rolling_avg_cases\'], label=\'7-Day Rolling Avg\', linestyle=\'--\') plt.title(f\'COVID-19 Daily Cases and Rolling Average for {country}\') plt.xlabel(\'Date\') plt.ylabel(\'Number of Cases\') plt.legend() plt.show()"},{"question":"# Advanced Python Coding Assessment Background The `traceback` module in Python provides a robust set of functionalities to manipulate and present stack trace information. Understanding how to use this module effectively can help you debug and resolve issues with complex programs by providing detailed stack trace information. Task You are required to implement a function `detailed_traceback_report` that generates a detailed traceback report of a given function when an exception occurs. This report should include: 1. The stack trace. 2. Exception details including the type and value. 3. Locals available at each frame. 4. Proper formatting to make the report readable. Additionally, implement a helper function `sample_function` which will deliberately raise an exception to test `detailed_traceback_report`. Function Signatures ```python def detailed_traceback_report(func): Executes a function and provides a detailed traceback report in case of exception. Parameters: func (callable): The target function to execute. Returns: str: A formatted string containing the detailed traceback report. pass def sample_function(): A sample function that will raise an exception. Raises: ValueError: An example exception to demonstrate traceback. pass ``` Requirements 1. Use the `traceback` module to capture the stack trace and exception details. 2. Ensure all locals in each frame are captured and displayed. 3. Format the report in a readable manner, ideally similar to the built-in traceback but with additional local variable information. 4. The `sample_function` should raise a deliberate exception, e.g., by attempting to access an invalid index of a list. 5. Do not use any third-party libraries. Example Here is an example usage of your implementation: ```python try: detailed_traceback_report(sample_function) except Exception as e: print(e) ``` The output should contain: - The full stack trace. - Exception type and message. - Local variables in each frame involved in the traceback. Constraints - `func` is guaranteed to be a callable that may raise exceptions. - You can assume Python 3.10+ for your implementation. Good luck!","solution":"import traceback import sys def detailed_traceback_report(func): Executes a function and provides a detailed traceback report in case of exception. Parameters: func (callable): The target function to execute. Returns: str: A formatted string containing the detailed traceback report. try: func() except Exception: exc_type, exc_value, exc_tb = sys.exc_info() tb_lines = traceback.format_exception(exc_type, exc_value, exc_tb) detailed_report = [] for line in tb_lines: detailed_report.append(line) tb = exc_tb while tb is not None: frame = tb.tb_frame lineno = tb.tb_lineno co = frame.f_code filename = co.co_filename name = co.co_name detailed_report.append(f\' File \\"{filename}\\", line {lineno}, in {name}n\') locals_list = [] for key, value in frame.f_locals.items(): locals_list.append(f\\" {key} = {value!r}\\") if locals_list: detailed_report.append(\\" Local variables:n\\") detailed_report.extend(locals_list) detailed_report.append(\\"n\\") tb = tb.tb_next return \'\'.join(detailed_report) def sample_function(): A sample function that will raise an exception. Raises: ValueError: An example exception to demonstrate traceback. numbers = [1, 2, 3] # This will raise an IndexError print(numbers[5])"},{"question":"# Complex Number Operations Objective: Implement a Python class `PythonComplex` that mimics complex number operations using the Python C API functions provided in the documentation. This class should support basic operations such as addition, subtraction, negation, multiplication, division, and exponentiation by leveraging the underlying C functions. Class Specification: ```python class PythonComplex: def __init__(self, real: float, imag: float): Initialize with real and imaginary parts. pass def __add__(self, other: \'PythonComplex\') -> \'PythonComplex\': Return the sum of self and other. pass def __sub__(self, other: \'PythonComplex\') -> \'PythonComplex\': Return the difference between self and other. pass def __neg__(self) -> \'PythonComplex\': Return the negation of self. pass def __mul__(self, other: \'PythonComplex\') -> \'PythonComplex\': Return the product of self and other. pass def __truediv__(self, other: \'PythonComplex\') -> \'PythonComplex\': Return the division of self by other. pass def __pow__(self, power: \'PythonComplex\') -> \'PythonComplex\': Return self raised to the power. pass def __repr__(self): Return a string representation of the complex number. pass ``` Requirements: 1. **Initialization**: Implement the `__init__` method to initialize the complex number with `real` and `imag` parts. 2. **Addition**: Implement the `__add__` method to return the sum of two complex numbers using `_Py_c_sum`. 3. **Subtraction**: Implement the `__sub__` method to return the difference between two complex numbers using `_Py_c_diff`. 4. **Negation**: Implement the `__neg__` method to return the negation of a complex number using `_Py_c_neg`. 5. **Multiplication**: Implement the `__mul__` method to return the product of two complex numbers using `_Py_c_prod`. 6. **Division**: Implement the `__truediv__` method to return the quotient of two complex numbers using `_Py_c_quot`. 7. **Exponentiation**: Implement the `__pow__` method to return the result of raising a complex number to another using `_Py_c_pow`. 8. **Representation**: Implement the `__repr__` method to return a string representation of the complex number in the form \\"(real + imagj)\\". Input and Output Formats: - Input: - For initialization: `(real, imag)` where `real` and `imag` are floats. - For operations: Two instances of `PythonComplex`. - Output: - For operations: A new instance of `PythonComplex` representing the result. - For representation: String format of the complex number. Constraints: - Ensure the `other` object in all operations is also an instance of `PythonComplex`. - Handle potential division by zero appropriately by raising a `ZeroDivisionError`. Example Usage: ```python num1 = PythonComplex(1, 2) num2 = PythonComplex(3, 4) print(num1 + num2) # Output: (4.0 + 6.0j) print(num1 - num2) # Output: (-2.0 - 2.0j) print(-num1) # Output: (-1.0 - 2.0j) print(num1 * num2) # Output: (-5.0 + 10.0j) print(num1 / num2) # Output: (0.44 + 0.08j) print(num1 ** num2) # Output: (something complex based on pow calculation) ```","solution":"class PythonComplex: def __init__(self, real: float, imag: float): Initialize with real and imaginary parts. self.real = real self.imag = imag def __add__(self, other: \'PythonComplex\') -> \'PythonComplex\': Return the sum of self and other. return PythonComplex(self.real + other.real, self.imag + other.imag) def __sub__(self, other: \'PythonComplex\') -> \'PythonComplex\': Return the difference between self and other. return PythonComplex(self.real - other.real, self.imag - other.imag) def __neg__(self) -> \'PythonComplex\': Return the negation of self. return PythonComplex(-self.real, -self.imag) def __mul__(self, other: \'PythonComplex\') -> \'PythonComplex\': Return the product of self and other. real = self.real * other.real - self.imag * other.imag imag = self.real * other.imag + self.imag * other.real return PythonComplex(real, imag) def __truediv__(self, other: \'PythonComplex\') -> \'PythonComplex\': Return the division of self by other. if other.real == 0 and other.imag == 0: raise ZeroDivisionError(\\"division by zero\\") denom = other.real ** 2 + other.imag ** 2 real = (self.real * other.real + self.imag * other.imag) / denom imag = (self.imag * other.real - self.real * other.imag) / denom return PythonComplex(real, imag) def __pow__(self, power: \'PythonComplex\') -> \'PythonComplex\': Return self raised to the power. # For simplicity, use Python\'s built-in complex type for power operation base = complex(self.real, self.imag) exponent = complex(power.real, power.imag) result = base ** exponent return PythonComplex(result.real, result.imag) def __repr__(self): Return a string representation of the complex number. return f\\"({self.real} + {self.imag}j)\\""},{"question":"You are tasked with designing a task management system with various task states using python\'s `enum` module. The states of the tasks must not overlap and should be compared in a specific order. Additionally, each task state should have a unique color code associated with it. # Guidelines: 1. **Create an Enum Class**: Define an enumeration `TaskState` with the following states and their corresponding color codes: - TODO (\\"Red\\") - IN_PROGRESS (\\"Yellow\\") - REVIEW (\\"Blue\\") - DONE (\\"Green\\") 2. **Maintain Order**: The states should maintain the order defined above such that TODO < IN_PROGRESS < REVIEW < DONE. 3. **Unique Constraints**: Ensure no two states have the same color code within your enum. 4. **Functionalities**: - A method to compare two tasks to determine which one has a higher state. - Retrieve the color code of a specific state. # Function to Implement ```python from enum import Enum, unique, auto @unique class TaskState(Enum): # Define states here with auto() def __init__(self, color): # Initialization logic here to map color @classmethod def compare_states(cls, state1, state2): Compare two states and returns: -1 if state1 < state2 1 if state1 > state2 0 if state1 == state2 # Implement comparison logic here def get_color_code(self): Return the color code associated with the task state # Implement retrieval logic here ``` # Example: ```python ts1 = TaskState.TODO ts2 = TaskState.REVIEW assert TaskState.compare_states(ts1, ts2) == -1 # TODO is less than REVIEW assert ts1.get_color_code() == \\"Red\\" # Color code of TODO state assert ts2.get_color_code() == \\"Blue\\" # Color code of REVIEW state ``` # Constraints: - The states ordered TODO < IN_PROGRESS < REVIEW < DONE must strictly be maintained. - No two states should share the same color code. Ensure that your solution adheres strictly to the guidelines specified.","solution":"from enum import Enum, unique @unique class TaskState(Enum): TODO = (\\"Red\\", 1) IN_PROGRESS = (\\"Yellow\\", 2) REVIEW = (\\"Blue\\", 3) DONE = (\\"Green\\", 4) def __init__(self, color, order): self.color = color self.order = order @classmethod def compare_states(cls, state1, state2): Compare two states and returns: -1 if state1 < state2 1 if state1 > state2 0 if state1 == state2 if state1.order < state2.order: return -1 elif state1.order > state2.order: return 1 else: return 0 def get_color_code(self): Return the color code associated with the task state return self.color"},{"question":"# Question: Sparse Data Analysis and Conversion **Objective:** You will implement functions to analyze sparse data and convert it between different formats using pandas\' sparse data structures. This will help you demonstrate your understanding of creating, manipulating, and converting sparse data efficiently. **Problem Statement:** 1. **Create a Sparse DataFrame:** Implement a function `create_sparse_dataframe` that takes an integer `n` and a floating-point `density`, and returns a pandas SparseDataFrame of size `n x n` with random values between `0` and `1`. The density parameter should control the fraction of non-zero elements in the dataframe. 2. **Analyze Sparse Data:** Implement a function `analyze_sparse_dataframe` that takes a pandas SparseDataFrame as input and returns a dictionary with the following keys: - `\'non_zero_count\'`: Total number of non-zero (non-missing) elements. - `\'density\'`: The density of the sparse dataframe (i.e., the fraction of non-zero elements). - `\'memory_usage\'`: Memory usage of the sparse dataframe. 3. **Convert Sparse to Dense:** Implement a function `sparse_to_dense` that takes a pandas SparseDataFrame and returns its dense equivalent (regular DataFrame). 4. **Convert Dense to Sparse:** Implement a function `dense_to_sparse` that takes a regular pandas DataFrame and returns its sparse equivalent, assuming the fill value is zero. **Function Definitions:** ```python import pandas as pd import numpy as np def create_sparse_dataframe(n: int, density: float) -> pd.DataFrame: # Your code here pass def analyze_sparse_dataframe(sdf: pd.DataFrame) -> dict: # Your code here pass def sparse_to_dense(sdf: pd.DataFrame) -> pd.DataFrame: # Your code here pass def dense_to_sparse(df: pd.DataFrame) -> pd.DataFrame: # Your code here pass ``` **Constraints:** - `1 <= n <= 1000` - `0.0 <= density <= 1.0` - Avoid generating a DataFrame with no non-zero elements. **Example Usage:** ```python # Example 1: Create a Sparse DataFrame sdf = create_sparse_dataframe(n=10, density=0.2) print(sdf) # Example 2: Analyze Sparse DataFrame analysis = analyze_sparse_dataframe(sdf) print(analysis) # Example 3: Convert Sparse to Dense dense_df = sparse_to_dense(sdf) print(dense_df) # Example 4: Convert Dense to Sparse sparse_df = dense_to_sparse(dense_df) print(sparse_df) ``` **Evaluation:** Your functions will be evaluated based on correctness, efficiency, and the ability to handle edge cases within the constraints provided.","solution":"import pandas as pd import numpy as np def create_sparse_dataframe(n: int, density: float) -> pd.DataFrame: if density < 0 or density > 1: raise ValueError(\\"Density must be between 0 and 1\\") if n < 1 or n > 1000: raise ValueError(\\"n must be between 1 and 1000\\") # Create a dense array with the appropriate number of non-zero elements size = n * n num_non_zeros = int(size * density) num_zeros = size - num_non_zeros # Avoid generating a DataFrame with all zeros if num_non_zeros == 0: num_non_zeros = 1 num_zeros -= 1 values = np.array([0] * num_zeros + list(np.random.rand(num_non_zeros))) np.random.shuffle(values) # Reshape to the required dimensions dense_array = values.reshape((n, n)) # Create and return a sparse dataframe sdf = pd.DataFrame(dense_array).astype(pd.SparseDtype(\\"float\\", 0.0)) return sdf def analyze_sparse_dataframe(sdf: pd.DataFrame) -> dict: non_zero_count = sdf.astype(bool).sum().sum() total_elements = sdf.shape[0] * sdf.shape[1] density = non_zero_count / total_elements memory_usage = sdf.memory_usage(deep=True).sum() return { \'non_zero_count\': non_zero_count, \'density\': density, \'memory_usage\': memory_usage } def sparse_to_dense(sdf: pd.DataFrame) -> pd.DataFrame: return sdf.sparse.to_dense() def dense_to_sparse(df: pd.DataFrame) -> pd.DataFrame: return df.astype(pd.SparseDtype(\\"float\\", 0.0))"},{"question":"# Asynchronous Chat Room Server Using the `asyncio` streams in Python, design and implement a basic asynchronous chat room server. Multiple clients should be able to connect to this server, and any message sent by one client should be broadcasted to all other connected clients. Requirements: 1. **Start the Server:** - Use `asyncio.start_server` to start the chat server that can handle multiple client connections. - The server should be listening on a specific host (e.g., \'127.0.0.1\') and port (e.g., 8888). 2. **Handle Client Connections:** - For each client connection, you should stream incoming data from the client. - Each message should be broadcasted to all other connected clients. - Ensure the server is non-blocking and can handle multiple clients asynchronously. 3. **Client Message Handling:** - For simplicity, each message should end with a newline character (`\'n\'`). - Read full lines of text sent by clients using `StreamReader.readline()` and write messages to all clients using `StreamWriter.write()` and `StreamWriter.drain()`. 4. **Connection Management:** - Keep track of all connected clients. - Correctly handle clients disconnecting from the server. Constraints: - The server should handle up to 100 simultaneous connections. - Each message can be up to 256 characters in length. Input and Output: - **Input:** Clients send lines of text to the server. - **Output:** Server broadcasts each received message to all connected clients. Example: Suppose we start the server on `127.0.0.1:8888`. Two clients connect to the server: - Client 1 sends \\"Hello from Client 1n\\". - The server receives the message and broadcasts it to all clients, including Client 1. - Client 2 sends \\"Hello from Client 2n\\". - The server receives and broadcasts it to all clients, including Client 2. Here is the template to start working on the solution: ```python import asyncio clients = [] async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') print(f\\"Client connected: {addr}\\") clients.append(writer) try: while True: data = await reader.readline() if not data: break message = data.decode() print(f\\"Received {message.strip()} from {addr}\\") for client in clients: if client != writer: client.write(data) await client.drain() except asyncio.CancelledError: pass finally: print(f\\"Client disconnected: {addr}\\") clients.remove(writer) writer.close() await writer.wait_closed() async def main(): server = await asyncio.start_server( handle_client, \'127.0.0.1\', 8888) addrs = \', \'.join(str(sock.getsockname()) for sock in server.sockets) print(f\'Serving on {addrs}\') async with server: await server.serve_forever() asyncio.run(main()) ``` --- Implement any additional functionality you deem necessary, but ensure the core requirements are met.","solution":"import asyncio clients = [] async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') print(f\\"Client connected: {addr}\\") clients.append(writer) try: while True: data = await reader.readline() if not data: break message = data.decode() print(f\\"Received {message.strip()} from {addr}\\") for client in clients: if client != writer: client.write(data) await client.drain() except asyncio.CancelledError: pass finally: print(f\\"Client disconnected: {addr}\\") clients.remove(writer) writer.close() await writer.wait_closed() async def main(): server = await asyncio.start_server( handle_client, \'127.0.0.1\', 8888) addrs = \', \'.join(str(sock.getsockname()) for sock in server.sockets) print(f\'Serving on {addrs}\') async with server: await server.serve_forever() if __name__ == \'__main__\': asyncio.run(main())"},{"question":"Objective: Demonstrate your understanding of tensor creation, manipulation, random sampling, and gradient computations using PyTorch. Problem Statement: You are required to implement a function `compute_gradients_and_average` that performs the following steps: 1. **Tensor Creation and Initialization:** - Create a 3D tensor of shape (10, 20, 30) with random values sampled from a normal distribution. - Ensure that the tensor requires gradient computation. 2. **Tensor Manipulation:** - Compute the sum along the axis 0. - Compute the maximum along the axis 1 of the resulting tensor. 3. **Gradient Computation:** - Calculate the mean of the elements of the tensor obtained from the previous step. - Compute gradients of this mean value with respect to the original 3D tensor. 4. **Random Sampling:** - Create another tensor of the same shape (10, 20, 30) with random values sampled uniformly in the range [0, 1). - Compute the element-wise product of the two 3D tensors (one from step 1 and one from step 4). 5. **Result Computation:** - Calculate the average value of the resultant tensor from the previous step. Function Signature: ```python import torch def compute_gradients_and_average(): # Step 1: Tensor Creation and Initialization original_tensor = torch.randn(10, 20, 30, requires_grad=True) # Step 2: Tensor Manipulation sum_along_axis0 = torch.sum(original_tensor, dim=0) max_along_axis1 = torch.max(sum_along_axis0, dim=1).values # Step 3: Gradient Computation mean_value = torch.mean(max_along_axis1) mean_value.backward() # Step 4: Random Sampling another_tensor = torch.rand(10, 20, 30) elementwise_product = original_tensor * another_tensor # Step 5: Result Computation average_value = torch.mean(elementwise_product) return average_value.item(), original_tensor.grad average_value, gradients = compute_gradients_and_average() print(f\\"Average Value: {average_value}\\") print(f\\"Gradients: {gradients}\\") ``` Expected Output: - The function should return: - The average value of the resultant tensor from step 5. - The gradients of the mean with respect to the original tensor created in step 1. Constraints: - Ensure your solution is efficient and leverages PyTorch functionalities effectively. - The tensor operations should be performed using PyTorch\'s built-in methods. - Properly handle gradient computation ensuring no unnecessary computations are performed outside the context blocks. Performance Requirements: - Your code should run efficiently and handle tensors of the stated size. - Aim to optimize tensor operations to minimize computation time.","solution":"import torch def compute_gradients_and_average(): # Step 1: Tensor Creation and Initialization original_tensor = torch.randn(10, 20, 30, requires_grad=True) # Step 2: Tensor Manipulation sum_along_axis0 = torch.sum(original_tensor, dim=0) max_along_axis1 = torch.max(sum_along_axis0, dim=1).values # Step 3: Gradient Computation mean_value = torch.mean(max_along_axis1) mean_value.backward() # Step 4: Random Sampling another_tensor = torch.rand(10, 20, 30) elementwise_product = original_tensor * another_tensor # Step 5: Result Computation average_value = torch.mean(elementwise_product) return average_value.item(), original_tensor.grad average_value, gradients = compute_gradients_and_average() print(f\\"Average Value: {average_value}\\") print(f\\"Gradients: {gradients}\\")"},{"question":"# Coding Assessment Task: Implement a Thread-Safe Queue Objective: Implement a thread-safe queue using Python\'s `threading` module. The queue should support multiple producers and consumers without data corruption and ensure that all items are processed exactly once. Requirements: 1. **Class Definition:** - Define a class `ThreadSafeQueue` which has the following methods: - `enqueue(item)`: Adds an item to the queue. - `dequeue()`: Removes and returns an item from the queue. If the queue is empty, this method should block until an item is available. - `size()`: Returns the current size of the queue. 2. **Thread Synchronization:** - Use `threading.Condition` to manage synchronization within the queue operations. - Ensure that `enqueue` and `dequeue` are thread-safe and properly synchronized to handle multiple producer and consumer threads. 3. **Performance Requirements:** - The `enqueue` method should not block as long as there is space available in the queue. - The `dequeue` method should block if the queue is empty, until an item is enqueued. Example Usage: ```python import threading import time import random class ThreadSafeQueue: def __init__(self): # Your code here pass def enqueue(self, item): # Your code here pass def dequeue(self): # Your code here pass def size(self): # Your code here pass # Producer function def producer(queue, items): for item in items: print(f\\"Producing {item}\\") queue.enqueue(item) time.sleep(random.uniform(0.1, 0.5)) # Consumer function def consumer(queue, id): while True: item = queue.dequeue() if item is None: # Assuming None as a signal to stop break print(f\\"Consumer {id} consumed {item}\\") time.sleep(random.uniform(0.1, 0.5)) # Test the ThreadSafeQueue class if __name__ == \\"__main__\\": queue = ThreadSafeQueue() items_to_produce = [\'item1\', \'item2\', \'item3\', \'item4\', \'item5\', None, None] # None signals end-of-production producer_thread = threading.Thread(target=producer, args=(queue, items_to_produce)) consumer_threads = [threading.Thread(target=consumer, args=(queue, i)) for i in range(2)] producer_thread.start() for t in consumer_threads: t.start() producer_thread.join() for t in consumer_threads: t.join() ``` Constraints: - Do not use any built-in or third-party thread-safe queue implementations (e.g., `queue.Queue`). - The solution should make use of `threading.Condition`. Evaluation: - Correct implementation of thread-safe `enqueue` and `dequeue` methods. - Proper use of `Condition` for managing producer-consumer synchronization. - Graceful handling of multiple producer and consumer threads. - Code readability and adherence to Python\'s coding conventions.","solution":"import threading class ThreadSafeQueue: def __init__(self): self.queue = [] self.condition = threading.Condition() def enqueue(self, item): with self.condition: self.queue.append(item) self.condition.notify() # Notify one waiting thread def dequeue(self): with self.condition: while not self.queue: self.condition.wait() # Wait until a new item is added return self.queue.pop(0) def size(self): with self.condition: return len(self.queue)"},{"question":"# Clustering and Dimensionality Reduction Assessment Problem Statement You are provided with a dataset containing features of various consumer products. Your task is to apply clustering and dimensionality reduction techniques using scikit-learn to identify groups of similar products and visualize these clusters in a two-dimensional space. Instructions 1. **Data Loading and Preparation:** - Generate a synthetic dataset using `make_blobs` function from `sklearn.datasets` with 300 samples and 4 centers. Set the random state to 42 for reproducibility. - Standardize the dataset using `StandardScaler`. 2. **Clustering:** - Apply K-Means clustering from `sklearn.cluster` with the number of clusters set to 4. - Retrieve the cluster labels for each sample. 3. **Dimensionality Reduction:** - Apply PCA (Principal Component Analysis) from `sklearn.decomposition` to reduce the dataset to 2 dimensions. 4. **Visualization:** - Create a scatter plot of the PCA-transformed dataset, coloring each sample based on its cluster label. Function Signature ```python def cluster_and_visualize(): pass ``` Expected Output Your function should display a scatter plot showing the clusters. Example When you run your function, it should display a scatter plot with data points colored according to their assigned cluster. Each cluster should be visually distinguishable. Constraints - You must use only scikit-learn and matplotlib libraries for this task. - Ensure that your code is efficient and runs within a reasonable time frame. Grading Criteria - Correct usage of scikit-learn functions for data preparation, clustering, and dimensionality reduction. - Proper implementation of the scatter plot for visualization. - Clarity and readability of the code, including appropriate comments and variable naming conventions.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import make_blobs from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans from sklearn.decomposition import PCA def cluster_and_visualize(): # Step 1: Generate dataset X, _ = make_blobs(n_samples=300, centers=4, random_state=42) # Step 2: Standardize the dataset scaler = StandardScaler() X_std = scaler.fit_transform(X) # Step 3: Apply K-Means Clustering kmeans = KMeans(n_clusters=4, random_state=42) cluster_labels = kmeans.fit_predict(X_std) # Step 4: Apply PCA for dimensionality reduction pca = PCA(n_components=2) X_pca = pca.fit_transform(X_std) # Step 5: Create scatter plot for visualization plt.figure(figsize=(10, 7)) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap=\'viridis\') plt.title(\'Clusters of Products (PCA Reduced)\') plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.colorbar(label=\'Cluster Label\') plt.show()"},{"question":"Objective: You are to demonstrate your understanding of Python\'s functional programming capabilities by implementing a custom iterator and a generator, as well as utilizing `itertools` and `functools` library functions. Task: 1. **Custom Range Iterator** Implement a custom iterator class `CustomRange` that mimics the behavior of Python\'s built-in `range` function. Your iterator should: - Take three arguments: `start`, `stop`, and `step`. - Yield numbers from `start` to `stop` (excluding `stop`) with increments of `step`. 2. **Prime Number Generator** Implement a generator function `prime_numbers(limit)` that generates prime numbers up to a given limit. 3. **Enhanced Functional Program** Using your custom iterator and prime number generator, write a function `process_primes_and_range(N)` that: - Uses the `CustomRange` iterator to generate numbers from 1 to `N`. - Uses the `prime_numbers` generator to generate prime numbers up to `N`. - Combines these two sequences using `itertools.chain`. - Filters out even numbers using `itertools.filterfalse`. - Multiplies all remaining numbers using `functools.reduce`. Specifications: 1. **CustomRange Class:** - **Input:** `CustomRange(start, stop, step)` - **Output:** An iterator yielding numbers in the specified range. 2. **prime_numbers Function:** - **Input:** `limit` (an integer) - **Output:** A generator yielding prime numbers up to the given limit. 3. **process_primes_and_range Function:** - **Input:** `N` (an integer) - **Output:** An integer representing the product of all remaining numbers after filtering. Example: ```python # CustomRange Example cr = CustomRange(1, 10, 2) print(list(cr)) # Output: [1, 3, 5, 7, 9] # prime_numbers Example primes = prime_numbers(10) print(list(primes)) # Output: [2, 3, 5, 7] # process_primes_and_range Example result = process_primes_and_range(10) print(result) # Output will depend on the exact processing ``` Constraints and Requirements: - The `CustomRange` class must correctly handle any combination of positive and negative `start`, `stop`, and `step`. - The `prime_numbers` generator must be efficient in generating prime numbers. - Use `itertools.chain`, `itertools.filterfalse`, and `functools.reduce` appropriately in `process_primes_and_range`. # Submission - Implement `CustomRange`, `prime_numbers`, and `process_primes_and_range`. - Ensure your code is well-documented with comments explaining your logic.","solution":"import itertools import functools import math class CustomRange: def __init__(self, start, stop, step=1): self.start = start self.stop = stop self.step = step def __iter__(self): current = self.start if self.step > 0: while current < self.stop: yield current current += self.step else: while current > self.stop: yield current current += self.step def prime_numbers(limit): Generator to yield prime numbers up to the given limit. def is_prime(n): if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True for num in range(2, limit): if is_prime(num): yield num def process_primes_and_range(N): custom_range = CustomRange(1, N+1, 1) primes = prime_numbers(N) combined = itertools.chain(custom_range, primes) filtered = itertools.filterfalse(lambda x: x % 2 == 0, combined) product = functools.reduce(lambda x, y: x * y, filtered, 1) return product"},{"question":"Context As a PyTorch user, you often need to design data pipelines for training machine learning models. PyTorch provides the `torch.utils.data` module, which includes classes like `Dataset` and `DataLoader` to facilitate this process. In this assignment, you are required to implement a custom dataset and customize a data loader to handle a specific data-loading scenario. Problem Statement You are given a directory containing text files. Each text file contains multiple lines of numeric values separated by spaces. Your task is to: 1. Implement a custom dataset class `TextFileDataset` that reads these text files and provides batches of data as tensors. 2. Implement a custom batch processing function to collate the data into batches. Details 1. **Custom Dataset (`TextFileDataset`)**: - **Initialization**: The dataset should accept the directory path containing text files and a boolean `shuffle` parameter to specify whether to shuffle the data. - **Reading Data**: Read all numerical values from text files, convert them to floats, and store them in a list. - **Length**: Implement the `__len__` method to return the total number of data points. - **Get Item**: Implement `__getitem__` to fetch the i-th data point as a tensor. 2. **Custom Collate Function (`custom_collate_fn`)**: - **Input**: A batch of data points. - **Output**: A batch tensor created by stacking individual tensors from the batch. Expected Input and Output Formats - **Input to Dataset**: Directory path (str), and shuffle (bool). - **Output from Dataset**: Tensor batches via DataLoader. Constraints - Assume that the directory contains only text files. - Each file contains a valid set of numerical values separated by spaces. Implementation # Example usage of the Dataset and DataLoader: ```python import torch from torch.utils.data import DataLoader # Implement the custom dataset # Your implementation here # Implement the custom collate function # Your implementation here # Directory containing text files dir_path = \\"path/to/text/files\\" # Create the dataset dataset = TextFileDataset(dir_path, shuffle=True) # Create the DataLoader with the custom collate function data_loader = DataLoader(dataset, batch_size=4, collate_fn=custom_collate_fn) # Iterate through the DataLoader and print each batch for batch in data_loader: print(batch) ``` # Required Methods and Functions: 1. **TextFileDataset Class**: ```python class TextFileDataset(torch.utils.data.Dataset): def __init__(self, directory_path, shuffle): # Your code here def __len__(self): # Your code here def __getitem__(self, index): # Your code here ``` 2. **Custom Collate Function**: ```python def custom_collate_fn(batch): # Your code here ``` Provide the implementation for `TextFileDataset` and `custom_collate_fn` to complete the data pipeline.","solution":"import os import glob import torch from torch.utils.data import Dataset, DataLoader class TextFileDataset(Dataset): def __init__(self, directory_path, shuffle=False): Initializes the dataset by reading all text files in the directory, and storing the numeric data as a list of floats. :param directory_path: Directory containing the text files. :param shuffle: Whether to shuffle the data. self.data = [] self.directory_path = directory_path self.shuffle = shuffle # Read all text files in the directory for file_path in glob.glob(os.path.join(directory_path, \'*.txt\')): with open(file_path, \'r\') as file: for line in file: numbers = list(map(float, line.split())) self.data.extend(numbers) if self.shuffle: self.data = torch.tensor(self.data) self.data = self.data[torch.randperm(len(self.data))] self.data = self.data.tolist() def __len__(self): Returns the total number of data points. return len(self.data) def __getitem__(self, index): Retrieves the data point at the given index. :param index: Index of the data point. :return: Float tensor containing the data point. return torch.tensor(self.data[index], dtype=torch.float32) def custom_collate_fn(batch): Custom collate function to stack the batch of data points into a single tensor batch. :param batch: List of tensors to be included in the batch :return: Batch tensor return torch.stack(batch) # Example usage (uncomment the following lines to test with actual dataset): # dir_path = \\"path/to/text/files\\" # dataset = TextFileDataset(dir_path, shuffle=True) # data_loader = DataLoader(dataset, batch_size=4, collate_fn=custom_collate_fn) # for batch in data_loader: # print(batch)"},{"question":"# Question Your task is to write a command-line program using the `argparse` module that processes a text file. The program should offer the following functionalities: 1. **Positional Argument**: - `file_path`: The path to the text file to be processed. 2. **Optional Arguments**: - `--count-lines`: When specified, the program outputs the number of lines in the file. - `--count-words`: When specified, the program outputs the number of words in the file. - `--count-chars`: When specified, the program outputs the number of characters in the file. - `-v` or `--verbose`: When specified, the program runs in verbose mode, printing additional information about what it is doing. - Only one of `--count-lines`, `--count-words`, or `--count-chars` can be specified at a time. # Requirements: - **Input**: - The program should accept a positional argument for the file path. - The program should accept optional arguments for counting lines, words, or characters. - The program should reject scripts with more than one counting argument (`--count-lines`, `--count-words`, `--count-chars`) being specified simultaneously. - **Output**: - If in verbose mode, the program should print steps of what it is doing. - The program should print the count based on the specified optional argument. - If none of the count arguments are specified, the program should print a help message. # Example: Command: ``` python count.py example.txt --count-lines -v ``` Output: ``` Reading file example.txt... Number of lines: 10 ``` Command: ``` python count.py example.txt --count-words ``` Output: ``` Number of words: 50 ``` # Constraints: - If more than one of the counting arguments (`--count-lines`, `--count-words`, `--count-chars`) is specified, the program should display an appropriate error message and exit. - The file path provided must be valid and accessible; otherwise, the program should display an appropriate error message. # Notes: - Remember to handle errors such as file not found or permission issues gracefully. - Make sure to utilize the appropriate argparse functionalities to achieve the required command-line interface. # Implementation ```python import argparse def count_lines(file_path): with open(file_path, \'r\') as file: return sum(1 for _ in file) def count_words(file_path): with open(file_path, \'r\') as file: return sum(len(line.split()) for line in file) def count_chars(file_path): with open(file_path, \'r\') as file: return sum(len(line) for line in file) def main(): parser = argparse.ArgumentParser(description=\\"Process a text file to count lines, words, or characters.\\") parser.add_argument(\\"file_path\\", help=\\"Path to the text file to be processed\\") group = parser.add_mutually_exclusive_group(required=True) group.add_argument(\\"--count-lines\\", action=\\"store_true\\", help=\\"Count the number of lines in the file\\") group.add_argument(\\"--count-words\\", action=\\"store_true\\", help=\\"Count the number of words in the file\\") group.add_argument(\\"--count-chars\\", action=\\"store_true\\", help=\\"Count the number of characters in the file\\") parser.add_argument(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", help=\\"Print additional information\\") args = parser.parse_args() if args.verbose: print(f\\"Reading file {args.file_path}...\\") try: if args.count_lines: result = count_lines(args.file_path) if args.verbose: print(f\\"Number of lines: {result}\\") else: print(result) elif args.count_words: result = count_words(args.file_path) if args.verbose: print(f\\"Number of words: {result}\\") else: print(result) elif args.count_chars: result = count_chars(args.file_path) if args.verbose: print(f\\"Number of characters: {result}\\") else: print(result) except FileNotFoundError: print(f\\"Error: File \'{args.file_path}\' not found.\\") except PermissionError: print(f\\"Error: Permission denied for file \'{args.file_path}\'.\\") if __name__ == \\"__main__\\": main() ```","solution":"import argparse def count_lines(file_path): with open(file_path, \'r\') as file: return sum(1 for _ in file) def count_words(file_path): with open(file_path, \'r\') as file: return sum(len(line.split()) for line in file) def count_chars(file_path): with open(file_path, \'r\') as file: return sum(len(line) for line in file) def main(): parser = argparse.ArgumentParser(description=\\"Process a text file to count lines, words, or characters.\\") parser.add_argument(\\"file_path\\", help=\\"Path to the text file to be processed\\") group = parser.add_mutually_exclusive_group(required=True) group.add_argument(\\"--count-lines\\", action=\\"store_true\\", help=\\"Count the number of lines in the file\\") group.add_argument(\\"--count-words\\", action=\\"store_true\\", help=\\"Count the number of words in the file\\") group.add_argument(\\"--count-chars\\", action=\\"store_true\\", help=\\"Count the number of characters in the file\\") parser.add_argument(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", help=\\"Print additional information\\") args = parser.parse_args() if args.verbose: print(f\\"Reading file {args.file_path}...\\") try: if args.count_lines: result = count_lines(args.file_path) if args.verbose: print(f\\"Number of lines: {result}\\") else: print(result) elif args.count_words: result = count_words(args.file_path) if args.verbose: print(f\\"Number of words: {result}\\") else: print(result) elif args.count_chars: result = count_chars(args.file_path) if args.verbose: print(f\\"Number of characters: {result}\\") else: print(result) except FileNotFoundError: print(f\\"Error: File \'{args.file_path}\' not found.\\") except PermissionError: print(f\\"Error: Permission denied for file \'{args.file_path}\'.\\") if __name__ == \\"__main__\\": main()"},{"question":"**Objective:** Demonstrate your understanding of loading and using toy datasets from the scikit-learn library, along with your ability to implement and evaluate a basic machine learning model. **Problem Statement:** Your task is to load the \\"digits\\" dataset using scikit-learn, perform basic exploration of the dataset, and implement a simple classification algorithm to classify the digits. You are required to follow the steps outlined below: 1. **Loading the Dataset:** - Use the function `load_digits()` from `sklearn.datasets` to load the \\"digits\\" dataset. 2. **Exploring the Dataset:** - Print the shape of the data and target arrays. - Display images of the first 5 digits using Matplotlib (you can use `matplotlib.pyplot.imshow` for displaying images). 3. **Preprocessing:** - Split the dataset into training and testing sets using `train_test_split` from `sklearn.model_selection`. Use 80% of the data for training and the remaining 20% for testing. 4. **Model Implementation:** - Implement a support vector classifier (`sklearn.svm.SVC`) to train on the dataset. Use the default parameters for this classifier. 5. **Model Evaluation:** - Evaluate the model on the testing set and print the classification report using `classification_report` from `sklearn.metrics`. - Calculate and print the overall accuracy of the model on the testing set. **Constraints:** - Your solution should be implemented in Python. - You must use the scikit-learn library for dataset loading, model implementation, and evaluation. **Expected Input and Output:** - Input: None (the dataset is loaded internally within the code). - Output: The shape of data and target arrays, visualization of the first 5 digits, classification report, and accuracy of the model. ```python # Sample Output Format # (for reference only, real output will depend on the data and model performance) # Data shape: (1797, 64) # Target shape: (1797,) # [Visualization of first 5 images of digits] # Classification Report: # [actual classification report output] # Accuracy: 0.97 ``` **Note:** Make sure to handle any necessary imports and dependencies within your code.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.metrics import classification_report, accuracy_score def load_and_process_digits(): # Loading the dataset digits = load_digits() data, target = digits.data, digits.target # Exploring the dataset print(f\\"Data shape: {data.shape}\\") print(f\\"Target shape: {target.shape}\\") # Displaying images of the first 5 digits for i in range(5): plt.imshow(digits.images[i], cmap=\'gray\') plt.title(f\\"Digit: {target[i]}\\") plt.show() # Splitting the dataset X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42) # Model implementation clf = SVC() clf.fit(X_train, y_train) # Model evaluation y_pred = clf.predict(X_test) class_report = classification_report(y_test, y_pred) accuracy = accuracy_score(y_test, y_pred) # Printing results print(\\"Classification Report:\\") print(class_report) print(f\\"Accuracy: {accuracy}\\") return data, target, class_report, accuracy # Running the function to get the outputs load_and_process_digits()"},{"question":"Implementing Sequence Operations In this task, you will implement a class called `CustomSequence` that mimics some of the behaviors of Python\'s sequence protocol. You should define member functions that perform the following operations: 1. **Check if an element exists in the sequence** (`contains` functionality). 2. **Return the size of the sequence** (`length` functionality). 3. **Concatenate two sequences** (`concat` functionality). 4. **Return a slice of the sequence** (`get_slice` functionality). # Class Definition ```python class CustomSequence: def __init__(self, items: list): self.items = items def contains(self, value): Return True if value is in the sequence, otherwise False. Args: value (any): The value to check for in the sequence. Returns: bool: True if value is found, False otherwise. # Implement this method def length(self): Return the number of items in the sequence. Returns: int: The length of the sequence. # Implement this method def concat(self, other_sequence): Concatenate two sequences and return a new sequence. Args: other_sequence (CustomSequence): Another sequence to concatenate. Returns: CustomSequence: A new sequence resulting from the concatenation. # Implement this method def get_slice(self, start, end): Return a slice of the sequence from start to end (not inclusive). Args: start (int): The starting index. end (int): The ending index (not inclusive). Returns: CustomSequence: A new sequence representing the slice. # Implement this method ``` # Function Implementations 1. **contains**: This method should check whether a given value exists in the sequence. 2. **length**: This method should return the number of elements in the sequence. 3. **concat**: This method should concatenate the current sequence with another `CustomSequence` and return a new `CustomSequence`. 4. **get_slice**: This method should return a new `CustomSequence` that contains a slice of the original sequence from the given start to end indices (end not inclusive). # Example Usage ```python seq1 = CustomSequence([1, 2, 3]) seq2 = CustomSequence([4, 5, 6]) # Check contains assert seq1.contains(2) == True assert seq1.contains(4) == False # Check length assert seq1.length() == 3 # Concatenate sequences seq3 = seq1.concat(seq2) assert seq3.items == [1, 2, 3, 4, 5, 6] # Get slice seq4 = seq3.get_slice(1, 4) assert seq4.items == [2, 3, 4] ``` # Constraints - You may assume that the sequences are always lists of integers for simplicity. - Indexing for slices starts at 0. The goal of this exercise is to ensure you understand how basic sequence operations can be implemented and used, mimicking Python\'s sequence protocol at a high level.","solution":"class CustomSequence: def __init__(self, items: list): self.items = items def contains(self, value): Return True if value is in the sequence, otherwise False. return value in self.items def length(self): Return the number of items in the sequence. return len(self.items) def concat(self, other_sequence): Concatenate two sequences and return a new sequence. return CustomSequence(self.items + other_sequence.items) def get_slice(self, start, end): Return a slice of the sequence from start to end (not inclusive). return CustomSequence(self.items[start:end])"},{"question":"<|Analysis Begin|> The provided documentation is about the \\"email.mime\\" module in Python, which is used to create different types of MIME (Multipurpose Internet Mail Extensions) objects such as `MIMEText`, `MIMEImage`, `MIMEAudio`, and `MIMEApplication`. It also explains the base classes like `MIMEBase`, `MIMENonMultipart`, and `MIMEMultipart`. The module is used to build and manipulate email messages, especially in cases where the messages contain multiple types of content (e.g., text and images). The descriptions include various constructors of the classes along with their parameters and usage. Given the context and usage of the package, we can craft a question that assesses a student\'s understanding of how to use these classes to construct a MIME email with multiple parts. <|Analysis End|> <|Question Begin|> # Email Message Construction with MIME You are tasked with constructing a MIME email message using the `email.mime` module in Python. The email should include the following components: 1. Plain text content. 2. An HTML version of the same content. 3. An image attachment. **Objective:** Write a Python function `create_mime_email` that creates and returns a complete MIME email message as a string. **Function Signature:** ```python def create_mime_email(to_address: str, from_address: str, subject: str, plain_text: str, html_text: str, image_path: str) -> str: pass ``` # Input - `to_address` (str): The recipient\'s email address. - `from_address` (str): The sender\'s email address. - `subject` (str): The subject line of the email. - `plain_text` (str): The plain text version of the email content. - `html_text` (str): The HTML version of the email content. - `image_path` (str): The file path to the image to be attached. # Output - The function should return the complete MIME email message as a string. # Constraints - Ensure the MIME email is correctly formatted and includes a plain text part, an HTML part, and an image attachment. - The email should comply with MIME standards and should be able to be sent via an SMTP server. # Example ```python plain_text = \\"Hello,nThis is a plain text version of the email.\\" html_text = <html> <head></head> <body> <p>Hello,<br> This is an HTML version of the email.</p> </body> </html> to_address = \\"recipient@example.com\\" from_address = \\"sender@example.com\\" subject = \\"Test Email\\" image_path = \\"/path/to/image.jpg\\" print(create_mime_email(to_address, from_address, subject, plain_text, html_text, image_path)) ``` # Note You may use the following classes and functions from the `email.mime` module: - `MIMEMultipart` - `MIMEText` - `MIMEImage` - `email.encoders.encode_base64` - And other necessary modules from `email.mime` for constructing and encoding your message. Your solution should demonstrate an understanding of how to build and configure a MIME email with multiple parts.","solution":"import os from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.encoders import encode_base64 def create_mime_email(to_address: str, from_address: str, subject: str, plain_text: str, html_text: str, image_path: str) -> str: # Create multipart MIME message msg = MIMEMultipart(\\"alternative\\") msg[\\"To\\"] = to_address msg[\\"From\\"] = from_address msg[\\"Subject\\"] = subject # Attach the plain text and HTML parts plain_part = MIMEText(plain_text, \\"plain\\") html_part = MIMEText(html_text, \\"html\\") msg.attach(plain_part) msg.attach(html_part) # Attach the image with open(image_path, \\"rb\\") as img_file: img = MIMEImage(img_file.read()) encode_base64(img) img.add_header(\\"Content-Disposition\\", \\"attachment\\", filename=os.path.basename(image_path)) msg.attach(img) # Return the full MIME message as a string return msg.as_string()"},{"question":"You are to implement a function that sets up an audio device, configures its parameters, and plays a series of audio samples. Here are the detailed requirements: - Write a function `play_audio_samples(device_path: str, mode: str, format: int, nchannels: int, samplerate: int, samples: bytes) -> tuple` that performs the following operations: - Open the specified audio device with the given mode. - Set the audio parameters using the format, number of channels, and samplerate provided. - Write the given audio samples to the device. - Return a tuple containing: - The actual format set by the device. - The actual number of channels set by the device. - The actual sample rate set by the device. - The number of bytes written to the device. **Function Signature**: ```python def play_audio_samples(device_path: str, mode: str, format: int, nchannels: int, samplerate: int, samples: bytes) -> tuple: pass ``` # Constraints and Notes: 1. The `device_path` is a string representing the path to the audio device (e.g., \\"/dev/dsp\\"). 2. The `mode` is either `\'r\'`, `\'w\'`, or `\'rw\'`, indicating read-only, write-only, or read-write access. 3. The `format` is an integer representing the desired audio format (e.g., `ossaudiodev.AFMT_S16_LE`). 4. The `nchannels` is an integer indicating the number of audio channels (commonly 1 for mono and 2 for stereo). 5. The `samplerate` is an integer representing the sample rate in samples per second (e.g., 44100 for CD quality). 6. The `samples` is a bytes-like object containing the audio data to be written to the device. 7. Handle any possible exceptions that may arise during device operations and ensure the device is properly closed even if an error occurs. 8. Utilize the `ossaudiodev` module methods to perform the operations. # Example: ```python from ossaudiodev import open, AFMT_S16_LE device_path = \\"/dev/dsp\\" mode = \'w\' format = AFMT_S16_LE nchannels = 2 samplerate = 44100 samples = b\'x00x01\' * 22050 # Example byte data result = play_audio_samples(device_path, mode, format, nchannels, samplerate, samples) print(result) # Example output: (AFMT_S16_LE, 2, 44100, 44100) ``` # Performance Requirements: - Ensure the function handles audio configurations efficiently. - Handle any buffering or blocking modes appropriately depending on implementation settings. Good luck!","solution":"import ossaudiodev def play_audio_samples(device_path: str, mode: str, format: int, nchannels: int, samplerate: int, samples: bytes) -> tuple: try: # Open the audio device dsp = ossaudiodev.open(device_path, mode) # Configure the audio parameters dsp.setfmt(format) dsp.channels(nchannels) dsp.speed(samplerate) # Get the actual parameters set by the device actual_format = dsp.getfmt() actual_channels = dsp.channels() actual_samplerate = dsp.speed() # Write the audio samples to the device bytes_written = dsp.write(samples) # Close the device dsp.close() return (actual_format, actual_channels, actual_samplerate, bytes_written) except Exception as e: # Handle any errors by closing the device if it\'s open if \'dsp\' in locals(): dsp.close() # Reraise the exception after clean up raise e"},{"question":"# Question: Objective: Create a class hierarchy in Python, where the base class and its derived classes demonstrate certain functionalities of type objects. You will need to implement methods to check types, modify class attributes, and ensure type correctness. Instructions: 1. **Create a base class `BaseType`**: - The class should have an `__init__` method which accepts a single argument `name` and stores it in an instance variable. - Implement a class method `is_instance(obj)` that checks if an object is an instance of `BaseType`. 2. **Create a derived class `DerivedType`**: - This class should inherit from `BaseType`. - Add an additional method `display_name()` that prints the name of the instance. 3. **Type Checking and Manipulation**: - Write a function `check_and_modify(obj)` that: 1. Checks if the object `obj` is an instance of `BaseType`. 2. If it is, modifies its `name` attribute by appending `_modified` to it. 3. If it is an instance of `DerivedType`, call the `display_name()` method after modification. 4. If it is not an instance of `BaseType`, return the message \\"Not a BaseType object\\". 4. **Ensure cache clearing**: - Write another function `clear_and_validate_cache()` that keeps track of objects created using the `DerivedType` class and clears the cache (if any) and validates if objects are instances of `DerivedType` after modification. ```python class BaseType: def __init__(self, name: str): self.name = name @classmethod def is_instance(cls, obj) -> bool: # Implement logic to check if obj is instance of cls class DerivedType(BaseType): def display_name(self): # Implement logic to print self.name def check_and_modify(obj): # Implement logic to check type and modify name attribute accordingly def clear_and_validate_cache(): # Implement logic to manage and validate cache clearing ``` Constraints: - You may use Python\'s built-in `type` and `isinstance` functions. - Ensure that the implementation handles different types and shows appropriate output or messages. Example: ```python b = BaseType(\\"base\\") d = DerivedType(\\"derived\\") print(BaseType.is_instance(b)) # Expected: True print(BaseType.is_instance(d)) # Expected: True print(BaseType.is_instance(\\"string\\")) # Expected: False check_and_modify(b) check_and_modify(d) check_and_modify(\\"string\\") clear_and_validate_cache() ``` Expected Output: ```python True True False Not a BaseType object derived_modified ```","solution":"class BaseType: def __init__(self, name: str): self.name = name @classmethod def is_instance(cls, obj) -> bool: return isinstance(obj, cls) class DerivedType(BaseType): def display_name(self): print(self.name) def check_and_modify(obj): if BaseType.is_instance(obj): obj.name += \'_modified\' if isinstance(obj, DerivedType): obj.display_name() else: return \\"Not a BaseType object\\" def clear_and_validate_cache(): cache = [] # Creating some DerivedType objects and adding them to cache d1 = DerivedType(\\"first\\") d2 = DerivedType(\\"second\\") cache.append(d1) cache.append(d2) # Assuming we want to clear cache cache.clear() # Checking if objects are still correct after clearing cache assert DerivedType.is_instance(d1) assert DerivedType.is_instance(d2) return \\"Cache cleared and validation successful\\""},{"question":"**Coding Assessment Question:** # Objective: You are required to implement a class that utilizes PyTorch\'s `FakeTensorMode` to perform symbolic tensor operations and preserve metadata without actual computation. This exercise will test your understanding of creating and manipulating fake tensors, along with their integration into a computational graph. # Question: Implement a class `FakeTensorOperations` with the following methods: 1. **`__init__(self)`**: - Initialize a `FakeTensorMode` and store converters for later use. 2. **`create_fake_tensor(self, real_tensor: torch.Tensor) -> torch.Tensor`**: - Convert a given real tensor into a fake tensor. 3. **`perform_operations(self, fake_tensor: torch.Tensor, operation: str, *args, **kwargs) -> torch.Tensor`**: - Perform a specified operation (e.g., `add`, `mul`, `matmul`) on the given fake tensor within the `FakeTensorMode` context. - Return the resulting fake tensor. 4. **`trace_and_propagate(self, real_tensors: List[torch.Tensor]) -> Dict[str, torch.Tensor]`**: - Given a list of real tensors, convert them to fake tensors. - Perform a series of operations on these tensors as specified in a sequence of operations (you may define a predefined sequence). - Trace the operations and return a dictionary mapping operation names to the resulting fake tensors. # Example Usage: ```python import torch class FakeTensorOperations: def __init__(self): # Initialize FakeTensorMode and converters pass def create_fake_tensor(self, real_tensor: torch.Tensor) -> torch.Tensor: # Convert real tensor to fake tensor pass def perform_operations(self, fake_tensor: torch.Tensor, operation: str, *args, **kwargs) -> torch.Tensor: # Perform an operation on fake tensor pass def trace_and_propagate(self, real_tensors: List[torch.Tensor]) -> Dict[str, torch.Tensor]: # Convert real tensors to fake tensors and perform predefined sequence of operations pass # Example operations = FakeTensorOperations() real_tensor1 = torch.randn(2, 3) real_tensor2 = torch.ones(2, 3) fake_tensor1 = operations.create_fake_tensor(real_tensor1) result = operations.perform_operations(fake_tensor1, \'add\', fake_tensor1) propagation_result = operations.trace_and_propagate([real_tensor1, real_tensor2]) ``` # Constraints: - **Input**: For `create_fake_tensor` and `perform_operations`, the input tensor will have valid dimensions and data types suitable for the operations specified. - **Output**: The methods should correctly return the expected fake tensors corresponding to the operations performed. - **Performance**: Ensure minimal overhead when working within the `FakeTensorMode` context. This question aims to evaluate the student\'s ability to integrate fake tensors into tensor computations and understand the underlying mechanics provided by PyTorch\'s `FakeTensorMode`.","solution":"import torch from torch.fx.experimental.proxy_tensor import make_fx from torch.fx.experimental.proxy_tensor import DecompositionInterpreter class FakeTensorOperations: def __init__(self): pass def create_fake_tensor(self, real_tensor: torch.Tensor) -> torch.Tensor: fake_tensor = real_tensor.clone() fake_tensor.requires_grad_() return fake_tensor def perform_operations(self, fake_tensor: torch.Tensor, operation: str, *args, **kwargs) -> torch.Tensor: if operation == \'add\': return fake_tensor + args[0] elif operation == \'mul\': return fake_tensor * args[0] elif operation == \'matmul\': return fake_tensor.matmul(args[0]) else: raise ValueError(f\\"Operation \'{operation}\' is not supported.\\") def trace_and_propagate(self, real_tensors: list) -> dict: fake_tensors = [self.create_fake_tensor(tensor) for tensor in real_tensors] def predefined_sequence(fake_tensors): a, b = fake_tensors c = a + b d = c * a e = d.matmul(a.T) return {\'add\': c, \'mul\': d, \'matmul\': e} return predefined_sequence(fake_tensors)"},{"question":"# Concurrent Data Processing and Synchronization You are tasked with creating a system that processes a large dataset concurrently, divides the workload across multiple threads, and uses inter-process communication to aggregate results. This problem will help assess your understanding of both thread-based and process-based parallelism, as well as synchronization techniques in Python. Problem Description Given a large list of integers, your task is to split the list into equal parts and process each part using threads. Each thread should compute the sum of integers in its assigned part. Once the sums from all threads are computed, these sums should be aggregated using a separate process to provide the final total sum of all integers in the list. Implement the following functions: 1. `split_list(data: List[int], n: int) -> List[List[int]]`: Split the input list `data` into `n` equal parts. If the list cannot be divided equally, distribute the remaining elements. 2. `compute_partial_sum(data: List[int]) -> int`: Compute the sum of the integers in the given list `data`. 3. `compute_sum_multithreaded(data: List[int], n: int) -> List[int]`: Use `split_list` to divide the data into `n` parts and compute the sum of each part in a separate thread. Return a list of partial sums computed by each thread. 4. `aggregate_sums(sums: List[int]) -> int`: Use a separate process to compute the final sum from the list of partial sums. 5. `concurrent_sum(data: List[int], n: int) -> int`: This is the main function that drives the entire process: - Split the data into `n` parts. - Compute partial sums using multiple threads. - Aggregate these partial sums using a separate process. - Return the final total sum. Example ```python data = [1, 2, 3, 4, 5, 6, 7, 8, 9] n = 3 result = concurrent_sum(data, n) print(result) # Output should be 45 ``` Constraints and Assumptions 1. You can assume that the list `data` will contain at most 10^6 integers. 2. Each integer in `data` will be between -10^6 and 10^6. 3. The number of threads `n` will be between 1 and 32. 4. You must utilize the `threading` and `multiprocessing` modules provided in Python 3.10. Implementation Notes - Use the `threading` module to create and manage threads for partial sum computation. - Use the `multiprocessing` module to create a separate process for aggregating the sums. - Utilize synchronization primitives where necessary to ensure correct concurrent execution.","solution":"from typing import List from threading import Thread from multiprocessing import Process, Queue def split_list(data: List[int], n: int) -> List[List[int]]: Split the input list `data` into `n` equal parts. If the list cannot be divided equally, distribute the remaining elements. length = len(data) avg = length // n remainders = length % n parts = [] start = 0 for i in range(n): end = start + avg + (1 if i < remainders else 0) parts.append(data[start:end]) start = end return parts def compute_partial_sum(data: List[int]) -> int: Compute the sum of the integers in the given list `data`. return sum(data) def compute_sum_multithreaded(data: List[int], n: int) -> List[int]: Use `split_list` to divide the data into `n` parts and compute the sum of each part in a separate thread. Return a list of partial sums computed by each thread. splits = split_list(data, n) results = [0] * n threads = [] def worker(i: int, data: List[int]): results[i] = compute_partial_sum(data) for i in range(n): thread = Thread(target=worker, args=(i, splits[i])) threads.append(thread) thread.start() for thread in threads: thread.join() return results def aggregate_sums(sums: List[int]) -> int: Use a separate process to compute the final sum from the list of partial sums. def worker(queue: Queue, sums: List[int]): queue.put(sum(sums)) queue = Queue() process = Process(target=worker, args=(queue, sums)) process.start() process.join() return queue.get() def concurrent_sum(data: List[int], n: int) -> int: This is the main function that drives the entire process: - Split the data into `n` parts. - Compute partial sums using multiple threads. - Aggregate these partial sums using a separate process. - Return the final total sum. partial_sums = compute_sum_multithreaded(data, n) total_sum = aggregate_sums(partial_sums) return total_sum"},{"question":"# Asynchronous Event Loop Configuration and Platform-Specific Implementation Problem Statement You are tasked with implementing a Python application that needs to run different event loop configurations based on the operating system it is running on. Your application will handle socket connections and subprocesses. However, given the limitations of certain event loops on different platforms as per the asyncio module documentation, you will need to configure your event loop appropriately to ensure compatibility. Requirements 1. **Platform Detection**: - Detect whether the application is running on Windows or macOS. 2. **Event Loop Configuration**: - On Windows: - Use `ProactorEventLoop` as the default event loop. - Ensure your code can handle subprocess creation. - On macOS: - Ensure compatibility with character devices on macOS versions <= 10.8 by manually configuring the event loop if needed. 3. **Function Implementation**: - Implement a function `configure_event_loop()` that configures the appropriate event loop based on the operating system. - Implement a simple demonstration function `run_demo()` that: - For Windows, starts a subprocess and prints its PID. - For macOS, binds a server socket to a local port and prints the bound address. Input The function `configure_event_loop()` takes no input. Output Both `configure_event_loop()` and `run_demo()` print out relevant information based on the platform-specific tasks. Constraints and Notes - Use the `asyncio` module for event loop configuration. - Ensure that your implementation adheres to platform-specific limitations described in the documentation. - Python 3.8+ is required. Example ```python import sys import platform import asyncio import selectors async def subprocess_demo(): process = await asyncio.create_subprocess_exec( \'python\', \'--version\', stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await process.communicate() print(f\'PID: {process.pid}\') print(f\'STDOUT: {stdout.decode().strip()}\') print(f\'STDERR: {stderr.decode().strip()}\') async def server_socket_demo(): server = await asyncio.start_server(lambda r, w: None, \'127.0.0.1\', 8888) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') server.close() await server.wait_closed() def configure_event_loop(): current_os = platform.system() if current_os == \'Windows\': asyncio.set_event_loop(asyncio.ProactorEventLoop()) print(\\"Configured ProactorEventLoop for Windows.\\") elif current_os == \'Darwin\' and sys.version_info[:2] <= (10, 8): selector = selectors.SelectSelector() loop = asyncio.SelectorEventLoop(selector) asyncio.set_event_loop(loop) print(\\"Configured SelectorEventLoop for older macOS version.\\") else: print(\\"Using default event loop configuration.\\") def run_demo(): current_os = platform.system() configure_event_loop() if current_os == \'Windows\': asyncio.run(subprocess_demo()) elif current_os == \'Darwin\': asyncio.run(server_socket_demo()) if __name__ == \\"__main__\\": run_demo() ```","solution":"import sys import platform import asyncio import selectors async def subprocess_demo(): process = await asyncio.create_subprocess_exec( \'python\', \'--version\', stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await process.communicate() print(f\'PID: {process.pid}\') print(f\'STDOUT: {stdout.decode().strip()}\') print(f\'STDERR: {stderr.decode().strip()}\') async def server_socket_demo(): server = await asyncio.start_server(lambda r, w: None, \'127.0.0.1\', 8888) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') server.close() await server.wait_closed() def configure_event_loop(): current_os = platform.system() if current_os == \'Windows\': asyncio.set_event_loop(asyncio.ProactorEventLoop()) print(\\"Configured ProactorEventLoop for Windows.\\") elif current_os == \'Darwin\' and sys.version_info[:2] <= (10, 8): selector = selectors.SelectSelector() loop = asyncio.SelectorEventLoop(selector) asyncio.set_event_loop(loop) print(\\"Configured SelectorEventLoop for older macOS version.\\") else: print(\\"Using default event loop configuration.\\") def run_demo(): current_os = platform.system() configure_event_loop() if current_os == \'Windows\': asyncio.run(subprocess_demo()) elif current_os == \'Darwin\': asyncio.run(server_socket_demo()) if __name__ == \\"__main__\\": run_demo()"},{"question":"**Question: Implement a Type-Safe Generic Data Pipeline** **Objective:** To demonstrate an understanding of Python\'s `typing` capabilities, including generics, callables, and type constraints. **Description:** You are required to implement a type-safe generic data pipeline that can process items through a series of stages. Each stage is a callable that takes an item of type `T` and returns a modified item of type `T`. Here\'s the detailed specification: 1. **Type Definitions:** - Define a type variable `T` that will represent the item type. - Define a type alias `Stage` for a callable that takes an item of type `T` and returns an item of type `T`. 2. **Pipeline Class:** - Implement a class `Pipeline` that can handle a series of stages. - The `Pipeline` class should be generic over `T`. 3. **Methods:** - `add_stage(stage: Stage[T]) -> None`: Adds a processing stage to the pipeline. - `process(item: T) -> T`: Processes the given item through all the stages in the same order they were added. **Requirements:** - Use type hints to ensure that the Pipeline class is type-safe. - Utilize `Callable`, `Generics`, and `TypeVar` from the `typing` module to implement type constraints. **Input:** - A series of stages (functions) that modify an item of type `T`. - An item of type `T` to be processed through the pipeline. **Output:** - The final modified item after it has been processed through all the stages. **Example:** ```python from typing import TypeVar, Callable, Generic T = TypeVar(\'T\') Stage = Callable[[T], T] class Pipeline(Generic[T]): def __init__(self) -> None: self.stages = [] def add_stage(self, stage: Stage[T]) -> None: self.stages.append(stage) def process(self, item: T) -> T: for stage in self.stages: item = stage(item) return item # Usage def stage_one(x: int) -> int: return x + 1 def stage_two(x: int) -> int: return x * 2 pipeline = Pipeline[int]() pipeline.add_stage(stage_one) pipeline.add_stage(stage_two) result = pipeline.process(10) print(result) # Output: 22 ``` In this example, the `Pipeline` class ensures type safety such that all stages are consistent with the type `T`, and it processes the item through each stage sequentially. **Constraints:** - Use the provided types and ensure that your implementation utilizes the `typing` module constructs for type safety. - Each stage should be a callable that adheres to the `Stage[T]` signature. Good luck, and ensure your implementation passes both static type checks and dynamic behavior!","solution":"from typing import TypeVar, Callable, Generic, List T = TypeVar(\'T\') Stage = Callable[[T], T] class Pipeline(Generic[T]): def __init__(self) -> None: self.stages: List[Stage[T]] = [] def add_stage(self, stage: Stage[T]) -> None: self.stages.append(stage) def process(self, item: T) -> T: for stage in self.stages: item = stage(item) return item"},{"question":"**Question:** Assume you are building a Python application where you need to fetch data from a secured website that requires HTTP basic authentication. Additionally, the server offers some data that need to be posted to an endpoint and you need to handle potential redirections and other HTTP errors. 1. Write a Python function `fetch_secure_data(url: str, post_url: str, post_data: dict, username: str, password: str) -> str` that: - Takes a URL to fetch data from and another URL for posting data. - Accepts POST data as a dictionary and sends it in the body of a POST request. - Requires `username` and `password` for HTTP Basic Authentication. - Handles HTTP error responses gracefully and prints a user-friendly message for each error. - Adds a custom User-Agent header to mimic a browser request. - Returns the content of the fetched data as a string. **Constraints:** - You should use the `urllib.request` module. - Handle redirections and print the final URL after redirection. - Catch and manage potential errors such as HTTP errors or URL errors properly. **Function Signature:** ```python def fetch_secure_data(url: str, post_url: str, post_data: dict, username: str, password: str) -> str: pass ``` **Example Usage:** ```python url = \\"http://example.com/data\\" post_url = \\"http://example.com/post-endpoint\\" post_data = {\\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\"} username = \\"yourusername\\" password = \\"yourpassword\\" result = fetch_secure_data(url, post_url, post_data, username, password) print(result) ```","solution":"import urllib.request import urllib.parse import json from urllib.error import HTTPError, URLError def fetch_secure_data(url: str, post_url: str, post_data: dict, username: str, password: str) -> str: try: # Create a password manager and add the username and password password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm() password_mgr.add_password(None, url, username, password) handler = urllib.request.HTTPBasicAuthHandler(password_mgr) opener = urllib.request.build_opener(handler) opener.addheaders = [(\'User-Agent\', \'Mozilla/5.0\')] # Install the opener urllib.request.install_opener(opener) # Fetch the data with urllib.request.urlopen(url) as response: data = response.read().decode(\'utf-8\') # Prepare the POST data post_data_encoded = urllib.parse.urlencode(post_data).encode(\'utf-8\') req = urllib.request.Request(post_url, data=post_data_encoded, method=\'POST\') req.add_header(\'User-Agent\', \'Mozilla/5.0\') # Send the POST request with urllib.request.urlopen(req) as response: post_response_data = response.read().decode(\'utf-8\') return data except HTTPError as e: return f\\"HTTP error occurred: {e.code} - {e.reason}\\" except URLError as e: return f\\"URL error occurred: {e.reason}\\" except Exception as e: return f\\"An unexpected error occurred: {str(e)}\\""},{"question":"**Secure File Integrity Verifier** You are tasked to implement a utility that verifies the integrity of files using secure hash functions. The program should have two parts: 1. **Hash Generation**: Read a file and generate a hash digest using a specified hash algorithm. 2. **Hash Verification**: Compare the hash digest of a file with a provided hash to verify its integrity. Your utility should support at least the following hash algorithms: `md5`, `sha1`, `sha256`, `sha512`, `blake2b`, and `blake2s`. **Requirements:** 1. Implement a function `generate_file_hash(file_path: str, hash_alg: str) -> str` that takes a file path and the name of the hash algorithm as input, and returns the hexadecimal hash digest of the file. 2. Implement a function `verify_file_integrity(file_path: str, provided_hash: str, hash_alg: str) -> bool` that takes a file path, a provided hash value, and the name of the hash algorithm, and returns `True` if the computed hash matches the provided hash, otherwise `False`. **Function Specifications:** - `generate_file_hash(file_path: str, hash_alg: str) -> str` - **Input**: - `file_path`: Path to the file (type: `str`) - `hash_alg`: Name of the hash algorithm (`md5`, `sha1`, `sha256`, `sha512`, `blake2b`, `blake2s`) (type: `str`) - **Output**: - Returns the hexadecimal hash digest of the file content (type: `str`) - **Constraints**: - The function should handle errors gracefully, such as invalid file paths or unsupported algorithms. - `verify_file_integrity(file_path: str, provided_hash: str, hash_alg: str) -> bool` - **Input**: - `file_path`: Path to the file (type: `str`) - `provided_hash`: Hash value to compare against (type: `str`) - `hash_alg`: Name of the hash algorithm (`md5`, `sha1`, `sha256`, `sha512`, `blake2b`, `blake2s`) (type: `str`) - **Output**: - Returns `True` if the computed hash matches the provided hash, otherwise `False` (type: `bool`) - **Constraints**: - The function should handle errors gracefully, such as invalid file paths or unsupported algorithms. **Example Usage:** ```python # Example for generating hash hash_value = generate_file_hash(\\"example.txt\\", \\"sha256\\") print(hash_value) # Output: (A valid sha256 hash string) # Example for verifying integrity is_valid = verify_file_integrity(\\"example.txt\\", \\"the_provided_hash_value\\", \\"sha256\\") print(is_valid) # Output: True if the hash matches, False otherwise ``` **Additional Information:** - You may use the `hashlib` module to implement the hash functions.","solution":"import hashlib def generate_file_hash(file_path: str, hash_alg: str) -> str: Generates the hash digest of a file using the specified hash algorithm. :param file_path: Path to the file :param hash_alg: Hash algorithm to use (md5, sha1, sha256, sha512, blake2b, blake2s) :return: Hexadecimal hash digest of the file content try: hash_function = getattr(hashlib, hash_alg)() except AttributeError: raise ValueError(f\\"Unsupported hash algorithm: {hash_alg}\\") try: with open(file_path, \\"rb\\") as f: for chunk in iter(lambda: f.read(4096), b\\"\\"): hash_function.update(chunk) return hash_function.hexdigest() except FileNotFoundError: raise FileNotFoundError(f\\"File not found: {file_path}\\") def verify_file_integrity(file_path: str, provided_hash: str, hash_alg: str) -> bool: Verifies the integrity of a file by comparing its hash digest with a provided hash value. :param file_path: Path to the file :param provided_hash: Provided hash value to compare against :param hash_alg: Hash algorithm to use (md5, sha1, sha256, sha512, blake2b, blake2s) :return: True if the computed hash matches the provided hash, otherwise False computed_hash = generate_file_hash(file_path, hash_alg) return computed_hash == provided_hash"},{"question":"**Histogram and Bivariate Analysis with Seaborn\'s `histplot`** **Objective:** Given a dataset, create both univariate and bivariate histograms using the seaborn `histplot` function. Demonstrate understanding of custom bin widths, density estimates, handling of skewed data, and annotated bivariate heatmaps. **Dataset:** You will be using the `diamonds` dataset available in seaborn. Load it with the following code: ```python import seaborn as sns diamonds = sns.load_dataset(\\"diamonds\\") ``` **Tasks:** 1. **Univariate Histogram:** Create a univariate histogram for the `price` of diamonds. - Use a custom bin width of 500. - Apply a kernel density estimate (KDE). - Use the style of a polygon (element=\'poly\'). 2. **Multiple Distributions:** Plot multiple histograms for the `price` feature, grouped by the `cut` of the diamonds. - Plot the histograms with hue mapping based on the `cut`. - Stack the histograms (multiple=\'stack\'). - Normalize the histograms so that each bar’s height shows a density (stat=\'density\'). 3. **Skewed Data Handling:** Plot a histogram for the `price` feature, handling the skewed data with a logarithmic scale. - Apply a log scale on the x-axis (log_scale=True). 4. **Bivariate Histogram:** Create a bivariate histogram to visualize the relationship between `carat` and `price`. - Add a color bar to annotate the heatmap. - Customize the color bar to shrink to 75% of its original size using `cbar_kws`. 5. **Bivariate Histogram with Subsets:** Add a third dimension to the bivariate histogram by applying a hue on the `clarity` of diamonds. - Handle overlapping by reducing opacity (alpha=0.6). **Constraints/Requirements:** - Ensure that all plots include proper titles and labels for both axes. - Add a legend where necessary. - Use appropriate scaling and annotations to make visual interpretations easy. **Expected Output:** Your final implementation should generate five plots, each demonstrating the key features requested in the tasks. Use matplotlib functions as required to enhance the visual appearance of the plots. **Evaluation Criteria:** - Correct implementation of univariate and bivariate histograms. - Proper use of custom bin widths, KDE, and log scales. - Effective handling of multiple distributions with hue and stacking. - Clear and informative visualizations with appropriate titles, labels, and legends. *Solve the question by implementing the tasks in a Jupyter Notebook or a standalone Python file.*","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset diamonds = sns.load_dataset(\\"diamonds\\") def plot_univariate_histogram(): plt.figure(figsize=(10, 6)) sns.histplot(diamonds[\'price\'], kde=True, binwidth=500, element=\'poly\') plt.title(\'Univariate Histogram of Diamond Prices with KDE\') plt.xlabel(\'Price\') plt.ylabel(\'Frequency\') plt.show() def plot_multiple_distributions(): plt.figure(figsize=(10, 6)) sns.histplot(diamonds, x=\'price\', hue=\'cut\', multiple=\'stack\', stat=\'density\') plt.title(\'Distributions of Diamond Prices by Cut\') plt.xlabel(\'Price\') plt.ylabel(\'Density\') plt.legend(title=\'Cut\') plt.show() def plot_skewed_data_handling(): plt.figure(figsize=(10, 6)) sns.histplot(diamonds[\'price\'], kde=True, log_scale=True) plt.title(\'Histogram of Diamond Prices (Log Scale)\') plt.xlabel(\'Log Price\') plt.ylabel(\'Frequency\') plt.show() def plot_bivariate_histogram(): plt.figure(figsize=(10, 6)) sns.histplot(data=diamonds, x=\'carat\', y=\'price\', cbar=True, cbar_kws={\'shrink\': .75}) plt.title(\'Bivariate Histogram of Carat vs Price\') plt.xlabel(\'Carat\') plt.ylabel(\'Price\') plt.show() def plot_bivariate_histogram_with_subset(): plt.figure(figsize=(10, 6)) sns.histplot(data=diamonds, x=\'carat\', y=\'price\', hue=\'clarity\', alpha=0.6) plt.title(\'Bivariate Histogram of Carat vs Price with Clarity Subset\') plt.xlabel(\'Carat\') plt.ylabel(\'Price\') plt.legend(title=\'Clarity\') plt.show()"},{"question":"**Question: Implementing a Non-blocking Echo Server** In this task, you are required to implement a non-blocking echo server using Python\'s socket module. An echo server is a server that sends back the same message it receives from a client. Your server should handle multiple clients simultaneously in a non-blocking manner and include proper error handling. # Requirements 1. **Function to Create Server (create_server):** - Input: - `host` (string): The hostname or IP address to bind the server to. - `port` (int): The port number on which the server will listen for connections. - Output: - A non-blocking socket object set up to accept connections. - Constraints: - The server must handle both IPv4 and IPv6 addresses. - The server should be set to reuse the address (`SO_REUSEADDR`). 2. **Function to Accept Connections (accept_connections):** - Input: - `server_socket` (socket object): The socket object representing the server. - Output: - A list of new client sockets that have been accepted. 3. **Function to Handle Client Communication (handle_client):** - Input: - `client_socket` (socket object): The client socket object. - Output: - None - Functionality: - Receive data from the client. If data is received, send it back to the client. - If no data is received, handle the case appropriately and close the connection if necessary. - Constraints: - Use non-blocking mode to handle client communication. 4. **Non-blocking Echo Server Implementation:** - Use the above functions to implement a server that can handle multiple clients simultaneously. - The server should keep running and handle client connections in an infinite loop. # Example Usage ```python if __name__ == \\"__main__\\": server_socket = create_server(\'localhost\', 50007) while True: client_sockets = accept_connections(server_socket) for client in client_sockets: handle_client(client) ``` # Constraints - Use Python 3.7 or later. - Ensure that all socket operations are non-blocking. - Properly handle exceptions and errors. # Evaluation - Correctness of the server creation and binding. - Ability to handle multiple clients simultaneously in a non-blocking manner. - Proper error handling and closing of sockets. - Adherence to socket programming best practices. **Hints:** - You may find the `select` module useful for handling multiple client sockets in a non-blocking manner. - Ensure you close sockets properly to avoid resource leaks.","solution":"import socket import select def create_server(host, port): Create a non-blocking server socket. # Create a socket object server_socket = socket.socket(socket.AF_INET6, socket.SOCK_STREAM) # Allow the address to be reused server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) # Bind to the host and port server_socket.bind((host, port)) # Listen for incoming connections server_socket.listen() # Set the socket to non-blocking mode server_socket.setblocking(False) return server_socket def accept_connections(server_socket): Accept new connections and return the list of new client sockets. client_sockets = [] while True: try: client_socket, _ = server_socket.accept() client_sockets.append(client_socket) except BlockingIOError: break # No more connections are waiting to be accepted return client_sockets def handle_client(client_socket): Handle communication with a client. try: data = client_socket.recv(1024) if data: client_socket.sendall(data) else: client_socket.close() except BlockingIOError: pass # No data is received, continue the loop"},{"question":"**Objective**: Design a Python application using the `curses` library to create a simple text-based to-do list manager. This application should demonstrate your understanding of window management, handling user input, and displaying text with attributes and colors. **Requirements**: 1. **Initialization**: - Initialize the curses application with `curses.initscr()`. - Enable keypad mode, no echo, and cbreak mode. 2. **Primary Window**: - Create a main window that covers the entire screen. - Display a header at the top with the title \\"To-Do List Manager\\". - Display a footer at the bottom with instructions for adding, marking as done, and deleting tasks. 3. **Task Management**: - Allow the user to add tasks. - List the tasks in the main window. - Allow the user to mark tasks as done, which will change their appearance (e.g., with a different color or attribute like bold or underline). - Allow the user to delete tasks. 4. **User Input Handling**: - Handle user inputs for adding tasks (`a` key), marking tasks as done (`d` key), and deleting tasks (`x` key). - Handle navigation keys (`curses.KEY_UP`, `curses.KEY_DOWN`) to navigate through the list of tasks. 5. **Cleanup**: - Ensure that the terminal is restored to its original state when the application exits or encounters an error. **Input and Output**: - **Input**: User keyboard input to manage the to-do list. - **Output**: Display of the to-do list with tasks and their statuses. **Performance Requirements**: - The application should handle up to 100 tasks without significant latency. - The screen should update smoothly with minimal flickering. **Constraints**: - Use only the `curses` library for managing the text display and user input. # Code Template ```python import curses def main(stdscr): # Initialize curses application curses.curs_set(0) curses.noecho() curses.cbreak() stdscr.keypad(True) curses.start_color() curses.init_pair(1, curses.COLOR_WHITE, curses.COLOR_BLUE) curses.init_pair(2, curses.COLOR_GREEN, curses.COLOR_BLACK) height, width = stdscr.getmaxyx() title = \\"To-Do List Manager\\" tasks = [] current_task = 0 while True: stdscr.clear() # Draw header and footer stdscr.attron(curses.color_pair(1)) stdscr.addstr(0, 0, title) stdscr.addstr(height - 1, 0, \\"Press \'a\' to add, \'d\' to mark done, \'x\' to delete, \'q\' to quit\\") stdscr.attroff(curses.color_pair(1)) # Display tasks for idx, (task, done) in enumerate(tasks): if idx == current_task: stdscr.attron(curses.A_REVERSE) if done: stdscr.attron(curses.color_pair(2)) stdscr.addstr(idx + 2, 0, task) if done: stdscr.attroff(curses.color_pair(2)) if idx == current_task: stdscr.attroff(curses.A_REVERSE) # Refresh screen stdscr.refresh() # Handle user input key = stdscr.getch() if key == ord(\'q\'): break elif key == ord(\'a\'): stdscr.addstr(height - 2, 0, \\"Enter new task: \\") stdscr.refresh() curses.echo() new_task = stdscr.getstr(height - 2, 17, 40).decode(\\"utf-8\\") curses.noecho() tasks.append([new_task, False]) elif key == curses.KEY_UP and current_task > 0: current_task -= 1 elif key == curses.KEY_DOWN and current_task < len(tasks) - 1: current_task += 1 elif key == ord(\'d\') and tasks: tasks[current_task][1] = not tasks[current_task][1] elif key == ord(\'x\') and tasks: del tasks[current_task] current_task = min(current_task, len(tasks) - 1) # Cleanup curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() if __name__ == \\"__main__\\": curses.wrapper(main) ``` **Instructions:** 1. Implement the above code template to create the To-Do List Manager with the required functionalities. 2. Test the application to ensure it matches the requirements and handles user inputs as expected. 3. Submit your implementation along with any additional test cases or examples you\'ve used.","solution":"import curses def main(stdscr): # Initialize curses application curses.curs_set(0) curses.noecho() curses.cbreak() stdscr.keypad(True) curses.start_color() curses.init_pair(1, curses.COLOR_WHITE, curses.COLOR_BLUE) curses.init_pair(2, curses.COLOR_GREEN, curses.COLOR_BLACK) height, width = stdscr.getmaxyx() title = \\"To-Do List Manager\\" tasks = [] current_task = 0 while True: stdscr.clear() # Draw header and footer stdscr.attron(curses.color_pair(1)) stdscr.addstr(0, 0, title) stdscr.addstr(height - 1, 0, \\"Press \'a\' to add, \'d\' to mark done, \'x\' to delete, \'q\' to quit\\") stdscr.attroff(curses.color_pair(1)) # Display tasks for idx, (task, done) in enumerate(tasks): if idx == current_task: stdscr.attron(curses.A_REVERSE) if done: stdscr.attron(curses.color_pair(2)) stdscr.addstr(idx + 2, 0, task) if done: stdscr.attroff(curses.color_pair(2)) if idx == current_task: stdscr.attroff(curses.A_REVERSE) # Refresh screen stdscr.refresh() # Handle user input key = stdscr.getch() if key == ord(\'q\'): break elif key == ord(\'a\'): stdscr.addstr(height - 2, 0, \\"Enter new task: \\") stdscr.refresh() curses.echo() new_task = stdscr.getstr(height - 2, 17, 40).decode(\\"utf-8\\") curses.noecho() tasks.append([new_task, False]) elif key == curses.KEY_UP and current_task > 0: current_task -= 1 elif key == curses.KEY_DOWN and current_task < len(tasks) - 1: current_task += 1 elif key == ord(\'d\') and tasks: tasks[current_task][1] = not tasks[current_task][1] elif key == ord(\'x\') and tasks: del tasks[current_task] current_task = min(current_task, len(tasks) - 1) # Cleanup curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"Memoized Fibonacci and Factorial Calculations You are required to implement a class `MathUtils` that provides efficient methods to calculate Fibonacci numbers and factorials. To achieve this, you must use Python\'s `functools` module features such as `lru_cache` for caching results. Additionally, implement a method to compare the performance of cached versus non-cached calculations. Requirements 1. **Class Definition**: - Define a class `MathUtils`. 2. **Methods**: - `fibonacci(self, n: int) -> int` - Calculates the nth Fibonacci number. - Use `lru_cache` to cache results for efficient computation. - `factorial(self, n: int) -> int` - Calculates the factorial of n. - Use `lru_cache` to cache results for efficient computation. - `compare_performance(self, func: callable, n: int) -> None` - Takes a function and an integer n, and compares the performance of the cached and non-cached versions of the function when called with n. - Print the time taken for each version to complete. Input and Output Format - Both `fibonacci` and `factorial` methods take an integer `n` and return the respective calculated value. - The `compare_performance` method takes a function `func` and an integer `n`, and prints out the time taken by the cached and non-cached versions to compute the result. Constraints - `n` for `fibonacci` and `factorial` methods should be a non-negative integer. - The comparison should accurately reflect the performance differences. Example Usage ```python from functools import lru_cache import time class MathUtils: @lru_cache(maxsize=None) def fibonacci(self, n: int) -> int: if n < 2: return n return self.fibonacci(n-1) + self.fibonacci(n-2) @lru_cache(maxsize=None) def factorial(self, n: int) -> int: if n == 0: return 1 return n * self.factorial(n-1) def compare_performance(self, func: callable, n: int) -> None: # Non-cached performance start_time = time.time() result = func(n) non_cached_time = time.time() - start_time print(f\\"Non-Cached {func.__name__}({n}) = {result}, Time: {non_cached_time:.4f} seconds\\") # Clear cache for fresh start func.cache_clear() # Cached performance start_time = time.time() result = func(n) cached_time = time.time() - start_time print(f\\"Cached {func.__name__}({n}) = {result}, Time: {cached_time:.4f} seconds\\") # Example: math_utils = MathUtils() math_utils.compare_performance(math_utils.fibonacci, 30) math_utils.compare_performance(math_utils.factorial, 30) ``` By completing this question, you should demonstrate your understanding of how to utilize `functools.lru_cache` and compare the efficiency improvements provided by caching.","solution":"from functools import lru_cache import time class MathUtils: @lru_cache(maxsize=None) def fibonacci(self, n: int) -> int: if n < 2: return n return self.fibonacci(n-1) + self.fibonacci(n-2) @lru_cache(maxsize=None) def factorial(self, n: int) -> int: if n == 0: return 1 return n * self.factorial(n-1) def compare_performance(self, func: callable, n: int) -> None: no_cache_func = func.__wrapped__ # Non-cached performance start_time = time.time() result = no_cache_func(self, n) non_cached_time = time.time() - start_time print(f\\"Non-Cached {func.__name__}({n}) = {result}, Time: {non_cached_time:.4f} seconds\\") # Clear cache for fresh start func.cache_clear() # Cached performance start_time = time.time() result = func(n) cached_time = time.time() - start_time print(f\\"Cached {func.__name__}({n}) = {result}, Time: {cached_time:.4f} seconds\\") # Example usage: # math_utils = MathUtils() # math_utils.compare_performance(math_utils.fibonacci, 30) # math_utils.compare_performance(math_utils.factorial, 30)"},{"question":"# PyTorch Accelerator Management In this task, you are required to write a few functions using PyTorch\'s `torch.accelerator` module to manage device resources and synchronize operations. This will demonstrate your understanding of handling GPUs in PyTorch. Task 1: Check Device Availability Write a function `check_device_availability` that returns `True` if at least one accelerator (GPU) is available, and `False` otherwise. ```python def check_device_availability() -> bool: Checks if any accelerators are available. Returns: bool: True if at least one accelerator is available, False otherwise. pass ``` Task 2: Get Current Device Index Write a function `get_current_device` that returns the index of the current device being used. ```python def get_current_device() -> int: Gets the current accelerator device index. Returns: int: The index of the current device. pass ``` Task 3: Set Device by Index Write a function `set_device_by_index` that sets the device to the provided index. ```python def set_device_by_index(device_index: int) -> None: Sets the current device to the specified index. Args: device_index (int): The index of the device to set. pass ``` Task 4: Synchronize Device Write a function `synchronize_device` that synchronizes the current device, ensuring that all previously enqueued operations are complete. ```python def synchronize_device() -> None: Synchronizes the current device to ensure all previous operations are complete. pass ``` Task 5: Test the Module Create a test function `test_accelerator_module` that demonstrates the following: 1. Checks if an accelerator is available. 2. If available, sets the device to the first accelerator (index 0). 3. Prints the index of the current device. 4. Synchronizes the current device. ```python def test_accelerator_module() -> None: Tests the accelerator module by: 1. Checking device availability. 2. Setting the device to the first accelerator. 3. Printing the current device index. 4. Synchronizing the current device. pass ``` # Notes: - Handle the cases where no accelerator is available gracefully. - You are only allowed to use functions from the `torch.accelerator` module. # Example The exact output may vary based on the environment and the devices available, but here\'s an example of how it might look: ```python if check_device_availability(): print(\\"Device available.\\") else: print(\\"No device available.\\") set_device_by_index(0) print(f\\"Current device index: {get_current_device()}\\") synchronize_device() print(\\"Device synchronized.\\") ```","solution":"import torch def check_device_availability() -> bool: Checks if any accelerators are available. Returns: bool: True if at least one accelerator is available, False otherwise. return torch.cuda.is_available() def get_current_device() -> int: Gets the current accelerator device index. Returns: int: The index of the current device. return torch.cuda.current_device() def set_device_by_index(device_index: int) -> None: Sets the current device to the specified index. Args: device_index (int): The index of the device to set. if torch.cuda.is_available(): torch.cuda.set_device(device_index) def synchronize_device() -> None: Synchronizes the current device to ensure all previous operations are complete. if torch.cuda.is_available(): torch.cuda.synchronize() def test_accelerator_module() -> None: Tests the accelerator module by: 1. Checking device availability. 2. Setting the device to the first accelerator. 3. Printing the current device index. 4. Synchronizing the current device. if check_device_availability(): print(\\"Device available.\\") set_device_by_index(0) print(f\\"Current device index: {get_current_device()}\\") synchronize_device() print(\\"Device synchronized.\\") else: print(\\"No device available.\\")"},{"question":"# Question: Advanced Graph Manipulation with torch.fx **Objective:** Develop a transformation that decomposes all ReLU operations into their mathematical equivalent operations and replaces all additions (`torch.add`) with multiplications (`torch.mul`) within a given PyTorch model. Validate the transformation by comparing the transformed and original model outputs for a given input. **Task:** 1. **Symbolically Trace the Model**: Capture the model as a `Graph`. 2. **Decompose ReLU Operations**: Implement a transformation that replaces all `F.relu(x)` operations with `(x > 0) * x`. 3. **Replace Addition with Multiplication**: Modify the graph to replace `torch.add` with `torch.mul`. 4. **Recompile the Transformed Graph**: Ensure the transformed graph is executable. 5. **Compare Outputs**: Verify that the output of the transformed model matches the original model when provided with the same input. **Input and Output:** - **Function Signature**: ```python def transform_and_validate(model: torch.nn.Module, input_tensor: torch.Tensor) -> bool: # Your implementation here ``` - **Inputs**: - `model` (torch.nn.Module): The PyTorch model to be transformed. - `input_tensor` (torch.Tensor): A tensor input to validate the model. - **Output**: - `bool`: Return `True` if the transformed model produces the same result as the original model for the given input tensor; otherwise, `False`. **Constraints:** - The model may contain multiple ReLU and addition operations. - Ensure numerical stability by using `torch.allclose()` for comparison. - Handle edge cases where nodes might not have direct replacements. **Examples:** 1. Given a simple model with ReLU and addition operations: ```python class SimpleModel(torch.nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = torch.nn.Linear(10, 10) def forward(self, x): y = self.linear(x) return F.relu(y) + y ``` and an input tensor: ```python input_tensor = torch.randn(1, 10) ``` 2. After the transformation, the graph should replace `F.relu(y)` with `(y > 0) * y` and `y + y` with `y * y`. # Implementation Steps: 1. **Import Required Modules**: ```python import torch import torch.fx import torch.nn.functional as F ``` 2. **Decompose ReLU Transformation Function**: ```python def relu_decomposition(x): return (x > 0) * x ``` 3. **Graph Transformation**: - Trace the model. - Transform the graph: - Replace ReLU. - Replace `torch.add` with `torch.mul`. - Recompile the graph. 4. **Validation**: - Feed the input through both the original and transformed models. - Compare the results using `torch.allclose()`. **Note**: Implement error handling to gracefully manage any unexpected graph structures. ```python def transform_and_validate(model: torch.nn.Module, input_tensor: torch.Tensor) -> bool: decomposition_rules = { F.relu: lambda x: (x > 0) * x } class CustomTracer(torch.fx.Tracer): def is_leaf_module(self, m, module_qualified_name): return False def transform(graph: torch.fx.Graph) -> torch.fx.Graph: new_graph = torch.fx.Graph() env = {} tracer = torch.fx.proxy.GraphAppendingTracer(new_graph) for node in graph.nodes: if node.op == \'call_function\' and node.target in decomposition_rules: proxy_args = [torch.fx.Proxy(env[x.name], tracer) if isinstance(x, torch.fx.Node) else x for x in node.args] output_proxy = decomposition_rules[node.target](*proxy_args) new_node = output_proxy.node env[node.name] = new_node elif node.op == \'call_function\' and node.target == torch.add: with new_graph.inserting_after(node): new_node = new_graph.call_function(torch.mul, node.args, node.kwargs) env[node.name] = new_node else: new_node = new_graph.node_copy(node, lambda x: env[x.name]) env[node.name] = new_node new_graph.lint() return new_graph def apply_transform(module): original_graph = CustomTracer().trace(module) transformed_graph = transform(original_graph) transformed_module = torch.fx.GraphModule(module, transformed_graph) transformed_module.recompile() return transformed_module transformed_model = apply_transform(model) original_output = model(input_tensor) transformed_output = transformed_model(input_tensor) return torch.allclose(original_output, transformed_output) # Example usage: if __name__ == \\"__main__\\": model = SimpleModel() input_tensor = torch.randn(1, 10) result = transform_and_validate(model, input_tensor) print(f\\"Transformation validation result: {result}\\") ```","solution":"import torch import torch.fx import torch.nn.functional as F def transform_and_validate(model: torch.nn.Module, input_tensor: torch.Tensor) -> bool: decomposition_rules = { F.relu: lambda x: (x > 0) * x, } class CustomTracer(torch.fx.Tracer): def is_leaf_module(self, m, module_qualified_name): return False def transform(graph: torch.fx.Graph) -> torch.fx.Graph: new_graph = torch.fx.Graph() env = {} tracer = torch.fx.proxy.GraphAppendingTracer(new_graph) for node in graph.nodes: if node.op == \'call_function\' and node.target in decomposition_rules: proxy_args = [torch.fx.Proxy(env[x.name], tracer) if isinstance(x, torch.fx.Node) else x for x in node.args] output_proxy = decomposition_rules[node.target](*proxy_args) new_node = output_proxy.node env[node.name] = new_node elif node.op == \'call_function\' and node.target == torch.add: proxy_args = [torch.fx.Proxy(env[x.name], tracer) if isinstance(x, torch.fx.Node) else x for x in node.args] new_node = new_graph.call_function(torch.mul, tuple(proxy_args)) env[node.name] = new_node else: new_node = new_graph.node_copy(node, lambda x: env[x.name]) env[node.name] = new_node new_graph.lint() return new_graph def apply_transform(module): original_graph = CustomTracer().trace(module) transformed_graph = transform(original_graph) transformed_module = torch.fx.GraphModule(module, transformed_graph) transformed_module.recompile() return transformed_module transformed_model = apply_transform(model) model.eval() transformed_model.eval() with torch.no_grad(): original_output = model(input_tensor) transformed_output = transformed_model(input_tensor) return torch.allclose(original_output, transformed_output, rtol=1e-3, atol=1e-6)"},{"question":"**Coding Assessment Question** # Objective: To assess your understanding of environment variable manipulation and file handling in Python using the `os` module, which supersedes the `posix` module interface. # Problem Statement: You are required to write a Python function that modifies the system\'s environment variables and performs file-related operations using the `os` module. Your function needs to demonstrate the following functionalities: 1. **Environment Variable Modification:** - Retrieve the value of an existing environment variable (e.g., `USER`). - Modify or add a new environment variable (e.g., `MY_VAR` with the value `\'Hello, World!\'`). - Verify the modification by retrieving the updated value. 2. **File Handling Operations:** - Create a new file named `testfile.txt` in the current directory. - Write a specified string (e.g., \\"This is a test file.\\") to the file. - Read the content from `testfile.txt` and return it. # Function Signature ```python def modify_env_and_file_operations() -> str: pass ``` # Input: The function does not take any arguments. # Output: - A string containing the content read from `testfile.txt`. # Requirements: 1. Do not use the `posix` module directly; use the `os` module instead. 2. Handle any potential exceptions that might occur during environment manipulation and file operations. 3. Ensure that the file `testfile.txt` is created and written to in the current working directory. 4. If `os` does not support permission to modify environment variables, handle the exception and proceed with the file operations. # Example Usage: ```python result = modify_env_and_file_operations() print(result) # Output should be: \\"This is a test file.\\" ``` # Constraints: - The function should be compatible with Python 3.8 or later. - No additional libraries other than `os` should be used. # Evaluation Criteria: - Correctness of environment variable modification. - Proper file creation, writing, and reading. - Exception handling. - Code readability and comments explaining the process.","solution":"import os def modify_env_and_file_operations() -> str: try: # Retrieve the value of an existing environment variable (for example, USER) existing_env = os.getenv(\'USER\') print(f\'Existing USER environment variable: {existing_env}\') # Modify or add a new environment variable (e.g., MY_VAR with the value \'Hello, World!\') os.environ[\'MY_VAR\'] = \'Hello, World!\' # Verify the modification by retrieving the updated value modified_env = os.getenv(\'MY_VAR\') print(f\'Modified MY_VAR environment variable: {modified_env}\') # File Handling Operations file_path = \'testfile.txt\' data_to_write = \\"This is a test file.\\" # Create and write to the file with open(file_path, \'w\') as file: file.write(data_to_write) # Read the content from the file with open(file_path, \'r\') as file: file_content = file.read() return file_content except Exception as e: print(f\'An error occurred: {e}\') return str(e)"},{"question":"# PyTorch Coding Assessment Question **Objective:** Implement a metric logger for distributed training using PyTorch, specifically leveraging the `torch.distributed.elastic.metrics` module. **Description:** You are tasked with implementing a custom metric logger using the `torch.distributed.elastic.metrics` module. Your implementation should configure the metrics system, log specific metrics during the training process, and profile certain training steps to evaluate performance. **Requirements:** 1. **Configuration**: - Write a function `configure_metrics()` that configures the metrics system using the `configure` method from the `torch.distributed.elastic.metrics` module. 2. **Logging Metrics**: - Implement a function `log_metric(step: int, value: float)` that logs a given metric value for a specified training step using the `put_metric` method from the `torch.distributed.elastic.metrics` module. The metric should be appropriately named to indicate the step value. 3. **Profiling Training Steps**: - Create a context manager `profile_step()` using the `prof` method that profiles the execution time of a specific training step or code block within the distributed training process. **Details:** 1. The `configure_metrics()` function will not take any input and does not need to return anything. It should configure the metric system using default parameters. 2. The `log_metric(step: int, value: float)` function takes in an integer `step` value and a float `value` representing the metric to log. It should use the `put_metric` method to log this information. 3. The `profile_step()` context manager should use the `prof` method to record the start and end times of the enclosed code block, outputting the time taken for this code block. **Function Signatures:** ```python def configure_metrics() -> None: pass def log_metric(step: int, value: float) -> None: pass from contextlib import contextmanager @contextmanager def profile_step() -> None: pass ``` **Constraints:** - You should assume the necessary imports from `torch.distributed.elastic.metrics` are already handled. - Your implementations should handle any edge cases that may arise, such as invalid input types. # Example Usage: ```python configure_metrics() for step in range(10): with profile_step(): # Simulating training code here metric_value = step * 1.5 # Example metric value computation log_metric(step, metric_value) ``` In this example: - The `configure_metrics()` function sets up the metric system. - In each training step, the `profile_step()` context manager profiles how long the code block (that simulates training) takes. - The `log_metric()` function logs a computed metric value for each step.","solution":"import torch.distributed.elastic.metrics as metrics from contextlib import contextmanager def configure_metrics() -> None: Configures the metrics system using the torch.distributed.elastic.metrics module. metrics.configure() def log_metric(step: int, value: float) -> None: Logs the given metric value for the specified training step. Parameters: step (int): The training step number. value (float): The metric value to be logged. metrics.put_metric(f\\"metric_step_{step}\\", value) @contextmanager def profile_step(): Context manager that profiles the execution time of a specific training step or code block using the `prof` method from the torch.distributed.elastic.metrics module. with metrics.prof(\\"profile_step\\") as p: yield p"},{"question":"# Problem 1: Customizing Seaborn Plots You are tasked with analyzing and visualizing a dataset using the Seaborn plotting library. The objective is to create clear, context-appropriate, and visually appealing graphs. You are provided with a dataset and required to implement a function that sets the appropriate context and visual properties. Function Signature ```python def customize_plot(data, x, y, context, font_scale=1, rc_params={}): Customize the seaborn plot based on the given context, font scale, and other specific parameters. Parameters: data (DataFrame): The input data frame. x (str): The x-axis variable name. y (str): The y-axis variable name. context (str): The context for the seaborn plot (e.g. \'paper\', \'notebook\', \'talk\', \'poster\'). font_scale (float): The scale of the font elements. rc_params (dict): Dictionary of other rc parameters to override. Returns: matplotlib.axes._subplots.AxesSubplot: The seaborn plot object. pass ``` Input - `data`: A pandas DataFrame containing the data to plot. - `x`: A string representing the column to be used for the x-axis. - `y`: A string representing the column to be used for the y-axis. - `context`: A string to set the context parameter for Seaborn (\'paper\', \'notebook\', \'talk\', \'poster\'). - `font_scale`: A float to independently scale the font elements (default is 1). - `rc_params`: A dictionary that overrides specific parameters in Seaborn\'s context settings (default is an empty dictionary). Output - The function should return the seaborn plot object of type `matplotlib.axes._subplots.AxesSubplot`. Constraints - The `data` DataFrame will have at least two columns for the x and y variables. - The `context` variable will be one of \'paper\', \'notebook\', \'talk\', \'poster\'. # Example Usage ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Example dataframe data = pd.DataFrame({ \'time\': [0, 1, 2, 3, 4, 5], \'value\': [1, 3, 2, 5, 4, 6] }) # Customizing the plot ax = customize_plot(data=data, x=\'time\', y=\'value\', context=\'talk\', font_scale=0.8, rc_params={\'lines.linewidth\': 2}) # Display the plot plt.show() ``` This function should demonstrate students\' comprehension of setting contexts and customizing plot appearance using Seaborn.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customize_plot(data, x, y, context, font_scale=1, rc_params={}): Customize the seaborn plot based on the given context, font scale, and other specific parameters. Parameters: data (DataFrame): The input data frame. x (str): The x-axis variable name. y (str): The y-axis variable name. context (str): The context for the seaborn plot (e.g. \'paper\', \'notebook\', \'talk\', \'poster\'). font_scale (float): The scale of the font elements. rc_params (dict): Dictionary of other rc parameters to override. Returns: matplotlib.axes._subplots.AxesSubplot: The seaborn plot object. # Set the Seaborn context and font scale sns.set_context(context, font_scale=font_scale, rc=rc_params) # Create the plot using Seaborn\'s \'lineplot\' ax = sns.lineplot(data=data, x=x, y=y) return ax"},{"question":"# Coding Challenge: Module Import Mechanism In this task, you will implement a function that mimics a key feature of the old `imp` module: finding and loading a module. You will use `importlib` to implement this mechanism instead of `imp`. Function Specification - **Function Name:** `custom_import` - **Input:** - `module_name` (str): The name of the module to import. - **Output:** - Returns the loaded module object. - If the module cannot be found or loaded, returns a custom `ImportError`. Requirements: 1. **Find and load** the module using `importlib`. 2. Handle potential exceptions that might occur during the process. 3. Ensure thread safety using appropriate locking mechanisms. Constraints: - You cannot use the `imp` module directly. ```python import importlib.util import sys import threading # Implement a thread-safe custom import function def custom_import(module_name): Custom logic to import and load a module by name using importlib. :param module_name: str : name of the module :return: module object if the module was successfully imported and loaded :raises: ImportError if the module cannot be found or loaded lock = threading.Lock() with lock: try: # Find the module spec spec = importlib.util.find_spec(module_name) if spec is None: raise ImportError(f\\"Module {module_name} not found\\") # Create a new module based on the spec module = importlib.util.module_from_spec(spec) sys.modules[module_name] = module # Execute the module in its own namespace spec.loader.exec_module(module) return module except Exception as e: raise ImportError(f\\"Error loading module {module_name}: {str(e)}\\") # Example usage: # custom_module = custom_import(\'math\') # print(custom_module.sqrt(16)) # Expected output: 4.0 ``` Notes: 1. `custom_import` maintains thread safety using a lock. 2. If the module cannot be found, an ImportError is raised with a custom error message. 3. Use the provided example to test your implementation.","solution":"import importlib.util import sys import threading # Implement a thread-safe custom import function def custom_import(module_name): Custom logic to import and load a module by name using importlib. :param module_name: str : name of the module :return: module object if the module was successfully imported and loaded :raises: ImportError if the module cannot be found or loaded lock = threading.Lock() with lock: try: # Find the module spec spec = importlib.util.find_spec(module_name) if spec is None: raise ImportError(f\\"Module {module_name} not found\\") # Create a new module based on the spec module = importlib.util.module_from_spec(spec) sys.modules[module_name] = module # Execute the module in its own namespace spec.loader.exec_module(module) return module except Exception as e: raise ImportError(f\\"Error loading module {module_name}: {str(e)}\\")"},{"question":"**Question: Implement a File Organizer** **Objective:** Write a Python script that organizes a given directory by moving files into subdirectories based on their file extensions. The goal is to demonstrate your understanding of file manipulation and directory operations using Python\'s standard library modules such as `os`, `shutil`, `glob`, and `pathlib`. **Description:** You need to implement a function `organize_directory(dir_path: str) -> None` that takes the path to a directory as input and organizes the files in that directory into subdirectories named after their file extensions. For example, all `.txt` files should be moved into a subdirectory named `txt`, all `.jpg` files should be moved into a subdirectory named `jpg`, and so on. **Requirements:** 1. Ignore any subdirectories present in the input directory. 2. Only move files, not directories. 3. Create subdirectories as needed based on file extensions. 4. If a file doesn\'t have an extension, it should be moved to a subdirectory named `others`. 5. Use the `pathlib` module for path manipulations. **Function Signature:** ```python def organize_directory(dir_path: str) -> None: ``` **Input:** - `dir_path`: A string representing the path to the directory that needs to be organized. **Output:** - The function does not return anything. **Example:** Suppose you have a directory structure as follows: ``` /example_dir file1.txt file2.jpg file3.txt readme song.mp3 ``` After calling `organize_directory(\\"/example_dir\\")`, the structure should be: ``` /example_dir txt/ file1.txt file3.txt jpg/ file2.jpg others/ readme mp3/ song.mp3 ``` **Constraints:** - The function should handle directories containing a large number of files efficiently. **Hints:** - Use `pathlib.Path.iterdir()` to iterate over directory contents. - Use `pathlib.Path.suffix` to get the file extension. - Use `shutil.move` to move files to the new directories.","solution":"import shutil from pathlib import Path def organize_directory(dir_path: str) -> None: # Convert the directory path to a Path object base_dir = Path(dir_path) # Iterate over all files and subdirectories in the given directory for item in base_dir.iterdir(): # Ignore directories if item.is_dir(): continue # Determine the target directory based on the file extension if item.suffix: target_dir = base_dir / item.suffix.lstrip(\'.\') else: target_dir = base_dir / \'others\' # Create the target directory if it does not exist target_dir.mkdir(exist_ok=True) # Move the file to the target directory shutil.move(str(item), str(target_dir / item.name))"},{"question":"# XML Document Manipulation and Querying You are tasked with creating and manipulating an XML document representing a book catalog. The catalog will have multiple books, and each book will have properties such as title, author, and year of publication. You are required to perform the following operations using the `xml.dom` module: Task: 1. Create an XML document containing a root element `<catalog>`. 2. Add three books to the catalog. Each book should be an element `<book>` with the attributes `id` (a unique identifier for the book). Each book element should contain: - A `<title>` child element with the title of the book. - An `<author>` child element with the author of the book. - A `<year>` child element with the year the book was published. 3. Write a function `find_books_by_author` that takes the XML document and an author\'s name, and returns a list of book titles written by that author. 4. Write a function `remove_book_by_id` that takes the XML document and a book id, and removes that book from the catalog. 5. Ensure to handle any potential exceptions that could arise from DOM operations (e.g., trying to remove a book that doesn\'t exist). Expected Input and Output: **Input:** - For function `find_books_by_author`: - `document`: The XML document object. - `author_name`: A string representing the author\'s name. - For function `remove_book_by_id`: - `document`: The XML document object. - `book_id`: A string representing the book\'s unique identifier. **Output:** - The function `find_books_by_author` should return a list of strings, each representing a book title by the specified author. - The function `remove_book_by_id` should return `True` if a book was removed successfully, otherwise `False` if no book with the given id was found. Example: ```python from xml.dom.minidom import Document, parseString # Create the XML document as specified doc = Document() root = doc.createElement(\'catalog\') doc.appendChild(root) # Adding books to the catalog books = [ {\'id\': \'1\', \'title\': \'Book One\', \'author\': \'Author A\', \'year\': \'2001\'}, {\'id\': \'2\', \'title\': \'Book Two\', \'author\': \'Author B\', \'year\': \'2002\'}, {\'id\': \'3\', \'title\': \'Book Three\', \'author\': \'Author A\', \'year\': \'2003\'} ] for book in books: book_element = doc.createElement(\'book\') book_element.setAttribute(\'id\', book[\'id\']) title_element = doc.createElement(\'title\') title_element.appendChild(doc.createTextNode(book[\'title\'])) author_element = doc.createElement(\'author\') author_element.appendChild(doc.createTextNode(book[\'author\'])) year_element = doc.createElement(\'year\') year_element.appendChild(doc.createTextNode(book[\'year\'])) book_element.appendChild(title_element) book_element.appendChild(author_element) book_element.appendChild(year_element) root.appendChild(book_element) # Function implementations def find_books_by_author(document, author_name): # Your implementation here pass def remove_book_by_id(document, book_id): # Your implementation here pass # Example Usage print(find_books_by_author(doc, \\"Author A\\")) # Output: [\'Book One\', \'Book Three\'] print(remove_book_by_id(doc, \\"2\\")) # Output: True print(remove_book_by_id(doc, \\"5\\")) # Output: False ``` Constraints: 1. Each book has a unique id attribute. 2. Document manipulation should be performed using the `xml.dom` module. 3. Ensure that the code is efficient and handles potential exceptions gracefully.","solution":"from xml.dom.minidom import Document def create_catalog(): doc = Document() root = doc.createElement(\'catalog\') doc.appendChild(root) return doc def add_book(document, book_id, title, author, year): book_element = document.createElement(\'book\') book_element.setAttribute(\'id\', book_id) title_element = document.createElement(\'title\') title_element.appendChild(document.createTextNode(title)) book_element.appendChild(title_element) author_element = document.createElement(\'author\') author_element.appendChild(document.createTextNode(author)) book_element.appendChild(author_element) year_element = document.createElement(\'year\') year_element.appendChild(document.createTextNode(year)) book_element.appendChild(year_element) document.documentElement.appendChild(book_element) def find_books_by_author(document, author_name): titles = [] for book in document.getElementsByTagName(\'book\'): author = book.getElementsByTagName(\'author\')[0].childNodes[0].data if author == author_name: title = book.getElementsByTagName(\'title\')[0].childNodes[0].data titles.append(title) return titles def remove_book_by_id(document, book_id): removed = False books = document.getElementsByTagName(\'book\') for book in books: if book.getAttribute(\'id\') == book_id: document.documentElement.removeChild(book) removed = True break return removed # Example usage: doc = create_catalog() add_book(doc, \'1\', \'Book One\', \'Author A\', \'2001\') add_book(doc, \'2\', \'Book Two\', \'Author B\', \'2002\') add_book(doc, \'3\', \'Book Three\', \'Author A\', \'2003\')"},{"question":"**Objective:** Using the Seaborn package, visualize the distribution of data to show insightful relationships using `swarmplot` and `catplot`. **Problem Statement:** You are provided with the `\\"tips\\"` dataset from Seaborn\'s built-in datasets. The goal is to create a set of visualizations that highlight the relationships between total bill amounts, the day of the week, and the time of day, while also incorporating smoker status. 1. **Load the Dataset:** Write a function `load_tips_dataset()` that loads the `\\"tips\\"` dataset from Seaborn. ```python def load_tips_dataset(): Loads the \'tips\' dataset from seaborn. Returns: DataFrame: The tips dataset. pass ``` 2. **Swarmplot Visualization:** Write a function `create_swarmplot(tips)` that takes in a DataFrame `tips` and creates a `swarmplot` showing the distribution of total bill amounts (`total_bill`) across different days of the week (`day`), colored by the time of day (`time`). Customize the plot to: - Split the categories using `hue` - Set the marker for points to \\"x\\" and increase the point size for better visibility. ```python def create_swarmplot(tips): Creates a swarmplot showing the distribution of total bill amounts across different days of the week, colored by time of day. Args: tips (DataFrame): The tips dataset. Returns: None pass ``` 3. **Faceted Swarmplot Comparison:** Write a function `create_facet_swarmplot(tips)` that takes in a DataFrame `tips` and uses `catplot` to create a faceted swarmplot for total bill amounts (`total_bill`) versus smoking status (`smoker`), with separate plots for each day of the week (`day`). ```python def create_facet_swarmplot(tips): Creates a faceted swarmplot showing the distribution of total bill amounts across smoker status, with separate plots for each day of the week. Args: tips (DataFrame): The tips dataset. Returns: None pass ``` **Input:** 1. The functions `create_swarmplot` and `create_facet_swarmplot` both take a Pandas DataFrame `tips` as input. **Output:** 1. `load_tips_dataset` returns a DataFrame containing the \\"tips\\" dataset. 2. `create_swarmplot` and `create_facet_swarmplot` do not return any value but should display the respective plots. **Constraints:** 1. Ensure proper handling of the dataset loading. 2. Customize the plots as specified. 3. Use appropriate Seaborn and Matplotlib functions for visualization. **Hints:** - Use `sns.swarmplot` for the `create_swarmplot` function. - Use `sns.catplot` with `kind=\\"swarm\\"` for the `create_facet_swarmplot` function. - Refer to the Seaborn documentation for additional customization options. **Performance Requirements:** - The functions should efficiently load and process the dataset and generate the visualizations without significant delay. ```python # Function signatures def load_tips_dataset(): Loads the \'tips\' dataset from seaborn. Returns: DataFrame: The tips dataset. return sns.load_dataset(\\"tips\\") def create_swarmplot(tips): Creates a swarmplot showing the distribution of total bill amounts across different days of the week, colored by time of day. Args: tips (DataFrame): The tips dataset. Returns: None sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"time\\", marker=\\"x\\", size=6) def create_facet_swarmplot(tips): Creates a faceted swarmplot showing the distribution of total bill amounts across smoker status, with separate plots for each day of the week. Args: tips (DataFrame): The tips dataset. Returns: None sns.catplot( data=tips, kind=\\"swarm\\", x=\\"total_bill\\", y=\\"smoker\\", hue=\\"time\\", col=\\"day\\", aspect=0.5 ) ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_tips_dataset(): Loads the \'tips\' dataset from seaborn. Returns: DataFrame: The tips dataset. return sns.load_dataset(\\"tips\\") def create_swarmplot(tips): Creates a swarmplot showing the distribution of total bill amounts across different days of the week, colored by time of day. Args: tips (DataFrame): The tips dataset. Returns: None plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", marker=\\"x\\", size=6) plt.title(\\"Total Bill Amounts by Day of Week, Colored by Time of Day\\") plt.show() def create_facet_swarmplot(tips): Creates a faceted swarmplot showing the distribution of total bill amounts across smoker status, with separate plots for each day of the week. Args: tips (DataFrame): The tips dataset. Returns: None g = sns.catplot( data=tips, kind=\\"swarm\\", x=\\"total_bill\\", y=\\"smoker\\", hue=\\"time\\", col=\\"day\\", aspect=0.6, height=6 ) g.fig.subplots_adjust(top=0.9) g.fig.suptitle(\\"Total Bill Amounts by Smoker Status and Day of the Week\\") plt.show()"},{"question":"**Objective:** Implement a Python function utilizing the `shelve` module to manage a persistent dictionary. This function should allow storing, updating, and retrieving nested dictionaries by handling mutable objects properly. Problem Statement: You are required to implement a function `manage_shelf(filename: str, operations: List[Tuple[str, Any]]) -> List[Any]` that performs a sequence of operations on a shelf stored in the file specified by `filename`. Each operation in the `operations` list is a tuple where the first element is a string specifying the operation type, and the remaining elements are operation-specific arguments. The function should return a list of results corresponding to retrieval operations. The supported operations are: - `\\"store\\"`: Store a value in the shelf. The tuple format is `(\\"store\\", key, value)`, where `key` is a string and `value` is the value to be stored (an arbitrary Python object). - `\\"retrieve\\"`: Retrieve a value from the shelf. The tuple format is `(\\"retrieve\\", key)`, where `key` is a string, and the function should append the value associated with `key` to the results list. If the key does not exist, append `None`. - `\\"update\\"`: Update an existing list in the shelf by appending a new element. The tuple format is `(\\"update\\", key, element)`, where `key` is a string, and `element` is the item to append. If the key does not exist or the value at `key` is not a list, raise a `KeyError`. Constraints: - You must use `shelve` without setting the `writeback` parameter to `True`. - You should handle exceptions where appropriate. - The shelf must be properly closed after all operations. Example: ```python def manage_shelf(filename, operations): # Your implementation here pass # Example usage: filename = \\"my_shelf.db\\" operations = [ (\\"store\\", \\"key1\\", [1, 2, 3]), (\\"retrieve\\", \\"key1\\"), # should retrieve [1, 2, 3] (\\"update\\", \\"key1\\", 4), # should update key1\'s list to [1, 2, 3, 4] (\\"retrieve\\", \\"key1\\"), # should retrieve [1, 2, 3, 4] (\\"retrieve\\", \\"key2\\") # should retrieve None since key2 does not exist ] results = manage_shelf(filename, operations) print(results) # Output: [[1, 2, 3], [1, 2, 3, 4], None] ``` In your implementation, ensure to handle the case where modifications to mutable objects (like lists) are properly stored back in the shelf. **Note:** Be sure to import the necessary modules and handle any potential exceptions that might arise during shelf operations, such as `KeyError` when a non-existent key is accessed.","solution":"import shelve from typing import List, Tuple, Any def manage_shelf(filename: str, operations: List[Tuple[str, Any]]) -> List[Any]: results = [] with shelve.open(filename) as shelf: for operation in operations: op_type = operation[0] if op_type == \\"store\\": _, key, value = operation shelf[key] = value elif op_type == \\"retrieve\\": _, key = operation results.append(shelf.get(key, None)) elif op_type == \\"update\\": _, key, element = operation if key in shelf and isinstance(shelf[key], list): temp_list = shelf[key] temp_list.append(element) shelf[key] = temp_list else: raise KeyError(f\\"Key {key} does not exist or is not a list.\\") return results"},{"question":"**Objective**: Demonstrate your knowledge of dynamic type creation and manipulation in Python using the `types` module. # Problem Statement You are required to dynamically create a class using the `types.new_class` function and manipulate its attributes using `types.SimpleNamespace`. Specifically, you will create a class representing a dynamic configuration system where you can add, update, and retrieve configuration settings. The class should also support iteration over its settings. # Requirements 1. **Class Creation**: Use `types.new_class` to create a class called `DynamicConfig` with no base classes. 2. **Attributes Management**: Use `SimpleNamespace` to manage the configuration attributes dynamically. 3. **Methods to Implement**: - `add_setting(name: str, value: Any)`: Adds a new configuration setting. - `update_setting(name: str, value: Any)`: Updates the value of an existing configuration setting. - `get_setting(name: str) -> Any`: Retrieves the value of a configuration setting. - `__iter__() -> Iterator[Tuple[str, Any]]`: Iterates over all configuration settings as `(name, value)` pairs. # Constraints - The configuration settings should be managed using `types.SimpleNamespace`. - Raise a `KeyError` if attempting to update a setting that does not exist. - Implement all methods exactly as described. # Example Usage ```python from types import new_class, SimpleNamespace def implement_dynamic_config(): # Step 1: Define the exec_body function def exec_body(ns): # A SimpleNamespace to hold configuration settings ns[\'settings\'] = SimpleNamespace() # Method to add a new setting def add_setting(cls, name, value): setattr(cls.settings, name, value) # Method to update an existing setting def update_setting(cls, name, value): if not hasattr(cls.settings, name): raise KeyError(f\\"Setting \'{name}\' does not exist\\") setattr(cls.settings, name, value) # Method to get a setting value def get_setting(cls, name): if not hasattr(cls.settings, name): raise KeyError(f\\"Setting \'{name}\' does not exist\\") return getattr(cls.settings, name) # Method to iterate over settings def __iter__(cls): for name in cls.settings.__dict__: yield (name, getattr(cls.settings, name)) ns[\'add_setting\'] = classmethod(add_setting) ns[\'update_setting\'] = classmethod(update_setting) ns[\'get_setting\'] = classmethod(get_setting) ns[\'__iter__\'] = classmethod(__iter__) # Step 2: Use new_class to create the DynamicConfig class DynamicConfig = new_class(\'DynamicConfig\', exec_body=exec_body) return DynamicConfig # Test the implementation DynamicConfig = implement_dynamic_config() config = DynamicConfig() config.add_setting(\'host\', \'localhost\') config.add_setting(\'port\', 8080) # Iterating over settings should yield (\'host\', \'localhost\') and (\'port\', 8080) for name, value in config: print(f\'{name}: {value}\') config.update_setting(\'port\', 9090) print(config.get_setting(\'port\')) # Output should be 9090 try: config.update_setting(\'username\', \'admin\') except KeyError as e: print(e) # Output should be \\"Setting \'username\' does not exist\\" ``` # Additional Notes - Ensure your implementation matches the example usage and handles edge cases appropriately. - This question assesses your understanding of dynamic class creation, attribute management using `SimpleNamespace`, and iterable protocol in Python.","solution":"from types import new_class, SimpleNamespace def implement_dynamic_config(): # Step 1: Define the exec_body function def exec_body(ns): # A SimpleNamespace to hold configuration settings ns[\'settings\'] = SimpleNamespace() # Method to add a new setting def add_setting(cls, name, value): setattr(cls.settings, name, value) # Method to update an existing setting def update_setting(cls, name, value): if not hasattr(cls.settings, name): raise KeyError(f\\"Setting \'{name}\' does not exist\\") setattr(cls.settings, name, value) # Method to get a setting value def get_setting(cls, name): if not hasattr(cls.settings, name): raise KeyError(f\\"Setting \'{name}\' does not exist\\") return getattr(cls.settings, name) # Method to iterate over settings def __iter__(cls): for name in cls.settings.__dict__: yield (name, getattr(cls.settings, name)) ns[\'add_setting\'] = classmethod(add_setting) ns[\'update_setting\'] = classmethod(update_setting) ns[\'get_setting\'] = classmethod(get_setting) ns[\'__iter__\'] = classmethod(__iter__) # Step 2: Use new_class to create the DynamicConfig class DynamicConfig = new_class(\'DynamicConfig\', exec_body=exec_body) return DynamicConfig"},{"question":"**Data Analysis with Pandas Extension Arrays and Data Types** # Objective: Your task is to create a series of operations on a DataFrame that utilizes various pandas data types and extension arrays effectively. # Background: You have been provided with sales data that includes dates, product categories, quantities sold, and sales amounts. The data types for this DataFrame need to be optimized for efficient storage and operations. The sales data is as follows: ```plaintext Date | Category | Quantity | Sales Amount ------------------------------------------------------------ 2022-01-01 08:00:00 | A | 12 | 120.0 2022-01-01 09:30:00 | B | 5 | 65.0 2022-01-02 11:00:00 | A | 8 | 90.0 2022-01-02 13:15:00 | C | 7 | 80.0 2022-01-03 14:45:00 | A | 10 | 110.0 2022-01-03 16:05:00 | B | 15 | 200.0 (add more rows if needed for a robust analysis) ``` # Task: 1. **DataFrame Creation and Type Conversion:** - Create a pandas DataFrame from the provided sales data. - Ensure that the `Date` column is converted to a timezone-aware `datetime64` dtype. - Convert the `Category` column to a `Categorical` dtype. - Convert the `Quantity` column to a `Nullable Integer` dtype. - Convert the `Sales Amount` column to a `Nullable Float` dtype. 2. **Data Manipulation and Analysis:** - Add a new column `Weekday` which indicates the day of the week for each sale. - Add a new column `Status` which categorizes sales amounts as \'High\' (>100), \'Medium\' (50-100), or \'Low\' (<50) using a `Categorical` dtype. - Compute the total sales amount for each weekday. - Filter the DataFrame to only include sales from weekends. - Create a summary report showing total quantities and sales amounts for each product category. # Constraints: - You must use pandas\' custom data types and extension arrays where applicable. - Efficiently handle missing data in Quantity and Sales Amount columns. - Assume the input data can be extended with more rows, so your solution should be scalable. # Input and Output: - **Input:** Hardcoded within the script as shown above. - **Output:** DataFrame manipulations and summary report printed to standard output. # Implementation Requirements: You must write a function named `process_sales_data()` that accomplishes the following: - Implements the aforementioned tasks. - Prints the original and manipulated DataFrames. - Prints the summary report. # Example Function Signature: ```python import pandas as pd def process_sales_data(): # Step-by-step implementation here ``` ```python if __name__ == \\"__main__\\": process_sales_data() ``` Use appropriate pandas methods and ensure to handle any potential issues with missing data and type conversions correctly.","solution":"import pandas as pd import numpy as np def process_sales_data(): # Create the sales data as a dictionary sales_data = { \'Date\': [ \'2022-01-01 08:00:00\', \'2022-01-01 09:30:00\', \'2022-01-02 11:00:00\', \'2022-01-02 13:15:00\', \'2022-01-03 14:45:00\', \'2022-01-03 16:05:00\' ], \'Category\': [\'A\', \'B\', \'A\', \'C\', \'A\', \'B\'], \'Quantity\': [12, 5, 8, 7, 10, 15], \'Sales Amount\': [120.0, 65.0, 90.0, 80.0, 110.0, 200.0] } # Create DataFrame df = pd.DataFrame(sales_data) # Convert the Date column to timezone-aware datetime df[\'Date\'] = pd.to_datetime(df[\'Date\']).dt.tz_localize(\'UTC\') # Convert Category to Categorical dtype df[\'Category\'] = df[\'Category\'].astype(\'category\') # Convert Quantity to Nullable Integer dtype df[\'Quantity\'] = df[\'Quantity\'].astype(\'Int64\') # Convert Sales Amount to Nullable Float dtype df[\'Sales Amount\'] = df[\'Sales Amount\'].astype(\'float64\') # Add Weekday column df[\'Weekday\'] = df[\'Date\'].dt.day_name() # Add Status column df[\'Status\'] = pd.cut(df[\'Sales Amount\'], bins=[-np.inf, 50, 100, np.inf], labels=[\'Low\', \'Medium\', \'High\']) # Compute total sales amount for each weekday total_sales_by_weekday = df.groupby(\'Weekday\')[\'Sales Amount\'].sum() # Filter DataFrame to include only weekend sales df_weekends = df[df[\'Weekday\'].isin([\'Saturday\', \'Sunday\'])] # Create summary report showing total quantities and sales amounts for each product category summary_report = df.groupby(\'Category\').agg({\'Quantity\': \'sum\', \'Sales Amount\': \'sum\'}) # Print the results print(\\"Original DataFrame:\\") print(df) print(\\"nTotal Sales Amount by Weekday:\\") print(total_sales_by_weekday) print(\\"nWeekend Sales DataFrame:\\") print(df_weekends) print(\\"nSummary Report by Category:\\") print(summary_report) if __name__ == \\"__main__\\": process_sales_data()"},{"question":"Objective Demonstrate comprehension of the seaborn library\'s `dark_palette` function by creating and applying color palettes to a plot. Task 1. Write a function `create_dark_palette(base_color, num_colors, is_continuous, input_system)` that: - Creates a sequential color palette using seaborn\'s `dark_palette()` function. - Parameters: - `base_color` (str or tuple): The base color for the palette, can be specified using a color name, hex code, or HUSL tuple. - `num_colors` (int): The number of colors in the palette. - `is_continuous` (bool): If True, return a continuous colormap, otherwise return a discrete palette. - `input_system` (str): The color system used to specify the base color (\'standard\', \'hex\', \'husl\'). - Returns: A seaborn palette or colormap object. 2. Generate a dataset containing 100 samples with two normally distributed variables, \'x\' and \'y\'. 3. Use the generated palette to plot a scatter plot of \'x\' vs \'y\' using seaborn. Input - `base_color`: A string or tuple representing the base color. Examples: `\\"seagreen\\"` for standard, `\\"#79C\\"` for hex code, `(20, 60, 50)` for HUSL tuple. - `num_colors`: An integer representing the number of colors in the palette. - `is_continuous`: A boolean indicating if a continuous colormap (`True`) or discrete palette (`False`) should be returned. - `input_system`: A string indicating the color system used (`\'standard\'`, `\'hex\'`, `\'husl\'`). Output - A scatter plot of \'x\' vs \'y\' with the color palette applied. Example ```python base_color = \\"seagreen\\" num_colors = 8 is_continuous = False input_system = \\"standard\\" create_dark_palette(base_color, num_colors, is_continuous, input_system) ``` Constraints - The number of colors `num_colors` should be at least 2 and at most 20. - The `input_system` should be one of `\'standard\'`, `\'hex\'`, `\'husl\'`. Performance Requirements - The function should execute within a reasonable time frame, suitable for Jupyter notebook execution. Note Use the seaborn library for your implementation. Make sure to import `seaborn` as `sns` and `matplotlib.pyplot` as `plt`.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_dark_palette(base_color, num_colors, is_continuous, input_system): Creates a sequential color palette using seaborn\'s dark_palette() function. Parameters: - base_color (str or tuple): The base color for the palette, can be specified using a color name, hex code, or HUSL tuple. - num_colors (int): The number of colors in the palette. - is_continuous (bool): If True, return a continuous colormap, otherwise return a discrete palette. - input_system (str): The color system used to specify the base color (\'standard\', \'hex\', \'husl\'). Returns: A seaborn palette or colormap object. if input_system not in [\'standard\', \'hex\', \'husl\']: raise ValueError(\\"Invalid input_system value. Must be \'standard\', \'hex\', or \'husl\'.\\") if num_colors < 2 or num_colors > 20: raise ValueError(\\"num_colors must be between 2 and 20, inclusive.\\") palette = sns.dark_palette(base_color, n_colors=num_colors, input=input_system, as_cmap=is_continuous) return palette # Generate a dataset containing 100 samples with two normally distributed variables np.random.seed(0) data = np.random.randn(100, 2) def plot_scatter_with_palette(palette, data): Plots a scatter plot of the provided data using the given palette. Parameters: - palette: The seaborn palette or colormap object. - data: The numpy array containing the data to plot. x = data[:, 0] y = data[:, 1] sns.scatterplot(x=x, y=y, palette=palette) plt.show() # Example usage base_color = \\"seagreen\\" num_colors = 8 is_continuous = False input_system = \\"standard\\" palette = create_dark_palette(base_color, num_colors, is_continuous, input_system) plot_scatter_with_palette(palette, data)"},{"question":"<|Analysis Begin|> The provided documentation details the functionality of the `zlib` module in Python, which provides compression and decompression methods using the zlib library. It includes descriptions of various functions such as `compress`, `decompress`, `adler32`, and `crc32`, as well as compression and decompression objects that facilitate working with data streams. Key points from the documentation: - `compress(data, level)`: Compresses bytes in `data`. - `decompress(data, wbits, bufsize)`: Decompresses bytes in `data`. - `compressobj()` and `decompressobj()`: Create compression and decompression objects for data streams. - `adler32(data, value)`: Computes an Adler-32 checksum. - `crc32(data, value)`: Computes a CRC checksum. The `zlib` module is fully detailed, covering the various compression levels, optional parameters, and exceptions raised. This information is sufficient for creating a challenging and clear coding problem that tests students\' understanding of using the `zlib` module in practice. <|Analysis End|> <|Question Begin|> # Problem Statement: You are working on a large-scale data storage project where data compression and decompression are critical for optimizing storage space and ensuring fast data transfer rates. Using the `zlib` module, you need to implement a utility that compresses and decompresses data read from files, calculating checksums for data integrity verification. **Function Requirements:** 1. **compress_file(input_filename: str, output_filename: str, level: int = -1) -> None** - Reads the content from `input_filename`, compresses it using the specified compression `level`, and writes the compressed data to `output_filename`. - Calculate and return the Adler-32 checksum of the original data. 2. **decompress_file(input_filename: str, output_filename: str, expected_checksum: int) -> bool** - Reads the compressed content from `input_filename`, decompresses it, and writes the decompressed data to `output_filename`. - Calculate the Adler-32 checksum of the decompressed data and compare it with `expected_checksum`. Return `True` if they match, `False` otherwise. **Input:** - `input_filename`: Path to the input file to be read. - `output_filename`: Path to the output file to write. - `level` (optional for `compress_file`): Compression level (default: `-1`). - `expected_checksum` (for `decompress_file`): Expected Adler-32 checksum of the decompressed data. **Output:** - For `compress_file`: None. Writes the compressed data to `output_filename` and calculates the checksum. - For `decompress_file`: Returns `True` if the checksum matches, `False` otherwise. Writes the decompressed data to `output_filename`. **Constraints:** - Ensure that the functions handle potential exceptions (e.g., file read/write errors, compression/decompression errors). - Use efficient memory management to handle large files. **Examples:** ```python # Example usage of compress_file and decompress_file functions # Compress the file \'input.txt\' to \'compressed.gz\' checksum = compress_file(\'input.txt\', \'compressed.gz\', level=6) # Decompress the file \'compressed.gz\' to \'output.txt\' and verify checksum is_valid = decompress_file(\'compressed.gz\', \'output.txt\', checksum) assert is_valid, \\"Checksum validation failed!\\" ``` # Notes: - You may use `zlib.compress` and `zlib.decompress` for single-step compression and decompression. - For stream-based compression/decompression, use `zlib.compressobj()` and `zlib.decompressobj()`. - Properly handle file I/O and exceptions to ensure robustness of the utility.","solution":"import zlib def compress_file(input_filename, output_filename, level=-1): Compresses the file specified by input_filename and writes the output to output_filename. Returns the Adler-32 checksum of the original file data. try: with open(input_filename, \'rb\') as f_input: data = f_input.read() checksum = zlib.adler32(data) compressed_data = zlib.compress(data, level) with open(output_filename, \'wb\') as f_output: f_output.write(compressed_data) return checksum except Exception as e: print(f\\"An error occurred during compression: {e}\\") return None def decompress_file(input_filename, output_filename, expected_checksum): Decompresses the file specified by input_filename and writes the output to output_filename. Verifies that the Adler-32 checksum of the decompressed data matches the expected_checksum. Returns True if the checksum is valid, False otherwise. try: with open(input_filename, \'rb\') as f_input: compressed_data = f_input.read() decompressed_data = zlib.decompress(compressed_data) checksum = zlib.adler32(decompressed_data) with open(output_filename, \'wb\') as f_output: f_output.write(decompressed_data) return checksum == expected_checksum except Exception as e: print(f\\"An error occurred during decompression: {e}\\") return False"},{"question":"You are tasked with designing a web crawler that respects the rules specified in a website\'s \\"robots.txt\\" file. To achieve this, you need to employ the `urllib.robotparser.RobotFileParser` class. # Question Write a function `web_crawler_analysis(base_url: str, user_agent: str) -> dict` that performs the following tasks: 1. Sets the URL of the \\"robots.txt\\" file of the given `base_url`. 2. Reads and parses the \\"robots.txt\\" file. 3. Checks whether the given `user_agent` is allowed to fetch a specific set of URLs (provided in a list within the function) from the website. 4. Retrieves the crawl delay for the given `user_agent`. 5. Retrieves the request rate for the given `user_agent`. 6. Retrieves the sitemaps listed in the \\"robots.txt\\" file. The function should return a dictionary with the following structure: ```python { \'can_fetch\': { \'<url_1>\': <True/False>, \'<url_2>\': <True/False>, ... }, \'crawl_delay\': <crawl_delay_value or None>, \'request_rate\': { \'requests\': <number_of_requests or None>, \'seconds\': <number_of_seconds or None> }, \'site_maps\': [<list_of_sitemaps or None>] } ``` # Expected Input and Output - **Input:** - `base_url` (str): The base URL of the website. - `user_agent` (str): The user agent string. - **Output:** - A dictionary containing the results of whether the user agent can fetch the given URLs, the crawl delay, the request rate, and the sitemaps. # Example ```python def web_crawler_analysis(base_url: str, user_agent: str) -> dict: # Your implementation here # Example usage: result = web_crawler_analysis(\\"http://www.example.com\\", \\"my_crawler\\") print(result) ``` # Constraints and Notes - The URLs to be checked for fetching should be defined within the function as a list. - Handle cases where the \\"robots.txt\\" file or specific rules are not available. - Use the provided methods from the `urllib.robotparser.RobotFileParser` class wherever applicable. - Ensure the function performs its task efficiently, even if some parts of the \\"robots.txt\\" file are not present or poorly formatted.","solution":"from urllib.robotparser import RobotFileParser from urllib.parse import urljoin def web_crawler_analysis(base_url: str, user_agent: str) -> dict: Analyzes the robots.txt file of a given website for a specified user agent. Parameters: - base_url (str): The base URL of the website to be analyzed. - user_agent (str): The user agent string to evaluate based on the robots.txt rules. Returns: - dict: A dictionary containing the results of the analysis. # URLs to be checked urls_to_check = [ base_url, # base url itself urljoin(base_url, \'/about\'), # adding some sample URLs for checking urljoin(base_url, \'/contact\') ] robots_url = urljoin(base_url, \'robots.txt\') robot_parser = RobotFileParser() robot_parser.set_url(robots_url) robot_parser.read() # Check if user agent can fetch the given URLs can_fetch = {url: robot_parser.can_fetch(user_agent, url) for url in urls_to_check} # Get crawl delay crawl_delay = robot_parser.crawl_delay(user_agent) # Get request rate request_rate = robot_parser.request_rate(user_agent) if request_rate is not None: request_rate = { \'requests\': request_rate.requests, \'seconds\': request_rate.seconds } # Get sitemaps site_maps = robot_parser.site_maps() result = { \'can_fetch\': can_fetch, \'crawl_delay\': crawl_delay, \'request_rate\': request_rate, \'site_maps\': site_maps } return result"},{"question":"**Problem Statement:** You are tasked with developing a program that reads CSV data, processes it, and writes the results back to a CSV file using Python\'s built-in `csv` module. This task is designed to assess your understanding of reading from and writing to CSV files. **Task:** Write a function `process_csv(input_file: str, output_file: str) -> None` that performs the following operations: 1. Reads data from a CSV file specified by `input_file`. Assume the CSV file contains a header row with the following columns: `id`, `name`, `score`. 2. Processes the CSV data as follows: - For each row, update the `score` by adding 10% to its current value. - Convert the `score` to an integer after updating. 3. Writes the processed data to a new CSV file specified by `output_file`. The output CSV file should have the same column structure as the input file. 4. If the input file is malformed (e.g., missing headers, rows with differing column counts), your function should handle these errors gracefully and print an appropriate error message. **Function Signature:** ```python def process_csv(input_file: str, output_file: str) -> None: ``` **Input:** - `input_file` (str): The path to the input CSV file. - `output_file` (str): The path to the output CSV file. **Output:** - The function should not return anything. It should generate a new CSV file with the processed data. **Example:** If the content of `input_file` is: ``` id,name,score 1,Alice,80 2,Bob,95 3,Charlie,70 ``` After processing, the content of `output_file` should be: ``` id,name,score 1,Alice,88 2,Bob,104 3,Charlie,77 ``` **Constraints:** - The CSV files are small enough to be read into memory. - The `score` in each row is guaranteed to be a valid integer. - The function should handle cases where the input file does not strictly follow the CSV format by printing an error message without terminating the program abruptly. **Notes:** - Consider using the `csv.DictReader` and `csv.DictWriter` classes to simplify handling CSV data with headers. - Remember to handle edge cases, such as when the `input_file` does not exist or is empty. **Example Implementation:** The solution should properly use the `csv` module functionalities, handle necessary exceptions, and adhere to the specified input and output formats.","solution":"import csv def process_csv(input_file: str, output_file: str) -> None: try: # Read data from input_file with open(input_file, mode=\'r\', newline=\'\') as infile: reader = csv.DictReader(infile) # Ensure the input file has the correct headers if reader.fieldnames != [\'id\', \'name\', \'score\']: print(\\"Error: Input file does not have the required headers [\'id\', \'name\', \'score\'].\\") return rows = [] for row in reader: try: score = int(row[\'score\']) # Update the score by adding 10% new_score = int(score + (score * 0.10)) row[\'score\'] = str(new_score) rows.append(row) except ValueError: print(f\\"Error: Malformed \'score\' value in row {row}.\\") return # Write the processed data to output_file with open(output_file, mode=\'w\', newline=\'\') as outfile: writer = csv.DictWriter(outfile, fieldnames=[\'id\', \'name\', \'score\']) writer.writeheader() for row in rows: writer.writerow(row) except FileNotFoundError: print(f\\"Error: File {input_file} not found.\\") except IOError as e: print(f\\"Error: I/O error({e.errno}): {e.strerror}.\\")"},{"question":"**Question:** # Fetching and Processing Web Data with `urllib.request` You are required to implement a Python function called `fetch_data` that fetches and processes data from a specified URL. The function should perform the following tasks: 1. **Fetch Data:** - Use the `urlopen` function from the `urllib.request` module to fetch data from a given URL. - The URL to be fetched is provided as an argument to the function. 2. **Handling POST Requests:** - If a dictionary of data is provided as an argument, send this data as a POST request. Ensure the data is appropriately encoded as HTTP form data. 3. **Adding Headers:** - The function should allow for optional headers to be added to the request. If headers are provided as an argument, they should be included in the request. 4. **Handling Exceptions:** - Implement robust exception handling to manage `URLError` and `HTTPError`. - For `URLError`, the function should print \\"Failed to reach the server.\\" along with the reason. - For `HTTPError`, the function should print \\"Server couldn\'t fulfill the request.\\" along with the error code. 5. **Return Data:** - If the request is successful, read and return the fetched data as a string. # Function Signature: ```python import urllib.request def fetch_data(url: str, data: dict = None, headers: dict = None) -> str: ... ``` # Input: - `url` (str): The URL to fetch data from. - `data` (dict, optional): Dictionary of data to be sent in a POST request. If `None`, a GET request should be made. - `headers` (dict, optional): Dictionary of HTTP headers to be included in the request. # Output: - A string containing the fetched data. # Constraints: - The provided URL will always be a valid HTTP or HTTPS URL. - The function should be capable of handling URLs that respond with both GET and POST methods. # Example Usage: ```python url = \'http://example.com/api/data\' data = {\'key1\': \'value1\', \'key2\': \'value2\'} headers = {\'User-Agent\': \'Mozilla/5.0\'} result = fetch_data(url, data, headers) print(result) ``` # Implementation Requirements: - Use `urllib.request.urlopen` for making the HTTP request. - Handle POST request data using `urllib.parse.urlencode` and ensure it is properly encoded as bytes. - Add headers to the request if provided. - Ensure robust exception handling for network errors and print appropriate messages for `URLError` and `HTTPError`. - Return the fetched data as a string if the request is successful.","solution":"import urllib.request import urllib.parse from urllib.error import URLError, HTTPError def fetch_data(url: str, data: dict = None, headers: dict = None) -> str: Fetch data from a given URL with optional POST data and headers. Parameters: url (str): The URL to fetch data from. data (dict, optional): Dictionary of data to be sent in a POST request. headers (dict, optional): Dictionary of HTTP headers to be included in the request. Returns: str: The fetched data as a string. try: if data is not None: # Encode the data for POST request data = urllib.parse.urlencode(data).encode(\'ascii\') req = urllib.request.Request(url, data=data) else: # GET request req = urllib.request.Request(url) # Adding headers if provided if headers is not None: for key, value in headers.items(): req.add_header(key, value) # Perform the request with urllib.request.urlopen(req) as response: return response.read().decode(\'utf-8\') except HTTPError as e: print(f\\"Server couldn\'t fulfill the request. Error code: {e.code}\\") except URLError as e: print(f\\"Failed to reach the server. Reason: {e.reason}\\") return \\"\\""},{"question":"**Coding Assessment Question:** # Secure Data Storage Using Hashlib Objective Design a secure system to store and verify user passwords using the hashlib module in Python. This includes generating secure password hashes and performing verification checks to authenticate users. Problem Statement You are required to implement a class `PasswordManager` that provides methods to hash passwords securely and verify them. Class: `PasswordManager` 1. **Method `hash_password`**: - **Input**: - `password` (string): The user\'s password. - `salt` (bytes, optional): A random salt to be added to the password hash. If no salt is provided, generate a new one using `os.urandom()`. - `iterations` (int, optional): The number of iterations to use for the key derivation. If not specified, use a default value of 100,000. - **Output**: - Returns a tuple `(hashed_password, salt)`. - `hashed_password` (string): The generated hashed password in hexadecimal format. - `salt` (bytes): The salt used in hashing. 2. **Method `verify_password`**: - **Input**: - `stored_password` (string): The stored hashed password. - `provided_password` (string): The password to verify. - `salt` (bytes): The salt used for hashing the stored password. - `iterations` (int, optional): The number of iterations used for the key derivation. If not specified, use a default value of 100,000. - **Output**: - Returns `True` if the provided password matches the stored password; `False` otherwise. Constraints - The `hash_password` function must generate a secure hash using `hashlib.pbkdf2_hmac`. - The `verify_password` function must validate the password using the same hash method and parameters. - Ensure that the salt is of sufficient length (e.g., 16 bytes) for security purposes. - The default number of iterations for key derivation should be at least 100,000. Example Usage ```python import os from password_manager import PasswordManager # Assume the class is defined in this module # Initialize PasswordManager pm = PasswordManager() # Generate salt salt = os.urandom(16) # Hash the password hashed_password, used_salt = pm.hash_password(\\"my_secure_password123\\", salt=salt) # Store and later verify password is_valid = pm.verify_password(hashed_password, \\"my_secure_password123\\", used_salt) print(is_valid) # Output: True # Attempt verification with a wrong password is_valid = pm.verify_password(hashed_password, \\"wrong_password\\", used_salt) print(is_valid) # Output: False ``` Code Implementation Implement the `PasswordManager` class with the specified methods.","solution":"import hashlib import os class PasswordManager: def hash_password(self, password, salt=None, iterations=100000): Hashes a password given a salt and number of iterations. Args: password (str): The user\'s password. salt (bytes, optional): A random salt. If not provided, it is generated. iterations (int, optional): The number of iterations. Defaults to 100000. Returns: tuple: (hashed_password, salt). hashed_password (str): The hashed password in hexadecimal format. salt (bytes): The salt used in hashing. if salt is None: salt = os.urandom(16) hashed_password = hashlib.pbkdf2_hmac( \'sha256\', password.encode(\'utf-8\'), salt, iterations ).hex() return hashed_password, salt def verify_password(self, stored_password, provided_password, salt, iterations=100000): Verifies a provided password against the stored hashed password. Args: stored_password (str): The stored hashed password. provided_password (str): The password to verify. salt (bytes): The salt used for hashing the stored password. iterations (int, optional): The number of iterations. Defaults to 100000. Returns: bool: True if the provided password matches the stored password, otherwise False. provided_hashed = hashlib.pbkdf2_hmac( \'sha256\', provided_password.encode(\'utf-8\'), salt, iterations ).hex() return stored_password == provided_hashed"},{"question":"**Objective:** Implement a Python class using the `dataclasses` module which models a task management system. This class should demonstrate the use of `dataclass` features such as default values, frozen dataclasses, `__post_init__` method, and keyword-only arguments. Additionally, you should provide a function that returns the details of all created tasks. **Requirements:** 1. **Class: `Task`** - **Fields:** - `title` (str): The title of the task. (Required) - `description` (str): A brief description of the task. (Optional, default: \\"No description provided\\") - `due_date` (str): The due date of the task in the format \'YYYY-MM-DD\'. (Keyword-only, default: \\"No due date set\\") - `priority` (int): The priority level of the task. (Keyword-only, default: 3) - `id` (int): A unique identifier for the task. (Auto-generated, read-only) - **Behavior:** - The class should be immutable (frozen). - The `id` field should be initialized automatically and should increment with each new instance of the `Task` class. - Utilize the `__post_init__` method to validate that the `due_date` follows the format \'YYYY-MM-DD\'. 2. **Function: `get_all_tasks()`** - Should return a list of dictionaries containing details of all created tasks. Use the `asdict()` from the `dataclasses` module to convert the tasks to dictionaries. **Constraints:** - Ensure that the `due_date` is a valid date in the format \'YYYY-MM-DD\'. - Raise a `ValueError` if the `due_date` is not in the correct format. **Input/Output Format:** - The `Task` class should be instantiated to create new tasks. - The `get_all_tasks()` function should be called to retrieve all created tasks. **Example:** ```python from dataclasses import dataclass, field, InitVar, asdict from typing import List @dataclass(frozen=True) class Task: title: str description: str = \\"No description provided\\" due_date: InitVar[str] = \\"No due date set\\" priority: int = 3 id: int = field(init=False, default_factory=lambda: Task._increment_id()) _id_counter: int = 0 # Class variable to track the unique IDs. def __post_init__(self, due_date: str): import re if due_date != \\"No due date set\\" and not re.match(r\'d{4}-d{2}-d{2}\', due_date): raise ValueError(f\\"Invalid due date format: {due_date}\\") object.__setattr__(self, \'due_date\', due_date) Task._id_counter += 1 @classmethod def _increment_id(cls): return cls._id_counter + 1 def get_all_tasks() -> List[dict]: return [asdict(task) for task in Task._task_list] # Example usage: task1 = Task(title=\\"Complete assignment\\") task2 = Task(title=\\"Buy groceries\\", due_date=\\"2023-11-01\\", priority=1) all_tasks = get_all_tasks() print(all_tasks) ``` **Note:** This question requires the understanding of several advanced `dataclass` features including `frozen`, `__post_init__`, keyword-only fields, and class-level field incrementing.","solution":"from dataclasses import dataclass, field, asdict from typing import List, ClassVar import re @dataclass(frozen=True) class Task: title: str description: str = \\"No description provided\\" due_date: str = field(default=\\"No due date set\\", repr=False) priority: int = field(default=3, repr=False) id: int = field(init=False) _id_counter: ClassVar[int] = 0 # Class variable to track the unique IDs. _task_list: ClassVar[List[\'Task\']] = [] # Class variable to keep track of all tasks. def __post_init__(self): if self.due_date != \\"No due date set\\" and not re.match(r\'d{4}-d{2}-d{2}\', self.due_date): raise ValueError(f\\"Invalid due date format: {self.due_date}\\") # Immutable, so using object.__setattr__ to set the auto-incremented id. object.__setattr__(self, \'id\', Task._id_counter + 1) Task._id_counter += 1 Task._task_list.append(self) def get_all_tasks() -> List[dict]: return [asdict(task) for task in Task._task_list]"},{"question":"**Question:** In this exercise, you are required to write a Python program using the `imaplib` module that connects to an IMAP server, logs in using a username and password, searches for emails that meet specific criteria, fetches the content of those emails, and performs certain operations on them. # Requirements: 1. Connect to the IMAP server securely using the `IMAP4_SSL` class. 2. Log in to the server with given credentials. 3. Select the \'INBOX\' mailbox. 4. Search for all emails from a specific sender (e.g., \\"sender@example.com\\"). 5. Fetch and print the \'Subject\' and \'Date\' of these emails. 6. Add a custom flag to these emails. 7. Close the connection properly. # Input: - IMAP server address (e.g., \\"imap.example.com\\") - Username (e.g., \\"user@example.com\\") - Password (e.g., \\"password\\") - Sender\'s email address to search for (e.g., \\"sender@example.com\\") # Output: - Print the \'Subject\' and \'Date\' of each fetched email. - Confirm flag addition for each email. # Example: ```python Email Subject: Meeting at 3 PM Email Date: 20-Oct-2023 Email Subject: Quarterly Report Email Date: 18-Oct-2023 Flag added to email with Subject: Meeting at 3 PM Flag added to email with Subject: Quarterly Report ``` # Constraints: - Ensure proper error handling and resource management. - Use appropriate methods from the `imaplib.IMAP4_SSL` class. - You may assume that the server address, username, password, and sender\'s email address are provided and valid. # Solution Template: ```python import imaplib import getpass import email from email.header import decode_header def main(): # Inputs server = \\"imap.example.com\\" username = \\"user@example.com\\" password = getpass.getpass(prompt=\\"Enter your password: \\") search_sender = \\"sender@example.com\\" # Connect to the server mail = imaplib.IMAP4_SSL(server) try: # Login to the account mail.login(username, password) # Select the mailbox mail.select(\\"inbox\\") # Search for emails from the specific sender status, messages = mail.search(None, f\'FROM \\"{search_sender}\\"\') # Convert messages to a list of email IDs message_ids = messages[0].split() for msg_id in message_ids: # Fetch the email by ID status, msg_data = mail.fetch(msg_id, \\"(RFC822)\\") for response_part in msg_data: if isinstance(response_part, tuple): msg = email.message_from_bytes(response_part[1]) # Decode email subject and date subject, encoding = decode_header(msg[\\"Subject\\"])[0] if isinstance(subject, bytes): subject = subject.decode(encoding if encoding else \\"utf-8\\") date = msg[\\"Date\\"] # Print the email subject and date print(f\\"Email Subject: {subject}\\") print(f\\"Email Date: {date}\\") # Add a custom flag to the email mail.store(msg_id, \'+FLAGS\', \'MyCustomFlag\') print(f\\"Flag added to email with Subject: {subject}\\") # Close the mailbox mail.close() # Logout from the mail mail.logout() except Exception as e: print(f\\"An error occurred: {e}\\") mail.logout() if __name__ == \\"__main__\\": main() ``` In this exercise, you will need to replace the email addresses and server address with appropriate values and handle any potential errors for robustness.","solution":"import imaplib import getpass import email from email.header import decode_header def fetch_emails_from_sender(server, username, password, search_sender): # Connect to the server using the IMAP4_SSL mail = imaplib.IMAP4_SSL(server) try: # Login to the account mail.login(username, password) # Select the mailbox mail.select(\\"inbox\\") # Search for emails from the specific sender status, messages = mail.search(None, f\'FROM \\"{search_sender}\\"\') if status != \'OK\': print(\\"No messages found!\\") return # Convert messages to a list of email IDs message_ids = messages[0].split() fetched_emails = [] for msg_id in message_ids: # Fetch the email by ID status, msg_data = mail.fetch(msg_id, \\"(RFC822)\\") for response_part in msg_data: if isinstance(response_part, tuple): msg = email.message_from_bytes(response_part[1]) # Decode email subject and date subject, encoding = decode_header(msg[\\"Subject\\"])[0] if isinstance(subject, bytes): subject = subject.decode(encoding if encoding else \\"utf-8\\") date = msg[\\"Date\\"] # Print the email subject and date print(f\\"Email Subject: {subject}\\") print(f\\"Email Date: {date}\\") # Add a custom flag to the email mail.store(msg_id, \'+FLAGS\', \'MyCustomFlag\') print(f\\"Flag added to email with Subject: {subject}\\") # Append fetched email details to the list fetched_emails.append({ \\"subject\\": subject, \\"date\\": date, \\"flagged\\": True }) # Close the mailbox mail.close() # Logout from the mail mail.logout() return fetched_emails except Exception as e: print(f\\"An error occurred: {e}\\") mail.logout() return None"},{"question":"**Question: Exploring Data with Pandas Plotting** You are provided with a dataset `student_scores.csv` containing students\' scores in various subjects. Your task is to analyze this dataset using the `pandas.plotting` module and create several plots that help visualize different aspects of the data. **Dataset Format:** The dataset `student_scores.csv` has the following columns: - `StudentID`: Unique identifier for each student. - `Math`: Score in Mathematics. - `Science`: Score in Science. - `English`: Score in English. - `History`: Score in History. An example of the dataset: ``` StudentID,Math,Science,English,History 1,78,88,90,70 2,55,65,70,60 3,99,95,93,85 ... ``` **Tasks:** 1. **Boxplot of Scores**: Create a boxplot for the scores in each subject to understand the distribution and identify any outliers. 2. **Scatter Matrix**: Create a scatter matrix for the scores to visualize pairwise relationships between different subjects. 3. **Parallel Coordinates Plot**: Create a parallel coordinates plot to visualize the scores of each student across all subjects. 4. **Andrews Curves**: Create an Andrews Curves plot to identify patterns in the students\' scores across different subjects. 5. **Autocorrelation Plot**: Create an autocorrelation plot for the Mathematics scores to analyze if scores are correlated over the student index. **Requirements:** - Use the `pandas.plotting` module to generate the plots. - Ensure that all plots have appropriate titles and labels for clarity. **Constraints:** - You may assume that the `student_scores.csv` file is available in the current working directory. **Function Signature:** ```python def analyze_student_scores(file_path: str) -> None: pass ``` **Input:** - `file_path`: A string representing the file path to the `student_scores.csv`. **Output:** - The function should display the required plots visually. There is no need to return any value. **Example Usage:** ```python analyze_student_scores(\'student_scores.csv\') ``` **Note:** - Ensure that you handle any potential issues with reading the dataset, such as missing values or incorrect file paths gracefully. - Demonstrate the usage of at least five different plotting functions from the `pandas.plotting` module.","solution":"import pandas as pd import matplotlib.pyplot as plt from pandas.plotting import ( scatter_matrix, parallel_coordinates, andrews_curves, autocorrelation_plot ) def analyze_student_scores(file_path: str) -> None: Analyzes the student scores by generating various plots using pandas.plotting. Parameters: file_path (str): The file path to the \'student_scores.csv\'. try: # Load the dataset df = pd.read_csv(file_path) # Check if the necessary columns exist required_columns = [\'Math\', \'Science\', \'English\', \'History\'] for column in required_columns: if column not in df.columns: raise ValueError(f\\"Missing required column: {column}\\") # Boxplot of Scores plt.figure(figsize=(10, 6)) df.boxplot(column=required_columns) plt.title(\'Boxplot of Student Scores by Subject\') plt.ylabel(\'Scores\') plt.show() # Scatter Matrix scatter_matrix(df[required_columns], figsize=(10, 10), diagonal=\'kde\') plt.suptitle(\'Scatter Matrix of Student Scores\') plt.show() # Parallel Coordinates Plot plt.figure(figsize=(12, 6)) parallel_coordinates(df[[\'StudentID\'] + required_columns], class_column=\'StudentID\', colormap=\'viridis\') plt.title(\'Parallel Coordinates Plot of Student Scores\') plt.ylabel(\'Scores\') plt.show() # Andrews Curves plt.figure(figsize=(12, 6)) andrews_curves(df, class_column=\'StudentID\', colormap=\'cool\') plt.title(\'Andrews Curves of Student Scores\') plt.ylabel(\'Scores\') plt.show() # Autocorrelation Plot plt.figure(figsize=(10, 6)) autocorrelation_plot(df[\'Math\']) plt.title(\'Autocorrelation Plot of Mathematics Scores\') plt.xlabel(\'Lag\') plt.ylabel(\'Autocorrelation\') plt.show() except FileNotFoundError: print(f\\"Error: The file at {file_path} was not found.\\") except pd.errors.EmptyDataError: print(f\\"Error: The file at {file_path} is empty.\\") except pd.errors.ParserError: print(f\\"Error: The file at {file_path} could not be parsed.\\") except ValueError as e: print(f\\"Error: {e}\\") # Example usage # analyze_student_scores(\'student_scores.csv\')"},{"question":"**Coding Question: Titanic Survival Data Visualization** You are provided with the Titanic dataset, which contains information about the passengers on the Titanic. Your task is to create a customized visualization using the Seaborn package to analyze the survival rates of different groups of passengers. # Requirements: 1. Load the Titanic dataset from Seaborn. 2. Create a single categorical plot with two subplots: - The first subplot should visualize the survival rates of passengers based on their `class` and `sex` using a bar plot with different colors for each sex. - The second subplot should visualize the age distribution of passengers based on their `class` and `who` using a violin plot with a split view for each group in the `who` category (man, woman, and child). 3. Customize the overall appearance: - Arrange the subplots in a single row. - Set the figure size to have a height of 5 units and an aspect ratio of 3. - Adjust the titles and axis labels for better readability. 4. Ensure the final plot is clean and properly structured, with no overlapping elements, and uses an aesthetically pleasing color scheme. # Input: - No input needed; the Titanic dataset is loaded within the code. # Output: - The output should be a Seaborn `FacetGrid` plot with the described customizations. # Constraints: - The code must use the `sns.catplot` function for creating plots. - The visualization should handle the dataset size efficiently without performance issues. # Example Function: ```python def visualize_titanic_data(): import seaborn as sns import matplotlib.pyplot as plt # Load Titanic dataset df = sns.load_dataset(\\"titanic\\") # Create a bar plot for survival rates based on class and sex g = sns.catplot( data=df, x=\\"class\\", y=\\"survived\\", hue=\\"sex\\", kind=\\"bar\\", height=5, aspect=3, col=\\"sex\\" ) g.set_axis_labels(\\"\\", \\"Survival Rate\\") g.set_titles(\\"{col_name}\\") g.set(ylim=(0, 1)) g.despine(left=True) # Create a violin plot for age distribution based on class and who sns.catplot( data=df, x=\\"who\\", y=\\"age\\", hue=\\"who\\", kind=\\"violin\\", split=True, height=5, aspect=3, col=\\"class\\" ) # Display the plots plt.show() # Call the function to display the visualization visualize_titanic_data() ``` Make sure your visualization meets all the specified requirements and looks clean and professional.","solution":"def visualize_titanic_data(): import seaborn as sns import matplotlib.pyplot as plt # Load Titanic dataset df = sns.load_dataset(\\"titanic\\") # Create figure with specified size plt.figure(figsize=(15, 5)) # Bar plot for survival rates based on class and sex bar_plot = sns.catplot( data=df, x=\\"class\\", y=\\"survived\\", hue=\\"sex\\", kind=\\"bar\\", height=5, aspect=3, ci=None ) bar_plot.set_axis_labels(\\"Passenger Class\\", \\"Survival Rate\\") bar_plot.set_titles(\\"Survival Rate by Class and Sex\\") bar_plot.set(ylim=(0, 1)) # Violin plot for age distribution based on class and who violin_plot = sns.catplot( data=df, x=\\"class\\", y=\\"age\\", hue=\\"who\\", kind=\\"violin\\", split=True, height=5, aspect=3 ) violin_plot.set_axis_labels(\\"Passenger Class\\", \\"Age Distribution\\") violin_plot.set_titles(\\"Age Distribution by Class and Who\\") # Display the plots plt.tight_layout() plt.show()"},{"question":"Objective Demonstrate proficiency in configuring plot themes and display settings using the seaborn.objects (`so`) module. Problem Statement You are provided with a dataset and need to visualize it using Seaborn. Your task is to create a seaborn plot with specific theme and display configurations using the seaborn.objects (`so`) module. Follow the steps and conditions below to complete the task. Dataset Assume you are given a DataFrame `df` with the following structure: ```python import pandas as pd data = { \\"Category\\": [\\"A\\", \\"B\\", \\"C\\", \\"D\\"], \\"Values\\": [4, 7, 1, 8] } df = pd.DataFrame(data) ``` Requirements 1. Import the required libraries. 2. Set the plot configuration theme so that: - The axes face color is white. - The overall style is \\"whitegrid\\". 3. Ensure the plot is displayed in SVG format. 4. Disable HiDPI scaling. 5. Set the display scaling to 0.8. 6. Create a bar plot using seaborn.objects (`so`) to visualize the `df` DataFrame such that: - The x-axis represents the `Category`. - The y-axis represents the `Values`. 7. Provide a title for the plot. 8. Reset the theme to Seaborn defaults after creating the plot. Expected Input and Output Formats - **Input**: No specific input function, but the DataFrame `df` will be provided as described. - **Output**: Display the plot with the specified configurations. Implementation Provide the complete implementation in the form of a function `create_customized_plot` that performs the above requirements. ```python def create_customized_plot(df): # Step 1: Import libraries import seaborn.objects as so import matplotlib.pyplot as plt # Step 2: Set plot configuration theme so.Plot.config.theme[\\"axes.facecolor\\"] = \\"white\\" from seaborn import axes_style so.Plot.config.theme.update(axes_style(\\"whitegrid\\")) # Step 3: Set display configurations so.Plot.config.display[\\"format\\"] = \\"svg\\" so.Plot.config.display[\\"hidpi\\"] = False so.Plot.config.display[\\"scaling\\"] = 0.8 # Step 4: Create the bar plot p = ( so.Plot(df, x=\\"Category\\", y=\\"Values\\") .add(so.Bars()) .label(title=\\"Custom Bar Plot with Seaborn.objects\\") ) # Display the plot p.show() # Step 5: Reset theme to Seaborn defaults so.Plot.config.theme.reset() ``` Use this function with the provided DataFrame `df` to generate a plot. Constraints - Ensure that the function does not raise exceptions. - The final plot should adhere to all specified configurations. Performance Requirements - The function must execute within a reasonable time for small to medium datasets (up to 1000 rows).","solution":"def create_customized_plot(df): # Step 1: Import libraries import seaborn.objects as so import matplotlib.pyplot as plt import seaborn as sns # Step 2: Set plot configuration theme sns.set_theme(style=\\"whitegrid\\") sns.set(rc={\'axes.facecolor\':\'white\'}) # Step 3: Set display configurations import IPython.display as display display.set_matplotlib_formats(\'svg\') plt.rcParams[\'figure.dpi\'] = 100 # Disable HiDPI scaling plt.rcParams[\'figure.figsize\'] = [6.4 * 0.8, 4.8 * 0.8] # Scale display size # Step 4: Create the bar plot p = ( so.Plot(df, x=\\"Category\\", y=\\"Values\\") .add(so.Bars()) .label(title=\\"Custom Bar Plot with Seaborn.objects\\") ) # Display the plot p.show() # Step 5: Reset theme to Seaborn defaults sns.set_theme() display.set_matplotlib_formats(\'png\')"},{"question":"# Pandas Global Options and Settings Objective Your task is to write a function `configure_pandas_settings(df: pd.DataFrame) -> pd.DataFrame` that performs the following: 1. **Set Global Options**: - Change the display precision for floating-point numbers to 3 decimal places. - Ensure that large integer values above 10**5 are displayed using scientific notation. - Increase the maximum number of rows displayed in the output to 100. 2. **Describe the options**: - Print descriptions for the `display.precision`, `display.float_format`, and `display.max_rows` options. 3. **Apply Context Manager**: - Within a context manager, temporarily set the maximum column width to 50 characters and display the DataFrame column-wise. Print the DataFrame after applying this context. 4. **Return the DataFrame** showing that the global settings persist outside the context. Constraints - Use `pandas` functions mentioned in the documentation snippet. - Assume `df` is a well-formed DataFrame and does not contain missing values. - The function should not change the content of the DataFrame, only its display settings. Input - `df`: A pandas DataFrame. Output - The function returns the input DataFrame `df`. Performance - The function should have minimal performance overhead, primarily concerned with changing display settings. Example Usage ```python import pandas as pd # Sample dataframe data = { \'A\': [123456, 234567, 345678], \'B\': [1.23456, 2.34567, 3.45678] } df = pd.DataFrame(data) # Your function df = configure_pandas_settings(df) ``` When you call the function, you should see: - Floating-point numbers displayed to 3 decimal places. - Large integers in scientific notation. - The context-specific display adjustments for column width and orientation. # Solution Template You can use the following template to help structure your solution: ```python import pandas as pd def configure_pandas_settings(df: pd.DataFrame) -> pd.DataFrame: # Set global options pd.set_option(\'display.precision\', 3) pd.set_option(\'display.float_format\', \'{:.3e}\'.format) pd.set_option(\'display.max_rows\', 100) # Print descriptions for specific options pd.describe_option(\'display.precision\') pd.describe_option(\'display.float_format\') pd.describe_option(\'display.max_rows\') # Apply context manager for temporary settings with pd.option_context(\'display.max_colwidth\', 50, \'display.expand_frame_repr\', False): print(df) return df ```","solution":"import pandas as pd def configure_pandas_settings(df: pd.DataFrame) -> pd.DataFrame: # Set global options pd.set_option(\'display.precision\', 3) pd.set_option(\'display.float_format\', \'{:.3e}\'.format) pd.set_option(\'display.max_rows\', 100) # Print descriptions for specific options pd.describe_option(\'display.precision\') pd.describe_option(\'display.float_format\') pd.describe_option(\'display.max_rows\') # Apply context manager for temporary settings with pd.option_context(\'display.max_colwidth\', 50, \'display.expand_frame_repr\', False): print(df) return df"},{"question":"You are required to implement a function that leverages parallelism to perform hyperparameter tuning on a machine learning model using scikit-learn. The function should use higher-level parallelism with joblib and lower-level parallelism with OpenMP. Additionally, you should ensure that the function can handle oversubscription issues by limiting the number of threads used. Function Signature ```python def tune_model_with_parallelism(data: Tuple[np.ndarray, np.ndarray], param_grid: dict, n_jobs: int) -> dict: Performs hyperparameter tuning on a given dataset using parallelism. Parameters: - data (Tuple[np.ndarray, np.ndarray]): A tuple containing the features (X) and target labels (y). - param_grid (dict): Dictionary with parameters names (string) as keys and lists of parameter settings to try as values. - n_jobs (int): Number of jobs/parallel processes to run. Returns: - dict: The best parameters found during the tuning. ``` Requirements 1. **Input Formats:** - `data`: A tuple containing two elements: - `X` (numpy.ndarray): Feature matrix. - `y` (numpy.ndarray): Target labels array. - `param_grid`: Dictionary with parameter names as keys and lists of parameter settings to try as values. - `n_jobs`: Integer specifying the number of parallel jobs to use. 2. **Output Format:** - Returns a dictionary containing the best parameters from the hyperparameter tuning. 3. **Constraints:** - Ensure that any oversubscription of threads is handled, i.e., do not use more threads than CPUs available. - Use proper configurations for both joblib and OpenMP parallelism. 4. **Performance Requirements:** - The implementation should be efficient in utilizing multi-core systems for parallel processing to reduce execution time. Instructions 1. Use the `GridSearchCV` class from scikit-learn for hyperparameter tuning. 2. Set up the `n_jobs` parameter for joblib parallelism. 3. Control the number of threads used by OpenMP to avoid oversubscription. 4. Implement additional configurations or environment variables if needed to meet these constraints. Example Usage ```python import numpy as np from sklearn.datasets import load_iris from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split # Load dataset iris = load_iris() X, y = iris.data, iris.target # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) data = (X_train, y_train) param_grid = {\'n_estimators\': [10, 50, 100], \'max_depth\': [None, 10, 20]} n_jobs = 4 # Tune model best_params = tune_model_with_parallelism(data, param_grid, n_jobs) print(\\"Best Parameters:\\", best_params) ``` **You will be evaluated on:** - Correctness and completeness of the function implementation. - Handling the parallelism configurations correctly. - Efficiency in execution time.","solution":"import numpy as np from sklearn.model_selection import GridSearchCV from sklearn.ensemble import RandomForestClassifier import os def tune_model_with_parallelism(data, param_grid, n_jobs): Performs hyperparameter tuning on a given dataset using parallelism. Parameters: - data (tuple): A tuple containing the features (X) and target labels (y). - param_grid (dict): Dictionary with parameters names (string) as keys and lists of parameter settings to try as values. - n_jobs (int): Number of jobs/parallel processes to run. Returns: - dict: The best parameters found during the tuning. X, y = data # Prevent oversubscription by setting the number of threads for OpenMP os.environ[\\"OMP_NUM_THREADS\\"] = str(max(1, n_jobs // 2)) # Create the model model = RandomForestClassifier() # Create GridSearchCV object grid_search = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=n_jobs, cv=5, verbose=1) # Fit the GridSearchCV object to the data grid_search.fit(X, y) # Return the best parameters found return grid_search.best_params_"},{"question":"**Objective:** Demonstrate your comprehension of seaborn by loading a dataset, manipulating data, and creating advanced visualizations. **Context**: You are provided with two datasets - \\"penguins\\" and \\"flights\\" for the year 1960. Your task is to perform data visualization using the seaborn package to create informative plots. **Task**: 1. Load the \\"penguins\\" and \\"flights\\" datasets using seaborn. 2. Create a horizontal bar plot showing the number of passengers each month in the year 1960. 3. Create a bar plot of the average body mass of penguins, separated by species and further grouped by sex. Add error bars representing the standard deviation. 4. Customize the bar plot to include different colors, transparency (alpha), and edge styles for different sexes. You need to implement the following function: ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_visualizations(): # 1. Load the datasets penguins = load_dataset(\\"penguins\\") flights = load_dataset(\\"flights\\").query(\\"year == 1960\\") # 2. Horizontal bar plot of passengers per month in 1960 so.Plot(flights[\\"month\\"], flights[\\"passengers\\"]).add(so.Bar()).scale(xscale=\'log\') plt.show() # 3. Bar plot of average body mass of penguins by species and sex with error bars plot = so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") plot.add(so.Bar(alpha=0.5), so.Agg(), so.Dodge()) plot.add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) plt.show() # Call the function to create the visualizations create_visualizations() ``` **Input Format**: - No inputs are needed as the datasets will be loaded within the function. **Output Format**: - The function should generate two plots: 1. A horizontal bar plot of passengers per month in 1960. 2. A bar plot of the average body mass of penguins with colors and error bars. **Constraints**: - Ensure that the plots are clear and properly labeled. - Use appropriate seaborn functionalities as demonstrated in the documentation. **Performance Requirements**: - The function should efficiently load and process the datasets. - The function should be capable of rendering the plots within a reasonable time frame typically expected for seaborn visualizations. By completing this task, you will demonstrate your proficiency in using seaborn for data visualization and your ability to apply advanced plotting techniques.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_visualizations(): # 1. Load the datasets penguins = sns.load_dataset(\\"penguins\\") flights = sns.load_dataset(\\"flights\\") flights_1960 = flights[flights[\'year\'] == 1960] # 2. Horizontal bar plot of passengers per month in 1960 plt.figure(figsize=(10, 8)) sns.barplot( x=\'passengers\', y=\'month\', data=flights_1960, palette=\'viridis\', order=[ \'January\', \'February\', \'March\', \'April\', \'May\', \'June\', \'July\', \'August\', \'September\', \'October\', \'November\', \'December\' ]) plt.title(\'Number of Passengers Each Month in 1960\') plt.xlabel(\'Number of Passengers\') plt.ylabel(\'Month\') plt.show() # 3. Bar plot of average body mass of penguins by species and sex with error bars plt.figure(figsize=(10, 8)) sns.barplot( x=\'species\', y=\'body_mass_g\', hue=\'sex\', data=penguins, ci=\'sd\', palette=\'Set2\', alpha=0.6, edgecolor=\'.2\') plt.title(\'Average Body Mass of Penguins by Species and Sex\') plt.xlabel(\'Species\') plt.ylabel(\'Body Mass (g)\') plt.legend(title=\'Sex\', loc=\'upper right\') plt.show() # Call the function to create the visualizations create_visualizations()"},{"question":"# Question: Custom Telnet Client Utility You are required to implement a custom Telnet client utility in Python using the `telnetlib` module. Your client will connect to a Telnet server, send a series of commands, handle responses, and perform basic command automation. Implement the following functions: 1. **connect_to_server(host: str, port: int = 23, timeout: int = 10) -> telnetlib.Telnet** - Connects to the specified Telnet server using the provided hostname, port, and timeout values. - Returns an instance of `telnetlib.Telnet` connected to the server. 2. **send_commands(tn: telnetlib.Telnet, commands: List[str]) -> List[str]** - Sends a list of commands to the connected Telnet server. - Each command is executed after receiving and verifying the prompt (`b\\" \\"`). - Returns the server responses to each command as a list of strings. 3. **expect_patterns(tn: telnetlib.Telnet, patterns: List[bytes], timeout: int = 5) -> Tuple[int, Any, bytes]** - Reads from the server until one of the patterns in the list matches. - Uses the `expect` method of the `Telnet` class. - Returns a tuple containing the index of the matched pattern, the match object, and the bytes read up till and including the match. 4. **set_callback(tn: telnetlib.Telnet, callback: Callable[[socket, int, int], None])** - Sets a callback function for option negotiation using `set_option_negotiation_callback`. Example usage: ```python import telnetlib def option_negotiation_callback(telnet_socket, command, option): print(f\\"Command: {command}, Option: {option}\\") # Connect to a local Telnet server tn = connect_to_server(\'localhost\') # Set the option negotiation callback set_callback(tn, option_negotiation_callback) # Send a series of commands and capture responses responses = send_commands(tn, [\'ls\', \'pwd\', \'whoami\']) for response in responses: print(response) # Read until \'login\' or \'Password\' patterns are encountered pattern_index, match, received_data = expect_patterns(tn, [b\\"login\\", b\\"Password\\"]) print(f\\"Pattern Index: {pattern_index}, Received Data: {received_data.decode(\'ascii\')}\\") # Close the connection tn.close() ``` Constraints: - Ensure the connection is properly closed using the `close` method after operations. - Handle potential exceptions such as `EOFError`, `OSError`, and any other relevant errors during connection and data operations. Evaluation: Your solution will be evaluated based on: - Correctness and robustness of the implementation. - Error handling and resource management. - Adherence to the function specifications. - Readability and organization of the code.","solution":"import telnetlib from typing import List, Tuple, Callable, Any def connect_to_server(host: str, port: int = 23, timeout: int = 10) -> telnetlib.Telnet: Connects to the specified Telnet server using the provided hostname, port, and timeout values. Returns an instance of `telnetlib.Telnet` connected to the server. tn = telnetlib.Telnet(host, port, timeout) return tn def send_commands(tn: telnetlib.Telnet, commands: List[str]) -> List[str]: Sends a list of commands to the connected Telnet server. Each command is executed after receiving and verifying the prompt (b\\" \\"). Returns the server responses to each command as a list of strings. responses = [] for command in commands: tn.write(command.encode(\'ascii\') + b\\"n\\") tn.read_until(b\\" \\") response = tn.read_very_eager().decode(\'ascii\') responses.append(response) return responses def expect_patterns(tn: telnetlib.Telnet, patterns: List[bytes], timeout: int = 5) -> Tuple[int, Any, bytes]: Reads from the server until one of the patterns in the list matches. Uses the `expect` method of the `Telnet` class. Returns a tuple containing the index of the matched pattern, the match object, and the bytes read up till and including the match. index, match, data = tn.expect(patterns, timeout) return index, match, data def set_callback(tn: telnetlib.Telnet, callback: Callable[[Any, int, int], None]): Sets a callback function for option negotiation using `set_option_negotiation_callback`. tn.set_option_negotiation_callback(callback)"},{"question":"Objective: Implement a simple distributed worker system using `PyTorch`\'s multiprocessing and error handling mechanisms described in the provided documentation. You will need to create a set of worker processes that perform a given task and properly handle any errors that occur during their execution. Description: You need to implement a function `distributed_worker_system` that: 1. Spawns a specified number of worker processes. 2. Each worker executes a provided function (`worker_function`) with given arguments. 3. Uses the `torch.distributed.elastic.multiprocessing.errors` module to handle and record any errors that occur in the worker processes. 4. Returns a dictionary with two keys: `success_count` (number of workers that completed successfully) and `errors` (list of error messages). Function Signature: ```python def distributed_worker_system(worker_function: Callable, args_list: List[Tuple], num_workers: int) -> Dict: Spawns multiple worker processes to execute the given worker function with the provided arguments. Parameters: - worker_function (Callable): a function to be executed by each worker process. - args_list (List[Tuple]): a list of argument tuples for each worker process. - num_workers (int): number of worker processes to spawn. Returns: - Dict: a dictionary with \'success_count\' indicating successful worker executions and \'errors\' listing any errors that occurred. pass ``` Constraints: - `worker_function` is any function that takes a tuple of arguments and may raise exceptions. - `args_list` contains exactly `num_workers` tuples. - Properly use the `torch.distributed.elastic.multiprocessing.errors` module for error handling. - Ensure the function is efficient and handles large numbers of workers gracefully. Example Usage: ```python def example_worker(x): if x < 0: raise ValueError(\\"Negative value!\\") return x * 2 args = [(1,), (2,), (-1,), (4,)] num_workers = 4 result = distributed_worker_system(example_worker, args, num_workers) print(result) # Output example: {\'success_count\': 3, \'errors\': [\'Negative value!\']} ``` Note: - You do not need to implement the `example_worker` function, it is provided only for clarity. - Focus on the implementation and correct usage of error handling mechanisms within the PyTorch package as described.","solution":"import torch import torch.multiprocessing as mp from torch.multiprocessing import Queue import torch.distributed.elastic.multiprocessing.errors as errors from typing import Callable, List, Tuple, Dict def worker_wrapper(worker_function: Callable, args: Tuple, error_queue: Queue): try: worker_function(*args) except Exception as e: error_queue.put(str(e)) errors.record_exception() # Record the error def distributed_worker_system(worker_function: Callable, args_list: List[Tuple], num_workers: int) -> Dict: if num_workers != len(args_list): raise ValueError(\\"num_workers must be equal to the length of args_list\\") error_queue = Queue() processes = [] success_count = 0 # Starting worker processes for i in range(num_workers): p = mp.Process(target=worker_wrapper, args=(worker_function, args_list[i], error_queue)) p.start() processes.append(p) # Collecting results for p in processes: p.join() errors_list = [] while not error_queue.empty(): errors_list.append(error_queue.get()) success_count = num_workers - len(errors_list) return { \\"success_count\\": success_count, \\"errors\\": errors_list }"},{"question":"# Question: Secure Note Storage System You are tasked with designing a secure note storage system using Python\'s cryptographic services. Your system will allow users to store notes securely by encrypting the contents and providing a means to verify the data\'s integrity. Task: 1. Implement a class `SecureNoteStorage` with the following methods: - `__init__(self)`: Initializes an empty storage system. - `store_secure_note(self, note_id: str, note_content: str, key: bytes) -> None`: Stores a securely hashed note. - `retrieve_secure_note(self, note_id: str, key: bytes) -> str`: Retrieves and verifies the note. 2. Use appropriate methods from the `hashlib`, `hmac`, and `secrets` modules to ensure: - Notes are securely hashed using a keyed-hashing technique. - Notes can only be retrieved correctly if the correct key is provided. Requirements: 1. **Storing Notes**: - The `store_secure_note` method should hash the `note_content` using HMAC (keyed-hashing) with the given `key` before storing it to ensure its security. - Each note should be stored with its `note_id` as the key in a dictionary, along with its HMAC. 2. **Retrieving Notes**: - The `retrieve_secure_note` method should verify the hashed content with the provided key. - If the key matches, the note should be returned. If the key does not match, an error message should be raised. Input and Output Formats: - The `note_id` is a string representing the unique identifier for the note. - The `note_content` is a string representing the message to be stored securely. - The `key` is a bytes object used for hashing and verifying the note. Constraints: 1. The `note_id` must be unique within the storage system. 2. The `key` used for storing and retrieving must be the same; otherwise, retrieval should fail. Examples: ```python # Example Usage storage = SecureNoteStorage() # Storing a note storage.store_secure_note(\'note1\', \'This is a private note\', b\'secret_key123\') # Retrieving the note try: content = storage.retrieve_secure_note(\'note1\', b\'secret_key123\') print(content) # Output: This is a private note except ValueError as e: print(e) # Attempting to retrieve with an incorrect key try: content = storage.retrieve_secure_note(\'note1\', b\'wrong_key\') except ValueError as e: print(e) # Output: Key does not match! ``` Ensure that your implementation is efficient and uses the cryptographic functions properly to store and verify the note contents.","solution":"import hashlib import hmac import secrets class SecureNoteStorage: def __init__(self): self._storage = {} def store_secure_note(self, note_id: str, note_content: str, key: bytes) -> None: hmac_obj = hmac.new(key, note_content.encode(\'utf-8\'), hashlib.sha256) self._storage[note_id] = (note_content, hmac_obj.hexdigest()) def retrieve_secure_note(self, note_id: str, key: bytes) -> str: if note_id not in self._storage: raise ValueError(\\"Note ID does not exist!\\") note_content, stored_hmac = self._storage[note_id] hmac_obj = hmac.new(key, note_content.encode(\'utf-8\'), hashlib.sha256) if not hmac.compare_digest(stored_hmac, hmac_obj.hexdigest()): raise ValueError(\\"Key does not match!\\") return note_content"},{"question":"**Coding Assessment Question** You are provided with the `penguins` dataset from seaborn\'s `load_dataset` function, which contains information about various species of penguins and measurements related to their body and bill sizes. Your task is to create a multifaceted seaborn plot using the `seaborn.objects` module that demonstrates the following capabilities: 1. Create a scatter plot showing the relationship between `bill_length_mm` and `bill_depth_mm` for different penguin species, with each species represented by a different color. 2. Facet the plot by the `sex` of the penguins. 3. Customize the labels for the x-axis, y-axis, and the color legend. 4. Ensure the title of each facet includes the sex of the penguins in that facet, formatted in uppercase. 5. Add a legend title describing the measurement type. # Input - **No input is required from the user**; the dataset should be loaded within the function. # Output - **A seaborn Plot** displaying `bill_length_mm` vs. `bill_depth_mm` scatter plot: - Each point should be colored by species. - The plot should be faceted by `sex`. - Custom x and y-axis labels. - A color legend for species with a custom legend title. - Facet titles should indicate the sex in uppercase. # Constraints - Use only seaborn and pandas libraries to load and manipulate the dataset. - The seaborn plot should be created using the `seaborn.objects` module. # Performance - The plot should render efficiently, even for the full penguins dataset, which contains a manageable number of entries. # Sample Python Code ```python import seaborn.objects as so from seaborn import load_dataset def create_penguin_plot(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot p = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .add(so.Dot(), color=\\"species\\") ) # Apply faceting, labels, and custom legend title p = ( p.facet(\\"sex\\") .label(x=\\"Bill Length (mm)\\", y=\\"Bill Depth (mm)\\", color=\\"Species\\") .label(title=str.upper, legend=\\"Species Legend\\") ) return p # Call the function to render the plot create_penguin_plot() ```","solution":"import seaborn.objects as so from seaborn import load_dataset def create_penguin_plot(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot p = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", color=\\"species\\") .add(so.Dot()) .facet(\\"sex\\") .label(x=\\"Bill Length (mm)\\", y=\\"Bill Depth (mm)\\", color=\\"Species\\") .label(title=lambda sex: sex.upper(), legend=\\"Species Legend\\") ) p.show() # Calling the function to create the plot create_penguin_plot()"},{"question":"# **Email Parsing and Analysis using `email.parser`** **Problem Statement:** You have been provided with email data that you need to parse, analyze and extract meaningful information from. The data may be in the form of bytes or strings and may come either from a complete file or incrementally from a stream. **Task:** 1. **Parsing Function:** Implement a function `parse_email_content` that can take input in three different formats: - A bytes-like object - A string - A file-like object (binary or text mode). 2. **Analyze Email:** Implement another function `analyze_email` that takes the parsed email object and: - Checks if the email is multipart. - Extracts all headers and prints them. - Lists any defects found during parsing. **Function Signatures:** ```python def parse_email_content(source: Union[bytes, str, io.IOBase]) -> email.message.EmailMessage: Parses the email content from the given source. Parameters: source (Union[bytes, str, io.IOBase]): The source of the email content, which can be a bytes-like object, a string, or a file-like object. Returns: email.message.EmailMessage: The parsed EmailMessage object. pass def analyze_email(email_object: email.message.EmailMessage) -> None: Analyzes the parsed email object and prints the analysis results. Parameters: email_object (email.message.EmailMessage): The email object to analyze. Returns: None pass ``` **Input Constraints:** - The function `parse_email_content` should be able to handle three types of inputs: - `bytes` (strictly ASCII or UTF-8 encoded) - `str` (strictly ASCII or UTF-8 encoded) - `io.IOBase` (file-like object in binary or text mode) **Output:** - The function `parse_email_content` should return an `EmailMessage` object. - The function `analyze_email` should print: - Whether the email is multipart. - All headers present in the email. - Any defects found during parsing. **Performance Requirements:** - Efficient handling of emails with large attachments (binary data). - Correct identification and reporting of parsing defects. **Example Usage:** ```python email_bytes = b\\"From: sender@example.comnTo: receiver@example.comnSubject: TestnnThis is a test email.\\" email_message = parse_email_content(email_bytes) analyze_email(email_message) # Expected Output: # Is Multipart: False # Headers: # From: sender@example.com # To: receiver@example.com # Subject: Test # Defects: [] ``` # **Additional Notes:** - Be sure to handle MIME and non-MIME emails appropriately. - Consider the compatibility and policy settings as mentioned in the documentation.","solution":"import email from email.parser import BytesParser, Parser from email.policy import default import io from typing import Union def parse_email_content(source: Union[bytes, str, io.IOBase]) -> email.message.EmailMessage: Parses the email content from the given source. Parameters: source (Union[bytes, str, io.IOBase]): The source of the email content, which can be a bytes-like object, a string, or a file-like object. Returns: email.message.EmailMessage: The parsed EmailMessage object. if isinstance(source, bytes): return BytesParser(policy=default).parsebytes(source) elif isinstance(source, str): return Parser(policy=default).parsestr(source) elif isinstance(source, io.IOBase): if isinstance(source, io.TextIOBase): return Parser(policy=default).parse(source) elif isinstance(source, io.BufferedIOBase): return BytesParser(policy=default).parse(source) raise TypeError(\\"Unsupported input type\\") def analyze_email(email_object: email.message.EmailMessage) -> None: Analyzes the parsed email object and prints the analysis results. Parameters: email_object (email.message.EmailMessage): The email object to analyze. Returns: None is_multipart = email_object.is_multipart() defects = email_object.defects headers = email_object.items() print(\\"Is Multipart:\\", is_multipart) print(\\"Headers:\\") for header, value in headers: print(f\\"{header}: {value}\\") print(\\"Defects:\\", defects)"},{"question":"Problem Statement You are provided with an audit event table containing information about events raised by `sys.audit()` calls in the CPython runtime. The table is structured as follows: - **Audit event**: The name of the audit event. - **Arguments**: The arguments taken by the event. - **References**: The documentation references for the event. You need to implement a function that processes this table and extracts events based on specific criteria. # Function Signature ```python def extract_audit_events(event_table: list, search_criteria: dict) -> list: Extract audit events from the event table based on the provided search criteria. Parameters: - event_table (list): A list of dictionaries, where each dictionary represents an audit event and contains keys: - \'Audit event\': (str) The name of the audit event. - \'Arguments\': (list) A list of argument names taken by the event. - \'References\': (list) A list of references for the event. - search_criteria (dict): A dictionary containing search criteria with keys: - \'event_name_contains\': (str) A substring that should be present in the \'Audit event\' name. - \'arguments_include\': (list) A list of argument names; the audit event should include all these arguments. - \'references_include\': (list) A list of references; the audit event should include all these references. Returns: - list: A list of dictionaries representing the audit events that match the given criteria. # Example ```python event_table = [ { \'Audit event\': \'array.__new__\', \'Arguments\': [\'typecode\', \'initializer\'], \'References\': [\'[1]\'] }, { \'Audit event\': \'builtins.breakpoint\', \'Arguments\': [\'breakpointhook\'], \'References\': [\'[1]\'] }, # Add more events as necessary ] search_criteria = { \'event_name_contains\': \'builtins\', \'arguments_include\': [\'breakpointhook\'], \'references_include\': [\'[1]\'] } result = extract_audit_events(event_table, search_criteria) # Result should be: # [ # { # \'Audit event\': \'builtins.breakpoint\', # \'Arguments\': [\'breakpointhook\'], # \'References\': [\'[1]\'] # } # ] ``` # Constraints - The `event_table` list is guaranteed to have at least one entry. - The `search_criteria` dictionary will always have keys `event_name_contains`, `arguments_include`, and `references_include`. - The function should strictly match all criteria provided within the `search_criteria` dictionary. - You may assume all entries in `event_table` and `search_criteria` are well-formed. Implement the `extract_audit_events` function to pass the above requirements and constraints.","solution":"def extract_audit_events(event_table, search_criteria): Extract audit events from the event table based on the provided search criteria. Parameters: - event_table (list): A list of dictionaries, where each dictionary represents an audit event and contains keys: - \'Audit event\': (str) The name of the audit event. - \'Arguments\': (list) A list of argument names taken by the event. - \'References\': (list) A list of references for the event. - search_criteria (dict): A dictionary containing search criteria with keys: - \'event_name_contains\': (str) A substring that should be present in the \'Audit event\' name. - \'arguments_include\': (list) A list of argument names; the audit event should include all these arguments. - \'references_include\': (list) A list of references; the audit event should include all these references. Returns: - list: A list of dictionaries representing the audit events that match the given criteria. result = [] for event in event_table: if (search_criteria[\'event_name_contains\'] in event[\'Audit event\'] and all(arg in event[\'Arguments\'] for arg in search_criteria[\'arguments_include\']) and all(ref in event[\'References\'] for ref in search_criteria[\'references_include\'])): result.append(event) return result"},{"question":"# Question: Asyncio Exception Handling You are tasked with implementing an asyncio-based function to perform several asynchronous I/O operations. The function should handle specific exceptions defined in the `asyncio` module, ensuring robust and error-resistant execution. You must demonstrate proper exception handling for the following scenarios: 1. Operation timeout. 2. Operation cancellation. 3. Invalid internal state. 4. Sendfile syscall unavailability. 5. Incomplete read operations. 6. Buffer size limit overrun during read. Implement the function `perform_async_operations` that manages the described exceptions and returns a dictionary indicating the result of each operation or the exception caught. Function Signature ```python import asyncio async def perform_async_operations() -> dict: pass ``` Expected Input and Output The function does not take any input but performs predefined asynchronous operations. It returns a dictionary with keys indicating the type of operation and values being either the result of the operation or the exception message that was caught. Example Suppose the following sample operations are implemented within the function: ```python async def operation_1(): raise asyncio.TimeoutError() async def operation_2(): raise asyncio.CancelledError() async def operation_3(): raise asyncio.InvalidStateError() async def operation_4(): raise asyncio.SendfileNotAvailableError() async def operation_5(): raise asyncio.IncompleteReadError(10, b\'partial data\') async def operation_6(): raise asyncio.LimitOverrunError(5, \'consumed\') ``` The `perform_async_operations` should catch and handle these exceptions appropriately and return: ```python { \\"operation_1\\": \\"TimeoutError: The operation has exceeded the given deadline.\\", \\"operation_2\\": \\"CancelledError: The operation has been cancelled.\\", \\"operation_3\\": \\"InvalidStateError: Invalid internal state.\\", \\"operation_4\\": \\"SendfileNotAvailableError: The sendfile syscall is not available.\\", \\"operation_5\\": \\"IncompleteReadError: The requested read operation did not complete fully with partial data before the end.\\", \\"operation_6\\": \\"LimitOverrunError: Reached the buffer size limit while looking for a separator.\\" } ``` Constraints - All operations must be asynchronous. - Handle each of the specified exceptions with meaningful messages. - Use the given exception documentation to craft appropriate exception messages. Notes - You can define and use auxiliary asynchronous functions to simulate the operations. - Implement detailed exception handling to distinguish between the different asyncio exceptions. Good luck!","solution":"import asyncio async def operation_1(): raise asyncio.TimeoutError() async def operation_2(): raise asyncio.CancelledError() async def operation_3(): raise asyncio.InvalidStateError() async def operation_4(): raise asyncio.SendfileNotAvailableError() async def operation_5(): raise asyncio.IncompleteReadError(10, b\'partial data\') async def operation_6(): raise asyncio.LimitOverrunError(5, \'consumed\') async def perform_async_operations() -> dict: result = {} operations = [ (\'operation_1\', operation_1), (\'operation_2\', operation_2), (\'operation_3\', operation_3), (\'operation_4\', operation_4), (\'operation_5\', operation_5), (\'operation_6\', operation_6) ] for name, op in operations: try: await op() except asyncio.TimeoutError: result[name] = \\"TimeoutError: The operation has exceeded the given deadline.\\" except asyncio.CancelledError: result[name] = \\"CancelledError: The operation has been cancelled.\\" except asyncio.InvalidStateError: result[name] = \\"InvalidStateError: Invalid internal state.\\" except asyncio.SendfileNotAvailableError: result[name] = \\"SendfileNotAvailableError: The sendfile syscall is not available.\\" except asyncio.IncompleteReadError: result[name] = \\"IncompleteReadError: The requested read operation did not complete fully with partial data before the end.\\" except asyncio.LimitOverrunError: result[name] = \\"LimitOverrunError: Reached the buffer size limit while looking for a separator.\\" return result"},{"question":"**Coding Assessment Question: Draw a Tree with Recursive Branching** # Objective Design a Python program using the `turtle` module to draw a fractal tree. The tree should start with a trunk and have branches that recursively split into smaller branches. # Problem Statement Write a function `draw_branch(t, branch_length, angle, factor)` which recursively draws branches of a tree, based on the following parameters: - **t**: A turtle instance. - **branch_length**: The length of the current branch. - **angle**: The angle at which branches split off from the current branch. - **factor**: The factor by which the branch length reduces each time it splits. The program should include a `main()` function that sets up the turtle screen, initializes the turtle, and calls the `draw_branch()` function to start drawing the tree. # Requirements 1. **Input**: The `main()` function should take no input. 2. **Output**: The turtle graphics window showing the fractal tree. 3. **Constraints**: - The branch length should stop growing when it is less than a predefined threshold value (e.g., 10). - Branches should split into two sub-branches at each recursive step. - Use turtle methods such as `forward()`, `backward()`, `right()`, `left()` in your implementation. 4. **Visual Specifications**: - The trunk should be drawn in the middle of the screen, branching upwards. - Different branches should have different colors based on their length. Longer branches should be brown (`\\"brown\\"`) and shorter branches green (`\\"green\\"`). - There should be no visible turtle after drawing the tree. # Example ```python import turtle def draw_branch(t, branch_length, angle, factor): if branch_length > 10: t.forward(branch_length) t.right(angle) draw_branch(t, branch_length - branch_length * factor, angle, factor) t.left(angle * 2) draw_branch(t, branch_length - branch_length * factor, angle, factor) t.right(angle) t.backward(branch_length) def main(): screen = turtle.Screen() screen.setup(width=800, height=600) t = turtle.Turtle() t.speed(0) t.left(90) t.penup() t.goto(0, -250) t.pendown() draw_branch(t, 100, 30, 0.15) t.hideturtle() screen.exitonclick() if __name__ == \\"__main__\\": main() ``` # Considerations - Ensure your turtle moves and draws correctly without generating errors. - The tree should look proportionate and visually appealing. - Use recursive thinking to achieve the branching effect. - Handle the turtle\'s visibility to ensure a clean drawing without distractions.","solution":"import turtle def draw_branch(t, branch_length, angle, factor): if branch_length > 10: # Set color based on branch length t.color(\\"brown\\" if branch_length > 20 else \\"green\\") t.forward(branch_length) t.right(angle) draw_branch(t, branch_length * factor, angle, factor) t.left(2 * angle) draw_branch(t, branch_length * factor, angle, factor) t.right(angle) t.backward(branch_length) def main(): screen = turtle.Screen() screen.setup(width=800, height=600) t = turtle.Turtle() t.speed(0) t.left(90) t.penup() t.goto(0, -250) t.pendown() draw_branch(t, 100, 30, 0.67) t.hideturtle() screen.exitonclick() if __name__ == \\"__main__\\": main()"},{"question":"**Problem Statement:** You are given data from the famous Iris dataset consisting of 150 samples from three species of Iris flower (`setosa`, `versicolor`, and `virginica`). Each sample contains four features: `sepal length`, `sepal width`, `petal length`, and `petal width`. Your task is to design a decision tree classifier to predict the species of the Iris flower using the scikit-learn package. Implement the following steps: 1. **Data Loading and Preparation:** - Load the Iris dataset using `sklearn.datasets.load_iris`. - Split the dataset into training and testing sets (80% train, 20% test). 2. **Model Training:** - Initialize and train a `DecisionTreeClassifier` on the training set. Use `criterion=\'entropy\'`. 3. **Model Evaluation:** - Evaluate the accuracy of the model on the testing set. - Generate and plot the confusion matrix. 4. **Visualization:** - Visualize the trained decision tree using `plot_tree` from `sklearn.tree`. 5. **Advanced Features:** - Handle any missing values in the dataset. - Implement pruning using minimal cost-complexity pruning (`ccp_alpha`). - Implement a multi-output decision tree model using synthetic data if an example is not available in the Iris dataset. 6. **Documentation and Insights:** - Document each step with appropriate comments and markdown cells (if using Jupyter Notebook). - Provide insights on feature importance and the performance of your model. # Input - You will specifically work with the `load_iris()` dataset from `sklearn.datasets`. # Output - Accuracy of the classifier on the test set. - Visualization of the decision tree. - Confusion matrix. - Insights about feature importance and model behavior. # Constraints - Use `random_state=42` for any random operations to ensure reproducibility. - Ensure your code is well-documented and follows best practices. # Example Here is a brief outline to help you get started: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, plot_tree from sklearn.metrics import accuracy_score, confusion_matrix import matplotlib.pyplot as plt # 1. Load dataset iris = load_iris() X, y = iris.data, iris.target # 2. Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 3. Train Decision Tree Classifier clf = DecisionTreeClassifier(criterion=\'entropy\', random_state=42) clf.fit(X_train, y_train) # 4. Evaluate model accuracy = accuracy_score(y_test, clf.predict(X_test)) print(f\'Accuracy: {accuracy}\') # 5. Plot confusion matrix conf_matrix = confusion_matrix(y_test, clf.predict(X_test)) print(\\"Confusion Matrix:\\") print(conf_matrix) # 6. Visualize the decision tree plt.figure(figsize=(20,10)) plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.show() # 7. Handle missing values, ccp_alpha for pruning, and multi-output (can be added if needed) ``` **Note:** Ensure to include meaningful comments and explanations in your actual implementation.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, plot_tree from sklearn.metrics import accuracy_score, confusion_matrix import matplotlib.pyplot as plt import numpy as np # 1. Load dataset iris = load_iris() X, y = iris.data, iris.target # Advanced Feature: Handle missing values # For demonstration, let us manually introduce some missing values and handle them np.random.seed(42) missing_rate = 0.1 # 10% missing data n_missing_samples = int(np.floor(missing_rate * X.size)) missing_samples = np.hstack((np.random.choice(X.shape[0], n_missing_samples), np.random.choice(X.shape[1], n_missing_samples))) X.ravel()[missing_samples] = np.nan # Handle missing values by imputing with the mean of the column from sklearn.impute import SimpleImputer imputer = SimpleImputer(strategy=\'mean\') X = imputer.fit_transform(X) # 2. Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 3. Train Decision Tree Classifier clf = DecisionTreeClassifier(criterion=\'entropy\', random_state=42, ccp_alpha=0.01) # adding ccp_alpha for pruning clf.fit(X_train, y_train) # 4. Evaluate model accuracy = accuracy_score(y_test, clf.predict(X_test)) print(f\'Accuracy: {accuracy}\') # 5. Plot confusion matrix conf_matrix = confusion_matrix(y_test, clf.predict(X_test)) print(\\"Confusion Matrix:\\") print(conf_matrix) # 6. Visualize the decision tree plt.figure(figsize=(20,10)) plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.show()"},{"question":"**Question: Implementing a Simple Authentication System** **Objective:** In this task, you are required to implement a simple authentication system using the `getpass` module. **Requirements:** 1. Implement a function `authenticate_user(stored_password: str) -> bool` that performs the following: - Prompts the user to enter their username and password. - Retrieves the current user\'s login name using `getpass.getuser()`. - Compares the retrieved login name with the entered username. - Compares the entered password with the stored password (provided as an argument to the function). - Returns `True` if both username and password match; otherwise, returns `False`. 2. Assume that the stored password is provided as a plain text string (not hashed). In a real-world scenario, you should use hashed passwords for better security. **Function Specification:** ```python def authenticate_user(stored_password: str) -> bool: Prompts the user for their username and password and validates them against the stored password. Args: stored_password (str): The password to be checked against the user\'s input. Returns: bool: True if the username matches the current user\'s login name and the password matches the stored password, otherwise False. ``` **Example:** ```python # Example usage stored_password = \\"securepassword\\" result = authenticate_user(stored_password) print(result) # Output depends on user input ``` **Constraints:** - You can assume the function `getpass.getuser()` will always successfully retrieve the current user\'s login name. - Use the `getpass.getpass` function to securely prompt for the password. **Note:** If you are using an environment where `getpass.getpass` does not work properly (e.g., some IDEs or Jupyter notebooks), consider running your code on the command line for the correct behavior.","solution":"import getpass def authenticate_user(stored_password: str) -> bool: Prompts the user for their username and password and validates them against the stored password. Args: stored_password (str): The password to be checked against the user\'s input. Returns: bool: True if the username matches the current user\'s login name and the password matches the stored password, otherwise False. current_user = getpass.getuser() entered_username = input(\\"Enter your username: \\") entered_password = getpass.getpass(\\"Enter your password: \\") if entered_username == current_user and entered_password == stored_password: return True return False"},{"question":"# Function Object Manipulation in Python Using the provided functions, your task is to create a set of utility functions in Python using the C API to perform specific operations on Python function objects. You will implement the following functions: 1. **is_function(obj)**: Given a PyObject, check if it is a function object. 2. **create_function(code_obj, globals_dict, qualname=None)**: Create a new function object given a code object, globals dictionary, and optionally, a qualified name. 3. **get_function_details(func_obj)**: Given a function object, return a dictionary containing its code object, globals dictionary, module name, default arguments, closure, and annotations. 4. **set_function_defaults(func_obj, defaults_tuple)**: Set the default arguments for a given function object. 5. **set_function_annotations(func_obj, annotations_dict)**: Set the annotations for a given function object. Constraints: - The `create_function` function should handle proper default values for arguments and qualified names. - The `set_function_defaults` function should raise an appropriate exception if the defaults cannot be set. - The `set_function_annotations` function should raise an appropriate exception if the annotations cannot be set. - Performance is not the primary concern, but efficient usage of the C API functions is encouraged. Input and Output Formats: - **is_function(obj)** - Input: `PyObject *obj` - Output: `bool` - **create_function(code_obj, globals_dict, qualname=None)** - Input: `PyObject *code_obj`, `PyObject *globals_dict`, `PyObject *qualname` - Output: `PyObject *` - **get_function_details(func_obj)** - Input: `PyObject *func_obj` - Output: `dict` - **set_function_defaults(func_obj, defaults_tuple)** - Input: `PyObject *func_obj`, `PyObject *defaults_tuple` - Output: `None` - **set_function_annotations(func_obj, annotations_dict)** - Input: `PyObject *func_obj`, `PyObject *annotations_dict` - Output: `None` Example: ```python # Assuming you have the following C API declarations and usage methods # Check if the object is a function assert is_function(some_object) == True # Create a new function new_func = create_function(code_obj, globals_dict, qualname=\\"qualified_name\\") # Retrieve function details details = get_function_details(new_func) print(details) # Set new defaults set_function_defaults(new_func, (\\"default_arg1\\", \\"default_arg2\\")) # Set new annotations set_function_annotations(new_func, {\\"arg1\\": int, \\"arg2\\": str}) ``` Please ensure proper error handling and memory management as per C API requirements. Perform detailed testing for each function to ensure correct behavior.","solution":"import types def is_function(obj): Check if the object is a function. return isinstance(obj, types.FunctionType) def create_function(code_obj, globals_dict, qualname=None): Create a new function object given a code object, globals dictionary, and optionally, a qualified name. if not isinstance(code_obj, types.CodeType): raise TypeError(\\"code_obj must be a code object\\") if not isinstance(globals_dict, dict): raise TypeError(\\"globals_dict must be a dictionary\\") new_func = types.FunctionType(code_obj, globals_dict, qualname) return new_func def get_function_details(func_obj): Given a function object, return a dictionary containing its code object, globals dictionary, module name, default arguments, closure, and annotations. if not is_function(func_obj): raise TypeError(\\"func_obj must be a function object\\") details = { \\"code\\": func_obj.__code__, \\"globals\\": func_obj.__globals__, \\"module\\": func_obj.__module__, \\"defaults\\": func_obj.__defaults__, \\"closure\\": func_obj.__closure__, \\"annotations\\": func_obj.__annotations__ } return details def set_function_defaults(func_obj, defaults_tuple): Set the default arguments for a given function object. if not is_function(func_obj): raise TypeError(\\"func_obj must be a function object\\") if not isinstance(defaults_tuple, tuple): raise TypeError(\\"defaults_tuple must be a tuple\\") func_obj.__defaults__ = defaults_tuple def set_function_annotations(func_obj, annotations_dict): Set the annotations for a given function object. if not is_function(func_obj): raise TypeError(\\"func_obj must be a function object\\") if not isinstance(annotations_dict, dict): raise TypeError(\\"annotations_dict must be a dictionary\\") func_obj.__annotations__ = annotations_dict"},{"question":"**Question: Audio Processing with the `wave` Module** **Objective**: Demonstrate your understanding of the `wave` module in the `python310` package by implementing a function that reads a WAV audio file, processes the audio data to reduce its volume by half, and writes the processed audio back to a new WAV file. **Function Signature**: ```python def reduce_volume(input_filename: str, output_filename: str) -> None: Reduces the volume of the audio in the input WAV file by half and writes the result to the output WAV file. Parameters: - input_filename (str): The path to the input WAV file. - output_filename (str): The path where the output WAV file will be saved. Returns: - None ``` # Requirements 1. **Reading the Input File**: - Use the `wave` module to open and read the input WAV file. - Extract the audio parameters and data from the input file. 2. **Processing the Audio Data**: - Convert the binary audio data into an appropriate format (e.g., PCM integers). - Reduce the audio volume by half by scaling down the amplitude of the audio samples. 3. **Writing the Output File**: - Use the `wave` module to create a new WAV file and write the modified audio data. # Constraints - Ensure that the output file has the same audio parameters (channels, sample width, frame rate) as the input file except for the modified data. # Example If you have an input WAV file named \'input.wav\', calling the function as follows: ```python reduce_volume(\'input.wav\', \'output.wav\') ``` will create an output WAV file named \'output.wav\' with the volume reduced by half. # Notes - Handle edge cases such as files that do not exist, incorrect file formats, or read/write errors. # Hints - Explore the `wave` module\'s documentation for reading and writing WAV files. - You might find Python\'s `struct` module helpful for converting between binary data and Python values. **Performance**: - The solution should handle reasonably sized audio files (e.g., up to 100 MB) efficiently. Good luck! Your implementation will be tested with various audio files to ensure correctness and robustness.","solution":"import wave import struct def reduce_volume(input_filename: str, output_filename: str) -> None: Reduces the volume of the audio in the input WAV file by half and writes the result to the output WAV file. Parameters: - input_filename (str): The path to the input WAV file. - output_filename (str): The path where the output WAV file will be saved. Returns: - None with wave.open(input_filename, \'rb\') as infile: params = infile.getparams() frames = infile.readframes(params.nframes) # Unpack the binary data to PCM samples fmt = \'<\' + \'h\' * params.nframes * params.nchannels samples = struct.unpack(fmt, frames) # Reduce volume by halving the sample values reduced_samples = [int(sample * 0.5) for sample in samples] # Pack the reduced samples back to binary data reduced_frames = struct.pack(fmt, *reduced_samples) with wave.open(output_filename, \'wb\') as outfile: outfile.setparams(params) outfile.writeframes(reduced_frames)"},{"question":"# Question: Implementing a Parallel Web Scraper using Python\'s `multiprocessing` Module You are assigned to develop a web scraping utility that extracts data from a list of URLs concurrently. Your solution should leverage Python\'s `multiprocessing` module for efficiency and scalability. # Requirements: 1. **Function `fetch_data`**: - Input: A URL (string). - Output: A tuple containing the URL and the length of the response content (integer). - Behavior: This function should fetch the HTML content from the provided URL using the `requests` library and return the URL and the content length. 2. **Function `parallel_scraper`**: - Input: A list of URLs (list of strings), the number of worker processes (integer). - Output: A list of tuples, each tuple containing a URL and its content length. - Behavior: This function should: - Utilize a pool of worker processes to fetch data from the URLs concurrently. - Collect the results from the worker processes and return them in the same order as the input URLs using appropriate synchronization primitives. # Constraints: - For simplicity, assume that all URLs will successfully return a response and no error handling is required. - You should not use any global variables to share state between processes. - Implement proper synchronization to ensure results are collected correctly. # Example: ```python urls = [ \'http://example.com\', \'http://example.org\', \'http://example.net\' ] result = parallel_scraper(urls, 3) print(result) ``` Expected Output (assuming content lengths): ```python [ (\'http://example.com\', 1270), (\'http://example.org\', 3987), (\'http://example.net\', 2054) ] ``` # Notes: - You will need to install the `requests` library, which can be done using `pip install requests`. - Include imports and ensure your code is encapsulated within functions and the `if __name__ == \\"__main__\\":` guard for multiprocessing compatibility. Good luck!","solution":"import requests from multiprocessing import Pool, Manager def fetch_data(url): Fetch the HTML content from the provided URL and returns the URL and the content length. response = requests.get(url) return url, len(response.content) def parallel_scraper(urls, num_workers): Fetch data from a list of URLs concurrently using a pool of worker processes. with Pool(num_workers) as pool: results = pool.map(fetch_data, urls) return results if __name__ == \\"__main__\\": urls = [ \'http://example.com\', \'http://example.org\', \'http://example.net\' ] print(parallel_scraper(urls, 3))"},{"question":"# Python310 Object Handling Functionality Problem Description You are asked to implement a Python class that mimics object attribute handling and comparison functionalities similar to the Python C API functions described below. The class should provide the following methods: 1. `has_attr(obj, attr_name)`: Check if `obj` has an attribute named `attr_name`. 2. `get_attr(obj, attr_name)`: Get the value of the attribute named `attr_name` from `obj`. 3. `set_attr(obj, attr_name, value)`: Set the value of the attribute named `attr_name` in `obj` to `value`. 4. `del_attr(obj, attr_name)`: Delete the attribute named `attr_name` from `obj`. 5. `is_instance(obj, cls)`: Check if `obj` is an instance of class `cls` or any subclass thereof. 6. `is_subclass(cls1, cls2)`: Check if class `cls1` is a subclass of class `cls2`. 7. `compare(obj1, obj2, op)`: Compare `obj1` and `obj2` using the operation specified by `op`, which can be one of `\\"<\\"`, `\\"<=\\"`, `\\"==\\"`, `\\"!=\\"`, `\\">\\"`, or `\\">=\\"`. Constraints - You may not use any built-in Python functions like `hasattr`, `getattr`, `setattr`, `delattr`, `isinstance`, or `issubclass`. - You must handle exceptions and edge cases appropriately. - Your implementation should work efficiently for commonly sized inputs. Input and Output Format - `has_attr(obj, attr_name)`: - **Input**: obj (any object), attr_name (string) - **Output**: True (if attribute exists), False (otherwise) - `get_attr(obj, attr_name)`: - **Input**: obj (any object), attr_name (string) - **Output**: attribute value (if exists), raise AttributeError (otherwise) - `set_attr(obj, attr_name, value)`: - **Input**: obj (any object), attr_name (string), value (any value) - **Output**: None - `del_attr(obj, attr_name)`: - **Input**: obj (any object), attr_name (string) - **Output**: None - `is_instance(obj, cls)`: - **Input**: obj (any object), cls (a class or tuple of classes) - **Output**: True (if obj is an instance of cls or its subclass), False (otherwise) - `is_subclass(cls1, cls2)`: - **Input**: cls1 (a class), cls2 (a class or tuple of classes) - **Output**: True (if cls1 is a subclass of cls2), False (otherwise) - `compare(obj1, obj2, op)`: - **Input**: obj1 (any object), obj2 (any object), op (string, one of `\\"<\\"`, `\\"<=\\"`, `\\"==\\"`, `\\"!=\\"`, `\\">\\"`, `\\">=\\"`) - **Output**: Result of the comparison operation Example Usage ```python handler = ObjectHandler() # Attribute Handling class Test: x = 5 obj = Test() print(handler.has_attr(obj, \'x\')) # True print(handler.get_attr(obj, \'x\')) # 5 handler.set_attr(obj, \'y\', 10) print(handler.get_attr(obj, \'y\')) # 10 handler.del_attr(obj, \'y\') print(handler.has_attr(obj, \'y\')) # False # Instance and Subclass Checks print(handler.is_instance(5, int)) # True print(handler.is_subclass(int, object)) # True # Comparison print(handler.compare(5, 3, \'>\')) # True print(handler.compare(5, 5, \'==\')) # True ``` Your Task Implement the `ObjectHandler` class with the methods described above.","solution":"class ObjectHandler: def has_attr(self, obj, attr_name): try: object.__getattribute__(obj, attr_name) return True except AttributeError: return False def get_attr(self, obj, attr_name): try: return object.__getattribute__(obj, attr_name) except AttributeError: raise AttributeError(f\\"\'{type(obj).__name__}\' object has no attribute \'{attr_name}\'\\") def set_attr(self, obj, attr_name, value): object.__setattr__(obj, attr_name, value) def del_attr(self, obj, attr_name): try: object.__delattr__(obj, attr_name) except AttributeError: raise AttributeError(f\\"\'{type(obj).__name__}\' object has no attribute \'{attr_name}\'\\") def is_instance(self, obj, cls): return isinstance(obj, cls) def is_subclass(self, cls1, cls2): return issubclass(cls1, cls2) def compare(self, obj1, obj2, op): if op == \\"<\\": return obj1 < obj2 elif op == \\"<=\\": return obj1 <= obj2 elif op == \\"==\\": return obj1 == obj2 elif op == \\"!=\\": return obj1 != obj2 elif op == \\">\\": return obj1 > obj2 elif op == \\">=\\": return obj1 >= obj2 else: raise ValueError(f\\"Invalid comparison operator \'{op}\'\\")"},{"question":"# Custom ExtensionArray and ExtensionDtype Context: You are tasked with creating a custom extension array and data type in pandas, allowing you to store and manipulate complex numbers within a pandas dataframe. This exercise is intended to test your understanding of extending pandas\' functionality. Requirements: 1. **CustomDtype**: Implement a custom dtype inheriting from `pandas.api.extensions.ExtensionDtype`. 2. **ComplexArray**: Implement a custom extension array inheriting from `pandas.api.extensions.ExtensionArray`. Details: 1. **CustomDtype**: - Name your dtype `ComplexDtype`. - It should be recognized as a numeric type. 2. **ComplexArray**: - The array should internally use two numpy arrays, one for real parts and one for imaginary parts. - Implement at least the following methods: `__getitem__`, `__len__`, `to_numpy`, `astype`, `isna`, `take`, `concat`, `unique`. - The array should be able to handle basic arithmetic operations (addition, subtraction) for complex numbers within a dataframe. - Ensure proper handling of missing values (use np.nan for real and imaginary parts). Input: No explicit input is provided. The question revolves around creating reusable classes and methods. Expected Output: You should implement the following classes: ```python import pandas as pd import numpy as np from pandas.api.extensions import ExtensionDtype, ExtensionArray class ComplexDtype(ExtensionDtype): ... class ComplexArray(ExtensionArray): ... ``` Additionally, provide example usage showcasing the creation of a DataFrame using the custom `ComplexArray`: ```python complex_data = ComplexArray([ complex(1, 2), complex(3, 4), complex(5, np.nan) ]) df = pd.DataFrame({\'complex_column\': complex_data}) print(df) ``` Constraints: - Ensure the custom dtype and array properly integrate with pandas’ existing methods and mechanisms. - Provide clear documentation/comments for each method you implement. - Performance: Optimize your array for operations on datasets up to 1000 elements.","solution":"import pandas as pd import numpy as np from pandas.api.extensions import ExtensionDtype, ExtensionArray class ComplexDtype(ExtensionDtype): name = \'complex\' _na_value = np.nan @property def type(self): return complex @property def kind(self): return \'O\' @property def name(self): return \'complex\' @classmethod def construct_array_type(cls): return ComplexArray class ComplexArray(ExtensionArray): def __init__(self, data): real_parts = [] imag_parts = [] for item in data: if pd.isna(item): real_parts.append(np.nan) imag_parts.append(np.nan) else: real_parts.append(item.real) imag_parts.append(item.imag) self.real_data = np.array(real_parts, dtype=\'float64\') self.imag_data = np.array(imag_parts, dtype=\'float64\') @property def dtype(self): return ComplexDtype() @property def shape(self): return self.real_data.shape def __len__(self): return len(self.real_data) def __getitem__(self, item): if isinstance(item, int): return complex(self.real_data[item], self.imag_data[item]) else: return ComplexArray([complex(r, i) for r,i in zip(self.real_data[item], self.imag_data[item])]) def isna(self): return np.isnan(self.real_data) def take(self, indices, allow_fill=False, fill_value=None): if fill_value is None: fill_value = complex(np.nan, np.nan) n = len(self) if allow_fill: indices = np.asarray(indices, dtype=\'intp\') taken = [self[i] if i >= 0 else fill_value for i in indices] else: taken = [self[i] for i in indices] return ComplexArray(taken) def copy(self): return ComplexArray([complex(r, i) for r, i in zip(self.real_data, self.imag_data)]) def to_numpy(self): return np.array([complex(r, i) for r, i in zip(self.real_data, self.imag_data)]) def astype(self, dtype, copy=True): if dtype == self.dtype: if copy: return self.copy() else: return self return np.array([complex(r, i) for r, i in zip(self.real_data, self.imag_data)], dtype=dtype) @classmethod def _concat_same_type(cls, to_concat): real_parts = np.concatenate([x.real_data for x in to_concat]) imag_parts = np.concatenate([y.imag_data for y in to_concat]) return ComplexArray([complex(r, i) for r, i in zip(real_parts, imag_parts)]) def unique(self): unique_vals = np.unique(self.to_numpy()) return ComplexArray(unique_vals) def __add__(self, other): if isinstance(other, ComplexArray): return ComplexArray(self.to_numpy() + other.to_numpy()) return ComplexArray(self.to_numpy() + other) def __sub__(self, other): if isinstance(other, ComplexArray): return ComplexArray(self.to_numpy() - other.to_numpy()) return ComplexArray(self.to_numpy() - other)"},{"question":"# Question: Unicode Encodings and String Manipulation Objective: You have been given a string that contains a mix of plain ASCII characters and Unicode characters. Your task is to write a function that performs the following tasks: 1. Encodes the string in UTF-8. 2. Writes the encoded bytes to a file. 3. Reads the encoded bytes from the file. 4. Decodes the bytes back to a Unicode string. 5. Normalizes the Unicode string to \'NFC\' form. 6. Returns a tuple that contains: - The original string. - The bytes object read from the file. - The decoded and normalized Unicode string. Function Signature: ```python def process_unicode_string(input_str: str, file_name: str) -> tuple: ``` Input: - `input_str` (str): A Unicode string that may contain both ASCII and non-ASCII characters. - `file_name` (str): The name of the file to which the encoded string should be written and from which it should be read. Output: - A tuple containing: - Original string (`str`). - The bytes object read from the file (`bytes`). - The decoded and normalized Unicode string (`str`). Constraints: - Handle any valid Unicode string. Example: ```python input_str = \\"Hello, こんにちは, Привет, 👋\\" file_name = \\"unicode_test.txt\\" result = process_unicode_string(input_str, file_name) print(result) ``` Expected Output: ``` ( \\"Hello, こんにちは, Привет, 👋\\", b\'Hello, xe3x81x93xe3x82x93xe3x81xabxe3x81xa1xe3x81xaf, xd0x9fxd1x80xd0xb8xd0xb2xd0xb5xd1x82, xf0x9fx91x8b\', \\"Hello, こんにちは, Привет, 👋\\" ) ``` Notes: - Ensure you handle any exceptions that may arise during file operations. - Be mindful of proper encoding and decoding as well as the normalization process.","solution":"import unicodedata def process_unicode_string(input_str: str, file_name: str) -> tuple: # Encode the string in UTF-8 encoded_bytes = input_str.encode(\'utf-8\') # Write the encoded bytes to a file with open(file_name, \'wb\') as file: file.write(encoded_bytes) # Read the encoded bytes from the file with open(file_name, \'rb\') as file: file_bytes = file.read() # Decode the bytes back to a Unicode string decoded_str = file_bytes.decode(\'utf-8\') # Normalize the Unicode string to \'NFC\' form normalized_str = unicodedata.normalize(\'NFC\', decoded_str) return (input_str, file_bytes, normalized_str)"},{"question":"As a data scientist, you are tasked with analyzing the residuals of a linear regression model to check for any violations of its assumptions using the `seaborn` library. You will employ the `mpg` dataset from `seaborn`. Dataset Description The `mpg` dataset includes the following columns relevant to your task: - `mpg`: Miles per gallon. - `weight`: Vehicle weight. - `displacement`: Displacement (engine volume). - `horsepower`: Engine horsepower. Task Requirements: 1. Create a scatter plot of the residuals after fitting a simple regression model of `displacement` on `weight`. 2. Analyze the residuals by plotting them with `mpg` on the y-axis and `horsepower` on the x-axis. Comment on any structure found. 3. Remove higher-order trends by adjusting the regression order to 2 and re-plot the residuals from step 2. 4. Add a LOWESS curve to emphasize any structure in the residuals from step 3. 5. Based on the plots, provide a brief explanation (2-3 sentences) about the presence of any patterns or trends in the residuals and their implications. Implementation: Write the following functions: 1. **plot_simple_residuals(data: pd.DataFrame) -> None**: - **Input**: A DataFrame (`data`) containing the `mpg` dataset. - **Output**: No return value, but should display a scatter plot of residuals from a regression of `displacement` on `weight`. 2. **plot_horsepower_vs_mpg_residuals(data: pd.DataFrame) -> None**: - **Input**: A DataFrame (`data`) containing the `mpg` dataset. - **Output**: No return value, but should display a scatter plot of residuals with `mpg` against `horsepower`. 3. **plot_higher_order_residuals(data: pd.DataFrame) -> None**: - **Input**: A DataFrame (`data`) containing the `mpg` dataset. - **Output**: No return value, but should display a scatter plot of residuals with `mpg` against `horsepower` using a second-order trend. 4. **plot_lowess_residuals(data: pd.DataFrame) -> None**: - **Input**: A DataFrame (`data`) containing the `mpg` dataset. - **Output**: No return value, but should display a scatter plot of residuals with `mpg` against `horsepower`, including a LOWESS curve in the plot. 5. **explain_residual_plots() -> str**: - **Output**: A brief explanation (2-3 sentences) about the patterns or trends observed in the residual plots and their implications. Example Usage: ```python import seaborn as sns import pandas as pd # Load the dataset mpg = sns.load_dataset(\\"mpg\\") plot_simple_residuals(mpg) plot_horsepower_vs_mpg_residuals(mpg) plot_higher_order_residuals(mpg) plot_lowess_residuals(mpg) explanation = explain_residual_plots() print(explanation) ``` Constraints: - You must use the `seaborn` library to create the plots. - Your explanation should be concise and based on the observed plots.","solution":"import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import statsmodels.api as sm from statsmodels.nonparametric.smoothers_lowess import lowess def plot_simple_residuals(data: pd.DataFrame) -> None: Display a scatter plot of residuals from a regression of \'displacement\' on \'weight\'. # Fit regression model model = sm.OLS(data[\'displacement\'], sm.add_constant(data[\'weight\'])).fit() residuals = model.resid # Plot residuals plt.figure(figsize=(10, 6)) sns.scatterplot(x=data[\'weight\'], y=residuals) plt.axhline(0, color=\'red\', linestyle=\'--\') plt.title(\'Simple Linear Regression Residuals: Displacement vs. Weight\') plt.xlabel(\'Weight\') plt.ylabel(\'Residuals\') plt.show() def plot_horsepower_vs_mpg_residuals(data: pd.DataFrame) -> None: Display a scatter plot of residuals with \'mpg\' against \'horsepower\'. # Fit regression model to get residuals model = sm.OLS(data[\'displacement\'], sm.add_constant(data[\'weight\'])).fit() residuals = model.resid # Plot residuals against mpg and horsepower plt.figure(figsize=(10, 6)) sns.scatterplot(x=data[\'horsepower\'], y=residuals) plt.axhline(0, color=\'red\', linestyle=\'--\') plt.title(\'Residuals of Displacement vs. Weight vs. Horsepower\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals\') plt.show() def plot_higher_order_residuals(data: pd.DataFrame) -> None: Display a scatter plot of residuals with \'mpg\' against \'horsepower\' using a second-order trend. # Fit a second order polynomial regression model data[\'weight2\'] = data[\'weight\']**2 model = sm.OLS(data[\'displacement\'], sm.add_constant(data[[\'weight\', \'weight2\']])).fit() residuals = model.resid # Plot residuals against horsepower plt.figure(figsize=(10, 6)) sns.scatterplot(x=data[\'horsepower\'], y=residuals) plt.axhline(0, color=\'red\', linestyle=\'--\') plt.title(\'Second-Order Polynomial Regression Residuals: Displacement vs. Weight\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals\') plt.show() def plot_lowess_residuals(data: pd.DataFrame) -> None: Display a scatter plot of residuals with \'mpg\' against \'horsepower\', including a LOWESS curve in the plot. # Fit a second order polynomial regression model data[\'weight2\'] = data[\'weight\']**2 model = sm.OLS(data[\'displacement\'], sm.add_constant(data[[\'weight\', \'weight2\']])).fit() residuals = model.resid # Plot residuals against horsepower plt.figure(figsize=(10, 6)) sns.scatterplot(x=data[\'horsepower\'], y=residuals) # Add LOWESS curve lowess_fit = lowess(residuals, data[\'horsepower\']) plt.plot(lowess_fit[:, 0], lowess_fit[:, 1], color=\'red\') plt.axhline(0, color=\'red\', linestyle=\'--\') plt.title(\'Second-Order Polynomial Regression Residuals with LOWESS: Displacement vs. Weight\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals\') plt.show() def explain_residual_plots() -> str: Provide a brief explanation about the patterns or trends observed in the residual plots. explanation = ( \\"The initial residuals plot of displacement vs. weight shows slight patterns, \\" \\"indicating some non-linearity. The plot of residuals against horsepower also displays \\" \\"patterns that might suggest higher-order terms are required to capture the relationship. \\" \\"After adjusting with a second-order trend, residuals appear more randomly dispersed but still with some structure. \\" \\"Using a LOWESS curve emphasizes these trends, suggesting the presence of non-linearity and potential omitted variable bias.\\" ) return explanation"},{"question":"# Dynamic Code Execution in Python **Objective:** To assess the students\' understanding of different input modes in Python and their ability to dynamically execute code using `exec()` and `eval()` functions. **Problem Statement:** You are required to write a Python function that accepts a source of code as input and executes it in a specific mode. The function should support three modes of input - complete program, file input, and expression input. Based on the mode specified, the function should execute the code and return the necessary output if applicable. **Function Signature:** ```python def execute_code(input_source: str, mode: str) -> any: pass ``` **Input:** - `input_source` (str): A string containing the source code to be executed. - `mode` (str): A string specifying the mode of input. It can be one of the following: - \\"program\\": Indicates a complete program. - \\"file\\": Indicates the file input. - \\"expression\\": Indicates an expression input for evaluation. **Output:** - The output depends on the mode of execution: - For \\"program\\" and \\"file\\" modes, the function should directly execute the code. It can return `None` or any specific output generated by the program. - For \\"expression\\" mode, the function should evaluate the expression and return the result. **Constraints:** - You must use `exec()` for \\"program\\" and \\"file\\" modes and `eval()` for \\"expression\\" mode. - Ensure that the global and local namespaces are appropriately handled for isolated execution of the code snippets. - Handle basic syntax errors and return appropriate error messages if the input code is invalid. **Examples:** ```python # Example 1 code = \\"print(\'Hello, World!\')\\" mode = \\"program\\" execute_code(code, mode) # Output: # Hello, World! # Returns: None # Example 2 code = \\"a = 5nb = 10na + b\\" mode = \\"file\\" execute_code(code, mode) # Output: # Returns: None (since it is executed as a complete program) # Example 3 code = \\"3 + 4 * 2\\" mode = \\"expression\\" result = execute_code(code, mode) # Output: 11 ``` **Notes:** - For the purpose of this task, you can assume that the input code snippets are relatively simple and do not involve complex dependencies or modules. - The focus should be on correctly parsing and executing the provided input in the specified mode.","solution":"def execute_code(input_source: str, mode: str) -> any: try: if mode == \'program\': exec(input_source) return None elif mode == \'file\': exec(input_source) return None elif mode == \'expression\': result = eval(input_source) return result else: raise ValueError(\\"Invalid mode specified.\\") except SyntaxError as e: return f\\"SyntaxError: {str(e)}\\" except Exception as e: return f\\"Error: {str(e)}\\""},{"question":"# Python Coding Assessment: Custom Serialization and Deserialization Objective: Implement a custom serialization and deserialization system to store and retrieve Python objects to/from a file using the concepts outlined in the Python marshalling documentation. Problem Statement: You are required to design two functions to serialize and deserialize Python objects using the marshalled data format. 1. **serialize_objects(objects, filename):** - **Input:** - `objects` (a list of Python objects to serialize). - `filename` (string, the name of the file to write the serialized data). - **Output:** - This function should not return anything. - **Procedure:** - Open the given file in binary write mode. - Serialize and write each Python object in the list to the file using marshalled data format. - Ensure to use version 2 of the data format. 2. **deserialize_objects(filename):** - **Input:** - `filename` (string, the name of the file to read the serialized data). - **Output:** - A list of Python objects that were deserialized from the file. - **Procedure:** - Open the given file in binary read mode. - Read and deserialize each Python object from the file. - Return the deserialized objects as a list. Constraints: - The list of objects to be serialized can include different data types (e.g., integers, strings, lists, dictionaries). - You are required to handle exceptions related to file operations and marshalling/unmarshalling errors. For simplicity, print an error message when an exception occurs. Example: ```python # Example objects to serialize objects_to_serialize = [42, \\"hello\\", [1, 2, 3], {\\"key\\": \\"value\\"}] # Filename to use for serialization filename = \\"datafile.bin\\" # Serialize the objects serialize_objects(objects_to_serialize, filename) # Deserialize the objects deserialized_objects = deserialize_objects(filename) # Output the deserialized objects print(deserialized_objects) # Output should be: [42, \\"hello\\", [1, 2, 3], {\\"key\\": \\"value\\"}] ``` Notes: - Ensure that the order of objects is maintained during serialization and deserialization. - Handle binary file operations properly to avoid data corruption. Good luck!","solution":"import marshal def serialize_objects(objects, filename): Serializes a list of Python objects to a file using marshal. Parameters: objects (list): List of Python objects to serialize. filename (str): The name of the file to write the serialized data. try: with open(filename, \'wb\') as file: for obj in objects: marshal.dump(obj, file, 2) except Exception as e: print(f\\"An error occurred during serialization: {e}\\") def deserialize_objects(filename): Deserializes Python objects from a file using marshal. Parameters: filename (str): The name of the file to read the serialized data. Returns: list: List of deserialized Python objects. objects = [] try: with open(filename, \'rb\') as file: while True: try: obj = marshal.load(file) objects.append(obj) except EOFError: break except Exception as e: print(f\\"An error occurred during deserialization: {e}\\") return objects"},{"question":"**Task: Implement a Function to Convert String Encodings** **Objective:** You are required to implement a function `convert_encoding(input_str, source_encoding, target_encoding)` in Python that demonstrates comprehension of Unicode handling and codec usage. **Function Signature:** ```python def convert_encoding(input_str: str, source_encoding: str, target_encoding: str) -> str: pass ``` **Description:** - Write a function that takes a string `input_str`, converts it from `source_encoding` to `target_encoding`, and returns the result. **Input:** - `input_str` (str): The input string that needs to be converted. - `source_encoding` (str): The encoding of the input string (e.g., \'utf-8\', \'ascii\', \'latin-1\'). - `target_encoding` (str): The target encoding to which the input string should be converted. **Output:** - The function should return a string that is the result of encoding conversion. **Constraints:** - The function should handle standard encoding formats supported by Python. - The conversion should be memory-efficient and handle any necessary error. - Assume `input_str` is always a valid string in `source_encoding` encoding. **Examples:** ```python # Example 1 result = convert_encoding(\\"hello\\", \\"utf-8\\", \\"latin-1\\") print(result) # Expected Output: \\"hello\\" # Example 2 result = convert_encoding(\\"Привет\\", \\"utf-8\\", \\"utf-16\\") print(result) # The output should be the correct utf-16 encoded representation of the input ``` **Note:** - You may use Python\'s built-in `codecs` module and relevant Unicode handling functions discussed in the documentation. - Ensure to handle any errors gracefully and handle different possible byte orders when applicable. **Additional Information:** - Pay attention to details in Unicode transformations and memory handling as stated in the provided documentation.","solution":"def convert_encoding(input_str: str, source_encoding: str, target_encoding: str) -> str: Converts a given input string from source encoding to target encoding. Args: input_str (str): The input string. source_encoding (str): The encoding of the input string. target_encoding (str): The encoding to convert the string to. Returns: str: The string converted to the target encoding. # Decode the input string from the source encoding to a Python unicode string unicode_str = input_str.encode(source_encoding).decode(source_encoding) # Encode the Python unicode string to the target encoding and then decode to get the final string result = unicode_str.encode(target_encoding).decode(target_encoding) return result"},{"question":"Problem Statement You are tasked with developing a simple multi-threaded web scraper using the `_thread` module in Python. The scraper will query a list of URLs and store the HTML content in a dictionary. To ensure thread safety when accessing the shared dictionary, you must use lock mechanisms. Requirements 1. Write a function, `fetch_content`, that takes a URL and a lock object as parameters. This function should: - Retrieve the HTML content of the URL. - Store the content in a shared dictionary `WEB_CONTENT` with the URL as the key. - Use the lock object to synchronize access to the shared dictionary. 2. Write a function, `scrape_urls`, that takes a list of URLs. This function should: - Create a lock object using `_thread.allocate_lock()`. - Start a new thread for each URL in the list, each executing the `fetch_content` function. - Wait for all threads to complete before finishing. Implementation Details - You must use the `_thread` module for threading and synchronization. - Use a global dictionary `WEB_CONTENT` to store the HTML content with URLs as keys. - Use the `requests` library to fetch webpage content. Ensure the library is installed in your environment. Constraints - Each URL in the list is unique. - Do not use high-level threading libraries like `threading`. Stick to the `_thread` module as specified. - Ensure that the solution is thread-safe and handles synchronization correctly. Expected Input and Output - **Input**: A list of URLs (e.g., `[\\"http://example.com\\", \\"http://example.org\\"]`) - **Output**: The function `scrape_urls` should not return anything but should populate the global dictionary `WEB_CONTENT` with the retrieved HTML content. ```python import _thread import requests # Global dictionary to store web content WEB_CONTENT = {} def fetch_content(url, lock): # Write your code here to fetch the content and store it in WEB_CONTENT dictionary pass def scrape_urls(url_list): # Write your code here to create threads and scrape URLs using fetch_content function pass # Example usage: # urls = [\\"http://example.com\\", \\"http://example.org\\"] # scrape_urls(urls) # print(WEB_CONTENT) ``` **Note**: Ensure to handle any exceptions that may arise during the HTTP request. Example Given the list of URLs: ```python urls = [\\"http://example.com\\", \\"http://example.org\\"] ``` After running `scrape_urls(urls)`, the `WEB_CONTENT` dictionary should be populated with the HTML content of each URL: ```python { \\"http://example.com\\": \\"<html>...</html>\\", \\"http://example.org\\": \\"<html>...</html>\\" } ``` You are expected to demonstrate thread synchronization using locks to protect shared resources properly.","solution":"import _thread import requests from time import sleep # Global dictionary to store web content WEB_CONTENT = {} def fetch_content(url, lock): try: response = requests.get(url) lock.acquire() try: WEB_CONTENT[url] = response.text finally: lock.release() except requests.RequestException as e: lock.acquire() try: WEB_CONTENT[url] = None finally: lock.release() print(f\\"Error fetching {url}: {e}\\") def scrape_urls(url_list): lock = _thread.allocate_lock() threads = [] for url in url_list: _thread.start_new_thread(fetch_content, (url, lock)) # Wait for a while to ensure all threads are finished sleep(5)"},{"question":"Manipulating Sparse Data Structures in Pandas **Objective**: This question assesses your ability to work with sparse data structures in pandas and perform various operations including creating sparse arrays, performing calculations, and converting between different formats. **Problem Statement**: You have been given a large dataset representing sensor readings across multiple devices over a period. Most of the readings are missing or zero due to downtime of devices. You are required to perform the following tasks using pandas\' sparse functionalities: 1. **Create a Sparse DataFrame**: Given a 2D NumPy array of sensor readings where most values are `0`, convert this array to a pandas `DataFrame` and then convert it to a sparse DataFrame. 2. **Perform Calculations**: Apply a NumPy universal function (ufunc) to perform an element-wise absolute value operation on the sparse DataFrame. 3. **Conversion**: Convert the sparse DataFrame back to a dense DataFrame, and provide the memory usage difference between the sparse and dense DataFrames. 4. **Interaction with scipy.sparse**: Convert the sparse DataFrame into a scipy CSR (Compressed Sparse Row) matrix, and then convert this matrix back to a pandas DataFrame. **Input and Output Specifications**: - **Input**: - A 2D NumPy array `sensor_readings` of shape `(1000, 5)` where most elements are `0`. - **Output**: 1. A pandas sparse DataFrame. 2. The result of applying `np.abs` on the sparse DataFrame. 3. The memory usage (in bytes) of both the original dense DataFrame and the sparse DataFrame. 4. The scipy CSR matrix converted from the sparse DataFrame. 5. The pandas DataFrame converted back from the scipy CSR matrix. **Constraints**: - The provided 2D NumPy array has at least 90% of its elements as `0`. **Guidelines**: - Use pandas\' `DataFrame` and `SparseDtype` to create and manipulate sparse DataFrames. - Use `np.abs` to perform the absolute value operation. - Use memory usage methods to compare the sizes. - Use `scipy.sparse.csr_matrix` for conversion to CSR format. - Ensure to handle any necessary imports. You may assume that all necessary imports (numpy, pandas, scipy) have been made. Write the function signature as follows: ```python def manipulate_sparse_data(sensor_readings: np.ndarray) -> tuple: Manipulate sparse data structures in pandas and perform various operations. Parameters: sensor_readings (np.ndarray): A 2D numpy array of sensor readings. Returns: tuple: - pd.DataFrame: Sparse DataFrame. - pd.DataFrame: Result of applying np.abs on the sparse DataFrame. - dict: Memory usage of the original dense and sparse DataFrames. - csr_matrix: Scipy CSR matrix converted from sparse DataFrame. - pd.DataFrame: pandas DataFrame converted back from CSR matrix. pass ``` You should implement the function as per the described operations and requirements.","solution":"import numpy as np import pandas as pd from scipy.sparse import csr_matrix def manipulate_sparse_data(sensor_readings: np.ndarray) -> tuple: Manipulate sparse data structures in pandas and perform various operations. Parameters: sensor_readings (np.ndarray): A 2D numpy array of sensor readings. Returns: tuple: - pd.DataFrame: Sparse DataFrame. - pd.DataFrame: Result of applying np.abs on the sparse DataFrame. - dict: Memory usage of the original dense and sparse DataFrames. - csr_matrix: Scipy CSR matrix converted from sparse DataFrame. - pd.DataFrame: pandas DataFrame converted back from CSR matrix. # Step 1: Create a dense DataFrame from the numpy array dense_df = pd.DataFrame(sensor_readings) # Convert the dense DataFrame to a sparse DataFrame sparse_df = dense_df.astype(pd.SparseDtype(\\"float\\", 0.0)) # Step 2: Perform element-wise absolute value operation using numpy\'s ufunc abs_sparse_df = sparse_df.applymap(np.abs) # Step 3: Memory usage comparison between dense and sparse DataFrames memory_usage = { \\"dense\\": dense_df.memory_usage(deep=True).sum(), \\"sparse\\": sparse_df.memory_usage(deep=True).sum() } # Step 4: Convert the sparse DataFrame to a scipy CSR matrix csr_matrix_sparse = csr_matrix(sparse_df.sparse.to_coo()) # Convert the CSR matrix back to a pandas DataFrame dense_back_df = pd.DataFrame.sparse.from_spmatrix(csr_matrix_sparse, columns=sparse_df.columns) return sparse_df, abs_sparse_df, memory_usage, csr_matrix_sparse, dense_back_df"},{"question":"# Seaborn Swarmplot Customization and Analysis Objective: Design and implement a function that performs swarm plot visualization on the \\"tips\\" dataset, showcasing different levels of customization and analysis. Problem: Write a function `custom_swarmplot()` that performs the following tasks: 1. Loads the \\"tips\\" dataset from Seaborn. 2. Creates a vertical swarm plot with \\"day\\" as the x-axis and \\"total_bill\\" as the y-axis. 3. Adds the \\"sex\\" variable to the plot, using the `hue` parameter. 4. Customizes the plot by: - Changing the marker style to \'x\'. - Decreasing the point size to 4. - Adjusting the palette to \\"deep\\". 5. Separates the swarm plot based on the \\"time\\" variable using the `sns.catplot` function, arranging plots in multiple columns. Constraints: - Ensure the points do not overlap significantly. - The function should not return any value, but the plots should be displayed. Input and Output Format: The function `custom_swarmplot()` does not take any parameters or return any values. It should directly use the Seaborn library to generate and display the required plots. Function Signature: ```python def custom_swarmplot(): pass ``` Example: Executing `custom_swarmplot()` should produce swarm plots as described above, visualizing the relationship in the \\"tips\\" dataset.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_swarmplot(): # Load the \\"tips\\" dataset from Seaborn tips = sns.load_dataset(\\"tips\\") # Create the swarm plot using sns.catplot for facetting by \\"time\\" g = sns.catplot( x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", data=tips, kind=\\"swarm\\", col=\\"time\\", marker=\\"x\\", s=4, palette=\\"deep\\" ) # Show the plot plt.show()"},{"question":"**Objective:** Implement an enhanced interactive console in Python using the `code` module. **Description:** You are required to design a class `EnhancedInteractiveConsole` that extends the functionality of `InteractiveConsole` from the `code` module. This console should have additional features such as: 1. **Tracking Command History:** Maintain a list of all commands entered by the user. - Method `get_history()`: Returns the list of all commands entered. 2. **Executing Commands from History:** Allow executing any previously entered command by its index in history. - Method `execute_from_history(index)`: Accepts an integer index and executes the corresponding command from history. 3. **Clearing Command History:** Provide an option to clear the command history. - Method `clear_history()`: Clears the command history list. **Input and Output:** - The core input will be provided via the command line as in a typical REPL. - The command history methods will be invoked as needed and their results will be managed within the console. **Constraints:** - The command history should only keep valid executed commands, not incomplete or erroneous ones. - Consider safety and performance; the solution should handle typical use scenarios gracefully. **Performance Requirements:** - Ensure that the class methods operate efficiently, even with a substantial number of commands in history. Example Usage: ```python from mymodule import EnhancedInteractiveConsole console = EnhancedInteractiveConsole() console.interact(\\"Welcome to Enhanced Interactive Console!\\") # After interacting with the console, user enters some commands # Retrieve and print command history print(console.get_history()) # Execute a previously entered command by its index in history console.execute_from_history(2) # Clear the command history console.clear_history() print(console.get_history()) # Should print an empty list ``` **Your task:** Implement the `EnhancedInteractiveConsole` class as described. Utilize the `code` module and its classes/methods appropriately to achieve the desired functionality.","solution":"import code class EnhancedInteractiveConsole(code.InteractiveConsole): def __init__(self, locals=None, filename=\\"<console>\\"): super().__init__(locals, filename) self.command_history = [] def runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\"): more = super().runsource(source, filename, symbol) if not more: # Only store complete and valid commands self.command_history.append(source) return more def get_history(self): return self.command_history.copy() def execute_from_history(self, index): if 0 <= index < len(self.command_history): code_to_execute = self.command_history[index] exec(code_to_execute, self.locals) else: raise IndexError(\\"Invalid index; command history does not have the specified index.\\") def clear_history(self): self.command_history.clear()"},{"question":"**Question: Implement Incremental PCA for Large Datasets** As a data scientist, you are given a large dataset that cannot fit into memory all at once. Your task is to implement a function that performs Incremental Principal Component Analysis (Incremental PCA) to reduce the dimensionality of this dataset in a memory-efficient manner. # Function Signature ```python def incremental_pca(data_batches: List[np.ndarray], n_components: int) -> np.ndarray: Applies Incremental PCA to a list of data batches and returns the transformed data. Parameters: - data_batches (List[np.ndarray]): A list containing numpy arrays of data batches. Each batch is an array of shape (n_samples, n_features). - n_components (int): The number of principal components to keep. Returns: - transformed_data (np.ndarray): The transformed data in the lower-dimensional space with shape (total_samples, n_components). pass ``` # Input - `data_batches`: A list of numpy arrays, where each array represents a batch of the data to be processed incrementally by the PCA algorithm. Each array has dimensions `(n_samples, n_features)`. - `n_components`: An integer representing the number of principal components to retain. # Output - Returns a single numpy array, representing the transformed data in the reduced-dimensional space. The array should have dimensions `(total_samples, n_components)`, where `total_samples` is the sum of the samples across all batches. # Example ```python import numpy as np # Example data batches data_batches = [ np.array([[1.0, 2.0], [3.0, 4.0]]), np.array([[5.0, 6.0], [7.0, 8.0]]) ] # Number of components n_components = 1 # Expected transformed data transformed_data = incremental_pca(data_batches, n_components) print(transformed_data) ``` # Constraints - Each data batch array has the same number of features (`n_features`). - The total number of samples (`total_samples`) across all batches is greater than the number of principal components (`n_components`). - Handle the data incrementally to ensure memory efficiency. # Notes - You may use the `IncrementalPCA` class from the `sklearn.decomposition` module to implement this function. - Ensure that the function processes the data in a way that optimizes memory usage and computational efficiency.","solution":"import numpy as np from sklearn.decomposition import IncrementalPCA from typing import List def incremental_pca(data_batches: List[np.ndarray], n_components: int) -> np.ndarray: Applies Incremental PCA to a list of data batches and returns the transformed data. Parameters: - data_batches (List[np.ndarray]): A list containing numpy arrays of data batches. Each batch is an array of shape (n_samples, n_features). - n_components (int): The number of principal components to keep. Returns: - transformed_data (np.ndarray): The transformed data in the lower-dimensional space with shape (total_samples, n_components). ipca = IncrementalPCA(n_components=n_components) for batch in data_batches: ipca.partial_fit(batch) transformed_batches = [ipca.transform(batch) for batch in data_batches] transformed_data = np.vstack(transformed_batches) return transformed_data"},{"question":"**Problem Statement: Pharmaceutical Inventory Management** You have been tasked with developing a small part of a pharmaceutical inventory management system. Specifically, you need to implement functionality to manage a inventory of certain products using Python lists. # Requirements: You need to implement the following functions: 1. **add_product(inventory, product_name)**: - **Input**: - `inventory`: A list of strings where each string represents a product in the current inventory. - `product_name`: A string representing the name of the product to add to the inventory. - **Output**: - None - **Description**: Adds the product to the end of the inventory list. 2. **remove_product(inventory, product_name)**: - **Input**: - `inventory`: A list of strings where each string represents a product in the current inventory. - `product_name`: A string representing the name of the product to be removed from the inventory. - **Output**: - None - **Description**: Removes the product from the inventory list. If the product does not exist, raises a `ValueError` with the message \\"Product not in inventory\\". 3. **count_product(inventory, product_name)**: - **Input**: - `inventory`: A list of strings where each string represents a product in the current inventory. - `product_name`: A string representing the name of the product to count in the inventory list. - **Output**: - An integer representing the number of times the product appears in the inventory. - **Description**: Returns the count of the specified product in the inventory. 4. **get_inventory()**: - **Output**: - Returns a list (inventory) which is initially empty and will be used to store product names. - **Description**: Initializes and returns an empty inventory list. # Constraints: - All product names are non-empty strings. - Inventory can contain duplicate products. # Example usage: ```python inventory = get_inventory() add_product(inventory, \'Aspirin\') add_product(inventory, \'Tylenol\') add_product(inventory, \'Aspirin\') print(count_product(inventory, \'Aspirin\')) # Output: 2 print(count_product(inventory, \'Tylenol\')) # Output: 1 remove_product(inventory, \'Aspirin\') print(count_product(inventory, \'Aspirin\')) # Output: 1 # Try removing a product not in the inventory try: remove_product(inventory, \'Ibuprofen\') except ValueError as e: print(e) # Expected Output: Product not in inventory ``` # Notes: - The function `remove_product` should raise a `ValueError` with the message \\"Product not in inventory\\" if the product is not found in the inventory. - The `count_product` function should correctly count all occurrences of the product in the inventory. - Make sure to use list methods where possible, as this assessment focuses on your understanding of list operations.","solution":"def add_product(inventory, product_name): Adds the product to the end of the inventory list. inventory.append(product_name) def remove_product(inventory, product_name): Removes the product from the inventory list. If the product does not exist, raises a ValueError. if product_name in inventory: inventory.remove(product_name) else: raise ValueError(\\"Product not in inventory\\") def count_product(inventory, product_name): Returns the count of the specified product in the inventory. return inventory.count(product_name) def get_inventory(): Initializes and returns an empty inventory list. return []"},{"question":"# Complex Logging Configuration and Custom Filtering **Objective:** Implement a structured logging system using the Python logging module. The logging configuration should demonstrate the ability to: 1. Set up multiple loggers with different logging levels. 2. Create custom handlers to direct log messages to specific destinations. 3. Implement a custom filter to add contextual information to the log messages. 4. Customize log message formatting using formatters. **Question:** You are tasked with setting up a logging system for a multi-component Python application. Your logging system should: 1. Create a logger named `app` that outputs DEBUG level messages to the console. 2. Create another logger named `audit` that writes WARNING level messages to a file called `audit.log`. 3. Implement a custom filter `ContextFilter` that injects the current user information (`user` parameter) into each log message. 4. Use a custom formatter for the console logger that displays the log level, logger name, and message. 5. Use another formatter for the file logger that includes the time, log level, logger name, and message. **Instructions:** 1. **Logger Setup:** - Create the `app` logger with DEBUG level. - Create the `audit` logger with WARNING level. 2. **Handlers and Filters:** - Create a stream handler for the `app` logger. - Create a file handler for the `audit` logger directed at `audit.log`. - Implement and add the `ContextFilter` to both handlers. 3. **Formatters:** - For the console logger, format should be `\'<LogLevel> - <LoggerName> - <Message>\'`. - For the file logger, format should be `\'<Time> - <LogLevel> - <LoggerName> - <Message>\'`. 4. **Logging Example:** - Demonstrate logging in both loggers. - Show how the log message will appear with and without the user context. **Constraints:** - You can assume that the user has been defined as an environmental variable or passed through some context manager. - Attribute `user` should be part of each log message. **Expected Functions:** ```python import logging class ContextFilter(logging.Filter): def __init__(self, user): super().__init__() self.user = user def filter(self, record): record.user = self.user return True def setup_logging(): user = \\"test_user\\" # assuming user is predefined # Creating and configuring logger \'app\' app_logger = logging.getLogger(\'app\') app_logger.setLevel(logging.DEBUG) console_handler = logging.StreamHandler() console_formatter = logging.Formatter(\'%(levelname)s - %(name)s - %(message)s\') console_handler.setFormatter(console_formatter) context_filter = ContextFilter(user) console_handler.addFilter(context_filter) app_logger.addHandler(console_handler) # Creating and configuring logger \'audit\' audit_logger = logging.getLogger(\'audit\') audit_logger.setLevel(logging.WARNING) file_handler = logging.FileHandler(\'audit.log\') file_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(name)s - %(message)s\') file_handler.setFormatter(file_formatter) file_handler.addFilter(context_filter) audit_logger.addHandler(file_handler) return app_logger, audit_logger def log_messages(): app_logger, audit_logger = setup_logging() # Logging messages app_logger.debug(\'This is a debug message\') app_logger.info(\'This is an informational message\') app_logger.warning(\'This is a warning message\') app_logger.error(\'This is an error message\') app_logger.critical(\'This is a critical message\') audit_logger.debug(\'Audit debug message\') audit_logger.warning(\'Audit warning message\') audit_logger.error(\'Audit error message\') audit_logger.critical(\'Audit critical message\') # Example output: # - From console: # DEBUG - app - This is a debug message # INFO - app - This is an informational message # WARNING - app - This is a warning message # ERROR - app - This is an error message # CRITICAL - app - This is a critical message # - In audit.log: # 2023-10-01 12:00:00 - WARNING - audit - Audit warning message # 2023-10-01 12:00:01 - ERROR - audit - Audit error message # 2023-10-01 12:00:02 - CRITICAL - audit - Audit critical message ``` **Deliverables:** - Implement the `ContextFilter` class. - Create and configure the two loggers as described. - Write a function to demonstrate logging different levels of messages.","solution":"import logging class ContextFilter(logging.Filter): def __init__(self, user): super().__init__() self.user = user def filter(self, record): record.user = self.user return True def setup_logging(): user = \\"test_user\\" # assuming user is predefined # Creating and configuring logger \'app\' app_logger = logging.getLogger(\'app\') app_logger.setLevel(logging.DEBUG) console_handler = logging.StreamHandler() console_formatter = logging.Formatter(\'%(levelname)s - %(name)s - %(message)s - [user: %(user)s]\') console_handler.setFormatter(console_formatter) context_filter = ContextFilter(user) console_handler.addFilter(context_filter) app_logger.addHandler(console_handler) # Creating and configuring logger \'audit\' audit_logger = logging.getLogger(\'audit\') audit_logger.setLevel(logging.WARNING) file_handler = logging.FileHandler(\'audit.log\') file_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(name)s - %(message)s - [user: %(user)s]\') file_handler.setFormatter(file_formatter) file_handler.addFilter(context_filter) audit_logger.addHandler(file_handler) return app_logger, audit_logger def log_messages(): app_logger, audit_logger = setup_logging() # Logging messages for \'app\' logger to console app_logger.debug(\'This is a debug message\') app_logger.info(\'This is an informational message\') app_logger.warning(\'This is a warning message\') app_logger.error(\'This is an error message\') app_logger.critical(\'This is a critical message\') # Logging messages for \'audit\' logger to file audit_logger.debug(\'Audit debug message\') audit_logger.warning(\'Audit warning message\') audit_logger.error(\'Audit error message\') audit_logger.critical(\'Audit critical message\')"},{"question":"# HTTP Status Code Checker **Objective**: You are required to write a function that checks HTTP status codes and categorizes them. This function should make use of the `http.HTTPStatus` enum class. **Problem Statement**: Implement a function `categorize_http_status(codes: List[int]) -> Tuple[Dict[str, List[int]], Dict[int, str]]` that takes a list of HTTP status codes as input and returns a tuple containing two dictionaries: 1. The first dictionary categorizes the status codes into five categories: - \'Informational\': 1xx - \'Successful\': 2xx - \'Redirection\': 3xx - \'Client Error\': 4xx - \'Server Error\': 5xx 2. The second dictionary maps each status code from the input to its corresponding phrase provided by `http.HTTPStatus`. **Constraints**: - Assume the input list will only contain valid HTTP status codes defined in `http.HTTPStatus`. **Input**: - A list of integers representing HTTP status codes. **Output**: - A tuple of two dictionaries: - The first dictionary has keys representing categories (\'Informational\', \'Successful\', \'Redirection\', \'Client Error\', \'Server Error\') and values as lists of status codes belonging to those categories. - The second dictionary maps the status code (int) to its corresponding phrase (str). **Example**: ```python from http import HTTPStatus def categorize_http_status(codes): result1 = { \'Informational\': [], \'Successful\': [], \'Redirection\': [], \'Client Error\': [], \'Server Error\': [] } result2 = {} for code in codes: status = HTTPStatus(code) if 100 <= code < 200: result1[\'Informational\'].append(code) elif 200 <= code < 300: result1[\'Successful\'].append(code) elif 300 <= code < 400: result1[\'Redirection\'].append(code) elif 400 <= code < 500: result1[\'Client Error\'].append(code) elif 500 <= code < 600: result1[\'Server Error\'].append(code) result2[code] = status.phrase return result1, result2 # Example use case codes = [200, 301, 404, 500, 102] categorized_statuses, status_phrases = categorize_http_status(codes) print(categorized_statuses) # Output: {\'Informational\': [102], \'Successful\': [200], \'Redirection\': [301], \'Client Error\': [404], \'Server Error\': [500]} print(status_phrases) # Output: {200: \'OK\', 301: \'Moved Permanently\', 404: \'Not Found\', 500: \'Internal Server Error\', 102: \'Processing\'} ``` **Notes**: - Make sure to test your implementation with various lists of status codes to ensure correctness. - Handle edge cases where the list might be empty.","solution":"from typing import List, Dict, Tuple from http import HTTPStatus def categorize_http_status(codes: List[int]) -> Tuple[Dict[str, List[int]], Dict[int, str]]: Categorizes HTTP status codes into five categories and maps each status code to its corresponding phrase. # Initialize the dictionaries categories = { \'Informational\': [], \'Successful\': [], \'Redirection\': [], \'Client Error\': [], \'Server Error\': [] } status_phrases = {} # Categorize each code and get its phrase for code in codes: status = HTTPStatus(code) if 100 <= code < 200: categories[\'Informational\'].append(code) elif 200 <= code < 300: categories[\'Successful\'].append(code) elif 300 <= code < 400: categories[\'Redirection\'].append(code) elif 400 <= code < 500: categories[\'Client Error\'].append(code) elif 500 <= code < 600: categories[\'Server Error\'].append(code) status_phrases[code] = status.phrase return categories, status_phrases"},{"question":"# Context Variables and Token Management in Python The `contextvars` module allows for managing context-local state, which is essential in asynchronous programming and other complex applications. You are required to implement a simple context variable management system using the Python `contextvars` package. Demonstrating your understanding of this module, you will create a few helper functions to manage context variables and contexts. Task: 1. **Create a new context and set it as the current context.** - Implement a function `create_and_set_context()` that: - Creates a new context using `PyContext_New`. - Sets this new context as the current context for the thread using `PyContext_Enter`. - Returns the created context. 2. **Define and set context variables within the current context.** - Implement a function `define_and_set_context_var(context, var_name, var_value)` that: - Creates a new context variable using `PyContextVar_New` with the provided `var_name` and sets its value to `var_value` within the provided `context` using `PyContextVar_Set`. - Returns the created context variable and the associated token. 3. **Retrieve the value of a context variable.** - Implement a function `get_context_var_value(context, var)` that: - Retrieves the value of the provided context variable `var` within the provided `context` using `PyContextVar_Get`. - Returns the retrieved value. 4. **Reset the context variable to its previous state using the provided token.** - Implement a function `reset_context_var(context, var, token)` that: - Resets the context variable `var` within the provided `context` to its previous state using the provided `token` with `PyContextVar_Reset`. - Returns a boolean indicating whether the reset was successful. Constraints: - Do not use global variables; all state should be managed within function scopes. - Handle possible errors gracefully, providing meaningful output for failure cases. Example: ```python # Create and set a new context ctx = create_and_set_context() # Define and set a context variable within the new context var, token = define_and_set_context_var(ctx, \'user_id\', 12345) # Retrieve the value of the context variable value = get_context_var_value(ctx, var) print(value) # Output should be 12345 # Reset the context variable using the token success = reset_context_var(ctx, var, token) print(success) # Output should be True # Retrieve the value again after reset (should be the default or None) value = get_context_var_value(ctx, var) print(value) # Output should be None or the default value set ``` Notes: - Make sure to import `contextvars` for your implementation. - Use appropriate error handling techniques to cover edge cases and invalid scenarios. - Write concise and clear comments in your code to explain your logic.","solution":"import contextvars def create_and_set_context(): Creates a new context and sets it as the current context. new_context = contextvars.copy_context() return new_context def define_and_set_context_var(context, var_name, var_value): Defines and sets a context variable within the provided context. Parameters: context - The context to set the variable in var_name - The name of the context variable var_value - The value of the context variable Returns: (context variable, token) context_var = contextvars.ContextVar(var_name) token = context_var.set(var_value) return context_var, token def get_context_var_value(context, var): Retrieves the value of the provided context variable within the given context. Parameters: context - The context to get the variable value from var - The context variable to retrieve the value of Returns: The value of the context variable return var.get(None) def reset_context_var(context, var, token): Resets the context variable to its previous state using the provided token. Parameters: context - The context to reset the variable in var - The context variable to reset token - The token representing the previous state Returns: Boolean indicating whether the reset was successful try: var.reset(token) return True except Exception as e: return False"},{"question":"**Title**: Predicting Housing Prices using an Ensemble Method **Objective**: Assess the ability to implement, train, and evaluate an ensemble method (e.g., Random Forest) for a regression problem using scikit-learn. **Problem Statement**: You are provided with a dataset containing information about various aspects of houses. Your task is to predict the housing prices using a Random Forest Regression model. You will need to preprocess the data, train the model, make predictions, and evaluate its performance. # Input: A CSV file named `housing_data.csv` which contains the following columns: - `LotArea`: numeric, size of the house lot - `OverallQual`: numeric, overall material and finish quality - `YearBuilt`: numeric, original construction date - `TotalBsmtSF`: numeric, total square feet of basement area - `GrLivArea`: numeric, above grade (ground) living area in square feet - `FullBath`: numeric, number of full bathrooms - `GarageCars`: numeric, number of cars that can fit in the garage - `SalePrice`: numeric, the sale price of the house (this is the target variable) # Requirements: 1. Load the dataset. 2. Perform any necessary data preprocessing. 3. Split the dataset into training and testing sets (80% train, 20% test). 4. Initialize and train a Random Forest Regressor on the training set. 5. Make predictions on the test set. 6. Evaluate the model using Mean Squared Error (MSE) and R^2 Score. 7. Print the MSE and R^2 Score for the test set. # Constraints: - You are expected to utilize the `RandomForestRegressor` class from scikit-learn. - Address any missing values and encode categorical features if necessary. - You can assume that the dataset does not contain any categorical features, only numerical features as mentioned. # Performance Requirements: - Efficiently handle the dataset preprocessing and model training. - Provide well-commented code for clarity. # Example Function Signature: ```python def predict_housing_prices(file_path: str) -> tuple: # Implementation here ``` **Example Usage**: ```python file_path = \'housing_data.csv\' mse, r2 = predict_housing_prices(file_path) print(f\\"Mean Squared Error: {mse}\\") print(f\\"R^2 Score: {r2}\\") ``` This question will assess the student\'s ability to handle a common machine learning workflow using scikit-learn, from data preprocessing through model evaluation.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error, r2_score def predict_housing_prices(file_path: str) -> tuple: # Load the dataset data = pd.read_csv(file_path) # Data preprocessing: check for missing values (not expected as per problem statement) # In a real scenario, we should handle missing values here # Selecting features and target variable features = [\'LotArea\', \'OverallQual\', \'YearBuilt\', \'TotalBsmtSF\', \'GrLivArea\', \'FullBath\', \'GarageCars\'] target = \'SalePrice\' X = data[features] y = data[target] # Splitting the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initializing and training the Random Forest Regressor rf = RandomForestRegressor(random_state=42) rf.fit(X_train, y_train) # Making predictions on the test set y_pred = rf.predict(X_test) # Evaluating the model mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) # Return the evaluation metrics return mse, r2"},{"question":"**Coding Assessment Question** **Objective:** The goal of this project is to test your proficiency with the Seaborn library, specifically the `sns.diverging_palette` function. You will be implementing a function to create and visualize various types of diverging palettes based on given parameters. **Problem Statement:** Write a function `generate_diverging_palette` that takes in the following parameters: 1. `h_neg` (int): The hue of the negative end of the palette. 2. `h_pos` (int): The hue of the positive end of the palette. 3. `center` (str): Center color of the palette, either \\"light\\" or \\"dark\\". Default is \\"light\\". 4. `as_cmap` (bool): Whether to return a continuous colormap instead of a discrete palette. Default is False. 5. `sep` (int): Amount of separation around the center value. Default is 1. 6. `s` (int): Saturation of the endpoints. Default is 100. 7. `l` (int): Lightness of the endpoints. Default is 50. The function should generate a diverging palette using the provided parameters and then visualize it using Seaborn. The function should return a Seaborn palette or colormap object as specified by the `as_cmap` parameter. **Constraints:** - `h_neg` and `h_pos` should be within the range [0, 360]. - `center` should be either \\"light\\" or \\"dark\\". - `sep` should be a non-negative integer. - `s` should be within the range [0, 100]. - `l` should be within the range [0, 100]. **Input:** - `h_neg` (int): 0 <= `h_neg` <= 360 - `h_pos` (int): 0 <= `h_pos` <= 360 - `center` (str): \\"light\\" or \\"dark\\" - `as_cmap` (bool): True or False - `sep` (int): sep >= 0 - `s` (int): 0 <= `s` <= 100 - `l` (int): 0 <= `l` <= 100 **Output:** - A Seaborn palette or colormap object. **Function Signature:** ```python def generate_diverging_palette(h_neg: int, h_pos: int, center: str = \\"light\\", as_cmap: bool = False, sep: int = 1, s: int = 100, l: int = 50): pass ``` **Example:** ```python # Example usage palette = generate_diverging_palette(240, 20, center=\\"dark\\", as_cmap=True, sep=30, s=70, l=40) # This should generate and return a continuous colormap with the specified properties ``` **Additional Requirements:** - Include docstrings in your function for clarity. - Implement a main function to test multiple cases of the `generate_diverging_palette` function. **Note:** You may utilize any of the methods provided by the Seaborn library to validate the implemented function.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_diverging_palette(h_neg: int, h_pos: int, center: str = \\"light\\", as_cmap: bool = False, sep: int = 1, s: int = 100, l: int = 50): Generates and visualizes a diverging palette based on the given parameters. Parameters: h_neg (int): Hue of the negative end of the palette. h_pos (int): Hue of the positive end of the palette. center (str): Center color of the palette, either \\"light\\" or \\"dark\\". Default is \\"light\\". as_cmap (bool): Whether to return a continuous colormap instead of a discrete palette. Default is False. sep (int): Amount of separation around the center value. Default is 1. s (int): Saturation of the endpoints. Default is 100. l (int): Lightness of the endpoints. Default is 50. Returns: sns_palette or sns_cmap: Seaborn palette or colormap object. # Validate parameters if not (0 <= h_neg <= 360): raise ValueError(\\"h_neg must be within the range [0, 360]\\") if not (0 <= h_pos <= 360): raise ValueError(\\"h_pos must be within the range [0, 360]\\") if center not in [\\"light\\", \\"dark\\"]: raise ValueError(\'center must be either \\"light\\" or \\"dark\\"\') if sep < 0: raise ValueError(\\"sep must be a non-negative integer\\") if not (0 <= s <= 100): raise ValueError(\\"saturation (s) must be within the range [0, 100]\\") if not (0 <= l <= 100): raise ValueError(\\"lightness (l) must be within the range [0, 100]\\") # Generate diverging palette palette = sns.diverging_palette(h_neg, h_pos, sep=sep, s=s, l=l, center=center, as_cmap=as_cmap) # Visualize the palette if not returning as cmap if not as_cmap: sns.palplot(palette) plt.show() return palette # Example usage # palette = generate_diverging_palette(240, 20, center=\\"dark\\", as_cmap=True, sep=30, s=70, l=40)"},{"question":"**Question: Implementing and Utilizing Subprocess Handlers in PyTorch Distributed** # Background PyTorch provides a module `torch.distributed.elastic.multiprocessing.subprocess_handler` to manage subprocesses for distributed computing tasks. Understanding how to use this module is critical for developing scalable and efficient distributed machine learning applications. # Objective You are tasked with implementing a class `DistributedTrainer` that utilizes the `SubprocessHandler` to manage multiple subprocesses for a distributed training scenario. # Requirements 1. **Class Definition:** - Define a class named `DistributedTrainer`. 2. **Initialization:** - The class should be initialized with a list of commands where each command represents a subprocess to be executed. The commands will be strings (e.g., `[\\"python train.py --rank 0\\", \\"python train.py --rank 1\\"]`). 3. **Method `run_all`:** - Implement a method `run_all` that executes all the commands in parallel using the `SubprocessHandler`. - Ensure that you capture the output and error streams of each subprocess. 4. **Method `collect_results`:** - Implement a method `collect_results` that gathers the outputs of all subprocesses and returns them as a dictionary where the key is the command and the value is its corresponding output. # Inputs and Outputs - **Input:** - A list of command strings that will be executed as subprocesses. - **Output:** - A dictionary mapping each command to its output produced during the execution. # Constraints - Assume that the commands are valid and executable in the given environment. - The function should handle any exceptions raised during subprocess execution and record them in the result dictionary. # Example ```python from torch.distributed.elastic.multiprocessing.subprocess_handler import SubprocessHandler, get_subprocess_handler class DistributedTrainer: def __init__(self, commands): self.commands = commands self.handlers = {} def run_all(self): for command in self.commands: handler = SubprocessHandler(command) self.handlers[command] = handler handler.start() def collect_results(self): results = {} for command, handler in self.handlers.items(): handler.join() if handler.returncode == 0: results[command] = handler.stdout.read().decode(\'utf-8\') else: results[command] = handler.stderr.read().decode(\'utf-8\') return results # Example Usage: trainer = DistributedTrainer([\\"python train.py --rank 0\\", \\"python train.py --rank 1\\"]) trainer.run_all() results = trainer.collect_results() print(results) ``` In this example, you will need to start the subprocesses, manage their lifecycles via the `SubprocessHandler`, and collect their outputs into a dictionary, ensuring appropriate handling of exceptions. **Note:** - You may need to mock subprocess execution or ensure a proper environment to test the distributed training commands.","solution":"from torch.distributed.elastic.multiprocessing.subprocess_handler import SubprocessHandler import subprocess class DistributedTrainer: def __init__(self, commands): self.commands = commands self.handlers = [] def run_all(self): self.handlers = [ subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE) for command in self.commands ] def collect_results(self): results = {} for command, handler in zip(self.commands, self.handlers): stdout, stderr = handler.communicate() if handler.returncode == 0: results[command] = stdout.decode(\'utf-8\') else: results[command] = stderr.decode(\'utf-8\') return results"},{"question":"**Problem Statement:** You are given a set of CSV files that hold information about books in a library. Each file has a different structure, and they all come from different sources. Your task is to: 1. Create a function `parse_csv(file_name, delimiter, quotechar, quoting, has_header)` that reads a CSV file and returns the data in a structured format. The function should automatically handle the presence or absence of headers and different quoting conventions. 2. Create a function `merge_csv(files_params, output_file, output_dialect=\'unix\')` that reads multiple CSV files, merges them into a single CSV file, and writes the result in a specified dialect. **Function Specifications:** 1. **`parse_csv` function:** - **Parameters:** - `file_name` (string): The name of the CSV file. - `delimiter` (char): The character that separates fields in the CSV file. - `quotechar` (char): The character used to quote fields containing special characters. - `quoting` (int): Controls when quotes are recognized (one of the `csv.QUOTE_*` constants). - `has_header` (boolean): Specifies whether the CSV file has a header row. - **Output:** - If `has_header` is `True`, returns a list of dictionaries where each dictionary represents a row. - If `has_header` is `False`, returns a list of lists. ```python def parse_csv(file_name, delimiter, quotechar, quoting, has_header): import csv result = [] with open(file_name, mode=\'r\', newline=\'\', encoding=\'utf-8\') as csvfile: if has_header: reader = csv.DictReader(csvfile, delimiter=delimiter, quotechar=quotechar, quoting=quoting) for row in reader: result.append(row) else: reader = csv.reader(csvfile, delimiter=delimiter, quotechar=quotechar, quoting=quoting) for row in reader: result.append(row) return result ``` 2. **`merge_csv` function:** - **Parameters:** - `files_params` (list of tuples): Each tuple contains the parameters for `parse_csv` function. - `output_file` (string): The name of the output CSV file. - `output_dialect` (string): The dialect to use for the output CSV file (default is \'unix\'). - **Output:** Creates a merged CSV file at the specified location. ```python def merge_csv(files_params, output_file, output_dialect=\'unix\'): import csv merged_data = [] fieldnames = set() # Read and merge data from all files for params in files_params: data = parse_csv(*params) if isinstance(data[0], dict): fieldnames.update(data[0].keys()) merged_data.extend(data) fieldnames = list(fieldnames) with open(output_file, mode=\'w\', newline=\'\', encoding=\'utf-8\') as csvfile: writer = csv.DictWriter(csvfile, fieldnames=fieldnames, dialect=output_dialect) writer.writeheader() for row in merged_data: if isinstance(row, dict): writer.writerow(row) else: writer.writerow({fieldnames[i]: row[i] for i in range(len(row))}) ``` **Example Usage:** ```python # Parse a single CSV file data = parse_csv(\'books1.csv\', \',\', \'\\"\', csv.QUOTE_MINIMAL, True) print(data) # Merge multiple CSV files files_params = [ (\'books1.csv\', \',\', \'\\"\', csv.QUOTE_MINIMAL, True), (\'books2.csv\', \';\', \\"\'\\", csv.QUOTE_ALL, False) ] merge_csv(files_params, \'merged_books.csv\') ``` **Constraints:** - The input files can contain a maximum of 10000 rows. - Each row can have a maximum of 50 fields. - Handle possible errors such as file not found or parsing issues gracefully, by raising appropriate exceptions and providing meaningful error messages.","solution":"import csv def parse_csv(file_name, delimiter, quotechar, quoting, has_header): Parses a CSV file and returns the data in structured format. Parameters: - file_name (string): The name of the CSV file. - delimiter (char): The character that separates fields in the CSV file. - quotechar (char): The character used to quote fields containing special characters. - quoting (int): Controls when quotes are recognized (one of the `csv.QUOTE_*` constants). - has_header (boolean): Specifies whether the CSV file has a header row. Returns: - If has_header is True, returns a list of dictionaries where each dictionary represents a row. - If has_header is False, returns a list of lists. result = [] try: with open(file_name, mode=\'r\', newline=\'\', encoding=\'utf-8\') as csvfile: if has_header: reader = csv.DictReader(csvfile, delimiter=delimiter, quotechar=quotechar, quoting=quoting) for row in reader: result.append(row) else: reader = csv.reader(csvfile, delimiter=delimiter, quotechar=quotechar, quoting=quoting) for row in reader: result.append(row) except FileNotFoundError: raise FileNotFoundError(f\\"File {file_name} not found.\\") except Exception as e: raise ValueError(f\\"Error parsing the file {file_name}: {str(e)}\\") return result def merge_csv(files_params, output_file, output_dialect=\'unix\'): Reads multiple CSV files, merges them into a single CSV file, and writes the result in a specified dialect. Parameters: - files_params (list of tuples): Each tuple contains the parameters for `parse_csv` function. - output_file (string): The name of the output CSV file. - output_dialect (string): The dialect to use for the output CSV file (default is \'unix\'). Output: - Creates a merged CSV file at the specified location. merged_data = [] fieldnames = set() # Read and merge data from all files for params in files_params: data = parse_csv(*params) if data: if isinstance(data[0], dict): fieldnames.update(data[0].keys()) merged_data.extend(data) fieldnames = list(fieldnames) with open(output_file, mode=\'w\', newline=\'\', encoding=\'utf-8\') as csvfile: writer = csv.DictWriter(csvfile, fieldnames=fieldnames, dialect=output_dialect) writer.writeheader() for row in merged_data: if isinstance(row, dict): writer.writerow(row) else: writer.writerow({fieldnames[i]: row[i] if i < len(row) else \'\' for i in range(len(fieldnames))})"},{"question":"**Question: Visualizing Complex Relationships with Seaborn** You are provided with a dataset containing information about the monthly sales of different products across various regions. Your task is to create a function `visualize_sales` that generates two plots using seaborn: a scatter plot and a line plot, displaying different aspects of the data. # Dataset The dataset is provided as a CSV file with the following columns: - `date`: The date (format: \\"YYYY-MM\\"). - `region`: The region where the sales were made. - `product`: The product name. - `sales`: The number of units sold. # Function Signature ```python def visualize_sales(file_path: str) -> None: Visualizes the sales data using seaborn. Parameters: - file_path: str - The path to the CSV file containing the sales data. Returns: - None pass ``` # Instructions 1. Read the dataset from the provided CSV file using pandas. 2. Create a scatter plot to visualize the relationship between the months (x-axis) and the total sales (y-axis), with points colored by region. 3. Create a line plot to show the trend of sales over the months for each product, with different line styles for each region. 4. Customize the plots by: - Using a cubehelix palette for the scatter plot. - Showing error bars representing the standard deviation in the line plot. - Adjusting the size of the scatter plot points based on the total sales. # Expected Output The function should display two plots: - A scatter plot with dates on the x-axis, total sales on the y-axis, points colored by region, and sized by total sales. - A line plot showing the trend of sales over the months for each product, with regions differentiated by line style and error bars showing standard deviation. # Example Usage ```python visualize_sales(\\"monthly_sales.csv\\") ``` # Constraints - You can assume the dataset has no missing values and is correctly formatted. - The function should not return any value; it should only display the plots. # Performance Considerations - Optimize the data aggregation step for visualization to handle large datasets efficiently. Good luck!","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np def visualize_sales(file_path: str) -> None: Visualizes the sales data using seaborn. Parameters: - file_path: str - The path to the CSV file containing the sales data. Returns: - None # Read the dataset from the provided CSV file df = pd.read_csv(file_path) # Convert the \'date\' column to datetime df[\'date\'] = pd.to_datetime(df[\'date\']) # Aggregate total sales by date and region for the scatter plot scatter_data = df.groupby([\'date\', \'region\'])[\'sales\'].sum().reset_index() # Create a scatter plot plt.figure(figsize=(12, 6)) sns.scatterplot( data=scatter_data, x=\'date\', y=\'sales\', hue=\'region\', size=\'sales\', sizes=(20, 200), palette=\'cubehelix\' ) plt.title(\'Total Sales by Month and Region\') plt.xlabel(\'Date\') plt.ylabel(\'Total Sales\') plt.legend(title=\'Region\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # Create a line plot line_data = df.groupby([\'date\', \'region\', \'product\'])[\'sales\'].agg([\'mean\', \'std\']).reset_index() plt.figure(figsize=(12, 6)) sns.lineplot( data=line_data, x=\'date\', y=\'mean\', hue=\'product\', style=\'region\', markers=True, err_style=\'bars\', ci=\'sd\' ) plt.title(\'Sales Trend by Month for Each Product\') plt.xlabel(\'Date\') plt.ylabel(\'Average Sales\') plt.legend(title=\'Product\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show()"},{"question":"**Coding Question: System Resource Management with Python\'s `resource` Module** # **Objective:** Implement a Python function called `manage_resources` that: 1. Retrieves and prints the current CPU time limit for the calling process. 2. Sets a new CPU time soft limit if it is below a specified threshold. 3. Retrieves and prints resource usage statistics of the calling process after performing a CPU-intensive task. # **Function Signature:** ```python def manage_resources(threshold: int) -> dict: pass ``` # **Inputs:** - `threshold` (int): An integer specifying the CPU time soft limit threshold in seconds. # **Expected Outputs:** - A dictionary with the resource usage statistics after performing a task. # **Detailed Requirements:** 1. **Retrieve and Display Current CPU Limits:** - Use `resource.getrlimit(resource.RLIMIT_CPU)` to fetch the current CPU time limits. - Print the current soft and hard CPU time limits. 2. **Set New CPU Time Soft Limit:** - If the current soft CPU time limit is greater than the `threshold`, update it to the `threshold`. Ensure the new soft limit does not exceed the hard limit. - Use `resource.setrlimit(resource.RLIMIT_CPU, (new_soft_limit, hard_limit))` to apply the new limit. - Exception Handling: Ensure to handle `ValueError` and `OSError` properly while setting the limits. 3. **Perform a CPU-Intensive Task and Retrieve Resource Usage:** - Simulate a CPU-intensive task by running a loop (for example, sum numbers up to a very large number). - Use `resource.getrusage(resource.RUSAGE_SELF)` to retrieve resource usage statistics after completing the task. - Return the resource usage statistics in the form of a dictionary with keys corresponding to the indices/fields mentioned in the documentation (e.g., \'ru_utime\', \'ru_stime\', etc.). # **Constraints:** - You may assume that the underlying operating system supports `resource` control functionalities specified. - The function should handle edge cases and exceptions gracefully. # **Example Usage:** ```python usage_stats = manage_resources(5) print(usage_stats) # Expected Output: # Current CPU time limits: (soft, hard) # Updated CPU time limits: (new_soft, hard) # If applicable # Resource usage after task: {\\"ru_utime\\": ..., \\"ru_stime\\": ..., ...} ``` # **Notes:** - Pay attention to exception handling when dealing with system calls. - Ensure efficient handling of tasks to not exceed the actual limits during testing.","solution":"import resource def manage_resources(threshold: int) -> dict: Manage CPU time limits and get resource usage statistics after a CPU-intensive task. Parameters: threshold (int): The CPU time soft limit threshold in seconds. Returns: dict: Resource usage statistics after performing a task. # Retrieve current CPU time limits soft_limit, hard_limit = resource.getrlimit(resource.RLIMIT_CPU) print(f\\"Current CPU time limits: (soft={soft_limit}, hard={hard_limit})\\") if soft_limit > threshold: try: # Set new soft limit new_soft_limit = threshold resource.setrlimit(resource.RLIMIT_CPU, (new_soft_limit, hard_limit)) print(f\\"Updated CPU time limits: (soft={new_soft_limit}, hard={hard_limit})\\") except (ValueError, OSError) as e: print(f\\"Failed to set new CPU time limit: {e}\\") # Perform a CPU-intensive task _ = sum(i for i in range(1000000)) # Retrieve resource usage statistics usage = resource.getrusage(resource.RUSAGE_SELF) usage_dict = { \'ru_utime\': usage.ru_utime, # user time used \'ru_stime\': usage.ru_stime, # system time used \'ru_maxrss\': usage.ru_maxrss, # maximum resident set size \'ru_ixrss\': usage.ru_ixrss, # shared memory size \'ru_idrss\': usage.ru_idrss, # unshared data size \'ru_isrss\': usage.ru_isrss, # unshared stack size \'ru_minflt\': usage.ru_minflt, # page reclaims (soft page faults) \'ru_majflt\': usage.ru_majflt, # page faults (hard page faults) \'ru_nswap\': usage.ru_nswap, # swaps \'ru_inblock\': usage.ru_inblock, # block input operations \'ru_oublock\': usage.ru_oublock, # block output operations \'ru_msgsnd\': usage.ru_msgsnd, # IPC messages sent \'ru_msgrcv\': usage.ru_msgrcv, # IPC messages received \'ru_nsignals\': usage.ru_nsignals, # signals received \'ru_nvcsw\': usage.ru_nvcsw, # voluntary context switches \'ru_nivcsw\': usage.ru_nivcsw # involuntary context switches } return usage_dict"},{"question":"Objective Your task is to implement and train a simple neural network model using the `torch.optim` library in PyTorch. You will demonstrate your understanding of constructing optimizers, setting per-parameter options, taking optimization steps, and using learning rate schedulers. Problem Statement 1. **Model Construction**: - Define a simple neural network model with one input layer, one hidden layer, and one output layer. - The input layer should have 10 neurons, the hidden layer should have 5 neurons, and the output layer should have 1 neuron. 2. **Optimizer Setup**: - Initialize two optimizers for the model: - An SGD optimizer (`optimizer1`) with a learning rate of 0.01 and momentum of 0.9 for parameters in the hidden layer. - An Adam optimizer (`optimizer2`) with a learning rate of 0.001 for parameters in the input and output layers. 3. **Training Loop**: - Create a synthetic dataset with 100 samples where each sample has 10 features. - Implement the training loop for 50 epochs: - In each epoch, randomly divide the synthetic dataset into batches of size 10. - For each batch: - Zero the gradients of both optimizers. - Perform a forward pass to compute the output. - Compute the loss using Mean Squared Error (MSE) loss function. - Perform backpropagation to calculate the gradients. - Take an optimization step using both optimizers. - Implement a learning rate scheduler that reduces the learning rate of the SGD optimizer by 0.1 every 10 epochs. 4. **Performance Evaluation**: - Evaluate the model on the synthetic dataset after training. - Return the final loss value. Function Signature ```python import torch import torch.nn as nn import torch.optim as optim class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.input_layer = nn.Linear(10, 5) self.hidden_layer = nn.Linear(5, 1) def forward(self, x): x = torch.relu(self.input_layer(x)) x = self.hidden_layer(x) return x def train_model(): # Initialize the model model = SimpleNN() # Initialize the optimizers optimizer1 = optim.SGD(model.hidden_layer.parameters(), lr=0.01, momentum=0.9) optimizer2 = optim.Adam([ {\'params\': model.input_layer.parameters()}, {\'params\': model.hidden_layer.parameters()} ], lr=0.001) # Initialize the learning rate scheduler scheduler = optim.lr_scheduler.StepLR(optimizer1, step_size=10, gamma=0.1) # Create synthetic dataset torch.manual_seed(0) data = torch.randn(100, 10) target = torch.randn(100, 1) # Training loop for epoch in range(50): for i in range(0, 100, 10): batch_data = data[i:i+10] batch_target = target[i:i+10] optimizer1.zero_grad() optimizer2.zero_grad() output = model(batch_data) loss = nn.MSELoss()(output, batch_target) loss.backward() optimizer1.step() optimizer2.step() scheduler.step() # Evaluate the model with torch.no_grad(): final_output = model(data) final_loss = nn.MSELoss()(final_output, target).item() return final_loss ``` Constraints - The model should be trained for exactly 50 epochs. - Use PyTorch\'s random number generator to create reproducible synthetic data. - The learning rate for the SGD optimizer should be reduced by 0.1 every 10 epochs. Expected Output The function `train_model` should return the final loss value after training the model on the synthetic dataset.","solution":"import torch import torch.nn as nn import torch.optim as optim class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.input_layer = nn.Linear(10, 5) self.hidden_layer = nn.Linear(5, 1) def forward(self, x): x = torch.relu(self.input_layer(x)) x = self.hidden_layer(x) return x def train_model(): # Initialize the model model = SimpleNN() # Initialize the optimizers optimizer1 = optim.SGD(model.hidden_layer.parameters(), lr=0.01, momentum=0.9) optimizer2 = optim.Adam(model.input_layer.parameters(), lr=0.001) # Initialize the learning rate scheduler scheduler = optim.lr_scheduler.StepLR(optimizer1, step_size=10, gamma=0.1) # Create synthetic dataset torch.manual_seed(0) data = torch.randn(100, 10) target = torch.randn(100, 1) # Training loop for epoch in range(50): for i in range(0, 100, 10): batch_data = data[i:i+10] batch_target = target[i:i+10] optimizer1.zero_grad() optimizer2.zero_grad() output = model(batch_data) loss = nn.MSELoss()(output, batch_target) loss.backward() optimizer1.step() optimizer2.step() scheduler.step() # Evaluate the model with torch.no_grad(): final_output = model(data) final_loss = nn.MSELoss()(final_output, target).item() return final_loss"},{"question":"# Advanced Coding Assessment: Optimizing Machine Learning Algorithms **Objective:** Assess the student\'s ability to implement and optimize a machine learning algorithm using scikit-learn, NumPy, Cython, and joblib. **Question:** You are required to implement an optimized version of the K-Means clustering algorithm. Your implementation should focus on efficient memory and CPU usage. Specifically, you must: 1. **Step 1: Implement a basic version of K-Means using NumPy.** - Function signature: `def kmeans_basic(X: np.ndarray, n_clusters: int, max_iter: int) -> (np.ndarray, np.ndarray):` - **Input:** - `X`: A 2D NumPy array of shape (n_samples, n_features) representing the dataset. - `n_clusters`: An integer representing the number of clusters. - `max_iter`: An integer representing the maximum number of iterations. - **Output:** - `centroids`: A 2D NumPy array of shape (n_clusters, n_features) representing the final cluster centers. - `labels`: A 1D NumPy array of shape (n_samples,) representing the cluster assignment for each sample. - **Constraints:** - The implementation should avoid nested loops by making use of NumPy vectorized operations as much as possible. 2. **Step 2: Profile your basic implementation to identify bottlenecks using `line_profiler` and `memory_profiler`.** - Provide a summary of the profiling results highlighting the main bottlenecks (both CPU and memory usage). 3. **Step 3: Implement an optimized version of the K-Means algorithm using Cython.** - Name your Cython function `kmeans_cython`. - Ensure you add proper static type declarations to improve performance further. - Function signature: `def kmeans_cython(np.ndarray[float, ndim=2] X, int n_clusters, int max_iter) -> (np.ndarray, np.ndarray):` - **Constraints:** - The optimized version must show a performance improvement over the basic version based on profiling results. 4. **Step 4: Use `joblib.Parallel` to parallelize the initialization step of K-Means (i.e., the random selection of initial centroids) across multiple CPU cores.** - Function signature: `def kmeans_parallel(X: np.ndarray, n_clusters: int, max_iter: int, n_jobs: int=-1) -> (np.ndarray, np.ndarray):` - **Input:** - Same as the basic version with an additional parameter `n_jobs`: An integer representing the number of CPU cores to use (default: -1, which uses all available cores). - **Output:** - Same as the basic version. 5. **Step 5: Evaluate your optimized K-Means implementation on a provided dataset and report the total execution time and memory usage. Use the digits dataset from scikit-learn for this purpose.** - Dataset: Use `from sklearn.datasets import load_digits` to load the dataset. - Ensure to compare the execution time of all three implementations (basic, Cython optimized, and parallel). **Constraints and Performance Requirements:** - Your optimized implementation (Cython and parallel) should significantly reduce both execution time and memory usage when compared to the basic version. - **Input size:** Assume the dataset size can be up to 20,000 samples and 64 features. **Submission:** 1. Python and Cython code files for all implementations. 2. Profiling reports in text or PDF format. 3. A written summary of the profiling results and optimization steps taken. 4. A script to evaluate the implementations with the digits dataset. # Example Usage: ```python from sklearn.datasets import load_digits import numpy as np X, _ = load_digits(return_X_y=True) n_clusters = 10 max_iter = 300 # Basic version centroids_basic, labels_basic = kmeans_basic(X, n_clusters, max_iter) # Cython optimized version centroids_cython, labels_cython = kmeans_cython(X, n_clusters, max_iter) # Parallel initialization version centroids_parallel, labels_parallel = kmeans_parallel(X, n_clusters, max_iter) ``` **Note:** Ensure you have installed the necessary packages (`numpy`, `scipy`, `cython`, `joblib`, `line_profiler`, `memory_profiler`) before starting the implementation. Good luck!","solution":"import numpy as np def kmeans_basic(X: np.ndarray, n_clusters: int, max_iter: int) -> (np.ndarray, np.ndarray): Basic implementation of K-Means clustering algorithm. Parameters ---------- X : np.ndarray A 2D NumPy array of shape (n_samples, n_features) representing the dataset. n_clusters : int Number of clusters. max_iter : int Maximum number of iterations. Returns ------- centroids : np.ndarray A 2D NumPy array of shape (n_clusters, n_features) representing the final cluster centers. labels : np.ndarray A 1D NumPy array of shape (n_samples,) representing the cluster assignment for each sample. np.random.seed(42) n_samples, n_features = X.shape # Randomly initialize the centroids by sampling from the dataset centroids = X[np.random.choice(n_samples, n_clusters, replace=False)] for _ in range(max_iter): # Compute the distance between each sample and each centroid distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2) # Assign each sample to the nearest centroid labels = np.argmin(distances, axis=1) # Compute new centroids as the means of the samples in each cluster new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(n_clusters)]) # If the centroids do not change, stop the algorithm if np.all(centroids == new_centroids): break centroids = new_centroids return centroids, labels"},{"question":"Objective Demonstrate your ability to manipulate plot styles and visualize data using seaborn. You will be required to set different styles for each plot and customize the appearance of grid elements. Problem Statement Write a function `create_custom_plots` that takes no parameters and performs the following tasks: 1. Creates a bar plot with the style set to \\"whitegrid\\" and uses the following data: - Categories: [\\"A\\", \\"B\\", \\"C\\"] - Values: [4, 7, 3] 2. Creates a line plot with a custom style \\"darkgrid\\" where: - Grid lines have a color of \\"red\\" and linestyle of \\"--\\". - Uses the following data: - Categories: [\\"X\\", \\"Y\\", \\"Z\\"] - Values: [5, 6, 2] 3. Both plots should be displayed in the same figure, side by side. Constraints - Ensure that both plots are displayed in a single figure using subplots. - Use seaborn to set the styles and create the plots. - Do not use any other plotting libraries (e.g., Matplotlib directly) for setting styles or creating plots. Expected Output The function should produce a figure with two subplots: a bar plot with \\"whitegrid\\" style on the left and a line plot with a customized \\"darkgrid\\" style on the right. Function Signature ```python def create_custom_plots(): pass ``` Example Usage When `create_custom_plots()` is called, it should display the plots in the specified styles and layout.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plots(): Creates two subplots with customized styles and displays them side by side in the same figure. Subplot 1: A bar plot with the \'whitegrid\' style using given data. Subplot 2: A line plot with a customized \'darkgrid\' style using given data and custom gridline. # Bar plot data categories_bar = [\\"A\\", \\"B\\", \\"C\\"] values_bar = [4, 7, 3] # Line plot data categories_line = [\\"X\\", \\"Y\\", \\"Z\\"] values_line = [5, 6, 2] # Create subplots fig, axes = plt.subplots(1, 2, figsize=(12, 6)) # Bar plot with \'whitegrid\' style sns.set_style(\\"whitegrid\\") sns.barplot(x=categories_bar, y=values_bar, ax=axes[0]) axes[0].set_title(\'Bar Plot (whitegrid style)\') # Line plot with customized \'darkgrid\' style sns.set_style(\\"darkgrid\\", {\\"grid.color\\": \\"red\\", \\"grid.linestyle\\": \\"--\\"}) sns.lineplot(x=categories_line, y=values_line, marker=\'o\', ax=axes[1]) axes[1].set_title(\'Line Plot (custom darkgrid style)\') # Display plots plt.tight_layout() plt.show()"},{"question":"**Title:** Implement a Custom Probability Distribution using PyTorch Special Functions **Objective:** The goal of this task is to implement a custom probability density function (PDF) and its cumulative distribution function (CDF) using PyTorch\'s special functions. This will test your ability to use `torch.special` effectively in a practical scenario. **Problem Statement:** You are required to implement a custom class `CustomDistribution`, which represents a probability distribution defined by a specific formula. The class should include methods to compute the probability density function (PDF) and the cumulative distribution function (CDF). Custom Distribution: The PDF of the custom distribution is defined as: [ f(x) = begin{cases} frac{1}{2sqrt{pi}}e^{-x^2}, & x ge 0 frac{1}{2sqrt{pi}}e^{-x^2} cdot frac{1}{1 + x^2}, & x < 0 end{cases} ] The CDF can be integrated numerically from the PDF. For simplicity, you may use existing PyTorch special functions to aid in the numerical integration. **Requirements:** 1. **Probability Density Function (PDF):** - Implement a method `pdf(self, x: torch.Tensor) -> torch.Tensor` which computes the PDF values for a given input tensor `x`. 2. **Cumulative Distribution Function (CDF):** - Implement a method `cdf(self, x: torch.Tensor) -> torch.Tensor` which computes the CDF values for a given input tensor `x`. - Use numerical integration techniques and PyTorch special functions to ensure accuracy. **Input:** - A one-dimensional PyTorch tensor `x` containing float values. **Output:** - The `pdf` method returns a tensor of the same shape as `x`, containing the PDF values. - The `cdf` method returns a tensor of the same shape as `x`, containing the CDF values. **Constraints:** - Ensure that your implementations handle edge cases such as large positive or negative values of `x`. - Make use of special functions from the `torch.special` module as necessary. - Avoid using explicit loops; leverage PyTorch\'s vectorized operations for efficiency. **Examples:** ```python import torch from torch.special import erf, erfc, expm1 class CustomDistribution: def pdf(self, x: torch.Tensor) -> torch.Tensor: Computes the PDF of the custom distribution for a given input tensor x. positive_part = (1 / (2 * torch.sqrt(torch.tensor(torch.pi)))) * torch.exp(-x**2) negative_part = positive_part / (1 + x**2) return torch.where(x >= 0, positive_part, negative_part) def cdf(self, x: torch.Tensor) -> torch.Tensor: Computes the CDF of the custom distribution for a given input tensor x by integrating the PDF numerically using PyTorch special functions. # This is a placeholder implementation; you should replace it with the actual implementation. cdf_vals = torch.zeros_like(x) for i, val in enumerate(x): cdf_vals[i] = torch.trapz(self.pdf(torch.linspace(-10, val, 1000)), torch.linspace(-10, val, 1000)) return cdf_vals # Example Usage dist = CustomDistribution() x = torch.tensor([-2.0, 0.0, 2.0]) print(dist.pdf(x)) # Expected output: Tensor of PDF values print(dist.cdf(x)) # Expected output: Tensor of CDF values ``` **Notes:** - Ensure numerical stability in your calculations. - The provided example usage demonstrates how to instantiate the class and compute the PDF and CDF for specific values.","solution":"import torch from torch.special import erf class CustomDistribution: def pdf(self, x: torch.Tensor) -> torch.Tensor: Computes the PDF of the custom distribution for a given input tensor x. positive_part = (1 / (2 * torch.sqrt(torch.tensor(torch.pi)))) * torch.exp(-x**2) negative_part = positive_part / (1 + x**2) return torch.where(x >= 0, positive_part, negative_part) def cdf(self, x: torch.Tensor) -> torch.Tensor: Computes the CDF of the custom distribution for a given input tensor x. positive_part = 0.5 * (1 + erf(x)) negative_part = 0.5 * (1 + erf(x)) - 0.5 * torch.exp(-x**2) / torch.sqrt(torch.tensor(torch.pi)) * (torch.atan(x) + torch.pi / 2) return torch.where(x >= 0, positive_part, negative_part) # Example Usage dist = CustomDistribution() x = torch.tensor([-2.0, 0.0, 2.0]) print(dist.pdf(x)) # Example output: Tensor of PDF values print(dist.cdf(x)) # Example output: Tensor of CDF values"},{"question":"**Advanced Coding Assessment Question:** # Mocking Web Requests with `unittest.mock` You are required to implement a function `fetch_user_data(user_id: int) -> dict` that fetches user data from a web API and returns it as a dictionary. Additionally, write a unit test to verify the behavior of this function using the `unittest.mock` package. # Function Specification The function `fetch_user_data` makes an HTTP GET request to `https://api.example.com/users/{user_id}` where `{user_id}` is the user ID passed to the function. The function should return the user data as a dictionary. # Example Usage ```python def fetch_user_data(user_id: int) -> dict: response = requests.get(f\\"https://api.example.com/users/{user_id}\\") return response.json() # Example Call user_data = fetch_user_data(1) print(user_data) ``` # Unit Test Specification Using the `unittest.mock` library, write a unit test for `fetch_user_data` that mocks the web request and checks: 1. The function makes a request to the correct URL. 2. The function correctly processes the response and returns the expected data. # Constraints - You may assume `requests` module is available and imported. - Construct the expected behavior using `Mock` and `patch`. # Input - An integer representing the `user_id`. # Output - A dictionary containing user data. # Example Unit Test ```python import unittest from unittest.mock import Mock, patch import requests class TestFetchUserData(unittest.TestCase): @patch(\'requests.get\') def test_fetch_user_data(self, mock_get): # Create a mock response object with the desired json method return value mock_response = Mock() expected_data = {\'id\': 1, \'name\': \'John Doe\', \'email\': \'john.doe@example.com\'} mock_response.json.return_value = expected_data mock_get.return_value = mock_response # Call the function result = fetch_user_data(1) # Assert that the function makes a request to the correct URL mock_get.assert_called_with(\\"https://api.example.com/users/1\\") # Assert that the function returns the expected data self.assertEqual(result, expected_data) if __name__ == \'__main__\': unittest.main() ``` # Notes - Writing assertions that ensure the mock was called correctly is key. - Using `patch()` to intercept the `requests.get` calls will isolate your function from actual network calls during testing.","solution":"import requests def fetch_user_data(user_id: int) -> dict: Fetches user data from the web API and returns it as a dictionary. Parameters: user_id (int): The ID of the user to fetch the data for Returns: dict: The user data retrieved from the API response = requests.get(f\\"https://api.example.com/users/{user_id}\\") return response.json()"},{"question":"Background: In this exercise, you are required to create a Python script that utilizes the `stat` module to process and interpret file metadata. You will be designing a function that traverses a given directory and outputs information about each file in a specific format. Additionally, you will be implementing functionality to filter files based on certain criteria. Objective: Write a Python function called `get_file_info` that takes the following input parameters: - `directory` (str): The path to the directory to be traversed. - `file_type` (str): The type of files to be filtered. This can be one of the following: `\'dir\'`, `\'file\'`, `\'link\'`, `\'char\'`, `\'block\'`, `\'fifo\'`, `\'sock\'`. If `file_type` is `None`, all file types should be included. - `min_size` (int): The minimum file size in bytes. Files smaller than this size should be excluded from the result. If `min_size` is `None`, no filtering based on size should occur. The function should return a list of dictionaries where each dictionary contains the following information for each file that meets the criteria: - `path`: The full path to the file. - `type`: The type of the file (`\'dir\'`, `\'file\'`, etc.). - `size`: The size of the file in bytes. - `permissions`: A human-readable string of the file\'s permissions (same format as `\'-rwxrwxrwx\'`). Constraints: - You cannot use the `os.path.is*` functions; you must use functions from the `stat` module to determine the file types. - Ensure your solution handles symbolic links appropriately. - Your solution should be efficient and should not perform redundant `os.stat()` calls. Example: ```python import os import stat def get_file_info(directory, file_type=None, min_size=None): # Implement your solution here return [] # Example usage: result = get_file_info(\'/path/to/directory\', file_type=\'file\', min_size=1024) print(result) ``` *Expected output:* For a directory structure like: ``` /path/to/directory: file1.txt (2 KB, \'-rw-r--r--\') file2.log (500 bytes, \'-rwx------\') subdir (directory) link (symbolic link to file1.txt) ``` If filtering for regular files larger than 1 KB: ``` [ { \'path\': \'/path/to/directory/file1.txt\', \'type\': \'file\', \'size\': 2048, \'permissions\': \'-rw-r--r--\' } ] ``` Evaluation Criteria: - Correctness: The function should return the correct information based on the given filters. - Efficiency: The function should avoid redundant `os.stat()` calls. - Robustness: The function should handle all typical edge cases, including symbolic links and nested directories. - Clarity: The code should be well-organized and easy to read.","solution":"import os import stat def get_file_info(directory, file_type=None, min_size=None): Traverses the given directory and filters files based on type and size. Parameters: directory (str): Path to the directory to be traversed. file_type (str): The type of files to be filtered. Acceptable values are \'dir\', \'file\', \'link\', \'char\', \'block\', \'fifo\', \'sock\'. If None, all file types will be included. min_size (int): Minimum file size in bytes. Files smaller than this size will be excluded. If None, no filtering based on size will occur. Returns: List[dict]: A list of dictionaries where each dictionary contains the file information. type_flags = { \'dir\': stat.S_IFDIR, \'file\': stat.S_IFREG, \'link\': stat.S_IFLNK, \'char\': stat.S_IFCHR, \'block\': stat.S_IFBLK, \'fifo\': stat.S_IFIFO, \'sock\': stat.S_IFSOCK } result = [] for root, dirs, files in os.walk(directory): for name in files + dirs: path = os.path.join(root, name) try: st = os.lstat(path) except FileNotFoundError: continue st_mode = st.st_mode # Determine the type of the file if stat.S_ISDIR(st_mode): ftype = \'dir\' elif stat.S_ISREG(st_mode): ftype = \'file\' elif stat.S_ISLNK(st_mode): ftype = \'link\' elif stat.S_ISCHR(st_mode): ftype = \'char\' elif stat.S_ISBLK(st_mode): ftype = \'block\' elif stat.S_ISFIFO(st_mode): ftype = \'fifo\' elif stat.S_ISSOCK(st_mode): ftype = \'sock\' else: continue # Filter by file_type if necessary if file_type and ftype != file_type: continue # Filter by file size if necessary if min_size and st.st_size < min_size: continue permissions = stat.filemode(st_mode) result.append({ \'path\': path, \'type\': ftype, \'size\': st.st_size, \'permissions\': permissions }) return result"},{"question":"**Advanced Python File Handling and String Manipulation** **Objective:** Implement a function `file_handling_with_apis()` in Python that demonstrates various file handling operations using Python\'s \\"io\\" module and simulates the workings of the described C API functions provided in the documentation. **Function Signature:** ```python def file_handling_with_apis(): pass ``` **Function Requirements:** 1. Open an existing file `example.txt` (which you will create if it does not exist) for reading and writing using a file descriptor. 2. Write a given string `\\"Hello, world!\\"` to the file. 3. Write the `repr()` of a given Python dictionary object `{\\"name\\": \\"Alice\\", \\"age\\": 30}` to the file. 4. Read back the content of the file line by line. 5. Retrieve and print the file descriptor of the file. 6. Handle and demonstrate proper usage of file handling exceptions. **Details and Constraints:** - You must use the \\"io\\" module for file operations. - Ensure that the created file `example.txt` contains the following lines after execution: - The written string. - The string representation of the Python dictionary object. - The content should be read and printed line by line. - Implement error checking for file operations; if an error occurs, handle it gracefully and print an appropriate error message. - Assume the file is in the current working directory. **Example Output:** ``` File Descriptor: <some_integer_value> Line read: Hello, world! Line read: {\'name\': \'Alice\', \'age\': 30} ``` **Implementation Hints:** - Use `open()` from the \\"io\\" module to open the file and get the file descriptor. - Use `write()` to write strings to the file. - Use `fileno()` to get the file descriptor. - Use `readline()` to read the file content line by line. - Handle exceptions using Python\'s try-except blocks. **Sample File (`example.txt`) Content After Execution:** ``` Hello, world! {\'name\': \'Alice\', \'age\': 30} ``` You must ensure all operations are performed correctly, and the file content matches the expected output. This exercise tests your understanding of Python\'s file I/O operations and exception handling.","solution":"import io def file_handling_with_apis(): filename = \'example.txt\' try: # Open file for reading and writing, create if it does not exist with io.open(filename, \'w+\', encoding=\'utf-8\') as file: fd = file.fileno() print(f\\"File Descriptor: {fd}\\") # Write given string to the file file.write(\\"Hello, world!n\\") # Write the repr of the given Python dictionary to the file my_dict = {\\"name\\": \\"Alice\\", \\"age\\": 30} file.write(repr(my_dict) + \'n\') # Move file pointer to the beginning to read the content file.seek(0) # Read and print each line for line in file: print(f\\"Line read: {line.strip()}\\") except IOError as e: print(f\\"File operation failed: {e}\\")"}]'),D={name:"App",components:{PoemCard:I},data(){return{searchQuery:"",visibleCount:4,poemsData:A,isLoading:!1}},computed:{filteredPoems(){const n=this.searchQuery.trim().toLowerCase();return n?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(n)||e.solution&&e.solution.toLowerCase().includes(n)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(n=>setTimeout(n,1e3)),this.visibleCount+=4,this.isLoading=!1}}},z={class:"search-container"},F={class:"card-container"},R={key:0,class:"empty-state"},q=["disabled"],N={key:0},j={key:1};function O(n,e,l,m,i,o){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",z,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>i.searchQuery=r),placeholder:"Search..."},null,512),[[y,i.searchQuery]]),i.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>i.searchQuery="")}," ✕ ")):d("",!0)]),t("div",F,[(a(!0),s(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),s("div",R,' No results found for "'+c(i.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:i.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[i.isLoading?(a(),s("span",j,"Loading...")):(a(),s("span",N,"See more"))],8,q)):d("",!0)])}const L=p(D,[["render",O],["__scopeId","data-v-e7e922ab"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"library/60.md","filePath":"library/60.md"}'),M={name:"library/60.md"},H=Object.assign(M,{setup(n){return(e,l)=>(a(),s("div",null,[x(L)]))}});export{Y as __pageData,H as default};
