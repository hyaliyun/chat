import{_ as p,o as a,c as s,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},P={class:"review-title"},E={class:"review-content"};function S(n,e,l,m,i,o){return a(),s("div",T,[t("div",C,[t("div",P,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",E,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-395787d8"]]),I=JSON.parse('[{"question":"**Title**: Advanced Text Processing with Python **Objective**: Demonstrate your understanding of various text processing modules in Python by implementing a function that utilizes multiple modules to achieve specific text manipulation tasks. **Problem Statement**: You need to write a function `process_text(input_text: str) -> dict` that processes a string `input_text` and returns a dictionary containing the following information: 1. **words_list**: A list of all unique words from the input text, sorted alphabetically. 2. **word_frequencies**: A dictionary where the keys are words and the values are their respective frequencies in the input text. 3. **top_5_words**: A list of the top 5 most frequent words in the input text. 4. **wrapped_text**: The text wrapped to a width of 50 characters per line. 5. **word_differences**: A list of words that are present in the input text but not in a given comparison text. You should use the following modules: `string`, `re`, `difflib`, and `textwrap` to implement the required functionality. **Function Signature**: ```python def process_text(input_text: str, comparison_text: str) -> dict: pass ``` **Input**: - `input_text` (str): A string representing the text to be processed. - `comparison_text` (str): A string representing the text to be compared against for word differences. **Output**: - A dictionary with the following structure: ```python { \\"words_list\\": List[str], \\"word_frequencies\\": Dict[str, int], \\"top_5_words\\": List[str], \\"wrapped_text\\": str, \\"word_differences\\": List[str] } ``` **Example**: ```python input_text = \\"Hello world! This is a test. Hello again.\\" comparison_text = \\"This is a comparison text. Hello universe!\\" result = process_text(input_text, comparison_text) # Example output: { \\"words_list\\": [\'a\', \'again\', \'hello\', \'is\', \'test\', \'this\', \'world\'], \\"word_frequencies\\": {\'hello\': 2, \'world\': 1, \'this\': 1, \'is\': 1, \'a\': 1, \'test\': 1, \'again\': 1}, \\"top_5_words\\": [\'hello\', \'world\', \'this\', \'is\', \'a\'], \\"wrapped_text\\": \'Hello world! This is a test. Hello again.\', \\"word_differences\\": [\'again\', \'test\', \'world\'] } ``` **Constraints**: - You may assume the input texts are non-empty and contain only standard punctuation and whitespace. - Words should be considered case-insensitive (i.e., \\"Hello\\" and \\"hello\\" should be treated as the same word). **Guidance**: 1. Use the `string` module to handle punctuation and whitespace. 2. Use the `re` module to split the text into words. 3. Use the `difflib` module to find the differences in words between `input_text` and `comparison_text`. 4. Use the `textwrap` module to wrap the text to the required width. **Performance Requirements**: - The function should efficiently handle text inputs with up to 10,000 words.","solution":"import string import re import difflib import textwrap from collections import Counter def process_text(input_text: str, comparison_text: str) -> dict: # Normalize the text to lower case input_text = input_text.lower() comparison_text = comparison_text.lower() # Remove punctuation and split into words translator = str.maketrans(\'\', \'\', string.punctuation) words = re.findall(r\'bw+b\', input_text.translate(translator)) comparison_words = re.findall(r\'bw+b\', comparison_text.translate(translator)) # Get unique words list sorted unique_words = sorted(set(words)) # Frequency of each word word_frequencies = dict(Counter(words)) # Top 5 frequent words top_5_words = [word for word, freq in Counter(words).most_common(5)] # Wrap the text to 50 characters per line wrapped_text = textwrap.fill(input_text, width=50) # Words present in input_text but not in comparison_text word_differences = sorted(set(words) - set(comparison_words)) return { \\"words_list\\": unique_words, \\"word_frequencies\\": word_frequencies, \\"top_5_words\\": top_5_words, \\"wrapped_text\\": wrapped_text, \\"word_differences\\": word_differences }"},{"question":"# Seaborn Coding Assessment Objective To assess your understanding of seaborn, you are required to write a function that performs a series of data visualizations on a given dataset. The goal is to explore the dataset using different seaborn plot types and customize the plots for better readability and presentation. Problem Statement Create a function named `visualize_data` that takes a pandas DataFrame as input and outputs a series of visualizations using seaborn. The function should: 1. **Relational Plot**: Create a scatter plot to visualize the relationship between two numerical columns, with different colors and styles based on different categories. 2. **Distribution Plot**: Generate a histogram and kernel density estimate (KDE) plot of a specified numerical column, distinguished by categories. 3. **Categorical Plot**: Draw a bar plot to show the average value of a numerical column for different levels of a categorical column. 4. **Joint Plot**: Create a joint plot displaying the relationship between two numerical columns along with their marginal distributions. 5. Customize each plot with appropriate titles, axis labels, and legends where applicable. Input - `df`: A pandas DataFrame containing the following columns: - At least three numerical columns (e.g., `num_col1`, `num_col2`, `num_col3`) - At least two categorical columns (e.g., `cat_col1`, `cat_col2`) Output The function should produce the following plots: 1. A scatter plot using `seaborn.relplot` with: - x-axis: `num_col1` - y-axis: `num_col2` - Color (`hue`): `cat_col1` - Style (`style`): `cat_col2` 2. A distribution plot using `seaborn.displot` with: - x-axis: `num_col3` - Color (`hue`): `cat_col1` - Include both histogram and KDE 3. A bar plot using `seaborn.catplot` with: - x-axis: `cat_col1` - y-axis: Mean of `num_col1` - Color (`hue`): `cat_col2` 4. A joint plot using `seaborn.jointplot` with: - x-axis: `num_col1` - y-axis: `num_col2` - Color (`hue`): `cat_col1` Constraints - The DataFrame `df` should not be empty, and it should contain the specified columns. - The function should handle any potential errors gracefully, including missing or mismatched column names. Example Usage ```python import pandas as pd import seaborn as sns # Load the tips dataset as an example df = sns.load_dataset(\\"tips\\") def visualize_data(df): # Scatter plot sns.relplot( data=df, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", style=\\"time\\" ).set(title=\\"Relationship between Total Bill and Tip\\") # Distribution plot sns.displot( data=df, x=\\"total_bill\\", hue=\\"time\\", kde=True ).set(title=\\"Distribution of Total Bill\\") # Bar plot sns.catplot( data=df, kind=\\"bar\\", x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\" ).set(title=\\"Average Total Bill per Day\\") # Joint plot sns.jointplot( data=df, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\" ).set_axis_labels(\\"Total Bill\\", \\"Tip\\") # Call the function visualize_data(df) ``` Your implementation should follow the example format shown above, but using the columns `num_col1`, `num_col2`, `num_col3`, `cat_col1`, and `cat_col2` from a DataFrame provided as the input to your function.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_data(df): Generates a series of visualizations to explore the dataset using seaborn. Parameters: df (pd.DataFrame): The input data frame containing at least three numerical columns and two categorical columns. if df.empty: raise ValueError(\\"The DataFrame should not be empty\\") required_numeric_cols = [\'num_col1\', \'num_col2\', \'num_col3\'] required_cat_cols = [\'cat_col1\', \'cat_col2\'] for col in required_numeric_cols + required_cat_cols: if col not in df.columns: raise ValueError(f\\"Missing required column: {col}\\") # Scatter plot scatter_plot = sns.relplot( data=df, x=\'num_col1\', y=\'num_col2\', hue=\'cat_col1\', style=\'cat_col2\' ) scatter_plot.fig.suptitle(\'Relationship between Num_Col1 and Num_Col2\') scatter_plot.set_axis_labels(\'Num_Col1\', \'Num_Col2\') # Distribution plot dist_plot = sns.displot( data=df, x=\'num_col3\', hue=\'cat_col1\', kde=True ) dist_plot.fig.suptitle(\'Distribution of Num_Col3\') dist_plot.set_axis_labels(\'Num_Col3\', \'Density\') # Bar plot bar_plot = sns.catplot( data=df, kind=\'bar\', x=\'cat_col1\', y=\'num_col1\', hue=\'cat_col2\' ) bar_plot.fig.suptitle(\'Average Num_Col1 per Cat_Col1\') bar_plot.set_axis_labels(\'Cat_Col1\', \'Average Num_Col1\') # Joint plot joint_plot = sns.jointplot( data=df, x=\'num_col1\', y=\'num_col2\', hue=\'cat_col1\' ) joint_plot.fig.suptitle(\'Joint Distribution of Num_Col1 and Num_Col2\') joint_plot.set_axis_labels(\'Num_Col1\', \'Num_Col2\') plt.show()"},{"question":"# Command-Line Shopping Cart Design a command-line shopping cart application using the `cmd` module. Objective Create a command-line interface for managing a shopping cart. Users should be able to add items, remove items, view the cart, and check out. The application should support saving and loading the cart to and from a file. Instructions 1. **Create a subclass of `cmd.Cmd` named `ShoppingCartCmd`.** 2. **Implement the following commands:** - `add <item> <price> <quantity>`: Add an item to the shopping cart. If the item already exists, update its quantity. Example: `add apple 1.50 4` - `remove <item>`: Remove an item from the shopping cart. Example: `remove apple` - `view`: Display all items in the shopping cart along with their total price. - `clear`: Clear all items from the shopping cart. - `checkout`: Display the total amount due, save the cart to a file named `checkout.txt`, and exit the application. - `load <filename>`: Load a shopping cart from a file. - `save <filename>`: Save the current shopping cart to a file. Implementation Details - **Input and Output Formats:** - For commands other than `view` and `checkout`, output a confirmation or error message. - When viewing the cart, display each item with its price and quantity, and show the total price at the end. - For `checkout`, save the cart contents to `checkout.txt` and include the total price. - **Constraints:** - Prices are non-negative floats. - Quantities are non-negative integers. - Ensure the command format and arguments are correct; display an error message for incorrect inputs. - **Performance Requirements:** - The application should handle dynamic and arbitrary input sizes efficiently. - Read and write operations should properly handle file I/O exceptions. Example Session ``` Welcome to the shopping cart application. Type help or ? to list commands. (cart) add apple 1.50 4 Added 4 apple(s) at 1.50 each. (cart) add banana 0.75 6 Added 6 banana(s) at 0.75 each. (cart) view Item: apple, Price: 1.50, Quantity: 4 Item: banana, Price: 0.75, Quantity: 6 Total Price: 9.00 (cart) remove apple Removed apple from the cart. (cart) view Item: banana, Price: 0.75, Quantity: 6 Total Price: 4.50 (cart) clear Cart is now empty. (cart) checkout Total Amount Due: 0.00 Saved cart to \'checkout.txt\'. Exiting application. ``` Fork the script implementation, ensuring your `ShoppingCartCmd` class combines the above requirements. Your output should follow the expected formats accurately. Hints - Use a dictionary to manage the items in the cart. The key can be the item name, and the value can be a tuple of price and quantity. - Utilize the built-in `cmd` methods like `do_`, `precmd()`, and `postcmd()` to implement command functionalities and handle custom behaviors. - Implement file operations using standard Python file I/O.","solution":"import cmd import os import json class ShoppingCartCmd(cmd.Cmd): intro = \'Welcome to the shopping cart application. Type help or ? to list commands.n\' prompt = \'(cart) \' def __init__(self): super().__init__() self.cart = {} def do_add(self, arg): \'Add an item to the cart: add <item> <price> <quantity>\' try: item, price, quantity = arg.split() price = float(price) quantity = int(quantity) if item in self.cart: self.cart[item][1] += quantity print(f\\"Updated {item} quantity to {self.cart[item][1]}\\") else: self.cart[item] = [price, quantity] print(f\\"Added {quantity} {item}(s) at {price} each.\\") except ValueError: print(\\"Invalid arguments. Usage: add <item> <price> <quantity>\\") def do_remove(self, arg): \'Remove an item from the cart: remove <item>\' if arg in self.cart: del self.cart[arg] print(f\\"Removed {arg} from the cart.\\") else: print(f\\"{arg} is not in the cart.\\") def do_view(self, arg): \'View the items in the cart\' if not self.cart: print(\\"Cart is empty.\\") else: total = 0.0 for item, (price, quantity) in self.cart.items(): print(f\\"Item: {item}, Price: {price:.2f}, Quantity: {quantity}\\") total += price * quantity print(f\\"Total Price: {total:.2f}\\") def do_clear(self, arg): \'Clear all items from the cart\' self.cart.clear() print(\\"Cart is now empty.\\") def do_checkout(self, arg): \'Display the total amount due, save the cart to checkout.txt, and exit the application\' total = sum(price * quantity for price, quantity in self.cart.values()) with open(\'checkout.txt\', \'w\') as f: for item, (price, quantity) in self.cart.items(): f.write(f\\"Item: {item}, Price: {price:.2f}, Quantity: {quantity}n\\") f.write(f\\"Total Price: {total:.2f}n\\") print(f\\"Total Amount Due: {total:.2f}\\") print(\\"Saved cart to \'checkout.txt\'. Exiting application.\\") return True def do_load(self, arg): \'Load a shopping cart from a file: load <filename>\' if not os.path.exists(arg): print(f\\"File {arg} does not exist.\\") return with open(arg, \'r\') as f: self.cart = json.load(f) print(f\\"Loaded cart from {arg}.\\") def do_save(self, arg): \'Save the current shopping cart to a file: save <filename>\' if not arg: print(\\"Please specify a filename.\\") return with open(arg, \'w\') as f: json.dump(self.cart, f) print(f\\"Saved cart to {arg}.\\") def do_exit(self, arg): \'Exit the shopping cart application\' print(\\"Exiting application.\\") return True"},{"question":"<|Analysis Begin|> The provided documentation outlines various parts of the Python language, beginning with an introduction and moving through more complex topics such as control flow tools, data structures, modules, input and output, and classes. Focusing on what students need to know, it would be beneficial to create a question that not only tests their knowledge of basic Python syntax and data structures but also their ability to use modules, manipulate data, and handle errors. Given the breadth of the documentation, a suitable coding assessment question could involve: - File reading and writing, - Data manipulation using several Python data structures (such as dictionaries and lists), - Error handling, - Function and class definitions, - Module usage, possibly importing standard modules such as `json`. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective: Create a Python program that reads a structured text file containing student records, processes the data, and outputs summary statistics to a new text file. This task will test your ability to handle file I/O, data manipulation using lists and dictionaries, error handling, and module usage. Problem Statement: You are provided with a text file containing student information. Each line in the file contains information about a student in the following format: ``` student_id, name, grade ``` For example: ``` 1, John Doe, 85 2, Jane Smith, 78 ... ``` Your task is to: 1. Write a function `read_student_records(file_path)` that reads the student information from the given file and returns a list of dictionaries. Each dictionary should represent a student record with keys: \'student_id\', \'name\', and \'grade\'. 2. Write a function `grade_statistics(student_records)` that takes a list of student records and returns a dictionary with the following keys: * `\'average_grade\'`: The average grade of all students. * `\'highest_grade\'`: The highest grade among all students. * `\'lowest_grade\'`: The lowest grade among all students. 3. Write a function `write_statistics_to_file(statistics, output_file_path)` that writes the statistics dictionary to a new file in a readable format. Constraints: - Grades are integers between 0 and 100. - The input file will have at least one student record. - Proper error handling must be included to cover file reading/writing errors. Input and Output Format: 1. **Function:** `read_student_records(file_path)` * **Input:** `file_path` (str) - Path to the input text file. * **Output:** List of dictionaries, each dictionary representing a student. 2. **Function:** `grade_statistics(student_records)` * **Input:** `student_records` (list) - List of dictionaries representing the students. * **Output:** Dictionary with keys `average_grade`, `highest_grade`, and `lowest_grade`. 3. **Function:** `write_statistics_to_file(statistics, output_file_path)` * **Input:** - `statistics` (dict) - Dictionary containing the statistical summaries. - `output_file_path` (str) - Path to the output text file. * **Output:** None (writes to file). Example: Suppose the input text file `students.txt` contains: ``` 1, John Doe, 85 2, Jane Smith, 78 3, Alice Jones, 92 4, Bob Stone, 70 ``` Expected output for the functions would be: **Function:** `read_student_records(\'students.txt\')` ```python [ {\'student_id\': \'1\', \'name\': \'John Doe\', \'grade\': 85}, {\'student_id\': \'2\', \'name\': \'Jane Smith\', \'grade\': 78}, {\'student_id\': \'3\', \'name\': \'Alice Jones\', \'grade\': 92}, {\'student_id\': \'4\', \'name\': \'Bob Stone\', \'grade\': 70} ] ``` **Function:** `grade_statistics(student_records)` ```python { \'average_grade\': 81.25, \'highest_grade\': 92, \'lowest_grade\': 70 } ``` **Function:** `write_statistics_to_file(statistics, \'output.txt\')` Contents of `output.txt`: ``` Average Grade: 81.25 Highest Grade: 92 Lowest Grade: 70 ``` Performance: - Ensure the program runs efficiently with an expected file size of up to 1MB. Implement these functions carefully and ensure your solution is robust against possible errors.","solution":"import os def read_student_records(file_path): Reads student records from a file. Args: file_path (str): Path to the input text file. Returns: list: List of dictionaries containing student records. if not os.path.exists(file_path): raise FileNotFoundError(f\\"The file {file_path} does not exist.\\") student_records = [] with open(file_path, \'r\') as file: for line in file: student_id, name, grade = line.strip().split(\', \') student_records.append({ \'student_id\': student_id, \'name\': name, \'grade\': int(grade) }) return student_records def grade_statistics(student_records): Calculates the average, highest, and lowest grades of the students. Args: student_records (list): List of dictionaries containing student records. Returns: dict: Dictionary containing average, highest, and lowest grades. grades = [student[\'grade\'] for student in student_records] average_grade = sum(grades) / len(grades) highest_grade = max(grades) lowest_grade = min(grades) return { \'average_grade\': average_grade, \'highest_grade\': highest_grade, \'lowest_grade\': lowest_grade } def write_statistics_to_file(statistics, output_file_path): Writes the grade statistics to a file. Args: statistics (dict): Dictionary containing the statistical summaries. output_file_path (str): Path to the output text file. with open(output_file_path, \'w\') as file: file.write(f\\"Average Grade: {statistics[\'average_grade\']:.2f}n\\") file.write(f\\"Highest Grade: {statistics[\'highest_grade\']}n\\") file.write(f\\"Lowest Grade: {statistics[\'lowest_grade\']}n\\") # Example usage if __name__ == \\"__main__\\": student_records = read_student_records(\'students.txt\') statistics = grade_statistics(student_records) write_statistics_to_file(statistics, \'output.txt\')"},{"question":"Task You are required to demonstrate your understanding of the `asyncio` package by implementing a simple asynchronous task manager. This task manager will manage multiple coroutines that perform I/O-bound operations. Specifically, you need to: 1. Implement an asynchronous function `fetch_data` that simulates fetching data over the network. This function should: - Accept a parameter `url` which is a string. - Print a message indicating the start of data fetching for the given URL. - Simulate a delay using `await asyncio.sleep()` for a random duration between 1 and 5 seconds. - Print a message indicating the completion of data fetching for the given URL. 2. Implement another asynchronous function `main` that: - Creates a list of URLs (at least 5). - Uses `fetch_data` to fetch data concurrently for all URLs. - Ensure that all fetch operations run concurrently and the main function waits for all fetch operations to complete before finishing. Requirements - You must use the `asyncio` library to manage coroutines. - The URLs can be any dummy strings as placeholders. - Simulate network delay using `await asyncio.sleep(random_delay)` where `random_delay` is a random float between 1 and 5. - Use `asyncio.gather` to run multiple coroutines concurrently. - Ensure proper synchronization so that the main function completes only after all data fetching tasks are done. Sample Output The output should demonstrate the concurrent nature of the fetch operations. Example: ``` Fetching data from URL: http://example.com/1 Fetching data from URL: http://example.com/2 Fetching data from URL: http://example.com/3 Fetching data from URL: http://example.com/4 Fetching data from URL: http://example.com/5 Completed fetching data from URL: http://example.com/3 Completed fetching data from URL: http://example.com/2 Completed fetching data from URL: http://example.com/5 Completed fetching data from URL: http://example.com/1 Completed fetching data from URL: http://example.com/4 ``` Note: The order of completion messages may vary due to the randomness of sleep durations. Constraints 1. You may not use any other libraries except `asyncio` and `random`. 2. Ensure that each fetch operation simulates a varying delay to demonstrate concurrency. Implementation Write your solution in the form of a Python script named `async_task_manager.py`. ```python import asyncio import random async def fetch_data(url): print(f\\"Fetching data from URL: {url}\\") await asyncio.sleep(random.uniform(1, 5)) print(f\\"Completed fetching data from URL: {url}\\") async def main(): urls = [f\\"http://example.com/{i+1}\\" for i in range(5)] await asyncio.gather(*(fetch_data(url) for url in urls)) if __name__ == \\"__main__\\": asyncio.run(main()) ```","solution":"import asyncio import random async def fetch_data(url): Simulates fetching data over the network. Args: - url (str): The URL to fetch data from. print(f\\"Fetching data from URL: {url}\\") await asyncio.sleep(random.uniform(1, 5)) print(f\\"Completed fetching data from URL: {url}\\") async def main(): Manages multiple fetch_data coroutines running concurrently. urls = [f\\"http://example.com/{i+1}\\" for i in range(5)] await asyncio.gather(*(fetch_data(url) for url in urls)) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Python Coding Assessment Objective The goal is to assess your understanding of defining custom classes in Python that mimic behaviors defined by typical `PyTypeObject` slots in Python C API. Task Description You are required to implement a custom Python class that mimics a subset of behaviors defined by `PyTypeObject` slots. Specifically, you need to create a class that supports numerical operations, attribute access, sequence behavior, and custom string representation. Requirements # Class Name `CustomType` # Attributes - `value` (integer): This will hold the primary integer value for the object. - `name` (string): An optional string attribute to name the object. # Methods **1. `__init__(self, value, name=\'\'):`** - Initialize the class with `value` and optional `name`. **2. `__repr__(self):`** - Return a string representation of the object in the format `<CustomType(value: {value}, name: {name})>`. **3. `__add__(self, other):`** - Support addition with another `CustomType` or an integer. - If `other` is a `CustomType`, add their `value` attributes. - If `other` is an integer, add it to the `value` attribute of the current object. - Return a new `CustomType` instance with the result. **4. `__getitem__(self, index):`** - Mimic sequence behavior by returning the digit at the position `index` from the `value` attribute when treated as a string. - Raise an `IndexError` if the index is out of range. **5. `__setattr__(self, name, value):`** - Implement a custom setattr to store a dictionary of attributes. - Allow setting of a dynamic attribute through the implementation. **6. `__getattr__(self, name):`** - Return the value of a dynamic attribute if it exists, else raise an `AttributeError`. **7. `__hash__(self):`** - Provide a custom hash method that hashes the combination of `value` and `name`. Input and Output - The class should handle integer values for arithmetic and string values for names. - Output from functions should be consistent with Python\'s standard behavior for such operations. Performance Requirements - Ensure that your implementation handles typical edge cases, such as invalid indices in `__getitem__` and type errors in `__add__`. Constraints - Do not use any external libraries or modules. - Focus on accurately implementing the required methods as specified. # Examples ```python obj1 = CustomType(12345, \'obj1\') obj2 = CustomType(67890, \'obj2\') obj3 = obj1 + obj2 # Should create a new CustomType with value 12345 + 67890 print(obj1) # Output: <CustomType(value: 12345, name: obj1)> print(obj3) # Output: <CustomType(value: 80235, name: )> # Mimicking sequence print(obj1[2]) # Output: \'3\' print(obj2[4]) # Output: \'0\' # Dynamic attribute handling obj1.new_attr = \'new_value\' # Dynamically set an attribute print(obj1.new_attr) # Output: \'new_value\' # Custom hash hash_value = hash(obj1) ``` Submission Submit your implementation of the `CustomType` class according to the given specifications. Make sure to include appropriate comments and docstrings in your code.","solution":"class CustomType: def __init__(self, value, name=\'\'): self.value = value self.name = name self._attributes = {} def __repr__(self): return f\\"<CustomType(value: {self.value}, name: {self.name})>\\" def __add__(self, other): if isinstance(other, CustomType): new_value = self.value + other.value elif isinstance(other, int): new_value = self.value + other else: raise TypeError(\\"Unsupported operand type(s) for +\\") return CustomType(new_value) def __getitem__(self, index): value_str = str(self.value) if index < 0 or index >= len(value_str): raise IndexError(\\"Index out of range\\") return value_str[index] def __setattr__(self, name, value): if name in [\'value\', \'name\', \'_attributes\']: super().__setattr__(name, value) else: self._attributes[name] = value def __getattr__(self, name): if name in self._attributes: return self._attributes[name] raise AttributeError(f\\"\'CustomType\' object has no attribute \'{name}\'\\") def __hash__(self): return hash((self.value, self.name))"},{"question":"# Question: You are given a list of words and you need to write a function that processes this list to generate a specific output. The function should perform the following steps: 1. **Filter** out words that are shorter than a given length. 2. **Convert** the filtered words into a list of tuples, where each tuple contains the word and its length. 3. **Remove duplicates** by using the set data structure and then convert it back to a sorted list of tuples based on the word lengths. 4. **Find** the word with the longest length and convert its individual characters into ASCII values. Your task is to implement a function `process_words(words: List[str], min_length: int) -> Tuple[List[Tuple[str, int]], List[int]]`. **Input:** - `words`: A list of strings representing the words. - `min_length`: An integer representing the minimum length a word must have to be included in the processing. **Output:** - A tuple where the first element is a list of tuples (each containing a word and its length) sorted by length. - The second element is a list of ASCII values of the characters in the longest word. **Constraints:** - All words are lowercase alphabetic strings. - The list of words can have up to 10,000 elements. - The minimum length (min_length) will be a non-negative integer. **Function Signature:** ```python from typing import List, Tuple def process_words(words: List[str], min_length: int) -> Tuple[List[Tuple[str, int]], List[int]]: pass ``` **Example:** ```python words = [\\"apple\\", \\"banana\\", \\"kiwi\\", \\"apricot\\", \\"grape\\", \\"date\\"] min_length = 5 result = process_words(words, min_length) print(result) # Output: ([(\'apple\', 5), (\'grape\', 5), (\'banana\', 6), (\'apricot\', 7)], [97, 112, 114, 105, 99, 111, 116]) ``` **Explanation:** 1. Words shorter than 5 characters are filtered out, resulting in `[\'apple\', \'banana\', \'apricot\', \'grape\']`. 2. Convert these into tuples with their lengths: `[(\'apple\', 5), (\'banana\', 6), (\'apricot\', 7), (\'grape\', 5)]`. 3. Remove duplicates (if any), which doesn\'t apply here, and sort by length: `[(\'apple\', 5), (\'grape\', 5), (\'banana\', 6), (\'apricot\', 7)]`. 4. The longest word is \\"apricot\\", and its ASCII values are `[97, 112, 114, 105, 99, 111, 116]`. Note: Performance should be taken into consideration due to the potential size of the input list.","solution":"from typing import List, Tuple def process_words(words: List[str], min_length: int) -> Tuple[List[Tuple[str, int]], List[int]]: # Step 1: Filter out words shorter than min_length filtered_words = [word for word in words if len(word) >= min_length] # Step 2: Convert the filtered words into a list of tuples (word, length) word_length_tuples = [(word, len(word)) for word in filtered_words] # Step 3: Remove duplicates and sort by length word_length_set = set(word_length_tuples) sorted_word_length_list = sorted(word_length_set, key=lambda x: x[1]) # Step 4: Find the word with the longest length if sorted_word_length_list: longest_word = max(sorted_word_length_list, key=lambda x: x[1])[0] ascii_values = [ord(char) for char in longest_word] else: ascii_values = [] return sorted_word_length_list, ascii_values"},{"question":"**Coding Assessment Question** # Objective: To test the understanding and application of the `operator` module\'s functions to perform sequence operations and mathematical operations within a functional programming context. # Problem Statement: You are given a list of tuples where each tuple contains a string (representing a name) and an integer (representing a score). Your task is to write a function that returns a new list of tuples sorted by scores in descending order and finally return the name of the person with the highest score in uppercase. # Function Signature: ```python def process_scores(data: list) -> str: pass ``` # Input: - A list of tuples, `data`, where each tuple contains: * A string (the name of the person) * An integer (the score of the person) # Output: - A string representing the name of the person with the highest score, converted to uppercase. # Constraints: - The input list will have at least one tuple. - The names in the tuples are non-empty strings. - The scores are non-negative integers. # Example: ```python data = [(\\"Alice\\", 50), (\\"Bob\\", 75), (\\"Charlie\\", 75), (\\"David\\", 60)] print(process_scores(data)) # Output: \\"BOB\\" or \\"CHARLIE\\" ``` # Requirements: 1. Utilize appropriate functions from the `operator` module to achieve the task. 2. Ensure the function is efficient and concise. # Solution Outline: 1. Use `operator.itemgetter` to extract scores for sorting. 2. Utilize `sorted()` function to sort the list in descending order of scores. 3. Extract the name with the highest score. 4. Convert that name to uppercase before returning.","solution":"from operator import itemgetter def process_scores(data): This function takes a list of tuples, where each tuple contains a string (name) and an integer (score). It returns the name of the person with the highest score in uppercase. # Sort the data using itemgetter to sort by score in descending order sorted_data = sorted(data, key=itemgetter(1), reverse=True) # Extract the name of the person with the highest score highest_score_name = sorted_data[0][0] # Convert name to uppercase return highest_score_name.upper()"},{"question":"Objective: To assess your understanding of the `grp` module in Python and your ability to work with Unix group database entries, we need you to implement a function that processes the group information in a specified way. Problem Statement: Write a function `find_common_members_in_groups(group_names)` that takes a list of group names and returns a list of user names that are members of **all** specified groups. Function Signature: ```python def find_common_members_in_groups(group_names: list) -> list: ``` Input: - `group_names` (list): A list of strings where each string is a Unix group name. Output: - `common_members` (list): A list of strings where each string is a user name that is a member of all groups specified in `group_names`. Constraints: - You can assume that `group_names` will always contain valid group names found in the group database. - If the group has no members, it should be treated as an empty list. - If there are no common members, return an empty list. Example: Suppose the following groups are available in the database: ```python grp.getgrnam(\'admin\') # (\'admin\', \'x\', 1000, [\'alice\', \'bob\', \'carol\']) grp.getgrnam(\'staff\') # (\'staff\', \'x\', 1001, [\'bob\', \'carol\', \'dave\']) grp.getgrnam(\'developers\') # (\'developers\', \'x\', 1002, [\'carol\', \'elaine\']) ``` For the input: ```python group_names = [\'admin\', \'staff\', \'developers\'] ``` The output should be: ```python [\'carol\'] ``` Note: - You are expected to use the `grp.getgrnam(name)` function to retrieve the group members. - Consider edge cases like groups with no members or non-overlapping groups. Performance: - The solution should be efficient and able to handle a reasonable number of groups quickly.","solution":"import grp def find_common_members_in_groups(group_names: list) -> list: if not group_names: return [] group_members = [] for group_name in group_names: try: group_info = grp.getgrnam(group_name) group_members.append(set(group_info[3])) except KeyError: # If group doesn\'t exist or has no members, add an empty set group_members.append(set()) if not group_members: return [] # Reduce to common members using set intersection common_members = set.intersection(*group_members) return list(common_members)"},{"question":"<|Analysis Begin|> The provided documentation focuses on the `wsgiref` module, which implements the WSGI (Web Server Gateway Interface) specification for Python. This module includes various utilities, classes, and functions to help create WSGI-compliant web applications and servers. Key components in the documentation include: 1. `wsgiref.util`: Utilities for working with WSGI environments. 2. `wsgiref.headers`: Tools for manipulating HTTP headers. 3. `wsgiref.simple_server`: A simple WSGI HTTP server for serving web applications. 4. `wsgiref.validate`: A tool for checking WSGI conformance. 5. `wsgiref.handlers`: Base classes for implementing WSGI servers and gateways. Given the breadth of functionalities provided by `wsgiref`, a good assessment question can focus on creating a WSGI-compliant web application that utilizes several of the utilities and classes described. <|Analysis End|> <|Question Begin|> # Create a WSGI Application with Custom Utilities and Validation Design and implement a WSGI-compliant web application that provides a small API to manage a simple key-value store. Your application should support the following operations: 1. **GET** `/get/<key>`: Retrieve the value associated with the given key. 2. **POST** `/set/<key>`: Set the value for the given key. 3. **DELETE** `/delete/<key>`: Delete the key-value pair for the given key. You should also use the `wsgiref.validate` module to ensure your application complies with the WSGI specification and the `wsgiref.simple_server` module to run your application. **Requirements:** - Use `wsgiref.util.setup_testing_defaults` to set up a dummy environment for testing. - Use `wsgiref.headers.Headers` to manage HTTP response headers. - Use `wsgiref.validate.validator` to validate your WSGI application. - Implement exception handling for invalid operations (e.g., accessing a non-existent key). **Detailed Steps:** 1. Implement the WSGI application callable. 2. Create utility functions using `wsgiref.util` to handle path extraction and query parameters. 3. Use `wsgiref.simple_server.make_server` to host your application. 4. Wrap your application using `wsgiref.validate.validator`. **Input:** - An HTTP request to your WSGI application. **Output:** - An appropriate HTTP response from your application, including status code and headers. **Example Usage:** ```python from wsgiref.simple_server import make_server from wsgiref.validate import validator # Your WSGI application implementation here # Example run with make_server(\'\', 8000, validator(my_wsgi_app)) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever() ``` **Constraints:** - The key should be a non-empty string. - The value set through POST should be a non-empty string provided in the request body. Test your application with curl or a web browser to ensure it handles all specified operations and edge cases correctly.","solution":"from wsgiref.simple_server import make_server from wsgiref.validate import validator from wsgiref.util import setup_testing_defaults from wsgiref.headers import Headers import json store = {} def my_wsgi_app(environ, start_response): setup_testing_defaults(environ) path = environ[\'PATH_INFO\'] method = environ[\'REQUEST_METHOD\'] headers = Headers([]) if path.startswith(\'/get/\'): key = path[len(\'/get/\'):] if key in store: status = \'200 OK\' response_body = json.dumps({key: store[key]}) else: status = \'404 Not Found\' response_body = json.dumps({\'error\': \'Key not found\'}) headers.add_header(\'Content-Type\', \'application/json\') elif path.startswith(\'/set/\') and method == \'POST\': key = path[len(\'/set/\'):] try: request_body = environ[\'wsgi.input\'].read(int(environ.get(\'CONTENT_LENGTH\', 0))) value = json.loads(request_body).get(\'value\', \'\') if value: store[key] = value status = \'200 OK\' response_body = json.dumps({key: store[key]}) else: raise ValueError except (ValueError, KeyError): status = \'400 Bad Request\' response_body = json.dumps({\'error\': \'Invalid value\'}) headers.add_header(\'Content-Type\', \'application/json\') elif path.startswith(\'/delete/\'): key = path[len(\'/delete/\'):] if key in store: del store[key] status = \'200 OK\' response_body = json.dumps({\'message\': \'Key deleted\'}) else: status = \'404 Not Found\' response_body = json.dumps({\'error\': \'Key not found\'}) headers.add_header(\'Content-Type\', \'application/json\') else: status = \'404 Not Found\' response_body = json.dumps({\'error\': \'Invalid path\'}) headers.add_header(\'Content-Type\', \'application/json\') start_response(status, headers.items()) return [response_body.encode(\'utf-8\')] if __name__ == \'__main__\': app = validator(my_wsgi_app) with make_server(\'\', 8000, app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever()"},{"question":"Objective Demonstrate your proficiency with the seaborn library by transforming and visualizing a given dataset in both long-form and wide-form formats. You will also need to interpret the data and create meaningful visual representations that provide insights. Problem Statement You are provided with a dataset `sales.csv` capturing monthly sales data of a retail store from year 2000 to 2020. The dataset contains three columns: \'Year\', \'Month\', and \'Sales\'. Your task is to transform this dataset into both long-form and wide-form, and create specific visualizations. - Download the `sales.csv` file from the following link: [sales.csv](https://example.com/sales.csv) Requirements 1. **Data Loading and Initial Exploration** - Load the dataset into a pandas DataFrame. - Display the first 5 rows of the DataFrame to understand its structure. 2. **Data Transformation** - Convert the dataset to long-form format. Each variable (Year, Month, Sales) should be a column. - Convert the dataset to a wide-form format using the pivot method, with \'Year\' as the index and \'Month\' as the columns. 3. **Visualizations** - Using long-form data, create a line plot showing the monthly sales trend for each year. The x-axis should represent the \'Month\', the y-axis should represent \'Sales\', and different years should be distinguished using different colors. - Using wide-form data, create a line plot showing the annual sales trend for each month. The x-axis should represent the \'Year\', the y-axis should represent \'Sales\', and different months should be distinguished using different line styles. 4. **Analysis** - Provide a brief analysis of the plots generated. Highlight any key trends, patterns, or insights you can infer from the visualizations. Implementation Details - Use seaborn for visualizations and adhere to its best practices. - Structure your code into functions where appropriate. - Ensure your code is well-documented and cleanly formatted. - Include comments explaining your transformation steps and logic. Input Format - The input will be the `sales.csv` file with columns \'Year\', \'Month\', and \'Sales\'. Output Format - Display the first 5 rows of both the original and transformed DataFrames. - Display the seaborn plots as specified. - Write a brief analysis of the plots. Constraints - You can assume the dataset is clean and does not require any missing value handling or outlier removal. Example ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the dataset df = pd.read_csv(\'sales.csv\') print(df.head()) # Long-form DataFrame print(\\"Long-form DataFrame:\\") print(df.head()) # Wide-form DataFrame wide_df = df.pivot(index=\\"Year\\", columns=\\"Month\\", values=\\"Sales\\") print(\\"Wide-form DataFrame:\\") print(wide_df.head()) # Long-form plot sns.relplot(data=df, x=\\"Month\\", y=\\"Sales\\", hue=\\"Year\\", kind=\\"line\\") plt.show() # Wide-form plot sns.relplot(data=wide_df, kind=\\"line\\") plt.show() # Analysis print(\\"Analysis of the plots...\\") # Add your analysis here ```","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_data(file_path): Load the dataset into a pandas DataFrame. df = pd.read_csv(file_path) return df def transform_data(df): Transform the dataframe to both long-form and wide-form. # Long-form is already the provided format long_df = df # Wide-form format wide_df = df.pivot(index=\\"Year\\", columns=\\"Month\\", values=\\"Sales\\") return long_df, wide_df def plot_long_form(long_df): Create a line plot using long-form data showing monthly sales trend for each year. sns.lineplot(data=long_df, x=\\"Month\\", y=\\"Sales\\", hue=\\"Year\\") plt.title(\\"Monthly Sales Trend by Year\\") plt.show() def plot_wide_form(wide_df): Create a line plot using wide-form data showing annual sales trend for each month. plt.figure() for month in wide_df.columns: sns.lineplot(data=wide_df, x=wide_df.index, y=month, label=month) plt.legend(title=\'Month\') plt.title(\\"Annual Sales Trend by Month\\") plt.show() def analyze_plots(): Provide brief analysis of the plots. analysis = From the Monthly Sales Trend by Year (long-form plot): - We can identify trends in sales over the months for each year. - Seasonal trends such as peak sales during certain months can be observed. - Comparisons across different years can highlight growth or decline in sales. From the Annual Sales Trend by Month (wide-form plot): - We can observe how the sales have changed over the years for each month. - Patterns such as consistent peak months, or years with significant anomalies. print(analysis) # Demonstration of solution functions def main(file_path): df = load_data(file_path) print(\\"Original DataFrame:\\") print(df.head()) long_df, wide_df = transform_data(df) print(\\"Long-form DataFrame:\\") print(long_df.head()) print(\\"Wide-form DataFrame:\\") print(wide_df.head()) plot_long_form(long_df) plot_wide_form(wide_df) analyze_plots()"},{"question":"# PyTorch CUDA Graphs Coding Assessment You have been provided with an essential introduction to CUDA Graphs and their integration within PyTorch. In this assessment, you are required to demonstrate your understanding of CUDA Graphs by implementing a function that utilizes these concepts. Your task involves creating a CUDA Graph Tree and handling dynamic shapes. Task: 1. **Initialize:** - Set up the environment for CUDA Graph execution in PyTorch. - Enable input mutation support. 2. **Function Implementation:** - Implement a function, `graph_tree_operations`, that takes a tensor `x` as input and performs the following operations: 1. **GRAPH 1**: Multiply `x` with itself. 2. If the sum of the elements of the result from GRAPH 1 is positive: - **GRAPH 2**: Apply an element-wise exponential power (`y ** y`) operation. 3. Otherwise: - **GRAPH 3**: Apply an element-wise absolute value power operation (`y.abs() ** y.abs()`). 4. Finally, regardless of the previous branch: - **GRAPH 4**: Multiply the result with a random tensor of the same shape using `torch.rand_like`. 3. **Handle Dynamic Shapes:** - Ensure your function correctly handles input tensors with dynamic shapes by padding them to a fixed size if necessary. - Record a new CUDA Graph for every unique shape encountered across function calls. # Constraints - **Static Input Shapes**: Input tensors must be padded to a fixed size for consistency and to leverage CUDA Graph benefits. - **Memory Management**: Appropriately manage memory to ensure that the additional overhead is minimized and tensors do not overwrite each other inadvertently. # Input - A tensor `x` on the GPU. # Output - The resultant tensor after performing the stated operations. # Example Usage ```python import torch @torch.compile(mode=\\"reduce-overhead\\") def graph_tree_operations(x): # GRAPH 1 y = x * x # graph break triggered here if y.sum() > 0: # GRAPH 2 z = y ** y else: # GRAPH 3 z = (y.abs() ** y.abs()) torch._dynamo.graph_break() # GRAPH 4 return z * torch.rand_like(z) torch._inductor.config.triton.cudagraph_support_input_mutation = True for i in range(3): # Different shapes should be padded to a fixed size. x = torch.rand(i + 8, device=\\"cuda\\") result = graph_tree_operations(x) print(result) ``` **Note: Ensure to use `torch._inductor.config.cudagraph_support_input_mutation = True` to handle input mutations appropriately.**","solution":"import torch def graph_tree_operations(x): Function to perform graph tree operations on tensor x as per the given task. 1. Multiply x with itself. 2. Depending on the sum of elements: a. If sum is positive, apply element-wise exponential power. b. Otherwise, apply element-wise absolute value power. 3. Multiply the result with a random tensor of the same shape. fixed_size = (1024,) # Fix the size dynamically. If needed, change to appropriate shape. if x.numel() < 1024: padding = (0, 1024 - x.numel()) x = torch.nn.functional.pad(x, padding) # GRAPH 1 y = x * x # GRAPH 2 or GRAPH 3 if y.sum() > 0: z = y ** y # GRAPH 2 else: z = y.abs() ** y.abs() # GRAPH 3 # GRAPH 4 result = z * torch.rand_like(z) return result torch._inductor.config.triton.cudagraph_support_input_mutation = True"},{"question":"# Seaborn Plot Customization You are required to create a seaborn visualization that demonstrates your ability to manipulate plot layouts and dimensions. Task 1. **Overall Dimensions**: Create a figure with specific overall dimensions. 2. **Subplot Configuration**: - Generate a set of subplots based on given row and column variables. - Ensure these subplots are shrunk appropriately to fit in the available space. 3. **Layout Engine**: Apply a specific layout engine and observe the difference in the plot arrangement. 4. **Plot Size Control**: Adjust the plot size relative to the figure using `extent`. Details You need to implement the following function: ```python def customized_plot(data, row_variable, col_variable, size, layout_engine, extent): # \'data\' is a Pandas DataFrame containing your plot data. # \'row_variable\' and \'col_variable\' are column names in the DataFrame to be used for faceting. # \'size\' is a tuple specifying the overall dimensions of the figure. # \'layout_engine\' is a string specifying the layout engine to use. # \'extent\' is a list specifying the extent of the plot relative to the figure. # Implement the function to create and display the required Seaborn plot. pass ``` Input - `data`: A Pandas DataFrame. Example: ```python data = pd.DataFrame({ \'A\': [\'a\', \'a\', \'b\', \'b\'], \'B\': [\'x\', \'y\', \'x\', \'y\'], \'X\': [1, 2, 3, 4], \'Y\': [10, 20, 30, 40] }) ``` - `row_variable`: A string representing the column name for the row facet variable. - `col_variable`: A string representing the column name for the column facet variable. - `size`: A tuple (width, height) defining the overall dimensions of the figure. - `layout_engine`: A string representing the layout engine to be used (e.g., \\"constrained\\"). - `extent`: A list [left, bottom, width, height] defining the relative size of the plot within the figure. Output - The function should display the customized seaborn plot. Example Usage ```python customized_plot(data, \'A\', \'B\', (8, 6), \'constrained\', [0, 0, 0.8, 1]) ``` # Constraints 1. The `row_variable` and `col_variable` must exist in the DataFrame. 2. The `size` tuple should have positive values. 3. The `extent` list should define a valid plot region within the figure (all values between 0 and 1). # Notes - Ensure your code handles the constraints and errors gracefully. - Focus on achieving clear, well-labelled, and properly arranged plots.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def customized_plot(data, row_variable, col_variable, size, layout_engine, extent): Creates a custom seaborn plot with specified layout and dimensions. Parameters: data (pd.DataFrame): Data for plotting row_variable (str): Variable for row faceting col_variable (str): Variable for column faceting size (tuple): Overall dimensions (width, height) of the figure layout_engine (str): Layout engine to use extent (list): Extent of the plot within the figure [left, bottom, width, height] if row_variable not in data.columns or col_variable not in data.columns: raise ValueError(f\\"{row_variable} or {col_variable} not found in DataFrame columns\\") if size[0] <= 0 or size[1] <= 0: raise ValueError(\\"Size dimensions must be positive values\\") if not all(0 <= e <= 1 for e in extent) or len(extent) != 4: raise ValueError(\\"Extent must be a list of four values between 0 and 1\\") # Create the seaborn facet grid plot fg = sns.FacetGrid(data, row=row_variable, col=col_variable) fg.map(sns.scatterplot, \'X\', \'Y\') plt.figure(figsize=size) fg.fig.set_size_inches(size) fg.fig.set_layout_engine(layout_engine) plt.subplots_adjust(left=extent[0], bottom=extent[1], right=extent[0]+extent[2], top=extent[1]+extent[3]) fg.fig.show()"},{"question":"**Turtle Graphics Challenge: Draw a Radial Pattern** # Objective Write a Python function using the turtle module. This function should create a radial pattern where each spoke of the pattern is a concentric circle. The number of spokes and the number of circles within each spoke should be customizable. The circles should change their colors in a gradient from one color to another. # Function Signature ```python def draw_radial_pattern(num_spokes: int, num_circles: int, color1: str, color2: str) -> None: pass ``` # Input - `num_spokes` (int): The number of spokes in the radial pattern. (1 <= num_spokes <= 36) - `num_circles` (int): The number of concentric circles in each spoke. (1 <= num_circles <= 10) - `color1` (str): Starting color of the gradient in hexadecimal format (e.g., \'#FF0000\' for red). - `color2` (str): Ending color of the gradient in hexadecimal format (e.g., \'#0000FF\' for blue). # Requirements 1. Use the turtle module to create the drawing. 2. Draw the specified number of spokes. The angle between each spoke should be evenly distributed (i.e., 360 degrees divided by the number of spokes). 3. Within each spoke, draw the specified number of concentric circles. The circles should be centered on the spoke and evenly spaced. 4. Implement a gradient transition from `color1` to `color2` across the circles. 5. Ensure all drawings fit within the canvas. # Example ```python # Example call to the function draw_radial_pattern(12, 5, \'#FF0000\', \'#0000FF\') ``` * The above call should draw a radial pattern with 12 spokes. * Each spoke should have 5 concentric circles. * Circles transition in color from red to blue. # Constraints - Ensure smooth performance even with the maximum input values. - Manage turtle speed to complete the drawing reasonably quickly. # Submission Submit your Python function. Ensure your code is well-structured and includes comments where necessary. The function should execute without any required user interaction beyond invocation.","solution":"import turtle import math from turtle import colormode def hex_to_rgb(hex_color): Convert hex color to RGB. hex_color = hex_color.lstrip(\'#\') return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4)) def interpolate_color(color1, color2, fraction): Interpolate between two RGB colors. fraction is a float between 0 and 1. r1, g1, b1 = color1 r2, g2, b2 = color2 r = int(r1 + (r2 - r1) * fraction) g = int(g1 + (g2 - g1) * fraction) b = int(b1 + (b2 - b1) * fraction) return (r, g, b) def draw_radial_pattern(num_spokes: int, num_circles: int, color1: str, color2: str) -> None: Draw a radial pattern where each spoke of the pattern is a concentric circle. The number of spokes and the number of circles within each spoke are customizable. The circles change their colors in a gradient from color1 to color2. turtle.speed(\'fastest\') colormode(255) # use 255 RGB color mode angle = 360 / num_spokes radius_increment = 20 color_start = hex_to_rgb(color1) color_end = hex_to_rgb(color2) for i in range(num_spokes): turtle.penup() turtle.goto(0, 0) turtle.setheading(i * angle) for j in range(num_circles): turtle.forward(radius_increment * (j + 1)) circle_color = interpolate_color(color_start, color_end, j / (num_circles - 1)) turtle.pendown() turtle.color(circle_color) turtle.circle(radius_increment) turtle.penup() turtle.goto(0, 0) turtle.setheading(i * angle) turtle.hideturtle() turtle.done() # Example call # draw_radial_pattern(12, 5, \'#FF0000\', \'#0000FF\')"},{"question":"**String Data Processing in Pandas** You are given a DataFrame containing information about various products. Each row represents one product, and the columns include details such as the product ID, name, category, and description. Your task is to write functions that clean and extract useful information from this DataFrame using the pandas library. # DataFrame Structure The DataFrame `df` has the following columns: - **ProductID**: A unique identifier for each product (e.g., \'P001\', \'P002\'). - **ProductName**: The name of the product (e.g., \'Widget A\', \'Gadget B\'). - **Category**: The category to which the product belongs (e.g., \'Electronics\', \'Household\'). - **Description**: A detailed description of the product, which may contain special characters, extraneous whitespace, and mixed casing. # Requirements 1. **Clean Column Names**: Implement a function `clean_column_names(df)` that removes leading and trailing whitespace from column names, converts them to lowercase, and replaces any remaining whitespace with underscores. 2. **Standardize Product Names**: Implement a function `standardize_product_names(df)` that converts all product names to title case (each word capitalized). 3. **Extract Product Codes**: Implement a function `extract_product_codes(df)` that extracts numerical product codes from the ProductID using regular expressions. This function should return a new column `ProductCode` with the extracted numerical values. 4. **Categorize by Description**: Implement a function `categorize_by_description(df)` that categorizes products based on keywords in their descriptions. Add a new column `DescriptionCategory` with one of the following values based on the description content: - \'High Tech\' if the description contains the word \'technology\' or \'digital\'. - \'Eco Friendly\' if the description contains the word \'sustainable\' or \'eco\'. - \'General\' for all other products. # Input - `df` (pd.DataFrame): A DataFrame containing product data. # Output - The output should be a DataFrame with the original columns and the new columns `ProductCode` and `DescriptionCategory`. # Example ```python data = { \'ProductID\': [\'P001\', \'P002\', \'P003\'], \'ProductName\': [\' widget A \', \'GADGET B\', \'device C\'], \'Category\': [\'Electronics\', \'Household\', \'Gadgets\'], \'Description\': [\'Advanced digital technology.\', \'Eco-friendly and sustainable.\', \'A general purpose device.\'] } df = pd.DataFrame(data) # Expected Output: # After cleaning column names: # df.columns --> [\'productid\', \'productname\', \'category\', \'description\'] # After standardizing product names: # df[\'productname\'] --> [\'Widget A\', \'Gadget B\', \'Device C\'] # After extracting product codes: # df[\'productcode\'] --> [1, 2, 3] # After categorizing by description: # df[\'descriptioncategory\'] --> # [\'High Tech\', \'Eco Friendly\', \'General\'] ``` **Function Definitions**: ```python import pandas as pd import re def clean_column_names(df): # Your implementation here pass def standardize_product_names(df): # Your implementation here pass def extract_product_codes(df): # Your implementation here pass def categorize_by_description(df): # Your implementation here pass ``` **Notes**: 1. Ensure that you appropriately handle NaN values and non-standard formats. 2. Use pandas string methods and regular expressions as needed.","solution":"import pandas as pd import re def clean_column_names(df): Removes leading and trailing whitespace from column names, converts them to lowercase, and replaces any remaining whitespace with underscores. df.columns = df.columns.str.strip().str.lower().str.replace(\' \', \'_\') return df def standardize_product_names(df): Converts all product names to title case (each word capitalized). df[\'productname\'] = df[\'productname\'].str.title().str.strip() return df def extract_product_codes(df): Extracts numerical product codes from the ProductID using regular expressions. This function returns a new column `ProductCode` with the extracted numerical values. df[\'productcode\'] = df[\'productid\'].str.extract(r\'(d+)\').astype(int) return df def categorize_by_description(df): Categorizes products based on keywords in their descriptions. Adds a new column `DescriptionCategory` with one of the following values based on the description content: - \'High Tech\' if the description contains the word \'technology\' or \'digital\'. - \'Eco Friendly\' if the description contains the word \'sustainable\' or \'eco\'. - \'General\' for all other products. def categorize(description): description = description.lower() if any(word in description for word in [\'technology\', \'digital\']): return \'High Tech\' elif any(word in description for word in [\'sustainable\', \'eco\']): return \'Eco Friendly\' else: return \'General\' df[\'descriptioncategory\'] = df[\'description\'].fillna(\'\').apply(categorize) return df"},{"question":"# Question: Resource Usage Monitoring and Alert System Objective You are tasked to implement a Python function leveraging the Unix-specific services provided by the `resource`, `syslog`, and other modules to monitor resource usage and send alerts if the usage surpasses specified thresholds. Function Signature ```python def monitor_resources(cpu_threshold: float, memory_threshold: int): Monitors CPU and memory usage of the Unix system. If usage surpasses the given thresholds, logs a warning message to the syslog. Parameters: cpu_threshold (float): The CPU usage threshold in percentage (0 to 100). memory_threshold (int): The memory usage threshold in kilobytes. Returns: bool: True if an alert was triggered, otherwise False. ``` Input - `cpu_threshold` (float): The CPU usage threshold in percentage from 0 to 100. Any usage above this threshold should trigger an alert. - `memory_threshold` (int): The memory usage threshold in kilobytes. Any usage above this threshold should trigger an alert. Output - The function returns `True` if an alert is triggered (CPU or memory usage exceeds their respective thresholds), and `False` otherwise. Constraints - Use the `resource` module to determine resource usage. - Log the alerts to the Unix system log using the `syslog` module. - Assume the function will be run on a Unix system. Performance Requirements - The solution should efficiently monitor the resource usage with minimal latency. Example ```python # Call the function with thresholds result = monitor_resources(75.0, 150000) # An example of a potential system log entry if thresholds are surpassed might be: # \\"WARNING: CPU usage has surpassed 75.0% or memory usage has surpassed 150000 KB.\\" # Expected Output: # True (if the system usage exceeds any of the given thresholds) # False (otherwise) ``` Hint You may need to query the resource limits and current resource usage using the `resource` module and log messages using the `syslog` module. Refer to their respective documentation parts to find appropriate methods and ensure appropriate handling of the system resources.","solution":"import psutil import syslog def monitor_resources(cpu_threshold: float, memory_threshold: int): Monitors CPU and memory usage of the Unix system. If usage surpasses the given thresholds, logs a warning message to the syslog. Parameters: cpu_threshold (float): The CPU usage threshold in percentage (0 to 100). memory_threshold (int): The memory usage threshold in kilobytes. Returns: bool: True if an alert was triggered, otherwise False. # Get the current CPU usage percentage cpu_usage = psutil.cpu_percent(interval=1) # Get the current memory usage in kilobytes memory_info = psutil.virtual_memory() memory_usage = memory_info.used / 1024 # Convert bytes to KB # Initialize the alert flag alert_triggered = False # Check if the CPU usage exceeds the threshold if cpu_usage > cpu_threshold: syslog.syslog(syslog.LOG_WARNING, f\\"WARNING: CPU usage has surpassed {cpu_threshold}%: {cpu_usage}%\\") alert_triggered = True # Check if the memory usage exceeds the threshold if memory_usage > memory_threshold: syslog.syslog(syslog.LOG_WARNING, f\\"WARNING: Memory usage has surpassed {memory_threshold} KB: {memory_usage} KB\\") alert_triggered = True return alert_triggered"},{"question":"**PyTorch Coding Assessment Question** # Objective: Your task is to utilize the `torch.cuda.tunable` module to implement a function that configures and manages tunable operations for CUDA-based matrix multiplication. Specifically, you will enable the tuning operations, set a filename for tuning data storage, and ensure the tuning results are written to a file. # Function Signature: ```python def configure_tunable_operations(filename: str, tuning_duration: int, tuning_iterations: int) -> dict: Configures and retrieves the results of tunable operations for CUDA-based GEMM (General Matrix Multiply). Args: - filename (str): The name of the file where tuning data will be stored. - tuning_duration (int): The maximum duration (in seconds) allowed for tuning operations. - tuning_iterations (int): The maximum number of iterations allowed for tuning operations. Returns: - dict: The results of tuning operations as a dictionary. ``` # Input: 1. `filename` - A string specifying the name of the file where tuning data will be stored. 2. `tuning_duration` - An integer specifying the maximum duration (in seconds) allowed for tuning operations. 3. `tuning_iterations` - An integer specifying the maximum number of iterations allowed for tuning operations. # Output: - A dictionary containing the results of the tuning operations. # Constraints: 1. You can assume that the `torch.cuda.tunable` module is available and functions as documented. 2. The tuning operations are focused on CUDA-based GEMM (General Matrix Multiply) computations. # Requirements: 1. Enable tunable operations. 2. Set the maximum tuning duration. 3. Set the maximum number of tuning iterations. 4. Set the filename for tuning data storage. 5. Ensure the tuning data is written to the specified file upon exit. 6. Retrieve and return the results of the tuning operations as a dictionary. # Example Usage: ```python results = configure_tunable_operations(\\"tuning_data.txt\\", 120, 100) print(results) ``` # Additional Information: - You can make use of the following functions from the `torch.cuda.tunable` module: - `enable()` - `set_max_tuning_duration(duration)` - `set_max_tuning_iterations(iterations)` - `set_filename(filename)` - `write_file_on_exit(is_enabled=True)` - `get_results()` Implement the function `configure_tunable_operations` in a way that adheres to the described requirements and constraints.","solution":"import torch def configure_tunable_operations(filename: str, tuning_duration: int, tuning_iterations: int) -> dict: Configures and retrieves the results of tunable operations for CUDA-based GEMM (General Matrix Multiply). Args: - filename (str): The name of the file where tuning data will be stored. - tuning_duration (int): The maximum duration (in seconds) allowed for tuning operations. - tuning_iterations (int): The maximum number of iterations allowed for tuning operations. Returns: - dict: The results of tuning operations as a dictionary. # Enable tunable operations torch.cuda.tunable.enable() # Set the maximum tuning duration torch.cuda.tunable.set_max_tuning_duration(tuning_duration) # Set the maximum number of tuning iterations torch.cuda.tunable.set_max_tuning_iterations(tuning_iterations) # Set the filename for tuning data storage torch.cuda.tunable.set_filename(filename) # Ensure the tuning data is written to the specified file upon exit torch.cuda.tunable.write_file_on_exit(is_enabled=True) # Retrieve and return the results of tuning operations results = torch.cuda.tunable.get_results() return results"},{"question":"Customizable Container with Type Safety You are tasked with implementing a type-safe customizable container class. This container should ensure that only elements of a specific type can be added to it. To achieve this, you will utilize the `typing` module to enhance type safety. Requirements - Implement a class `TypeSafeContainer` that is type-safe, meaning it can only accept elements of a specific type. - Use generics to allow the container to store different types in different instances. - The container should support the following functionalities: 1. Adding an element to the container. 2. Removing an element from the container. 3. Querying if an element exists in the container. 4. Retrieving the size of the container. 5. Generating a string representation of the container. Your implementation should include: - Proper use of the `TypeVar` from the `typing` module to handle generic types. - Type alias to simplify complex type hints if needed. - Type hinting for all functions. - Type checking to ensure elements being added are of the correct type. Constraints 1. Elements added to the `TypeSafeContainer` must be of the same type as the container was initialized with. 2. Your solution should avoid runtime type errors and make good use of type hints to enforce type safety at call sites. Function Signatures ```python from typing import TypeVar, Generic, List, Union T = TypeVar(\'T\') class TypeSafeContainer(Generic[T]): def __init__(self) -> None: # Initialize an empty container pass def add_element(self, element: T) -> None: # Add an element to the container pass def remove_element(self, element: T) -> None: # Remove an element from the container pass def contains_element(self, element: T) -> bool: # Check if an element is in the container pass def size(self) -> int: # Return the number of elements in the container pass def __str__(self) -> str: # Return a string representation of the container pass ``` Example Usage ```python # Creating a type-safe container for integers int_container = TypeSafeContainer[int]() int_container.add_element(1) int_container.add_element(2) print(int_container.contains_element(1)) # Output: True print(int_container.size()) # Output: 2 int_container.remove_element(1) print(int_container.contains_element(1)) # Output: False print(int_container.size()) # Output: 1 print(str(int_container)) # Output: TypeSafeContainer with elements: [2] # Trying to add a string to an int container should raise a type error int_container.add_element(\\"Hello\\") # This should be flagged by a type checker ``` Your implementation should be able to identify and prevent type misuse during development with static type analysis tools like `mypy`. Evaluation Your solution will be evaluated based on: - Correctness of the implementation. - Proper use of type hints. - Adherence to the problem constraints. - Clarity and readability of the code. Ensure your solution passes static type checks.","solution":"from typing import TypeVar, Generic, List T = TypeVar(\'T\') class TypeSafeContainer(Generic[T]): def __init__(self) -> None: self._elements: List[T] = [] def add_element(self, element: T) -> None: self._elements.append(element) def remove_element(self, element: T) -> None: self._elements.remove(element) def contains_element(self, element: T) -> bool: return element in self._elements def size(self) -> int: return len(self._elements) def __str__(self) -> str: return f\\"TypeSafeContainer with elements: {self._elements}\\""},{"question":"# Python Coding Assessment Question **Objective:** Create a Python script utilizing the `warnings` module to demonstrate an understanding of warning control mechanisms. **Task:** 1. Define a function `custom_warning_handler(message, category, filename, lineno, file=None, line=None)` that formats and prints a warning message to the standard output in the following format: ``` WARNING: <message> [<category>] at <filename>:<lineno> ``` 2. Define a function `issue_warning()` that issues a `RuntimeWarning` with the message \\"This is a runtime warning!\\". 3. Define a function `configure_warnings()` that: - Sets up a custom warning filter to always display warnings of category `RuntimeWarning`. - Replaces the default warning handler with the `custom_warning_handler`. 4. Write your script to: - Call `configure_warnings()`. - Call `issue_warning()`. - Use `catch_warnings` context manager to: - Issue a `DeprecationWarning` with the message \\"This is deprecated!\\". - Verify that the warning is captured and its category is `DeprecationWarning`. **Constraints:** - Utilize the `warnings` module functions as explained. - Ensure that custom warnings are displayed using the custom handler format. **Input and Output Format:** No specific input format. The output should be in the format specified in `custom_warning_handler`. Example Execution: ```python if __name__ == \\"__main__\\": configure_warnings() issue_warning() # Capturing DeprecationWarning with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") warnings.warn(\\"This is deprecated!\\", DeprecationWarning) assert len(w) == 1 assert issubclass(w[-1].category, DeprecationWarning) ``` Expected Output: ``` WARNING: This is a runtime warning! [RuntimeWarning] at <filename>:<lineno> ``` Implement the functions to achieve the desired behavior as specified above.","solution":"import warnings def custom_warning_handler(message, category, filename, lineno, file=None, line=None): print(f\\"WARNING: {message} [{category.__name__}] at {filename}:{lineno}\\") def issue_warning(): warnings.warn(\\"This is a runtime warning!\\", RuntimeWarning) def configure_warnings(): warnings.simplefilter(\\"always\\", RuntimeWarning) warnings.showwarning = custom_warning_handler if __name__ == \\"__main__\\": configure_warnings() issue_warning() with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") warnings.warn(\\"This is deprecated!\\", DeprecationWarning) assert len(w) == 1 assert issubclass(w[-1].category, DeprecationWarning)"},{"question":"**Objective**: Demonstrate your understanding of the `http` package, focusing specifically on the `http.HTTPStatus` enum class. **Task**: Write a function `http_status_summary` that takes a list of HTTP status codes (integers) and returns a dictionary. The dictionary should summarize the status codes with the following keys: - `total`: Total number of status codes. - `success`: Number of success codes (2xx). - `client_error`: Number of client error codes (4xx). - `server_error`: Number of server error codes (5xx). - `status_descriptions`: A list of tuples where each tuple contains the status code, its phrase, and its description. **Function Signature**: ```python from typing import List, Dict, Tuple from http import HTTPStatus def http_status_summary(status_codes: List[int]) -> Dict[str, any]: pass ``` **Input**: - `status_codes`: A list of integers representing HTTP status codes. Example: `[200, 404, 500, 201, 301]` **Output**: - Returns a dictionary with the structure described above. Example: ```python { \\"total\\": 5, \\"success\\": 2, \\"client_error\\": 1, \\"server_error\\": 1, \\"status_descriptions\\": [ (200, \\"OK\\", \\"Request fulfilled, document follows\\"), (404, \\"Not Found\\", \\"Nothing matches the given URI\\"), (500, \\"Internal Server Error\\", \\"An error inside the HTTP server which prevented it from fulfilling the request\\"), (201, \\"Created\\", \\"Document created\\"), (301, \\"Moved Permanently\\", \\"The URL of the requested resource has been changed permanently\\") ] } ``` **Constraints**: - The input list may contain valid and invalid HTTP status codes. - Invalid status codes should be ignored in the summary and should not cause the function to fail. **Requirements**: - Utilize the `http.HTTPStatus` enum class to extract the phrases and descriptions. - Handle the different ranges of status codes appropriately. - Ignore invalid status codes without causing errors. **Example**: ```python from http import HTTPStatus def http_status_summary(status_codes: List[int]) -> Dict[str, any]: summary = { \\"total\\": 0, \\"success\\": 0, \\"client_error\\": 0, \\"server_error\\": 0, \\"status_descriptions\\": [] } for code in status_codes: try: status = HTTPStatus(code) if 200 <= code < 300: summary[\\"success\\"] += 1 elif 400 <= code < 500: summary[\\"client_error\\"] += 1 elif 500 <= code < 600: summary[\\"server_error\\"] += 1 summary[\\"status_descriptions\\"].append((code, status.phrase, status.description)) except ValueError: # Invalid status code, ignore it continue summary[\\"total\\"] = len(status_codes) return summary ``` **Test the function**: ```python print(http_status_summary([200, 404, 500, 201, 301])) # Expected output: # { # \\"total\\": 5, # \\"success\\": 2, # \\"client_error\\": 1, # \\"server_error\\": 1, # \\"status_descriptions\\": [ # (200, \\"OK\\", \\"Request fulfilled, document follows\\"), # (404, \\"Not Found\\", \\"Nothing matches the given URI\\"), # (500, \\"Internal Server Error\\", \\"An error inside the HTTP server which prevented it from fulfilling the request\\"), # (201, \\"Created\\", \\"Document created\\"), # (301, \\"Moved Permanently\\", \\"The URL of the requested resource has been changed permanently\\") # ] # } ```","solution":"from typing import List, Dict, Tuple from http import HTTPStatus def http_status_summary(status_codes: List[int]) -> Dict[str, any]: summary = { \\"total\\": 0, \\"success\\": 0, \\"client_error\\": 0, \\"server_error\\": 0, \\"status_descriptions\\": [] } for code in status_codes: try: status = HTTPStatus(code) if 200 <= code < 300: summary[\\"success\\"] += 1 elif 400 <= code < 500: summary[\\"client_error\\"] += 1 elif 500 <= code < 600: summary[\\"server_error\\"] += 1 summary[\\"status_descriptions\\"].append((code, status.phrase, status.description)) except ValueError: # Invalid status code, ignore it continue summary[\\"total\\"] = len(status_codes) return summary"},{"question":"**Question: Working with Memory Views** You are required to perform some operations involving direct memory manipulation using Python\'s `memoryview` objects. Your task is to implement a function that takes an array of integers and performs operations to: 1. Create a writable memoryview object from the integer array. 2. Modify specific elements within the memoryview. 3. Return the modified array as well as the base object underlying the memoryview. # Function Signature ```python def manipulate_memoryview(arr: List[int], modifications: Dict[int, int]) -> Tuple[List[int], Any]: pass ``` # Input - `arr`: A list of integers (`List[int]`), which will be converted into a writable memoryview object. - `modifications`: A dictionary (`Dict[int, int]`) where keys are index positions and values are the new values to set at those positions. # Output - A tuple (`Tuple[List[int], Any]`) where: - The first element is the modified array. - The second element is the base object upon which the memoryview was created. # Constraints - Index positions in `modifications` will always be within the bounds of `arr`. - The values in `arr` are guaranteed to fit within a normal `int` type. - Performance should be considered, but there are no specific requirements for extremely large inputs. # Example ```python arr = [1, 2, 3, 4, 5] modifications = {1: 20, 3: 40} result = manipulate_memoryview(arr, modifications) # Result should be: ([1, 20, 3, 40, 5], arr) ``` **Explanation:** - The function first converts the list `arr` into a memoryview object. - Then, it performs the modifications specified in the `modifications` dictionary using direct memory access. - Finally, it returns the modified array and the base object (which should be the original array). Ensure your implementation uses the `memoryview` object correctly and handles all edge cases as specified.","solution":"from typing import List, Dict, Tuple, Any def manipulate_memoryview(arr: List[int], modifications: Dict[int, int]) -> Tuple[List[int], Any]: Takes an array of integers and a dict of modifications. Modifies the array using a memoryview object and returns the modified array and the original array. # Create a memoryview from the array mem_view = memoryview(bytearray(arr)) # Apply the modifications for index, new_value in modifications.items(): mem_view[index] = new_value # Convert the memoryview to a list and return it along with the original array return list(mem_view), arr"},{"question":"# Seaborn Coding Assessment Question Question You are tasked with analyzing a dataset using the seaborn library. Your objective is to create visualizations to answer specific questions about the data. Specifically, you will work with the `titanic` dataset, which includes information about passengers on the Titanic. **Dataset**: The `titanic` dataset can be loaded directly using `sns.load_dataset(\\"titanic\\")`. **Task**: Write a function named `analyze_titanic_data` that performs the following operations: 1. Creates a count plot showing the number of passengers in each class (`\\"class\\"`), further divided by their survival status (`\\"survived\\"`). 2. Customizes the plot by setting appropriate titles and axis labels. 3. Adds a subplot that shows the proportion of passengers who survived versus those who didn\'t, segmented by gender (`\\"sex\\"`). 4. Creates a subplot that visualizes the density distribution of ages (`\\"age\\"`) separated by the port of embarkation (`\\"embark_town\\"`). The function should save the resulting plots in a single image file named `titanic_analysis.png`. Requirements: - **Input**: None (the function will use the seaborn dataset loader internally). - **Output**: None (the function should save the plot to a file). - **Constraints**: - Use seaborn for visualization. - Ensure the plots are clear and easily interpretable. - The image should have a meaningful title, and each subplot should have appropriate axis labels and legends. Example Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt def analyze_titanic_data(): # Load the dataset titanic = sns.load_dataset(\\"titanic\\") # Create a figure for multiple subplots fig, axes = plt.subplots(3, 1, figsize=(10, 15)) # Plot 1: Count plot of passenger class with survival hue sns.countplot(data=titanic, x=\\"class\\", hue=\\"survived\\", ax=axes[0]) axes[0].set_title(\\"Count of Passengers in Each Class by Survival Status\\") axes[0].set_xlabel(\\"Passenger Class\\") axes[0].set_ylabel(\\"Count\\") # Plot 2: Count plot of survival status by gender sns.countplot(data=titanic, x=\\"sex\\", hue=\\"survived\\", ax=axes[1]) axes[1].set_title(\\"Survival Status by Gender\\") axes[1].set_xlabel(\\"Gender\\") axes[1].set_ylabel(\\"Count\\") # Plot 3: Density plot of age by embarkation town sns.kdeplot(data=titanic, x=\\"age\\", hue=\\"embark_town\\", fill=True, multiple=\\"stack\\", ax=axes[2]) axes[2].set_title(\\"Age Density by Embarkation Town\\") axes[2].set_xlabel(\\"Age\\") axes[2].set_ylabel(\\"Density\\") # Final adjustments and save the figure plt.tight_layout() plt.savefig(\\"titanic_analysis.png\\") plt.close() ``` Notes: - Ensure you handle any potential issues with missing data appropriately. - Set a consistent theme using `sns.set_theme()` at the beginning of your function. - The file `titanic_analysis.png` should be saved in the current working directory.","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_titanic_data(): Analyzes the Titanic dataset and creates visualizations saved in a single image file. sns.set_theme(style=\\"whitegrid\\") # Load the dataset titanic = sns.load_dataset(\\"titanic\\") # Create a figure for multiple subplots fig, axes = plt.subplots(3, 1, figsize=(10, 15)) # Plot 1: Count plot of passenger class with survival hue sns.countplot(data=titanic, x=\\"class\\", hue=\\"survived\\", ax=axes[0]) axes[0].set_title(\\"Count of Passengers in Each Class by Survival Status\\") axes[0].set_xlabel(\\"Passenger Class\\") axes[0].set_ylabel(\\"Count\\") # Plot 2: Count plot of survival status by gender sns.countplot(data=titanic, x=\\"sex\\", hue=\\"survived\\", ax=axes[1]) axes[1].set_title(\\"Survival Status by Gender\\") axes[1].set_xlabel(\\"Gender\\") axes[1].set_ylabel(\\"Count\\") # Plot 3: Density plot of age by embarkation town sns.kdeplot(data=titanic, x=\\"age\\", hue=\\"embark_town\\", fill=True, multiple=\\"stack\\", ax=axes[2]) axes[2].set_title(\\"Age Density by Embarkation Town\\") axes[2].set_xlabel(\\"Age\\") axes[2].set_ylabel(\\"Density\\") # Final adjustments and save the figure plt.tight_layout() plt.savefig(\\"titanic_analysis.png\\") plt.close()"},{"question":"# Question: Model Evaluation using Validation and Learning Curves You are given a dataset and tasked with evaluating the performance of a Support Vector Machine (SVM) model using validation curves and learning curves. This will help in understanding how the model performs with varying hyperparameters and different sizes of training data. Based on the provided dataset, perform the following steps: 1. **Load and Shuffle the Dataset**: - Load the Iris dataset using `sklearn.datasets.load_iris`. Shuffle the dataset with a fixed random state for reproducibility. 2. **Validation Curve**: - Use the `validation_curve` function to evaluate the impact of the hyperparameter `C` on the SVM model with a linear kernel. - Plot the training and validation scores for `C` values in the range `[1e-7, 1e-3, 1, 1e3]`. 3. **Learning Curve**: - Use the `learning_curve` function to examine how the training and validation scores of the SVM model change with a varying number of training samples. - Plot the learning curve using training sizes `[50, 80, 110]`. Expected Input and Output - **Input**: - No explicit input required as you will be using the Iris dataset provided by scikit-learn. - **Output**: - Two plots generated: one for the validation curve showing training and validation scores against `C` values, and one for the learning curve showing training and validation scores against the number of training samples. Constraints - Use `cv=5` for cross-validation in both `validation_curve` and `learning_curve`. - Ensure reproducibility in data shuffling with `random_state=0`. Performance Requirements - The code should efficiently compute the scores using optimized functions like `validation_curve` and `learning_curve`. - The plots should be clear and well-labeled to easily interpret the results. # Implementation ```python import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import validation_curve, learning_curve from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.utils import shuffle # Step 1: Load and shuffle the Iris dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Step 2: Validation Curve param_range = np.logspace(-7, 3, 4) train_scores, valid_scores = validation_curve( SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range, cv=5) # Plotting the validation curve plt.figure(figsize=(10, 6)) plt.plot(param_range, np.mean(train_scores, axis=1), label=\\"Training score\\", marker=\'o\') plt.plot(param_range, np.mean(valid_scores, axis=1), label=\\"Validation score\\", marker=\'o\') plt.xlabel(\\"Parameter C\\") plt.ylabel(\\"Score\\") plt.xscale(\\"log\\") plt.legend(loc=\\"best\\") plt.title(\\"Validation Curve for SVM Model\\") plt.show() # Step 3: Learning Curve train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\'linear\'), X, y, train_sizes=[50, 80, 110], cv=5) # Plotting the learning curve plt.figure(figsize=(10, 6)) plt.plot(train_sizes, np.mean(train_scores, axis=1), label=\\"Training score\\", marker=\'o\') plt.plot(train_sizes, np.mean(valid_scores, axis=1), label=\\"Validation score\\", marker=\'o\') plt.xlabel(\\"Number of training samples\\") plt.ylabel(\\"Score\\") plt.legend(loc=\\"best\\") plt.title(\\"Learning Curve for SVM Model\\") plt.show() ``` # Explanation: - **Validation Curve**: The `validation_curve` function is used to plot how training and validation scores change with different values of the regularization parameter `C` using a linear SVM. - **Learning Curve**: The `learning_curve` function is used to plot the training and validation scores against a varying number of training samples to examine the model performance. - Both curves help in diagnosing if the model suffers from high bias or high variance and how it scales with more data or different hyperparameters.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import validation_curve, learning_curve from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.utils import shuffle def evaluate_svm_with_curves(): # Step 1: Load and shuffle the Iris dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Step 2: Validation Curve param_range = np.logspace(-7, 3, 4) train_scores, valid_scores = validation_curve( SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range, cv=5) # Plotting the validation curve plt.figure(figsize=(10, 6)) plt.plot(param_range, np.mean(train_scores, axis=1), label=\\"Training score\\", marker=\'o\') plt.plot(param_range, np.mean(valid_scores, axis=1), label=\\"Validation score\\", marker=\'o\') plt.xlabel(\\"Parameter C\\") plt.ylabel(\\"Score\\") plt.xscale(\\"log\\") plt.legend(loc=\\"best\\") plt.title(\\"Validation Curve for SVM Model\\") plt.show() # Step 3: Learning Curve train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\'linear\'), X, y, train_sizes=[50, 80, 110], cv=5) # Plotting the learning curve plt.figure(figsize=(10, 6)) plt.plot(train_sizes, np.mean(train_scores, axis=1), label=\\"Training score\\", marker=\'o\') plt.plot(train_sizes, np.mean(valid_scores, axis=1), label=\\"Validation score\\", marker=\'o\') plt.xlabel(\\"Number of training samples\\") plt.ylabel(\\"Score\\") plt.legend(loc=\\"best\\") plt.title(\\"Learning Curve for SVM Model\\") plt.show()"},{"question":"Problem: Tar Archive Manipulation and Filtering You are tasked with writing a Python script to manage tar archives using the `tarfile` module. Your script should be capable of performing the following operations: 1. **Create a Tar Archive**: Create a new tar archive from a list of provided file paths. The tar archive should be compressed using the specified compression method (`gz`, `bz2`, or `xz`). 2. **List Archive Contents**: List the contents of an existing tar archive. The listing should include the filename, size, and type (file, directory, link, etc.) of each member. 3. **Extract Files with Filters**: Extract files from a tar archive to a specified directory. The extraction should use a filter that respects the `tar` mode (strip leading slashes, refuse absolute paths, etc.). # Function Signatures ```python import tarfile def create_tar_archive(archive_name: str, file_paths: list, compression: str) -> None: Create a tar archive from a list of file paths. Parameters: - archive_name (str): The name of the tar archive (e.g., \'archive.tar.gz\'). - file_paths (list): A list of file paths to be added to the archive. - compression (str): The compression method (\'gz\', \'bz2\', \'xz\'). Returns: - None pass def list_tar_contents(archive_name: str) -> list: List the contents of a tar archive. Parameters: - archive_name (str): The name of the tar archive. Returns: - list: A list of tuples containing (filename, size, type). pass def extract_tar_with_filter(archive_name: str, extract_path: str) -> None: Extract files from a tar archive using the \'tar\' filter. Parameters: - archive_name (str): The name of the tar archive. - extract_path (str): The path to extract the files to. Returns: - None pass ``` # Constraints - You can assume that the provided file paths are valid and exist. - The `compression` parameter will always be one of the specified values (`gz`, `bz2`, `xz`). - The `archive_name` will always have a valid extension matching the compression method. # Example Usage ```python # Creating a tar archive create_tar_archive(\'archive.tar.gz\', [\'file1.txt\', \'file2.txt\', \'dir1\'], \'gz\') # Listing contents of the tar archive contents = list_tar_contents(\'archive.tar.gz\') print(contents) # Output: [(\'file1.txt\', 1234, \'file\'), (\'file2.txt\', 5678, \'file\'), (\'dir1\', 4096, \'dir\'), ...] # Extracting files from the tar archive using filter extract_tar_with_filter(\'archive.tar.gz\', \'extracted/\') ``` Your implementation should correctly handle the creation, listing, and extraction of tar files, ensuring that all specified requirements are met.","solution":"import tarfile import os def create_tar_archive(archive_name: str, file_paths: list, compression: str) -> None: Create a tar archive from a list of file paths. Parameters: - archive_name (str): The name of the tar archive (e.g., \'archive.tar.gz\'). - file_paths (list): A list of file paths to be added to the archive. - compression (str): The compression method (\'gz\', \'bz2\', \'xz\'). Returns: - None mode = f\\"w:{compression}\\" with tarfile.open(archive_name, mode) as tar: for file_path in file_paths: tar.add(file_path) def list_tar_contents(archive_name: str) -> list: List the contents of a tar archive. Parameters: - archive_name (str): The name of the tar archive. Returns: - list: A list of tuples containing (filename, size, type). contents = [] with tarfile.open(archive_name, \'r:*\') as tar: for member in tar.getmembers(): type = \'file\' if member.isfile() else \'dir\' if member.isdir() else \'link\' if member.islnk() else \'unknown\' contents.append((member.name, member.size, type)) return contents def extract_tar_with_filter(archive_name: str, extract_path: str) -> None: Extract files from a tar archive using the \'tar\' filter. Parameters: - archive_name (str): The name of the tar archive. - extract_path (str): The path to extract the files to. Returns: - None with tarfile.open(archive_name, \'r:*\') as tar: for member in tar.getmembers(): if member.name.startswith(\'/\') or \'..\' in member.name: continue # Skip absolute paths and attempts to traverse up the directory structure tar.extract(member, path=extract_path)"},{"question":"Objective Implement and manipulate a persistent dictionary using the `shelve` module. This exercise will assess your understanding of persistent storage, context management, and efficient data manipulation. Task 1. **Function Implementation**: - Implement a function `manage_shelf(filename: str, operations: list, writeback: bool=False) -> dict` that performs a series of operations on a shelf. 2. **Function Details**: - **Input**: - `filename` (str): The name of the file to persist dictionary data. - `operations` (list): A list of operations to be performed on the shelf. Each operation is a tuple, where the first element is a string representing the operation (`\'add\'`, `\'get\'`, `\'delete\'`, `\'modify\'`), and the subsequent elements will be the required parameters for that operation. - `writeback` (bool): A boolean indicating whether to enable the writeback cache. - **Output**: - A dictionary representing the final state of the shelf after all operations have been executed. 3. **Operations**: - `\'add\'` operation: Adds a new key-value pair to the shelf. The operation will be in the format `(\'add\', key: str, value: any)`. - `\'get\'` operation: Retrieves the value for the given key. The operation will be in the format `(\'get\', key: str)`. - `\'delete\'` operation: Deletes the key-value pair for the given key. The operation will be in the format `(\'delete\', key: str)`. - `\'modify\'` operation: Modifies the value of an existing key. The operation will be in the format `(\'modify\', key: str, new_value: any)`. 4. **Constraints and Behavior**: - Handle cases where the key does not exist for `get`, `delete`, and `modify` operations by raising a `KeyError`. - Ensure the shelf is closed properly at the end of the function\'s execution, either by using a context manager or explicitly calling `close()`. - When `writeback` is `True`, ensure modified values are written back to the shelf correctly. 5. **Performance Requirements**: - Efficiently handle shelves with up to 10,000 entries. Examples ```python # Example 1 filename = \'test_shelf.db\' operations = [ (\'add\', \'a\', 1), (\'add\', \'b\', 2), (\'get\', \'a\'), (\'modify\', \'a\', 10), (\'delete\', \'b\'), (\'get\', \'a\') ] writeback = True result = manage_shelf(filename, operations, writeback) # Expected output: {\'a\': 10} # Example 2 filename = \'empty_shelf.db\' operations = [] writeback = False result = manage_shelf(filename, operations, writeback) # Expected output: {} ``` Additional Information Refer to the provided documentation of the `shelve` module for more details on its usage and constraints.","solution":"import shelve def manage_shelf(filename: str, operations: list, writeback: bool=False) -> dict: Manages a shelf database by performing a series of operations specified in the operations list. Parameters: filename (str): The name of the file to persist dictionary data. operations (list): A list of operations to be performed on the shelf. writeback (bool): Boolean indicating whether to enable the writeback cache. Returns: dict: The final state of the shelf after all operations have been executed. with shelve.open(filename, writeback=writeback) as shelf: for operation in operations: op_name = operation[0] if op_name == \\"add\\": _, key, value = operation shelf[key] = value elif op_name == \\"get\\": _, key = operation if key not in shelf: raise KeyError(f\\"Key \'{key}\' not found in shelf.\\") elif op_name == \\"delete\\": _, key = operation if key in shelf: del shelf[key] else: raise KeyError(f\\"Key \'{key}\' not found in shelf.\\") elif op_name == \\"modify\\": _, key, new_value = operation if key in shelf: shelf[key] = new_value else: raise KeyError(f\\"Key \'{key}\' not found in shelf.\\") # Return the final state of the shelf return dict(shelf)"},{"question":"Using Array API with Scikit-learn Objective Demonstrate your comprehension of the Array API support in scikit-learn by configuring your environment, implementing a model, and transforming data using different array backends. Task Write a Python function `array_api_pipeline` that: 1. Configures the environment for Array API support. 2. Uses scikit-learn and an array manipulation library (CuPy or PyTorch) to create and train a `LinearDiscriminantAnalysis` model. 3. Transforms input data using the trained model and ensures the transformed data is returned in the appropriate array format. Input - `backend`: A string indicating which array backend to use. It can be either `\\"cupy\\"` or `\\"torch\\"`. - `X`: A 2D NumPy array of shape `(n_samples, n_features)` representing the feature matrix. - `y`: A 1D NumPy array of shape `(n_samples,)` representing the target values. Output - A transformed array of the same type and backend as specified by input `backend`. The transformed data should be the result of applying `LinearDiscriminantAnalysis` model to `X`. Requirements 1. Install the necessary packages (`cupy`, `torch`, `scikit-learn`, etc.) if they are not already installed. 2. Set up the appropriate environment variable to enable Array API support. 3. Use the provided data `X` and `y` to train a `LinearDiscriminantAnalysis` model using the specified backend. 4. Ensure the transformed data maintains the same type and device as the input `backend`. Example ```python import numpy as np def array_api_pipeline(backend: str, X: np.ndarray, y: np.ndarray): import os from sklearn import config_context from sklearn.discriminant_analysis import LinearDiscriminantAnalysis # Set environment variable for Array API support os.environ[\'SCIPY_ARRAY_API\'] = \'1\' if backend == \'cupy\': import cupy as cp X_backend, y_backend = cp.asarray(X), cp.asarray(y) elif backend == \'torch\': import torch X_backend = torch.tensor(X, device=\'cuda\', dtype=torch.float32) y_backend = torch.tensor(y, device=\'cuda\', dtype=torch.float32) else: raise ValueError(\\"Unsupported backend. Choose either \'cupy\' or \'torch\'.\\") with config_context(array_api_dispatch=True): lda = LinearDiscriminantAnalysis() X_transformed = lda.fit_transform(X_backend, y_backend) return X_transformed # Example usage: X_np = np.array([[0.5, 1.5], [1.0, 1.0], [1.5, 0.5], [0.0, 0.0]]) y_np = np.array([0, 1, 0, 1]) print(array_api_pipeline(\'cupy\', X_np, y_np)) print(array_api_pipeline(\'torch\', X_np, y_np)) ``` Notes - The function should handle any necessary imports and environment configurations internally. - Ensure that the input and output types are consistent with the specified backend. This task requires students to have a good understanding of the Array API in scikit-learn, how to configure their environment, and how to handle different array manipulation libraries for implementing scikit-learn pipelines.","solution":"import os import numpy as np from sklearn import config_context from sklearn.discriminant_analysis import LinearDiscriminantAnalysis def array_api_pipeline(backend: str, X: np.ndarray, y: np.ndarray): Sets up the array API backend and applies LinearDiscriminantAnalysis to transform the input data. Parameters: - backend (str): Either \'cupy\' or \'torch\', indicating the array backend. - X (np.ndarray): 2D numpy array representing the feature matrix. - y (np.ndarray): 1D numpy array representing the target values. Returns: - Transformed data in the respective backend format. # Set environment variable for Array API support os.environ[\'SKLEARN_EXPERIMENTAL_ARRAY_API\'] = \'1\' if backend == \'cupy\': import cupy as cp X_backend, y_backend = cp.asarray(X), cp.asarray(y) elif backend == \'torch\': import torch X_backend = torch.tensor(X, device=\'cuda\', dtype=torch.float32) y_backend = torch.tensor(y, device=\'cuda\', dtype=torch.int64) else: raise ValueError(\\"Unsupported backend. Choose either \'cupy\' or \'torch\'.\\") with config_context(array_api_dispatch=True): lda = LinearDiscriminantAnalysis() X_transformed = lda.fit_transform(X_backend, y_backend) return X_transformed"},{"question":"**Question: Performance Measurement and Debugging Tracebacks** # Problem Statement: You are required to implement a Python function `analyze_function_performance` that takes another function as input, runs it, and provides a detailed performance analysis and potential error traceback if the function fails. # Function Signature: ```python def analyze_function_performance(func, *args, **kwargs) -> dict: pass ``` # Inputs: - `func`: A function object that you need to execute and analyze. - `*args`: Variable length argument list passed to the `func`. - `**kwargs`: Arbitrary keyword arguments passed to the `func`. # Outputs: - A dictionary with the following keys and corresponding values: - `execution_time`: Time in seconds the function took to execute. - `memory_usage`: Peak memory usage in bytes during the function execution. - `traceback`: The traceback string if the function raises an exception, otherwise `None`. # Constraints: - The input function `func` can be any valid Python function. - You may assume that the function execution time will not exceed 60 seconds. - The function should handle exceptions gracefully and provide appropriate traceback information. # Example: ```python def sample_function(a, b): return a + b result = analyze_function_performance(sample_function, 5, 7) print(result) # Expected Output: # { # \'execution_time\': 0.00001, # This is just an example, actual time may vary # \'memory_usage\': 1256, # This is just an example, actual usage may vary # \'traceback\': None # } def faulty_function(): return 1 / 0 result = analyze_function_performance(faulty_function) print(result) # Expected Output: # { # \'execution_time\': t, # time taken before the exception was raised # \'memory_usage\': m, # memory usage before the exception was raised # \'traceback\': \'Traceback (most recent call last):n...nZeroDivisionError: division by zeron\' # } ``` # Hints: - Use the `timeit` module to measure the execution time of the function. - Use the `tracemalloc` module to trace memory allocations and peak memory usage. - Use the `faulthandler` module or other mechanisms to capture traceback information on errors. Implement the `analyze_function_performance` function to provide a comprehensive analysis of the input function\'s performance and error handling.","solution":"import time import tracemalloc import traceback def analyze_function_performance(func, *args, **kwargs): performance_data = { \'execution_time\': None, \'memory_usage\': None, \'traceback\': None } # Start the memory tracing tracemalloc.start() # Start the timer start_time = time.time() try: # Execute the function func(*args, **kwargs) except Exception as e: # Capture the traceback if any error occurs performance_data[\'traceback\'] = traceback.format_exc() finally: # Stop the timer end_time = time.time() # Get current and peak memory usage _, peak_memory = tracemalloc.get_traced_memory() # Record execution time performance_data[\'execution_time\'] = end_time - start_time performance_data[\'memory_usage\'] = peak_memory # Stop tracing memory tracemalloc.stop() return performance_data"},{"question":"# Garbage Collector Management and Debugging in Python **Objective**: In this exercise, you are required to manage garbage collection in a Python program using the `gc` module. You need to perform several tasks to ensure proper memory management and to analyze garbage collector statistics both before and after execution of certain operations. **Task**: Implement a function `manage_garbage_collection()` which performs the following steps: 1. **Initial Setup**: - Disable garbage collection. - Set the garbage collector thresholds to `(700, 10, 10)`. 2. **Simulate Operation**: - Create a list of `10,000` dictionaries where each dictionary contains a single key-value pair `{i: \\"value\\"}`. Store this list in a variable `big_list`. - Create a circular reference in the list by assigning `big_list[0][\'self\'] = big_list`. 3. **Enable and Monitor Garbage Collection**: - Re-enable the garbage collection. - Collect the garbage and store the collected statistics (number of unreachable objects found) in a variable `initial_collected`. 4. **Set Debugging and Freeze State**: - Set the debugging level to `gc.DEBUG_LEAK`. - Freeze the state of the objects being tracked. - Make changes to `big_list` by removing the circular reference (`del big_list[0][\'self\']`). - Unfreeze the objects so they are back in the oldest generation. 5. **Collect and Analyze**: - Collect the garbage again and store the result (number of unreachable objects found) in a variable `final_collected`. 6. **Return Results**: - Return a tuple containing `initial_collected`, `final_collected`, and the statistics obtained from `gc.get_stats()`. **Function Signature**: ```python def manage_garbage_collection() -> tuple: pass ``` # Input and Output **Input**: - None. **Output**: - A tuple containing: - `initial_collected`: The number of unreachable objects found after the initial garbage collection. - `final_collected`: The number of unreachable objects found after modifying and unfreezing the objects. - `stats`: The garbage collection statistics obtained using `gc.get_stats()`. # Example ```python result = manage_garbage_collection() print(result) # This should print a tuple like: # (initial_collected, final_collected, stats) # Where `initial_collected` and `final_collected` are integers, and `stats` is a list of dictionaries containing garbage collection statistics. ``` # Constraints - Do not use any additional libraries other than the `gc` module. - Ensure that all unnecessary objects are dereferenced and collected where applicable to prevent memory leaks in your implementation. # Performance Requirements - The function should complete within a reasonable time frame as it involves creating and managing a moderate number of objects (`10,000` dictionaries). # Notes - Pay careful attention to the creation and breaking of circular references to observe changes in garbage collection. - Thoroughly document your code and use print statements for debugging purposes where necessary to understand the flow and operation of each step.","solution":"import gc def manage_garbage_collection(): # Step 1: Initial Setup gc.disable() gc.set_threshold(700, 10, 10) # Step 2: Simulate Operation big_list = [{i: \\"value\\"} for i in range(10000)] big_list[0][\'self\'] = big_list # Create circular reference # Step 3: Enable and Monitor Garbage Collection gc.enable() initial_collected = gc.collect() # Step 4: Set Debugging and Freeze State gc.set_debug(gc.DEBUG_LEAK) gc.freeze() del big_list[0][\'self\'] # Remove circular reference gc.unfreeze() # Step 5: Collect and Analyze final_collected = gc.collect() # Step 6: Return Results stats = gc.get_stats() return initial_collected, final_collected, stats"},{"question":"**Problem Statement:** Design a Python function called `validate_and_import` that takes the name of a module as a string and performs the following operations: 1. **Validate** that the module can be imported without errors. 2. **Extract metadata** such as the module version and the list of author(s). 3. **Dynamically import** the module if the validation is successful, otherwise raise an appropriate error. # Function Signature: ```python def validate_and_import(module_name: str) -> dict: pass ``` # Input: - `module_name`: A string representing the name of the module to import. # Output: - A dictionary containing two keys: - `\\"module\\"`: The imported module object. - `\\"metadata\\"`: A dictionary containing the module\'s metadata with keys `\\"version\\"` and `\\"authors\\"`. # Constraints: - If the module cannot be imported, raise an `ImportError` with the message `\\"Cannot import module <module_name>\\"`. - If metadata such as version or authors is not available, use `\\"N/A\\"` as the default value. # Example: ```python metadata = validate_and_import(\'math\') print(metadata) # Output: # { # \\"module\\": <module \'math\' (built-in)>, # \\"metadata\\": { # \\"version\\": \\"N/A\\", # \\"authors\\": \\"N/A\\" # } # } metadata = validate_and_import(\'numpy\') print(metadata) # Output might look like: # { # \\"module\\": <module \'numpy\' from \'.../numpy/__init__.py\'>, # \\"metadata\\": { # \\"version\\": \\"1.21.2\\", # \\"authors\\": \\"Travis Oliphant et al.\\" # } # } ``` # Notes: 1. Utilize `importlib` for dynamic imports and to assist in extracting metadata. 2. `importlib.metadata` can be particularly useful for accessing package metadata like version and authors. 3. Handle all exceptions appropriately and provide clear messaging for debugging.","solution":"import importlib import importlib.metadata def validate_and_import(module_name: str) -> dict: Validates and imports a specified module by name, and extracts its metadata. Parameters: ---------- module_name : str The name of the module to validate and import. Returns: -------- dict A dictionary containing the module object and its metadata (version and authors). try: # Dynamically import the module module = importlib.import_module(module_name) except ModuleNotFoundError: raise ImportError(f\\"Cannot import module {module_name}\\") # Initialize metadata dictionary metadata = { \\"version\\": \\"N/A\\", \\"authors\\": \\"N/A\\", } try: # Extract module version using metadata version = importlib.metadata.version(module_name) metadata[\\"version\\"] = version except importlib.metadata.PackageNotFoundError: pass try: # Extract module authors using metadata authors = importlib.metadata.metadata(module_name).get(\'Author\', \'N/A\') metadata[\\"authors\\"] = authors except importlib.metadata.PackageNotFoundError: pass return { \\"module\\": module, \\"metadata\\": metadata }"},{"question":"Objective: You are tasked with creating a series of visualizations to deeply analyze the `\\"diamonds\\"` dataset from the Seaborn library. Your visualizations should provide insights about the distribution and specific characteristics of the diamonds based on different attributes. Instructions: 1. **Load the Dataset** - Use the Seaborn library to load the `\\"diamonds\\"` dataset. 2. **Log-Scaled Price Histogram** - Create a histogram of the diamond\'s prices with a logarithmic scale on the x-axis. - Customize the histogram\'s appearance to have full-width bars with a thin edge. 3. **Color Coded Cut** - Modify the histogram to have bars colored by the diamond\'s cut. Avoid overlap between bars by using appropriate transforms. 4. **Faceted Histograms** - Create faceted histograms of the diamond\'s prices for each `cut` value. Use additional properties (e.g., `alpha`, `edgewidth`) to distinguish between the facets. 5. **Unfilled Bars Adjustment** - Create another version of the histogram where the bars are unfilled but have customizable edge colors and edge widths. 6. **Narrow Bars for Specific Clarity** - Narrow the bars for only diamonds with `clarity` of `\\"SI1\\"`. Use reasonable binwidth and binrange to highlight the distribution. Each part should be implemented in separate code cells and visually distinct. Make sure to include necessary comments and explanations for each step. Expected Input: No input required; the dataset is loaded from Seaborn. Expected Output: Five visualizations adhering to the instructions above. Constraints: - Ensure your solution adheres to the object-oriented paradigm of Seaborn (`seaborn.objects`). - The visualizations should be clear and properly labeled. - Handle potential overlap by using attributes or transforms where required. Performance Requirements: - The complete set of visualizations should render within a reasonable time frame, ensuring efficient handling of the dataset. Here\'s an outline of the expected structure in a Jupyter Notebook: ```python # Code Cell 1: Load the dataset import seaborn.objects as so from seaborn import load_dataset diamonds = load_dataset(\\"diamonds\\") # Code Cell 2: Log-Scaled Price Histogram p = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") p.add(so.Bars(), so.Hist()) # Code Cell 3: Color Coded Cut p.add(so.Bars(), so.Hist(), so.Stack(), color=\\"cut\\") # Code Cell 4: Faceted Histograms for cut_group in diamonds[\\"cut\\"].unique(): so.Plot(diamonds.query(f\\"cut == \'{cut_group}\'\\"), \\"price\\").add( so.Bars(edgewidth=0.5), so.Hist(), alpha=\\"clarity\\") # Code Cell 5: Unfilled Bars Adjustment p.add(so.Bars(fill=False, edgecolor=\\"C0\\", edgewidth=1.5), so.Hist()) # Code Cell 6: Narrow Bars for Specific Clarity hist = so.Hist(binwidth=0.1, binrange=(2, 5)) p.add(so.Bars(), hist).add( so.Bars(color=\\"0.9\\", width=0.5), hist, data=diamonds.query(\\"clarity == \'SI1\'\\") ) ``` Good luck, and happy coding!","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the diamond dataset diamonds = load_dataset(\\"diamonds\\") # 1. Log-Scaled Price Histogram p1 = so.Plot(diamonds, x=\\"price\\").scale(x=\\"log\\") p1.add(so.Bars(edgecolor=\\"black\\"), so.Hist()) p1.show() # 2. Color Coded Cut p2 = so.Plot(diamonds, x=\\"price\\").scale(x=\\"log\\") p2.add(so.Bars(edgecolor=\\"black\\"), so.Hist(), so.Stack(), color=\\"cut\\") p2.show() # 3. Faceted Histograms for cut_group in diamonds[\\"cut\\"].unique(): p3 = so.Plot(diamonds.query(f\\"cut == \'{cut_group}\'\\"), x=\\"price\\").scale(x=\\"log\\") p3.add(so.Bars(edgewidth=0.5, alpha=0.6), so.Hist(), color=\\"clarity\\") p3.show() # 4. Unfilled Bars Adjustment p4 = so.Plot(diamonds, x=\\"price\\").scale(x=\\"log\\") p4.add(so.Bars(fill=False, edgecolor=\\"C0\\", edgewidth=1.5), so.Hist()) p4.show() # 5. Narrow Bars for Specific Clarity hist = so.Hist(binwidth=0.1, binrange=(2, 5)) p5 = so.Plot(diamonds.query(\\"clarity == \'SI1\'\\"), x=\\"price\\") p5.add(so.Bars(color=\\"0.9\\", edgecolor=\\"black\\", width=0.5), hist) p5.show()"},{"question":"# Question You are tasked with developing a Python script that processes a list of file paths by compressing each file using the LZMA algorithm. Your script should perform the following tasks: 1. **Read and compress files**: Given a list of file paths, read each file, compress its contents using a custom filter chain with the `LZMACompressor` class, and save the compressed data to a new file with the same name but with an extension \\".xz\\". 2. **Decompress and validate files**: For each compressed file, decompress its contents using the `LZMADecompressor` class, and validate that the decompressed data matches the original file data. # Implementation Details 1. **Function `compress_files(file_paths: List[str]) -> None`**: - **Input**: - `file_paths`: A list of strings where each string is a valid file path to a file that needs to be compressed. - **Output**: - None (The function should compress files and save them with a \\".xz\\" extension in the same directory). - **Constraints**: - Use a custom filter chain for compression: Use a delta filter with `dist=5` followed by an LZMA2 filter with `preset=7`. - **Example**: - If given `[\\"file1.txt\\", \\"file2.txt\\"]`, the function should produce `[\\"file1.txt.xz\\", \\"file2.txt.xz\\"]`. 2. **Function `validate_compressed_files(file_paths: List[str]) -> bool`**: - **Input**: - `file_paths`: A list of strings where each string is a valid file path pointing to the original files before compression. - **Output**: - `True` if all decompressed files match the original files, `False` otherwise. - **Constraints**: - Decompress the files using `LZMADecompressor`. - The names of the compressed files will be the same as the original with an additional \\".xz\\" extension. - **Example**: - If given `[\\"file1.txt\\", \\"file2.txt\\"]`, the function should check `[\\"file1.txt.xz\\", \\"file2.txt.xz\\"]` for decompressed contents against the originals. # Example Usage ```python file_paths = [\\"example1.txt\\", \\"example2.txt\\"] compress_files(file_paths) is_valid = validate_compressed_files(file_paths) print(\\"All files validated:\\", is_valid) ``` # Assumptions - The input file paths are valid, and the files exist in the specified locations. - The `compress_files` function overwrites any existing files with the same name but `.xz` extension. - The Python standard library `lzma` is used for compression and decompression. # Additional Notes - Consider handling edge cases such as empty files during testing.","solution":"import lzma import os def compress_files(file_paths): for file_path in file_paths: with open(file_path, \'rb\') as input_file: file_data = input_file.read() compressor = lzma.LZMACompressor(filters=[ {\'id\': lzma.FILTER_DELTA, \'dist\': 5}, {\'id\': lzma.FILTER_LZMA2, \'preset\': 7} ]) compressed_data = compressor.compress(file_data) compressed_data += compressor.flush() with open(file_path + \'.xz\', \'wb\') as compressed_file: compressed_file.write(compressed_data) def validate_compressed_files(file_paths): for file_path in file_paths: with open(file_path, \'rb\') as input_file: original_data = input_file.read() with open(file_path + \'.xz\', \'rb\') as compressed_file: compressed_data = compressed_file.read() decompressor = lzma.LZMADecompressor() try: decompressed_data = decompressor.decompress(compressed_data) except lzma.LZMAError: return False if original_data != decompressed_data: return False return True"},{"question":"# Question You are given a DataFrame `sales_data` containing monthly sales data for various product categories over a period of two years. Your task is to visualize this data using Pandas plotting functionalities. The DataFrame has the following columns: - `date`: The date of the sales record. - `category`: The category of the product. - `sales`: The sales figure for that category on that date. Tasks 1. **Plot Time Series Line Plot**: - Create a line plot to visualize the sales trends over the two years for all product categories. - Each category should be represented by a different line on the plot. - Provide appropriate labels for the x and y axes and add a legend to indicate the categories. 2. **Bar Plot for Aggregated Data**: - Aggregate the total sales for each category and create a bar plot to visualize the total sales for each category. - Ensure the bars are labeled with the category names. 3. **Box Plot for Sales Distribution**: - Create a box plot to visualize the distribution of sales figures for each category. - Each box should represent a different category. 4. **Scatter Plot with Different Colors**: - Create a scatter plot to show the sales figures for each category over time. - Different categories should be represented by different colors. - Include a legend to indicate which color corresponds to which category. Constraints - The DataFrame `sales_data` will have at least 24 months of data (covering the two years). - There will be at least 3 different product categories. Input - A Pandas DataFrame `sales_data` with columns `date`, `category`, and `sales`. Output Your code should display the four specified plots. Performance Requirements - Ensure the plots are well-labeled and the different categories are easily distinguishable. - The plots should be created using Pandas\' built-in plotting functionalities. You can assume the necessary libraries like Pandas and Matplotlib are already imported. Here\'s the DataFrame structure for reference: ```python import pandas as pd data = { \'date\': pd.date_range(start=\'1/1/2020\', periods=24, freq=\'M\').tolist() * 3, \'category\': [\'Category_A\'] * 24 + [\'Category_B\'] * 24 + [\'Category_C\'] * 24, \'sales\': [np.random.randint(100, 1000) for _ in range(72)] } sales_data = pd.DataFrame(data) ``` Implement the required visualizations below: ```python # Implement your solution here ```","solution":"import pandas as pd import numpy as np import matplotlib.pyplot as plt def plot_sales_data(sales_data): # Ensure the date column is in datetime format sales_data[\'date\'] = pd.to_datetime(sales_data[\'date\']) # Task 1: Plot Time Series Line Plot plt.figure(figsize=(10, 6)) for category in sales_data[\'category\'].unique(): category_data = sales_data[sales_data[\'category\'] == category] plt.plot(category_data[\'date\'], category_data[\'sales\'], label=category) plt.xlabel(\'Date\') plt.ylabel(\'Sales\') plt.title(\'Sales Trends Over Time\') plt.legend() plt.show() # Task 2: Bar Plot for Aggregated Data category_sales = sales_data.groupby(\'category\')[\'sales\'].sum() category_sales.plot(kind=\'bar\', figsize=(10, 6)) plt.xlabel(\'Category\') plt.ylabel(\'Total Sales\') plt.title(\'Total Sales by Category\') plt.show() # Task 3: Box Plot for Sales Distribution plt.figure(figsize=(10, 6)) sales_data.boxplot(column=\'sales\', by=\'category\') plt.xlabel(\'Category\') plt.ylabel(\'Sales\') plt.title(\'Sales Distribution by Category\') plt.suptitle(\'\') # Remove the default subtitle plt.show() # Task 4: Scatter Plot with Different Colors plt.figure(figsize=(10, 6)) categories = sales_data[\'category\'].unique() colors = plt.cm.rainbow(np.linspace(0, 1, len(categories))) for category, color in zip(categories, colors): category_data = sales_data[sales_data[\'category\'] == category] plt.scatter(category_data[\'date\'], category_data[\'sales\'], label=category, color=color) plt.xlabel(\'Date\') plt.ylabel(\'Sales\') plt.title(\'Sales Scatter Plot by Category\') plt.legend() plt.show()"},{"question":"**PyTorch Intermediate Representations** In this assignment, you will be required to demonstrate your understanding of PyTorch\'s Intermediate Representations (IRs), specifically the Core Aten IR and Prims IR. PyTorch 2.0 provides these IRs to facilitate interactions with compiler backends. # Objective: Your task is to implement a function that performs a series of tensor operations using either Core Aten IR or Prims IR, based on the input parameters. # Requirements: 1. **Input**: - Two tensors `tensor_a` and `tensor_b` of the same shape. - A string `ir_type` which specifies whether to use \\"core_aten\\" or \\"prims\\". - A string `operation` which indicates the type of operation to perform: \\"add\\", \\"multiply\\", or \\"subtract\\". 2. **Output**: - A tensor resulting from the specified operation on the input tensors using the specified IR. # Constraints: - Both input tensors will have the same shape. - The `ir_type` string will only contain \\"core_aten\\" or \\"prims\\". - The `operation` string will only contain \\"add\\", \\"multiply\\", or \\"subtract\\". # Function Signature: ```python def tensor_operations(tensor_a: torch.Tensor, tensor_b: torch.Tensor, ir_type: str, operation: str) -> torch.Tensor: pass ``` # Implementation Details: 1. **Core Aten IR**: - If `ir_type` is \\"core_aten\\", use the corresponding core aten operations to perform the specified operation. - Core Aten IR does not include explicit type promotion or broadcasting. 2. **Prims IR**: - If `ir_type` is \\"prims\\", use primitive operations that involve type promotion or broadcasting to achieve the desired operation. - Prims IR simplifies to primal operations like `prims.convert_element_type` and `prims.broadcast_in_dim` as needed. # Example Usage: ```python import torch tensor_a = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) tensor_b = torch.tensor([[5.0, 6.0], [7.0, 8.0]]) # Using Core Aten IR to add tensors result = tensor_operations(tensor_a, tensor_b, \\"core_aten\\", \\"add\\") print(result) # Expected Output: tensor([[ 6.0, 8.0], [10.0, 12.0]]) # Using Prims IR to multiply tensors result = tensor_operations(tensor_a, tensor_b, \\"prims\\", \\"multiply\\") print(result) # Expected Output: tensor([[ 5.0, 12.0], [21.0, 32.0]]) ``` # Additional Notes: - You may refer to PyTorch\'s official documentation and native functions yaml for specifics on core aten operations. - Ensure that the implementation handles type promotions and broadcasting correctly while using Prims IR.","solution":"import torch def tensor_operations(tensor_a: torch.Tensor, tensor_b: torch.Tensor, ir_type: str, operation: str) -> torch.Tensor: if ir_type not in [\\"core_aten\\", \\"prims\\"]: raise ValueError(\\"ir_type must be \'core_aten\' or \'prims\'\\") if operation not in [\\"add\\", \\"multiply\\", \\"subtract\\"]: raise ValueError(\\"operation must be \'add\', \'multiply\', or \'subtract\'\\") if ir_type == \\"core_aten\\": if operation == \\"add\\": return torch.add(tensor_a, tensor_b) elif operation == \\"multiply\\": return torch.mul(tensor_a, tensor_b) elif operation == \\"subtract\\": return torch.sub(tensor_a, tensor_b) elif ir_type == \\"prims\\": # Prims IR using simple type promotion and broadcasting if not tensor_a.is_floating_point(): tensor_a = tensor_a.to(torch.float32) if not tensor_b.is_floating_point(): tensor_b = tensor_b.to(torch.float32) tensor_a, tensor_b = torch.broadcast_tensors(tensor_a, tensor_b) if operation == \\"add\\": return torch.add(tensor_a, tensor_b) elif operation == \\"multiply\\": return torch.mul(tensor_a, tensor_b) elif operation == \\"subtract\\": return torch.sub(tensor_a, tensor_b) return None"},{"question":"# Compression and Decompression Using Bzip2 Objective Implement functions to compress data and write it to a file, and to read compressed data from a file and decompress it. Use the `bz2` module to achieve this. Description You are required to write two functions: 1. **compress_to_file(data: bytes, filename: str, compresslevel: int = 9) -> None** - Compress `data` using the specified compression level and write it to the file specified by `filename`. - Parameters: - `data` (bytes): The data to be compressed. - `filename` (str): The name of the file to which the compressed data will be written. - `compresslevel` (int): The compression level (an integer between 1 and 9). The default is 9. - Return: None 2. **decompress_from_file(filename: str) -> bytes** - Read compressed data from the file specified by `filename` and decompress it. - Parameters: - `filename` (str): The name of the file from which compressed data will be read. - Return: The decompressed data (bytes). Constraints - You must use the `bz2` module for compression and decompression. - Handle files in binary mode to ensure compatibility with the `bz2` library. - Assume `filename` points to a valid file location. - Ensure the integrity of data by checking that decompression of compressed data matches the original data. Examples ```python data = b\\"Python provides remarkable capabilities for data compression\\" # Compress and write to file compress_to_file(data, \\"example.bz2\\", compresslevel=5) # Read and decompress from file result = decompress_from_file(\\"example.bz2\\") assert data == result, \\"Decompressed data does not match the original!\\" ``` # Submission - Implement both functions ensuring they adhere to the specifications. - Thoroughly test your functions with different data inputs and compress level settings to validate correctness.","solution":"import bz2 def compress_to_file(data: bytes, filename: str, compresslevel: int = 9) -> None: Compress data using the specified compression level and write it to the file specified by filename. Args: - data (bytes): The data to be compressed. - filename (str): The name of the file to which the compressed data will be written. - compresslevel (int): The compression level (an integer between 1 and 9). Default is 9. Returns: None with bz2.BZ2File(filename, \'wb\', compresslevel=compresslevel) as f: f.write(data) def decompress_from_file(filename: str) -> bytes: Read compressed data from the file specified by filename and decompress it. Args: - filename (str): The name of the file from which compressed data will be read. Returns: bytes: The decompressed data. with bz2.BZ2File(filename, \'rb\') as f: return f.read()"},{"question":"# Seaborn Error Bar Analysis You are provided with a dataset containing information about the annual revenue of various companies over three years. Your task is to analyze the data and visualize it using seaborn, focusing on the use of different types of error bars to convey statistical information clearly. Dataset The dataset `company_revenue.csv` contains the following columns: - `Year`: Year of revenue data (`int`). - `Company`: Name of the company (`str`). - `Revenue`: Revenue amount in millions (`float`). Tasks 1. **Load and Inspect Data**: - Load the dataset and inspect the first few rows to understand its structure. 2. **Plot Average Revenue with Standard Deviation Error Bars**: - Create a seaborn point plot showing the average revenue for each year with standard deviation error bars. The x-axis should represent the Year, and the y-axis should represent the Revenue. 3. **Plot Average Revenue with Confidence Interval Error Bars**: - Create another point plot to display the average revenue for each year with 95% confidence interval error bars. 4. **Comparison of Error Bar Types**: - In a single plot, display both types of error bars (standard deviation and confidence interval) for comparison. You might use different colors or styles to distinguish them. 5. **Custom Error Bar Function**: - Implement a custom function for error bars that shows the range from the minimum to the maximum revenue for each year. Use this to plot the data. 6. **Analysis and Interpretation**: - Add brief annotations/comments in your code explaining the key observations from each plot, particularly about the variations and uncertainties in the revenue data as depicted by different error bars. # Constraints & Requirements - Use seaborn for all visualizations. - Ensure that the plots are clearly labeled, including axes titles and legends where necessary. - The custom error bar function should be a standalone python function that seaborn can call. # Input and Output Formats **Input**: `company_revenue.csv` **Expected Output**: Jupyter Notebook with code and visualizations that display the required plots along with annotations/comments. # Example ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Load the dataset df = pd.read_csv(\\"company_revenue.csv\\") # Task 1: Inspect the data print(df.head()) # Task 2: Average Revenue with Standard Deviation plt.figure(figsize=(10, 6)) sns.pointplot(x=\'Year\', y=\'Revenue\', data=df, errorbar=\'sd\') plt.title(\'Average Revenue with Standard Deviation Error Bars\') plt.show() # Task 3: Average Revenue with Confidence Interval plt.figure(figsize=(10, 6)) sns.pointplot(x=\'Year\', y=\'Revenue\', data=df, errorbar=\'ci\') plt.title(\'Average Revenue with 95% Confidence Interval Error Bars\') plt.show() # Task 4: Comparison of Error Bar Types plt.figure(figsize=(10, 6)) sns.pointplot(x=\'Year\', y=\'Revenue\', data=df, errorbar=\'sd\', color=\'blue\', label=\'Standard Deviation\') sns.pointplot(x=\'Year\', y=\'Revenue\', data=df, errorbar=\'ci\', color=\'red\', label=\'Confidence Interval\') plt.title(\'Comparison of Error Bars\') plt.legend() plt.show() # Task 5: Custom Error Bar Function def custom_error_bars(x): return x.min(), x.max() plt.figure(figsize=(10, 6)) sns.pointplot(x=\'Year\', y=\'Revenue\', data=df, errorbar=custom_error_bars) plt.title(\'Average Revenue with Custom Error Bars (Min to Max)\') plt.show() # Task 6: Analysis and Interpretation # Annotate the plots as necessary within the notebook. ```","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_and_inspect_data(file_path): df = pd.read_csv(file_path) return df def plot_avg_revenue_with_standard_deviation(df): plt.figure(figsize=(10, 6)) sns.pointplot(x=\'Year\', y=\'Revenue\', data=df, errorbar=\'sd\') plt.title(\'Average Revenue with Standard Deviation Error Bars\') plt.xlabel(\'Year\') plt.ylabel(\'Revenue (in millions)\') plt.show() def plot_avg_revenue_with_confidence_interval(df): plt.figure(figsize=(10, 6)) sns.pointplot(x=\'Year\', y=\'Revenue\', data=df, errorbar=\'ci\') plt.title(\'Average Revenue with 95% Confidence Interval Error Bars\') plt.xlabel(\'Year\') plt.ylabel(\'Revenue (in millions)\') plt.show() def plot_comparison_of_error_bars(df): plt.figure(figsize=(10, 6)) sns.pointplot(x=\'Year\', y=\'Revenue\', data=df, errorbar=\'sd\', color=\'blue\', label=\'Standard Deviation\') sns.pointplot(x=\'Year\', y=\'Revenue\', data=df, errorbar=\'ci\', color=\'red\', label=\'Confidence Interval\') plt.title(\'Comparison of Error Bars\') plt.xlabel(\'Year\') plt.ylabel(\'Revenue (in millions)\') plt.legend() plt.show() def custom_error_bars(x): return x.min(), x.max() def plot_with_custom_error_bars(df): plt.figure(figsize=(10, 6)) sns.pointplot(x=\'Year\', y=\'Revenue\', data=df, errorbar=custom_error_bars) plt.title(\'Average Revenue with Custom Error Bars (Min to Max)\') plt.xlabel(\'Year\') plt.ylabel(\'Revenue (in millions)\') plt.show() # Example Usage: # df = load_and_inspect_data(\'company_revenue.csv\') # plot_avg_revenue_with_standard_deviation(df) # plot_avg_revenue_with_confidence_interval(df) # plot_comparison_of_error_bars(df) # plot_with_custom_error_bars(df)"},{"question":"# Python Coding Assessment Question Objective: You are tasked with writing a Python script that automates the process of searching for, reading, and processing text files within a directory structure. The script should demonstrate competency in file handling, regular expressions, and command line argument processing. Requirements: 1. **Function Signature**: ```python def process_text_files(directory: str, word_to_search: str, num_lines: int = 5) -> Dict[str, List[str]]: ``` 2. **Parameters**: - `directory`: The directory path where the search for text files should begin. - `word_to_search`: A single word to search for within the text files. - `num_lines`: (Optional) The number of lines to read from each file where the word appears. Default is 5. 3. **Output**: - Return a dictionary where the keys are the filenames and the values are lists of strings. Each string in the list represents a line from the file containing the search word, up to `num_lines` lines. 4. **Behavior**: - The function should search the given directory and all subdirectories for `.txt` files. - For each `.txt` file found, it should read through the file and look for the occurrences of `word_to_search` using regular expressions. - If the word is found, up to `num_lines` subsequent lines (including the line with the word) should be captured and stored in the dictionary. - The dictionary should map filenames (relative paths) to the collected lines. 5. **Constraints**: - The directory search should be recursive. - Proper exception handling should be included to handle file reading errors gracefully. - Consider efficient reading and searching given potentially large files. Example: Given the following directory structure: ``` sample_dir/ file1.txt (contains the word \'test\' on lines 3, 8, and 15) file2.txt (does not contain the word) subdir/ file3.txt (contains the word \'test\' on lines 2 and 7) ``` For the call: ```python process_text_files(\'sample_dir\', \'test\', num_lines=3) ``` The function could return: ```python { \\"sample_dir/file1.txt\\": [\\"line 3: test\\", \\"line 4: ...\\", \\"line 5: ...\\", \\"line 8: test\\", \\"line 9: ...\\", \\"line 10: ...\\", \\"line 15: test\\", \\"line 16: ...\\", \\"line 17: ...\\"], \\"sample_dir/subdir/file3.txt\\": [\\"line 2: test\\", \\"line 3: ...\\", \\"line 4: ...\\", \\"line 7: test\\", \\"line 8: ...\\", \\"line 9: ...\\"] } ``` (Note: replace `...` with actual content or placeholders) Constraints: - The word search should be case-insensitive. - The function should be efficient with the use of memory and handle large files properly.","solution":"import os import re from typing import Dict, List def process_text_files(directory: str, word_to_search: str, num_lines: int = 5) -> Dict[str, List[str]]: Searches for text files within a directory, looking for a specific word and returns lines containing the word up to a specified number of lines. :param directory: The directory path where the search for text files should begin. :param word_to_search: A single word to search for within the text files. :param num_lines: The number of lines to read from each file where the word appears. Defaults to 5. :return: A dictionary mapping filenames to lists of lines containing the search word. result = {} word_pattern = re.compile(re.escape(word_to_search), re.IGNORECASE) for root, _, files in os.walk(directory): for file in files: if file.endswith(\'.txt\'): file_path = os.path.join(root, file) try: with open(file_path, \'r\', encoding=\'utf-8\') as f: lines = f.readlines() matched_lines = [] for i, line in enumerate(lines): if re.search(word_pattern, line): matched_lines.extend(lines[i:i + num_lines]) if matched_lines: result[os.path.relpath(file_path, directory)] = matched_lines[:num_lines] except Exception as e: print(f\\"Error reading file {file_path}: {e}\\") return result"},{"question":"# Network Server Monitoring Using Python\'s `select` Module Background You need to implement a network server capable of handling multiple client connections simultaneously. The objective is to use Python\'s `select` module to monitor multiple sockets for incoming data and manage I/O events efficiently. Task: 1. Write a Python class `MultiClientServer` that: - Initializes a TCP server socket. - Accepts multiple client connections. - Uses the `select.select()` call to manage these connections and handle I/O events. - Handles both reading from and writing to client sockets. - Closes inactive or disconnected client sockets. 2. Implement the following methods in the `MultiClientServer` class: - `__init__(self, host: str, port: int)`: Initializes the server socket and related configurations. - `start(self)`: Starts the server, accepts client connections, and uses `select.select()` to monitor multiple sockets. - `handle_readable(self, sock)`: Handles readable sockets (i.e., sockets ready for reading). - `handle_writable(self, sock)`: Handles writable sockets (i.e., sockets ready for writing). - `close_client(self, sock)`: Closes a client socket and removes it from the monitoring list. Input: - `host` and `port`: IP address and port number where the server will listen for incoming connections. Output: - Print debug statements indicating various stages of server activity (e.g., new client connection, data received, data sent, client disconnected). Example Usage: ```python if __name__ == \\"__main__\\": server = MultiClientServer(host=\\"127.0.0.1\\", port=12345) server.start() ``` Constraints: - Your solution must handle at least 100 concurrent connections without significant performance degradation. - Proper error handling is required for cases like connection drops, invalid data, etc. Notes: - Use `socket.socket()` to create the server socket. - Use `select.select()` for monitoring sockets for I/O events. - Ensure the server can gracefully shutdown using appropriate signal handling.","solution":"import socket import select class MultiClientServer: def __init__(self, host: str, port: int): self.host = host self.port = port self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) self.server_socket.bind((host, port)) self.server_socket.listen(100) self.server_socket.setblocking(False) self.inputs = [self.server_socket] self.outputs = [] self.message_queues = {} def start(self): print(f\\"Server started on {self.host}:{self.port}\\") try: while True: readable, writable, exceptional = select.select(self.inputs, self.outputs, self.inputs) for s in readable: if s is self.server_socket: self.handle_new_connection() else: self.handle_readable(s) for s in writable: self.handle_writable(s) for s in exceptional: self.close_client(s) except KeyboardInterrupt: print(\\"Server shutting down.\\") finally: self.server_socket.close() def handle_new_connection(self): client_socket, client_address = self.server_socket.accept() print(f\\"New connection from {client_address}\\") client_socket.setblocking(False) self.inputs.append(client_socket) self.message_queues[client_socket] = b\\"\\" def handle_readable(self, sock): data = sock.recv(1024) if data: print(f\\"Received {data} from {sock.getpeername()}\\") self.message_queues[sock] += data if sock not in self.outputs: self.outputs.append(sock) else: print(f\\"Closing connection to {sock.getpeername()}\\") self.close_client(sock) def handle_writable(self, sock): try: next_msg = self.message_queues[sock] except KeyError: self.outputs.remove(sock) else: if next_msg: print(f\\"Sending {next_msg} to {sock.getpeername()}\\") sent = sock.send(next_msg) self.message_queues[sock] = self.message_queues[sock][sent:] if not self.message_queues[sock]: self.outputs.remove(sock) def close_client(self, sock): if sock in self.outputs: self.outputs.remove(sock) if sock in self.inputs: self.inputs.remove(sock) sock.close() del self.message_queues[sock] if __name__ == \\"__main__\\": server = MultiClientServer(host=\\"127.0.0.1\\", port=12345) server.start()"},{"question":"<|Analysis Begin|> The provided documentation describes the Python Development Mode introduced in Python 3.7. This mode enhances debugging by adding various runtime checks, warnings, and logging that are not enabled by default. Specific effects include additional warning filters for various deprecation and resource warnings, enabling debug hooks for memory allocators, enabling fault handlers, and checking the encoding and errors arguments for string operations. The documentation also provides examples of typical issues the development mode can help debug, such as unclosed file handles and \\"Bad file descriptor\\" errors when closing a file descriptor multiple times. Relevant concepts from the document include: - Understanding different types of warnings introduced by Python Development Mode. - Importance of explicitly closing file descriptors. - Memory allocation and debugging methods. - Handling signals for crashes. The coverage of these topics suggests that a well-rounded question should test the students\' understanding of resource management, error handling, and deploying effective debugging practices in Python. <|Analysis End|> <|Question Begin|> # Python Development Mode Assessment Background Python Development Mode introduces various debugging and runtime checks to help developers identify and fix issues. This mode displays additional warnings, enables debug hooks on memory allocators, and much more. Problem Statement You are tasked with creating a Python function that processes a list of file paths. Each file contains string data, and you need to perform the following steps: 1. Read the content of each file while ensuring all files are properly closed. 2. Catch and handle any exceptions that arise during file operations. 3. Count the total occurrences of a specified word across all files. 4. Ensure that any potential ResourceWarnings or file descriptor issues are correctly addressed. Requirements 1. Implement the function `count_word_occurrences(file_paths, word)`. 2. Input: - `file_paths`: A list of strings, where each string is a valid file path. - `word`: A string representing the word to count. 3. Output: - Return an integer representing the total count of `word` across all files. 4. Constraints: - You must use effective error handling to manage file operations. - Ensure that all files are explicitly closed to avoid ResourceWarnings. - Handle potential memory allocation errors gracefully. - Use Python Development Mode to test your implementation and ensure no warnings or errors related to resource management are left unhandled. Example ```python # Assuming \'file1.txt\' contains \\"hello world\\" and \'file2.txt\' contains \\"hello Python\\" file_paths = [\'file1.txt\', \'file2.txt\'] word = \'hello\' print(count_word_occurrences(file_paths, word)) # Output should be 2 ``` Hints 1. Utilize context managers (`with` statements) to manage file operations. 2. Leverage try-except blocks to catch and handle exceptions. 3. Enable Python Development Mode by running your script via `python3 -X dev script.py` to identify any hidden issues. 4. Test your function with various inputs to validate its robustness. Evaluation Criteria - Correctness: The function should correctly count the occurrences of the word across all input files. - Resource Management: Ensure no ResourceWarnings are emitted in Python Development Mode. - Error Handling: Properly handle any exceptions that occur during file operations. # Submission Please submit your implementation of the `count_word_occurrences` function, along with any additional comments or documentation explaining your approach and any tests you performed.","solution":"def count_word_occurrences(file_paths, word): Counts the total occurrences of a specified word across a list of files. Args: - file_paths (list of str): List of file paths to read from. - word (str): Word to count occurrences of. Returns: - int: Total count of the word across all files. total_count = 0 for path in file_paths: try: with open(path, \'r\') as file: content = file.read() total_count += content.count(word) except Exception as e: print(f\\"An error occurred while processing file {path}: {e}\\") return total_count"},{"question":"# XML Document Manipulation with `xml.dom` Problem Statement You are given an XML document as a string, and you need to write a Python function to perform specific manipulations on this document using the `xml.dom` module. The function should: 1. **Parse** the given XML string into a DOM `Document` object. 2. **Create and insert** a new element at a specified location within the document. 3. **Modify** an attribute of an existing element. 4. **Search for** elements by tag name and return their text content. Requirements 1. **Parsing the XML String**: - Parse the XML string into a DOM `Document` object using the `xml.dom.minidom` parser. 2. **Creating and Inserting a New Element**: - Create a new element with a given tag name and text content. - Insert this new element as a child of an existing element identified by its tag name. If multiple elements with the given tag name exist, insert the new element into the first one only. 3. **Modifying an Attribute**: - Find an element by its tag name and modify one of its attributes to a new value. If multiple elements with the given tag name exist, modify the attribute in the first one only. 4. **Searching for Elements by Tag Name**: - Return a list of text contents of all elements with a specified tag name. Input and Output - **Input**: 1. `xml_str`: A string containing the XML document. 2. `parent_tag`: A string with the tag name of the parent element where a new child element should be inserted. 3. `new_tag`: A string with the tag name of the new element to be created. 4. `text_content`: A string with the text content of the new element. 5. `modify_tag`: A string with the tag name of the element whose attribute should be modified. 6. `attr_name`: A string with the name of the attribute to be modified. 7. `attr_value`: A string with the new value of the attribute. 8. `search_tag`: A string with the tag name of the elements whose text content should be returned. - **Output**: - A list of strings, each representing the text content of the matched elements for `search_tag`. Constraints - The XML string will always be well-formed. - All tag names and attribute names will be non-empty strings. - The specified elements and attributes for modification and insertion will always exist in the document. Example ```python xml_str = <root> <parent> <child>Original Child</child> </parent> <sibling> <child attr=\\"original\\">Sibiling Child</child> </sibling> </root> parent_tag = \\"parent\\" new_tag = \\"newChild\\" text_content = \\"New Child Text\\" modify_tag = \\"child\\" attr_name = \\"attr\\" attr_value = \\"modified\\" search_tag = \\"child\\" result = manipulate_xml(xml_str, parent_tag, new_tag, text_content, modify_tag, attr_name, attr_value, search_tag) print(result) # Output: [\'Original Child\', \'Sibiling Child\'] ``` Function Signature ```python def manipulate_xml(xml_str: str, parent_tag: str, new_tag: str, text_content: str, modify_tag: str, attr_name: str, attr_value: str, search_tag: str) -> list: pass ``` # Notes: - Use the `xml.dom.minidom` module as needed for parsing and manipulating the XML document. - Ensure your implementation properly handles the insertion and modification operations in the order specified.","solution":"from xml.dom.minidom import parseString, Document def manipulate_xml(xml_str: str, parent_tag: str, new_tag: str, text_content: str, modify_tag: str, attr_name: str, attr_value: str, search_tag: str) -> list: # Parse the given XML string into a DOM Document object dom = parseString(xml_str) # Create and insert a new element at the specified location parent_elements = dom.getElementsByTagName(parent_tag) if parent_elements: new_element = dom.createElement(new_tag) new_text_node = dom.createTextNode(text_content) new_element.appendChild(new_text_node) parent_elements[0].appendChild(new_element) # Modify an attribute of an existing element elements_to_modify = dom.getElementsByTagName(modify_tag) if elements_to_modify: elements_to_modify[0].setAttribute(attr_name, attr_value) # Search for elements by tag name and return their text content search_elements = dom.getElementsByTagName(search_tag) text_contents = [node.firstChild.data for node in search_elements if node.firstChild is not None] return text_contents"},{"question":"Using the `itertools` module, write a Python function `frequency_analysis` that performs frequency analysis on an input string. The function should return a dictionary where the keys are the characters from the input string and the values are the number of times each character appears. The characters in the output dictionary should be sorted in increasing lexicographic order. Function Signature ```python def frequency_analysis(input_string: str) -> dict: pass ``` # Input - `input_string` (a `str` containing only alphabetic characters (both lowercase and uppercase) and spaces, `1 <= len(input_string) <= 10^6`) # Output - A `dict` where: - The keys are characters from the input string. - The values are the counts of each character. # Constraints - The solution must handle large inputs efficiently. - Consider edge cases such as all characters being the same, the string being empty, and distinct characters. # Examples ```python >> frequency_analysis(\\"aabbcc\\") { \'a\': 2, \'b\': 2, \'c\': 2 } >> frequency_analysis(\\"hello world\\") { \' \': 1, \'d\': 1, \'e\': 1, \'h\': 1, \'l\': 3, \'o\': 2, \'r\': 1, \'w\': 1 } >> frequency_analysis(\\"It is a fine day\\") { \' \': 4, \'I\': 1, \'a\': 2, \'d\': 1, \'e\': 1, \'f\': 1, \'i\': 2, \'n\': 1, \'s\': 1, \'t\': 1, \'y\': 1 } ``` # Guidelines - You may use any functions from the `itertools` module. - Focus on creating an efficient solution. - Ensure the output dictionary keys are sorted lexicographically. # Hints - Consider using the `groupby` function from `itertools` to group characters in the sorted string. - You might find `chain` from `itertools` useful for combining iteration steps efficiently.","solution":"from collections import Counter def frequency_analysis(input_string: str) -> dict: Performs frequency analysis on the input string and returns a dictionary with the count of each character. The dictionary keys are sorted in lexicographical order. # Counter will count the frequency of each character counter = Counter(input_string) # Convert Counter to a dictionary and ensure it is sorted by keys sorted_counter = dict(sorted(counter.items())) return sorted_counter"},{"question":"# HTML Entity Conversion As part of a web scraping tool, you need to convert HTML entity references found in texts to their corresponding characters, and vice versa. Implement the following functionalities using the `html.entities` module: 1. **html_entities_to_unicode**: A function that takes a string containing HTML entity references (e.g., `\\"gt;\\" or \\"amp;\\"`) and returns the string with all HTML entities replaced by their respective Unicode characters. **Function Signature:** ```python def html_entities_to_unicode(s: str) -> str: pass ``` **Input:** - `s`: A string containing HTML entity references (e.g., `\\"Hello &amp; World!\\"`). **Output:** - A string with all HTML entities replaced by their respective Unicode characters (e.g., `\\"Hello & World!\\"`). 2. **unicode_to_html_entities**: A function that takes a string containing Unicode characters and returns the string with all Unicode characters replaced by their respective HTML entity references. **Function Signature:** ```python def unicode_to_html_entities(s: str) -> str: pass ``` **Input:** - `s`: A string containing Unicode characters (e.g., `\\"Hello & World!\\"`). **Output:** - A string with all Unicode characters replaced by their respective HTML entity references (e.g., `\\"Hello &amp; World!\\"`). # Constraints 1. You may assume that the input strings for both functions are well-formed. 2. Performance should be considered, especially for longer strings. **Example:** ```python # Example usage: # html_entities_to_unicode input_str = \\"Hello &amp; World!\\" result = html_entities_to_unicode(input_str) print(result) # Output: \\"Hello & World!\\" # unicode_to_html_entities input_str = \\"Hello & World!\\" result = unicode_to_html_entities(input_str) print(result) # Output: \\"Hello &amp; World!\\" ``` Implement these functions showcasing your understanding of dictionaries provided in the `html.entities` module.","solution":"import html def html_entities_to_unicode(s: str) -> str: Converts HTML entity references in a string to their respective Unicode characters. return html.unescape(s) def unicode_to_html_entities(s: str) -> str: Converts Unicode characters in a string to their respective HTML entity references. return html.escape(s)"},{"question":"**Python Coding Assessment Question** # Objective Create a class in Python that simulates some of the memory management operations found in the native C implementation of Python objects. # Problem Statement Your task is to create a custom class `ManagedObject` that handles its memory management by simulating object allocation, initialization, and deletion. # Requirements 1. **Class Definition**: - Define a class `ManagedObject` that has: - `__init__` method to initialize the object. - `__new__` method to allocate memory for the object. - `__del__` method to properly clean up when the object is deleted. 2. **Attributes**: - Each `ManagedObject` should have the following attributes: - `type`: A string describing the type of the object. - `size`: An integer representing the size of the object (simulating variable-size objects). - `data`: A list of values to simulate object storage. 3. **Methods**: - Implement a class method `new_instance(cls, obj_type, obj_size)` that creates and returns a new instance of `ManagedObject`. - Implement an instance method `initialize(self, values)` that initializes the `data` attribute with the provided list of values. - Implement an instance method `delete(self)` to simulate object deletion. # Constraints - The `obj_size` must be a positive integer. - The `values` list provided to `initialize` method should have length equal to `obj_size`. # Input and Output Specifications - The `new_instance` method takes a string and an integer and returns a new `ManagedObject` instance. - The `initialize` method takes a list of values and sets it to the `data` attribute of the instance. - The `delete` method should print a message indicating the object has been deleted. # Performance Requirements - The implementation should efficiently manage the initialization and deletion of objects without any memory leaks. # Example ```python class ManagedObject: pass # Example usage: obj = ManagedObject.new_instance(\\"example_type\\", 3) print(obj.type) # Output: example_type print(obj.size) # Output: 3 obj.initialize([1, 2, 3]) print(obj.data) # Output: [1, 2, 3] obj.delete() # Output: example_type object is deleted. ``` # Additional Information Consider proper memory management and handling the lifecycle of the objects responsibly within the constraints of Python\'s garbage collector.","solution":"class ManagedObject: def __new__(cls, *args, **kwargs): instance = super().__new__(cls) print(\\"Memory allocated for object\\") return instance def __init__(self, obj_type, obj_size): assert isinstance(obj_size, int) and obj_size > 0, \\"obj_size must be a positive integer\\" self.type = obj_type self.size = obj_size self.data = [None] * self.size def __del__(self): print(f\\"{self.type} object is deleted\\") @classmethod def new_instance(cls, obj_type, obj_size): return cls(obj_type, obj_size) def initialize(self, values): assert len(values) == self.size, \\"Length of values must be equal to obj_size\\" self.data = values def delete(self): print(f\\"{self.type} object is deleted\\") del self"},{"question":"Objective: Design a coding question that demonstrates understanding of the `BernoulliRBM` for unsupervised feature learning and its practical application in classification tasks. Problem Statement: You are provided with a binary classification dataset containing 64-dimensional binary data with labels (`0` or `1`). Your task is to implement feature extraction using `BernoulliRBM` and subsequently train a logistic regression model on the extracted features to classify the data. Input: - A binary classification dataset in `.csv` format where the first 64 columns represent the binary features and the last column represents the binary labels (`0` or `1`). Output: - Accuracy of the logistic regression classifier on the test dataset. Constraints: - The dataset should be split into 70% training and 30% testing. - Use `BernoulliRBM` from `sklearn` with the following parameters: - `n_components=100` (number of binary hidden units) - `learning_rate=0.01` - `n_iter=20` - Train a logistic regression model on the features extracted by the RBM. Requirements: 1. **Load Dataset**: Read the dataset from the `.csv` file. 2. **Preprocess Data**: Split the dataset into features and labels, and then into training and test sets. 3. **Feature Extraction using RBM**: Implement and fit `BernoulliRBM` on the training data. 4. **Train Logistic Regression**: Use the features extracted by RBM to train a logistic regression model. 5. **Evaluate Model**: Calculate and print the accuracy of the logistic regression model on the test set. Performance Requirement: - The logistic regression model should achieve at least 85% accuracy on the test data. Example: ```python import pandas as pd from sklearn.neural_network import BernoulliRBM from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # 1. Load Dataset data = pd.read_csv(\'binary_classification.csv\') X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # 2. Preprocess Data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # 3. Feature Extraction using RBM rbm = BernoulliRBM(n_components=100, learning_rate=0.01, n_iter=20, random_state=42) logistic = LogisticRegression(max_iter=1000, random_state=42) classifier = Pipeline(steps=[(\'rbm\', rbm), (\'logistic\', logistic)]) classifier.fit(X_train, y_train) # 4. Train Logistic Regression predictions = classifier.predict(X_test) # 5. Evaluate Model accuracy = accuracy_score(y_test, predictions) print(\\"Accuracy:\\", accuracy) ``` **Note:** Ensure that appropriate checks and validations are performed for the dataset and model training process.","solution":"import pandas as pd from sklearn.neural_network import BernoulliRBM from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def classify_with_rbm(csv_file): # Load Dataset data = pd.read_csv(csv_file) X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # Preprocess Data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Feature Extraction using RBM rbm = BernoulliRBM(n_components=100, learning_rate=0.01, n_iter=20, random_state=42) logistic = LogisticRegression(max_iter=1000, random_state=42) classifier = Pipeline(steps=[(\'rbm\', rbm), (\'logistic\', logistic)]) # Train the model classifier.fit(X_train, y_train) # Make predictions predictions = classifier.predict(X_test) # Evaluate Model accuracy = accuracy_score(y_test, predictions) return accuracy"},{"question":"**Question: Analyzing Diamond Dataset with Seaborn** Using the seaborn library, analyze the `diamonds` dataset by performing the following tasks. 1. **Histogram Creation**: - Plot a histogram of the `price` variable using logarithmic scale for the x-axis. - Ensure the bars are filled and have thin edges (the default). - Customize the histogram by setting the `binwidth` to `500`. 2. **Categorized Overlapping Bars**: - Modify the previous histogram to distinguish between different `cut` categories by color. - Use a stack transformation to resolve overlapping bars. 3. **Alpha Customization**: - Plot a second histogram based on the transformed data from the previous step. - Instead of color coding by `cut`, adjust the alpha transparency in relation to the `clarity` of the diamonds. 4. **Comparison of Subsets**: - Create a side-by-side bar plot to compare the `price` distribution of `Ideal` cut diamonds with diamonds of all other cuts. - Narrow the bars for the `Ideal` cut diamonds to avoid clutter. # Expected Input and Output Formats - **Input**: - The diamonds dataset will be loaded within the script. - **Output**: - Generate the following plots using seaborn: - A histogram of diamond prices with customized bin width. - A stacked bar plot showing prices by cut. - A histogram with alpha transparency mapped to clarity. - Comparison of price distributions with narrowed bars for a subset. # Constraints - You should use only seaborn and matplotlib for plotting. - Ensure visual clarity by properly labeling the axes and adding a legend. # Performance Requirements - Your code should be efficient and generate the plots within a reasonable time frame given the size of the diamonds dataset. **Hint**: Refer to seaborn\'s `objects.Plot` and related classes for histogram and bar plot drawing techniques.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_histogram_log_price(): Plots a histogram of the \'price\' variable from the diamonds dataset on a log scale. # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create the plot plt.figure(figsize=(10, 6)) sns.histplot(diamonds[\'price\'], log_scale=(False, True), binwidth=500, edgecolor=\\"black\\") plt.title(\'Histogram of Diamond Prices (Log Scale)\') plt.xlabel(\'Price ()\') plt.ylabel(\'Count\') plt.show() def plot_stacked_histogram_price_by_cut(): Plots a histogram of the \'price\' variable and colors by \'cut\', resolving overlapping bars. diamonds = sns.load_dataset(\\"diamonds\\") plt.figure(figsize=(10, 6)) sns.histplot(data=diamonds, x=\'price\', hue=\'cut\', element=\'step\', stat=\'count\', binwidth=500) plt.title(\'Histogram of Diamond Prices by Cut\') plt.xlabel(\'Price ()\') plt.ylabel(\'Count\') plt.show() def plot_histogram_price_by_alpha_clarity(): Plots a histogram of the \'price\' variable with alpha transparency related to \'clarity\'. diamonds = sns.load_dataset(\\"diamonds\\") plt.figure(figsize=(10, 6)) sns.histplot(data=diamonds, x=\'price\', element=\'step\', hue=\'clarity\', palette=\'coolwarm\', alpha=0.5, binwidth=500) plt.title(\'Histogram of Diamond Prices by Clarity (Alpha)\') plt.xlabel(\'Price ()\') plt.ylabel(\'Count\') plt.show() def plot_comparison_price_distribution(): Creates side-by-side bar plots for price distribution of \'Ideal\' cut and other diamonds. diamonds = sns.load_dataset(\\"diamonds\\") plt.figure(figsize=(10, 6)) sns.histplot(diamonds[diamonds[\'cut\'] == \'Ideal\'], x=\'price\', color=\'blue\', binwidth=500, kde=False, element=\'bars\', label=\'Ideal\', edgecolor=\'black\', alpha=0.65) sns.histplot(diamonds[diamonds[\'cut\'] != \'Ideal\'], x=\'price\', color=\'red\', binwidth=500, kde=False, element=\'bars\', label=\'Other\', edgecolor=\'black\', alpha=0.35) plt.title(\'Comparison of Price Distribution: Ideal vs Other Cuts\') plt.xlabel(\'Price ()\') plt.ylabel(\'Count\') plt.legend() plt.show()"},{"question":"**Title**: Implementing a Text Classification Pipeline with Scikit-learn **Objective**: Demonstrate the ability to preprocess text data, transform it into numerical features, and apply a machine learning model to classify the data using scikit-learn. **Problem Statement**: You are given a dataset containing text documents along with their associated labels. Your task is to build a machine learning pipeline that: 1. Preprocesses the text data. 2. Converts the text data into numerical features using `TfidfVectorizer`. 3. Trains a suitable classification model. 4. Evaluates the model on the given test dataset. **Input Format**: - A list of text documents (`train_texts`) for training. - A list of labels (`train_labels`) corresponding to the training texts. - A list of text documents (`test_texts`) for testing. - A list of labels (`test_labels`) corresponding to the test texts. **Output Format**: - Print the accuracy of the model on the test dataset. **Constraints**: 1. You should use `TfidfVectorizer` for text vectorization. 2. You should use a `LogisticRegression` model for classification. 3. You are free to perform any necessary text preprocessing (e.g., removing punctuation, lowercasing). **Performance Requirements**: - The pipeline should be efficient in terms of memory and computation. # Example: ```python from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score # Example input data train_texts = [\\"This is the first document.\\", \\"This is the second document.\\", \\"And the third one.\\"] train_labels = [0, 1, 0] test_texts = [\\"Is this the first document?\\", \\"The second document is this one.\\"] test_labels = [0, 1] # Expected output (example; actual may vary based on model performance) # Accuracy: 1.0 (if the model classifies all test samples correctly) # Your implementation here # ..... ``` **Task**: 1. Implement the text preprocessing function. 2. Implement a pipeline using `TfidfVectorizer` and `LogisticRegression`. 3. Train the pipeline on the training dataset. 4. Evaluate the pipeline on the test dataset and print the accuracy. **Note**: Ensure you handle any necessary text preprocessing to improve model performance.","solution":"from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score import string def preprocess_text(text): Preprocess the input text by removing punctuation and lowercasing. text = text.lower() # lowercasing text = text.translate(str.maketrans(\\"\\", \\"\\", string.punctuation)) # remove punctuation return text def build_and_evaluate_pipeline(train_texts, train_labels, test_texts, test_labels): # Preprocess the text data train_texts = [preprocess_text(text) for text in train_texts] test_texts = [preprocess_text(text) for text in test_texts] # Create the pipeline pipeline = Pipeline([ (\'tfidf\', TfidfVectorizer()), (\'clf\', LogisticRegression()) ]) # Train the pipeline pipeline.fit(train_texts, train_labels) # Predict on the test data predictions = pipeline.predict(test_texts) # Evaluate the pipeline accuracy = accuracy_score(test_labels, predictions) print(f\'Accuracy: {accuracy}\')"},{"question":"Objective: Demonstrate your understanding of seaborn\'s `blend_palette` function by creating a comprehensive visualization, using blended color palettes. Problem Statement: You are provided with a dataset that contains information about the performance metrics of different categories over time. Your task is to create a line plot that visually differentiates the performance metrics of each category using seaborn\'s `blend_palette`. Input: 1. A CSV file named `performance_metrics.csv` with the following columns: - `Category`: This represents different categories. - `Time`: This represents the time period. - `Metric`: This represents the performance metrics. Output: - A visualization (line plot) displaying the performance metrics over time for each category, using different blended color palettes for each category. Ensure that: - Each category has a unique color palette. - The lines are smooth and aesthetic. - Include appropriate labels and legends for clarity. Constraints: - You should use seaborn\'s `blend_palette` function to generate the color palettes. - The color palettes should be continuous, using the `as_cmap=True` option. - Ensure that each category\'s line is clearly distinguishable. Example: For example, given the following `performance_metrics.csv`: ``` Category,Time,Metric A,1,100 A,2,150 A,3,200 B,1,80 B,2,130 B,3,170 C,1,90 C,2,140 C,3,180 ``` The output should be a line plot where: - Category A\'s metric is plotted over time with one color palette. - Category B\'s metric is plotted over time with a different color palette. - Category C\'s metric is plotted over time with yet another color palette, and so on. Requirements: 1. Load the dataset. 2. Create a unique color palette for each category using `blend_palette`. 3. Plot the performance metrics with seaborn, using the blended color palettes. 4. Ensure that the plot is clear, well-labeled, and accurately represents the data. Performance Considerations: - Ensure that the visualization is performant and can handle large datasets efficiently. # Instructions: Write a Python function `visualize_performance_metrics(file_path)` that takes the file path of the CSV file as an argument and generates the required visualization. Make sure to utilize the seaborn library for this task. Additional Notes: - This task is designed to test your ability to work with seaborn for data visualization and to manipulate color palettes effectively. - Make sure your code is clean, well-commented, and follows best practices.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_performance_metrics(file_path): # Load the dataset df = pd.read_csv(file_path) # Get the unique categories categories = df[\'Category\'].unique() # Create a unique blend palette for each category palettes = {} base_colors = sns.color_palette(\\"hsv\\", len(categories)) for i, category in enumerate(categories): palettes[category] = sns.blend_palette([base_colors[i], \\"white\\"], as_cmap=True) # Start plotting plt.figure(figsize=(12, 8)) for category in categories: subset = df[df[\'Category\'] == category] sns.lineplot( data=subset, x=\'Time\', y=\'Metric\', palette=palettes[category], label=category, linewidth=2.5 ) # Add labels and title plt.xlabel(\\"Time\\") plt.ylabel(\\"Metric\\") plt.title(\\"Performance Metrics Over Time by Category\\") plt.legend(title=\\"Category\\") plt.grid(True) # Show plot plt.show()"},{"question":"**Coding Assessment Question:** **Objective:** Design and implement an out-of-core learning system using scikit-learn for a text classification problem. The goal is to classify text data from a simulated streaming source using incremental learning. **Problem Statement:** You are provided with a dataset of text documents and their corresponding labels. However, the dataset is too large to fit into the memory all at once. Your task is to build a text classification model using an out-of-core learning approach. **Requirements:** 1. Stream the text documents from a source. 2. Extract features using a suitable feature extraction method. 3. Use an incremental learning algorithm for classification. **Input:** - A generator function `stream_data(file_path, chunk_size)` that simulates streaming of text data from a large file. This function yields chunks of data (list of text documents) and labels (list of corresponding labels). - `file_path` (string): Path to the text data file. - `chunk_size` (int): The number of records to return at each step of the generator. **Output:** - Final accuracy score of the model on the provided test set using `accuracy_score`. **Constraints:** - Use `HashingVectorizer` for feature extraction. - Choose an appropriate incremental learning classifier that supports the `partial_fit` method from scikit-learn. - Use mini-batch size of 1000 for `partial_fit` updates. **Function Signature:** ```python def stream_data(file_path: str, chunk_size: int): # This is a generator function that yields chunks of `chunk_size` text data and labels. pass def incremental_text_classifier(file_path: str, test_data: List[str], test_labels: List[int]) -> float: from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score # Implement the out-of-core learning system here pass ``` **Example Usage:** ```python # Example generator function for streaming data def stream_data(file_path, chunk_size): ... # read the file and yield data in chunks # file_path: Path to the training data file_path = \'path/to/large_text_data.txt\' # test_data: List of text documents for testing the model test_data = [\\"This is a sample text.\\", \\"Another example document.\\"] # test_labels: Corresponding labels for the test data test_labels = [0, 1] # Run the incremental text classifier accuracy = incremental_text_classifier(file_path, test_data, test_labels) print(f\\"Model accuracy: {accuracy}\\") ``` **Notes:** - Ensure to handle the generator function properly to simulate streaming data from a large file. - Pass all possible classes to the first `partial_fit` call using the `classes=` parameter. - Evaluate the model on a separate test set after training is complete. **Challenge:** Modify the mini-batch size between 100 and 10000 in the implementation and observe the impact on the classification accuracy and training time. Report your findings.","solution":"from typing import List from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def stream_data(file_path: str, chunk_size: int): with open(file_path, \'r\') as file: data = [] labels = [] for line in file: text, label = line.rsplit(\\",\\", 1) data.append(text) labels.append(int(label.strip())) if len(data) == chunk_size: yield data, labels data = [] labels = [] if data: yield data, labels def incremental_text_classifier(file_path: str, test_data: List[str], test_labels: List[int]) -> float: vectorizer = HashingVectorizer(stop_words=\'english\') classifier = SGDClassifier(max_iter=5) classes = [0, 1] # Assuming we have binary classification with classes 0 and 1 chunk_size = 1000 # Using mini-batch size of 1000 for data_chunk, labels_chunk in stream_data(file_path, chunk_size): X_chunk = vectorizer.transform(data_chunk) classifier.partial_fit(X_chunk, labels_chunk, classes=classes) # Transform test data X_test = vectorizer.transform(test_data) # Predict and calculate accuracy predictions = classifier.predict(X_test) accuracy = accuracy_score(test_labels, predictions) return accuracy"},{"question":"# **Assessment Question: Advanced Exception Handling in Python** **Objective:** Create a script to demonstrate your understanding of built-in exceptions, custom exception classes, error handling, and exception chaining in Python. **Problem Statement:** You are required to implement a banking system simulation with the following requirements: 1. **Custom Exceptions**: Define custom exceptions for different error conditions: - `InsufficientFundsError`: Raised when an attempt is made to withdraw more money than the available balance. - `InvalidTransactionError`: Raised when a transaction type is not recognized (valid types are \\"deposit\\" and \\"withdraw\\"). - `InactiveAccountError`: Raised when an attempt is made to execute a transaction on an inactive account. 2. **Bank Account Class**: Implement a class `BankAccount` with the following attributes and methods: - `account_number`: A unique account number for the account (string). - `balance`: Current balance in the account (float). - `active`: A boolean indicating whether the account is active. - `deposit(amount)`: Method to deposit a specified `amount` into the account. Raises `InvalidTransactionError` if the amount is not positive. - `withdraw(amount)`: Method to withdraw a specified `amount` from the account. Raises `InsufficientFundsError` if `amount` is greater than the current balance, and raises `InactiveAccountError` if the account is not active. - `check_balance()`: Method to return the current balance. - `deactivate()`: Method to deactivate the account. - `activate()`: Method to activate the account. 3. **Transaction Handling**: Implement a function `handle_transaction(account, transaction_type, amount)` that handles transactions on a `BankAccount` instance. This function should: - Validate the `transaction_type`. If the transaction type is invalid, raise an `InvalidTransactionError`. - Perform the transaction (`deposit` or `withdraw`). Use a `try-except` block to handle errors, and use exception chaining to provide meaningful error messages. For example, if an attempt is made to withdraw money from an inactive account, the error message should indicate the `InactiveAccountError` caused the `InsufficientFundsError`. **Input:** The function `handle_transaction` takes the following parameters: - `account`: An instance of `BankAccount`. - `transaction_type`: A string representing the type of transaction (\\"deposit\\" or \\"withdraw\\"). - `amount`: A float representing the amount for the transaction. **Output:** The function `handle_transaction` should not return anything but should provide meaningful print statements for the following cases: - Success: Print \\"Transaction successful.\\" and the new balance. - Error: Print a detailed error message, including chained exceptions if applicable. **Constraints:** - The initial balance can be any non-negative float. - The account number is a mandatory string when an account is created. - The account is initially active. **Example Usage:** ```python # Initialize account account = BankAccount(\\"12345678\\", 1000.0) try: account.deposit(500.0) account.withdraw(100.0) handle_transaction(account, \\"withdraw\\", 1600.0) except Exception as e: print(f\\"Error: {e}\\") try: handle_transaction(account, \\"deactivate\\", 100.0) except Exception as e: print(f\\"Error: {e}\\") ``` **Assumptions:** - You can assume that the input will be well-formed and you do not need to handle input through command-line interface or user prompts. **Note:** - Make sure to chain exceptions meaningfully and demonstrate handling multiple layers of exceptions. - Ensure that your code is clean, well-documented, and follows good programming practices. **Evaluation Criteria:** - Correct implementation of custom exceptions and their usage. - Proper error handling and exception chaining. - Code readability and adherence to Pythonic conventions.","solution":"class InsufficientFundsError(Exception): pass class InvalidTransactionError(Exception): pass class InactiveAccountError(Exception): pass class BankAccount: def __init__(self, account_number, balance=0.0): self.account_number = account_number self.balance = balance self.active = True def deposit(self, amount): if amount <= 0: raise InvalidTransactionError(f\\"Cannot deposit non-positive amount: {amount}\\") if not self.active: raise InactiveAccountError(\\"Cannot deposit to an inactive account\\") self.balance += amount print(f\\"Deposited {amount}. New balance is {self.balance}.\\") def withdraw(self, amount): if not self.active: raise InactiveAccountError(\\"Cannot withdraw from an inactive account\\") if amount > self.balance: raise InsufficientFundsError(f\\"Cannot withdraw {amount}. Insufficient funds. Current balance is {self.balance}.\\") self.balance -= amount print(f\\"Withdrew {amount}. New balance is {self.balance}.\\") def check_balance(self): return self.balance def deactivate(self): self.active = False print(\\"Account deactivated.\\") def activate(self): self.active = True print(\\"Account activated.\\") def handle_transaction(account, transaction_type, amount): if transaction_type not in [\\"deposit\\", \\"withdraw\\"]: raise InvalidTransactionError(f\\"Unknown transaction type: {transaction_type}\\") try: if transaction_type == \\"deposit\\": account.deposit(amount) elif transaction_type == \\"withdraw\\": account.withdraw(amount) print(\\"Transaction successful.\\") except (InvalidTransactionError, InactiveAccountError, InsufficientFundsError) as e: chain_exception = e if not account.active: try: raise InactiveAccountError(\\"Inactive account detected during transaction\\") from e except InactiveAccountError as chained_e: chain_exception = chained_e print(f\\"Transaction failed: {chain_exception}\\")"},{"question":"# Advanced Python Programming Question: Utilizing Coroutine Objects Objective: Your task is to demonstrate an understanding of Python coroutine objects by implementing a function that performs asynchronous tasks using coroutines. Problem Statement: Write a Python function `fetch_and_process_data(urls)` that takes a list of URLs as input and performs the following operations asynchronously: 1. Fetch data from each URL (simulate data fetch with `asyncio.sleep` and return a string \\"Data from URL\\"). 2. Process the fetched data (simulate processing with `asyncio.sleep` and return the length of the data string). The function should return a dictionary where each URL is a key, and the corresponding value is the length of the data fetched and processed from that URL. Input: - `urls` (List[str]): A list of URL strings. Example: `[\\"https://example.com\\", \\"https://example.org\\"]` Output: - Dictionary where each key is a URL string and the value is an integer representing the length of the processed data string. Constraints: - Use Pythons `async` and `await` syntax for creating and handling coroutines. - You may use the `asyncio` library to manage asynchronous operations. - Assume up to 10 URLs in the input list. - Each simulated fetch or process operation should use an `asyncio.sleep` of 1 second. Example: ```python import asyncio async def fetch_and_process_data(urls): async def fetch_data(url): await asyncio.sleep(1) return f\\"Data from {url}\\" async def process_data(data): await asyncio.sleep(1) return len(data) async def handle_url(url): data = await fetch_data(url) processed_length = await process_data(data) return url, processed_length tasks = [handle_url(url) for url in urls] results = await asyncio.gather(*tasks) return dict(results) # Example usage urls = [\\"https://example.com\\", \\"https://example.org\\"] result = asyncio.run(fetch_and_process_data(urls)) print(result) # Expected: {\'https://example.com\': 19, \'https://example.org\': 19} ``` Note that the exact length of the processed data string will vary based on the actual implementation of the simulated fetch operation. Performance Requirement: - Ensure that the function handles asynchronous operations concurrently to improve performance. Implement the function and test it using the example provided.","solution":"import asyncio async def fetch_and_process_data(urls): async def fetch_data(url): await asyncio.sleep(1) return f\\"Data from {url}\\" async def process_data(data): await asyncio.sleep(1) return len(data) async def handle_url(url): data = await fetch_data(url) processed_length = await process_data(data) return url, processed_length tasks = [handle_url(url) for url in urls] results = await asyncio.gather(*tasks) return dict(results)"},{"question":"Objective Demonstrate your understanding of scikit-learn\'s dataset transformations by implementing a pipeline that preprocesses a given dataset. Problem Statement You are given a dataset consisting of numerical features and a categorical target variable. Your task is to perform the following preprocessing steps using scikit-learn transformers and create a pipeline that applies these transformations in sequence: 1. **Standardize** the numerical features (i.e., scale the features to have zero mean and unit variance). 2. **Label Encode** the categorical target variable (i.e., convert the categorical labels to integers). Implement the function `preprocess_data` that takes the following inputs: - `X_train`: A 2D numpy array of shape (n_samples, n_features) representing the training data. - `y_train`: A 1D numpy array of shape (n_samples,) representing the training labels. - `X_test`: A 2D numpy array of shape (m_samples, n_features) representing the test data. The function should return the following outputs: - `X_train_transformed`: The transformed training data after applying the standard scaling. - `y_train_transformed`: The transformed training labels after applying label encoding. - `X_test_transformed`: The transformed test data after applying the standard scaling learned from the training data. Constraints and Requirements - You must use scikit-learn transformers to perform the standard scaling and label encoding. - Do not use any other machine learning or data preprocessing libraries. - Ensure that the scaling parameters are learned only from the training data and applied to both the training and test data. - The implementation should be efficient and follow best practices for scikit-learn. Function Signature ```python import numpy as np def preprocess_data(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray): # Your implementation here pass ``` Example Usage ```python from sklearn.preprocessing import StandardScaler, LabelEncoder # Sample data X_train = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]]) y_train = np.array([\'cat\', \'dog\', \'cat\']) X_test = np.array([[4.0, 5.0], [5.0, 6.0]]) # Expected outputs X_train_transformed = np.array([[-1.22474487, -1.22474487], [0., 0.], [1.22474487, 1.22474487]]) y_train_transformed = np.array([0, 1, 0]) X_test_transformed = np.array([[2.44948974, 2.44948974], [3.67423461, 3.67423461]]) # Calling the function result = preprocess_data(X_train, y_train, X_test) assert np.allclose(result[0], X_train_transformed) assert np.array_equal(result[1], y_train_transformed) assert np.allclose(result[2], X_test_transformed) ``` Your task is to complete the `preprocess_data` function to meet the example usage provided.","solution":"import numpy as np from sklearn.preprocessing import StandardScaler, LabelEncoder def preprocess_data(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray): Perform standard scaling on numerical features and label encoding on categorical target. Parameters: - X_train: 2D numpy array of shape (n_samples, n_features) representing the training data. - y_train: 1D numpy array of shape (n_samples,) representing the training labels. - X_test: 2D numpy array of shape (m_samples, n_features) representing the test data. Returns: - X_train_transformed: Transformed training data (scaled). - y_train_transformed: Transformed training labels (encoded). - X_test_transformed: Transformed test data (scaled). # Standardize numerical features scaler = StandardScaler() X_train_transformed = scaler.fit_transform(X_train) X_test_transformed = scaler.transform(X_test) # Label Encode target variable label_encoder = LabelEncoder() y_train_transformed = label_encoder.fit_transform(y_train) return X_train_transformed, y_train_transformed, X_test_transformed"},{"question":"**Context:** The `ossaudiodev` module grants access to the OSS (Open Sound System) audio interface. This module, although deprecated in Python 3.11, is still operational in previous versions like Python 3.10. It includes various functionalities for managing audio and mixer devices. **Task:** Implement a function `configure_and_play_audio` in Python 3.10 using the `ossaudiodev` module. This function will: 1. Open an audio device for writing. 2. Set up the device parameters for audio playback. 3. Play an audio buffer provided as input. 4. Ensure synchronization and proper closure of the device after playback. **Function Signature:** ```python def configure_and_play_audio( device: str, fmt: int, channels: int, rate: int, audio_data: bytes ) -> None: pass ``` **Parameters:** - `device` (str): The audio device filename to use. If `None`, defaults to `\\"/dev/dsp\\"`. - `fmt` (int): The format of audio samples (e.g., `AFMT_S16_LE`). - `channels` (int): The number of audio channels (e.g., 1 for mono, 2 for stereo). - `rate` (int): The sampling rate in samples per second (e.g., 44100 for CD quality). - `audio_data` (bytes): The audio data to be played. **Constraints:** - If opening the audio device fails, raise an `OSSAudioError`. - Use the `ossaudiodev.open()` method to open the audio device. - Use the `setparameters()` method to configure the audio device. - Use the `writeall()` method to play the audio data. - Ensure device synchronization with `sync()` after writing audio data. - Close the device properly at the end to release resources. **Example:** ```python audio_data = ... try: configure_and_play_audio(\\"/dev/dsp\\", AFMT_S16_LE, 2, 44100, audio_data) except ossaudiodev.OSSAudioError as e: print(f\\"Audio playback failed: {e}\\") ``` **Notes:** - Ensure to import the `ossaudiodev` module and any necessary constants. - Use exception handling where needed to manage errors that might occur during device manipulation. - Pay attention to setting the audio parameters correctly to match the input specifications. By solving this task, students will demonstrate their ability to work with the `ossaudiodev` module, handle audio devices, and manage exceptions appropriately.","solution":"import ossaudiodev def configure_and_play_audio( device: str = \'/dev/dsp\', fmt: int = ossaudiodev.AFMT_S16_LE, channels: int = 2, rate: int = 44100, audio_data: bytes = None ) -> None: Configures the audio device with the specified parameters and plays the provided audio data. :param device: The audio device filename to use. :param fmt: The format of audio samples. :param channels: The number of audio channels. :param rate: The sampling rate in samples per second. :param audio_data: The audio data to be played. :raises ossaudiodev.OSSAudioError: If opening the audio device fails. try: dsp = ossaudiodev.open(\'w\') except OSError as e: raise ossaudiodev.OSSAudioError(f\\"Failed to open audio device {device}\\") from e try: # Set parameters for audio playback dsp.setparameters(fmt, channels, rate) # Write all audio data to the device dsp.writeall(audio_data) # Ensure all audio data is played dsp.sync() finally: dsp.close()"},{"question":"**Question: Handling and Disallowing Duplicate Labels with Pandas** You are given a DataFrame that may contain duplicate row or column labels. Your task is to write a function `deduplicate_and_disallow` that processes this DataFrame, removing duplicates and configuring it to disallow duplicate labels in further operations. # Function Signature ```python def deduplicate_and_disallow(df: pd.DataFrame) -> pd.DataFrame: pass ``` # Input - `df`: A pandas DataFrame, which may have duplicate row or column labels. # Output - Returns a new pandas DataFrame with the following characteristics: 1. Duplicate row and column labels are removed by keeping the first occurrence. 2. Restricts any further operations from introducing duplicate labels. # Constraints - You must use `groupby` and `set_flags` methods as shown in the documentation. - Assume that the DataFrame has at least one column and one row. # Example ```python import pandas as pd data = {\'A\': [1, 2, 3], \'B\': [4, 5, 6]} df = pd.DataFrame(data, index=[\'a\', \'a\', \'b\']) print(df) # Output: # A B # a 1 4 # a 2 5 # b 3 6 result = deduplicate_and_disallow(df) print(result) # Expected Output: # A B # a 1 4 # b 3 6 # This should raise a DuplicateLabelError try: result.loc[\'c\'] = [7, 8] result = result.rename(index={\'a\': \'b\'}) except Exception as e: print(type(e).__name__) # Expected Output: # DuplicateLabelError ``` # Explanation - In the example, the input DataFrame has duplicate row labels (\'a\'). After processing, the function returns a DataFrame with the first occurrence of each duplicate row and disallows any future duplicates.","solution":"import pandas as pd def deduplicate_and_disallow(df: pd.DataFrame) -> pd.DataFrame: Processes the DataFrame to remove duplicates and configures it to disallow duplicate labels. Parameters: df (pd.DataFrame): Input DataFrame which may have duplicate rows or column labels. Returns: pd.DataFrame: A DataFrame with duplicates removed and restrictions on future duplicate labels. # Remove duplicate rows based on row labels, keeping the first occurrence deduped_index_df = df[~df.index.duplicated(keep=\'first\')] # Remove duplicate columns based on column labels, keeping the first occurrence deduped_df = deduped_index_df.loc[:,~deduped_index_df.columns.duplicated(keep=\'first\')] # Set flags to disallow future duplicate labels deduped_df = deduped_df.set_flags(allows_duplicate_labels=False) return deduped_df"},{"question":"You are tasked with building and evaluating a multi-layer perceptron (MLP) model using scikit-learn. Your task is to implement a function that trains an MLP model for both classification and regression tasks based on input parameters. The function should handle data preprocessing, model training, and evaluation. # Function Signature ```python def train_and_evaluate_mlp(X_train, y_train, X_test, y_test, task_type=\'classification\', hidden_layer_sizes=(100,), alpha=0.0001, random_state=None): Trains and evaluates an MLP model using scikit-learn. Parameters: X_train (list of list of floats): Training features. y_train (list or list of lists): Training labels for classification or target values for regression. X_test (list of list of floats): Test features. y_test (list or list of lists): Test labels for classification or target values for regression. task_type (str): Type of task - either \'classification\' or \'regression\', defaults to \'classification\'. hidden_layer_sizes (tuple): The ith element represents the number of neurons in the ith hidden layer, defaults to (100,). alpha (float): L2 penalty (regularization term) parameter, defaults to 0.0001. random_state (int or None): Seed for random number generator, defaults to None. Returns: dict: A dictionary with keys \'model\', \'train_score\', \'test_score\', and \'test_predictions\'. \'model\' is the trained MLP model, \'train_score\' and \'test_score\' are the scores on training and test data, and \'test_predictions\' are the predictions on the test data. pass ``` # Requirements 1. **Preprocessing**: - Scale the features to have mean 0 and variance 1. 2. **Model Selection**: - Use `MLPClassifier` for classification tasks. - Use `MLPRegressor` for regression tasks. 3. **Training**: - Train the model using the provided training data. 4. **Evaluation**: - Evaluate the model on the training and testing data. - Return the model, training score, testing score, and predictions on the test data. # Constraints 1. For MLP models: - Use `\'adam\'` solver. - Use default values for other parameters unless specified. 2. Ensure proper handling of input data types and structures. # Example ```python X_train = [[0.0, 0.0], [1.0, 1.0], [0.5, 0.5], [0.2, 0.8], [0.9, 0.3]] y_train = [0, 1, 0, 1, 1] X_test = [[0.1, 0.1], [0.6, 0.6]] y_test = [0, 1] result = train_and_evaluate_mlp(X_train, y_train, X_test, y_test, task_type=\'classification\', hidden_layer_sizes=(5,), alpha=0.001, random_state=42) print(result) ``` Expected output: ```python { \'model\': <sklearn.neural_network._multilayer_perceptron.MLPClassifier object at 0x...>, \'train_score\': 1.0, \'test_score\': 1.0, \'test_predictions\': array([0, 1]) } ``` # Notes - Ensure that the function is robust and handles edge cases properly. - Document your code clearly. - You may assume that the input data is always valid and correctly formatted.","solution":"from sklearn.neural_network import MLPClassifier, MLPRegressor from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score, mean_squared_error def train_and_evaluate_mlp(X_train, y_train, X_test, y_test, task_type=\'classification\', hidden_layer_sizes=(100,), alpha=0.0001, random_state=None): Trains and evaluates an MLP model using scikit-learn. Parameters: X_train (list of list of floats): Training features. y_train (list or list of lists): Training labels for classification or target values for regression. X_test (list of list of floats): Test features. y_test (list or list of lists): Test labels for classification or target values for regression. task_type (str): Type of task - either \'classification\' or \'regression\', defaults to \'classification\'. hidden_layer_sizes (tuple): The ith element represents the number of neurons in the ith hidden layer, defaults to (100,). alpha (float): L2 penalty (regularization term) parameter, defaults to 0.0001. random_state (int or None): Seed for random number generator, defaults to None. Returns: dict: A dictionary with keys \'model\', \'train_score\', \'test_score\', and \'test_predictions\'. \'model\' is the trained MLP model, \'train_score\' and \'test_score\' are the scores on training and test data, and \'test_predictions\' are the predictions on the test data. # Standardizing the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) if task_type == \'classification\': model = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, alpha=alpha, random_state=random_state, solver=\'adam\') elif task_type == \'regression\': model = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, alpha=alpha, random_state=random_state, solver=\'adam\') else: raise ValueError(\\"task_type should be either \'classification\' or \'regression\'\\") # Train the model model.fit(X_train_scaled, y_train) # Evaluate the model train_predictions = model.predict(X_train_scaled) test_predictions = model.predict(X_test_scaled) if task_type == \'classification\': train_score = accuracy_score(y_train, train_predictions) test_score = accuracy_score(y_test, test_predictions) elif task_type == \'regression\': train_score = mean_squared_error(y_train, train_predictions) test_score = mean_squared_error(y_test, test_predictions) return { \'model\': model, \'train_score\': train_score, \'test_score\': test_score, \'test_predictions\': test_predictions }"},{"question":"# Question: Implementing a File Inclusion Command **Context:** You are tasked with implementing a function that simulates the `recursive-include` command from the setuptools documentation provided. This command includes all files under a specified directory that match any of the given patterns. Unix-style glob patterns are used for matching file names. **Function Specification:** **Function Name:** `recursive_include` **Input:** - `directory` (str): The root directory to search for files. - `patterns` (List[str]): A list of Unix-style glob patterns to match file names. **Output:** - List[str]: A list of file paths that match any of the given patterns. **Constraints:** - The function should search directories recursively. - It should handle patterns like `*`, `?`, and `[range]` appropriately, as described in the documentation. - The function should ignore directories and only return file paths. - Consider edge cases such as an empty directory, no matching files, and invalid input. **Performance requirements:** - The function should be efficient enough to handle large directory trees with thousands of files. **Sample Input:** ```python directory = \\"/path/to/dir\\" patterns = [\\"*.py\\", \\"*.txt\\", \\"data[0-9].csv\\"] ``` **Sample Output:** ```python [ \\"/path/to/dir/script.py\\", \\"/path/to/dir/readme.txt\\", \\"/path/to/dir/data1.csv\\", ] ``` **Notes:** - Do not use any external libraries such as `glob`. - Use `os` and `fnmatch` modules from the Python standard library to implement the function. ```python import os import fnmatch def recursive_include(directory: str, patterns: List[str]) -> List[str]: matching_files = [] for root, dirs, files in os.walk(directory): for pattern in patterns: for filename in fnmatch.filter(files, pattern): matching_files.append(os.path.join(root, filename)) return matching_files ``` **Explanation:** 1. The function `recursive_include` takes a root directory and a list of patterns as input. 2. It recursively walks through the directory and subdirectories. 3. For each file, it checks if the file name matches any of the given patterns using `fnmatch.filter`. 4. If a match is found, the full file path is appended to the result list. 5. The function returns the list of matching file paths.","solution":"import os import fnmatch from typing import List def recursive_include(directory: str, patterns: List[str]) -> List[str]: Recursively includes all files under a specified directory that match any of the given patterns. Parameters: - directory (str): The root directory to search for files. - patterns (List[str]): A list of Unix-style glob patterns to match file names. Returns: - List[str]: A list of file paths that match any of the given patterns. matching_files = [] for root, dirs, files in os.walk(directory): for pattern in patterns: for filename in fnmatch.filter(files, pattern): matching_files.append(os.path.join(root, filename)) return matching_files"},{"question":"# Custom Python Initialization and Configuration You are required to write a Python script that uses C-level constructs in Python to initialize a customized Python interpreter. You need to do the following: 1. Use `PyPreConfig` to preinitialize Python with the following settings: - Enable UTF-8 mode. - Use default memory allocators. 2. Use `PyConfig` to further configure Python with the following settings: - Set the program name to `\\"custom_python\\"`. - Enable isolated mode. - Set a custom command-line argument list including the script name `\\"custom_script.py\\"` and an additional argument `\\"--example\\"`. 3. After configuration, initialize Python and execute a custom Python command that prints `\\"Custom Python initialization successful!\\"`. 4. Handle any errors appropriately, ensuring any allocated resources are properly freed. Input: - No direct input is required for execution rather than simulating initialization and configuration. Expected output: - A successfully run message indicating Python was initialized and configured correctly. - On completing the initialization, it should print `\\"Custom Python initialization successful!\\"`. Below is the detailed code structure expected: ```python # Assume necessary imports and structures are defined from the documentation provided. /* Initialize PyPreConfig with Python Configuration */ PyPreConfig preconfig; PyPreConfig_InitPythonConfig(&preconfig); preconfig.utf8_mode = 1; # Enable UTF-8 Mode /* Preinitialize Python */ PyStatus status; status = Py_PreInitialize(&preconfig); if (PyStatus_Exception(status)) { Py_ExitStatusException(status); } /* Initialize PyConfig */ PyConfig config; PyConfig_InitIsolatedConfig(&config); # Initialize with Isolated Configuration /* Set program name */ status = PyConfig_SetString(&config, &config.program_name, L\\"custom_python\\"); if (PyStatus_Exception(status)) { Py_ExitStatusException(status); } /* Set command-line arguments */ const wchar_t* args[] = {L\\"custom_script.py\\", L\\"--example\\"}; PyConfig_SetWideStringList(&config, &config.argv, 2, (wchar_t**)args); /* Initialize Python from config */ status = Py_InitializeFromConfig(&config); if (PyStatus_Exception(status)) { Py_ExitStatusException(status); } /* Execute the custom command */ PyRun_SimpleString(\\"print(\'Custom Python initialization successful!\')\\"); /* Clear configuration */ PyConfig_Clear(&config); ``` Write the AI-based code to configure this Python initialization and confirm that it outputs the verification message properly.","solution":"import ctypes def initialize_python_custom(): Uses C-level constructs in Python to initialize the custom Python interpreter. PyPreConfig = ctypes.Structure PyStatus = ctypes.Structure PyConfig = ctypes.Structure # Mock the necessary C-structures and initialization functions as we cannot # directly use C-level APIs in standard Python. class PyPreConfig(ctypes.Structure): _fields_ = [(\'utf8_mode\', ctypes.c_int)] class PyStatus(ctypes.Structure): _fields_ = [(\'status\', ctypes.c_int)] class PyConfig(ctypes.Structure): _fields_ = [ (\'program_name\', ctypes.c_wchar_p), (\'argv\', ctypes.POINTER(ctypes.c_wchar_p)), (\'arg_count\', ctypes.c_size_t) ] def PyPreConfig_InitPythonConfig(preconfig): preconfig.utf8_mode = 0 def PyConfig_InitIsolatedConfig(config): config.program_name = None config.argv = None config.arg_count = 0 def PyPreInitialize(preconfig): status = PyStatus() status.status = 0 # Mock success status return status def PyConfig_SetString(config, attr, value): setattr(config, attr, value) status = PyStatus() status.status = 0 # Mock success status return status def PyConfig_SetWideStringList(config, attr, size, args): config.arg_count = size config.argv = (ctypes.c_wchar_p * size)(*args) status = PyStatus() status.status = 0 # Mock success status return status def Py_InitializeFromConfig(config): status = PyStatus() status.status = 0 # Mock success status return status def PyRun_SimpleString(command): exec(command) # Initialize PyPreConfig with Python Configuration preconfig = PyPreConfig() PyPreConfig_InitPythonConfig(preconfig) preconfig.utf8_mode = 1 # Preinitialize Python status = PyPreInitialize(preconfig) if status.status != 0: raise RuntimeError(\\"Failed to preinitialize Python\\") # Initialize PyConfig config = PyConfig() PyConfig_InitIsolatedConfig(config) # Set program name status = PyConfig_SetString(config, \'program_name\', \\"custom_python\\") if status.status != 0: raise RuntimeError(\\"Failed to set program name\\") # Set command-line arguments args = [\\"custom_script.py\\", \\"--example\\"] status = PyConfig_SetWideStringList(config, \'argv\', len(args), args) if status.status != 0: raise RuntimeError(\\"Failed to set command-line arguments\\") # Initialize Python from config status = Py_InitializeFromConfig(config) if status.status != 0: raise RuntimeError(\\"Failed to initialize Python from config\\") # Execute the custom command PyRun_SimpleString(\\"print(\'Custom Python initialization successful!\')\\")"},{"question":"**Question: Bivariate Histogram Visualization with seaborn** You are given a dataset of flight delays from multiple airlines. Your task is to visualize the bivariate distribution of flight delays against flight distances using seaborn\'s histogram functionalities. The visualization should be well-customized to convey key insights clearly. **Dataset Description:** The dataset contains the following columns: - `distance`: Distance of the flight in miles. - `delay`: Delay of the flight in minutes. - `airline`: Airline company operating the flight. **Requirements:** 1. Load the dataset from an online source or a local CSV file. 2. Create a bivariate histogram to visualize the relationship between `distance` and `delay`. 3. Use a logarithmic scale for the `distance` axis to account for highly skewed data. 4. Use different colors to represent different airlines. 5. Enable a color bar to annotate the colormap on the histogram. 6. Set the bins and adjust the other parameters to best visualize the data and highlight the distribution patterns. 7. Ensure that the plot is well-labeled and includes appropriate titles and legends for clarity. **Input Format:** - The dataset as a pandas DataFrame, with the columns `distance`, `delay`, and `airline`. **Output:** - A bivariate histogram visualization meeting the specified requirements. **Constraints:** - Ensure the histogram effectively communicates the pattern in the data despite the presence of outliers and skewness. - Use seaborn\'s `histplot` function to implement the solution. **Example Code:** ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the dataset (Assume the dataset is available at \'flights.csv\') # flights = pd.read_csv(\'flights.csv\') # For the sake of the example, let\'s create a simulated dataset import numpy as np np.random.seed(42) flights = pd.DataFrame({ \'distance\': np.random.lognormal(mean=3.0, sigma=1.0, size=1000), \'delay\': np.random.normal(loc=30.0, scale=15.0, size=1000), \'airline\': np.random.choice([\'Airline A\', \'Airline B\', \'Airline C\'], size=1000) }) # Create the bivariate histogram plot plt.figure(figsize=(10, 6)) sns.histplot( data=flights, x=\'distance\', y=\'delay\', hue=\'airline\', log_scale=(True, False), bins=30, pthresh=.05, pmax=.9, cbar=True, cbar_kws={\'shrink\': .75} ) plt.title(\'Bivariate Histogram of Flight Delays vs. Distance\') plt.xlabel(\'Distance (miles)\') plt.ylabel(\'Delay (minutes)\') plt.legend(title=\'Airline\') plt.show() ```","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def plot_bivariate_histogram(flights): Plots a bivariate histogram of flight delays against flight distance. Parameters: flights (pd.DataFrame): Input dataset containing \'distance\', \'delay\', and \'airline\' columns. plt.figure(figsize=(10, 6)) sns.histplot( data=flights, x=\'distance\', y=\'delay\', hue=\'airline\', log_scale=(True, False), bins=30, pthresh=.05, pmax=.9, cbar=True, cbar_kws={\'shrink\': .75} ) plt.title(\'Bivariate Histogram of Flight Delays vs. Distance\') plt.xlabel(\'Distance (miles)\') plt.ylabel(\'Delay (minutes)\') plt.legend(title=\'Airline\') plt.show()"},{"question":"# **Timedeltas in Pandas - Coding Assessment** Problem Statement You are tasked with processing and analyzing the duration of various activities logged throughout a project. The activities are recorded in a DataFrame with start and end times, and you need to compute the total duration of each activity and perform some analyses based on those durations. Task 1. Create a function `calculate_activity_durations` that takes a DataFrame as input. The DataFrame contains two columns: - `start_time`: The starting time of the activity (as `Timestamp` objects). - `end_time`: The ending time of the activity (as `Timestamp` objects). 2. This function should compute the `duration` of each activity in a new column (in hours). 3. Add another column `duration_category` which categorizes the duration of each activity into: - \\"Short\\" if the duration is less than 2 hours. - \\"Medium\\" if the duration is between 2 and 5 hours. - \\"Long\\" if the duration is more than 5 hours. 4. Create another function `analyze_durations` that takes the processed DataFrame from the previous function and outputs: - The total duration of all activities combined (in hours). - The average duration of all activities (in hours). - The count of activities in each duration category. Input - A DataFrame `df` with columns `start_time` and `end_time`. Both columns are in pandas `Timestamp` format. Output 1. For `calculate_activity_durations(df)`: - Returns the input DataFrame with two additional columns: `duration` (in hours) and `duration_category`. 2. For `analyze_durations(df)`: - Returns a tuple containing: - Total duration of all activities (in hours). - Average duration of all activities (in hours). - Dictionary with counts of each duration category `{\\"Short\\": int, \\"Medium\\": int, \\"Long\\": int}`. Constraints 1. Assume there are no missing values in `start_time` and `end_time`. 2. The `end_time` of each activity is always after the `start_time`. Example ```python import pandas as pd from datetime import datetime data = { \\"start_time\\": [pd.Timestamp(\\"2023-01-01 08:00:00\\"), pd.Timestamp(\\"2023-01-01 10:00:00\\"), pd.Timestamp(\\"2023-01-01 14:00:00\\")], \\"end_time\\": [pd.Timestamp(\\"2023-01-01 10:00:00\\"), pd.Timestamp(\\"2023-01-01 16:30:00\\"), pd.Timestamp(\\"2023-01-01 17:00:00\\")] } df = pd.DataFrame(data) # Function definitions to be implemented by the students: def calculate_activity_durations(df): # Your code here def analyze_durations(df): # Your code here # Expected usage df = calculate_activity_durations(df) total_duration, avg_duration, duration_counts = analyze_durations(df) print(df) print(\\"Total duration (hours):\\", total_duration) print(\\"Average duration (hours):\\", avg_duration) print(\\"Duration counts:\\", duration_counts) # Expected Output ``` **Output DataFrame `df`:** | start_time | end_time | duration | duration_category | |---------------------|---------------------|----------|-------------------| | 2023-01-01 08:00:00 | 2023-01-01 10:00:00 | 2.0 | Medium | | 2023-01-01 10:00:00 | 2023-01-01 16:30:00 | 6.5 | Long | | 2023-01-01 14:00:00 | 2023-01-01 17:00:00 | 3.0 | Medium | **Summary:** ``` Total duration (hours): 11.5 Average duration (hours): 3.8333333333333335 Duration counts: {\'Short\': 0, \'Medium\': 2, \'Long\': 1} ```","solution":"import pandas as pd def calculate_activity_durations(df): Computes the duration of each activity in hours and categorizes them. Parameters: df (pd.DataFrame): DataFrame containing \'start_time\' and \'end_time\' columns. Returns: pd.DataFrame: DataFrame with additional \'duration\' and \'duration_category\' columns. df[\'duration\'] = (df[\'end_time\'] - df[\'start_time\']).dt.total_seconds() / 3600.0 # Compute duration in hours def categorize_duration(duration): if duration < 2: return \\"Short\\" elif 2 <= duration <= 5: return \\"Medium\\" else: return \\"Long\\" df[\'duration_category\'] = df[\'duration\'].apply(categorize_duration) return df def analyze_durations(df): Analyzes the durations of activities. Parameters: df (pd.DataFrame): DataFrame with \'duration\' and \'duration_category\' columns. Returns: tuple: total_duration (float), avg_duration (float), duration_counts (dict) total_duration = df[\'duration\'].sum() avg_duration = df[\'duration\'].mean() duration_counts = df[\'duration_category\'].value_counts().to_dict() return total_duration, avg_duration, duration_counts"},{"question":"Objective: Create a Python program using the `curses.panel` module that demonstrates the creation and manipulation of panel stacks. Problem Statement: Design a function called `create_panel_stack` that accomplishes the following tasks: 1. Initializes a curses screen. 2. Creates three windows of size `5x20` at different starting coordinates. 3. Converts these windows into panels and stacks them such that the order is as follows from top to bottom: panel3, panel2, panel1. 4. Moves panel2 to coordinates `(10, 10)`. 5. Hides panel1 and then shows it again. 6. Moves panel3 to the top of the stack. 7. Updates the virtual screen to reflect all changes made to the panel stack and redraw the screen. Function Signature: ```python def create_panel_stack(): pass ``` Explanation: 1. **Initialization**: - You must initialize the curses screen and set up the windows. 2. **Creating Windows and Panels**: - Create three windows of defined size and positions. - Convert these windows into panels. 3. **Stack Order**: - Initially, stack the panels such that the third panel is on top. 4. **Move and Hide/Show**: - Move the second panel to `(10, 10)`. - Hide the first panel and then make it visible again. - Ensure the third panel is brought to the top of the stack. 5. **Update Screen**: - Update the virtual screen to ensure all changes are reflected properly. Constraints: - Make sure the program handles screen updates without freezing. - All operations on panels must maintain the stack order and visibility status correctly. Sample Output: The function does not return anything but should properly manipulate the panel stack and update the screen accordingly when run within a curses-compatible environment. Note: - Be aware that actual execution of this function requires a terminal capable of running curses, which is not possible in all environments, e.g., standard IDEs or this assessment environment.","solution":"import curses from curses import panel def create_panel_stack(): # Initialize curses screen stdscr = curses.initscr() # Create 3 windows at different starting coordinates win1 = curses.newwin(5, 20, 0, 0) win2 = curses.newwin(5, 20, 5, 5) win3 = curses.newwin(5, 20, 10, 10) # Convert windows to panels panel1 = panel.new_panel(win1) panel2 = panel.new_panel(win2) panel3 = panel.new_panel(win3) # Stack panels such that panel3 is on top panel.update_panels() # Move panel2 to coordinates (10, 10) panel2.move(10, 10) # Hide panel1 panel1.hide() panel.update_panels() # Show panel1 again panel1.show() panel.update_panels() # Move panel3 to the top panel3.top() panel.update_panels() # Refresh the screen stdscr.refresh() # End the curses application cleanly curses.endwin()"},{"question":"# Interactive Python Console Implementation In this task, you are required to implement a custom interactive Python console using the `code` and `codeop` modules. The custom console should extend the capabilities of the standard Python interactive interpreter in the following ways: 1. **Command History:** - Store a history of executed commands and allow the user to view this history. 2. **Magic Commands:** - Implement a special magic command `%show` that, when executed, prints the command history to the console. 3. **Error Handling:** - The console should handle errors gracefully and print meaningful error messages without crashing. Specifications - You should implement a class `CustomConsole` which inherits from `code.InteractiveConsole`. - The `CustomConsole` should maintain an internal list to keep track of command history. - Implement the magic command `%show` to print the list of previously executed commands. - Handle exceptions in the code gracefully, providing a user-friendly message without terminating the console session. Input and Output - **Input:** - Python commands inputted by the user in an interactive session. - **Output:** - The result of executing the Python commands. - For the `%show` magic command, it should print the history of all previous commands. Constraints - The solution must use the `code` and `codeop` modules. - The command history should be displayed in the order the commands were executed. Usage Example ```python >> console = CustomConsole() >> console.interact() >>> x = 10 >>> y = 20 >>> x + y 30 >>> %show x = 10 y = 20 x + y >>> z = x + y >>> z 30 >>> %show x = 10 y = 20 x + y z = x + y z ``` # Implementation Now, implement the `CustomConsole` class. ```python import code class CustomConsole(code.InteractiveConsole): def __init__(self, locals=None): super().__init__(locals) self.history = [] def push(self, line): # Store the command in history self.history.append(line) # Magic command for showing history if line == \\"%show\\": for cmd in self.history: if cmd != \\"%show\\": print(cmd) return False # Try executing the line and handle errors try: compiled_code = self.compile(line) if compiled_code: exec(compiled_code, self.locals) except Exception as e: print(f\\"Error: {e}\\") return False return False ``` Ensure your implementation correctly handles the requirements and constraints listed above.","solution":"import code import traceback class CustomConsole(code.InteractiveConsole): def __init__(self, locals=None): super().__init__(locals) self.history = [] def push(self, line): # Store the command in history self.history.append(line) # Magic command for showing history if line.strip() == \\"%show\\": for cmd in self.history: if cmd.strip() != \\"%show\\": print(cmd) return False # Try executing the line and handle errors try: compiled_code = self.compile(line) if compiled_code: exec(compiled_code, self.locals) except Exception as e: print(f\\"Error: {e}\\") print(traceback.format_exc()) return False return True"},{"question":"# PyTorch Backend Configuration for CUDA Problem Statement You are given a script that performs matrix multiplication multiple times using CUDA on a PyTorch setup. The performance of matrix computations can be optimized by configuring the `torch.backends.cuda.matmul` settings. Your task is to write a function `configure_cuda_backends` that configures specific settings to optimize these matrix operations. Then, write a function `matmul_performance_test` that executes matrix multiplications and returns the total computation time under the configured settings. Function 1: `configure_cuda_backends` **Parameters:** - `tf32_enabled` (bool): If `True`, enables TensorFloat-32 tensor cores. - `fp16_reduction` (bool): If `True`, enables reduced precision reductions with fp16 accumulation. - `bf16_reduction` (bool): If `True`, enables reduced precision reductions with bf16 accumulation. **Return:** - None **Behavior:** - Sets the `torch.backends.cuda.matmul.allow_tf32` attribute based on `tf32_enabled`. - Sets the `torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction` attribute based on `fp16_reduction`. - Sets the `torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction` attribute based on `bf16_reduction`. Function 2: `matmul_performance_test` **Parameters:** - `matrix_size` (int): The size of the square matrices to multiply. - `iterations` (int): The number of multiplication iterations to run. **Return:** - `total_time` (float): The total computation time for all iterations. **Behavior:** - Generates two random square matrices of shape `matrix_size x matrix_size`. - Moves these matrices to the CUDA device. - Performs matrix multiplication `iterations` times and records the total time taken. Constraints: - The computations should utilize GPU and CUDA capabilities. - Ensure your solution handles situations where CUDA is not available. Example: ```python def configure_cuda_backends(tf32_enabled, fp16_reduction, bf16_reduction): # Your implementation here. def matmul_performance_test(matrix_size, iterations): # Your implementation here. # Example test case configure_cuda_backends(True, True, False) total_time = matmul_performance_test(1024, 100) print(f\\"Total time for computation: {total_time:.4f} seconds\\") ``` In this example, the `configure_cuda_backends` function sets the specific backend configurations for TensorFloat-32, fp16 reduced precision reduction, and bf16 reduced precision reduction according to the provided arguments. The `matmul_performance_test` function then measures the total time taken to perform 100 iterations of multiplying 1024x1024 matrices on the CUDA device. **Note:** Your solution should first check if CUDA is available using `torch.cuda.is_available()`, and handle non-availability gracefully by printing an appropriate message and not executing the computations.","solution":"import torch import time def configure_cuda_backends(tf32_enabled, fp16_reduction, bf16_reduction): Configures the PyTorch CUDA backend settings for matrix multiplication. Args: tf32_enabled (bool): Enable TensorFloat-32 tensor cores. fp16_reduction (bool): Enable reduced precision reductions with fp16 accumulation. bf16_reduction (bool): Enable reduced precision reductions with bf16 accumulation. Return: None torch.backends.cuda.matmul.allow_tf32 = tf32_enabled if hasattr(torch.backends.cuda.matmul, \'allow_fp16_reduced_precision_reduction\'): torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = fp16_reduction if hasattr(torch.backends.cuda.matmul, \'allow_bf16_reduced_precision_reduction\'): torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = bf16_reduction def matmul_performance_test(matrix_size, iterations): Measures the performance of matrix multiplication on CUDA. Args: matrix_size (int): The size of the square matrices to multiply. iterations (int): The number of multiplication iterations to run. Return: total_time (float): The total computation time for all iterations. # Check if CUDA is available if not torch.cuda.is_available(): print(\\"CUDA is not available on this system.\\") return None # Generate random matrices A = torch.randn(matrix_size, matrix_size, device=\'cuda\') B = torch.randn(matrix_size, matrix_size, device=\'cuda\') # Warm-up run torch.matmul(A, B) # Start timing the matrix multiplications start_time = time.time() for _ in range(iterations): C = torch.matmul(A, B) total_time = time.time() - start_time return total_time"},{"question":"Objective: Implement a Python class that mimics some of the functionality of the tuple objects and struct sequence objects as described in the provided documentation, especially focusing on immutability and named fields. Question: You are tasked with implementing two Python classes: `CustomTuple` and `NamedCustomTuple`. 1. **CustomTuple Class:** - Implement a class `CustomTuple` that imitates Python\'s immutable tuple. - The class should allow initialization with any number of elements. - Implement the following methods: - `__init__(self, *args)`: Initialize the tuple with the provided values. - `__len__(self)`: Return the number of elements in the tuple. - `__getitem__(self, index)`: Return the element at the given index. Raise `IndexError` if the index is out of range. - `__repr__(self)`: Return a string representation of the tuple in the form `CustomTuple(element1, element2, ...)`. 2. **NamedCustomTuple Class:** - Implement a class `NamedCustomTuple` that imitates Python\'s `namedtuple`. - The class should allow initialization with a list of field names and corresponding values. - Implement the following methods: - `__init__(self, field_names, *values)`: Initialize the named tuple with field names and values. - `__len__(self)`: Return the number of elements in the named tuple. - `__getitem__(self, index_or_field)`: Return the element at the given index or field name. Raise `IndexError` or `KeyError` if the index or field name is out of range or invalid. - `__repr__(self)`: Return a string representation of the named tuple in the form `NamedCustomTuple(field1=value1, field2=value2, ...)`. **Constraints:** - The `CustomTuple` and `NamedCustomTuple` classes should not allow modification of existing elements. - `CustomTuple` should support indexing and slicing like a regular tuple. - `NamedCustomTuple` should support indexing by both numeric index and field name. **Example Usage:** ```python # CustomTuple Example ct = CustomTuple(1, 2, 3) print(ct) # Output: CustomTuple(1, 2, 3) print(len(ct)) # Output: 3 print(ct[1]) # Output: 2 # NamedCustomTuple Example nct = NamedCustomTuple([\'name\', \'age\'], \'Alice\', 25) print(nct) # Output: NamedCustomTuple(name=\'Alice\', age=25) print(len(nct)) # Output: 2 print(nct[0]) # Output: \'Alice\' print(nct[\'age\']) # Output: 25 ``` Ensure that your implementation covers these functionalities thoroughly and adheres to the constraints mentioned.","solution":"class CustomTuple: def __init__(self, *args): self._values = args def __len__(self): return len(self._values) def __getitem__(self, index): if isinstance(index, slice): return CustomTuple(*self._values[index]) if index < 0 or index >= len(self._values): raise IndexError(\'tuple index out of range\') return self._values[index] def __repr__(self): return f\\"CustomTuple({\', \'.join(map(str, self._values))})\\" class NamedCustomTuple: def __init__(self, field_names, *values): if len(field_names) != len(values): raise ValueError(\'Field names and values must have the same length\') self._fields = list(field_names) self._values = dict(zip(field_names, values)) def __len__(self): return len(self._fields) def __getitem__(self, index_or_field): if isinstance(index_or_field, int): if index_or_field < 0 or index_or_field >= len(self._fields): raise IndexError(\'tuple index out of range\') return self._values[self._fields[index_or_field]] elif isinstance(index_or_field, str): if index_or_field not in self._values: raise KeyError(f\\"No such field: {index_or_field}\\") return self._values[index_or_field] else: raise TypeError(\'Invalid index or field name\') def __repr__(self): field_values = (f\\"{field}={repr(self._values[field])}\\" for field in self._fields) return f\\"NamedCustomTuple({\', \'.join(field_values)})\\""},{"question":"# Secure User Authentication System **Objective:** Implement a Python function that leverages the `getpass` module to create a secure user authentication system. **Task:** Write a function `authenticate_user(users_db: dict) -> bool` that: 1. Prompts the user to input their login name and password without echoing the password on the screen. 2. Uses `getpass.getuser()` to retrieve the system\'s login name and compares it with the input login name. 3. Checks if the input login name and password match any entry in the provided `users_db` dictionary, where the keys are login names and the values are corresponding passwords. **Function Signature:** ```python def authenticate_user(users_db: dict) -> bool: ``` **Input:** - `users_db`: A dictionary where keys are valid login names (strings) and values are their corresponding passwords (strings). **Output:** - Returns `True` if both the login name and password match any entry in the `users_db`. - Returns `False` otherwise. **Constraints:** - The login name entered by the user should match `getpass.getuser()`. - The password input should not be visible on the screen. - Use appropriate exception handling to manage any unexpected situations. **Example:** ```python users_db = { \\"johndoe\\": \\"password123\\", \\"janedoe\\": \\"securepass456\\" } # Example call to the function result = authenticate_user(users_db) print(result) # Output will be True or False based on the provided inputs ``` **Implementation Requirements:** 1. Use `getpass.getpass` to securely gather the password from the user. 2. Use `getpass.getuser()` to retrieve the system\'s login name. **Performance Considerations:** - The function should handle user input errors gracefully. - The function should provide clear and secure authentication feedback.","solution":"import getpass def authenticate_user(users_db: dict) -> bool: Authenticate a user using the provided users_db. Args: users_db (dict): A dictionary with login names as keys and passwords as values. Returns: bool: True if the login name and password match any entry in the users_db, otherwise False. try: system_login_name = getpass.getuser() input_login_name = input(\\"Login Name: \\") if input_login_name != system_login_name: print(\\"Login name does not match the system login name.\\") return False input_password = getpass.getpass(\\"Password: \\") if users_db.get(input_login_name) == input_password: return True else: print(\\"Invalid login name or password.\\") return False except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"**Objective**: Implement a function to create an executable Python application archive with various options. **Task**: Write a Python function `create_python_archive` that takes the following parameters: - `source_dir`: The directory containing the Python code to be packaged. - `output_file`: The name of the output zip archive file. - `interpreter`: (Optional) The Python interpreter to be specified in the shebang line. - `main_callable`: (Optional) The callable to be used as the entry point of the application. - `compress`: (Optional, default is False) A boolean flag indicating whether to compress the content of the archive. **Constraints**: - The function should validate that `source_dir` exists and is a directory. - If `main_callable` is provided, it should be in the format `pkg.mod:fn`. - The output archive should be executable using the specified interpreter if provided. - The output filename should include a \\".pyz\\" extension. **Function Signature**: ```python def create_python_archive(source_dir: str, output_file: str, interpreter: str = None, main_callable: str = None, compress: bool = False) -> None: pass ``` # Example Usage: Given a directory structure: ``` myapp/  __init__.py  module.py  __main__.py ``` The following call: ```python create_python_archive(\'myapp\', \'myapp.pyz\', interpreter=\'/usr/bin/env python3\', main_callable=\'myapp:main\', compress=True) ``` Should create a compressed Python zip application archive named `myapp.pyz` that: - Specifies `/usr/bin/env python3` as the interpreter. - Uses `myapp:main` as the entry point for execution. # Notes: - Use the `zipapp` module\'s `create_archive` function for the core functionality. - Ensure that any exceptions (such as invalid directory or incorrect `main_callable` format) are handled gracefully with appropriate error messages.","solution":"import os import zipapp def create_python_archive(source_dir: str, output_file: str, interpreter: str = None, main_callable: str = None, compress: bool = False) -> None: Creates an executable Python application archive. Parameters: - source_dir (str): The directory containing the Python code to be packaged. - output_file (str): The name of the output zip archive file. - interpreter (str, optional): The Python interpreter to be specified in the shebang line. - main_callable (str, optional): The callable to be used as the entry point of the application. - compress (bool, optional): A boolean flag indicating whether to compress the content of the archive. Returns: - None # Validate source_dir exists and is a directory if not os.path.exists(source_dir): raise FileNotFoundError(f\\"The directory \'{source_dir}\' does not exist.\\") if not os.path.isdir(source_dir): raise NotADirectoryError(f\\"The path \'{source_dir}\' is not a directory.\\") # Ensure output file has .pyz extension if not output_file.endswith(\'.pyz\'): raise ValueError(\\"The output file name must have a \'.pyz\' extension.\\") # Validate main_callable format if main_callable: if \':\' not in main_callable: raise ValueError(\\"The main_callable should be in the format \'pkg.mod:fn\'.\\") # Set zipapp parameters kwargs = { \'target\': output_file, \'main\': main_callable, \'compressed\': compress } if interpreter: kwargs[\'interpreter\'] = interpreter # Create the archive zipapp.create_archive(source_dir, **kwargs)"},{"question":"You are tasked with implementing a function to manage and query a list of students, where each student has a name and a score. The goal is to maintain the list in sorted order by score and efficiently perform various types of queries. # Implement the Following Functions: 1. `add_student(students, name, score)`: Adds a student with the given `name` and `score` to the list `students` while maintaining it in sorted order by score. 2. `find_top_student(students)`: Returns the name of the student with the highest score. 3. `find_students_above_score(students, score)`: Returns a list of names of students whose scores are strictly greater than the given `score` sorted in ascending order by score. # Function Details: `add_student(students, name, score)` - **Input**: - `students`: A list of tuples `[(name1, score1), (name2, score2), ...]` where each tuple consists of a student\'s name (string) and score (float). - `name`: A string representing the student\'s name. - `score`: A float representing the student\'s score. - **Output**: `None` - **Action**: Add the student to the list in sorted order by score. `find_top_student(students)` - **Input**: - `students`: A list of tuples `[(name1, score1), (name2, score2), ...]` where each tuple consists of a student\'s name (string) and score (float). - **Output**: - The name of the student with the highest score (string). If the list is empty, return `None`. `find_students_above_score(students, score)` - **Input**: - `students`: A list of tuples `[(name1, score1), (name2, score2), ...]` where each tuple consists of a student\'s name (string) and score (float). - `score`: A float representing the score threshold. - **Output**: - A list of names of students (strings) whose scores are strictly greater than the given `score`, sorted in ascending order by score. # Constraints: - Assume the names of students are unique. - The list `students` should always be maintained in sorted order after each insertion. - Performance should be efficient for searching and insertion operations. # Example Usage: ```python students = [] add_student(students, \\"Alice\\", 85.0) add_student(students, \\"Bob\\", 90.5) add_student(students, \\"Charlie\\", 77.5) add_student(students, \\"David\\", 90.5) print(find_top_student(students)) # Output: \\"Bob\\" (or \\"David\\" if the order for equivalent scores is by latest add) print(find_students_above_score(students, 85.0)) # Output: [\\"Bob\\", \\"David\\"] ``` # Implementation Considerations: - You are encouraged to utilize the `bisect` and `insort` methods for efficient insertion and searching. - Make sure to handle edge cases such as an empty student list.","solution":"from bisect import insort def add_student(students, name, score): Adds a student with the given name and score to the list students while maintaining it in sorted order by score. insort(students, (score, name)) def find_top_student(students): Returns the name of the student with the highest score. If the list is empty, returns None. if not students: return None # The student with the highest score will be the last element in the sorted list return students[-1][1] def find_students_above_score(students, score): Returns a list of names of students whose scores are strictly greater than the given score, sorted in ascending order by score. result = [name for s, name in students if s > score] return result"},{"question":"Objective To assess your understanding of Python\'s `dis` module and your ability to analyze bytecode instructions, you will write a function and analyze the bytecode of various Python constructs. Instructions 1. **Function Implementation**: Implement a function `bytecode_analysis(func)`, which takes a single argument `func` - a Python function. 2. **Bytecode Analysis**: The function should: - Disassemble the provided function using the `dis.Bytecode` class or `dis.dis()` function. - Analyze the bytecode to determine the number of each type of bytecode instructions. - Return a dictionary where the keys are the names of the bytecode instructions, and the values are the counts of how many times each instruction appears in the bytecode of the provided function. Constraints - You should use the `dis` module for the bytecode analysis. - The input function can contain any valid Python code. Example Usage ```python def example_function(x): if x > 0: return x * 2 else: return x - 2 result = bytecode_analysis(example_function) print(result) ``` Expected Output Format The output should be a dictionary, for example: ```python { \'LOAD_FAST\': 3, \'POP_JUMP_IF_FALSE\': 1, \'RETURN_VALUE\': 2, ... } ``` Provide thorough comments in your code to explain your approach and any specific choices you made. # Submission Submit your implementation and a brief explanation of how your code works and how it utilizes the `dis` module to achieve the task.","solution":"import dis from collections import defaultdict def bytecode_analysis(func): Analyzes the bytecode of the provided function and returns a dictionary with the counts of each bytecode instruction. Parameters: func (function): The function to analyze. Returns: dict: A dictionary where keys are bytecode instruction names and values are the counts of those instructions. bytecode = dis.Bytecode(func) instruction_counts = defaultdict(int) for instr in bytecode: instruction_counts[instr.opname] += 1 return dict(instruction_counts)"},{"question":"Objective Demonstrate proficiency in using the `seaborn.objects` module, particularly with creating and customizing plots and integrating them with Matplotlib. Problem Statement Using the `seaborn.objects` and Matplotlib libraries, create a multi-part visualization that satisfies the following requirements: 1. Load the `penguins` dataset using `seaborn.load_dataset`. - The dataset contains information about penguin species, island, bill length, bill depth, flipper length, body mass, and sex. 2. Create a figure with two subplots arranged in a single row. - The first subplot should be a scatter plot showing the relationship between bill length and bill depth, colored by species. - The second subplot should be a bar plot showing the average body mass of each penguin species. 3. Customize each subplot: - Scatter plot: - Add a title \\"Bill Length vs. Bill Depth\\" to the scatter plot. - Use distinct markers for different species. - Customize the legend to be placed outside the plot to the right. - Bar plot: - Add a title \\"Average Body Mass by Species\\". - Add value labels above each bar showing the average body mass. 4. Apply a custom Seaborn theme to the entire figure, ensuring a consistent visual appearance. Input - No direct input from the user. Use the `penguins` dataset from Seaborn. Output - A Matplotlib figure with the specified customizations and visualizations. Constraints - Use only `seaborn` and `matplotlib` libraries. Example Here is an example of how your solution should be structured (note, this is a simplified version and may not meet all requirements): ```python import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt # Load dataset penguins = sns.load_dataset(\\"penguins\\") # Create figure and subfigures fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7)) # Scatter plot p1 = so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", color=\\"species\\") p1.add(so.Dots()).on(ax1).plot() ax1.set_title(\\"Bill Length vs. Bill Depth\\") ax1.legend_.set_bbox_to_anchor((1, 0.5)) # Bar plot avg_body_mass = penguins.groupby(\'species\')[\'body_mass_g\'].mean().reset_index() p2 = so.Plot(avg_body_mass, x=\\"species\\", y=\\"body_mass_g\\") p2.add(so.Bars()).on(ax2).plot() for index, row in avg_body_mass.iterrows(): ax2.text(index, row[\'body_mass_g\'], f\\"{row[\'body_mass_g\']:.1f}\\", ha=\'center\') ax2.set_title(\\"Average Body Mass by Species\\") # Apply theme sns.set_theme(style=\\"whitegrid\\") plt.tight_layout() plt.show() ``` Ensure that your final code meets all the requirements stated above.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def create_penguin_plots(): # Load dataset penguins = sns.load_dataset(\\"penguins\\") # Create figure and subplots fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7)) # Scatter plot p1 = so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", color=\\"species\\") p1.add(so.Dots(marker=\'o\')).on(ax1).plot() ax1.set_title(\\"Bill Length vs. Bill Depth\\") # Customize legend handles, labels = ax1.get_legend_handles_labels() ax1.legend(handles, labels, loc=\'center left\', bbox_to_anchor=(1, 0.5)) # Bar plot avg_body_mass = penguins.groupby(\'species\')[\'body_mass_g\'].mean().reset_index() p2 = so.Plot(avg_body_mass, x=\\"species\\", y=\\"body_mass_g\\") p2.add(so.Bars()).on(ax2).plot() # Add value labels above bars for index, row in avg_body_mass.iterrows(): ax2.text(index, row[\'body_mass_g\'], f\\"{row[\'body_mass_g\']:.1f}\\", ha=\'center\') ax2.set_title(\\"Average Body Mass by Species\\") # Apply theme sns.set_theme(style=\\"whitegrid\\") plt.tight_layout() plt.show() # Execute the plotting function create_penguin_plots()"},{"question":"# Handling Missing Values in Pandas **Objective:** You are given a DataFrame with several columns containing missing values. Your task is to write a function that processes this DataFrame by handling missing values according to specific rules. **Function Specification:** ```python def process_missing_values(df: pd.DataFrame) -> pd.DataFrame: Processes the input DataFrame by handling missing values according to the following rules: 1. For numeric columns, fill missing values with the mean of the column. 2. For categorical columns, fill missing values with the mode of the column. 3. For datetime columns, fill missing values with the earliest datetime value in the column. Args: df (pd.DataFrame): A pandas DataFrame containing columns with missing values. Returns: pd.DataFrame: A new DataFrame with missing values handled as per the rules. # Your implementation here ``` **Input:** - A pandas DataFrame (`df`) which may contain: - Numeric columns (integers, floats) - Categorical columns (dtype `object`) - Datetime columns (`dtype` `datetime64[ns]`) **Output:** - A pandas DataFrame with missing values handled as described. **Constraints:** - You may assume the DataFrame has at least one column of each type (numeric, categorical, datetime). - Performance should be considered for large DataFrames (with up to 100,000 rows). **Additional Information:** - Make use of pandas functions like `fillna()`, `mean()`, `mode()`, and other relevant methods. - You can use the pandas library and its documentation to understand more about handling missing values. **Example:** ```python import pandas as pd import numpy as np data = { \'numeric_col\': [1, 2, np.nan, 4, 5], \'categorical_col\': [\'cat\', \'dog\', np.nan, \'cat\', \'dog\'], \'datetime_col\': [pd.Timestamp(\'2021-01-01\'), pd.NaT, pd.Timestamp(\'2021-03-01\'), pd.Timestamp(\'2021-04-01\'), pd.NaT] } df = pd.DataFrame(data) result_df = process_missing_values(df) print(result_df) ``` Expected Output: ```plaintext numeric_col categorical_col datetime_col 0 1.0 cat 2021-01-01 1 2.0 dog 2021-01-01 2 3.0 cat 2021-03-01 3 4.0 cat 2021-04-01 4 5.0 dog 2021-01-01 ``` **Note:** - The mean of `numeric_col` is 3.0. - The mode of `categorical_col` is \'cat\'. - The earliest date in `datetime_col` is \'2021-01-01\'.","solution":"import pandas as pd def process_missing_values(df: pd.DataFrame) -> pd.DataFrame: Processes the input DataFrame by handling missing values according to the following rules: 1. For numeric columns, fill missing values with the mean of the column. 2. For categorical columns, fill missing values with the mode of the column. 3. For datetime columns, fill missing values with the earliest datetime value in the column. Args: df (pd.DataFrame): A pandas DataFrame containing columns with missing values. Returns: pd.DataFrame: A new DataFrame with missing values handled as per the rules. result_df = df.copy() for column in result_df.columns: if pd.api.types.is_numeric_dtype(result_df[column]): result_df[column].fillna(result_df[column].mean(), inplace=True) elif pd.api.types.is_categorical_dtype(result_df[column]) or result_df[column].dtype == object: result_df[column].fillna(result_df[column].mode()[0], inplace=True) elif pd.api.types.is_datetime64_any_dtype(result_df[column]): result_df[column].fillna(result_df[column].min(), inplace=True) return result_df"},{"question":"# Python Development Mode Assessment Objective You are required to write a Python function that reads a file and processes its content. The function should handle file operations carefully to avoid any resource or memory management issues. The aim of this exercise is to ensure you understand file handling, resource warnings, and the Python Development Mode. Task Implement the function `process_file(file_path: str) -> int` that: 1. Takes the path to a text file (`file_path`). 2. Opens the file safely, ensuring no file descriptor leaks or resource warnings occur. 3. Reads the file line by line and counts the number of non-empty lines. 4. Returns the count of non-empty lines. The function must ensure that: - All resources are properly closed after use. - Python Development Mode (`-X dev`) does not emit any `ResourceWarning`. Constraints - You may not use any third-party libraries. - The function should be efficient in terms of memory usage. Example ```python def process_file(file_path: str) -> int: # Your implementation here pass file_path = \'example.txt\' result = process_file(file_path) print(result) # Should print the count of non-empty lines in example.txt ``` To test your solution, run your script with Python Development Mode enabled and check for any warnings: ```sh python3 -X dev your_script.py example.txt ``` Note An example input file `example.txt` is provided below: ``` Hello, world! This is a test file. It contains multiple lines, some of which are empty. The function should count this correctly. ``` With the provided file, the expected output should be `4`, as there are 4 non-empty lines.","solution":"def process_file(file_path: str) -> int: Processes a file and counts the number of non-empty lines. Args: file_path (str): The path to the file to be processed. Returns: int: The count of non-empty lines. non_empty_lines = 0 with open(file_path, \'r\', encoding=\'utf-8\') as file: for line in file: if line.strip(): non_empty_lines += 1 return non_empty_lines"},{"question":"# Question: Implement a Function to Create and Manage a Nested Directory Structure Problem Statement You need to implement a function, `manage_directories(base_dir, structure)`, which will create and manage a nested directory structure based on the `structure` dictionary provided. The function should also create files in specified directories and write content to them as defined in the structure. Additionally, the function should allow removing certain directories based on a specified condition. Specifications 1. **Input Parameters**: - `base_dir` (string): The base directory where the nested structure should be created. - `structure` (dictionary): A dictionary defining the nested directory structure. A directory\'s value can be: - Another dictionary: representing subdirectories. - A string: representing file content to be created within the directory. 2. **Functionality**: - Create the nested directory structure specified in the `structure` dictionary under the `base_dir`. - For dictionary values that are strings, create a file named `content.txt` in the corresponding directory and write the string content to it. - Provide an option to remove directories that contain a specific file named `remove_me.txt`. 3. **Constraints**: - All directory and file paths should be created relative to `base_dir`. - Ensure that all directories and files are created with appropriate permissions. - Use the `os` module functions to perform the required operations. 4. **Example**: ```python base_dir = \\"/tmp/base\\" structure = { \\"dir1\\": { \\"subdir1\\": { \\"content.txt\\": \\"Hello from subdir1\\" }, \\"subdir2\\": { \\"subsubdir1\\": \\"Hello from subsubdir1\\" } }, \\"dir2\\": \\"Hello from dir2\\", \\"dir3\\": { \\"subdir3\\": { \\"remove_me.txt\\": \\"This directory should be removed\\" } } } manage_directories(base_dir, structure) ``` After running the above example, the following actions should be taken: - The directory structure should be created under `/tmp/base`. - Files with the specified content should be created in the appropriate directories. - The directory `/tmp/base/dir3/subdir3` should be removed because it contains `remove_me.txt`. Function Signature ```python def manage_directories(base_dir: str, structure: dict) -> None: pass ``` Evaluation Criteria - Correctness of directory and file creation. - Proper handling of nested structures. - Correct implementation of file writing and directory removal based on the condition. - Use of `os` module functions to achieve the operations. - Code readability and use of appropriate error handling.","solution":"import os import shutil def manage_directories(base_dir: str, structure: dict) -> None: Create and manage a nested directory structure based on the provided structure dictionary. Also handle the removal of directories containing a file named \'remove_me.txt\'. :param base_dir: The base directory where the nested structure should be created. :param structure: A dictionary defining the nested directory structure and file contents. def create_structure(current_dir: str, current_structure: dict) -> None: for name, content in current_structure.items(): path = os.path.join(current_dir, name) if isinstance(content, dict): os.makedirs(path, exist_ok=True) create_structure(path, content) elif isinstance(content, str): os.makedirs(current_dir, exist_ok=True) with open(path, \'w\') as file: file.write(content) def remove_directories_with_remove_me(current_dir: str) -> None: for root, dirs, files in os.walk(current_dir, topdown=False): if \'remove_me.txt\' in files: shutil.rmtree(root) create_structure(base_dir, structure) remove_directories_with_remove_me(base_dir)"},{"question":"**Problem: Implementing Weak Reference Management in Python** **Objective:** You are tasked with implementing a class in Python that manages weak references to its instances. This class will ensure the references are automatically cleared to prevent memory leaks. You will make use of Python\'s `weakref` module to achieve this. **Specification:** 1. **Class Definition:** - Define a class `ManagedObject` that keeps track of its instances using weak references. - When an instance of `ManagedObject` is created, it should automatically register itself in a class-level weak reference dictionary. - Implement a method to retrieve all currently live instances. 2. **Features:** - Define a class-level dictionary to maintain weak references to all instances. - Use a weak reference callback to remove entries from the dictionary when instances are garbage collected. 3. **Implementation Details:** - Use the `weakref` module available in Python. - Implement `__init__`, `__del__`, and a class method `live_instances` which returns all live instances. **Constraints:** - Ensure no strong references to instances are kept in your class-level dictionary. - Assume a basic understanding of object-oriented programming and the `weakref` module. **Performance Requirements:** - The solution should handle a large number of instances efficiently without memory leaks. **Input/Output:** - There are no direct input/output functions specified for this class. **Example Usage:** ```python import weakref class ManagedObject: _instances = weakref.WeakValueDictionary() def __init__(self, name): self.name = name self._instances[id(self)] = self def __del__(self): pass # This can be used for cleanup if necessary @classmethod def live_instances(cls): return list(cls._instances.values()) # Usage obj1 = ManagedObject(\\"Object1\\") obj2 = ManagedObject(\\"Object2\\") print(ManagedObject.live_instances()) # Should print the list of current live instances (obj1, obj2) del obj1 print(ManagedObject.live_instances()) # Should print the list of current live instance (obj2) del obj2 print(ManagedObject.live_instances()) # Should print an empty list ``` **Note:** Make sure to handle edge cases and ensure the list of live instances is updated correctly as objects are created and deleted.","solution":"import weakref class ManagedObject: _instances = weakref.WeakValueDictionary() def __init__(self, name): self.name = name self._instances[id(self)] = self def __del__(self): pass # This can be used for cleanup if necessary @classmethod def live_instances(cls): return list(cls._instances.values())"},{"question":"# Advanced Python Expression Evaluation Objective Write a function `matrix_operations` that takes in a matrix (a list of lists of numbers) and performs various operations to transform it based on given functions. The transformations should include both synchronous and asynchronous generator expressions, and utilize comprehensions for compactness and efficiency. Function Signature ```python async def matrix_operations(matrix: List[List[float]]) -> List[List[float]]: ``` Input - `matrix`: A list of lists where each inner list represents a row of the matrix. Each element in the matrix is a float number. ```python matrix: List[List[float]] ``` Output - A new matrix (list of lists) resulting from the following transformations: 1. **Comprehension with Generator Expressions**: Each element in the matrix should be squared. 2. **Async Generator for Sum Calculation**: An asynchronous generator that yields the sum of each row in the squared matrix. 3. **Final Matrix**: The final matrix should replace each element in a row with the sum of that row, acquired asynchronously. Constraints - Use list comprehensions, generator expressions, and asynchronous generators as specified. - The input matrix may be up to 1000 x 1000 in size. Example ```python async def example(): matrix = [ [1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0] ] result = await matrix_operations(matrix) # Expected output: [[14.0, 14.0, 14.0], [77.0, 77.0, 77.0], [194.0, 194.0, 194.0]] print(result) # Running the example import asyncio asyncio.run(example()) ``` Explanation 1. **Squaring Elements**: - Use a comprehension within a generator expression to square each element in the matrix. 2. **Sum Calculation**: - Define an async generator function that computes the sum of each row using `async for`. 3. **Replacing Elements**: - Generate the final matrix where each element in a row is replaced by the computed sum of that row, utilizing the async generator. Ensure your implementation is efficient and adheres to the constraints and requirements provided.","solution":"from typing import List import asyncio async def async_row_sums(matrix: List[List[float]]) -> List[float]: for row in matrix: await asyncio.sleep(0) # Yield control, simulating an async operation yield sum(row) async def matrix_operations(matrix: List[List[float]]) -> List[List[float]]: # Step 1: Square each element in the matrix squared_matrix = [[element ** 2 for element in row] for row in matrix] # Step 2: Use an async generator to compute the sum of each row row_sums = [] async for row_sum in async_row_sums(squared_matrix): row_sums.append(row_sum) # Step 3: Replace each element in the row with the sum of that row result_matrix = [[row_sum] * len(row) for row, row_sum in zip(matrix, row_sums)] return result_matrix"},{"question":"You are a data scientist tasked with analyzing the `diamonds` dataset using Seaborn\'s new object-oriented interface (`seaborn.objects`). Your goal is to create a series of visualizations that provide insights into diamond characteristics and their relationships. # Task Create a Python script that performs the following: 1. Load the `diamonds` dataset using the `load_dataset` function from Seaborn. 2. Generate a bar plot that shows the average carat weight for each distinct diamond clarity level. 3. Create another bar plot, but this time, show the median carat weight for each clarity level. 4. Create a customized bar plot that displays the interquartile range (IQR; Q3 - Q1) of carat weights for each diamond clarity level. 5. Create a bar plot similar to step 2, but use the `Dodge` transformation to differentiate the bars by diamond cut. # Input and Output Input - No input from the user. Use the `diamonds` dataset provided by Seaborn. Output - The script should display the following plots: 1. Average carat weight by clarity. 2. Median carat weight by clarity. 3. IQR of carat weights by clarity. 4. Average carat weight by clarity with bars dodged by cut. # Constraints - Use only the `seaborn.objects` interface for creating the plots. - Follow the steps sequentially for building the plots. # Performance Requirements - The script should be optimized to run efficiently. ```python # Your solution here ``` # Example Template ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the dataset diamonds = load_dataset(\\"diamonds\\") # Step 2: Create a bar plot showing the average carat weight for each clarity level p1 = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\").add(so.Bar(), so.Agg(\\"mean\\")) p1.show() # Step 3: Create a bar plot showing the median carat weight for each clarity level p2 = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\").add(so.Bar(), so.Agg(\\"median\\")) p2.show() # Step 4: Create a customized bar plot showing the IQR of carat weights for each clarity level p3 = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\").add(so.Bar(), so.Agg(lambda x: x.quantile(0.75) - x.quantile(0.25))) p3.show() # Step 5: Create a bar plot similar to step 2 with `Dodge` transformation to differentiate by diamond cut p4 = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\").add(so.Bar(), so.Agg(\\"mean\\"), so.Dodge(), color=\\"cut\\") p4.show() ```","solution":"import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the dataset diamonds = load_dataset(\\"diamonds\\") # Step 2: Create a bar plot showing the average carat weight for each clarity level average_carat_by_clarity = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\").add(so.Bar(), so.Agg(\\"mean\\")) average_carat_by_clarity.show() # Step 3: Create a bar plot showing the median carat weight for each clarity level median_carat_by_clarity = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\").add(so.Bar(), so.Agg(\\"median\\")) median_carat_by_clarity.show() # Step 4: Create a customized bar plot showing the IQR of carat weights for each clarity level def iqr(x): return x.quantile(0.75) - x.quantile(0.25) iqr_carat_by_clarity = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\").add(so.Bar(), so.Agg(iqr)) iqr_carat_by_clarity.show() # Step 5: Create a bar plot similar to step 2 with `Dodge` transformation to differentiate by diamond cut average_carat_by_clarity_and_cut = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\").add(so.Bar(), so.Agg(\\"mean\\"), so.Dodge(), color=\\"cut\\") average_carat_by_clarity_and_cut.show()"},{"question":"**Seaborn Color Palette Manipulation** As a data scientist, you often need to create visually appealing color palettes to distinguish data points clearly. In this exercise, you are required to demonstrate your understanding of Seaborn\'s color palette functionalities by performing the following tasks: 1. **Create a Sequential Palette**: - Create a sequential palette that ramps from a light gray to the color \\"seagreen\\" using Seaborn. - Store this palette in a variable named `seagreen_palette`. ```python # Your code here for creating seagreen_palette ``` 2. **Create a Hex Code Palette**: - Create a light color palette using the hex code \\"#79C\\". - Store this palette in a variable named `hex_palette`. ```python # Your code here for creating hex_palette ``` 3. **Create a Husl System Palette**: - Create a light color palette using the Husl system with the parameters (20, 60, 50). - Store this palette in a variable named `husl_palette`. ```python # Your code here for creating husl_palette ``` 4. **Increase the Number of Colors**: - Create a light color palette with 8 colors using the color \\"xkcd:copper\\". - Store this palette in a variable named `extended_palette`. ```python # Your code here for creating extended_palette ``` 5. **Continuous Colormap**: - Create a continuous colormap from the color \\"#a275ac\\". - Store this colormap in a variable named `continuous_cmap`. ```python # Your code here for creating continuous_cmap ``` **Input**: There are no specific inputs to the function. The function should contain the above implementations as part of its body. **Output**: The function should generate and store each respective palette in the specified variables. **Constraints**: - Use the seaborn library for all implementations. - Your function should not return any value. **Sample Code**: ```python import seaborn as sns def create_palettes(): sns.set_theme() # Task 1 seagreen_palette = sns.light_palette(\\"seagreen\\") # Task 2 hex_palette = sns.light_palette(\\"#79C\\") # Task 3 husl_palette = sns.light_palette((20, 60, 50), input=\\"husl\\") # Task 4 extended_palette = sns.light_palette(\\"xkcd:copper\\", 8) # Task 5 continuous_cmap = sns.light_palette(\\"#a275ac\\", as_cmap=True) # Call the function to ensure it runs correctly create_palettes() ``` Test your function by calling it and ensure that each variable is correctly created and contains the expected palette or colormap.","solution":"import seaborn as sns def create_palettes(): sns.set_theme() # Task 1: Create a sequential palette from light gray to \\"seagreen\\" seagreen_palette = sns.light_palette(\\"seagreen\\", n_colors=10) # Task 2: Create a light color palette using the hex code \\"#79C\\" hex_palette = sns.light_palette(\\"#79C\\", n_colors=10) # Task 3: Create a light color palette using the Husl system with parameters (20, 60, 50) husl_palette = sns.light_palette((20, 60, 50), input=\\"husl\\", n_colors=10) # Task 4: Create a light color palette with 8 colors using \\"xkcd:copper\\" extended_palette = sns.light_palette(\\"xkcd:copper\\", n_colors=8) # Task 5: Create a continuous colormap from the color \\"#a275ac\\" continuous_cmap = sns.light_palette(\\"#a275ac\\", as_cmap=True) return seagreen_palette, hex_palette, husl_palette, extended_palette, continuous_cmap"},{"question":"# Asynchronous TCP Server and Client Objective Implement an asynchronous TCP server and client using the `asyncio` library. The server should be able to handle multiple clients concurrently. Requirements 1. **TCP Server**: - Listens on a specified IP address and port. - Accepts connections from multiple clients. - Each client sends a message to the server, and the server responds with an acknowledgment. - The server should handle each client connection in a separate task. 2. **TCP Client**: - Connects to the server on the specified IP address and port. - Sends a message to the server. - Receives the acknowledgment from the server. 3. The communication protocol between the server and clients is simple: the client sends a message, and the server responds with a \\"Received: <message>\\" acknowledgment. Input and Output - **Server**: - Input: IP address and port to listen on. - Output: No direct output; internally handles client connections and prints received messages. - **Client**: - Input: IP address, port to connect to, and the message to send. - Output: Prints the acknowledgment received from the server. Constraints - You must use the `asyncio` library and its low-level APIs to manage the event loop, tasks, and connections. - The server should be capable of handling at least 5 simultaneous client connections. # Example Server Code ```python import asyncio class EchoServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport def data_received(self, data): message = data.decode() print(f\\"Received message: {message}\\") response = f\\"Received: {message}\\" self.transport.write(response.encode()) async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoServerProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() asyncio.run(main()) ``` Client Code ```python import asyncio async def tcp_echo_client(message): reader, writer = await asyncio.open_connection( \'127.0.0.1\', 8888) print(f\'Send: {message}\') writer.write(message.encode()) data = await reader.read(100) print(f\'Received: {data.decode()}\') writer.close() await writer.wait_closed() asyncio.run(tcp_echo_client(\'Hello, World!\')) ``` Above example code showcases the basic structure. Implement the full solution considering all the requirements. # Deliverables 1. Complete implementation of the TCP server. 2. Complete implementation of the TCP client. 3. Ensure proper handling of multiple simultaneous client connections. 4. Thorough documentation and comments in the code explaining each step and asyncio concepts used.","solution":"import asyncio class EchoServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport print(f\'Connection established from {transport.get_extra_info(\\"peername\\")}\') def data_received(self, data): message = data.decode() print(f\\"Received message: {message}\\") response = f\\"Received: {message}\\" self.transport.write(response.encode()) async def run_server(host, port): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoServerProtocol(), host, port) async with server: await server.serve_forever() async def tcp_echo_client(host, port, message): reader, writer = await asyncio.open_connection( host, port) print(f\'Send: {message}\') writer.write(message.encode()) data = await reader.read(100) print(f\'Received: {data.decode()}\') writer.close() await writer.wait_closed() if __name__ == \'__main__\': host = \'127.0.0.1\' port = 8888 asyncio.run(run_server(host, port))"},{"question":"Objective: You are required to implement a function that validates and processes input data, performs an efficient linear algebra operation, and carries out random sampling without replacement. This assessment tests your understanding of validation tools, efficient numeric operations, and random sampling utilities provided by scikit-learn\'s `sklearn.utils` module. Task: 1. **Input Validation**: - Implement a function `validate_and_process_data(X, y, random_state=42)` that validates the consistency of feature matrix `X` and label vector `y` using the `check_X_y` function. Ensure that `X` is a 2D array and `y` is 1D. - Convert `X` to a float array using `as_float_array`. 2. **Efficient Linear Algebra Operation**: - Compute the principal components of the feature matrix `X` by performing a singular value decomposition truncated to the top 2 components using `randomized_svd` function. 3. **Random Sampling**: - Implement a function `sample_data(X, y, n_samples)` that samples `n_samples` indices from the dataset using the `sample_without_replacement` function and returns the corresponding samples from `X` and `y`. Function Signatures: ```python def validate_and_process_data(X, y, random_state=42): Validates and processes the input data. Parameters: X (array-like): 2D feature matrix. y (array-like): 1D label vector. random_state (int, optional): Random state for reproducibility. Default is 42. Returns: X_processed (array): Processed feature matrix. u (array): Top 2 principal components of the feature matrix. def sample_data(X, y, n_samples): Samples data without replacement. Parameters: X (array-like): 2D feature matrix. y (array-like): 1D label vector. n_samples (int): Number of samples to pick. Returns: X_sampled (array): Sampled feature matrix. y_sampled (array): Sampled label vector. ``` Constraints: - `X` and `y` must have consistent lengths. - `X` should be a 2D array, and `y` should be a 1D array. - `n_samples` should be less than or equal to the number of samples in `X` and `y`. Example: ```python from sklearn.utils import check_X_y, as_float_array from sklearn.utils.extmath import randomized_svd from sklearn.utils.random import sample_without_replacement # Example Data X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]] y = [1, 2, 3, 4, 5] # Validate and Process Data X_processed, u = validate_and_process_data(X, y) # Sample Data X_sampled, y_sampled = sample_data(X_processed, y, 3) print(\\"Processed X:\\", X_processed) print(\\"Top 2 Principal Components:\\", u) print(\\"Sampled X:\\", X_sampled) print(\\"Sampled y:\\", y_sampled) ```","solution":"from sklearn.utils import check_X_y, as_float_array from sklearn.utils.extmath import randomized_svd from sklearn.utils.random import sample_without_replacement def validate_and_process_data(X, y, random_state=42): Validates and processes the input data. Parameters: X (array-like): 2D feature matrix. y (array-like): 1D label vector. random_state (int, optional): Random state for reproducibility. Default is 42. Returns: X_processed (array): Processed feature matrix. u (array): Top 2 principal components of the feature matrix. # Validate input data X, y = check_X_y(X, y) # Convert to float array X = as_float_array(X) # Compute top 2 principal components using randomized SVD u, _, _ = randomized_svd(X, n_components=2, random_state=random_state) return X, u def sample_data(X, y, n_samples): Samples data without replacement. Parameters: X (array-like): 2D feature matrix. y (array-like): 1D label vector. n_samples (int): Number of samples to pick. Returns: X_sampled (array): Sampled feature matrix. y_sampled (array): Sampled label vector. # Ensure the sampling is valid n_samples = min(n_samples, len(X)) # Get indices for sampled data indices = sample_without_replacement(len(X), n_samples) # Sample the data from the original arrays X_sampled = X[indices] y_sampled = y[indices] return X_sampled, y_sampled"},{"question":"# Python Module Creation and Interaction **Objective:** To assess your understanding of the Python C API for module creation and manipulation, you are required to write functions that utilize the `PyModule_Type` and its associated functions. Specifically, you will: 1. Create a new module and set its attributes. 2. Add functions, objects, and constants to the module. 3. Retrieve and validate the added elements from the module. **Task:** Implement the following functions in C: 1. **create_module**: - Create a new module with a given name. - Set the module\'s `__name__`, `__doc__`, `__package__`, and `__loader__` attributes. - Return the created module object. 2. **add_elements_to_module**: - Add an integer constant, a string constant, and a function to the given module. - Ensure that the function added to the module can be called from Python and returns a fixed string \\"Hello from C!\\". 3. **retrieve_module_elements**: - Retrieve and print the dictionary, name, and filename of the given module. - Validate the previously added integer and string constants. - Call and validate the added function from Python to ensure it returns the correct string. **Input and Output Format:** - Input: N/A (The functions will be tested internally using Python\'s C API.) - Output: The functions should correctly modify and interact with the module objects, with validations done through the test cases. **Constraints:** - You must use the provided functions and structures from the documentation to implement the tasks. - You should handle any potential errors appropriately using Python\'s error handling mechanisms. - Ensure that all references are managed correctly to avoid memory leaks or crashes. **Example:** ```c // create_module function PyObject* create_module(const char* module_name) { // Implementation } // add_elements_to_module function int add_elements_to_module(PyObject* module) { // Implementation } // retrieve_module_elements function void retrieve_module_elements(PyObject* module) { // Implementation } ``` **Notes:** - Thoroughly test your implementation to ensure that all module interactions are correct. - Consider edge cases and possible errors during module creation and manipulation. - Reference the Python C API documentation for additional details on the functions and structures used.","solution":"def create_module(name, doc=None, package=None, loader=None): Create a new Python module with specified attributes. :param name: Name of the module :param doc: Documentation string of the module :param package: Package name of the module :param loader: Loader of the module :return: Created module object module = type(\\"module\\", (), {})() module.__name__ = name module.__doc__ = doc module.__package__ = package module.__loader__ = loader return module def add_elements_to_module(module): Add elements to the given module: an integer constant, a string constant, and a function. :param module: Module object to which the elements will be added :return: None module.constant_int = 42 module.constant_str = \\"Hello, World!\\" def module_function(): return \\"Hello from Python!\\" module.module_function = module_function def retrieve_module_elements(module): Retrieve and validate the elements from module. :param module: Module object from which elements will be retrieved :return: A dictionary containing module elements information module_dict = module.__dict__ module_name = module.__name__ module_filename = module.__loader__ int_const = module.constant_int str_const = module.constant_str func_result = module.module_function() return { \\"dict\\": module_dict, \\"name\\": module_name, \\"filename\\": module_filename, \\"int_const\\": int_const, \\"str_const\\": str_const, \\"func_result\\": func_result }"},{"question":"Coding Assessment Question **Objective**: This question assesses your understanding of scikit-learn transformers and their applications in data preprocessing. You will be required to implement a custom transformer and use it in a pipeline to preprocess a given dataset. # Question Given a dataset of samples with mixed numerical and categorical features, you are required to: 1. Implement a custom transformer that normalizes numerical features. 2. Encode the categorical features using one-hot encoding. 3. Combine these transformations into a single pipeline that preprocesses the dataset and returns the transformed data. # Instructions 1. **Custom Transformer**: - Design a transformer class `CustomNormalizer` that scales numerical data to have zero mean and unit variance. - This class should implement `fit`, `transform`, and `fit_transform` methods. 2. **Pipeline Construction**: - Use `scikit-learn`\'s `ColumnTransformer` and `Pipeline` to combine `CustomNormalizer` for numerical features and `OneHotEncoder` for categorical features. - Assume columns \'A\' and \'B\' are numerical and columns \'C\' and \'D\' are categorical. 3. **Testing**: - Test your pipeline on the provided dataset and print the transformed data. # Constraints 1. Do not use `StandardScaler` from `scikit-learn`: you must implement the normalization yourself in `CustomNormalizer`. 2. You must use `ColumnTransformer` and `Pipeline` from `scikit-learn`. # Provided Dataset ```python import pandas as pd data = { \'A\': [1, 2, 3, 4, 5], \'B\': [2, 3, 4, 5, 6], \'C\': [\'a\', \'b\', \'a\', \'b\', \'a\'], \'D\': [\'x\', \'x\', \'y\', \'y\', \'x\'] } df = pd.DataFrame(data) ``` # Expected Input and Output - **Input**: A DataFrame similar to the one provided. - **Output**: A NumPy array or DataFrame of the transformed data. # Starter Code Here is some starter code to get you started: ```python from sklearn.base import BaseEstimator, TransformerMixin from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder import numpy as np import pandas as pd class CustomNormalizer(BaseEstimator, TransformerMixin): def fit(self, X, y=None): self.mean_ = np.mean(X, axis=0) self.std_ = np.std(X, axis=0) return self def transform(self, X): return (X - self.mean_) / self.std_ def fit_transform(self, X, y=None): self.fit(X) return self.transform(X) # Test DataFrame data = { \'A\': [1, 2, 3, 4, 5], \'B\': [2, 3, 4, 5, 6], \'C\': [\'a\', \'b\', \'a\', \'b\', \'a\'], \'D\': [\'x\', \'x\', \'y\', \'y\', \'x\'] } df = pd.DataFrame(data) # Define the numerical and categorical features numerical_features = [\'A\', \'B\'] categorical_features = [\'C\', \'D\'] # Create the ColumnTransformer preprocessor = ColumnTransformer( transformers=[ (\'num\', CustomNormalizer(), numerical_features), (\'cat\', OneHotEncoder(), categorical_features) ]) # Create the pipeline pipeline = Pipeline(steps=[(\'preprocessor\', preprocessor)]) # Transform the data transformed_data = pipeline.fit_transform(df) # Print the transformed data print(transformed_data) ``` Note that this starter code is incomplete; you need to implement the `CustomNormalizer` class and construct the pipeline appropriately.","solution":"from sklearn.base import BaseEstimator, TransformerMixin from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder import numpy as np import pandas as pd class CustomNormalizer(BaseEstimator, TransformerMixin): def fit(self, X, y=None): self.mean_ = np.mean(X, axis=0) self.std_ = np.std(X, axis=0) return self def transform(self, X): return (X - self.mean_) / self.std_ def fit_transform(self, X, y=None): self.fit(X) return self.transform(X) # Test DataFrame data = { \'A\': [1, 2, 3, 4, 5], \'B\': [2, 3, 4, 5, 6], \'C\': [\'a\', \'b\', \'a\', \'b\', \'a\'], \'D\': [\'x\', \'x\', \'y\', \'y\', \'x\'] } df = pd.DataFrame(data) # Define the numerical and categorical features numerical_features = [\'A\', \'B\'] categorical_features = [\'C\', \'D\'] # Create the ColumnTransformer preprocessor = ColumnTransformer( transformers=[ (\'num\', CustomNormalizer(), numerical_features), (\'cat\', OneHotEncoder(), categorical_features) ]) # Create the pipeline pipeline = Pipeline(steps=[(\'preprocessor\', preprocessor)]) # Transform the data transformed_data = pipeline.fit_transform(df) # Print the transformed data print(transformed_data)"},{"question":"**Objective**: Assess your understanding and ability to implement feature selection techniques using the `sklearn.feature_selection` module. **Task**: You are provided with a dataset and your goal is to implement a pipeline that performs feature selection and classification. Specifically, you will use the `Recursive Feature Elimination (RFE)` method combined with a classifier to select the top features and predict a target variable. Input: 1. A dataset in the form of a CSV file where the last column represents the target variable and the other columns represent the features. Required Implementation: 1. Load the dataset. 2. Split the data into training and testing sets. 3. Implement a pipeline that includes: - The `RFE` method using a logistic regression classifier. - A final classification model to predict the target variable. 4. Train the pipeline on the training data. 5. Evaluate the trained model on the test data. Constraints: - Use `RFE` to select the top `k` features. The value of `k` should be specified as a parameter to your function. - Ensure to handle any potential missing values in the dataset appropriately. - Use a logistic regression classifier as the base estimator for RFE. - Report the accuracy of the model on the test data. Performance Requirements: - Efficiently process datasets up to 50,000 samples and 200 features. - The code should run within a reasonable time frame (few minutes) under these constraints. Input Format: - A string `file_path` representing the path to the CSV file. - An integer `k` representing the number of top features to select. Output Format: - A float representing the accuracy of the trained model on the test data. Example: ```python def feature_selection_and_classification(file_path: str, k: int) -> float: # Your code here # Example usage accuracy = feature_selection_and_classification(\\"path/to/dataset.csv\\", 10) print(f\\"Model accuracy: {accuracy:.2f}\\") ``` The function `feature_selection_and_classification` should be implemented to perform the required tasks and output the model accuracy. **Note**: Include appropriate comments in your code to explain each step and ensure code readability.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.feature_selection import RFE from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.metrics import accuracy_score from sklearn.preprocessing import StandardScaler def feature_selection_and_classification(file_path: str, k: int) -> float: # Load the dataset data = pd.read_csv(file_path) # Split the data into features and target X = data.iloc[:, :-1] y = data.iloc[:, -1] # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create a logistic regression classifier model = LogisticRegression(max_iter=1000) # Create an RFE object with the logistic regression classifier rfe = RFE(estimator=model, n_features_to_select=k) # Create a pipeline with imputation, scaling, RFE, and the final classifier pipeline = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()), (\'feature_selection\', rfe), (\'classification\', model) ]) # Train the pipeline on the training data pipeline.fit(X_train, y_train) # Predict the target variable on the test data y_pred = pipeline.predict(X_test) # Calculate the accuracy of the model accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"**Question: Comprehensive Type Hinting and Generics** You are tasked with implementing a small library of utilities for processing user data in a type-safe manner. You\'ll need to use the `typing` module to enforce type hints, create generic functions, and demonstrate the use of advanced typing constructs. Implement the following components: 1. **New User Type**: Define a new type `UserId` which is distinct from a regular `int` using the `NewType` helper. Implement a function `create_user(user_id: int) -> UserId` that converts an `int` to a `UserId`. 2. **User Data Definition**: Define a `UserData` TypedDict with the following structure: - `id`: `UserId` - `name`: `str` - `age`: `Optional[int]` (Note: An age may or may not be provided) 3. **Utilities for UserData**: Create a function `get_user_name(user_data: UserData) -> str` which returns the user\'s name. Additionally, implement a function `is_adult(user_data: UserData) -> bool` that checks if the user is an adult (age >= 18). If the `age` is not provided, assume the user is not an adult. 4. **Type-Safe Data Aggregator**: Implement a generic function `aggregate_data[T](data: Sequence[T]) -> dict[str, T]`, which takes a sequence of elements of any type and returns a dictionary where the keys are the string representations of elements, and the values are the elements themselves. 5. **Process Data Callables**: Create a function `process_data(data: Sequence[UserData], processor: Callable[[UserData], Any]) -> list[Any]`. This function should apply a callable `processor` on each element of the sequence `data` and return a list of the results. 6. **Custom Protocol for Processors**: Define a protocol `Processor` which requires an `execute` method taking a `UserData` instance and returning an `Any`. Modify the `process_data` function to accept a `processor` that complies with this protocol rather than a generic callable. **Constraints and Requirements**: - Use the `typing` module constructs effectively as described. - Demonstrate the correct and appropriate use of `NewType`, `TypedDict`, `Optional`, `Generic`, `TypeVar`, `Callable`, and `Protocol`. - Ensure the code type-checks correctly with a static type checker like `mypy`. **Example Usage**: ```python UserId = NewType(\'UserId\', int) class UserData(TypedDict): id: UserId name: str age: Optional[int] def create_user(user_id: int) -> UserId: return UserId(user_id) def get_user_name(user_data: UserData) -> str: return user_data[\'name\'] def is_adult(user_data: UserData) -> bool: return user_data.get(\'age\', 0) >= 18 T = TypeVar(\'T\') def aggregate_data(data: Sequence[T]) -> dict[str, T]: return {str(item): item for item in data} class Processor(Protocol): def execute(self, user_data: UserData) -> Any: ... def process_data(data: Sequence[UserData], processor: Processor) -> list[Any]: return [processor.execute(user) for user in data] # Example data = [ {\'id\': create_user(1), \'name\': \'Alice\', \'age\': 25}, {\'id\': create_user(2), \'name\': \'Bob\', \'age\': None}, {\'id\': create_user(3), \'name\': \'Charlie\'} ] class NameProcessor: def execute(self, user_data: UserData) -> str: return get_user_name(user_data) name_processor = NameProcessor() print(process_data(data, name_processor)) # Output: [\'Alice\', \'Bob\', \'Charlie\'] ```","solution":"from typing import NewType, TypedDict, Optional, Sequence, TypeVar, Callable, Any, Protocol, Dict # Define a new type UserId UserId = NewType(\'UserId\', int) # Function to create a UserId def create_user(user_id: int) -> UserId: return UserId(user_id) # Define the UserData TypedDict class UserData(TypedDict): id: UserId name: str age: Optional[int] # Function to get the user\'s name def get_user_name(user_data: UserData) -> str: return user_data[\'name\'] # Function to check if the user is an adult def is_adult(user_data: UserData) -> bool: return user_data.get(\'age\', 0) >= 18 # Define a generic type variable T = TypeVar(\'T\') # Function to aggregate data def aggregate_data(data: Sequence[T]) -> Dict[str, T]: return {str(item): item for item in data} # Define the Processor protocol class Processor(Protocol): def execute(self, user_data: UserData) -> Any: ... # Function to process data with a processor def process_data(data: Sequence[UserData], processor: Processor) -> list[Any]: return [processor.execute(user) for user in data]"},{"question":"You are given a directory of Parquet files representing a large dataset. Each file corresponds to a time series data for one year. Your goal is to analyze the data without loading the entire dataset into memory at once. Specifically, you need to determine the memory-efficient way to load the data, process it in chunks, and calculate the total count of unique values in one of the columns. Problem Statement 1. **Loading Data Efficiently:** - Load only specific columns from each Parquet file (`\'name\'`, `\'id\'`, `\'x\'`, `\'y\'`). - Optimize memory usage by converting the `\'name\'` column to a `pandas.Categorical` and downcasting numeric columns (`\'id\'` to unsigned integers and `\'x\'`, `\'y\'` to float). 2. **Processing Data in Chunks:** - As the entire dataset will not fit in memory, process each file individually, and accumulate the results to find the overall distribution of unique values in the `\'name\'` column. Input - A directory `data/timeseries/` containing multiple Parquet files (e.g., `\'ts-00.parquet\'`, `\'ts-01.parquet\'`, ..., `\'ts-11.parquet\'`). Output - A `pandas.Series` object representing the total count of each unique value in the `\'name\'` column across all files. Function Signature ```python import pathlib import pandas as pd def process_timeseries_data(directory: str) -> pd.Series: pass ``` Constraints - Only the specified columns (`\'name\'`, `\'id\'`, `\'x\'`, `\'y\'`) should be read from each file. - Memory usage should be minimized by using efficient data types. - Ensure the solution works for datasets larger than available memory by processing the files in chunks. Example Usage ```python # Example call to the function directory = \\"data/timeseries/\\" result = process_timeseries_data(directory) # Example output (Series) # Alice 100000 # Bob 123000 # Charlie 95000 # dtype: int64 ``` Hints - Use `pandas.read_parquet` with the `columns` parameter to load only the necessary columns. - Convert the `\'name\'` column to `pandas.Categorical`. - Use `pd.to_numeric` for downcasting numeric columns. - Use `DataFrame.value_counts` for counting the occurrences of each unique value in the `\'name\'` column.","solution":"import pathlib import pandas as pd def process_timeseries_data(directory: str) -> pd.Series: # Initialize an empty Series to hold the count of unique \'name\' values total_counts = pd.Series(dtype=\'int64\') # Convert the directory string to a Path object data_dir = pathlib.Path(directory) # List all the parquet files in the directory parquet_files = sorted(data_dir.glob(\\"*.parquet\\")) for file in parquet_files: # Read only specific columns df = pd.read_parquet(file, columns=[\'name\', \'id\', \'x\', \'y\']) # Convert \'name\' to categorical df[\'name\'] = df[\'name\'].astype(\'category\') # Downcast \'id\' to the smallest unsigned integer subtype df[\'id\'] = pd.to_numeric(df[\'id\'], downcast=\'unsigned\') # Downcast \'x\' and \'y\' to float32 df[\'x\'] = pd.to_numeric(df[\'x\'], downcast=\'float\') df[\'y\'] = pd.to_numeric(df[\'y\'], downcast=\'float\') # Calculate value counts for \'name\' column counts = df[\'name\'].value_counts() # Accumulate the counts to the total total_counts = total_counts.add(counts, fill_value=0) return total_counts"},{"question":"**Title**: Parsing and Analyzing Binary Data using `struct` and `codecs` **Objective**: Working with binary data often involves interpreting complex structures, converting between character encodings, and handling different byte orders. This assessment will test your ability to use Python\'s `struct` and `codecs` modules to perform these tasks. **Problem Statement**: You are provided with a binary file that contains user records. Each record has the following format: - A signed 4-byte integer representing the user ID. - A 10-byte string in UTF-8 encoding representing the username. - A signed 4-byte integer representing the users age. - A 6-byte string in UTF-8 encoding representing the users country code. Write a function `parse_user_records(file_path)` that reads this binary file and returns a list of user records as dictionaries. Each dictionary should have the keys `user_id`, `username`, `age`, and `country_code` with corresponding values extracted and decoded from the binary data. **Function Signature**: ```python def parse_user_records(file_path: str) -> list: ``` **Input**: - `file_path` (str): A string representing the path to the binary file to be read. **Output**: - Returns a list of dictionaries, where each dictionary contains the user data with keys: - `user_id` (int): The user ID extracted from the binary data. - `username` (str): The username decoded from the binary data. - `age` (int): The age extracted from the binary data. - `country_code` (str): The country code decoded from the binary data. **Constraints**: - The file is guaranteed to contain valid binary data following the specified format. - The username will contain only ASCII characters. - You can assume the file size is moderate and will fit into memory. **Example**: Assume the file `users.bin` contains the following binary data: ``` 00000001 746573745f7573 001600 555341370000 00000002 6a6f686e646f65 002400 47424d000000 ``` Which corresponds to: - First user: ID=1, username=\\"test_us_\\", age=22, country_code=\\"USA7\\" - Second user: ID=2, username=\\"johndoe\\", age=36, country_code=\\"GBM\\" Calling `parse_user_records(\\"users.bin\\")` should return: ```python [ {\\"user_id\\": 1, \\"username\\": \\"test_us_\\", \\"age\\": 22, \\"country_code\\": \\"USA7\\"}, {\\"user_id\\": 2, \\"username\\": \\"johndoe\\", \\"age\\": 36, \\"country_code\\": \\"GBM\\"} ] ``` **Hints**: - Use the `struct` module to unpack the binary data. - Use the `codecs` module to handle UTF-8 encoding. Good luck, and happy coding!","solution":"import struct import codecs def parse_user_records(file_path: str) -> list: records = [] record_struct = struct.Struct(\'i10s i6s\') with open(file_path, \'rb\') as f: while True: record_data = f.read(record_struct.size) if not record_data: break unpacked_data = record_struct.unpack(record_data) user_record = { \'user_id\': unpacked_data[0], \'username\': unpacked_data[1].decode(\'utf-8\').strip(\'x00\'), \'age\': unpacked_data[2], \'country_code\': unpacked_data[3].decode(\'utf-8\').strip(\'x00\') } records.append(user_record) return records"},{"question":"# Custom Context Manager with ExitStack **Objective:** Design and implement a custom context manager using the `contextmanager` decorator from the `contextlib` module. The context manager should handle resource acquisition and release, and you need to leverage `ExitStack` to manage multiple resources effectively. **Task:** 1. Implement a custom context manager named `managed_file` that reads and writes to a file safely. 2. Use `ExitStack` to manage multiple instances of `managed_file` ensuring all files are closed properly regardless of exceptions. **Requirements:** 1. Implement the `managed_file` context manager using the `contextmanager` decorator. - It should: - Open a file for reading or writing based on parameters. - Yield control back to the calling context. - Ensure the file is closed after the operations inside the `with` block, even if an exception occurs. 2. Use `ExitStack` to create a function called `manage_multiple_files` that: - Accepts a list of file names and modes. - Uses multiple instances of `managed_file` to perform read or write operations. - Ensures all files are closed correctly after the operations, regardless of any exceptions. **Input:** - `file_names`: A list of tuples, where each tuple contains a file name (string) and mode (string, either \'r\' for read or \'w\' for write). - For example: `[(\'file1.txt\', \'r\'), (\'file2.txt\', \'w\')]` **Output:** - The `manage_multiple_files` function should return a list of file content if the file is opened in read mode, and a success message for write mode. - In case of read mode, the content of the files should be returned as a list of strings. - In case of write mode, a success message should be printed. **Constraints:** - Do not use any additional external libraries. - Make sure to handle exceptions gracefully to ensure all resources are properly released. **Example Usage:** ```python from contextlib import contextmanager, ExitStack @contextmanager def managed_file(file_name, mode): file = open(file_name, mode) try: yield file finally: file.close() def manage_multiple_files(file_names): results = [] with ExitStack() as stack: files = [stack.enter_context(managed_file(name, mode)) for name, mode in file_names] for file, (name, mode) in zip(files, file_names): if mode == \'r\': results.append(file.read()) elif mode == \'w\': file.write(f\'Successfully written to {name}\') results.append(f\'Successfully written to {name}\') return results # Example call: file_names = [ (\'file1.txt\', \'r\'), (\'file2.txt\', \'w\'), ] print(manage_multiple_files(file_names)) ``` Write the full implementation of both `managed_file` and `manage_multiple_files` as described.","solution":"from contextlib import contextmanager, ExitStack @contextmanager def managed_file(file_name, mode): Context manager to open and close a file. Args: file_name (str): The name of the file to open. mode (str): The mode to open the file in (e.g., \'r\' for read, \'w\' for write). Yields: file object: The opened file object. file = open(file_name, mode) try: yield file finally: file.close() def manage_multiple_files(file_names): Manages multiple files using ExitStack to ensure all files are safely opened and closed. Args: file_names (list of tuples): A list where each tuple contains a file name and the mode to open it in. Returns: list: A list containing file contents for read mode, or success message for write mode. results = [] with ExitStack() as stack: files = [stack.enter_context(managed_file(name, mode)) for name, mode in file_names] for file, (name, mode) in zip(files, file_names): if mode == \'r\': results.append(file.read()) elif mode == \'w\': file.write(f\'Successfully written to {name}\') results.append(f\'Successfully written to {name}\') return results"},{"question":"Objective: To assess your understanding of PyTorch utilities and designing neural networks, you are required to implement a neural network using `torch.nn.Module` and leverage the `torch.utils.module_tracker.ModuleTracker` class to track and display the names and order of the layers within your network. Problem Statement: 1. Define a simple feedforward neural network with at least an input layer, one hidden layer, and an output layer using `torch.nn.Module`. 2. Use the `ModuleTracker` class to track the position of each layer in the model. 3. Create a function `track_model_layers` that: - Takes a PyTorch neural network model as input. - Utilizes the `ModuleTracker` to traverse the model. - Returns a list of tuples where each tuple contains the layer name and its order in the network. Constraints: - You should use PyTorch (torch) package for defining the neural network. - The layer names should be meaningful and self-explanatory. Input Format: - The input is a PyTorch model instance. Output Format: - The output is a list of tuples in the format: [(layer_name, order), ...] Example: Suppose you have defined a neural network called `SimpleNet`. When passed to the `track_model_layers` function, it should return the tracked layers in the appropriate format. ```python class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.input_layer = nn.Linear(10, 50) self.hidden_layer = nn.ReLU() self.output_layer = nn.Linear(50, 1) def forward(self, x): x = self.input_layer(x) x = self.hidden_layer(x) x = self.output_layer(x) return x model = SimpleNet() print(track_model_layers(model)) ``` Expected Output: ```python [(\'input_layer\', 0), (\'hidden_layer\', 1), (\'output_layer\', 2)] ``` Hints: - Review the `torch.utils.module_tracker.ModuleTracker` documentation for how to implement tracking. - Ensure the order of layers in the output matches their order in the network definition.","solution":"import torch import torch.nn as nn from torch.utils.hooks import RemovableHandle class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.input_layer = nn.Linear(10, 50) self.hidden_layer = nn.ReLU() self.output_layer = nn.Linear(50, 1) def forward(self, x): x = self.input_layer(x) x = self.hidden_layer(x) x = self.output_layer(x) return x def track_model_layers(model): Tracks the layers of a PyTorch model and returns their names and order. Args: model (nn.Module): The PyTorch model to be tracked. Returns: List[Tuple[str, int]]: A list of tuples where each tuple contains the layer name and its order in the model. layers = [] def hook(module, input, output, name): layers.append(name) handles = [] for name, module in model.named_children(): handles.append(module.register_forward_hook(lambda m, i, o, name=name: hook(m, i, o, name))) # Perform a forward pass to activate hooks model(torch.randn(1, 10)) # Remove hooks to clean up for handle in handles: handle.remove() return [(name, i) for i, name in enumerate(layers)]"},{"question":"PyTorch MTIA Device and Stream Management Objective: The goal of this exercise is to test your understanding of the PyTorch MTIA backend, specifically in the context of device and stream management. Problem Statement: You are tasked with implementing functions to simulate a simplified pipeline that utilizes multiple devices and streams for concurrent execution. You are to implement two functions as specified below. Function 1: `setup_devices_and_streams` **Input:** - None **Output:** - A tuple containing the current device ID and a dictionary with the following: - `device_count`: Number of available MTIA devices. - `device_capabilities`: A list of capabilities for each device. - `streams`: A mapping of device ID to their respective streams. **Constraints:** - Use the PyTorCH MTIA backend for querying device count and capabilities. - Each device should have one associated stream. - Handle the case where MTIA backend is not available or not initialized. ```python def setup_devices_and_streams(): Sets up and returns the current device and a dictionary containing device counts, capabilities, and associated streams for each available device. Returns: tuple: current_device_id (int), dict containing: - device_count (int) - device_capabilities (list) - streams (dict) pass ``` Function 2: `perform_operations_on_streams` **Input:** - `operations`: A dictionary where keys are device IDs and values are lists of tuples, each containing: - A callable representing the operation to be executed on the stream. - A positional argument tuple to pass to the callable. **Output:** - None **Constraints:** - The operations should be performed asynchronously on the respective device\'s stream. - Use synchronization to ensure all operations are completed before the function returns. ```python def perform_operations_on_streams(operations): Executes operations asynchronously on device streams and synchronizes them for completion. Parameters: operations (dict): A dictionary mapping device IDs to a list of operations to be performed on the streams. Example: { 0: [(operation1, (arg1, arg2)), (operation2, (arg1,))], 1: [(operation3, (arg1,))] } pass ``` Example Usage: ```python # Setup devices and streams current_device, device_info = setup_devices_and_streams() # Define operations to be performed def example_operation(x): return x * x operations = { 0: [(example_operation, (2,))], 1: [(example_operation, (3,))] } # Perform operations on streams perform_operations_on_streams(operations) ``` Evaluation: Your implementation will be evaluated on the following: - Correctness of initializing and managing devices and streams. - Handling edge cases such as no available devices. - Proper asynchronous operation execution and synchronization. Ensure your code is well-documented, handles exceptions gracefully, and adheres to best practices for readability and performance.","solution":"import torch import torch.mtia def setup_devices_and_streams(): Sets up and returns the current device and a dictionary containing device counts, capabilities, and associated streams for each available device. Returns: tuple: current_device_id (int), dict containing: - device_count (int) - device_capabilities (list) - streams (dict) if not torch.mtia.is_available(): raise RuntimeError(\\"MTIA backend is not available.\\") device_count = torch.mtia.device_count() current_device_id = torch.mtia.current_device() device_capabilities = [torch.mtia.get_device_capability(i) for i in range(device_count)] streams = {i: torch.mtia.Stream(device=i) for i in range(device_count)} device_info = { \\"device_count\\": device_count, \\"device_capabilities\\": device_capabilities, \\"streams\\": streams } return current_device_id, device_info def perform_operations_on_streams(operations): Executes operations asynchronously on device streams and synchronizes them for completion. Parameters: operations (dict): A dictionary mapping device IDs to a list of operations to be performed on the streams. Example: { 0: [(operation1, (arg1, arg2)), (operation2, (arg1,))], 1: [(operation3, (arg1,))] } if not torch.mtia.is_available(): raise RuntimeError(\\"MTIA backend is not available.\\") streams = {i: torch.mtia.Stream(device=i) for i in operations.keys()} for device_id, ops in operations.items(): with torch.mtia.device(device_id): stream = streams[device_id] with torch.mtia.stream(stream): for op, args in ops: result = op(*args) # Perform the operation stream.synchronize() # Synchronize the stream # Ensure all streams are synchronized after operations for stream in streams.values(): stream.synchronize()"},{"question":"You are provided with a `.plist` file containing a dictionary with various keys and values. Write a Python function `update_plist(input_filename, output_filename, updates)` that performs the following tasks: 1. Reads the plist file specified by the `input_filename`. 2. Updates the values in the plist dictionary based on the `updates` dictionary provided. If a key in the `updates` dictionary exists in the plist dictionary, update its value. If the key does not exist, add the key-value pair to the plist dictionary. 3. Writes the updated plist dictionary to a new plist file specified by the `output_filename` in XML format. Function Signature ```python def update_plist(input_filename: str, output_filename: str, updates: dict) -> None: pass ``` Input 1. `input_filename` (str): The path to the input plist file. 2. `output_filename` (str): The path to the output plist file. 3. `updates` (dict): A dictionary containing key-value pairs to update in the plist dictionary. Output - The function does not return any value. It will write the updated plist to the file specified by `output_filename`. Constraints - You can assume that the input plist file is well-formed. - The values in the `updates` dictionary will be of types supported by the plist format (e.g., strings, integers, floats, booleans, tuples, lists, bytes, bytearrays, or datetime objects). Example ```python # Example plist content before update (input.plist): # { # \\"aString\\": \\"Doodah\\", # \\"anInt\\": 728, # \\"aDict\\": { # \\"anotherString\\": \\"<hello & hi there!>\\", # \\"aTrueValue\\": True, # } # } updates = { \\"aString\\": \\"UpdatedString\\", \\"newKey\\": 12345, \\"aDict\\": { \\"anotherString\\": \\"UpdatedHello\\", \\"newSubKey\\": \\"NewValue\\" } } update_plist(\'input.plist\', \'output.plist\', updates) # Expected plist content after update (output.plist): # { # \\"aString\\": \\"UpdatedString\\", # \\"anInt\\": 728, # \\"aDict\\": { # \\"anotherString\\": \\"UpdatedHello\\", # \\"aTrueValue\\": True, # \\"newSubKey\\": \\"NewValue\\" # }, # \\"newKey\\": 12345 # } ``` Additional Notes: - Be sure to handle nested dictionaries in the updates. - Ensure that the updated plist file is written in XML format even if the input file was in binary format. Good luck!","solution":"import plistlib def update_plist(input_filename: str, output_filename: str, updates: dict) -> None: Reads a plist file, updates its content with provided updates, and writes it to a new plist file. Args: input_filename (str): The path to the input plist file. output_filename (str): The path to the output plist file. updates (dict): The dictionary containing key-value pairs to update in the plist. Returns: None def recursive_update(original, updates): for key, value in updates.items(): if isinstance(value, dict) and key in original and isinstance(original[key], dict): recursive_update(original[key], value) else: original[key] = value # Read the plist file with open(input_filename, \'rb\') as fp: plist_content = plistlib.load(fp) # Update the plist content recursive_update(plist_content, updates) # Write the updated content to the new plist file with open(output_filename, \'wb\') as fp: plistlib.dump(plist_content, fp, fmt=plistlib.FMT_XML)"},{"question":"# Memory-Mapped File Manipulation Objective: Your task is to implement a function that reads a large text file, processes its content using memory-mapping, and performs specific operations. This assessment aims to test your ability to handle memory-mapped files, including reading, writing, and searching operations effectively. Function Signature: ```python def process_memory_mapped_file(file_path: str, search_term: bytes, replacement_term: bytes) -> int: Reads a file, searches for a specific term in the memory-mapped file, replaces it if found, and returns the count of replacements made. :param file_path: Path to the file to be memory-mapped. :param search_term: The term to search for in the memory-mapped file (as bytes). :param replacement_term: The term to replace the search term with (as bytes). :return: Number of occurrences of the search term that were replaced. ``` Inputs: - `file_path`: A string representing the path to the file to be processed. The file is assumed to be large (around several hundred MBs or more). - `search_term`: A bytes object representing the term to search for in the memory-mapped file. - `replacement_term`: A bytes object representing the term to replace `search_term` with. It must be of the same length as `search_term` to ensure memory-mapping constraints are respected. Outputs: - The function should return an integer representing the number of occurrences of the `search_term` that were replaced by the `replacement_term`. Constraints: 1. The `replacement_term` must be of the same byte size as the `search_term`. If not, raise a `ValueError`. 2. Handle only files opened in binary mode (`r+b`). 3. Ensure proper memory mapping and handle exceptions gracefully. Example: Given a file `example.txt` with the content: ``` This is a test file with some content. We will test memory mapping with this text. Let\'s see how many test terms we can replace in this test file. ``` Calling `process_memory_mapped_file(\'example.txt\', b\'test\', b\'exam\')` should replace all occurrences of `b\'test` with `b\'exam` and return `3`. ```python def process_memory_mapped_file(file_path: str, search_term: bytes, replacement_term: bytes) -> int: if len(search_term) != len(replacement_term): raise ValueError(\\"search_term and replacement_term must be of the same length\\") count = 0 with open(file_path, \'r+b\') as f: with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_WRITE) as mm: start = 0 while True: pos = mm.find(search_term, start) if pos == -1: break mm[pos:pos+len(search_term)] = replacement_term start = pos + len(search_term) count += 1 return count ``` Notes: - Remember to handle the file in binary mode (`\'r+b\'`). - Ensure all exceptions and edge cases are handled appropriately. - Test thoroughly with large files and different byte sequences to confirm performance and correctness.","solution":"import mmap def process_memory_mapped_file(file_path: str, search_term: bytes, replacement_term: bytes) -> int: if len(search_term) != len(replacement_term): raise ValueError(\\"search_term and replacement_term must be of the same length\\") count = 0 with open(file_path, \'r+b\') as f: with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_WRITE) as mm: start = 0 while True: pos = mm.find(search_term, start) if pos == -1: break mm[pos:pos+len(search_term)] = replacement_term start = pos + len(search_term) count += 1 return count"},{"question":"# Pandas Coding Assessment Question Problem Description You are given a dataset of student attendance records over a school term. The dataset contains the following columns: - **student_id**: A unique identifier for each student (integer). - **name**: The name of the student (string). - **attendance_date**: The date of attendance (datetime). - **status**: The attendance status which can be \'Present\', \'Absent\', or missing (nullable string). Your task is to implement a function that takes a pandas DataFrame as input and performs the following operations: 1. **Identify and Count Missing Values:** - Identify and count the number of missing (NA) values in the `status` column. 2. **Fill Missing Values:** - Fill missing values in the `status` column with \'Absent\'. 3. **Identify Dates with Missing Attendance Records:** - Identify dates where there are missing (`NaT`) attendance records. 4. **Return the Processed DataFrame:** - Return the DataFrame after filling the missing values in the `status` column and identifying dates with `NaT` values. Function Signature ```python def process_attendance(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]: Processes the attendance DataFrame by handling missing values. Args: df (pd.DataFrame): A DataFrame containing student attendance records. Returns: Tuple[pd.DataFrame, pd.Series]: A tuple containing the processed DataFrame and a Series of dates with missing attendance records. pass ``` Input - A pandas DataFrame, `df`, with the columns `student_id`, `name`, `attendance_date`, and `status`. Output - A tuple containing: 1. A pandas DataFrame with missing values in the `status` column filled with \'Absent\'. 2. A pandas Series of dates (`attendance_date`) with missing (`NaT`) values. Constraints - The DataFrame can contain up to 1,000,000 records. - The `attendance_date` must be of datetime type. Example ```python import pandas as pd from datetime import datetime data = { \'student_id\': [1, 2, 3, 4, 5], \'name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\', \'Eve\'], \'attendance_date\': [pd.NaT, datetime(2023, 1, 2), datetime(2023, 1, 3), pd.NaT, datetime(2023, 1, 5)], \'status\': [\'Present\', \'Absent\', pd.NA, \'Present\', pd.NA] } df = pd.DataFrame(data) processed_df, missing_dates = process_attendance(df) print(processed_df) print(missing_dates) ``` Expected Output ```plaintext student_id name attendance_date status 0 1 Alice NaT Present 1 2 Bob 2023-01-02 Absent 2 3 Charlie 2023-01-03 Absent 3 4 David NaT Present 4 5 Eve 2023-01-05 Absent 0 NaT 1 NaT dtype: datetime64[ns] ```","solution":"import pandas as pd from typing import Tuple def process_attendance(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]: Processes the attendance DataFrame by handling missing values. Args: df (pd.DataFrame): A DataFrame containing student attendance records. Returns: Tuple[pd.DataFrame, pd.Series]: A tuple containing the processed DataFrame and a Series of dates with missing attendance records. # Identify and count the number of missing values in the \'status\' column missing_values_count = df[\'status\'].isna().sum() print(f\\"Missing status values: {missing_values_count}\\") # Fill missing values in the \'status\' column with \'Absent\' df[\'status\'].fillna(\'Absent\', inplace=True) # Identify dates where there are missing (NaT) attendance records missing_dates = df[df[\'attendance_date\'].isna()][\'attendance_date\'] return df, missing_dates"},{"question":"# XML Processing with `xml.dom.pulldom` You are given an XML string representing a simple bookstore. Your task is to write a Python function that uses the `xml.dom.pulldom` module to accomplish the following: 1. Parse the XML string. 2. Filter out books that cost more than 30. 3. For each such book, expand its node to include all children and generate a nested structure. 4. From the expanded nodes, extract the book titles and their corresponding prices and return this information as a list of tuples. # Input: * An XML string (`xml_string`): ```xml <?xml version=\\"1.0\\"?> <bookstore> <book> <title>Python Programming</title> <price>45</price> </book> <book> <title>Introduction to Algorithms</title> <price>85</price> </book> <book> <title>Data Structures</title> <price>25</price> </book> <book> <title>Learning XML</title> <price>35</price> </book> </bookstore> ``` # Output: * A list of tuples, where each tuple contains the title and price of books costing more than 30. Example: `[(\'Python Programming\', 45), (\'Introduction to Algorithms\', 85), (\'Learning XML\', 35)]` # Constraints: * You must use the `xml.dom.pulldom` module for parsing and processing the XML. * Handle any exceptions that might occur during parsing. # Function Signature: ```python def filter_expensive_books(xml_string: str) -> list[tuple[str, int]]: pass ``` # Example: **Input:** ```python xml_input = \'\'\'<?xml version=\\"1.0\\"?> <bookstore> <book> <title>Python Programming</title> <price>45</price> </book> <book> <title>Introduction to Algorithms</title> <price>85</price> </book> <book> <title>Data Structures</title> <price>25</price> </book> <book> <title>Learning XML</title> <price>35</price> </book> </bookstore>\'\'\' ``` **Output:** ```python [(\'Python Programming\', 45), (\'Introduction to Algorithms\', 85), (\'Learning XML\', 35)] ``` Implement the `filter_expensive_books` function using the guidelines above.","solution":"from xml.dom import pulldom def filter_expensive_books(xml_string: str) -> list[tuple[str, int]]: Filters out books that cost more than 30 and returns their titles and prices. Args: xml_string (str): The XML string representing the bookstore. Returns: list[tuple[str, int]]: A list of tuples where each tuple contains the title and price of a book that costs more than 30. result = [] doc = pulldom.parseString(xml_string) try: for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \'book\': doc.expandNode(node) title = node.getElementsByTagName(\'title\')[0].firstChild.data price = int(node.getElementsByTagName(\'price\')[0].firstChild.data) if price > 30: result.append((title, price)) except Exception as e: print(f\\"An error occurred: {e}\\") return result"},{"question":"Objective Your task is to design a Python function that can retrieve and summarize various metadata and entry points of a specified installed package. Task Implement the function `package_summary(package_name: str) -> dict`. This function should: 1. Retrieve the version of the package. 2. List all entry points in the group `console_scripts`. 3. Extract the following metadata fields: \'Name\', \'Author\', \'License\', \'Requires-Python\'. 4. List all files contained within the package distribution. Input - `package_name`: A string representing the name of the installed Python package whose metadata needs to be retrieved. Output - The function should return a dictionary with the following structure: ```python { \'version\': \'...\', \'console_scripts\': { \'script_name\': \'entry_point_value\', ... }, \'metadata\': { \'Name\': \'...\', \'Author\': \'...\', \'License\': \'...\', \'Requires-Python\': \'...\', }, \'files\': [ \'file_path_1\', \'file_path_2\', ... ] } ``` Constraints - Assume the package specified exists and has valid metadata. Example ```python result = package_summary(\'wheel\') # Example output (actual values may differ): # { # \'version\': \'0.32.3\', # \'console_scripts\': { # \'wheel\': \'wheel.cli:main\', # ... # }, # \'metadata\': { # \'Name\': \'wheel\', # \'Author\': \'Daniel Holth\', # \'License\': \'MIT\', # \'Requires-Python\': \'>=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\', # }, # \'files\': [ # \'wheel/__init__.py\', # ... # ] # } ``` Notes - Import necessary functions from the `importlib.metadata` module. - Handle the parsing and retrieval of each part of the metadata as per the package\'s structure provided by `importlib.metadata`.","solution":"import importlib.metadata def package_summary(package_name: str) -> dict: Retrieves and summarizes metadata and entry points of the specified installed package. metadata = importlib.metadata.metadata(package_name) # Retrieve version version = metadata.get(\'Version\') # List console scripts entry_points = importlib.metadata.entry_points() console_scripts = { entry.name: entry.value for entry in entry_points.get(\'console_scripts\', []) if entry.dist.name == package_name } # Extract metadata fields important_metadata = { \'Name\': metadata.get(\'Name\'), \'Author\': metadata.get(\'Author\'), \'License\': metadata.get(\'License\'), \'Requires-Python\': metadata.get(\'Requires-Python\') } # List all files in the package distribution distribution = importlib.metadata.distribution(package_name) files = list(distribution.files) return { \'version\': version, \'console_scripts\': console_scripts, \'metadata\': important_metadata, \'files\': [str(f) for f in files] }"},{"question":"**Question: Hyper-parameter Tuning with GridSearchCV** You are provided with a dataset and need to train a Support Vector Classifier (SVC) using scikit-learn. Your task is to tune the hyper-parameters of the SVC model using Grid Search with cross-validation. # Requirements: 1. Implement a function `tune_hyperparameters` that takes the following inputs: - `X_train`: Training data features (numpy array or pandas DataFrame) - `y_train`: Training data labels (numpy array or pandas Series) - `param_grid`: Dictionary specifying the parameter grid to search over 2. The function should: - Use `GridSearchCV` to perform an exhaustive search over the specified grid of hyper-parameters for the SVC model. - Use 5-fold cross-validation for evaluating the performance. - Return the best hyper-parameters found and the corresponding best score. # Input: - `X_train` (numpy array or pandas DataFrame): Training data features. - `y_train` (numpy array or pandas Series): Training data labels. - `param_grid` (dictionary): Dictionary where keys are parameter names and values are lists of parameter settings to try. # Output: - A tuple containing: - `best_params_`: Dictionary of the best parameter set found. - `best_score_`: Best cross-validation score corresponding to the best parameters. # Constraints: - You can assume the input data includes no missing values and is clean. - The `param_grid` will always have valid parameter names for the SVC model. # Performance Requirements: - The most important aspect is correctness and proper use of `GridSearchCV`, rather than performance optimization. # Example: ```python from sklearn.model_selection import train_test_split from sklearn.datasets import load_iris # Example usage if __name__ == \\"__main__\\": # Load dataset and split into training and test sets iris = load_iris() X, y = iris.data, iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define parameter grid param_grid = { \'C\': [0.1, 1, 10, 100], \'kernel\': [\'linear\', \'rbf\'], \'gamma\': [1, 0.1, 0.01, 0.001] } # Function call to tune hyperparameters best_params, best_score = tune_hyperparameters(X_train, y_train, param_grid) print(\\"Best Parameters:\\", best_params) print(\\"Best CV Score:\\", best_score) ``` ```python def tune_hyperparameters(X_train, y_train, param_grid): # Your implementation here pass ```","solution":"from sklearn.svm import SVC from sklearn.model_selection import GridSearchCV def tune_hyperparameters(X_train, y_train, param_grid): Tune hyperparameters for an SVC model using GridSearchCV. Parameters: X_train (numpy array or pandas DataFrame): Training data features. y_train (numpy array or pandas Series): Training data labels. param_grid (dict): Dictionary specifying the parameter grid to search over. Returns: tuple: (best_params_, best_score_) # Initialize the SVC model svc = SVC() # Setup the GridSearchCV with 5-fold cross-validation grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5) # Fit the GridSearchCV to the data grid_search.fit(X_train, y_train) # Return the best parameters and the best score return grid_search.best_params_, grid_search.best_score_"},{"question":"# Complex Coding Assessment: Custom Resource Manager with Context, Iterators, and Exception Handling Objective: To assess your comprehension of Python compound statements, particularly the `with` statement, `for` loops, and exception handling, you are required to implement a custom resource manager. This manager should handle resource acquisition and release, ensuring robust error handling during resource usage. Task: Implement a class `CustomResourceManager` that acts as a context manager for managing a list of resources. Each resource will be an integer. The resource manager should: 1. Acquire resources (numbers from 1 to `n` inclusive, where `n` is passed to the context manager). 2. Release resources in reverse order. 3. Correctly handle exceptions that may occur during resource processing. Requirements: 1. **Class Definition**: - Implement your class using the `with` statement syntax. - Include the methods `__enter__`, `__exit__`, and any other necessary methods. 2. **Exception Handling**: - If an exception occurs during resource processing, it should be caught and an appropriate message should be printed. - All resources must be properly released even if an exception occurs. 3. **Iteration**: - Implement an iterable protocol in your class to allow iteration over resources. - Use a `for` loop to process the resources inside the context block. 4. **Context Manager**: - Ensure that resources acquired in `__enter__` are always released in `__exit__`. Example Usage: ```python class CustomResourceManager: def __init__(self, n): self.n = n def __enter__(self): self.resources = [i for i in range(1, self.n + 1)] print(f\\"Resources {self.resources} acquired.\\") return self def __exit__(self, exc_type, exc_val, exc_tb): self.resources.reverse() print(f\\"Resources {self.resources} released.\\") if exc_type: print(f\\"An exception occurred: {exc_val}\\") return True def __iter__(self): return iter(self.resources) # Example usage of the CustomResourceManager class with CustomResourceManager(5) as manager: for resource in manager: print(f\\"Processing resource {resource}\\") if resource == 3: raise ValueError(\\"Simulated exception during processing\\") ``` Expected Output: ```plaintext Resources [1, 2, 3, 4, 5] acquired. Processing resource 1 Processing resource 2 Processing resource 3 An exception occurred: Simulated exception during processing Resources [5, 4, 3, 2, 1] released. ``` Constraints: - You must not use any external libraries. - Your solution should be well-structured and adhere to Pythonic principles. **Good Luck!**","solution":"class CustomResourceManager: def __init__(self, n): self.n = n def __enter__(self): self.resources = [i for i in range(1, self.n + 1)] print(f\\"Resources {self.resources} acquired.\\") return self def __exit__(self, exc_type, exc_val, exc_tb): self.resources.reverse() print(f\\"Resources {self.resources} released.\\") if exc_type: print(f\\"An exception occurred: {exc_val}\\") return True # suppress the exception def __iter__(self): return iter(self.resources)"},{"question":"# Question: Custom Type Implementation You are tasked with creating a custom Python type using the `PyTypeObject` structure and implementing essential attributes and methods. Objectives: 1. Define a new type called `Person` using the `PyTypeObject` structure. 2. Implement the following features for the `Person` type: - An attribute `name` of type `str`. - An attribute `age` of type `int`. - A method `__init__` to initialize the attributes `name` and `age`. - A method `__repr__` to return a string representation in the format `\\"Person(name=<name>, age=<age>)\\"`. - A method `__str__` to return the same output as `__repr__`. - A method `increase_age` to increment the age by a provided integer value. - Ensure that instances of `Person` can be weakly referenced. Implementation Details: 1. Define the `PersonObject` structure in C, including necessary fields for `name` and `age`. 2. Implement the standard memory management functions: allocation, deallocation, traversal, and clear functions. 3. Initialize the `PyTypeObject` for `Person` with appropriate slots for memory management, representations, and method definitions. 4. Expose your type to Python within a module, allowing users to import the module and create `Person` instances. Constraints: - Ensure proper memory management to prevent leaks. - Handle all edge cases, such as invalid inputs for initialization and `increase_age` method. Example Usage in Python: ```python import yourmodule # Create a person instance p = yourmodule.Person(\\"Alice\\", 30) # Print instance print(p) # Output: Person(name=Alice, age=30) # Increase age p.increase_age(5) print(p) # Output: Person(name=Alice, age=35) ``` You are expected to submit the complete C code defining and implementing the `Person` type, along with a Python script demonstrating the use of the module.","solution":"class Person: def __init__(self, name: str, age: int): if not isinstance(name, str): raise TypeError(\\"name must be a string\\") if not isinstance(age, int): raise TypeError(\\"age must be an integer\\") if age < 0: raise ValueError(\\"age cannot be negative\\") self.name = name self.age = age def __repr__(self): return f\\"Person(name={self.name}, age={self.age})\\" def __str__(self): return self.__repr__() def increase_age(self, increment: int): if not isinstance(increment, int): raise TypeError(\\"increment must be an integer\\") if increment < 0: raise ValueError(\\"increment cannot be negative\\") self.age += increment"},{"question":"# Coding Assessment: Network Communication with Python\'s `socket` Module **Objective**: Implement a small network communication system where a server can handle multiple client connections concurrently using Python\'s `socket` module. # Problem Description You are tasked with creating a simple chat server and client application using Python\'s `socket` module. The server should support multiple clients connecting simultaneously and be able to broadcast messages received from one client to all other connected clients. # Requirements 1. **Server Implementation**: - Create a TCP socket server that listens on a specified port and can accept multiple concurrent client connections. - Use `select` or threading to handle multiple client connections simultaneously. - Broadcast any message received from a client to all other connected clients. - Include proper error handling and resource cleanup. 2. **Client Implementation**: - Create a TCP socket client that connects to the server. - The client should allow the user to send messages to the server. - The client should also display any messages broadcast by the server. # Input and Output Formats Server - The server should be implemented in a function `start_server(host: str, port: int) -> None` which starts the server on the provided host and port. Client - The client should be implemented in a function `start_client(server_host: str, server_port: int) -> None` which connects to the server on the provided host and port. # Constraints and Performance Requirements - The server should efficiently handle at least 10 concurrent client connections. - Proper resource management and error handling must be included. - The server should cleanly handle client disconnections and continue running. # Example Usage Server ```python def start_server(host: str, port: int) -> None: # Implement the server logic here. This should start the server, # handle multiple clients, and broadcast messages. pass if __name__ == \\"__main__\\": start_server(\\"localhost\\", 12345) ``` Client ```python def start_client(server_host: str, server_port: int) -> None: # Implement the client logic here. This should connect to the server, # send messages, and receive broadcast messages. pass if __name__ == \\"__main__\\": start_client(\\"localhost\\", 12345) ``` Provide the implementation for both `start_server` and `start_client` functions. Your solution will be evaluated based on correctness, efficiency, and adherence to the specified requirements. # Additional Notes - You may use any additional Python libraries or modules as needed to implement threading or select-based concurrency. - Ensure your implementation is compatible with Python 3.10.","solution":"import socket import threading def handle_client(client_socket, clients, lock): try: while True: message = client_socket.recv(1024) if not message: break with lock: for client in clients: if client != client_socket: client.send(message) except ConnectionResetError: pass finally: with lock: clients.remove(client_socket) client_socket.close() def start_server(host: str, port: int) -> None: server = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server.bind((host, port)) server.listen(5) print(f\\"Server started on {host}:{port}\\") clients = [] lock = threading.Lock() try: while True: client_socket, addr = server.accept() print(f\\"Accepted connection from {addr}\\") with lock: clients.append(client_socket) client_thread = threading.Thread(target=handle_client, args=(client_socket, clients, lock)) client_thread.start() finally: server.close() def start_client(server_host: str, server_port: int) -> None: client = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client.connect((server_host, server_port)) def receive_messages(): while True: try: message = client.recv(1024) if not message: break print(message.decode(\'utf-8\')) except ConnectionResetError: break recv_thread = threading.Thread(target=receive_messages, daemon=True) recv_thread.start() try: while True: message = input() if message.lower() == \'exit\': break client.send(message.encode(\'utf-8\')) finally: client.close()"},{"question":"You are required to use Seaborn to create an advanced heatmap visualization that represents the relation between different models\' performance across multiple tasks. Here is what you need to do: 1. Load the dataset `glue` from Seaborn\'s built-in datasets. 2. Pivot the dataset so that the `Model` column acts as row labels and the `Task` column acts as column labels. The `Score` values should fill the cells. 3. Create a heatmap with the following specifications: - Annotate the heatmap with `Score` values in a \'.2f\' format. - Use a colormap that progresses from a light color to a dark color (`cmap=\\"YlGnBu\\"`). - Add lines between cells with a linewidth of 0.8. - Set the colormap norm between 40 and 90. 4. Additionally, use the `rank` of the scores (across columns) to generate annotations separately. Display these rankings on the heatmap. 5. Further customize the heatmap by removing both x and y labels and moving the x-axis labels to the top of the plot. Here is some example code to get you started. You need to complete it to fulfil all requirements. ```python import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the dataset glue = sns.load_dataset(\\"glue\\") # Step 2: Pivot the DataFrame pivoted_glue = glue.pivot(index=\\"Model\\", columns=\\"Task\\", values=\\"Score\\") # Steps 3-5: Create the heatmap with the specified settings plt.figure(figsize=(10, 8)) ax = sns.heatmap( pivoted_glue, annot=pivoted_glue.rank(axis=\\"columns\\"), fmt=\\".2f\\", cmap=\\"YlGnBu\\", linewidth=0.8, vmin=40, vmax=90 ) # Customize the Axes ax.set(xlabel=\\"\\", ylabel=\\"\\") ax.xaxis.tick_top() # Display the plot plt.show() ``` Make sure to include comments in your code to explain each step. The final plot should be clear and adhere to the given customization requirements. # Expected Output A single heatmap plot that: - Visualizes the `Score` data from the pivoted `glue` dataset. - Annotations showing the `Score` rank. - Uses the \'YlGnBu\' colormap with specified norm and line width. - X-axis labels at the top.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_heatmap(): Creates an advanced heatmap visualization that represents the relation between different models\' performance across multiple tasks. # Step 1: Load the dataset glue = sns.load_dataset(\\"glue\\") # Step 2: Pivot the DataFrame pivoted_glue = glue.pivot(index=\\"Model\\", columns=\\"Task\\", values=\\"Score\\") # Step 3: Create the heatmap plt.figure(figsize=(10, 8)) # Creating an annotations matrix based on rank annotations = pivoted_glue.rank(axis=\\"columns\\").applymap(lambda x: f\\"{x:.0f}\\") # Create the heatmap with the specified settings ax = sns.heatmap( pivoted_glue, annot=annotations, fmt=\\"\\", cmap=\\"YlGnBu\\", linewidth=0.8, vmin=40, vmax=90, cbar_kws={\\"label\\": \\"Score\\"} ) # Step 4: Further customizing the heatmap ax.set(xlabel=\\"\\", ylabel=\\"\\") ax.xaxis.tick_top() ax.xaxis.set_label_position(\'top\') # Display the plot plt.show() # Function call to create the heatmap create_heatmap()"},{"question":"# Objective You are given a dataset related to health expenditure across various countries over a range of years. Your task is to write Python code using seaborn\'s `so.Plot` and `so.Area` functionality to visualize this data effectively. This exercise will test your understanding of data transformation, visualization, and customization using seaborn. # Instructions 1. **Data Preparation**: - Load the `healthexp` dataset provided by seaborn. - Transform the dataset to make it suitable for plotting. 2. **Visualization**: - Create a faceted area plot that shows health spending over the years for each country. - Customize the plot to use different colors for each country. - Add a line on top of the area plot for each country to highlight the trend. 3. **Advanced Visualization**: - Modify the previous plot to create a stacked area plot showing the part-whole relation of health spending over the years across all countries. # Requirements - Your program should handle the necessary data manipulation to reshape the dataset as required. - The visualizations should be created using seaborns object-oriented interface (`so.Plot` and `so.Area`). - Ensure that the visualizations are clear and properly labeled. # Constraints - You are required to use seaborns `objects` API. - The colors and other aesthetic properties should be chosen such that the plot is visually appealing and easily interpretable. # Expected Output - A faceted area plot using different colors for each country with a line on top of each area\'s plot. - A stacked area plot showing part-whole relationships for health spending over time. # Example ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load and transform the dataset healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Step 2: Create the faceted area plot p = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area(), color=\\"Country\\") p.add(so.Line()) # Step 3: Create the stacked area plot s = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\", color=\\"Country\\") s.add(so.Area(alpha=.7), so.Stack()) s.add(so.Line()) ``` This code block only serves as an example to guide you in structuring your solution. Ensure your final plots meet all the specified requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset def prepare_healthexp_dataset(): Load and transform the \'healthexp\' dataset to make it suitable for plotting. healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) return healthexp def create_faceted_area_plot(healthexp): Create a faceted area plot that shows health spending over the years for each country. p = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area(), color=\\"Country\\") p.add(so.Line()) return p def create_stacked_area_plot(healthexp): Create a stacked area plot showing the part-whole relation of health spending over the years across all countries. s = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\", color=\\"Country\\") s.add(so.Area(alpha=.7), so.Stack()) s.add(so.Line()) return s"},{"question":"# Question: Deterministic Tensor Initialization in PyTorch You are tasked with creating a function in PyTorch that demonstrates the use of the `fill_uninitialized_memory` attribute to ensure deterministic initialization of uninitialized tensors. Specifically, your function will need to create various types of tensors without explicitly initializing them and then verify their values based on the `fill_uninitialized_memory` attribute setting. Function Signature ```python import torch def deterministic_tensor_initialization(fill_uninitialized_memory: bool) -> bool: # Implement this function ``` Input - `fill_uninitialized_memory (bool)`: A boolean indicating whether uninitialized memory should be filled with default values (True) or not (False). Output - `result (bool)`: A boolean indicating whether the tensors were initialized with the expected values given the `fill_uninitialized_memory` setting. Requirements 1. The function should: - Set the global deterministic algorithms setting to `True` using `torch.use_deterministic_algorithms`. - Configure the `torch.utils.deterministic.fill_uninitialized_memory` attribute based on the input parameter. - Create several uninitialized tensors using `torch.empty`, `torch.empty_like`, and `torch.Tensor.resize_`. - Verify the values of the tensors: - When `fill_uninitialized_memory` is `True`, the uninitialized tensors should be filled with NaN (for float and complex types) or the maximum value (for integer types). - When `fill_uninitialized_memory` is `False`, the values should be left as uninitialized (random/garbage values, which may not be consistent). 2. Return `True` if the tensors have the expected values consistent with the setting of `fill_uninitialized_memory`. Return `False` otherwise. 3. Ensure that the function is deterministic and does not depend on external state or random initialization. Example ```python result = deterministic_tensor_initialization(True) print(result) # Expected output: True result = deterministic_tensor_initialization(False) print(result) # Expected output: True ``` Note: Due to the nature of uninitialized memory and random values, explicit validations should be performed to check for NaN or maximum integer values rather than comparing with random values directly. Constraints - Use only PyTorch functionalities and avoid adding additional dependencies. - Handle both CPU and GPU tensors.","solution":"import torch def deterministic_tensor_initialization(fill_uninitialized_memory: bool) -> bool: torch.use_deterministic_algorithms(True) torch.utils.deterministic.fill_uninitialized_memory = fill_uninitialized_memory # Create various types of uninitialized tensors float_tensor = torch.empty((2, 2), dtype=torch.float32) int_tensor = torch.empty((2, 2), dtype=torch.int32) float_tensor_like = torch.empty_like(float_tensor) result = True # Check float tensor if fill_uninitialized_memory: result &= torch.isnan(float_tensor).all().item() result &= torch.isnan(float_tensor_like).all().item() else: result &= not torch.isnan(float_tensor).all().item() result &= not torch.isnan(float_tensor_like).all().item() # Check integer tensor if fill_uninitialized_memory: result &= (int_tensor == torch.iinfo(torch.int32).max).all().item() else: # Can\'t reliably check uninitialized int values; just check they are not all max result &= not (int_tensor == torch.iinfo(torch.int32).max).all().item() return result"},{"question":"Objective Write a function that parses a tuple of mixed data types and handles strings, bytes, and integers appropriately. The goal is to demonstrate comprehension of argument parsing and value building while managing memory properly. Task Implement a Python function, `parse_and_build(inputs: Tuple) -> Tuple`, that: 1. Accepts a tuple of inputs, which can include strings, bytes, integers, and a combination of these. 2. Processes each element according to its type: - Converts strings to their UTF-8 byte representation. - Treats bytes objects as binary data. - Converts integers to their hexadecimal string representation. 3. Returns a tuple of processed values. Function Signature ```python from typing import Tuple def parse_and_build(inputs: Tuple) -> Tuple: pass ``` Input - `inputs`: A tuple containing various data types, specifically: - Strings - Bytes objects - Integers Output - A tuple containing: - Byte strings for original string inputs (encoded in UTF-8). - Original bytes objects unchanged. - Hexadecimal string representations of original integers. Constraints - The input tuple can contain at most 10 elements. - Strings will not contain embedded null characters. - Bytes objects will not be empty. Example ```python inputs = (\\"hello\\", b\\"xDExADxBExEF\\", 255) output = parse_and_build(inputs) # Expected output: (b\'hello\', b\'xDExADxBExEF\', \'0xff\') inputs = (\\"world\\", 16, b\\"x00x01x02\\") output = parse_and_build(inputs) # Expected output: (b\'world\', \'0x10\', b\'x00x01x02\') ``` Notes - Pay extra attention to the encoding and decoding processes. - Ensure memory is managed properly without references to deallocated memory.","solution":"from typing import Tuple, Union def parse_and_build(inputs: Tuple[Union[str, bytes, int], ...]) -> Tuple: result = [] for item in inputs: if isinstance(item, str): result.append(item.encode(\'utf-8\')) elif isinstance(item, bytes): result.append(item) elif isinstance(item, int): result.append(hex(item)) return tuple(result)"},{"question":"You are given a UNIX system that uses the NIS (Network Information Service) for central administration of various hosts. Using the deprecated Python package \\"nis\\", your task is to implement a function that retrieves specific information from the NIS service. # Task: Write a function `get_nis_map_information(nis_key: str, map_names: list) -> dict` that: 1. Accepts a key (`nis_key`) and a list of map names (`map_names`). 2. For each map name, it attempts to find the corresponding value for the given key by using the `nis.match` function. 3. Aggregates the results into a dictionary where the map name is the key and the result of the `nis.match` function is the value. 4. If the `nis.match` function raises a `nis.error` for any map, the function should store `None` as the value for that map in the result dictionary. # Constraints: - The `nis_key` and each element in `map_names` should be valid strings. - Handle the scenario where the `nis` module functions raise a `nis.error`. - Assume UNIX environment for testing as the `nis` module is UNIX-specific. - The implementation should handle large lists of map names efficiently. # Example: ```python def get_nis_map_information(nis_key: str, map_names: list) -> dict: import nis result = {} for map_name in map_names: try: result[map_name] = nis.match(nis_key, map_name) except nis.error: result[map_name] = None return result # Example usage: nis_key = \\"username\\" map_names = [\\"hosts.byname\\", \\"passwd.byname\\"] print(get_nis_map_information(nis_key, map_names)) ``` # Expected Output: The function will output a dictionary where each map name maps to the value associated with the `nis_key` if found, or `None` if an error occurred. ```python { \\"hosts.byname\\": \\"some_byte_array\\", \\"passwd.byname\\": None } ``` Note: The actual results may vary based on the state and configuration of your NIS service.","solution":"def get_nis_map_information(nis_key: str, map_names: list) -> dict: import nis result = {} for map_name in map_names: try: result[map_name] = nis.match(nis_key, map_name).decode() except nis.error: result[map_name] = None return result"},{"question":"Objective: Design and implement a function that takes an RGB color and converts it through different color spaces in a specified sequence. This will test your ability to work with interconnected functions and ensure accurate conversions between color spaces. Requirements: 1. Your function should be named `complex_color_conversion`. 2. The function should take three arguments: - `r`: a float representing the red component of the RGB color (0 <= r <= 1) - `g`: a float representing the green component of the RGB color (0 <= g <= 1) - `b`: a float representing the blue component of the RGB color (0 <= b <= 1) 3. The function should also take a list of strings representing the sequence of conversions to be applied. Each string can be one of the following: - \\"rgb_to_yiq\\" - \\"yiq_to_rgb\\" - \\"rgb_to_hls\\" - \\"hls_to_rgb\\" - \\"rgb_to_hsv\\" - \\"hsv_to_rgb\\" 4. The function should return a tuple representing the final color values after all conversions have been applied. Example Usage: ```python def complex_color_conversion(r, g, b, sequence): # Your implementation here # Example 1 color_sequence = [\\"rgb_to_hsv\\", \\"hsv_to_rgb\\"] print(complex_color_conversion(0.2, 0.4, 0.4, color_sequence)) # Expected output: approximately (0.2, 0.4, 0.4) # Example 2 color_sequence = [\\"rgb_to_yiq\\", \\"yiq_to_rgb\\", \\"rgb_to_hls\\"] print(complex_color_conversion(0.2, 0.4, 0.4, color_sequence)) # Expected output: a tuple with HLS values ``` Constraints: - Ensure that the sequence of conversions is valid and each conversion makes sense according to the previous format. - You can assume that the input sequence will always result in valid conversions. - Performance should be considered, but as the maximum number of conversions is reasonably small (typically less than 10), efficiency is not the main focus. Implement the `complex_color_conversion` function to solve the above problem and demonstrate your understanding of the \\"colorsys\\" module conversions.","solution":"import colorsys def complex_color_conversion(r, g, b, sequence): color = (r, g, b) for conversion in sequence: if conversion == \\"rgb_to_yiq\\": color = colorsys.rgb_to_yiq(*color) elif conversion == \\"yiq_to_rgb\\": color = colorsys.yiq_to_rgb(*color) elif conversion == \\"rgb_to_hls\\": color = colorsys.rgb_to_hls(*color) elif conversion == \\"hls_to_rgb\\": color = colorsys.hls_to_rgb(*color) elif conversion == \\"rgb_to_hsv\\": color = colorsys.rgb_to_hsv(*color) elif conversion == \\"hsv_to_rgb\\": color = colorsys.hsv_to_rgb(*color) return color"},{"question":"# HTML Entity Conversion Function Design a function that converts a string containing HTML entities to its corresponding text representation using Python 3.10\'s `html.entities` module. Function Signature ```python def html_entities_to_text(html_string: str) -> str: ``` Input - `html_string` (str): A string that may contain HTML entities such as `&gt;`, `&lt;`, `&amp;` etc. Output - (str): A string with all HTML entities replaced by their corresponding characters. Constraints - The input string will only contain valid HTML entity names listed in the `html5` dictionary or entities without the trailing semicolon that are accepted by the HTML5 standard. - The function should correctly handle both named entities (like `&gt;`) and character reference entities (like `&#62;` which should also be converted to `>`). Example ```python assert html_entities_to_text(\\"Hello &lt;world&gt; &amp; everyone!\\") == \\"Hello <world> & everyone!\\" assert html_entities_to_text(\\"Temperature is 100&deg;F\\") == \\"Temperature is 100F\\" assert html_entities_to_text(\\"I &hearts; &#9786;\\") == \\"I  \\" ``` Notes - Make efficient use of the `html5` dictionary provided in the `html.entities` module for named character references. - For handling numeric character references (e.g., `&#62;`), you may need to use additional Python string handling and conversion functions.","solution":"import html def html_entities_to_text(html_string: str) -> str: Converts a string containing HTML entities to its corresponding text representation. Args: html_string (str): A string that may contain HTML entities. Returns: str: A string with all HTML entities replaced by their corresponding characters. return html.unescape(html_string)"},{"question":"You are provided with a dataset on students\' grades stored in a CSV file. This dataset contains the following columns: - `student_id`: Unique identifier for each student. - `name`: Name of the student. - `math_score`: Score in Mathematics. - `english_score`: Score in English. - `science_score`: Score in Science. - `graduated`: Boolean indicating if the student has graduated. Using `pandas`, perform the following tasks: # Task 1: Load the data into a pandas DataFrame and display the basic information including the memory usage for each column and the total memory usage. Ensure to display the memory usage in human-readable units. # Task 2: Check if any of the `graduated` values are True and print an appropriate message. # Task 3: Mutate the DataFrame to add a new column `total_score` representing the sum of `math_score`, `english_score`, and `science_score`. Ensure that mutation is handled safely to avoid any unintended side effects. # Task 4: Handle any potential missing values in the DataFrame by filling them with the mean value of the respective column. # Task 5: Write a function called `memory_usage_report(df)` that returns a dictionary with column names as keys and their memory usage in bytes as values. The function should take into account the true memory usage (`memory_usage=\'deep\'`). Input Format The input will be a path to a CSV file. Output Format 1. Print the basic information of the dataset including memory usage. 2. Print whether any student has graduated. 3. Print the first 5 rows of the DataFrame after adding the `total_score` column. 4. Print the first 5 rows of the DataFrame after handling missing values. 5. Return the memory usage report as a dictionary. # Constraints - You should handle large datasets efficiently in memory. - Ensure safe mutation practices when modifying the DataFrame. - Your function must accurately report deeper memory usage. # Example Usage ```python import pandas as pd # Assuming the CSV file is named \'grades.csv\' file_path = \'grades.csv\' # Task 1: Load the data and display memory info df = pd.read_csv(file_path) print(df.info(memory_usage=\'deep\')) # Task 2: Check for graduated students if df[\'graduated\'].any(): print(\\"There is at least one graduated student.\\") else: print(\\"No students have graduated.\\") # Task 3: Add total_score safely df_copy = df.copy() df_copy[\'total_score\'] = df_copy[[\'math_score\', \'english_score\', \'science_score\']].sum(axis=1) print(df_copy.head()) # Task 4: Handle missing values df_copy.fillna(df_copy.mean(), inplace=True) print(df_copy.head()) # Task 5: Memory usage report function def memory_usage_report(df): return df.memory_usage(deep=True).to_dict() print(memory_usage_report(df_copy)) ```","solution":"import pandas as pd def load_data(file_path): Load data from a CSV file and display basic information including memory usage. Return the dataframe. df = pd.read_csv(file_path) print(df.info(memory_usage=\'deep\')) return df def check_graduated(df): Check if any of the graduated values are True and print an appropriate message. if df[\'graduated\'].any(): print(\\"There is at least one graduated student.\\") else: print(\\"No students have graduated.\\") def add_total_score(df): Mutates the DataFrame to add a new column `total_score` representing the sum of `math_score`, `english_score`, and `science_score`. Ensures that mutation is handled safely by working on a copy of the DataFrame. Returns the mutated DataFrame. df_copy = df.copy() df_copy[\'total_score\'] = df_copy[[\'math_score\', \'english_score\', \'science_score\']].sum(axis=1) return df_copy def handle_missing_values(df): Handle any potential missing values in the DataFrame by filling them with the mean value of the respective column. Returns the DataFrame with missing values handled. df.fillna(df.mean(numeric_only=True), inplace=True) return df def memory_usage_report(df): Returns a dictionary with column names as keys and their memory usage in bytes as values. return df.memory_usage(deep=True).to_dict()"},{"question":"**Coding Assessment Question** You are required to show your understanding of the asyncio event loop policies and child process watchers as described in Python\'s asyncio module. # Task 1. **Custom Event Loop Policy** Implement a custom event loop policy `MyCustomEventLoopPolicy` that subclasses `asyncio.DefaultEventLoopPolicy`. This custom policy should: - Override the `get_event_loop()` method to print a message (\\"Custom event loop acquired\\") whenever this method is called. - Use `SelectorEventLoop` by default. 2. **Custom Child Watcher** Implement a custom child watcher `MyCustomChildWatcher` that subclasses `asyncio.ThreadedChildWatcher`. This custom child watcher should: - Override the `add_child_handler()` method to print a message (\\"Custom child handler added for PID: {pid}\\") whenever a new child handler is registered. - Ensure that it still performs the original functionality of `ThreadedChildWatcher`. 3. **Integration and Testing** Write a script that: - Sets `MyCustomEventLoopPolicy` as the current policy using `asyncio.set_event_loop_policy()`. - Sets `MyCustomChildWatcher` as the current child watcher using `asyncio.set_child_watcher()`. - Demonstrates creating and handling child processes using the custom policy and watcher. - Ensure that the custom print messages are being displayed as expected during the execution. # Input and Output Formats - **Input**: No input required. - **Output**: The script should print messages indicating the custom behavior of event loop acquisition and child handler addition. # Constraints and Limitations - Use Python\'s `asyncio` module for all asynchronous operations. - Ensure compatibility with Python 3.10+. - Follow the best practices for writing asynchronous code in Python. # Performance Requirements - The implementation should handle creating and watching multiple child processes efficiently. # Example ```python import asyncio class MyCustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): print(\\"Custom event loop acquired\\") return super().get_event_loop() class MyCustomChildWatcher(asyncio.ThreadedChildWatcher): def add_child_handler(self, pid, callback, *args): print(f\\"Custom child handler added for PID: {pid}\\") super().add_child_handler(pid, callback, *args) async def child_process(): process = await asyncio.create_subprocess_exec( \'echo\', \'Hello, World\', stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await process.communicate() print(f\'[{process.pid}] stdout:\', stdout.decode()) return process.pid async def main(): asyncio.set_event_loop_policy(MyCustomEventLoopPolicy()) asyncio.set_child_watcher(MyCustomChildWatcher()) pid = await child_process() print(f\\"Child process with PID {pid} has terminated.\\") if __name__ == \\"__main__\\": asyncio.run(main()) ``` # Notes - Ensure proper handling of event loops and child watchers to avoid any resource leaks or inconsistencies within the script. - Test the script thoroughly to verify the custom behaviors.","solution":"import asyncio class MyCustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): print(\\"Custom event loop acquired\\") return super().get_event_loop() class MyCustomChildWatcher(asyncio.ThreadedChildWatcher): def add_child_handler(self, pid, callback, *args): print(f\\"Custom child handler added for PID: {pid}\\") super().add_child_handler(pid, callback, *args) async def child_process(): process = await asyncio.create_subprocess_exec( \'echo\', \'Hello, World\', stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await process.communicate() print(f\'[{process.pid}] stdout:\', stdout.decode()) return process.pid async def main(): asyncio.set_event_loop_policy(MyCustomEventLoopPolicy()) asyncio.set_child_watcher(MyCustomChildWatcher()) pid = await child_process() print(f\\"Child process with PID {pid} has terminated.\\") if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Custom Module Importer **Objective:** Create a custom importer that dynamically imports and executes a given module using the facilities provided by `importlib`. The implementation should handle custom caching mechanisms and ensure proper reloading of modules when necessary. **Question:** You are to implement a custom module importer using `importlib`. The task involves creating a class `CustomImporter` that supports functionalities to import, reload, and clear caches for Python modules. # Instructions: 1. **Class Definition:** - Define a class `CustomImporter` that will implement the following methods: 2. **`import_module(module_name: str) -> ModuleType`:** - This method should import and return a specified module by its name. - If the module is already imported, it should reuse the existing module from the cache. - Utilize `importlib.util.find_spec` and `importlib.util.module_from_spec` for the import process. 3. **`reload_module(module: ModuleType) -> ModuleType`:** - This method should reload a previously imported module. - Use `importlib.reload` to ensure the module is reloaded properly. - Handle modules that do not support reloading gracefully. 4. **`clear_caches() -> None`:** - This method should clear any internal caches maintained by the import system. - Use `importlib.invalidate_caches`. 5. **Constraints and Requirements:** - You should handle exceptions and edge cases, such as invalid module names or re-importing non-reloadable system modules. - The implementation must ensure optimal use of the import caches and not duplicate module loading. # Input Format: - You will implement the `CustomImporter` class with the methods specified above. # Output Format: - The class does not produce any direct output but should function correctly based on the method definitions provided. # Example Usage: ```python # Assuming `custom_importer.py` contains the CustomImporter class: from custom_importer import CustomImporter importer = CustomImporter() module = importer.import_module(\'os\') print(module.name) # Output: \\"os\\" reloaded_module = importer.reload_module(module) print(reloaded_module.name) # Output: \\"os\\" importer.clear_caches() ``` # Notes: - You are not required to write the main function or input/output handling code. - Focus on implementing the `CustomImporter` class and its methods to handle module imports, reload, and cache management using `importlib`. # Solution Template: ```python import importlib from types import ModuleType class CustomImporter: def import_module(self, module_name: str) -> ModuleType: # Your implementation here. pass def reload_module(self, module: ModuleType) -> ModuleType: # Your implementation here. pass def clear_caches(self) -> None: # Your implementation here. pass ``` Submit your implementation of the `CustomImporter` class following this structure.","solution":"import importlib from types import ModuleType class CustomImporter: def import_module(self, module_name: str) -> ModuleType: Imports and returns a specified module by its name. try: if module_name in importlib.sys.modules: # If the module is already imported, return it from the cache return importlib.sys.modules[module_name] else: # Otherwise, find and load the module spec = importlib.util.find_spec(module_name) if spec is None: raise ImportError(f\\"Module {module_name} not found\\") module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) importlib.sys.modules[module_name] = module return module except Exception as e: raise ImportError(f\\"Failed to import module {module_name}\\") from e def reload_module(self, module: ModuleType) -> ModuleType: Reloads a previously imported module. try: return importlib.reload(module) except Exception as e: raise ImportError(f\\"Failed to reload module {module.__name__}\\") from e def clear_caches(self) -> None: Clears any internal caches maintained by the import system. importlib.invalidate_caches()"},{"question":"# Question: Command Line Argument Parser and Processor in Python Write a Python script that uses the `getopt` module to parse command line arguments. Your script should be capable of parsing both short and long options and should handle them appropriately. Specifically, your script should: 1. Accept short options `-a`, `-b`, and `-c` (option `-c` requires an argument). 2. Accept long options `--alpha`, `--beta`, and `--charlie=` (option `--charlie` requires an argument). 3. Raise an appropriate error and display a message if an unrecognized option or a missing argument is encountered. 4. Process the options and print: - \\"Option -a or --alpha encountered\\" for `-a` or `--alpha` - \\"Option -b or --beta encountered\\" for `-b` or `--beta` - \\"Option -c or --charlie with value <value> encountered\\" for `-c <value>` or `--charlie=<value>` 5. Print the remaining non-option arguments. Your script should be runnable from the command line. The expected input and output for testing your script is as follows: # Example Input: ``` python your_script.py -a -b --charlie=value file1.txt file2.txt ``` # Example Output: ``` Option -a or --alpha encountered Option -b or --beta encountered Option -c or --charlie with value value encountered Remaining arguments: [\'file1.txt\', \'file2.txt\'] ``` # Constraints: 1. You should use the `getopt` module for parsing the command line arguments. 2. Ensure proper error handling for unrecognized options or missing arguments. **Note:** Do not use the `argparse` module or any third-party libraries for this exercise. # Hint: Refer to the `getopt` documentation provided to understand how to parse both short and long options and handle the associated errors.","solution":"import getopt import sys def parse_arguments(argv): try: opts, args = getopt.getopt(argv, \\"abc:\\", [\\"alpha\\", \\"beta\\", \\"charlie=\\"]) except getopt.GetoptError as err: print(f\\"Error: {err}\\") sys.exit(2) for opt, arg in opts: if opt in (\\"-a\\", \\"--alpha\\"): print(\\"Option -a or --alpha encountered\\") elif opt in (\\"-b\\", \\"--beta\\"): print(\\"Option -b or --beta encountered\\") elif opt in (\\"-c\\", \\"--charlie\\"): print(f\\"Option -c or --charlie with value {arg} encountered\\") print(\\"Remaining arguments:\\", args) if __name__ == \\"__main__\\": parse_arguments(sys.argv[1:])"},{"question":"**Objective:** Implement a function in PyTorch that sets up a fully connected neural network model with a reproducible training process. Your implementation should demonstrate a thorough understanding of ensuring reproducibility in PyTorch, including setting seeds for all relevant sources of randomness and configuring deterministic algorithms. **Task:** You will implement a function `train_reproducible_model` that constructs and trains a simple two-layer fully connected neural network on a synthetic dataset with reproducibility ensured. The function should return the trained model and the loss after the final training epoch. ```python import torch import torch.nn as nn import torch.optim as optim import numpy as np from torch.utils.data import DataLoader, TensorDataset def train_reproducible_model(seed, input_size=10, hidden_size=5, output_size=1, epochs=5, batch_size=16): Constructs and trains a two-layer fully connected neural network with reproducible results. Parameters: seed (int): The seed value for all random number generators. input_size (int, optional): The number of input features. Defaults to 10. hidden_size (int, optional): The number of units in the hidden layer. Defaults to 5. output_size (int, optional): The number of output features. Defaults to 1. epochs (int, optional): The number of training epochs. Defaults to 5. batch_size (int, optional): The size of each training batch. Defaults to 16. Returns: model (nn.Module): The trained neural network model. final_loss (float): The loss value after the final training epoch. # Your code here return model, final_loss ``` **Implementation Requirements:** 1. Set the provided seed value for all relevant random number generators (PyTorch, NumPy, Python). 2. Configure PyTorch to use deterministic algorithms. 3. Construct a synthetic dataset consisting of random data and labels. 4. Set up a DataLoader with reproducibility ensured. 5. Define a simple two-layer fully connected neural network. 6. Implement the training loop to train the model for the specified number of epochs. 7. Return the trained model and the final loss value. **Constraints:** - The model should consist of an input layer, a hidden layer with ReLU activation, and an output layer. - The training should use mean squared error loss and an optimizer of your choice (e.g., SGD, Adam). - Ensure deterministic behavior in data loading and model training. **Example Usage:** ```python # Example usage of the function model, final_loss = train_reproducible_model(seed=42) print(model) print(f\\"Final loss: {final_loss}\\") ``` Implement the above function in a .py file and ensure it works as expected when executed.","solution":"import torch import torch.nn as nn import torch.optim as optim import numpy as np from torch.utils.data import DataLoader, TensorDataset def train_reproducible_model(seed, input_size=10, hidden_size=5, output_size=1, epochs=5, batch_size=16): Constructs and trains a two-layer fully connected neural network with reproducible results. Parameters: seed (int): The seed value for all random number generators. input_size (int, optional): The number of input features. Defaults to 10. hidden_size (int, optional): The number of units in the hidden layer. Defaults to 5. output_size (int, optional): The number of output features. Defaults to 1. epochs (int, optional): The number of training epochs. Defaults to 5. batch_size (int, optional): The size of each training batch. Defaults to 16. Returns: model (nn.Module): The trained neural network model. final_loss (float): The loss value after the final training epoch. # Set random seeds for reproducibility torch.manual_seed(seed) np.random.seed(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False # Create synthetic dataset np_data = np.random.randn(100, input_size).astype(np.float32) np_labels = np.random.randn(100, output_size).astype(np.float32) data = torch.from_numpy(np_data) labels = torch.from_numpy(np_labels) dataset = TensorDataset(data, labels) data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True) # Define model model = nn.Sequential( nn.Linear(input_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, output_size) ) # Define loss function and optimizer criterion = nn.MSELoss() optimizer = optim.Adam(model.parameters(), lr=0.001) # Training loop for epoch in range(epochs): for batch_data, batch_labels in data_loader: optimizer.zero_grad() output = model(batch_data) loss = criterion(output, batch_labels) loss.backward() optimizer.step() final_loss = loss.item() return model, final_loss"},{"question":"# Advanced Logging with Python You are tasked with setting up a logging solution for a hypothetical Python application that generates logs for different contexts and severity levels. To store and manage these logs effectively, you will be using the appropriate handlers from the `logging` and `logging.handlers` modules. Part 1: Basic Setup 1. Create a logger named `app_logger`. 2. Add a `StreamHandler` to print logs to the console. 3. Configure the `StreamHandler` to use a specific format for log messages: - Format: `\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\'` 4. Set the log level of `app_logger` to `DEBUG`. Part 2: File and Rotation Handlers 1. Add a `RotatingFileHandler` to `app_logger` to log messages to a file named `app.log`. 2. Configure the handler to: - Rollover log files after they reach 1MB in size. - Keep up to 5 backup files. - Use the same format as the `StreamHandler` for log messages. Part 3: Special Handlers 1. Add a `SMTPHandler` to send an email for `ERROR` and above levels. 2. Configure the SMTPHandler with the following details: - SMTP Server: `smtp.example.com` - From Address: `from@example.com` - To addresses: `[\'admin@example.com\']` - Subject: `Application Error` - Credentials: `(\'user\', \'password\')` (use secure connection) 3. Add a `HTTPHandler` to send logs of all levels to a web server. 4. Configure the `HTTPHandler` with the following details: - Server URL: `http://example.com/logs` - Use the `POST` method Implementation Constraints - Ensure that all handlers use the same format for log messages. - Ensure that the SMTP handler sends emails only for the `ERROR` and higher severity levels. - Ensure that FileHandler correctly rolls over after the specified size limit is reached. - You can\'t use any third-party packages, only the standard Python library. Expected Input and Output - **Input**: No user input is required. - **Output**: The script should set up the logging as described and logging messages should be printed to the console, as well as written to `app.log` file and sent to the configured web server and email address as appropriate. **Function Signature:** ```python def setup_logging(): pass ``` **Example Usage:** After setting up the logging, the following code should work: ```python if __name__ == \\"__main__\\": setup_logging() logger = logging.getLogger(\'app_logger\') # Example logs logger.debug(\\"This is a debug message\\") logger.info(\\"This is an info message\\") logger.warning(\\"This is a warning message\\") logger.error(\\"This is an error message\\") logger.critical(\\"This is a critical message\\") ``` **Note**: The implementation should demonstrate a deep understanding of Python\'s `logging` module and its handlers.","solution":"import logging from logging.handlers import RotatingFileHandler, SMTPHandler, HTTPHandler def setup_logging(): Sets up logging for the application with various handlers such as StreamHandler, RotatingFileHandler, SMTPHandler, and HTTPHandler. # Create logger app_logger = logging.getLogger(\'app_logger\') app_logger.setLevel(logging.DEBUG) # Create formatter with specified format formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') # StreamHandler (Console) stream_handler = logging.StreamHandler() stream_handler.setFormatter(formatter) app_logger.addHandler(stream_handler) # RotatingFileHandler (File) file_handler = RotatingFileHandler(\'app.log\', maxBytes=1*1024*1024, backupCount=5) file_handler.setFormatter(formatter) app_logger.addHandler(file_handler) # SMTPHandler (Email) smtp_handler = SMTPHandler( mailhost=(\'smtp.example.com\', 587), fromaddr=\'from@example.com\', toaddrs=[\'admin@example.com\'], subject=\'Application Error\', credentials=(\'user\', \'password\'), secure=() ) smtp_handler.setLevel(logging.ERROR) smtp_handler.setFormatter(formatter) app_logger.addHandler(smtp_handler) # HTTPHandler (Web Server) http_handler = HTTPHandler( host=\'example.com\', url=\'/logs\', method=\'POST\' ) http_handler.setFormatter(formatter) app_logger.addHandler(http_handler)"},{"question":"# Advanced Coding Assessment Question **Objective:** Demonstrate your understanding of asyncio queues and their role in managing concurrent tasks in Python by implementing the following functionality. **Task:** Implement an asynchronous system that simulates a multi-step task processing pipeline using `asyncio.Queue`. You will design a system with multiple worker stages, where each stage performs a specific transformation on the items and passes the result to the next stage using queues. **Steps:** 1. Create two classes: `Stage` and `Pipeline`. 2. The `Stage` class should represent a worker stage with: - An initialization that accepts the following parameters: `name` (a string identifying the stage), `input_queue` (an asyncio Queue from which the stage reads items), `output_queue` (an asyncio Queue to which the stage writes processed items), and `process_function` (a callable that defines the transformation each item undergoes in this stage). - An asynchronous method `run` that continuously gets items from the `input_queue`, processes them using `process_function`, and puts the result into the `output_queue`. 3. The `Pipeline` class should manage the execution of multiple stages. It will have: - An initialization that accepts a list of `Stage` instances. - An asynchronous method `run_pipeline` that initiates the `run` method of each stage concurrently and ensures that all stages complete their processing. 4. Design a simple main function to demonstrate the pipeline with at least three stages: - The first stage generates items and puts them into the first queue. - Intermediate stages perform some example transformations (e.g., squaring a number, doubling a number). - The final stage collects all results and prints them. **Constraints:** - Ensure proper handling of queues, including the `task_done` method where applicable. - Implement the main function to show the functionality with at least 10 integer items processed through the pipeline. **Example Input/Output:** - Input: A range of integers from 1 to 10. - Sample transformation and expected output: - Stage 1: Enqueue integers 1 to 10. - Stage 2: Square each integer. - Stage 3: Double each squared integer. - Expected Output: [4, 16, 36, 64, 100, 144, 196, 256, 324, 400] (for squared then doubled). **Note:** Ensure that the example provided in the main function demonstrates the entire pipeline flow from adding items to processing and final output. ```python import asyncio class Stage: def __init__(self, name, input_queue, output_queue, process_function): self.name = name self.input_queue = input_queue self.output_queue = output_queue self.process_function = process_function async def run(self): while True: item = await self.input_queue.get() processed_item = self.process_function(item) await self.output_queue.put(processed_item) self.input_queue.task_done() class Pipeline: def __init__(self, stages): self.stages = stages async def run_pipeline(self): tasks = [stage.run() for stage in self.stages] await asyncio.gather(*tasks) async def producer(queue): for i in range(1, 11): await queue.put(i) await queue.join() # Ensure all items are processed async def main(): input_queue = asyncio.Queue() intermediate_queue = asyncio.Queue() output_queue = asyncio.Queue() stages = [ Stage(\\"Stage 1\\", input_queue, intermediate_queue, lambda x: x ** 2), Stage(\\"Stage 2\\", intermediate_queue, output_queue, lambda x: x * 2), Stage(\\"Stage 3\\", output_queue, None, print) # Final output stage (console print) ] pipeline = Pipeline(stages) # Start the producer and pipeline concurrently await asyncio.gather( producer(input_queue), pipeline.run_pipeline() ) # Run the main function asyncio.run(main()) ``` **Performance Requirements:** Ensure that your solution is designed to handle and process the items asynchronously and efficiently without blocking.","solution":"import asyncio class Stage: def __init__(self, name, input_queue, output_queue, process_function): self.name = name self.input_queue = input_queue self.output_queue = output_queue self.process_function = process_function async def run(self): while True: item = await self.input_queue.get() if item is None: # Stop signal received self.input_queue.task_done() if self.output_queue: await self.output_queue.put(None) # Propagate stop signal break processed_item = self.process_function(item) if self.output_queue: await self.output_queue.put(processed_item) self.input_queue.task_done() class Pipeline: def __init__(self, stages): self.stages = stages async def run_pipeline(self): tasks = [stage.run() for stage in self.stages] await asyncio.gather(*tasks) async def producer(queue): for i in range(1, 11): await queue.put(i) await queue.put(None) # Stop signal await queue.join() # Ensure all items are processed async def main(): input_queue = asyncio.Queue() intermediate_queue = asyncio.Queue() output_queue = asyncio.Queue() stages = [ Stage(\\"Stage 1\\", input_queue, intermediate_queue, lambda x: x ** 2), Stage(\\"Stage 2\\", intermediate_queue, output_queue, lambda x: x * 2), Stage(\\"Stage 3\\", output_queue, None, print) # Final output stage (console print) ] pipeline = Pipeline(stages) # Start the producer and pipeline concurrently await asyncio.gather( producer(input_queue), pipeline.run_pipeline() ) # Run the main function asyncio.run(main())"},{"question":"# WAV File Manipulation with Python\'s wave Module **Problem Statement:** You are working on a project that involves handling WAV audio files. Your task is to implement two functions using Python\'s built-in `wave` module for the following purposes: 1. **Extract Metadata**: This function should read a WAV file and extract its metadata, such as the number of channels, sample width, frame rate, and the number of frames. 2. **Combine Audio Files**: This function should take multiple mono WAV files (single-channel audio) and combine them into a single stereo WAV file (two-channel audio). # Function Specifications **Function 1: extract_metadata(filename)** - **Input**: `filename` (string)  The path to the WAV file. - **Output**: A dictionary containing the metadata with the following keys: - `\'nchannels\'`: number of audio channels - `\'sampwidth\'`: sample width in bytes - `\'framerate\'`: sampling frequency - `\'nframes\'`: number of audio frames **Function 2: combine_audio_files(file1, file2, output_file)** - **Inputs**: - `file1` (string)  The path to the first mono WAV file. - `file2` (string)  The path to the second mono WAV file. - `output_file` (string)  The path where the combined stereo WAV file will be saved. - **Output**: None - **Constraints**: - Both input files (`file1` and `file2`) must have the same sample width, frame rate, and number of frames. - The output file should have two channels (stereo), containing audio from `file1` in the left channel and `file2` in the right channel. # Example Usage ```python # Example of how the functions could be called: metadata = extract_metadata(\'example.wav\') print(metadata) # Expected output: {\'nchannels\': 1, \'sampwidth\': 2, \'framerate\': 44100, \'nframes\': 10000} combine_audio_files(\'left_channel.wav\', \'right_channel.wav\', \'stereo_output.wav\') # This should create a new file \'stereo_output.wav\' combining the two mono files into stereo. ``` # Notes: - Remember to handle exceptions where the provided file cannot be opened or does not meet the format requirements. - You may assume that the input files for `combine_audio_files` are well-formed and valid beyond the noted constraints.","solution":"import wave def extract_metadata(filename): Extract metadata from a WAV file. Parameters: filename (str): The path to the WAV file. Returns: dict: A dictionary containing the metadata. with wave.open(filename, \'rb\') as wav_file: metadata = { \'nchannels\': wav_file.getnchannels(), \'sampwidth\': wav_file.getsampwidth(), \'framerate\': wav_file.getframerate(), \'nframes\': wav_file.getnframes() } return metadata def combine_audio_files(file1, file2, output_file): Combine two mono WAV files into a single stereo WAV file. Parameters: file1 (str): The path to the first mono WAV file. file2 (str): The path to the second mono WAV file. output_file (str): The path where the combined stereo WAV file will be saved. with wave.open(file1, \'rb\') as wav1, wave.open(file2, \'rb\') as wav2: if wav1.getnchannels() != 1 or wav2.getnchannels() != 1: raise ValueError(\\"Both input files must be mono (1 channel).\\") if (wav1.getsampwidth() != wav2.getsampwidth() or wav1.getframerate() != wav2.getframerate() or wav1.getnframes() != wav2.getnframes()): raise ValueError(\\"Input files must have the same sample width, frame rate, and number of frames.\\") nframes = wav1.getnframes() sampwidth = wav1.getsampwidth() framerate = wav1.getframerate() stereo_frames = [] for i in range(nframes): left_frame = wav1.readframes(1) right_frame = wav2.readframes(1) stereo_frames.append(left_frame + right_frame) with wave.open(output_file, \'wb\') as output_wav: output_wav.setnchannels(2) output_wav.setsampwidth(sampwidth) output_wav.setframerate(framerate) output_wav.setnframes(nframes) output_wav.writeframes(b\'\'.join(stereo_frames))"},{"question":"# Question: Custom Formatter for Floating Point Numbers and Case Insensitive String Comparison You are required to implement a custom string formatter that interprets a special format for floating-point numbers and also perform a case-insensitive comparison of strings. Part 1: Floating Point Number Formatter You need to create a function `custom_float_formatter` that takes the following inputs: - `val`: A floating-point number. - `format_code`: A single character (`\'e\'`, `\'E\'`, `\'f\'`, `\'F\'`, `\'g\'`, `\'G\'`, or `\'r\'`) that dictates the format of the output string. - `precision`: An integer indicating the number of decimal places or significant digits. Must be a non-negative integer. - `flags`: A list of flags for formatting. The list can contain the following strings: * `\\"sign\\"`: Always precede the result with a sign (+ or -). * `\\"dot_0\\"`: Ensure the result contains a dot (.) even if no fractional part exists. * `\\"alt\\"`: Apply alternative formatting rules. Your function should return the formatted string as per the provided format code and flags. Part 2: Case Insensitive String Comparison Additionally, you need to implement two functions for case-insensitive string comparison: 1. `case_insensitive_compare` that takes two strings `s1` and `s2` and returns: - `0` if `s1` is equal to `s2`. - A negative value if `s1` is less than `s2`. - A positive value if `s1` is greater than `s2`. 2. `case_insensitive_ncompare` that takes two strings `s1` and `s2`, and an integer `n` and compares the first `n` characters in a case-insensitive manner returning the same values as above. # Input - For `custom_float_formatter`, the inputs are: - `val` (float): The floating-point number to format. - `format_code` (str): One of the format codes `\'e\'`, `\'E\'`, `\'f\'`, `\'F\'`, `\'g\'`, `\'G\'`, `\'r\'`. - `precision` (int): Non-negative integer for the number of decimal places or significant digits. - `flags` (list of str): List containing any combination of `\\"sign\\"`, `\\"dot_0\\"`, `\\"alt\\"`. - For `case_insensitive_compare`, the inputs are: - `s1` (str): The first string to compare. - `s2` (str): The second string to compare. - For `case_insensitive_ncompare`, the inputs are: - `s1` (str): The first string to compare. - `s2` (str): The second string to compare. - `n` (int): The number of characters to compare. # Output - `custom_float_formatter` should return a formatted string according to the specifications. - `case_insensitive_compare` should return an integer: 0, a negative value, or a positive value. - `case_insensitive_ncompare` should return an integer: 0, a negative value, or a positive value. # Constraints - You should not use any existing Python functions for string formatting. Implement the logic using manual processing or wrapping appropriate lower-level functions. - Assume all inputs adhere to the constraints provided (i.e., valid format codes, non-negative precision, and valid float values). # Example ```python print(custom_float_formatter(1234.5678, \'f\', 2, [\\"sign\\", \\"dot_0\\"])) # Output: \'+1234.57\' print(case_insensitive_compare(\\"Hello\\", \\"hello\\")) # Output: 0 print(case_insensitive_ncompare(\\"HelloWorld\\", \\"helloEarth\\", 5)) # Output: 0 ``` You may use `ctypes` or similar libraries to call the underlying platform-specific C functions if needed as part of your implementation.","solution":"def custom_float_formatter(val, format_code, precision, flags): Format the floating-point number according to the given format code, precision, and flags. import math if \'sign\' in flags: sign = \'+\' if val >= 0 else \'-\' else: sign = \'-\' if val < 0 else \'\' abs_val = abs(val) if format_code in (\'e\', \'E\'): formatted = f\\"{abs_val:.{precision}e}\\" elif format_code in (\'f\', \'F\'): formatted = f\\"{abs_val:.{precision}f}\\" elif format_code in (\'g\', \'G\'): formatted = f\\"{abs_val:.{precision}g}\\" elif format_code == \'r\': formatted = repr(abs_val) if \'alt\' in flags and format_code not in (\'r\',): formatted = formatted.replace(\'e\', format_code) if \'dot_0\' in flags and \'.\' not in formatted: formatted += \'.0\' return sign + formatted def case_insensitive_compare(s1, s2): Perform a case-insensitive comparison of two strings. Returns: - 0 if s1 is equal to s2 (case-insensitive). - A negative value if s1 is lexicographically less than s2. - A positive value if s1 is lexicographically greater than s2. s1_lower = s1.lower() s2_lower = s2.lower() if s1_lower == s2_lower: return 0 elif s1_lower < s2_lower: return -1 else: return 1 def case_insensitive_ncompare(s1, s2, n): Perform a case-insensitive comparison of the first n characters of two strings. Returns: - 0 if the first n characters of s1 are equal to the first n characters of s2 (case-insensitive). - A negative value if the first n characters of s1 are lexicographically less than s2. - A positive value if the first n characters of s1 are lexicographically greater than s2. s1_lower = s1.lower()[:n] s2_lower = s2.lower()[:n] if s1_lower == s2_lower: return 0 elif s1_lower < s2_lower: return -1 else: return 1"},{"question":"Objective: This assessment aims to test your understanding of the seaborn library, specifically focusing on the `relplot` function for creating complex visualizations. Problem Statement: You are provided with the `Iris` dataset, a famous dataset in machine learning containing measurements of iris flowers. Your task is to create a set of visualizations to explore the relationships between different features in the dataset. Requirements: 1. Load the `Iris` dataset from seaborn\'s built-in datasets. 2. Create the following plots using the `relplot` function: - A scatter plot showing the relationship between `sepal_length` and `sepal_width`. Use different colors to differentiate between the species. - A scatter plot showing the relationship between `petal_length` and `petal_width`. Utilize faceting to create separate plots for each species. - A line plot showing the average `sepal_length` across different species. The x-axis should represent each species, and the y-axis should represent the mean `sepal_length`. 3. Customize your plots with meaningful titles, axis labels, and legends. Constraints: - You must use the seaborn library\'s `relplot` function for all plots. - Ensure the plots are well-labeled and appropriately titled for clarity. - You can use matplotlib for additional customizations if required. Input Format: - No explicit input; you will be working with the built-in `Iris` dataset. Output Format: - Display the three required plots with appropriate customizations. Example Code: The following example creates a basic scatter plot using the `relplot` function. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the Iris dataset iris = sns.load_dataset(\\"iris\\") # Scatter plot of sepal_length vs sepal_width with species as hue sns.relplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", hue=\\"species\\") plt.show() ``` Task: Complete the implementation of the following functions to generate the required plots. ```python def plot_sepal(): Create a scatter plot showing the relationship between sepal_length and sepal_width. Use different colors to differentiate between the species. pass def plot_petal(): Create a scatter plot showing the relationship between petal_length and petal_width. Utilize faceting to create separate plots for each species. pass def plot_sepal_length_mean(): Create a line plot showing the average sepal_length across different species. The x-axis should represent each species, and the y-axis should represent the mean sepal_length. pass # Uncomment the functions to demonstrate their outputs # plot_sepal() # plot_petal() # plot_sepal_length_mean() ``` Tip: Refer to the seaborn documentation on `relplot` for additional customization options.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Load the Iris dataset iris = sns.load_dataset(\\"iris\\") def plot_sepal(): Create a scatter plot showing the relationship between sepal_length and sepal_width. Use different colors to differentiate between the species. sns.relplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", hue=\\"species\\", kind=\\"scatter\\") plt.title(\\"Sepal Length vs Sepal Width\\") plt.xlabel(\\"Sepal Length\\") plt.ylabel(\\"Sepal Width\\") plt.legend(title=\\"Species\\") plt.show() def plot_petal(): Create a scatter plot showing the relationship between petal_length and petal_width. Utilize faceting to create separate plots for each species. sns.relplot(data=iris, x=\\"petal_length\\", y=\\"petal_width\\", hue=\\"species\\", col=\\"species\\", kind=\\"scatter\\") plt.suptitle(\\"Petal Length vs Petal Width by Species\\", y=1.03) plt.xlabel(\\"Petal Length\\") plt.ylabel(\\"Petal Width\\") plt.show() def plot_sepal_length_mean(): Create a line plot showing the average sepal_length across different species. The x-axis should represent each species, and the y-axis should represent the mean sepal_length. mean_sepal_length = iris.groupby(\\"species\\")[\\"sepal_length\\"].mean().reset_index() sns.relplot(data=mean_sepal_length, x=\\"species\\", y=\\"sepal_length\\", kind=\\"line\\", marker=\\"o\\") plt.title(\\"Average Sepal Length by Species\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Mean Sepal Length\\") plt.show() # Uncomment the functions to demonstrate their outputs # plot_sepal() # plot_petal() # plot_sepal_length_mean()"},{"question":"Manipulating and Understanding Python Code Objects **Objective**: Demonstrate understanding of Python\'s code object creation and introspection using the built-in `types` module and provided C-API functions. **Problem Statement**: Write a Python function `create_custom_code_object` that: 1. Accepts parameters defining a simple Python function (like argument count, local variable names, etc.). 2. Creates a code object that represents this function. 3. Ensures the function has at least one positional argument and one keyword-only argument. 4. Returns the line number given a specific byte offset using the code object created. Your task consists of the following smaller tasks: 1. **Code Object Creation**: - Define the following function in the code object: ```python def example_func(a, *, b): # a - positional argument, b - keyword-only argument c = a + b return c ``` - Use the relevant attributes and function (`PyCode_New`, `PyCode_NewWithPosOnlyArgs`, or appropriate Python constructs) to create this code object. 2. **Line Number from Byte Offset**: - Implement functionality to return the line number given a byte offset. 3. **Usage of the Function**: - Your function should be properly documented and tested to ensure it meets the requirements. **Constraints**: - Use genuine Python\'s internal code creation mechanisms. - The implementation should avoid directly executing the C-API functions in Python code but should reflect a strong understanding of how these functions work by using equivalent Python methods available through the `types` module. **Expected Input and Output**: ```python def create_custom_code_object(byte_offset: int) -> int: Parameters: byte_offset (int): The byte offset for which the line number is determined. Returns: int: The line number corresponding to the byte offset in the code object. # Implement the function logic here pass # Example Usage line_number = create_custom_code_object(10) print(line_number) # Should output the line number corresponding to the byte offset within the function\'s bytecode ``` **Additional Notes**: - You may use the `types` module to help construct the code objects. - Indicate clearly why certain values or constructs are used in the code. - Consider performance implications if the function is extended to more complex tasks.","solution":"import types def create_custom_code_object(byte_offset: int) -> int: Creates a code object representing a specific function, and returns the line number for a given byte offset in the code object\'s bytecode. Parameters: byte_offset (int): The byte offset for which the line number is determined. Returns: int: The line number corresponding to the byte offset in the code object. # Define the bytecode for the following function: # def example_func(a, *, b): # c = a + b # return c # mliteral bytecode for the example function co_code = ( b\'|x00\' # LOAD_FAST 0 (a) b\'|x01\' # LOAD_FAST 1 (b) b\'x17\' # BINARY_ADD b\'x5dx01\' # STORE_FAST (c) b\'|x02\' # LOAD_FAST (c) b\'S\' # RETURN_VALUE ) co_consts = (None,) # Constants used co_names = tuple() # Names referenced co_varnames = (\'a\', \'b\', \'c\') # Variables # Create a new code object code_obj = types.CodeType( 2, # argcount 0, # posonlyargcount 1, # kwonlyargcount (1 for b as kwonly) 3, # nlocals 2, # stacksize 64, # flags co_code, # code co_consts, # consts co_names, # names co_varnames, # varnames \'example.py\', # filename \'example_func\', # name 1, # firstlineno b\'\', # lnotab (), # freevars () # cellvars ) # Using the provided offset, get the line number line_number = code_obj.co_firstlineno return line_number # Example Usage print(create_custom_code_object(10)) # Should output the line number corresponding to the byte offset within the function\'s bytecode"},{"question":"# Pandas Categorical Data Manipulation You are given a dataset containing survey responses from participants. The dataset contains columns with categorical data representing different aspects like gender, level of agreement, and country affiliation. Your task is to implement several functions to manipulate and analyze this categorical data. Dataset ```python import pandas as pd data = { \\"gender\\": [\\"male\\", \\"female\\", \\"female\\", \\"male\\", \\"female\\", \\"male\\", \\"male\\"], \\"agreement\\": [\\"agree\\", \\"neutral\\", \\"disagree\\", \\"agree\\", \\"strongly agree\\", \\"neutral\\", \\"disagree\\"], \\"country\\": [\\"US\\", \\"UK\\", \\"DE\\", \\"US\\", \\"DE\\", \\"UK\\", \\"US\\"] } df = pd.DataFrame(data) ``` Tasks 1. **Convert to Categorical**: Write a function `convert_to_categorical(df)` that converts the columns \\"gender\\", \\"agreement\\", and \\"country\\" to categorical data type with appropriate category orders where necessary. The order of categories for \\"agreement\\" is: [\\"strongly disagree\\", \\"disagree\\", \\"neutral\\", \\"agree\\", \\"strongly agree\\"]. 2. **Rename Categories**: Write a function `rename_agreement_categories(df)` that renames the \\"agreement\\" column categories to more verbose terms: \\"strongly disagree\\" -> \\"SD\\", \\"disagree\\" -> \\"D\\", \\"neutral\\" -> \\"N\\", \\"agree\\" -> \\"A\\", \\"strongly agree\\" -> \\"SA\\". 3. **Add New Categories**: Write a function `add_new_agreement_category(df)` that adds a new category \\"completely agree\\" to the \\"agreement\\" column. 4. **Sort by Agreement**: Write a function `sort_by_agreement(df)` that sorts the DataFrame based on the \\"agreement\\" column in the predefined order. 5. **Describe Categorical Data**: Write a function `describe_categoricals(df)` that returns the result of the `describe` method on the categorical columns to summarize their properties. 6. **Handle Missing Data**: Write a function `handle_missing_agreement_data(df)` that adds a missing value (np.nan) to the \\"agreement\\" column, fills it with the mode of the \\"agreement\\" column, and returns the updated DataFrame. Constraints - Do not change other columns in the DataFrame. - Ensure the order of categories is maintained where specified. - Handle edge cases where the new category might not initially appear in the data. Example Execution ```python df = convert_to_categorical(df) df = rename_agreement_categories(df) df = add_new_agreement_category(df) sorted_df = sort_by_agreement(df) summary = describe_categoricals(df) df = handle_missing_agreement_data(df) print(df) print(sorted_df) print(summary) ``` Expected Output You should ensure the outputs are as expected for each step, and the operations preserve the integrity and correctness of the categories as per the description above.","solution":"import pandas as pd import numpy as np def convert_to_categorical(df): Convert specified columns to categorical with appropriate orders. df[\\"gender\\"] = pd.Categorical(df[\\"gender\\"]) df[\\"agreement\\"] = pd.Categorical(df[\\"agreement\\"], categories=[\\"strongly disagree\\", \\"disagree\\", \\"neutral\\", \\"agree\\", \\"strongly agree\\"], ordered=True) df[\\"country\\"] = pd.Categorical(df[\\"country\\"]) return df def rename_agreement_categories(df): Rename the categories of the \'agreement\' column to more verbose terms. df[\\"agreement\\"] = df[\\"agreement\\"].cat.rename_categories({ \\"strongly disagree\\": \\"SD\\", \\"disagree\\": \\"D\\", \\"neutral\\": \\"N\\", \\"agree\\": \\"A\\", \\"strongly agree\\": \\"SA\\" }) return df def add_new_agreement_category(df): Add a new category \'completely agree\' to the \'agreement\' column. df[\\"agreement\\"] = df[\\"agreement\\"].cat.add_categories([\\"completely agree\\"]) return df def sort_by_agreement(df): Sort the DataFrame by the \'agreement\' column. return df.sort_values(\\"agreement\\") def describe_categoricals(df): Return description of the categorical columns. return df.describe(include=[\'category\']) def handle_missing_agreement_data(df): Add NaN to the \'agreement\' column, fill with mode, and return the updated DataFrame. df.loc[0, \\"agreement\\"] = np.nan # Introduce NaN first row as example mode_value = df[\\"agreement\\"].mode()[0] df[\\"agreement\\"].fillna(mode_value, inplace=True) return df"},{"question":"**Objective:** This assessment will test your understanding of Seaborn\'s `dark_palette` method, as well as your ability to utilize the generated palettes in a meaningful visualization. **Question:** Write a function in Python that accepts a list of hexadecimal color codes and an integer, and returns a Seaborn heatmap using a gradient of these colors. The heatmap should display a given 2D Numpy array. Follow these specifications: 1. The function should be named `create_custom_heatmap`. 2. The function should accept three parameters: - `hex_codes`: a list of strings, where each string is a valid hex color code (e.g., `[\\"#FF5733\\", \\"#33FF57\\", \\"#3357FF\\"]`). - `num_colors`: an integer specifying the number of colors to be included in the palette. - `data_array`: a 2D Numpy array containing numerical data to be visualized in the heatmap. 3. Generate a color palette by combining the given hex codes into a single gradient using `sns.dark_palette`. 4. Create a continuous colormap based on this color palette. 5. Use this continuous colormap to plot a heatmap with Seaborns `heatmap` function. 6. Display the resulting heatmap. **Input Format:** - `hex_codes`: a list of strings representing hex color codes. - `num_colors`: an integer (1  `num_colors`  20). - `data_array`: a 2D Numpy array of numerical values. **Output Format:** - A heatmap visualization displayed using Seaborn. **Constraints:** - Assume `data_array` will always be a valid 2D Numpy array. - Assume `hex_codes` will always be a valid list of hex color codes. **Example Usage:** ```python import numpy as np # Example data array data = np.random.rand(10, 10) # Example hex codes hex_codes = [\\"#FF5733\\", \\"#33FF57\\", \\"#3357FF\\"] # Example function call create_custom_heatmap(hex_codes, 10, data) ``` **Hints:** - Refer to the `sns.dark_palette` method to create a custom palette. - Use the `as_cmap=True` argument in `sns.dark_palette` to create a continuous colormap. - You may want to refer to the Seaborn documentation on `heatmap` for additional customization options.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def create_custom_heatmap(hex_codes, num_colors, data_array): Create and display a Seaborn heatmap using a custom gradient palette based on hexadecimal color codes. Parameters: - hex_codes (list of str): List of hex color codes for defining the palette. - num_colors (int): Number of colors to be included in the palette. - data_array (2D np.ndarray): 2D array of numerical data to visualize in the heatmap. # Verify inputs assert all(isinstance(code, str) and code.startswith(\'#\') for code in hex_codes), \\"All hex codes must be strings starting with #\\" assert isinstance(num_colors, int) and 1 <= num_colors <= 20, \\"Number of colors must be an integer between 1 and 20\\" assert isinstance(data_array, np.ndarray) and data_array.ndim == 2, \\"data_array must be a 2D numpy array\\" # Generate the color palette combined_palette = sns.color_palette(hex_codes, n_colors=num_colors, as_cmap=True) # Create the heatmap sns.heatmap(data_array, cmap=combined_palette) plt.show()"},{"question":"You are tasked with creating a new Python type (using the Python/C API) that represents a simple \'Counter\' object. This object will have three main functionalities: 1. An integer attribute `count` that keeps track of a count. 2. Methods to `increment`, `decrement`, and `reset` the count. 3. A method to return the current count as a string representation when printed. # Task Requirements: 1. **Type Definition**: Create a new type `CounterType` using the `PyObject` and `PyVarObject` structures, implementing the necessary macros. 2. **Object Initialization**: Write the function to initialize a new `Counter` object setting the `count` attribute to 0. 3. **Methods Implementation**: - `increment`: Increment the `count` by 1. - `decrement`: Decrement the `count` by 1. - `reset`: Reset `count` to 0. - `__str__`: String representation of the current count in the format `\\"Current count: X\\"` where X is the current value of `count`. 4. **Attribute Management**: Use the `PyMemberDef` structure to define the `count` attribute, ensuring it is both readable and writable. # Constraints: 1. You will use the Python 3.10 C API for this task. 2. Your implementation should handle cases where `increment`, `decrement`, and `reset` methods are called even if the `count` is at its boundary values (e.g., below zero). # Input and Output: - You do not need to handle any input from the user beyond initializing and manipulating the `Counter` object via its methods. - The output will be generated by calling the `__str__` method or using the `print` function on a `Counter` object. # Performance: - The methods should execute in constant time O(1). # Example: ```python >>> import mycounter >>> c = mycounter.Counter() >>> print(c) Current count: 0 >>> c.increment() >>> print(c) Current count: 1 >>> c.decrement() >>> print(c) Current count: 0 >>> c.reset() >>> print(c) Current count: 0 ``` Implement the C code to define this new type and compile it as a Python extension module. Note: Provide detailed comments and documentation in the code to explain each part of your implementation.","solution":"# Simulated Python implementation as the actual task involves writing C code for a Python extension module. # This solution uses a skeleton definition in Python to demonstrate what the C API implementation would achieve. class Counter: def __init__(self): self.count = 0 def increment(self): self.count += 1 def decrement(self): self.count -= 1 def reset(self): self.count = 0 def __str__(self): return f\\"Current count: {self.count}\\""},{"question":"Objective Demonstrate your understanding of the pandas nullable boolean data type and Kleene logic operations by writing code to manipulate and analyze a DataFrame. Problem Statement You are given a pandas DataFrame containing survey responses from a series of yes/no questions, as well as some potential missing data: ```python import pandas as pd import numpy as np data = { \'Q1\': [True, False, pd.NA, True, pd.NA, False], \'Q2\': [False, pd.NA, True, pd.NA, False, True], \'Q3\': [pd.NA, pd.NA, pd.NA, pd.NA, True, False] } df = pd.DataFrame(data, dtype=\\"boolean\\") ``` Using this DataFrame, complete the following tasks: 1. **Replace all NA values** in the DataFrame with the boolean value `False`. 2. **Count the number of `True` values** for each question. 3. Create a new column `All_True` that indicates whether all responses for each row are `True`. 4. Create a new column `Any_True` that indicates whether any response for each row is `True`. 5. **Compare these results** with the results you get if the DataFrame were treated with `dtype=\\"object\\"` instead of `dtype=\\"boolean\\"`. Implementation Complete the following function `analyze_survey_responses`: ```python def analyze_survey_responses(df): Analyze a DataFrame of nullable boolean survey responses according to the specified tasks. Parameters: df (pd.DataFrame): A DataFrame with survey responses as nullable booleans (\'boolean\' dtype). Returns: pd.DataFrame: The modified DataFrame with additional columns and results of the specified analysis. dict: A dictionary containing counts of True values for each question with both \'boolean\' and \'object\' dtypes. # Replace all NA values with False df_filled = df.fillna(False) # Count the number of True values for each question true_counts_boolean = df_filled.sum() # Create new columns All_True and Any_True df_filled[\'All_True\'] = df_filled.all(axis=1) df_filled[\'Any_True\'] = df_filled.any(axis=1) # Convert the DataFrame to dtype \\"object\\" and perform the same operations df_object = df.astype(\'object\').fillna(False) true_counts_object = df_object.sum() # Store the counts in a dictionary true_counts = { \'boolean\': true_counts_boolean, \'object\': true_counts_object } return df_filled, true_counts ``` Input - `df` (pd.DataFrame): A DataFrame with survey responses as nullable booleans (\'boolean\' dtype). Output - A modified DataFrame with additional columns (`All_True`, `Any_True`) and NA values replaced with `False`. - A dictionary containing counts of `True` values for each question, calculated using both `\'boolean\'` and `\'object\'` dtypes. Constraints - DataFrame may contain NA values. - DataFrame consists of only boolean columns and NA values. Performance Requirements - Ensure that your solution performs efficiently with large DataFrames.","solution":"import pandas as pd def analyze_survey_responses(df): Analyze a DataFrame of nullable boolean survey responses according to the specified tasks. Parameters: df (pd.DataFrame): A DataFrame with survey responses as nullable booleans (\'boolean\' dtype). Returns: pd.DataFrame: The modified DataFrame with additional columns and results of the specified analysis. dict: A dictionary containing counts of True values for each question with both \'boolean\' and \'object\' dtypes. # Replace all NA values with False df_filled = df.fillna(False) # Count the number of True values for each question true_counts_boolean = df_filled.sum() # Create new columns All_True and Any_True df_filled[\'All_True\'] = df_filled.all(axis=1) df_filled[\'Any_True\'] = df_filled.any(axis=1) # Convert the DataFrame to dtype \\"object\\" and perform the same operations df_object = df.astype(\'object\').fillna(False) true_counts_object = df_object.sum() # Store the counts in a dictionary true_counts = { \'boolean\': true_counts_boolean.to_dict(), \'object\': true_counts_object.to_dict() } return df_filled, true_counts"},{"question":"Objective: Implement a function `py_long_operations` that demonstrates the creation and conversion of Python\'s `PyLongObject` from various C types, handling potential errors and overflows appropriately. Function Signature: ```python def py_long_operations(values: Dict[str, Union[int, float, str]]) -> Dict[str, Union[int, float, str, None]]: ``` Inputs: - `values`: A dictionary where the keys are strings representing the types (`\'long\'`, `\'unsigned long\'`, `\'long long\'`, `\'unsigned long long\'`, `\'ssize\'`, `\'size\'`, `\'double\'`, `\'str\'`, `\'unicode\'`), and the values are the corresponding C-type values. Outputs: - A dictionary where the keys are the same as the input dictionary. The corresponding values are the result of converting these C types to PyLongObject and back to the original types. - In case of any error or overflow, the result should be `None` for that specific entry. Example: ```python input_values = { \'long\': 2147483647, \'unsigned long\': 4294967295, \'long long\': 9223372036854775807, \'unsigned long long\': 18446744073709551615, \'ssize\': -2147483648, \'size\': 2147483647, \'double\': 3.14, \'str\': \'1234\', \'unicode\': \'5678\' } output_values = py_long_operations(input_values) # Expected output may vary. For correct implementation, it should match the outputs after conversion back to the original types. ``` Constraints: - You must use the `PyLongObject` functions as provided in the documentation. - Handle any potential conversion errors by setting the result for that key to `None`. - Ensure that your implementation adheres to the performance considerations for larger numbers and edge cases. Here is a template to get started: ```python from typing import Dict, Union def py_long_operations(values: Dict[str, Union[int, float, str]]) -> Dict[str, Union[int, float, str, None]]: # Initialize output dictionary output = {} # Iterate through the input values and process them accordingly for key, value in values.items(): try: if key == \'long\': # Convert from long and back to long py_long = PyLong_FromLong(value) result = PyLong_AsLong(py_long) elif key == \'unsigned long\': # Convert from unsigned long and back to unsigned long py_long = PyLong_FromUnsignedLong(value) result = PyLong_AsUnsignedLong(py_long) elif key == \'long long\': # Convert from long long and back to long long py_long = PyLong_FromLongLong(value) result = PyLong_AsLongLong(py_long) elif key == \'unsigned long long\': # Convert from unsigned long long and back to unsigned long long py_long = PyLong_FromUnsignedLongLong(value) result = PyLong_AsUnsignedLongLong(py_long) elif key == \'ssize\': # Convert from ssize_t and back to Py_ssize_t py_long = PyLong_FromSsize_t(value) result = PyLong_AsSsize_t(py_long) elif key == \'size\': # Convert from size_t and back to size_t py_long = PyLong_FromSize_t(value) result = PyLong_AsSize_t(py_long) elif key == \'double\': # Convert from double and back to double py_long = PyLong_FromDouble(value) result = PyLong_AsDouble(py_long) elif key == \'str\': # Convert from string and back to string py_long = PyLong_FromString(value.encode(), None, 10) result = PyLong_AsString(py_long) elif key == \'unicode\': # Convert from Unicode object and back to Unicode py_long = PyLong_FromUnicodeObject(value, 10) result = str(PyLong_AsLong(py_long)) else: result = None except: result = None # Store result in output dictionary output[key] = result return output ``` Note: The exact implementation will require the use of appropriate C API calls and error handling as specified in the PyLongObject documentation provided.","solution":"from typing import Dict, Union def py_long_operations(values: Dict[str, Union[int, float, str]]) -> Dict[str, Union[int, float, str, None]]: output = {} for key, value in values.items(): try: if key == \'long\': result = int(value) elif key == \'unsigned long\': if value < 0: raise ValueError(\\"Unsigned long cannot be negative\\") result = int(value) elif key == \'long long\': result = int(value) elif key == \'unsigned long long\': if value < 0: raise ValueError(\\"Unsigned long long cannot be negative\\") result = int(value) elif key == \'ssize\': result = int(value) elif key == \'size\': if value < 0: raise ValueError(\\"Size cannot be negative\\") result = int(value) elif key == \'double\': result = float(value) elif key == \'str\': result = int(value) elif key == \'unicode\': result = int(value) else: result = None except: result = None output[key] = result return output"},{"question":"# CSV File Manipulation **Objective:** Create a Python script that reads a CSV file, processes its contents, and writes the processed data to a new CSV file. **Task:** You are provided with a CSV file named `students.csv` that contains information about students and their scores in various subjects. The file has the following columns: `StudentID`, `Name`, `Math`, `Science`, `English`. You need to write a function `process_students(input_file: str, output_file: str) -> None` that performs the following operations: 1. Read the content of `students.csv`. 2. Calculate the average score for each student across the subjects. 3. Append a new column `Average` that contains the calculated average score. 4. Write the new content, including the `Average` column, to a new CSV file specified by `output_file`. **Input:** - `input_file`: Path to the input CSV file (e.g., \\"students.csv\\"). - `output_file`: Path where the output CSV file with the processed data should be saved. **Output:** - The function writes the processed data to the `output_file`. This CSV file should have a new column `Average` with each student\'s average score. **Constraints:** - The CSV files may contain a large number of rows. Your solution should handle large files efficiently. - Handle any missing or incorrect data gracefully. If a score is missing or invalid, assume it to be zero for the average calculation. **Example:** Given an input CSV file `students.csv` with the following contents: ``` StudentID,Name,Math,Science,English 1,John Doe,85,78,92 2,Jane Smith,90,88,84 3,Bob Johnson,95,80,83 4,Alice Brown,NaN,85,87 ``` Your function should write the following to the output file: ``` StudentID,Name,Math,Science,English,Average 1,John Doe,85,78,92,85.0 2,Jane Smith,90,88,84,87.33 3,Bob Johnson,95,80,83,86.0 4,Alice Brown,0,85,87,57.33 ``` **Note:** Ensure the average value is rounded to two decimal places. Implement the function as specified. Ensure to handle file operations and potential exceptions appropriately in your code.","solution":"import csv def process_students(input_file: str, output_file: str) -> None: Reads a CSV file containing student scores, calculates average scores, and writes the processed data with the average scores to a new CSV file. with open(input_file, mode=\'r\', newline=\'\') as infile: reader = csv.DictReader(infile) fieldnames = reader.fieldnames + [\\"Average\\"] students_data = [] for row in reader: scores = [float(row[subject]) if row[subject].replace(\'.\',\'\',1).isdigit() else 0 for subject in [\\"Math\\", \\"Science\\", \\"English\\"]] average_score = sum(scores) / len(scores) if len(scores) > 0 else 0 row[\\"Average\\"] = round(average_score, 2) students_data.append(row) with open(output_file, mode=\'w\', newline=\'\') as outfile: writer = csv.DictWriter(outfile, fieldnames=fieldnames) writer.writeheader() writer.writerows(students_data)"},{"question":"# Python Coding Assessment: Abstract Base Classes Objective: Implement abstract base classes using the `abc` module in Python. This question will test your understanding of abstract methods, virtual subclasses, and subclass relationships. Problem Statement: You are required to create an abstract base class for different types of shapes and implement concrete subclasses. The abstract base class should define the following abstract methods: 1. `area`: Should return the area of the shape. 2. `perimeter`: Should return the perimeter of the shape. You should also provide a concrete subclass for each of the following shapes: 1. `Rectangle`: Should have a constructor that takes `width` and `height` as parameters. 2. `Circle`: Should have a constructor that takes `radius` as a parameter. Additionally, create a class `Square` that inherits from `Rectangle`. Implement the necessary methods. Lastly, you need to register `Square` as a virtual subclass of a new abstract base class `Polygon` via the `register()` method. The `Polygon` class will not contain any abstract methods or properties. Requirements: 1. Define an abstract base class `Shape` with the required abstract methods. 2. Implement the `Rectangle` and `Circle` concrete subclasses. 3. Implement the `Square` class inheriting from `Rectangle`. 4. Define the `Polygon` class as an abstract base class and register `Square` as its virtual subclass. 5. Write a function `is_instance_of(cls, instance)` that takes a class `cls` and an instance `instance` and returns `True` if the instance is considered a subclass of the class (directly or virtually), otherwise `False`. Input: - There is no direct input from the user. Implement the classes and function as described. Output: - The output should be demonstrated via docstring examples or comments within the code. Constraints: - Use the `abc` module for defining ABCs. - Ensure methods are overridden in subclasses correctly. - Use the `register()` method to register the virtual subclass. Example: ```python from abc import ABC, abstractmethod # Define the Shape abstract base class class Shape(ABC): @abstractmethod def area(self): pass @abstractmethod def perimeter(self): pass # Implement Rectangle class class Rectangle(Shape): def __init__(self, width, height): self.width = width self.height = height def area(self): return self.width * self.height def perimeter(self): return 2 * (self.width + self.height) # Implement Circle class class Circle(Shape): def __init__(self, radius): self.radius = radius def area(self): import math return math.pi * self.radius ** 2 def perimeter(self): import math return 2 * math.pi * self.radius # Implement Square class class Square(Rectangle): def __init__(self, side): super().__init__(side, side) # Define Polygon abstract base class class Polygon(ABC): pass # Register Square as a virtual subclass of Polygon Polygon.register(Square) # Function to check if instance is a subclass of cls def is_instance_of(cls, instance): return isinstance(instance, cls) # Examples rectangle = Rectangle(3, 4) circle = Circle(5) square = Square(5) assert rectangle.area() == 12 assert circle.perimeter() == 31.41592653589793 assert is_instance_of(Polygon, square) == True ``` Explanation: 1. The abstract class `Shape` defines the abstract methods `area` and `perimeter`. 2. The `Rectangle` and `Circle` classes implement these methods. 3. The `Square` class inherits from `Rectangle` and correctly implements the methods. 4. The `Polygon` class is an abstract base class, and `Square` is registered as its virtual subclass. 5. The `is_instance_of` function checks if an instance is a subclass of a particular class. Implement the classes and function as described, run the example assertions to ensure correctness, and you\'re good to go!","solution":"from abc import ABC, abstractmethod class Shape(ABC): @abstractmethod def area(self): pass @abstractmethod def perimeter(self): pass class Rectangle(Shape): def __init__(self, width, height): self.width = width self.height = height def area(self): return self.width * self.height def perimeter(self): return 2 * (self.width + self.height) class Circle(Shape): def __init__(self, radius): self.radius = radius def area(self): import math return math.pi * self.radius ** 2 def perimeter(self): import math return 2 * math.pi * self.radius class Square(Rectangle): def __init__(self, side): super().__init__(side, side) class Polygon(ABC): pass Polygon.register(Square) def is_instance_of(cls, instance): return isinstance(instance, cls) # Examples for docstring rectangle = Rectangle(3, 4) circle = Circle(5) square = Square(5) assert rectangle.area() == 12 assert circle.perimeter() == 2 * 3.141592653589793 * 5 assert is_instance_of(Polygon, square) == True"},{"question":"Understanding Floating Point Arithmetic in Python **Objective:** You need to demonstrate a thorough understanding of floating-point arithmetic in Python. This includes handling common issues and using appropriate packages to achieve exact representations when required. **Task:** Write a Python function `compare_float_sums(sum1, sum2)` that: 1. Takes two lists of floating-point numbers `sum1` and `sum2`. 2. Returns whether the sum of numbers in `sum1` is equal to the sum of numbers in `sum2` with a precision of up to 10 decimal places. You must consider the nuances of floating-point arithmetic and use appropriate Python utilities to ensure accurate comparison. **Input:** - `sum1`: List of floating-point numbers. - `sum2`: List of floating-point numbers. **Output:** - `True` if the sums of `sum1` and `sum2` are equal up to 10 decimal places, otherwise `False`. **Example:** ```python # Example 1: sum1 = [0.1, 0.1, 0.1] sum2 = [0.3] print(compare_float_sums(sum1, sum2)) # Expected output: True # Example 2: sum1 = [0.1, 0.2, 0.3] sum2 = [0.6] print(compare_float_sums(sum1, sum2)) # Expected output: True # Example 3: sum1 = [0.3, 0.3, 0.3] sum2 = [0.9] print(compare_float_sums(sum1, sum2)) # Expected output: False ``` **Constraints and Limitations:** 1. Floating-point numbers in the lists can range from `-10^9` to `10^9`. 2. The length of each list `sum1` and `sum2` can be up to `10^6`. **Performance Requirements:** - Your function should be optimized to handle large lists efficiently. - Ensure that the comparison is precise and free from common floating-point errors. **Guidelines:** - Make use of `math.fsum()` to ensure accurate summation. - Use rounding to achieve the desired precision. **Hints:** - Consider the use of `math.isclose()` for reliable floating-point comparison. - Investigate the `decimal` module for precision handling. ```python import math def compare_float_sums(sum1, sum2): # Your implementation here. pass ``` Please write the function implementation along with test cases to verify your solution.","solution":"import math def compare_float_sums(sum1, sum2): Compares the sum of two lists of floating-point numbers with a precision of up to 10 decimal places. Args: sum1 (list of float): The first list of floating-point numbers. sum2 (list of float): The second list of floating-point numbers. Returns: bool: True if the sums are equal up to 10 decimal places, otherwise False. sum1_total = math.fsum(sum1) sum2_total = math.fsum(sum2) return round(sum1_total, 10) == round(sum2_total, 10)"},{"question":"# Question: Implement a Data Loading and Preprocessing Pipeline You are required to write a Python function `load_and_prepare_data` that: 1. Downloads a dataset from OpenML using the `fetch_openml` function. 2. Converts any categorical features in the dataset into numerical features using one-hot encoding. 3. Prepares the data for training by splitting it into training and testing sets. 4. Trains a simple classifier on the training data and evaluates it on the testing data. You should use the `miceprotein` dataset from OpenML, which contains gene expression data from mice. Input: - None Output: - A float representing the classification accuracy of the trained model on the testing data. Requirements: 1. **Data Fetching**: Use `fetch_openml` to download the **miceprotein** dataset using its name. 2. **Data Preprocessing**: Use `OneHotEncoder` to transform categorical features to numerical. 3. **Splitting the Data**: Use `train_test_split` to split the data into training (80%) and testing (20%) sets. 4. **Training and Evaluation**: Train a `RandomForestClassifier` on the training set and evaluate it on the testing set. Example Function Signature: ```python def load_and_prepare_data() -> float: # Step 1: Download the dataset # Step 2: Preprocess the dataset # Step 3: Split the data # Step 4: Train and evaluate a classifier pass ``` Constraints: - You must use `sklearn` for all the operations. - The data split should be reproducible; ensure the split is done with a fixed random state. Performance: - Aim for a function that runs efficiently within reasonable time limits for such dataset sizes. Optimizations such as using sparse matrices are encouraged but not mandatory.","solution":"from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score import pandas as pd import numpy as np def load_and_prepare_data() -> float: # Step 1: Download the dataset dataset = fetch_openml(name=\'miceprotein\', version=4, as_frame=True) df = dataset.frame # Step 2: Preprocess the dataset # Drop rows with missing values df.dropna(inplace=True) # Separate features and target X = df.drop(columns=\'class\') y = df[\'class\'] # OneHotEncode the categorical features X = pd.get_dummies(X, drop_first=True) # Step 3: Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 4: Train a RandomForestClassifier and evaluate it clf = RandomForestClassifier(random_state=42) clf.fit(X_train, y_train) # Predict on the test set and calculate accuracy y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"# Python Coding Assessment: `unittest` Framework Problem Statement You are tasked with creating and organizing tests for a basic `ShoppingCart` class using Pythons `unittest` module. Your tests should cover various functionalities and edge cases. The `ShoppingCart` class allows for adding, removing, and listing items, as well as calculating the total price. Here is a simple implementation of the `ShoppingCart` class: ```python class Item: def __init__(self, name, price): self.name = name self.price = price class ShoppingCart: def __init__(self): self.items = [] def add_item(self, item): self.items.append(item) def remove_item(self, item_name): self.items = [item for item in self.items if item.name != item_name] def total_price(self): return sum(item.price for item in self.items) def item_list(self): return [item.name for item in self.items] ``` Tasks 1. **Test Case Implementation**: - Create a `TestShoppingCart` class derived from `unittest.TestCase`. - Implement test methods to cover the following scenarios: - Adding an item to the cart. - Removing an item from the cart. - Calculating the total price with and without discounts (you will add a discount feature). - Listing items in the cart. 2. **Setup and Teardown**: - Use `setUp` and `tearDown` methods to prepare any reusable setup/cleanup code for your tests if necessary. 3. **Skipping and Expected Failures**: - Incorporate at least one test that demonstrates the use of a skipping decorator. - Incorporate at least one test that demonstrates the use of the `expectedFailure` decorator. 4. **Command Line Test Discovery**: - Provide example command lines to run your tests with the `unittest` command-line interface. Include an example to run all tests in the file with detailed output. Example Test To assist you, here is an example test method to get you started: ```python import unittest class TestShoppingCart(unittest.TestCase): def setUp(self): self.cart = ShoppingCart() def test_add_item(self): item = Item(\\"Apple\\", 0.5) self.cart.add_item(item) self.assertIn(\\"Apple\\", self.cart.item_list()) self.assertEqual(self.cart.total_price(), 0.5) def tearDown(self): pass if __name__ == \'__main__\': unittest.main() ``` Command Line Examples Provide command lines like the following to run your tests: - To run all tests: ```sh python -m unittest test_shopping_cart.py ``` - To run with detailed verbosity: ```sh python -m unittest -v test_shopping_cart.py ``` Your solution should be a fully functional Python file with the above assertions and features well-demonstrated. **Note**: Ensure you include necessary imports and complete your tests thoroughly. Each test method should validate its scenarios effectively using appropriate assert methods provided by `unittest`.","solution":"import unittest class Item: def __init__(self, name, price): self.name = name self.price = price class ShoppingCart: def __init__(self): self.items = [] def add_item(self, item): self.items.append(item) def remove_item(self, item_name): self.items = [item for item in self.items if item.name != item_name] def total_price(self, discount=0): total = sum(item.price for item in self.items) return total * ((100 - discount) / 100) def item_list(self): return [item.name for item in self.items]"},{"question":"Objective Write an asynchronous Python program using the `asyncio` library. The program should manage multiple coroutines that perform IO-bound tasks concurrently and synchronize their execution using `asyncio` synchronization primitives. Problem Statement Design an asynchronous program that simulates a scenario where multiple workers (coroutines) are fetching data from a server, processing it, and storing the results. Use the following specifications: 1. **Workers**: - There are `N` workers (`N` is an integer input to the program). - Each worker fetches data every second (simulated using `await asyncio.sleep(1)`). - After fetching data, each worker processes it (simulated as a simple transformation like converting a string to uppercase). 2. **Data Fetching**: - Data fetching should be synchronized such that only one worker fetches data at a time. - Use `asyncio.Lock` to enforce synchronization. 3. **Result Storage**: - Store the processed results in a shared list. Ensure that adding results to this list is thread-safe. 4. **Execution Control**: - Run the workers concurrently for a specified duration `T` seconds (`T` is an integer input to the program). - After `T` seconds, all workers should stop. 5. **Output**: - Print the contents of the shared list after all workers have completed their execution. Input Format - An integer `N` representing the number of workers. - An integer `T` representing the duration for which the workers should run. Output Format - A list of processed results obtained by all workers. Constraints - `1 <= N <= 10` - `1 <= T <= 60` Example Input: ``` 3 5 ``` Output (example, actual output may vary): ``` [\'DATA1\', \'DATA2\', \'DATA3\', \'DATA4\', \'DATA5\', \'DATA6\', \'DATA7\', \'DATA8\', \'DATA9\', \'DATA10\'] ``` Function Signature Implement your solution in a function called `run_workers`. ```python import asyncio async def run_workers(N: int, T: int) -> None: # Your implementation here pass ``` Requirements - Use `asyncio` to manage concurrent execution. - Use `asyncio.Lock` to synchronize data fetching. - Ensure thread-safe modifications to the shared list. Good luck!","solution":"import asyncio async def fetch_data(lock: asyncio.Lock, worker_id: int): Simulates fetching data from a server and processing it. The lock ensures that only one worker fetches data at a time. async with lock: await asyncio.sleep(1) # Simulates the fetch delay data = f\\"data_from_worker_{worker_id}\\" # Process the fetched data processed_data = data.upper() return processed_data async def worker(worker_id: int, lock: asyncio.Lock, shared_list: list, duration: int): Worker coroutine that fetches, processes data, and stores the result. start_time = asyncio.get_event_loop().time() while (asyncio.get_event_loop().time() - start_time) < duration: processed_data = await fetch_data(lock, worker_id) shared_list.append(processed_data) async def run_workers(N: int, T: int): Runs N workers concurrently for T seconds and prints the results stored in a shared list. lock = asyncio.Lock() shared_list = [] # Create worker tasks tasks = [worker(worker_id, lock, shared_list, T) for worker_id in range(N)] # Run worker tasks concurrently await asyncio.gather(*tasks) # Output the shared list print(shared_list)"},{"question":"# **Pattern Matching and Data Extraction using Regular Expressions** You are given a series of text entries from a blog comment section. Each entry contains the username of the commenter, the date of the comment, and the content of the comment itself. For example: ``` username: johndoe01, date: 2023-10-05, comment: \\"Great article! Learned a lot.\\" username: janedoe_92, date: 2023-10-06, comment: \\"I found a typo in the second paragraph.\\" username: tech_guru_007, date: 2023-10-07, comment: \\"Can you write more about deep learning?\\" ... ``` Your task is to implement a Python function using the `re` module that: 1. Extracts and returns a list of tuples where each tuple contains the username, date, and comment. 2. Filters out comments that contain specific keywords (e.g., \\"learned\\", \\"typo\\", \\"deep learning\\"). # **Function Signature** ```python def extract_and_filter_comments(text: str, keywords: List[str]) -> List[Tuple[str, str, str]]: pass ``` # **Input** - `text` (str): A single multi-line string containing all the comment entries. - `keywords` (List[str]): A list of keywords to filter out from the comments. # **Output** - Returns a list of tuples. Each tuple contains three elements: `username` (str), `date` (str), and `comment` (str) where the `comment` does not include any of the keywords specified. # **Example** ```python text = username: johndoe01, date: 2023-10-05, comment: \\"Great article! Learned a lot.\\" username: janedoe_92, date: 2023-10-06, comment: \\"I found a typo in the second paragraph.\\" username: tech_guru_007, date: 2023-10-07, comment: \\"Can you write more about deep learning?\\" username: alicewonder, date: 2023-10-08, comment: \\"Interesting perspective on the topic.\\" keywords = [\\"learned\\", \\"typo\\", \\"deep learning\\"] print(extract_and_filter_comments(text, keywords)) ``` # **Expected Output** ```python [ (\'alicewonder\', \'2023-10-08\', \'Interesting perspective on the topic.\'), ] ``` # **Constraints** 1. The input text can have an arbitrary number of comment entries. 2. Make sure your regex pattern handles all the provided formats correctly. 3. Optimize for readability and efficiency. # **Notes** - Use regex groups to capture the `username`, `date`, and `comment` components. - Use the `re` module\'s `search` and `sub` methods where appropriate. - Ensure to handle cases where the comment may contain special characters.","solution":"import re from typing import List, Tuple def extract_and_filter_comments(text: str, keywords: List[str]) -> List[Tuple[str, str, str]]: Extracts and returns a list of tuples (username, date, comment) from the text, filtering out comments that contain any of the specified keywords. pattern = re.compile(r\'username: (.*?), date: (.*?), comment: \\"(.*?)\\"\') matches = pattern.findall(text) filtered_comments = [] for match in matches: username, date, comment = match if not any(keyword.lower() in comment.lower() for keyword in keywords): filtered_comments.append((username, date, comment)) return filtered_comments"},{"question":"**Question: Implement a Custom Exception Handler Using `cgitb`** You are required to implement a custom exception handler for a CGI script using the `cgitb` module. Your handler should intercept uncaught exceptions, format the traceback in HTML, and optionally log it to a file. The functions you need to implement are as follows: # Function 1: `setup_custom_cgitb_handler(display: bool, logdir: str, context: int) -> None` This function sets up the `cgitb` module to handle uncaught exceptions using the specified parameters. Input: - `display` (bool): - If `True`, format and display the traceback in the browser. - If `False`, do not display the traceback in the browser. - `logdir` (str): The directory path where the traceback logs should be saved. If empty, do not log to a file. - `context` (int): The number of lines of context to display around the current line of source code in the traceback. Default is 5. Output: None # Function 2: `custom_exception_handler(exc_info: tuple) -> str` This function handles an exception using `cgitb.html()` and returns the formatted traceback string. Input: - `exc_info` (tuple): A 3-tuple containing the result of `sys.exc_info()`. Output: - `traceback_str` (str): The formatted HTML string of the traceback. # Requirements: 1. The `setup_custom_cgitb_handler()` function should correctly configure `cgitb` using the `enable` method, and handle the `display`, `logdir`, and `context` parameters appropriately. 2. The `custom_exception_handler()` function should format the exception traceback using `cgitb.html()` with the given `exc_info` and return it as a string. # Example Usage: ```python import cgitb import sys def setup_custom_cgitb_handler(display: bool, logdir: str, context: int) -> None: display_int = 1 if display else 0 cgitb.enable(display=display_int, logdir=logdir if logdir else None, context=context) def custom_exception_handler(exc_info: tuple) -> str: return cgitb.html(exc_info, context=5) # Example of usage if __name__ == \\"__main__\\": try: setup_custom_cgitb_handler(display=True, logdir=\\"/path/to/logdir\\", context=5) # Simulate a division by zero exception 1 / 0 except: exc_info = sys.exc_info() traceback_str = custom_exception_handler(exc_info) # For illustration, print the formatted traceback print(traceback_str) ``` The above example demonstrates how you can set up the custom handler and simulate an exception to validate its functionality.","solution":"import cgitb import sys def setup_custom_cgitb_handler(display: bool, logdir: str, context: int) -> None: Set up the cgitb module to handle uncaught exceptions using the specified parameters. :param display: If True, format and display the traceback in the browser. If False, do not display the traceback in the browser. :param logdir: The directory path where the traceback logs should be saved. If empty, do not log to a file. :param context: The number of lines of context to display around the current line of source code in the traceback. display_int = 1 if display else 0 cgitb.enable(display=display_int, logdir=logdir if logdir else None, context=context) def custom_exception_handler(exc_info: tuple) -> str: Handle an exception using cgitb.html() and return the formatted traceback string. :param exc_info: A 3-tuple containing the result of sys.exc_info(). :return: The formatted HTML string of the traceback. return cgitb.html(exc_info)"},{"question":"# Custom JSON Encoder and Decoder **Overview:** You are given a task to extend the JSON serialization and deserialization process to handle instances of a custom class `Employee`. Specifically, you need to write custom encoder and decoder classes that can convert `Employee` objects to and from JSON strings. **Specifications:** 1. **Employee Class:** - Attributes: - `name` (str): The name of the employee. - `age` (int): The age of the employee. - `position` (str): The position of the employee. 2. **Custom Encoder:** - Subclass `json.JSONEncoder` to create a custom encoder `EmployeeEncoder`. - Implement the `default` method to handle `Employee` instances by serializing them into a JSON-compatible dictionary format. 3. **Custom Decoder:** - Subclass `json.JSONDecoder` to create a custom decoder `EmployeeDecoder`. - You need to implement a custom decoding method that can convert the JSON dictionary back into an `Employee` instance. **Input and Output Formats:** - **Input:** - A JSON string representing an `Employee` object. - **Output:** - An `Employee` instance. **Function Signature:** ```python class Employee: def __init__(self, name: str, age: int, position: str): self.name = name self.age = age self.position = position class EmployeeEncoder(json.JSONEncoder): def default(self, obj): # Implement this method to handle Employee instances class EmployeeDecoder(json.JSONDecoder): def __init__(self, *args, **kwargs): super().__init__(object_hook=self.object_hook, *args, **kwargs) def object_hook(self, obj): # Implement this method to handle JSON dictionaries and convert them into Employee instances # Example usage: employee = Employee(\'John Doe\', 30, \'Software Engineer\') json_str = json.dumps(employee, cls=EmployeeEncoder) print(json_str) loaded_employee = json.loads(json_str, cls=EmployeeDecoder) print(loaded_employee.name, loaded_employee.age, loaded_employee.position) ``` **Constraints:** 1. The JSON encoding must correctly represent all attributes of the `Employee` class. 2. The custom decoding should handle cases where the JSON does not represent an `Employee` correctly by raising a `ValueError`. **Performance Requirements:** - Ensure that the custom encoding and decoding processes efficiently handle the serialization and deserialization without significant performance overhead. **Example:** Given the `Employee` class: ```python employee = Employee(\'Alice Smith\', 28, \'Data Scientist\') json_str = json.dumps(employee, cls=EmployeeEncoder) print(json_str) # Should output: \'{\\"name\\": \\"Alice Smith\\", \\"age\\": 28, \\"position\\": \\"Data Scientist\\"}\' ``` Given the JSON string: ```python json_str = \'{\\"name\\": \\"Alice Smith\\", \\"age\\": 28, \\"position\\": \\"Data Scientist\\"}\' employee = json.loads(json_str, cls=EmployeeDecoder) print(employee.name) # Should output: \'Alice Smith\' print(employee.age) # Should output: 28 print(employee.position) # Should output: \'Data Scientist\' ``` **Notes:** - Make sure to validate the input JSON in the decoder to ensure it correctly represents an `Employee`.","solution":"import json class Employee: def __init__(self, name: str, age: int, position: str): self.name = name self.age = age self.position = position class EmployeeEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Employee): return {\'name\': obj.name, \'age\': obj.age, \'position\': obj.position} return super().default(obj) class EmployeeDecoder(json.JSONDecoder): def __init__(self, *args, **kwargs): super().__init__(object_hook=self.object_hook, *args, **kwargs) def object_hook(self, obj): if \'name\' in obj and \'age\' in obj and \'position\' in obj: return Employee(obj[\'name\'], obj[\'age\'], obj[\'position\']) return obj"},{"question":"**Objective:** Write a Python program utilizing the `ftplib` module that connects to a given FTP server, uploads a file, verifies the upload, downloads the same file, and then deletes it from the server. All operations must handle appropriate exceptions gracefully and log each step\'s output. **Requirements:** 1. Define a function `ftp_operations(server: str, username: str, password: str, local_filepath: str, remote_filename: str) -> None` that performs the following steps: * Connect to the FTP server using the given `server`, `username`, and `password`. * Change the working directory to `/upload`. * Upload the file specified by `local_filepath` to the server with the name `remote_filename`. * Verify that the file exists on the server by listing directory contents. * Download the file back to a local temporary file and verify its integrity. * Delete the file from the server. * Close the FTP connection cleanly. 2. Implement exception handling for connection errors, login errors, file transfer errors, and any other potential issues. 3. Log detailed messages for each step including successful connections, file operations, and error messages if encountered. 4. Use the secure connections via the `FTP_TLS` class if the `username` and `password` are provided, otherwise, proceed with anonymous login via `FTP`. **Constraints:** - Assume the server is always reachable, but login credentials may be incorrect. - The remote directory `/upload` exists and is writable. - The local file specified will always be present before running the script. **Expected Input:** - `server` (str) - The FTP server address. - `username` (str) - The user\'s username for the FTP server. - `password` (str) - The user\'s password for the FTP server. - `local_filepath` (str) - The path to the local file to upload. - `remote_filename` (str) - The name to assign to the file on the server. **Example Usage:** ```python ftp_operations( server=\'ftp.example.com\', username=\'user\', password=\'password\', local_filepath=\'/path/to/local/file.txt\', remote_filename=\'uploaded_file.txt\' ) ``` **Note:** Ensure the integrity of the downloaded file by comparing it with the uploaded file. You may use file size or checksum verification.","solution":"import ftplib import os import logging def ftp_operations(server: str, username: str, password: str, local_filepath: str, remote_filename: str) -> None: logging.basicConfig(level=logging.INFO) # Determine the FTP class to use (secure or regular) ftp_cls = ftplib.FTP_TLS if username and password else ftplib.FTP try: # Connect to FTP server ftp = ftp_cls(server) ftp.login(user=username, passwd=password) logging.info(\\"Connected and logged into the server\\") # Change directory to /upload ftp.cwd(\'/upload\') logging.info(\\"Changed directory to /upload\\") # Upload the file with open(local_filepath, \'rb\') as file: ftp.storbinary(f\'STOR {remote_filename}\', file) logging.info(f\\"Uploaded the file {local_filepath} as {remote_filename}\\") # Verify upload by listing directory contents files = ftp.nlst() if remote_filename in files: logging.info(f\\"Verified that {remote_filename} exists on the server\\") else: raise Exception(f\\"Upload verification failed: {remote_filename} not found on the server\\") # Download the file to a local temporary file local_temp_filepath = f\'/tmp/{remote_filename}\' with open(local_temp_filepath, \'wb\') as file: ftp.retrbinary(f\'RETR {remote_filename}\', file.write) logging.info(f\\"Downloaded {remote_filename} to {local_temp_filepath}\\") # Check file integrity by comparing file sizes original_size = os.path.getsize(local_filepath) downloaded_size = os.path.getsize(local_temp_filepath) if original_size == downloaded_size: logging.info(\\"File integrity verified: sizes match\\") else: raise Exception(\\"File integrity verification failed: sizes do not match\\") # Delete the file from the server ftp.delete(remote_filename) logging.info(f\\"Deleted {remote_filename} from the server\\") except ftplib.all_errors as e: logging.error(f\\"FTP error occurred: {e}\\") except Exception as e: logging.error(f\\"An error occurred: {e}\\") finally: # Close the FTP connection if \'ftp\' in locals(): ftp.quit() logging.info(\\"FTP connection closed\\")"},{"question":"# Python Coding Assessment: DateTime Objects **Objective:** Implement a series of functions that utilize the `datetime` module to demonstrate your understanding of datetime creation and manipulation. **Question:** You need to create the following functions: 1. **create_date_object(year: int, month: int, day: int) -> datetime.date**: - This function should take year, month, and day as inputs and return a `datetime.date` object representing the given date. 2. **create_datetime_object(year: int, month: int, day: int, hour: int, minute: int, second: int, microsecond: int) -> datetime.datetime**: - This function should take year, month, day, hour, minute, second, and microsecond as inputs and return a `datetime.datetime` object representing the given datetime. 3. **create_time_object(hour: int, minute: int, second: int, microsecond: int) -> datetime.time**: - This function should take hour, minute, second, and microsecond as inputs and return a `datetime.time` object representing the given time. 4. **create_timedelta_object(days: int, seconds: int, microseconds: int) -> datetime.timedelta**: - This function should take days, seconds, and microseconds as inputs and return a `datetime.timedelta` object representing the given time delta. 5. **create_timezone_object(offset_days: int, offset_seconds: int, offset_microseconds: int, name: str) -> datetime.timezone**: - This function should take offset in days, seconds, and microseconds along with a name, and return a `datetime.timezone` object with the specified offset and name. 6. **extract_date_fields(date_obj: datetime.date) -> tuple**: - This function should take a `datetime.date` object and return a tuple containing the year, month, and day extracted from the date object. 7. **extract_datetime_fields(datetime_obj: datetime.datetime) -> tuple**: - This function should take a `datetime.datetime` object and return a tuple containing the year, month, day, hour, minute, second, and microsecond extracted from the datetime object. 8. **extract_time_fields(time_obj: datetime.time) -> tuple**: - This function should take a `datetime.time` object and return a tuple containing the hour, minute, second, and microsecond extracted from the time object. 9. **extract_timedelta_fields(timedelta_obj: datetime.timedelta) -> tuple**: - This function should take a `datetime.timedelta` object and return a tuple containing the days, seconds, and microseconds extracted from the timedelta object. # Input Format: - For all create functions, the input parameters are integers representing the respective date, time, timedelta, and timezone fields. - For all extract functions, the input is a respective datetime object. # Output Format: - The create functions return the respective datetime objects. - The extract functions return tuples containing the extracted fields. # Constraints: - Year, month, and day must form a valid date. - Hour, minute, second, and microsecond must form a valid time. - Offset for timezone must be a valid timedelta. # Example: ```python # Example for create_date_object print(create_date_object(2023, 10, 15)) # Output: 2023-10-15 # Example for extract_date_fields date_obj = create_date_object(2023, 10, 15) print(extract_date_fields(date_obj)) # Output: (2023, 10, 15) ``` Implement these functions demonstrating your understanding of the `datetime` module and its capabilities.","solution":"import datetime def create_date_object(year: int, month: int, day: int) -> datetime.date: return datetime.date(year, month, day) def create_datetime_object(year: int, month: int, day: int, hour: int, minute: int, second: int, microsecond: int) -> datetime.datetime: return datetime.datetime(year, month, day, hour, minute, second, microsecond) def create_time_object(hour: int, minute: int, second: int, microsecond: int) -> datetime.time: return datetime.time(hour, minute, second, microsecond) def create_timedelta_object(days: int, seconds: int, microseconds: int) -> datetime.timedelta: return datetime.timedelta(days=days, seconds=seconds, microseconds=microseconds) def create_timezone_object(offset_days: int, offset_seconds: int, offset_microseconds: int, name: str) -> datetime.timezone: offset = datetime.timedelta(days=offset_days, seconds=offset_seconds, microseconds=offset_microseconds) return datetime.timezone(offset, name) def extract_date_fields(date_obj: datetime.date) -> tuple: return (date_obj.year, date_obj.month, date_obj.day) def extract_datetime_fields(datetime_obj: datetime.datetime) -> tuple: return (datetime_obj.year, datetime_obj.month, datetime_obj.day, datetime_obj.hour, datetime_obj.minute, datetime_obj.second, datetime_obj.microsecond) def extract_time_fields(time_obj: datetime.time) -> tuple: return (time_obj.hour, time_obj.minute, time_obj.second, time_obj.microsecond) def extract_timedelta_fields(timedelta_obj: datetime.timedelta) -> tuple: return (timedelta_obj.days, timedelta_obj.seconds, timedelta_obj.microseconds)"},{"question":"You are tasked with implementing a small Python program that performs data transformation and extraction using some of Python 3.10\'s new features. Specifically, you need to demonstrate an understanding of pattern matching, list comprehensions, and error handling. The following functionalities must be implemented: 1. **Dictionary Filter and Transformation**: - Implement a function `filter_and_transform(data: dict) -> dict` that takes a dictionary where: - Keys are strings. - Values can be either integers or another dictionary with similar structure. - This function should: - Filter out dictionary entries where the key starts with a lowercase letter. - If the value is an integer, add 10 to it. - If the value is a dictionary, recursively apply the same transformation rules. 2. **Pattern Matching Transformation**: - Implement a function `pattern_match(transformation: str, values: list) -> list` that takes a string specifying a transformation type and a list of tuples. - The tuples can have the structure either `(int, int)` representing two integers or `(str, bool)` representing a string and a boolean. - Depending on the transformation type, which can be \\"swap\\" or \\"double\\": - For \\"swap\\", swap the elements of the tuples. - For \\"double\\", double the integer values in the tuples where applicable. - Return the transformed list of tuples. 3. **Comprehension and Error Handling**: - Implement a function `even_square_numbers(nums: list) -> list` that takes a list of integers. - Use list comprehension to: - Square the number if it is even. - Raise a custom exception `OddNumberException` for any odd number encountered, stopping the transformation. - Catch the `OddNumberException` and return the message \\"Odd number encountered\\". # Requirements - Use Python 3.10. - Utilize the pattern matching feature introduced in Python 3.10. - Apply list comprehensions as needed. - Implement custom exception handling. - Ensure the code is efficient and handles edge cases. # Constraints - Input to the functions will always follow the specified formats. - Dictionary depth will not exceed 3 levels. - The transformation string will always be either \\"swap\\" or \\"double\\". - The list of numbers will contain at least one integer. # Example ```python def example_usage(): data = { \'A1\': 5, \'b2\': { \'C3\': 8, \'d4\': 2 }, \'E5\': 3 } filter_and_transform(data) # Expected Output: {\'A1\': 15, \'E5\': 13} values = [(1, 2), (3, True), (4, \'five\', False)] pattern_match(\'swap\', values) # Expected Output: [(2, 1), (True, 3)] numbers = [2, 4, 6, 7] even_square_numbers(numbers) # Expected Output: \\"Odd number encountered\\" ``` # Functions to Implement ```python def filter_and_transform(data: dict) -> dict: # Implement the function here pass def pattern_match(transformation: str, values: list) -> list: # Implement the function here pass def even_square_numbers(nums: list) -> list: # Implement the function here pass class OddNumberException(Exception): pass ```","solution":"def filter_and_transform(data: dict) -> dict: if not isinstance(data, dict): raise ValueError(\\"Input must be a dictionary\\") def transform(value): if isinstance(value, int): return value + 10 elif isinstance(value, dict): return filter_and_transform(value) raise ValueError(\\"Unsupported value type\\") return { key: transform(value) for key, value in data.items() if key[0].isupper() } def pattern_match(transformation: str, values: list) -> list: if transformation not in [\'swap\', \'double\']: raise ValueError(\\"Transformation must be either \'swap\' or \'double\'\\") def swap(t): return t[::-1] def double(t): if isinstance(t[0], int) and isinstance(t[1], int): return (t[0] * 2, t[1] * 2) return t match transformation: case \'swap\': return [swap(t) for t in values] case \'double\': return [double(t) for t in values] class OddNumberException(Exception): pass def even_square_numbers(nums: list) -> list: result = [] try: for num in nums: if num % 2 != 0: raise OddNumberException(\\"Odd number encountered\\") result.append(num ** 2) except OddNumberException as e: return str(e) return result"},{"question":"# **Coding Assessment Question** **Objective:** Demonstrate your understanding of using PyArrow with pandas to create and manipulate data structures. You are expected to leverage PyArrow-enhanced functionality to perform various operations on data. Instructions: 1. **Data Creation:** Create a pandas DataFrame named `df` with the following data: ``` A = [-1.5, 0.2, NaN, 3.7] B = [1, 0, 10, NaN] C = [\'a\', \'b\', None, \'c\'] ``` - Column `A` should be of type `float32[pyarrow]`. - Column `B` should be of type `int64[pyarrow]`. - Column `C` should be of type `string[pyarrow]`. 2. **DataFrame Manipulation:** Perform the following operations on `df`: - Calculate the mean of column `A`. - Fill missing values in column `B` with the mean value of column `B`. - Replace any `None` or missing values in column `C` with the string \\"unknown\\". - Convert the PyArrow-backed DataFrame `df` to a PyArrow Table object named `table`. 3. **Output:** Your function should return a tuple of: - The mean of column `A`. - The modified DataFrame `df`. - The PyArrow Table object `table`. **Function Signature:** ```python import pandas as pd import pyarrow as pa def manipulate_data(): # Step 1: Create DataFrame with PyArrow-backed columns # Step 2: Calculate mean of column \'A\' # Step 3: Fill missing values in column \'B\' # Step 4: Replace missing values in column \'C\' # Step 5: Convert DataFrame to PyArrow Table return (mean_a, df, table) ``` **Constraints:** - You should use the information from the provided documentation to ensure the correct usage of PyArrow-backed data structures. **Performance Requirements:** - Ensure that all operations are efficient and leverage PyArrow\'s performance optimizations where applicable. Example Output: ```python mean_a, modified_df, pa_table = manipulate_data() print(mean_a) # Output: 0.8 print(modified_df) # Expected DataFrame: # A B C # 0 -1.5 1.0 a # 1 0.2 0.0 b # 2 NaN 10.0 unknown # 3 3.7 3.6667 c print(pa_table) # Expected PyArrow Table: # pyarrow.Table # A: float # B: int64 # C: string ```","solution":"import pandas as pd import pyarrow as pa import numpy as np def manipulate_data(): # Step 1: Create DataFrame with PyArrow-backed columns df = pd.DataFrame({ \'A\': pd.array([-1.5, 0.2, np.nan, 3.7], dtype=\'float32[pyarrow]\'), \'B\': pd.array([1, 0, 10, np.nan], dtype=\'int64[pyarrow]\'), \'C\': pd.array([\'a\', \'b\', None, \'c\'], dtype=\'string[pyarrow]\') }) # Step 2: Calculate mean of column \'A\' mean_a = df[\'A\'].mean() # Step 3: Fill missing values in column \'B\' mean_b = df[\'B\'].mean() df[\'B\'] = df[\'B\'].fillna(mean_b) # Step 4: Replace missing values in column \'C\' df[\'C\'] = df[\'C\'].fillna(\'unknown\') # Step 5: Convert DataFrame to PyArrow Table table = pa.Table.from_pandas(df, preserve_index=False) return mean_a, df, table"},{"question":"**Objective:** To assess your understanding of asynchronous execution using the `concurrent.futures` module in Python. You are required to implement a multi-threaded solution to a problem involving data fetching and processing. **Problem Statement:** You are tasked with developing a program that retrieves information from a list of URLs and processes the data concurrently. The URL fetching and data processing should be done using the `concurrent.futures.ThreadPoolExecutor`. **Requirements:** 1. **Function Definition:** Implement a function `fetch_and_process_urls(urls: List[str], timeout: float) -> Dict[str, int]`. This function should: - Accept a list of URLs (strings) and a timeout value (in seconds). - Retrieve the content of each URL and count the number of words in it concurrently. - Return a dictionary mapping each URL to its word count. 2. **Concurrent Execution:** - Use `ThreadPoolExecutor` to fetch the contents of the URLs concurrently. - Ensure that each fetch operation has a timeout, raising an appropriate exception if the operation times out. 3. **Error Handling:** - If fetching a URL fails (due to a timeout or any other exception), log the error and continue with the remaining URLs. - Use Python\'s built-in `logging` module to log errors. 4. **Performance:** - Use a maximum of 5 threads for concurrent fetching and processing. - Ensure the program handles a large number of URLs efficiently. **Constraints:** - URLs provided will be valid and reachable in normal conditions. - The content of the URLs will be plain text. **Example Usage:** ```python import logging from typing import List, Dict from concurrent.futures import ThreadPoolExecutor, as_completed import urllib.request # Implement the function here def fetch_and_process_urls(urls: List[str], timeout: float) -> Dict[str, int]: logging.basicConfig(level=logging.ERROR) def load_url(url, timeout): try: with urllib.request.urlopen(url, timeout=timeout) as conn: return conn.read().decode(\'utf-8\') except Exception as e: logging.error(f\\"Error fetching {url}: {e}\\") return None def count_words(text): return len(text.split()) results = {} with ThreadPoolExecutor(max_workers=5) as executor: future_to_url = {executor.submit(load_url, url, timeout): url for url in urls} for future in as_completed(future_to_url): url = future_to_url[future] try: data = future.result() if data is not None: results[url] = count_words(data) except Exception as exc: logging.error(f\\"{url} generated an exception: {exc}\\") return results # Example URLs urls = [ \'http://www.foxnews.com/\', \'http://www.cnn.com/\', \'http://europe.wsj.com/\', \'http://www.bbc.co.uk/\', \'http://nonexistant-subdomain.python.org/\' # This will cause an exception ] if __name__ == \\"__main__\\": word_counts = fetch_and_process_urls(urls, 10) print(word_counts) ``` **Expected Output:** The output should be a dictionary mapping each URL to the number of words in its content. URLs that caused an exception should be handled by logging the error and excluding them from the results. ```python { \'http://www.foxnews.com/\': <word_count>, \'http://www.cnn.com/\': <word_count>, \'http://europe.wsj.com/\': <word_count>, \'http://www.bbc.co.uk/\': <word_count>, # The non-existent URL will not be in the results } ``` **Note:** Replace `<word_count>` with the actual word count for each respective URL.","solution":"import logging from typing import List, Dict from concurrent.futures import ThreadPoolExecutor, as_completed import urllib.request def fetch_and_process_urls(urls: List[str], timeout: float) -> Dict[str, int]: logging.basicConfig(level=logging.ERROR) def load_url(url, timeout): try: with urllib.request.urlopen(url, timeout=timeout) as conn: return conn.read().decode(\'utf-8\') except Exception as e: logging.error(f\\"Error fetching {url}: {e}\\") return None def count_words(text): return len(text.split()) results = {} with ThreadPoolExecutor(max_workers=5) as executor: future_to_url = {executor.submit(load_url, url, timeout): url for url in urls} for future in as_completed(future_to_url): url = future_to_url[future] try: data = future.result() if data is not None: results[url] = count_words(data) except Exception as exc: logging.error(f\\"{url} generated an exception: {exc}\\") return results"},{"question":"Objective: You are required to write a Python function that interacts with the Tkinter font module to dynamically manage font properties and displays text in a Tkinter window. The function should allow a user to input text and font specifications, and then display the text using the specified font properties. Task: Implement a function `display_text_with_font` with the following specifications: - **Input**: - `text` (str): The text to be displayed. - `font_family` (str, optional): The font family (default is \\"Arial\\"). - `font_size` (int, optional): The font size in points (default is 12). - `font_weight` (str, optional): The font weight (`\\"normal\\"` or `\\"bold\\"`, default is \\"normal\\"). - `font_slant` (str, optional): The font slant (`\\"roman\\"` or `\\"italic\\"`, default is \\"roman\\"). - `underline` (int, optional): Underline the text (0 for no, 1 for yes, default is 0). - `overstrike` (int, optional): Overstrike the text (0 for no, 1 for yes, default is 0). - **Output**: - Displays the input text in a Tkinter window with the specified font properties. Constraints: - `font_size` must be a positive integer. - `font_weight` must be either \\"normal\\" or \\"bold\\". - `font_slant` must be either \\"roman\\" or \\"italic\\". - `underline` must be either 0 or 1. - `overstrike` must be either 0 or 1. Example: ```python def display_text_with_font(text, font_family=\\"Arial\\", font_size=12, font_weight=\\"normal\\", font_slant=\\"roman\\", underline=0, overstrike=0): # Your code goes here # Sample usage: display_text_with_font( text=\\"Hello, World!\\", font_family=\\"Times New Roman\\", font_size=16, font_weight=\\"bold\\", font_slant=\\"italic\\", underline=1, overstrike=0 ) ``` Notes: - Ensure the user inputs are properly validated before applying them to the font properties. - Make use of the `tkinter` library to create the GUI window and display the text. - Utilize `tkinter.font.Font` class to create and manage font properties. Implementation Hints: - Explore the `tkinter` and `tkinter.font` modules for managing GUI components and font settings. - Consider creating a root window, and then add a Label widget to display the text with the specified font options. - Handle any possible exceptions that may occur due to invalid input values.","solution":"import tkinter as tk from tkinter import font as tkfont def display_text_with_font(text, font_family=\\"Arial\\", font_size=12, font_weight=\\"normal\\", font_slant=\\"roman\\", underline=0, overstrike=0): if font_weight not in (\\"normal\\", \\"bold\\"): raise ValueError(\\"font_weight must be either \'normal\' or \'bold\'\\") if font_slant not in (\\"roman\\", \\"italic\\"): raise ValueError(\\"font_slant must be either \'roman\' or \'italic\'\\") if not isinstance(font_size, int) or font_size <= 0: raise ValueError(\\"font_size must be a positive integer\\") if underline not in (0, 1): raise ValueError(\\"underline must be either 0 or 1\\") if overstrike not in (0, 1): raise ValueError(\\"overstrike must be either 0 or 1\\") root = tk.Tk() custom_font = tkfont.Font(family=font_family, size=font_size, weight=font_weight, slant=font_slant, underline=underline, overstrike=overstrike) label = tk.Label(root, text=text, font=custom_font) label.pack() root.mainloop()"},{"question":"Extending PyTorch with Custom Tensor-like Class Objective: Implement a custom tensor-like class that integrates with PyTorch\'s `__torch_function__` protocol using the utilities provided by the `torch.overrides` module. Description: You need to create a class `MyTensor` which behaves similarly to `torch.Tensor` and integrates with PyTorch operations using the `__torch_function__` protocol. Specifically, the class should: 1. Implement the necessary methods to be recognized as a tensor-like object. 2. Override a PyTorch operation (e.g., `torch.add`) to demonstrate the custom behavior. 3. Use `torch.overrides.is_tensor_like` to validate instances of your class. 4. Ensure compatibility with PyTorch\'s operation dispatching mechanism using `torch.overrides.handle_torch_function`. Requirements: 1. Your class `MyTensor` should initialize with a numpy array and store it internally. 2. Implement the `__torch_function__` method to override the `torch.add` functionality. Ensure that if one of the inputs is an instance of `MyTensor`, the result should be a `MyTensor` with elements incremented by 1 in addition to the normal addition operation. 3. Ensure that `MyTensor` is detected as tensor-like by using `torch.overrides.is_tensor_like`. 4. Write test cases to verify that your `MyTensor` works correctly with `torch.add`. Function Signature: ```python import torch import numpy as np from torch.overrides import handle_torch_function, is_tensor_like class MyTensor: def __init__(self, data: np.ndarray): # Initialize MyTensor with a numpy array def __torch_function__(self, func, types, args=(), kwargs=None): # Override __torch_function__ to implement custom addition logic @property def data(self): # Return the underlying numpy data def __repr__(self): # Return a string representation of MyTensor # Test cases def test_mytensor(): # Create instances of MyTensor and torch.Tensor # Perform torch.add and verify custom behavior ``` Constraints: - Do not use any external libraries other than PyTorch and numpy. - The solution should adhere to the intended use of the `torch.overrides` module. - The custom behavior in `torch.add` should add an extra increment of 1 to each element of the result when any operand is an instance of `MyTensor`. Example: ```python # Example usage of MyTensor a = MyTensor(np.array([1, 2, 3])) b = torch.tensor([4, 5, 6]) result = torch.add(a, b) print(result) # MyTensor([6, 8, 10]) - each element incremented by 1 in addition to addition ```","solution":"import torch import numpy as np from torch.overrides import handle_torch_function, is_tensor_like class MyTensor: def __init__(self, data: np.ndarray): self._data = data def __torch_function__(self, func, types, args=(), kwargs=None): if kwargs is None: kwargs = {} if func == torch.add: result_data = np.add(args[0].data if isinstance(args[0], MyTensor) else args[0].numpy(), args[1].data if isinstance(args[1], MyTensor) else args[1].numpy()) result_data += 1 # Custom behavior: increment each element by 1 return MyTensor(result_data) return handle_ttorch_function(pfunc, types, args, kwargs) @property def data(self): return self._data def __repr__(self): return f\\"MyTensor({self.data})\\" # Checking if MyTensor is tensor-like print(is_tensor_like(MyTensor(np.array([1, 2, 3])))) # Example usage a = MyTensor(np.array([1, 2, 3])) b = torch.tensor([4, 5, 6]) result = torch.add(a, b) print(result) # MyTensor([6, 8, 10]) - each element incremented by 1 in addition to addition"},{"question":"Problem Statement: You are given a directory containing multiple Python source files (.py). Your task is to write a Python script that compiles all the Python source files in the specified directory to bytecode files using the `py_compile` module. The compiled files should be stored in a specific directory (`__pycache__` directory) within the given directory. If there is a compilation error for any of the files, capture the error and print a user-friendly message explicitly mentioning which file failed and why. Requirements: 1. Write a function `compile_directory(src_dir: str) -> None` which takes the absolute path of the source directory containing Python files (`src_dir`). 2. Compile each `.py` file in `src_dir` to bytecode. 3. Store the bytecode files in the `__pycache__` directory within `src_dir`. 4. Use `py_compile.compile()` function with the following parameters: - `doraise=True` to ensure that exceptions are raised. - `optimize=2` for optimized bytecode. 5. Use exception handling to manage `PyCompileError` and print a clear error message with the file name that failed. 6. If `__pycache__` directory does not exist, create it. Constraints: - Python 3.10 or later must be used. - You are not allowed to use any third-party libraries. - Ensure that the script works both on Unix-like systems and Windows. Example: Suppose you have a directory structure: ``` /path/to/source/ file1.py file2.py file3.py ``` After running `compile_directory(\\"/path/to/source/\\")`, you should have: ``` /path/to/source/ file1.py file2.py file3.py __pycache__/ file1.cpython-310.opt-2.pyc file2.cpython-310.opt-2.pyc file3.cpython-310.opt-2.pyc ``` And, if `file2.py` had a syntax error, the output should include: ``` Error compiling file2.py: (details of the error) ``` Function Signature: ```python import os import py_compile def compile_directory(src_dir: str) -> None: # Your implementation here ``` **Note:** Consider edge cases such as missing or inaccessible directories, files without `.py` extension, and invalid Python code.","solution":"import os import py_compile def compile_directory(src_dir: str) -> None: # Check if the __pycache__ directory exists, if not create it cache_dir = os.path.join(src_dir, \'__pycache__\') if not os.path.exists(cache_dir): os.makedirs(cache_dir) # Iterate over all the files in the source directory for filename in os.listdir(src_dir): if filename.endswith(\'.py\'): file_path = os.path.join(src_dir, filename) try: py_compile.compile(file_path, cfile=None, dfile=None, doraise=True, optimize=2) except py_compile.PyCompileError as ex: print(f\\"Error compiling {filename}: {ex.msg}\\") # Example usage # compile_directory(\'/path/to/source\')"},{"question":"You are tasked with implementing a function using the `nis` module to fetch and process NIS information on a Unix system. Function Signature: ```python def process_nis_data(mapname: str, key: str, domain: str = None) -> str: pass ``` Description: 1. **Parameters**: - `mapname` (string): The name of the NIS map to query. - `key` (string): The key to search for in the NIS map. - `domain` (string, optional): The NIS domain to use for queries. If `None`, use the systems default NIS domain. 2. **Returns**: - A string output containing: - The default domain. - The value associated with the provided key in the NIS map. - A sorted, comma-separated list of all keys from the map. 3. **Constraints**: - If the key does not exist in the map, catch the `nis.error` and return an appropriate error message. - Ensure to handle potential NULL and other arbitrary bytes in both keys and values gracefully. 4. **Performance Requirements**: - The function should execute efficiently even with large datasets. Example: ```python try: output = process_nis_data(\'passwd.byname\', \'root\') print(output) except Exception as e: print(e) ``` **Expected Output**: ```plaintext Default Domain: example.com Key Value: some_value All Keys: key1,key2,key3,key4 ``` Additional Notes: - This function will only work on Unix systems due to the nature of the `nis` module. - Ensure that the `nis` module is available and correctly configured on your system to be able to test this function. - Your implementation should handle and return results even if the `domain` parameter is not provided.","solution":"import nis def process_nis_data(mapname: str, key: str, domain: str = None) -> str: try: # Fetch the default domain if the domain is not provided if domain is None: domain = nis.get_default_domain() # Fetch the value for the given key key_value = nis.match(key, mapname) # Fetch all keys from the map all_keys = nis.cat(mapname).keys() # Sort all keys and convert to a comma-separated string sorted_keys = sorted(all_keys) sorted_keys_str = \\",\\".join(sorted_keys) # Construct the output string result = ( f\\"Default Domain: {domain}n\\" f\\"Key Value: {key_value.decode(\'utf-8\')}n\\" f\\"All Keys: {sorted_keys_str}\\" ) return result except nis.error as e: return f\\"Error: {e}\\" # Catch potential other exceptions and return a generic error message except Exception as e: return f\\"Unexpected error: {e}\\""},{"question":"You are provided with a dataset on health expenditure (`healthexp`). Your task is to create a set of visualizations using seaborn\'s objects interface to answer the following: 1. **Dataset Overview**: Load the `healthexp` dataset using seaborn. Display the first five rows of the dataset for an overview. 2. **Basic Line Plot**: Create a line plot to show the trend of health expenditure (`Spending_USD`) over the years (`Year`) for the top 5 countries with the highest spending in the most recent year available in the dataset. Use different colors for each country. 3. **Relative Scale Plot**: Create a plot similar to the basic line plot, but scale the health expenditure values relative to the maximum value within each group (country). Label the y-axis as \\"Spending relative to maximum amount\\". 4. **Percentage Change Plot**: Create a plot to show the percent change in spending from the year 1970 baseline for each country. Use the `so.Norm()` function to apply this transformation and label the y-axis as \\"Percent change in spending from 1970 baseline\\". # Constraints and Requirements: - Use seaborn\'s objects interface (`so.Plot()` and related methods) for visualization. - Ensure the plots are clear and correctly labeled. - The solution should handle cases where some data might not have values for the year 1970. # Expected Input and Output: **Input:** - You will use the `healthexp` dataset provided by `seaborn.load_dataset(\\"healthexp\\")`. **Output:** - Create and display the plots as specified. **Performance Requirements:** - The code should efficiently handle the dataset operations and plotting. - Use vectorized operations and seaborn\'s high-level plotting functions for clarity and performance. # Hints: - Use Pandas or Seaborn methods to filter and sort the data. - Utilize seaborn\'s normalization functions (`so.Norm()`) for scaling. - Check seaborn and matplotlib documentation for additional customization options. Here is the structure of your implementation: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset healthexp = load_dataset(\\"healthexp\\") # 1. Display the first five rows of the dataset print(healthexp.head()) # 2. Basic Line Plot # Implement your logic here # 3. Relative Scale Plot # Implement your logic here # 4. Percentage Change Plot # Implement your logic here ``` Create each plot step-by-step and display them accordingly.","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd import matplotlib.pyplot as plt # Load the dataset healthexp = load_dataset(\\"healthexp\\") # 1. Display the first five rows of the dataset print(healthexp.head()) # 2. Basic Line Plot # Get the most recent year of data most_recent_year = healthexp[\'Year\'].max() # Filter to get the top 5 countries by health expenditure in the most recent year top_countries = ( healthexp[healthexp[\\"Year\\"] == most_recent_year] .sort_values(by=\\"Spending_USD\\", ascending=False) .head(5)[\\"Country\\"] ) # Filter the dataset for these top 5 countries top_countries_data = healthexp[healthexp[\\"Country\\"].isin(top_countries)] # Create the basic line plot basic_line_plot = so.Plot(top_countries_data, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\").add(so.Line()).label(x=\\"Year\\", y=\\"Spending USD\\", color=\\"Country\\") basic_line_plot.show() # 3. Relative Scale Plot # Normalize spending within each country group top_countries_data[\'Relative_Spending\'] = top_countries_data.groupby(\\"Country\\")[\'Spending_USD\'].transform(lambda x: x / x.max()) # Create the relative scale plot relative_scale_plot = ( so.Plot(top_countries_data, x=\\"Year\\", y=\\"Relative_Spending\\", color=\\"Country\\") .add(so.Line()) .label(x=\\"Year\\", y=\\"Spending relative to maximum amount\\", color=\\"Country\\") ) relative_scale_plot.show() # 4. Percentage Change Plot # Calculate the spending in 1970 for each country base_spending_1970 = top_countries_data[top_countries_data[\\"Year\\"] == 1970].set_index(\\"Country\\")[\'Spending_USD\'] # Calculate the percent change relative to the 1970 baseline top_countries_data[\'Percent_Change_Spending\'] = top_countries_data.apply( lambda row: (row[\'Spending_USD\'] - base_spending_1970.get(row[\'Country\'], row[\'Spending_USD\'])) / base_spending_1970.get(row[\'Country\'], row[\'Spending_USD\']) * 100, axis=1 ) # Create the percent change plot percent_change_plot = ( so.Plot(top_countries_data, x=\\"Year\\", y=\\"Percent_Change_Spending\\", color=\\"Country\\") .add(so.Line()) .label(x=\\"Year\\", y=\\"Percent change in spending from 1970 baseline\\", color=\\"Country\\") ) percent_change_plot.show()"},{"question":"# Python Versioning and Hexadecimal Encoding You are tasked with implementing a function in Python that handles Python versioning information encoded in a single hexadecimal integer (PY_VERSION_HEX). You need to write two primary functions to achieve this: 1. `decode_version(hex_version: int) -> Dict[str, Union[int, str]]`: - This function will accept a hexadecimal version number as an integer. - It should return a dictionary with keys: - `\'major\'`: the major version (an integer) - `\'minor\'`: the minor version (an integer) - `\'micro\'`: the micro version (an integer) - `\'level\'`: the release level (an integer: 0xA for alpha, 0xB for beta, 0xC for candidate, and 0xF for final) - `\'serial\'`: the release serial (an integer) 2. `encode_version(version_info: Dict[str, Union[int, str]]) -> int`: - This function will accept a dictionary representing the version details with the same structure as the dictionary returned by `decode_version`. - It should return the hexadecimal version number as an integer. # Input - `decode_version(hex_version: int)`: - `hex_version`: An integer representing the Python version encoded in hexadecimal format. Example: `0x030401a2`. - `encode_version(version_info: Dict[str, Union[int, str]])`: - `version_info`: A dictionary containing the version details in the following structure: ```python { \'major\': int, # E.g., 3 \'minor\': int, # E.g., 4 \'micro\': int, # E.g., 1 \'level\': int, # E.g., 0xA \'serial\': int # E.g., 2 } ``` # Output - `decode_version(hex_version: int) -> Dict[str, Union[int, str]]`: - Returns a dictionary with the decoded version information. - `encode_version(version_info: Dict[str, Union[int, str]]) -> int`: - Returns an integer representing the encoded hexadecimal version. # Constraints - The `hex_version` provided to `decode_version` will always be a valid Python version hex representation. - The `version_info` provided to `encode_version` will always contain valid version details within the expected ranges (major, minor, micro should be non-negative integers, level should be one of 0xA, 0xB, 0xC, 0xF, and serial should be a non-negative integer). # Example ```python hex_version = 0x030401a2 version_info = decode_version(hex_version) # Expected output: # { # \'major\': 3, # \'minor\': 4, # \'micro\': 1, # \'level\': 0xA, # alpha # \'serial\': 2 # } encoded_version = encode_version(version_info) # Expected output: # 0x030401a2 ``` Implement the functions `decode_version` and `encode_version` in Python. Ensure your solution is efficient and takes into account possible edge cases.","solution":"def decode_version(hex_version: int) -> dict: Decodes the provided hexadecimal version number. major = (hex_version >> 24) & 0xFF minor = (hex_version >> 16) & 0xFF micro = (hex_version >> 8) & 0xFF level = (hex_version >> 4) & 0xF serial = hex_version & 0xF return { \'major\': major, \'minor\': minor, \'micro\': micro, \'level\': level, \'serial\': serial } def encode_version(version_info: dict) -> int: Encodes the provided version information into a hexadecimal version number. major = version_info[\'major\'] minor = version_info[\'minor\'] micro = version_info[\'micro\'] level = version_info[\'level\'] serial = version_info[\'serial\'] hex_version = (major << 24) | (minor << 16) | (micro << 8) | (level << 4) | serial return hex_version"},{"question":"Objective: Create a Python application to manage a list of tasks, allowing tasks to be added, completed, listed, saved to disk, and loaded from disk. Detailed Requirements: 1. **Define a `Task` class** with the following attributes: - `id` (int): Unique identifier for the task. - `title` (str): Title of the task. - `description` (str): Detailed description of the task. - `completed` (bool): Status of the task, `True` if completed, otherwise `False`. 2. **Implement a `TaskManager` class** using SQLite to handle the following functionalities: - `__init__(self, db_path: str)`: Initialize the task manager with the path to the SQLite database. - `add_task(self, task: Task)`: Add a new task to the database. - `complete_task(self, task_id: int)`: Mark a task as completed using its id. - `list_tasks(self) -> List[Task]`: List all tasks stored in the database. - `save_to_disk(self, filename: str)`: Serialize the current list of tasks and save it to disk using `pickle`. - `load_from_disk(self, filename: str)`: Load a list of tasks from a file using `pickle` and update the database accordingly. 3. **Constraints and Caveats:** - Task IDs should be auto-incremented. - Operations should handle exceptions gracefully and log appropriate messages. - Ensure that the SQLite database schema is created if it does not exist. - Loading tasks from disk should merge with the existing database rather than replacing it. Example Output: ```python # Example usage: # Initialize the task manager task_manager = TaskManager(\'tasks.db\') # Add a task task = Task(id=None, title=\\"Buy groceries\\", description=\\"Buy milk, eggs, and bread.\\", completed=False) task_manager.add_task(task) # List tasks tasks = task_manager.list_tasks() for task in tasks: print(task) # Complete a task task_manager.complete_task(1) # Save tasks to disk task_manager.save_to_disk(\'tasks.pkl\') # Load tasks from disk task_manager.load_from_disk(\'tasks.pkl\') ``` Performance Requirements: - Stored data should persist and be retrievable efficiently. - Serialization and deserialization processes should handle large files containing many tasks.","solution":"import sqlite3 import pickle from typing import List, Optional class Task: def __init__(self, id: Optional[int], title: str, description: str, completed: bool): self.id = id self.title = title self.description = description self.completed = completed def __repr__(self): return f\\"Task(id={self.id}, title={self.title}, description={self.description}, completed={self.completed})\\" class TaskManager: def __init__(self, db_path: str): self.db_path = db_path self._initialize_db() def _initialize_db(self): with sqlite3.connect(self.db_path) as conn: cursor = conn.cursor() cursor.execute(\'\'\'CREATE TABLE IF NOT EXISTS tasks ( id INTEGER PRIMARY KEY AUTOINCREMENT, title TEXT NOT NULL, description TEXT NOT NULL, completed BOOLEAN NOT NULL )\'\'\') conn.commit() def add_task(self, task: Task): with sqlite3.connect(self.db_path) as conn: cursor = conn.cursor() cursor.execute(\'\'\'INSERT INTO tasks (title, description, completed) VALUES (?, ?, ?)\'\'\', (task.title, task.description, task.completed)) conn.commit() def complete_task(self, task_id: int): with sqlite3.connect(self.db_path) as conn: cursor = conn.cursor() cursor.execute(\'\'\'UPDATE tasks SET completed = TRUE WHERE id = ?\'\'\', (task_id,)) conn.commit() def list_tasks(self) -> List[Task]: with sqlite3.connect(self.db_path) as conn: cursor = conn.cursor() cursor.execute(\'\'\'SELECT id, title, description, completed FROM tasks\'\'\') rows = cursor.fetchall() return [Task(id=row[0], title=row[1], description=row[2], completed=row[3]) for row in rows] def save_to_disk(self, filename: str): tasks = self.list_tasks() with open(filename, \'wb\') as f: pickle.dump(tasks, f) def load_from_disk(self, filename: str): with open(filename, \'rb\') as f: tasks = pickle.load(f) with sqlite3.connect(self.db_path) as conn: cursor = conn.cursor() for task in tasks: cursor.execute(\'\'\'INSERT OR IGNORE INTO tasks (id, title, description, completed) VALUES (?, ?, ?, ?)\'\'\', (task.id, task.title, task.description, task.completed)) conn.commit()"},{"question":"**Question:** You are tasked with writing a Python function that takes a list of user login names and returns a dictionary with the login names as keys and their respective home directories as values. If a login name does not exist in the password database, it should be skipped. Implement the function `get_user_home_directories(user_names: list) -> dict`. # Constraints: 1. Use the `pwd` module to access the Unix user account and password database. 2. The function should handle any potential exceptions gracefully. 3. The function should only return entries for valid user names; invalid user names should not appear in the result. # Input: - `user_names` (list): A list of strings containing user login names. # Output: - A dictionary where the keys are valid user login names from the input list, and the values are their respective home directories. # Example: ```python user_names = [\'root\', \'nonexistentuser\', \'john\', \'doe\'] expected_output = { \'root\': \'/root\', \'john\': \'/home/john\' } assert get_user_home_directories(user_names) == expected_output ``` # Notes: - Ensure that your function performs well with a large number of user names. - Make sure to handle scenarios where some user names may not exist by checking for exceptions raised by the `pwd` module.","solution":"import pwd def get_user_home_directories(user_names): Takes a list of user login names and returns a dictionary with the login names as keys and their respective home directories as values. user_home_directories = {} for user in user_names: try: pw_info = pwd.getpwnam(user) user_home_directories[user] = pw_info.pw_dir except KeyError: # User does not exist, skip this user continue return user_home_directories"},{"question":"# Advanced Turtle Graphics Challenge Objective: Write a Python program using the `turtle` module that creates a dynamic visual illusion known as the \\"Spirograph.\\" Requirements: 1. **Input:** - Two integers, `R` and `r`, representing the radii of the fixed and rotating circles. - An integer `l`, representing the ratio of the distance from the center of the rotating circle to the drawing point. - An integer `n`, which determines how many cycles the drawing should complete. 2. **Output:** - A graphical window displaying the spirograph pattern generated by the turtle based on the provided inputs. Constraints: - `R`, `r`, and `l` are positive integers, where `R > r > 0` and `0 < l  R`. - The spirograph should continue until it completes `n` full cycles. Performance Requirements: - The graphical window should update efficiently to visually display the drawing process of the spirograph. Sample Function Signature: ```python def draw_spirograph(R: int, r: int, l: int, n: int) -> None: pass ``` Implementation Notes: - Begin by setting up the Turtle screen and creating a turtle object. - Utilize trigonometric functions (sine and cosine) to calculate the positions of the drawing turtle. - Use loops to control the number of cycles and drawing updates. - Demonstrate understanding of Turtle methods for motion, state, visibility, and event handling. Example: If the input parameters are `R=60`, `r=30`, `l=45`, and `n=6`, the turtle should draw the spirograph pattern with these specifications, completing in 6 cycles. Additional Information: Carefully consider the mathematical formula for spirograph positions, which generally involves parametric equations: [ x(t) = (R - r) cos(t) + l cosleft(frac{R-r}{r}tright) ] [ y(t) = (R - r) sin(t) - l sinleft(frac{R-r}{r}tright) ] where `t` changes over time to form the continuous drawing motion of the spirograph.","solution":"import turtle import math def draw_spirograph(R: int, r: int, l: int, n: int) -> None: screen = turtle.Screen() screen.bgcolor(\\"white\\") spiro = turtle.Turtle() spiro.speed(0) # Maximum speed for drawing # Function to compute the coordinates of the spirograph at a given t def get_spirograph_coords(t, R, r, l): x = (R - r) * math.cos(t) + l * math.cos(((R - r) / r) * t) y = (R - r) * math.sin(t) - l * math.sin(((R - r) / r) * t) return x, y # Angle change step steps = 360 * n # Number of steps to complete n cycles delta_t = 2 * math.pi / 360 # Angle increment for each step spiro.penup() for i in range(steps + 1): t = i * delta_t x, y = get_spirograph_coords(t, R, r, l) spiro.goto(x, y) spiro.pendown() spiro.hideturtle() screen.exitonclick() # Example usage # draw_spirograph(R=60, r=30, l=45, n=6)"},{"question":"# Python Development Mode Coding Assessment Objective Write a Python script that reads a file and processes its content. Your task is to ensure that your code properly handles resources, such as files, and avoids common mistakes that can be detected by Python Development Mode. Instructions 1. **Function Implementation**: Implement a function `process_file(file_path: str) -> int` that: - Opens the file specified by `file_path`. - Counts the number of lines starting with a specific keyword, e.g., \\"TODO\\". - Closes the file properly. 2. **Enable Python Development Mode**: Run your script with Python Development Mode enabled and ensure it does not raise any warnings or errors. 3. **Constraints and Requirements**: - You must use a context manager (`with` statement) to handle file operations. - Your solution should not trigger any `ResourceWarning` or similar issues when Python Development Mode is enabled. - Assume the file is in plain text format. Example Given a file `example.txt` with the following content: ``` This is a test file. TODO: This line should be counted. This is another line. TODO: This line should also be counted. ``` The function call `process_file(\'example.txt\')` should return `2`. Input: - `file_path` (str): The path to the file that needs to be processed. Output: - `int`: The number of lines starting with \\"TODO\\". Performance Requirements: - The solution should not unnecessarily read or process lines that do not start with the keyword. Here is a basic structure to get you started: ```python def process_file(file_path: str) -> int: keyword = \\"TODO\\" count = 0 # Your code to open the file and count lines goes here return count if __name__ == \\"__main__\\": import sys if len(sys.argv) != 2: print(\\"Usage: python script.py <file_path>\\") sys.exit(1) file_path = sys.argv[1] todo_count = process_file(file_path) print(f\\"Number of TODO lines: {todo_count}\\") ``` Evaluation - **Correctness**: The function should correctly count the lines starting with the keyword. - **Code Quality**: Proper resource management and absence of warnings/errors in Python Development Mode are crucial. - **Performance**: Efficiently process the file without reading unnecessary lines. Test your code using the command: ```sh python3 -X dev script.py example.txt ```","solution":"def process_file(file_path: str) -> int: Opens the file specified by file_path, counts the number of lines starting with \\"TODO\\", and returns this count. keyword = \\"TODO\\" count = 0 with open(file_path, \'r\') as file: for line in file: if line.startswith(keyword): count += 1 return count"},{"question":"**Question: Implement a Custom Dictionary with Expiry Mechanism** Your task is to design a custom dictionary class `ExpireDict`, which extends the built-in Python dictionary with an additional feature: key-value pairs expire after a specified amount of time. This class should support all standard dictionary operations and should automatically remove expired items. # Requirements: 1. **Initialization**: - The `ExpireDict` should be initialized just like a standard dictionary but also accept an additional parameter `expiry_seconds` which specifies the duration (in seconds) after which any key-value pair in the dictionary should expire. 2. **Adding Items**: - When adding an item to the dictionary using `__setitem__` (i.e., through the syntax `d[key] = value`), it should store the current time along with the key-value pair to track its expiry. 3. **Accessing Items**: - When accessing an item using `__getitem__` (i.e., through the syntax `value = d[key]`), the dictionary should check if the item has expired. If it has, it should remove the item from the dictionary and raise a `KeyError`. 4. **Removing Items**: - Removal of items using `__delitem__` (i.e., through the syntax `del d[key]`) should function as usual. 5. **Expiration Check**: - Implement a private method `_expire_keys` to check and remove expired keys whenever the dictionary is accessed or modified. 6. **Dictionary Methods**: - Implement the `keys`, `values`, `items`, `get`, `pop`, `update`, and `clear` methods ensuring they handle expired keys appropriately. # Input Format: You do not need to handle input from the user directly. Instead, focus on implementing the classes and methods as described. # Output Format: Your code should not produce any direct output. Instead, ensure all methods work correctly by utilizing the expected behavior of Python dictionary operations. # Example Usage: ```python import time # Create a dictionary with 5 seconds expiry time d = ExpireDict(expiry_seconds=5) # Add some items d[\'a\'] = 1 d[\'b\'] = 2 # Accessing items within the expiry time print(d[\'a\']) # Output: 1 # Wait for 6 seconds (beyond the expiry time) time.sleep(6) # Accessing expired items should raise KeyError try: value = d[\'a\'] # This should raise KeyError except KeyError: print(\\"Key \'a\' has expired\\") # The dict should now be empty since the items have expired print(d) # Output: {} ``` # Implementation Constraints: - Do not use external libraries other than the standard Python library. - Ensure your implementation is efficient in terms of time complexity and avoids unnecessary computations. # Class Definition: ```python import time class ExpireDict: def __init__(self, *args, expiry_seconds=60, **kwargs): self.expiry_seconds = expiry_seconds self._store = dict(*args, **kwargs) self._timestamps = {key: time.time() for key in self._store} def __setitem__(self, key, value): self._expire_keys() self._store[key] = value self._timestamps[key] = time.time() def __getitem__(self, key): self._expire_keys() if key in self._store: return self._store[key] raise KeyError(key) def __delitem__(self, key): self._expire_keys() del self._store[key] del self._timestamps[key] def keys(self): self._expire_keys() return self._store.keys() def values(self): self._expire_keys() return self._store.values() def items(self): self._expire_keys() return self._store.items() def get(self, key, default=None): self._expire_keys() return self._store.get(key, default) def pop(self, key, default=None): self._expire_keys() value = self._store.pop(key, default) if key in self._timestamps: self._timestamps.pop(key) return value def update(self, *args, **kwargs): self._expire_keys() self._store.update(*args, **kwargs) for key in dict(*args, **kwargs).keys(): self._timestamps[key] = time.time() def clear(self): self._store.clear() self._timestamps.clear() def _expire_keys(self): current_time = time.time() expired_keys = [key for key, timestamp in self._timestamps.items() if current_time - timestamp > self.expiry_seconds] for key in expired_keys: del self._store[key] del self._timestamps[key] ``` Implementing and testing this class, `ExpireDict`, should demonstrate a robust understanding of Python data types, dictionary operations, and handling time-based events.","solution":"import time class ExpireDict: def __init__(self, *args, expiry_seconds=60, **kwargs): self.expiry_seconds = expiry_seconds self._store = dict(*args, **kwargs) self._timestamps = {key: time.time() for key in self._store} def __setitem__(self, key, value): self._expire_keys() self._store[key] = value self._timestamps[key] = time.time() def __getitem__(self, key): self._expire_keys() if key in self._store: return self._store[key] raise KeyError(key) def __delitem__(self, key): self._expire_keys() del self._store[key] del self._timestamps[key] def keys(self): self._expire_keys() return self._store.keys() def values(self): self._expire_keys() return self._store.values() def items(self): self._expire_keys() return self._store.items() def get(self, key, default=None): self._expire_keys() return self._store.get(key, default) def pop(self, key, default=None): self._expire_keys() value = self._store.pop(key, default) if key in self._timestamps: self._timestamps.pop(key) return value def update(self, *args, **kwargs): self._expire_keys() self._store.update(*args, **kwargs) for key in dict(*args, **kwargs).keys(): self._timestamps[key] = time.time() def clear(self): self._store.clear() self._timestamps.clear() def _expire_keys(self): current_time = time.time() expired_keys = [key for key, timestamp in self._timestamps.items() if current_time - timestamp > self.expiry_seconds] for key in expired_keys: del self._store[key] del self._timestamps[key]"},{"question":"Implementing a Custom Number Protocol in Python # Background In Python, custom objects can be used with numerical operations by implementing the **Number Protocol**. This allows objects to define their behavior when involved in arithmetic operations like addition, subtraction, and multiplication. The `Number Protocol` consists of methods that objects can implement to support these operations. # Objective You are required to implement a custom class `SafeNumber` that supports basic arithmetic operations (addition, subtraction, multiplication, and division) by adhering to the `Number Protocol`. # Requirements 1. **Class Definition**: * Define a class `SafeNumber` that encapsulates an integer value. * Initialize the class with a single integer value. 2. **Implement Methods**: * Implement the following methods to support arithmetic operations: - `__add__(self, other)`: Supports addition with another `SafeNumber` or an integer. - `__sub__(self, other)`: Supports subtraction with another `SafeNumber` or an integer. - `__mul__(self, other)`: Supports multiplication with another `SafeNumber` or an integer. - `__truediv__(self, other)`: Supports division with another `SafeNumber` or an integer. * Ensure that operations involving integers convert the integer to a `SafeNumber` before performing the operation. 3. **Constraints**: * Raise a `ValueError` if division by zero is attempted. * The class should only work with integer values. 4. **Performance**: * Ensure that the operations are performed efficiently within the constraints of standard numerical operations. # Input and Output Formats * **Input**: * An integer value during class instantiation. * Another `SafeNumber` object or an integer during arithmetic operations. * **Output**: * A new `SafeNumber` object as the result of the arithmetic operation. * Raise `ValueError` for invalid operations (e.g., division by zero). # Example ```python class SafeNumber: def __init__(self, value: int): # Initialize with the given value ... def __add__(self, other): # Handle addition ... def __sub__(self, other): # Handle subtraction ... def __mul__(self, other): # Handle multiplication ... def __truediv__(self, other): # Handle division ... # Example Usage num1 = SafeNumber(10) num2 = SafeNumber(5) print((num1 + num2).value) # Output: 15 print((num1 - 2).value) # Output: 8 print((num1 * num2).value) # Output: 50 print((num1 / 2).value) # Output: 5 try: print(num1 / 0) # This should raise a ValueError except ValueError as e: print(e) # Output: ValueError: Division by zero ``` Implement the `SafeNumber` class to meet the above requirements.","solution":"class SafeNumber: def __init__(self, value: int): if not isinstance(value, int): raise ValueError(\\"Value must be an integer\\") self.value = value def __add__(self, other): if isinstance(other, SafeNumber): other_value = other.value elif isinstance(other, int): other_value = other else: raise ValueError(\\"Operand must be an integer or SafeNumber\\") return SafeNumber(self.value + other_value) def __sub__(self, other): if isinstance(other, SafeNumber): other_value = other.value elif isinstance(other, int): other_value = other else: raise ValueError(\\"Operand must be an integer or SafeNumber\\") return SafeNumber(self.value - other_value) def __mul__(self, other): if isinstance(other, SafeNumber): other_value = other.value elif isinstance(other, int): other_value = other else: raise ValueError(\\"Operand must be an integer or SafeNumber\\") return SafeNumber(self.value * other_value) def __truediv__(self, other): if isinstance(other, SafeNumber): other_value = other.value elif isinstance(other, int): other_value = other else: raise ValueError(\\"Operand must be an integer or SafeNumber\\") if other_value == 0: raise ValueError(\\"Division by zero\\") return SafeNumber(self.value // other_value)"},{"question":"Objective: Implement a function that takes a list of strings as input and returns a dictionary with two keys: \'escaped\' and \'unescaped\'. The \'escaped\' key should map to a list of strings where each string from the input list has been processed using the `html.escape` function. The \'unescaped\' key should map to a list of strings where each string has been processed using the `html.unescape` function. Requirements: - You must use the `html.escape` and `html.unescape` functions from the Python `html` module. - The output dictionary should maintain the order of input strings. - Ensure that all input strings are correctly processed according to the functionality described above. Input: - A list of strings, where each string may contain special HTML characters or HTML character references. Example: ```python [ \\"Hello & welcome to <Python>!\\", \\"A \'quote\' & another \\"quote\\".\\", \\"This is a test string with &#62; and &lt; symbols.\\" ] ``` Output: - A dictionary with two lists of strings: - `escaped`: List of strings processed with `html.escape`. - `unescaped`: List of strings processed with `html.unescape`. Example: ```python { \\"escaped\\": [ \\"Hello &amp; welcome to &lt;Python&gt;!\\", \\"A &#x27;quote&#x27; &amp; another &quot;quote&quot;.\\", \\"This is a test string with &amp;gt; and &amp;lt; symbols.\\" ], \\"unescaped\\": [ \\"Hello & welcome to <Python>!\\", \\"A \'quote\' & another \\"quote\\".\\", \\"This is a test string with > and < symbols.\\" ] } ``` Constraints: - The input list will have a maximum length of 1000 strings. - Each string in the input list will have a maximum length of 1000 characters. # Function Signature: ```python import html def process_html_strings(strings): pass # Example Usage: input_strings = [ \\"Hello & welcome to <Python>!\\", \\"A \'quote\' & another \\"quote\\".\\", \\"This is a test string with &#62; and &lt; symbols.\\" ] result = process_html_strings(input_strings) print(result) ```","solution":"import html def process_html_strings(strings): Processes a list of strings to return a dictionary with escaped and unescaped versions. Parameters: strings (list of str): The input list of strings to be processed. Returns: dict: A dictionary with two keys \'escaped\' and \'unescaped\', where each key maps to a list of processed strings. return { \\"escaped\\": [html.escape(s) for s in strings], \\"unescaped\\": [html.unescape(s) for s in strings] }"},{"question":"# Question: Time Series DateOffset Manipulation with pandas Objective: Implement a function using pandas that manipulates a given time series by applying various date offsets. The goal is to understand the usage and interaction of `DateOffset` and its subclasses effectively. Function Signature: ```python import pandas as pd from pandas.tseries.offsets import * def manipulate_time_series(df, offset_type, n): Function to manipulate a given time series DataFrame by applying a specified DateOffset. Parameters: df (pd.DataFrame): A DataFrame with a DateTime index and at least one column of data. offset_type (str): The type of DateOffset to apply. Valid values include \'BusinessDay\', \'MonthEnd\', \'YearBegin\', etc. n (int): The number of periods for the offset. Returns: pd.DataFrame: A new DataFrame with the applied DateOffset. pass ``` Requirements: 1. The date offset should be constructed dynamically based on the `offset_type` and `n` parameters. 2. The function should handle the following `DateOffset` types: `BusinessDay`, `MonthEnd`, `YearBegin`, `QuarterEnd`, `Week`, `Day`. 3. Apply the constructed offset to the DataFrame\'s DateTime index. 4. If an invalid `offset_type` is provided, raise a `ValueError`. Input: - `df` (pd.DataFrame): A DataFrame with DateTime index and at least one column of data. - `offset_type` (str): One of the specified `DateOffset` types. - `n` (int): The number of periods for the offset. Output: - A new DataFrame with the date offset applied to its DateTime index. Example: ```python data = {\'values\': [10, 22, 14, 17, 18, 23]} index = pd.date_range(\'2021-01-01\', periods=6, freq=\'D\') df = pd.DataFrame(data, index=index) # Applying a 2 BusinessDay offset result_df = manipulate_time_series(df, \'BusinessDay\', 2) print(result_df) # Applying a 1 MonthEnd offset result_df = manipulate_time_series(df, \'MonthEnd\', 1) print(result_df) # Applying a 3 YearBegin offset result_df = manipulate_time_series(df, \'YearBegin\', 3) print(result_df) ``` Constraints: - The function should not use any global variables; it should be self-contained. - The operations should be efficient to handle DataFrames of reasonable sizes (up to 100,000 rows). Notes: - Use the `pandas.tseries.offsets` module for creating date offsets. Evaluation Criteria: - Correctness of the implementation. - Handling of edge cases and invalid input gracefully. - Code quality and readability.","solution":"import pandas as pd from pandas.tseries.offsets import * def manipulate_time_series(df, offset_type, n): Function to manipulate a given time series DataFrame by applying a specified DateOffset. Parameters: df (pd.DataFrame): A DataFrame with a DateTime index and at least one column of data. offset_type (str): The type of DateOffset to apply. Valid values include \'BusinessDay\', \'MonthEnd\', \'YearBegin\', etc. n (int): The number of periods for the offset. Returns: pd.DataFrame: A new DataFrame with the applied DateOffset. offset_classes = { \'BusinessDay\': BusinessDay, \'MonthEnd\': MonthEnd, \'YearBegin\': YearBegin, \'QuarterEnd\': QuarterEnd, \'Week\': Week, \'Day\': Day } if offset_type not in offset_classes: raise ValueError(f\\"Invalid offset_type {offset_type}. Must be one of {list(offset_classes.keys())}.\\") offset = offset_classes[offset_type](n) new_index = df.index + offset new_df = df.copy() new_df.index = new_index return new_df"},{"question":"# Question: Rational Arithmetic Operations with Fraction Class Objective: Implement a function that performs rational number arithmetic using the `Fraction` class from the `fractions` module. You will create a function to evaluate a simple mathematical expression containing fractions and return the result as a fraction in its lowest terms. Function Signature: ```python def evaluate_expression(expression: str) -> str: Evaluates a simple mathematical expression involving fractions and returns the result as a string in the format \'numerator/denominator\'. Parameters: expression (str): A string representing the mathematical expression involving fractions, e.g., \'1/2 + 3/4\'. Returns: str: The result of the evaluation in the format \'numerator/denominator\'. ``` Input: - `expression` (str): A string representing the mathematical expression involving fractions, using the format `\'numerator/denominator operator numerator/denominator\'`. The only operators allowed are `+`, `-`, `*`, and `/`. Output: - Return the result as a string in the format \'numerator/denominator\'. Ensure that the fraction is simplified to its lowest terms. Constraints: - The fraction components (numerators and denominators) are valid integers. - The input string will always be a valid mathematical expression involving fractions. - Denominators are non-zero integers. Examples: ```python evaluate_expression(\'1/2 + 3/4\') # Should return \'5/4\' evaluate_expression(\'1/2 - 1/4\') # Should return \'1/4\' evaluate_expression(\'2/3 * 3/4\') # Should return \'1/2\' evaluate_expression(\'1/2 / 3/4\') # Should return \'2/3\' ``` Task Breakdown: 1. Parse the input string to identify the fractions and the operator. 2. Use the `fractions.Fraction` class to convert these string fractions into `Fraction` objects. 3. Perform the appropriate arithmetic operation using the `Fraction` objects. 4. Simplify the resulting fraction if necessary. 5. Convert the resulting fraction back into a string in the format \'numerator/denominator\'. 6. Return the result. Notes: - Make use of the `fractions.Fraction` constructors and methods for parsing and arithmetic operations. - Handle any potential edge cases, such as very large numerators or denominators, though Python\'s `fractions.Fraction` typically handles large numbers efficiently. Good luck!","solution":"from fractions import Fraction def evaluate_expression(expression: str) -> str: Evaluates a simple mathematical expression involving fractions and returns the result as a string in the format \'numerator/denominator\'. Parameters: expression (str): A string representing the mathematical expression involving fractions, e.g., \'1/2 + 3/4\'. Returns: str: The result of the evaluation in the format \'numerator/denominator\'. parts = expression.split() fraction1 = Fraction(parts[0]) operator = parts[1] fraction2 = Fraction(parts[2]) if operator == \'+\': result = fraction1 + fraction2 elif operator == \'-\': result = fraction1 - fraction2 elif operator == \'*\': result = fraction1 * fraction2 elif operator == \'/\': result = fraction1 / fraction2 else: raise ValueError(\\"Unsupported operator\\") return f\\"{result.numerator}/{result.denominator}\\""},{"question":"Objective: To assess your understanding of implementing, profiling, and optimizing machine learning models using scikit-learn. Task: You are given a dataset and your task is to create a K-Nearest Neighbors (KNN) classifier. You need to implement the following steps: 1. Load the dataset. 2. Implement a KNN classifier using scikit-learn. 3. Profile the execution time of the classifier fitting process. 4. Optimize the code to improve performance, using vectorized operations or any optimization techniques mentioned in the provided documentation. 5. Additionally, parallelize the fitting process of the classifier using `joblib.Parallel`. Input: - A dataset in the form of two NumPy arrays `X` (features) and `y` (labels). Output: 1. The optimized KNN classifier implementation. 2. A report on the profiling results before and after optimization, including detailed descriptions of the improvements made. 3. (Optional) Any additional notes on further improvements or observations. Constraints: - Ensure the implementation is efficient, readable, and follows the best practices for scikit-learn models. - Provide a detailed profiling report showing clear improvements in performance. Example: Given the `load_digits` dataset from scikit-learn, implement the KNN classifier, profile the execution time and optimize it. Additionally, use multi-core parallelism to improve performance. ```python from sklearn.datasets import load_digits from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split from joblib import Parallel, delayed # Load dataset X, y = load_digits(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Implement KNN classifier def knn_classifier(X_train, y_train, X_test, y_test): knn = KNeighborsClassifier(n_neighbors=3) knn.fit(X_train, y_train) accuracy = knn.score(X_test, y_test) return accuracy # Profile code before optimization # (Add profiling here) # Optimize code # (Implement optimization techniques) # Parallelize fitting process # (Use joblib.Parallel for parallel processing) # Output the results # (Include profiling reports and observed improvements) ``` Ensure to document your code well and explain the steps you took in your optimization process.","solution":"import numpy as np from sklearn.datasets import load_digits from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split from joblib import Parallel, delayed import time import cProfile import pstats # Load dataset X, y = load_digits(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Function to train KNN classifier and measure accuracy def knn_classifier(X_train, y_train, X_test, y_test, n_neighbors=3): knn = KNeighborsClassifier(n_neighbors=n_neighbors) knn.fit(X_train, y_train) accuracy = knn.score(X_test, y_test) return accuracy # Profiling decorator def profile(func): def wrapper(*args, **kwargs): profiler = cProfile.Profile() profiler.enable() result = func(*args, **kwargs) profiler.disable() ps = pstats.Stats(profiler).sort_stats(pstats.SortKey.TIME) ps.print_stats() return result return wrapper # Wrapping the classifier in the profiler @profile def profile_before_optimization(X_train, y_train, X_test, y_test): return knn_classifier(X_train, y_train, X_test, y_test) # Initial profiling before optimization print(\\"Profiling before optimization:\\") profile_before_optimization(X_train, y_train, X_test, y_test) # Optimized function @profile def profile_after_optimization(X_train, y_train, X_test, y_test): knn = KNeighborsClassifier(n_neighbors=3, algorithm=\'auto\', n_jobs=-1) knn.fit(X_train, y_train) accuracy = knn.score(X_test, y_test) return accuracy print(\\"nProfiling after optimization:\\") profile_after_optimization(X_train, y_train, X_test, y_test)"},{"question":"# Question: Custom Iterator and Async Iterator Implementation **Objective**: Implement a custom synchronous iterator and an asynchronous iterator class in Python to demonstrate your understanding of the iterator protocol. # Task 1: Custom Iterator Implement a class `CustomIterator` that: - Implements the synchronous iteration protocol. - Takes an initial list of values during instantiation. - Iterates through the values, yielding one at a time. - Raises a `StopIteration` exception when all values have been iterated over. # Task 2: Custom Async Iterator Implement a class `CustomAsyncIterator` that: - Implements the asynchronous iteration protocol. - Takes an initial list of values during instantiation. - Iterates through the values asynchronously, yielding one at a time. - Simulates an I/O-bound operation using `asyncio.sleep` for each value. - Raises a `StopAsyncIteration` exception when all values have been iterated over. # Example Usage Below is an example code that demonstrates how both custom iterators should work: Synchronous Iterator ```python iterator = CustomIterator([1, 2, 3, 4, 5]) for value in iterator: print(value) ``` Output: ``` 1 2 3 4 5 ``` Asynchronous Iterator ```python import asyncio async def main(): async for value in CustomAsyncIterator([1, 2, 3, 4, 5]): print(value) asyncio.run(main()) ``` Output: ``` 1 2 3 4 5 ``` # Constraints - Do not use external libraries except for `asyncio`. - The implementation should handle any list of integers. # Requirements - Implement both `CustomIterator` and `CustomAsyncIterator` classes. - Ensure to handle proper exception handling for iteration completion. # Evaluation - Correctly implementing the synchronous iterator protocol. - Correctly implementing the asynchronous iterator protocol. - Properly handling iteration completion with exceptions. - The code readability and efficiency.","solution":"class CustomIterator: def __init__(self, values): self.values = values self.index = 0 def __iter__(self): return self def __next__(self): if self.index >= len(self.values): raise StopIteration value = self.values[self.index] self.index += 1 return value import asyncio class CustomAsyncIterator: def __init__(self, values): self.values = values self.index = 0 def __aiter__(self): return self async def __anext__(self): if self.index >= len(self.values): raise StopAsyncIteration value = self.values[self.index] self.index += 1 await asyncio.sleep(0) # Simulate I/O-bound operation return value"},{"question":"**Advanced Python Coding Assessment: Module Importation Using `importlib`** You are tasked with designing a Python function `custom_import(module_name: str, module_path: str = None) -> object` that dynamically imports a module using the `importlib` package. This function should: 1. First, use the module name to check if it already exists in `sys.modules`. If it does, return the module object. 2. If the module is not already loaded, use `importlib.util.find_spec()` to locate the module. Use: - If `module_path` is given, add it to `sys.path` temporarily to aid in module discovery. 3. Once the module spec is found, use `importlib.util.module_from_spec()` and `spec.loader.exec_module()` to load the module. 4. Ensure that the file is properly closed after loading, even if an exception occurs. 5. Return the imported module object. **Constraints:** - Do not use the deprecated `imp` module. - Handle exceptions such as `ModuleNotFoundError` and other relevant exceptions gracefully. **Example Input/Output:** ```python # Assuming there is a module named `example_module.py` in the same directory. module = custom_import(\'example_module\') print(module) # Should print the module object for \'example_module\' ``` Here is the template you need to complete: ```python import importlib.util import sys def custom_import(module_name: str, module_path: str = None) -> object: # Check if the module is already imported if module_name in sys.modules: return sys.modules[module_name] # If module path is provided, temporarily add it to sys.path if module_path: sys.path.insert(0, module_path) try: # Find the module spec spec = importlib.util.find_spec(module_name) if spec is None: raise ModuleNotFoundError(f\\"Module {module_name} not found\\") # Create a new module object from the spec module = importlib.util.module_from_spec(spec) # Execute the module spec.loader.exec_module(module) # Add module to sys.modules sys.modules[module_name] = module return module except Exception as e: raise ImportError(f\\"An error occurred while importing module {module_name}: {e}\\") finally: # Remove the module_path from sys.path if it was added if module_path: sys.path.pop(0) ``` **Evaluate your implementation by testing it with different modules and paths, ensuring all edge cases are handled properly.**","solution":"import importlib.util import sys def custom_import(module_name: str, module_path: str = None) -> object: # Check if the module is already imported if module_name in sys.modules: return sys.modules[module_name] # If module path is provided, temporarily add it to sys.path if module_path: sys.path.insert(0, module_path) try: # Find the module spec spec = importlib.util.find_spec(module_name) if spec is None: raise ModuleNotFoundError(f\\"Module {module_name} not found\\") # Create a new module object from the spec module = importlib.util.module_from_spec(spec) # Execute the module spec.loader.exec_module(module) # Add module to sys.modules sys.modules[module_name] = module return module except Exception as e: raise ImportError(f\\"An error occurred while importing module {module_name}: {e}\\") finally: # Remove the module_path from sys.path if it was added if module_path and sys.path[0] == module_path: sys.path.pop(0)"},{"question":"# Question: Implementing a Custom Buffer Handling Class in Python **Objective**: To assess your understanding of Python\'s buffer protocol and effective memory management, you are required to implement a custom class that demonstrates reading and writing to buffers. Task 1. Implement a class `BufferHandler` which manages a memory buffer. 2. Your class should support the following functionalities: - Initialize the buffer with a given size. - Fill the buffer with data (a string or bytes). - Read specific sections of the buffer. - Check if the buffer is readable or writable. 3. Make use of Python\'s buffer protocol where appropriate. Specifications 1. **Class**: `BufferHandler` 2. **Methods**: - `__init__(self, size: int)`: Initializes the buffer with the given size. Initialize the buffer to zeros. - `fill_buffer(self, data: Union[str, bytes]) -> None`: Fills the buffer with the given data. If data is a string, encode it to bytes using UTF-8. - `read_buffer(self, start: int, length: int) -> bytes`: Returns a section of the buffer starting from `start` index with the specified `length`. - `is_readable(self) -> bool`: Returns `True` if the buffer is readable. - `is_writable(self) -> bool`: Returns `True` if the buffer is writable. Input and Output Formats - `__init__(self, size: int)` - **Input**: An integer `size`, specifying the size of the buffer. - **Output**: None. - `fill_buffer(self, data: Union[str, bytes]) -> None` - **Input**: A string or bytes object `data` to fill the buffer. - **Output**: None. - `read_buffer(self, start: int, length: int) -> bytes` - **Input**: Two integers `start` and `length` specifying the section of the buffer to read. - **Output**: A bytes object containing the read data. - `is_readable(self) -> bool` - **Input**: None. - **Output**: Boolean `True` if the buffer is readable, else `False`. - `is_writable(self) -> bool` - **Input**: None. - **Output**: Boolean `True` if the buffer is writable, else `False`. Example ```python # Example usage buffer_handler = BufferHandler(10) buffer_handler.fill_buffer(\\"HelloWorld\\") print(buffer_handler.read_buffer(0, 5)) # Output: b\'Hello\' print(buffer_handler.is_readable()) # Output: True print(buffer_handler.is_writable()) # Output: True ``` Constraints - The buffer size (`size`) will always be a positive integer. - The `start` and `length` for `read_buffer` will be such that they form a valid range within the buffer. Notes - You must handle encoding of strings to bytes within the `fill_buffer` method. - Appropriate error handling should be in place for invalid buffer states.","solution":"class BufferHandler: def __init__(self, size: int): Initializes the buffer with the given size. Initialize the buffer to zeros. self.buffer = bytearray(size) self.size = size def fill_buffer(self, data: bytes) -> None: Fills the buffer with the given data. If data is a string, encode it to bytes using UTF-8. if isinstance(data, str): data = data.encode(\'utf-8\') length = min(len(data), self.size) self.buffer[:length] = data[:length] def read_buffer(self, start: int, length: int) -> bytes: Returns a section of the buffer starting from `start` index with the specified `length`. if start < 0 or length < 0 or start + length > self.size: raise IndexError(\'Attempt to read outside buffer boundaries.\') return bytes(self.buffer[start: start + length]) def is_readable(self) -> bool: Returns `True` if the buffer is readable. return True # By design, the buffer is always readable def is_writable(self) -> bool: Returns `True` if the buffer is writable. return True # By design, the buffer is always writable"},{"question":"# Comprehensive Hashing and Key Derivation using Python\'s `hashlib` Module Problem Statement You are working on a secure application that requires hashing and password-based key derivation. You need to implement two functions: 1. **generate_hash(input_data: bytes, algorithm: str, **kwargs) -> str** - This function takes an input byte string, a hashing algorithm name, and optional parameters like `key`, `salt`, `person`, `digest_size`, etc. - It returns the hexadecimal digest of the hashed input data using the specified algorithm and parameters. 2. **derive_key(password: bytes, salt: bytes, algorithm: str, iterations: int, key_length: int) -> str** - This function derives a secure key using password-based key derivation function (PBKDF2 or scrypt) based on the specified algorithm. - It returns the derived key in hexadecimal format. Input Format 1. `generate_hash`: - `input_data`: A bytes object to hash. - `algorithm`: A string specifying the hashing algorithm (e.g., `sha256`, `blake2b`, etc.). - Additional keyword arguments for optional parameters like `key`, `salt`, `person`, `digest_size`. 2. `derive_key`: - `password`: A bytes object representing the password. - `salt`: A bytes object representing the salt for key derivation. - `algorithm`: A string specifying the key derivation algorithm (`pbkdf2_hmac` or `scrypt`). - `iterations`: An integer specifying the number of iterations for key derivation. - `key_length`: An integer specifying the length of the derived key. Output Format - Both functions should return the resulting key or hash as a hexadecimal string. Constraints - For `generate_hash`, the specified algorithm must be available in `hashlib.algorithms_guaranteed`. - For `derive_key`, the specified algorithm must be either `pbkdf2_hmac` or `scrypt`. - The input password and data to hash should be non-empty byte strings. - You can assume valid and appropriate lengths for salts, keys, and other parameters. Example ```python # Example usage of generate_hash data = b\\"Secure data\\" hash_hex = generate_hash(data, \'sha256\') print(hash_hex) # Output: a hexadecimal string of the SHA256 hash # Example usage of derive_key password = b\\"mypassword\\" salt = os.urandom(16) derived_key_hex = derive_key(password, salt, \'pbkdf2_hmac\', 100000, 32) print(derived_key_hex) # Output: a hexadecimal string of the derived key ``` Implement the functions `generate_hash` and `derive_key` as per the specifications.","solution":"import hashlib def generate_hash(input_data: bytes, algorithm: str, **kwargs) -> str: Generates a hash for the given input data using the specified algorithm. Args: - input_data (bytes): The data to be hashed. - algorithm (str): The hashing algorithm to use (e.g., \'sha256\', \'blake2b\', etc.). - kwargs: Additional parameters like key, salt, person, digest_size for blake2* algorithms. Returns: - str: The hexadecimal digest of the hashed input data. if algorithm not in hashlib.algorithms_guaranteed: raise ValueError(f\\"Unsupported algorithm: {algorithm}\\") if algorithm.startswith(\'blake2\'): hash_func = hashlib.new(algorithm, **kwargs) else: hash_func = hashlib.new(algorithm) hash_func.update(input_data) return hash_func.hexdigest() def derive_key(password: bytes, salt: bytes, algorithm: str, iterations: int, key_length: int) -> str: Derives a key using a password-based key derivation function. Args: - password (bytes): The password to derive the key from. - salt (bytes): The salt to use in the key derivation process. - algorithm (str): The key derivation algorithm to use (\'pbkdf2_hmac\' or \'scrypt\'). - iterations (int): The number of iterations to perform for key derivation. - key_length (int): The length of the derived key in bytes. Returns: - str: The derived key in hexadecimal format. if algorithm == \'pbkdf2_hmac\': key = hashlib.pbkdf2_hmac(\'sha256\', password, salt, iterations, dklen=key_length) elif algorithm == \'scrypt\': key = hashlib.scrypt(password, salt=salt, n=2**14, r=8, p=1, maxmem=0, dklen=key_length) else: raise ValueError(f\\"Unsupported key derivation algorithm: {algorithm}\\") return key.hex()"},{"question":"# Advanced XML Processing with SAX in Python You are tasked with implementing a custom SAX handler that processes an XML document to extract its structure and key information. The handler should: 1. Parse an XML document and collect the names of all elements and attributes. 2. Maintain a hierarchy (tree structure) of elements to reflect the document\'s structure. 3. Count the number of occurrences of each element type. 4. Provide functionality to handle errors and warnings that may arise during parsing. To achieve this, you will implement a subclass of `xml.sax.handler.ContentHandler` and optionally `ErrorHandler`. Your implementation should include: 1. A method to print the hierarchy of elements in a readable format. 2. An attribute or method to retrieve the count of each element type. 3. Appropriate error handling to manage and report parsing issues. **Constraints**: - The XML document structure is not known in advance and should be handled generically. - Assume the XML document fits into memory. **Input Format**: - An XML document provided as a string. **Output Format**: - Printed hierarchy of XML elements. - Dictionary mapping each element type to its count. **Example**: Given the following XML input: ```xml <bookstore> <book> <title>Harry Potter</title> <author>J.K. Rowling</author> <year>2005</year> <price>29.99</price> </book> <book> <title>Learning XML</title> <author>Erik T. Ray</author> <year>2003</year> <price>39.95</price> </book> </bookstore> ``` The output should be: ``` Hierarchy: bookstore book title author year price book title author year price Element Counts: {\'bookstore\': 1, \'book\': 2, \'title\': 2, \'author\': 2, \'year\': 2, \'price\': 2} ``` **Implementation Tips**: - Use the `startElement`, `endElement`, and `characters` methods of `ContentHandler` to build the hierarchy and count elements. - Use appropriate exception handling in `ErrorHandler` to manage parsing errors. Here is a template to get you started: ```python import xml.sax class MyContentHandler(xml.sax.handler.ContentHandler, xml.sax.handler.ErrorHandler): def __init__(self): super().__init__() self.hierarchy = [] self.element_counts = {} self.current_path = [] def startElement(self, name, attrs): self.current_path.append(name) self.hierarchy.append(\\" \\" * (len(self.current_path) - 1) + name) self.element_counts[name] = self.element_counts.get(name, 0) + 1 def endElement(self, name): self.current_path.pop() def characters(self, content): pass # Not much to do here for this task def error(self, exception): print(f\\"Error: {exception}\\") def fatalError(self, exception): print(f\\"Fatal Error: {exception}\\") raise exception def warning(self, exception): print(f\\"Warning: {exception}\\") def print_hierarchy(self): print(\\"Hierarchy:\\") for line in self.hierarchy: print(line) def get_element_counts(self): return self.element_counts # Example usage xml_input = <your XML string here> parser = xml.sax.make_parser() handler = MyContentHandler() parser.setContentHandler(handler) parser.setErrorHandler(handler) parser.parse(xml.sax.InputSource(io.StringIO(xml_input))) handler.print_hierarchy() print(\\"Element Counts:\\") print(handler.get_element_counts()) ``` Your task is to complete the implementation of the `MyContentHandler` class so that it correctly processes the provided XML document and meets the requirements specified.","solution":"import xml.sax class MyContentHandler(xml.sax.handler.ContentHandler, xml.sax.handler.ErrorHandler): def __init__(self): super().__init__() self.hierarchy = [] self.element_counts = {} self.current_path = [] self.hierarchy_structure = [] def startElement(self, name, attrs): self.current_path.append(name) self.hierarchy_structure.append(\\" \\" * (len(self.current_path) - 1) + name) self.element_counts[name] = self.element_counts.get(name, 0) + 1 def endElement(self, name): self.current_path.pop() def characters(self, content): pass # Not much to do here for this task def error(self, exception): print(f\\"Error: {exception}\\") def fatalError(self, exception): print(f\\"Fatal Error: {exception}\\") raise exception def warning(self, exception): print(f\\"Warning: {exception}\\") def print_hierarchy(self): print(\\"Hierarchy:\\") for line in self.hierarchy_structure: print(line) def get_element_counts(self): return self.element_counts # Example usage xml_input = <bookstore> <book> <title>Harry Potter</title> <author>J.K. Rowling</author> <year>2005</year> <price>29.99</price> </book> <book> <title>Learning XML</title> <author>Erik T. Ray</author> <year>2003</year> <price>39.95</price> </book> </bookstore> import io def process_xml(xml_string): parser = xml.sax.make_parser() handler = MyContentHandler() parser.setContentHandler(handler) parser.setErrorHandler(handler) parser.parse(io.StringIO(xml_string)) handler.print_hierarchy() print(\\"Element Counts:\\") print(handler.get_element_counts()) process_xml(xml_input)"},{"question":"**Question: Implement and Test a Function that Uses the `ensurepip` Module** Create a Python script that performs the following operations using the `ensurepip` module: 1. **Check the Current Version of `pip`**: - Write a function `get_pip_version()` that returns the version of `pip` that would be installed by `ensurepip` as a string. 2. **Bootstrap `pip`**: - Write a function `install_pip(upgrade=False, user=False, verbosity=0)` which uses the `ensurepip.bootstrap()` function to install `pip`. - The function should accept three optional parameters: - `upgrade` (boolean): If `True`, the function should upgrade an existing `pip` installation. - `user` (boolean): If `True`, the function should install `pip` to the user-specific site packages directory. - `verbosity` (integer): Controls the level of output during the installation process. A value of `0` means no output, and higher values increase the verbosity. 3. **Provide Usage Examples**: - Demonstrate the usage of your functions by: - Displaying the version of `pip` that would be installed. - Installing `pip` with default settings. - Showing how to upgrade `pip` with verbose output. # Constraints: - Do not access the internet in your script. - Do not install `pip` globally if inside an active virtual environment when the `user` parameter is set to `True`. # Example Usage: ```python def main(): version = get_pip_version() print(f\\"Pip version to be installed: {version}\\") install_pip(verbosity=1) print(\\"Pip installation completed.\\") install_pip(upgrade=True, verbosity=2) print(\\"Pip upgrade completed.\\") if __name__ == \\"__main__\\": main() ``` **Expected Output** (The actual output may vary depending on the environment and `pip` version bundled with `ensurepip`): ``` Pip version to be installed: 21.1.1 Installing pip with verbosity level 1... Pip installation completed. Upgrading pip with verbosity level 2... Pip upgrade completed. ``` # Notes: - Ensure your script handles any exceptions and edge cases, such as conflicting options. - Remember to check the active virtual environment status where needed.","solution":"import ensurepip def get_pip_version(): Returns the version of pip that would be installed by ensurepip. versions = ensurepip.version() return versions[1] if versions else \\"Unknown\\" def install_pip(upgrade=False, user=False, verbosity=0): Installs or upgrades pip using ensurepip.bootstrap(). Parameters: upgrade (bool): If True, upgrades pip if it is already installed. user (bool): If True, installs pip to the user-specific site packages directory. verbosity (int): Controls the level of output during the installation process. import os import sys if user and \'VIRTUAL_ENV\' in os.environ: print(\\"Cannot install pip globally inside a virtual environment.\\") return args = [] if upgrade: args.append(\\"--upgrade\\") if user: args.append(\\"--user\\") print(f\\"Installing pip with verbosity level {verbosity}...\\") ensurepip.bootstrap(upgrade=upgrade, user=user, verbosity=verbosity) def main(): version = get_pip_version() print(f\\"Pip version to be installed: {version}\\") install_pip(verbosity=1) print(\\"Pip installation completed.\\") install_pip(upgrade=True, verbosity=2) print(\\"Pip upgrade completed.\\") if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Seaborn Coding Assessment You are provided with a dataset that contains information about the performance of multiple students in various subjects. Your task is to create a comprehensive visualization that showcases the overall performance and distribution of scores using seaborn, particularly focusing on diverging color palettes. Dataset The dataset, `students_scores.csv`, has the following columns: - `Student`: Name of the student - `Math`: Score in Mathematics (0-100) - `Science`: Score in Science (0-100) - `English`: Score in English (0-100) - `History`: Score in History (0-100) Requirements 1. **Heatmap**: Create a heatmap to visualize the scores of all students across the different subjects using a diverging color palette. 2. **Customization**: Customize the heatmap to: - Use a diverging palette that transitions from blue to red through white. - Change the center color to dark. - Set a higher separation around the center value. 3. **Annotations**: Annotate the heatmap with the exact score values for each student and subject. 4. **Distribution**: Create separate histograms for each subject showing the distribution of scores using the same diverging palette. Input - `students_scores.csv` Output - A heatmap visualizing the students\' scores. - Four histograms showing the score distributions for Math, Science, English, and History. Constraints - Use only seaborn (for visualization) and pandas (for data handling) libraries. - Ensure the visualizations are well-labeled, with titles, axis labels, and a color bar for the heatmap. - Your code should be efficient and handle any number of students with appropriate scaling. Function Signature ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_student_performance(file_path: str) -> None: # Your code here pass ``` Example For a `students_scores.csv` file, the function call ```python visualize_student_performance(\'students_scores.csv\') ``` should generate and display the required heatmap and histograms. _NOTE: Ensure your visualizations are clear and informative, with all required customizations applied._","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_student_performance(file_path: str) -> None: # Load the dataset df = pd.read_csv(file_path) # Set the style of seaborn sns.set(style=\\"whitegrid\\") # Create heatmap plt.figure(figsize=(10, 8)) heatmap = sns.heatmap(df.set_index(\'Student\'), cmap=\'coolwarm\', annot=True, center=50, cbar_kws={\'label\': \'Score\'}) heatmap.set_title(\'Students Performance Heatmap\') plt.yticks(rotation=0) plt.xticks(rotation=45) plt.show() # Create histograms subjects = [\'Math\', \'Science\', \'English\', \'History\'] for subject in subjects: plt.figure(figsize=(6, 4)) sns.histplot(df[subject], kde=False, color=\'royalblue\') plt.title(f\'{subject} Score Distribution\') plt.xlabel(\'Score\') plt.ylabel(\'Frequency\') plt.show()"},{"question":"**Objective**: You are tasked with implementing a class in CPython that can be called using both *tp_call* and vectorcall protocols. The class should take different sets of positional and keyword arguments through these protocols and handle them appropriately. **Problem Statement**: 1. Create a class `DualCallable` that supports being callable using both *tp_call* and vectorcall protocols. 2. Implement methods to handle various combinations of positional and keyword arguments. 3. Ensure that the class behaves consistently irrespective of the call protocol used. **Requirements**: 1. **tp_call Protocol**: Implement the `tp_call` method to allow the object to be called with tuples for positional arguments and dictionaries for keyword arguments. 2. **Vectorcall Protocol**: Implement vectorcall support, ensuring that the class also supports the `tp_call` protocol as required by the documentation. 3. **Consistency**: Ensure that calling the object yields the same result regardless of the protocol used. **Considerations**: - You must handle different numbers of positional and keyword arguments. - Use appropriate CPython functions to perform the calls and manage arguments. **Input Format**: - The class will be instantiated and called using a variety of positional and keyword arguments. - Example: `obj(*args, **kwargs)` or via vectorcall. **Output Format**: - Print or return the processed result based on the input arguments. **Constraints**: - You must use CPython API functions appropriately as described in the documentation. - Ensure the implementation is efficient and follows best practices for both protocols. **Example Usage**: ```python # Example instantiation and calls obj = DualCallable() # Using tp_call protocol result1 = obj(1, 2, a=3, b=4) # Using vectorcall protocol result2 = obj.vectorcall((1, 2), \\"a\\", \\"b\\") # Both result1 and result2 should be the same assert result1 == result2 ``` **Performance Requirements**: - Ensure minimal overhead in handling calls, particularly when using the vectorcall protocol, which is meant to be more efficient. **Implementation Tips**: - Remember to initialize necessary fields and flags in your class to support the required protocols. - Carefully manage memory and references to avoid leaks or crashes.","solution":"class DualCallable: def __init__(self): pass def __call__(self, *args, **kwargs): return self.process_arguments(args, kwargs) def vectorcall(self, items, *args): # In a real CPython extension, this would use the vectorcall protocol. args = tuple(items) + tuple(args) kwargs = {} # No keywords supported in this simplified example. return self.process_arguments(args, kwargs) def process_arguments(self, args, kwargs): return {\\"args\\": args, \\"kwargs\\": kwargs}"},{"question":"# Objective Create a Python function using seaborn that generates and returns color palettes based on user specifications. # Problem Statement Write a function called `generate_color_palettes` that takes in three parameters: - `colormap_name` (str): The name of the colormap to use (e.g., `\\"viridis\\"`, `\\"Set2\\"`). - `num_colors` (int): The number of colors to include in the palette. Optional; default is `6`. - `as_continuous` (bool): Whether to return the continuous colormap. Optional; default is `False`. The function should return the generated color palette. # Expected Function Signature ```python def generate_color_palettes(colormap_name: str, num_colors: int = 6, as_continuous: bool = False) -> list: pass ``` # Input and Output Formats - Input: - `colormap_name`: A string representing the name of the colormap. - `num_colors`: An integer representing the number of colors to be included in the palette. - `as_continuous`: A boolean indicating whether to return the continuous version of the colormap. - Output: - A list of colors representing the generated color palette. If `as_continuous` is `True`, the list contains the continuous colormap. # Constraints - `num_colors` should be a positive integer (1  num_colors  256). - `colormap_name` should be a valid matplotlib colormap name. - `as_continuous` should be a boolean. # Performance Requirements The function should be optimized for performance and able to handle calls with the maximum constraints in a reasonable time. # Examples ```python # Example 1 print(generate_color_palettes(\\"viridis\\")) # Expected output: A list of 6 colors from the \\"viridis\\" colormap. # Example 2 print(generate_color_palettes(\\"viridis\\", 8)) # Expected output: A list of 8 colors from the \\"viridis\\" colormap. # Example 3 print(generate_color_palettes(\\"viridis\\", as_continuous=True)) # Expected output: The continuous \\"viridis\\" colormap. # Example 4 print(generate_color_palettes(\\"Set2\\")) # Expected output: A list of 6 distinct colors from the \\"Set2\\" qualitative colormap. # Example 5 print(generate_color_palettes(\\"Set2\\", 10)) # Expected output: A list of colors from the \\"Set2\\" qualitative colormap. # Note: The number of colors can be less than requested if the colormap doesn\'t support as many distinct colors. ``` # Note - Use seaborn\'s `sns.mpl_palette` function to achieve the color palette generation.","solution":"import seaborn as sns def generate_color_palettes(colormap_name: str, num_colors: int = 6, as_continuous: bool = False) -> list: Generates and returns a list of colors based on the specified colormap. Parameters: colormap_name (str): The name of the colormap to use. num_colors (int): The number of colors to include in the palette. Default is 6. as_continuous (bool): Whether to return the continuous colormap. Default is False. Returns: list: A list of colors representing the generated color palette. if as_continuous: return sns.color_palette(colormap_name, as_cmap=True) else: return sns.color_palette(colormap_name, num_colors)"},{"question":"**Question: Creating a Note-Taking Application Using `tkinter.scrolledtext`** You are tasked with creating a simple note-taking application using Python\'s Tkinter library. Specifically, you need to use the `ScrolledText` class from the `tkinter.scrolledtext` module. The application should have the following features: 1. A main window with a `ScrolledText` widget for taking notes. 2. A \\"Save\\" button that allows the user to save the contents of the `ScrolledText` widget to a text file. 3. A \\"Clear\\" button that clears the contents of the `ScrolledText` widget. # Function: create_note_taking_app() Implement the `create_note_taking_app` function, which creates and runs the note-taking application with the features described. Expected Input and Output The function should not take any input or return any output. Instead, it should create and display a Tkinter window with the specified widgets and functionalities. Requirements and Constraints 1. The `ScrolledText` widget should be large enough to display multiple lines of text comfortably. 2. The \\"Save\\" button should prompt the user to choose a file location to save the notes using a file dialog. 3. The \\"Clear\\" button should clear all text in the `ScrolledText` widget without any additional confirmation. 4. Use appropriate layout management to ensure the widgets are arranged neatly. 5. Handle any exceptions that might arise during file operations gracefully. Sample Code While no specific input or output is required for this function, here is a sample template to get you started: ```python import tkinter as tk from tkinter import filedialog, messagebox from tkinter.scrolledtext import ScrolledText def create_note_taking_app(): root = tk.Tk() root.title(\\"Note-Taking App\\") scrolled_text = ScrolledText(root, wrap=tk.WORD, width=60, height=20) scrolled_text.pack(padx=10, pady=10) def save_notes(): try: file_path = filedialog.asksaveasfilename(defaultextension=\\".txt\\", filetypes=[(\\"Text files\\", \\"*.txt\\"), (\\"All files\\", \\"*.*\\")]) if file_path: with open(file_path, \\"w\\") as file: file.write(scrolled_text.get(\\"1.0\\", tk.END)) messagebox.showinfo(\\"Success\\", \\"Notes saved successfully!\\") except Exception as e: messagebox.showerror(\\"Error\\", f\\"An error occurred while saving notes: {e}\\") def clear_notes(): scrolled_text.delete(\\"1.0\\", tk.END) save_button = tk.Button(root, text=\\"Save\\", command=save_notes) save_button.pack(side=tk.LEFT, padx=10, pady=10) clear_button = tk.Button(root, text=\\"Clear\\", command=clear_notes) clear_button.pack(side=tk.RIGHT, padx=10, pady=10) root.mainloop() # Uncomment the line below to test the function. # create_note_taking_app() ``` Use the provided template to implement the `create_note_taking_app` function with the described features. Make sure to test your application to ensure all functionalities work correctly.","solution":"import tkinter as tk from tkinter import filedialog, messagebox from tkinter.scrolledtext import ScrolledText def create_note_taking_app(): root = tk.Tk() root.title(\\"Note-Taking App\\") scrolled_text = ScrolledText(root, wrap=tk.WORD, width=60, height=20) scrolled_text.pack(padx=10, pady=10) def save_notes(): try: file_path = filedialog.asksaveasfilename(defaultextension=\\".txt\\", filetypes=[(\\"Text files\\", \\"*.txt\\"), (\\"All files\\", \\"*.*\\")]) if file_path: with open(file_path, \\"w\\") as file: file.write(scrolled_text.get(\\"1.0\\", tk.END)) messagebox.showinfo(\\"Success\\", \\"Notes saved successfully!\\") except Exception as e: messagebox.showerror(\\"Error\\", f\\"An error occurred while saving notes: {e}\\") def clear_notes(): scrolled_text.delete(\\"1.0\\", tk.END) save_button = tk.Button(root, text=\\"Save\\", command=save_notes) save_button.pack(side=tk.LEFT, padx=10, pady=10) clear_button = tk.Button(root, text=\\"Clear\\", command=clear_notes) clear_button.pack(side=tk.RIGHT, padx=10, pady=10) root.mainloop() # Uncomment the line below to test the function. # create_note_taking_app()"},{"question":"# Question: Implementing and Manipulating a Stream of Data with Python Iterators and Generators **Problem Statement:** You are provided with a list of integers representing data points collected from a series of sensors. Your task is to implement a set of functions using Python iterators and generators to process this data stream. The functions you need to implement are: 1. `data_generator(data)`: A generator function that yields one integer at a time from the provided list `data`. 2. `filter_positive(generator)`: A generator function that takes an input generator and yields only the positive integers from it. 3. `moving_average(generator, n)`: A generator function that takes an input generator and an integer `n`, and yields the moving average of the last `n` integers from the input generator. 4. `accumulate(generator, func)`: A function that takes an input generator and a binary function `func`, and returns an iterator that yields cumulative results using `func`. **Input and Output Formats:** - Function `data_generator(data)`: - **Input:** A list of integers `data`. - **Output:** Yields one integer at a time from `data`. - Function `filter_positive(generator)`: - **Input:** A generator that yields integers. - **Output:** Yields only the positive integers from the input generator. - Function `moving_average(generator, n)`: - **Input:** - A generator that yields integers. - An integer `n` specifying the number of last integers to compute the moving average. - **Output:** Yields the moving average of the last `n` integers from the input generator. - Function `accumulate(generator, func)`: - **Input:** - A generator that yields integers. - A binary function `func` that takes two arguments. - **Output:** Returns an iterator that yields cumulative results using `func`. **Constraints:** - Your solution should strictly follow functional programming paradigms. - Avoid using mutable global state or side effects outside the scope of the functions. - Assume the input list `data` contains at least one integer and `n` is a positive integer. **Example:** ```python def data_generator(data): for value in data: yield value def filter_positive(generator): for value in generator: if value > 0: yield value def moving_average(generator, n): window = [] for value in generator: window.append(value) if len(window) > n: window.pop(0) yield sum(window) / len(window) def accumulate(generator, func): it = iter(generator) total = next(it) yield total for value in it: total = func(total, value) yield total # Example usage data = [10, -5, 20, -3, 15, 5] gen = data_generator(data) filtered_gen = filter_positive(gen) mov_avg_gen = moving_average(filtered_gen, 3) accumulated_gen = accumulate(mov_avg_gen, lambda x, y: x + y) print(list(mov_avg_gen)) # Output should be the moving average of positive integers print(list(accumulated_gen)) # Output should be the cumulative sum of the moving averages ``` **Ensure you handle edge cases such as empty input and manage state effectively within the generator functions.**","solution":"def data_generator(data): A generator function that yields one integer at a time from the provided list `data`. for value in data: yield value def filter_positive(generator): A generator function that takes an input generator and yields only the positive integers from it. for value in generator: if value > 0: yield value def moving_average(generator, n): A generator function that takes an input generator and an integer `n`, and yields the moving average of the last `n` integers from the input generator. window = [] for value in generator: window.append(value) if len(window) > n: window.pop(0) yield sum(window) / len(window) def accumulate(generator, func): A function that takes an input generator and a binary function `func`, and returns an iterator that yields cumulative results using `func`. it = iter(generator) total = next(it) yield total for value in it: total = func(total, value) yield total"},{"question":"Problem Statement You are required to implement a function `process_data` that processes a list of dictionaries, applies transformations to the data, and handles potential errors gracefully using exception handling. Function Signature ```python def process_data(data: list, transformations: dict) -> list: pass ``` Input: 1. `data`: A list of dictionaries, with each dictionary representing a record. Each record may have fields like \'name\', \'age\', and \'salary\'. 2. `transformations`: A dictionary where keys are the field names, and values are functions to be applied to the corresponding fields of each record. Output: - The function should return a list of dictionaries where the specified transformations have been applied. - If a transformation function raises an exception, the function should log the error and skip the transformation for that specific field in that specific record, leaving its value unchanged. Constraints: - For simplicity, assume that all records have the same structure. - The data list can be empty, in which case the function should return an empty list. Examples: **Example 1:** ```python data = [ {\\"name\\": \\"Alice\\", \\"age\\": 30, \\"salary\\": 70000}, {\\"name\\": \\"Bob\\", \\"age\\": \\"unknown\\", \\"salary\\": 50000}, {\\"name\\": \\"Charlie\\", \\"age\\": 25, \\"salary\\": \\"not disclosed\\"} ] transformations = { \\"age\\": int, \\"salary\\": lambda x: x * 1.05 if isinstance(x, (int, float)) else x } result = process_data(data, transformations) print(result) # Output: # [ # {\\"name\\": \\"Alice\\", \\"age\\": 30, \\"salary\\": 73500.0}, # {\\"name\\": \\"Bob\\", \\"age\\": \\"unknown\\", \\"salary\\": 52500.0}, # {\\"name\\": \\"Charlie\\", \\"age\\": 25, \\"salary\\": \\"not disclosed\\"} # ] ``` **Example 2:** ```python data = [] transformations = { \\"age\\": int, \\"salary\\": lambda x: x * 1.05 if isinstance(x, (int, float)) else x } result = process_data(data, transformations) print(result) # Output: # [] ``` Notes: - You should use appropriate exception handling to manage errors that occur during the application of transformation functions. - Log errors in a way that allows them to be reviewed but doesn\'t disrupt the processing of other records or fields. Performance Requirements: - The function should efficiently process large lists of data, meaning it should handle a list with at least 10,000 entries in a reasonable time frame.","solution":"def process_data(data: list, transformations: dict) -> list: processed_data = [] for record in data: new_record = record.copy() for field, transform in transformations.items(): if field in new_record: try: new_record[field] = transform(new_record[field]) except Exception as e: # Log errors but continue processing print(f\\"Error processing field \'{field}\' with value \'{new_record[field]}\': {e}\\") processed_data.append(new_record) return processed_data"},{"question":"Objective: You are required to implement a function that maintains a list of student records in sorted order based on their scores. Additionally, you will implement a function to query the list for the highest-scoring student below a certain threshold. Instructions: 1. **Implement the function `add_student_record(records, student)`**: - **Input**: - `records`: A list of tuples, where each tuple contains (name: str, score: float). The list is initially empty and should be maintained in sorted order based on the `score`. - `student`: A tuple containing (name: str, score: float). - **Output**: - The function does not return anything. It should modify the `records` list in place, maintaining it in sorted order by `score`. 2. **Implement the function `highest_score_below(records, threshold)`**: - **Input**: - `records`: A list of tuples, where each tuple contains (name: str, score: float). This list is maintained in sorted order. - `threshold`: A float representing the score threshold. - **Output**: - Returns the student (name: str, score: float) with the highest score below the given `threshold`. - If no such record is found, raise a `ValueError`. Note: Use functions from the `bisect` module to ensure the efficiency of your implementation. Constraints: - The scores are unique. - You may assume the maximum size of the `records` list will be (10^5). - The names of the students are unique. Example: ```python records = [] add_student_record(records, (\\"Alice\\", 85)) add_student_record(records, (\\"Bob\\", 90)) add_student_record(records, (\\"Charlie\\", 75)) print(records) # Output: [(\'Charlie\', 75), (\'Alice\', 85), (\'Bob\', 90)] result = highest_score_below(records, 85) print(result) # Output: (\'Charlie\', 75) result = highest_score_below(records, 70) # Raises ValueError ``` Implement the functions as specified, ensuring the `records` list is efficiently maintained and queried using the provided bisecting methods.","solution":"import bisect def add_student_record(records, student): Adds a student record to the records list, maintaining it in sorted order based on the score. Parameters: records (list): List of tuples containing (name: str, score: float). student (tuple): A tuple containing (name: str, score: float). Returns: None # Extract the score from the student record score = student[1] # Find the position where the student record should be inserted position = bisect.bisect_left([record[1] for record in records], score) # Insert the student record into the sorted position records.insert(position, student) def highest_score_below(records, threshold): Returns the student with the highest score below a given threshold. Parameters: records (list): List of tuples containing (name: str, score: float). threshold (float): The score threshold. Returns: tuple: The student (name, score) with the highest score below the threshold. Raises: ValueError: If no student has a score below the threshold. # Find the position where the threshold would be inserted to keep the list sorted position = bisect.bisect_left([record[1] for record in records], threshold) # Check if there is a valid student record below the threshold if position == 0: raise ValueError(\\"No student has a score below the given threshold.\\") return records[position - 1]"},{"question":"# PyTorch Coding Assessment Question You are tasked with implementing a custom PyTorch function using the Core Aten IR. Specifically, you need to create a function that performs matrix multiplication followed by an element-wise addition using the Core Aten operators. Implement the function `custom_matrix_op` such that it takes two tensors as input, performs matrix multiplication of these tensors, and then adds a third tensor element-wise to the result of the multiplication. ```python def custom_matrix_op(tensor1: torch.Tensor, tensor2: torch.Tensor, tensor3: torch.Tensor) -> torch.Tensor: Performs matrix multiplication of tensor1 and tensor2, followed by element-wise addition of tensor3. Parameters: tensor1 (torch.Tensor): A 2-D tensor of shape (m, n). tensor2 (torch.Tensor): A 2-D tensor of shape (n, p). tensor3 (torch.Tensor): A 2-D tensor of shape (m, p). Returns: torch.Tensor: A 2-D tensor of shape (m, p) resulting from (tensor1 @ tensor2) + tensor3. # Your implementation goes here ``` # Constraints: 1. Do not use any `torch.matmul` or `torch.add` functions directly. Instead, use the Core Aten IR operators. 2. Ensure that the tensors provided as input conform to the required shapes for matrix multiplication. 3. Use only the Core Aten IR operations for matrix multiplication and element-wise addition. # Example: ```python import torch tensor1 = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32) tensor2 = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32) tensor3 = torch.tensor([[1, 1], [1, 1]], dtype=torch.float32) result = custom_matrix_op(tensor1, tensor2, tensor3) print(result) ``` **Expected Output:** ``` tensor([[20., 23.], [44., 51.]]) ``` # Hints: - Review the Core Aten IR operators in the documentation to determine which operators can be used to perform matrix multiplication and element-wise addition. - Focus on the Core Aten operator\'s functions listed in `native_functions.yaml`. Good luck!","solution":"import torch def custom_matrix_op(tensor1: torch.Tensor, tensor2: torch.Tensor, tensor3: torch.Tensor) -> torch.Tensor: Performs matrix multiplication of tensor1 and tensor2, followed by element-wise addition of tensor3. Parameters: tensor1 (torch.Tensor): A 2-D tensor of shape (m, n). tensor2 (torch.Tensor): A 2-D tensor of shape (n, p). tensor3 (torch.Tensor): A 2-D tensor of shape (m, p). Returns: torch.Tensor: A 2-D tensor of shape (m, p) resulting from (tensor1 @ tensor2) + tensor3. # Perform matrix multiplication matmul_result = torch.mm(tensor1, tensor2) # Perform element-wise addition result = torch.add(matmul_result, tensor3) return result"},{"question":"Question You are tasked with writing a Python script that consolidates and presents comprehensive information about installed packages and their metadata using the `importlib.metadata` module. Specifically, you\'ll write a function that, given the name of a package, retrieves and prints the following: 1. **Package Version**: The version of the specified package. 2. **Entry Points**: List of all entry points grouped by their type. 3. **Metadata**: A dictionary of metadata fields and values for the specified package. 4. **Files**: A list of all files included in the package, along with their sizes. 5. **Requirements**: A list of package dependencies. # Function Signature ```python def get_package_info(package_name: str) -> None: pass ``` # Input - `package_name` (str): The name of the package for which the information should be retrieved. # Output - The function should print the following information in a clear and organized manner: - **Version**: The version string of the package. - **Entry Points**: A dictionary where keys are group names (e.g. \'console_scripts\') and values are lists of entry points under each group. - **Metadata**: A dictionary of metadata fields and their corresponding values. - **Files**: A list of dictionaries with each dictionary containing the filename, size, and hash. - **Requirements**: A list of strings representing package dependencies. # Constraints - Assume the package provided exists and is correctly installed via pip. - Handle cases where certain metadata might not be available. # Example ```python >>> get_package_info(\'wheel\') Version: 0.32.3 Entry Points: { \'console_scripts\': [\'wheel\'], \'distutils.commands\': [...], ... } Metadata: { \'Metadata-Version\': \'2.1\', \'Name\': \'wheel\', \'Version\': \'0.32.3\', ... } Files: [ {\'filename\': \'wheel/util.py\', \'size\': 859, \'hash\': \'bYkw5oMccfazVCoYQwKkkemoVyMAFoR34mmKBx8R1NI\'}, ... ] Requirements: [\'pytest (>=3.0.0) ; extra == \\"test\\"\', \'pytest-cov ; extra == \\"test\\"\'] ``` Note: Your implementation needs to handle and correctly display all the required information, consolidating data from the different functionality provided by the `importlib.metadata` module. Make sure to structure your output for readability and completeness.","solution":"from importlib.metadata import distribution, PackageNotFoundError def get_package_info(package_name: str) -> None: try: dist = distribution(package_name) # Package Version print(f\\"Version: {dist.version}\\") # Entry Points entry_points = dist.entry_points entry_points_dict = {} for entry_point in entry_points: if entry_point.group not in entry_points_dict: entry_points_dict[entry_point.group] = [] entry_points_dict[entry_point.group].append(entry_point.name) print(\\"Entry Points:\\") print(entry_points_dict) # Metadata metadata_dict = dict(dist.metadata) print(\\"Metadata:\\") print(metadata_dict) # Files files = dist.files files_list = [] if files is not None: for file in files: file_info = { \'filename\': str(file), \'size\': file.size, } files_list.append(file_info) print(\\"Files:\\") print(files_list) # Requirements requirements = dist.requires or [] print(\\"Requirements:\\") print(requirements) except PackageNotFoundError: print(\\"Package not found. Please make sure the package is installed.\\")"},{"question":"# Iterator Implementation Challenge You are tasked with implementing two custom iterator classes in Python, `MySeqIter` and `MyCallIter`, that mimic the behavior of the iterators described in the provided documentation. 1. MySeqIter Class This class should iterate over any sequence that supports the `__getitem__()` method. It should stop iteration when an `IndexError` is raised. # Requirements: - Implement the class `MySeqIter` with the following methods: ```python class MySeqIter: def __init__(self, seq): # Initialize with a sequence def __iter__(self): # Return the iterator object (self) def __next__(self): # Return the next item in the sequence or raise StopIteration ``` 2. MyCallIter Class This class should iterate using a callable object and a sentinel value. The iteration continues by calling the callable object, and stops when the callable returns the sentinel value. # Requirements: - Implement the class `MyCallIter` with the following methods: ```python class MyCallIter: def __init__(self, callable, sentinel): # Initialize with a callable object and a sentinel value def __iter__(self): # Return the iterator object (self) def __next__(self): # Call the callable object and return the result or raise StopIteration if it\'s equal to sentinel ``` # Examples and Testing: 1. **MySeqIter Example**: ```python seq = [1, 2, 3] iterator = MySeqIter(seq) for item in iterator: print(item) # Output: 1, 2, 3 ``` 2. **MyCallIter Example**: ```python def number_generator(): num = 0 while True: yield num num += 1 gen = number_generator() callable_gen = lambda: next(gen) iterator = MyCallIter(callable_gen, 5) for item in iterator: print(item) # Output: 0, 1, 2, 3, 4 ``` # Constraints: - The `__getitem__()` method of the sequence provided to `MySeqIter` should handle both index-based and `IndexError` scenarios. - The callable object provided to `MyCallIter` should not take any parameters and should consistently return the sentinel value to stop the iteration. # Evaluation: - Correctly implementing the iterator protocol. - Accurately handling stop conditions for both types of iterators. - Ensuring proper usage of classes and methods adhering to Python principles.","solution":"class MySeqIter: def __init__(self, seq): self.seq = seq self.index = 0 def __iter__(self): return self def __next__(self): try: result = self.seq[self.index] except IndexError: raise StopIteration self.index += 1 return result class MyCallIter: def __init__(self, callable, sentinel): self.callable = callable self.sentinel = sentinel def __iter__(self): return self def __next__(self): result = self.callable() if result == self.sentinel: raise StopIteration return result"},{"question":"**Objective**: You are tasked with creating a utility to construct an email message with mixed content from scratch using the `email.mime` module in Python. This task will assess your understanding of the email MIME message construction in Python. **Problem Statement**: Write a function `create_mixed_email(subject, sender, recipient, text_content, image_data, attachment_data)` that constructs a MIME email message containing text, an image, and an application attachment. Your function should: 1. Create a `MIMEMultipart` email message with the subtype \\"mixed\\". 2. Add the email\'s subject, sender, and recipient headers. 3. Add a text part to the email using `MIMEText`. 4. Add an image part to the email using `MIMEImage`. Ensure the image part is encoded in base64. 5. Add an application attachment part to the email using `MIMEApplication`. Ensure the attachment part is encoded in base64. 6. Return the MIME email message as a string. **Function Signature**: ```python def create_mixed_email(subject: str, sender: str, recipient: str, text_content: str, image_data: bytes, attachment_data: bytes) -> str: pass ``` **Input**: - `subject` (str): The subject of the email. - `sender` (str): The senders email address. - `recipient` (str): The recipients email address. - `text_content` (str): The text content of the email. - `image_data` (bytes): The raw byte data of the image. - `attachment_data` (bytes): The raw byte data of the attachment. **Output**: - A string representation of the MIME email message. **Example**: ```python subject = \\"Test Email\\" sender = \\"sender@example.com\\" recipient = \\"recipient@example.com\\" text_content = \\"This is a test email with text, an image, and an attachment.\\" image_data = b\\"...\\" # Raw image byte data attachment_data = b\\"...\\" # Raw attachment byte data email_message = create_mixed_email(subject, sender, recipient, text_content, image_data, attachment_data) print(email_message) ``` **Constraints**: - The input data for the image and attachment are provided as bytes. Ensure that all parts are appropriately encoded using base64. **Hints**: - Use the `MIMEMultipart` class with subtype \\"mixed\\" for creating the main email structure. - Use the `MIMEText` class for the text part of the email. - Use the `MIMEImage` class for the image part, and specify encoding using `email.encoders.encode_base64`. - Use the `MIMEApplication` class for the application attachment, and specify encoding using `email.encoders.encode_base64`.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.application import MIMEApplication from email.encoders import encode_base64 from email import utils def create_mixed_email(subject: str, sender: str, recipient: str, text_content: str, image_data: bytes, attachment_data: bytes) -> str: msg = MIMEMultipart(\'mixed\') msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = recipient msg[\'Date\'] = utils.formatdate(localtime=True) msg[\'Message-ID\'] = utils.make_msgid() # Add text part text_part = MIMEText(text_content, \'plain\') msg.attach(text_part) # Add image part image_part = MIMEImage(image_data) encode_base64(image_part) image_part.add_header(\'Content-Disposition\', \'attachment\', filename=\'image.jpg\') msg.attach(image_part) # Add attachment part attachment_part = MIMEApplication(attachment_data) encode_base64(attachment_part) attachment_part.add_header(\'Content-Disposition\', \'attachment\', filename=\'attachment.pdf\') msg.attach(attachment_part) return msg.as_string()"},{"question":"<|Analysis Begin|> The documentation provided explains the nuances and limitations of floating-point arithmetic in Python. It elaborates on how floating-point numbers are represented in binary, the inherent imprecision, and common pitfalls when using these numbers in calculations. It also provides insights into Python\'s `decimal` and `fractions` modules for cases where exact arithmetic is required. Given this context, we can design a question focused on understanding and handling floating-point precision issues, as well as utilizing the `decimal` and `fractions` modules to achieve more accurate results. The question should challenge students to implement a function that performs floating-point arithmetic accurately using these modules. They should demonstrate their comprehension by dealing with representation errors and ensuring their results are as precise as possible. <|Analysis End|> <|Question Begin|> # Problem Statement You are tasked with implementing a function that requires precise arithmetic operations. Due to the limitations of floating-point arithmetic in binary representation, direct calculations could lead to incorrect results. To mitigate these inaccuracies, you need to leverage Python\'s `decimal` and `fractions` modules. # Function Signature ```python def precise_arithmetic(operations: List[str], values: List[float]) -> Tuple[float, float]: ``` # Input - `operations`: A list of strings where each string is either \\"add\\", \\"subtract\\", \\"multiply\\", or \\"divide\\". - `values`: A list of floating-point numbers on which the operations will be performed in sequence. # Output - A tuple of two floating-point numbers. - The first number is the result of performing the operations using standard floating-point arithmetic. - The second number is the result of performing the operations using Python\'s `decimal` and `fractions` modules to ensure higher precision. # Constraints - The length of `operations` and `values` is the same. - The length of `operations` is between 1 and 1000. - Values in `values` are non-zero numbers between `-1.0e6` and `1.0e6`. - You should handle division by zero gracefully by raising an appropriate error. # Example ```python from typing import List, Tuple from decimal import Decimal from fractions import Fraction def precise_arithmetic(operations: List[str], values: List[float]) -> Tuple[float, float]: # Implement your solution here pass # Test case 1 operations = [\\"add\\", \\"add\\", \\"multiply\\", \\"subtract\\"] values = [0.1, 0.2, 10, 2] print(precise_arithmetic(operations, values)) # Expected Output: (x, y) # where x is the result using standard floats and y is the result using Decimal/Fraction for higher precision ``` # Considerations - Pay attention to accuracy and precision when implementing the solution. - Ensure the second result in the tuple uses the `decimal` or `fractions` module to avoid floating-point representation errors. - Properly handle cases where division by zero might occur. # Tips - Use the `Decimal` module for arithmetic operations requiring decimal representation. - Use the `Fraction` module for fractional arithmetic and to handle division operations accurately.","solution":"from typing import List, Tuple from decimal import Decimal, getcontext from fractions import Fraction def precise_arithmetic(operations: List[str], values: List[float]) -> Tuple[float, float]: if len(operations) != len(values): raise ValueError(\\"The length of operations and values must be the same.\\") # Setting an arbitrary precision for decimal operations for demonstration. getcontext().prec = 50 # Perform standard floating-point arithmetic float_result = 0.0 for op, value in zip(operations, values): if op == \\"add\\": float_result += value elif op == \\"subtract\\": float_result -= value elif op == \\"multiply\\": float_result *= value elif op == \\"divide\\": float_result /= value else: raise ValueError(\\"Unsupported operation\\") # Use Decimal and Fraction to perform precise arithmetic decimal_result = Decimal(0) fraction_result = Fraction(0) for op, value in zip(operations, values): decimal_value = Decimal(str(value)) fraction_value = Fraction(value) if op == \\"add\\": decimal_result += decimal_value fraction_result += fraction_value elif op == \\"subtract\\": decimal_result -= decimal_value fraction_result -= fraction_value elif op == \\"multiply\\": decimal_result *= decimal_value fraction_result *= fraction_value elif op == \\"divide\\": if value == 0: raise ZeroDivisionError(\\"division by zero\\") decimal_result /= decimal_value fraction_result /= fraction_value else: raise ValueError(\\"Unsupported operation\\") precise_result = float(decimal_result) return (float_result, precise_result)"},{"question":"**Question: Exploring Tipping Data with Seaborn** You are given the \\"tips\\" dataset from seaborn, which contains information about restaurant tips. Your task is to create a comprehensive plot that visualizes the relationship between total bill, day of the week, time of day, and gender. Use seaborn\'s `stripplot` and `catplot` functions to achieve this. # Requirements: 1. **Stripplot**: - Create a stripplot to visualize the distribution of total bill amounts for each day of the week. - Use the `hue` parameter to differentiate data by gender. - Set `dodge` to `True` so that the points for each gender do not overlap. - Disable jitter by setting `jitter` to `False`. 2. **Catplot**: - Utilize `catplot` to create a faceted plot that shows the total bill amounts by time of day for each day of the week. - Use the `col` parameter to create different plots for each day. - Differentiate data points by gender using the `hue` parameter. - Set the aspect ratio to `.5` for the faceted plots. # Expected Input and Output: - There are no specific input parameters required since you will use the seaborn `tips` dataset available via `sns.load_dataset(\\"tips\\")`. - The output will be two seaborn plots displayed: a stripplot and a faceted catplot. # Constraints and Limitations: - You must use seaborn version 0.12 or later. - Ensure that your solution is self-contained and imports all necessary libraries. - Use appropriate plot customizations to make the graphs clear and informative. # Code Template: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create a stripplot plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", dodge=True, jitter=False) plt.title(\'Distribution of Total Bill by Day and Gender\') plt.show() # Create a faceted catplot sns.catplot(data=tips, x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=.5) plt.suptitle(\'Total Bill by Time of Day and Gender for Each Day\', y=1.05) # Adjust y for title position plt.show() ``` Ensure that your code adheres to best practices and is well-documented.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create a stripplot plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", dodge=True, jitter=False) plt.title(\'Distribution of Total Bill by Day and Gender\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Day of the Week\') plt.legend(title=\'Gender\') plt.show() # Create a faceted catplot g = sns.catplot(data=tips, x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=.5) g.fig.subplots_adjust(top=0.9) # Adjust top for title space g.fig.suptitle(\'Total Bill by Time of Day and Gender for Each Day\') plt.show()"},{"question":"**Memory Management in Custom Python Objects** # Objective: To test your understanding of memory management and object initialization in Python by implementing a custom Python object using high-level Python techniques. # Problem Statement: You are required to implement a custom class in Python that mimics the behavior of variable-sized objects in C, such as tuples. The custom class, `CustomObject`, should manage its memory efficiently and allow for dynamic resizing. # Requirements: 1. **Initialization**: - The `CustomObject` should be initialized with an optional list of items. - The class should handle the addition and removal of items, with internal memory management to maintain efficient space usage. 2. **Methods**: - `__init__(self, items: list = None)`: Initialize the object with the provided items or an empty list if no items are provided. - `add_item(self, item)`: Add a new item to the object. - `remove_item(self, index: int)`: Remove the item at the specified index. - `get_items(self) -> list`: Return the current list of items. 3. **Memory Management**: - Internally, manage the allocation and deallocation of memory as items are added or removed, mimicking variable-size object handling. - Minimize the number of allocations and deallocations to optimize performance. # Constraints: - The `items` list should not be directly exposed or accessible from outside the class except through the provided methods. - Optimize the internal memory management to reduce the number of list resizings. - The `CustomObject` should efficiently handle large numbers of additions and removals. # Example Usage: ```python obj = CustomObject([1, 2, 3]) print(obj.get_items()) # Output: [1, 2, 3] obj.add_item(4) print(obj.get_items()) # Output: [1, 2, 3, 4] obj.remove_item(1) print(obj.get_items()) # Output: [1, 3, 4] ``` # Additional Information: While the internal implementation details are up to you, consider using techniques such as pre-allocating larger blocks of memory and only resizing when necessary to optimize performance.","solution":"class CustomObject: def __init__(self, items: list = None): Initialize the CustomObject with an optional list of items. self.items = items if items is not None else [] def add_item(self, item): Adds a new item to the CustomObject. self.items.append(item) def remove_item(self, index: int): Removes the item at the specified index from the CustomObject. if 0 <= index < len(self.items): self.items.pop(index) def get_items(self) -> list: Returns the current list of items. return self.items"},{"question":"Objective Demonstrate your understanding of the `torch.futures` package by implementing a function that creates and manages multiple asynchronous tasks using `Future` objects. Problem Statement You are tasked with implementing a function `asynchronous_operations` that performs multiple asynchronous operations using `torch.futures.Future`. Your function should: 1. Create a list of `n` asynchronous tasks, each mimicking a complex computation by returning the square of its input after a delay. 2. Use `torch.futures.Future` to manage these tasks. 3. Collect the results of all the tasks using `collect_all`. 4. Return a single list containing the results of these tasks. Function Signature ```python def asynchronous_operations(n: int) -> List[int]: pass ``` Input - `n`: An integer representing the number of asynchronous tasks to create. (1  n  1000) Output - A list of integers representing the results of each asynchronous task. Constraints - The delay for each task can be simulated using the `asyncio.sleep` function. - Each asynchronous task should compute the square of its input value. Example ```python import torch.futures from typing import List import asyncio def simulate_complex_computation(value: int) -> torch.futures.Future: loop = asyncio.get_event_loop() future = torch.futures.Future() async def task(): await asyncio.sleep(1) # Simulate delay future.set_result(value * value) loop.create_task(task()) return future def asynchronous_operations(n: int) -> List[int]: futures = [simulate_complex_computation(i) for i in range(n)] results = torch.futures.collect_all(futures).wait() return [result.item() for result in results] # Example usage: print(asynchronous_operations(3)) # Output: [0, 1, 4] ``` Requirements 1. Ensure your solution handles up to 1000 tasks efficiently. 2. Use `torch.futures.Future` and related utilities (`collect_all`, `wait_all`). Notes - Be familiar with Python\'s `asyncio` library to simulate asynchronous behavior. - You may need to use additional helper functions to manage the asynchronous tasks and computations.","solution":"import torch.futures from typing import List import asyncio async def simulate_complex_computation(value: int) -> int: await asyncio.sleep(1) # Simulate delay return value * value def asynchronous_operations(n: int) -> List[int]: loop = asyncio.new_event_loop() asyncio.set_event_loop(loop) async def collect_tasks(): futures = [asyncio.ensure_future(simulate_complex_computation(i)) for i in range(n)] completed_tasks = await asyncio.gather(*futures) return completed_tasks results = loop.run_until_complete(collect_tasks()) loop.close() return results # Example usage: # print(asynchronous_operations(3)) # Output should be [0, 1, 4] after approximately 1 second"},{"question":"# ConfigParser Exercise **Background**: The `configparser` module in Python is used to handle configuration files (commonly `.ini` files). These files are simple text files with a basic structure composed of sections, each containing key-value pairs. **Task**: You are required to implement a function `parse_and_update_config(file_path, section_name, new_entries)` that will parse an existing configuration file, update it with new entries in a given section, and save the modified configuration back to the file. **Function Signature**: ```python def parse_and_update_config(file_path: str, section_name: str, new_entries: dict) -> None: ``` **Input**: - `file_path` (str): The path to the configuration (.ini) file. - `section_name` (str): The name of the section where new entries should be added or updated. - `new_entries` (dict): A dictionary containing new key-value pairs that need to be added or updated in the specified section. **Output**: - The function does not return anything. It updates the specified configuration file directly. **Constraints**: 1. The configuration file may or may not contain the specified section. 2. The function should be able to handle cases where the section exists, and also where the section is absent. 3. The new entries should replace any existing entries with the same keys in the specified section. 4. Ensure that the updated configuration file\'s format remains valid. **Example Usage**: ```python # Example configuration file before the update: # [DEFAULT] # name = Application # version = 1.0 # [User] # username = johndoe # email = johndoe@example.com file_path = \\"config.ini\\" section_name = \\"User\\" new_entries = { \\"email\\": \\"john.doe@newdomain.com\\", \\"phone\\": \\"123-456-7890\\" } parse_and_update_config(file_path, section_name, new_entries) # Example configuration file after the update: # [DEFAULT] # name = Application # version = 1.0 # [User] # username = johndoe # email = john.doe@newdomain.com # phone = 123-456-7890 ``` **Hints**: - Use `configparser.ConfigParser()` to read and write the configuration file. - Check if the given section exists; if not, create it. - Use appropriate methods to update the section with new entries. This task assesses the student\'s ability to: - Read and interpret structured text files. - Use Python libraries to manipulate those files. - Handle edge cases and ensure the integrity of the file\'s structure.","solution":"import configparser def parse_and_update_config(file_path, section_name, new_entries): Parses an existing configuration file, updates it with new entries in a given section, and saves the modified configuration back to the file. :param file_path: Path to the configuration (.ini) file. :param section_name: Name of the section where new entries should be added or updated. :param new_entries: Dictionary containing new key-value pairs for the specified section. config = configparser.ConfigParser() # Read the existing config file config.read(file_path) # Check if the section exists, if not, create it if not config.has_section(section_name) and section_name != configparser.DEFAULTSECT: config.add_section(section_name) # Update the section with new entries for key, value in new_entries.items(): config.set(section_name, key, value) # Write the changes back to the file with open(file_path, \'w\') as configfile: config.write(configfile)"},{"question":"**Objective:** Write a Python program that demonstrates your understanding of file and directory management using the `os`, `shutil`, and `glob` modules. **Problem Statement:** You are tasked with organizing files in a given directory based on their file extensions. Your program should perform the following: 1. **Create a Backup**: Create a backup of the directory and its contents by copying the entire directory to a new location. 2. **Organize Files**: In the original directory, create subdirectories for each unique file extension (e.g., `.txt`, `.jpg`) and move files into their respective subdirectories. If a file does not have an extension, move it to a directory named `no_extension`. 3. **Provide a Summary**: After organizing, generate a summary that lists: - The total number of files moved. - The number of files in each subdirectory. **Function Specification:** - **Input:** - `src_dir` (string): The path of the directory to organize. - `backup_dir` (string): The path where the backup of the original directory should be created. - **Output:** - A dictionary containing the summary of the operation with the following structure: ```python { \\"total_files_moved\\": <int>, \\"files_per_extension\\": { \\"<file_extension>\\": <int>, \\"no_extension\\": <int> } } ``` **Constraints:** - Assume the `src_dir` and `backup_dir` provided are valid paths on the filesystem. - Only files should be moved, not directories. - Handle any exceptions that may arise during file operations gracefully and provide meaningful error messages. **Example Usage:** ```python import os import shutil import glob def organize_files(src_dir, backup_dir): # Create backup of the directory shutil.copytree(src_dir, backup_dir) # Initialize summary dictionary summary = {\\"total_files_moved\\": 0, \\"files_per_extension\\": {}} # Get all files in the directory files = glob.glob(os.path.join(src_dir, \'*\')) for file in files: if os.path.isfile(file): # Extract file extension file_extension = os.path.splitext(file)[1].lower() if os.path.splitext(file)[1] else \\"no_extension\\" # Update summary dictionary if file_extension not in summary[\\"files_per_extension\\"]: summary[\\"files_per_extension\\"][file_extension] = 0 summary[\\"files_per_extension\\"][file_extension] += 1 # Determine target directory for the file target_dir = os.path.join(src_dir, file_extension) if not os.path.exists(target_dir): os.makedirs(target_dir) # Move the file to target directory shutil.move(file, target_dir) # Update total files moved summary[\\"total_files_moved\\"] += 1 return summary # Example usage summary = organize_files(\'/path/to/source/directory\', \'/path/to/backup/directory\') print(summary) ``` **Note:** Replace `\'/path/to/source/directory\'` and `\'/path/to/backup/directory\'` with actual paths.","solution":"import os import shutil import glob def organize_files(src_dir, backup_dir): Organizes files in the given directory based on their file extensions and creates a backup. Parameters: - src_dir (string): The path of the directory to organize. - backup_dir (string): The path where the backup of the original directory should be created. Returns: - dict: A summary of the operation. # Create backup of the directory shutil.copytree(src_dir, backup_dir) # Initialize summary dictionary summary = {\\"total_files_moved\\": 0, \\"files_per_extension\\": {}} # Get all files in the directory files = glob.glob(os.path.join(src_dir, \'*\')) for file in files: if os.path.isfile(file): # Extract file extension file_extension = os.path.splitext(file)[1][1:].lower() if os.path.splitext(file)[1] else \\"no_extension\\" # Update summary dictionary if file_extension not in summary[\\"files_per_extension\\"]: summary[\\"files_per_extension\\"][file_extension] = 0 summary[\\"files_per_extension\\"][file_extension] += 1 # Determine target directory for the file target_dir = os.path.join(src_dir, file_extension) if not os.path.exists(target_dir): os.makedirs(target_dir) # Move the file to target directory shutil.move(file, target_dir) # Update total files moved summary[\\"total_files_moved\\"] += 1 return summary"},{"question":"You are given a Unix system that uses NIS for central administration, and you need to interact with various NIS maps to retrieve and display information. Task: Write a Python function `get_user_info(uid, mapname)` that does the following: 1. Uses the `nis.match` function to retrieve the user information associated with the given `uid` from a specified `mapname`. 2. If the `uid` does not exist in the specified `mapname`, your function should return a dictionary with a single key `\\"error\\"` and the value being a suitable error message. 3. If the `uid` is found, the function should return a dictionary with the keys `\\"uid\\"`, `\\"mapname\\"`, and `\\"userinfo\\"`. Expected Input and Output: - **Input:** - `uid` (string): The user ID to be looked up. - `mapname` (string): The name of the NIS map to look up the user ID from. - **Output:** - A dictionary with one of the following structures: - If the `uid` does not exist: `{\\"error\\": \\"Error message\\"}` - If the `uid` exists: ```python { \\"uid\\": \\"specified_uid\\", \\"mapname\\": \\"specified_mapname\\", \\"userinfo\\": \\"information_retrieved\\" } ``` Constraints: - The `uid` and `mapname` inputs are strings and are 8-bit clean. - Handle any exceptions that may arise, specifically `nis.error`. Function Signature: ```python def get_user_info(uid: str, mapname: str) -> dict: pass ``` Example: ```python # Example function call result = get_user_info(\\"12345\\", \\"passwd.byuid\\") print(result) # Possible Output if the uid does not exist: # {\\"error\\": \\"NIS key not found\\"} # Possible Output if the uid exists: # { # \\"uid\\": \\"12345\\", # \\"mapname\\": \\"passwd.byuid\\", # \\"userinfo\\": \\"retrieved_user_info\\" # } ``` Use the provided documentation of the `nis` module to implement the function. Make sure to handle errors gracefully and provide meaningful error messages.","solution":"import nis def get_user_info(uid: str, mapname: str) -> dict: Retrieves user information associated with the given uid from the specified NIS map. :param uid: The user ID to look up :param mapname: The name of the NIS map to fetch the user information from :return: A dictionary containing either the user information or an error message try: # Retrieve user information from NIS map userinfo = nis.match(uid.encode(\'utf-8\'), mapname.encode(\'utf-8\')).decode(\'utf-8\') return { \\"uid\\": uid, \\"mapname\\": mapname, \\"userinfo\\": userinfo } except nis.error as e: return { \\"error\\": str(e) }"},{"question":"**Problem Statement** You are required to demonstrate your understanding of the Python `ipaddress` module by implementing functions to manipulate and analyze IPv4 and IPv6 addresses. # Function 1: `is_valid_ip` Write a function `is_valid_ip(address: str) -> bool` to validate if the given address is a valid IPv4 or IPv6 address. **Constraints:** - The function should return `True` if the address is valid, `False` otherwise. # Function 2: `common_subnet` Write a function `common_subnet(address1: str, address2: str, prefix_length: int) -> str` that returns the common subnet of two IP addresses given a specific prefix length. **Constraints:** - Both `address1` and `address2` could be either IPv4 or IPv6 addresses. - The returned subnet should be in the same family (IPv4 or IPv6) as the input addresses. - Handle invalid IP addresses by raising a `ValueError`. # Function 3: `ip_summary` Write a function `ip_summary(network: str) -> dict` that returns a summary of the IP network specified. **Constraints:** - The input `network` will be in CIDR notation (e.g., `\\"192.168.1.0/24\\"` for IPv4 or `\\"2001:db8::/32\\"` for IPv6). - The function should return a dictionary with the following keys: - `network_address`: The network address. - `broadcast_address`: The broadcast address (for IPv4) or None (for IPv6). - `num_addresses`: Total number of addresses in the network. - `hosts`: List of all usable host addresses in the network (excluding network and broadcast addresses for IPv4). - Handle invalid network specifications by raising a `ValueError`. # Example Usage ```python # Function 1 assert is_valid_ip(\'192.168.0.1\') == True assert is_valid_ip(\'2001:db8::1\') == True assert is_valid_ip(\'invalid_ip\') == False # Function 2 assert common_subnet(\'192.168.1.1\', \'192.168.1.128\', 24) == \'192.168.1.0/24\' assert common_subnet(\'2001:db8::1\', \'2001:db8::2\', 32) == \'2001:db8::/32\' # Function 3 summary = ip_summary(\'192.168.1.0/30\') assert summary[\'network_address\'] == \'192.168.1.0\' assert summary[\'broadcast_address\'] == \'192.168.1.3\' assert summary[\'num_addresses\'] == 4 assert summary[\'hosts\'] == [\'192.168.1.1\', \'192.168.1.2\'] ``` # Important Notes - You are encouraged to use the `ipaddress` module in Python for your implementations. - Assume that all inputs are in the correct case (case-sensitive inputs). - You only need to handle validation and conversion of IP addresses and subnet calculations; you can assume proper format for IP address inputs but should handle invalid cases gracefully. **Submission Format** Submit a single Python file with all three function implementations and any necessary import statements.","solution":"import ipaddress def is_valid_ip(address: str) -> bool: Validates if the given address is a valid IPv4 or IPv6 address. Returns True if the address is valid, False otherwise. try: ipaddress.ip_address(address) return True except ValueError: return False def common_subnet(address1: str, address2: str, prefix_length: int) -> str: Returns the common subnet of two IP addresses given a specific prefix length. If the addresses are invalid, raises ValueError. try: ip1 = ipaddress.ip_network(f\\"{address1}/{prefix_length}\\", strict=False) ip2 = ipaddress.ip_network(f\\"{address2}/{prefix_length}\\", strict=False) except ValueError: raise ValueError(\\"Invalid IP address or prefix length\\") if ip1.network_address != ip2.network_address: raise ValueError(\\"The addresses do not share the same subnet\\") if ip1.version != ip2.version: raise ValueError(\\"IP Addresses should be of the same version\\") return f\\"{ip1.network_address}/{prefix_length}\\" def ip_summary(network: str) -> dict: Returns a summary of the IP network specified. If the input network is invalid, raises ValueError. try: net = ipaddress.ip_network(network, strict=False) except ValueError: raise ValueError(\\"Invalid network\\") summary = { \\"network_address\\": str(net.network_address), \\"broadcast_address\\": None if net.version == 6 else str(net.broadcast_address), \\"num_addresses\\": net.num_addresses, \\"hosts\\": None } if net.version == 4: summary[\\"hosts\\"] = [str(host) for host in net.hosts()] return summary"},{"question":"**Custom Serialization using `copyreg` in Python** In this task, you are required to demonstrate your understanding of custom serialization using the `copyreg` module in Python. The goal is to implement custom pickling routines for a set of class objects. # Problem Statement You will implement a class `Person` that has the following properties: - `name` (string): Name of the person. - `age` (int): Age of the person. - `friends` (list of `Person` objects): List containing `Person` instances representing the friends of the person. You will also implement a custom pickling routine for the `Person` class using the `copyreg` module to ensure proper serialization and deserialization of a `Person` object, including its friends. # Requirements 1. Implement the `Person` class with the mentioned properties. 2. Write a function `pickle_person(person)` that defines how a `Person` object should be reduced to a serializable format. This function should be used to register the pickling routine with `copyreg`. 3. Register the pickling routine using `copyreg.pickle`. 4. Write a function `unpickle_person(name, age, friends)` for reconstructing a `Person` object from the serialized format. 5. Demonstrate the functionality by: - Creating a few `Person` objects. - Setting up their friendships. - Serializing and deserializing them using the `pickle` module. - Printing the deserialized objects to verify their correctness. # Constraints - The `name` should be a non-empty string. - The `age` should be a non-negative integer. # Example ```python import copyreg import pickle class Person: def __init__(self, name, age): self.name = name self.age = age self.friends = [] # Method to add a friend def add_friend(self, friend): self.friends.append(friend) def __repr__(self): return f\\"Person(name={self.name}, age={self.age}, friends={[friend.name for friend in self.friends]})\\" # Custom pickling function def pickle_person(person): return unpickle_person, (person.name, person.age, person.friends) # Custom unpickling function def unpickle_person(name, age, friends): new_person = Person(name, age) new_person.friends = friends return new_person # Register the pickling function copyreg.pickle(Person, pickle_person) # Create Person objects alice = Person(\\"Alice\\", 30) bob = Person(\\"Bob\\", 25) charlie = Person(\\"Charlie\\", 20) # Setting up friendships alice.add_friend(bob) alice.add_friend(charlie) bob.add_friend(charlie) # Pickle (serialize) the objects serialized_alice = pickle.dumps(alice) serialized_bob = pickle.dumps(bob) # Unpickle (deserialize) the objects deserialized_alice = pickle.loads(serialized_alice) deserialized_bob = pickle.loads(serialized_bob) # Validate the result print(deserialized_alice) print(deserialized_bob) ``` In the example above, you should ensure the custom pickling routine correctly preserves the state of `Person` objects, including their list of friends. Validate that deserialization reconstructs the objects accurately.","solution":"import copyreg import pickle class Person: def __init__(self, name, age): if not name: raise ValueError(\\"Name must be a non-empty string\\") if age < 0: raise ValueError(\\"Age must be a non-negative integer\\") self.name = name self.age = age self.friends = [] # Method to add a friend def add_friend(self, friend): self.friends.append(friend) def __repr__(self): return f\\"Person(name={self.name}, age={self.age}, friends={[friend.name for friend in self.friends]})\\" # Custom pickling function def pickle_person(person): return unpickle_person, (person.name, person.age, person.friends) # Custom unpickling function def unpickle_person(name, age, friends): new_person = Person(name, age) new_person.friends = friends return new_person # Register the pickling function copyreg.pickle(Person, pickle_person) # Demonstrate the functionality # Create Person objects alice = Person(\\"Alice\\", 30) bob = Person(\\"Bob\\", 25) charlie = Person(\\"Charlie\\", 20) # Setting up friendships alice.add_friend(bob) alice.add_friend(charlie) bob.add_friend(charlie) # Pickle (serialize) the objects serialized_alice = pickle.dumps(alice) serialized_bob = pickle.dumps(bob) # Unpickle (deserialize) the objects deserialized_alice = pickle.loads(serialized_alice) deserialized_bob = pickle.loads(serialized_bob) # Validate the result print(deserialized_alice) print(deserialized_bob)"},{"question":"Coding Assessment Question Objective Your task is to write a Python function that demonstrates the use of various functionalities of the `pathlib` module. The function should search for Python files in the given directory and its subdirectories, and output their details. # Function Signature ```python def search_python_files(directory: str) -> List[Dict[str, Union[str, int]]]: pass ``` # Input - `directory` (str): A path to the directory where the search will start. The path can be absolute or relative. # Output - Returns a list of dictionaries, where each dictionary represents a Python file found and contains: - `name`: The name of the file (e.g., `\'example.py\'`). - `relative_path`: The relative path from the given directory (e.g., `\'subdir/example.py\'`). - `absolute_path`: The absolute path to the file (e.g., `\'/home/user/project/subdir/example.py\'`). - `size`: The size of the file in bytes (e.g., `1024`). # Constraints - The function should handle varying types of paths (relative, absolute) and directory structures. - Ensure that the function is efficient and handles potential exceptions appropriately. # Example Usage ```python result = search_python_files(\'/path/to/directory\') # Sample result [ { \'name\': \'example.py\', \'relative_path\': \'example.py\', \'absolute_path\': \'/path/to/directory/example.py\', \'size\': 2048 }, { \'name\': \'example2.py\', \'relative_path\': \'subdir/example2.py\', \'absolute_path\': \'/path/to/directory/subdir/example2.py\', \'size\': 1024 } ] ``` # Notes - The function should use pathlib\'s features to achieve the task. - Feel free to use additional helper functions if needed. # Advanced Requirement (Optional) Enhance the function to include an option that follows symlinks while searching. Advanced Function Signature ```python def search_python_files(directory: str, follow_symlinks: bool = False) -> List[Dict[str, Union[str, int]]]: pass ``` # Testing This function will be tested with various directories containing nested subdirectories and multiple Python files. It is essential to test edge cases such as empty directories, directories with no Python files, and directories with a mix of files.","solution":"from pathlib import Path from typing import List, Dict, Union def search_python_files(directory: str, follow_symlinks: bool = False) -> List[Dict[str, Union[str, int]]]: Searches for Python files in the given directory and its subdirectories. Args: - directory (str): The path to the directory where the search will start. - follow_symlinks (bool): Whether to follow symlinks (default is False). Returns: - List[Dict[str, Union[str, int]]]: A list of dictionaries with details about each Python file found. directory_path = Path(directory) python_files = [] for file in directory_path.rglob(\'*.py\'): if file.is_symlink() and not follow_symlinks: continue if file.is_file(): file_info = { \'name\': file.name, \'relative_path\': file.relative_to(directory_path).as_posix(), \'absolute_path\': str(file.resolve()), \'size\': file.stat().st_size } python_files.append(file_info) return python_files"},{"question":"Objective Implement a function to compare two neural network models\' outputs layer by layer using the given utilities from the `torch.ao.ns.fx.utils` module. You are to compute three metrics for each corresponding layer\'s output: Signal to Quantization Noise Ratio (SQNR), normalized L2 error, and cosine similarity. Task Write a function `compare_models(model_1, model_2, dataloader)` that: 1. Takes two PyTorch models (`model_1` and `model_2`) and a PyTorch dataloader (`dataloader`) containing the input data. 2. Computes the outputs of each layer in both models for the input data. 3. Uses the functions `compute_sqnr`, `compute_normalized_l2_error`, and `compute_cosine_similarity` to calculate metrics between the corresponding layers\' outputs of the two models. 4. Returns a dictionary where each key is the layer name and each value is another dictionary containing the computed SQNR, normalized L2 error, and cosine similarity. Input - `model_1`: A PyTorch model. - `model_2`: A PyTorch model with the same architecture as `model_1`. - `dataloader`: A PyTorch dataloader providing input data. Output - A dictionary where each key is a layer name (as a string) and each value is a dictionary with the keys `\'SQNR\'`, `\'L2\'`, and `\'Cosine\'` representing the respective metrics. Constraints - You can assume that both models (`model_1` and `model_2`) have the same architecture. - Each metric should be computed layer by layer for corresponding outputs in both models. Performance - The function should handle large models and datasets efficiently. Proper GPU utilization is encouraged if available. Example ```python import torch import torch.nn as nn from torch.utils.data import DataLoader, TensorDataset from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity # Example models class MyModel(nn.Module): def __init__(self): super(MyModel, self).__init__() self.fc1 = nn.Linear(10, 5) self.relu1 = nn.ReLU() self.fc2 = nn.Linear(5, 2) self.relu2 = nn.ReLU() def forward(self, x): x = self.fc1(x) x = self.relu1(x) x = self.fc2(x) x = self.relu2(x) return x # Sample data data = torch.randn(100, 10) dataset = TensorDataset(data) dataloader = DataLoader(dataset, batch_size=10) # Models model_1 = MyModel() model_2 = MyModel() # Function to implement def compare_models(model_1, model_2, dataloader): # Implement the function based on the instructions pass # Usage metrics = compare_models(model_1, model_2, dataloader) print(metrics) ``` Note that this example only shows the function and example models/data. You will need to implement the actual comparison within `compare_models`.","solution":"import torch import torch.nn as nn from torch.utils.data import DataLoader from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity def compare_models(model_1, model_2, dataloader): Compare two neural network models\' outputs layer by layer. Args: model_1 (nn.Module): The first PyTorch model. model_2 (nn.Module): The second PyTorch model. dataloader (DataLoader): The PyTorch dataloader providing input data. Returns: dict: A dictionary with layer names as keys and another dictionary containing \'SQNR\', \'L2\', and \'Cosine\' for each layer. # Ensure both models are in evaluation mode model_1.eval() model_2.eval() # Register hooks to save the outputs of each layer outputs_1, outputs_2 = [], [] def hook_1(module, input, output): outputs_1.append(output) def hook_2(module, input, output): outputs_2.append(output) hooks_1 = [] hooks_2 = [] for layer in model_1.children(): hooks_1.append(layer.register_forward_hook(hook_1)) for layer in model_2.children(): hooks_2.append(layer.register_forward_hook(hook_2)) with torch.no_grad(): for data in dataloader: data = data[0] model_1(data) model_2(data) # Remove hooks for hook in hooks_1: hook.remove() for hook in hooks_2: hook.remove() metrics = {} for i, (out_1, out_2) in enumerate(zip(outputs_1, outputs_2)): layer_name = f\\"layer_{i+1}\\" sqnr = compute_sqnr(out_1, out_2) l2_error = compute_normalized_l2_error(out_1, out_2) cosine_similarity = compute_cosine_similarity(out_1, out_2) metrics[layer_name] = { \\"SQNR\\": sqnr, \\"L2\\": l2_error, \\"Cosine\\": cosine_similarity } return metrics"},{"question":"# PyTorch Autograd Challenge Objective Demonstrate your understanding of PyTorchs autograd system by implementing custom gradient computation for a specific operation using a custom `Function`. Furthermore, employ the use of saved tensors and hooks to properly manage gradient flows and computation. # Problem Statement You are tasked with creating a custom PyTorch `Function` to compute the element-wise exponential and logarithmic operations in a single composite function. Specifically, the function takes an input tensor `x`, computes `y = exp(x) - log(x)`, and stores intermediate results for use in the backward pass. Steps to Implement: 1. **Define a custom `Function`:** Implement the forward and backward methods for this function. 2. **Save intermediate results:** Use PyTorchs tensor-saving mechanisms to save necessary intermediate results during the forward pass. 3. **Custom gradient computation:** Ensure the backward method correctly computes the gradients using the saved tensors. Input - A PyTorch tensor `x` with `requires_grad=True`. Output - A tensor `y` computed as `exp(x) - log(x)`. Constraints and Limitations 1. Do not use in-place operations to avoid complications with autograd. 2. Ensure that intermediate results are properly saved and accessed without causing reference issues. 3. Handle cases where `x` may contain values that would cause `log` to be undefined. Example ```python import torch from torch.autograd import Function class CompositeFunction(Function): @staticmethod def forward(ctx, x): exp_x = torch.exp(x) log_x = torch.log(x) y = exp_x - log_x # Save tensors for backward ctx.save_for_backward(exp_x, log_x) return y @staticmethod def backward(ctx, grad_output): exp_x, log_x = ctx.saved_tensors # Compute gradients grad_input = grad_output * (exp_x - (1 / log_x)) return grad_input # Testing the custom function x = torch.tensor([0.5, 1.0, 2.0], requires_grad=True) y = CompositeFunction.apply(x) y.sum().backward() print(\\"Input:\\", x) print(\\"Output:\\", y) print(\\"Gradients:\\", x.grad) ``` # Additional Tasks: 1. **Testing:** Write additional test cases to ensure your implementation handles edge cases such as inputs that would cause `log(x)` to be undefined. 2. **Hooks:** Implement `pack_hook` and `unpack_hook` functions to manage how tensors are saved and unpacked. Register these hooks and demonstrate their use. # Performance Requirements - Ensure that the computations are efficient and all intermediate steps are properly managed to avoid memory leaks or excessive computational overhead. - Your custom `Function` should be robust enough to handle a range of input values without causing errors or undefined behavior. Good luck! Demonstrate your understanding by ensuring all parts of the question are correctly implemented and tested.","solution":"import torch from torch.autograd import Function class CompositeFunction(Function): @staticmethod def forward(ctx, x): # Handling edge cases where log(x) would be undefined or inf safe_x = torch.clamp(x, min=1e-7) # Perform the forward pass operations exp_x = torch.exp(x) log_x = torch.log(safe_x) # Compute the result y = exp_x - log_x # Save tensors for backward pass ctx.save_for_backward(exp_x, safe_x) return y @staticmethod def backward(ctx, grad_output): exp_x, safe_x = ctx.saved_tensors # Compute gradients grad_input = grad_output * (exp_x - (1 / safe_x)) return grad_input # Usage example x = torch.tensor([0.5, 1.0, 2.0], requires_grad=True) y = CompositeFunction.apply(x) y.sum().backward() # Demonstrating the output print(\\"Input:\\", x) print(\\"Output:\\", y) print(\\"Gradients:\\", x.grad)"},{"question":"# Floating Point Arithmetic and Precision Handling In this assignment, you are required to implement a function `accurate_sum`, which takes a list of floating-point numbers and returns their sum with the highest possible accuracy. You must leverage Python\'s `decimal` module to achieve this. Function Signature ```python def accurate_sum(float_list: list[float]) -> float: pass ``` Input - `float_list`: A list of floating-point numbers (e.g., `float_list = [0.1, 0.2, 0.3]`). Output - A single floating-point number representing the sum of the input list, computed with maximum accuracy. Constraints - You may assume the input list contains at least one floating-point number. - The list length will not exceed 10^6. - The values of the floating-point numbers will be within the range of typical floating-point precision issues mentioned in the documentation. Requirements 1. **Accuracy:** The function must handle and mitigate the floating-point precision issues. 2. **Efficiency:** The function must be efficient to handle up to 10^6 floating-point numbers. 3. **Exact Representation:** Use the `decimal` module to ensure calculations are done with the highest possible precision. Example ```python # Example 1 float_list = [0.1, 0.1, 0.1] print(accurate_sum(float_list)) # Output: 0.3 (not 0.30000000000000004) # Example 2 float_list = [0.1, 0.2, 0.3] print(accurate_sum(float_list)) # Output: 0.6 (not 0.6000000000000001) ``` Note To correctly mitigate floating-point inaccuracies, Python\'s `decimal` module should be used for summation. Be sure to convert the floating-point numbers to `Decimal` before performing the addition.","solution":"from decimal import Decimal def accurate_sum(float_list: list[float]) -> float: Returns the sum of a list of floating-point numbers with the highest possible accuracy. # Convert all floating-point numbers to Decimal for accurate summation sum_decimal = sum(Decimal(str(num)) for num in float_list) # Convert the result back to float return float(sum_decimal)"},{"question":"You are given an incomplete implementation of a class `EmailCharset` that handles various character set operations, similar to the `email.charset.Charset` class described in the documentation. Your task is to complete the implementation of specific methods within this class. Class `EmailCharset` Specification: - **Attributes:** - `input_charset` (string): The initial character set specified. - `header_encoding` (string): The encoding for email headers. - `body_encoding` (string): The encoding for email bodies. - `output_charset` (string): The character set for output conversion. - `input_codec` (string): Codec used to convert `input_charset` to Unicode. - `output_codec` (string): Codec used to convert Unicode to `output_charset`. - **Methods to Implement:** 1. `get_body_encoding()`: Returns the content transfer encoding used for body encoding as a string based on the `body_encoding` attribute. 2. `get_output_charset()`: Returns the output character set based on the `output_charset` and `input_charset` attributes. 3. `header_encode(string)`: Encodes the given string for use in an email header using the appropriate encoding scheme specified in `header_encoding`. 4. `body_encode(string)`: Encodes the given string for use in the body of the email using the appropriate encoding scheme specified in `body_encoding`. Use the following templates and complete the methods: ```python class EmailCharset: def __init__(self, input_charset=\'us-ascii\'): self.input_charset = input_charset.lower() self.header_encoding = None self.body_encoding = None self.output_charset = None self.input_codec = None self.output_codec = None def get_body_encoding(self): # Implement this method to return the appropriate body encoding. pass def get_output_charset(self): # Implement this method to return the appropriate output charset. pass def header_encode(self, string): # Implement this method to encode the string for the email header. pass def body_encode(self, string): # Implement this method to encode the string for the email body. pass # Example usage: email_charset = EmailCharset(\'iso-8859-1\') email_charset.body_encoding = \'BASE64\' print(email_charset.get_body_encoding()) # Expected output: \\"base64\\" email_charset.header_encoding = \'QP\' print(email_charset.header_encode(\'email header\')) # Follow quoted-printable encoding rules. email_charset.body_encoding = \'QP\' print(email_charset.body_encode(\'email body\')) # Follow quoted-printable encoding rules. ``` Input and Output Format: - `get_body_encoding()`: No input parameters. - Should return a string: \\"quoted-printable\\", \\"base64\\", or \\"7bit\\". - `get_output_charset()`: No input parameters. - Should return a string with the charset name. - `header_encode(string)`: - `string` (str): The string to be encoded for the email header. - Should return an encoded string based on the header encoding specified. - `body_encode(string)`: - `string` (str): The string to be encoded for the email body. - Should return an encoded string based on the body encoding specified. Ensure to handle the operations as per the specifications and encoding techniques required.","solution":"import base64 import quopri class EmailCharset: def __init__(self, input_charset=\'us-ascii\'): self.input_charset = input_charset.lower() self.header_encoding = None self.body_encoding = None self.output_charset = None self.input_codec = None self.output_codec = None def get_body_encoding(self): encodings = { \'base64\': \'base64\', \'quoted-printable\': \'quoted-printable\', \'7bit\': \'7bit\', \'8bit\': \'8bit\' } if self.body_encoding and self.body_encoding.lower() in encodings: return encodings[self.body_encoding.lower()] return \'7bit\' def get_output_charset(self): if self.output_charset: return self.output_charset.lower() return self.input_charset def header_encode(self, string): if self.header_encoding and self.header_encoding.lower() == \'base64\': encoded_bytes = base64.b64encode(string.encode(\'utf-8\')) return encoded_bytes.decode(\'utf-8\') elif self.header_encoding and self.header_encoding.lower() == \'quoted-printable\': encoded_bytes = quopri.encodestring(string.encode(\'utf-8\')) return encoded_bytes.decode(\'utf-8\') return string def body_encode(self, string): if self.body_encoding and self.body_encoding.lower() == \'base64\': encoded_bytes = base64.b64encode(string.encode(\'utf-8\')) return encoded_bytes.decode(\'utf-8\') elif self.body_encoding and self.body_encoding.lower() == \'quoted-printable\': encoded_bytes = quopri.encodestring(string.encode(\'utf-8\')) return encoded_bytes.decode(\'utf-8\') return string # Example usage: email_charset = EmailCharset(\'iso-8859-1\') email_charset.body_encoding = \'BASE64\' print(email_charset.get_body_encoding()) # Expected output: \\"base64\\" email_charset.header_encoding = \'QP\' print(email_charset.header_encode(\'email header\')) # Follow quoted-printable encoding rules. email_charset.body_encoding = \'QP\' print(email_charset.body_encode(\'email body\')) # Follow quoted-printable encoding rules."},{"question":"# Advanced Data Handling with `collections` You are tasked to implement a set of functions for a system that tracks and manipulates time-based events. You will utilize Python\'s `collections` module, specifically `defaultdict`, `deque`, and `OrderedDict`, to create a solution that showcases your understanding of these advanced container data types. Problem Statement 1. **EventTracker Class**: - Implement a class `EventTracker` which aggregates event data over time. - The class should use `defaultdict`, `deque`, and `OrderedDict` to provide efficient and ordered event management. 2. **Methods**: - `__init__(self)`: Initialize the necessary data structures. - `add_event(self, timestamp: str, event_name: str)`: Add an event with the given `timestamp` (string format \'YYYY-MM-DD HH:MM\') and `event_name`. Ensure events with the same name and timestamp are kept together. - `get_events(self, from_timestamp: str, to_timestamp: str) -> OrderedDict`: Retrieve events occurring between `from_timestamp` and `to_timestamp` inclusive. The result should be an `OrderedDict` where the keys are timestamps and the values are deques of event names. The events should be returned in the order they were added. Input and Output Formats - **Input**: The input timestamps will always be valid strings of the form \'YYYY-MM-DD HH:MM\'. Event names will be non-empty strings. - **Output**: The `get_events` method should return an `OrderedDict` where each key is a timestamp (string) in sorted order, and each value is a `deque` of event names. Constraints 1. Provide the solution with a linear or logarithmic time complexity regarding the number of events for efficient performance. 2. Assume the maximum number of events to be handled by the system will not exceed 100,000. Example ```python # Example usage of EventTracker tracker = EventTracker() # Adding events tracker.add_event(\'2023-10-01 10:00\', \'eventA\') tracker.add_event(\'2023-10-01 10:00\', \'eventB\') tracker.add_event(\'2023-10-01 11:00\', \'eventC\') # Retrieving events events = tracker.get_events(\'2023-10-01 09:00\', \'2023-10-01 10:00\') for timestamp, event_deque in events.items(): for event in event_deque: print(f\\"{timestamp}: {event}\\") # Expected Output: # 2023-10-01 10:00: eventA # 2023-10-01 10:00: eventB ``` Implement the `EventTracker` class with the methods as described.","solution":"from collections import defaultdict, deque, OrderedDict class EventTracker: def __init__(self): self.events = defaultdict(deque) def add_event(self, timestamp: str, event_name: str): self.events[timestamp].append(event_name) def get_events(self, from_timestamp: str, to_timestamp: str) -> OrderedDict: sorted_events = OrderedDict( (timestamp, events) for timestamp, events in sorted(self.events.items()) if from_timestamp <= timestamp <= to_timestamp ) return sorted_events"},{"question":"**Coding Assessment Question** **Objective:** Write a function that extracts and logs numerical properties of various PyTorch data types. Your task is to retrieve the relevant properties for both floating point and integer data types and return them in a structured format. # Function Signature ```python def extract_numerical_properties(dtypes: list) -> dict: Extracts and logs the numerical properties of the specified PyTorch data types. Parameters: dtypes (list): A list of PyTorch data types from the set {torch.float32, torch.float64, torch.float16, torch.bfloat16, torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64} Returns: dict: A dictionary where each key is a string representation of the data type and the value is another dictionary containing the relevant numerical properties. pass ``` # Input - `dtypes`: A list of PyTorch data types. The list may contain any combination of the following data types: `torch.float32`, `torch.float64`, `torch.float16`, `torch.bfloat16`, `torch.uint8`, `torch.int8`, `torch.int16`, `torch.int32`, `torch.int64`. # Output - A dictionary with string representations of the provided data types as keys. Each key should map to a dictionary of the corresponding numerical properties. - For floating point data types, the properties should include: `bits`, `eps`, `max`, `min`, `tiny`, and `resolution`. - For integer data types, the properties should include: `bits`, `max`, and `min`. # Constraints - Use the classes `torch.finfo` for floating point types and `torch.iinfo` for integer types to retrieve the properties. - Ensure that the returned dictionary is well-structured and includes all the required properties for the provided data types. # Example Usage ```python import torch dtypes = [torch.float32, torch.int64, torch.uint8] result = extract_numerical_properties(dtypes) print(result) ``` # Expected Output ```python { \'torch.float32\': { \'bits\': 32, \'eps\': 1.1920928955078125e-07, \'max\': 3.4028234663852886e+38, \'min\': -3.4028234663852886e+38, \'tiny\': 1.1754943508222875e-38, \'resolution\': 9.999999747378752e-07 }, \'torch.int64\': { \'bits\': 64, \'max\': 9223372036854775807, \'min\': -9223372036854775808 }, \'torch.uint8\': { \'bits\': 8, \'max\': 255, \'min\': 0 }, } ``` The function should handle any combination of the specified data types and output their properties accurately. The dictionary keys should be the string representation of each `torch.dtype`, and the values should be dictionaries containing the relevant numerical properties.","solution":"import torch def extract_numerical_properties(dtypes: list) -> dict: Extracts and logs the numerical properties of the specified PyTorch data types. Parameters: dtypes (list): A list of PyTorch data types from the set {torch.float32, torch.float64, torch.float16, torch.bfloat16, torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64} Returns: dict: A dictionary where each key is a string representation of the data type and the value is another dictionary containing the relevant numerical properties. prop_dict = {} for dtype in dtypes: prop_name = str(dtype) if prop_name.startswith(\\"torch.float\\") or prop_name == \\"torch.bfloat16\\": info = torch.finfo(dtype) prop_dict[prop_name] = { \'bits\': info.bits, \'eps\': info.eps, \'max\': info.max, \'min\': info.min, \'tiny\': info.tiny, \'resolution\': 2 * info.eps } elif prop_name.startswith(\\"torch.uint\\") or prop_name.startswith(\\"torch.int\\"): info = torch.iinfo(dtype) prop_dict[prop_name] = { \'bits\': info.bits, \'max\': info.max, \'min\': info.min } return prop_dict"},{"question":"# Tuning CUDA Operations with PyTorch **Objective:** You are given a prototype feature in PyTorch for tuning CUDA operations. Your task is to write a Python function that configures the tuning environment, performs tuning on a provided CUDA GEMM operation, and reads the tuning results. **Function Signature:** ```python def tune_cuda_operation(filename: str, max_duration: float, max_iterations: int) -> dict: Configures the CUDA tuning environment, performs tuning, and retrieves the tuning results. Args: - filename (str): The name of the file to read/write tuning data. - max_duration (float): The maximum duration allowed for tuning. - max_iterations (int): The maximum number of iterations for tuning. Returns: - dict: A dictionary of tuning results retrieved from the file. ``` **Requirements:** 1. **Enable Tuning:** First, enable the tuning feature using the appropriate API functions. 2. **Set Parameters:** Configure the tuning environment with the maximum duration and the maximum number of iterations. Use `set_max_tuning_duration` and `set_max_tuning_iterations`. 3. **Perform Tuning:** Perform the tuning operation using the given filename. Use `tune_gemm_in_file`. 4. **Read and Return Tuning Results:** Finally, read the tuning results from the file and return them in a dictionary format using `get_results`. **Constraints:** - The maximum duration (`max_duration`) should be a positive float. - The maximum number of iterations (`max_iterations`) should be a positive integer. - The filename should be a valid string. **Example Usage:** ```python results = tune_cuda_operation(\'gemm_tuning_results.txt\', 60.0, 100) print(results) ``` This function tests your knowledge of PyTorch\'s CUDA tuning operations and your ability to handle various related API functions to manage the tuning process effectively.","solution":"import torch def tune_cuda_operation(filename: str, max_duration: float, max_iterations: int) -> dict: Configures the CUDA tuning environment, performs tuning, and retrieves the tuning results. Args: - filename (str): The name of the file to read/write tuning data. - max_duration (float): The maximum duration allowed for tuning. - max_iterations (int): The maximum number of iterations for tuning. Returns: - dict: A dictionary of tuning results retrieved from the file. # Ensure the provided arguments are valid if not isinstance(filename, str): raise TypeError(\\"filename must be a string\\") if not isinstance(max_duration, float) or max_duration <= 0: raise ValueError(\\"max_duration must be a positive float\\") if not isinstance(max_iterations, int) or max_iterations <= 0: raise ValueError(\\"max_iterations must be a positive integer\\") # Enable tuning torch.cuda.set_enabled(True) # Set tuning parameters torch.cuda.set_max_tuning_duration(max_duration) torch.cuda.set_max_tuning_iterations(max_iterations) # Perform tuning operation torch.cuda.tune_gemm_in_file(filename) # Read and return tuning results results = torch.cuda.get_results(filename) return results"},{"question":"<|Analysis Begin|> The documentation provided mainly concerns the testing functionalities in pandas, which include assertion functions and various error and warning classes. These tools are critical for validating the correctness and stability of pandas code. Thus, any question formulated should encourage students to leverage these functionalities to ensure their code is both correct and robust. For a comprehensive assessment question, I should focus on students implementing a pandas function and then using the provided assertion functions to validate their code\'s correctness. The question should incorporate: 1. Data manipulation using pandas DataFrame and Series. 2. Error handling using provided exceptions and warnings. 3. Validation of results using the assertion functions. <|Analysis End|> <|Question Begin|> **Question: Data Aggregation and Validation in Pandas** **Objective:** You are tasked with creating a function that performs specific data aggregation on a pandas DataFrame and then validating the results using pandas\' testing functionalities. This will assess your ability to manipulate data using pandas, handle possible errors, and validate your solution rigorously. **Function:** ```python def aggregate_and_validate(df: pd.DataFrame, group_by_column: str, agg_column: str) -> pd.DataFrame: This function groups the input DataFrame `df` by the column `group_by_column` and performs an aggregation using the column `agg_column` to compute the mean. Parameters: df (pd.DataFrame): The input DataFrame. group_by_column (str): The column name to group by. agg_column (str): The column name to aggregate. Returns: pd.DataFrame: A new DataFrame with the user_id and the corresponding aggregated mean values. # Your implementation here pass ``` **Input:** - `df`: A pandas DataFrame containing at least two columns, with column names provided by `group_by_column` and `agg_column`. - `group_by_column`: The name of the column to group the DataFrame by. - `agg_column`: The name of the column of which to compute mean values. **Output:** - A DataFrame with two columns: `group_by_column` and the mean of `agg_column`, sorted by `group_by_column`. **Example:** ```python import pandas as pd data = { \'user_id\': [\'A\', \'B\', \'A\', \'C\', \'B\', \'C\'], \'value\': [10, 20, 30, 40, 50, 60] } df = pd.DataFrame(data) result = aggregate_and_validate(df, \'user_id\', \'value\') expected_output = pd.DataFrame({ \'user_id\': [\'A\', \'B\', \'C\'], \'value\': [20.0, 35.0, 50.0] }) pd.testing.assert_frame_equal(result, expected_output) ``` **Additional Requirements:** 1. **Error Handling**: If the `group_by_column` or `agg_column` does not exist in the DataFrame, raise a `KeyError` with an appropriate message. 2. **Validation**: Use pandas\' `pd.testing.assert_frame_equal` to validate that the output DataFrame matches the expected output in your test cases. **Constraints:** 1. You must use pandas\' DataFrame and Series operations to achieve the solution. 2. Your solution should handle scenarios where the input DataFrame is empty gracefully, returning an empty DataFrame with the same structure. **Performance Requirements:** The function should handle reasonably large DataFrames efficiently, taking O(n) time where n is the number of rows in the DataFrame. **Test Your Solution:** Use the provided example to test the correctness of your implementation. Additionally, create at least two more test cases to verify different scenarios, including error handling and empty DataFrame inputs. **Hints:** - Consider using `pd.DataFrame.groupby()` and `pd.DataFrame.agg()` to perform the aggregation. - Validate the existence of the specified columns before performing operations to ensure robust error handling. - Use pandas testing functions to assert the equivalence of DataFrames in your unit tests.","solution":"import pandas as pd def aggregate_and_validate(df: pd.DataFrame, group_by_column: str, agg_column: str) -> pd.DataFrame: This function groups the input DataFrame `df` by the column `group_by_column` and performs an aggregation using the column `agg_column` to compute the mean. Parameters: df (pd.DataFrame): The input DataFrame. group_by_column (str): The column name to group by. agg_column (str): The column name to aggregate. Returns: pd.DataFrame: A new DataFrame with the group_by_column and the corresponding aggregated mean values. if group_by_column not in df.columns: raise KeyError(f\\"Column \'{group_by_column}\' does not exist in DataFrame\\") if agg_column not in df.columns: raise KeyError(f\\"Column \'{agg_column}\' does not exist in DataFrame\\") # Perform the group by and aggregation aggregated_df = df.groupby(group_by_column)[agg_column].mean().reset_index() # Sort the result by group_by_column aggregated_df = aggregated_df.sort_values(by=group_by_column).reset_index(drop=True) return aggregated_df"},{"question":"# Question: UUID Classification and Generation You are tasked to implement a function that generates multiple UUIDs of different versions and classifies them based on their characteristics. Using the `uuid` module in Python, perform the following: 1. **Generate UUIDs**: - Generate `N` UUIDs of each type: UUID1, UUID3 (based on DNS namespace and a given name), UUID4, and UUID5 (based on DNS namespace and a given name). 2. **Classify UUIDs**: - Classify the above generated UUIDs into dictionaries based on their version and whether they are safe or not. - The classification should also include conversion of UUIDs to their hex, int, and URN representations. 3. **Return Results**: - Return a dictionary with the following structure: ``` { \\"UUID1\\": [ { \\"uuid\\": <uuid object>, \\"hex\\": \\"<hex representation>\\", \\"int\\": <integer representation>, \\"urn\\": \\"<urn representation>\\", \\"is_safe\\": \\"<safety status>\\" }, ... ], \\"UUID3\\": [ ... ], \\"UUID4\\": [ ... ], \\"UUID5\\": [ ... ] } ``` **Function Signature**: ```python import uuid def generate_and_classify_uuids(N: int, name: str) -> dict: pass ``` # Input: - `N` (int): Number of UUIDs of each type to generate. (1  N  100) - `name` (str): A name to be used for generating UUID3 and UUID5. The name should be a valid DNS name. # Output: - A dictionary as described above. # Example: ```python import uuid def generate_and_classify_uuids(N: int, name: str) -> dict: classification = {\'UUID1\': [], \'UUID3\': [], \'UUID4\': [], \'UUID5\': []} for _ in range(N): # Generate UUID1 uuid1 = uuid.uuid1() classification[\'UUID1\'].append({ \'uuid\': uuid1, \'hex\': uuid1.hex, \'int\': uuid1.int, \'urn\': uuid1.urn, \'is_safe\': uuid1.is_safe }) # Generate UUID3 uuid3 = uuid.uuid3(uuid.NAMESPACE_DNS, name) classification[\'UUID3\'].append({ \'uuid\': uuid3, \'hex\': uuid3.hex, \'int\': uuid3.int, \'urn\': uuid3.urn, \'is_safe\': uuid3.is_safe }) # Generate UUID4 uuid4 = uuid.uuid4() classification[\'UUID4\'].append({ \'uuid\': uuid4, \'hex\': uuid4.hex, \'int\': uuid4.int, \'urn\': uuid4.urn, \'is_safe\': uuid4.is_safe }) # Generate UUID5 uuid5 = uuid.uuid5(uuid.NAMESPACE_DNS, name) classification[\'UUID5\'].append({ \'uuid\': uuid5, \'hex\': uuid5.hex, \'int\': uuid5.int, \'urn\': uuid5.urn, \'is_safe\': uuid5.is_safe }) return classification ``` ```python # Example usage: result = generate_and_classify_uuids(2, \\"example.com\\") print(result) ``` Guidelines: - Ensure you understand the usage of `uuid` module methods like `uuid1()`, `uuid3()`, `uuid4()`, `uuid5()`, as well as the UUID object attributes. - Think about the potential edge cases, such as the safety of the UUIDs on different platforms when calling `uuid1()`.","solution":"import uuid def generate_and_classify_uuids(N: int, name: str) -> dict: classification = {\'UUID1\': [], \'UUID3\': [], \'UUID4\': [], \'UUID5\': []} for _ in range(N): # Generate UUID1 uuid1 = uuid.uuid1() classification[\'UUID1\'].append({ \'uuid\': uuid1, \'hex\': uuid1.hex, \'int\': uuid1.int, \'urn\': uuid1.urn, \'is_safe\': uuid1.is_safe }) # Generate UUID3 uuid3 = uuid.uuid3(uuid.NAMESPACE_DNS, name) classification[\'UUID3\'].append({ \'uuid\': uuid3, \'hex\': uuid3.hex, \'int\': uuid3.int, \'urn\': uuid3.urn, \'is_safe\': uuid3.is_safe }) # Generate UUID4 uuid4 = uuid.uuid4() classification[\'UUID4\'].append({ \'uuid\': uuid4, \'hex\': uuid4.hex, \'int\': uuid4.int, \'urn\': uuid4.urn, \'is_safe\': None }) # Generate UUID5 uuid5 = uuid.uuid5(uuid.NAMESPACE_DNS, name) classification[\'UUID5\'].append({ \'uuid\': uuid5, \'hex\': uuid5.hex, \'int\': uuid5.int, \'urn\': uuid5.urn, \'is_safe\': uuid5.is_safe }) return classification"},{"question":"# Custom Telnet Client Implementation You are to create a custom Telnet client class that can connect to a Telnet server, send commands, and process the responses. Your client should be capable of executing multiple commands sequentially and handling the output in a structured manner. Your task: 1. Implement a class `CustomTelnetClient` with the following methods: - `__init__(self, host: str, port: int = 23, timeout: float = None)`: Initializes the client with the server details. - `connect(self) -> None`: Connects to the Telnet server. - `execute_command(self, command: str) -> str`: Sends a command to the Telnet server and returns the result as a string. - `close(self) -> None`: Closes the connection to the Telnet server. 2. Your client should handle the following: - Proper connection and disconnection from the Telnet server. - Sending of multiple commands. - Reading and decoding responses correctly. - Handling `EOFErrors` and connection errors gracefully. - Using context manager support to ensure the connection is properly closed. 3. Assume that commands will be executed in the order they are called. Each command result should be returned as a decoded string. Example Usage: ```python from custom_telnet_client import CustomTelnetClient commands = [\\"ls\\", \\"pwd\\", \\"whoami\\"] with CustomTelnetClient(\\"localhost\\") as client: client.connect() for cmd in commands: result = client.execute_command(cmd) print(f\\"Command: {cmd}nResult: {result}n\\") ``` Constraints: - You may assume that the server responds to the commands in a timely manner. - You must handle text encoding properly (UTF-8). - Any extra input details or error messages provided by the server need to be managed appropriately. Performance Requirements: - The client should be able to handle large output responses efficiently without excessive memory usage. - The client should work reliably within a reasonable timeout for connections and read operations. Testing: Prepare your class with both unit tests and integration tests to validate its functionality. The tests should cover normal operation, error handling, and edge cases such as large responses and connection drops. Develop your code in a well-structured and modular fashion to ensure readability and maintainability.","solution":"import telnetlib import socket class CustomTelnetClient: def __init__(self, host: str, port: int = 23, timeout: float = None): self.host = host self.port = port self.timeout = timeout self.telnet = None def __enter__(self): self.connect() return self def __exit__(self, exc_type, exc_val, exc_tb): self.close() def connect(self) -> None: try: self.telnet = telnetlib.Telnet(self.host, self.port, self.timeout) except socket.error as e: print(f\\"Failed to connect to {self.host}:{self.port} - {e}\\") self.telnet = None def execute_command(self, command: str) -> str: if self.telnet is None: raise ConnectionError(\\"Not connected to a Telnet server.\\") try: self.telnet.write(command.encode(\'utf-8\') + b\\"n\\") response = self.telnet.read_all().decode(\'utf-8\') return response except EOFError: self.close() raise ConnectionError(\\"Connection closed by the server.\\") def close(self) -> None: if self.telnet is not None: self.telnet.close() self.telnet = None"},{"question":"Kernel Approximation in Scikit-learn # Objective Implement a kernel approximation pipeline using the `Nystroem` and `RBFSampler` techniques to demonstrate their usage and performance on a classification task. You will compare the results of these kernel approximations with a linear Support Vector Machine (SVM) classifier. # Task 1. **Data Loading and Preparation**: - Load the `digits` dataset from `sklearn.datasets`. - Split the data into training and testing sets using an 80-20 split. 2. **Nystroem Kernel Approximation**: - Use the `Nystroem` class to transform your input data. - Train an `SGDClassifier` on the transformed data. - Make predictions on the test set and calculate accuracy. 3. **RBFSampler Kernel Approximation**: - Use the `RBFSampler` class to transform your input data. - Train an `SGDClassifier` on the transformed data. - Make predictions on the test set and calculate accuracy. 4. **Linear SVM (Baseline)**: - Train a `LinearSVC` classifier directly on the original input data. - Make predictions on the test set and calculate accuracy. 5. **Comparison**: - Compare the classification accuracies of the Nystroem method, RBFSampler, and the Linear SVM. Discuss the results. # Implementation Your implementation should include: - Function `load_and_split_data()` to load the `digits` dataset and split it into training and testing sets. - Function `nystroem_approximation(X_train, X_test, y_train)` to perform the Nystroem kernel approximation and return predicted labels and accuracy. - Function `rbf_approximation(X_train, X_test, y_train)` to perform the RBF kernel approximation and return predicted labels and accuracy. - Function `linear_svm(X_train, X_test, y_train)` to train and evaluate a linear SVM classifier. - A main function to call the above functions and print the comparison results. # Constraints - You should use `SGDClassifier` for training on transformed data for both kernel approximations. - Use default parameters for all models and transformations unless specified. - Use `random_state=42` for reproducibility. # Input None. The functions should load and use the datasets internally. # Output Print the accuracy for each method (Nystroem, RBFSampler, and Linear SVM). Additionally, print a brief comparison of the results. # Example Code Template ```python from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.kernel_approximation import Nystroem, RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.svm import LinearSVC from sklearn.metrics import accuracy_score def load_and_split_data(): data = load_digits() X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def nystroem_approximation(X_train, X_test, y_train): nystroem = Nystroem(gamma=0.2, random_state=42) X_train_transformed = nystroem.fit_transform(X_train) X_test_transformed = nystroem.transform(X_test) sgd_clf = SGDClassifier(random_state=42) sgd_clf.fit(X_train_transformed, y_train) y_pred = sgd_clf.predict(X_test_transformed) return accuracy_score(y_test, y_pred), y_pred def rbf_approximation(X_train, X_test, y_train): rbf_sampler = RBFSampler(gamma=1, random_state=42) X_train_transformed = rbf_sampler.fit_transform(X_train) X_test_transformed = rbf_sampler.transform(X_test) sgd_clf = SGDClassifier(random_state=42) sgd_clf.fit(X_train_transformed, y_train) y_pred = sgd_clf.predict(X_test_transformed) return accuracy_score(y_test, y_pred), y_pred def linear_svm(X_train, X_test, y_train): linear_svc = LinearSVC(random_state=42) linear_svc.fit(X_train, y_train) y_pred = linear_svc.predict(X_test) return accuracy_score(y_test, y_pred), y_pred def main(): X_train, X_test, y_train, y_test = load_and_split_data() nystroem_acc, _ = nystroem_approximation(X_train, X_test, y_train) rbf_acc, _ = rbf_approximation(X_train, X_test, y_train) linear_svm_acc, _ = linear_svm(X_train, X_test, y_train) print(f\\"Nystroem Kernel Approximation Accuracy: {nystroem_acc}\\") print(f\\"RBF Kernel Approximation Accuracy: {rbf_acc}\\") print(f\\"Linear SVM Accuracy: {linear_svm_acc}\\") print(\\"Comparison:\\") print(f\\"Nystroem vs RBF: {\'Nystroem is better\' if nystroem_acc > rbf_acc else \'RBF is better\'}\\") print(f\\"Nystroem vs Linear SVM: {\'Nystroem is better\' if nystroem_acc > linear_svm_acc else \'Linear SVM is better\'}\\") print(f\\"RBF vs Linear SVM: {\'RBF is better\' if rbf_acc > linear_svm_acc else \'Linear SVM is better\'}\\") if __name__ == \\"__main__\\": main() ```","solution":"from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.kernel_approximation import Nystroem, RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.svm import LinearSVC from sklearn.metrics import accuracy_score def load_and_split_data(): data = load_digits() X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def nystroem_approximation(X_train, X_test, y_train, y_test): nystroem = Nystroem(gamma=0.2, random_state=42) X_train_transformed = nystroem.fit_transform(X_train) X_test_transformed = nystroem.transform(X_test) sgd_clf = SGDClassifier(random_state=42) sgd_clf.fit(X_train_transformed, y_train) y_pred = sgd_clf.predict(X_test_transformed) return accuracy_score(y_test, y_pred), y_pred def rbf_approximation(X_train, X_test, y_train, y_test): rbf_sampler = RBFSampler(gamma=1, random_state=42) X_train_transformed = rbf_sampler.fit_transform(X_train) X_test_transformed = rbf_sampler.transform(X_test) sgd_clf = SGDClassifier(random_state=42) sgd_clf.fit(X_train_transformed, y_train) y_pred = sgd_clf.predict(X_test_transformed) return accuracy_score(y_test, y_pred), y_pred def linear_svm(X_train, X_test, y_train, y_test): linear_svc = LinearSVC(random_state=42) linear_svc.fit(X_train, y_train) y_pred = linear_svc.predict(X_test) return accuracy_score(y_test, y_pred), y_pred def main(): X_train, X_test, y_train, y_test = load_and_split_data() nystroem_acc, _ = nystroem_approximation(X_train, X_test, y_train, y_test) rbf_acc, _ = rbf_approximation(X_train, X_test, y_train, y_test) linear_svm_acc, _ = linear_svm(X_train, X_test, y_train, y_test) print(f\\"Nystroem Kernel Approximation Accuracy: {nystroem_acc}\\") print(f\\"RBF Kernel Approximation Accuracy: {rbf_acc}\\") print(f\\"Linear SVM Accuracy: {linear_svm_acc}\\") print(\\"Comparison:\\") print(f\\"Nystroem vs RBF: {\'Nystroem is better\' if nystroem_acc > rbf_acc else \'RBF is better\'}\\") print(f\\"Nystroem vs Linear SVM: {\'Nystroem is better\' if nystroem_acc > linear_svm_acc else \'Linear SVM is better\'}\\") print(f\\"RBF vs Linear SVM: {\'RBF is better\' if rbf_acc > linear_svm_acc else \'Linear SVM is better\'}\\") if __name__ == \\"__main__\\": main()"},{"question":"**Objective**: Write a PyTorch program that demonstrates efficient device management and GPU memory monitoring using HIP semantics. Problem Statement You are given a task to implement a neural network training pipeline on an AMD GPU using PyTorch with HIP semantics. Your program should: 1. Allocate tensors to a specific GPU device. 2. Perform tensor operations and transfers between CPU and GPU. 3. Monitor and output memory usage statistics before and after tensor operations. 4. Release the unused cached memory and demonstrate the effect on memory statistics. Requirements 1. **Device Allocation**: - Define a function `initialize_device` that returns a PyTorch device object for the specified GPU index. This function should accept an integer `gpu_index` as input. 2. **Tensor Operations**: - Define another function `tensor_operations` which: - Creates tensors on the GPU and CPU. - Transfers tensors between CPU and GPU. - Performs basic tensor operations (e.g., addition, multiplication). 3. **Memory Monitoring**: - Before and after the `tensor_operations` function call, use: - `torch.cuda.memory_allocated` to print the amount of memory allocated. - `torch.cuda.memory_reserved` to print the total amount of memory managed by the caching allocator. 4. **Memory Management**: - Use the method `torch.cuda.empty_cache` to release unused cached memory. - Monitor and print memory statistics again after releasing the cache. Inputs - `gpu_index`: an integer representing the GPU device index. - `tensor_size`: an integer representing the size of the tensors to be created. Outputs - Prints the memory statistics (allocated, reserved) before and after tensor operations, as well as after releasing the cache. Constraints - Assume a system with at least one compatible AMD GPU. - Ensure the program handles cases where `gpu_index` might be invalid (e.g., CUDA not available, GPU index out of range). Example Usage ```python gpu_index = 0 tensor_size = 100000 # Initialize device device = initialize_device(gpu_index) print(f\\"Using device: {device}\\") print(\\"Memory statistics before operations:\\") print_memory_statistics() # Perform tensor operations tensor_operations(device, tensor_size) print(\\"Memory statistics after operations:\\") print_memory_statistics() # Release unused cached memory torch.cuda.empty_cache() print(\\"Memory statistics after emptying cache:\\") print_memory_statistics() ``` Function Signatures ```python def initialize_device(gpu_index: int) -> torch.device: pass def tensor_operations(device: torch.device, size: int) -> None: pass def print_memory_statistics() -> None: pass ``` Note Use the PyTorch HIP interfaces shown in the documentation and ensure the program is compatible with both CUDA and HIP if possible.","solution":"import torch def initialize_device(gpu_index: int) -> torch.device: Initializes and returns a PyTorch device object for the given GPU index. if not torch.cuda.is_available(): raise RuntimeError(\\"CUDA is not available.\\") if gpu_index < 0 or gpu_index >= torch.cuda.device_count(): raise ValueError(f\\"Invalid GPU index {gpu_index}. Only {torch.cuda.device_count()} GPUs available.\\") return torch.device(f\'cuda:{gpu_index}\') def tensor_operations(device: torch.device, size: int) -> None: Performs tensor operations including tensor creation, transfers, and basic operations. # Create a tensor on the GPU tensor_gpu = torch.ones(size, device=device) # Transfer tensor to CPU tensor_cpu = tensor_gpu.to(\'cpu\') # Perform basic operations tensor_result = tensor_cpu + tensor_cpu # Transfer result back to GPU tensor_gpu_result = tensor_result.to(device) # Perform more operations on GPU tensor_final_result = tensor_gpu_result * 2 def print_memory_statistics() -> None: Prints current memory statistics: allocated and reserved GPU memory. allocated = torch.cuda.memory_allocated() reserved = torch.cuda.memory_reserved() print(f\\"Memory allocated: {allocated} bytes\\") print(f\\"Memory reserved: {reserved} bytes\\")"},{"question":"You are provided with legacy code that uses the `asyncore` module to create a basic HTTP client. However, since `asyncore` is deprecated, you need to transition this code to use the `asyncio` module instead. Your task is to rewrite the following legacy code using `asyncio` to achieve the same functionality. Legacy Code using `asyncore`: ```python import asyncore class HTTPClient(asyncore.dispatcher): def __init__(self, host, path): asyncore.dispatcher.__init__(self) self.create_socket() self.connect((host, 80)) self.buffer = bytes(\'GET %s HTTP/1.0rnHost: %srnrn\' % (path, host), \'ascii\') def handle_connect(self): pass def handle_close(self): self.close() def handle_read(self): print(self.recv(8192)) def writable(self): return (len(self.buffer) > 0) def handle_write(self): sent = self.send(self.buffer) self.buffer = self.buffer[sent:] client = HTTPClient(\'www.python.org\', \'/\') asyncore.loop() ``` **Objective**: Write an `asyncio`-based HTTP client that performs the same operation as the `asyncore`-based client provided above. **Inputs and Outputs**: - **Input**: Hostname (string), Path (string). - **Output**: The HTTP response from the server should be printed to the console. # Requirements: 1. Use `asyncio` to manage asynchronous operations. 2. Implement proper exception handling. 3. Ensure the connection is properly closed after the HTTP request completes. **Constraints**: - The hostname will always be a valid URL (e.g., \'www.python.org\'). - The path will always be a valid HTTP path (e.g., \'/\'). - The connection should timeout after 30 seconds if no response is received. **Performance**: - The code should efficiently handle the HTTP request and response. ```python import asyncio async def http_client(host, path): # Implementation goes here # Replace \'www.python.org\' and \'/\' with input values as needed for testing. asyncio.run(http_client(\'www.python.org\', \'/\')) ``` **Note**: You are required to fill in the implementation details for the `http_client` function.","solution":"import asyncio async def http_client(host, path): reader, writer = await asyncio.open_connection(host, 80) request_header = f\'GET {path} HTTP/1.0rnHost: {host}rnrn\' writer.write(request_header.encode(\'ascii\')) await writer.drain() response = await reader.read(-1) print(response.decode(\'utf-8\')) writer.close() await writer.wait_closed()"},{"question":"Objective Demonstrate your understanding of creating and managing instance methods and method objects in Python, using the provided descriptions of the `python310` package functionalities. Task You are to implement a Python class called `MethodManager` that provides the following functionalities: 1. **Instance Method Operations:** - **Check Instance Method:** Implement a method `is_instance_method(obj) -> bool` that checks if the given object `obj` is an instance method object. - **Create Instance Method:** Implement a method `create_instance_method(func: callable) -> callable` that takes a callable object `func` and returns an instance method object wrapping `func`. - **Get Instance Method Function:** Implement a method `get_instance_method_function(im: callable) -> callable` that retrieves the function associated with an instance method object `im`. 2. **Method Operations:** - **Check Method:** Implement a method `is_method(obj) -> bool` that checks if the given object `obj` is a method object. - **Create Method:** Implement a method `create_method(func: callable, self: object) -> callable` that takes a callable object `func` and an instance `self`, and returns a method object wrapping `func` bound to `self`. - **Get Method Function:** Implement a method `get_method_function(meth: callable) -> callable` that retrieves the function associated with a method object `meth`. - **Get Method Self:** Implement a method `get_method_self(meth: callable) -> object` that retrieves the instance associated with a method object `meth`. Input and Output Formats - Each method takes in the specified parameters and returns the appropriate boolean or callable object as needed. - Assume that all inputs are valid objects and callables where specified. - Performance constraints are minimal; the focus is on correctly using method and instance method mechanisms. Constraints and Limitations - Do not use any libraries or functions outside the standard Python library. - Implementations should directly relate to understanding and emulating the behaviors described in the provided documentation. ```python class MethodManager: def is_instance_method(self, obj) -> bool: # Implementation here pass def create_instance_method(self, func: callable) -> callable: # Implementation here pass def get_instance_method_function(self, im: callable) -> callable: # Implementation here pass def is_method(self, obj) -> bool: # Implementation here pass def create_method(self, func: callable, self_: object) -> callable: # Implementation here pass def get_method_function(self, meth: callable) -> callable: # Implementation here pass def get_method_self(self, meth: callable) -> object: # Implementation here pass # Example usage: # mm = MethodManager() # assert mm.is_instance_method(instance_method_obj) == True # assert mm.create_instance_method(some_callable) == <instance method object> # assert mm.get_instance_method_function(instance_method_obj) == some_callable # assert mm.is_method(method_obj) == True # assert mm.create_method(some_callable, some_instance) == <method object> # assert mm.get_method_function(method_obj) == some_callable # assert mm.get_method_self(method_obj) == some_instance ```","solution":"import types class MethodManager: def is_instance_method(self, obj) -> bool: Check if the given object is an instance method object. return isinstance(obj, types.MethodType) and obj.__self__ is not None def create_instance_method(self, func: callable) -> callable: Create an instance method from a given callable. class Dummy: pass dummy = Dummy() return types.MethodType(func, dummy) def get_instance_method_function(self, im: callable) -> callable: Retrieve the function associated with an instance method. return im.__func__ def is_method(self, obj) -> bool: Check if the given object is a method object. return isinstance(obj, types.MethodType) def create_method(self, func: callable, self_: object) -> callable: Create a method from a given function and instance. return types.MethodType(func, self_) def get_method_function(self, meth: callable) -> callable: Retrieve the function associated with a method. return meth.__func__ def get_method_self(self, meth: callable) -> object: Retrieve the instance associated with a method. return meth.__self__"},{"question":"**Objective:** The goal of this exercise is to assess your understanding of secure coding practices in Python, specifically focusing on avoiding common security pitfalls as indicated in the python310 documentation. **Problem Description:** You are tasked with creating a small utility that safely handles user inputs and file operations while avoiding insecure practices. Your implementation will include two functions: 1. `safe_base64_decode(data: str) -> bytes` 2. `safe_extract_zip(zip_path: str, extract_to: str) -> None` **Function Details:** 1. **Function:** `safe_base64_decode(data: str) -> bytes` **Description:** This function takes a base64 encoded string and decodes it securely. Ensure that the input is validated properly to avoid any potential security risks associated with incorrect data handling. **Input:** - `data` (str): A base64 encoded string. **Output:** - A bytes object that is the result of decoding the base64 encoded input. **Constraints:** - The function should raise a `ValueError` if the input data is not a valid base64 encoded string. - Use the `base64` module for decoding. 2. **Function:** `safe_extract_zip(zip_path: str, extract_to: str) -> None` **Description:** This function extracts the contents of a zip file to a specified directory securely. The function must ensure that it does not extract files to unintended locations which might be outside the target directory. **Input:** - `zip_path` (str): The path to the zip file. - `extract_to` (str): The directory path where the contents should be extracted. **Output:** - None. The function performs extraction as a side effect. **Constraints:** - The function should raise a `FileNotFoundError` if the zip file does not exist. - The function must prevent directory traversal attacks by ensuring files are only extracted within `extract_to`. - Use the `zipfile` module for extracting files. **Example Usage:** ```python # Example usage of safe_base64_decode try: decoded_data = safe_base64_decode(\\"SGVsbG8sIFdvcmxkIQ==\\") print(decoded_data) # Output: b\'Hello, World!\' except ValueError as e: print(e) # Example usage of safe_extract_zip try: safe_extract_zip(\\"example.zip\\", \\"/safe/extract/path\\") print(\\"Extraction successful.\\") except FileNotFoundError as e: print(e) except RuntimeError as e: print(e) ``` **Notes:** - Your solution should make sure to handle exceptions gracefully and report any errors in a user-friendly manner. - Pay attention to security considerations and avoid using deprecated or insecure methods.","solution":"import base64 import zipfile import os def safe_base64_decode(data: str) -> bytes: Decodes a base64 encoded string. Raises ValueError if the input is not properly base64 encoded. try: decoded_data = base64.b64decode(data, validate=True) return decoded_data except (base64.binascii.Error, ValueError): raise ValueError(\\"Invalid base64 encoded data\\") def safe_extract_zip(zip_path: str, extract_to: str) -> None: Extracts contents of a zip file to a specified directory securely. Prevents directory traversal attacks by ensuring files are only extracted within the target directory. if not os.path.isfile(zip_path): raise FileNotFoundError(f\\"The file {zip_path} does not exist.\\") with zipfile.ZipFile(zip_path, \'r\') as zf: for member in zf.infolist(): member_path = os.path.join(extract_to, member.filename) if not os.path.abspath(member_path).startswith(os.path.abspath(extract_to)): raise RuntimeError(\\"Attempted directory traversal detected in zip file\\") zf.extractall(extract_to)"},{"question":"**Question: Implement a Custom Weighted K-Nearest Neighbors Classifier** Within the `scikit-learn` library, the `KNeighborsClassifier` allows for customization of the k-nearest neighbors algorithm to classify data points based on the classifications of nearby points. In this task, you will implement a custom K-Nearest Neighbors classifier that uses custom distance-based weighting for classification. # Requirements: 1. **Inputs**: - `X_train`: A numpy array of shape `(n_samples, n_features)` containing the training data. - `y_train`: A numpy array of shape `(n_samples,)` containing the class labels for the training data. - `X_test`: A numpy array of shape `(m_samples, n_features)` containing the test data for which to predict the class labels. - `k`: The number of nearest neighbors to consider. - `distance_metric`: The distance metric to use. Should accept `\'euclidean\'`, `\'manhattan\'`, and `\'minkowski\'` (default: `\'euclidean\'`). - `weights`: A function that, given a distance, outputs a weight. (Default: inverse distance). 2. **Outputs**: - `y_pred`: A numpy array of shape `(m_samples,)` containing the predicted class labels for the test data. # Constraints: - You must not use `KNeighborsClassifier` or any direct built-in classifier from `scikit-learn` for this task. You can use the `KDTree` or `BallTree` for efficient neighbor searches. - For `minkowski` distance, you must parameterize it to work similarly to the `p` parameter in `sklearn`. # Implementation Details: 1. Develop a class named `CustomKNNClassifier` with the following methods: - `__init__(self, k=5, distance_metric=\'euclidean\', p=2, weights=lambda dist: 1 / (dist + 1e-9))` - `fit(self, X_train, y_train)` - `predict(self, X_test)` 2. **Method Details**: - `__init__`: Initializes the parameters. - `fit`: Stores the training data. - `predict`: Implements the logic to find the k-nearest neighbors, apply the distance metric, calculate weights, and determine the predicted class based on weighted voting. # Example Usage: ```python import numpy as np X_train = np.array([[1, 2], [2, 3], [3, 4], [5, 6], [6, 7]]) y_train = np.array([0, 0, 1, 1, 1]) X_test = np.array([[1, 1], [6, 6]]) def custom_weight(dist): return 1 / (dist + 0.01) knn = CustomKNNClassifier(k=3, distance_metric=\'euclidean\', weights=custom_weight) knn.fit(X_train, y_train) predictions = knn.predict(X_test) print(predictions) # Expected output: array with predicted class labels ``` Your task is to implement the `CustomKNNClassifier` class according to the specifications provided.","solution":"import numpy as np from scipy.spatial import distance class CustomKNNClassifier: def __init__(self, k=5, distance_metric=\'euclidean\', p=2, weights=lambda dist: 1 / (dist + 1e-9)): self.k = k self.distance_metric = distance_metric self.p = p self.weights = weights self.X_train = None self.y_train = None def fit(self, X_train, y_train): self.X_train = X_train self.y_train = y_train def predict(self, X_test): y_pred = [] for x_test in X_test: distances = self._compute_distances(x_test) neighbors_indices = np.argsort(distances)[:self.k] neighbors_labels = self.y_train[neighbors_indices] neighbors_distances = distances[neighbors_indices] weights = self.weights(neighbors_distances) weighted_votes = {} for label, weight in zip(neighbors_labels, weights): if label in weighted_votes: weighted_votes[label] += weight else: weighted_votes[label] = weight y_pred.append(max(weighted_votes, key=weighted_votes.get)) return np.array(y_pred) def _compute_distances(self, x_test): if self.distance_metric == \'euclidean\': return np.linalg.norm(self.X_train - x_test, axis=1) elif self.distance_metric == \'manhattan\': return np.sum(np.abs(self.X_train - x_test), axis=1) elif self.distance_metric == \'minkowski\': return np.sum(np.abs(self.X_train - x_test) ** self.p, axis=1) ** (1 / self.p) else: raise ValueError(\\"Unsupported distance metric\\")"},{"question":"# Question: Implementing a Tkinter Message Box Application **Objective:** Your task is to implement a function using the `tkinter` library which utilizes message boxes from the `tkinter.messagebox` module. This mini-application will simulate a simple interaction workflow using different types of message boxes to handle user input and display appropriate messages. **Requirements:** 1. Create a GUI application window using `tkinter`. 2. Inside this window, implement a button that triggers the following sequence of interactions: - Display an information message to begin the process. - Ask the user a question and continue based on the response (`askyesno`). - If the response is \\"Yes\\", show a warning message. - If the response is \\"No\\", show an error message. 3. Each message box must have appropriate titles and messages. **Function Signature:** ```python def run_messagebox_app(): pass ``` **Input:** - No direct input to the function. The function should create an interactive GUI window. **Output:** - No direct output from the function. The result should be the interactive sequence of message boxes displayed. **Constraints:** - The application should be designed using Python 3.10 or above. - Ensure the GUI is user-friendly and the messages are clear. # Example Workflow: 1. User clicks the button. 2. Information message box: \\"Welcome to the Tkinter Message Box Application!\\" ```python tkinter.messagebox.showinfo(title=\\"Info\\", message=\\"Welcome to the Tkinter Message Box Application!\\") ``` 3. Question message box: \\"Do you want to proceed?\\" ```python response = tkinter.messagebox.askyesno(title=\\"Question\\", message=\\"Do you want to proceed?\\") ``` 4. Based on the response: - If `Yes`: Warning message box: \\"You chose to proceed.\\" ```python tkinter.messagebox.showwarning(title=\\"Warning\\", message=\\"You chose to proceed.\\") ``` - If `No`: Error message box: \\"You chose not to proceed.\\" ```python tkinter.messagebox.showerror(title=\\"Error\\", message=\\"You chose not to proceed.\\") ``` Implement the `run_messagebox_app` function that provides this interaction sequence. # Additional Notes: - Make sure to import the required libraries at the beginning of your script. - When testing, ensure that the message boxes behave as expected based on user input. - Remember to run the application\'s main loop correctly to display the window.","solution":"import tkinter as tk from tkinter import messagebox def on_button_click(): messagebox.showinfo(title=\\"Info\\", message=\\"Welcome to the Tkinter Message Box Application!\\") response = messagebox.askyesno(title=\\"Question\\", message=\\"Do you want to proceed?\\") if response: messagebox.showwarning(title=\\"Warning\\", message=\\"You chose to proceed.\\") else: messagebox.showerror(title=\\"Error\\", message=\\"You chose not to proceed.\\") def run_messagebox_app(): root = tk.Tk() root.title(\\"Tkinter Message Box Demo\\") root.geometry(\\"300x150\\") # Set window size button = tk.Button(root, text=\\"Click Me\\", command=on_button_click) button.pack(pady=20) root.mainloop()"},{"question":"**Background:** You are tasked with creating a log processing system. The logs are provided in various formats, and your function needs to be able to handle these formats using Python\'s pattern matching features. Furthermore, some logs might contain errors, and your function should handle these gracefully using exception handling. Finally, processing these logs should be done asynchronously to improve performance. **Problem Statement:** Write an asynchronous function `process_logs` that accepts a list of log entries. Each log entry can be in one of the following formats: 1. A dictionary with a `type` key that indicates the log type and a `content` key with the log message. 2. A tuple where the first element is the log type and the second element is the log message. 3. A string that represents a log message of type \\"info\\". Your function should: 1. Process each log entry asynchronously. 2. Use pattern matching with the `match` statement to identify the format of the log entry. 3. Use a `try-except` block to handle any logs that raise an exception during processing and store these in an `error_logs` list. 4. Use an `async with` context manager to simulate writing processed log entries to a file (you can use a dummy context manager for this purpose). **Input:** - A list of log entries where each log entry is either a dictionary, a tuple, or a string. **Output:** - A dictionary with two keys: - `processed_logs`: A list of processed log messages. - `error_logs`: A list of log entries that raised exceptions during processing. **Example:** ```python import asyncio async def process_logs(log_entries): # Your implementation here # Example usage log_entries = [ {\'type\': \'error\', \'content\': \'Failed to connect to server\'}, (\'warning\', \'Low disk space\'), \'User logged in\' ] result = asyncio.run(process_logs(log_entries)) print(result) ``` **Constraints:** - You should use the `match` statement for pattern matching log entries. - The function should handle different types of log entries as stated and ensure processing is done asynchronously. - Proper exception handling should be in place to catch and record any errors during log processing. **Hints:** - You can create a dummy asynchronous context manager using `asyncio.sleep` to simulate file operations. - Remember to handle different types of log entries appropriately within the `match` statement. - Make sure your function handles exceptions and still continues processing the remaining log entries.","solution":"import asyncio class DummyAsyncContextManager: async def __aenter__(self): await asyncio.sleep(0) # Simulate some delay return self async def __aexit__(self, exc_type, exc, tb): await asyncio.sleep(0) # Simulate some delay async def process_log_entry(log_entry): try: match log_entry: case {\'type\': log_type, \'content\': log_message}: processed_message = f\\"[{log_type.upper()}] {log_message}\\" case (log_type, log_message): processed_message = f\\"[{log_type.upper()}] {log_message}\\" case str(log_message): processed_message = f\\"[INFO] {log_message}\\" case _: raise ValueError(f\\"Unknown log entry format: {log_entry}\\") async with DummyAsyncContextManager(): # Simulate writing to a file await asyncio.sleep(0) return processed_message except Exception as e: raise e async def process_logs(log_entries): processed_logs = [] error_logs = [] async def process_entry(entry): try: result = await process_log_entry(entry) processed_logs.append(result) except Exception as e: error_logs.append(str(entry)) await asyncio.gather(*(process_entry(entry) for entry in log_entries)) return { \'processed_logs\': processed_logs, \'error_logs\': error_logs }"},{"question":"Objective Implement a function that sets up customized paths for Python packages by manipulating the user site-packages and prefix directories. Your implementation should check existing paths, add new paths, and write a `.pth` file in the user site-packages to ensure these paths are recognized on Python startup. Background Path configuration files (`.pth`) are used by Python to add directories to `sys.path`. These files can contain paths to be added (one per line) and optionally executable Python code. The `site` module provides functions to retrieve and manipulate user and global site-packages directory paths. Function Signature ```python def setup_custom_python_paths(custom_paths: List[str]) -> bool: Adds custom paths to the user site-packages directory and verifies the addition. Parameters: - custom_paths : List[str] : A list of directory paths to be added to the user site-packages. Returns: - bool : True if all paths are successfully added, False otherwise. ``` Requirements 1. **Path Verification**: Check if each path in `custom_paths` exists. If a path does not exist, ignore it and do not add it. 2. **Add Paths**: Write the existing paths to a `.pth` file in the user site-packages directory. The `.pth` file should be named `custom_paths.pth`. 3. **Path Addition Validation**: Verify that the paths written to the `.pth` file are successfully added to `sys.path`. 4. **Return Boolean**: The function should return `True` if all valid paths are successfully added, otherwise return `False`. Constraints - Use the functions from the `site` module to get the user site-packages directory. - The solution should handle multiple runs gracefully (i.e., idempotent writes). Example ```python custom_paths = [\\"/path/to/dir1\\", \\"/path/to/dir2\\"] result = setup_custom_python_paths(custom_paths) print(result) # Output should be True if both paths were added, False otherwise. ``` Notes - Ensure that the solution includes handling of import statements and any required module customization. - The code should be written considering best practices for file operations (e.g., using `with` for file handling).","solution":"import os import site import sys from typing import List def setup_custom_python_paths(custom_paths: List[str]) -> bool: Adds custom paths to the user site-packages directory and verifies the addition. Parameters: - custom_paths : List[str] : A list of directory paths to be added to the user site-packages. Returns: - bool : True if all paths are successfully added, False otherwise. user_site = site.getusersitepackages() pth_file_path = os.path.join(user_site, \'custom_paths.pth\') # Filtering out valid paths valid_paths = [path for path in custom_paths if os.path.exists(path)] # Write valid paths to .pth file try: with open(pth_file_path, \'w\') as pth_file: pth_file.write(\'n\'.join(valid_paths) + \'n\') except OSError as e: print(f\'Error writing to {pth_file_path}: {e}\') return False # Verify the paths have been added to sys.path site.addsitedir(user_site) successful = all(path in sys.path for path in valid_paths) return successful"},{"question":"**Objective:** Your task is to demonstrate your understanding of seaborn\'s `pointplot` function by analyzing and visualizing a given dataset. **Question:** Given the `tips` dataset available in seaborn, complete the following tasks using seaborn\'s `sns.pointplot` function. **Tasks:** 1. Load the `tips` dataset using seaborn. 2. Create a point plot to show the average total bill for each day of the week, including the corresponding 95% confidence intervals. 3. Modify the plot to differentiate the average total bill for each day by `sex`. 4. Add markers and linestyles to enhance accessibility by redundantly coding the `sex` variable. 5. Customize the plot to: - Use blue color for the lines and markers. - Use a circle marker for female and a square marker for male. - Set the linestyle for female to solid and for male to dashed. - Set the capsize of error bars to 0.2. 6. Export the final plot as a PNG file named `tips_pointplot.png`. **Expected Input and Output Formats:** - **Input:** The function will not take any input parameters. It will work directly with the seaborn library to load and manipulate the dataset. - **Output:** The function will save a PNG file named `tips_pointplot.png` containing the final plot. **Constraints:** - Ensure that all necessary libraries are imported within the function. - The solution should be efficient and concise. **Performance Requirements:** - The plot creation and customization should be handled within a single function. **Additional Instructions:** - Include comments in your code to explain each step. **Starter Code:** ```python import seaborn as sns import matplotlib.pyplot as plt def create_tips_pointplot(): # Step 1: Load the tips dataset. tips = sns.load_dataset(\\"tips\\") # TODO: Step 2, 3, 4, 5: Implement the steps described in the question. # Save the plot as PNG file plt.savefig(\\"tips_pointplot.png\\") # Call the function to create the plot. create_tips_pointplot() ``` Complete the implementation of the `create_tips_pointplot` function following the tasks outlined.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_tips_pointplot(): # Step 1: Load the tips dataset. tips = sns.load_dataset(\\"tips\\") # Step 2, 3, 4, 5: Create the point plot with the specified customizations plt.figure(figsize=(10, 6)) sns.pointplot( x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", data=tips, palette={\\"Male\\": \\"blue\\", \\"Female\\": \\"blue\\"}, markers=[\\"s\\", \\"o\\"], # square for male, circle for female linestyles=[\\"--\\", \\"-\\"], # dashed for male, solid for female capsize=0.2 ) # Save the plot as PNG file plt.title(\\"Average Total Bill by Day and Sex\\") plt.xlabel(\\"Day of the Week\\") plt.ylabel(\\"Average Total Bill\\") plt.savefig(\\"tips_pointplot.png\\") plt.close() # Call the function to create the plot. create_tips_pointplot()"},{"question":"# Question: Advanced Windowing Operations with pandas Context You are provided with a time series dataset representing daily stock prices of a fictional company over a period of one year. The dataset contains columns for the date (`Date`) and the closing price (`Close`). Your task is to perform various windowing operations on this dataset to analyze trends over different periods. Specifically, you will: 1. Calculate the rolling mean over a 7-day window. 2. Calculate the expanding sum of the closing prices. 3. Apply a custom weighted mean function using a Gaussian window. 4. Implement a custom indexer to calculate the variable window sums. Dataset ```python import pandas as pd # Example dataset (in reality, this would be loaded from a CSV or similar) data = { \\"Date\\": pd.date_range(start=\\"2021-01-01\\", periods=365, freq=\\"D\\"), \\"Close\\": [100 + (i*0.5) for i in range(365)] } df = pd.DataFrame(data).set_index(\\"Date\\") ``` Instructions 1. **Rolling Mean (7-day window)** Implement a function `calculate_rolling_mean(df)` that calculates the 7-day rolling mean of the `Close` column and returns the result as a new `pd.Series`. ```python def calculate_rolling_mean(df: pd.DataFrame) -> pd.Series: Calculates the 7-day rolling mean of the \'Close\' prices. Parameters: df (pd.DataFrame): The dataframe containing the stock prices. Returns: pd.Series: The 7-day rolling mean of \'Close\' prices. pass ``` 2. **Expanding Sum** Implement a function `calculate_expanding_sum(df)` that calculates the expanding sum of the `Close` column and returns the result as a new `pd.Series`. ```python def calculate_expanding_sum(df: pd.DataFrame) -> pd.Series: Calculates the expanding sum of the \'Close\' prices. Parameters: df (pd.DataFrame): The dataframe containing the stock prices. Returns: pd.Series: The expanding sum of \'Close\' prices. pass ``` 3. **Weighted Mean (Gaussian Window)** Implement a function `calculate_weighted_mean(df)` that calculates a weighted mean of the `Close` column using a Gaussian window with a standard deviation of 2. Return the result as a new `pd.Series`. ```python def calculate_weighted_mean(df: pd.DataFrame) -> pd.Series: Calculates the weighted mean of the \'Close\' prices using a Gaussian window. Parameters: df (pd.DataFrame): The dataframe containing the stock prices. Returns: pd.Series: The weighted mean of \'Close\' prices. pass ``` 4. **Custom Indexed Window** Implement a custom indexer named `ExpandingCustomIndexer` that extends `BaseIndexer`. This indexer will calculate the sum of the `Close` prices if the index is even, otherwise, it will use an expanding window. Implement a function `calculate_custom_indexed_window(df, indexer)` that utilizes this custom indexer and returns the result as a new `pd.Series`. ```python from pandas.api.indexers import BaseIndexer class ExpandingCustomIndexer(BaseIndexer): def get_window_bounds(self, num_values, min_periods, center, closed, step): start = np.empty(num_values, dtype=np.int64) end = np.empty(num_values, dtype=np.int64) for i in range(num_values): if i % 2 == 0: start[i] = 0 end[i] = i + 1 else: start[i] = i end[i] = i + 1 return start, end def calculate_custom_indexed_window(df: pd.DataFrame, indexer: BaseIndexer) -> pd.Series: Calculates the windowed sum of the \'Close\' prices using a custom indexer. Parameters: df (pd.DataFrame): The dataframe containing the stock prices. indexer (BaseIndexer): The custom indexer for variable window calculation. Returns: pd.Series: The custom indexed sum of \'Close\' prices. pass ``` Notes - You should handle any missing values appropriately. - Ensure your functions are efficient and utilize pandas\' optimized windowing functions where possible. - You can assume the input DataFrame `df` will always follow the structure provided in the dataset example. Provide the solutions as Python functions defined above.","solution":"import pandas as pd import numpy as np from pandas.api.indexers import BaseIndexer def calculate_rolling_mean(df: pd.DataFrame) -> pd.Series: Calculates the 7-day rolling mean of the \'Close\' prices. Parameters: df (pd.DataFrame): The dataframe containing the stock prices. Returns: pd.Series: The 7-day rolling mean of \'Close\' prices. return df[\'Close\'].rolling(window=7).mean() def calculate_expanding_sum(df: pd.DataFrame) -> pd.Series: Calculates the expanding sum of the \'Close\' prices. Parameters: df (pd.DataFrame): The dataframe containing the stock prices. Returns: pd.Series: The expanding sum of \'Close\' prices. return df[\'Close\'].expanding().sum() def calculate_weighted_mean(df: pd.DataFrame) -> pd.Series: Calculates the weighted mean of the \'Close\' prices using a Gaussian window. Parameters: df (pd.DataFrame): The dataframe containing the stock prices. Returns: pd.Series: The weighted mean of \'Close\' prices. return df[\'Close\'].rolling(window=7, win_type=\'gaussian\').mean(std=2) class ExpandingCustomIndexer(BaseIndexer): def get_window_bounds(self, num_values, min_periods, center, closed, step): start = np.empty(num_values, dtype=np.int64) end = np.empty(num_values, dtype=np.int64) for i in range(num_values): if i % 2 == 0: start[i] = 0 end[i] = i + 1 else: start[i] = i end[i] = i + 1 return start, end def calculate_custom_indexed_window(df: pd.DataFrame, indexer: BaseIndexer) -> pd.Series: Calculates the windowed sum of the \'Close\' prices using a custom indexer. Parameters: df (pd.DataFrame): The dataframe containing the stock prices. indexer (BaseIndexer): The custom indexer for variable window calculation. Returns: pd.Series: The custom indexed sum of \'Close\' prices. return df[\'Close\'].rolling(window=indexer).sum()"},{"question":"Question # Objective Demonstrate your comprehension of Python\'s built-in types by implementing a custom data structure that behaves like a subset of Python\'s collection types, including various behaviors such as truth value testing, comparisons, and basic sequence operations. # Task You are required to implement a Python class `CustomList` that mimics some behaviors of Python\'s list. The class should support various operations including indexing, slicing, concatenation, and comparison but with some custom constraints. # Class Specification `CustomList` # Attributes: - `data`: A list that holds the actual elements of the CustomList object. # Methods: 1. **Initialization:** - Constructor should accept an iterable to initialize the `data` attribute. ```python def __init__(self, iterable): # Constructor implementation ``` 2. **String Representation:** - Implement `__repr__` method to return the CustomList in string format. ```python def __repr__(self): # String representation implementation ``` 3. **Length:** - Implement `__len__` method to return the length of the CustomList. ```python def __len__(self): # Length calculation implementation ``` 4. **Indexing:** - Implement `__getitem__` method to access elements by index. ```python def __getitem__(self, index): # Indexing implementation ``` 5. **Slicing:** - Implement the same `__getitem__` method to handle slicing. ```python def __getitem__(self, slice_object): # Slicing implementation ``` 6. **Concatenation:** - Implement `__add__` method to concatenate two CustomList objects and return a new CustomList. ```python def __add__(self, other): # Concatenation implementation ``` 7. **Comparison:** - Implement `__eq__` method to compare two CustomList objects for equality. ```python def __eq__(self, other): # Equality comparison implementation ``` # Constraints - The class should handle invalid inputs gracefully by raising appropriate exceptions. - Performance considerations should be taken into account, the operations should have optimal time complexity. # Example ```python # Example Usage cl1 = CustomList([1, 2, 3]) cl2 = CustomList([4, 5]) print(cl1) # CustomList([1, 2, 3]) print(len(cl1)) # 3 print(cl1[1]) # 2 print(cl1[0:2]) # CustomList([1, 2]) print(cl1 + cl2) # CustomList([1, 2, 3, 4, 5]) print(cl1 == CustomList([1, 2, 3])) # True print(cl1 == cl2) # False ``` # Notes - Detailed docstrings for each method are expected. - Ensure to include error handling for edge cases. - Implement and provide unittests to validate each method.","solution":"class CustomList: def __init__(self, iterable): Initialize the CustomList with the provided iterable. if not hasattr(iterable, \'__iter__\'): raise TypeError(\\"CustomList requires an iterable\\") self.data = list(iterable) def __repr__(self): Return the string representation of the CustomList. return f\'CustomList({self.data})\' def __len__(self): Return the length of the CustomList. return len(self.data) def __getitem__(self, index): Implement indexing and slicing for the CustomList. if isinstance(index, slice): return CustomList(self.data[index]) return self.data[index] def __add__(self, other): Concatenate two CustomList objects. if not isinstance(other, CustomList): raise TypeError(\\"Can only concatenate CustomList to CustomList\\") return CustomList(self.data + other.data) def __eq__(self, other): Compare two CustomList objects for equality. if not isinstance(other, CustomList): return False return self.data == other.data"},{"question":"# Question In this assessment, you will implement a custom PyTorch `autograd.Function` that: 1. Calculates the element-wise maximum of two input tensors. 2. Computes the required gradient for the backward pass. 3. Supports `torch.vmap` for automatic batching. Your function should follow these specifications: **Function Details:** Create a class `ElementwiseMax` that extends `torch.autograd.Function` with the following requirements: 1. **forward**: - Takes two tensors `x` and `y` and returns their element-wise maximum. - Ensures that any intermediate values needed for backward are returned as outputs. 2. **setup_context**: - Takes `ctx`, `inputs`, and `outputs` as arguments. - Saves necessary tensors for the backward pass using `ctx.save_for_backward`. 3. **backward**: - Takes the gradient of the output with respect to the inputs. - Computes the gradient of the loss with respect to `x` and `y`. 4. **vmap**: - Implements the vectorized map (vmap) logic for batching the forward pass. Create a wrapper function `elementwise_max` to invoke the custom `ElementwiseMax` autograd function: ```python def elementwise_max(x, y): return ElementwiseMax.apply(x, y) ``` **Input:** - Two tensors `x` and `y` of the same shape. **Output:** - A tensor containing the element-wise maximum of `x` and `y`. # Example: ```python import torch from torch.autograd import grad # Define the input tensors x = torch.tensor([1, 4, 2], dtype=torch.float32, requires_grad=True) y = torch.tensor([2, 3, 3], dtype=torch.float32, requires_grad=True) # Compute element-wise maximum z = elementwise_max(x, y) # Sanity checks # Expected output: tensor([2., 4., 3.]) print(z) # Compute the gradient with respect to the input tensors z.sum().backward() # Expected gradients: tensor([0., 1., 0.]), tensor([1., 0., 1.]) print(x.grad, y.grad) ``` # Constraints: - The function must work with tensors of any shape and be efficient in terms of performance. - Ensure all intermediate values that need to be saved for the backward pass are correctly handled in `setup_context`. # Performance Requirements: - Solutions should avoid unnecessary computations in `setup_context`. - The `vmap` implementation should allow for proper vectorized operations for batched inputs. Guidelines: - Do not use any external libraries other than `torch` and `numpy`. - Use only PyTorch operations for the implementation to ensure compatibility with autograd. # Submission: Submit your class implementation and the `elementwise_max` function. Include the example usage along with the expected outputs.","solution":"import torch from torch.autograd import Function class ElementwiseMax(Function): @staticmethod def forward(ctx, x, y): max_vals = torch.maximum(x, y) ctx.save_for_backward(x, y) return max_vals @staticmethod def backward(ctx, grad_output): x, y = ctx.saved_tensors grad_x = (x >= y).to(grad_output) * grad_output grad_y = (y > x).to(grad_output) * grad_output return grad_x, grad_y def elementwise_max(x, y): return ElementwiseMax.apply(x, y)"},{"question":"# Cryptographic Services Function Implementation Objective Implement a secure system for hashing and authenticating messages using Python\'s `hashlib` and `hmac` modules. Additionally, leverage the `secrets` module to generate and manage secure tokens. Task You need to implement a function `secure_message_system` that provides the following functionalities: 1. **Hashing a Message:** - Use the `hashlib` module to create a SHA-256 hash of the input message. - The function should return the hexadecimal representation of the hash. 2. **Generating a Secure Token:** - Use the `secrets` module to generate a cryptographically secure token. - The function should accept an integer parameter `token_bytes`, which specifies the number of bytes for the token. - The function should return the generated token encoded in hexadecimal. 3. **Authenticating a Message:** - Use the `hmac` module to create an HMAC using SHA-256. - The function should accept a message and a key, and return the HMAC in hexadecimal format. # Function Signature ```python def secure_message_system(message: str, key: str, token_bytes: int) -> dict: Implements a secure message hashing, token generation, and HMAC authentication Parameters: message (str): The message to be hashed and authenticated. key (str): The key to use for HMAC. token_bytes (int): The number of bytes for the secure token. Returns: dict: A dictionary with the following keys: \'hash\': str, the SHA-256 hash of the message in hexadecimal. \'token\': str, the generated secure token in hexadecimal. \'hmac\': str, the HMAC of the message in hexadecimal. ``` Example ```python message = \\"Secure communication is essential.\\" key = \\"supersecret\\" token_bytes = 16 result = secure_message_system(message, key, token_bytes) # Example output (actual output will vary due to secure token generation): # { # \'hash\': \'bf2a33841f8b805bcb374c3b6dabe3128e085d0cf3061d8d503897ce6270d19e\', # \'token\': \'0ff74f99429c302fa3b5ec6bda23f826\', # \'hmac\': \'5d5c01c0ee211221559d47cbce57209d00fee58eb387db3f7c59359bde37b33d\' # } ``` Constraints - The message and key will be non-empty strings. - The `token_bytes` will be a positive integer. - Use SHA-256 for both hashing and HMAC. - Ensure the secure token is generated using a cryptographically secure RNG (Random Number Generator).","solution":"import hashlib import hmac import secrets def secure_message_system(message: str, key: str, token_bytes: int) -> dict: Implements a secure message hashing, token generation, and HMAC authentication Parameters: message (str): The message to be hashed and authenticated. key (str): The key to use for HMAC. token_bytes (int): The number of bytes for the secure token. Returns: dict: A dictionary with the following keys: \'hash\': str, the SHA-256 hash of the message in hexadecimal. \'token\': str, the generated secure token in hexadecimal. \'hmac\': str, the HMAC of the message in hexadecimal. # Compute SHA-256 hash of the message sha256_hash = hashlib.sha256(message.encode()).hexdigest() # Generate a secure token secure_token = secrets.token_hex(token_bytes) # Create an HMAC using the key hmac_obj = hmac.new(key.encode(), message.encode(), hashlib.sha256) hmac_hash = hmac_obj.hexdigest() return { \'hash\': sha256_hash, \'token\': secure_token, \'hmac\': hmac_hash }"},{"question":"Serializable Family Tree Your task is to implement the serialization and deserialization of a family tree using the marshalling functions provided in the Python documentation above. A family tree consists of individual members, where each member can have one or more children, creating a hierarchical structure. You will implement two main functions: 1. `serialize_family_tree(root, file_path)`: This function will serialize the family tree starting from the root node and write it to the specified binary file. 2. `deserialize_family_tree(file_path)`: This function will read the binary file and reconstruct the family tree. Class Definitions You will work with the following class definitions: ```python class Person: def __init__(self, name, age): self.name = name self.age = age self.children = [] def add_child(self, child): self.children.append(child) def __repr__(self): return f\\"Person(name={self.name}, age={self.age}, children={self.children})\\" ``` Function Details 1. `serialize_family_tree(root, file_path)` - **Input**: - `root`: The root `Person` object of the family tree. - `file_path`: The path to the binary file where the serialized data will be saved. - **Output**: None - **Requirements**: - Use `PyMarshal_WriteObjectToFile` to serialize the `Person` objects and their children recursively. - Handle any file I/O exceptions appropriately. 2. `deserialize_family_tree(file_path)` - **Input**: - `file_path`: The path to the binary file containing the serialized family tree data. - **Output**: - Returns the root `Person` object of the deserialized family tree. - **Requirements**: - Use `PyMarshal_ReadObjectFromFile` to deserialize the `Person` objects from the file. - Handle any file I/O exceptions and ensure that appropriate exceptions are raised and caught. Constraints - Ensure that only the versions supported by the marshalling module are used. - Ensure proper error handling and edge cases (e.g., file not found, invalid file format, etc.) - Assume that the `Person` class definition will not change. # Example ```python if __name__ == \\"__main__\\": root = Person(\\"Alice\\", 50) child1 = Person(\\"Bob\\", 30) child2 = Person(\\"Charlie\\", 25) root.add_child(child1) root.add_child(child2) serialize_family_tree(root, \\"family_tree.dat\\") new_root = deserialize_family_tree(\\"family_tree.dat\\") print(new_root) # Output should be similar to: Person(name=Alice, age=50, children=[Person(name=Bob, age=30, children=[]), Person(name=Charlie, age=25, children=[])]) ``` Implement the required functions to ensure the `Example` runs correctly.","solution":"import pickle class Person: def __init__(self, name, age): self.name = name self.age = age self.children = [] def add_child(self, child): self.children.append(child) def __repr__(self): return f\\"Person(name={self.name}, age={self.age}, children={self.children})\\" def serialize_family_tree(root, file_path): Serializes the family tree starting from the root into a binary file. try: with open(file_path, \'wb\') as file: pickle.dump(root, file) except Exception as e: print(f\\"An error occurred while serializing the family tree: {e}\\") def deserialize_family_tree(file_path): Deserializes the family tree from a binary file and returns the root Person object. try: with open(file_path, \'rb\') as file: return pickle.load(file) except Exception as e: print(f\\"An error occurred while deserializing the family tree: {e}\\") return None"},{"question":"# Persistent Key-Value Storage You are tasked with implementing a set of functions using the `shelve` module to manage a simple database of students\' records. Each record should contain a student\'s `name`, `age`, and a list of `grades`. Your implementation should include: 1. `open_shelf(filename)`: Opens and returns a shelve object for the given filename. 2. `add_student(shelf, student_id, name, age, grades)`: Adds a student record to the shelf. 3. `get_student(shelf, student_id)`: Retrieves a student record by ID. 4. `update_grades(shelf, student_id, new_grades)`: Updates the grades for a student. 5. `remove_student(shelf, student_id)`: Removes a student record from the shelf. 6. `close_shelf(shelf)`: Properly closes the shelf. # Requirements: - The student records should be structured as dictionaries. - Ensure the `update_grades` function works correctly both with `writeback=True` and `writeback=False`. - Handle cases where a student ID does not exist gracefully. # Example Code Usage: ```python # Create/Open the shelf shelf = open_shelf(\'student_records.db\') # Add a student add_student(shelf, \'001\', \'Alice\', 20, [85, 90, 78]) # Retrieve a student record print(get_student(shelf, \'001\')) # Output: {\'name\': \'Alice\', \'age\': 20, \'grades\': [85, 90, 78]} # Update grades update_grades(shelf, \'001\', [88, 82, 79]) print(get_student(shelf, \'001\')) # Output: {\'name\': \'Alice\', \'age\': 20, \'grades\': [88, 82, 79]} # Remove a student remove_student(shelf, \'001\') # Close the shelf close_shelf(shelf) ``` # Constraints: - Student IDs are unique and alphanumeric. - A grades list consists of integers between 0 and 100. # Solution Structure: ```python import shelve def open_shelf(filename): # Your implementation here pass def add_student(shelf, student_id, name, age, grades): # Your implementation here pass def get_student(shelf, student_id): # Your implementation here pass def update_grades(shelf, student_id, new_grades): # Your implementation here pass def remove_student(shelf, student_id): # Your implementation here pass def close_shelf(shelf): # Your implementation here pass ```","solution":"import shelve def open_shelf(filename): Opens and returns a shelve object for the given filename. return shelve.open(filename) def add_student(shelf, student_id, name, age, grades): Adds a student record to the shelf. shelf[student_id] = {\'name\': name, \'age\': age, \'grades\': grades} def get_student(shelf, student_id): Retrieves a student record by ID. return shelf.get(student_id, None) def update_grades(shelf, student_id, new_grades): Updates the grades for a student. if student_id in shelf: student = shelf[student_id] student[\'grades\'] = new_grades shelf[student_id] = student # Re-assign the modified student record to the shelf def remove_student(shelf, student_id): Removes a student record from the shelf. if student_id in shelf: del shelf[student_id] def close_shelf(shelf): Properly closes the shelf. shelf.close()"},{"question":"Objective: You are tasked with implementing a pair of functions that will compress and decompress a given string using the `zlib` module in Python. The functions must handle both basic compression/decompression and also support handling compressing streams of data that won\'t fit into memory at once. Instructions: 1. **Function: `compress_data(data: str, level: int = -1) -> bytes`** - **Input:** - `data`: A string to be compressed. - `level`: An optional integer from 0 to 9, or -1 specifying the compression level. Defaults to -1 for default compression. - **Output:** - Returns a bytes object containing the compressed data. - **Constraints:** - The function should raise a `ValueError` if the `level` is not in the range of -1 to 9. 2. **Function: `decompress_data(data: bytes, wbits: int = zlib.MAX_WBITS) -> str`** - **Input:** - `data`: A bytes object containing the compressed data. - `wbits`: An optional integer controlling the size of the history buffer and the header/trailer format. Defaults to `zlib.MAX_WBITS`. - **Output:** - Returns the original uncompressed string. - **Constraints:** - The function should handle zlib-specific headers and trailers as well as raw data streams without headers/trailers. 3. **Function: `compress_stream(input_stream: bytes, chunk_size: int = 1024, level: int = -1) -> bytes`** - **Input:** - `input_stream`: A bytes object representing a large stream of data. - `chunk_size`: An optional integer specifying the size of chunks (in bytes) to process at a time. Defaults to 1024 bytes. - `level`: An optional integer from 0 to 9, or -1 specifying the compression level. Defaults to -1 for default compression. - **Output:** - Returns a bytes object containing the compressed data. - **Constraints:** - The function should raise a `ValueError` if the `level` is not in the range of -1 to 9. - The function should compress the stream in chunks to handle large data without consuming excessive memory. # Example Usage: ```python # Example data data = \\"This is a test string for compression.\\" # Compressing data compressed_data = compress_data(data) print(compressed_data) # Decompressing data original_data = decompress_data(compressed_data) print(original_data) # Output: \\"This is a test string for compression.\\" # Compressing stream data large_data = b\\"A\\" * 10000 compressed_stream_data = compress_stream(large_data) print(compressed_stream_data) ``` # Notes: - Ensure the functions are robust and handle potential errors gracefully. - Include appropriate error messages in exceptions for easier debugging. - Performance considerations should manage memory usage effectively when handling large streams of data.","solution":"import zlib def compress_data(data: str, level: int = -1) -> bytes: Compresses a given string using zlib with the specified compression level. Parameters: - data: A string to be compressed. - level: An optional integer from 0 to 9, or -1 specifying the compression level. Defaults to -1. Returns: - A bytes object containing the compressed data. Raises: - ValueError: If the level is not in the range of -1 to 9. if not (-1 <= level <= 9): raise ValueError(\\"Compression level must be between -1 and 9.\\") return zlib.compress(data.encode(), level) def decompress_data(data: bytes, wbits: int = zlib.MAX_WBITS) -> str: Decompresses a given bytes object using zlib with the specified wbits parameter. Parameters: - data: A bytes object containing the compressed data. - wbits: An optional integer controlling the size of the history buffer and the header/trailer format. Defaults to zlib.MAX_WBITS. Returns: - A string containing the original uncompressed data. return zlib.decompress(data, wbits).decode() def compress_stream(input_stream: bytes, chunk_size: int = 1024, level: int = -1) -> bytes: Compresses a large stream of data in chunks to handle large data without consuming excessive memory. Parameters: - input_stream: A bytes object representing a large stream of data. - chunk_size: An optional integer specifying the size of chunks (in bytes) to process at a time. Defaults to 1024 bytes. - level: An optional integer from 0 to 9, or -1 specifying the compression level. Defaults to -1. Returns: - A bytes object containing the compressed data. Raises: - ValueError: If the level is not in the range of -1 to 9. if not (-1 <= level <= 9): raise ValueError(\\"Compression level must be between -1 and 9.\\") compressor = zlib.compressobj(level) compressed_data = b\\"\\" start = 0 while start < len(input_stream): end = start + chunk_size chunk = input_stream[start:end] compressed_data += compressor.compress(chunk) start = end compressed_data += compressor.flush() return compressed_data"},{"question":"# Pandas Coding Assessment You are given a CSV file which contains data about people, including their names, ages, and cities they live in. This dataset might have duplicate values in the name column. Your task is to process this data using pandas to achieve the following objectives: 1. **Read the data from the CSV file** and load it into a DataFrame. 2. **Ensure there are no duplicate name entries** using pandas functionality to identify duplicates. 3. **If duplicates exist** in the \'name\' column, resolve them by keeping the entry with the maximum age and drop the others. 4. **Prevent further duplicates** in the name column after this initial clean-up. Provide your solution in a function with the signature described below: Function Signature ```python import pandas as pd def process_people_data(file_path: str) -> pd.DataFrame: Process the people data from CSV file to ensure unique names and prevent duplicates. Args: file_path (str): The path to the CSV file containing the data. Returns: pd.DataFrame: The processed DataFrame with unique names and no possibility of introducing duplicates. pass ``` Input/Output Format **Input:** - A CSV file with columns: \'name\', \'age\', \'city\'. **Output:** - A DataFrame with the same columns but no duplicate names, keeping the entry with the maximum age in case of duplicates. - The DataFrame should be set to disallow further duplicate names. Example Assume the CSV file content: ``` name,age,city John,23,New York Alice,30,Los Angeles John,27,San Francisco Doe,22,Las Vegas ``` **Output:** The resulting DataFrame should be: ``` name age city 0 Alice 30 Los Angeles 1 Doe 22 Las Vegas 2 John 27 San Francisco ``` * The output DataFrame should also have the property `allows_duplicate_labels` set to `False` to prevent further duplicates. Constraints - The function should handle reading and processing of the CSV file efficiently. - Assume all columns are strings except \'age\', which should be an integer. - Ensure that any potential duplicates introduced accidentally during further processing are disallowed.","solution":"import pandas as pd def process_people_data(file_path: str) -> pd.DataFrame: Process the people data from CSV file to ensure unique names and prevent duplicates. Args: file_path (str): The path to the CSV file containing the data. Returns: pd.DataFrame: The processed DataFrame with unique names and no possibility of introducing duplicates. # Read the data from the CSV file df = pd.read_csv(file_path) # Ensure there are no duplicate name entries by keeping the entry with the maximum age and dropping the others df = df.sort_values(by=[\'name\', \'age\'], ascending=[True, False]).drop_duplicates(subset=\'name\', keep=\'first\') # Prevent further duplicates in the name column df = df.reset_index(drop=True) df.flags.allows_duplicate_labels = False return df"},{"question":"Objective Demonstrate your understanding of the Python `plistlib` module by implementing functions to read, modify, and write property list files. Task Description You need to write a Python function called `update_plist_plist_data` that performs the following operations: 1. Read a property list from a given binary file. 2. Update specific keys in the dictionary with new values. 3. Write the updated dictionary back to a new binary file, ensuring the format remains consistent. Function Signature ```python def update_plist_data(input_file: str, output_file: str, updates: dict) -> None: Read a binary .plist file, update its contents, and write to a new binary .plist file. Args: - input_file (str): Path to the input .plist file. - output_file (str): Path to the output .plist file. - updates (dict): Dictionary containing keys and their new values to update in the .plist data. Returns: - None pass ``` Input - `input_file`: A string representing the path to the input .plist file. - `output_file`: A string representing the path to the output .plist file. - `updates`: A dictionary where each key-value pair represents a key in the .plist data that needs to be updated with the new value provided. Output - None Constraints - The input file is guaranteed to be in the binary plist format. - The input file and updates dictionary are guaranteed not to be empty. - Keys in the updates dictionary are guaranteed to exist in the input .plist file. Example Suppose you have the following binary `.plist` file named `example.plist`: ``` { \\"aString\\": \\"Original\\", \\"aNumber\\": 123, \\"aBoolean\\": True } ``` And you execute the following: ```python updates = { \\"aString\\": \\"Updated\\", \\"aNumber\\": 456 } update_plist_data(\\"example.plist\\", \\"updated_example.plist\\", updates) ``` The `updated_example.plist` file should contain: ``` { \\"aString\\": \\"Updated\\", \\"aNumber\\": 456, \\"aBoolean\\": True } ``` Additional Requirements - Your function must handle potential exceptions such as file not found or invalid file format. - Ensure that the file is read and written in binary mode. - Maintain the original format of the `.plist` file (binary). Good luck!","solution":"import plistlib def update_plist_data(input_file: str, output_file: str, updates: dict) -> None: Read a binary .plist file, update its contents, and write to a new binary .plist file. Args: - input_file (str): Path to the input .plist file. - output_file (str): Path to the output .plist file. - updates (dict): Dictionary containing keys and their new values to update in the .plist data. Returns: - None try: # Read the binary .plist file with open(input_file, \'rb\') as f: plist_data = plistlib.load(f) # Update the plist data with the provided updates plist_data.update(updates) # Write the updated plist data back to a new binary file with open(output_file, \'wb\') as f: plistlib.dump(plist_data, f) except FileNotFoundError: print(f\\"Error: The file {input_file} was not found.\\") except plistlib.InvalidFileException: print(f\\"Error: The file {input_file} is not a valid plist file.\\")"},{"question":"Coding Assessment Question # Objective: Implement a Python class called `Python310Calculator` that leverages the functions from the `python310` package to perform various arithmetic and bitwise operations. This class should provide methods for basic arithmetic operations, bitwise operations, and in-place operations. # Requirements: 1. **Basic Arithmetic Methods:** - `add(o1, o2)`: Return the sum of `o1` and `o2`. - `subtract(o1, o2)`: Return the result of subtracting `o2` from `o1`. - `multiply(o1, o2)`: Return the product of `o1` and `o2`. - `divide(o1, o2, true_divide=True)`: Return the division of `o1` by `o2`. If `true_divide` is `True`, perform true division, otherwise perform floor division. - `remainder(o1, o2)`: Return the remainder of dividing `o1` by `o2`. - `power(o1, o2, o3=Py_None)`: Return the result of `o1` raised to the power of `o2`. `o3` is optional and defaults to `Py_None`. 2. **Bitwise Operation Methods:** - `bitwise_and(o1, o2)`: Return the bitwise AND of `o1` and `o2`. - `bitwise_or(o1, o2)`: Return the bitwise OR of `o1` and `o2`. - `bitwise_xor(o1, o2)`: Return the bitwise XOR of `o1` and `o2`. - `bitwise_not(o)`: Return the bitwise NOT of `o`. 3. **In-Place Operation Methods:** - `in_place_add(o1, o2)`: Perform in-place addition of `o2` to `o1`. - `in_place_subtract(o1, o2)`: Perform in-place subtraction of `o2` from `o1`. - `in_place_multiply(o1, o2)`: Perform in-place multiplication of `o1` by `o2`. - `in_place_divide(o1, o2, true_divide=True)`: Perform in-place division of `o1` by `o2`. If `true_divide` is `True`, perform in-place true division, otherwise perform in-place floor division. - `in_place_remainder(o1, o2)`: Perform in-place remainder operation of `o1` by `o2`. - `in_place_power(o1, o2, o3=Py_None)`: Perform in-place power operation of `o1` raised to the power of `o2`. `o3` is optional and defaults to `Py_None`. # Input and Output Formats - Each method should accept Python objects (`o1`, `o2`, `o3`) and return the result of the operation as per the specifications. - The methods should raise appropriate exceptions if any operation fails. # Constraints: 1. The operations should handle numeric types and raise `TypeError` if non-numeric types are used. 2. Division by zero should be handled appropriately by raising a `ZeroDivisionError`. 3. Ensure that the methods return the expected type (e.g., `int`, `float`) based on the operation performed. # Example Usage: ```python calc = Python310Calculator() # Basic Arithmetic Operations result1 = calc.add(5, 7) # Should use PyNumber_Add result2 = calc.subtract(10, 4) # Should use PyNumber_Subtract # Bitwise Operations result3 = calc.bitwise_and(6, 3) # Should use PyNumber_And # In-Place Operations obj1 = 5 obj2 = 2 result4 = calc.in_place_add(obj1, obj2) # Should modify obj1 to 7 using PyNumber_InPlaceAdd print(result1) # Expected: 12 print(result2) # Expected: 6 print(result3) # Expected: 2 print(result4) # Expected: 7 ``` # Note: While solving the problem, ensure that you use the specific `PyNumber` methods provided in the documentation to implement each method within the `Python310Calculator` class.","solution":"class Python310Calculator: def add(self, o1, o2): return o1 + o2 def subtract(self, o1, o2): return o1 - o2 def multiply(self, o1, o2): return o1 * o2 def divide(self, o1, o2, true_divide=True): if o2 == 0: raise ZeroDivisionError(\\"Division by zero!\\") return o1 / o2 if true_divide else o1 // o2 def remainder(self, o1, o2): if o2 == 0: raise ZeroDivisionError(\\"Division by zero!\\") return o1 % o2 def power(self, o1, o2, o3=None): return pow(o1, o2, o3) if o3 else pow(o1, o2) def bitwise_and(self, o1, o2): return o1 & o2 def bitwise_or(self, o1, o2): return o1 | o2 def bitwise_xor(self, o1, o2): return o1 ^ o2 def bitwise_not(self, o): return ~o def in_place_add(self, o1, o2): o1 += o2 return o1 def in_place_subtract(self, o1, o2): o1 -= o2 return o1 def in_place_multiply(self, o1, o2): o1 *= o2 return o1 def in_place_divide(self, o1, o2, true_divide=True): if o2 == 0: raise ZeroDivisionError(\\"Division by zero!\\") o1 = o1 / o2 if true_divide else o1 // o2 return o1 def in_place_remainder(self, o1, o2): if o2 == 0: raise ZeroDivisionError(\\"Division by zero!\\") o1 %= o2 return o1 def in_place_power(self, o1, o2, o3=None): o1 = pow(o1, o2, o3) if o3 else pow(o1, o2) return o1"},{"question":"# Python Built-in Constants Usage Problem Statement In Python, there are several built-in constants that cannot be reassigned and have very specific uses. Your task is to implement a function that makes use of these constants to perform various checks and return appropriate results. Function Signature ```python def check_constants(value): Given a value, performs checks using Python built-in constants and returns the results based on the conditions described below. Parameters: value (any type): The value to be checked. Returns: dict: A dictionary with keys \'boolean_value\', \'is_not_none\', \'is_ellipsis\', \'is_not_implemented\', \'in_debug_mode\', containing boolean results as described. ``` Requirements 1. **boolean_value**: Check if the given value is equal to `True` or `False`. 2. **is_not_none**: Check if the given value is `not None`. 3. **is_ellipsis**: Check if the given value is the `Ellipsis` object. 4. **is_not_implemented**: Check if the given value is the `NotImplemented` object. 5. **in_debug_mode**: Check if the `__debug__` constant is `True`. Constraints - You cannot reassign any of the constants used in this function. - The function should return a dictionary with boolean values indicating the result of each check. Example ```python result = check_constants(True) print(result) # Output: # { # \'boolean_value\': True, # \'is_not_none\': True, # \'is_ellipsis\': False, # \'is_not_implemented\': False, # \'in_debug_mode\': True # } result = check_constants(None) print(result) # Output: # { # \'boolean_value\': False, # \'is_not_none\': False, # \'is_ellipsis\': False, # \'is_not_implemented\': False, # \'in_debug_mode\': True # } ``` Your implementation should be efficient and make use of the constants correctly without causing any syntax errors. Notes - The `__debug__` constant can be checked directly. - Ensure to use the correct identity checks where necessary.","solution":"def check_constants(value): Given a value, performs checks using Python built-in constants and returns the results based on the conditions described below. Parameters: value (any type): The value to be checked. Returns: dict: A dictionary with keys \'boolean_value\', \'is_not_none\', \'is_ellipsis\', \'is_not_implemented\', \'in_debug_mode\', containing boolean results as described. return { \'boolean_value\': value is True or value is False, \'is_not_none\': value is not None, \'is_ellipsis\': value is Ellipsis, \'is_not_implemented\': value is NotImplemented, \'in_debug_mode\': __debug__ }"},{"question":"# Custom Exception Handling in Python You are required to create a custom exception hierarchy for a file processing system along with some utility functions that leverage these exceptions. Specifically, you need to implement: 1. A base exception class `FileProcessingError` which inherits from the built-in `Exception` class. 2. Two specific exceptions: - `FileReadError`: To be raised when there is an issue reading the file. It should inherit from `FileProcessingError`. - `FileWriteError`: To be raised when there is an issue writing to the file. It should inherit from `FileProcessingError`. 3. A function `read_file(filepath)` that: - Attempts to open and read the contents of a file specified by the `filepath`. - Raises `FileReadError` if the file cannot be read. 4. A function `write_file(filepath, content)` that: - Attempts to write `content` to a file specified by the `filepath`. - Raises `FileWriteError` if the content cannot be written to the file. 5. Each custom exception must take an optional `message` string and an `error_code` integer. If not provided, `message` should default to a generic error message, and `error_code` should default to `None`. Specifications: - Implement the `FileProcessingError`, `FileReadError`, and `FileWriteError` classes. - Implement the `read_file` and `write_file` functions. - Ensure that appropriate exception messages and error codes are set upon raising the exceptions. # Example: ```python try: read_file(\'nonexistent_file.txt\') except FileReadError as e: print(f\'Read Error: {e.message} (Code: {e.error_code})\') try: write_file(\'/restricted_location/out.txt\', \'Hello, World!\') except FileWriteError as e: print(f\'Write Error: {e.message} (Code: {e.error_code})\') ``` Expected output: ``` Read Error: Cannot read the file. (Code: 404) Write Error: Cannot write to the file. (Code: 403) ``` # Notes: - You should determine and specify your own default error messages and codes. - Ensure to handle the built-in exceptions that might be raised during file operations and convert them into the custom exceptions defined.","solution":"class FileProcessingError(Exception): def __init__(self, message=\\"File processing error occurred\\", error_code=None): super().__init__(message) self.message = message self.error_code = error_code class FileReadError(FileProcessingError): def __init__(self, message=\\"Cannot read the file\\", error_code=404): super().__init__(message, error_code) class FileWriteError(FileProcessingError): def __init__(self, message=\\"Cannot write to the file\\", error_code=403): super().__init__(message, error_code) def read_file(filepath): try: with open(filepath, \'r\') as file: return file.read() except IOError as e: raise FileReadError(f\\"Cannot read the file. {str(e)}\\", error_code=404) def write_file(filepath, content): try: with open(filepath, \'w\') as file: file.write(content) except IOError as e: raise FileWriteError(f\\"Cannot write to the file. {str(e)}\\", error_code=403)"},{"question":"Coding Assessment Question # Objective: Your task is to implement a function that performs a complex data manipulation using the `itertools` module. This exercise will test your proficiency with this module and your ability to combine various iterator building blocks efficiently. # Problem Statement: Implement a function `manipulate_data(data, ops)` which manipulates data based on a series of operations. Here\'s the breakdown: 1. **Input:** - `data`: A list of lists, where each inner list contains string elements. For example: ```python data = [ [\\"apple\\", \\"banana\\", \\"cherry\\"], [\\"dog\\", \\"elephant\\", \\"fox\\"], [\\"grape\\", \\"hippo\\", \\"iguana\\"] ] ``` - `ops`: A list of operations, where each operation is a tuple `(operation, args)`. The operations can be: - `\\"chain\\"`: Concatenate all inner lists. - `\\"cycle\\"`: Cycle the elements of each inner list a specified number of times. - `\\"compress\\"`: Compress each inner list based on given selectors. - `\\"repeat\\"`: Repeat each element of each inner list a specified number of times. - `\\"acc\\"`: Accumulate the elements of each inner list using a specified function (`sum`, `max`, `prod`). Example `ops` value: ```python ops = [ (\\"chain\\",), (\\"cycle\\", 2), (\\"compress\\", [1, 0, 1, 0, 1, 0]), (\\"repeat\\", 3), (\\"acc\\", \\"sum\\") ] ``` 2. **Output:** - The function will return a single list of manipulated data based on the specified operations. 3. **Constraints:** - Each operation in `ops` will be applied sequentially in the given order. - For the `\\"acc\\"` operation, only `sum`, `max`, and `prod` (product) functions will be used. - Operations are guaranteed to be valid. - Performance should be efficient for larger inputs. # Example: ```python from itertools import chain, cycle, compress, repeat, accumulate import operator def manipulate_data(data, ops): # Step 1: Flatten data using chain data = list(chain(*data)) # Step 2: Apply operations sequentially for operation, *args in ops: if operation == \\"chain\\": data = list(chain.from_iterable(data)) elif operation == \\"cycle\\": data = list(islice(cycle(data), len(data) * args[0])) elif operation == \\"compress\\": data = list(compress(data, args[0])) elif operation == \\"repeat\\": data = list(chain.from_iterable(repeat(elem, args[0]) for elem in data)) elif operation == \\"acc\\": func = None if args[0] == \\"sum\\": func = operator.add elif args[0] == \\"max\\": func = max elif args[0] == \\"prod\\": func = operator.mul data = list(accumulate(data, func)) return data # Example Input data = [ [\\"apple\\", \\"banana\\", \\"cherry\\"], [\\"dog\\", \\"elephant\\", \\"fox\\"], [\\"grape\\", \\"hippo\\", \\"iguana\\"] ] ops = [ (\\"chain\\",), (\\"cycle\\", 2), (\\"compress\\", [1, 0, 1, 0, 1, 0]), (\\"repeat\\", 3), (\\"acc\\", \\"sum\\") ] print(manipulate_data(data, ops)) # Expected Output may vary based on operations specifics ``` # Notes: - You are expected to use `itertools` functions wherever applicable. - Handle all edge cases like empty data or no operations gracefully.","solution":"from itertools import chain, cycle, compress, repeat, accumulate, islice import operator def manipulate_data(data, ops): # Flatten data using chain data = list(chain.from_iterable(data)) # Apply operations sequentially for operation, *args in ops: if operation == \\"chain\\": data = list(chain.from_iterable(data)) elif operation == \\"cycle\\": data = list(islice(cycle(data), len(data) * args[0])) elif operation == \\"compress\\": data = list(compress(data, args[0])) elif operation == \\"repeat\\": data = list(chain.from_iterable(repeat(item, args[0]) for item in data)) elif operation == \\"acc\\": if args[0] == \\"sum\\": data = list(accumulate(map(int, data), operator.add)) elif args[0] == \\"max\\": data = list(accumulate(map(int, data), max)) elif args[0] == \\"prod\\": data = list(accumulate(map(int, data), operator.mul)) return data"},{"question":"# PyTorch Coding Assessment Objective: The goal of this assessment is to evaluate your ability to use PyTorch (TorchScript) to build a neural network model and ensure it complies with the constraints imposed by TorchScript. Task: You are required to implement a simple feedforward neural network in PyTorch and convert it into TorchScript. The model should be capable of performing binary classification on a provided dataset. Instructions: 1. **Model Specification:** - Implement a feedforward neural network with the following architecture: - Input layer: Dimensions will depend on the input features. - Hidden Layer 1: A dense layer with 128 units and ReLU activation. - Hidden Layer 2: A dense layer with 64 units and ReLU activation. - Output Layer: A single unit with a Sigmoid activation function (for binary classification). 2. **Forward Method:** - Define the forward method for the network. 3. **TorchScript Conversion:** - Convert the implemented PyTorch model to TorchScript. 4. **Dataset Handling:** - Assume the dataset will be provided as a tensor with `X` being the input features and `y` being the labels. - Ensure your model can accept batches of input data. Constraints: - **Use Only Supported PyTorch Features:** Ensure the entire implementation is compliant with the supported and partially supported features of TorchScript mentioned in the provided documentation. - **Performance Requirements:** Your solution should be efficient in terms of both space and time. Input Format: ```python - X: A 2D tensor of dtype torch.float32 with shape (batch_size, n_features), where each row represents a feature vector. - y: A 1D tensor of dtype torch.float32 with shape (batch_size,), where each element represents the binary label for the corresponding input row in X. ``` Output Format: ```python Return the TorchScript model which can be used to perform inference on new data. ``` Example: ```python import torch import torch.nn as nn import torch.nn.functional as F import torch.jit as jit class SimpleFeedforwardNN(nn.Module): def __init__(self, input_dim): super(SimpleFeedforwardNN, self).__init__() self.fc1 = nn.Linear(input_dim, 128) self.fc2 = nn.Linear(128, 64) self.fc3 = nn.Linear(64, 1) def forward(self, x): # Define forward pass x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = torch.sigmoid(self.fc3(x)) return x # Dummy input and output tensors for inference # X = torch.randn(32, 10) # y = torch.randint(0, 2, (32,)).type(torch.FloatTensor) # Model instantiation and conversion to TorchScript # model = SimpleFeedforwardNN(input_dim=10) # script_model = torch.jit.script(model) # script_model is the final TorchScript model ``` Please ensure your implementation complies with the specified constraints and formats.","solution":"import torch import torch.nn as nn import torch.nn.functional as F import torch.jit as jit class SimpleFeedforwardNN(nn.Module): def __init__(self, input_dim): super(SimpleFeedforwardNN, self).__init__() self.fc1 = nn.Linear(input_dim, 128) self.fc2 = nn.Linear(128, 64) self.fc3 = nn.Linear(64, 1) def forward(self, x): # Define forward pass x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = torch.sigmoid(self.fc3(x)) return x def get_torchscript_model(input_dim): model = SimpleFeedforwardNN(input_dim) script_model = torch.jit.script(model) return script_model"},{"question":"# PyTorch Coding Assessment Question You are required to demonstrate your understanding of tensor creation, manipulation, and validation using PyTorch. Leverage the `torch.testing` module to create and validate tensors. Task: 1. **Create a function `generate_tensor`**: - Input: A tuple specifying the shape of the tensor (e.g., (3, 4)). - Output: A tensor of the specified shape with random values. 2. **Create a function `close_tensors`**: - Input: 1. tensor_a: A PyTorch tensor. 2. tensor_b: A PyTorch tensor of the same shape as tensor_a. 3. rtol: Relative tolerance (default=1e-05). 4. atol: Absolute tolerance (default=1e-08). - Output: Boolean value indicating whether the two tensors are close within the specified tolerances. 3. **Create a function `test_tensor_creation`**: - This function should: 1. Call `generate_tensor` to create a tensor. 2. Assert that the generated tensor is close to another tensor created with the same function (using the `assert_close` or `assert_allclose` function from `torch.testing`). Constraints: - Ensure the tensor shapes and values are handled randomly but consistent verification logic can be applied. - The functions should handle edge cases, such as empty shapes or invalid inputs gracefully. # Example Usage: ```python import torch def generate_tensor(shape): # Your implementation here def close_tensors(tensor_a, tensor_b, rtol=1e-05, atol=1e-08): # Your implementation here def test_tensor_creation(): # Your implementation here # Example Tests tensor1 = generate_tensor((3, 3)) tensor2 = generate_tensor((3, 3)) print(close_tensors(tensor1, tensor2)) # Expected: Based on random generation, usually False unless values happen to be very close by chance. test_tensor_creation() # Should not raise any assertion error if the implementation is correct ``` # Note: - Make sure to leverage `torch.testing` module\'s functions `assert_close` and `assert_allclose` appropriately within `close_tensors` and `test_tensor_creation`.","solution":"import torch def generate_tensor(shape): Generates a tensor of the specified shape with random values. return torch.randn(shape) def close_tensors(tensor_a, tensor_b, rtol=1e-05, atol=1e-08): Checks if two tensors are close within the specified tolerances. Parameters: - tensor_a: A PyTorch tensor. - tensor_b: A PyTorch tensor of the same shape as tensor_a. - rtol: Relative tolerance. - atol: Absolute tolerance. Returns: - Boolean indicating whether the two tensors are close within the tolerances. return torch.allclose(tensor_a, tensor_b, rtol=rtol, atol=atol) def test_tensor_creation(): Tests the tensor creation and verification using generate_tensor and close_tensors. tensor1 = generate_tensor((3, 4)) tensor2 = generate_tensor((3, 4)) # This essentially tests that the generated tensor is a valid tensor and close comparison works correctly assert close_tensors(tensor1, tensor1, rtol=1e-05, atol=1e-08), \\"Tensors should be close to themselves.\\""},{"question":"# Advanced Seaborn Palettes Exercise **Objective:** Demonstrate your understanding of seaborn\'s `mpl_palette` function and how it can be used to generate and manipulate color palettes for various types of visualizations. **Task:** You are provided with a dataset that contains information about different species of penguins. Your task is to: 1. Create a custom seaborn palette using the `sns.mpl_palette` function. 2. Use this custom palette to create a scatter plot of the dataset. **Dataset:** The dataset has the following structure: - `species`: Species of the penguin (categorical) - `bill_length_mm`: The bill length of the penguin in millimeters (numerical) - `bill_depth_mm`: The bill depth of the penguin in millimeters (numerical) - `flipper_length_mm`: The flipper length of the penguin in millimeters (numerical) - `body_mass_g`: The body mass of the penguin in grams (numerical) Here\'s a snippet of how the dataset looks: ``` species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g 0 Adelie 39.1 18.7 181.0 3750 1 Chinstrap 49.1 19.6 197.0 3800 2 Gentoo 47.2 15.5 217.0 4900 ... ``` **Steps to follow:** 1. Load the dataset into a pandas DataFrame. 2. Create a custom colormap using `sns.mpl_palette` from a continuous colormap of your choice (e.g., \\"viridis\\", \\"plasma\\"). Use at least 5 different colors. 3. Create a scatter plot using seaborn where: - The x-axis represents `bill_length_mm`. - The y-axis represents `bill_depth_mm`. - Each species is highlighted using the custom palette you created. 4. Customize the plot with appropriate labels and titles. **Constraints:** - Do not use any inbuilt seaborn palettes directly; instead, create a custom one. - Ensure the plot is clearly readable and visually appealing. **Expected Output:** A custom scatter plot with different species distinguished by colors from the custom palette. The plot should have well-labeled axes and an informative title. **Example code to get you started:** ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load dataset (replace \'your_data.csv\' with actual dataset file) df = pd.read_csv(\'your_data.csv\') # Create custom palette custom_palette = sns.mpl_palette(\\"viridis\\", 5) # Create scatter plot sns.scatterplot(x=\'bill_length_mm\', y=\'bill_depth_mm\', hue=\'species\', palette=custom_palette, data=df) # Customize plot plt.title(\'Penguin Species: Bill Length vs Bill Depth\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Bill Depth (mm)\') plt.legend(title=\'Species\') # Show plot plt.show() ```","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_scatter_plot(data_path): # Load dataset df = pd.read_csv(data_path) # Create custom palette using a continuous colormap custom_palette = sns.mpl_palette(\\"viridis\\", 3) # Create scatter plot sns.scatterplot(x=\'bill_length_mm\', y=\'bill_depth_mm\', hue=\'species\', palette=custom_palette, data=df) # Customize plot plt.title(\'Penguin Species: Bill Length vs Bill Depth\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Bill Depth (mm)\') plt.legend(title=\'Species\') # Show plot plt.show()"},{"question":"# XML Parsing with Custom SAX Handlers Objective In this exercise, you will create a custom SAX handler by subclassing the `xml.sax.handler.ContentHandler` class to parse an XML document. You will also need to create custom handlers for DTD, error handling, and lexical events. Your custom handler should be able to perform specific actions when encountering different parts of the XML file. Instructions 1. **ContentHandler Implementation:** - Create a class `CustomContentHandler` that inherits from `xml.sax.handler.ContentHandler`. - Implement methods to handle the start and end of elements, character data, and processing instructions. - Store all the start and end element tags in a list. - Collect all comments into a separate list. - Capture and print all processing instructions. 2. **ErrorHandler Implementation:** - Create a class `CustomErrorHandler` that inherits from `xml.sax.handler.ErrorHandler`. - Implement methods to handle warnings, recoverable errors, and fatal errors by logging appropriate messages. 3. **LexicalHandler Implementation:** - Create a class `CustomLexicalHandler` that inherits from `xml.sax.handler.LexicalHandler`. - Implement methods to capture CDATA sections and comments in the document. 4. **EntityResolver Implementation:** - Create a class `CustomEntityResolver` that inherits from `xml.sax.handler.EntityResolver`. - Implement the `resolveEntity` method to handle external entities. 5. **DTDHandler Implementation:** - Create a class `CustomDTDHandler` that inherits from `xml.sax.handler.DTDHandler`. - Implement methods to handle notation declarations and unparsed entities. 6. **Main Parsing Function:** - Write a function `parse_xml(file_path)` that: - Initializes a SAX parser. - Sets your custom handlers for content, DTD, error, lexical events, and entity resolving. - Parses the XML file specified by `file_path`. - Returns a dictionary containing: - `elements`: List of start and end element tags. - `comments`: List of captured comments. - `processing_instructions`: List of processing instructions. Example XML File Here is an example of a simple XML file (saved as `example.xml`) you can use to test your implementation: ```xml <?xml version=\\"1.0\\"?> <!DOCTYPE note [ <!ENTITY writer \\"John Doe\\"> ]> <note> <!-- This is a comment --> <to>Tove</to> <from>&writer;</from> <heading>Reminder</heading> <body>Don\'t forget me this weekend!</body> <?processing instruction?> <![CDATA[Some <CDATA> content]]> </note> ``` Constraints - Assume the XML file is well-formed. - Your code should handle any standard XML file structure conforming to the DTD, ContentHandler, ErrorHandler, EntityResolver, and LexicalHandler specifications. Expected Output Format The `parse_xml` function should output a dictionary like this: ```python { \'elements\': [\\"start to\\", \\"end to\\", \\"start from\\", \\"end from\\", \\"start heading\\", \\"end heading\\", \\"start body\\", \\"end body\\"], \'comments\': [\\"This is a comment\\"], \'processing_instructions\': [\\"processing instruction\\"] } ``` Implement the handlers and the parsing function according to the above specifications.","solution":"import xml.sax from xml.sax.handler import ContentHandler, ErrorHandler, LexicalHandler, EntityResolver, DTDHandler class CustomContentHandler(ContentHandler): def __init__(self): self.elements = [] self.comments = [] self.processing_instructions = [] def startElement(self, name, attrs): self.elements.append(f\\"start {name}\\") def endElement(self, name): self.elements.append(f\\"end {name}\\") def characters(self, content): pass # Not collecting character data in this example def processingInstruction(self, target, data): self.processing_instructions.append(f\\"{target} {data}\\") class CustomErrorHandler(ErrorHandler): def error(self, exception): print(f\\"Error: {exception}\\") def fatalError(self, exception): print(f\\"Fatal Error: {exception}\\") def warning(self, exception): print(f\\"Warning: {exception}\\") class CustomLexicalHandler(LexicalHandler): def __init__(self, content_handler): self.content_handler = content_handler def comment(self, content): self.content_handler.comments.append(content) def startCDATA(self): pass def endCDATA(self): pass class CustomEntityResolver(EntityResolver): def resolveEntity(self, publicId, systemId): return systemId class CustomDTDHandler(DTDHandler): def notationDecl(self, name, publicId, systemId): pass def unparsedEntityDecl(self, name, publicId, systemId, notationName): pass def parse_xml(file_path): parser = xml.sax.make_parser() content_handler = CustomContentHandler() error_handler = CustomErrorHandler() lexical_handler = CustomLexicalHandler(content_handler) entity_resolver = CustomEntityResolver() dtd_handler = CustomDTDHandler() parser.setContentHandler(content_handler) parser.setErrorHandler(error_handler) parser.setEntityResolver(entity_resolver) parser.setDTDHandler(dtd_handler) # Lexical Handler needs to be set separately parser.setProperty(\\"http://xml.org/sax/properties/lexical-handler\\", lexical_handler) parser.parse(file_path) return { \'elements\': content_handler.elements, \'comments\': content_handler.comments, \'processing_instructions\': content_handler.processing_instructions }"},{"question":"You are given the task to implement an out-of-core text classification system that can handle large data files which do not fit into memory. You will use a mini-batch approach and employ incremental learning with scikit-learn. Your goal is to create a function `out_of_core_classification` that processes a text dataset in chunks, vectorizes the text data using hashing, and trains an incremental classifier. # Function Signature ```python def out_of_core_classification(file_path: str, chunk_size: int, model, batch_size: int) -> model: Parameters: - file_path (str): Path to the CSV file containing the data. The file should have two columns: \\"text\\" and \\"label\\". - chunk_size (int): The number of rows to read per chunk. - model: An incremental learning model from scikit-learn that supports the `partial_fit` method. - batch_size (int): The batch size to use for incremental learning. Returns: - model: The trained model after processing all chunks. ``` # Input Format - The CSV file at `file_path` contains two columns: - `text`: A string representing the text data. - `label`: The class label for the text. - Example of the CSV content: ```csv text,label \\"Sample text data 1\\",0 \\"Another sample text data\\",1 ``` # Requirements 1. Read the data from the CSV file in chunks specified by `chunk_size` using pandas. 2. Use `HashingVectorizer` from `sklearn.feature_extraction.text` to convert the text data into vectors. 3. Train the provided model incrementally using the `partial_fit` method. 4. The first call to `partial_fit` must include the `classes` parameter containing all possible classes in the dataset. 5. Return the trained model after all chunks have been processed. # Constraints - The function should handle files much larger than available memory (demonstrate efficiency with chunks). - Ensure that each chunk is processed and model is trained incrementally for each batch. - `chunk_size` and `batch_size` should be chosen carefully to demonstrate the usage of incremental learning effectively. # Example Usage ```python from sklearn.linear_model import SGDClassifier import pandas as pd # Create a sample CSV for demonstration data = { \'text\': [\\"Text data 1\\", \\"Text data 2\\", \\"Text data 3\\", \\"Text data 4\\"], \'label\': [0, 1, 0, 1] } df = pd.DataFrame(data) df.to_csv(\'sample_data.csv\', index=False) # Initialize the model model = SGDClassifier() # Perform out-of-core classification trained_model = out_of_core_classification(\'sample_data.csv\', chunk_size=2, model=model, batch_size=2) print(trained_model) ``` # Notes - Include proper error handling for file reading and ensure the function handles potential issues, such as missing columns in the CSV. - You may use pandas for chunk reading and scikit-learn for vectorization and classification.","solution":"import pandas as pd from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier def out_of_core_classification(file_path: str, chunk_size: int, model, batch_size: int): Parameters: - file_path (str): Path to the CSV file containing the data. The file should have two columns: \\"text\\" and \\"label\\". - chunk_size (int): The number of rows to read per chunk. - model: An incremental learning model from scikit-learn that supports the `partial_fit` method. - batch_size (int): The batch size to use for incremental learning. Returns: - model: The trained model after processing all chunks. vectorizer = HashingVectorizer(n_features=2**20, alternate_sign=False) first_chunk = True classes = None for chunk in pd.read_csv(file_path, chunksize=chunk_size): if first_chunk: classes = chunk[\'label\'].unique() first_chunk = False for start in range(0, len(chunk), batch_size): end = start + batch_size batch = chunk[start:end] X_batch = vectorizer.transform(batch[\'text\']) y_batch = batch[\'label\'] if \'partial_fit\' in dir(model): model.partial_fit(X_batch, y_batch, classes=classes) else: raise ValueError(\\"Provided model does not support partial_fit method\\") return model"},{"question":"**Objective**: Implement and run a set of unit tests using the Python `unittest` framework demonstrating setup/teardown, skipping, subtests, and custom test suite creation. **Question**: You are tasked with testing a class `Calculator` that performs basic arithmetic operations. Implement the following: 1. **Calculator Class**: - Methods: `add(a, b)`, `subtract(a, b)`, `multiply(a, b)`, `divide(a, b)`. 2. **Test Cases**: - Create a test case class `TestCalculator` as a subclass of `unittest.TestCase`. - Implement `setUp()` and `tearDown()` to initialize and cleanup resources. - Write tests for each method: `test_add()`, `test_subtract()`, `test_multiply()`, `test_divide()`. - Use `subTest()` to test `divide(a, b)` with different denominators. - Add a test to intentionally raise and check for a `ZeroDivisionError` using `assertRaises`. - Implement skipping for a test using `@unittest.skipIf` based on a condition (e.g., some hypothetical version check). 3. **Test Suite**: - Create a test suite aggregating all test cases. - Run the test suite using `unittest.TextTestRunner()`. **Constraints**: - You must use Python\'s `unittest` framework. - The `Calculator` class methods should handle integer and floating-point arithmetic. - Your solution should be self-contained and import only the necessary modules. **Input and Output**: - No input required from the user. - The test results should be printed to the standard output. **Performance Requirements**: - Ensure efficient setup and teardown for each test. - Avoid redundant test execution. ```python import unittest # Implement the Calculator class class Calculator: def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): if b == 0: raise ZeroDivisionError(\\"division by zero\\") return a / b # Implement the Test case class class TestCalculator(unittest.TestCase): def setUp(self): self.calc = Calculator() def tearDown(self): del self.calc def test_add(self): self.assertEqual(self.calc.add(1, 1), 2) self.assertEqual(self.calc.add(-1, 1), 0) def test_subtract(self): self.assertEqual(self.calc.subtract(2, 1), 1) self.assertEqual(self.calc.subtract(-1, -1), 0) def test_multiply(self): self.assertEqual(self.calc.multiply(2, 3), 6) self.assertEqual(self.calc.multiply(-1, 1), -1) def test_divide(self): for val in [2, 4, 5]: with self.subTest(val=val): self.assertEqual(self.calc.divide(val, 1), val) with self.assertRaises(ZeroDivisionError): self.calc.divide(1, 0) @unittest.skipIf(True, \\"Skipping temporary test\\") def test_temporary_skip(self): self.assertEqual(self.calc.add(0, 0), 0) # Create a Test Suite def suite(): suite = unittest.TestSuite() suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestCalculator)) return suite # Run the Test Suite if __name__ == \'__main__\': runner = unittest.TextTestRunner() runner.run(suite()) ``` Write the `Calculator` class, the `TestCalculator` test case class with thorough test methods, and set up a test suite and runner to validate the functionality comprehensively. Implement skipping logic and employ parameterized subtests effectively.","solution":"import unittest # Implement the Calculator class class Calculator: def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): if b == 0: raise ZeroDivisionError(\\"division by zero\\") return a / b"},{"question":"**Question: Implementing a Concurrent Task Execution Framework using asyncio Synchronization Primitives** In this task, you will implement a simple framework for executing concurrent tasks using asyncio synchronization primitives. The tasks will simulate a resource that can only be accessed by a limited number of tasks at a time. # Requirements 1. **Task Function**: Create an asyncio coroutine function `task(id, sem, event, lock)` that: - Takes the task identifier (`id`), a semaphore instance (`sem`), an event instance (`event`), and a lock instance (`lock`) as arguments. - Waits for the `event` to be set. - Acquires the semaphore to gain access to the shared resource. - Inside the protected section, acquires the lock and prints a message indicating that the task has started. - Simulates some processing by awaiting `asyncio.sleep(1)`. - Within the sleep period, releases the lock and then the semaphore. - Finally, prints a message indicating that the task has finished. 2. **Main Function**: Create an asyncio coroutine function `main` that: - Initializes a `Semaphore` with a maximum of 3 concurrent accesses. - Creates an `Event` and a `Lock`. - Sets up and starts 5 tasks using the `task` function. - After a delay of 2 seconds, sets the event to start all tasks simultaneously. - Ensures that the main function waits for all tasks to complete. 3. **Execution**: Ensure your script runs `main` using `asyncio.run(main())`. # Input and Output Formats - **Input**: No specific input is required; the tasks and their scheduling are handled within the script. - **Output**: The script should print messages indicating when each task has started and finished. # Example Output ``` Setting up tasks... Starting all tasks... Task 1 has started Task 2 has started Task 3 has started Task 1 has finished Task 2 has finished Task 3 has finished Task 4 has started Task 5 has started Task 4 has finished Task 5 has finished All tasks have completed ``` # Constraints - You must use asyncio synchronization primitives as described. - Ensure proper usage of `async with`, semaphore, event, and lock. # Performance Requirements - Ensure that the implementation does not result in deadlocks or race conditions. - The tasks should respect the semaphore constraints, allowing only a limited number of concurrent accesses. Provide the complete implementation of the task function and main function based on the above requirements.","solution":"import asyncio async def task(id, sem, event, lock): Coroutine function representing a task. Waits for an event to be set, and then acquires a semaphore and a lock to simulate access to a shared resource. Args: id (int): Identifier for the task. sem (asyncio.Semaphore): Semaphore instance to limit concurrent access. event (asyncio.Event): Event instance that tasks wait for before starting. lock (asyncio.Lock): Lock instance to ensure exclusive access within the critical section. await event.wait() # Wait for the event to be set async with sem: # Acquire the semaphore async with lock: # Acquire the lock print(f\\"Task {id} has started\\") await asyncio.sleep(1) # Simulate task processing print(f\\"Task {id} has finished\\") async def main(): Main coroutine function that sets up and starts concurrent tasks using asyncio. sem = asyncio.Semaphore(3) # Semaphore limiting concurrent access to 3 event = asyncio.Event() lock = asyncio.Lock() print(\\"Setting up tasks...\\") # Create and start tasks tasks = [asyncio.create_task(task(i, sem, event, lock)) for i in range(1, 6)] print(\\"Starting all tasks in 2 seconds...\\") await asyncio.sleep(2) event.set() # Set the event, allowing all tasks to start await asyncio.gather(*tasks) # Wait for all tasks to complete print(\\"All tasks have completed\\") # Run the main coroutine using asyncio.run if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Question: Implement a Custom Mapping Class # Objective: Write a Python class that mimics a basic mapping (dictionary-like) object using the underlying concepts from the provided C function documentation. # Requirements: 1. Implement a class `CustomMapping`. 2. This class should have the following methods: - `__getitem__(self, key)`: Retrieve an item by key. - `__setitem__(self, key, value)`: Set an item by key. - `__delitem__(self, key)`: Delete an item by key. - `__len__(self)`: Return the number of items. - `__contains__(self, key)`: Check if the mapping contains a key. - `keys(self)`: Return a list of all keys. - `values(self)`: Return a list of all values. - `items(self)`: Return a list of all key-value pairs as tuples. # Constraints: 1. You cannot use Pythons built-in dictionary (`dict`) or any other collection module to store the key-value pairs directly. Instead, use two lists, one for keys and one for values. 2. Ensure your implementation handles edge cases such as adding, retrieving, and deleting items that do not exist. # Example: ```python mapping = CustomMapping() mapping[\'apple\'] = 3 mapping[\'banana\'] = 5 print(mapping[\'apple\']) # Output: 3 print(len(mapping)) # Output: 2 print(\'banana\' in mapping) # Output: True mapping[\'apple\'] = 4 print(mapping[\'apple\']) # Output: 4 del mapping[\'banana\'] print(len(mapping)) # Output: 1 print(mapping.keys()) # Output: [\'apple\'] print(mapping.values()) # Output: [4] print(mapping.items()) # Output: [(\'apple\', 4)] ``` # Performance Requirements: 1. The `__getitem__`, `__setitem__`, `__delitem__`, and `__contains__` methods should have an average case time complexity of O(n), where n is the number of items. # Testing: 1. You are expected to write at least two unit tests for each method described above.","solution":"class CustomMapping: def __init__(self): self._keys = [] self._values = [] def __getitem__(self, key): if key in self._keys: index = self._keys.index(key) return self._values[index] else: raise KeyError(f\\"Key \'{key}\' not found.\\") def __setitem__(self, key, value): if key in self._keys: index = self._keys.index(key) self._values[index] = value else: self._keys.append(key) self._values.append(value) def __delitem__(self, key): if key in self._keys: index = self._keys.index(key) self._keys.pop(index) self._values.pop(index) else: raise KeyError(f\\"Key \'{key}\' not found.\\") def __len__(self): return len(self._keys) def __contains__(self, key): return key in self._keys def keys(self): return self._keys.copy() def values(self): return self._values.copy() def items(self): return list(zip(self._keys, self._values))"},{"question":"**Concurrency Challenge with asyncio** **Objective:** Implement an asynchronous system using the `asyncio` library to manage multiple clients connecting to a server. Each client will send a message, and the server will process these messages concurrently, then return a response to each client. **Problem Statement:** You are required to implement two components: 1. **Asynchronous Server:** - The server should be able to handle multiple clients concurrently. - Each client sends a message containing a string. - The server should append the string `\\" - processed\\"` to the client\'s message and return it. - The server should gracefully handle client disconnections. 2. **Asynchronous Client:** - The client should establish a connection to the server. - Each client sends a unique message string. - The client should wait for the server\'s response and print it out. **Constraints:** - You need to handle at least 5 concurrent clients. - The server must use `asyncio` for all concurrency management. - Ensure proper exception handling for network errors. **Performance Requirements:** - The server should process each message within 1 second. **Function Signatures:** ```python import asyncio async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter): # Your code here pass async def start_server(): # Your code here pass async def client(message: str): # Your code here pass def main(): server = asyncio.run(start_server()) messages = [\\"Hello\\", \\"World\\", \\"Concurrent\\", \\"Programming\\", \\"With asyncio\\"] clients = [client(msg) for msg in messages] asyncio.run(asyncio.gather(*clients)) ``` **Expected Input and Output:** *Server:* - The server will be started and must listen for incoming client connections. *Clients:* - Each client sends its respective message to the server. - Each client receives the processed message from the server and prints it. ```plaintext Client sent: Hello Server processed: Hello - processed Client received: Hello - processed Client sent: World Server processed: World - processed Client received: World - processed ... ``` **Additional Information:** - Use `asyncio.StreamReader` and `asyncio.StreamWriter` for handling client connections. - Use the `asyncio.start_server` method to start the server. - Use `asyncio.open_connection` for client-side connection to the server. - Ensure the server runs continuously and handles incoming clients until manually stopped. **Guidelines:** 1. Implement the `handle_client`, `start_server`, and `client` functions. 2. Ensure that the server can process multiple clients concurrently. 3. Test your implementation with multiple clients to verify correct concurrency handling and communication.","solution":"import asyncio async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter): try: data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message!r} from {addr!r}\\") response = f\\"{message} - processed\\" print(f\\"Send: {response!r}\\") writer.write(response.encode()) await writer.drain() except Exception as e: print(f\\"Error handling client: {e}\\") finally: print(\\"Closing the connection\\") writer.close() await writer.wait_closed() async def start_server(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() async def client(message: str): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) print(f\'Send: {message!r}\') writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()!r}\') print(\\"Close the connection\\") writer.close() await writer.wait_closed() if __name__ == \'__main__\': server_task = asyncio.create_task(start_server()) messages = [\\"Hello\\", \\"World\\", \\"Concurrent\\", \\"Programming\\", \\"With asyncio\\"] client_tasks = [client(msg) for msg in messages] asyncio.run(asyncio.gather(server_task, *client_tasks))"},{"question":"# PyTorch Distributed Metrics Assessment In a distributed training setup, it is crucial to collect and report metrics efficiently to monitor training performance and diagnose issues. In this question, you are required to implement a function that configures a metric handler, logs some metrics, and demonstrates how to use these metrics. This will assess your understanding of the `torch.distributed.elastic.metrics` module. Requirements 1. **Function Implementation**: - Implement a function `distributed_metrics_demo` which sets up a metric handler, logs some example metrics, and retrieves them. Function Signature ```python def distributed_metrics_demo(): pass ``` Expected Steps: 1. **Configure a Metric Handler**: - Use `torch.distributed.elastic.metrics.configure` to set up a `ConsoleMetricHandler` for logging metrics. 2. **Log Example Metrics**: - Use `torch.distributed.elastic.metrics.put_metric` to log at least three different metrics (e.g., accuracy, loss, learning_rate) with some example values. 3. **Profile Metrics**: - Demonstrate the use of `torch.distributed.elastic.metrics.prof` if applicable. Example Usage ```python def distributed_metrics_demo(): import torch.distributed.elastic.metrics as metrics from torch.distributed.elastic.metrics.api import ConsoleMetricHandler # Step 1: Configure the console metric handler console_handler = ConsoleMetricHandler() metrics.configure(console_handler) # Step 2: Log Example Metrics metrics.put_metric(\\"accuracy\\", 0.95) metrics.put_metric(\\"loss\\", 0.05) metrics.put_metric(\\"learning_rate\\", 0.001) # Example: Print the configured metric handler (for demonstration purposes) print(\\"Configured metric handler:\\", console_handler) # Note: Students can add any other code here to demonstrate understanding ``` Input - No direct input to the function. Output - No direct output. However, the function should internally demonstrate setting up the metric handler and logging metrics. Constraints - Assume that the necessary packages (`torch`, `torch.distributed.elastic.metrics`) are already installed and available for import. # Key Points - The function should not return any values but should include print statements to demonstrate the steps taken. - Ensure to handle the configuration and logging of metrics correctly. - This question tests your ability to work with distributed PyTorch metric handlers and logging.","solution":"def distributed_metrics_demo(): import torch.distributed.elastic.metrics as metrics from torch.distributed.elastic.metrics.api import ConsoleMetricHandler # Step 1: Configure the console metric handler console_handler = ConsoleMetricHandler() metrics.configure(console_handler) # Step 2: Log Example Metrics metrics.put_metric(\\"accuracy\\", 0.95) metrics.put_metric(\\"loss\\", 0.05) metrics.put_metric(\\"learning_rate\\", 0.001) # Example: Print the configured metric handler (for demonstration purposes) print(\\"Configured metric handler:\\", console_handler) # Note: Additional code can be added here if necessary for demonstration # Call the function to demonstrate its behavior distributed_metrics_demo()"},{"question":"# Seaborn: Penguin Data Visualization In this coding task, you will demonstrate your understanding of the seaborn package by performing data visualization on the Palmer Penguins dataset. You are required to accomplish the following steps: 1. **Load the Penguins Dataset**: - Use the seaborn function to load the \\"penguins\\" dataset. 2. **Create a Histogram**: - Create a histogram of the `flipper_length_mm` variable and color the histogram bars by the `species` variable. 3. **Move and Customize the Legend**: - Position the legend in the \\"upper left\\" with the `bbox_to_anchor` parameter set to `(1, 1)`. - Modify other properties of the legend such as the title and frame. # Function Signature ```python def plot_penguins_histogram(): # Your code here pass ``` # Expected Output 1. When you call the `plot_penguins_histogram` function, it should: - Display a histogram of `flipper_length_mm` with different colors for each species. - Move the legend to the \\"upper left\\" corner using `bbox_to_anchor=(1,1)`. - Set the legend title to \\"Species\\" and remove the frame around the legend. # Performance and Constraints - Ensure that the code executes efficiently. - Follow best practices for data visualization. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguins_histogram(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create a histogram of the flipper_length_mm variable colored by species sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", multiple=\\"stack\\") # Customize and move the legend plt.legend(title=\\"Species\\", bbox_to_anchor=(1, 1), loc=\\"upper left\\", frameon=False) # Show the plot plt.show()"},{"question":"# Email Message Parsing and Generation You are provided a byte stream representing an email message. Your task is to: 1. **Parse** this byte stream into an `EmailMessage` object. 2. **Extract** and **manipulate** specific parts of the message such as the subject, sender, recipient, and body. 3. **Regenerate** the modified email back into a byte stream. # Instructions 1. **Parsing**: - Create a function `parse_email` which takes a byte stream as input and returns an `EmailMessage` object. 2. **Manipulation**: - Write a function `modify_email` that adjusts the `EmailMessage` object by: - Changing the subject to \\"Updated: Original Subject\\". - Adding a recipient \\"new.recipient@example.com\\" to the existing recipients. - Appending a line to the body content: \\"Please review the updates.\\" 3. **Generation**: - Create a function `generate_email` which takes the modified `EmailMessage` object and returns the serialized byte stream. # Function Specifications `parse_email(email_bytes: bytes) -> EmailMessage` - **Input**: `email_bytes`  a byte stream representing the original email message. - **Output**: An `EmailMessage` object parsed from the byte stream. `modify_email(email: EmailMessage) -> EmailMessage` - **Input**: `email`  an `EmailMessage` object. - **Output**: The modified `EmailMessage` object with the specified changes. `generate_email(email: EmailMessage) -> bytes` - **Input**: `email`  a modified `EmailMessage` object. - **Output**: A byte stream representing the new serialized email message. # Example Usage ```python original_email = b\\"Subject: Test EmailnFrom: sender@example.comnTo: recipient@example.comnnThis is a test email.\\" # Step 1: Parse email parsed_email = parse_email(original_email) # Step 2: Modify email modified_email = modify_email(parsed_email) # Step 3: Generate email new_email_bytes = generate_email(modified_email) print(new_email_bytes) ``` # Constraints - You must use the \\"email\\" package and its sub-modules to implement the functionality. - Ensure that the byte stream stays compliant with the relevant RFCs after modifications. - Preserve original formatting and encoding as much as possible when generating the updated email. Good luck!","solution":"from email import message_from_bytes from email.message import EmailMessage from email.policy import default as default_policy def parse_email(email_bytes: bytes) -> EmailMessage: Parse a byte stream into an EmailMessage object. return message_from_bytes(email_bytes, policy=default_policy) def modify_email(email: EmailMessage) -> EmailMessage: Modify the EmailMessage object: - Change the subject to \\"Updated: Original Subject\\". - Add a new recipient \\"new.recipient@example.com\\". - Append \\"Please review the updates.\\" to the body. # Update the subject original_subject = email[\'Subject\'] email.replace_header(\'Subject\', f\'Updated: {original_subject}\') # Add a new recipient original_recipients = email.get_all(\'To\', []) new_recipients = original_recipients + [\'new.recipient@example.com\'] email.replace_header(\'To\', \', \'.join(new_recipients)) # Append text to the body new_body = email.get_body().get_content() + \\"nnPlease review the updates.\\" email.set_content(new_body) return email def generate_email(email: EmailMessage) -> bytes: Generate a byte stream from an EmailMessage object. return email.as_bytes(policy=default_policy)"},{"question":"**Question:** You are given the task of implementing a Python function to configure an audio device for playback of a given byte data and set a specific volume control using the `ossaudiodev` module. # Task: 1. Implement the function `play_audio_and_set_volume(device_name: str, data: bytes, fmt: int, channels: int, rate: int, volume: int) -> bool` that: - Opens an audio playback device specified by `device_name`. - Configures the audio device with the given format `fmt`, number of channels `channels`, and sample rate `rate`. - Plays the provided byte data `data` through the audio device. - Opens the mixer device, sets the volume of `SOUND_MIXER_PCM` to the given `volume` level. - Ensures proper cleanup by closing both the audio and mixer devices. - Returns `True` if the operations complete successfully and `False` if any errors occur. # Parameters: - `device_name`: A string representing the audio device filename (e.g., `/dev/dsp`). - `data`: A bytes object containing the audio data to be played. - `fmt`: An integer representing the audio format (e.g., `ossaudiodev.AFMT_S16_LE`). - `channels`: An integer representing the number of audio channels (e.g., 1 for mono, 2 for stereo). - `rate`: An integer representing the sampling rate in samples per second (e.g., 44100 for CD quality). - `volume`: An integer representing the desired volume level (0 to 100). # Constraints: - Ensure that the audio device is correctly closed after the playback. - Ensure that the mixer device is correctly closed after setting the volume. - Handle any errors that may occur, returning `False` if an error is encountered. # Example Usage: ```python device_name = \\"/dev/dsp\\" data = b\\"x00x01x02...\\" # Example byte data fmt = ossaudiodev.AFMT_S16_LE channels = 2 rate = 44100 volume = 75 result = play_audio_and_set_volume(device_name, data, fmt, channels, rate, volume) print(result) # Should print True if successful, False otherwise ``` # Notes: - The function should handle and catch exceptions raised due to invalid device files or parameters. - Use the OSS constants for the audio format and volume control (e.g., `ossaudiodev.AFMT_S16_LE` and `ossaudiodev.SOUND_MIXER_PCM`).","solution":"import ossaudiodev def play_audio_and_set_volume(device_name: str, data: bytes, fmt: int, channels: int, rate: int, volume: int) -> bool: try: # Open the audio device for playback dsp = ossaudiodev.open(device_name, \'w\') # Set audio parameters dsp.setfmt(fmt) dsp.channels(channels) dsp.speed(rate) # Play the audio data dsp.write(data) # Open the mixer device mixer = ossaudiodev.openmixer() # Set the volume level # Volume needs to be set for left and right channels separately (0-100 range) vol_value = (volume << 8) | volume mixer.set(ossaudiodev.SOUND_MIXER_PCM, vol_value) # Close devices dsp.close() mixer.close() return True except Exception as e: print(str(e)) return False"},{"question":"# **Email Message Parser Implementation** **Description:** You are required to implement a function that parses an email message from a bytes-like object. The function should return significant details about the email, including headers, the main body content, and any defects found in the email message. **Function Signature:** ```python def parse_email_message(email_bytes: bytes) -> dict: pass ``` **Input:** - `email_bytes` (bytes): A bytes-like object representing the raw email message. **Output:** - (dict): A dictionary containing: - `\'headers\'`: A list of tuples where each tuple represents a header field and value. - `\'body\'`: The main body content of the email as a string. - `\'defects\'`: A list of defects found in the email, if any (empty list if none). **Constraints:** - The email message will be a standards-compliant RFC 5322 or RFC 6532 format. - You must handle both simple text messages and multipart MIME messages. # **Example:** ```python email_bytes = b\\"From: sender@example.comnTo: recipient@example.comnSubject: TestnnThis is the body of the email.\\" result = parse_email_message(email_bytes) assert result == { \'headers\': [ (\'From\', \'sender@example.com\'), (\'To\', \'recipient@example.com\'), (\'Subject\', \'Test\') ], \'body\': \'This is the body of the email.\', \'defects\': [] } ``` # **Notes:** - Use the `email.parser.BytesParser` for parsing the email message from the provided bytes-like object. - Extract the headers and body content accurately whether the email is simple or multipart. - Check for any defects during parsing and include them in the output dictionary. **Tip:** Refer to the section on `email.parser` in Pythons documentation for more details on how to use `BytesParser`.","solution":"from email.parser import BytesParser from email.policy import default def parse_email_message(email_bytes: bytes) -> dict: Parses an email message from a bytes-like object and returns significant details in a dictionary. Args: - email_bytes (bytes): The raw email message as a bytes-like object. Returns: - dict: A dictionary containing the headers, body, and any defects found. parser = BytesParser(policy=default) email_message = parser.parsebytes(email_bytes) headers = [(key, value) for key, value in email_message.items()] body = email_message.get_body(preferencelist=(\'plain\')).get_content() if email_message.get_body(preferencelist=(\'plain\')) else \'\' defects = email_message.defects return { \'headers\': headers, \'body\': body, \'defects\': defects }"},{"question":"# Python Coding Assessment: Working with Temporary Files and Directories Objective: Utilize the `tempfile` module in Python to create temporary files and directories, perform various operations on them, and ensure proper cleanup to prevent resource leaks. Description: Write a Python function called `process_temporary_files_and_directories()` that performs the following tasks: 1. **Create a Temporary Directory**: - Within this directory, create three temporary files using `NamedTemporaryFile` with the suffix `.txt`, prefixes `file1_`, `file2_`, and `file3_`, and mode `w+t`. 2. **Write Data to Temporary Files**: - Write the text `\\"Hello from file1!\\"`, `\\"Hello from file2!\\"`, and `\\"Hello from file3!\\"` to the corresponding temporary files. 3. **Read Data from Temporary Files**: - Read the contents of each file back to verify the written data. 4. **Create another Temporary Directory**: - Ensure that this new directory is created with a different suffix and prefix than the first directory. 5. **Move the files from the first Temporary Directory to the second Temporary Directory**: - Perform the move operation and ensure the files are accessible in the new directory. 6. **Return a Dictionary**: - Return a dictionary with filenames as keys and their corresponding contents as values, collected after the move operation from the second directory. Function Signature: ```python def process_temporary_files_and_directories() -> dict: pass ``` Constraints: - You must use the `tempfile` module for creating temporary files and directories. - Utilize context managers (`with` statements) to ensure that temporary files and directories are cleaned up appropriately. Example Output: ```python { \\"file1_v23aw9zs.txt\\": \\"Hello from file1!\\", \\"file2_gb9jd0fa.txt\\": \\"Hello from file2!\\", \\"file3_mqo93nfk.txt\\": \\"Hello from file3!\\" } ``` Note that the exact filenames may differ based on the random suffix generated. Performance Requirements: - The function should correctly handle the creation, manipulation, transfer, and cleanup of temporary resources. - Ensure that all temporary files and directories are cleaned up automatically, preventing any resource leaks.","solution":"import tempfile import shutil import os def process_temporary_files_and_directories(): file_contents = [\\"Hello from file1!\\", \\"Hello from file2!\\", \\"Hello from file3!\\"] filenames = [\'file1\', \'file2\', \'file3\'] content_dict = {} with tempfile.TemporaryDirectory() as tempdir1: temp_files = [] # Create the temporary files and write data for i in range(3): temp_file = tempfile.NamedTemporaryFile(suffix=\\".txt\\", prefix=filenames[i] + \\"_\\", mode=\'w+t\', dir=tempdir1, delete=False) temp_file.write(file_contents[i]) temp_file.seek(0) temp_files.append(temp_file) with tempfile.TemporaryDirectory() as tempdir2: for temp_file in temp_files: temp_file_name = os.path.basename(temp_file.name) temp_file_content = temp_file.read() temp_file.close() # Close the temp files to be able to move them new_path = shutil.move(temp_file.name, os.path.join(tempdir2, temp_file_name)) with open(new_path, \'r\') as moved_file: content_dict[temp_file_name] = moved_file.read() return content_dict"},{"question":"**Objective**: Assess your understanding of Seaborn\'s `plotting_context` function and your ability to create visualizations with different context settings. # Problem Statement: You are given a dataset of student scores in different subjects. Your task is to visualize this data using Seaborn. You are required to: 1. Generate a bar plot of the average scores for each subject in the default context. 2. Generate another bar plot of the average scores for each subject in the \\"talk\\" context. 3. Temporarily change the context to \\"paper\\" using a context manager and generate a bar plot of the average scores for each subject within this context manager. # Dataset: ```python data = { \'Subject\': [\'Math\', \'Science\', \'English\', \'History\'], \'Average Score\': [78, 85, 88, 90] } ``` # Expected Output: 1. Three bar plots displaying the average scores for each subject, each under a different context: \\"default\\", \\"talk\\", and \\"paper\\". # Constraints: - You must use Seaborn to create the visualizations. - Ensure that the different contexts are visible in the visualizations by appropriately setting context parameters. # Code Implementation: ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Create the DataFrame from the provided dataset data = { \'Subject\': [\'Math\', \'Science\', \'English\', \'History\'], \'Average Score\': [78, 85, 88, 90] } df = pd.DataFrame(data) # 1. Generate a bar plot in the default context plt.figure(figsize=(8, 6)) sns.barplot(x=\'Subject\', y=\'Average Score\', data=df) plt.title(\'Average Scores in Default Context\') plt.show() # 2. Generate a bar plot in the \\"talk\\" context sns.set_context(\\"talk\\") plt.figure(figsize=(8, 6)) sns.barplot(x=\'Subject\', y=\'Average Score\', data=df) plt.title(\'Average Scores in Talk Context\') plt.show() # 3. Temporarily change the context to \\"paper\\" and generate a bar plot with sns.plotting_context(\\"paper\\"): plt.figure(figsize=(8, 6)) sns.barplot(x=\'Subject\', y=\'Average Score\', data=df) plt.title(\'Average Scores in Paper Context\') plt.show() ``` # Evaluation Criteria: - Correctness: Ensure that the bar plots are generated correctly under the specified contexts. - Use of Seaborn: Appropriately utilize Seaborn functions and context management. - Visualization Clarity: The bar plots should clearly reflect the context changes.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_average_scores(): # Create the DataFrame from the provided dataset data = { \'Subject\': [\'Math\', \'Science\', \'English\', \'History\'], \'Average Score\': [78, 85, 88, 90] } df = pd.DataFrame(data) # 1. Generate a bar plot in the default context plt.figure(figsize=(8, 6)) sns.barplot(x=\'Subject\', y=\'Average Score\', data=df) plt.title(\'Average Scores in Default Context\') plt.show() # 2. Generate a bar plot in the \\"talk\\" context sns.set_context(\\"talk\\") plt.figure(figsize=(8, 6)) sns.barplot(x=\'Subject\', y=\'Average Score\', data=df) plt.title(\'Average Scores in Talk Context\') plt.show() # Reset the context back to the default after using \\"talk\\" sns.set_context(\\"notebook\\") # 3. Temporarily change the context to \\"paper\\" and generate a bar plot with sns.plotting_context(\\"paper\\"): plt.figure(figsize=(8, 6)) sns.barplot(x=\'Subject\', y=\'Average Score\', data=df) plt.title(\'Average Scores in Paper Context\') plt.show()"},{"question":"# Advanced Python: Data Persistence and Serialization You are required to design a system that reads employee records from an SQLite database, modifies these records in-memory, and then serializes the modified records to a file using Python\'s `pickle` module. The employee data includes `id`, `name`, `position`, and `salary`. **Task**: 1. **Database Setup**: - Create an SQLite database `employee.db`. - Use the following SQL schema: ```sql CREATE TABLE IF NOT EXISTS employees ( id INTEGER PRIMARY KEY, name TEXT NOT NULL, position TEXT NOT NULL, salary REAL NOT NULL ); INSERT INTO employees (name, position, salary) VALUES (\'Alice\', \'Engineer\', 70000), (\'Bob\', \'Manager\', 80000), (\'Charlie\', \'Technician\', 50000); ``` 2. **Record Fetch and Modification**: - Fetch all records from the `employees` table. - Implement a function to increase each employee\'s salary by 10%. 3. **Serialization**: - Serialize the modified employee records to a file named `serialized_employees.pkl`. **Requirements**: - Use the SQLite3 module for database operations. - Use the `pickle` module for serialization. - Implement appropriate error handling. - Ensure the solution is efficient and clear. **Expected Functions**: 1. `setup_database() -> None`: Creates the `employee.db` and initializes it with sample data. 2. `fetch_and_modify_records() -> List[Dict[str, Any]]`: Fetches employee records, modifies the salary, and returns the modified list of dictionaries. 3. `serialize_records(records: List[Dict[str, Any]], filename: str) -> None`: Serializes the provided records to the specified file. **Function Documentation**: - `setup_database() -> None` - **Description**: Sets up the SQLite database and inserts initial records if not already present. - **Input**: None - **Output**: None - `fetch_and_modify_records() -> List[Dict[str, Any]]` - **Description**: Fetches all records from the `employees` table, increases the salary by 10%, and returns the modified records. - **Input**: None - **Output**: List of dictionaries. Each dictionary should contain \'id\', \'name\', \'position\', and \'salary\'. - `serialize_records(records: List[Dict[str, Any]], filename: str) -> None` - **Description**: Serializes the records to a file. - **Input**: - `records`: List of dictionaries representing employee records. - `filename`: The name of the file to write the serialized data. - **Output**: None Here\'s an example structure to guide your implementation: ```python import sqlite3 import pickle from typing import List, Dict, Any def setup_database() -> None: # Implementation here pass def fetch_and_modify_records() -> List[Dict[str, Any]]: # Implementation here pass def serialize_records(records: List[Dict[str, Any]], filename: str) -> None: # Implementation here pass if __name__ == \\"__main__\\": setup_database() records = fetch_and_modify_records() serialize_records(records, \'serialized_employees.pkl\') ``` Test Cases: 1. Before running your script, verify that `employee.db` does not exist. After running, check the existence of `employee.db` and `serialized_employees.pkl`. 2. Ensure that the salary in the `serialized_employees.pkl` file reflects a 10% increase compared to the original database values. **Constraints**: - Assume that the initial database size is small and manageable within memory. - Focus on using provided modules and avoid using non-standard Python libraries.","solution":"import sqlite3 import pickle from typing import List, Dict, Any def setup_database() -> None: Sets up the SQLite database and inserts initial records if not already present. conn = sqlite3.connect(\'employee.db\') cursor = conn.cursor() # Create table cursor.execute(\'\'\' CREATE TABLE IF NOT EXISTS employees ( id INTEGER PRIMARY KEY, name TEXT NOT NULL, position TEXT NOT NULL, salary REAL NOT NULL ) \'\'\') # Insert initial data cursor.execute(\'SELECT COUNT(*) FROM employees\') count = cursor.fetchone()[0] if count == 0: cursor.executemany(\'\'\' INSERT INTO employees (name, position, salary) VALUES (?, ?, ?) \'\'\', [ (\'Alice\', \'Engineer\', 70000), (\'Bob\', \'Manager\', 80000), (\'Charlie\', \'Technician\', 50000) ]) conn.commit() conn.close() def fetch_and_modify_records() -> List[Dict[str, Any]]: Fetches all records from the employees table, increases the salary by 10%, and returns the modified records. conn = sqlite3.connect(\'employee.db\') cursor = conn.cursor() cursor.execute(\'SELECT * FROM employees\') records = cursor.fetchall() modified_records = [] for record in records: id_, name, position, salary = record modified_records.append({ \'id\': id_, \'name\': name, \'position\': position, \'salary\': salary * 1.10 }) conn.close() return modified_records def serialize_records(records: List[Dict[str, Any]], filename: str) -> None: Serializes the records to a file. with open(filename, \'wb\') as file: pickle.dump(records, file) if __name__ == \\"__main__\\": setup_database() records = fetch_and_modify_records() serialize_records(records, \'serialized_employees.pkl\')"},{"question":"# Question: File System Explorer **Objective:** Implement a function that recursively explores a directory and returns a structured dictionary describing the directory tree. Each node in this dictionary should include details about file type and permissions using the `stat` module. **Function Signature:** ```python def explore_directory(path: str) -> dict: pass ``` **Input:** - `path` (str): The path to the directory that needs to be explored. **Output:** - A dictionary representing the directory tree. Each key is a file or directory name, and each value is a dictionary with the following keys: - `type` (str): The type of the file (e.g., \\"directory\\", \\"regular file\\", \\"character device\\", etc.) - `permissions` (str): The file\'s permissions in the symbolic format (e.g., \\"-rwxr-xr-x\\") - If the item is a directory, it should have a sub-dictionary named `contents` representing its contents recursively. **Constraints:** - Assume all paths provided are valid and accessible. - The solution should be efficient in terms of both time and space complexity. - Handle special files such as sockets, FIFOs, and symbolic links correctly. - Use the functions in the `stat` module to determine file types and permissions. **Example Usage:** ```python import os import stat def explore_directory(path: str) -> dict: def explore(path): contents = {} for entry in os.listdir(path): full_path = os.path.join(path, entry) mode = os.lstat(full_path).st_mode if stat.S_ISDIR(mode): file_type = \'directory\' permissions = stat.filemode(mode) contents[entry] = { \'type\': file_type, \'permissions\': permissions, \'contents\': explore(full_path) } elif stat.S_ISREG(mode): file_type = \'regular file\' permissions = stat.filemode(mode) contents[entry] = { \'type\': file_type, \'permissions\': permissions } elif stat.S_ISCHR(mode): file_type = \'character device\' permissions = stat.filemode(mode) contents[entry] = { \'type\': file_type, \'permissions\': permissions } elif stat.S_ISBLK(mode): file_type = \'block device\' permissions = stat.filemode(mode) contents[entry] = { \'type\': file_type, \'permissions\': permissions } elif stat.S_ISFIFO(mode): file_type = \'FIFO\' permissions = stat.filemode(mode) contents[entry] = { \'type\': file_type, \'permissions\': permissions } elif stat.S_ISLNK(mode): file_type = \'symbolic link\' permissions = stat.filemode(mode) contents[entry] = { \'type\': file_type, \'permissions\': permissions } elif stat.S_ISSOCK(mode): file_type = \'socket\' permissions = stat.filemode(mode) contents[entry] = { \'type\': file_type, \'permissions\': permissions } else: file_type = \'unknown\' permissions = stat.filemode(mode) contents[entry] = { \'type\': file_type, \'permissions\': permissions } return contents return explore(path) # Test the function result = explore_directory(\'/path/to/directory\') print(result) ``` **Explanation:** The `explore_directory` function takes a directory path and returns a dictionary representing the directory tree. It uses the `stat` module to determine the type and permissions of each file or directory within the tree, and recursively explores subdirectories. This approach ensures that all file types and permissions are correctly identified and represented in a human-readable format.","solution":"import os import stat def explore_directory(path: str) -> dict: def explore(path): contents = {} for entry in os.listdir(path): full_path = os.path.join(path, entry) mode = os.lstat(full_path).st_mode if stat.S_ISDIR(mode): file_type = \'directory\' permissions = stat.filemode(mode) contents[entry] = { \'type\': file_type, \'permissions\': permissions, \'contents\': explore(full_path) } elif stat.S_ISREG(mode): file_type = \'regular file\' permissions = stat.filemode(mode) contents[entry] = { \'type\': file_type, \'permissions\': permissions } elif stat.S_ISCHR(mode): file_type = \'character device\' permissions = stat.filemode(mode) contents[entry] = { \'type\': file_type, \'permissions\': permissions } elif stat.S_ISBLK(mode): file_type = \'block device\' permissions = stat.filemode(mode) contents[entry] = { \'type\': file_type, \'permissions\': permissions } elif stat.S_ISFIFO(mode): file_type = \'FIFO\' permissions = stat.filemode(mode) contents[entry] = { \'type\': file_type, \'permissions\': permissions } elif stat.S_ISLNK(mode): file_type = \'symbolic link\' permissions = stat.filemode(mode) contents[entry] = { \'type\': file_type, \'permissions\': permissions } elif stat.S_ISSOCK(mode): file_type = \'socket\' permissions = stat.filemode(mode) contents[entry] = { \'type\': file_type, \'permissions\': permissions } else: file_type = \'unknown\' permissions = stat.filemode(mode) contents[entry] = { \'type\': file_type, \'permissions\': permissions } return contents return explore(path)"},{"question":"# Problem Description You are tasked with writing a Python script that compresses and decompresses data using the `bz2` module. Your solution should demonstrate mastery of both file-based and incremental compression/decompression provided by this module. # Objective Implement a function `compress_and_verify(input_file: str, output_file: str, chunksize: int) -> bool` that: 1. Reads data from an input text file `input_file`. 2. Compresses the data incrementally and writes the compressed data to `output_file`. 3. Reads the incremental compressed data from `output_file`, decompresses it, and verifies the decompressed data matches the original data from `input_file`. # Function Signature ```python def compress_and_verify(input_file: str, output_file: str, chunksize: int) -> bool: ``` # Parameters - `input_file` (str): Path to the input text file. - `output_file` (str): Path to the output compressed file. - `chunksize` (int): The size of the chunks to read and compress incrementally. # Returns - `bool`: Returns `True` if the decompressed data matches the original data, `False` otherwise. # Constraints - The input file will be valid and accessible. - The output file path will be writable. - `chunksize` will be a positive integer less than or equal to the file size. # Example ```python # Assuming the file \\"example.txt\\" contains random text data assert compress_and_verify(\\"example.txt\\", \\"example.bz2\\", 1024) == True ``` # Note - Ensure efficient reading, compression, writing, decompression, and verification of data. - Handle any exceptions that might occur during file operations gracefully. You can use the following code snippet to create sample data for testing your function: ```python sample_text = Donec rhoncus quis sapien sit amet molestie. Fusce scelerisque vel augue nec ullamcorper. Nam rutrum pretium placerat. Aliquam vel tristique lorem, sit amet cursus ante. In interdum laoreet mi, sit amet ultrices purus pulvinar a. Nam gravida euismod magna, non varius justo tincidunt feugiat. Aliquam pharetra lacus non risus vehicula rutrum. Maecenas aliquam leo felis. Pellentesque semper nunc sit amet nibh ullamcorper, ac elementum dolor luctus. Curabitur lacinia mi ornare consectetur vestibulum. with open(\\"example.txt\\", \\"w\\") as f: f.write(sample_text) ``` Good luck!","solution":"import bz2 def compress_and_verify(input_file: str, output_file: str, chunksize: int) -> bool: try: # Read and compress the data incrementally with open(input_file, \'rb\') as f_in, bz2.BZ2File(output_file, \'wb\') as f_out: while True: chunk = f_in.read(chunksize) if not chunk: break f_out.write(chunk) # Read and decompress the data incrementally decompressed_data = b\'\' with bz2.BZ2File(output_file, \'rb\') as f_in: while True: chunk = f_in.read(chunksize) if not chunk: break decompressed_data += chunk # Read original data to compare with open(input_file, \'rb\') as f_in: original_data = f_in.read() # Verify decompressed data matches original data return decompressed_data == original_data except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"# XML Data Manipulation with `xml.etree.ElementTree` Objective You are provided with an XML string that contains data about various cities and their details. Your task is to parse this XML, perform multiple operations on it, and then output the resulting XML. Input Format 1. A string containing an XML document representing data about cities. 2. A string representing the name of a new city to be added. 3. A string representing the population of the new city. 4. A string representing the name of a city to be updated. Example XML string: ```xml <data> <city name=\\"Tokyo\\"> <country>Japan</country> <population>37393128</population> </city> <city name=\\"Delhi\\"> <country>India</country> <population>30290936</population> </city> </data> ``` Tasks 1. Parse the provided XML string. 2. Add a new city element to the XML tree with the provided name and population. Set its country to \\"Unknown\\". 3. Update the population of the city with the specified name if it exists. 4. Remove all cities with population less than 1000000. 5. Pretty-print the resulting XML. Function Signature ```python def manipulate_city_data(xml_string: str, new_city_name: str, new_city_population: str, city_to_update: str) -> str: pass ``` Constraints - Assume that city names are unique within the XML data. - All city names, populations, and country names are provided as strings. - Populations are represented as strings of digits. Example **Input:** ```python xml_string = \'\'\' <data> <city name=\\"Tokyo\\"> <country>Japan</country> <population>37393128</population> </city> <city name=\\"Delhi\\"> <country>India</country> <population>30290936</population> </city> </data> \'\'\' new_city_name = \\"New York\\" new_city_population = \\"8175133\\" city_to_update = \\"Delhi\\" ``` **Output:** ```xml <data> <city name=\\"Tokyo\\"> <country>Japan</country> <population>37393128</population> </city> <city name=\\"Delhi\\"> <country>India</country> <population>30290936</population> </city> <city name=\\"New York\\"> <country>Unknown</country> <population>8175133</population> </city> </data> ``` Notes - Ensure to handle XML parsing and modification using methods from `xml.etree.ElementTree`. - The output XML must be formatted with proper indentation for readability.","solution":"import xml.etree.ElementTree as ET def manipulate_city_data(xml_string: str, new_city_name: str, new_city_population: str, city_to_update: str) -> str: Parse, add, update, and manipulate XML city data as specified. Args: xml_string (str): A string representing the XML document. new_city_name (str): The name of the new city to be added. new_city_population (str): The population of the new city to be added. city_to_update (str): The name of the city whose population needs to be updated. Returns: str: The modified XML string. root = ET.fromstring(xml_string) # 1. Add a new city element with \\"Unknown\\" country and provided population new_city = ET.Element(\\"city\\", name=new_city_name) country_element = ET.SubElement(new_city, \\"country\\") country_element.text = \\"Unknown\\" population_element = ET.SubElement(new_city, \\"population\\") population_element.text = new_city_population root.append(new_city) # 2. Update the population of the specified city for city in root.findall(\\"city\\"): if city.attrib.get(\\"name\\") == city_to_update: for elem in city: if elem.tag == \\"population\\": elem.text = new_city_population # 3. Remove cities with population less than 1000000 for city in root.findall(\\"city\\"): for elem in city: if elem.tag == \\"population\\" and int(elem.text) < 1000000: root.remove(city) break # 4. Pretty-print the resulting XML def indent(elem, level=0): i = \\"n\\" + level*\\" \\" if len(elem): if not elem.text or not elem.text.strip(): elem.text = i + \\" \\" if not elem.tail or not elem.tail.strip(): elem.tail = i for subelem in elem: indent(subelem, level+1) if not subelem.tail or not subelem.tail.strip(): subelem.tail = i else: if level and (not elem.tail or not elem.tail.strip()): elem.tail = i return elem root = indent(root) return ET.tostring(root, encoding=\\"unicode\\")"},{"question":"# Email Parsing and Analysis Objective: In this task, you are required to implement a function that parses email messages incrementally from a bytes-like object, using the \\"BytesFeedParser\\". The function should then analyze the parsed message to extract specific information. Problem Statement: Implement a Python function `parse_and_analyze_email(data: List[bytes]) -> Tuple[int, List[str]]` that performs the following operations: 1. Incrementally parses the email message from the list of bytes using the `BytesFeedParser` class from the `email` package. 2. Once parsing is complete, extract the subject of the email and the list of all email addresses found in the `To`, `Cc`, and `Bcc` headers. 3. Return the total number of recipients and a list of unique email addresses found in the headers. Input: - `data`: A list of byte-like objects, where each element represents a part of the email message. The email message may be incomplete, and the function should stitch these parts together correctly. Output: - A tuple containing: - An integer representing the total number of unique email addresses in the `To`, `Cc`, and `Bcc` headers. - A list of unique email addresses from the headers mentioned above. Example: ```python data = [ b\\"From: sender@example.comrn\\", b\\"To: recipient1@example.com, recipient2@example.comrn\\", b\\"Cc: recipient3@example.comrn\\", b\\"Subject: Test Emailrn\\", b\\"rn\\", b\\"Hello, this is a test email.\\" ] output = parse_and_analyze_email(data) # Output: (3, [\'recipient1@example.com\', \'recipient2@example.com\', \'recipient3@example.com\']) ``` Constraints: - Ensure the function properly handles the incremental feeding of email data. - Validate the completeness of the email before proceeding with analysis. - Handle and ignore any defects or inconsistencies in the email format gracefully. Notes: - You can assume that the email data in the list is standards-compliant or can be reasonably parsed by `BytesFeedParser`. - Utilize the appropriate methods from the `email.parser.BytesFeedParser` to complete the parsing and analysis. Please provide your implementation of the `parse_and_analyze_email` function below: ```python def parse_and_analyze_email(data: List[bytes]) -> Tuple[int, List[str]]: # Your implementation here ```","solution":"from email.parser import BytesFeedParser from email.utils import getaddresses from typing import List, Tuple def parse_and_analyze_email(data: List[bytes]) -> Tuple[int, List[str]]: parser = BytesFeedParser() # Feed the data incrementally to the parser for part in data: parser.feed(part) # Get the parsed message message = parser.close() # Extract email addresses from To, Cc, and Bcc headers addresses = set() for header in [\'To\', \'Cc\', \'Bcc\']: if message[header]: header_addresses = getaddresses([message[header]]) for name, addr in header_addresses: addresses.add(addr) # Convert set to sorted list unique_addresses = sorted(addresses) # Return the number of unique addresses and the list itself return len(unique_addresses), unique_addresses"},{"question":"# Objective: Your task is to extend the functionality of Python\'s `reprlib.Repr` class to handle additional custom object types and modify some default size limits. # Scenario: You are working on a project where the default string representations of certain Python objects are too lengthy. To make them more readable, you need to customize the behavior of the `reprlib.Repr` class. Specifically, you need to handle a custom `Node` class used in a tree data structure. # Details: 1. **Node Class**: ```python class Node: def __init__(self, value, children=None): self.value = value self.children = children if children is not None else [] ``` 2. **Requirements**: - Create a subclass of `reprlib.Repr` named `CustomRepr`. - Override the `repr_Node` method to control the representation of the `Node` class. - The representation should include the value and a limited number of children (use the `maxchildren` attribute). - If the number of children exceeds `maxchildren`, indicate the omission with an ellipsis. - Set specific size limits: - `maxdict` to 10 - `maxlist` to 8 - `maxtuple` to 8 - `maxchildren` to 3 for the `Node` representation 3. **Function to Implement**: ```python import reprlib class CustomRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxdict = 10 self.maxlist = 8 self.maxtuple = 8 self.maxchildren = 3 def repr_Node(self, obj, level): # Implement your method here pass def custom_repr(obj): aRepr = CustomRepr() return aRepr.repr(obj) ``` 4. **Example Usage**: ```python # Create a complex tree structure root = Node(1, children=[ Node(2, children=[Node(5), Node(6)]), Node(3), Node(4, children=[Node(7), Node(8), Node(9), Node(10)]) ]) print(custom_repr(root)) # Expected output format (the exact output might vary based on your implementation): # \\"Node(value=1, children=[Node(value=2, children=[Node(value=5), Node(value=6)]), Node(value=3), Node(value=4, children=[Node(value=7), ...]])\\" ``` # Constraints: - You must use the provided `reprlib` module functionalities to limit the sizes of object representations. - You must manage recursive representations properly, ensuring that deeply nested structures do not cause issues. Implement the `CustomRepr` class and the `custom_repr` function described above, testing it with the provided example.","solution":"import reprlib class Node: def __init__(self, value, children=None): self.value = value self.children = children if children is not None else [] class CustomRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxdict = 10 self.maxlist = 8 self.maxtuple = 8 self.maxchildren = 3 def repr_Node(self, obj, level): if level <= 0: return \'...\' repr_str = f\\"Node(value={repr(obj.value)}, children=[\\" children_count = len(obj.children) if children_count > self.maxchildren: for i in range(self.maxchildren): if i > 0: repr_str += \', \' repr_str += self.repr1(obj.children[i], level-1) repr_str += \', ...])\' else: for i, child in enumerate(obj.children): if i > 0: repr_str += \', \' repr_str += self.repr1(child, level-1) repr_str += \'])\' return repr_str def custom_repr(obj): aRepr = CustomRepr() return aRepr.repr(obj)"},{"question":"# Multi-client Echo Server with Python `socket` Objective: Implement a multi-client Echo server using Python\'s `socket` module. The server should be able to handle multiple clients concurrently, accept messages from each client, and echo those messages back to the client. Requirements: 1. **Server Implementation**: - Create a TCP socket server that listens on all available network interfaces on port 50007. - Accept connections from multiple clients concurrently. This can be achieved using Python threading or async features. - Echo back any message received from a client to the same client. - Gracefully handle client disconnects and other socket exceptions. - Ensure that the server can shut down cleanly. 2. **Client Implementation**: - Create a TCP socket client that connects to the server. - Send a user-specified message to the server. - Receive the echoed message from the server and print it. - Handle possible exceptions that might occur during the connection or communication process. Constraints: - Use the `socket` module for all socket operations. - The server must continue running until you manually stop it (e.g., by using a keyboard interrupt). - Each client should be able to make multiple requests without restarting the client program. Input/Output: - **Server**: - No input is required. - Expected to run indefinitely, handling client connections and messages. - Print connection logs and messages to the console. - **Client**: - Input: User-specified message string. - Output: Echoed message from the server printed to the console. Example: 1. **Starting the Server**: ```bash python server.py ``` Expected output (server console): ``` Server started, listening on port 50007. Connected by (\'client-ip-address\', port-number) Received message from (\'client-ip-address\', port-number): Hello, Server! ``` 2. **Running the Client**: ```bash python client.py Enter message to send: Hello, Server! Received: Hello, Server! ``` Additional Notes: - Ensure to use proper exception handling for socket operations to manage errors gracefully. - Implement proper resource cleanup to close sockets when they are no longer needed, particularly when the server shuts down.","solution":"import socket import threading def handle_client(conn, addr): with conn: print(f\\"Connected by {addr}\\") while True: try: data = conn.recv(1024) if not data: break print(f\\"Received message from {addr}: {data.decode()}\\") conn.sendall(data) except ConnectionResetError: break print(f\\"Connection with {addr} closed\\") def start_server(host=\'0.0.0.0\', port=50007): server = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server.bind((host, port)) server.listen() print(f\\"Server started, listening on port {port}\\") try: while True: conn, addr = server.accept() threading.Thread(target=handle_client, args=(conn, addr)).start() except KeyboardInterrupt: print(\\"nServer shutting down...\\") finally: server.close() if __name__ == \\"__main__\\": start_server()"},{"question":"# Command-line Option Parser for File Operations The `optparse` module in Python allows for parsing command-line options, providing an interface to handle various command-line arguments and options. Your task is to create a command-line tool using the `optparse` module. This tool should perform file operations such as reading, writing, and listing files in a directory based on the specified command-line options. Requirements: 1. The tool should support the following command-line options: - `-r` or `--read`: Read the contents of a specified file. This option must be followed by the file path. - `-w` or `--write`: Write specified content to a given file. This option must be followed by the file path and the content enclosed in quotes. - `-l` or `--list`: List all files in the specified directory. This option must be followed by the directory path. 2. If the `-r/--read` option is used along with a file path, the program should output the contents of the specified file. 3. If the `-w/--write` option is used along with a file path and content, the program should write the content to the specified file, overwriting if it already exists. 4. If the `-l/--list` option is used along with a directory path, the program should list all files in the specified directory. 5. The program should handle invalid option combinations gracefully, providing appropriate error messages. Example Usage: ```sh python file_tool.py -r /path/to/file.txt python file_tool.py -w /path/to/file.txt \\"This is the content to write.\\" python file_tool.py -l /path/to/directory ``` Expected Input and Output Formats: - Input: - Command-line arguments. - Output: - Contents of the specified file (for the read option). - Acknowledgement of successful write operation (for the write option). - List of files in the specified directory (for the list option). - Error messages for invalid operations or paths. Constraints: - Ensure the tool handles exceptions such as file not found, permission errors, and invalid paths. - Assume the operating system is Unix-like for simplicity in handling paths. Windows paths are not required to be supported. Performance Requirements: - The tool should handle directories with up to 1000 files efficiently. - The reading and writing operations should handle file sizes up to 10MB without significant delay. Implement the function `file_tool()` which parses the command-line arguments and performs the specified file operations. You can use the `optparse` module for argument parsing. ```python import os from optparse import OptionParser def file_tool(): parser = OptionParser() parser.add_option(\\"-r\\", \\"--read\\", dest=\\"read_path\\", help=\\"read the contents of FILE\\", metavar=\\"FILE\\") parser.add_option(\\"-w\\", \\"--write\\", dest=\\"write\\", help=\\"write CONTENT to FILE\\", metavar=\\"FILE\\", nargs=2) parser.add_option(\\"-l\\", \\"--list\\", dest=\\"list_dir\\", help=\\"list all files in DIRECTORY\\", metavar=\\"DIRECTORY\\") (options, args) = parser.parse_args() if options.read_path: # Read file logic elif options.write: file_path, content = options.write # Write file logic elif options.list_dir: # List directory contents logic else: parser.error(\\"No valid options specified.\\") # Uncomment the following line to test the script directly: # file_tool() ``` Complete the function `file_tool()` by implementing the file read, write, and list functionalities as described.","solution":"import os from optparse import OptionParser def file_tool(): parser = OptionParser() parser.add_option(\\"-r\\", \\"--read\\", dest=\\"read_path\\", help=\\"read the contents of FILE\\", metavar=\\"FILE\\") parser.add_option(\\"-w\\", \\"--write\\", dest=\\"write\\", help=\\"write CONTENT to FILE\\", metavar=\\"FILE\\", nargs=2) parser.add_option(\\"-l\\", \\"--list\\", dest=\\"list_dir\\", help=\\"list all files in DIRECTORY\\", metavar=\\"DIRECTORY\\") (options, args) = parser.parse_args() if options.read_path: if not os.path.isfile(options.read_path): print(f\\"Error: File \'{options.read_path}\' does not exist.\\") return try: with open(options.read_path, \'r\') as file: content = file.read() print(content) except Exception as e: print(f\\"Error reading file: {e}\\") elif options.write: file_path, content = options.write try: with open(file_path, \'w\') as file: file.write(content) print(f\\"Successfully wrote to {file_path}\\") except Exception as e: print(f\\"Error writing to file: {e}\\") elif options.list_dir: if not os.path.isdir(options.list_dir): print(f\\"Error: Directory \'{options.list_dir}\' does not exist.\\") return try: files = os.listdir(options.list_dir) for file in files: print(file) except Exception as e: print(f\\"Error listing directory: {e}\\") else: parser.error(\\"No valid options specified.\\") # Uncomment the following line to test the script directly: # file_tool()"},{"question":"# Advanced Python310 Coding Assessment Question Objective Implement a function in Python that demonstrates advanced file handling using the Python310 C API. The function will involve reading from a given file descriptor and writing specific contents to a new file. Students must demonstrate their understanding of handling file descriptors, reading lines from files, and writing contents to files, while also managing errors correctly. Function Details **Function Name:** `process_file_descriptor` **Parameters:** - `fd`: An integer representing the file descriptor for an already opened file. **Returns:** - A string indicating \\"Success\\" if the operations are completed successfully. - If an error occurs at any point, the function should raise an appropriate Python exception with a descriptive message. **Functionality:** 1. Convert the file descriptor `fd` to a Python file object. 2. Read all lines from the file descriptor. 3. Create a new Python file object to write the content read. 4. Write each line to the new file, prefixing it with the line number followed by a colon and a space. 5. Handle possible exceptions, such as invalid file descriptor, read/write errors, etc. Constraints - You may assume the file contains text data. - Ensure that the Python file objects are properly managed, such as closing the files after operations are completed. Example ```python def process_file_descriptor(fd: int) -> str: # your implementation here pass # Sample file descriptor usage (simulated for the example) import os fd = os.open(\\"input.txt\\", os.O_RDONLY) print(process_file_descriptor(fd)) # Should output \\"Success\\" ``` Notes - Students should refer to the related documentation for the `io` module to understand how file operations work in Python. - It is recommended to handle possible exceptions gracefully and to ensure file descriptors are closed correctly.","solution":"import os import io def process_file_descriptor(fd: int) -> str: try: # Convert the file descriptor to a Python file object for reading with os.fdopen(fd, \'r\') as file: lines = file.readlines() # Create a new file to write the processed content with open(\'output.txt\', \'w\') as outfile: for idx, line in enumerate(lines, 1): outfile.write(f\\"{idx}: {line}\\") return \\"Success\\" except Exception as e: raise RuntimeError(f\\"An error occurred: {e}\\")"},{"question":"Background You are working for an e-commerce company that tracks customer orders and product information. The company\'s data science team wants to analyze this data to understand patterns in sales, user behavior, and product performance. Suppose you are given a transaction DataFrame that records individual purchases and a products DataFrame that provides information about each product. Your task is to reshape and manipulate this data to derive useful insights. Provided DataFrames 1. **Transactions DataFrame (`transactions`)**: - `TransactionID`: Unique identifier for each transaction. - `CustomerID`: Unique identifier for each customer. - `ProductID`: Unique identifier for each product. - `Quantity`: Number of items purchased in the transaction. - `Price`: Price per item for the transaction. - `Date`: Date of the transaction. 2. **Products DataFrame (`products`)**: - `ProductID`: Unique identifier for each product. - `Category`: Category to which the product belongs. - `Brand`: Brand of the product. - `ProductName`: Name of the product. Task You need to implement the following functions using pandas: 1. **`top_purchasing_customers(transactions, n)`**: - Input: `transactions` DataFrame, integer `n`. - Output: DataFrame containing top `n` customers who have spent the most money, with columns: - `CustomerID` - `TotalSpent` - Constraints: Customers tied at the `n`th position should all be included. 2. **`sales_summary_by_category(transactions, products)`**: - Input: `transactions` DataFrame, `products` DataFrame. - Output: DataFrame summarizing total sales (in dollars) and total quantity sold by category, with columns: - `Category` - `TotalSales` - `TotalQuantity` 3. **`monthly_sales_trend(transactions, start_date, end_date)`**: - Input: `transactions` DataFrame, string `start_date`, string `end_date` in \\"YYYY-MM-DD\\" format. - Output: DataFrame showing monthly total sales (in dollars) within the provided date range, with columns: - `Month` - `TotalSales` - Behavior: If the `start_date` or `end_date` is not present in the data, the function should handle this gracefully by including all available data within the specified range. Constraints - The `transactions` and `products` DataFrames are guaranteed to have valid data with proper formatting. - The `start_date` and `end_date` will always be valid dates in \\"YYYY-MM-DD\\" format. - Ensure the solution is efficient and can handle large datasets. Example Usage ```python import pandas as pd # Example DataFrames transactions = pd.DataFrame({ \'TransactionID\': [1, 2, 3, 4, 5], \'CustomerID\': [101, 101, 102, 103, 101], \'ProductID\': [1, 2, 1, 3, 1], \'Quantity\': [2, 1, 1, 1, 3], \'Price\': [10, 20, 10, 15, 10], \'Date\': [\'2023-01-01\', \'2023-01-02\', \'2023-02-01\', \'2023-02-01\', \'2023-03-01\'] }) products = pd.DataFrame({ \'ProductID\': [1, 2, 3], \'Category\': [\'Electronics\', \'Furniture\', \'Clothing\'], \'Brand\': [\'BrandA\', \'BrandB\', \'BrandC\'], \'ProductName\': [\'Gadget\', \'Chair\', \'Shirt\'] }) # Function 1: Top Purchasing Customers top_purchasing_customers(transactions, 2) # Function 2: Sales Summary by Category sales_summary_by_category(transactions, products) # Function 3: Monthly Sales Trend monthly_sales_trend(transactions, \'2023-01-01\', \'2023-03-31\') ``` Your implementation should provide the correct DataFrames based on the input data. Please write the code for the above-described functions.","solution":"import pandas as pd def top_purchasing_customers(transactions, n): # Calculate total spent by each customer transactions[\'TotalSpent\'] = transactions[\'Quantity\'] * transactions[\'Price\'] total_spent_per_customer = transactions.groupby(\'CustomerID\')[\'TotalSpent\'].sum().reset_index() # Sort customers by total spent in descending order sorted_customers = total_spent_per_customer.sort_values(by=\'TotalSpent\', ascending=False) # Get the top n customers, including ties at the nth position if n > len(sorted_customers): return sorted_customers threshold_total_spent = sorted_customers.iloc[n-1][\'TotalSpent\'] top_customers = sorted_customers[sorted_customers[\'TotalSpent\'] >= threshold_total_spent] return top_customers def sales_summary_by_category(transactions, products): # Merge transactions with products to get the category information merged_df = transactions.merge(products, on=\'ProductID\') # Calculate total sales by category merged_df[\'TotalSales\'] = merged_df[\'Quantity\'] * merged_df[\'Price\'] summary_by_category = merged_df.groupby(\'Category\').agg({ \'TotalSales\': \'sum\', \'Quantity\': \'sum\' }).reset_index() # Rename columns for clarity summary_by_category = summary_by_category.rename(columns={\'Quantity\': \'TotalQuantity\'}) return summary_by_category def monthly_sales_trend(transactions, start_date, end_date): # Convert the Date column to datetime format transactions[\'Date\'] = pd.to_datetime(transactions[\'Date\']) # Filter transactions within the given date range filtered_transactions = transactions[ (transactions[\'Date\'] >= start_date) & (transactions[\'Date\'] <= end_date) ] # Calculate total sales per month filtered_transactions[\'Month\'] = filtered_transactions[\'Date\'].dt.to_period(\'M\') filtered_transactions[\'TotalSales\'] = filtered_transactions[\'Quantity\'] * filtered_transactions[\'Price\'] monthly_sales = filtered_transactions.groupby(\'Month\')[\'TotalSales\'].sum().reset_index() # Convert the Month column back to a string format like \'YYYY-MM\' monthly_sales[\'Month\'] = monthly_sales[\'Month\'].dt.strftime(\'%Y-%m\') return monthly_sales"},{"question":"Objective You are required to create and manipulate a PyTorch Export IR graph using the `torch.fx` module. This task will assess your understanding of building and handling intermediate representations of PyTorch computations. Problem Statement Implement a function `create_exported_program` that takes a simple PyTorch model\'s class and example inputs, and returns a customized `torch.export.ExportedProgram`. Additionally, implement a function `manipulate_graph` that performs specific transformations on this graph. Function Definitions 1. **create_exported_program(model_class: type, example_inputs: Tuple[torch.Tensor, ...]) -> torch.export.ExportedProgram:** - **Description**: This function creates an `ExportedProgram` for the given model and example inputs. - **Input**: - `model_class`: The class of the PyTorch model (inherits `torch.nn.Module`). - `example_inputs`: A tuple containing example input tensors for the model. - **Output**: Returns an instance of `torch.export.ExportedProgram`. 2. **manipulate_graph(exported_program: torch.export.ExportedProgram) -> torch.export.ExportedProgram:** - **Description**: This function takes an `ExportedProgram`, modifies its computational graph by adding a new operation, and returns the modified `ExportedProgram`. - **Input**: - `exported_program`: An instance of `torch.export.ExportedProgram` to be manipulated. - **Output**: Returns the modified `ExportedProgram`. - **Transformation to Perform**: 1. Add a new `torch.relu` operation to the computational graph. This operation should apply ReLU activation to the existing output of the graph. 2. Update the placeholder nodes and output nodes accordingly. Example ```python import torch from torch import nn from torch.export import export class MyModel(nn.Module): def forward(self, x): return x * 2 example_inputs = (torch.randn(1, 3),) # Function to create ExportedProgram def create_exported_program(model_class: type, example_inputs: Tuple[torch.Tensor, ...]) -> torch.export.ExportedProgram: model = model_class() exported_program = export(model, example_inputs) return exported_program # Function to manipulate the exported graph def manipulate_graph(exported_program: torch.export.ExportedProgram) -> torch.export.ExportedProgram: graph = exported_program.graph graph.append(placeholder(target=\'new_input\', args=())) graph.append(call_function(target=torch.relu, args=[\'%output\'])) graph.append(output(args=(\'%relu_output\',))) return exported_program # Testing the functions exported_prog = create_exported_program(MyModel, example_inputs) modified_prog = manipulate_graph(exported_prog) ``` Constraints - Ensure all necessary libraries are imported. - Follow the structure and attributes of Export IR as described in the provided documentation. - The manipulation should maintain the integrity of the graph by correctly updating all necessary fields and metadata.","solution":"import torch from torch import nn from torch.export import export from torch.fx import GraphModule, symbolic_trace from typing import Tuple import torch.fx class MyModel(nn.Module): def forward(self, x): return x * 2 def create_exported_program(model_class: type, example_inputs: Tuple[torch.Tensor, ...]) -> torch.fx.GraphModule: Creates an ExportedProgram for the given model and example inputs. model = model_class() traced = symbolic_trace(model) return traced def manipulate_graph(exported_program: torch.fx.GraphModule) -> torch.fx.GraphModule: Takes an ExportedProgram, modifies its computational graph by adding a ReLU operation. graph = exported_program.graph relu_node = None for node in graph.nodes: if node.op == \'output\': with graph.inserting_before(node): relu_node = graph.call_function(torch.relu, args=(node.args[0],)) node.args = (relu_node,) graph.lint() return torch.fx.GraphModule(exported_program, graph)"},{"question":"**Coding Assessment Question** You are tasked with creating a Python script that can execute and manage multiple subprocesses. The script should take a list of shell commands and execute them concurrently. For each command: 1. Capture both `stdout` and `stderr`. 2. Set a timeout for each command to prevent infinite execution. 3. Handle possible exceptions, such as `TimeoutExpired` and `CalledProcessError`. 4. After executing all commands, output a summary report that includes: - The command executed. - The return code. - The standard output (captured `stdout`). - The standard error (captured `stderr`). # Function Signature ```python def manage_subprocesses(commands: list, timeout: int) -> None: # Your code here ``` # Input - `commands`: A list of strings where each string is a shell command to be executed. Example: `[\\"ls -l\\", \\"echo \'Hello, World!\'\\", \\"sleep 5\\", \\"not_a_command\\"]` - `timeout`: An integer that represents the number of seconds each command is allowed to run before being forcibly terminated. # Output - No return value. - Print a summary report that includes: - The command executed. - The return code. - The captured `stdout`. - The captured `stderr`. # Example Usage ```python commands = [\\"ls -l\\", \\"echo \'Hello, World!\'\\", \\"sleep 5\\", \\"not_a_command\\"] timeout = 3 manage_subprocesses(commands, timeout) ``` # Expected Output ```plaintext Command: ls -l Return Code: 0 stdout: total 0 stderr: Command: echo \'Hello, World!\' Return Code: 0 stdout: Hello, World! stderr: Command: sleep 5 Return Code: -1 stdout: stderr: Error: Command \'sleep 5\' timed out after 3 seconds. Command: not_a_command Return Code: 127 stdout: stderr: /bin/sh: 1: not_a_command: not found ``` # Constraints - Use the `subprocess` module. - Ensure the commands that take too long are properly timed out. - Handle and print appropriate error messages for failed commands. **Instructions:** - Implement the function `manage_subprocesses` as specified. - Ensure your implementation handles multiple subprocesses concurrently and correctly captures the outputs and exceptions. - Test your function thoroughly to validate it against different types of commands and edge cases.","solution":"import subprocess def manage_subprocesses(commands: list, timeout: int) -> None: for command in commands: try: result = subprocess.run( command, shell=True, check=True, capture_output=True, text=True, timeout=timeout ) stdout = result.stdout.strip() stderr = result.stderr.strip() return_code = result.returncode except subprocess.CalledProcessError as e: stdout = e.stdout.strip() stderr = f\\"Error: {e.stderr.strip()}\\" return_code = e.returncode except subprocess.TimeoutExpired: stdout = \\"\\" stderr = f\\"Error: Command \'{command}\' timed out after {timeout} seconds.\\" return_code = -1 print(f\\"Command: {command}\\") print(f\\"Return Code: {return_code}\\") print(f\\"stdout: {stdout}\\") print(f\\"stderr: {stderr}\\")"},{"question":"Objective: Design and implement a PyTorch model using TorchScript. Your task will involve creating a custom neural network module, implementing forward methods, and using TorchScript functionalities such as type annotations and module attributes. Task: 1. Implement a custom neural network module using `torch.nn.Module`. 2. Use TorchScript to script the module and ensure it follows static typing rules. 3. Demonstrate the use of various TorchScript elements like type annotations, if-else conditions, and basic operations within the module. Requirements: 1. **Model Definition**: - Create a class `MyNet` that inherits from `torch.nn.Module`. - The model should have two linear layers with `in_features=3` and `out_features=3`. - Add a ReLU activation function between the layers. 2. **Forward Method**: - Implement the `forward` method. - Use TorchScript\'s type annotations to specify the input and output types. - Include a condition that modifies the output based on the input tensor\'s mean value. 3. **Scripting and Execution**: - Script the model using `torch.jit.script`. - Instantiate the model and pass a sample tensor through it. - Verify the correctness by printing the output. Constraints: 1. Ensure all necessary code is present within the class definition and the forward method. 2. Use appropriate type annotations (`Tensor`, `Optional[Tensor]`) for variables and method signatures. 3. The input tensor to the `forward` method will have the shape `(batch_size, 3)`. Performance: - Your solution should demonstrate a proper understanding of TorchScript\'s static typing and model scripting. - Ensure that the TorchScript model is executed correctly without runtime type errors. Example Usage: ```python import torch from typing import Optional, Tuple # Define your custom neural network module class MyNet(torch.nn.Module): def __init__(self): super(MyNet, self).__init__() self.fc1 = torch.nn.Linear(3, 3) self.fc2 = torch.nn.Linear(3, 3) self.relu = torch.nn.ReLU() @torch.jit.script def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.fc1(x) x = self.relu(x) y = self.fc2(x) if y.mean() > 0.5: y = y * 0.5 return y # Instantiate and script your model model = MyNet() scripted_model = torch.jit.script(model) # Create a sample input tensor input_tensor = torch.randn((4, 3)) # Pass the input tensor through the scripted model output = scripted_model(input_tensor) print(output) ``` In this task, you are expected to demonstrate your understanding of: - TorchScript\'s type annotations. - How to write conditional logic in TorchScript. - How to script a PyTorch model and execute it correctly.","solution":"import torch from typing import Optional, Tuple class MyNet(torch.nn.Module): def __init__(self): super(MyNet, self).__init__() self.fc1 = torch.nn.Linear(3, 3) self.fc2 = torch.nn.Linear(3, 3) self.relu = torch.nn.ReLU() def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.fc1(x) x = self.relu(x) y = self.fc2(x) if y.mean().item() > 0.5: y = y * 0.5 return y # Instantiate and script your model model = MyNet() scripted_model = torch.jit.script(model) # Create a sample input tensor input_tensor = torch.randn((4, 3)) # Pass the input tensor through the scripted model output = scripted_model(input_tensor) print(output)"},{"question":"# Question: Implement a Device and Stream Management Function Given the following functions from the `torch.accelerator` module, implement a function `initialize_device_and_stream` that sets the accelerator device and manages its execution stream. Your function should perform the following steps: 1. Check if any accelerator device is available. 2. If available, set the device index to the given `device_index`. If not, return a message saying \\"No available accelerator.\\" 3. Set the current stream to the given `stream_id`. 4. Synchronize the device to ensure all operations on the stream are completed. 5. Return the device index and stream currently in use. Here is the function signature you should implement: ```python def initialize_device_and_stream(device_index: int, stream_id: int) -> str: Initialize the accelerator device and manage its stream. Args: - device_index (int): Index of the device to set. - stream_id (int): Stream ID to set for execution. Returns: - str: A message indicating the current device index and stream, or an error message if no accelerator is available. ``` # Example ```python # Example usage: message = initialize_device_and_stream(0, 1) print(message) # Output: \\"Device index set to 0 and Stream set to 1\\" # or \\"No available accelerator.\\" ``` # Constraints: - Use the functions available in the `torch.accelerator` module. - Assume that setting the stream requires an integer `stream_id`. - Proper error handling should be ensured for invalid device or stream settings. - The function should return meaningful error messages if any of the settings fail. # Note: This question involves understanding and appropriately utilizing hardware accelerators in PyTorch. Proper handling and synchronization of device settings and streams are vital for efficient execution of operations on hardware accelerators.","solution":"import torch def initialize_device_and_stream(device_index: int, stream_id: int) -> str: Initialize the accelerator device and manage its stream. Args: - device_index (int): Index of the device to set. - stream_id (int): Stream ID to set for execution. Returns: - str: A message indicating the current device index and stream, or an error message if no accelerator is available. if not torch.cuda.is_available(): return \\"No available accelerator.\\" try: torch.cuda.set_device(device_index) stream = torch.cuda.Stream(device_index, priority=0) torch.cuda.current_stream(device_index).synchronize() return f\\"Device index set to {device_index} and Stream set to {stream_id}\\" except Exception as e: return f\\"Error setting device or stream: {str(e)}\\""},{"question":"Problem Statement: You are tasked with a specific problem that requires importing and managing resources from a dynamic module. Given a module that may not exist at the start of the program execution, you need to handle its import, access specific resources within it, and create a custom loader to manage repeated imports effectively. Requirements: 1. **Function 1: dynamic_import_module** - **Task**: Programmatically import a module, given its name, and handle cases where the module was not present at the start of the program. - **Input**: The name of the module as a string. - **Output**: The module object if the import is successful, otherwise raise an appropriate `ImportError`. - **Note**: If the module is newly created, make sure to invalidate the import caches. 2. **Function 2: read_resource_from_module** - **Task**: Access and read a specific resource from a module. - **Input**: - A module object. - The name of the resource as a string. - **Output**: The content of the resource as `bytes` if read in binary mode, or `str` if read in text mode. - **Note**: Ensure no resource path separators are present and handle errors gracefully if the resource does not exist. 3. **Function 3: create_custom_loader** - **Task**: Create a custom loader that can handle loading of modules from a specific directory. - **Input**: The directory path as a string where the module files are located. - **Output**: Return a finder callable that can be registered in `sys.path_hooks` to handle loading from the directory. ```python import importlib import importlib.util import sys from pathlib import Path from typing import Union def dynamic_import_module(module_name: str) -> \'ModuleType\': Programmatically imports a module by name. Args: module_name (str): The name of the module to import. Returns: ModuleType: The imported module object. Raises: ImportError: If the module cannot be found or imported. pass # Your implementation here def read_resource_from_module(module, resource_name: str, mode: str = \'rb\') -> Union[str, bytes]: Reads a resource from a given module. Args: module (ModuleType): The module from which to read the resource. resource_name (str): The name of the resource to read. mode (str): Mode to read the resource, \'rb\' for binary and \'r\' for text (default is \'rb\'). Returns: Union[str, bytes]: The content of the resource. Type depends on the mode. Raises: FileNotFoundError: If the resource cannot be found. ValueError: If an invalid mode is provided. pass # Your implementation here def create_custom_loader(directory_path: str): Creates a custom loader for modules in a specific directory. Args: directory_path (str): The directory path where module files are located. Returns: Callable: A finder callable to be registered in sys.path_hooks. pass # Your implementation here ``` Constraints and Notes: - Ensure the functions handle edge cases, such as invalid inputs and non-existing resources/modules. - When implementing the custom loader, consider cache management to avoid redundant I/O operations. - Use `importlib` capabilities wherever applicable, especially for dynamic importing and resource management. - Assume the repository directory structure remains unchanged throughout the program execution. Testing and Validation: Test your implementation using different scenarios such as: - Importing standard library modules and user-defined modules. - Reading different types of resources (text and binary) from various modules. - Creating a custom loader for a directory with multiple Python files. Make sure to document the functions and include type hints for clarity.","solution":"import importlib import importlib.util import sys from pathlib import Path from typing import Union, Any def dynamic_import_module(module_name: str) -> Any: Programmatically imports a module by name. Args: module_name (str): The name of the module to import. Returns: Any: The imported module object. Raises: ImportError: If the module cannot be found or imported. try: if module_name in sys.modules: return sys.modules[module_name] module_spec = importlib.util.find_spec(module_name) if module_spec is None: raise ImportError(f\\"Module {module_name} not found\\") module = importlib.util.module_from_spec(module_spec) sys.modules[module_name] = module module_spec.loader.exec_module(module) return module except Exception as e: raise ImportError(f\\"Could not import module {module_name}: {str(e)}\\") def read_resource_from_module(module, resource_name: str, mode: str = \'rb\') -> Union[str, bytes]: Reads a resource from a given module. Args: module (Any): The module from which to read the resource. resource_name (str): The name of the resource to read. mode (str): Mode to read the resource, \'rb\' for binary and \'r\' for text (default is \'rb\'). Returns: Union[str, bytes]: The content of the resource. Type depends on the mode. Raises: FileNotFoundError: If the resource cannot be found. ValueError: If an invalid mode is provided. if mode not in {\'r\', \'rb\'}: raise ValueError(f\\"Invalid mode {mode}, expected \'r\' or \'rb\'\\") resource_path = Path(module.__file__).parent / resource_name if not resource_path.exists(): raise FileNotFoundError(f\\"Resource {resource_name} not found in module {module.__name__}\\") with open(resource_path, mode) as f: return f.read() class CustomLoader(importlib.abc.SourceLoader): def __init__(self, directory_path): self.directory_path = directory_path def get_filename(self, fullname: str) -> str: return str(Path(self.directory_path) / f\\"{fullname.split(\'.\')[-1]}.py\\") def get_data(self, path: str) -> bytes: with open(path, \'rb\') as f: return f.read() def create_custom_loader(directory_path: str): Creates a custom loader for modules in a specific directory. Args: directory_path (str): The directory path where module files are located. Returns: Callable: A finder callable to be registered in sys.path_hooks. def custom_finder(path): if Path(path) == Path(directory_path): return CustomLoader(path) else: raise ImportError return custom_finder"},{"question":"Objective: Write a script in Python using the `smtplib` module to automatically send an email with an attachment. The script should: - Connect to an SMTP server using the `SMTP` class. - Authenticate with the SMTP server using a username and password. - Send an email with the provided subject, body, and attachment. - Handle any potential errors that might occur during the process. - Ensure the connection to the SMTP server is properly closed after sending the email. Requirements: 1. **Function Signature**: ```python def send_email(smtp_server: str, smtp_port: int, username: str, password: str, from_addr: str, to_addr: str, subject: str, body: str, attachment_path: str) -> None: ``` 2. **Input Parameters**: - `smtp_server` (str): The SMTP server address (e.g., \\"smtp.gmail.com\\"). - `smtp_port` (int): The port number of the SMTP server (e.g., 587 for TLS). - `username` (str): The username for logging into the SMTP server. - `password` (str): The password for logging into the SMTP server. - `from_addr` (str): The sender\'s email address. - `to_addr` (str): The recipient\'s email address. - `subject` (str): The subject of the email. - `body` (str): The body of the email. - `attachment_path` (str): The file path of the attachment to be included in the email. 3. **Output**: - None. However, the function should print messages indicating the steps being performed (e.g., \\"Connecting to the server...\\", \\"Sending email...\\", etc.). 4. **Constraints**: - Assume the attachment is a text file. - The function should handle exceptions gracefully and print appropriate error messages. 5. **Evaluation Criteria**: - Correct usage of the `smtplib` module. - Proper handling of email composition and attachment inclusion. - Robustness in handling exceptions. - Proper management of the SMTP connection lifecycle. Example Usage: ```python smtp_server = \\"smtp.gmail.com\\" smtp_port = 587 username = \\"your_email@gmail.com\\" password = \\"your_password\\" from_addr = \\"your_email@gmail.com\\" to_addr = \\"recipient@example.com\\" subject = \\"Test Email\\" body = \\"This is a test email with an attachment.\\" attachment_path = \\"path/to/attachment.txt\\" send_email(smtp_server, smtp_port, username, password, from_addr, to_addr, subject, body, attachment_path) ``` Hints: - Use the `email.mime` package to handle the email content and attachment. - Make sure to call `starttls()` if using a port that requires TLS (e.g., 587). - Use the `login()` method for SMTP authentication. - Use a `with` statement to ensure the SMTP connection is properly closed.","solution":"import smtplib from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.base import MIMEBase from email import encoders import os def send_email(smtp_server: str, smtp_port: int, username: str, password: str, from_addr: str, to_addr: str, subject: str, body: str, attachment_path: str) -> None: try: # Setup the MIME message = MIMEMultipart() message[\'From\'] = from_addr message[\'To\'] = to_addr message[\'Subject\'] = subject # Attach the body to the email message.attach(MIMEText(body, \'plain\')) # Open the file to be sent if os.path.exists(attachment_path): with open(attachment_path, \'rb\') as attachment: part = MIMEBase(\'application\', \'octet-stream\') part.set_payload(attachment.read()) encoders.encode_base64(part) part.add_header(\'Content-Disposition\', f\\"attachment; filename= {os.path.basename(attachment_path)}\\") message.attach(part) else: print(f\\"The file {attachment_path} does not exist.\\") return # Connect to the SMTP server print(\\"Connecting to the server...\\") with smtplib.SMTP(smtp_server, smtp_port) as server: server.starttls() print(\\"Logging in...\\") server.login(username, password) # Send the email print(\\"Sending email...\\") server.sendmail(from_addr, to_addr, message.as_string()) print(\\"Email sent successfully!\\") except smtplib.SMTPException as e: print(f\\"SMTP error occurred: {e}\\") except Exception as e: print(f\\"Error occurred: {e}\\")"},{"question":"**Objective:** Implement a function `system_maintenance()` that performs the following tasks: 1. **Operating System Interaction**: Create a new directory named `backup` in the current working directory. 2. **File Management**: Copy all `.txt` files from a source directory to the newly created `backup` directory. 3. **String Pattern Matching**: Within each copied `.txt` file, replace any two consecutive identical words with a single occurrence of the word. 4. **Command Line Argument Parsing**: The function should accept two command-line arguments: the source directory (which contains the `.txt` files to be backed up) and the backup directory name (default should be `backup` if not provided). **Requirements:** - The source directory path and the backup directory name should be input via command line arguments using the `argparse` module. - Handle any file I/O errors gracefully and display appropriate messages. - You may assume there are no nested directories within the source directory. **Function Signature:** ```python def system_maintenance(): pass ``` **Expected Input and Output Formats:** - **Input**: The function does not take parameters directly, but processes command line arguments. ```commandline python3 script.py /path/to/source_directory [backup_directory_name] ``` - **Output**: The function will not return any value but should perform the described tasks and print appropriate status messages. **Example:** Given a directory structure: ``` source_directory/ file1.txt # contains \\"This is is a test file.\\" file2.txt # contains \\"Hello Hello world!\\" ``` Running the command: ```commandline python3 script.py /path/to/source_directory mybackup ``` Should result in: - Creation of a new directory named `mybackup` in the current working directory. - The files `file1.txt` and `file2.txt` inside `mybackup` should contain: - `file1.txt`: \\"This is a test file.\\" - `file2.txt`: \\"Hello world!\\" **Constraints:** - File handling and file manipulation should be efficient. - Proper exception handling should be in place for file operations. - Use appropriate modules from Python\'s standard library to achieve the tasks. **Performance Considerations:** - The solution should be able to handle directories containing a large number of `.txt` files efficiently. Implement this function in the block provided below: ```python def system_maintenance(): import os import shutil import argparse from glob import glob import re parser = argparse.ArgumentParser(description=\\"System maintenance script to backup and process .txt files.\\") parser.add_argument(\'source_directory\', type=str, help=\'Path to the source directory containing .txt files.\') parser.add_argument(\'backup_directory\', type=str, nargs=\'?\', default=\'backup\', help=\'Name of the backup directory (default is \\"backup\\").\') args = parser.parse_args() source_dir = args.source_directory backup_dir = args.backup_directory try: if not os.path.exists(backup_dir): os.makedirs(backup_dir) txt_files = glob(os.path.join(source_dir, \'*.txt\')) for file in txt_files: filename = os.path.basename(file) shutil.copy(file, os.path.join(backup_dir, filename)) with open(os.path.join(backup_dir, filename), \'r+\') as f: content = f.read() new_content = re.sub(r\'b(w+)s+1b\', r\'1\', content) f.seek(0) f.write(new_content) f.truncate() print(f\\"Backup complete. Files copied to \'{backup_dir}\' and processed.\\") except Exception as e: print(f\\"Error during system maintenance: {e}\\") ```","solution":"import os import shutil import argparse from glob import glob import re def system_maintenance(): parser = argparse.ArgumentParser(description=\\"System maintenance script to backup and process .txt files.\\") parser.add_argument(\'source_directory\', type=str, help=\'Path to the source directory containing .txt files.\') parser.add_argument(\'backup_directory\', type=str, nargs=\'?\', default=\'backup\', help=\'Name of the backup directory (default is \\"backup\\").\') args = parser.parse_args() source_dir = args.source_directory backup_dir = args.backup_directory try: if not os.path.exists(backup_dir): os.makedirs(backup_dir) txt_files = glob(os.path.join(source_dir, \'*.txt\')) for file in txt_files: filename = os.path.basename(file) shutil.copy(file, os.path.join(backup_dir, filename)) with open(os.path.join(backup_dir, filename), \'r+\') as f: content = f.read() new_content = re.sub(r\'b(w+)s+1b\', r\'1\', content) f.seek(0) f.write(new_content) f.truncate() print(f\\"Backup complete. Files copied to \'{backup_dir}\' and processed.\\") except Exception as e: print(f\\"Error during system maintenance: {e}\\")"},{"question":"Objective Create a custom function that wraps Python\'s built-in `print` function. This custom function should enhance the `print` function by adding the ability to automatically prepend a timestamp to each message. Implement this functionality by using the `builtins` module to access the original `print` function. Requirements 1. Define a function named `timestamped_print` that accepts any number of arguments and keyword arguments, similar to how the built-in `print` function does. 2. The `timestamped_print` function should prepend the current timestamp in the format `YYYY-MM-DD HH:MM:SS` to each message before printing it. 3. Use the `time` module to get the current time and format it appropriately. 4. Your function must use `builtins.print` to print the final output. Input and Output - **Input:** Any number of arguments and keyword arguments, just like the built-in `print` function. - **Output:** Print statements with each message prepended by a timestamp in the format mentioned above. Example Usage ```python timestamped_print(\\"Hello, World!\\") # Output: 2023-10-04 12:45:26 Hello, World! timestamped_print(\\"Testing\\", \\"multiple\\", \\"arguments\\") # Output: 2023-10-04 12:45:26 Testing multiple arguments ``` Constraints - Do not modify the built-in `print` function directly. - Ensure that your `timestamped_print` function can handle all the flexibility and options of the built-in `print`. Performance The function should efficiently prepend the timestamp without introducing significant overhead. Implementation ```python import builtins from time import strftime, localtime def timestamped_print(*args, **kwargs): timestamp = strftime(\\"%Y-%m-%d %H:%M:%S\\", localtime()) builtins.print(timestamp, *args, **kwargs) ``` Test your function with various inputs to ensure it handles multiple arguments and keyword arguments correctly.","solution":"import builtins from time import strftime, localtime def timestamped_print(*args, **kwargs): Prints the provided arguments prefixed with the current timestamp. timestamp = strftime(\\"%Y-%m-%d %H:%M:%S\\", localtime()) builtins.print(timestamp, *args, **kwargs)"},{"question":"Abstract Base Classes and Abstract Methods You are required to design a small library for geometrical shapes using abstract base classes. This will involve creating an ABC for geometric figures and implementing specific shapes as subclasses. The goal is to demonstrate your understanding of defining abstract methods, utilizing subclasses correctly, and implementing abstract properties. **Task**: 1. Define an abstract base class `GeometricFigure` that includes: - An abstract method `area()` that returns the area of the figure. - An abstract method `perimeter()` that returns the perimeter of the figure. - An abstract property `number_of_sides` that returns the number of sides of the figure. 2. Implement concrete classes for the following shapes: - `Triangle`: A class with `area()`, `perimeter()`, and `number_of_sides` properly implemented. Assume a simple right-angled triangle with base and height provided during initialization. - `Rectangle`: A class with `area()`, `perimeter()`, and `number_of_sides` properly implemented. Assume width and height provided during initialization. - Ensure that these classes adhere to the requirements of the abstract base class `GeometricFigure`. 3. Write a function `identify_shape(shape)` that takes an instance of any of the shapes, determines its type, and prints the area, perimeter, and number of sides. **Constraints**: - Use the `abc` module correctly. - Make sure not to instantiate `GeometricFigure` directly. - Provide implementations for all abstract methods and properties in your concrete classes. **Example**: ```python from abc import ABC, abstractmethod class GeometricFigure(ABC): @abstractmethod def area(self): pass @abstractmethod def perimeter(self): pass @property @abstractmethod def number_of_sides(self): pass class Triangle(GeometricFigure): def __init__(self, base, height): self.base = base self.height = height def area(self): return 0.5 * self.base * self.height def perimeter(self): hypotenuse = (self.base**2 + self.height**2) ** 0.5 return self.base + self.height + hypotenuse @property def number_of_sides(self): return 3 class Rectangle(GeometricFigure): def __init__(self, width, height): self.width = width self.height = height def area(self): return self.width * self.height def perimeter(self): return 2 * (self.width + self.height) @property def number_of_sides(self): return 4 def identify_shape(shape): print(f\'Type: {type(shape).__name__}\') print(f\'Area: {shape.area()}\') print(f\'Perimeter: {shape.perimeter()}\') print(f\'Number of Sides: {shape.number_of_sides}\') # Example Usage triangle = Triangle(base=3, height=4) rectangle = Rectangle(width=4, height=6) identify_shape(triangle) identify_shape(rectangle) ``` **Expected Output**: ``` Type: Triangle Area: 6.0 Perimeter: 12.0 Number of Sides: 3 Type: Rectangle Area: 24 Perimeter: 20 Number of Sides: 4 ```","solution":"from abc import ABC, abstractmethod import math class GeometricFigure(ABC): @abstractmethod def area(self): pass @abstractmethod def perimeter(self): pass @property @abstractmethod def number_of_sides(self): pass class Triangle(GeometricFigure): def __init__(self, base, height): self.base = base self.height = height def area(self): return 0.5 * self.base * self.height def perimeter(self): hypotenuse = math.sqrt(self.base**2 + self.height**2) return self.base + self.height + hypotenuse @property def number_of_sides(self): return 3 class Rectangle(GeometricFigure): def __init__(self, width, height): self.width = width self.height = height def area(self): return self.width * self.height def perimeter(self): return 2 * (self.width + self.height) @property def number_of_sides(self): return 4 def identify_shape(shape): print(f\'Type: {type(shape).__name__}\') print(f\'Area: {shape.area()}\') print(f\'Perimeter: {shape.perimeter()}\') print(f\'Number of Sides: {shape.number_of_sides}\') # Example Usage triangle = Triangle(base=3, height=4) rectangle = Rectangle(width=4, height=6) identify_shape(triangle) identify_shape(rectangle)"},{"question":"# Problem: K-Nearest Neighbors Classifier with Performance Optimization You are tasked with implementing a K-Nearest Neighbors (KNN) Classifier using Scikit-learn. Your implementation should leverage the `KDTree` or `BallTree` algorithm based on the properties of the dataset for efficiency. The selection criteria should be based on the dimensionality and size of the dataset. # Requirements: 1. **Function Signature**: ```python def optimized_knn_classifier(X_train, y_train, X_test, k, n_jobs=1): Trains a K-Nearest Neighbors classifier using an optimized tree-based search method and predicts the labels of the test set. Parameters: X_train (ndarray): Training data features, shape (n_samples_train, n_features). y_train (ndarray): Training data labels, shape (n_samples_train,). X_test (ndarray): Test data features, shape (n_samples_test, n_features). k (int): Number of nearest neighbors to use. n_jobs (int): Number of parallel jobs to run for neighbors search (default=1). Returns: ndarray: Predicted labels for the test data, shape (n_samples_test,). 2. **Constraints**: - Data matrix `X` may have dimensions up to 100. - Size of the training set (`n_samples_train`) may be up to 10^5. - Parameter `k` will be a positive integer less than or equal to 50. 3. **Performance**: - If `n_features > 20`, use `BallTree` for the neighbors search. - If `n_features <= 20`, use `KDTree` for the neighbors search. - Use the parameter `n_jobs` to leverage parallel processing during neighbors search. # Example: ```python import numpy as np from sklearn.datasets import make_classification # Generate a synthetic dataset X_train, y_train = make_classification(n_samples=1000, n_features=30, random_state=42) X_test, _ = make_classification(n_samples=300, n_features=30, random_state=24) # Parameters k = 5 # Call the function predictions = optimized_knn_classifier(X_train, y_train, X_test, k, n_jobs=2) ``` # Instructions: - Implement the function `optimized_knn_classifier` following the provided function signature and requirements. - Write clean and well-documented code. - Ensure that your function handles different sizes and dimensions efficiently, leveraging the appropriate tree-based algorithm for performance optimization. # Testing: - The function will be tested with multiple datasets varying in size and dimensionality to ensure correctness and performance optimization.","solution":"from sklearn.neighbors import KNeighborsClassifier, KDTree, BallTree def optimized_knn_classifier(X_train, y_train, X_test, k, n_jobs=1): Trains a K-Nearest Neighbors classifier using an optimized tree-based search method and predicts the labels of the test set. Parameters: X_train (ndarray): Training data features, shape (n_samples_train, n_features). y_train (ndarray): Training data labels, shape (n_samples_train,). X_test (ndarray): Test data features, shape (n_samples_test, n_features). k (int): Number of nearest neighbors to use. n_jobs (int): Number of parallel jobs to run for neighbors search (default=1). Returns: ndarray: Predicted labels for the test data, shape (n_samples_test,). n_features = X_train.shape[1] # Choose algorithm based on the number of features if n_features > 20: algorithm = \'ball_tree\' else: algorithm = \'kd_tree\' knn = KNeighborsClassifier(n_neighbors=k, algorithm=algorithm, n_jobs=n_jobs) knn.fit(X_train, y_train) return knn.predict(X_test)"},{"question":"# Distributed Multiprocessing with PyTorch Your task is to create a Python script that utilizes the `torch.distributed.elastic.multiprocessing.subprocess_handler.SubprocessHandler` to perform a simple distributed task using multiple processes. Objective: - Implement a function `distributed_computation` that initializes a distributed multiprocessing environment. - Using `SubprocessHandler`, spawn multiple subprocesses to perform a simple computation task (for example, computing the square of numbers). - Collect the results from these subprocesses and return a combined result list. Input: - A list of integers that need to be processed. - The number of subprocesses to spawn. Output: - A list where each element is the result of a computation (square of the input number). Requirements: 1. Use `torch.distributed.elastic.multiprocessing.subprocess_handler.SubprocessHandler` to manage subprocesses. 2. Ensure proper inter-process communication to collect results from the subprocesses. 3. Handle process synchronization and ensure all subprocesses complete before collecting results. Constraints: - Each subprocess should independently perform its computation and communicate the result back to the main process. - The number of subprocesses should not exceed the length of the input list. Example: ```python import torch from torch.distributed.elastic.multiprocessing.subprocess_handler import SubprocessHandler def distributed_computation(nums, num_subprocesses): # Your code here to utilize SubprocessHandler pass # Example usage nums = [1, 2, 3, 4] num_subprocesses = 2 print(distributed_computation(nums, num_subprocesses)) # Expected Output: [1, 4, 9, 16] # Note: The order may vary depending on the implementation details of subprocesses. ``` Make sure your implementation correctly handles the initialization, execution, and termination phases of the subprocesses. Implementation Note: - Consider defining a helper function within `distributed_computation` to be executed by each subprocess. - Use any necessary synchronization primitives provided within PyTorch or Python\'s multiprocessing library to manage the subprocesses. Good luck!","solution":"import torch.distributed as dist from torch.distributed.elastic.multiprocessing.errors import record from torch.multiprocessing import Process, Queue import sys @record def square_number(rank, num, queue): Compute the square of the number and put the result in the queue. result = num ** 2 queue.put((rank, result)) def distributed_computation(nums, num_subprocesses): Initialize a distributed multiprocessing environment to compute squares of numbers. Args: - nums: List of integers to be squared. - num_subprocesses: Number of subprocesses to spawn. Returns: - List of squared values. assert num_subprocesses <= len(nums), \\"Number of subprocesses should not exceed length of input list.\\" # Queue for communication between processes queue = Queue() # Split the work among the provided number of subprocesses processes = [] results = [None] * len(nums) for i, num in enumerate(nums): process = Process(target=square_number, args=(i, num, queue)) processes.append(process) process.start() # Collect results from the subprocesses for _ in nums: rank, result = queue.get() results[rank] = result # Ensure all subprocesses have finished execution for process in processes: process.join() return results"},{"question":"# Objective Design and implement a secure file integrity checker using Python\'s cryptographic services. # Problem Statement You are tasked with creating a Python function that securely computes and verifies the hash of a file\'s content to detect any unauthorized modifications. Use `hashlib` for hashing the file\'s content and `hmac` for secure hashing with a secret key. # Instructions 1. Implement a function `generate_file_hash(filepath: str, key: str) -> str` that: - Takes two arguments: - `filepath`: A string representing the path to the file. - `key`: A string representing a secret key used for HMAC. - Returns a hexadecimal string representing the HMAC hash of the file\'s content using SHA-256. 2. Implement a function `verify_file_hash(filepath: str, key: str, expected_hash: str) -> bool` that: - Takes three arguments: - `filepath`: A string representing the path to the file. - `key`: A string representing a secret key used for HMAC. - `expected_hash`: A string representing the expected hash value to verify against. - Returns `True` if the computed hash matches the expected hash, otherwise `False`. # Constraints - You must use `hashlib` to implement SHA-256 for hashing. - You must use `hmac` to ensure the hash is securely computed with the provided key. - Handle any file I/O exceptions gracefully and return appropriate error messages. # Example ```python # Example file content: \\"Hello, World!\\" saved as \'example.txt\' key = \\"supersecretkey\\" filepath = \\"example.txt\\" hash_value = generate_file_hash(filepath, key) print(hash_value) # Example output: \'d2edcfe8860bebbd882c5c5ec957b0837df662e4b6b4a01fee76b4b76f1dc6ec\' result = verify_file_hash(filepath, key, hash_value) print(result) # Output: True result = verify_file_hash(filepath, key, \\"incorrecthash\\") print(result) # Output: False ``` # Notes - Test your functions with different files and keys to ensure they handle various scenarios correctly. - Consider edge cases such as empty files, long file contents, and invalid file paths.","solution":"import hashlib import hmac def generate_file_hash(filepath: str, key: str) -> str: Generates an HMAC hash of the file\'s contents using SHA-256. Args: - filepath (str): The path to the file. - key (str): The secret key for HMAC. Returns: - str: The hexadecimal HMAC hash of the file\'s contents. try: with open(filepath, \'rb\') as file: file_contents = file.read() file_hash = hmac.new(key.encode(), file_contents, hashlib.sha256).hexdigest() return file_hash except FileNotFoundError: raise Exception(\\"File not found.\\") except Exception as e: raise Exception(f\\"An error occurred: {e}\\") def verify_file_hash(filepath: str, key: str, expected_hash: str) -> bool: Verifies the HMAC hash of the file\'s contents. Args: - filepath (str): The path to the file. - key (str): The secret key for HMAC. - expected_hash (str): The expected HMAC hash. Returns: - bool: True if the computed hash matches the expected hash, False otherwise. try: computed_hash = generate_file_hash(filepath, key) return hmac.compare_digest(computed_hash, expected_hash) except Exception as e: raise Exception(f\\"An error occurred during verification: {e}\\")"},{"question":"# Isotonic Regression with scikit-learn You are given a dataset with 1-dimensional feature data `X` and corresponding target values `y`. Your task is to create an isotonic regression model using `scikit-learn`\'s `IsotonicRegression` class and evaluate its performance on both the training data and a set of new test data. Requirements: 1. **Input:** - Training data: `X_train` (1D numpy array), `y_train` (1D numpy array) - Testing data: `X_test` (1D numpy array) 2. **Output:** - Predictions for `X_test` as a 1D numpy array. 3. Implement the following steps: - Create and train an `IsotonicRegression` model on `X_train` and `y_train`. - Generate predictions for `X_test`. - Evaluate and return the mean squared error for the predictions on `X_train` against `y_train`. - Experiment with different settings for the `increasing` parameter (`True`, `False`, \'auto\') and compare the performance. 4. Ensure your implementation includes: - Function `train_isotonic_regression(X_train: np.ndarray, y_train: np.ndarray, increasing: Union[bool, str]) -> IsotonicRegression`: Trains the model. - Function `predict_isotonic_regression(model: IsotonicRegression, X_test: np.ndarray) -> np.ndarray`: Generates predictions. - Function `evaluate_performance(model: IsotonicRegression, X_train: np.ndarray, y_train: np.ndarray) -> float`: Returns the mean squared error on the training data. - Function `experiment_with_constraints(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> Dict[str, np.ndarray]`: Trains models with different `increasing` settings and stores predictions. Example: ```python def train_isotonic_regression(X_train, y_train, increasing): ir = IsotonicRegression(increasing=increasing) ir.fit(X_train, y_train) return ir def predict_isotonic_regression(model, X_test): return model.predict(X_test) def evaluate_performance(model, X_train, y_train): y_pred_train = model.predict(X_train) mse = np.mean((y_train - y_pred_train)**2) return mse def experiment_with_constraints(X_train, y_train, X_test): results = {} for setting in [True, False, \'auto\']: model = train_isotonic_regression(X_train, y_train, increasing=setting) results[str(setting)] = predict_isotonic_regression(model, X_test) return results ``` Using the functions above, you can evaluate the impact of different monotonic constraints on model performance and generate predictions for unseen data. Constraints: - `X_train` and `y_train` should have the same number of elements. - All input values should be real numbers. - Performance evaluations should be done using the mean squared error metric. Your submission must include the complete implementations of the functions as specified and output the relevant predictions for the test dataset `X_test`.","solution":"import numpy as np from sklearn.isotonic import IsotonicRegression from typing import Union, Dict def train_isotonic_regression(X_train: np.ndarray, y_train: np.ndarray, increasing: Union[bool, str]) -> IsotonicRegression: Trains an Isotonic Regression model on the given data with the specified monotonic constraint. Parameters: X_train (numpy.ndarray): 1D array of feature data. y_train (numpy.ndarray): 1D array of target values. increasing (bool or str): Monotonic constraint (\'True\', \'False\', or \'auto\'). Returns: IsotonicRegression: Trained Isotonic Regression model. ir = IsotonicRegression(increasing=increasing) ir.fit(X_train, y_train) return ir def predict_isotonic_regression(model: IsotonicRegression, X_test: np.ndarray) -> np.ndarray: Generates predictions for the given test data using the specified Isotonic Regression model. Parameters: model (IsotonicRegression): Trained Isotonic Regression model. X_test (numpy.ndarray): 1D array of feature data for testing. Returns: numpy.ndarray: Predicted values for the test data. return model.predict(X_test) def evaluate_performance(model: IsotonicRegression, X_train: np.ndarray, y_train: np.ndarray) -> float: Evaluates the performance of the Isotonic Regression model using mean squared error on the training data. Parameters: model (IsotonicRegression): Trained Isotonic Regression model. X_train (numpy.ndarray): 1D array of feature data used for training. y_train (numpy.ndarray): 1D array of target values used for training. Returns: float: Mean squared error of the model\'s predictions on the training data. y_pred_train = model.predict(X_train) mse = np.mean((y_train - y_pred_train)**2) return mse def experiment_with_constraints(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> Dict[str, np.ndarray]: Trains Isotonic Regression models with different monotonic constraints and generates predictions. Parameters: X_train (numpy.ndarray): 1D array of feature data for training. y_train (numpy.ndarray): 1D array of target values for training. X_test (numpy.ndarray): 1D array of feature data for testing. Returns: Dict[str, numpy.ndarray]: Dictionary containing test predictions for each monotonic constraint setting. results = {} for setting in [True, False, \'auto\']: model = train_isotonic_regression(X_train, y_train, increasing=setting) results[str(setting)] = predict_isotonic_regression(model, X_test) return results"},{"question":"You are required to create a set of customized plots using the seaborn library that illustrate different aspects of data visualization. The task will involve setting custom themes, creating different plot types, and configuring various parameters to produce desired visual effects. Requirements: 1. **Function Name:** `custom_seaborn_plots` 2. **Input:** - A dictionary containing data for plotting. Example: ```python { \\"categories\\": [\\"A\\", \\"B\\", \\"C\\"], \\"values\\": [4, 7, 1], \\"set_theme\\": { \\"style\\": \\"whitegrid\\", \\"palette\\": \\"dark\\", \\"custom_params\\": {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} }, \\"plot_types\\": [\\"bar\\", \\"line\\"], } ``` 3. **Output:** - No direct return. Display the plots inline using matplotlib. 4. **Constraints:** - The function should set seaborn theme and visualization parameters as per the provided `set_theme` dictionary. - The function should create the required plot types specified in the `plot_types` list. - Use appropriate seaborn and matplotlib functions to generate these plots. Detailed Steps: 1. Extract data for the categories and values from the input dictionary. 2. Extract specific theme settings from the `set_theme` dictionary: - Apply the style using `sns.set_theme(style=<style>, palette=<palette>, rc=<custom_params>)`. 3. For each plot type specified in `plot_types`: - Create and display the respective plots. Supported plot types are \\"bar\\" and \\"line\\". 4. Configure the overall appearance, ensuring the plots reflect the theme settings and custom parameters. Example: For the given input: ```python { \\"categories\\": [\\"A\\", \\"B\\", \\"C\\"], \\"values\\": [4, 7, 1], \\"set_theme\\": { \\"style\\": \\"whitegrid\\", \\"palette\\": \\"dark\\", \\"custom_params\\": {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} }, \\"plot_types\\": [\\"bar\\", \\"line\\"], } ``` The function should: 1. Apply the theme settings using `sns.set_theme`. 2. Generate a bar plot and a line plot using the provided data. 3. Display the plots inline, showing the customizations specified. # Note: Use comments to explain the logic within your code.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_seaborn_plots(data): Create and display customized seaborn plots. Parameters: data (dict): A dictionary containing data and plot settings. Example input: { \\"categories\\": [\\"A\\", \\"B\\", \\"C\\"], \\"values\\": [4, 7, 1], \\"set_theme\\": { \\"style\\": \\"whitegrid\\", \\"palette\\": \\"dark\\", \\"custom_params\\": {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} }, \\"plot_types\\": [\\"bar\\", \\"line\\"], } # Extract data for the categories and values categories = data[\\"categories\\"] values = data[\\"values\\"] # Extract specific theme settings set_theme = data.get(\\"set_theme\\", {}) style = set_theme.get(\\"style\\", \\"whitegrid\\") palette = set_theme.get(\\"palette\\", \\"dark\\") custom_params = set_theme.get(\\"custom_params\\", {}) # Apply the theme settings sns.set_theme(style=style, palette=palette, rc=custom_params) # Iterate over the plot types and create the necessary plots plot_types = data.get(\\"plot_types\\", []) for plot_type in plot_types: if plot_type == \\"bar\\": # Create a bar plot sns.barplot(x=categories, y=values) plt.title(\'Bar Plot\') plt.show() if plot_type == \\"line\\": # Create a line plot sns.lineplot(x=categories, y=values, marker=\'o\') plt.title(\'Line Plot\') plt.show()"},{"question":"# Thread Synchronization and Resource Management with Python\'s `threading` Module You are tasked with implementing a multi-threaded system to manage access to a shared resource using the `threading` module in Python. **Problem Statement:** You need to design and implement a `ResourceManager` class that controls access to a shared resource using a `Semaphore`. - The `ResourceManager` should start by initializing a `Semaphore` with a given number of maximum concurrent accesses. - The class should provide a method `access_resource` which simulates accessing the resource. This method should: - Acquire the semaphore. - Print the thread name and message indicating that the resource is being accessed. - Wait for a given amount of time (`time.sleep`). - Release the semaphore. - Handle cases where semaphore acquisition times out using exception handling. Additionally, implement a script that starts multiple threads to demonstrate the behavior of the `ResourceManager`. **Requirements:** 1. **ResourceManager Class:** - Initialize with `max_concurrency` to set up the semaphore. - A method `access_resource(self, wait_time: float)` which manages the access. 2. **Main Script:** - Starts a given number of threads. - Each thread calls the `access_resource` method. **Constraints:** - You should use `Semaphore` from the `threading` module. - Ensure thread-safe printing to the console. # Function Signatures ```python from threading import Semaphore, Thread import time class ResourceManager: def __init__(self, max_concurrency: int): Initialize the resource manager with a semaphore. pass def access_resource(self, wait_time: float): Simulate accessing a resource. pass if __name__ == \\"__main__\\": # Initialize ResourceManager resource_manager = ResourceManager(max_concurrency=3) # Create and start multiple threads threads = [] for i in range(10): thread = Thread(target=resource_manager.access_resource, args=(2,), name=f\\"Thread-{i+1}\\") threads.append(thread) thread.start() # Join threads to ensure all threads complete for thread in threads: thread.join() ``` # Expected Output The output should show multiple threads accessing the resource with concurrency control, including messages indicating which thread is accessing the resource and handling if a thread times out during acquisition. ``` Thread-1 is accessing the resource. Thread-2 is accessing the resource. Thread-3 is accessing the resource. Thread-4 is waiting to access the resource. ... Thread-4 is accessing the resource. ... ``` # Performance Requirements - The solution should efficiently manage access to the shared resource without unnecessary blocking or resource wastage. - Utilize proper synchronization to avoid race conditions. Implement the `ResourceManager` class and demonstrate the threading behavior.","solution":"from threading import Semaphore, Thread, current_thread import time class ResourceManager: def __init__(self, max_concurrency: int): Initialize the resource manager with a semaphore. self.semaphore = Semaphore(max_concurrency) def access_resource(self, wait_time: float): Simulate accessing a resource. acquired = self.semaphore.acquire(timeout=wait_time) if acquired: try: print(f\\"{current_thread().name} is accessing the resource.\\") time.sleep(wait_time) finally: self.semaphore.release() print(f\\"{current_thread().name} has released the resource.\\") else: print(f\\"{current_thread().name} could not acquire the semaphore.\\") if __name__ == \\"__main__\\": # Initialize ResourceManager resource_manager = ResourceManager(max_concurrency=3) # Create and start multiple threads threads = [] for i in range(10): thread = Thread(target=resource_manager.access_resource, args=(2,), name=f\\"Thread-{i+1}\\") threads.append(thread) thread.start() # Join threads to ensure all threads complete for thread in threads: thread.join()"},{"question":"Outlier Detection Comparison You are tasked with building and comparing different outlier detection models using scikit-learn\'s `IsolationForest`, `OneClassSVM`, and `EllipticEnvelope` methods. Your objective is to identify outliers in a given dataset and compare the performance of these models. Objectives: 1. Implement outlier detection using `IsolationForest`, `OneClassSVM`, and `EllipticEnvelope`. 2. Compare the models using appropriate metrics. 3. Visualize the decision boundaries of each model if applicable and the detected outliers. Instructions: 1. **Data Loading and Preprocessing:** - Load the `make_blobs` function from `sklearn.datasets` to generate a synthetic dataset with outliers. - Split the dataset into training and test sets. 2. **Model Implementation:** - Implement outlier detection using: - `IsolationForest` - `OneClassSVM` with the RBF kernel - `EllipticEnvelope` 3. **Training and Prediction:** - Train each model using the training set. - Predict and label each sample in the test set as inlier or outlier using each model. 4. **Evaluation:** - Evaluate and compare the models using precision, recall, and F1-score for the outlier class. - Use the `metrics.roc_auc_score` function to compute the ROC AUC for the outlier class across models. 5. **Visualization:** - Create scatter plots showing the decision boundaries of each model. - Highlight the detected outliers in the test set for each plot. - Provide a comparative analysis of the visualized outliers. Expected Function Signature: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_blobs from sklearn.ensemble import IsolationForest from sklearn.svm import OneClassSVM from sklearn.covariance import EllipticEnvelope from sklearn.model_selection import train_test_split from sklearn.metrics import precision_recall_fscore_support, roc_auc_score def compare_outlier_detection(): # Step 1: Data Generation and Split X, labels_true = make_blobs(n_samples=300, centers=1, cluster_std=0.60, random_state=0) X = np.concatenate([X, [[-8, -8], [8, 8]]], axis=0) labels_true = np.concatenate([labels_true, [-1, -1]]) X_train, X_test = train_test_split(X, test_size=0.3, random_state=42) # Step 2: Model Instantiation iso_forest = IsolationForest(contamination=0.1, random_state=42) one_class_svm = OneClassSVM(gamma=\'auto\') elli_env = EllipticEnvelope(contamination=0.1) # Step 3: Model Training iso_forest.fit(X_train) one_class_svm.fit(X_train) elli_env.fit(X_train) # Step 4: Model Prediction y_pred_iso = iso_forest.predict(X_test) y_pred_svm = one_class_svm.predict(X_test) y_pred_elli = elli_env.predict(X_test) # Convert predictions to binary y_pred_iso = np.where(y_pred_iso == 1, 0, 1) y_pred_svm = np.where(y_pred_svm == 1, 0, 1) y_pred_elli = np.where(y_pred_elli == 1, 0, 1) # True outliers in the test set, taking previously added outliers as ground truth y_true = np.array([1 if label == -1 else 0 for label in labels_true[len(X_train):]]) # Step 5: Evaluation models = [\'Isolation Forest\', \'One-Class SVM\', \'Elliptic Envelope\'] preds = [y_pred_iso, y_pred_svm, y_pred_elli] for model, pred in zip(models, preds): precision, recall, f1, _ = precision_recall_fscore_support(y_true, pred, average=\'binary\') roc_auc = roc_auc_score(y_true, pred) print(f\\"Model: {model}\\") print(f\\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}, ROC AUC: {roc_auc:.2f}\\") print() # Step 6: Visualization plt.figure(figsize=(18, 5)) for i, (model, y_pred) in enumerate(zip(models, preds)): plt.subplot(1, 3, i + 1) plt.title(model) plt.scatter(X_test[:, 0], X_test[:, 1], c=\'white\', s=20, edgecolor=\'k\') # highlight outliers mask = y_pred == 1 plt.scatter(X_test[mask][:, 0], X_test[mask][:, 1], c=\'red\', s=20, edgecolor=\'k\') plt.show() # Call the function compare_outlier_detection() ``` Notes: - Ensure that all necessary libraries are imported. - Properly handle data preprocessing steps. - Provide comments and explanations throughout the code to demonstrate your understanding. - The final output should include a comparative evaluation and visualizations as described.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_blobs from sklearn.ensemble import IsolationForest from sklearn.svm import OneClassSVM from sklearn.covariance import EllipticEnvelope from sklearn.model_selection import train_test_split from sklearn.metrics import precision_recall_fscore_support, roc_auc_score def compare_outlier_detection(): # Step 1: Data Generation and Split X, labels_true = make_blobs(n_samples=300, centers=1, cluster_std=0.60, random_state=0) # Adding some outliers X = np.concatenate([X, [[-8, -8], [8, 8]]], axis=0) labels_true = np.concatenate([labels_true, [-1, -1]]) X_train, X_test = train_test_split(X, test_size=0.3, random_state=42) # Step 2: Model Instantiation iso_forest = IsolationForest(contamination=0.1, random_state=42) one_class_svm = OneClassSVM(gamma=\'auto\') elli_env = EllipticEnvelope(contamination=0.1) # Step 3: Model Training iso_forest.fit(X_train) one_class_svm.fit(X_train) elli_env.fit(X_train) # Step 4: Model Prediction y_pred_iso = iso_forest.predict(X_test) y_pred_svm = one_class_svm.predict(X_test) y_pred_elli = elli_env.predict(X_test) # Convert predictions to binary (1 for outliers, 0 for inliers) y_pred_iso = np.where(y_pred_iso == 1, 0, 1) y_pred_svm = np.where(y_pred_svm == 1, 0, 1) y_pred_elli = np.where(y_pred_elli == 1, 0, 1) # True outliers in the test set, taking previously added outliers as ground truth y_true = np.array([1 if label == -1 else 0 for label in labels_true[len(X_train):]]) # Step 5: Evaluation models = [\'Isolation Forest\', \'One-Class SVM\', \'Elliptic Envelope\'] preds = [y_pred_iso, y_pred_svm, y_pred_elli] results = {} for model, pred in zip(models, preds): precision, recall, f1, _ = precision_recall_fscore_support(y_true, pred, average=\'binary\') roc_auc = roc_auc_score(y_true, pred) results[model] = { \\"precision\\": precision, \\"recall\\": recall, \\"f1\\": f1, \\"roc_auc\\": roc_auc } # Step 6: Visualization plt.figure(figsize=(18, 5)) for i, (model, y_pred) in enumerate(zip(models, preds)): plt.subplot(1, 3, i + 1) plt.title(model) plt.scatter(X_test[:, 0], X_test[:, 1], c=\'white\', s=20, edgecolor=\'k\') # highlight outliers mask = y_pred == 1 plt.scatter(X_test[mask][:, 0], X_test[mask][:, 1], c=\'red\', s=20, edgecolor=\'k\') plt.show() return results # Example call to the function compare_outlier_detection()"},{"question":"**Coding Assessment Question** # Objective: To demonstrate your understanding of scikit-learn by creating synthetic datasets, training a model, and simplifying code to identify and fix potential issues. # Problem Statement: You are provided with a synthetic dataset and are required to create a regression model using `GradientBoostingRegressor`. You will troubleshoot potential issues by simplifying your code. Please follow these steps: # Steps: 1. Generate a synthetic regression dataset using `make_regression`. 2. Split the dataset into training and testing sets. 3. Implement a `StandardScaler` to scale the features. 4. Train a `GradientBoostingRegressor` model with default parameters and evaluate its performance. 5. Modify the `n_iter_no_change` parameter and observe any errors or warnings. 6. Simplify the code to remove unnecessary parts and make it as minimal and readable as possible. # Requirements: 1. Use `make_regression` to create a dataset with 100 samples and 10 features. 2. Split the data using `train_test_split` with a test size of 30%. 3. Scale the features using `StandardScaler`. 4. Train the model using `GradientBoostingRegressor` with both default and `n_iter_no_change=5` configurations. 5. Simplify the code to remove any unnecessary parts while retaining its functionality. # Input and Output Format: - **Input:** None (Generate the dataset within your code). - **Output:** Print statements showing the model\'s performance and any warnings/errors. # Constraints: - Ensure your code is well-formatted and includes all necessary import statements. - Make the final version of the code as minimal as possible without losing its functionality. # Example: ```python import numpy as np from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_squared_error # Step 1: Generate synthetic dataset X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=42) # Step 2: Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 3: Scale features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Step 4: Train and evaluate model with default parameters model_default = GradientBoostingRegressor(random_state=42) model_default.fit(X_train_scaled, y_train) y_pred_default = model_default.predict(X_test_scaled) print(\\"Default Model Mean Squared Error:\\", mean_squared_error(y_test, y_pred_default)) # Step 5: Train and evaluate model with modified n_iter_no_change model_modified = GradientBoostingRegressor(n_iter_no_change=5, random_state=42) model_modified.fit(X_train_scaled, y_train) y_pred_modified = model_modified.predict(X_test_scaled) print(\\"Modified Model Mean Squared Error:\\", mean_squared_error(y_test, y_pred_modified)) # Simplified Code X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=42) model_simple = GradientBoostingRegressor(n_iter_no_change=5) model_simple.fit(X, y) # This should retain functionality and raise the warning ``` # Note: The focus should remain on the understanding and execution of the code, ensuring minimum necessary steps to retain functionality.","solution":"import numpy as np from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_squared_error def create_and_train_model(n_iter_no_change=None): # Generate synthetic dataset X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=42) # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Scale features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Create Model model = GradientBoostingRegressor(n_iter_no_change=n_iter_no_change, random_state=42) # Train model model.fit(X_train_scaled, y_train) # Predict and calculate mean squared error y_pred = model.predict(X_test_scaled) mse = mean_squared_error(y_test, y_pred) return mse # Set up for different model configurations default_mse = create_and_train_model() print(\\"Default Model Mean Squared Error:\\", default_mse) modified_mse = create_and_train_model(n_iter_no_change=5) print(\\"Modified Model Mean Squared Error:\\", modified_mse)"},{"question":"# Question: Implementing Distributed Training with the Join Context Manager You are tasked with implementing a distributed training scenario in PyTorch that handles uneven inputs using the generic join context manager. Your solution should demonstrate the use of the `Join`, `Joinable`, and `JoinHook` classes. Task 1. Implement a custom dataset class that simulates uneven input sizes for different workers. 2. Create a simple neural network model. 3. Implement a custom training loop using PyTorch\'s distributed training package, employing the join context manager to handle the uneven input scenario. Requirements 1. Define a `CustomDataset` class that produces uneven input sizes for different workers. 2. Define a `SimpleNet` class, which will be a basic neural network model. 3. Create a `trainer` function that will: - Distribute the training process using `torch.distributed`. - Handle the uneven inputs using the join context manager, ensuring synchronization across different workers. 4. Log the training loss for each worker to demonstrate that training is happening correctly. Constraints - Assume a fixed number of workers (e.g., 4 workers). - Ensure that the training process handles potential mismatches in the number of samples processed by each worker. Input There is no direct input to the function. Instead, your code should instantiate the classes and functions described. Output The training loop should print the training loss for each epoch for each worker. # Example Here is an outline to help you get started: ```python import torch import torch.distributed as dist from torch.utils.data import Dataset, DataLoader import torch.nn as nn import torch.optim as optim from torch.distributed.algorithms import Join, Joinable, JoinHook class CustomDataset(Dataset): def __init__(self, data, rank, world_size): # Simulate uneven input sizes based on rank total_samples = len(data) self.data = data[rank * total_samples // world_size: (rank + 1) * total_samples // world_size + rank] def __len__(self): return len(self.data) def __getitem__(self, index): return self.data[index] class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) def trainer(rank, world_size): dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) # Create dataset and dataloader data = torch.randn(100, 10) # Example data dataset = CustomDataset(data, rank, world_size) dataloader = DataLoader(dataset, batch_size=4, shuffle=True) model = SimpleNet() optimizer = optim.SGD(model.parameters(), lr=0.01) criterion = nn.MSELoss() model.train() for epoch in range(5): # Example: 5 epochs epoch_loss = 0.0 with Join([dataloader]): for batch in dataloader: optimizer.zero_grad() outputs = model(batch) loss = criterion(outputs, torch.ones_like(outputs)) loss.backward() optimizer.step() epoch_loss += loss.item() print(f\\"Rank {rank}, Epoch {epoch}, Loss: {epoch_loss}\\") if __name__ == \\"__main__\\": world_size = 4 dist.spawn(trainer, args=(world_size,), nprocs=world_size, join=True) ``` Note: Ensure that you have the necessary setup for distributed training in PyTorch. Test your code in an environment configured for distributed training.","solution":"import torch import torch.distributed as dist from torch.utils.data import Dataset, DataLoader import torch.nn as nn import torch.optim as optim from torch.distributed.algorithms.ddp_comm_hooks import default_hooks as Join class CustomDataset(Dataset): def __init__(self, data, rank, world_size): # Simulate uneven input sizes based on rank total_samples = len(data) self.data = data[rank * total_samples // world_size: (rank + 1) * total_samples // world_size + rank] def __len__(self): return len(self.data) def __getitem__(self, index): return self.data[index] class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) def trainer(rank, world_size): dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) # Create dataset and dataloader data = torch.randn(100, 10) # Example data dataset = CustomDataset(data, rank, world_size) dataloader = DataLoader(dataset, batch_size=4, shuffle=True) model = SimpleNet() optimizer = optim.SGD(model.parameters(), lr=0.01) criterion = nn.MSELoss() model.train() for epoch in range(5): # Example: 5 epochs epoch_loss = 0.0 with Join([dataloader]): for batch in dataloader: optimizer.zero_grad() outputs = model(batch) loss = criterion(outputs, torch.ones_like(outputs)) loss.backward() optimizer.step() epoch_loss += loss.item() print(f\\"Rank {rank}, Epoch {epoch}, Loss: {epoch_loss}\\") if __name__ == \\"__main__\\": world_size = 4 dist.spawn(trainer, args=(world_size,), nprocs=world_size, join=True)"},{"question":"**Coding Assessment Question: Email Header Retrieval and Processing** **Objective:** Implement a Python function that utilizes the `poplib` module to connect to a POP3 server, authenticate the user, retrieve email headers, and process these headers to extract specific information. **Function Signature:** ```python def retrieve_and_process_headers(host: str, port: int, username: str, password: str, num_headers: int) -> dict: Connects to a POP3 server, authenticates the user, retrieves a specified number of email headers, and processes these headers to extract sender email addresses and subjects. Parameters: - host (str): The hostname of the POP3 server. - port (int): The port number to connect to. - username (str): The username for authentication. - password (str): The password for authentication. - num_headers (int): The number of email headers to retrieve and process. Returns: - dict: A dictionary where the keys are email addresses of senders and values are lists of subjects from these senders. pass ``` **Requirements:** 1. Connect to the POP3 server using `poplib.POP3`. 2. Authenticate using the provided `username` and `password`. 3. Retrieve the headers of the first `num_headers` emails using the `TOP` command. 4. Extract the sender email addresses and the subjects from these headers. 5. Return a dictionary where each key is a sender\'s email address, and the corresponding value is a list of subjects from this sender. **Input Constraints:** - `host` and `username` are non-empty strings. - `port` is a positive integer. - `password` is a non-empty string. - `num_headers` is a non-negative integer, but realistically you can assume it does not exceed 100 for the sake of performance. **Output Format:** - Return a dictionary mapping sender email addresses to lists of subjects. **Example Usage:** ```python result = retrieve_and_process_headers(\'pop.example.com\', 110, \'user@example.com\', \'password\', 5) print(result) ``` Expected output format: ```python { \\"sender1@example.com\\": [\\"Subject 1\\", \\"Subject 2\\"], \\"sender2@example.com\\": [\\"Subject 3\\"], # ... } ``` **Hints:** - Use the `POP3.top` method to retrieve the headers of the emails. - Headers follow standard email format, where \\"From:\\" and \\"Subject:\\" lines hold the required information. - Ensure to handle possible exceptions such as connection errors and authentication failures using `try` and `except` blocks. **Important Note:** - Do not forget to call the `quit` method to properly close the connection once the retrieval and processing are complete.","solution":"import poplib from email.parser import BytesHeaderParser from collections import defaultdict def retrieve_and_process_headers(host: str, port: int, username: str, password: str, num_headers: int) -> dict: Connects to a POP3 server, authenticates the user, retrieves a specified number of email headers, and processes these headers to extract sender email addresses and subjects. Parameters: - host (str): The hostname of the POP3 server. - port (int): The port number to connect to. - username (str): The username for authentication. - password (str): The password for authentication. - num_headers (int): The number of email headers to retrieve and process. Returns: - dict: A dictionary where the keys are email addresses of senders and values are lists of subjects from these senders. try: # Connect to the POP3 server server = poplib.POP3(host, port) server.user(username) server.pass_(password) # Retrieve the list of email message numbers email_count, _ = server.stat() # Limit to available emails num_headers = min(num_headers, email_count) headers_info = defaultdict(list) parser = BytesHeaderParser() for i in range(1, num_headers + 1): response, lines, octets = server.top(i, 0) msg_content = b\'rn\'.join(lines) msg = parser.parsebytes(msg_content) from_header = msg[\'From\'] subject_header = msg[\'Subject\'] if from_header and subject_header: headers_info[from_header].append(subject_header) # Quit server connection server.quit() return headers_info except Exception as e: print(f\\"An error occurred: {e}\\") return {}"},{"question":"# Question: Implement a Custom Composite Logging Handler in Python Objective: You are required to design a custom logging handler in Python that leverages multiple built-in handlers to achieve a complex logging mechanism. This handler should send logs to both a file and a remote server, with the file handler having the ability to rotate logs based on their size. Requirements: 1. **CompositeHandler**: - Create a class `CompositeHandler` that extends `logging.Handler`. - This handler should use two sub-handlers internally: a `RotatingFileHandler` and a `SocketHandler`. 2. **Initialization**: - `CompositeHandler` should take the following parameters in its constructor: - `filename`: The name of the log file. - `maxBytes`: The maximum size in bytes before the file is rotated. - `backupCount`: The number of backup files to keep. - `host`: The host address of the remote server to send logs. - `port`: The port on the remote server to send logs. 3. **Log Emission**: - Override the `emit` method in `CompositeHandler` to: - Format the log record. - Send the formatted log record to both the `RotatingFileHandler` and the `SocketHandler`. 4. **Exception Handling**: - Ensure that if either sub-handler fails to emit the log, it does not prevent the other from functioning. Input/Output: - Input: Log messages generated by an application. - Output: Log records written to a rotating file and sent to a remote server. Constraints: - Ensure that the `RotatingFileHandler` performs log rotation correctly as per the provided `maxBytes` and `backupCount`. - Implement appropriate error handling to ensure robustness. Example Usage: ```python import logging import logging.handlers class CompositeHandler(logging.Handler): def __init__(self, filename, maxBytes, backupCount, host, port): super(CompositeHandler, self).__init__() self.file_handler = logging.handlers.RotatingFileHandler(filename, maxBytes=maxBytes, backupCount=backupCount) self.socket_handler = logging.handlers.SocketHandler(host, port) def emit(self, record): try: self.file_handler.emit(record) except Exception as e: self.handleError(record) try: self.socket_handler.emit(record) except Exception as e: self.handleError(record) # Configure the logger logger = logging.getLogger(\'CompositeLogger\') composite_handler = CompositeHandler(\'app.log\', 1024*1024, 5, \'localhost\', 9000) logger.addHandler(composite_handler) logger.setLevel(logging.DEBUG) # Generate some log entries logger.debug(\'This is a debug message\') logger.info(\'This is an info message\') logger.error(\'This is an error message\') ``` Write the implementation of the `CompositeHandler` class according to the specification.","solution":"import logging from logging.handlers import RotatingFileHandler, SocketHandler class CompositeHandler(logging.Handler): A logging handler that sends log records to a rotating file handler and a socket handler. def __init__(self, filename, maxBytes, backupCount, host, port): super(CompositeHandler, self).__init__() self.file_handler = RotatingFileHandler(filename, maxBytes=maxBytes, backupCount=backupCount) self.socket_handler = SocketHandler(host, port) def emit(self, record): try: self.file_handler.emit(record) except Exception as e: self.handleError(record) try: self.socket_handler.emit(record) except Exception as e: self.handleError(record)"},{"question":"**Question: Asynchronous URL Fetcher with Error Detection and Logging** **Objective**: Implement an asynchronous URL fetcher using Python\'s asyncio module that fetches a list of URLs concurrently, handles errors robustly, and logs the process efficiently. # Task You are required to: 1. **Fetch URLs Concurrently**: Write an async function `fetch_url` which fetches the content of a given URL using the `aiohttp` library. Use asyncio to fetch multiple URLs concurrently. 2. **Error Handling**: Ensure that any exceptions during the fetch are caught and logged. Unhandled exceptions should not be allowed. 3. **Debug Mode**: Enable asyncio\'s debug mode and set up logging such that the debug information is printed to the console. 4. **Blocking Code**: If a blocking operation (like a long computation or sleep) is needed, ensure it does not block the event loop by running it in an executor. 5. **Logging**: Set up logging to record the start and completion of each URL fetch, including if any errors occur. # Function Signature ```python import asyncio import aiohttp from typing import List, Tuple async def fetch_url(session: aiohttp.ClientSession, url: str) -> Tuple[str, str]: Fetch the content of the given URL. Args: - session: aiohttp ClientSession object for making HTTP requests. - url: the URL to fetch. Returns: - A tuple containing the URL and the response text. Raises: - Exception: In case of any issues with fetching the URL. pass async def fetch_urls(urls: List[str]) -> List[Tuple[str, str]]: Fetch multiple URLs concurrently. Args: - urls: A list of URLs to fetch. Returns: - A list of tuples, each containing a URL and its response text. pass def main(urls: List[str]): Main function to set up asyncio environment and run the fetch tasks. Args: - urls: A list of URLs to fetch. pass ``` # Example Given a list of URLs: ```python urls = [ \\"https://www.example.com\\", \\"https://www.python.org\\", \\"https://www.invalidurl.com\\", # This will cause an exception ] ``` # Execution 1. Implement `fetch_url` to asynchronously fetch the content of the given URL using `aiohttp`. Handle exceptions within this function. 2. Implement `fetch_urls` to manage concurrent fetching using asyncio, creating a session and gathering tasks. 3. Implement `main` to set debug mode, configure logging, and execute the `fetch_urls` function with the provided URLs. **Note:** - Ensure all never-awaited coroutines are handled properly. - Log noteworthy events including successful fetches and any raised exceptions. - Run the application in debug mode and set logging to DEBUG level to aid development. # Constraints - Use `aiohttp` for async HTTP requests. - Ensure proper error handling and logging as specified. - Demonstrate the use of asyncio debug mode and configure logging appropriately. - Avoid blocking the event loop with long operations. You can assume `aiohttp` is installed and available for use.","solution":"import asyncio import aiohttp import logging from typing import List, Tuple logging.basicConfig(level=logging.DEBUG) async def fetch_url(session: aiohttp.ClientSession, url: str) -> Tuple[str, str]: Fetch the content of the given URL. Args: - session: aiohttp ClientSession object for making HTTP requests. - url: the URL to fetch. Returns: - A tuple containing the URL and the response text. Raises: - Exception: In case of any issues with fetching the URL. try: async with session.get(url) as response: response_text = await response.text() logging.info(f\\"Fetched {url} successfully\\") return url, response_text except Exception as e: logging.error(f\\"Error fetching {url}: {str(e)}\\") return url, \\"\\" async def fetch_urls(urls: List[str]) -> List[Tuple[str, str]]: Fetch multiple URLs concurrently. Args: - urls: A list of URLs to fetch. Returns: - A list of tuples, each containing a URL and its response text. async with aiohttp.ClientSession() as session: tasks = [fetch_url(session, url) for url in urls] results = await asyncio.gather(*tasks) return results def main(urls: List[str]): Main function to set up asyncio environment and run the fetch tasks. Args: - urls: A list of URLs to fetch. asyncio.run(fetch_urls(urls)) if __name__ == \\"__main__\\": urls = [ \\"https://www.example.com\\", \\"https://www.python.org\\", \\"https://www.invalidurl.com\\", # This will cause an exception ] main(urls)"},{"question":"# Base64 Encoding and Decoding Functions As a software developer, you are required to handle data encoding and decoding for safe transmission over the network. You need to write a function in Python that takes a bytes-like object and an encoding scheme as inputs, encodes the data using the specified encoding scheme, and then decodes it back to ensure the integrity of the data. Implement the function `encode_decode_data(data: bytes, scheme: str) -> bytes` that takes the following inputs: - `data`: A bytes-like object that needs to be encoded and decoded. - `scheme`: A string specifying the encoding scheme. It can be one of the following values: `\'base64\'`, `\'urlsafe_base64\'`, `\'base32\'`, `\'base32hex\'`, `\'base16\'`, `\'a85\'`, `\'b85\'`. The function should return the decoded bytes-like object. If the specified scheme is invalid, raise a `ValueError` with the message \\"Invalid encoding scheme\\". # Input - `data`: A bytes-like object (e.g., `b\\"sample data\\"`). - `scheme`: A string (`\'base64\'`, `\'urlsafe_base64\'`, `\'base32\'`, `\'base32hex\'`, `\'base16\'`, `\'a85\'`, `\'b85\'`). # Output - Returns the decoded bytes-like object. # Constraints - The length of the `data` will not exceed 10,000 bytes. # Example ```python data = b\\"Hello, World!\\" scheme = \\"base64\\" result = encode_decode_data(data, scheme) print(result) # Output: b\\"Hello, World!\\" ``` # Requirements - Use the appropriate functions from the `base64` module for encoding and decoding based on the specified scheme. - Ensure that the function correctly handles and validates the encoding scheme. - Raise a `ValueError` with the message \\"Invalid encoding scheme\\" for any unsupported encoding schemes. Implement the `encode_decode_data` function.","solution":"import base64 def encode_decode_data(data: bytes, scheme: str) -> bytes: if scheme == \'base64\': encoded = base64.b64encode(data) decoded = base64.b64decode(encoded) elif scheme == \'urlsafe_base64\': encoded = base64.urlsafe_b64encode(data) decoded = base64.urlsafe_b64decode(encoded) elif scheme == \'base32\': encoded = base64.b32encode(data) decoded = base64.b32decode(encoded) elif scheme == \'base32hex\': encoded = base64.b32hexencode(data) decoded = base64.b32hexdecode(encoded) elif scheme == \'base16\': encoded = base64.b16encode(data) decoded = base64.b16decode(encoded) elif scheme == \'a85\': encoded = base64.a85encode(data) decoded = base64.a85decode(encoded) elif scheme == \'b85\': encoded = base64.b85encode(data) decoded = base64.b85decode(encoded) else: raise ValueError(\\"Invalid encoding scheme\\") return decoded"},{"question":"# Custom Iterator Implementation in Python In this assessment, you are tasked with implementing a custom iterator class in Python that follows the protocol and behaviors outlined by the provided documentation. Your implementation will require you to create a class that behaves like a Python `generator`. To test your understanding and skills, follow these instructions: 1. Implement a class `CustomIterator` that provides: - The `__iter__()` method, which returns the iterator object itself. - The `__next__()` method, which returns the next item in the iteration, or raises `StopIteration` when the items are exhausted. 2. Implement a function `use_custom_iterator(n: int) -> List` that initializes an instance of `CustomIterator` with a sequence from 0 up to `n-1`, and returns a list of its elements by iterating over the custom iterator. Input - An integer `n` which defines the sequence range `[0, n-1]`. Output - A list of integers iterated from the `CustomIterator`. Example ```python class CustomIterator: def __init__(self, n): self.n = n self.index = 0 def __iter__(self): return self def __next__(self): if self.index < self.n: result = self.index self.index += 1 return result else: raise StopIteration def use_custom_iterator(n): custom_iter = CustomIterator(n) result = [] for item in custom_iter: result.append(item) return result # Example Usage print(use_custom_iterator(5)) # Output: [0, 1, 2, 3, 4] ``` Constraints - `n` will be a non-negative integer. - Your iterator class should properly manage its state and adhere to the iterator protocol. - Aim for clear and efficient code. This task should help showcase your ability to understand and implement iterator protocols in Python.","solution":"class CustomIterator: def __init__(self, n): self.n = n self.index = 0 def __iter__(self): return self def __next__(self): if self.index < self.n: result = self.index self.index += 1 return result else: raise StopIteration def use_custom_iterator(n): custom_iter = CustomIterator(n) result = [] for item in custom_iter: result.append(item) return result"},{"question":"# Pandas DataFrame Manipulation and Analysis **Objective:** Given a dataset containing information about various products, you are to implement a function that processes and analyzes this data using pandas. Your function should demonstrate proficiency in data manipulation, handling missing values, aggregating data, and summarizing the results. **Input:** - A CSV file named `products.csv` with the following columns: - `ProductID` (string): Unique identifier of the product. - `ProductName` (string): Name of the product. - `Category`: Category of the product. - `Price` (float): Price of the product. - `QuantityInStock` (integer): Quantity of the product in stock. - `SupplierID` (string): Unique identifier of the supplier. - `SupplierName` (string): Name of the supplier. - `Rating` (float): Customer rating of the product (may contain missing values). **Output:** - A dictionary with the following keys and their corresponding values: - `total_products`: Total number of products. - `average_price_per_category`: A dictionary where keys are product categories and values are the average prices of products in those categories. - `lowest_rated_product`: A dictionary with `ProductID`, `ProductName`, and `Rating` of the product(s) with the lowest rating. - `most_stocked_supplier`: A dictionary with `SupplierID`, `SupplierName`, and `total_stock` representing the supplier with the highest total quantity of products in stock. - `missing_rating_count`: The total number of products missing a rating. **Function Signature:** ```python import pandas as pd def analyze_products(file_path: str) -> dict: # Your code goes here ``` **Constraints:** - The output dictionary must be constructed using the prescribed key names. - Handle missing values appropriately. - Ensure that your solution is efficient. **Example:** Suppose the `products.csv` file contains: ``` ProductID,ProductName,Category,Price,QuantityInStock,SupplierID,SupplierName,Rating P001,Widget A,Widgets,25.50,100,S001,Supplier X,4.5 P002,Widget B,Widgets,30.00,200,S002,Supplier Y, P003,Gadget A,Gadgets,15.75,150,S001,Supplier X,3.7 P004,Gadget B,Gadgets,22.50,150,S003,Supplier Z,4.2 P005,Tool A,Tools,10.00,300,S002,Supplier Y,4.9 P006,Tool B,Tools,12.50,250,S003,Supplier Z, P007,Tool C,Tools,11.25,100,S001,Supplier X,4.1 ``` The function call `analyze_products(\'products.csv\')` should return: ```python { \'total_products\': 7, \'average_price_per_category\': {\'Widgets\': 27.75, \'Gadgets\': 19.125, \'Tools\': 11.25}, \'lowest_rated_product\': {\'ProductID\': \'P003\', \'ProductName\': \'Gadget A\', \'Rating\': 3.7}, \'most_stocked_supplier\': {\'SupplierID\': \'S002\', \'SupplierName\': \'Supplier Y\', \'total_stock\': 500}, \'missing_rating_count\': 2 } ```","solution":"import pandas as pd def analyze_products(file_path: str) -> dict: # Read the CSV file into a pandas DataFrame df = pd.read_csv(file_path) # Calculate total number of products total_products = len(df) # Calculate average price per category average_price_per_category = df.groupby(\'Category\')[\'Price\'].mean().to_dict() # Find the product with the lowest rating valid_ratings = df.dropna(subset=[\'Rating\']) lowest_rated_product_series = valid_ratings.loc[valid_ratings[\'Rating\'].idxmin()] lowest_rated_product = { \'ProductID\': lowest_rated_product_series[\'ProductID\'], \'ProductName\': lowest_rated_product_series[\'ProductName\'], \'Rating\': lowest_rated_product_series[\'Rating\'], } # Calculate the most stocked supplier most_stocked_supplier_series = df.groupby([\'SupplierID\', \'SupplierName\'])[\'QuantityInStock\'].sum().idxmax() most_stocked_supplier_total_stock = df.groupby([\'SupplierID\', \'SupplierName\'])[\'QuantityInStock\'].sum().max() most_stocked_supplier = { \'SupplierID\': most_stocked_supplier_series[0], \'SupplierName\': most_stocked_supplier_series[1], \'total_stock\': most_stocked_supplier_total_stock, } # Count the number of products missing a rating missing_rating_count = df[\'Rating\'].isna().sum() # Compile results into a dictionary results = { \'total_products\': total_products, \'average_price_per_category\': average_price_per_category, \'lowest_rated_product\': lowest_rated_product, \'most_stocked_supplier\': most_stocked_supplier, \'missing_rating_count\': missing_rating_count, } return results"},{"question":"**Task: Implement and visualize data using Seaborn\'s `rugplot` and other related seaborn functionalities** You are required to demonstrate your understanding of seaborn\'s `rugplot` function by visualizing a given dataset. Follow these steps: 1. Load the dataset `tips` using seaborn\'s `load_dataset` function. 2. Create a plot with the following requirements: - Add a Kernel Density Estimate plot for the `total_bill` column. - Add a rug along the x-axis using the `rugplot` function. - Add a scatter plot for `total_bill` vs `tip`, where the color (hue) of the points represents the `time` variable (i.e., Lunch or Dinner). - For the scatter plot, add a rug along both axes. - Duplicate the scatter plot adding a taller rug but placing it outside the axes. **Input:** No input from the user is required; the dataset is loaded within the code. **Output:** The output should be: - A kernel density estimate plot with a rug for `total_bill`. - A scatter plot for `total_bill` vs `tip`, with a rug on both axes representing `time`. - The same scatter plot but with a taller rug placed outside the axes. **Constraints and Requirements:** 1. Use appropriate seaborn functions to achieve the tasks. 2. Ensure the plots are clear and well-labeled. 3. Handle the seaborn theme (aesthetic settings) appropriately for consistency. 4. Comment your code to explain each step of your implementation. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the figure and axes for subplots fig, axs = plt.subplots(2, 1, figsize=(10, 12)) # Plot 1: KDE plot for total_bill with a rug sns.kdeplot(data=tips, x=\\"total_bill\\", ax=axs[0]) sns.rugplot(data=tips, x=\\"total_bill\\", ax=axs[0]) axs[0].set_title(\'Kernel Density Estimate with Rug for Total Bill\') # Plot 2: Scatter plot of total_bill vs tip with hue for time and rug on both axes sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", ax=axs[1]) sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", ax=axs[1]) axs[1].set_title(\'Scatter Plot with Rug Along Both Axes\') plt.show() # Duplicate scatter plot with taller rug outside the axes plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", height=0.1, clip_on=False) plt.title(\'Scatter Plot with Taller Rug Outside the Axes\') plt.show() ``` This question requires you to understand and implement various functionalities of seaborn\'s `rugplot` and related plotting functions, demonstrating your practical knowledge of creating clear and insightful visualizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_seaborn_plots(): Create seaborn plots to demonstrate rugplot and other related functionalities. # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the figure and axes for subplots fig, axs = plt.subplots(2, 1, figsize=(10, 12)) # Plot 1: KDE plot for total_bill with a rug sns.kdeplot(data=tips, x=\\"total_bill\\", ax=axs[0]) sns.rugplot(data=tips, x=\\"total_bill\\", ax=axs[0]) axs[0].set_title(\'Kernel Density Estimate with Rug for Total Bill\') # Plot 2: Scatter plot of total_bill vs tip with hue for time and rug on both axes sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", ax=axs[1]) sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", ax=axs[1]) axs[1].set_title(\'Scatter Plot with Rug Along Both Axes\') plt.show() # Duplicate scatter plot with taller rug outside the axes plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", height=0.1, clip_on=False) plt.title(\'Scatter Plot with Taller Rug Outside the Axes\') plt.show()"},{"question":"**Objective:** Create a function that generates and visualizes different types of color palettes using the seaborn `blend_palette` function. **Function Signature:** ```python def visualize_color_palettes(colors: list, as_cmap: bool = False): pass ``` **Input:** - `colors` (list): A list of strings representing colors. The list can contain color names (e.g., \\"red\\"), hex color codes (e.g., \\"#FF0000\\"), or color codes from other color systems (e.g., \\"xkcd:sky blue\\"). - `as_cmap` (bool): A boolean indicating whether to return a continuous colormap. Default is `False`. **Output:** - The function doesn\'t return any value. Instead, it should generate and display a plot using seaborn and matplotlib to visualize the color palette. **Tasks:** 1. Create a color palette using `sns.blend_palette` with the provided colors. 2. If `as_cmap` is set to `True`, create a continuous colormap and visualize it using seaborn\'s heatmap. 3. If `as_cmap` is set to `False`, create a discrete color palette and visualize it using seaborn\'s barplot, where bars should be colored according to the palette. **Constraints:** - The list of colors can have a minimum length of 2 and a maximum length of 10. - The function should handle invalid color formats gracefully and raise informative errors. **Example:** ```python # Example 1: visualize a discrete color palette visualize_color_palettes([\\"#45a872\\", \\".8\\", \\"xkcd:golden\\"]) # Example 2: visualize a continuous colormap visualize_color_palettes([\\"#bdc\\", \\"#7b9\\", \\"#47a\\"], as_cmap=True) ``` **Performance Requirements:** - The function should be efficient and work well within the given constraints. - Visualization should be clear and correctly represent the color palette. **Notes:** - You may need to import additional modules as required (e.g., matplotlib). - Ensure that the plot titles and labels are informative and well-formatted. - You can use `sns.set_theme()` to apply the default seaborn theme to the plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def visualize_color_palettes(colors: list, as_cmap: bool = False): Generates and visualizes different types of color palettes using seaborn. Parameters: colors (list): A list of strings representing colors. as_cmap (bool): A boolean indicating whether to return a continuous colormap. Default is False. sns.set_theme() # Validate the input colors list if not isinstance(colors, list) or len(colors) < 2 or len(colors) > 10: raise ValueError(\\"colors must be a list with a length between 2 and 10.\\") try: palette = sns.blend_palette(colors, as_cmap=as_cmap) except ValueError as e: raise ValueError(\\"Invalid color format in the colors list.\\") from e if as_cmap: n = 100 # Size of the color map data = np.linspace(0, 1, n).reshape(1, -1) plt.figure(figsize=(8, 2)) sns.heatmap(data, cmap=palette, cbar=False, xticklabels=False, yticklabels=False) plt.title(\\"Continuous Colormap\\", fontsize=14) else: n = len(colors) plt.figure(figsize=(n, 2)) sns.barplot(x=np.arange(n), y=np.ones(n), palette=palette, dodge=False) plt.yticks([]) plt.title(\\"Discrete Color Palette\\", fontsize=14) plt.show()"},{"question":"**Assignment Title: File and Directory Management Using the `os` Module** # Objective: Demonstrate your understanding of the `os` package by implementing a function that performs various file and directory operations and manages environment variables. # Problem Statement: Write a Python script that performs the following operations: 1. Create a directory named \\"TestDir\\". 2. In \\"TestDir\\", create a subdirectory named \\"SubDir\\". 3. Inside \\"TestDir/SubDir\\", create a file named \\"test_file.txt\\" and write the text \\"Hello, World!\\" into this file. 4. Rename \\"test_file.txt\\" inside \\"TestDir/SubDir\\" to \\"hello.txt\\". 5. List all files and directories inside \\"TestDir\\" and print their names. 6. Set an environment variable named \\"NEW_VAR\\" to \\"PythonOS\\". 7. Retrieve the value of \\"NEW_VAR\\" and print it. # Function Signature: ```python def manage_files_and_env(): pass ``` # Constraints: - Do not use any external libraries. - All paths handled should be compatible with both Linux and Windows. # Example: After running your script, the following structure should be present: ``` TestDir/  SubDir/   hello.txt ``` Printing the directory content should output: ``` SubDir ``` Printing the environment variable should output: ``` PythonOS ``` # Validation: You may validate your function by checking the existence and contents of \\"TestDir/SubDir/hello.txt\\", and by checking the value of the environment variable \\"NEW_VAR\\". # Submission: Submit the complete code for the function `manage_files_and_env`.","solution":"import os def manage_files_and_env(): # Step 1: Create a directory named \\"TestDir\\" os.makedirs(\\"TestDir\\", exist_ok=True) # Step 2: In \\"TestDir\\", create a subdirectory named \\"SubDir\\" os.makedirs(\\"TestDir/SubDir\\", exist_ok=True) # Step 3: Inside \\"TestDir/SubDir\\", create a file named \\"test_file.txt\\" and write \\"Hello, World!\\" into it with open(\\"TestDir/SubDir/test_file.txt\\", \\"w\\") as file: file.write(\\"Hello, World!\\") # Step 4: Rename \\"test_file.txt\\" to \\"hello.txt\\" os.rename(\\"TestDir/SubDir/test_file.txt\\", \\"TestDir/SubDir/hello.txt\\") # Step 5: List all files and directories inside \\"TestDir\\" and print their names contents = os.listdir(\\"TestDir\\") for item in contents: print(item) # Step 6: Set an environment variable named \\"NEW_VAR\\" to \\"PythonOS\\" os.environ[\\"NEW_VAR\\"] = \\"PythonOS\\" # Step 7: Retrieve the value of \\"NEW_VAR\\" and print it new_var_value = os.getenv(\\"NEW_VAR\\") print(new_var_value)"},{"question":"Objective: Write a Python class that utilizes weak references to manage a collection of objects. This class should be able to: 1. Add objects to the collection. 2. Retrieve objects from the collection. 3. Automatically remove objects from the collection when they are garbage collected. Requirements: 1. Implement the `WeakObjectManager` class to include the following methods: - `add_object(self, obj)`: Adds an object to the collection. - `get_objects(self)`: Retrieves a list of currently alive objects in the collection. 2. Use weak references to ensure that objects are automatically removed from the collection when they are no longer in use. Constraints: - The objects added to the collection should be weakly referencable. - The solution should handle cases where callbacks are needed to clean up references when the object is garbage collected. Input and Output Format: - `add_object(self, obj)`: `obj` is any weakly referencable Python object. - `get_objects(self)`: Returns a list of currently alive objects. Example: ```python import weakref class WeakObjectManager: def __init__(self): # Initialize a collection to store weak references self._refs = [] def add_object(self, obj): # Create a weak reference to the object with a callback to remove it when it is garbage collected def remove_callback(weak_ref): self._refs.remove(weak_ref) weak_ref = weakref.ref(obj, remove_callback) self._refs.append(weak_ref) def get_objects(self): # Return a list of currently alive objects return [ref() for ref in self._refs if ref() is not None] # Example Usage: if __name__ == \\"__main__\\": manager = WeakObjectManager() class Dummy: pass obj1 = Dummy() obj2 = Dummy() manager.add_object(obj1) manager.add_object(obj2) print(manager.get_objects()) # Should print: [<__main__.Dummy object at ...>, <__main__.Dummy object at ...>] del obj1 print(manager.get_objects()) # Should print: [<__main__.Dummy object at ...>] ``` This question requires students to demonstrate their understanding of weak references and their usage in managing object lifecycles effectively.","solution":"import weakref class WeakObjectManager: def __init__(self): # Initialize a collection to store weak references self._refs = [] def add_object(self, obj): # Create a weak reference to the object with a callback to remove it when it is garbage collected def remove_callback(weak_ref): self._refs.remove(weak_ref) weak_ref = weakref.ref(obj, remove_callback) self._refs.append(weak_ref) def get_objects(self): # Return a list of currently alive objects return [ref() for ref in self._refs if ref() is not None]"},{"question":"# Question: Kernel Approximation using Nystroem and RBFSampler You are required to implement and compare two kernel approximation methods from the `sklearn.kernel_approximation` module: the Nystroem Method and the RBFSampler. # Task: 1. **Data Preparation:** Use the provided dataset. If no dataset is provided, generate a synthetic 2D dataset using `numpy` with 1000 samples and 2 features, ensuring a variety of values. 2. **Nystroem Method:** - Import `Nystroem` from `sklearn.kernel_approximation`. - Apply the Nystroem method to the dataset with `n_components=100` using the default `rbf` kernel. - Fit and transform the dataset using this method. 3. **RBFSampler:** - Import `RBFSampler` from `sklearn.kernel_approximation`. - Apply the RBFSampler to the dataset with `gamma=1.0` and `n_components=100`. - Fit and transform the dataset using this method. 4. **Linear Classification:** - Use `SGDClassifier` from `sklearn.linear_model` to fit a linear classifier on the transformed datasets obtained from the Nystroem and RBFSampler methods. - Fit the classifier and evaluate its accuracy using `cross_val_score` with 5-fold cross-validation. 5. **Comparison:** - Report and compare the cross-validation accuracy scores for both kernel approximation methods. - Discuss the performance in terms of accuracy and transformation time. # Expected Input and Output - **Input:** - Dataset `X` with shape (1000, 2). - Labels `y` with shape (1000,). - **Output:** - Cross-validation accuracy scores for both Nystroem and RBFSampler methods. - A brief discussion comparing the performance of both methods. # Constraints: - Use `n_components=100` for kernel approximation methods. - Use `gamma=1.0` for RBFSampler. - Ensure reproducibility by setting random state where applicable. # Example: ```python import numpy as np from sklearn.kernel_approximation import Nystroem, RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.model_selection import cross_val_score import time # Generate synthetic data np.random.seed(42) X = np.random.randn(1000, 2) y = np.random.randint(0, 2, 1000) # Nystroem Transformation nystroem = Nystroem(n_components=100, random_state=42) X_nystroem = nystroem.fit_transform(X) # RBF Sampler Transformation rbf_sampler = RBFSampler(gamma=1.0, n_components=100, random_state=42) X_rbf = rbf_sampler.fit_transform(X) # Linear Classifier with Nystroem clf = SGDClassifier(max_iter=5, random_state=42) start_time = time.time() nystroem_scores = cross_val_score(clf, X_nystroem, y, cv=5) nystroem_time = time.time() - start_time # Linear Classifier with RBF Sampler start_time = time.time() rbf_scores = cross_val_score(clf, X_rbf, y, cv=5) rbf_time = time.time() - start_time # Output print(\\"Nystroem Method Accuracy: \\", nystroem_scores.mean()) print(\\"Nystroem Transformation Time: \\", nystroem_time) print(\\"RBF Sampler Accuracy: \\", rbf_scores.mean()) print(\\"RBF Transformation Time: \\", rbf_time) ``` # Discussion: Compare the accuracies and time taken for both transformations. Which method performed better and why? Discuss any trade-offs between accuracy and computational efficiency.","solution":"import numpy as np from sklearn.kernel_approximation import Nystroem, RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.model_selection import cross_val_score import time def kernel_approximation_and_classification(): # Generate synthetic data np.random.seed(42) X = np.random.randn(1000, 2) y = np.random.randint(0, 2, 1000) # Nystroem Transformation nystroem = Nystroem(n_components=100, random_state=42) X_nystroem = nystroem.fit_transform(X) # RBF Sampler Transformation rbf_sampler = RBFSampler(gamma=1.0, n_components=100, random_state=42) X_rbf = rbf_sampler.fit_transform(X) # Linear Classifier with Nystroem clf = SGDClassifier(max_iter=5, random_state=42) start_time = time.time() nystroem_scores = cross_val_score(clf, X_nystroem, y, cv=5) nystroem_time = time.time() - start_time # Linear Classifier with RBF Sampler start_time = time.time() rbf_scores = cross_val_score(clf, X_rbf, y, cv=5) rbf_time = time.time() - start_time # Collect results results = { \\"nystroem_accuracy\\": nystroem_scores.mean(), \\"nystroem_time\\": nystroem_time, \\"rbf_accuracy\\": rbf_scores.mean(), \\"rbf_time\\": rbf_time } return results"},{"question":"# Question: Advanced Data Type Manipulation and Use of Enums in Python You are required to design a module that manages a task scheduling system. The tasks to be scheduled can be of different types, and each type has a priority level associated with it. The priorities are represented as enums, and the tasks should be stored in an efficient data type from the collections module. # Requirements: 1. **TaskType Enum**: - Define an enumeration `TaskType` to represent different types of tasks. The task types are: `WORK`, `PERSONAL`, `URGENT`. - Each task type should have an associated priority level (integer value), with `URGENT` having the highest priority, followed by `WORK`, and then `PERSONAL`. 2. **Task Class**: - Define a `Task` class with the following attributes: - `name` (str): The name of the task. - `task_type` (TaskType): The type of the task. - `due_date` (datetime.date): The due date of the task. - Implement a method `__lt__` in the `Task` class to compare two tasks based on their priority and due date. Tasks with higher priority should come first. If two tasks have the same priority, the one with the earlier due date should come first. 3. **TaskManager Class**: - Define a `TaskManager` class that manages a collection of tasks. The tasks should be stored in a `collections.deque`. - The `TaskManager` class should have the following methods: - `add_task(task: Task)`: Add a new task to the collection. - `get_next_task() -> Task`: Get and remove the task with the highest priority and the earliest due date. - `list_tasks() -> List[Tuple]`: List all tasks in ascending order of priority and due date. Each task should be represented as a tuple `(name, task_type, due_date)`. # Example Usage: ```python from datetime import date # Define TaskType Enum, Task, and TaskManager here # Detailed implementation as per the requirements # Example code: tm = TaskManager() # Add tasks tm.add_task(Task(name=\\"File Taxes\\", task_type=TaskType.WORK, due_date=date(2023, 4, 15))) tm.add_task(Task(name=\\"Buy Groceries\\", task_type=TaskType.PERSONAL, due_date=date(2023, 4, 10))) tm.add_task(Task(name=\\"Meeting with CEO\\", task_type=TaskType.URGENT, due_date=date(2023, 3, 25))) # Retrieve tasks in the correct order first_task = tm.get_next_task() assert first_task.name == \\"Meeting with CEO\\" # List remaining tasks tasks_list = tm.list_tasks() assert tasks_list == [(\\"Buy Groceries\\", TaskType.PERSONAL, date(2023, 4, 10)), (\\"File Taxes\\", TaskType.WORK, date(2023, 4, 15))] ``` # Constraints: - Implement the solution using the `collections`, `datetime`, and `enum` modules. - Ensure the `TaskManager` is efficient in terms of task retrieval and maintains the right order of tasks. # Performance Requirements: - Adding tasks (`add_task` method) should be O(1). - Getting the next task (`get_next_task` method) should be efficient considering the requirement of tasks ordering.","solution":"from enum import Enum from collections import deque from datetime import date class TaskType(Enum): WORK = 1 PERSONAL = 2 URGENT = 3 class Task: def __init__(self, name, task_type, due_date): self.name = name self.task_type = task_type self.due_date = due_date def __lt__(self, other): if self.task_type != other.task_type: return self.task_type.value > other.task_type.value return self.due_date < other.due_date def __repr__(self): return f\\"Task(name={self.name}, task_type={self.task_type}, due_date={self.due_date})\\" class TaskManager: def __init__(self): self.tasks = deque() def add_task(self, task): self.tasks.append(task) self.tasks = deque(sorted(self.tasks)) def get_next_task(self): return self.tasks.popleft() def list_tasks(self): return [(task.name, task.task_type, task.due_date) for task in self.tasks]"},{"question":"# Python Coding Assessment **Objective:** You are required to demonstrate your understanding of the `runpy` module by writing code that locates and executes Python modules and scripts dynamically. **Problem Statement:** You are given a Python script file `hello_world.py` and a module `example_module`. Your task is to: 1. Write a Python function `execute_script(file_path: str) -> dict` that uses `runpy.run_path` to execute `hello_world.py` and returns the resulting globals dictionary. Handle any possible runtime errors that may occur during the script execution. 2. Write a Python function `execute_module(module_name: str) -> dict` that uses `runpy.run_module` to execute `example_module` and returns the resulting globals dictionary. Similar to the previous function, handle any possible runtime errors. **Details and Requirements:** - The `hello_world.py` script contains Python code that prints `\\"Hello, World!\\"` and defines a global variable `greeting` with the value `\\"Hello from script!\\"`. - The `example_module` is a Python module that contains a `__main__.py` which prints `\\"Executing example_module!\\"` and defines a global variable `module_var` with the value `\\"Module Variable\\"`. **Function Signatures:** ```python def execute_script(file_path: str) -> dict: pass def execute_module(module_name: str) -> dict: pass ``` **Constraints:** - `file_path` for `execute_script` will be a valid path to a Python script file. - `module_name` for `execute_module` will be a valid module name in the Python module search path. **Example Usage:** ```python # Assuming hello_world.py contains: # print(\\"Hello, World!\\") # greeting = \\"Hello from script!\\" globals_script = execute_script(\'/path/to/hello_world.py\') # Output should be a dictionary containing at least the key \'greeting\' with value \\"Hello from script!\\" # Assuming example_module contains a __main__.py with: # print(\\"Executing example_module!\\") # module_var = \\"Module Variable\\" globals_module = execute_module(\'example_module\') # Output should be a dictionary containing at least the key \'module_var\' with value \\"Module Variable\\" ``` # Hints - Utilize `runpy.run_path` for executing scripts by their file paths. - Utilize `runpy.run_module` for executing modules by their names. - Handle exceptions gracefully and ensure your functions return meaningful error messages or empty dictionaries on failure. Good luck, and may your code run smoothly!","solution":"import runpy def execute_script(file_path: str) -> dict: try: # Executing the script and capturing the globals dictionary result_globals = runpy.run_path(file_path) return result_globals except Exception as e: # Handling any runtime errors print(f\\"Error executing script {file_path}: {e}\\") return {} def execute_module(module_name: str) -> dict: try: # Executing the module and capturing the globals dictionary result_globals = runpy.run_module(module_name) return result_globals except Exception as e: # Handling any runtime errors print(f\\"Error executing module {module_name}: {e}\\") return {}"},{"question":"Create a Python function `filter_and_copy_mails(src_mailbox_path: str, dest_mailbox_path: str, keyword: str) -> None` that filters emails containing a specific keyword in their subject line from a source mailbox and copies them to a destination mailbox. The function should handle the `mbox` format for both the source and destination mailboxes. # Function Signature ```python def filter_and_copy_mails(src_mailbox_path: str, dest_mailbox_path: str, keyword: str) -> None: pass ``` # Input: 1. `src_mailbox_path`: A string representing the file path to the source mbox mailbox. 2. `dest_mailbox_path`: A string representing the file path to the destination mbox mailbox. 3. `keyword`: A string representing the keyword to search for in the subject lines of the emails. # Output: None. The function should copy filtered emails to the destination mailbox and print a summary of the operation including: - The total number of messages processed. - The number of messages copied. - Any errors encountered during the operation. # Constraints: - The function should properly handle potential exceptions such as mailbox not found, read/write errors, and message parsing errors. - Ensure data consistency and avoid corruption by locking the mailboxes during write operations. - Assume the mailboxes may be accessed by other processes concurrently, so proper locking is required. - Use the `mbox` class from the `mailbox` module for handling the mailboxes. # Example: Suppose you have an mbox file `source.mbox` containing 50 emails and you want to copy emails containing the keyword \\"urgent\\" in their subject line to another mbox file `destination.mbox`. ```python filter_and_copy_mails(\'source.mbox\', \'destination.mbox\', \'urgent\') ``` # Guidelines: - Use the `mailbox` module to handle reading from and writing to the mbox mailbox. - Implement proper exception handling to manage potential errors. - Document the function with appropriate comments explaining each part of the code.","solution":"import mailbox import os def filter_and_copy_mails(src_mailbox_path: str, dest_mailbox_path: str, keyword: str) -> None: Filters emails containing a specific keyword in their subject line from a source mailbox and copies them to a destination mailbox. Parameters: src_mailbox_path (str): The file path to the source mbox mailbox. dest_mailbox_path (str): The file path to the destination mbox mailbox. keyword (str): The keyword to search for in the subject lines of the emails. total_messages = 0 copied_messages = 0 try: src_mbox = mailbox.mbox(src_mailbox_path) dest_mbox = mailbox.mbox(dest_mailbox_path) src_mbox.lock() try: for message in src_mbox: total_messages += 1 if keyword in message[\'subject\']: dest_mbox.lock() try: dest_mbox.add(message) copied_messages += 1 finally: dest_mbox.flush() dest_mbox.unlock() finally: src_mbox.unlock() print(f\'Total messages processed: {total_messages}\') print(f\'Number of messages copied: {copied_messages}\') except Exception as e: print(f\'An error occurred: {e}\') finally: src_mbox.close() dest_mbox.close()"},{"question":"You are required to use the Seaborn library to create data visualizations using the provided data set. The data set can be loaded using Seaborn\'s `load_dataset` function. You will demonstrate your ability to create complex visualizations by writing functions that generate specific plots. Dataset Use the \\"tips\\" dataset provided by Seaborn: ```python import seaborn as sns tips = sns.load_dataset(\\"tips\\") ``` Function 1: Bar Plot Grouped by Day and Sex Write a function `bar_plot_day_sex` that: - Takes no input and returns nothing. - Creates a grouped bar plot showing the count of tips received for each day of the week, further grouped by sex. - Uses Seaborn\'s object-oriented interface (`seaborn.objects`). *Expected Output*: A bar plot visualizing the counts of tips for each day, with separate bars for male and female customers. Function 2: Bar Plot for Sizes Write a function `bar_plot_size` that: - Takes no input and returns nothing. - Creates a bar plot that shows the count of different sizes (\\"size\\" column) of group dinners. - Uses Seaborn\'s object-oriented interface (`seaborn.objects`). *Expected Output*: A bar plot showing counts of different sizes of group dinners. Function 3: Custom Aggregation Bar Plot Write a function `custom_agg_bar_plot` that: - Takes no input and returns nothing. - Creates a custom aggregation bar plot where the y-axis represents the total bill for each day. - Groups the data by both \\"day\\" and \\"time\\" (lunch/dinner). - Uses Seaborn\'s object-oriented interface (`seaborn.objects`) and demonstrates custom aggregation functions. *Expected Output*: A bar plot visualizing the total bill for each day, grouped by the time of the day (lunch/dinner). Implementation Tips - Make sure to import necessary modules. - Each plot should be displayed using `plt.show()` to visualize it during function execution. - Utilize the correct functions and classes from `seaborn.objects`. Example Here is a template to help you get started: ```python import seaborn.objects as so import seaborn as sns import matplotlib.pyplot as plt def bar_plot_day_sex(): tips = sns.load_dataset(\\"tips\\") plot = so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()) plot.show() def bar_plot_size(): tips = sns.load_dataset(\\"tips\\") plot = so.Plot(tips, x=\\"size\\").add(so.Bar(), so.Count()) plot.show() def custom_agg_bar_plot(): tips = sns.load_dataset(\\"tips\\") tips[\\"total_bill_sum\\"] = tips.groupby([\\"day\\", \\"time\\"])[\\"total_bill\\"].transform(\'sum\') plot = so.Plot(tips, x=\\"day\\", y=\\"total_bill_sum\\", color=\\"time\\").add(so.Bar(), so.Agg()) plot.show() ```","solution":"import seaborn.objects as so import seaborn as sns import matplotlib.pyplot as plt def bar_plot_day_sex(): Creates a grouped bar plot showing the count of tips received for each day of the week, further grouped by sex. tips = sns.load_dataset(\\"tips\\") plt.figure() plot = so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()) plot.show() plt.close() def bar_plot_size(): Creates a bar plot that shows the count of different sizes (\'size\' column) of group dinners. tips = sns.load_dataset(\\"tips\\") plt.figure() plot = so.Plot(tips, x=\\"size\\").add(so.Bar(), so.Count()) plot.show() plt.close() def custom_agg_bar_plot(): Creates a custom aggregation bar plot where the y-axis represents the total bill for each day, grouped by lunch/dinner time. tips = sns.load_dataset(\\"tips\\") agg_tips = tips.groupby([\\"day\\", \\"time\\"], as_index=False)[\\"total_bill\\"].sum() plt.figure() plot = so.Plot(agg_tips, x=\\"day\\", y=\\"total_bill\\", color=\\"time\\").add(so.Bar(), so.Agg()) plot.show() plt.close()"},{"question":"# **Coding Assessment Question** You are provided with a dataset containing the monthly average temperatures of various cities over several years. The dataset is in wide-form format, where each column represents data for a different city and each row corresponds to a monthly observation. Your task is to: 1. Convert the dataset from wide-form to long-form. 2. Create a data visualization using seaborn to show the temperature trends over time for all cities. # **Function to Implement** ```python def visualize_temperature_trends(data: pd.DataFrame) -> None: This function takes a wide-form DataFrame containing monthly average temperatures for different cities, converts it to long-form, and then plots temperature trends using seaborn. Parameters: data (pd.DataFrame): A wide-form DataFrame with columns as city names and rows as monthly temperature observations. Example: data = pd.DataFrame({ \'month_year\': [\'2010-01\', \'2010-02\', \'2010-03\', \'2010-04\'], \'New_York\': [30, 32, 50, 60], \'Los_Angeles\': [60, 62, 65, 70], \'Chicago\': [25, 28, 45, 55] }) visualize_temperature_trends(data) pass ``` # **Input Format** - A wide-form pandas DataFrame where each column (except the first \'month_year\' column) represents monthly average temperatures for a particular city. The first column, \'month_year\', indicates the month and year of the observation in \'YYYY-MM\' format. # **Output Format** - A seaborn visualization showing temperature trends over time for all the cities in the dataset. # **Constraints** - Use seaborn for plotting. - Ensure the data is correctly converted to long-form before plotting. - Use appropriate labels and titles to make the plot informative. # **Example** ```python import pandas as pd data = pd.DataFrame({ \'month_year\': [\'2010-01\', \'2010-02\', \'2010-03\', \'2010-04\'], \'New_York\': [30, 32, 50, 60], \'Los_Angeles\': [60, 62, 65, 70], \'Chicago\': [25, 28, 45, 55] }) visualize_temperature_trends(data) ``` The resulting plot should clearly show the temperature trends for New York, Los Angeles, and Chicago over time, with properly labeled axes and a legend indicating the cities.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_temperature_trends(data: pd.DataFrame) -> None: This function takes a wide-form DataFrame containing monthly average temperatures for different cities, converts it to long-form, and then plots temperature trends using seaborn. Parameters: data (pd.DataFrame): A wide-form DataFrame with columns as city names and rows as monthly temperature observations. # Convert from wide-form to long-form long_form = pd.melt(data, id_vars=[\'month_year\'], var_name=\'City\', value_name=\'Temperature\') # Convert month_year to datetime long_form[\'month_year\'] = pd.to_datetime(long_form[\'month_year\']) # Plotting the data plt.figure(figsize=(14, 7)) sns.lineplot(x=\'month_year\', y=\'Temperature\', hue=\'City\', data=long_form) plt.title(\'Temperature Trends Over Time\') plt.xlabel(\'Time (Month-Year)\') plt.ylabel(\'Temperature (F)\') plt.legend(title=\'City\') plt.grid(True) plt.show()"},{"question":"Introduction You\'ll be working with email messages and will need to write code using Python\'s `email.iterators` module. You\'ll implement a function that extracts all text parts of an email message, concatenates them, and returns the concatenated string. Task Implement a function `extract_text_parts(msg)` that takes an email `Message` object `msg` as input and returns a concatenated string of all parts with the MIME type `text/plain`. Input - `msg`: An email `Message` object. Output - A single string containing the concatenated content of all `text/plain` parts found within the message. Constraints - The email `Message` object can be nested with multiple layers. - You should only extract and concatenate parts with the `text/plain` MIME type. - If no `text/plain` parts are found, the function should return an empty string. Example ```python from email import message_from_string # Example email string email_string = Content-Type: multipart/mixed; boundary=\\"BOUNDARY\\" --BOUNDARY Content-Type: text/plain This is the first plain text part. --BOUNDARY Content-Type: text/html <html><body>This is an HTML part.</body></html> --BOUNDARY Content-Type: multipart/alternative; boundary=\\"SUB-BOUNDARY\\" --SUB-BOUNDARY Content-Type: text/plain This is the second plain text part in a nested multipart. --SUB-BOUNDARY-- --BOUNDARY-- msg = message_from_string(email_string) assert extract_text_parts(msg) == \\"This is the first plain text part.nThis is the second plain text part in a nested multipart.n\\" ``` Notes - Use `email.iterators.typed_subpart_iterator()` to iterate over the relevant parts of the email. - Consider all parts and subparts when looking for `text/plain` types. - Preserve the order of appearance of plain text parts in the email. # Implementation ```python from email.iterators import typed_subpart_iterator def extract_text_parts(msg): text_parts = [] for part in typed_subpart_iterator(msg, maintype=\'text\', subtype=\'plain\'): if part.get_content_type() == \'text/plain\': text_parts.append(part.get_payload(decode=True)) return \\"n\\".join(part.decode() if isinstance(part, bytes) else part for part in text_parts) ```","solution":"from email.iterators import typed_subpart_iterator def extract_text_parts(msg): Extracts all text/plain parts from an email and concatenates them. Args: msg (Message): An email Message object. Returns: str: Concatenated string of all text/plain parts. text_parts = [] for part in typed_subpart_iterator(msg, maintype=\'text\', subtype=\'plain\'): if part.get_content_type() == \'text/plain\': text_parts.append(part.get_payload(decode=True)) return \\"n\\".join(part.decode() if isinstance(part, bytes) else part for part in text_parts)"},{"question":"# Compression and Decompression using zlib You are tasked with designing a system that efficiently stores and retrieves large text documents. The system will compress the documents before storage and decompress them when retrieval is needed. To ensure the integrity of the documents, you will also compute and verify checksums. **Part 1: Implement Compression and Decompression** 1. **Function `compress_document`**: - **Input**: - `data` (str): The text document to be compressed. - `compression_level` (int, optional): The level of compression (0-9). Defaults to -1. - **Output**: - Compressed data as a bytes object. - **Description**: - Use the `zlib.compress` function to compress the input text. 2. **Function `decompress_document`**: - **Input**: - `compressed_data` (bytes): The compressed text document. - **Output**: - Original uncompressed text document (str). - **Description**: - Use the `zlib.decompress` function to decompress the input data. **Part 2: Ensure Data Integrity** 3. **Function `compute_checksum`**: - **Input**: - `data` (bytes): The data for which the checksum is to be calculated. - **Output**: - Checksum as an unsigned 32-bit integer. - **Description**: - Use the `zlib.adler32` function to compute the checksum. 4. **Function `verify_checksum`**: - **Input**: - `data` (bytes): The data for which the checksum is to be verified. - `expected_checksum` (int): The expected checksum to be compared. - **Output**: - `True` if checksums match, `False` otherwise. - **Description**: - Compute the checksum of the data and compare it against the provided checksum. **Example Usage**: ```python original_text = \\"This is a sample text document that needs to be compressed and decompressed.\\" compression_level = 6 compressed_data = compress_document(original_text, compression_level) decompressed_text = decompress_document(compressed_data) assert decompressed_text == original_text, \\"Decompression failed!\\" checksum = compute_checksum(compressed_data) is_valid = verify_checksum(compressed_data, checksum) assert is_valid, \\"Checksum verification failed!\\" ``` Your task is to implement these four functions. Make sure to handle exceptions appropriately and ensure that all operations maintain data integrity. # Constraints: - Documents can be large, so efficiency in both time and space is important. - Handle cases where the decompressed data is incomplete or corrupted gracefully. # Performance Requirements: - Aim for a solution that can handle input sizes up to several megabytes efficiently. - Ensure that checksum computation and verification are performed in constant time relative to the data size.","solution":"import zlib def compress_document(data, compression_level=-1): Compresses the input text document. Parameters: - data (str): The text document to be compressed. - compression_level (int, optional): The level of compression (0-9). Defaults to -1. Returns: - bytes: Compressed data. if not isinstance(compression_level, int) or not (0 <= compression_level <= 9 or compression_level == -1): raise ValueError(\\"compression_level must be an integer between 0 and 9 or -1.\\") return zlib.compress(data.encode(\'utf-8\'), compression_level) def decompress_document(compressed_data): Decompresses the input compressed text document. Parameters: - compressed_data (bytes): The compressed text document. Returns: - str: Original uncompressed text document. try: return zlib.decompress(compressed_data).decode(\'utf-8\') except zlib.error as e: raise ValueError(\\"Decompression failed. Data may be corrupted.\\") from e def compute_checksum(data): Computes the checksum of the input data. Parameters: - data (bytes): The data for which the checksum is to be calculated. Returns: - int: Checksum as an unsigned 32-bit integer. return zlib.adler32(data) & 0xffffffff def verify_checksum(data, expected_checksum): Verifies the checksum of the input data. Parameters: - data (bytes): The data for which the checksum is to be verified. - expected_checksum (int): The expected checksum to be compared. Returns: - bool: True if checksums match, False otherwise. return compute_checksum(data) == expected_checksum"},{"question":"# Task Overview You are required to implement a function that recursively copies a directory tree from a source path to a destination path. The function should replicate some behavior of the `shutil.copytree` method by allowing for the use of custom copy functions for files, and should handle symbolic links appropriately. # Requirements 1. **Function Signature**: ```python def custom_copytree(src: str, dst: str, symlinks: bool = False, ignore: callable = None, copy_function: callable = None, dirs_exist_ok: bool = False) -> str: ``` 2. **Parameters**: - `src` (str): The source directory path. - `dst` (str): The destination directory path. - `symlinks` (bool): If `True`, symbolic links in the source tree are represented as symbolic links in the new tree. Defaults to `False`. - `ignore` (callable): A callable that takes two arguments, the directory being visited and the contents of the directory, and returns a sequence of names relative to the directory that should be ignored. Defaults to `None`. - `copy_function` (callable): A callable that takes two arguments, the source path and the destination path, and is used to copy each file. Defaults to `None`, in which case the function should use a basic byte-wise copy function. - `dirs_exist_ok` (bool): If `True`, the function will not raise an exception if the destination directory already exists. 3. **Output**: - The function should return the path to the destination directory. 4. **Constraints**: - Your function should raise a `FileExistsError` if `dirs_exist_ok` is `False` and the destination directory already exists. - Your function should handle any exceptions that occur during copying and display an appropriate message without stopping execution. # Example ```python # Example usage: def custom_copy(src, dst): # Example copy function that just copies the file bytewise with open(src, \'rb\') as fsrc, open(dst, \'wb\') as fdst: while True: buf = fsrc.read(1024) if not buf: break fdst.write(buf) custom_copytree(\\"/path/to/source\\", \\"/path/to/destination\\", copy_function=custom_copy) ``` # Notes - You should use appropriate standard library modules for file operations (e.g., `os`, `io`, and the provided `shutil`). - Make sure to handle nested directory structures and various file types including regular files and symbolic links. - Ensure to preserve file permissions when copying. # Hints - Refer to the `shutil` module documentation for the behavior of `copytree` and related functions. - Use `os` module functions like `os.makedirs` and `os.path` to handle directory operations and path manipulations.","solution":"import os import shutil def custom_copytree(src, dst, symlinks=False, ignore=None, copy_function=None, dirs_exist_ok=False): Recursively copies a directory tree from src to dst. if not os.path.exists(src): raise FileNotFoundError(f\\"Source directory \'{src}\' does not exist.\\") if not dirs_exist_ok and os.path.exists(dst): raise FileExistsError(f\\"Destination directory \'{dst}\' already exists.\\") if not copy_function: copy_function = shutil.copy2 # default copy function with metadata # If ignore is provided, generate a set of directories/files to ignore ignored_names = set() if ignore: ignored_names = ignore(os.path.dirname(src), os.listdir(src)) os.makedirs(dst, exist_ok=dirs_exist_ok) for item in os.listdir(src): if item in ignored_names: continue src_item = os.path.join(src, item) dst_item = os.path.join(dst, item) if symlinks and os.path.islink(src_item): linkto = os.readlink(src_item) os.symlink(linkto, dst_item) elif os.path.isdir(src_item): custom_copytree(src_item, dst_item, symlinks, ignore, copy_function, dirs_exist_ok=True) else: copy_function(src_item, dst_item) return dst"},{"question":"# Python Coding Assessment: Mocking with `unittest.mock` Objective Write a Python unit test using the `unittest.mock` library to demonstrate your understanding of mocking in unit tests. This question will assess your ability to mock objects, methods, and interactions effectively. Background You are given two Python classes: `EmailService` and `UserManager`. The `EmailService` class is responsible for sending emails, while the `UserManager` class uses `EmailService` to send a welcome email when a new user is registered. ```python class EmailService: def send_welcome_email(self, email_address: str) -> bool: # Simulate sending email print(f\\"Sending welcome email to {email_address}\\") return True class UserManager: def __init__(self, email_service: EmailService): self.email_service = email_service def register_user(self, username: str, email_address: str) -> str: # Simulate user registration logic if not username or not email_address: return \\"Registration failed: Missing username or email address.\\" # Send welcome email email_sent = self.email_service.send_welcome_email(email_address) if email_sent: return f\\"User {username} successfully registered.\\" else: return \\"Registration failed: Could not send welcome email.\\" ``` Task 1. Write a unit test class `TestUserManager` using the `unittest` framework. 2. Mock the `EmailService` using the `unittest.mock` library to: * Simulate sending an email successfully. * Simulate a failure in sending an email. 3. Write two test methods: * `test_register_user_success`: should verify that a user is successfully registered when the email is sent without issues. * `test_register_user_email_failure`: should verify that registration fails when the email cannot be sent. Constraints * You must use the `unittest` framework and `unittest.mock` library. * The test methods should not make actual HTTP calls or send real emails. * Assume that usernames and email addresses are valid strings when provided. Expected Output Your test methods should: * Verify the outcome of the `register_user` method based on different behaviors of `send_welcome_email`. * Ensure proper isolation of the `UserManager` class by mocking the `EmailService`. # Example ```python import unittest from unittest.mock import MagicMock class TestUserManager(unittest.TestCase): def test_register_user_success(self): # Arrange mock_email_service = MagicMock() mock_email_service.send_welcome_email.return_value = True user_manager = UserManager(mock_email_service) # Act result = user_manager.register_user(\\"testuser\\", \\"test@example.com\\") # Assert self.assertEqual(result, \\"User testuser successfully registered.\\") mock_email_service.send_welcome_email.assert_called_once_with(\\"test@example.com\\") def test_register_user_email_failure(self): # Arrange mock_email_service = MagicMock() mock_email_service.send_welcome_email.return_value = False user_manager = UserManager(mock_email_service) # Act result = user_manager.register_user(\\"testuser\\", \\"test@example.com\\") # Assert self.assertEqual(result, \\"Registration failed: Could not send welcome email.\\") mock_email_service.send_welcome_email.assert_called_once_with(\\"test@example.com\\") if __name__ == \'__main__\': unittest.main() ``` This problem requires students to demonstrate their understanding of unit testing and mocking using `unittest.mock`.","solution":"import unittest from unittest.mock import MagicMock class EmailService: def send_welcome_email(self, email_address: str) -> bool: # Simulate sending email print(f\\"Sending welcome email to {email_address}\\") return True class UserManager: def __init__(self, email_service: EmailService): self.email_service = email_service def register_user(self, username: str, email_address: str) -> str: # Simulate user registration logic if not username or not email_address: return \\"Registration failed: Missing username or email address.\\" # Send welcome email email_sent = self.email_service.send_welcome_email(email_address) if email_sent: return f\\"User {username} successfully registered.\\" else: return \\"Registration failed: Could not send welcome email.\\""},{"question":"Multi-threaded Task Manager # Objective: You are required to implement a multi-threaded task manager utilizing the `queue` module. Your solution must demonstrate a comprehensive understanding of various queue types and threading mechanisms in Python. # Task Description: Write a Python program that implements three types of task queues using `queue.Queue`, `queue.LifoQueue`, and `queue.PriorityQueue`. You will simulate task processing with multiple producer and consumer threads. # Requirements: 1. **Task Class**: - Create a `Task` class with attributes `task_id`, `task_type` (FIFO, LIFO, or Priority), and `priority` for priority tasks. - Implement a string representation method for easy identification of tasks. 2. **Queue Management**: - Instantiate three queues: `fifo_queue`, `lifo_queue`, and `priority_queue` with a `maxsize` of 10. - Implement a function to add tasks to the appropriate queue based on `task_type`. 3. **Producer Thread**: - Implement a producer thread that randomly creates tasks and adds them to the queues. Ensure that the producer can handle `Full` exceptions and retries after a delay. 4. **Consumer Thread**: - Implement consumer threads for each queue type. Consumers should retrieve and process tasks. Ensure that consumers handle `Empty` exceptions and wait if the queue is empty. 5. **Task Processing**: - Simulate task processing by having the consumer wait for a random short duration (e.g., `time.sleep`). 6. **Completion Check**: - Implement a mechanism using `task_done` and `join` to wait until all tasks are processed. # Constraints: - Use a maximum of 2 producer threads and 3 consumer threads (one for each queue type). - Each task should be a dictionary containing `task_id`, `task_type`, and, if applicable, `priority`. # Input: - No input is required from the user. The program should run autonomously. # Output: - Print statements indicating task creation, addition to queues, and task processing by consumers. - Ensure clear and formatted output showing the working of the task manager. # Example: ```python import queue from threading import Thread import time import random class Task: def __init__(self, task_id, task_type, priority=0): self.task_id = task_id self.task_type = task_type self.priority = priority def __str__(self): return f\\"Task({self.task_id}, {self.task_type}, {self.priority})\\" # Define queues fifo_queue = queue.Queue(maxsize=10) lifo_queue = queue.LifoQueue(maxsize=10) priority_queue = queue.PriorityQueue(maxsize=10) def add_task(task): if task.task_type == \\"FIFO\\": fifo_queue.put(task) elif task.task_type == \\"LIFO\\": lifo_queue.put(task) elif task.task_type == \\"PRIORITY\\": priority_queue.put((task.priority, task)) def producer(): task_id = 0 while True: task_type = random.choice([\\"FIFO\\", \\"LIFO\\", \\"PRIORITY\\"]) priority = random.randint(1, 10) if task_type == \\"PRIORITY\\" else 0 task = Task(task_id, task_type, priority) task_id += 1 print(f\\"Producer created {task}\\") try: add_task(task) print(f\\"Added {task} to {task_type} queue\\") except queue.Full: print(\\"Queue is full, retrying...\\") time.sleep(random.random()) def consumer(queue, queue_type): while True: try: if queue_type == \\"PRIORITY\\": priority, task = queue.get() else: task = queue.get() print(f\\"Processing {task} from {queue_type} queue\\") time.sleep(random.random()) queue.task_done() except queue.Empty: print(f\\"{queue_type} queue is empty, waiting...\\") time.sleep(random.random()) # Start producer threads producers = [Thread(target=producer) for _ in range(2)] for p in producers: p.start() # Start consumer threads consumers = [ Thread(target=consumer, args=(fifo_queue, \\"FIFO\\")), Thread(target=consumer, args=(lifo_queue, \\"LIFO\\")), Thread(target=consumer, args=(priority_queue, \\"PRIORITY\\")), ] for c in consumers: c.start() for p in producers: p.join() for q in [fifo_queue, lifo_queue, priority_queue]: q.join() print(\\"All tasks have been processed.\\") ``` # Notes: - Consider thread safety and ensure proper handling of shared resources. - Use appropriate print statements for debugging and visualizing the process flow. - You are encouraged to use other helper functions or classes as needed to keep the code modular and organized.","solution":"import queue from threading import Thread, current_thread import time import random class Task: def __init__(self, task_id, task_type, priority=0): self.task_id = task_id self.task_type = task_type self.priority = priority def __str__(self): return f\\"Task({self.task_id}, {self.task_type}, {self.priority})\\" # Define queues fifo_queue = queue.Queue(maxsize=10) lifo_queue = queue.LifoQueue(maxsize=10) priority_queue = queue.PriorityQueue(maxsize=10) def add_task(task): if task.task_type == \\"FIFO\\": fifo_queue.put(task, block=True, timeout=2) elif task.task_type == \\"LIFO\\": lifo_queue.put(task, block=True, timeout=2) elif task.task_type == \\"PRIORITY\\": priority_queue.put((task.priority, task), block=True, timeout=2) def producer(producer_id): task_id = producer_id * 1000 # unique starting point for each producer while True: task_type = random.choice([\\"FIFO\\", \\"LIFO\\", \\"PRIORITY\\"]) priority = random.randint(1, 10) if task_type == \\"PRIORITY\\" else 0 task = Task(task_id, task_type, priority) task_id += 1 print(f\\"{current_thread().name} created {task}\\") try: add_task(task) print(f\\"{current_thread().name} added {task} to {task_type} queue\\") except queue.Full: print(f\\"{current_thread().name} Queue is full, retrying...\\") time.sleep(random.random()) def consumer(queue, queue_type): while True: try: if queue_type == \\"PRIORITY\\": priority, task = queue.get(block=True, timeout=2) else: task = queue.get(block=True, timeout=2) print(f\\"{current_thread().name} processing {task} from {queue_type} queue\\") time.sleep(random.random()) queue.task_done() except queue.Empty: print(f\\"{queue_type} queue is empty, waiting...\\") time.sleep(random.random()) def start_threads(): # Start producer threads producers = [Thread(target=producer, args=(i,), name=f\\"Producer-{i}\\") for i in range(2)] for p in producers: p.start() # Start consumer threads consumers = [ Thread(target=consumer, args=(fifo_queue, \\"FIFO\\"), name=\\"Consumer-FIFO\\"), Thread(target=consumer, args=(lifo_queue, \\"LIFO\\"), name=\\"Consumer-LIFO\\"), Thread(target=consumer, args=(priority_queue, \\"PRIORITY\\"), name=\\"Consumer-PRIORITY\\"), ] for c in consumers: c.start() for p in producers: p.join() for q in [fifo_queue, lifo_queue, priority_queue]: q.join() print(\\"All tasks have been processed.\\") if __name__ == \\"__main__\\": start_threads()"},{"question":"**Question:** Analyzing and Visualizing Sales Data You have been provided with a dataset containing sales data for a retail company. The dataset includes the following columns: - `date`: the date of the sale - `store`: store identifier - `product`: product identifier - `sales`: number of units sold The dataset starts from January 1, 2020, and continues daily for the entire year. Your task is to perform data analysis and visualization based on the sales performance. **1. Data Loading and Preparation:** - Read the sales data from a CSV file named `sales_data.csv`. - Ensure that the `date` column is parsed as a datetime object. - The dataset might have missing values. Fill any missing `sales` data with `0`. **2. Monthly Sales Overview:** - Create a DataFrame `monthly_sales` that contains the total sales for each product for each month. - Plot the total monthly sales for each product as a line plot. Customize the plot with: - Title: \\"Monthly Sales Overview\\" - X-axis label: \\"Month\\" - Y-axis label: \\"Total Sales\\" - A legend showing product names **3. Top 5 Products Visualization:** - Identify the top 5 products based on total annual sales. - Create a bar plot showing the total annual sales of these top 5 products. - Customize the plot with: - Title: \\"Top 5 Products by Annual Sales\\" - X-axis label: \\"Product\\" - Y-axis label: \\"Total Sales\\" **4. Sales Distribution Analysis:** - Plot a histogram of the daily sales distribution for each product. - Customize the histogram with: - Title: \\"Sales Distribution per Product\\" - Use different colors for each product - Add a legend to distinguish between products **5. Correlation Analysis:** - Create a scatter plot to analyze the correlation between daily sales of the top-selling product and the second top-selling product. - Customize the plot with: - Title: \\"Sales Correlation between Product A and Product B\\" - X-axis label: \\"Product A Daily Sales\\" - Y-axis label: \\"Product B Daily Sales\\" **Constraints:** - Ensure the code is well-documented with comments explaining each step. - Provide clear and readable plots with appropriately labeled axes and titles. - Optimize performance to handle large datasets efficiently. **Input:** A CSV file named `sales_data.csv` with columns: `date`, `store`, `product`, and `sales`. **Output:** 1. A line plot showing the total monthly sales for each product. 2. A bar plot showing the total annual sales for the top 5 products. 3. A histogram showing the sales distribution for each product. 4. A scatter plot analyzing the correlation between the sales of the top 2 products. **Example CSV Contents:** ``` date,store,product,sales 2020-01-01,store_1,product_1,10 2020-01-01,store_1,product_2,5 ... 2020-12-31,store_3,product_5,7 ``` Ensure to include proper imports, reading functions, data manipulation operations, and plotting methods using pandas and matplotlib.","solution":"import pandas as pd import matplotlib.pyplot as plt def load_and_prepare_data(filepath): Load the sales data from a CSV file, ensure date is parsed as datetime, and handle missing values. Args: filepath (str): Path to the CSV file Returns: pd.DataFrame: Prepared DataFrame df = pd.read_csv(filepath, parse_dates=[\'date\']) df[\'sales\'].fillna(0, inplace=True) return df def create_monthly_sales_overview(df): Create a DataFrame containing total sales for each product for each month and generate a line plot. Args: df (pd.DataFrame): DataFrame containing the sales data Returns: pd.DataFrame: Grouped DataFrame containing monthly sales df[\'month\'] = df[\'date\'].dt.to_period(\'M\') monthly_sales = df.groupby([\'month\', \'product\'])[\'sales\'].sum().unstack().fillna(0) monthly_sales.plot(kind=\'line\', title=\\"Monthly Sales Overview\\") plt.xlabel(\\"Month\\") plt.ylabel(\\"Total Sales\\") plt.legend(title=\\"Product\\") plt.show() return monthly_sales def plot_top_5_products_by_annual_sales(df): Identify top 5 products by total annual sales and generate a bar plot. Args: df (pd.DataFrame): DataFrame containing the sales data total_annual_sales = df.groupby(\'product\')[\'sales\'].sum() top_5_products = total_annual_sales.nlargest(5) top_5_products.plot(kind=\'bar\', title=\\"Top 5 Products by Annual Sales\\") plt.xlabel(\\"Product\\") plt.ylabel(\\"Total Sales\\") plt.show() def plot_sales_distribution_per_product(df): Plot histogram of daily sales distribution for each product. Args: df (pd.DataFrame): DataFrame containing the sales data products = df[\'product\'].unique() for product in products: sales_data = df[df[\'product\'] == product][\'sales\'] plt.hist(sales_data, bins=30, alpha=0.5, label=product) plt.title(\\"Sales Distribution per Product\\") plt.legend(title=\\"Product\\") plt.show() def plot_correlation_between_top_two_products(df): Create a scatter plot to analyze correlation between daily sales of the top 2 products. Args: df (pd.DataFrame): DataFrame containing the sales data total_annual_sales = df.groupby(\'product\')[\'sales\'].sum() top_2_products = total_annual_sales.nlargest(2).index daily_sales_top_2 = df[df[\'product\'].isin(top_2_products)].pivot(index=\'date\', columns=\'product\', values=\'sales\') daily_sales_top_2 = daily_sales_top_2.fillna(0) plt.scatter(daily_sales_top_2[top_2_products[0]], daily_sales_top_2[top_2_products[1]]) plt.title(\\"Sales Correlation between Product A and Product B\\") plt.xlabel(f\\"{top_2_products[0]} Daily Sales\\") plt.ylabel(f\\"{top_2_products[1]} Daily Sales\\") plt.show()"},{"question":"**Question: Custom Clustering Evaluation with DBSCAN and KMeans** You are provided with a dataset, `data.csv` which contains multi-dimensional data points without labels. Your task is to: 1. Cluster the data using both the KMeans and DBSCAN algorithms. 2. Evaluate the quality of the clustering using the Silhouette Coefficient, Adjusted Rand Index, and Calinski-Harabasz Index for both algorithms. 3. Implement a function to perform the above tasks and return the evaluation metrics. **Instructions:** 1. Read the dataset `data.csv`. Assume it has no header and each row represents a data point. 2. Use the KMeans algorithm to cluster the data. For KMeans, set the number of clusters to 3. 3. Use the DBSCAN algorithm to cluster the data. The choice of `eps` and `min_samples` parameters should be explored to find the best possible clustering. 4. Implement the following function: ```python from sklearn.cluster import KMeans, DBSCAN from sklearn.metrics import silhouette_score, adjusted_rand_score, calinski_harabasz_score def evaluate_clustering(file_path: str) -> dict: Reads the data from file_path and clusters the data using KMeans and DBSCAN algorithms. Evaluates the quality of clustering using various metrics and returns the evaluation results. Args: file_path (str): The path to the dataset file (data.csv). Returns: dict: A dictionary containing the evaluation metrics for both algorithms with the following keys: \'kmeans_silhouette\', \'kmeans_ari\', \'kmeans_ch\', \'dbscan_silhouette\', \'dbscan_ari\', \'dbscan_ch\'. # Implement the function here ``` **Constraints and Requirements:** 1. Your implementation should handle cases where DBSCAN may not find any clusters. 2. Use the sklearn library for clustering and metric evaluations. 3. Assume the input data is clean and well-formatted. 4. Test and find suitable values for `eps` and `min_samples` parameters in DBSCAN to get meaningful clusters. 5. Ensure that your function returns a dictionary even if the DBSCAN does not form any valid clusters. **Expected Output:** ```python { \'kmeans_silhouette\': float, \'kmeans_ari\': float, \'kmeans_ch\': float, \'dbscan_silhouette\': float or None, \'dbscan_ari\': float or None, \'dbscan_ch\': float or None } ``` **Explanation:** - `\'kmeans_silhouette\'`: Silhouette Coefficient for KMeans clustering. - `\'kmeans_ari\'`: Adjusted Rand Index for KMeans clustering. - `\'kmeans_ch\'`: Calinski-Harabasz Index for KMeans clustering. - `\'dbscan_silhouette\'`: Silhouette Coefficient for DBSCAN clustering (None if DBSCAN did not form valid clusters). - `\'dbscan_ari\'`: Adjusted Rand Index for DBSCAN clustering (None if DBSCAN did not form valid clusters). - `\'dbscan_ch\'`: Calinski-Harabasz Index for DBSCAN clustering (None if DBSCAN did not form valid clusters). Write your complete implementation of the function `evaluate_clustering`.","solution":"import pandas as pd from sklearn.cluster import KMeans, DBSCAN from sklearn.metrics import silhouette_score, adjusted_rand_score, calinski_harabasz_score def evaluate_clustering(file_path: str) -> dict: # Load the dataset data = pd.read_csv(file_path, header=None) X = data.values # Define results dictionary results = { \'kmeans_silhouette\': None, \'kmeans_ari\': None, \'kmeans_ch\': None, \'dbscan_silhouette\': None, \'dbscan_ari\': None, \'dbscan_ch\': None } # KMeans Clustering kmeans = KMeans(n_clusters=3, random_state=42).fit(X) kmeans_labels = kmeans.labels_ results[\'kmeans_silhouette\'] = silhouette_score(X, kmeans_labels) # For ARI, usually ground truth labels needed, but here we will use the cluster labels themselves for the sake of a simple example results[\'kmeans_ari\'] = adjusted_rand_score(kmeans_labels, kmeans_labels) results[\'kmeans_ch\'] = calinski_harabasz_score(X, kmeans_labels) # DBSCAN Clustering dbscan = DBSCAN(eps=0.5, min_samples=5).fit(X) dbscan_labels = dbscan.labels_ # Check if DBSCAN found any clusters (-1 indicates noise points) if len(set(dbscan_labels) - {-1}) > 0: results[\'dbscan_silhouette\'] = silhouette_score(X, dbscan_labels) # For ARI, usually ground truth labels needed, but here we will use the cluster labels themselves for the sake of a simple example results[\'dbscan_ari\'] = adjusted_rand_score(dbscan_labels, dbscan_labels) results[\'dbscan_ch\'] = calinski_harabasz_score(X, dbscan_labels) return results"},{"question":"# Seaborn KDEPlot Assessment Objective: Design an assessment task that evaluates the student\'s understanding and ability to use the seaborn `kdeplot` function for creating and customizing kernel density estimation plots. Task: 1. Load the \\"penguins\\" dataset from seaborn. 2. Create a univariate kernel density plot for the \\"flipper_length_mm\\" variable. Adjust the plot such that it is displayed along the y-axis and uses moderate smoothing (`bw_adjust=1.5`). 3. Generate a bivariate kernel density plot illustrating the relationship between \\"flipper_length_mm\\" and \\"body_mass_g\\". 4. Create a univariate kernel density plot for \\"bill_length_mm\\", with conditional distributions divided by species. Use filling to show the density areas, ensure there is no line. 5. Plot the distribution for the \\"body_mass_g\\" variable, using weights derived from another variable in the dataset. 6. Customize the appearance of any one of the above plots to use a different color palette (\\"muted\\"), alpha transparency of 0.6, and no contour (outline). # Expected Input: - No input parameters are required. # Expected Output: - The script should save each of the generated plots as separate image files in the current directory: - `flipper_length_y_plot.png` - `flipper_body_mass_plot.png` - `bill_length_species_plot.png` - `body_mass_weighted_plot.png` - `customized_plot.png` Constraints: - Use the seaborn library version 0.11.2 or later. - Use matplotlib for any additional customization if needed. # Example: ```python import seaborn as sns import matplotlib.pyplot as plt # 1. Load the dataset penguins = sns.load_dataset(\\"penguins\\") # 2. Univariate KDE plot along y-axis with moderate smoothing sns.kdeplot(data=penguins, y=\\"flipper_length_mm\\", bw_adjust=1.5) plt.savefig(\'flipper_length_y_plot.png\') plt.clf() # 3. Bivariate KDE plot sns.kdeplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\") plt.savefig(\'flipper_body_mass_plot.png\') plt.clf() # 4. Univariate KDE plot with species as hue and filled areas sns.kdeplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", fill=True) plt.savefig(\'bill_length_species_plot.png\') plt.clf() # 5. Weighted KDE plot for body_mass_g penguins_agg = (penguins .groupby(\\"species\\") .agg(body_mass_g=(\\"body_mass_g\\", \\"mean\\"), n=(\\"body_mass_g\\", \\"count\\")) ) sns.kdeplot(data=penguins_agg, x=\\"body_mass_g\\", weights=\\"n\\") plt.savefig(\'body_mass_weighted_plot.png\') plt.clf() # 6. Customized plot ax = sns.kdeplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", fill=True, palette=\\"muted\\", alpha=0.6, linewidth=0) plt.savefig(\'customized_plot.png\') plt.clf() ``` Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Univariate KDE plot along y-axis with moderate smoothing sns.kdeplot(data=penguins, y=\\"flipper_length_mm\\", bw_adjust=1.5) plt.savefig(\'flipper_length_y_plot.png\') plt.clf() # 2. Bivariate KDE plot sns.kdeplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\") plt.savefig(\'flipper_body_mass_plot.png\') plt.clf() # 3. Univariate KDE plot with species as hue and filled areas sns.kdeplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", fill=True) plt.savefig(\'bill_length_species_plot.png\') plt.clf() # 4. Weighted KDE plot for body_mass_g sns.kdeplot(data=penguins, x=\\"body_mass_g\\", weights=\\"bill_depth_mm\\") plt.savefig(\'body_mass_weighted_plot.png\') plt.clf() # 5. Customized plot ax = sns.kdeplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", fill=True, palette=\\"muted\\", alpha=0.6, linewidth=0) plt.savefig(\'customized_plot.png\') plt.clf()"},{"question":"Coding Assessment Question **Objective**: Demonstrate your understanding of the `marshal` module for object serialization and deserialization in Python. # Task You are given a nested dictionary containing various data types that are supported by the `marshal` module. Your task is to: 1. Serialize this dictionary into a binary format using `marshal`. 2. Store the serialized data into a binary file. 3. Read the binary file and deserialize the data back to its original dictionary format. 4. Verify that the original dictionary matches the deserialized dictionary. # Input 1. A dictionary `data` containing nested supported types by `marshal`. # Expected Functions You should implement the following functions: 1. `serialize_dictionary(data: dict, filename: str) -> None` - **Input**: - `data`: A dictionary to serialize. - `filename`: The binary file name to store the serialized data. - **Output**: None (The function should write to the specified file). 2. `deserialize_dictionary(filename: str) -> dict` - **Input**: - `filename`: The binary file name from which to read the serialized data. - **Output**: - A dictionary that is the deserialized version of the binary data. # Constraints - The dictionary will only contain types supported by `marshal` (booleans, integers, floating point numbers, complex numbers, strings, bytes, bytearrays, tuples, lists, sets, frozensets, dictionaries, and supported singletons). - The nested depth of the dictionary will not exceed 5 levels. # Example Usage ```python data = { \\"number\\": 123, \\"string\\": \\"example\\", \\"list\\": [1, 2, 3], \\"nested_dict\\": { \\"float\\": 1.23, \\"bytes\\": b\\"binary\\" } } filename = \\"serialized_data.marshal\\" # Serialize the dictionary to a file serialize_dictionary(data, filename) # Deserialize the dictionary from the file result = deserialize_dictionary(filename) # Verify that the deserialized dictionary matches the original dictionary assert data == result ``` # Notes - Ensure that you handle any exceptions that might occur during file operations. - You are not required to handle unsupported types, as the input will always be according to the constraints.","solution":"import marshal def serialize_dictionary(data: dict, filename: str) -> None: Serializes the given dictionary into a binary file using the marshal module. Parameters: data (dict): The dictionary to serialize. filename (str): The filename where the serialized data will be stored. with open(filename, \'wb\') as file: marshal.dump(data, file) def deserialize_dictionary(filename: str) -> dict: Deserializes the data from the given binary file back into a dictionary using the marshal module. Parameters: filename (str): The filename from which to read the serialized data. Returns: dict: The deserialized dictionary. with open(filename, \'rb\') as file: data = marshal.load(file) return data"},{"question":"**Question: Implement Comprehensive Subnet Summarization** You are required to implement a function that takes a list of IPv4 or IPv6 addresses and network masks, summarizes the address ranges, and returns the merged network range summarization. # Function Signature ```python def summarize_subnets(subnet_list: list) -> list: pass ``` # Input: - `subnet_list`: list of tuples, where each tuple contains a string representing an IP address and an integer for the network prefix. E.g., `[(\'192.168.1.0\', 24), (\'192.168.1.128\', 25), (\'2001:db8::\', 32)]` # Output: - A list of summarized network ranges where each range is represented as a string in CIDR notation. The summarized network ranges must combine any overlapping or contiguous address ranges into the largest possible subnet. # Example: ```python subnets = [ (\'192.168.0.0\', 24), (\'192.168.1.0\', 24), (\'192.168.0.0\', 25), (\'2001:db8::\', 64), (\'2001:db8:0:1::\', 64) ] assert summarize_subnets(subnets) == [\'192.168.0.0/23\', \'2001:db8::/63\'] ``` # Constraints: - The function should handle both IPv4 and IPv6 addresses. - There should be no mixed versions in the input list. - The IPv4 addresses are valid if they fit in 32 bits and follow proper decimal-dot notation. - The IPv6 addresses are valid if they fit in 128 bits and follow the proper hexadecimal notation. # Notes: - Use the `ipaddress` module to manage IP address operations. - You may use `ipaddress.collapse_addresses` to help with address summarization. - Ensure the function raises a `TypeError` if the input contains mixed IPv4 and IPv6 addresses. - Address summarization should ensure no unnecessary subnets are created (i.e., combine as many networks as feasibly possible). # Implementation: ```python import ipaddress def summarize_subnets(subnet_list: list) -> list: if not subnet_list: return [] # Determine if the subnets are IPv4 or IPv6 version = None networks = [] for addr, prefix in subnet_list: if \':\' in addr: # IPv6 Address if version is None: version = 6 elif version == 4: raise TypeError(\'Mixed IPv4 and IPv6 addresses are not allowed\') networks.append(ipaddress.IPv6Network(f\\"{addr}/{prefix}\\", strict=False)) else: # IPv4 Address if version is None: version = 4 elif version == 6: raise TypeError(\'Mixed IPv4 and IPv6 addresses are not allowed\') networks.append(ipaddress.IPv4Network(f\\"{addr}/{prefix}\\", strict=False)) summarized_networks = ipaddress.collapse_addresses(networks) return [str(network) for network in summarized_networks] # Example Usage subnets = [ (\'192.168.0.0\', 24), (\'192.168.1.0\', 24), (\'192.168.0.0\', 25), (\'2001:db8::\', 64), (\'2001:db8:0:1::\', 64) ] print(summarize_subnets(subnets)) # Output: [\'192.168.0.0/23\', \'2001:db8::/63\'] ``` # Testing: Test your implementation with a variety of subnet combinations to ensure summarization works as expected, and that mixed IPv4/IPv6 subnets raise a `TypeError`.","solution":"import ipaddress def summarize_subnets(subnet_list: list) -> list: Summarizes a list of subnets into the largest possible combined subnets without overlaps. :param subnet_list: list of tuples, where each tuple contains a string representing an IP address and an integer for the network prefix. :return: a list of summarized network ranges in CIDR notation. if not subnet_list: return [] # Determine if the subnets are IPv4 or IPv6 version = None networks = [] for addr, prefix in subnet_list: if \':\' in addr: # IPv6 Address if version is None: version = 6 elif version == 4: raise TypeError(\'Mixed IPv4 and IPv6 addresses are not allowed\') networks.append(ipaddress.IPv6Network(f\\"{addr}/{prefix}\\", strict=False)) else: # IPv4 Address if version is None: version = 4 elif version == 6: raise TypeError(\'Mixed IPv4 and IPv6 addresses are not allowed\') networks.append(ipaddress.IPv4Network(f\\"{addr}/{prefix}\\", strict=False)) summarized_networks = ipaddress.collapse_addresses(networks) return [str(network) for network in summarized_networks]"},{"question":"# Custom JSON Encoder and Decoder for Special Data Types Objective Design and implement a custom JSON encoder and decoder to handle the serialization and deserialization of Python\'s `datetime` objects and complex numbers. This will demonstrate your understanding of extending the `json` module\'s functionality and handling specialized data types not supported by default. Problem Statement You are required to: 1. Create a subclass `CustomJSONEncoder` of `json.JSONEncoder` to handle encoding of `datetime.datetime` and complex numbers. 2. Implement a custom decoding mechanism to handle these encoded forms back into their original Python objects. Specifically: - For `datetime.datetime` objects, serialize them in the format `\\"YYYY-MM-DDTHH:MM:SS\\"`. - For complex numbers, serialize them as a dictionary with the form `{\\"real\\": x, \\"imag\\": y}`. - The decoder should correctly interpret these formats and recreate the original Python objects. Function Signatures 1. `CustomJSONEncoder` - Should override the `default` method. ```python class CustomJSONEncoder(json.JSONEncoder): def default(self, obj): # Implement special handling for datetime and complex numbers pass ``` 2. `custom_json_decoder(dct)` - This function should receive a dictionary and return a reconstructed `datetime.datetime` or complex number if the dictionary matches the expected serialized form. ```python def custom_json_decoder(dct): # Implement custom decoding for datetime and complex numbers pass ``` 3. `encode_data(data)`: - This function should use the `CustomJSONEncoder` to convert the input Python objects to JSON format. ```python def encode_data(data): # Use CustomJSONEncoder to encode data to JSON format pass ``` 4. `decode_data(json_str)`: - This function should use the custom decoder to convert JSON strings back to their original Python objects. ```python def decode_data(json_str): # Use custom_json_decoder to decode JSON string back to Python objects pass ``` Example ```python import json from datetime import datetime # Example data containing datetime and complex number data = { \'timestamp\': datetime(2023, 10, 5, 14, 30, 45), \'complex_number\': 3 + 4j } # Encoding data json_str = encode_data(data) print(json_str) # Expected output format: {\\"timestamp\\": \\"2023-10-05T14:30:45\\", \\"complex_number\\": {\\"real\\": 3.0, \\"imag\\": 4.0}} # Decoding data decoded_data = decode_data(json_str) print(decoded_data) # Expected output: {\'timestamp\': datetime(2023, 10, 5, 14, 30, 45), \'complex_number\': (3+4j)} ``` Constraints - You cannot use any third-party libraries other than `json`. - Ensure that the custom encoder and decoder handle non-special types in a standard manner. - Maintain performance and handle edge cases appropriately. - The input JSON string for decoding will conform to standard JSON format with possibly nested data structures. Evaluation Criteria - Correctness: Ensure the encoded and decoded data maintain structural and data integrity. - Clarity and readability of the code. - Efficient handling of edge cases and performance considerations.","solution":"import json from datetime import datetime class CustomJSONEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, datetime): return {\\"__type__\\": \\"datetime\\", \\"value\\": obj.strftime(\\"%Y-%m-%dT%H:%M:%S\\")} elif isinstance(obj, complex): return {\\"__type__\\": \\"complex\\", \\"real\\": obj.real, \\"imag\\": obj.imag} # Call the default method for other types return super().default(obj) def custom_json_decoder(dct): if \\"__type__\\" in dct: if dct[\\"__type__\\"] == \\"datetime\\": return datetime.strptime(dct[\\"value\\"], \\"%Y-%m-%dT%H:%M:%S\\") elif dct[\\"__type__\\"] == \\"complex\\": return complex(dct[\\"real\\"], dct[\\"imag\\"]) return dct def encode_data(data): return json.dumps(data, cls=CustomJSONEncoder) def decode_data(json_str): return json.loads(json_str, object_hook=custom_json_decoder)"},{"question":"# Secure Communication System **Objective:** Your task is to implement a secure communication system using Python\'s `hashlib` module. This system involves creating, sending, and verifying messages that have been securely hashed and authenticated. # Requirements: 1. **Functionality:** - **Message Hashing and Signing:** Create a function `sign_message` that takes a message `str`, a secret key `bytes`, and an optional personalization `str`. This function should return a secure hash of the message using the key and optional personalization string. - **Message Verification:** Create a function `verify_message` that takes the original message `str`, the secret key `bytes`, the optional personalization `str`, and the previously generated hash `bytes`. This function should return `True` if the hash matches the message, `False` otherwise. 2. **Input and Output Formats:** - Input for `sign_message`: - `message: str` - `secret_key: bytes` - `personalization: str` (optional) - Output for `sign_message`: - `hashed_message: bytes` - Input for `verify_message`: - `message: str` - `secret_key: bytes` - `personalization: str` (optional) - `hashed_message: bytes` - Output for `verify_message`: - `is_valid: bool` 3. **Constraints:** - The `secret_key` should be a byte string at least 16 bytes long. - The `message` should be less than or equal to 1024 characters. - Use `hashlib.blake2b` algorithm with a digest size of 32 bytes for hashing. - The `personalization` should default to an empty string if not provided. 4. **Performance:** - Ensure that the hashing and verification functions are efficient and execute in a reasonable time frame for messages up to 1024 characters. # Example Usage: ```python def sign_message(message: str, secret_key: bytes, personalization: str = \\"\\") -> bytes: # Implement the secure hashing and signing mechanism pass def verify_message(message: str, secret_key: bytes, personalization: str, hashed_message: bytes) -> bool: # Implement the verification mechanism pass # Example usage: message = \\"This is a secret message.\\" key = b\'supersecretkey12345\' personalization = \\"user1\\" hashed = sign_message(message, key, personalization) is_valid = verify_message(message, key, personalization, hashed) print(f\\"Hashed Message: {hashed}\\") print(f\\"Is the message valid?: {is_valid}\\") # Should print: Is the message valid?: True ``` # Notes: - Consider edge cases such as very short or very long messages, varying key lengths, and different personalization strings. - You can utilize `hashlib.blake2b` functions directly, and refer to the Python documentation if needed.","solution":"import hashlib def sign_message(message: str, secret_key: bytes, personalization: str = \\"\\") -> bytes: Returns a secure hash of the message using the secret key and optional personalization string. if len(secret_key) < 16: raise ValueError(\\"Secret key must be at least 16 bytes long.\\") if len(message) > 1024: raise ValueError(\\"Message must be 1024 characters or less.\\") hasher = hashlib.blake2b(key=secret_key, digest_size=32, person=personalization.encode()) hasher.update(message.encode()) return hasher.digest() def verify_message(message: str, secret_key: bytes, personalization: str, hashed_message: bytes) -> bool: Verifies if the given hashed_message matches the hash of the original message with the secret key and optional personalization string. expected_hash = sign_message(message, secret_key, personalization) return expected_hash == hashed_message"},{"question":"**Objective**: Demonstrate your understanding of concurrent execution using the `concurrent.futures` module. **Problem Statement**: You are given a list of URLs, and your task is to concurrently download the content of these URLs using both threading and future objects from the `concurrent.futures` module. Implement a function `download_pages(urls: List[str], max_workers: int) -> Dict[str, Union[bytes, str]]` that performs the following steps: 1. Uses `ThreadPoolExecutor` to concurrently download the content of the URLs. Each download should have a timeout of 10 seconds. 2. Handles any exceptions that might occur during the downloads, such as timeouts or connection errors. 3. Returns a dictionary mapping each URL to its downloaded content or an appropriate error message if the download failed. **Function Signature**: ```python from typing import List, Dict, Union import concurrent.futures import urllib.request def download_pages(urls: List[str], max_workers: int) -> Dict[str, Union[bytes, str]]: pass ``` **Input**: - `urls`: A list of URLs (strings) to be downloaded. - `max_workers`: An integer specifying the maximum number of worker threads. **Output**: - A dictionary where each key is a URL, and each value is either the content of the URL (as bytes) or an error message (as string). **Constraints**: - Each URL download should timeout after 10 seconds. **Example**: ```python urls = [ \\"http://www.example.com/\\", \\"http://www.nonexistentwebsite.org/\\", \\"http://www.google.com/\\" ] result = download_pages(urls, max_workers=3) # Example output might be: # { # \\"http://www.example.com/\\": b\\"...\\", # contents of the example.com page # \\"http://www.nonexistentwebsite.org/\\": \\"Error: failed to download\\", # error message # \\"http://www.google.com/\\": b\\"...\\" # contents of the google.com page # } ``` **Note**: - Use `ThreadPoolExecutor` for managing threads. - Use proper exception handling to catch and return error messages for failures. You are encouraged to write clean and well-documented code, following best practices for concurrent programming.","solution":"from typing import List, Dict, Union import concurrent.futures import urllib.request import urllib.error def download_page(url: str) -> Union[bytes, str]: try: with urllib.request.urlopen(url, timeout=10) as response: return response.read() except Exception as e: return f\\"Error: {str(e)}\\" def download_pages(urls: List[str], max_workers: int) -> Dict[str, Union[bytes, str]]: results = {} with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor: future_to_url = {executor.submit(download_page, url): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): url = future_to_url[future] try: result = future.result() results[url] = result except Exception as e: results[url] = f\\"Error: {str(e)}\\" return results"},{"question":"**Objective**: To assess your understanding of the `os` module in Python, you are required to implement a mini-filesystem manager. This task will involve working with file and directory operations, environment variables, and process management. **Question**: Implement a class `MiniFileSystemManager` with the following functionalities: 1. **Create Directory**: - **Method Signature**: `def create_directory(self, path: str) -> None` - **Description**: Creates a directory at the specified `path`. If the directory already exists, it should print a message \\"Directory already exists\\". 2. **Remove Directory**: - **Method Signature**: `def remove_directory(self, path: str) -> None` - **Description**: Removes the directory at the specified `path`. If the directory does not exist, it should print a message \\"Directory does not exist\\". 3. **List Directory**: - **Method Signature**: `def list_directory(self, path: str) -> None` - **Description**: Lists all files and directories inside the specified `path`. If the path is not a directory or does not exist, it should raise an appropriate exception and handle it by printing \\"Invalid path or not a directory\\". 4. **Set Environment Variable**: - **Method Signature**: `def set_env_variable(self, key: str, value: str) -> None` - **Description**: Sets an environment variable `key` to `value`. 5. **Get Environment Variable**: - **Method Signature**: `def get_env_variable(self, key: str) -> str` - **Description**: Retrieves the value of the environment variable `key`. If the environment variable does not exist, it should return \\"Environment variable not found\\". 6. **Execute Command**: - **Method Signature**: `def execute_command(self, command: str) -> int` - **Description**: Executes a shell command specified by `command` and returns the exit code. **Example Usage**: ```python # Create an instance of the MiniFileSystemManager fs_manager = MiniFileSystemManager() # Create a directory fs_manager.create_directory(\'/tmp/testdir\') # List contents of the directory fs_manager.list_directory(\'/tmp\') # Set an environment variable fs_manager.set_env_variable(\'MY_ENV_VAR\', \'SOME_VALUE\') # Get the environment variable print(fs_manager.get_env_variable(\'MY_ENV_VAR\')) # Output: SOME_VALUE # Execute a shell command exit_code = fs_manager.execute_command(\'echo Hello, World!\') print(exit_code) # Output: 0 ``` # Constraints and Requirements: - Do not use any external libraries except the `os` module and standard Python libraries. - Ensure proper exception handling for all methods. - Write clear and concise code with appropriate comments.","solution":"import os import subprocess class MiniFileSystemManager: def create_directory(self, path: str) -> None: try: os.makedirs(path) print(f\\"Directory `{path}` created successfully.\\") except FileExistsError: print(\\"Directory already exists.\\") def remove_directory(self, path: str) -> None: try: os.rmdir(path) print(f\\"Directory `{path}` removed successfully.\\") except FileNotFoundError: print(\\"Directory does not exist.\\") except OSError: print(\\"Directory is not empty or other error occurred.\\") def list_directory(self, path: str) -> None: try: if os.path.isdir(path): print(f\\"Contents of directory `{path}`:\\") for entry in os.listdir(path): print(entry) else: raise NotADirectoryError except (FileNotFoundError, NotADirectoryError): print(\\"Invalid path or not a directory\\") def set_env_variable(self, key: str, value: str) -> None: os.environ[key] = value def get_env_variable(self, key: str) -> str: return os.getenv(key, \\"Environment variable not found\\") def execute_command(self, command: str) -> int: result = subprocess.run(command, shell=True) return result.returncode"},{"question":"Objective The task is to use the `importlib.metadata` module to create a function that provides detailed information about an installed package. This task will test your understanding of querying package metadata and using various functionalities of the `importlib.metadata` module. Problem Statement You need to implement the function `get_package_info(package_name: str) -> dict`. This function takes a string `package_name` as input, representing the name of an installed package, and returns a dictionary with the following details about the package: - `name`: The name of the package. - `version`: The version of the package. - `summary`: A short summary of the package, if available. - `author`: The author of the package, if available. - `requires_python`: The Python version requirements for the package, if available. - `entry_points`: A dictionary where keys are entry point groups (e.g., `\'console_scripts\'`) and values are lists of entry point names in those groups. - `files`: A list of file paths (as strings) included in the package. Function Signature ```python def get_package_info(package_name: str) -> dict: pass ``` Example ```python # Example input get_package_info(\\"wheel\\") ``` # Expected Output ```python { \'name\': \'wheel\', \'version\': \'0.32.3\', \'summary\': \'A built-package format for Python.\', \'author\': \'Daniel Holth\', \'requires_python\': \'>=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\', \'entry_points\': { \'console_scripts\': [\'wheel\'] }, \'files\': [ \'wheel/__init__.py\', \'wheel/bdist_wheel.py\', \'wheel/util.py\', ... ] } ``` Constraints - You can assume the input `package_name` is always a valid and installed package. - If any of the fields (`summary`, `author`, `requires_python`) are not available in the package metadata, they should be returned as `None`. - If the package does not have any entry points, return an empty dictionary for `entry_points`. - The order of files in the `files` list does not matter. - You should handle possible exceptions that might occur while accessing the package metadata and return an appropriate error message in the returned dictionary. Hints 1. Use the `importlib.metadata.version` function to get the version of the package. 2. Use the `importlib.metadata.metadata` function to retrieve metadata information. 3. Use the `importlib.metadata.entry_points` function to list entry points. 4. Use the `importlib.metadata.files` function to list files in the package. Notes - Do not import any other modules or use any external packages apart from `importlib.metadata`. - Ensure your function handles errors gracefully and includes necessary error checks.","solution":"import importlib.metadata def get_package_info(package_name: str) -> dict: try: # Getting metadata and version information metadata = importlib.metadata.metadata(package_name) version = importlib.metadata.version(package_name) # Extracting necessary fields summary = metadata.get(\'Summary\', None) author = metadata.get(\'Author\', None) requires_python = metadata.get(\'Requires-Python\', None) # Extracting entry points entry_points = {} for entry_point in importlib.metadata.entry_points().get(package_name, []): group = entry_point.group if group not in entry_points: entry_points[group] = [] entry_points[group].append(entry_point.name) # Extracting files files = list(map(str, importlib.metadata.files(package_name))) return { \'name\': package_name, \'version\': version, \'summary\': summary, \'author\': author, \'requires_python\': requires_python, \'entry_points\': entry_points, \'files\': files } except importlib.metadata.PackageNotFoundError: return {\\"error\\": f\\"Package \'{package_name}\' not found\\"} except Exception as e: return {\\"error\\": str(e)}"},{"question":"You are tasked with designing a Python function that compresses a given directory into a .zip archive file. This function should include the ability to: 1. Recursively traverse a directory and compress all files within it. 2. Provide an option to include or exclude hidden files. 3. Handle potential exceptions that might arise during the compression process. 4. Output the resulting .zip file to a specified output directory. Function Signature ```python def compress_directory_to_zip(source_dir: str, output_zip: str, include_hidden: bool) -> None: pass ``` Parameters - `source_dir` (str): The path to the directory that needs to be compressed. - `output_zip` (str): The file path for the resulting .zip archive. - `include_hidden` (bool): If True, hidden files (those starting with \'.\') should be included in the archive; otherwise, they should be excluded. Constraints - The function should handle directories with extensive nesting. - The function should efficiently compress large directories containing numerous and/or large files. - All necessary directories within the output path should be created if they do not exist. Example Usage ```python # Example usage source_dir = \'/path/to/source_directory\' output_zip = \'/path/to/output_archive.zip\' include_hidden = True compress_directory_to_zip(source_dir, output_zip, include_hidden) ``` Upon execution, the function should create a .zip file named `output_archive.zip` in the specified output path that contains all files from `source_directory`, including hidden files if `include_hidden` is set to True. Hints 1. Consider using the `os` module to traverse the directory and to check for hidden files. 2. Make use of the `zipfile` module to create and write to .zip archives. 3. Employ proper exception handling to catch and report errors such as permission issues or missing directories.","solution":"import os import zipfile def compress_directory_to_zip(source_dir: str, output_zip: str, include_hidden: bool) -> None: Compresses a given directory into a .zip archive file. Args: source_dir (str): The path to the directory that needs to be compressed. output_zip (str): The file path for the resulting .zip archive. include_hidden (bool): If True, hidden files should be included in the archive; otherwise, excluded. try: # Create the output directory if it does not exist os.makedirs(os.path.dirname(output_zip), exist_ok=True) with zipfile.ZipFile(output_zip, \'w\', zipfile.ZIP_DEFLATED) as zipf: for root, dirs, files in os.walk(source_dir): for file in files: if include_hidden or not file.startswith(\'.\'): full_path = os.path.join(root, file) relative_path = os.path.relpath(full_path, source_dir) zipf.write(full_path, relative_path) except Exception as e: print(f\\"An error occurred while compressing the directory: {e}\\")"},{"question":"Objective Implement a function that retrieves and processes the annotations dictionary of a given object, ensuring compatibility across Python versions (3.9 and older, and 3.10 and newer). The function should un-stringize any stringized annotations and handle classes, functions, and modules appropriately. Function Signature ```python def get_and_process_annotations(obj: Any) -> Dict[str, Any]: pass ``` Input - `obj`: An object that can be a class, function, or module. Output - A dictionary representing the processed annotations of the object. Any stringized annotations should be converted to their actual Python values. Constraints - The function must handle both Python 3.9 and older, and Python 3.10 and newer. - The solution should use best practices for accessing annotations as described in the provided documentation. - The function should not assume the presence of the `inspect.get_annotations()` function when running in Python 3.9 and older. Examples Here are some examples to illustrate the expected behavior: ```python # Python 3.9 and older example class Base: a: int = 3 b: str = \\"abc\\" class Derived(Base): c: float = 5.0 annotations = get_and_process_annotations(Derived) assert annotations == {\'c\': float} # Python 3.10 and newer example from __future__ import annotations def foo(x: \\"int\\", y: \\"str\\") -> \\"None\\": pass annotations = get_and_process_annotations(foo) assert annotations == {\'x\': int, \'y\': str, \'return\': None} ``` Additional Information - If the object has no annotations, return an empty dictionary. - Use `eval()` to convert stringized annotations to their actual types where necessary. - Handle edge cases, such as objects without a `__dict__` attribute or using quoted strings that can\'t be evaluated.","solution":"import typing import sys def get_and_process_annotations(obj: typing.Any) -> typing.Dict[str, typing.Any]: Retrieves and processes the annotations dictionary of a given object. Args: - obj (Any): An object that can be a class, function, or module. Returns: - Dict[str, Any]: A dictionary representing the processed annotations of the object. Any stringized annotations are converted to their actual Python values. annotations = getattr(obj, \'__annotations__\', {}) if sys.version_info >= (3.10, 0): # Use inspect.get_annotations if available (Python 3.10+) processed_annotations = typing.get_type_hints(obj) else: # For Python 3.9 and older, manually process annotations processed_annotations = {} for key, value in annotations.items(): if isinstance(value, str): try: # Use eval to convert stringized annotations to actual Python objects processed_annotations[key] = eval(value, sys.modules[obj.__module__].__dict__) except: # If eval fails, keep the string as is processed_annotations[key] = value else: processed_annotations[key] = value return processed_annotations"},{"question":"# Asynchronous Task Manager using asyncio You are required to design an asynchronous task manager that can handle multiple tasks concurrently while considering platform-specific limitations discussed in the `asyncio` documentation provided. Task Description 1. Implement an asynchronous function called `fetch_data`, which takes a URL as an argument, performs an HTTP GET request to fetch data from the URL, and returns the response text. Use the `aiohttp` library for asynchronous HTTP requests. 2. Implement a function called `manage_tasks`, which takes a list of URLs and uses `asyncio` to fetch data from all provided URLs concurrently. 3. Ensure your implementation handles platform-specific limitations outlined: - Use only features supported across all platforms, such as file-based I/O operations. - Implement platform checks where necessary to avoid using unsupported features on Windows and older macOS versions. Function Specifications 1. **fetch_data(url: str) -> str** - **Input:** - `url` (str): The URL to fetch data from. - **Output:** - `response_text` (str): The text content of the HTTP response. 2. **manage_tasks(urls: List[str]) -> List[str]** - **Input:** - `urls` (List[str]): A list of URLs to fetch data from. - **Output:** - `responses` (List[str]): A list of response texts corresponding to each URL in the input list. Example ```python import asyncio urls = [ \\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\" ] # This should print list of response texts fetched from the given URLs responses = asyncio.run(manage_tasks(urls)) print(responses) ``` Notes - Make sure to handle exceptions that can occur during the HTTP requests (e.g., network issues, invalid URLs). - Your functions should be compatible with all platforms (Windows, macOS, Unix-like) despite the limitations discussed. - Ensure your code follows best practices for writing asynchronous functions and uses proper `async`/`await` syntax. Constraints - You are required to use the `aiohttp` library for performing asynchronous HTTP requests. - Do not use unsupported platform-specific features in `asyncio`, as discussed in the documentation. **Performance Requirements:** - Your solution should be able to handle at least 100 URLs concurrently without significant delay.","solution":"import aiohttp import asyncio import platform async def fetch_data(url): Fetch data from the given URL using asynchronous HTTP request. Args: url (str): The URL to fetch data from. Returns: str: The text content of the HTTP response. async with aiohttp.ClientSession() as session: async with session.get(url) as response: return await response.text() async def manage_tasks(urls): Manage multiple tasks concurrently to fetch data from a list of URLs. Args: urls (List[str]): A list of URLs to fetch data from. Returns: List[str]: A list of response texts corresponding to each URL. tasks = [fetch_data(url) for url in urls] responses = await asyncio.gather(*tasks) return responses # For Windows and older macOS compatibility workaround. if platform.system() == \'Windows\' or (platform.system() == \'Darwin\' and platform.release() < \'20\'): asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())"},{"question":"Objective To assess your understanding of the seaborn `objects` module, you are required to create advanced visualizations using the `Plot` class. This question will test your ability to: 1. Load and manipulate datasets. 2. Create layered plots with various marks and transformations. 3. Use faceting for comparative purposes. 4. Customize plot aesthetics. Question You are provided with the `tips` dataset in seaborn, which contains information about the tips received by waitstaff in a restaurant. Use this dataset to perform the following tasks: Tasks: 1. **Load the dataset** using the seaborn `load_dataset` function. 2. **Create a basic scatter plot** showing the relationship between `total_bill` and `tip` using the `Dot` mark. 3. **Add a linear regression line** to the scatter plot using the `Line` mark and the `PolyFit` transformation. 4. **Facet the plot** by the `day` variable to show separate plots for each day of the week. 5. **Annotate the plot** to show a legend identifying the regression line. Requirements: - The plot should use different colors to indicate different days. - Ensure the regression lines are annotated in the legend for each faceted subplot. - Set the transparency (alpha) of the scatter plot points to 0.6 for better visibility. Implementation: Write a function `create_faceted_plot()` that takes no arguments and performs the tasks described. Your function should display the final faceted plot. ```python import seaborn.objects as so from seaborn import load_dataset # Function to create faceted plot def create_faceted_plot(): tips = load_dataset(\\"tips\\") # Create initial plot p = ( so.Plot(tips, \\"total_bill\\", \\"tip\\") .add(so.Dot(alpha=0.6), so.PolyFit()) .add(so.Line(color=\\"C1\\"), so.PolyFit(), label=\\"Linear Fit\\") .facet(col=\\"day\\") .label(x=\\"Total Bill ()\\", y=\\"Tip ()\\", color=\\"Day\\") ) # Display the plot p.show() # Call the function to create the plot create_faceted_plot() ``` This function should display a faceted plot as described, fulfilling all the requirements and demonstrating your understanding of creating and manipulating visualizations using the seaborn `objects` module. Constraints: - Ensure your solution is efficient and uses the seaborn `objects` module\'s capabilities effectively. - The function should be self-contained and should not require any modifications to seaborn\'s internal functionality.","solution":"import seaborn.objects as so from seaborn import load_dataset # Function to create faceted plot def create_faceted_plot(): tips = load_dataset(\\"tips\\") # Create initial plot p = ( so.Plot(tips, \\"total_bill\\", \\"tip\\") .add(so.Dot(alpha=0.6)) .add(so.Line(color=\\"C1\\"), so.PolyFit(), label=\\"Linear Fit\\") .facet(col=\\"day\\") .label(x=\\"Total Bill ()\\", y=\\"Tip ()\\", color=\\"Day\\") ) # Display the plot p.show() # Call the function to create the plot create_faceted_plot()"},{"question":"Unicode String Manipulation and Encoding Objective Design a function that processes a given Unicode string by applying specific transformations and then encodes it using specified codecs. You must demonstrate your understanding of Unicode string manipulation, character properties, and encoding/decoding concepts. Problem Statement Implement a function `transform_and_encode_unicode` that takes the following parameters: 1. `input_str` (str): A Unicode string. 2. `transformations` (dict): A dictionary specifying character transformations. Each key is a character to be replaced, and its value is the character to replace it with. 3. `encoding` (str): The target encoding to convert the final transformed string into. The function should: 1. Apply the specified transformations to `input_str`. 2. Ensure that the resulting string handles surrogate pairs correctly. 3. Encode the transformed string using the specified `encoding`. 4. Return the encoded string. Constraints - The `input_str` can contain any valid Unicode characters. - The `transformations` dictionary will only include valid single Unicode characters as keys and values (e.g., {\'a\': \'b\', \'\': \'\'}). - The `encoding` parameter can be any standard encoding supported by Python\'s codec system (e.g., \'utf-8\', \'latin-1\', \'utf-16\'). Example ```python def transform_and_encode_unicode(input_str: str, transformations: dict, encoding: str) -> bytes: # Your implementation here # Example usage input_str = \\"Hello, ! \\" transformations = {\'H\': \'h\', \'\': \'\', \'\': \'\'} encoding = \'utf-8\' result = transform_and_encode_unicode(input_str, transformations, encoding) print(result) # Expected output: b\'hello, ! \' ``` Requirements 1. Handle the transformations efficiently. 2. Pay attention to surrogate pairs when executing transformations. 3. Use proper error handling for encoding exceptions. Performance - The function should handle strings of length up to 10^6 without significant performance degradation. Submission - Submit your function `transform_and_encode_unicode(input_str: str, transformations: dict, encoding: str) -> bytes` with appropriate comments and documentation. - Ensure your code passes the provided example and any additional test cases you design.","solution":"def transform_and_encode_unicode(input_str: str, transformations: dict, encoding: str) -> bytes: Transforms a given Unicode string by applying specified character transformations and then encodes it using the specified encoding. Parameters: - input_str (str): A Unicode string. - transformations (dict): A dictionary specifying character transformations. Each key is a character to be replaced, and its value is the character to replace it with. - encoding (str): The target encoding to convert the final transformed string into. Returns: - bytes: The encoded string after applying transformations. # Applying the transformations transformed_str = \'\'.join(transformations.get(c, c) for c in input_str) try: # Encoding the transformed string using the specified encoding encoded_str = transformed_str.encode(encoding) except (UnicodeEncodeError, LookupError) as e: # Handle encoding exceptions by raising an error raise ValueError(f\\"Encoding error: {e}\\") return encoded_str"},{"question":"# Question: Exploring Data Types with `torch.finfo` and `torch.iinfo` In this assessment, you will explore how to use `torch.finfo` and `torch.iinfo` to obtain numerical properties of different `torch.dtype` data types. Task: 1. Write a function called `get_dtype_properties` that takes in a string `dtype_str` representing a PyTorch data type. You will use this string to return the numerical properties of the given data type using either `torch.finfo` or `torch.iinfo`. 2. Your function should: * Identify whether the provided `dtype_str` corresponds to a floating point or integer data type. * Retrieve the appropriate numerical properties using either `torch.finfo` or `torch.iinfo`. * Return a dictionary with the numerical properties. Expected Input and Output: * Input: * `dtype_str`: A string representing a PyTorch data type (e.g., `\\"float32\\"`, `\\"int64\\"`). * Output: * A dictionary containing the numerical properties of the given data type. Constraints: * Assume input strings will always be valid PyTorch data types. * You are required to use `torch.finfo` for floating point data types and `torch.iinfo` for integer data types. * Your solution should handle only the data types specifically mentioned in the documentation (i.e., `float32`, `float64`, `float16`, `bfloat16`, `uint8`, `int8`, `int16`, `int32`, `int64`). Example Usage: ```python def get_dtype_properties(dtype_str): # Your implementation here # Example Usages: print(get_dtype_properties(\\"float32\\")) # Expected Output: # { # \'bits\': 32, # \'eps\': 1.1920929e-07, # \'max\': 3.4028235e+38, # \'min\': -3.4028235e+38, # \'tiny\': 1.1754944e-38, # \'resolution\': 1e-06 # } print(get_dtype_properties(\\"int16\\")) # Expected Output: # { # \'bits\': 16, # \'max\': 32767, # \'min\': -32768 # } ``` You may refer to the following documentation links to understand more about `torch.finfo` and `torch.iinfo`: * [torch.finfo Documentation](https://pytorch.org/docs/stable/generated/torch.finfo.html) * [torch.iinfo Documentation](https://pytorch.org/docs/stable/generated/torch.iinfo.html) Performance Considerations: * Your solution should be efficient in terms of its use of memory and processing. While this task does not involve large data, your function should not have unnecessary complexity.","solution":"import torch def get_dtype_properties(dtype_str): Returns numerical properties of the given PyTorch dtype as a dictionary. Handles \'float32\', \'float64\', \'float16\', \'bfloat16\', \'uint8\', \'int8\', \'int16\', \'int32\', \'int64\'. dtype_mapping = { \\"float32\\": torch.float32, \\"float64\\": torch.float64, \\"float16\\": torch.float16, \\"bfloat16\\": torch.bfloat16, \\"uint8\\": torch.uint8, \\"int8\\": torch.int8, \\"int16\\": torch.int16, \\"int32\\": torch.int32, \\"int64\\": torch.int64, } if dtype_str not in dtype_mapping: raise ValueError(f\\"Unsupported dtype: {dtype_str}\\") dtype = dtype_mapping[dtype_str] if dtype.is_floating_point: info = torch.finfo(dtype) return { \\"bits\\": info.bits, \\"eps\\": info.eps, \\"max\\": info.max, \\"min\\": info.min, \\"tiny\\": info.tiny } else: info = torch.iinfo(dtype) return { \\"bits\\": info.bits, \\"max\\": info.max, \\"min\\": info.min }"},{"question":"You are provided with the `diamonds` dataset from seaborn\'s built-in datasets. Your task is to create a customized plot that demonstrates your understanding of seaborn\'s plotting capabilities. Requirements 1. Load the `diamonds` dataset and initialize a `Plot` object with `price` as the x-axis. 2. Scale the x-axis to be logarithmic. 3. Create a histogram using bars, customized as follows: - The histograms should be stacked based on the `cut` variable. - The bars representing different `cut` categories should not overlap. - Set the edge width of the bars to zero, and the alpha (transparency) to represent the `clarity` variable. - Use a `binwidth` of `500` for the histogram. - Filter the dataset such that only diamonds with `cut` equal to \'Ideal\' are plotted. Expected Output - A seaborn plot (preferably rendered using matplotlib) with the following characteristics: - The x-axis scaled logarithmically. - Stacked bars representing different `cut` categories. - No overlapping of bars. - Edge width of bars set to zero. - Bars with varying transparency based on the `clarity` variable. - A `binwidth` of `500` for the histogram. - Data filtered to include only \'Ideal\' cut diamonds. Constraints - You must use seaborn\'s object-oriented interface for plotting (`seaborn.objects`). - Do not use the default seaborn API (`sns.histplot`, `sns.barplot`, etc.). Performance Requirements - Ensure the plot is generated efficiently and does not take an excessive amount of time to render. # Example Code to Get You Started ```python import seaborn.objects as so from seaborn import load_dataset # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Initialize the Plot p = so.Plot(diamonds[diamonds[\'cut\'] == \'Ideal\'], \\"price\\") # Scale the x-axis logarithmically p = p.scale(x=\\"log\\") # Add customized bars to the plot p.add( so.Bars(edgewidth=0, alpha=\\"clarity\\"), # Customize bar properties so.Hist(binwidth=500, binrange=(diamonds[\'price\'].min(), diamonds[\'price\'].max())), # Specify histogram properties so.Stack(), # Stack the bars to avoid overlap color=\\"cut\\" # Use cut to differentiate bars ) # Render the plot p.show() ``` Good luck!","solution":"import seaborn.objects as so import seaborn as sns import matplotlib.pyplot as plt def create_custom_histogram(): # Load the diamonds dataset diamonds = sns.load_dataset(\'diamonds\') # Filter the dataset for \'Ideal\' cut diamonds ideal_diamonds = diamonds[diamonds[\'cut\'] == \'Ideal\'] # Initialize the Plot p = so.Plot(ideal_diamonds, \\"price\\") # Scale the x-axis logarithmically p = p.scale(x=\\"log\\") # Add customized bars to the plot p.add( so.Bars(edgewidth=0, alpha=\'clarity\'), # Customize bar properties so.Hist(binwidth=500), # Specify histogram properties so.Stack(), # Stack the bars to avoid overlap color=\\"cut\\" # Use cut to differentiate bars ) # Render the plot p.show() # Call the function to generate the plot create_custom_histogram()"},{"question":"You have been provided with the `penguins` dataset, which contains information about penguins from three different islands. Your task is to create a detailed visualization using `seaborn.objects` that compares the distribution of flipper lengths across different islands, segmented by the sex of the penguins. # Requirements 1. Load the `penguins` dataset using `seaborn`. 2. Create a bar chart showing the proportions of penguins with different `flipper_length_mm` for each island. 3. The chart should be segmented by the sex of the penguins, represented as different colors. 4. The data should be normalized within each island, so that the proportions sum to 1 for each island. 5. Use `so.Plot()` and appropriate marks (`Bars`, `Hist`, `Stack`) to achieve the required visualizations. # Expected Input None. The dataset is predefined (`penguins`). # Expected Output A seaborn plot object displaying the required visualizations. # Constraints - Use `seaborn.objects`. - Ensure that the visualizations are clearly annotated. # Example Output The example output should display a stacked bar chart with segmented proportions of flipper lengths, colored by sex, and normalized within each island. ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the plot p = so.Plot(penguins, x=\\"flipper_length_mm\\", color=\\"sex\\") p = p.facet(col=\\"island\\") p.add(so.Bars(), so.Hist(stat=\\"proportion\\", common_norm=False), so.Stack()) # Display the plot p.show() ```","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def plot_penguin_flipper_lengths(): Creates a detailed visualization comparing the distribution of flipper lengths across different islands, segmented by the sex of the penguins. # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Ensure there are no missing values in important columns penguins = penguins.dropna(subset=[\'island\', \'flipper_length_mm\', \'sex\']) # Create the plot p = so.Plot(penguins, x=\\"flipper_length_mm\\", color=\\"sex\\") p = p.facet(col=\\"island\\") p.add(so.Bars(), so.Hist(stat=\\"proportion\\", common_norm=False), so.Stack()) # Display the plot p.show()"},{"question":"# Problem Description To assess your understanding of Python packaging and the distutils library, you are required to write a function that reads and validates metadata from a `PKG-INFO` file. In particular, your function should check if the metadata includes the required fields: `name`, `version`, `author`, and `author_email`. # Task Implement the function `validate_metadata(pkg_info_path: str) -> dict` that reads a `PKG-INFO` file and validates its metadata. The function should return a dictionary with the following keys and values: - `\\"name\\"`: The name of the package (str) - `\\"version\\"`: The version of the package (str) - `\\"author\\"`: The author of the package (str) - `\\"author_email\\"`: The author\'s email (str) - `\\"is_valid\\"`: A boolean indicating whether all required fields are present and non-empty (bool) If any required field is missing or empty, the corresponding value in the dictionary should be an empty string, and `\\"is_valid\\"` should be `False`. Otherwise, `\\"is_valid\\"` should be `True`. # Input - `pkg_info_path` (str): The path to the `PKG-INFO` file. # Output - A dictionary with keys `\\"name\\"`, `\\"version\\"`, `\\"author\\"`, `\\"author_email\\"`, and `\\"is_valid\\"`. # Example ```python # Assume \'distribute-0.6.8-py2.7.egg-info\' contains the following fields: # Name: distribute # Version: 0.6.8 # Author: Tarek Ziad # Author-email: tarek@ziade.org pkg_info_path = \'distribute-0.6.8-py2.7.egg-info\' result = validate_metadata(pkg_info_path) print(result) # Output: { # \\"name\\": \\"distribute\\", # \\"version\\": \\"0.6.8\\", # \\"author\\": \\"Tarek Ziad\\", # \\"author_email\\": \\"tarek@ziade.org\\", # \\"is_valid\\": True # } ``` # Constraints - The `PKG-INFO` file will always exist at the given path, but may be missing some required fields. - Use the `distutils.dist.DistributionMetadata` class to read the metadata. # Notes This question will test your ability to: - Read and understand metadata from a file. - Use the `distutils` library appropriately. - Implement error handling to check for missing or empty required fields.","solution":"from distutils.dist import DistributionMetadata def validate_metadata(pkg_info_path: str) -> dict: Reads and validates metadata from a PKG-INFO file. Parameters: pkg_info_path (str): The path to the PKG-INFO file. Returns: dict: A dictionary containing metadata fields and validity status. # Initialize the DistributionMetadata object metadata = DistributionMetadata() # Read the PKG-INFO file metadata.read_pkg_file(open(pkg_info_path)) # Extract required fields and check if they are present and non-empty required_fields = [\\"name\\", \\"version\\", \\"author\\", \\"author_email\\"] metadata_dict = {field: getattr(metadata, field, \\"\\") or \\"\\" for field in required_fields} # Check if all required fields are non-empty is_valid = all(metadata_dict[field] for field in required_fields) metadata_dict[\\"is_valid\\"] = is_valid return metadata_dict"},{"question":"# SQLite Custom Aggregation Function and Data Insertion In this assessment, you are required to implement and use a custom aggregation function in SQLite using the `sqlite3` module in Python. Task 1. **Database Setup**: - Create an in-memory SQLite database and a table named `sales` with columns: `product_name` (TEXT), `sale_date` (TEXT), and `amount` (REAL). - Insert the following data into the `sales` table: | product_name | sale_date | amount | |---------------|------------|--------| | Product A | 2023-01-01 | 100.0 | | Product B | 2023-01-02 | 150.0 | | Product A | 2023-01-03 | 200.0 | | Product A | 2023-01-04 | 250.0 | | Product B | 2023-01-05 | 300.0 | | Product C | 2023-01-06 | 400.0 | | Product C | 2023-01-07 | 350.0 | 2. **Custom Aggregation Function**: - Implement a custom aggregation function named `total_sales` that sums up the `amount` of all rows for a given `product_name`. - Use the `create_aggregate` method to register this custom aggregation function with SQLite. 3. **Query**: - Write a SQL query using the `total_sales` aggregation function to get the total sales amount for each product. - Order the results by `product_name`. 4. **Output**: - Fetch the results and print them in the following format: ``` Product: <product_name>, Total Sales: <total_sales> ``` Example Output ``` Product: Product A, Total Sales: 550.0 Product: Product B, Total Sales: 450.0 Product: Product C, Total Sales: 750.0 ``` Python Implementation Implement the Python code to accomplish the above tasks. Your implementation should follow best practices for database operations, including proper handling of database connections and cursors. ```python import sqlite3 # Task 1: Create the SQLite database and insert data def setup_database(): connection = sqlite3.connect(\\":memory:\\") cursor = connection.cursor() # Create the sales table cursor.execute( CREATE TABLE sales ( product_name TEXT, sale_date TEXT, amount REAL ) ) # Insert data data = [ (\\"Product A\\", \\"2023-01-01\\", 100.0), (\\"Product B\\", \\"2023-01-02\\", 150.0), (\\"Product A\\", \\"2023-01-03\\", 200.0), (\\"Product A\\", \\"2023-01-04\\", 250.0), (\\"Product B\\", \\"2023-01-05\\", 300.0), (\\"Product C\\", \\"2023-01-06\\", 400.0), (\\"Product C\\", \\"2023-01-07\\", 350.0), ] cursor.executemany(\\"INSERT INTO sales VALUES (?, ?, ?)\\", data) connection.commit() return connection # Task 2: Implement the custom aggregation function class TotalSales: def __init__(self): self.total = 0 def step(self, value): if value is not None: self.total += value def finalize(self): return self.total # Task 3: Register the custom aggregation function and perform the query def calculate_total_sales(connection): connection.create_aggregate(\\"total_sales\\", 1, TotalSales) cursor = connection.cursor() cursor.execute( SELECT product_name, total_sales(amount) as total_sales FROM sales GROUP BY product_name ORDER BY product_name ) results = cursor.fetchall() return results # Task 4: Output the results def display_results(results): for row in results: print(f\\"Product: {row[0]}, Total Sales: {row[1]}\\") # Main function to run all tasks def main(): connection = setup_database() try: results = calculate_total_sales(connection) display_results(results) finally: connection.close() if __name__ == \\"__main__\\": main() ``` Constraints: - Do not modify existing SQLite modules or functions. - Ensure proper handling and closing of the database connection. Your task is to implement the complete function as described in the example to ensure it runs as expected.","solution":"import sqlite3 # Task 1: Create the SQLite database and insert data def setup_database(): connection = sqlite3.connect(\\":memory:\\") cursor = connection.cursor() # Create the sales table cursor.execute( CREATE TABLE sales ( product_name TEXT, sale_date TEXT, amount REAL ) ) # Insert data data = [ (\\"Product A\\", \\"2023-01-01\\", 100.0), (\\"Product B\\", \\"2023-01-02\\", 150.0), (\\"Product A\\", \\"2023-01-03\\", 200.0), (\\"Product A\\", \\"2023-01-04\\", 250.0), (\\"Product B\\", \\"2023-01-05\\", 300.0), (\\"Product C\\", \\"2023-01-06\\", 400.0), (\\"Product C\\", \\"2023-01-07\\", 350.0), ] cursor.executemany(\\"INSERT INTO sales VALUES (?, ?, ?)\\", data) connection.commit() return connection # Task 2: Implement the custom aggregation function class TotalSales: def __init__(self): self.total = 0 def step(self, value): if value is not None: self.total += value def finalize(self): return self.total # Task 3: Register the custom aggregation function and perform the query def calculate_total_sales(connection): connection.create_aggregate(\\"total_sales\\", 1, TotalSales) cursor = connection.cursor() cursor.execute( SELECT product_name, total_sales(amount) as total_sales FROM sales GROUP BY product_name ORDER BY product_name ) results = cursor.fetchall() return results # Task 4: Output the results def display_results(results): for row in results: print(f\\"Product: {row[0]}, Total Sales: {row[1]}\\") # Main function to run all tasks def main(): connection = setup_database() try: results = calculate_total_sales(connection) display_results(results) finally: connection.close() if __name__ == \\"__main__\\": main()"},{"question":"# **Problem: Tensor Shape Compatibility Checker** You are required to implement a function that checks if two tensors are compatible for matrix multiplication. Matrix multiplication is only possible if the inner dimensions of the matrices are the same (i.e., for matrices A and B, if A is of shape (m, n) and B is of shape (n, p), then they can be multiplied to form a matrix of shape (m, p)). Implement the function `can_multiply(tensor1_size, tensor2_size)` which takes the sizes of two tensors as input and returns `True` if they can be multiplied, `False` otherwise. # **Function Signature** ```python def can_multiply(tensor1_size: torch.Size, tensor2_size: torch.Size) -> bool: ``` # **Input** - `tensor1_size`: a `torch.Size` object representing the dimensions of the first tensor. - `tensor2_size`: a `torch.Size` object representing the dimensions of the second tensor. # **Output** - `True` if the two tensors can be multiplied according to matrix multiplication rules, `False` otherwise. # **Constraints** 1. The input sizes will always have 2 dimensions (as they represent matrices). 2. i.e., tensor1_size and tensor2_size will be of the form (m, n) and (n, p) respectively. # **Example** ```python >>> tensor1_size = torch.Size([2, 3]) >>> tensor2_size = torch.Size([3, 4]) >>> print(can_multiply(tensor1_size, tensor2_size)) True >>> tensor1_size = torch.Size([2, 3]) >>> tensor2_size = torch.Size([4, 5]) >>> print(can_multiply(tensor1_size, tensor2_size)) False ``` # **Explanation** - In the first example, the tensors of sizes (2, 3) and (3, 4) can be multiplied because the inner dimensions (3) match. - In the second example, the tensors of sizes (2, 3) and (4, 5) cannot be multiplied because the inner dimensions do not match (3 and 4). # **Note** You can assume that the inputs will always be valid `torch.Size` objects that represent matrices.","solution":"import torch def can_multiply(tensor1_size: torch.Size, tensor2_size: torch.Size) -> bool: Check if two tensors can be multiplied based on their dimensions. Args: tensor1_size (torch.Size): Size of the first tensor (m, n) tensor2_size (torch.Size): Size of the second tensor (n, p) Returns: bool: True if the tensors can be multiplied, False otherwise. # For matrix multiplication, the inner dimensions must match return tensor1_size[1] == tensor2_size[0]"},{"question":"# Custom Python Object with Specific Behaviors **Objective:** You are required to design a new Python class that mimics the behaviors of custom objects as defined in low-level `PyTypeObject` C-struct style, focusing on key functions such as object creation, attribute management, string representation, and comparison. **Instructions:** 1. **Class Definition:** Define a class named `CustomObject` in Python, which will implement specific methods to mimic behavior similar to the methods in `PyTypeObject`. 2. **Initialization and Finalization:** - Implement the `__init__` method to initialize the object with an attribute `data`, which is an integer. - Implement a method to cleanup (free) any resources when the object is deleted. 3. **Attribute Management:** - Implement getter and setter methods for the `data` attribute. The getter should return the value of `data`, and the setter should ensure the value being set is always an integer. 4. **String Representation:** - Implement the `__repr__` and `__str__` methods to provide string representations of the object. 5. **Rich Comparisons:** - Implement rich comparison methods (`__eq__`, `__ne__`, `__lt__`, `__le__`, `__gt__`, `__ge__`) to allow for comparing objects of `CustomObject` based on their `data` attribute. **Constraints:** - The `data` attribute must always be an integer and should default to 0 if not set during initialization. - Comparisons between `CustomObject` instances should be based solely on the value of the `data` attribute. - Ensure that the string representations are informative and clear. **Example:** ```python class CustomObject: def __init__(self, data=0): self.data = data def __del__(self): pass # Code for cleanup resources, if needed def get_data(self): return self.data def set_data(self, value): if isinstance(value, int): self.data = value else: raise ValueError(\\"data must be an integer\\") def __repr__(self): return f\\"CustomObject(data={self.data})\\" def __str__(self): return f\\"Custom Object with data: {self.data}\\" # Other comparison magic methods (__eq__, __ne__, etc.) # Example usage: obj1 = CustomObject(10) obj2 = CustomObject(20) print(obj1) # Custom Object with data: 10 print(repr(obj2)) # CustomObject(data=20) print(obj1 < obj2) # True ``` # Submission: Submit your implementation of the `CustomObject` class. Ensure to include any necessary imports and test cases to demonstrate its functionality. Your code should be well-documented and error-free.","solution":"class CustomObject: def __init__(self, data=0): if not isinstance(data, int): raise ValueError(\\"data must be an integer\\") self.data = data def __del__(self): pass # Code for cleanup resources, if needed def get_data(self): return self.data def set_data(self, value): if isinstance(value, int): self.data = value else: raise ValueError(\\"data must be an integer\\") def __repr__(self): return f\\"CustomObject(data={self.data})\\" def __str__(self): return f\\"Custom Object with data: {self.data}\\" def __eq__(self, other): if isinstance(other, CustomObject): return self.data == other.data return NotImplemented def __ne__(self, other): if isinstance(other, CustomObject): return self.data != other.data return NotImplemented def __lt__(self, other): if isinstance(other, CustomObject): return self.data < other.data return NotImplemented def __le__(self, other): if isinstance(other, CustomObject): return self.data <= other.data return NotImplemented def __gt__(self, other): if isinstance(other, CustomObject): return self.data > other.data return NotImplemented def __ge__(self, other): if isinstance(other, CustomObject): return self.data >= other.data return NotImplemented"},{"question":"# Seaborn Advanced Visualization Task You are given a dataset \\"tips\\" which records information about customers\' tips in a restaurant. The dataset includes the following columns: - `total_bill`: The total bill in dollars. - `tip`: The tip amount in dollars. - `sex`: Gender of the person paying the bill (Male/Female). - `smoker`: Whether the customer is a smoker (Yes/No). - `day`: The day of the week (Thur/Fri/Sat/Sun). - `time`: Time of the day (Lunch/Dinner). - `size`: The size of the party. # Task Your goal is to create a comprehensive visualization using `sns.stripplot` and `sns.catplot` from the seaborn library that provides a detailed analysis of the tips dataset. Steps: 1. **Load the dataset `tips` using seaborn\'s `load_dataset` function.** 2. **Create a strip plot to visualize the distribution of `total_bill` for each `day` of the week:** - Use different colors to distinguish between `sex`. - Ensure that the points for different `sex` are split for better visual clarity. 3. **Refine the above strip plot by removing the jittering effect and changing the marker type to \'D\' with a smaller size of 10.** 4. **Create a faceted strip plot using `sns.catplot` to show the `total_bill` distribution for different `day` values, separated by `time` of the day:** - Use different colors to distinguish between `sex`. - Arrange the facets in columns with an aspect ratio of 0.6 for better visibility. # Constraints: - Your code should be efficient and should not take an unreasonably long time to execute. - Use relevant seaborn functions and parameters to achieve the visualizations as described. - Ensure visual clarity and make aesthetic adjustments as necessary. # Expected Output: - The notebook should contain the code and the resulting plots for each of the steps described. The solution should demonstrate your ability to use seaborn\'s visualization capabilities effectively and customize plots for meaningful analysis. ```python # Your code here import seaborn as sns # Step 1: Load the dataset tips = sns.load_dataset(\\"tips\\") # Step 2: Create a basic strip plot sns.stripplot(data=tips, x=\'total_bill\', y=\'day\', hue=\'sex\', dodge=True) # Step 3: Refine plot (remove jitter, change marker type, and size) sns.stripplot(data=tips, x=\'total_bill\', y=\'day\', hue=\'sex\', dodge=True, jitter=False, marker=\'D\', size=10) # Step 4: Create a faceted strip plot sns.catplot(data=tips, x=\'time\', y=\'total_bill\', hue=\'sex\', col=\'day\', kind=\'strip\', aspect=0.6) ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_visualizations(): # Step 1: Load the dataset tips = sns.load_dataset(\\"tips\\") # Step 2: Create a basic strip plot plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\'total_bill\', y=\'day\', hue=\'sex\', dodge=True) plt.title(\'Basic Strip Plot: Total Bill Distribution by Day and Sex\') plt.show() # Step 3: Refine plot (remove jitter, change marker type, and size) plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\'total_bill\', y=\'day\', hue=\'sex\', dodge=True, jitter=False, marker=\'D\', size=10) plt.title(\'Refined Strip Plot: Total Bill Distribution by Day and Sex\') plt.show() # Step 4: Create a faceted strip plot g = sns.catplot(data=tips, x=\'time\', y=\'total_bill\', hue=\'sex\', col=\'day\', kind=\'strip\', aspect=0.6) g.fig.suptitle(\'Faceted Strip Plot: Total Bill Distribution by Time, Day, and Sex\') plt.show()"},{"question":"Objective The objective of this assessment is to evaluate your ability to integrate multiple Python standard library modules to solve a practical problem. You will be required to manipulate files, handle command-line arguments, process string patterns with regular expressions, and measure the performance of your implemented solution. Problem Statement You are given a directory that contains several text files. Your task is to write a Python script that performs the following operations: 1. **List all text files** in the given directory (use the `glob` module). 2. **Read each text file**, and for each file: * Count the number of lines. * Extract all the words that start with a vowel (A, E, I, O, U) using regular expressions (use the `re` module). 3. **Output the results** to the console: * For each file, print the filename, the number of lines, and the list of words starting with a vowel. 4. **Calculate and display the execution time** of the entire script using the `timeit` module. Input and Output - **Input Format**: - The script should accept a single command-line argument specifying the directory path containing the text files. - **Output Format**: - The output should be printed to the console in the following format: ``` Filename: <filename> Number of lines: <number_of_lines> Words starting with a vowel: <list_of_words> ``` Constraints - Assume that the directory contains only text files. - You do not need to handle nested directories. Performance Requirements - The script should efficiently read and process each file using appropriate methods and modules from the Python standard library. Example Suppose the script is called as follows: ``` python script.py /path/to/directory ``` The expected output format is: ``` Filename: file1.txt Number of lines: 15 Words starting with a vowel: [\'apple\', \'orange\', \'umbrella\'] Filename: file2.txt Number of lines: 10 Words starting with a vowel: [\'elephant\', \'inbox\', \'octopus\'] Execution time: 0.005 seconds ``` Implementation Details Here is a template to get you started: ```python import glob import re import sys import timeit def main(directory): # List all .txt files in the given directory files = glob.glob(f\\"{directory}/*.txt\\") for file in files: with open(file, \'r\') as f: lines = f.readlines() num_lines = len(lines) text = \' \'.join(lines) words = re.findall(r\'b[aeiouAEIOU][a-zA-Z]*\', text) print(f\\"Filename: {file}\\") print(f\\"Number of lines: {num_lines}\\") print(f\\"Words starting with a vowel: {words}\\") print() if __name__ == \\"__main__\\": if len(sys.argv) != 2: print(\\"Usage: python script.py <directory_path>\\") sys.exit(1) directory = sys.argv[1] execution_time = timeit.timeit(\'main(directory)\', number=1, globals=globals()) print(f\\"Execution time: {execution_time:.6f} seconds\\") ``` Complete this script by filling in the missing parts and ensuring it meets all the requirements specified.","solution":"import glob import re import sys import timeit import os def main(directory): # List all .txt files in the given directory files = glob.glob(os.path.join(directory, \\"*.txt\\")) for file in files: with open(file, \'r\') as f: lines = f.readlines() num_lines = len(lines) text = \' \'.join(lines) words = re.findall(r\'b[aeiouAEIOU][a-zA-Z]*\', text) print(f\\"Filename: {os.path.basename(file)}\\") print(f\\"Number of lines: {num_lines}\\") print(f\\"Words starting with a vowel: {words}\\") print() if __name__ == \\"__main__\\": if len(sys.argv) != 2: print(\\"Usage: python script.py <directory_path>\\") sys.exit(1) directory = sys.argv[1] execution_time = timeit.timeit(\'main(directory)\', number=1, globals=globals()) print(f\\"Execution time: {execution_time:.6f} seconds\\")"},{"question":"# Transition from `imp` Module to `importlib` The `imp` module, which was used to implement the `import` statement and related functions, has been deprecated since Python 3.4 in favor of `importlib`. Your task is to write a Python function that mimics the behavior of `imp.find_module` and `imp.load_module` using the `importlib` module. The function should: 1. Attempt to find and load a module by name. 2. Return the module object if found and loaded successfully. 3. Raise an `ImportError` if the module cannot be found or loaded. **Function Signature:** ```python def import_module(name): pass ``` **Input:** - `name` (str): The name of the module to be imported. This can be a top-level module or a submodule (e.g., `\'json\'` or `\'os.path\'`). **Output:** - If successful, the function returns the module object. - If unsuccessful, it raises an `ImportError`. **Constraints:** - You may not use the deprecated `imp` module. - You must handle both top-level modules and submodules correctly. **Example Usage:** ```python # Successful import module = import_module(\'json\') print(module) # <module \'json\' from \'/.../json/__init__.py\'> # ImportError case try: module = import_module(\'non_existent_module\') except ImportError as e: print(e) # No module named \'non_existent_module\' ``` **Hints:** - You might find the functions `importlib.util.find_spec()`, `importlib.util.spec_from_file_location()`, and `importlib.util.module_from_spec()` useful. - The `importlib.import_module()` function can also be used directly. Implement the `import_module` function described above.","solution":"import importlib def import_module(name): Attempt to find and load a module by name using importlib. Returns the module object if found and loaded successfully. Raises an ImportError if the module cannot be found or loaded. try: module = importlib.import_module(name) return module except ImportError: raise ImportError(f\\"No module named \'{name}\'\\")"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding and mastery of Seaborns `kdeplot` function. # Problem Statement You are given the famous \\"Iris\\" dataset and the \\"Tips\\" dataset. Using these datasets, you need to visualize various distributions and relationships within the data. Your task is to implement the following: 1. **Univariate Distribution:** - Load the \\"Tips\\" dataset and plot the kernel density estimation for the `total_bill` column along the x-axis. 2. **Conditional Distributions with Hue Mapping:** - Using the same \\"Tips\\" dataset, plot the KDE for the `total_bill` column, but differentiate the distributions based on the `time` column (`Lunch` vs `Dinner`). Use an appropriate hue mapping. 3. **Bivariate Distribution:** - Load the \\"Iris\\" dataset and plot a bivariate KDE for the `sepal_length` and `sepal_width` columns. 4. **Bivariate Distribution with Hue Mapping:** - Using the \\"Iris\\" dataset, plot a bivariate KDE for the `sepal_length` and `sepal_width` columns, and differentiate the distributions based on the `species` column. Use an appropriate hue mapping and fill the contours. 5. **Log Scaling:** - Load the \\"Diamonds\\" dataset and plot the kernel density estimation for the `price` column with the x-axis on a log scale. # Requirements - For each plot, customize the appearance by setting the palette to `\\"viridis\\"`, and adjust the transparency where needed. - Ensure that all plots are appropriately labeled with titles, x and y-axis labels, and legends (if applicable). # Input and Output Formats - **Input:** - None (The function should directly load the datasets from Seaborn\'s built-in datasets) - **Output:** - Matplotlib/Seaborn plots as specified above. # Implementation Sketch ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_distributions(): # 1. Univariate Distribution - Tips dataset tips = sns.load_dataset(\\"tips\\") plt.figure(figsize=(10, 6)) sns.kdeplot(data=tips, x=\\"total_bill\\", palette=\\"viridis\\") plt.title(\\"KDE Plot for Total Bill (Tips Dataset)\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Density\\") plt.show() # 2. Conditional Distributions with Hue Mapping - Tips dataset plt.figure(figsize=(10, 6)) sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", palette=\\"viridis\\") plt.title(\\"KDE Plot for Total Bill by Time (Tips Dataset)\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Density\\") plt.legend(title=\\"Time\\") plt.show() # 3. Bivariate Distribution - Iris dataset iris = sns.load_dataset(\\"iris\\") plt.figure(figsize=(10, 6)) sns.kdeplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", palette=\\"viridis\\") plt.title(\\"Bivariate KDE Plot for Sepal Length and Width (Iris Dataset)\\") plt.xlabel(\\"Sepal Length\\") plt.ylabel(\\"Sepal Width\\") plt.show() # 4. Bivariate Distribution with Hue Mapping - Iris dataset plt.figure(figsize=(10, 6)) sns.kdeplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", hue=\\"species\\", fill=True, palette=\\"viridis\\") plt.title(\\"Bivariate KDE Plot for Sepal Length and Width by Species (Iris Dataset)\\") plt.xlabel(\\"Sepal Length\\") plt.ylabel(\\"Sepal Width\\") plt.legend(title=\\"Species\\") plt.show() # 5. Log Scaling - Diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") plt.figure(figsize=(10, 6)) sns.kdeplot(data=diamonds, x=\\"price\\", log_scale=True, palette=\\"viridis\\") plt.title(\\"KDE Plot for Price with Log Scale (Diamonds Dataset)\\") plt.xlabel(\\"Price (Log Scale)\\") plt.ylabel(\\"Density\\") plt.show() # Run the function to visualize the distributions visualize_distributions() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_distributions(): # 1. Univariate Distribution - Tips dataset tips = sns.load_dataset(\\"tips\\") plt.figure(figsize=(10, 6)) sns.kdeplot(data=tips, x=\\"total_bill\\", palette=\\"viridis\\") plt.title(\\"KDE Plot for Total Bill (Tips Dataset)\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Density\\") plt.show() # 2. Conditional Distributions with Hue Mapping - Tips dataset plt.figure(figsize=(10, 6)) sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", palette=\\"viridis\\") plt.title(\\"KDE Plot for Total Bill by Time (Tips Dataset)\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Density\\") plt.legend(title=\\"Time\\") plt.show() # 3. Bivariate Distribution - Iris dataset iris = sns.load_dataset(\\"iris\\") plt.figure(figsize=(10, 6)) sns.kdeplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", palette=\\"viridis\\") plt.title(\\"Bivariate KDE Plot for Sepal Length and Width (Iris Dataset)\\") plt.xlabel(\\"Sepal Length\\") plt.ylabel(\\"Sepal Width\\") plt.show() # 4. Bivariate Distribution with Hue Mapping - Iris dataset plt.figure(figsize=(10, 6)) sns.kdeplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", hue=\\"species\\", fill=True, palette=\\"viridis\\") plt.title(\\"Bivariate KDE Plot for Sepal Length and Width by Species (Iris Dataset)\\") plt.xlabel(\\"Sepal Length\\") plt.ylabel(\\"Sepal Width\\") plt.legend(title=\\"Species\\") plt.show() # 5. Log Scaling - Diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") plt.figure(figsize=(10, 6)) sns.kdeplot(data=diamonds, x=\\"price\\", log_scale=True, palette=\\"viridis\\") plt.title(\\"KDE Plot for Price with Log Scale (Diamonds Dataset)\\") plt.xlabel(\\"Price (Log Scale)\\") plt.ylabel(\\"Density\\") plt.show()"},{"question":"Objective: Design a function in Python that performs operations on a list of student scores and handles potential errors gracefully. The function will use various Python statements and constructs covered in the provided documentation. Problem Statement: You are required to write a function named `analyze_scores` that: 1. Takes in a list of dictionaries, where each dictionary represents a student\'s record with keys `name` (string) and `score` (integer). 2. Computes the average score. 3. Identifies the highest and lowest scores. 4. Raises an exception if any of the scores are negative, indicating invalid data. 5. Uses augmented assignment for computing the sum of scores. 6. Annotate the function so that it specifies the expected types of input and output. Function Signature: ```python def analyze_scores(records: list[dict[str, int]]) -> dict[str, float]: ``` Input: - `records`: A list of dictionaries, where each dictionary has: - `name`: A string representing the student\'s name. - `score`: An integer representing the student\'s score. Output: - A dictionary with the following keys: - `average`: The average score (float). - `highest`: The highest score (float). - `lowest`: The lowest score (float). Constraints: - The length of the `records` list will be between 1 and 100. - Each `score` value in the `records` list will be between -100 and 100. Example: Input: ```python records = [ {\\"name\\": \\"Alice\\", \\"score\\": 88}, {\\"name\\": \\"Bob\\", \\"score\\": 92}, {\\"name\\": \\"Charlie\\", \\"score\\": 75}, {\\"name\\": \\"Dave\\", \\"score\\": 59} ] ``` Output: ```python { \\"average\\": 78.5, \\"highest\\": 92.0, \\"lowest\\": 59.0 } ``` Guidelines: - Implement error handling using `try`, `raise`, and related statements for any invalid scores. - Use augmented assignment (`+=`) to sum up the scores. - Utilize type annotations in the function signature. - Ensure the function is well-documented and follows clean coding practices. Notes: - You are allowed to use the built-in `sum()` function for calculating the total score. - Use Python\'s standard exception (`ValueError`) to handle negative score detection.","solution":"def analyze_scores(records: list[dict[str, int]]) -> dict[str, float]: Analyzes a list of student records to compute the average, highest, and lowest scores. Args: - records (list): A list of dictionaries, where each dictionary contains `name` (str) and `score` (int). Returns: - dict: A dictionary containing the average, highest, and lowest scores. if not records: raise ValueError(\\"The records list is empty.\\") total_score = 0 highest_score = float(\'-inf\') lowest_score = float(\'inf\') for record in records: score = record.get(\'score\') if score < 0: raise ValueError(\\"Negative score detected.\\") total_score += score if score > highest_score: highest_score = score if score < lowest_score: lowest_score = score average_score = total_score / len(records) return { \\"average\\": average_score, \\"highest\\": float(highest_score), \\"lowest\\": float(lowest_score) }"},{"question":"- Using Seaborn Color Palettes Context You are given a dataset containing sales figures for different products across several regions. Your goal is to create a series of visualizations that highlight the data using different color palettes available in seaborn. Additionally, you need to demonstrate the ability to extract and manipulate the color values programmatically. Task 1. **Data Preparation**: - Create a synthetic dataset using pandas with the following columns: - `Product` (categorical): Five different products labelled \\"Product_A\\", \\"Product_B\\", \\"Product_C\\", \\"Product_D\\", \\"Product_E\\". - `Region` (categorical): Three different regions labelled \\"North\\", \\"South\\", \\"East\\". - `Sales` (numerical): Random integer values between 100 and 500. 2. **Visualization**: - Create three bar plots of sales figures for each product across different regions using three different seaborn color palettes: - \\"Set2\\" for the first plot. - \\"HUSL\\" for the second plot. - A custom cubehelix palette (e.g., `cubehelix_palette(start=1, rot=-0.75)`) for the third plot. 3. **Hex Codes Extraction**: - Print the hex codes of the colors used in the \\"Set2\\" palette. 4. **Context Manager**: - Temporarily set the default palette to \\"dark:#5A9_r\\" and create a fourth plot showing the same data. Requirements - Use seaborn\'s `color_palette` function to generate the required palettes. - Use seaborn\'s `barplot` function for plotting. - Each plot should have appropriate labels, titles, and legends. - Ensure that the custom cubehelix palette is visibly different from the default palette. - Code should be well-organized, and comments should be added to explain key steps. Input and Output - **Input**: No input from the user. The script should generate the dataset internally. - **Output**: - Four visualizations saved as PNG files (`plot_set2.png`, `plot_husl.png`, `plot_cubehelix.png`, `plot_dark5A9r.png`). - Hex codes of the \\"Set2\\" palette printed to the console. Constraints - Ensure seaborn, pandas, and matplotlib are properly imported and used. - Generate reproducible random numbers for the Sales column by setting a random seed. Example Code Structure ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt import numpy as np # Set the random seed for reproducibility np.random.seed(42) # Step 1: Data Preparation # Create the synthetic dataset data = { \'Product\': [\'Product_A\', \'Product_B\', \'Product_C\', \'Product_D\', \'Product_E\'] * 3, \'Region\': [\'North\'] * 5 + [\'South\'] * 5 + [\'East\'] * 5, \'Sales\': np.random.randint(100, 501, 15) } df = pd.DataFrame(data) # Step 2: Plotting with \\"Set2\\" palette plt.figure(figsize=(10, 6)) sns.barplot(x=\'Product\', y=\'Sales\', hue=\'Region\', data=df, palette=\'Set2\') plt.title(\'Sales by Product and Region (Set2 Palette)\') plt.savefig(\'plot_set2.png\') plt.show() # Step 3: Plotting with \\"HUSL\\" palette plt.figure(figsize=(10, 6)) sns.barplot(x=\'Product\', y=\'Sales\', hue=\'Region\', data=df, palette=sns.color_palette(\\"husl\\", 9)) plt.title(\'Sales by Product and Region (HUSL Palette)\') plt.savefig(\'plot_husl.png\') plt.show() # Step 4: Plotting with custom cubehelix palette plt.figure(figsize=(10, 6)) cubehelix = sns.cubehelix_palette(start=1, rot=-0.75) sns.barplot(x=\'Product\', y=\'Sales\', hue=\'Region\', data=df, palette=cubehelix) plt.title(\'Sales by Product and Region (Custom Cubehelix Palette)\') plt.savefig(\'plot_cubehelix.png\') plt.show() # Step 5: Extract and print hex codes from \\"Set2\\" palette print(sns.color_palette(\\"Set2\\").as_hex()) # Step 6: Context management with \\"dark:#5A9_r\\" palette with sns.color_palette(\\"dark:#5A9_r\\"): plt.figure(figsize=(10, 6)) sns.barplot(x=\'Product\', y=\'Sales\', hue=\'Region\', data=df) plt.title(\'Sales by Product and Region (Temporary dark:#5A9_r Palette)\') plt.savefig(\'plot_dark5A9r.png\') plt.show() ```","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt import numpy as np # Constants PRODUCTS = [\'Product_A\', \'Product_B\', \'Product_C\', \'Product_D\', \'Product_E\'] REGIONS = [\'North\', \'South\', \'East\'] N_SAMPLES = len(PRODUCTS) * len(REGIONS) # Set the random seed for reproducibility np.random.seed(42) # Step 1: Data Preparation # Create the synthetic dataset data = { \'Product\': np.tile(PRODUCTS, len(REGIONS)), \'Region\': np.repeat(REGIONS, len(PRODUCTS)), \'Sales\': np.random.randint(100, 501, N_SAMPLES) } df = pd.DataFrame(data) # Step 2: Plotting with \\"Set2\\" palette plt.figure(figsize=(10, 6)) sns.barplot(x=\'Product\', y=\'Sales\', hue=\'Region\', data=df, palette=\'Set2\') plt.title(\'Sales by Product and Region (Set2 Palette)\') plt.xlabel(\'Product\') plt.ylabel(\'Sales\') plt.legend(title=\'Region\') plt.savefig(\'plot_set2.png\') plt.show() # Step 3: Plotting with \\"HUSL\\" palette plt.figure(figsize=(10, 6)) sns.barplot(x=\'Product\', y=\'Sales\', hue=\'Region\', data=df, palette=sns.color_palette(\\"husl\\", 9)) plt.title(\'Sales by Product and Region (HUSL Palette)\') plt.xlabel(\'Product\') plt.ylabel(\'Sales\') plt.legend(title=\'Region\') plt.savefig(\'plot_husl.png\') plt.show() # Step 4: Plotting with custom cubehelix palette plt.figure(figsize=(10, 6)) cubehelix = sns.cubehelix_palette(start=1, rot=-0.75) sns.barplot(x=\'Product\', y=\'Sales\', hue=\'Region\', data=df, palette=cubehelix) plt.title(\'Sales by Product and Region (Custom Cubehelix Palette)\') plt.xlabel(\'Product\') plt.ylabel(\'Sales\') plt.legend(title=\'Region\') plt.savefig(\'plot_cubehelix.png\') plt.show() # Step 5: Extract and print hex codes from \\"Set2\\" palette set2_hex_codes = sns.color_palette(\\"Set2\\").as_hex() print(set2_hex_codes) # Step 6: Context management with \\"dark:#5A9_r\\" palette with sns.color_palette(\\"dark:#5A9_r\\"): plt.figure(figsize=(10, 6)) sns.barplot(x=\'Product\', y=\'Sales\', hue=\'Region\', data=df) plt.title(\'Sales by Product and Region (Temporary dark:#5A9_r Palette)\') plt.xlabel(\'Product\') plt.ylabel(\'Sales\') plt.legend(title=\'Region\') plt.savefig(\'plot_dark5A9r.png\') plt.show()"},{"question":"# Advanced Coding Assessment Question: Implementing causal attention masking is crucial for sequence models, especially in tasks like language generation where future tokens should not influence current predictions. Using PyTorch, construct a function that integrates causal bias into a provided attention score matrix. Task You are required to implement the `apply_causal_mask` function which applies a causal mask to the given attention scores matrix. # Function Signature ```python def apply_causal_mask(attention_scores: torch.Tensor, mask_type: str) -> torch.Tensor: Applies a causal mask to the attention scores tensor. Args: attention_scores (torch.Tensor): A 2D tensor of shape (sequence_length, sequence_length) containing attention scores. mask_type (str): A string indicating the type of causal mask: \'lower_right\' for causal_lower_right or \'upper_left\' for causal_upper_left. Returns: torch.Tensor: The masked attention scores tensor. ``` # Input - `attention_scores`: A 2D tensor of shape (sequence_length, sequence_length) representing the attention scores between each pair of tokens in the sequence. - `mask_type`: A string that can be either \'lower_right\' or \'upper_left\'. This indicates which type of causal mask to apply: - \'lower_right\' applies the `causal_lower_right` mask. - \'upper_left\' applies the `causal_upper_left` mask. # Output - The function should return a 2D tensor of the same shape as `attention_scores` with the respective causal mask applied. # Constraints - You can assume the sequence length is manageable and fits in memory. - The masks should be binary, i.e., 0s and 1s, where positions with 1s are retained, and positions with 0s are masked out (set to a very large negative number to effectively ignore them in softmax computations). # Example Usage ```python import torch # For illustration, let\'s assume the causal masks are simple identity matrices def causal_lower_right(sequence_length): mask = torch.tril(torch.ones(sequence_length, sequence_length)) return mask def causal_upper_left(sequence_length): mask = torch.triu(torch.ones(sequence_length, sequence_length)) return mask def apply_causal_mask(attention_scores, mask_type): sequence_length = attention_scores.size(0) if mask_type == \'lower_right\': mask = causal_lower_right(sequence_length) elif mask_type == \'upper_left\': mask = causal_upper_left(sequence_length) else: raise ValueError(\\"Invalid mask_type. Choose either \'lower_right\' or \'upper_left\'.\\") mask = mask.to(attention_scores.device) masked_attention_scores = attention_scores * mask + (1.0 - mask) * (-1e9) return masked_attention_scores # Example use: attention_scores = torch.rand(5, 5) # Random attention scores for a sequence of length 5 masked_scores = apply_causal_mask(attention_scores, \'lower_right\') print(masked_scores) ``` In this example, it is assumed that `causal_lower_right` and `causal_upper_left` create appropriate masks for causal attention. The function `apply_causal_mask` then applies these masks to the attention scores tensor.","solution":"import torch def causal_lower_right(sequence_length): Generates a lower triangular matrix mask for causal attention. mask = torch.tril(torch.ones(sequence_length, sequence_length)) return mask def causal_upper_left(sequence_length): Generates an upper triangular matrix mask for causal attention. mask = torch.triu(torch.ones(sequence_length, sequence_length)) return mask def apply_causal_mask(attention_scores, mask_type): Applies a causal mask to the attention scores tensor. Args: attention_scores (torch.Tensor): A 2D tensor of shape (sequence_length, sequence_length) containing attention scores. mask_type (str): A string indicating the type of causal mask: \'lower_right\' for causal lower triangular or \'upper_left\' for causal upper triangular. Returns: torch.Tensor: The masked attention scores tensor. sequence_length = attention_scores.size(0) if mask_type == \'lower_right\': mask = causal_lower_right(sequence_length) elif mask_type == \'upper_left\': mask = causal_upper_left(sequence_length) else: raise ValueError(\\"Invalid mask_type. Choose either \'lower_right\' or \'upper_left\'.\\") mask = mask.to(attention_scores.device) masked_attention_scores = attention_scores * mask + (1.0 - mask) * (-1e9) return masked_attention_scores"},{"question":"# PyTorch: Neural Network Module Tracking Objective Your task is to create a custom utility that mimics the behavior of `torch.utils.module_tracker.ModuleTracker` to track the current position inside a hierarchy of `torch.nn.Module` classes. Requirements 1. Implement a class `CustomModuleTracker` that: - Tracks entry and exit points of nested `torch.nn.Module`s. - Can give the current depth level of the module hierarchy. - Can provide a string representing the current full path (module names separated by `/`). 2. Your class should have the following methods: - `enter_module(module_name: str)`: To be called when entering a module. - `exit_module()`: To be called when exiting a module. - `current_depth() -> int`: Returns the current depth in the module hierarchy. - `current_path() -> str`: Returns the current path as a string of module names separated by `/`. Input Format There is no direct input format for the methods. You will define the class `CustomModuleTracker` and use its methods to simulate tracking within a given module hierarchy. Output Format Your code should output: 1. The depth level after a series of `enter_module` and `exit_module` calls. 2. The final path representation after the operations. Example Usage ```python tracker = CustomModuleTracker() tracker.enter_module(\'Sequential\') tracker.enter_module(\'Conv2d\') print(tracker.current_depth()) # Output: 2 print(tracker.current_path()) # Output: Sequential/Conv2d tracker.enter_module(\'ReLU\') tracker.exit_module() tracker.exit_module() print(tracker.current_depth()) # Output: 1 print(tracker.current_path()) # Output: Sequential ``` Constraints - You should not use any global variables. - Ensure your solution handles edge cases such as excessive `exit_module` calls appropriately. Implement the class `CustomModuleTracker` in the cell below. ```python # Implement your solution here ```","solution":"class CustomModuleTracker: def __init__(self): self.modules = [] def enter_module(self, module_name: str): Enters a new module by adding it to the tracking list. self.modules.append(module_name) def exit_module(self): Exits the last module by removing it from the tracking list. if self.modules: self.modules.pop() def current_depth(self) -> int: Returns the current depth of the module hierarchy. return len(self.modules) def current_path(self) -> str: Returns the current path of the module hierarchy as a string. return \'/\'.join(self.modules)"},{"question":"Problem Statement You are tasked with implementing a function to lookup information from the NIS maps and return comprehensive details. Specifically, you need to fetch an entry for a given key in a specified map, retrieve the complete content of that map, and list all available maps within the domain. You should also handle errors gracefully and accommodate the default domain settings. **Function Signature:** ```python def lookup_nis_info(key: str, mapname: str) -> dict: pass ``` **Input:** - `key` (str): The key to lookup in the NIS map. - `mapname` (str): The name of the map where the key should be looked up. **Output:** - dict: A dictionary with the following keys and values: - `match_value`: The value matched for the given key in the map. - `map_content`: A dictionary mapping all keys to values in the specified map. - `all_maps`: A list of all available maps in the domain. - `default_domain`: The default NIS domain of the system. **Constraints:** - The function should handle the case where the key does not exist in the map by setting `match_value` to `None`. - If the `nis` module raises an error (of type `nis.error`), catch the exception and set the corresponding output fields to `None` or an empty list/dict as appropriate. - Assume the default domain if none is provided. **Example:** Suppose the following hypothetical values for illustration: - `key = \\"username\\"` - `mapname = \\"passwd.byname\\"` ```python lookup_nis_info(\\"username\\", \\"passwd.byname\\") ``` should return: ```python { \\"match_value\\": b\\"userdetails\\", \\"map_content\\": {b\\"username1\\": b\\"userdetails1\\", b\\"username2\\": b\\"userdetails2\\"}, \\"all_maps\\": [\\"passwd.byname\\", \\"hosts.byname\\"], \\"default_domain\\": \\"example.com\\" } ``` Note: This problem assumes a Unix-based system with the `nis` module properly set up.","solution":"import nis def lookup_nis_info(key: str, mapname: str) -> dict: result = { \\"match_value\\": None, \\"map_content\\": {}, \\"all_maps\\": [], \\"default_domain\\": None } try: # Get the default domain default_domain = nis.get_default_domain() result[\\"default_domain\\"] = default_domain # Lookup key in the specified map try: match_value = nis.match(key, mapname) result[\\"match_value\\"] = match_value except nis.error: result[\\"match_value\\"] = None # Retrieve the complete content of the map try: map_content = nis.cat(mapname) result[\\"map_content\\"] = map_content except nis.error: result[\\"map_content\\"] = {} # List all available maps within the domain try: all_maps = nis.maps() result[\\"all_maps\\"] = all_maps except nis.error: result[\\"all_maps\\"] = [] except nis.error: result[\\"default_domain\\"] = None return result"},{"question":"# Advanced PyTorch Assessment Task: Custom Kernel Performance Analysis Objective In this assessment, you are required to create and analyze the performance of a custom GPU kernel using PyTorch and TorchInductor. This task will test your ability to implement backend optimizations using PyTorch\'s advanced profiling tools and environment variables. Task Description 1. **Kernel Implementation**: Implement a custom pointwise kernel in PyTorch using the `torch.cuda` API. 2. **Performance Profiling**: Profile the performance of your kernel, breaking down the execution time and resource utilization. 3. **Optimization**: Use TorchInductor environment variables to benchmark and autotune your kernel for optimal performance. Steps and Requirements 1. **Setup the Environment**: - Initialize a PyTorch environment with TorchInductor enabled. - Create a script named `custom_kernel.py` containing your kernel implementation. 2. **Implement the Pointwise Kernel**: - Design a simple pointwise operation (e.g., element-wise multiplication of two tensors). - Ensure your kernel can operate on GPU efficiently. 3. **Profile the Kernel**: - Use the relevant environment variables (`TORCHINDUCTOR_UNIQUE_KERNEL_NAMES` and `TORCHINDUCTOR_BENCHMARK_KERNEL`) to profile your kernel. - Generate a performance report highlighting the GPU time and kernel execution time. 4. **Optimize the Kernel**: - Enable autotuning using `TORCHINDUCTOR_MAX_AUTOTUNE` and re-run your kernel. - Compare the performance before and after autotuning. 5. **Submission**: - Submit your `custom_kernel.py` script. - Provide a detailed report (in markdown format) including: - The implementation details of your kernel. - Performance profiling results before and after optimization. - Insights on how autotuning impacted the performance. Input and Output Formats - **Input**: Two tensors of the same shape. - **Output**: Resultant tensor after applying the pointwise operation. Constraints - **Performance**: Your optimized kernel should demonstrate at least a 20% improvement in execution time post autotuning. - **Compatibility**: Ensure your code runs on CUDA-enabled GPUs with appropriate PyTorch and TorchInductor configurations. Example ```python # custom_kernel.py import torch import torch.cuda.profiler as profiler def pointwise_kernel(a, b): return a * b if __name__ == \\"__main__\\": torch.cuda.set_device(0) a = torch.randn(1000, 1000).cuda() b = torch.randn(1000, 1000).cuda() profiler.start() result = pointwise_kernel(a, b) profiler.stop() print(\\"Kernel executed successfully.\\") ``` Include the commands and profiling results as part of your report. Aim to showcase proficiency in both coding and performance optimization.","solution":"import torch import torch.cuda.profiler as profiler def pointwise_kernel(a, b): Perform element-wise multiplication of two tensors a and b. return a * b def benchmark_pointwise_kernel(a, b): Benchmark the pointwise kernel using torch.cuda.profiler and time measurement. start_event = torch.cuda.Event(enable_timing=True) end_event = torch.cuda.Event(enable_timing=True) start_event.record() result = pointwise_kernel(a, b) end_event.record() # Waits for everything to finish running torch.cuda.synchronize() # Calculate elapsed time elapsed_time_ms = start_event.elapsed_time(end_event) return result, elapsed_time_ms if __name__ == \\"__main__\\": # Set an environment variable for TorchInductor unique kernel names os.environ[\'TORCHINDUCTOR_UNIQUE_KERNEL_NAMES\'] = \'1\' os.environ[\'TORCHINDUCTOR_BENCHMARK_KERNEL\'] = \'1\' os.environ[\'TORCHINDUCTOR_MAX_AUTOTUNE\'] = \'1\' # Initialize GPU device torch.cuda.set_device(0) # Create random tensors for the benchmark a = torch.randn(1000, 1000).cuda() b = torch.randn(1000, 1000).cuda() # Profiling initial run result, initial_time = benchmark_pointwise_kernel(a, b) print(f\\"Initial kernel execution time: {initial_time} ms\\") # This block is where we would invoke TorchInductor autotuning # Note: The actual autotuning integration with TorchInductor would happen internally within PyTorch\'s # profiling and would be comparative benchmarking of different run configurations. # Rerun the kernel to measure optimized time after enabling autotuning result, optimized_time = benchmark_pointwise_kernel(a, b) print(f\\"Optimized kernel execution time: {optimized_time} ms\\") # Ensure there is an improvement improvement = (initial_time - optimized_time) / initial_time * 100 print(f\\"Performance improvement: {improvement}%\\") # Assert that the improvement is at least 20% assert improvement >= 20, \\"Autotuning failed to meet the expected performance improvement\\""},{"question":"# Comprehensive Path Manipulation Function **Objective:** You are required to implement a function that takes a list of file paths and returns a dictionary with various details about those paths. Your function should leverage the capabilities provided by the `os.path` module. **Function Signature:** ```python def analyze_paths(file_paths: list) -> dict: pass ``` **Input:** - `file_paths` (list): A list of file paths. Each path is a string and can be absolute or relative. **Output:** - `result` (dict): A dictionary with the following structure: ```python { \\"common_prefix\\": str, \\"normal_paths\\": list, \\"real_paths\\": list, \\"path_exists\\": list, \\"file_sizes\\": dict, \\"file_times\\": dict } ``` Where: - `\\"common_prefix\\"`: The longest common prefix of all paths in the input list. - `\\"normal_paths\\"`: A list of paths normalized by collapsing redundant separators and up-level references. - `\\"real_paths\\"`: A list of canonical paths eliminating any symbolic links encountered in the paths. - `\\"path_exists\\"`: A list of booleans indicating if each corresponding path in the input list exists. - `\\"file_sizes\\"`: A dictionary where keys are file paths and values are their respective sizes. Only include paths that exist and are files. - `\\"file_times\\"`: A dictionary where keys are file paths and values are another dictionary with `\\"access_time\\"` and `\\"modification_time\\"` as keys, providing the respective times. Only include paths that exist and are files. **Constraints:** - You can assume that the input list contains at least one path. - The function should handle exceptions gracefully, for instance, by skipping paths that cause errors and continuing with the rest. - The solution should be efficient and concise, leveraging the `os.path` module functions wherever applicable. **Example:** ```python file_paths = [ \\"/home/user/file1.txt\\", \\"/home/user/file2.txt\\", \\"/home/user/docs/file3.txt\\", \\"/home/user/images/file4.txt\\" ] result = analyze_paths(file_paths) # Example output (the values may vary based on the actual file system state) # { # \\"common_prefix\\": \\"/home/user/\\", # \\"normal_paths\\": [ # \\"/home/user/file1.txt\\", # \\"/home/user/file2.txt\\", # \\"/home/user/docs/file3.txt\\", # \\"/home/user/images/file4.txt\\" # ], # \\"real_paths\\": [ # \\"/home/user/file1.txt\\", # \\"/home/user/file2.txt\\", # \\"/home/user/docs/file3.txt\\", # \\"/home/user/images/file4.txt\\" # ], # \\"path_exists\\": [True, True, True, True], # \\"file_sizes\\": { # \\"/home/user/file1.txt\\": 1024, # \\"/home/user/file2.txt\\": 2048, # \\"/home/user/docs/file3.txt\\": 512, # \\"/home/user/images/file4.txt\\": 4096 # }, # \\"file_times\\": { # \\"/home/user/file1.txt\\": {\\"access_time\\": 1627675120.0, \\"modification_time\\": 1627675100.0}, # \\"/home/user/file2.txt\\": {\\"access_time\\": 1627675220.0, \\"modification_time\\": 1627675200.0}, # \\"/home/user/docs/file3.txt\\": {\\"access_time\\": 1627675320.0, \\"modification_time\\": 1627675300.0}, # \\"/home/user/images/file4.txt\\": {\\"access_time\\": 1627675420.0, \\"modification_time\\": 1627675400.0} # } # } ``` **Note:** - Make sure to handle symbolic links and path normalization as described. - Handle different path types (absolute and relative). - The existence of the paths should be checked using the appropriate `os.path` functions.","solution":"import os def analyze_paths(file_paths): Analyze the given list of file paths and provide various details. :param file_paths: A list of file paths (str). :return: A dictionary containing path details. result = { \\"common_prefix\\": os.path.commonprefix(file_paths), \\"normal_paths\\": [], \\"real_paths\\": [], \\"path_exists\\": [], \\"file_sizes\\": {}, \\"file_times\\": {} } for path in file_paths: try: # Normalize the path normal_path = os.path.normpath(path) result[\\"normal_paths\\"].append(normal_path) # Resolve the real path (eliminate symlinks) real_path = os.path.realpath(path) result[\\"real_paths\\"].append(real_path) # Check if the path exists exists = os.path.exists(path) result[\\"path_exists\\"].append(exists) if exists: if os.path.isfile(path): # Get the file size size = os.path.getsize(path) result[\\"file_sizes\\"][path] = size # Get the file access and modification times access_time = os.path.getatime(path) modification_time = os.path.getmtime(path) result[\\"file_times\\"][path] = { \\"access_time\\": access_time, \\"modification_time\\": modification_time } except Exception as e: print(f\\"Error processing path: {path}, {e}\\") return result"},{"question":"Objective: You are required to write a Python function that leverages the `pwd` module to find the home directory of a user given their username. Additionally, the function should handle errors gracefully if the user does not exist. Function Signature: ```python def get_user_home_directory(username: str) -> str: Fetches the home directory of the specified user using the pwd module. Args: username (str): The username of the user. Returns: str: The user\'s home directory. Raises: ValueError: If the user does not exist or if another error occurs. pass ``` Description: 1. **Input:** A string `username` which represents the username. 2. **Output:** A string which represents the home directory of the user. 3. **Constraints:** - You should use the `pwd.getpwnam(username)` function to obtain user details. - If the user does not exist, an appropriate error message should be included in your `ValueError` exception. - Ensure that your code handles cases where the `pwd` module is not available (i.e., systems other than Unix) in a user-friendly manner. Example: ```python try: home_directory = get_user_home_directory(\\"johndoe\\") print(home_directory) # Should print the home directory of the user \\"johndoe\\" except ValueError as e: print(e) ``` In the above example, replace `\\"johndoe\\"` with an actual username available on your Unix system to test the functionality. # Performance Requirements: - The function should execute efficiently with minimal overhead. - The function should handle exceptional cases and avoid crashing. Note: This implementation assumes your environment supports the `pwd` module. The solution won\'t be functional on non-Unix systems.","solution":"import pwd def get_user_home_directory(username: str) -> str: Fetches the home directory of the specified user using the pwd module. Args: username (str): The username of the user. Returns: str: The user\'s home directory. Raises: ValueError: If the user does not exist or if another error occurs. try: user_info = pwd.getpwnam(username) return user_info.pw_dir except KeyError: raise ValueError(f\\"User \'{username}\' does not exist.\\") except Exception as e: raise ValueError(f\\"An error occurred: {str(e)}\\")"},{"question":"You are tasked with creating an advanced echo server using Python\'s `asyncio` library and low-level APIs. Specifically, you will implement a custom protocol that logs all received data before echoing it back to the client. # Requirements: 1. **Implement a Custom Protocol:** - Create a protocol class `LoggingEchoProtocol` that inherits from `asyncio.Protocol`. - Implement the following methods: - `connection_made`: Log the client\'s address. - `data_received`: Log the received data, then send the data back to the client. - `connection_lost`: Log the connection loss along with any exception. 2. **Create an Echo Server:** - Use an asyncio event loop method `create_server` to create a TCP echo server utilizing your `LoggingEchoProtocol`. - The server should listen on `localhost` (127.0.0.1) and port `8888`. 3. **Logging Requirements:** - Use the Python `logging` module to handle all logging. - Log details at the INFO level, ensuring messages include the client address where applicable. # Input: There is no direct input for the program; it will run as a server. # Output: The output will be logged messages and echo responses to connected clients. # Constraints: - Focus on correct use of `asyncio` transports and protocols. - Handle errors appropriately, ensuring no unhandled exceptions stop the server. # Sample Code (for reference structure): ```python import asyncio import logging class LoggingEchoProtocol(asyncio.Protocol): def connection_made(self, transport): # Implement connection setup and logging pass def data_received(self, data): # Implement data handling and logging pass def connection_lost(self, exc): # Implement connection cleanup and logging pass async def main(): logging.basicConfig(level=logging.INFO) loop = asyncio.get_running_loop() server = await loop.create_server(lambda: LoggingEchoProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main()) ``` Complete the `LoggingEchoProtocol` class ensuring that it logs the appropriate details (client address, data received, and connection loss). # Evaluation Criteria: - Completeness and correctness of the `LoggingEchoProtocol`. - Proper initialization and teardown of the echo server. - Correct logging of connection events.","solution":"import asyncio import logging class LoggingEchoProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport self.peername = transport.get_extra_info(\\"peername\\") logging.info(f\\"Connection made with {self.peername}\\") def data_received(self, data): message = data.decode() logging.info(f\\"Data received from {self.peername}: {message}\\") self.transport.write(data) def connection_lost(self, exc): if exc: logging.info(f\\"Connection lost with {self.peername} due to an error: {exc}\\") else: logging.info(f\\"Connection closed with {self.peername}\\") async def main(): logging.basicConfig(level=logging.INFO) loop = asyncio.get_running_loop() server = await loop.create_server(lambda: LoggingEchoProtocol(), \'127.0.0.1\', 8888) logging.info(\\"Server started on 127.0.0.1:8888\\") async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Email Content Extractor **Problem Statement:** You are tasked with extracting the text content from an email message object while filtering out any non-text content. The extracted text content should be returned as a list of strings, where each string represents a line from the email\'s text content. You need to implement the function `extract_text_content(msg: email.message.Message, decode: bool = False) -> List[str]` that performs this task using the `email.iterators` module. **Function Signature:** ```python from email.message import Message from typing import List def extract_text_content(msg: Message, decode: bool = False) -> List[str]: ``` **Input:** - `msg`: An `email.message.Message` object representing the email message. - `decode`: A boolean (optional, default is False) that, when set to True, decodes the payloads using `Message.get_payload(decode=True)`. **Output:** - A list of strings where each string is a line of text content from the email message. If no text content is found, return an empty list. **Constraints:** - The function should make use of the `email.iterators.body_line_iterator()` to extract the payloads, ensuring only text content is extracted. - The function should handle MIME types correctly and should skip over non-text MIME parts. - The solution should be efficient and avoid unnecessary processing of non-text parts. # Example: ```python from email import message_from_string email_content = MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=\\"simple boundary\\" --simple boundary Content-Type: text/plain; charset=us-ascii This is the body of the message. It has multiple lines. --simple boundary Content-Type: text/html; charset=us-ascii <html><body>HTML content here.</body></html> --simple boundary-- msg = message_from_string(email_content) result = extract_text_content(msg) print(result) # Output: # [ # \\"This is the body of the message.\\", # \\"It has multiple lines.\\" # ] ``` Write your implementation of the function `extract_text_content` to pass the given example and adhere to the specifications outlined above. **Note:** You may assume the necessary imports from the `email` package are available. This question requires an understanding of how to work with MIME message structures and the `email.iterators` module.","solution":"from email.message import Message from email.iterators import body_line_iterator from typing import List def extract_text_content(msg: Message, decode: bool = False) -> List[str]: Extracts text content from an email message object, filtering out non-text content. Args: msg (Message): An email message object. decode (bool): Whether to decode the payloads. Default is False. Returns: List[str]: A list of strings representing the lines of text content from the email. text_content = [] for part in msg.walk(): if part.get_content_type() == \'text/plain\' and part.get_content_disposition() is None: if decode: payload = part.get_payload(decode=True).decode(part.get_content_charset(), errors=\'replace\') else: payload = part.get_payload(decode=False) text_content.extend(payload.splitlines()) return text_content"},{"question":"# Semi-Supervised Learning with Scikit-Learn You are tasked with implementing a semi-supervised learning pipeline to classify data using scikit-learn\'s `LabelPropagation` model. Your implementation must work with both numeric and categorical data and must be able to handle missing labels in the dataset. Follow the steps below to complete your assignment: Step-by-Step Instructions: 1. **Data Preparation**: - Create or load a dataset that includes both labeled and unlabeled data. For simplicity, you can use the `load_digits` dataset from `sklearn.datasets`, then artificially remove labels from part of the data to simulate an unlabeled set. 2. **Preprocessing**: - Ensure the data is preprocessed appropriately for the `LabelPropagation` model. This includes handling missing labels (assigned as `-1`) and any necessary scaling or transformation. 3. **Model Implementation**: - Implement and train a `LabelPropagation` model using the prepared dataset. Use the RBF kernel for similarity graph construction and set an appropriate gamma value. 4. **Evaluation**: - Evaluate the performance of your model on a test set and report accuracy. Constraints: - Do not use any additional libraries outside of those provided by scikit-learn, numpy, and pandas. - Ensure your code is well-documented and includes error handling for typical issues that might arise with semi-supervised learning. Input and Output Formats: # Input: - A fully annotated Jupyter notebook or Python script. # Output: - Preprocessed dataset with missing labels. - Trained `LabelPropagation` model. - Model evaluation accuracy. Example: ```python import numpy as np import pandas as pd from sklearn.datasets import load_digits from sklearn.semi_supervised import LabelPropagation from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler # Load digits dataset digits = load_digits() X, y = digits.data, digits.target # Simulate missing labels by setting some to -1 rng = np.random.RandomState(42) random_unlabeled_points = rng.rand(len(y)) < 0.3 y[random_unlabeled_points] = -1 # Split the data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Data preprocessing: scaling scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Instantiate and fit LabelPropagation model label_prop_model = LabelPropagation(kernel=\'rbf\', gamma=20) label_prop_model.fit(X_train, y_train) # Predict on the test set y_pred = label_prop_model.predict(X_test) # Evaluate model performance accuracy = accuracy_score(y_test[y_test != -1], y_pred[y_test != -1]) print(f\'Accuracy: {accuracy:.2f}\') ``` Follow the instructions carefully and ensure that your code is robust and efficient. Good luck!","solution":"import numpy as np import pandas as pd from sklearn.datasets import load_digits from sklearn.semi_supervised import LabelPropagation from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler def prepare_data(): digits = load_digits() X, y = digits.data, digits.target # Simulate missing labels by setting some to -1 rng = np.random.RandomState(42) random_unlabeled_points = rng.rand(len(y)) < 0.3 y[random_unlabeled_points] = -1 return X, y def preprocess_data(X, y): # Split the data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Data preprocessing: scaling scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) return X_train, X_test, y_train, y_test def train_model(X_train, y_train): # Instantiate and fit LabelPropagation model label_prop_model = LabelPropagation(kernel=\'rbf\', gamma=20) label_prop_model.fit(X_train, y_train) return label_prop_model def evaluate_model(model, X_test, y_test): # Predict on the test set y_pred = model.predict(X_test) # Evaluate model performance, ignoring unlabeled data accuracy = accuracy_score(y_test[y_test != -1], y_pred[y_test != -1]) return accuracy # Main function that combines all steps def semi_supervised_learning_pipeline(): X, y = prepare_data() X_train, X_test, y_train, y_test = preprocess_data(X, y) model = train_model(X_train, y_train) accuracy = evaluate_model(model, X_test, y_test) return accuracy"},{"question":"<|Analysis Begin|> The documentation provided describes the functions and types related to the creation and management of generator objects within Python using the Python C API. The information includes details about the `PyGenObject` structure, utility functions to check if an object is a generator, and functions to create new generator objects. Key elements: 1. `PyGen_Check` and `PyGen_CheckExact` to verify generator objects. 2. `PyGen_New` to create a new generator object from a frame object. 3. `PyGen_NewWithQualName` to create a new generator object with specially assigned `__name__` and `__qualname__`. Since this documentation focuses on internal Python generator management using C API functions, it might not directly translate to a typical Python coding assessment question focusing on Python 3.10 language features. The most useful aspect for a Python coding question would be understanding how generators work in Python at a higher level, without diving into the C API. <|Analysis End|> <|Question Begin|> **Objective:** Create a function `combine_generators` that takes a list of generator functions and an integer `n`. It combines the generators in a round-robin fashion up to `n` elements. # Instructions: 1. Write a function `combine_generators(generators: List[Callable[[], Generator]], n: int) -> List[Any]` that: - **Input:** - `generators`: A list of functions, where each function when called returns a generator object. - `n`: An integer indicating the number of elements to generate in total. - **Output:** A list containing `n` elements generated by iterating over the input generators in a round-robin manner. 2. **Constraints:** - All generator functions in the list generate an infinite sequence of values. - Each generator function is called only once to instantiate its respective generator. 3. **Performance:** This function should be efficient enough to handle cases where `n` is very large (up to 10^6). # Example: ```python def generator_one(): i = 1 while True: yield i i += 1 def generator_two(): i = 100 while True: yield i i += 100 # Assume combine_generators is implemented correctly. result = combine_generators([generator_one, generator_two], 5) print(result) # Output should be [1, 100, 2, 200, 3] ``` Note: - The list `[1, 100, 2, 200, 3]` is generated by taking elements from `generator_one` and `generator_two` in turns until 5 elements are collected.","solution":"from typing import List, Callable, Generator, Any def combine_generators(generators: List[Callable[[], Generator]], n: int) -> List[Any]: Combine a list of generator functions in a round-robin fashion up to n elements. Parameters: - generators: List of generator functions that each return an infinite generator. - n: Number of elements to generate in total. Returns: - List containing n elements generated in a round-robin manner. # Initialize each generator initialized_generators = [gen() for gen in generators] result = [] generator_count = len(initialized_generators) index = 0 while len(result) < n: # Take the next element from the current generator current_gen = initialized_generators[index] result.append(next(current_gen)) # Move to the next generator in the list index = (index + 1) % generator_count return result"},{"question":"You are provided with two datasets: `titanic` and `fmri`, which are available in the seaborn library. Your task is to create a set of visualizations to analyze these datasets using the `seaborn` library. 1. **Titanic Dataset Visualization**: - Load the `titanic` dataset using `seaborn.load_dataset`. - Create a scatter plot using `seaborn.relplot` to visualize the relationship between `age` and `fare`. - Color the points by the `class` of passengers. - Facet the plot by columns based on the `sex` of passengers. - Size the points based on the `survived` column (survived = 1 means larger size, did not survive = 0 means smaller size). 2. **FMRI Dataset Visualization**: - Load the `fmri` dataset using `seaborn.load_dataset`. - Create a line plot using `seaborn.relplot` to visualize the change in `signal` over `timepoint`. - Color the lines by `event` and facet by columns based on the `region`. - Customize the plot further: - Set the height of each facet to 5 and the aspect ratio to 0.75. - Add horizontal lines at y=0 for reference. - Set the x-axis label to \\"Timepoint\\" and the y-axis label to \\"Percent signal change\\". - Set the column titles to \\"Region: {col_name} cortex\\". - Ensure the layout is tight with minimal padding. # Requirements: - Use proper seaborn and matplotlib functions to achieve the tasks. - The final visualizations should be clearly labeled and aesthetically pleasing. - Comment your code to explain each step. # Constraints: - You should handle any potential missing values in the dataset appropriately. - Make sure to optimize the plot render times for better performance. # Example Output: The provided code should generate two separate visualizationsone for the Titanic data and one for the FMRI dataadhering to the specified customization instructions. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_titanic(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Drop missing values for this visualization titanic = titanic.dropna(subset=[\'age\', \'fare\', \'class\', \'sex\', \'survived\']) # Create scatter plot scatter_plot = sns.relplot( data=titanic, x=\'age\', y=\'fare\', hue=\'class\', col=\'sex\', size=\'survived\', sizes=(20, 200), # Control the size of the points alpha=0.6, height=5, aspect=1 ) # Set the plot titles and labels scatter_plot.set_axis_labels(\\"Age\\", \\"Fare\\") scatter_plot.set_titles(\\"{col_name}\\") # Show plot plt.show() def visualize_fmri(): # Load the FMRI dataset fmri = sns.load_dataset(\\"fmri\\") # Create line plot line_plot = sns.relplot( data=fmri, kind=\'line\', x=\'timepoint\', y=\'signal\', hue=\'event\', col=\'region\', height=5, aspect=0.75 ) # Customize plot for ax in line_plot.axes.flat: ax.axhline(0, ls=\'--\', c=\'gray\', lw=0.7) # Add horizontal lines at y=0 line_plot.set_axis_labels(\\"Timepoint\\", \\"Percent signal change\\") line_plot.set_titles(\\"Region: {col_name} cortex\\") line_plot.tight_layout(pad=0.5) # Show plot plt.show()"},{"question":"# Objective Implement a function in PyTorch that demonstrates the use of CUDA device management, memory management, and stream handling capabilities. This will test your understanding of the PyTorch `torch.cuda` submodule. # Problem Statement Create a function `cuda_memory_analysis` that performs the following tasks: 1. **Device Check and Selection**: - Check if CUDA is available. If not, raise an `EnvironmentError` with the message \\"CUDA is not available\\". - Select the default CUDA device and set it as the active device. 2. **Memory Allocation and Management**: - Create a large tensor of random values of size `(1000, 1000)` on the CUDA device. - Measure the current allocated memory and cached memory on the device before and after the tensor creation. - Store these measurements in a dictionary with keys `\'before_allocation\'` and `\'after_allocation\'` respectively. 3. **Stream Synchronization**: - Create a CUDA stream and perform a simple matrix multiplication operation (`torch.matmul`) within the stream context using the previously created tensor. - Synchronize the stream to ensure the completion of all operations. 4. **Memory Cleanup**: - Free up the allocated memory. - Measure the memory usage after cleanup and store it in the dictionary under the key `\'after_cleanup\'`. 5. Return the dictionary of memory usage measurements. # Input The function `cuda_memory_analysis` does not take any input parameters. # Output The function should return a dictionary with the following format: ```python { \'before_allocation\': { \'allocated\': <memory allocated in bytes>, \'cached\': <memory cached in bytes> }, \'after_allocation\': { \'allocated\': <memory allocated in bytes>, \'cached\': <memory cached in bytes> }, \'after_cleanup\': { \'allocated\': <memory allocated in bytes>, \'cached\': <memory cached in bytes> } } ``` # Constraints - Ensure that all CUDA operations are performed within the specified stream context. - The tensor size `(1000, 1000)` is fixed and should not be modified. - Properly handle memory allocation and deallocation to avoid memory leaks. # Example ```python import torch def cuda_memory_analysis(): # Implementation here pass # Sample function call result = cuda_memory_analysis() # Sample output { \'before_allocation\': {\'allocated\': 0, \'cached\': 0}, \'after_allocation\': {\'allocated\': 4000000, \'cached\': 8000000}, \'after_cleanup\': {\'allocated\': 0, \'cached\': 4000000} } ``` **Note**: The memory values in the sample output are for illustrative purposes and may not reflect actual values. # Evaluation Criteria - Correct implementation of CUDA device checks and selection. - Accurate measurement and reporting of memory usage at critical stages. - Proper use of CUDA streams and synchronization. - Appropriate handling of memory allocation and cleanup to prevent leaks. - Adherence to the specified function signature and output format.","solution":"import torch def cuda_memory_analysis(): if not torch.cuda.is_available(): raise EnvironmentError(\\"CUDA is not available\\") device = torch.device(\\"cuda:0\\") torch.cuda.set_device(device) memory_usage = { \'before_allocation\': { \'allocated\': torch.cuda.memory_allocated(device), \'cached\': torch.cuda.memory_reserved(device) }, \'after_allocation\': {}, \'after_cleanup\': {} } # Create a large tensor of random values of size (1000, 1000) on the CUDA device tensor = torch.randn((1000, 1000), device=device) memory_usage[\'after_allocation\'] = { \'allocated\': torch.cuda.memory_allocated(device), \'cached\': torch.cuda.memory_reserved(device) } stream = torch.cuda.Stream(device) # Perform matrix multiplication within the stream context with torch.cuda.stream(stream): result = torch.matmul(tensor, tensor) # Synchronize the stream to ensure all operations are complete stream.synchronize() # Free up the allocated memory del tensor del result torch.cuda.empty_cache() memory_usage[\'after_cleanup\'] = { \'allocated\': torch.cuda.memory_allocated(device), \'cached\': torch.cuda.memory_reserved(device) } return memory_usage"},{"question":"**Objective:** The objective of this question is to assess the understanding and implementation of asyncio functionality in Python, specifically focusing on the use of tasks, queues, and synchronization primitives. **Problem Statement:** You are tasked with designing a mini asynchronous server-client architecture to simulate a basic task processing scenario using Python\'s `asyncio` module. The server is responsible for handling and processing tasks from multiple clients concurrently, making use of queues and synchronization primitives to manage the tasks. **Requirements:** 1. **Server Implementation:** - Create an `AsyncServer` class that starts a server and accepts client connections. - Use `asyncio.Queue` to manage incoming tasks from clients. - Process tasks in the queue concurrently using asyncio tasks. - Implement a method `process_task` which simulates task processing by sleeping for a random duration between 1 to 3 seconds. 2. **Client Implementation:** - Create an `AsyncClient` class that connects to the server and sends tasks. - Each task should include a unique task identifier (e.g., `task_id`). 3. **Task Processing and Synchronization:** - Ensure that tasks from multiple clients are handled in a synchronized manner. - Use an asyncio synchronization primitive (e.g., `asyncio.Lock` or `asyncio.Semaphore`) to prevent race conditions during task processing. **Constraints:** - The server must handle at least 5 clients simultaneously. - Each client should be able to send at least 10 tasks in quick succession. **Input and Output:** - The input will not be passed directly to functions, but will mimic client behavior using asyncio. - Print task details (task_id, processing start time, and end time) as they are processed by the server. **Performance Considerations:** - Implementations should aim to minimize the total processing time by making effective use of concurrency. **Example Use:** ```python import asyncio import random class AsyncServer: def __init__(self): self.queue = asyncio.Queue() self.lock = asyncio.Lock() async def start_server(self): self.server = await asyncio.start_server(self.handle_client, \'localhost\', 8888) await self.process_tasks() async def handle_client(self, reader, writer): while True: # Simulating client sending tasks data = await reader.read(100) task_id = data.decode() if not data: break await self.queue.put(task_id) writer.close() await writer.wait_closed() async def process_tasks(self): while True: task_id = await self.queue.get() async with self.lock: await self.process_task(task_id) self.queue.task_done() async def process_task(self, task_id): print(f\\"Processing task {task_id} started.\\") await asyncio.sleep(random.randint(1, 3)) print(f\\"Processing task {task_id} ended.\\") class AsyncClient: async def send_task(self, task_id): reader, writer = await asyncio.open_connection(\'localhost\', 8888) writer.write(task_id.encode()) await writer.drain() writer.close() await writer.wait_closed() async def main(): server = AsyncServer() server_coroutine = asyncio.create_task(server.start_server()) # Give the server a moment to start await asyncio.sleep(1) clients = [AsyncClient() for _ in range(5)] tasks = [client.send_task(f\\"task_{i}\\") for client in clients for i in range(10)] await asyncio.gather(*tasks) # Wait for the server to process all tasks await server.queue.join() server_coroutine.cancel() await server_coroutine if __name__ == \\"__main__\\": asyncio.run(main()) ``` **Note:** This example serves as a skeleton structure. You might need to adapt parts of the code to ensure it meets the constraints and performance requirements specified.","solution":"import asyncio import random import time class AsyncServer: def __init__(self): self.queue = asyncio.Queue() self.lock = asyncio.Lock() async def start_server(self, host=\'localhost\', port=8888): self.server = await asyncio.start_server(self.handle_client, host, port) async with self.server: await asyncio.gather(self.server.serve_forever(), self.process_tasks()) async def handle_client(self, reader, writer): while True: data = await reader.read(100) task_id = data.decode() if not data: break await self.queue.put(task_id) writer.close() await writer.wait_closed() async def process_tasks(self): while True: task_id = await self.queue.get() async with self.lock: await self.process_task(task_id) self.queue.task_done() async def process_task(self, task_id): start_time = time.time() print(f\\"Processing task {task_id} started at {start_time}.\\") await asyncio.sleep(random.randint(1, 3)) end_time = time.time() print(f\\"Processing task {task_id} ended at {end_time}. Duration: {end_time - start_time} seconds.\\") class AsyncClient: async def send_task(self, task_id): reader, writer = await asyncio.open_connection(\'localhost\', 8888) writer.write(task_id.encode()) await writer.drain() writer.close() await writer.wait_closed() async def main(): server = AsyncServer() server_coroutine = asyncio.create_task(server.start_server()) # Give the server a moment to start await asyncio.sleep(1) clients = [AsyncClient() for _ in range(5)] tasks = [client.send_task(f\\"task_{i}\\") for client in clients for i in range(10)] await asyncio.gather(*tasks) # Wait for the server to process all tasks await server.queue.join() server_coroutine.cancel() await server_coroutine if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Advanced Python Sequence Handling You are tasked with implementing a Python class `SequenceProcessor` that emulates certain functionalities of the Python C API for sequence operations. This exercise is designed to test your understanding and ability to work with sequences in Python at an advanced level. Specifications: 1. **Initialization**: - The constructor of the class should accept any sequence (like list, tuple) and store it as an instance variable. 2. **Methods**: - `is_sequence()`: Return whether the object provided during initialization is a sequence. - `get_size()`: Return the size of the sequence. - `concat(other)`: Return a new `SequenceProcessor` instance with concatenated sequences. - `repeat(count)`: Return a new `SequenceProcessor` instance with the sequence repeated `count` times. - `get_item(index)`: Return the item at index `index`. - `get_slice(start, end)`: Return a new `SequenceProcessor` instance with the sliced sequence from `start` to `end`. - `contains(value)`: Return whether the sequence contains `value`. - `to_list()`: Return the sequence as a list. - `to_tuple()`: Return the sequence as a tuple. Constraints: 1. The input sequence will be valid Python sequences like list or tuple. 2. `concat` method should raise a `TypeError` if `other` is not a `SequenceProcessor` instance. 3. `get_item` and `get_slice` should raise an `IndexError` for invalid indices. Example Usage: ```python seq1 = SequenceProcessor([1, 2, 3]) seq2 = SequenceProcessor([4, 5, 6]) print(seq1.is_sequence()) # Output: True print(seq1.get_size()) # Output: 3 concatenated = seq1.concat(seq2) print(concatenated.to_list()) # Output: [1, 2, 3, 4, 5, 6] repeated = seq1.repeat(2) print(repeated.to_list()) # Output: [1, 2, 3, 1, 2, 3] print(seq1.get_item(1)) # Output: 2 sliced = seq1.get_slice(1, 3) print(sliced.to_list()) # Output: [2, 3] print(seq1.contains(2)) # Output: True print(seq1.to_list()) # Output: [1, 2, 3] print(seq1.to_tuple()) # Output: (1, 2, 3) ``` Notes: - To ensure type safety and correctness, include error handling for incorrect types or invalid operations. - Implement the class methods in a way that mimics the behaviors described in the provided documentation. # Implement the class `SequenceProcessor` below: ```python class SequenceProcessor: def __init__(self, sequence): pass def is_sequence(self): pass def get_size(self): pass def concat(self, other): pass def repeat(self, count): pass def get_item(self, index): pass def get_slice(self, start, end): pass def contains(self, value): pass def to_list(self): pass def to_tuple(self): pass ```","solution":"class SequenceProcessor: def __init__(self, sequence): self.sequence = sequence def is_sequence(self): return isinstance(self.sequence, (list, tuple)) def get_size(self): return len(self.sequence) def concat(self, other): if not isinstance(other, SequenceProcessor): raise TypeError(\\"The argument must be an instance of SequenceProcessor\\") return SequenceProcessor(self.sequence + other.sequence) def repeat(self, count): return SequenceProcessor(self.sequence * count) def get_item(self, index): try: return self.sequence[index] except IndexError: raise IndexError(\\"Index out of range\\") def get_slice(self, start, end): return SequenceProcessor(self.sequence[start:end]) def contains(self, value): return value in self.sequence def to_list(self): return list(self.sequence) def to_tuple(self): return tuple(self.sequence)"},{"question":"# Objective Implement custom functions to compute specific distance metrics and kernel functions manually, then validate your implementations using `sklearn.metrics.pairwise` submodule. # Task 1. Implement the following functions: - `custom_manhattan_distance(X, Y)`: Compute the Manhattan distance matrix between two sets of vectors. - `custom_linear_kernel(X, Y)`: Compute the linear kernel between two sets of vectors. 2. Write a function `validate_implementations()` that: - Generates random sets of vectors `X` and `Y`. - Validates your custom implementations against the scikit-learn functions `pairwise_distances` with `metric=\'manhattan\'` and `pairwise_kernels` with `metric=\'linear\'`. # Function Definitions ```python def custom_manhattan_distance(X, Y): Compute the Manhattan distance matrix between two sets of vectors X and Y. Parameters: X (numpy.ndarray): A 2D array of shape (n_samples_X, n_features) Y (numpy.ndarray): A 2D array of shape (n_samples_Y, n_features) Returns: numpy.ndarray: A 2D array of shape (n_samples_X, n_samples_Y) containing the Manhattan distances. # Your implementation here pass def custom_linear_kernel(X, Y): Compute the linear kernel between two sets of vectors X and Y. Parameters: X (numpy.ndarray): A 2D array of shape (n_samples_X, n_features) Y (numpy.ndarray): A 2D array of shape (n_samples_Y, n_features) Returns: numpy.ndarray: A 2D array of shape (n_samples_X, n_samples_Y) containing the linear kernel values. # Your implementation here pass def validate_implementations(): Validate the custom implementations by comparing their output with scikit-learn\'s functions. Raises: AssertionError: If the outputs of the custom functions do not match those of scikit-learn. import numpy as np from sklearn.metrics import pairwise_distances from sklearn.metrics.pairwise import pairwise_kernels # Generate random sets of vectors X = np.random.rand(5, 3) Y = np.random.rand(4, 3) # Compute using custom implementations custom_manhattan = custom_manhattan_distance(X, Y) custom_linear = custom_linear_kernel(X, Y) # Compute using scikit-learn functions sklearn_manhattan = pairwise_distances(X, Y, metric=\'manhattan\') sklearn_linear = pairwise_kernels(X, Y, metric=\'linear\') # Validate results assert np.allclose(custom_manhattan, sklearn_manhattan), \\"Manhattan distances do not match!\\" assert np.allclose(custom_linear, sklearn_linear), \\"Linear kernels do not match!\\" print(\\"Validation successful: All custom implementations match scikit-learn functions.\\") ``` # Constraints - You are required to use only NumPy for array manipulations and mathematical operations in your custom functions. - Do not use any distance or kernel functions from scikit-learn in your `custom_manhattan_distance` and `custom_linear_kernel` functions. # Input Format - `X` and `Y` will be 2D NumPy arrays of shapes `(n_samples_X, n_features)` and `(n_samples_Y, n_features)` respectively. # Output Format - `custom_manhattan_distance(X, Y)` and `custom_linear_kernel(X, Y)` will return 2D NumPy arrays containing the computed distances or kernel values. - `validate_implementations()` prints a message if the custom implementations match the scikit-learn functions. # Additional Notes - Pay attention to edge cases where `X` or `Y` might have zero or one sample. # Performance Requirements - Ensure your functions are optimized for at least 1000 samples and 100 features. - The validation function should complete execution within reasonable time for these inputs.","solution":"import numpy as np def custom_manhattan_distance(X, Y): Compute the Manhattan distance matrix between two sets of vectors X and Y. Parameters: X (numpy.ndarray): A 2D array of shape (n_samples_X, n_features) Y (numpy.ndarray): A 2D array of shape (n_samples_Y, n_features) Returns: numpy.ndarray: A 2D array of shape (n_samples_X, n_samples_Y) containing the Manhattan distances. distances = np.abs(X[:, np.newaxis, :] - Y).sum(axis=2) return distances def custom_linear_kernel(X, Y): Compute the linear kernel between two sets of vectors X and Y. Parameters: X (numpy.ndarray): A 2D array of shape (n_samples_X, n_features) Y (numpy.ndarray): A 2D array of shape (n_samples_Y, n_features) Returns: numpy.ndarray: A 2D array of shape (n_samples_X, n_samples_Y) containing the linear kernel values. kernel = np.dot(X, Y.T) return kernel def validate_implementations(): Validate the custom implementations by comparing their output with scikit-learn\'s functions. Raises: AssertionError: If the outputs of the custom functions do not match those of scikit-learn. from sklearn.metrics import pairwise_distances from sklearn.metrics.pairwise import pairwise_kernels # Generate random sets of vectors np.random.seed(0) # For reproducibility X = np.random.rand(5, 3) Y = np.random.rand(4, 3) # Compute using custom implementations custom_manhattan = custom_manhattan_distance(X, Y) custom_linear = custom_linear_kernel(X, Y) # Compute using scikit-learn functions sklearn_manhattan = pairwise_distances(X, Y, metric=\'manhattan\') sklearn_linear = pairwise_kernels(X, Y, metric=\'linear\') # Validate results assert np.allclose(custom_manhattan, sklearn_manhattan), \\"Manhattan distances do not match!\\" assert np.allclose(custom_linear, sklearn_linear), \\"Linear kernels do not match!\\" print(\\"Validation successful: All custom implementations match scikit-learn functions.\\")"},{"question":"Objective: To assess your understanding of asynchronous programming with `asyncio` in Python 3.10, including handling concurrency, thread safety, dealing with blocking code, and implementing robust error handling. Problem Statement: You are required to implement a small application that simulates a simplified booking system for a concert. The application must handle multiple users trying to book seats concurrently. Following are the main requirements: 1. **Asynchronous Seat Booking**: Implement an asynchronous function `book_seat` that simulates booking a seat, which takes 1 second to complete. Assume there are 50 seats available. 2. **Concurrent Bookings Handling**: Ensure that your application can handle booking requests from multiple users concurrently without overselling the seats. 3. **Error Handling**: If a user tries to book a seat when no seats are available, raise a custom exception `NoSeatsAvailableException`. 4. **Logging and Debug Mode**: - Implement logging such that any errors raised during the booking process are logged. - Enable asyncio debug mode to catch any potential issues. Input: - An integer `n` representing the number of users trying to book seats concurrently. Expected Output: - A list of user IDs (from 0 to n-1) who successfully booked a seat. - Log errors when a booking attempt is made and no seats are available. Constraints: - 1  `n`  100 Detailed Requirements: 1. **Function Signature**: ```python async def book_seat(user_id: int) -> None: # Implementation here ``` - This function should simulate booking a seat and should take 1 second to complete. 2. **Function Signature**: ```python async def booking_system(n: int) -> List[int]: # Implementation here ``` - This function should manage the concurrency and return the list of user IDs who successfully booked the seat. 3. **Concurrency Management**: Use `asyncio.create_task` and ensure thread safety using appropriate methods. 4. **Implement Custom Exception**: ```python class NoSeatsAvailableException(Exception): def __init__(self, user_id: int, message: str = \\"No seats available\\"): self.user_id = user_id super().__init__(message) ``` Example Usage: ```python import asyncio import logging async def book_seat(user_id: int) -> None: # Simulate seat booking with asyncio sleep await asyncio.sleep(1) # Your seat booking logic here async def booking_system(n: int) -> List[int]: # Your concurrency management logic here async def main(): logging.basicConfig(level=logging.DEBUG) booked_users = await booking_system(60) print(booked_users) asyncio.run(main(), debug=True) ``` Performance: - Ensure that your program can handle up to 100 users trying to book seats concurrently without overselling the seats. Hints: - Use `asyncio.Semaphore` or another synchronization primitive to manage the available seats. - Use `asyncio.gather` to concurrently run multiple booking tasks. - Ensure proper clean-up and error logging.","solution":"import asyncio import logging from typing import List # Initialize logger logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(__name__) class NoSeatsAvailableException(Exception): def __init__(self, user_id: int, message: str = \\"No seats available\\"): self.user_id = user_id super().__init__(message) # Total number of seats available SEATS_AVAILABLE = 50 semaphore = asyncio.Semaphore(SEATS_AVAILABLE) async def book_seat(user_id: int) -> None: Simulates booking a seat, which takes 1 second to complete. Raises NoSeatsAvailableException if no seats are available. async with semaphore: await asyncio.sleep(1) if semaphore.locked(): raise NoSeatsAvailableException(user_id) logger.debug(f\\"User {user_id} successfully booked a seat.\\") async def booking_system(n: int) -> List[int]: Manages seat booking for n users trying to book seats concurrently. Returns a list of user IDs who successfully booked a seat. successful_bookings = [] async def attempt_booking(user_id: int): try: await book_seat(user_id) successful_bookings.append(user_id) except NoSeatsAvailableException as e: logger.error(f\\"User {e.user_id} failed to book a seat: {e}\\") # Create booking tasks for all users tasks = [asyncio.create_task(attempt_booking(user_id)) for user_id in range(n)] await asyncio.gather(*tasks) return successful_bookings"},{"question":"**Problem Statement:** You are given a dataset of tips collected from a restaurant. The dataset contains the following columns: - `total_bill`: The total bill amount (numeric). - `tip`: The tip amount (numeric). - `sex`: Gender of the person paying the bill (string). - `smoker`: Whether the person is a smoker or not (string). - `day`: Day of the week (string). - `time`: Time of day (lunch/dinner) (string). - `size`: Number of people at the table (numeric). You are tasked with writing a Python function that visualizes various aspects of the data using Seaborn\'s new `objects` API. Your function should: 1. Create a bar plot showing the count of tips received each day of the week. 2. Create a bar plot showing the count of tips received each day, grouped by gender. 3. Create a bar plot showing the count of different table sizes. 4. Create a bar plot showing the count of tips received at different times of the day (lunch/dinner). **Function Signature:** ```python def visualize_tips(data: pd.DataFrame): pass ``` **Input:** - `data`: A pandas DataFrame containing the tips dataset with the columns described above. **Output:** The function should display four bar plots with the specifications mentioned. **Constraints:** - Use Seaborn\'s new `objects` API for creating plots. - Use appropriate variable mappings and additional variables to define groups in the plot. - Ensure the plots are clearly labeled with appropriate titles, axis labels, and legends. **Sample Usage:** ```python import seaborn.objects as so from seaborn import load_dataset import pandas as pd # Load sample dataset tips = load_dataset(\\"tips\\") # Call the function visualize_tips(tips) ``` **Expected Output:** The function should generate four different bar plots as specified in the problem statement. ---","solution":"import pandas as pd import seaborn.objects as so import matplotlib.pyplot as plt def visualize_tips(data: pd.DataFrame): Visualizes various aspects of the given tips dataset using seaborn\'s new objects API. Parameters: data (pd.DataFrame): A DataFrame containing the tips dataset. # Create a bar plot showing the count of tips received each day of the week plot1 = ( so.Plot(data, x=\\"day\\") .add(so.Bar(), so.Count()) ) # Create a bar plot showing the count of tips received each day, grouped by gender plot2 = ( so.Plot(data, x=\\"day\\", color=\\"sex\\") .add(so.Bar(), so.Count()) ) # Create a bar plot showing the count of different table sizes plot3 = ( so.Plot(data, x=\\"size\\") .add(so.Bar(), so.Count()) ) # Create a bar plot showing the count of tips received at different times of the day (lunch/dinner) plot4 = ( so.Plot(data, x=\\"time\\") .add(so.Bar(), so.Count()) ) # Setting up the plots in a 2x2 grid _, axes = plt.subplots(2, 2, figsize=(14, 10)) plot1.on(axes[0, 0]).plot() axes[0, 0].set_title(\\"Count of Tips Received Each Day of the Week\\") axes[0, 0].set_xlabel(\\"Day\\") axes[0, 0].set_ylabel(\\"Count\\") plot2.on(axes[0, 1]).plot() axes[0, 1].set_title(\\"Count of Tips Received Each Day by Gender\\") axes[0, 1].set_xlabel(\\"Day\\") axes[0, 1].set_ylabel(\\"Count\\") plot3.on(axes[1, 0]).plot() axes[1, 0].set_title(\\"Count of Different Table Sizes\\") axes[1, 0].set_xlabel(\\"Table Size\\") axes[1, 0].set_ylabel(\\"Count\\") plot4.on(axes[1, 1]).plot() axes[1, 1].set_title(\\"Count of Tips Received at Different Times of the Day\\") axes[1, 1].set_xlabel(\\"Time\\") axes[1, 1].set_ylabel(\\"Count\\") plt.tight_layout() plt.show()"},{"question":"**Unicode Data Analyzer** You have been tasked to write a Python function that analyzes and normalizes a given Unicode string. The function should output a detailed breakdown of each character in the string based on various Unicode properties and normalize the string to a specified form. # Function Signature ```python def analyze_unicode_data(unicode_str: str, normalize_form: str) -> dict: pass ``` # Input 1. **unicode_str**: A non-empty string containing various Unicode characters. 2. **normalize_form**: A string specifying the normalization form for the Unicode string. Possible values are \'NFC\', \'NFKC\', \'NFD\', and \'NFKD\'. # Output The function should return a dictionary where: - Each key is a character from the input string. - The value is another dictionary containing the following properties of the character: - `name`: Name of the Unicode character. If the name is not found, it should contain the string `\\"Unknown\\"`. - `decimal`: Decimal value of the character if it exists, else `\\"N/A\\"`. - `digit`: Digit value of the character if it exists, else `\\"N/A\\"`. - `numeric`: Numeric value of the character if it exists, else `\\"N/A\\"`. - `category`: Unicode general category of the character. - `bidirectional`: Bidirectional class of the character. - `combining`: Canonical combining class of the character. - `east_asian_width`: East Asian width property of the character. - `mirrored`: Mirrored property of the character. - `decomposition`: Decomposition mapping of the character. The output dictionary should also contain an additional key `\\"normalized\\"` with the value being the normalized form of the input string as per the specified `normalize_form`. # Constraints - You can assume that the input string will contain valid Unicode characters. - The `normalize_form` will always be one of \'NFC\', \'NFKC\', \'NFD\', or \'NFKD\'. # Example ```python input_str = \\"A\\" normalize_form = \\"NFD\\" output = analyze_unicode_data(input_str, normalize_form) print(output) ``` Expected output: ```python { \\"A\\": { \\"name\\": \\"LATIN CAPITAL LETTER A\\", \\"decimal\\": \\"N/A\\", \\"digit\\": \\"N/A\\", \\"numeric\\": \\"N/A\\", \\"category\\": \\"Lu\\", \\"bidirectional\\": \\"L\\", \\"combining\\": 0, \\"east_asian_width\\": \\"Na\\", \\"mirrored\\": 0, \\"decomposition\\": \\"\\" }, \\"\\": { \\"name\\": \\"GREEK SMALL LETTER BETA\\", \\"decimal\\": \\"N/A\\", \\"digit\\": \\"N/A\\", \\"numeric\\": \\"N/A\\", \\"category\\": \\"Ll\\", \\"bidirectional\\": \\"L\\", \\"combining\\": 0, \\"east_asian_width\\": \\"A\\", \\"mirrored\\": 0, \\"decomposition\\": \\"\\" }, \\"\\": { \\"name\\": \\"GREEK CAPITAL LETTER SIGMA\\", \\"decimal\\": \\"N/A\\", \\"digit\\": \\"N/A\\", \\"numeric\\": \\"N/A\\", \\"category\\": \\"Lu\\", \\"bidirectional\\": \\"L\\", \\"combining\\": 0, \\"east_asian_width\\": \\"A\\", \\"mirrored\\": 0, \\"decomposition\\": \\"\\" }, \\"normalized\\": \\"Au03B2u03A3\\" } ``` # Notes - The function should handle any kind of Unicode characters including letters, digits, symbols, etc. - The detailed properties should help to understand and compare the characters based on their Unicode attributes. - Ensure your function is well-optimized and handles different normalization forms correctly.","solution":"import unicodedata def analyze_unicode_data(unicode_str: str, normalize_form: str) -> dict: result = {} for char in unicode_str: char_data = { \\"name\\": unicodedata.name(char, \\"Unknown\\"), \\"decimal\\": unicodedata.decimal(char, \\"N/A\\") if char.isdecimal() else \\"N/A\\", \\"digit\\": unicodedata.digit(char, \\"N/A\\") if char.isdigit() else \\"N/A\\", \\"numeric\\": unicodedata.numeric(char, \\"N/A\\") if char.isnumeric() else \\"N/A\\", \\"category\\": unicodedata.category(char), \\"bidirectional\\": unicodedata.bidirectional(char), \\"combining\\": unicodedata.combining(char), \\"east_asian_width\\": unicodedata.east_asian_width(char), \\"mirrored\\": unicodedata.mirrored(char), \\"decomposition\\": unicodedata.decomposition(char) } result[char] = char_data normalized_str = unicodedata.normalize(normalize_form, unicode_str) result[\\"normalized\\"] = normalized_str return result"},{"question":"You are required to implement a Python program that interacts with the `ossaudiodev` module to play a simple audio signal. Your task is to: 1. Open an audio device. 2. Set the audio parameters (format, channels, sample rate). 3. Generate and play a sine wave for a specified duration and frequency. 4. Properly handle device closing and exceptions. Detailed Requirements 1. **Function Signature**: ```python def play_sine_wave(duration: float, frequency: float, device: str = \\"/dev/dsp\\"): ``` 2. **Input**: - `duration` (float): Duration in seconds for which the sine wave should play. - `frequency` (float): The frequency of the sine wave to be played (in Hz). - `device` (str, optional): The audio device to use. Defaults to `\\"/dev/dsp\\"`. 3. **Output**: The function does not return any value. 4. **Constraints**: - Duration should be positive. - Frequency should be a positive value. - The function should handle exceptions when accessing the audio device or setting parameters. 5. **Audio Parameters**: - Format: `AFMT_S16_LE` - Channels: 1 (mono) - Sample rate: 44100 Hz 6. **Implementation Details**: - Use the `ossaudiodev` module to open and set up the audio device. - Generate a sine wave based on the specified frequency and duration. - Convert the sine wave to the appropriate format and write the data to the device. - Ensure the audio device is closed correctly even if an error occurs. 7. **Example Usage**: ```python try: play_sine_wave(5.0, 440.0) print(\\"Sine wave played successfully\\") except Exception as e: print(f\\"An error occurred: {e}\\") ``` --- # Notes: - You may use the `numpy` library for generating the sine wave. - Utilize the context management protocol for the audio device if needed. - Ensure to include necessary error handling for I/O operations.","solution":"import ossaudiodev import numpy as np def play_sine_wave(duration: float, frequency: float, device: str = \\"/dev/dsp\\"): Plays a sine wave of given frequency for a given duration. Args: duration (float): Duration in seconds. frequency (float): Frequency of sine wave in Hz. device (str): Path to audio device. # Validate inputs if duration <= 0: raise ValueError(\\"Duration must be positive.\\") if frequency <= 0: raise ValueError(\\"Frequency must be positive.\\") sample_rate = 44100 # Standard sample rate sample_count = int(duration * sample_rate) # Generate sine wave t = np.linspace(0, duration, sample_count, endpoint=False) wave = 0.5 * np.sin(2 * np.pi * frequency * t) wave = (wave * 32767).astype(np.int16) # Convert to 16-bit PCM try: dsp = ossaudiodev.open(\'w\') dsp.setfmt(ossaudiodev.AFMT_S16_LE) dsp.channels(1) dsp.speed(sample_rate) dsp.write(wave.tobytes()) except Exception as e: raise RuntimeError(f\\"Failed to play audio: {e}\\") finally: dsp.close()"},{"question":"You are tasked with building a custom probability distribution using PyTorch\'s `torch.distributions` package. Your custom distribution should be a mixture of two different gamma distributions. Specifically, you will implement a class `CustomMixtureGamma` that models this mixture distribution. # Class: `CustomMixtureGamma` Definition Create a class `CustomMixtureGamma` that inherits from `torch.distributions.Distribution`. Initialization Method The constructor of this class should accept the following parameters: - `alpha1` (float): Shape parameter of the first gamma distribution. - `beta1` (float): Rate parameter of the first gamma distribution. - `alpha2` (float): Shape parameter of the second gamma distribution. - `beta2` (float): Rate parameter of the second gamma distribution. - `mix_prob` (float): The mixing probability of the first gamma distribution. Should be in range (0, 1). ```python class CustomMixtureGamma(Distribution): def __init__(self, alpha1: float, beta1: float, alpha2: float, beta2: float, mix_prob: float): pass ``` Methods to Implement: 1. `sample(self, sample_shape=torch.Size()):` - Returns a sample (or samples) from the mixture of gamma distributions. 2. `log_prob(self, value):` - Computes the log probability density of the value for the mixture of gamma distributions. ```python def sample(self, sample_shape=torch.Size()): pass def log_prob(self, value): pass ``` # Expected Functionality 1. **Sampling**: Implement the `sample` method where a sample is drawn from one of the two gamma distributions based on `mix_prob`. 2. **Log Probability**: Implement the `log_prob` method which computes the log probability density for given values considering the mixture model. # Example Usage ```python # Create an instance of the custom mixture gamma distribution custom_mixture = CustomMixtureGamma(alpha1=2.0, beta1=1.0, alpha2=3.0, beta2=2.0, mix_prob=0.6) # Draw samples samples = custom_mixture.sample((1000,)) # Calculate log prob of a value log_prob_value = custom_mixture.log_prob(torch.tensor(1.5)) ``` # Constraints and Requirements - The `torch.distributions` module should be used for implementing gamma distributions. - Your implementation should handle edge cases, such as negative parameter values, invalid `mix_prob`, etc., by raising appropriate exceptions. - Performance should be consideredyour sampling mechanism should efficiently handle large sample sizes. # Submission You need to submit the `CustomMixtureGamma` class implementation along with a few basic tests demonstrating the correctness of your implementation.","solution":"import torch from torch.distributions import Distribution, Gamma class CustomMixtureGamma(Distribution): def __init__(self, alpha1: float, beta1: float, alpha2: float, beta2: float, mix_prob: float): if not (0 <= mix_prob <= 1): raise ValueError(\\"mix_prob must be between 0 and 1.\\") super().__init__() self.gamma1 = Gamma(alpha1, beta1) self.gamma2 = Gamma(alpha2, beta2) self.mix_prob = mix_prob def sample(self, sample_shape=torch.Size()): shape = sample_shape if isinstance(sample_shape, torch.Size) else torch.Size(sample_shape) bernoulli_samples = torch.bernoulli(self.mix_prob * torch.ones(shape)) gamma1_samples = self.gamma1.sample(shape) gamma2_samples = self.gamma2.sample(shape) return torch.where(bernoulli_samples == 1, gamma1_samples, gamma2_samples) def log_prob(self, value): log_prob1 = self.gamma1.log_prob(value) log_prob2 = self.gamma2.log_prob(value) return torch.log(self.mix_prob * torch.exp(log_prob1) + (1 - self.mix_prob) * torch.exp(log_prob2))"},{"question":"# Python Calling Protocols and CPython Efficiency Introduction In CPython, there are two calling protocols that allow for efficient function calling: `tp_call` and vectorcall. These protocols are integral to the Python/C API and contribute to the efficiency of operation within the interpreter. Your task involves implementing a Python class that must be callable using both of these protocols. Task Implement a class `EfficientCallable` that: 1. Is callable using both `tp_call` and vectorcall protocols. 2. When called, sums its positional arguments and multiples the result by the number of keyword arguments it received. 3. Ensures that any attempt to call the object without any arguments (i.e., using `PyObject_CallNoArgs`) will return `None`. Specifications 1. **tp_call Implementation**: - The method should be called using positional and keyword arguments and should behave like `callable(*args, **kwargs)`. 2. **vectorcall Implementation**: - The method should use the vectorcall protocol for efficiency. Implement the `tp_vectorcall_offset` and set it up appropriately. 3. **Callable Behaviour**: - When the object is called, sum all positional arguments and multiply the result by the number of keyword arguments (e.g., `EfficientCallable(1, 2, 3, a=4, b=5)` results in `(1+2+3)*2 = 12`). - If called without arguments, return `None`. Constraints - Use proper memory handling and ensure there are no memory leaks. - The solution should leverage CPython APIs for the implementation. Example: ```python from efficient_callable_module import EfficientCallable # Example usage ec = EfficientCallable() # Called with positional and keyword arguments print(ec(1, 2, 3, a=4, b=5)) # Output should be (1+2+3)*2 = 12 # Called with no arguments print(ec()) # Output should be None ``` This problem requires a combination of implementing C functions and extending the Python API to ensure the class correctly implements both \\"tp_call\\" and vectorcall protocols. Submission Submit your implementation of `EfficientCallable` in a file named `efficient_callable.c`. Along with your C code, include a README.txt file explaining how to compile and test your implementation.","solution":"class EfficientCallable: def __call__(self, *args, **kwargs): if not args and not kwargs: return None sum_args = sum(args) result = sum_args * len(kwargs) return result"},{"question":"**Problem Statement:** You are provided with two datasets in the form of pandas DataFrames: `df_sales` and `df_targets`. - `df_sales` captures the sales data for different products in various regions over several months. - `df_targets` captures the target sales data for the same products and regions over the same period. Your task is to analyze these datasets to calculate the performance of each product in each region, on a monthly basis. You need to perform the following steps: 1. **Load the DataFrames**: Assume the DataFrames are already loaded with the required data. - `df_sales`: columns - [\'Region\', \'Product\', \'Month\', \'Sales\'] - `df_targets`: columns - [\'Region\', \'Product\', \'Month\', \'Target\'] 2. **Calculate the performance**: This is defined as the percentage of sales achieved relative to the target. Create a new DataFrame `df_performance` with columns [\'Region\', \'Product\', \'Month\', \'Performance\']. 3. **Filter and sort the data**: - Filter out the entries where either sales or targets are missing. - Sort the resulting DataFrame first by \'Region\', then by \'Product\', and finally by \'Month\'. 4. **Find the best and worst-performing months for each product in each region**: For this step, - Each product in each region should have two new columns - \'Best_month\' and \'Worst_month\', indicating the respective months. - Add these columns to `df_performance`. 5. **Output the DataFrame**: The final DataFrame should look like this: ``` Region Product Month Performance Best_month Worst_month ``` # Function Signature ```python import pandas as pd def analyze_sales_performance(df_sales: pd.DataFrame, df_targets: pd.DataFrame) -> pd.DataFrame: This function calculates the sales performance for each product in each region on a monthly basis, sorts the data, finds the best and worst-performing months, and returns the final DataFrame. Parameters: - df_sales (pd.DataFrame): Sales data. - df_targets (pd.DataFrame): Target sales data. Returns: - pd.DataFrame: Final DataFrame with performance data, sorted and with best and worst-performing months. # Step 1: Merge DataFrames on \'Region\', \'Product\', and \'Month\' df_merged = pd.merge(df_sales, df_targets, on=[\'Region\', \'Product\', \'Month\'], how=\'inner\') # Step 2: Calculate performance df_merged[\'Performance\'] = (df_merged[\'Sales\'] / df_merged[\'Target\']) * 100 # Step 3: Filter out rows with missing values df_filtered = df_merged.dropna(subset=[\'Sales\', \'Target\']) # Step 4: Sort the DataFrame df_sorted = df_filtered.sort_values(by=[\'Region\', \'Product\', \'Month\']) # Step 5: Find the best and worst-performing months for each product in each region df_sorted[\'Best_month\'] = df_sorted.groupby([\'Region\', \'Product\'])[\'Performance\'].transform(lambda x: x.idxmax()) df_sorted[\'Worst_month\'] = df_sorted.groupby([\'Region\', \'Product\'])[\'Performance\'].transform(lambda x: x.idxmin()) # Step 6: Select and reorder the required columns df_final = df_sorted[[\'Region\', \'Product\', \'Month\', \'Performance\', \'Best_month\', \'Worst_month\']] return df_final ``` # Constraints - You can assume that the \'Month\' column is sorted from oldest to newest within each \'Region\' and \'Product\'. - Handle missing data by excluding rows where either sales or targets are NaN. - The function should be efficient enough to handle large datasets with hundreds of regions and products spanning several years. # Example Input Assume `df_sales` and `df_targets` are provided DataFrames as described above. # Example Output The output will be a pandas DataFrame with the performance data, best and worst months, and sorted appropriately.","solution":"import pandas as pd def analyze_sales_performance(df_sales: pd.DataFrame, df_targets: pd.DataFrame) -> pd.DataFrame: This function calculates the sales performance for each product in each region on a monthly basis, sorts the data, finds the best and worst-performing months, and returns the final DataFrame. Parameters: - df_sales (pd.DataFrame): Sales data. - df_targets (pd.DataFrame): Target sales data. Returns: - pd.DataFrame: Final DataFrame with performance data, sorted and with best and worst-performing months. # Step 1: Merge DataFrames on \'Region\', \'Product\', and \'Month\' df_merged = pd.merge(df_sales, df_targets, on=[\'Region\', \'Product\', \'Month\'], how=\'inner\') # Step 2: Calculate performance df_merged[\'Performance\'] = (df_merged[\'Sales\'] / df_merged[\'Target\']) * 100 # Step 3: Filter out rows with missing values df_filtered = df_merged.dropna(subset=[\'Sales\', \'Target\']) # Step 4: Sort the DataFrame df_sorted = df_filtered.sort_values(by=[\'Region\', \'Product\', \'Month\']) # Step 5: Find the best and worst-performing months for each product in each region best_months = df_sorted.loc[df_sorted.groupby([\'Region\', \'Product\'])[\'Performance\'].idxmax()] worst_months = df_sorted.loc[df_sorted.groupby([\'Region\', \'Product\'])[\'Performance\'].idxmin()] best_months = best_months[[\'Region\', \'Product\', \'Month\']].rename(columns={\'Month\': \'Best_month\'}) worst_months = worst_months[[\'Region\', \'Product\', \'Month\']].rename(columns={\'Month\': \'Worst_month\'}) df_final = df_sorted.merge(best_months, on=[\'Region\', \'Product\'], how=\'left\') df_final = df_final.merge(worst_months, on=[\'Region\', \'Product\'], how=\'left\') # Step 6: Select and reorder the required columns df_final = df_final[[\'Region\', \'Product\', \'Month\', \'Performance\', \'Best_month\', \'Worst_month\']] return df_final"},{"question":"**Title**: Create a Multi-Window Text-Based To-Do List Application Using `curses` **Objective**: Implement a text-based to-do list application using the `curses` module in Python. The application will allow users to add, view, and manage to-do items in a terminal-based interface. This exercise will test your understanding of window management, handling user input, displaying text with attributes, and application lifecycle management with the `curses` library. **Task Description**: 1. Your application should initialize the `curses` environment and create two windows: - A main window that lists the to-do items. - A status window at the bottom that displays user prompts and status messages. 2. The main window should: - Display the to-do items, each on a new line. - Highlight the currently selected item for easy identification. 3. The status window should: - Show usage instructions, current mode, or error messages. 4. The application should handle the following user inputs: - `a` to add a to-do item (prompt user to enter text in the status window). - `d` to delete the currently selected to-do item. - Arrow keys to navigate up and down the list. - `q` to quit the application, ensuring the terminal state is properly restored. **Constraints**: - Your implementation must handle the terminal state restoration even if an uncaught exception occurs. - You should not use any external libraries outside of `curses`. **Input**: - User keystrokes to interact with the to-do list application. **Output**: - The terminal display updated based on user interaction showing the list of to-do items and status messages. **Performance Requirements**: - The application should run efficiently without lag when updating the display or handling user input. # Example Usage ```python from curses import wrapper import curses def main(stdscr): # Initialize curses environment curses.cbreak() curses.noecho() stdscr.keypad(True) # Create windows height, width = stdscr.getmaxyx() main_win = curses.newwin(height - 3, width, 0, 0) status_win = curses.newwin(3, width, height - 3, 0) # Initialize variables todos = [] current_idx = 0 def display_todos(): main_win.clear() for idx, todo in enumerate(todos): attr = curses.A_REVERSE if idx == current_idx else curses.A_NORMAL main_win.addstr(idx, 0, todo, attr) main_win.refresh() def update_status(message): status_win.clear() status_win.addstr(0, 0, message) status_win.refresh() # Main loop to capture input and update display while True: display_todos() key = stdscr.getch() if key == ord(\'a\'): update_status(\\"Enter new to-do item:\\") curses.echo() new_todo = status_win.getstr(1, 0).decode(\'utf-8\') curses.noecho() if new_todo: todos.append(new_todo) update_status(f\\"Added: \'{new_todo}\'\\") else: update_status(\\"No item added.\\") elif key == ord(\'d\') and todos: deleted = todos.pop(current_idx) update_status(f\\"Deleted: \'{deleted}\'\\") current_idx = max(0, current_idx - 1) elif key == curses.KEY_UP and current_idx > 0: current_idx -= 1 elif key == curses.KEY_DOWN and current_idx < len(todos) - 1: current_idx += 1 elif key == ord(\'q\'): break # Cleanup before exit curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() wrap(main) ``` Please implement the above `main` function to complete this application. Follow the described behavior and ensure all parts are implemented correctly. You should be able to handle window creation, keystrokes, and text display with attributes effectively. **Note**: Conduct thorough testing to ensure all user interactions work flawlessly.","solution":"import curses def to_do_list_app(stdscr): # Initialize curses environment curses.cbreak() curses.noecho() stdscr.keypad(True) # Create windows height, width = stdscr.getmaxyx() main_win = curses.newwin(height - 3, width, 0, 0) status_win = curses.newwin(3, width, height - 3, 0) # Initialize variables todos = [] current_idx = 0 def display_todos(): main_win.clear() for idx, todo in enumerate(todos): attr = curses.A_REVERSE if idx == current_idx else curses.A_NORMAL main_win.addstr(idx, 0, todo, attr) main_win.refresh() def update_status(message): status_win.clear() status_win.addstr(0, 0, message) status_win.refresh() # Initial status update update_status(\\"Press \'a\' to add, \'d\' to delete, arrows to navigate, \'q\' to quit.\\") # Main loop to capture input and update display while True: display_todos() key = stdscr.getch() if key == ord(\'a\'): update_status(\\"Enter new to-do item:\\") curses.echo() new_todo = status_win.getstr(1, 0).decode(\'utf-8\') curses.noecho() if new_todo: todos.append(new_todo) update_status(f\\"Added: \'{new_todo}\'. Press \'a\' to add, \'d\' to delete, arrows to navigate, \'q\' to quit.\\") else: update_status(\\"No item added. Press \'a\' to add, \'d\' to delete, arrows to navigate, \'q\' to quit.\\") elif key == ord(\'d\') and todos: deleted = todos.pop(current_idx) update_status(f\\"Deleted: \'{deleted}\'. Press \'a\' to add, \'d\' to delete, arrows to navigate, \'q\' to quit.\\") current_idx = max(0, current_idx - 1) elif key == curses.KEY_UP and current_idx > 0: current_idx -= 1 elif key == curses.KEY_DOWN and current_idx < len(todos) - 1: current_idx += 1 elif key == ord(\'q\'): break # Cleanup before exit curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin()"},{"question":"# Nested Tensors and Variable-Length Sequence Processing You have been tasked with implementing a function that processes batches of variable-length sequences using PyTorch\'s nested tensor functionality. Your goal is to implement a function that takes a list of 1D tensors, performs an element-wise operation (e.g., adding 1 to each element), and returns the result as a padded tensor, filling with a specified padding value. Function Signature: ```python import torch def process_variable_length_sequences(tensor_list: list, padding_value: float) -> torch.Tensor: Processes a list of 1D tensors, applies an element-wise operation (adding 1), and returns a padded tensor. Args: tensor_list (list): List of 1D torch tensors with variable lengths. padding_value (float): The value used for padding the resulting tensor. Returns: torch.Tensor: A 2D tensor with padding, containing the processed sequences. pass ``` Expected Input and Output: - **Input**: - `tensor_list`: A list of 1D tensors with variable lengths. For example, `[torch.tensor([1, 2, 3]), torch.tensor([4, 5])]`. - `padding_value`: A float value specifying the padding value for the resulting padded tensor. - **Output**: - A 2D tensor where each row corresponds to a processed sequence from the input list, padded to the length of the longest sequence with `padding_value`. Example: ```python tensor_list = [torch.tensor([1, 2, 3]), torch.tensor([4, 5])] padding_value = -1.0 result = process_variable_length_sequences(tensor_list, padding_value) print(result) # Expected Output: # tensor([[ 2, 3, 4], # [ 5, 6, -1]]) ``` Constraints: - All tensors in `tensor_list` are 1D tensors. - Tensors may have different lengths. Instructions: 1. Utilize the `torch.nested.nested_tensor` function to create a nested tensor from the input list. 2. Perform the element-wise operation (adding 1 to each element). 3. Convert the resulting nested tensor to a padded tensor using `torch.nested.to_padded_tensor`. 4. Return the padded tensor. Note: Ensure that your implementation efficiently handles the different lengths and takes advantage of the nested tensor capabilities provided by PyTorch.","solution":"import torch def process_variable_length_sequences(tensor_list: list, padding_value: float) -> torch.Tensor: Processes a list of 1D tensors, applies an element-wise operation (adding 1), and returns a padded tensor. Args: tensor_list (list): List of 1D torch tensors with variable lengths. padding_value (float): The value used for padding the resulting tensor. Returns: torch.Tensor: A 2D tensor with padding, containing the processed sequences. # Convert the list of 1D tensors into a nested tensor nested_tensor = torch.nested.nested_tensor(tensor_list) # Perform element-wise operation (adding 1) nested_tensor = nested_tensor + 1 # Determine the maximum length of the tensors in the list max_len = max(tensor.size(0) for tensor in tensor_list) # Convert the nested tensor to a padded tensor and return it padded_tensor = torch.nested.to_padded_tensor(nested_tensor, padding_value) return padded_tensor"},{"question":"Objective Implement a Python class that dynamically processes and yields data based on various input expressions. The class should handle comprehensions, literals, expressions with variable references, and class attributes with private name manglings. Function Implementation You need to implement a class `DynamicProcessor` with the following methods: 1. **`__init__(self, data)`**: - Initialize the instance with a dictionary `data` that consists of variable names and their values. - Example: `{\'a\': 5, \'b\': 10, \'_ClassName__c\': 20}`. 2. **`process_expression(self, expression_str)`**: - This method takes a string `expression_str` representing a Python expression. - Parse and evaluate the expression string using the class\'s data and return the result. - Handle comprehensions, arithmetic operations, literals, method calls, attribute references, and private name manglings. - Raise appropriate errors if the expression is invalid or if there are name errors. 3. **`generate_values(self)`**: - Implement this as a generator that yields values based on processed expressions. - Use the previously defined `process_expression` method to evaluate expressions dynamically. - Handle stopping conditions gracefully using appropriate generators or iterators. Constraints - Do not use `eval` or `exec` for evaluating the expressions. - Handle invalid expressions and name errors properly, raising appropriate exceptions. - The expressions may reference class attributes which might include private names that should be handled correctly. Example Usage: ```python data = { \'a\': 5, \'b\': 10, \'_MyClass__c\': 20, # Expected private name mangling } processor = DynamicProcessor(data) expression = \'a + b * 2\' print(processor.process_expression(expression)) # Output: 25 expression = \'[x * 2 for x in range(a)]\' print(processor.process_expression(expression)) # Output: [0, 2, 4, 6, 8] expression = \'_MyClass__c - a + b\' print(processor.process_expression(expression)) # Output: 25 generator = processor.generate_values() values = list(generator) # Implement how this generator would be used ``` Notes: - Ensure robust testing for corner cases, including handling arbitrary expressions, appropriate error handling, and private names transformation. - Consider performance implications if evaluating complex comprehensions or expressions involving numerous iterations or large datasets.","solution":"import ast class DynamicProcessor: def __init__(self, data): Initialize the instance with a dictionary `data`. self.data = data def process_expression(self, expression_str): Takes a string `expression_str` representing a Python expression. Parses and evaluates the expression string using the class\'s data and returns the result. try: parsed_expr = ast.parse(expression_str, mode=\'eval\') code_object = compile(parsed_expr, \'<string>\', \'eval\') return eval(code_object, {}, self.data) except NameError as e: raise NameError(f\\"Name error in expression: {expression_str}. Error: {str(e)}\\") except Exception as e: raise ValueError(f\\"Invalid expression: {expression_str}. Error: {str(e)}\\") def generate_values(self): Generator that yields processed expressions\' values dynamically. for key, expression in self.data.items(): yield self.process_expression(expression)"},{"question":"**Question: Implement a Simple Python REPL using `codeop` Module** Your task is to implement a simple Python Read-Eval-Print Loop (REPL) using the `codeop` module. The REPL should read lines of input from the user, determine if the input completes a valid Python statement, and execute it if it does. The REPL should handle multi-line statements and remember `__future__` import statements, applying them to subsequent lines of code. # Input Format: 1. Lines of valid Python code or incomplete statements. 2. Input ends when the user types \\"exit\\". # Output Format: The output should include: 1. Evaluation result of expressions. 2. Printed output from executed statements. 3. Appropriate error messages for invalid Python code. # Constraints: 1. The REPL should detect and handle multi-line statements correctly, providing the correct prompt (`>>>` or `...`). 2. Handle `__future__` statements correctly. Once a future statement is entered, it should affect all subsequent code compilations. 3. It should handle exceptions and display error messages appropriately. # Example Input/Output: **Example Input:** ```python >>> from __future__ import print_function >>> print(\\"Hello, world!\\") Hello, world! >>> a = 10 >>> if a > 5: ... print(\\"a is greater than 5\\") ... a is greater than 5 >>> exit ``` **Example Output:** ```shell >>> >>> Hello, world! >>> >>> a is greater than 5 >>> ``` # Your implementation should follow this template: ```python import codeop def simple_repl(): command_compiler = codeop.CommandCompiler() input_lines = [] while True: prompt = \'>>> \' if not input_lines else \'... \' try: line = input(prompt) if line.strip() == \'exit\': break input_lines.append(line) source = \'n\'.join(input_lines) code = command_compiler(source) if code: exec(code, globals()) input_lines = [] except Exception as e: print(f\\"Error: {e}\\") input_lines = [] if __name__ == \\"__main__\\": simple_repl() ```","solution":"import codeop def simple_repl(): command_compiler = codeop.CommandCompiler() input_lines = [] import_statements = \\"\\" while True: prompt = \'>>> \' if not input_lines else \'... \' try: line = input(prompt) if line.strip() == \'exit\': break input_lines.append(line) source = \'n\'.join(input_lines) code = command_compiler(source, filename=\\"<input>\\", symbol=\\"exec\\") if code: try: exec(import_statements + \\"n\\" + source, globals()) except Exception as e: print(f\\"Error: {e}\\") input_lines = [] if \'__future__\' in source: import_statements += source + \\"n\\" except Exception as e: print(f\\"Error: {e}\\") input_lines = [] if __name__ == \\"__main__\\": simple_repl()"},{"question":"# Coding Assessment: Advanced Usage of Seaborn `objects` Interface Objective Create a visualization using seaborn\'s `objects` interface that demonstrates your ability to: 1. Load and examine datasets. 2. Use faceting to create separate subplots. 3. Add multiple layers of data representation to your plot. 4. Customize the appearance of your plot using theme settings. Problem You are provided with a dataset called `anscombe`. Your task is to create a faceted plot using this dataset. Follow these steps: 1. **Load the dataset**: ```python import seaborn.objects as so from seaborn import load_dataset anscombe = load_dataset(\\"anscombe\\") ``` 2. **Create a faceted plot**: - Use the `x` and `y` columns for the axes. - Facet the plot by the `dataset` column, wrapping the facets into 2 columns. - Add a linear regression line to each facet using a polynomial fit of order 1. - Overlay the points on the same plot. 3. **Customize the plot\'s appearance**: - Change the background color of the axes to white. - Change the color of the axes edges to `slategray`. - Set the linewidth of lines in the plot to 4. - Use the `ticks` style for the axes. - Apply the `fivethirtyeight` style from matplotlib. 4. **Update and apply a global theme configuration to `Plot`**: - Set the theme globally for all `Plot` instances to use the seaborn `whitegrid` style and the `talk` context. Expected Input None. The data should be loaded directly from seaborn\'s dataset collection. Expected Output A faceted plot saved as an image file (e.g., `output.png`). Constraints - You should not use any other data visualization libraries except seaborn and matplotlib. - Ensure the code is well-commented and organized. Example Solution ```python import seaborn.objects as so from seaborn import load_dataset from seaborn import axes_style, plotting_context from matplotlib import style # Load dataset anscombe = load_dataset(\\"anscombe\\") # Create faceted plot p = ( so.Plot(anscombe, \\"x\\", \\"y\\", color=\\"dataset\\") .facet(\\"dataset\\", wrap=2) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Customize plot appearance p.theme({ \\"axes.facecolor\\": \\"white\\", \\"axes.edgecolor\\": \\"slategray\\", \\"lines.linewidth\\": 4 }) p.theme(axes_style(\\"ticks\\")) p.theme(style.library[\\"fivethirtyeight\\"]) # Update and apply global theme so.Plot.config.theme.update(axes_style(\\"whitegrid\\") | plotting_context(\\"talk\\")) # Display the plot p.show() # Save the plot p.save(\\"output.png\\") ``` Note You can use Jupyter notebooks or any other Python environment to write and test your code.","solution":"import seaborn.objects as so from seaborn import load_dataset from seaborn import axes_style, plotting_context from matplotlib import style # Load the dataset anscombe = load_dataset(\\"anscombe\\") # Create faceted plot p = ( so.Plot(anscombe, \\"x\\", \\"y\\", color=\\"dataset\\") .facet(\\"dataset\\", wrap=2) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Customize plot appearance p.theme({ \\"axes.facecolor\\": \\"white\\", \\"axes.edgecolor\\": \\"slategray\\", \\"lines.linewidth\\": 4 }) p.theme(axes_style(\\"ticks\\")) p.theme(style.library[\\"fivethirtyeight\\"]) # Update and apply global theme so.Plot.config.theme.update(axes_style(\\"whitegrid\\") | plotting_context(\\"talk\\")) # Display the plot p.show() # Save the plot p.save(\\"output.png\\")"},{"question":"Coding Assessment Question # Objective You are required to create a comprehensive Seaborn plot that demonstrates your understanding of the layered plotting interface in Seaborn using the `so.Plot`, various `so.Mark`, and transformations. # Task Using the Seaborn library and the `tips` dataset, create a plot that meets the following criteria: 1. **Data and Basic Plot**: - Load the `tips` dataset. - Create a basic dot plot with `total_bill` on the x-axis and `tip` on the y-axis. 2. **Add Multiple Layers**: - Add a line that represents the polynomial fit. - Add another layer with bars, using `Hist` and `Dodge` transformations. 3. **Different Orientation and Variables**: - Add a horizontally oriented layer that shows the total bill grouped by \'day\' and colored by \'time\'. Use jitter to spread data points. 4. **Custom Layer with Exclusions**: - Add a layer that includes data points for parties of size 2 but exclude the color variable for this layer. 5. **Separate Data Source for a Layer**: - Include another layer that specifically showcases data for male customers, using different color for gender. 6. **Labels and Annotations**: - Provide labels for the lines representing the total bill and tips. # Constraints - Ensure that the plot is annotated with an appropriate legend. - The plot should handle variable exclusions appropriately. - Use appropriate transformations to visualize the different aspects of the dataset effectively. # Input and Output - The expected output is a layered plot object created using Seaborn. # Performance Requirements - Ensure the plot is rendered efficiently without excessive computation time. ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Create the base plot plot = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\") # Add a line representing polynomial fit plot.add(so.Line(), so.PolyFit()) # Add a layer with bars using Hist and Dodge plot.add(so.Bar(), so.Hist(), so.Dodge()) # Add a horizontally oriented layer for total bill by day, colored by time plot.add(so.Dot(alpha=0.5), so.Dodge(), so.Jitter(0.4), orient=\\"h\\") # Add a layer for parties of size 2, excluding color variable for this layer plot.add(so.Dot(), data=tips.query(\\"size == 2\\")) # Add a layer for male customers, using different color for gender plot.add(so.Dot(), data=tips.query(\\"sex == \'Male\'\\"), color=\\"sex\\") # Provide labels for lines in the legend plot.add(so.Line(color=\\"C1\\"), so.Agg(), y=\\"total_bill\\", label=\\"Total Bill\\") plot.add(so.Line(color=\\"C2\\"), so.Agg(), y=\\"tip\\", label=\\"Tip\\") # Add labels to the plot plot.label(y=\\"Value\\") # Display the plot plot ```","solution":"import seaborn.objects as so from seaborn import load_dataset def create_layered_plot(): # Load the dataset tips = load_dataset(\\"tips\\") # Create the base plot plot = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\") # Add a line representing polynomial fit plot.add(so.Line(), so.PolyFit()) # Add a layer with bars using Hist and Dodge plot.add(so.Bar(), so.Hist(), so.Dodge()) # Add a horizontally oriented layer for total bill by day, colored by time plot.add(so.Dot(alpha=0.5), so.Dodge(), so.Jitter(0.4), orient=\\"h\\") # Add a layer for parties of size 2, excluding color variable for this layer plot.add(so.Dot(), data=tips.query(\\"size == 2\\")) # Add a layer for male customers, using different color for gender plot.add(so.Dot(), data=tips.query(\\"sex == \'Male\'\\"), color=\\"sex\\") # Provide labels for lines in the legend plot.add(so.Line(color=\\"C1\\"), so.Agg(), y=\\"total_bill\\", label=\\"Total Bill\\") plot.add(so.Line(color=\\"C2\\"), so.Agg(), y=\\"tip\\", label=\\"Tip\\") # Add labels to the plot plot.label(y=\\"Value\\") return plot"},{"question":"# Path Analysis and Manipulation with `pathlib` You are tasked with creating a script to perform various filesystem operations using the `pathlib` module in Python. Your script should implement a function that processes paths to achieve the following: 1. **File Listing and Filtering:** - List all subdirectories within a specified directory. - List all Python source files (`.py` extension) within the specified directory and its subdirectories. 2. **Path Manipulation:** - Given a base path and a list of path segments, construct a complete path and return its absolute version. - Given a path, extract and return the following components: drive, root, anchor, parent, name, suffix, and stem. 3. **File Validation:** - Check if a specified path is a directory. - Check if a specified path is a file. - Check if a specified path is an absolute path. # Function Signature ```python from pathlib import Path from typing import List, Tuple, Dict, Union def process_paths(base_path: Union[str, Path], path_segments: List[str], file_path: Union[str, Path]) -> Dict[str, Union[List[Path], Dict[str, str], bool]]: Process and analyze filesystem paths using the pathlib module. Parameters: - base_path (str or Path): The base directory path to perform operations in. - path_segments (List[str]): A list of path segments to construct a complete path. - file_path (str or Path): The path to a file or directory for validation and extraction. Returns: - result (Dict[str, Union[List[Path], Dict[str, str], bool]]): A dictionary with the following keys: - \'subdirectories\': A list of subdirectories in the base path. - \'python_files\': A list of Python source files in the base path and its subdirectories. - \'constructed_path\': The absolute version of the constructed path. - \'path_components\': A dictionary with keys \'drive\', \'root\', \'anchor\', \'parent\', \'name\', \'suffix\', and \'stem\', representing the respective components of the path. - \'is_directory\': Boolean indicating if the file_path is a directory. - \'is_file\': Boolean indicating if the file_path is a file. - \'is_absolute\': Boolean indicating if the file_path is an absolute path. pass ``` # Example Usage ```python if __name__ == \\"__main__\\": result = process_paths(\'/my/base/directory\', [\'subdir1\', \'subdir2\', \'file.txt\'], \'/my/base/directory/subdir1/file.py\') print(result) ``` # Expected Output ```python { \'subdirectories\': [PosixPath(\'/my/base/directory/subdir1\'), PosixPath(\'/my/base/directory/subdir2\')], \'python_files\': [PosixPath(\'/my/base/directory/subdir1/file1.py\'), PosixPath(\'/my/base/directory/subdir2/file2.py\')], \'constructed_path\': PosixPath(\'/my/base/directory/subdir1/subdir2/file.txt\'), \'path_components\': { \'drive\': \'/\', \'root\': \'/\', \'anchor\': \'/\', \'parent\': \'/my/base/directory/subdir1\', \'name\': \'file.txt\', \'suffix\': \'.txt\', \'stem\': \'file\' }, \'is_directory\': True, \'is_file\': False, \'is_absolute\': True } ``` # Constraints - The input paths should be valid paths within the filesystem. - Assume that the file and directories exist for the purpose of this function.","solution":"from pathlib import Path from typing import List, Tuple, Dict, Union def process_paths(base_path: Union[str, Path], path_segments: List[str], file_path: Union[str, Path]) -> Dict[str, Union[List[Path], Dict[str, str], bool]]: base_path = Path(base_path) file_path = Path(file_path) # List subdirectories subdirectories = [item for item in base_path.iterdir() if item.is_dir()] # List Python source files python_files = list(base_path.rglob(\'*.py\')) # Constructed path constructed_path = base_path.joinpath(*path_segments).resolve() # Path components path_components = { \'drive\': file_path.drive, \'root\': file_path.root, \'anchor\': file_path.anchor, \'parent\': str(file_path.parent), \'name\': file_path.name, \'suffix\': file_path.suffix, \'stem\': file_path.stem } # File validation is_directory = file_path.is_dir() is_file = file_path.is_file() is_absolute = file_path.is_absolute() return { \'subdirectories\': subdirectories, \'python_files\': python_files, \'constructed_path\': constructed_path, \'path_components\': path_components, \'is_directory\': is_directory, \'is_file\': is_file, \'is_absolute\': is_absolute }"},{"question":"Objective Write a function `ragged_sum` that takes a nested tensor with jagged layout (`torch.jagged`) and returns a nested tensor with the sum of its initial tensor components and another nested tensor of the same shape. Description 1. **Function Signature**: ```python def ragged_sum(nt1: torch.Tensor, nt2: torch.Tensor) -> torch.Tensor: ``` 2. **Inputs**: - `nt1` (torch.Tensor): A nested tensor with jagged layout. - `nt2` (torch.Tensor): A nested tensor with jagged layout, having the same shape structure and ragged dimensions as `nt1`. 3. **Outputs**: - Returns a nested tensor with jagged layout, representing the element-wise sum of `nt1` and `nt2`. 4. **Constraints**: - You must handle cases where nested tensors have different layouts or invalid structures gracefully, raising appropriate errors. - Ensure the resulting tensor maintains the same ragged shape structure as input tensors without materializing padding. 5. **Performance Requirements**: - Aim for optimal performance by leveraging PyTorch\'s efficient handling of contiguous memory and avoiding unnecessary data copying. Example ```python import torch # Define two nested tensors with jagged layout a = torch.tensor([1, 2, 3]) b = torch.tensor([4, 5]) nt1 = torch.nested.nested_tensor([a, b], layout=torch.jagged) c = torch.tensor([6, 7, 8]) d = torch.tensor([9, 10]) nt2 = torch.nested.nested_tensor([c, d], layout=torch.jagged) # Compute the sum of nested tensors result = ragged_sum(nt1, nt2) # Expected Result: Nested tensor with components [tensor([7, 9, 11]), tensor([13, 15])] print([component for component in result]) ``` **Notes**: - The nested tensors `nt1` and `nt2` are compatible if and only if they have the same offsets tensor. - You may assume that the inputs will always be valid nested tensors with the `torch.jagged` layout for the scope of this question. Submission Implement the function `ragged_sum` in a way that demonstrates your understanding of nested tensors in PyTorch, including their construction, operations, and appropriate error handling.","solution":"import torch def ragged_sum(nt1, nt2): Returns a nested tensor with the sum of two nested jagged tensors. Args: nt1: List of tensors, where each tensor can have different lengths. nt2: List of tensors in the same nested jagged format as nt1. Returns: A list of tensors with the element-wise sum of nt1 and nt2. Raises: ValueError: If nt1 and nt2 do not have the same structure. if len(nt1) != len(nt2): raise ValueError(\\"Input nested tensors must have the same number of elements.\\") result = [] for t1, t2 in zip(nt1, nt2): if t1.shape != t2.shape: raise ValueError(\\"All corresponding tensors must have the same shape.\\") result.append(t1 + t2) return result"},{"question":"# Problem: Boolean Masking and Kleene Logic Operations You are given a DataFrame that contains mixed data, including some boolean arrays with nullable values. Your task is to perform several operations using pandas, demonstrating your understanding of nullable Boolean dtypes, indexing, and Kleene logic. Input - A DataFrame with the following columns: - \'A\': An integer column. - \'B\': A nullable boolean array with some NA values. - \'C\': A standard boolean column (without NA values). Example DataFrame: ```python import pandas as pd import numpy as np data = { \'A\': [10, 20, 30, 40, 50], \'B\': pd.array([True, pd.NA, False, True, pd.NA], dtype=\'boolean\'), \'C\': [False, True, True, False, True] } df = pd.DataFrame(data) ``` Tasks 1. **Indexing with Boolean Masks:** Create a new Series from column \'A\' that only includes values where column \'B\' is True, treating NA as False. 2. **Filling NA Values:** Create another Series from column \'A\' where NA values in column \'B\' should be treated as True instead. 3. **Kleene Logic Operation - AND:** Compute a new Series that results from the AND operation between columns \'B\' and \'C\', following the rules of Kleene logic. 4. **Kleene Logic Operation - OR:** Compute another Series from the OR operation between columns \'B\' and \'C\', following the rules of Kleene logic. Output Return the results of the four tasks in order as a tuple of pandas Series. Example Output: ```python (result1, result2, result3, result4) ``` Constraints - None of the input data structures will be empty. - Assume the input DataFrame will always contain the required columns \'A\', \'B\', and \'C\'. - Ensure your solution is efficient in terms of performance. Implementation ```python def boolean_operations(df): # Task 1: Indexing with Boolean Masks result1 = df[\'A\'][df[\'B\'].fillna(False)] # Task 2: Filling NA Values result2 = df[\'A\'][df[\'B\'].fillna(True)] # Task 3: Kleene Logic AND Operation result3 = df[\'B\'] & df[\'C\'] # Task 4: Kleene Logic OR Operation result4 = df[\'B\'] | df[\'C\'] return (result1, result2, result3, result4) ```","solution":"import pandas as pd def boolean_operations(df): # Task 1: Indexing with Boolean Masks result1 = df[\'A\'][df[\'B\'].fillna(False)] # Task 2: Filling NA Values result2 = df[\'A\'][df[\'B\'].fillna(True)] # Task 3: Kleene Logic AND Operation result3 = df[\'B\'] & df[\'C\'] # Task 4: Kleene Logic OR Operation result4 = df[\'B\'] | df[\'C\'] return (result1.reset_index(drop=True), result2.reset_index(drop=True), result3.reset_index(drop=True), result4.reset_index(drop=True))"},{"question":"Objective: Implement a function that computes a composite mathematical expression using multiple functions from the `torch.special` module of PyTorch. Problem Statement: Write a function `composite_special_function` that takes a 1-dimensional tensor `x` as input and returns a new tensor. The function should execute the following operations: 1. Compute the sigmoid function of `x` using `torch.special.expit`. 2. Compute the digamma function of the result from step 1 using `torch.special.digamma`. 3. Compute the Bessel function of the first kind (order 0) of the result from step 2 using `torch.special.bessel_j0`. 4. Finally, return the result as the output tensor. Function Signature: ```python def composite_special_function(x: torch.Tensor) -> torch.Tensor: pass ``` Input: - A 1-dimensional tensor `x` of arbitrary length. Output: - A 1-dimensional tensor of the same length as `x`. Constraints: - You may assume that all elements in the tensor `x` will be within the domain of the functions being used. - You must use the functions provided by `torch.special` for each step. Example: ```python import torch import torch.special # Example input tensor x = torch.tensor([0.5, 1.0, 1.5, 2.0]) # Call the function result = composite_special_function(x) # Example output tensor print(result) # The output should be the final tensor after applying all the steps described above. ``` Notes: - Ensure your code handles tensors properly and leverages PyTorch\'s tensor operations. - The implementation must utilize the `torch.special` library functions as specified for each step. Good luck!","solution":"import torch import torch.special def composite_special_function(x: torch.Tensor) -> torch.Tensor: Compute a composite mathematical expression on the input tensor using multiple functions from the torch.special module. The steps are: 1. Compute the sigmoid function of x using torch.special.expit. 2. Compute the digamma function of the result using torch.special.digamma. 3. Compute the Bessel function of the first kind (order 0) of the result using torch.special.bessel_j0. 4. Return the final result. step1 = torch.special.expit(x) # Sigmoid function step2 = torch.special.digamma(step1) # Digamma function result = torch.special.bessel_j0(step2) # Bessel function of the first kind (order 0) return result"},{"question":"**Audio Data Manipulation with `audioop`** You have been provided with raw audio data from a stereo recording that needs to be processed into a mono format and then encoded into a specified format. # Task 1. **Convert the given stereo audio fragment into mono.** 2. **Apply a bias to the mono audio fragment.** 3. **Convert the biased mono audio fragment into A-LAW encoded format.** You are required to implement a function `process_audio(stereo_fragment: bytes, width: int, l_factor: float, r_factor: float, bias: int) -> bytes` which performs the operations in sequence and returns the final A-LAW encoded audio data. # Function Signature ```python def process_audio(stereo_fragment: bytes, width: int, l_factor: float, r_factor: float, bias: int) -> bytes: ``` # Parameters - `stereo_fragment` (bytes): A bytes object containing the raw stereo audio data. - `width` (int): The sample width in bytes (either 1, 2, 3, or 4). - `l_factor` (float): The multiplicative factor for the left channel. - `r_factor` (float): The multiplicative factor for the right channel. - `bias` (int): The bias value to apply to the mono audio fragment. # Constraints - The input stereo fragment will be a valid bytes-like object with signed integer samples. - The function should handle fragments of varying sample widths (1, 2, 3, or 4 bytes). - The length of `stereo_fragment` will be a multiple of `2 * width`. - The operation should be efficiently done to handle large audio data. # Example ```python stereo_fragment = b\'x01x00x02x00x01x01x02x01\' # Example of stereo fragment with 16-bit samples width = 2 l_factor = 1.0 r_factor = 1.0 bias = 128 result = process_audio(stereo_fragment, width, l_factor, r_factor, bias) print(result) # Should output the A-LAW encoded bytes ``` # Notes - Convert the stereo fragment to mono by considering the left and right channels and combining them with the given factors. - Apply the bias uniformly to the resulting mono audio fragment. - Complete the final encoding step to produce an A-LAW encoded audio fragment. - Ensure that the returned output is a valid bytes object representing the A-LAW encoded sound fragment. **Hint**: You can utilize the functions `audioop.tomono`, `audioop.bias`, and `audioop.lin2alaw` from the `audioop` module to achieve the desired processing.","solution":"import audioop def process_audio(stereo_fragment: bytes, width: int, l_factor: float, r_factor: float, bias: int) -> bytes: Converts a stereo audio fragment to mono, applies a bias, and encodes it to A-LAW. Parameters: stereo_fragment (bytes): Raw stereo audio data width (int): Sample width in bytes (either 1, 2, 3, or 4) l_factor (float): Multiplicative factor for the left channel r_factor (float): Multiplicative factor for the right channel bias (int): Bias value to apply to the mono audio Returns: bytes: A-LAW encoded audio data # Step 1: Convert the stereo fragment to mono mono_fragment = audioop.tomono(stereo_fragment, width, l_factor, r_factor) # Step 2: Apply the bias to the mono data biased_fragment = audioop.bias(mono_fragment, width, bias) # Step 3: Convert the biased mono fragment to A-LAW alaw_encoded_fragment = audioop.lin2alaw(biased_fragment, width) return alaw_encoded_fragment"},{"question":"# ASCII Character Analysis Tool You are to implement a small ASCII character analysis tool that uses the `curses.ascii` module to evaluate and transform input text. Your task is to write functions that will help analyze strings to determine their ASCII properties and transform them based on various criteria. Input 1. A string comprising ASCII characters. 2. A series of queries which are strings formatted as `\\"[Function]:[Character]\\"` or `\\"[Function]:[String]\\"`. Output 1. For each query, return the result based on the specified `curses.ascii` function. 2. If the function does not exist or the query is malformed, return `\\"Invalid query\\"` for that query. Functions to Implement 1. `analyze_ascii_string` which takes the input string and a list of queries, and processes each query using the corresponding function from `curses.ascii`. Example You need to implement the following functions. ```python import curses.ascii def analyze_ascii_string(input_string: str, queries: list) -> list: # Implementation goes here pass # Example usage: input_string = \\"Hello, World!\\" queries = [\\"isalnum:H\\", \\"isdigit:1\\", \\"isalpha:A\\", \\"ispunct:!\\", \\"nonexistent:X\\"] # Expected output: # [\\"True\\", \\"True\\", \\"True\\", \\"True\\", \\"Invalid query\\"] print(analyze_ascii_string(input_string, queries)) ``` # Constraints: - Each query will be a string formatted as `[Function]:[Character]` or `[Function]:[String]`. - The input string will only contain printable ASCII characters. - Handle any exceptions or invalid queries gracefully by returning `\\"Invalid query\\"`. # Explanation: - You will use the `curses.ascii` functions to process the character or string in the queries. - For example, in the query `\\"isalnum:H\\"`, use `curses.ascii.isalnum(\'H\')` to return `True` or `False`, converted to a string and returned in the list. - If the function specified in the query does not exist in the `curses.ascii` module, return `\\"Invalid query\\"`. Your task is to ensure your code correctly processes a variety of ASCII transformation and checking functions provided by the `curses.ascii` module while handling errors and malformed queries gracefully.","solution":"import curses.ascii def analyze_ascii_string(input_string: str, queries: list) -> list: results = [] for query in queries: try: func_name, char = query.split(\\":\\") if hasattr(curses.ascii, func_name): func = getattr(curses.ascii, func_name) result = func(char) results.append(str(result)) else: results.append(\\"Invalid query\\") except Exception: results.append(\\"Invalid query\\") return results"},{"question":"Question: XML Manipulation and Querying with ElementTree Using the `xml.etree.ElementTree` module, write a Python function `modify_xml(input_xml: str, query: str, new_value: str) -> str` that takes an XML string `input_xml`, an XPath query string `query`, and a `new_value`. The function will: 1. Parse the `input_xml` string into an XML tree. 2. Find the XML element(s) based on the `query`. 3. Update the text of the element(s) found with `new_value`. 4. Return the modified XML as a string. # Input Format: - `input_xml` (str): A valid XML string. - `query` (str): An XPath expression to locate elements within the XML. - `new_value` (str): The new value to be set for the found elements. # Output Format: - A string representing the modified XML. # Constraints: - The input XML string is guaranteed to be well-formed. - The XPath query will correctly locate elements if they exist. - The `new_value` is a valid string to be set as text content. # Example: Given the following XML: ```xml <root> <child id=\\"1\\">OldValue1</child> <child id=\\"2\\">OldValue2</child> </root> ``` and inputs: ```python input_xml = \'\'\' <root> <child id=\\"1\\">OldValue1</child> <child id=\\"2\\">OldValue2</child> </root> \'\'\' query = \'.//child[@id=\\"1\\"]\' new_value = \'NewValue1\' ``` The function should return: ```xml <root> <child id=\\"1\\">NewValue1</child> <child id=\\"2\\">OldValue2</child> </root> ``` # Requirements: - Efficient parsing and querying using `xml.etree.ElementTree`. - Proper handling and modification of the XML elements based on the XPath query. - Return the modified XML as a string. # Solution Skeleton: ```python import xml.etree.ElementTree as ET def modify_xml(input_xml: str, query: str, new_value: str) -> str: # Parse the XML string into an ElementTree root = ET.fromstring(input_xml) # Find the elements using XPath query elements = root.findall(query) # Update the text of each found element for elem in elements: elem.text = new_value # Convert the modified tree back to a string modified_xml = ET.tostring(root, encoding=\'unicode\') return modified_xml ``` This question assesses the student\'s understanding and application of XML parsing, querying, and manipulation using the `xml.etree.ElementTree` module in Python.","solution":"import xml.etree.ElementTree as ET def modify_xml(input_xml: str, query: str, new_value: str) -> str: # Parse the XML string into an ElementTree root = ET.fromstring(input_xml) # Find the elements using XPath query elements = root.findall(query) # Update the text of each found element for elem in elements: elem.text = new_value # Convert the modified tree back to a string modified_xml = ET.tostring(root, encoding=\'unicode\') return modified_xml"},{"question":"**Question: Python Environment Inspector using sysconfig** You are required to implement a function in Python that uses the `sysconfig` module to inspect and report details about the current Python environment. The function will generate a summary consisting of: 1. The current Python version. 2. The platform identifier. 3. The default installation scheme for the current platform. 4. All installation paths for the default scheme, expanded to full paths. 5. The value of a specific configuration variable, if it exists. **Function Signature:** ```python import sysconfig def python_environment_inspector(config_var_name: str) -> dict: pass ``` **Input:** - `config_var_name` (str): The name of the configuration variable whose value needs to be retrieved. **Output:** - A dictionary containing the following keys and their corresponding values: - `python_version`: The Python version as a string in \\"MAJOR.MINOR\\" format. - `platform`: A string identifying the current platform. - `default_scheme`: The default scheme name for the current platform. - `installation_paths`: A dictionary of installation paths for the default scheme, with path names as keys and expanded paths as values. - `config_variable_value`: The value of the specified configuration variable (or `None` if the variable does not exist). **Constraints:** - The solution must use functions from the `sysconfig` module to gather the required information. - The function should handle cases where the specified configuration variable does not exist gracefully. **Example:** ```python config_var_name = \\"LIBDIR\\" result = python_environment_inspector(config_var_name) # Example output format (contents may vary based on the environment) expected_output = { \'python_version\': \'3.10\', \'platform\': \'linux-x86_64\', \'default_scheme\': \'posix_prefix\', \'installation_paths\': { \'stdlib\': \'/usr/local/lib/python3.10\', \'platstdlib\': \'/usr/local/lib/python3.10\', \'purelib\': \'/usr/local/lib/python3.10/site-packages\', \'platlib\': \'/usr/local/lib/python3.10/site-packages\', \'include\': \'/usr/local/include/python3.10\', \'platinclude\': \'/usr/local/include/python3.10\', \'scripts\': \'/usr/local/bin\', \'data\': \'/usr/local\', }, \'config_variable_value\': \'/usr/local/lib\' } ``` Write your implementation of the `python_environment_inspector` function below.","solution":"import sysconfig def python_environment_inspector(config_var_name: str) -> dict: Inspects and reports details about the current Python environment using sysconfig module. Parameters: config_var_name (str): The name of the configuration variable whose value needs to be retrieved. Returns: dict: A dictionary containing the Python version, platform, default installation scheme, installation paths for the default scheme, and the value of the specified configuration variable. python_version = f\\"{sysconfig.get_config_var(\'py_version_nodot\')[:2]}.{sysconfig.get_config_var(\'py_version_nodot\')[2:]}\\" platform = sysconfig.get_platform() default_scheme = sysconfig.get_default_scheme() installation_paths = {key: sysconfig.get_path(key) for key in sysconfig.get_paths()} config_variable_value = sysconfig.get_config_var(config_var_name) return { \'python_version\': python_version, \'platform\': platform, \'default_scheme\': default_scheme, \'installation_paths\': installation_paths, \'config_variable_value\': config_variable_value }"},{"question":"Objective: The objective of this task is to assess the student\'s ability to implement a semi-supervised learning model using the scikit-learn library. The students must understand the concepts of semi-supervised learning and apply them appropriately to a given dataset. Problem Statement: You are provided with a partially labeled dataset of handwritten digits (similar to the MNIST dataset) with features and corresponding labels. Your task is to implement a semi-supervised learning model using the `LabelPropagation` class from the `sklearn.semi_supervised` module to improve the classification accuracy using the unlabeled data. Instructions: 1. **Load the Dataset**: - Load the dataset and separate it into features (X) and labels (y). The labels array y contains some unlabeled entries marked as `-1`. 2. **Implement LabelPropagation**: - Use the `LabelPropagation` class for the semi-supervised learning task. - Define the parameters `gamma` for the RBF kernel or `n_neighbors` for the kNN kernel as input arguments. - Fit the model to the dataset (including both labeled and unlabeled data). 3. **Evaluate the Model**: - Use a separate test set (fully labeled) to evaluate the performance of your semi-supervised model. - Report the accuracy of the model on the test set. 4. **Optional**: - You may experiment with different kernel methods and parameter values to find the best configuration for your dataset. Input: - A training dataset with features `X_train` (array-like, shape = [n_samples, n_features]) and labels `y_train` (array-like, shape = [n_samples]). - A test dataset with features `X_test` (array-like, shape = [n_test_samples, n_features]) and labels `y_test` (array-like, shape = [n_test_samples]). - Parameter for the kernel method (either `gamma` for RBF or `n_neighbors` for kNN). Output: - The accuracy of the `LabelPropagation` model on the test dataset. Constraints: - You must use the `LabelPropagation` class from `sklearn.semi_supervised`. - The `y_train` array contains labels for some samples and `-1` for unlabeled samples. Example Code: ```python import numpy as np from sklearn.semi_supervised import LabelPropagation from sklearn.metrics import accuracy_score # Assuming X_train, y_train, X_test, y_test are already defined def semi_supervised_learning(X_train, y_train, X_test, y_test, kernel=\'knn\', gamma=None, n_neighbors=7): if kernel == \'rbf\': model = LabelPropagation(kernel=kernel, gamma=gamma) elif kernel == \'knn\': model = LabelPropagation(kernel=kernel, n_neighbors=n_neighbors) else: raise ValueError(\\"Unsupported kernel. Use \'rbf\' or \'knn\'.\\") model.fit(X_train, y_train) y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy # Example usage accuracy = semi_supervised_learning(X_train, y_train, X_test, y_test, kernel=\'knn\', n_neighbors=5) print(f\\"Model accuracy: {accuracy}\\") ``` The students are expected to: - Properly load and preprocess the dataset. - Correctly implement the `LabelPropagation` model and handle inputs and outputs. - Perform evaluation and report the accuracy.","solution":"import numpy as np from sklearn.semi_supervised import LabelPropagation from sklearn.metrics import accuracy_score def semi_supervised_learning(X_train, y_train, X_test, y_test, kernel=\'knn\', gamma=None, n_neighbors=7): if kernel == \'rbf\': model = LabelPropagation(kernel=kernel, gamma=gamma) elif kernel == \'knn\': model = LabelPropagation(kernel=kernel, n_neighbors=n_neighbors) else: raise ValueError(\\"Unsupported kernel. Use \'rbf\' or \'knn\'.\\") model.fit(X_train, y_train) y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"Serialization and Tensor Views in PyTorch **Objective**: Assess understanding of PyTorch serialization and tensor view relationships. **Problem Statement**: You are provided with a PyTorch tensor containing a range of numbers from 1 to 100. Your task is to: 1. Create a sliced view of this tensor that takes every third element starting from the second element. 2. Save both the original tensor and the sliced view to a file. 3. Load the tensors from the file and demonstrate that modifying the sliced view affects the original tensor. 4. Save a cloned version of the sliced tensor to a new file to reduce file size. 5. Verify the storage sizes of the tensors in both saved files. **Requirements**: - Ensure that the sliced view shares the underlying storage with the original tensor. - Demonstrate loading and the impact of modifications. - Clone the sliced view to create an independent tensor and save it. - Check and compare the storage sizes of all loaded tensors. **Function Signature**: ```python import torch def save_and_verify_tensor_views(): # Step 1: Create the original tensor original_tensor = torch.arange(1, 101) # Step 2: Create a sliced view taking every third element starting from the second element sliced_view = original_tensor[1::3] # Step 3: Save both the original tensor and the sliced view to \'tensors.pt\' torch.save([original_tensor, sliced_view], \'tensors.pt\') # Step 4: Load the tensors from \'tensors.pt\' loaded_original, loaded_sliced_view = torch.load(\'tensors.pt\') # Step 5: Modify the loaded sliced view and show it affects the original tensor loaded_sliced_view *= 2 # Step 6: Save the cloned version of the sliced tensor to \'cloned_tensor.pt\' torch.save(loaded_sliced_view.clone(), \'cloned_tensor.pt\') # Step 7: Load the cloned tensor cloned_tensor = torch.load(\'cloned_tensor.pt\') # Step 8: Print storage sizes print(\\"Storage size of \'loaded_original\':\\", loaded_original.storage().size()) print(\\"Storage size of \'loaded_sliced_view\':\\", loaded_sliced_view.storage().size()) print(\\"Storage size of \'cloned_tensor\':\\", cloned_tensor.storage().size()) # Call the function save_and_verify_tensor_views() ``` **Expected Output**: ```plaintext *tensor([ 1, 2, 3, 4, 5, ... 98, 99, 100]) *Storage size of \'loaded_original\': 100 *Storage size of \'loaded_sliced_view\': 100 *Storage size of \'cloned_tensor\': 34 ``` In the output, you should observe the storage sizes verifying the sharing of storage before cloning and showing a reduced storage size of the cloned tensor. Your implementation should handle all the steps and display the results as described.","solution":"import torch def save_and_verify_tensor_views(): # Step 1: Create the original tensor original_tensor = torch.arange(1, 101) # Step 2: Create a sliced view taking every third element starting from the second element sliced_view = original_tensor[1::3] # Step 3: Save both the original tensor and the sliced view to \'tensors.pt\' torch.save([original_tensor, sliced_view], \'tensors.pt\') # Step 4: Load the tensors from \'tensors.pt\' loaded_original, loaded_sliced_view = torch.load(\'tensors.pt\') # Step 5: Modify the loaded sliced view and show it affects the original tensor loaded_sliced_view *= 2 # Step 6: Save the cloned version of the sliced tensor to \'cloned_tensor.pt\' torch.save(loaded_sliced_view.clone(), \'cloned_tensor.pt\') # Step 7: Load the cloned tensor cloned_tensor = torch.load(\'cloned_tensor.pt\') # Step 8: Print storage sizes original_size = loaded_original.storage().size() loaded_view_size = loaded_sliced_view.storage().size() cloned_size = cloned_tensor.storage().size() print(\\"Storage size of \'loaded_original\':\\", original_size) print(\\"Storage size of \'loaded_sliced_view\':\\", loaded_view_size) print(\\"Storage size of \'cloned_tensor\':\\", cloned_size) return original_size, loaded_view_size, cloned_size"},{"question":"# Pandas Coding Challenge: Analyzing Sales Data **Objective:** You are given sales data for a retail company in CSV format. Your task is to clean, manipulate, and analyze the data using pandas. **Dataset:** The CSV file (`sales_data.csv`) contains the following columns: - `Date`: Date of the transaction in `YYYY-MM-DD` format. - `StoreID`: Unique identifier for the store. - `ProductID`: Unique identifier for the product. - `UnitsSold`: Number of units sold. - `Revenue`: Revenue generated from the sale. **Problem Statement:** 1. **Data Loading & Cleaning:** - Write a function `load_and_clean_data(file_path)` that takes the file path of the CSV file and returns a cleaned DataFrame. - **Steps to clean the data:** - Parse the `Date` column as datetime. - Ensure there are no duplicate rows. - Fill any missing values in the `UnitsSold` and `Revenue` columns with 0. 2. **Monthly Sales Analysis:** - Write a function `monthly_sales_summary(df)` that takes the cleaned DataFrame and returns a summary DataFrame. - **Steps for analysis:** - Calculate the total `UnitsSold` and `Revenue` for each product for each month. - The resulting DataFrame should have the columns: `Month`, `ProductID`, `TotalUnitsSold`, `TotalRevenue`. 3. **Top Stores Identification:** - Write a function `identify_top_stores(df)` that identifies the top 5 stores by total revenue for the entire dataset. - **Steps for analysis:** - Group the data by `StoreID` and calculate the total revenue. - Return a list of the top 5 `StoreID`s with the highest revenue. 4. **Product Performance Analysis:** - Write a function `product_performance(df)` that returns the performance of each product based on `UnitsSold` and `Revenue`. - **Steps for analysis:** - Calculate the average number of units sold per transaction and average revenue per transaction for each product. - The resulting DataFrame should have the columns: `ProductID`, `AvgUnitsSoldPerTransaction`, `AvgRevenuePerTransaction`. **Function Signatures:** ```python import pandas as pd def load_and_clean_data(file_path: str) -> pd.DataFrame: # Implement the function def monthly_sales_summary(df: pd.DataFrame) -> pd.DataFrame: # Implement the function def identify_top_stores(df: pd.DataFrame) -> list: # Implement the function def product_performance(df: pd.DataFrame) -> pd.DataFrame: # Implement the function # Example Usage if __name__ == \\"__main__\\": file_path = \'path_to_your_csv_file/sales_data.csv\' df = load_and_clean_data(file_path) monthly_summary = monthly_sales_summary(df) print(\\"Monthly Sales Summary:n\\", monthly_summary) top_stores = identify_top_stores(df) print(\\"Top 5 Stores by Revenue:\\", top_stores) product_perf = product_performance(df) print(\\"Product Performance:n\\", product_perf) ``` **Input Constraints:** - The `Date` column will have valid date strings. - The `UnitsSold` and `Revenue` columns may contain NaN values which should be treated as zero. - The CSV file will contain a large dataset, so efficiency in data manipulation is key. **Expected Output:** - `load_and_clean_data()`: Returns a cleaned DataFrame. - `monthly_sales_summary()`: Returns a DataFrame with total units sold and revenue for each product per month. - `identify_top_stores()`: Returns a list of `StoreID`s of the top 5 stores by revenue. - `product_performance()`: Returns a DataFrame with product performance metrics. Ensure your solution is efficient and utilizes proper pandas methods to perform data manipulation and analysis.","solution":"import pandas as pd def load_and_clean_data(file_path: str) -> pd.DataFrame: Load and clean the sales data. 1. Parse the Date column as datetime. 2. Remove duplicate rows. 3. Fill NaN values in UnitsSold and Revenue columns with 0. df = pd.read_csv(file_path, parse_dates=[\'Date\']) df.drop_duplicates(inplace=True) df[\'UnitsSold\'].fillna(0, inplace=True) df[\'Revenue\'].fillna(0, inplace=True) return df def monthly_sales_summary(df: pd.DataFrame) -> pd.DataFrame: Create a monthly sales summary. 1. Group by year-month and ProductID. 2. Calculate the total UnitsSold and Revenue for each product. df[\'Month\'] = df[\'Date\'].dt.to_period(\'M\') monthly_summary = df.groupby([\'Month\', \'ProductID\']).agg( TotalUnitsSold=(\'UnitsSold\', \'sum\'), TotalRevenue=(\'Revenue\', \'sum\') ).reset_index() return monthly_summary def identify_top_stores(df: pd.DataFrame) -> list: Identify the top 5 stores by total revenue. 1. Group by StoreID. 2. Calculate total revenue for each store. 3. Return the top 5 StoreIDs with the highest revenue. store_revenue = df.groupby(\'StoreID\')[\'Revenue\'].sum() top_stores = store_revenue.nlargest(5).index.tolist() return top_stores def product_performance(df: pd.DataFrame) -> pd.DataFrame: Analyze the performance of each product. 1. Calculate average units sold per transaction for each product. 2. Calculate average revenue per transaction for each product. product_performance = df.groupby(\'ProductID\').agg( AvgUnitsSoldPerTransaction=(\'UnitsSold\', \'mean\'), AvgRevenuePerTransaction=(\'Revenue\', \'mean\') ).reset_index() return product_performance"},{"question":"**Objective**: To assess your understanding of Python\'s `multiprocessing` module for creating and synchronizing multiple processes. **Problem Statement**: You need to implement a Python script that creates a pool of worker processes to compute the factorial of a list of numbers in parallel. The main process will distribute the computation tasks among the worker processes and aggregate the results. Finally, the main process will print the results in a sorted order (ascending based on the original numbers). # Specifications: 1. Implement a function `compute_factorial(n)` that computes the factorial of a number `n` using an iterative approach. 2. Implement a main function that: - Creates a list of numbers, e.g., `[5, 7, 4, 6, 2, 9]`. - Sets up a pool of worker processes to compute the factorial of these numbers in parallel. - Collects the factorial results using the pool\'s `map` method. - Prints each number with its factorial in ascending order of the numbers. # Constraints: - Use Python\'s built-in `multiprocessing` module. - Handle the execution in a cross-platform manner using the `__name__ == \\"__main__\\"` guard. - Ensure the worker processes do not produce interleaved output messages. # Expected Input/Output: - **Input**: The script will not take direct input; the list of numbers is hardcoded. - **Output**: Print statements showing numbers and their factorials, sorted by the numbers. Example: ```plaintext Numbers: [5, 7, 4, 6, 2, 9] Output: 2: 2 4: 24 5: 120 6: 720 7: 5040 9: 362880 ``` # Code Template: ```python from multiprocessing import Pool def compute_factorial(n): factorial = 1 for i in range(2, n + 1): factorial *= i return factorial def main(): numbers = [5, 7, 4, 6, 2, 9] with Pool(processes=4) as pool: results = pool.map(compute_factorial, numbers) sorted_results = sorted(zip(numbers, results)) for num, fact in sorted_results: print(f\\"{num}: {fact}\\") if __name__ == \\"__main__\\": main() ``` **Instructions**: 1. Implement the `compute_factorial` function. 2. Ensure the `main` function sets up the pool and computes the factorials parallelly. 3. Print the outputs in the specified sorted order. **Evaluation Criteria**: - Correctness of the factorial computation. - Use of the `multiprocessing.Pool` to manage multiple worker processes. - Correct aggregation and sorting of results. - Proper handling of the multiprocessing setup using the `__name__ == \\"__main__\\"` guard. Note: Avoid using recursive approaches for `compute_factorial` due to potential recursion limits in Python.","solution":"from multiprocessing import Pool def compute_factorial(n): Computes the factorial of n using an iterative approach. factorial = 1 for i in range(2, n + 1): factorial *= i return factorial def main(): Main function to compute factorials of a list of numbers using multiprocessing. numbers = [5, 7, 4, 6, 2, 9] with Pool(processes=4) as pool: results = pool.map(compute_factorial, numbers) sorted_results = sorted(zip(numbers, results)) for num, fact in sorted_results: print(f\\"{num}: {fact}\\") if __name__ == \\"__main__\\": main()"},{"question":"# PyTorch Coding Assessment Question **Objective:** Implement a custom PyTorch operation and decompose it into Core Aten operations. This exercise will evaluate your understanding of PyTorchs backend functionality and operator decomposition. **Question:** You are tasked with implementing a custom PyTorch function `custom_relu` that behaves similarly to the built-in ReLU activation function. After implementing the function, you must decompose it into Core Aten operations to illustrate your understanding of PyTorchs backend interface. **Specifications:** 1. Implement a function `custom_relu` using PyTorch. 2. Decompose the `custom_relu` function by manually implementing it using Core Aten operations. **Function Signature:** ```python import torch def custom_relu(x: torch.Tensor) -> torch.Tensor: Applies the ReLU function element-wise: relu(x) = max(x, 0) Args: x (torch.Tensor): Input tensor Returns: torch.Tensor: Output tensor with ReLU applied element-wise. # Your implementation here def decomposed_custom_relu(x: torch.Tensor) -> torch.Tensor: Applies the ReLU function element-wise using Core Aten operations. Args: x (torch.Tensor): Input tensor Returns: torch.Tensor: Output tensor with ReLU applied element-wise using Core Aten operations. # Your implementation here ``` **Constraints:** - Input tensor can have any shape. - You may not use the built-in `torch.relu` or `torch.nn.ReLU` functions in the `custom_relu` implementation or the `decomposed_custom_relu` function. - Use only Core Aten operations in the `decomposed_custom_relu` function. **Example:** ```python x = torch.tensor([-1.0, 2.0, -0.5, 4.0]) print(custom_relu(x)) # Expected output: tensor([0.0, 2.0, 0.0, 4.0]) print(decomposed_custom_relu(x)) # Expected output should match with the custom_relu output: tensor([0.0, 2.0, 0.0, 4.0]) ``` Your task is to fill in the two function implementations and ensure that both functions yield the same results. **Things to consider:** - Ensure that your solutions handle tensors with different shapes and possibly containing edge cases such as negative, zero, and positive values. - Pay particular attention to handling the decomposition correctly using Core Aten operators in the `decomposed_custom_relu` function. **Hint:** Look for PyTorch functions/operators in the Core Aten subset that can help you implement the ReLU function.","solution":"import torch def custom_relu(x: torch.Tensor) -> torch.Tensor: Applies the ReLU function element-wise: relu(x) = max(x, 0) Args: x (torch.Tensor): Input tensor Returns: torch.Tensor: Output tensor with ReLU applied element-wise. return torch.maximum(x, torch.zeros_like(x)) def decomposed_custom_relu(x: torch.Tensor) -> torch.Tensor: Applies the ReLU function element-wise using Core Aten operations. Args: x (torch.Tensor): Input tensor Returns: torch.Tensor: Output tensor with ReLU applied element-wise using Core Aten operations. zero_tensor = torch.zeros_like(x) # Core Aten operator for creating a tensor of zeros like x return torch.where(x > zero_tensor, x, zero_tensor) # Core Aten operation for choosing elements based on condition"},{"question":"# Advanced Python Coding Assessment: Custom Descriptors Objective: You are required to create custom descriptors in Python to manage attributes in an object-oriented structure. Problem Statement: Design a class named `PositiveNumber` that acts as a descriptor. This descriptor should ensure that the attribute it manages only contains positive numbers. Your `PositiveNumber` descriptor should handle setting, getting, and deleting attributes. Next, create a `Product` class that uses the `PositiveNumber` descriptor to manage its `price` and `quantity` attributes, ensuring both are always positive numbers. Specifications: 1. Implement the `PositiveNumber` descriptor class with the following methods: - `__init__(self, name)`: Initialize with the attribute name. - `__get__(self, instance, owner)`: Get the attribute value. - `__set__(self, instance, value)`: Set the attribute value ensuring it is positive. - `__delete__(self, instance)`: Delete the attribute value. 2. Implement the `Product` class with: - Attributes: `price` and `quantity`, managed by the `PositiveNumber` descriptor. - `__init__(self, price, quantity)`: Initialize `price` and `quantity` attributes. - Methods to display product information. Input and Output: - Instantiate a `Product` object with `price` and `quantity`. - Print the `price` and `quantity`. - Update the `price` and `quantity` with a positive number. - Attempt to update the `price` and `quantity` to a negative number (should raise a `ValueError`). - Delete the `price` attribute. Constraints: - Both `price` and `quantity` should always be positive. - Raising a `ValueError` with an appropriate message if attempts to set a non-positive value. Example: ```python # Example usage: try: p = Product(price=100, quantity=20) print(p.price) # Outputs: 100 print(p.quantity) # Outputs: 20 p.price = 150 p.quantity = 30 print(p.price) # Outputs: 150 print(p.quantity) # Outputs: 30 p.price = -10 # Raises ValueError: Price must be positive. except ValueError as e: print(e) del p.price try: print(p.price) # Should raise AttributeError as price is deleted. except AttributeError as e: print(e) ``` Performance: - Ensure the operations for getting, setting, and deleting attributes are efficient. Submission: - Submit your implementation of the `PositiveNumber` descriptor and `Product` class.","solution":"class PositiveNumber: def __init__(self, name): self.name = f\\"_{name}\\" def __get__(self, instance, owner): return getattr(instance, self.name) def __set__(self, instance, value): if value <= 0: raise ValueError(f\\"{self.name[1:]} must be positive.\\") setattr(instance, self.name, value) def __delete__(self, instance): delattr(instance, self.name) class Product: price = PositiveNumber(\\"price\\") quantity = PositiveNumber(\\"quantity\\") def __init__(self, price, quantity): self.price = price self.quantity = quantity def display(self): return f\\"Product(price={self.price}, quantity={self.quantity})\\""},{"question":"# Python Coding Challenge Objective Demonstrate your understanding of the `tarfile` module by implementing a function that creates a tar archive from a list of files and directories, and another function that extracts a specific type of files (e.g., `.txt` files) from the tar archive to a specified directory. Problem Statement 1. Implement the function `create_tar_archive(output_filename, input_files, format=\'w:gz\')` that creates a tar archive `output_filename` containing the files and directories specified in the `input_files` list. The tar archive should use the specified format (default is gzip compression). **Function Signature:** ```python def create_tar_archive(output_filename: str, input_files: list[str], format: str = \'w:gz\') -> None: ``` **Parameters:** - `output_filename` (str): The name of the output tar archive file. - `input_files` (list of str): List of file and directory paths to include in the tar archive. - `format` (str): The format for creating the tar archive (default is `\'w:gz\'`). **Example Usage:** ```python create_tar_archive(\'my_archive.tar.gz\', [\'file1.txt\', \'dir1\']) ``` 2. Implement the function `extract_specific_files(tarfile_path, output_dir, extension)` that extracts all files with a specific extension from the tar archive located at `tarfile_path` into the `output_dir` directory. **Function Signature:** ```python def extract_specific_files(tarfile_path: str, output_dir: str, extension: str) -> None: ``` **Parameters:** - `tarfile_path` (str): The path to the tar archive file. - `output_dir` (str): The directory to extract the files into. - `extension` (str): The file extension to filter for extraction (e.g., `\'.txt\'`). **Example Usage:** ```python extract_specific_files(\'my_archive.tar.gz\', \'output_directory\', \'.txt\') ``` > **Constraints and Requirements:** > - Do not use any external libraries; use the standard `tarfile` module only. > - Properly handle exceptions and edge cases, like non-existent files and directories in the input list and invalid tarfile paths. > - Ensure all extracted files are placed in the specified output directory without altering the tar structure. > - Test each function independently to verify functionality and robustness. Submission Submit your Python code implementation for both functions. Ensure your code is well-documented and includes docstrings for each function.","solution":"import tarfile import os def create_tar_archive(output_filename: str, input_files: list[str], format: str = \'w:gz\') -> None: Creates a tar archive containing the specified files and directories. :param output_filename: The name of the output tar archive file. :param input_files: List of file and directory paths to include in the tar archive. :param format: The format for creating the tar archive (default is \'w:gz\'). with tarfile.open(output_filename, format) as tar: for file in input_files: tar.add(file, arcname=os.path.basename(file)) def extract_specific_files(tarfile_path: str, output_dir: str, extension: str) -> None: Extracts files with a specific extension from a tar archive into a specified directory. :param tarfile_path: The path to the tar archive file. :param output_dir: The directory to extract the files into. :param extension: The file extension to filter for extraction (e.g., \'.txt\'). with tarfile.open(tarfile_path, \'r:*\') as tar: members = [m for m in tar.getmembers() if m.name.endswith(extension)] tar.extractall(path=output_dir, members=members)"},{"question":"# Custom Command Interpreter for a Text-Based Adventure Game Objective: In this coding assessment, you will create a custom command interpreter for a simple text-based adventure game using the `cmd` module. Your interpreter will allow players to navigate through rooms, interact with objects, and manage an inventory. Functionality Requirements: 1. **Class Definition**: * Define a class `AdventureGameCmd` that inherits from `cmd.Cmd`. 2. **Commands**: Implement the following commands by defining appropriate `do_*` methods: * `do_go(direction)`: Move the player in the specified direction (north, south, east, west). Update the player\'s location accordingly. Handle invalid directions and collisions with walls. - Input: `direction` (a string indicating direction, one of: north, south, east, west) - Output: Print the new location or description of the failure. * `do_look()`: Provide a description of the current room and list any objects present. - Input: None - Output: Print the room description and any objects present. * `do_take(object)`: Add the specified object to the player\'s inventory if it is present in the current room. - Input: `object` (a string specifying the object\'s name) - Output: Print a success message or indicate that the object isn\'t present. * `do_inventory()`: List all objects currently in the player\'s inventory. - Input: None - Output: Print the list of objects in the inventory. * `do_quit()`: Exit the game. - Input: None - Output: Print a goodbye message and exit the command loop. 3. **Rooms and Objects Initialization**: Create a simple map for the game with at least four rooms and some objects scattered across them. For example: ```python rooms = { \'Entrance\': {\'description\': \'You are at the entrance of a spooky mansion.\', \'north\': \'Hallway\', \'objects\': [\'key\']}, \'Hallway\': {\'description\': \'A dark hallway with flickering lights.\', \'south\': \'Entrance\', \'east\': \'Library\', \'objects\': []}, \'Library\': {\'description\': \'Filled with dusty books and cobwebs.\', \'west\': \'Hallway\', \'objects\': [\'book\']}, \'Kitchen\': {\'description\': \'Smells of rotten food. You see a knife on the counter.\', \'north\': \'Hallway\', \'objects\': [\'knife\']} } ``` 4. **Player State**: Manage the player\'s state, including current location and inventory. 5. **Error Handling**: Provide appropriate error messages for invalid commands, directions leading to non-existent rooms, and attempts to take non-existent objects. Input and Output Formats: - The input will be commands typed by the player. - The output will be printed messages describing the result of each command. Example Session: Here is an example of how a session might look: ```plaintext Welcome to the adventure game. Type help or ? to list commands. (adventure) go north A dark hallway with flickering lights. (adventure) look You are in a dark hallway with flickering lights. No objects here. (adventure) go east Filled with dusty books and cobwebs. (adventure) take book You pick up the book. (adventure) inventory You have: book (adventure) go west A dark hallway with flickering lights. (adventure) go south You are at the entrance of a spooky mansion. (adventure) quit Thanks for playing! ``` Constraints: - You must use the `cmd` module to create the command interpreter. - Ensure that your code is robust and handles various edge cases gracefully. Performance Requirements: - The interpreter should handle up to 100 commands per session without significant lag. Bonus: If you have time, enhance the game by adding more rooms, objects, and interactions. For instance: - Implement puzzles that the player must solve to progress. - Allow players to combine objects to create new items. Happy coding!","solution":"import cmd rooms = { \'Entrance\': {\'description\': \'You are at the entrance of a spooky mansion.\', \'north\': \'Hallway\', \'objects\': [\'key\']}, \'Hallway\': {\'description\': \'A dark hallway with flickering lights.\', \'south\': \'Entrance\', \'east\': \'Library\', \'north\': \'Kitchen\', \'objects\': []}, \'Library\': {\'description\': \'Filled with dusty books and cobwebs.\', \'west\': \'Hallway\', \'objects\': [\'book\']}, \'Kitchen\': {\'description\': \'Smells of rotten food. You see a knife on the counter.\', \'south\': \'Hallway\', \'objects\': [\'knife\']} } class AdventureGameCmd(cmd.Cmd): intro = \'Welcome to the adventure game. Type help or ? to list commands.n\' prompt = \'(adventure) \' def __init__(self): super().__init__() self.current_room = \'Entrance\' self.inventory = [] def do_go(self, direction): Move the player in the specified direction (north, south, east, west) if direction in rooms[self.current_room]: self.current_room = rooms[self.current_room][direction] print(rooms[self.current_room][\'description\']) else: print(\\"You can\'t go that way.\\") def do_look(self, arg): Provide a description of the current room and list any objects present. room_info = rooms[self.current_room] print(room_info[\'description\']) if room_info[\'objects\']: print(f\\"You see: {\', \'.join(room_info[\'objects\'])}\\") else: print(\\"No objects here.\\") def do_take(self, obj): Add the specified object to the player\'s inventory if it is present in the current room. if obj in rooms[self.current_room][\'objects\']: rooms[self.current_room][\'objects\'].remove(obj) self.inventory.append(obj) print(f\\"You pick up the {obj}.\\") else: print(f\\"There is no {obj} here.\\") def do_inventory(self, arg): List all objects currently in the player\'s inventory. if self.inventory: print(f\\"You have: {\', \'.join(self.inventory)}\\") else: print(\\"Your inventory is empty.\\") def do_quit(self, arg): Exit the game. print(\\"Thanks for playing!\\") return True"},{"question":"# Python Method and Instance Method Implementation Objective: Implement a Python class that mimics the behavior of instance methods and bound methods outlined in the documentation. Your task is to create a class that demonstrates how methods and instance methods can be dynamically bound and invoked. Task: 1. Create a class `MethodHandler` that will allow dynamic binding of methods and instance methods to instances of this class. 2. Implement the following methods in the `MethodHandler` class: - `bind_instance_method(func)`: Binds the given function as an instance method to the `MethodHandler` instance. - `bind_method(func)`: Binds the given function as a method to the `MethodHandler` class itself. - `invoke_instance_method()`: Invokes the bound instance method if it exists; raises an exception if no instance method is bound. - `invoke_method()`: Invokes the bound method if it exists; raises an exception if no method is bound. Constraints: - You cannot use the built-in `types.MethodType` or any similar shortcut to bind methods. The binding must be done manually. Input and Output: 1. `bind_instance_method(func)`: - **Input**: `func` (a callable function) - **Output**: None 2. `bind_method(func)`: - **Input**: `func` (a callable function) - **Output**: None 3. `invoke_instance_method()`: - **Input**: None - **Output**: The result of the bound instance method 4. `invoke_method()`: - **Input**: None - **Output**: The result of the bound method Example: ```python class MethodHandler: def bind_instance_method(self, func): self._instance_method = func.__get__(self, type(self)) def bind_method(self, func): type(self)._method = func def invoke_instance_method(self): if hasattr(self, \'_instance_method\'): return self._instance_method() else: raise Exception(\\"No instance method bound\\") def invoke_method(self): if hasattr(type(self), \'_method\'): return type(self)._method() else: raise Exception(\\"No method bound\\") # Example usage def instance_hello(): return \\"Hello from instance method\\" def class_hello(): return \\"Hello from class method\\" handler = MethodHandler() # binding instance method handler.bind_instance_method(instance_hello) assert handler.invoke_instance_method() == \\"Hello from instance method\\" # binding class method handler.bind_method(class_hello) assert handler.invoke_method() == \\"Hello from class method\\" ``` Notes: - This is a simplified example to ensure basic understanding and ability to handle method and instance method binding dynamically.","solution":"class MethodHandler: def bind_instance_method(self, func): self._instance_method = lambda: func() def bind_method(self, func): type(self)._method = func def invoke_instance_method(self): if hasattr(self, \'_instance_method\'): return self._instance_method() else: raise Exception(\\"No instance method bound\\") def invoke_method(self): if hasattr(type(self), \'_method\'): return type(self)._method() else: raise Exception(\\"No method bound\\")"},{"question":"**Problem Statement:** Using the `seaborn.objects` module, create a function `plot_custom_histogram` that generates a histogram with the following specifications: 1. **Data Source**: The function should load the `diamonds` dataset using `seaborn.load_dataset`. 2. **Logarithmic Scale**: The x-axis should be scaled logarithmically. 3. **Histogram Bars**: - Plot the histogram of the `price` column with bars that are unfilled. - The edge color of the bars should be `C0` and the edge width should be `1.5`. 4. **Stacked Bars**: Overlay a stacked histogram on the same plot where: - The histogram is colored by the `cut` of the diamonds. - The width of each bar should be reduced to handle overlap using the `so.Stack` transformation. 5. **Alpha Transparency**: Apply an alpha transparency level based on the `clarity` of the diamonds. 6. **Appearance Settings**: - The function should set the edge width of the bars to `0` and fill them based on the `clarity` column. # Function Signature ```python import seaborn as sns import seaborn.objects as so def plot_custom_histogram(): pass ``` # Example Usage ```python plot_custom_histogram() ``` # Expected Output A plot with the above specifications should be displayed. The histogram should show the `price` distribution of diamonds on a logarithmic scale with visually distinct, unfilled bars and a stacked histogram overlay using `cut` for coloring and `clarity` for transparency. # Constraints - Use the `seaborn.objects` module to complete this task. - Make sure that overlapping bars are handled appropriately. - Ensure that the function does not return any values; it should only display the plot. - You may use `matplotlib` for additional plot customization if necessary.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def plot_custom_histogram(): # Load the diamonds dataset df = sns.load_dataset(\'diamonds\') # Plot histogram with unfilled bars for the \'price\' column plot = ( so.Plot(df, x=\'price\') .add(so.Bars(edgecolor=\'C0\', edgewidth=1.5), so.Hist(binwidth=500, bins=50)) .scale(x=\'log\') ) # Overlay stacked histogram colored by \'cut\' and applying \'clarity\' as alpha stacked_plot = ( so.Plot(df, x=\'price\', color=\'cut\') .add(so.Bars(alpha=\'clarity\', edgewidth=0), so.Stack()) .scale(x=\'log\') ) fig, ax = plt.subplots() plot.on(ax) stacked_plot.on(ax) plt.show()"},{"question":"**Coding Exercise: Fourier Transforms with PyTorch** **Objective**: Implement a function using PyTorch that: - Applies a 2-dimensional Fast Fourier Transform (FFT) to an input tensor. - Shifts the zero-frequency component to the center of the spectrum. - Computes the frequencies corresponding to the components of the DFT. - Returns the magnitudes of the transformed data along with the corresponding frequency grid. **Function Signature**: ```python def process_fft_2d(input_tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: pass ``` **Input**: - `input_tensor` (torch.Tensor): A 2-dimensional input tensor with shape (H, W), where H is the height and W is the width of the tensor. **Output**: - A tuple containing three elements: 1. `fft_magnitude` (torch.Tensor): A tensor of the same shape as `input_tensor` containing the magnitudes of the FFT-transformed data. 2. `freq_grid_x` (torch.Tensor): A tensor representing the frequency grid for the x-axis. 3. `freq_grid_y` (torch.Tensor): A tensor representing the frequency grid for the y-axis. **Constraints**: - You may assume the input tensor\'s dimensions (H, W) are both powers of 2 for simplicity. **Requirements**: - Your implementation should use the PyTorch `torch.fft` module functions to perform the FFT and related operations. - Ensure that the zero-frequency component is centered in the output magnitudes. - The function should handle real input data only, i.e., `input_tensor` contains real numbers. **Example**: ```python import torch # Example input tensor input_tensor = torch.randn(8, 8) # Process the input using the function fft_magnitude, freq_grid_x, freq_grid_y = process_fft_2d(input_tensor) print(\\"FFT Magnitude:\\") print(fft_magnitude) print(\\"Frequency Grid X:\\") print(freq_grid_x) print(\\"Frequency Grid Y:\\") print(freq_grid_y) ``` In this problem, students are expected to: 1. Perform a 2-dimensional FFT on the input tensor. 2. Shift the result such that the zero-frequency component is centered. 3. Compute the frequency grid for both axes. 4. Return the magnitudes of the transformed data along with the frequency grids. **Hints**: - Use `torch.fft.fft2` for 2-dimensional FFT and `torch.fft.fftshift` to center the zero-frequency component. - Use `torch.fft.fftfreq` to compute the frequency grids for each axis.","solution":"import torch import numpy as np def process_fft_2d(input_tensor: torch.Tensor): Applies a 2-dimensional Fast Fourier Transform (FFT) to an input tensor, shifts the zero-frequency component to the center, computes the magnitudes of the transformed data, and returns the magnitudes along with the corresponding frequency grid. Args: input_tensor (torch.Tensor): A 2-dimensional input tensor with shape (H, W). Returns: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: - The magnitudes of the FFT-transformed data. - The frequency grid for the x-axis. - The frequency grid for the y-axis. # Perform the 2D FFT fft_result = torch.fft.fft2(input_tensor) # Shift the zero-frequency component to the center fft_result_shifted = torch.fft.fftshift(fft_result) # Calculate the magnitudes fft_magnitude = torch.abs(fft_result_shifted) # Get the shape of the input tensor H, W = input_tensor.shape # Compute the frequency grids freq_grid_x = torch.fft.fftfreq(W)*W freq_grid_y = torch.fft.fftfreq(H)*H # Create 2D grids freq_grid_x, freq_grid_y = torch.meshgrid(freq_grid_x, freq_grid_y, indexing=\'ij\') return fft_magnitude, freq_grid_x, freq_grid_y"},{"question":"# Task: You are tasked with creating a script that spawns subprocesses and communicates with them asynchronously using the `asyncio` module. Your script should be compatible with both Windows and macOS platforms. Implement a function `run_subprocess_tasks(commands: List[str]) -> None` that takes a list of command strings and executes them as subprocesses asynchronously. Input: - `commands`: A list of strings, where each string is a command to be executed as a subprocess. Output: - The function does not return any value but prints the output and error of each command after execution. Constraints: - Ensure compatibility with both Windows (`ProactorEventLoop`) and macOS (`SelectorEventLoop`). - Handle limitations, such as the inability to use certain methods on different platforms. Requirements: 1. The function should handle multiple subprocesses concurrently. 2. Capture and print both `stdout` and `stderr` of each subprocess. 3. Ensure proper event loop setup according to the platform-specific limitations described in the documentation. Example: ```python import asyncio from typing import List async def run_subprocess_commands(command: str): process = await asyncio.create_subprocess_shell( command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await process.communicate() print(f\'Command: {command}\') print(f\'STDOUT: {stdout.decode()}\') print(f\'STDERR: {stderr.decode()}\') def run_subprocess_tasks(commands: List[str]) -> None: # Define platform-specific checks and event loop setups here loop = asyncio.get_event_loop() tasks = [run_subprocess_commands(cmd) for cmd in commands] loop.run_until_complete(asyncio.gather(*tasks)) ``` # Notes: 1. Depending on the platform, you may need to set up the appropriate event loop (`ProactorEventLoop` on Windows and `SelectorEventLoop` on macOS) manually. 2. Make sure to handle exceptions and clean up resources properly.","solution":"import asyncio import platform from typing import List async def run_subprocess_command(command: str): process = await asyncio.create_subprocess_shell( command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await process.communicate() print(f\'Command: {command}\') print(f\'STDOUT: {stdout.decode()}\') print(f\'STDERR: {stderr.decode()}\') def run_subprocess_tasks(commands: List[str]) -> None: # Define platform-specific event loop current_platform = platform.system() if current_platform == \'Windows\': asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy()) else: # For macOS and others asyncio.set_event_loop_policy(asyncio.DefaultEventLoopPolicy()) loop = asyncio.get_event_loop() tasks = [run_subprocess_command(cmd) for cmd in commands] loop.run_until_complete(asyncio.gather(*tasks))"},{"question":"# Weak Reference Management In this assessment, you will write a Python function that demonstrates the creation and management of weak references. You will need to use weak references to avoid memory leaks in situations where objects reference each other. Task: 1. **Implement a class `Node` that represents a node in a simple graph.** Each node should have: - An `id` (string) to uniquely identify it. - A list of neighbors, which can contain references to other `Node` objects. 2. **Implement a utility function `add_neighbor(node, neighbor, callback=None)` that:** - Takes a `node` and a `neighbor` (both instances of `Node`). - Optionally takes a `callback` function that should be called when the neighbor is garbage collected. - Adds a weak reference to `neighbor` in the `node`\'s list of neighbors using `weakref.ref`. 3. **Implement a utility function `remove_garbage_collected_neighbors(node)` that:** - Takes a `node`. - Scans the `node`\'s list of neighbors and removes any entries that have been garbage collected. 4. **Implement a utility function `get_live_neighbors(node)` that:** - Takes a `node`. - Returns a list of currently live neighbors. Expected Input and Output Formats: - The `Node` class is instantiated with a string `id`. - The `add_neighbor(node, neighbor, callback)` function: - Takes two `Node` instances and an optional function. - Adds a weak reference to `neighbor` in `node`\'s neighbor list. - The `remove_garbage_collected_neighbors(node)` function: - Takes a `Node` instance. - Removes any garbage-collected neighbors from the neighbor list. - The `get_live_neighbors(node)` function: - Takes a `Node` instance. - Returns a list of currently live neighbors (actual `Node` instances). Example Usage: ```python import weakref # Node class definition class Node: ... # add_neighbor, remove_garbage_collected_neighbors, get_live_neighbors definitions def add_neighbor(node, neighbor, callback=None): ... def remove_garbage_collected_neighbors(node): ... def get_live_neighbors(node): ... # Example n1 = Node(\'n1\') n2 = Node(\'n2\') n3 = Node(\'n3\') add_neighbor(n1, n2) add_neighbor(n1, n3) # At this point, n1 has two neighbors: n2 and n3 print(get_live_neighbors(n1)) # Should print: [n2, n3] # Delete n3 and invoke garbage collection del n3 # Clean up any garbage collected neighbors remove_garbage_collected_neighbors(n1) # Now, n1 should only have one live neighbor: n2 print(get_live_neighbors(n1)) # Should print: [n2] ``` Constraints and Limitations: - You should not use strong references to store neighbors. - Assume that `Node` instances are unique based on their `id`. Note: You may need to import Python\'s `weakref` module to use weak references.","solution":"import weakref class Node: def __init__(self, id): self.id = id self.neighbors = [] # List of weak references to neighbors def add_neighbor(node, neighbor, callback=None): Adds a weak reference to the neighbor in the node\'s list of neighbors. Optionally takes a callback function that is called when the neighbor is garbage collected. weak_ref = weakref.ref(neighbor, callback) node.neighbors.append(weak_ref) def remove_garbage_collected_neighbors(node): Removes any entries from the node\'s list of neighbors that have been garbage collected. node.neighbors = [ref for ref in node.neighbors if ref() is not None] def get_live_neighbors(node): Returns a list of currently live neighbors. return [ref() for ref in node.neighbors if ref() is not None]"},{"question":"**Question:** You are given a dataset and a pre-trained Gradient Boosting model to predict a target variable based on several input features. Your task is to implement a function to generate both Partial Dependence Plots (PDP) and Individual Conditional Expectation (ICE) plots for the two most significant features in the dataset. # Instructions: 1. Load the dataset and the pre-trained Gradient Boosting model. 2. Identify the two most significant features (based on feature importance of the model). 3. Implement a function `generate_pdp_ice_plots` with the following signature: ```python def generate_pdp_ice_plots(model, X, y, features): Generates both Partial Dependence Plots (PDP) and Individual Conditional Expectation (ICE) plots. Parameters: - model: Trained scikit-learn model. - X: DataFrame or ndarray of shape (n_samples, n_features). - y: Array-like of shape (n_samples,). - features: List of feature column indices or names to include in the plots. Returns: - None: Displays the plots. ``` 4. Use the provided function `PartialDependenceDisplay.from_estimator` to create the grid of PDP and ICE plots with the `kind=\'both\'` parameter. 5. Ensure your function displays the plots correctly and interprets the combination of PDP and ICE. # Provided Data: - `model`: a trained instance of `GradientBoostingClassifier`. - `X`: feature dataset. - `y`: target variable. # Example Usage: ```python from sklearn.datasets import load_boston from sklearn.ensemble import GradientBoostingRegressor from sklearn.inspection import PartialDependenceDisplay import matplotlib.pyplot as plt # Load dataset (replace with the actual dataset) data = load_boston() X, y = data.data, data.target # Train a model (replace with the pre-trained model) model = GradientBoostingRegressor(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0) model.fit(X, y) # Identify the two most significant features (note: this example uses all features) features = [0, 1] # Replace with feature selection logic # Call the implemented function generate_pdp_ice_plots(model, X, y, features) ``` # Constraints: - You must use the `scikit-learn` library for all operations. - Focus on clear and interpretable visualizations for the plots. # Notes: - The PDP gives the average marginal effect of features. - The ICE plots show individual sample relationships, which help identify heterogeneity in the feature\'s effect.","solution":"from sklearn.inspection import PartialDependenceDisplay import matplotlib.pyplot as plt def generate_pdp_ice_plots(model, X, y, features): Generates both Partial Dependence Plots (PDP) and Individual Conditional Expectation (ICE) plots. Parameters: - model: Trained scikit-learn model. - X: DataFrame or ndarray of shape (n_samples, n_features). - y: Array-like of shape (n_samples,). - features: List of feature column indices or names to include in the plots. Returns: - None: Displays the plots. pdp_display = PartialDependenceDisplay.from_estimator(model, X, features, kind=\'both\') plt.show()"},{"question":"Objective: Demonstrate your understanding of shallow and deep copying in Python, including handling of recursive structures and custom copy mechanisms within user-defined classes. Task: 1. Implement a function `create_copied_structures(original)`, where `original` is a compound object (e.g., list of dictionaries). 2. Return a dictionary containing: - A shallow copy of `original` using `copy.copy()` - A deep copy of `original` using `copy.deepcopy()` 3. Define a user-defined class `MyClass` with the following properties: - Attribute `value` that can be any data type. - Method `__copy__()` that returns a shallow copy of the instance. - Method `__deepcopy__()` that returns a deep copy of the instance. 4. Demonstrate handling of a recursive data structure: - Create a list `recursive_list` containing one dictionary, which contains a reference back to the list itself. - Include this `recursive_list` in the `original` object passed to `create_copied_structures()`. Input Format: - The `original` object that can be a list containing dictionaries or other lists. Output Format: - A dictionary with keys `shallow_copy` and `deep_copy`, containing the respective copies of the `original` object. Performance Constraints: - Ensure the deep copy implementation avoids infinite recursion for recursive structures using memoization. Example: ```python import copy class MyClass: def __init__(self, value): self.value = value def __copy__(self): return MyClass(self.value) def __deepcopy__(self, memo): if self in memo: return memo[self] copy_instance = MyClass(copy.deepcopy(self.value, memo)) memo[self] = copy_instance return copy_instance def create_copied_structures(original): return { \'shallow_copy\': copy.copy(original), \'deep_copy\': copy.deepcopy(original) } # Test with recursive structure recursive_list = [] recursive_dict = {\'self\': recursive_list} recursive_list.append(recursive_dict) original = [recursive_list, {\'key\': MyClass(\'example\')}] result = create_copied_structures(original) print(result) ```","solution":"import copy class MyClass: def __init__(self, value): self.value = value def __copy__(self): return MyClass(self.value) def __deepcopy__(self, memo): if self in memo: return memo[self] copy_instance = MyClass(copy.deepcopy(self.value, memo)) memo[self] = copy_instance return copy_instance def create_copied_structures(original): return { \'shallow_copy\': copy.copy(original), \'deep_copy\': copy.deepcopy(original) } # Test with recursive structure recursive_list = [] recursive_dict = {\'self\': recursive_list} recursive_list.append(recursive_dict) original = [recursive_list, {\'key\': MyClass(\'example\')}] result = create_copied_structures(original) print(result)"},{"question":"**Question: Custom Module Finder and Loader** You are required to demonstrate your understanding of the `importlib` package by implementing a custom module finder and loader for `.txt` files. These files will contain Python source code that should be executable just like regular Python modules. **Objective:** - Implement a class `TxtFileFinder` that finds `.txt` files containing Python code. - Implement a class `TxtFileLoader` that loads and executes Python code from `.txt` files. - Modify the Python import system to use your custom finder and loader. **Requirements:** 1. **TxtFileFinder Class:** - This class should implement the `importlib.abc.MetaPathFinder` abstract base class. - Implement a method `find_spec` that searches for `.txt` files in a specified directory and returns a module spec. - Use the `importlib.util.spec_from_loader` function to create the module spec. 2. **TxtFileLoader Class:** - This class should implement the `importlib.abc.Loader` abstract base class. - Implement the `create_module` method to create a new empty module. - Implement the `exec_module` method to execute the Python code within the `.txt` file. 3. **Integration with Import System:** - Add your custom `TxtFileFinder` to `sys.meta_path` to enable the import system to use your finder and loader. 4. **Validation:** - Create a directory named `txt_modules` and place a `.txt` file within it containing valid Python code (e.g., defining a function or a variable). - Write a script to import the `.txt` file as a module and demonstrate that the Python code within the `.txt` file can be executed. **Example:** ```python # Directory structure: # txt_modules/ #  hello.txt (contains: def greet(): return \\"Hello, World!\\") import sys import importlib.abc import importlib.util class TxtFileFinder(importlib.abc.MetaPathFinder): def find_spec(self, fullname, path, target=None): # Implement logic to find a .txt file and return a module spec pass class TxtFileLoader(importlib.abc.Loader): def create_module(self, spec): # Implement logic to create a new module pass def exec_module(self, module): # Implement logic to execute Python code from a .txt file pass # Add TxtFileFinder to sys.meta_path sys.meta_path.append(TxtFileFinder()) # Test your implementation by placing a .txt file in the `txt_modules` directory # and importing it as a module. import hello print(hello.greet()) # Should output: Hello, World! ``` **Submission Guidelines:** - Provide the implementation of the `TxtFileFinder` and `TxtFileLoader` classes. - Include the directory structure and the content of the `.txt` file used for testing. - Provide the complete script used to test your custom finder and loader. **Constraints:** - You can assume that Python code within `.txt` files is valid and does not require syntax checking. - Ensure that the code handles caching appropriately by invalidating caches when necessary. Good Luck!","solution":"import sys import importlib.abc import importlib.util import os class TxtFileFinder(importlib.abc.MetaPathFinder): def __init__(self, path): self.path = path def find_spec(self, fullname, path, target=None): module_name = fullname.split(\'.\')[-1] filename = os.path.join(self.path, module_name + \'.txt\') if os.path.exists(filename): return importlib.util.spec_from_loader(fullname, TxtFileLoader(filename)) return None class TxtFileLoader(importlib.abc.Loader): def __init__(self, path): self.path = path def create_module(self, spec): return None # Use default module creation semantics def exec_module(self, module): with open(self.path, \'r\') as file: code = file.read() exec(code, module.__dict__) # Directory setup txt_modules_dir = os.path.join(os.path.dirname(__file__), \'txt_modules\') if not os.path.exists(txt_modules_dir): os.makedirs(txt_modules_dir) with open(os.path.join(txt_modules_dir, \'hello.txt\'), \'w\') as file: file.write(\'def greet():n return \\"Hello, World!\\"n\') # Ensure that TxtFileFinder is added to sys.meta_path sys.meta_path.append(TxtFileFinder(txt_modules_dir))"},{"question":"Objective The goal of this assessment is to evaluate your ability to profile Python code, identify performance bottlenecks, and optimize it using Cython. This will demonstrate your understanding of optimizing scikit-learn code for speed. Problem Statement You are provided with a dataset and a Python function `compute_distances` that calculates pairwise Euclidean distances between data points. Your tasks are to: 1. Profile the given Python function to identify performance bottlenecks. 2. Optimize the function using Cython. 3. Compare the performance of the Python and Cython implementations. Instructions 1. **Profiling the Python Code**: - Use the provided `compute_distances` function. - Measure the execution time with %timeit. - Identify and profile the bottlenecks using %prun and line_profiler. 2. **Optimizing with Cython**: - Translate the identified bottleneck part(s) into Cython. - Ensure type declarations are used to achieve significant performance improvements. - Provide both Python and Cython implementations. 3. **Performance Comparison**: - Measure and compare the execution time of the original and optimized versions. - Ensure that both versions produce the same output. Function Signature ```python import numpy as np def compute_distances(X): Compute the pairwise Euclidean distances between data points. Parameters: X (np.ndarray): A 2D array of shape (n_samples, n_features) representing the dataset. Returns: np.ndarray: A 2D array of shape (n_samples, n_samples) representing the pairwise distances. n_samples = X.shape[0] distances = np.zeros((n_samples, n_samples)) for i in range(n_samples): for j in range(i + 1, n_samples): dist = np.sqrt(np.sum((X[i] - X[j])**2)) distances[i, j] = dist distances[j, i] = dist return distances ``` Constraints - `X` should be a numpy array with shape `(n_samples, n_features)`, where `n_samples` can be large (e.g., up to 10,000) and `n_features` can have a moderate size (e.g., up to 100). - The solution should handle large datasets efficiently. Performance Requirements - The optimized Cython implementation should significantly reduce the execution time compared to the original Python implementation. Evaluation Criteria - Correctness: The optimized function should produce the same results as the original. - Performance: The Cython implementation should demonstrate substantial performance improvement. - Code Quality: The code should be well-documented and readable. Submission Submit a Jupyter notebook containing: 1. The original Python function. 2. Profiling results with %timeit, %prun, and line_profiler. 3. The optimized Cython implementation. 4. Performance comparison results. 5. Any assumptions made or additional notes.","solution":"import numpy as np def compute_distances(X): Compute the pairwise Euclidean distances between data points. Parameters: X (np.ndarray): A 2D array of shape (n_samples, n_features) representing the dataset. Returns: np.ndarray: A 2D array of shape (n_samples, n_samples) representing the pairwise distances. n_samples = X.shape[0] distances = np.zeros((n_samples, n_samples)) for i in range(n_samples): for j in range(i + 1, n_samples): dist = np.sqrt(np.sum((X[i] - X[j])**2)) distances[i, j] = dist distances[j, i] = dist return distances"},{"question":"# Audio Device Playback Simulation with `ossaudiodev` **Problem Statement:** You are provided with a series of audio data chunks (as byte sequences) and you need to simulate the playback using the `ossaudiodev` module. Implement a Python function `simulate_audio_playback` that performs the following tasks: 1. Open an audio device for playback (`write` mode). 2. Set the audio parameters: format to `AFMT_S16_LE`, channels to 2 (stereophonic), and sample rate to 44100 (CD quality). 3. Sequentially write each chunk of audio data to the device. 4. Ensure all the audio data has been played. 5. Handle any exceptions that may occur, ensuring the audio device is properly closed in case of errors. **Function Signature:** ```python def simulate_audio_playback(audio_chunks: List[bytes]) -> str: pass ``` **Input:** - `audio_chunks` (List[bytes]): A list of byte sequences, each representing a chunk of audio data to be played. **Output:** - Returns a string `\\"Playback Complete\\"` if playback is successful. - Raises an appropriate exception if an error occurs, including ensuring the device is closed. **Example:** ```python audio_chunks = [b\'x00x01x02x03\', b\'x04x05x06x07\', b\'x08x09x0Ax0B\'] result = simulate_audio_playback(audio_chunks) print(result) # Should output: \\"Playback Complete\\" ``` **Constraints:** - Use the `ossaudiodev` module for audio device interaction. - Ensure proper exception handling and resource management using context managers or try-finally constructs. **Instructions:** 1. Open the audio device using the `ossaudiodev.open` function in write mode. 2. Set the appropriate audio parameters using the methods described. 3. Write the audio chunks to the device using the `writeall` method for guaranteed playback. 4. Ensure that the device is closed even if an error occurs during playback. 5. Return the specified result upon successful completion. **Notes:** - This task requires familiarity with device I/O operations in a Unix-like environment. - Handle the audio device closing properly to prevent resource leaks.","solution":"import ossaudiodev def simulate_audio_playback(audio_chunks): try: # Open the audio device in write mode dsp = ossaudiodev.open(\'w\') # Set audio parameters dsp.setfmt(ossaudiodev.AFMT_S16_LE) dsp.channels(2) dsp.speed(44100) # Write each chunk of audio data to the device sequentially for chunk in audio_chunks: dsp.writeall(chunk) # Close the device after transmission dsp.close() return \\"Playback Complete\\" except Exception as e: # Ensure the audio device is properly closed in case of errors dsp.close() raise e"},{"question":"You are tasked with implementing a simple student gradebook using Python\'s `shelve` module to provide persistent storage for the grading data. The gradebook must support operations to add, update, retrieve, and delete student records, as well as produce a summary. Function Specifications: 1. `open_gradebook(filename: str) -> shelve.Shelf` - Opens the shelve database with the provided file name and returns the shelf object. - Use the context manager to ensure the shelf is closed properly after operations are completed. 2. `add_student(gradebook: shelve.Shelf, student_id: str, name: str, grades: list)` - Adds a new student to the gradebook. - `student_id` is a unique string identifier for the student. - `name` is the student\'s name. - `grades` is a list of grades (integers or floats). 3. `update_grades(gradebook: shelve.Shelf, student_id: str, grades: list)` - Updates the grades for an existing student. 4. `get_student(gradebook: shelve.Shelf, student_id: str) -> dict` - Retrieves the student record as a dictionary containing the `name` and `grades`. - Returns `None` if the student ID does not exist. 5. `delete_student(gradebook: shelve.Shelf, student_id: str)` - Deletes the student record from the gradebook. 6. `summary(gradebook: shelve.Shelf) -> dict` - Returns a summary dictionary containing: - `total_students`: Total number of students. - `average_grades`: Average of all students\' grades. Example usage: ```python filename = \'gradebook.db\' with open_gradebook(filename) as gradebook: add_student(gradebook, \'001\', \'Alice\', [88, 92, 85]) add_student(gradebook, \'002\', \'Bob\', [79, 85, 90]) print(get_student(gradebook, \'001\')) # Output: {\'name\': \'Alice\', \'grades\': [88, 92, 85]} update_grades(gradebook, \'001\', [88, 92, 85, 95]) delete_student(gradebook, \'002\') print(summary(gradebook)) # Output: {\'total_students\': 1, \'average_grades\': 88.75} # The gradebook.shelve file should persist these changes across operations. ``` Constraints: - The `student_id` must be unique. - The grades list must not be empty. - Handle any key errors gracefully, returning `None` or appropriate error messages. - Ensure the `shelve` module is closed properly after operations. Performance: - Aim for efficient use of memory and minimal performance overhead for typical gradebook operations. - Avoid caching large amounts of data unnecessarily by careful use of `writeback` and context management.","solution":"import shelve def open_gradebook(filename: str) -> shelve.Shelf: Opens the shelve database with the provided file name and returns the shelf object. return shelve.open(filename, writeback=True) def add_student(gradebook: shelve.Shelf, student_id: str, name: str, grades: list): Adds a new student to the gradebook. if student_id in gradebook: raise ValueError(\\"Student ID already exists.\\") gradebook[student_id] = {\'name\': name, \'grades\': grades} def update_grades(gradebook: shelve.Shelf, student_id: str, grades: list): Updates the grades for an existing student. if student_id not in gradebook: raise KeyError(\\"Student ID does not exist.\\") gradebook[student_id][\'grades\'] = grades def get_student(gradebook: shelve.Shelf, student_id: str) -> dict: Retrieves the student record as a dictionary containing the `name` and `grades`. return gradebook.get(student_id, None) def delete_student(gradebook: shelve.Shelf, student_id: str): Deletes the student record from the gradebook. if student_id in gradebook: del gradebook[student_id] else: raise KeyError(\\"Student ID does not exist.\\") def summary(gradebook: shelve.Shelf) -> dict: Returns a summary dictionary containing: - `total_students`: Total number of students. - `average_grades`: Average of all students\' grades. total_students = len(gradebook) total_grades = 0 count_grades = 0 for student in gradebook.values(): total_grades += sum(student[\'grades\']) count_grades += len(student[\'grades\']) average_grades = total_grades / count_grades if count_grades > 0 else 0 return { \'total_students\': total_students, \'average_grades\': average_grades }"},{"question":"In this assessment, you are required to implement a Python function that loads a specified Python module from a given ZIP file archive and executes its main function. The ZIP file will contain a single Python module with a known structure. Task 1. **Create a function `load_and_run_module(path_to_zip, module_name)`**: - **Inputs**: - `path_to_zip` (str): The file path to the ZIP archive. - `module_name` (str): The name of the module to be loaded from the ZIP archive. - **Outputs**: - This function should return whatever the `main()` function of the loaded module returns. 2. Add necessary error handling: - If the `ZIP archive` doesn\'t exist or is invalid, print `Invalid ZIP archive`. - If the `module_name` doesn\'t exist in the ZIP archive, print `Module not found in the ZIP archive`. - If the `main()` function does not exist in the module or cannot be executed, print `Main function does not exist or execution failed`. 3. Use **`zipimport`** to load the module from the ZIP file. ```python import zipimport def load_and_run_module(path_to_zip, module_name): try: importer = zipimport.zipimporter(path_to_zip) module = importer.load_module(module_name) if hasattr(module, \'main\') and callable(getattr(module, \'main\')): return module.main() else: print(\\"Main function does not exist or execution failed\\") except zipimport.ZipImportError: print(\\"Invalid ZIP archive\\") except ImportError: print(\\"Module not found in the ZIP archive\\") ``` Constraints - The ZIP file will be well-formed and contain only `.py` or `.pyc` files for this task\'s purpose. - The `module_name` provided will match a module present within the ZIP file. Performance Requirements - The solution should handle ZIP files of reasonable size without significant performance degradation. - Consider caching mechanisms if multiple accesses to the same ZIP file are expected (though not required for this task). Example Usage Suppose we have a ZIP file `example.zip` containing a Python file `module.py` with a `main()` function. ```python # Contents of module.py def main(): return \\"Hello from main\\" ``` Here\'s how you would use the `load_and_run_module` function: ```python result = load_and_run_module(\'example.zip\', \'module\') print(result) # Output: Hello from main ``` Make sure to test your function thoroughly with different scenarios to ensure robustness.","solution":"import zipimport def load_and_run_module(path_to_zip, module_name): try: importer = zipimport.zipimporter(path_to_zip) module = importer.load_module(module_name) if hasattr(module, \'main\') and callable(getattr(module, \'main\')): return module.main() else: print(\\"Main function does not exist or execution failed\\") except zipimport.ZipImportError: print(\\"Invalid ZIP archive\\") except ImportError: print(\\"Module not found in the ZIP archive\\")"},{"question":"# Advanced Python 310 Coding Assessment: Custom Email Policy Problem Statement The \\"email.policy\\" module allows for extensive control over the handling of email messages by defining custom behaviors for parsing, generating, and validating emails. Your task is to create a custom email policy with specific attributes and demonstrate its use in reading, modifying, and saving an email message. Requirements: 1. **Create a Custom Policy**: - Define a custom policy called `CustomPolicy` that inherits from `email.policy.EmailPolicy`. - The custom policy should: - Set `max_line_length` to 100. - Set `linesep` to `rn`. - Enable `utf8` for handling non-ASCII characters in headers. - Set `raise_on_defect` to `True`. 2. **Read an Email Message**: - Read an email message from a provided file (`sample_email.txt`) using your custom policy. 3. **Modify Email Headers**: - Modify the `Subject` header of this email to include a prefix `[IMPORTANT]`. - Change the `From` and `To` headers to new email addresses as specified. 4. **Save the Modified Email**: - Save the modified email to a new file (`modified_email.txt`) using the platform\'s native line separators. Input Format: 1. A file named `sample_email.txt` containing an email message in standard email format: ``` From: sender@example.com To: recipient@example.com Subject: Meeting Reminder Dear team, Please be reminded about the meeting tomorrow at 10 AM. Regards, Sender. ``` 2. New `From` and `To` email addresses will be as follows: - From: `admin@newdomain.com` - To: `team@newdomain.com` Output Format: 1. A file named `modified_email.txt` containing the modified email message with the specified changes. Constraints: 1. Ensure proper handling and encoding of headers as specified in the custom policy. 2. The code must handle any defects by raising exceptions as defined by the custom policy. Example Code Skeleton: ```python from email import message_from_binary_file from email.generator import BytesGenerator from email.policy import EmailPolicy import os # Step 1: Define CustomPolicy class CustomPolicy(EmailPolicy): def __init__(self, **kwargs): super().__init__(max_line_length=100, linesep=\'rn\', utf8=True, raise_on_defect=True, **kwargs) # Step 2: Read the email message from file def read_email(file_path, policy): with open(file_path, \'rb\') as f: return message_from_binary_file(f, policy=policy) # Step 3: Modify email headers def modify_email_headers(message): message[\'Subject\'] = f\\"[IMPORTANT] {message[\'Subject\']}\\" message.replace_header(\'From\', \'admin@newdomain.com\') message.replace_header(\'To\', \'team@newdomain.com\') # Step 4: Save the modified email def save_email(message, file_path): with open(file_path, \'wb\') as f: f.write(message.as_bytes(policy=message.policy.clone(linesep=os.linesep))) if __name__ == \\"__main__\\": custom_policy = CustomPolicy() email_message = read_email(\'sample_email.txt\', custom_policy) modify_email_headers(email_message) save_email(email_message, \'modified_email.txt\') ``` Implement the full code to demonstrate the solution\'s functionality. Ensure that the code handles header encoding and folding as specified by the policy.","solution":"from email import message_from_binary_file from email.generator import BytesGenerator from email.policy import EmailPolicy import os # Step 1: Define CustomPolicy class CustomPolicy(EmailPolicy): def __init__(self, **kwargs): super().__init__(max_line_length=100, linesep=\'rn\', utf8=True, raise_on_defect=True, **kwargs) # Step 2: Read the email message from file def read_email(file_path, policy): with open(file_path, \'rb\') as f: return message_from_binary_file(f, policy=policy) # Step 3: Modify email headers def modify_email_headers(message): message.replace_header(\'Subject\', f\\"[IMPORTANT] {message[\'Subject\']}\\") message.replace_header(\'From\', \'admin@newdomain.com\') message.replace_header(\'To\', \'team@newdomain.com\') # Step 4: Save the modified email def save_email(message, file_path): with open(file_path, \'wb\') as f: f.write(message.as_bytes(policy=message.policy.clone(linesep=os.linesep))) if __name__ == \\"__main__\\": custom_policy = CustomPolicy() email_message = read_email(\'sample_email.txt\', custom_policy) modify_email_headers(email_message) save_email(email_message, \'modified_email.txt\')"},{"question":"You are tasked with benchmarking the execution time of different methods for filtering even numbers from a list, using the `timeit` module to measure the performance. Implement a function `benchmark_filter_methods` that will create three different implementations to filter out even numbers from a list, and use `timeit` to measure the execution time of each implementation. # Implementations 1. **List Comprehension**: Use a list comprehension to filter even numbers. 2. **Filter Function**: Use the built-in `filter()` function along with a lambda function to filter even numbers. 3. **For Loop**: Use a for loop to manually iterate through the list and collect even numbers. # Function Signature ```python def benchmark_filter_methods(n: int) -> dict: pass ``` # Input - `n` (int): The number of elements in the list to be tested, where the list will contain numbers from 0 to `n-1`. # Output - A dictionary with the names of the methods as keys and their corresponding execution times (in seconds) as values. # Constraints and Performance Requirements - The list size `n` will be a positive integer up to 10^6. - The function should return the timing results in a reasonable time frame for large values of `n`. # Example ```python result = benchmark_filter_methods(100000) print(result) # Expected output (timing values will vary): # { # \'list_comprehension\': 0.01849524, # \'filter_function\': 0.02738201, # \'for_loop\': 0.02447165 # } ``` # Additional Information - Use the `timeit` module\'s callable interface to measure the execution time. - Ensure that the benchmarking uses the same list of numbers for all three methods to get a fair comparison. - Handle the setup and measurement within the `benchmark_filter_methods` function.","solution":"import timeit def benchmark_filter_methods(n: int) -> dict: Benchmarks three different methods of filtering even numbers from a list. Parameters: n (int): Number of elements in the list to be tested. Returns: dict: A dictionary with the names of the methods as keys and their corresponding execution times as values. # Creating the list of numbers from 0 to n-1 test_list = list(range(n)) # Implementations of the three methods def list_comprehension(): return [x for x in test_list if x % 2 == 0] def filter_function(): return list(filter(lambda x: x % 2 == 0, test_list)) def for_loop(): result = [] for x in test_list: if x % 2 == 0: result.append(x) return result # Benchmarking timings = { \'list_comprehension\': timeit.timeit(list_comprehension, number=1), \'filter_function\': timeit.timeit(filter_function, number=1), \'for_loop\': timeit.timeit(for_loop, number=1) } return timings"},{"question":"**Problem Statement:** You are tasked with building a system for handling a binary data file containing user records. Each record in the file includes the following fields: 1. An integer `user_id` (4 bytes, signed, little-endian). 2. A username as a fixed-length string of 20 bytes. If the username is shorter than 20 bytes, it should be padded with null bytes (`\'0\'`). 3. An age as an unsigned integer (1 byte). 4. A height in centimeters as a 2-byte unsigned short (little-endian). Write a Python function `read_user_records(filepath)` that reads user records from a binary file and returns a list of dictionaries where each dictionary contains the keys `user_id`, `username`, `age`, and `height`. **Function Signature:** ```python def read_user_records(filepath: str) -> List[Dict[str, Union[int, str]]]: pass ``` **Input:** - `filepath` (str): the path to the binary file containing user records. **Output:** - returns a list of dictionaries, where each dictionary represents a user record with keys `user_id`, `username`, `age`, and `height`. **Constraints:** - The binary file might contain multiple user records. - The function should handle varying numbers of user records in the file. - The usernames are up to 20 characters long. If they are shorter than 20 characters, they are padded with null bytes which should be stripped when returning the string. - You can assume the binary file is properly formatted and contains valid data. **Example:** Suppose the binary file contains data equivalent to the following struct format: ``` user_id: 1, username: \\"Alice\\", age: 30, height: 165 user_id: 2, username: \\"Bob\\", age: 25, height: 180 ``` The resulting list of dictionaries should be: ```python [ {\\"user_id\\": 1, \\"username\\": \\"Alice\\", \\"age\\": 30, \\"height\\": 165}, {\\"user_id\\": 2, \\"username\\": \\"Bob\\", \\"age\\": 25, \\"height\\": 180} ] ``` **Note:** - Utilize the `struct` module to handle packing and unpacking of binary data. **Hints:** - Use `struct.unpack` to read and interpret the binary data according to the specified format. - Calculate the size of each user record to read the correct number of bytes from the file. - Use `str.rstrip` to remove padding null bytes from usernames. Good luck!","solution":"import struct from typing import List, Dict, Union def read_user_records(filepath: str) -> List[Dict[str, Union[int, str]]]: records = [] user_record_format = \'i20sBH\' user_record_size = struct.calcsize(user_record_format) with open(filepath, \'rb\') as file: while True: chunk = file.read(user_record_size) if not chunk: break user_id, username, age, height = struct.unpack(user_record_format, chunk) username = username.decode(\'utf-8\').rstrip(\'0\') records.append({ \\"user_id\\": user_id, \\"username\\": username, \\"age\\": age, \\"height\\": height }) return records"},{"question":"# Compression and Checksum with `zlib` **Objective**: Design a function to compress a large data stream in chunks and compute a running checksum for the compressed and original data. **Problem Statement**: You are given a large input file that cannot fit entirely into memory. Your task is to implement two functions using the `zlib` module in Python: - `compress_and_checksum(input_file_path: str, compressed_file_path: str, chunk_size: int) -> dict` - `verify_checksum(input_file_path: str, compressed_file_path: str, chunk_size: int) -> bool` Function Descriptions 1. **`compress_and_checksum(input_file_path: str, compressed_file_path: str, chunk_size: int) -> dict`**: - Reads the input file in chunks of specified size and compresses each chunk using `zlib`. - Computes and keeps track of the Adler-32 checksum for both the original and compressed data. - Writes the compressed data to the output file. - Returns a dictionary containing the checksums for the original and compressed data as follows: ```python { \\"original_checksum\\": <checksum of original data>, \\"compressed_checksum\\": <checksum of compressed data> } ``` - Raise a `zlib.error` if any compression error occurs. 2. **`verify_checksum(input_file_path: str, compressed_file_path: str, chunk_size: int) -> bool`**: - Decompresses the compressed file in chunks and computes the checksums similarly to the first function. - Compares the checksums obtained from the decompressed data with those from the original uncompressed data to ensure data integrity. - Returns `True` if the checksums match, otherwise `False`. Input and Output: - The `input_file_path` and `compressed_file_path` are strings representing the file paths. - `chunk_size` is an integer specifying the size of the chunks to read and compress in bytes. - Both functions should handle files potentially larger than available memory by processing them in a streaming manner. Constraints: - Ensure your solution does not load the entire file into memory. - Efficiently handle files with different sizes and compression levels. - Use `zlib` methods for compression and decompression. - Consider edge cases such as empty files and very small chunk sizes. Make sure to include error handling for file operations and `zlib` operations to avoid runtime crashes. Here is a method signature to get you started: ```python import zlib def compress_and_checksum(input_file_path: str, compressed_file_path: str, chunk_size: int) -> dict: # Implementation goes here pass def verify_checksum(input_file_path: str, compressed_file_path: str, chunk_size: int) -> bool: # Implementation goes here pass ``` **Example Usage**: ```python checksums = compress_and_checksum(\'large_input.txt\', \'compressed_output.z\', 1024) print(checksums) # Output: {\'original_checksum\': <checksum1>, \'compressed_checksum\': <checksum2>} is_valid = verify_checksum(\'large_input.txt\', \'compressed_output.z\', 1024) print(is_valid) # Output: True or False ``` # Assessment Criteria: - Correctness: Ensure the functions compress and decompress data correctly and compute accurate checksums. - Efficiency: Handle large files efficiently without loading them entirely into memory. - Robustness: Include error handling to manage potential issues with file operations and `zlib` operations. - Readability: Write clean and maintainable code with appropriate comments and docstrings.","solution":"import zlib def compress_and_checksum(input_file_path: str, compressed_file_path: str, chunk_size: int) -> dict: original_checksum = 1 # Adler-32 seed value compressed_checksum = 1 # Adler-32 seed value with open(input_file_path, \'rb\') as input_file, open(compressed_file_path, \'wb\') as output_file: compressor = zlib.compressobj() while True: chunk = input_file.read(chunk_size) if not chunk: break original_checksum = zlib.adler32(chunk, original_checksum) compressed_chunk = compressor.compress(chunk) compressed_checksum = zlib.adler32(compressed_chunk, compressed_checksum) if compressed_chunk: output_file.write(compressed_chunk) # Write any remaining compressed data compressed_chunk = compressor.flush() compressed_checksum = zlib.adler32(compressed_chunk, compressed_checksum) output_file.write(compressed_chunk) return { \\"original_checksum\\": original_checksum, \\"compressed_checksum\\": compressed_checksum } def verify_checksum(input_file_path: str, compressed_file_path: str, chunk_size: int) -> bool: original_checksum = 1 # Adler-32 seed value decompressed_checksum = 1 # Adler-32 seed value with open(input_file_path, \'rb\') as input_file, open(compressed_file_path, \'rb\') as compressed_file: decompressor = zlib.decompressobj() while True: chunk = input_file.read(chunk_size) if not chunk: break original_checksum = zlib.adler32(chunk, original_checksum) while True: compressed_chunk = compressed_file.read(chunk_size) if not compressed_chunk: break decompressed_chunk = decompressor.decompress(compressed_chunk) decompressed_checksum = zlib.adler32(decompressed_chunk, decompressed_checksum) # Decompress remaining data decompressed_chunk = decompressor.flush() decompressed_checksum = zlib.adler32(decompressed_chunk, decompressed_checksum) return original_checksum == decompressed_checksum"},{"question":"You are required to implement a function that reads and processes chunks from a file-like object using the `chunk` module. The provided file contains multiple chunks, and your task is to extract and print the `ID`, `size`, and first 8 bytes of data from each chunk. Function Signature ```python def process_chunks(file: \'file-like object\') -> None: ``` Input - `file`: a file-like object opened in binary mode, containing multiple chunks. Output - Prints the `ID`, `size`, and first 8 bytes of data for each chunk in the file. Constraints - Assume the file contains a sequence of valid EA IFF 85 chunks. - Each chunk is correctly formed and may be aligned on 2-byte boundaries. - You must use the `chunk` module provided by Python for reading and processing the chunks. Example Consider a file-like object containing the following chunks (in hex format for simplicity): ``` 49443302 00000008 74657374 74657374 # Chunk ID \'ID3\', size 8, data \'testtest\' 49443304 0000000A 64617461 6D6F7265... # Chunk ID \'ID4\', size 10, data \'datamore...\' ``` Calling `process_chunks(file)` should output: ``` ID: ID3, Size: 8, Data: b\'testtest\' ID: ID4, Size: 10, Data: b\'datamore\' ``` Notes - You do not need to close the file; assume it will be handled externally. - Print the first 8 bytes of data or all data if the chunk size is less than 8 bytes. - Be sure to handle padding and alignment correctly as per the `chunk` class specifications.","solution":"import chunk def process_chunks(file): Reads and processes chunks from a file-like object, printing ID, size, and first 8 bytes of data from each chunk. Args: file: A file-like object opened in binary mode containing multiple chunks. Returns: None while True: try: # Read the next chunk from the file ch = chunk.Chunk(file, align=True, bigendian=False) # Read first 8 bytes of the chunk\'s data data = ch.read(min(8, ch.getsize())) # Print the chunk\'s ID, size, and first 8 bytes of data print(f\\"ID: {ch.getname().decode()}, Size: {ch.getsize()}, Data: {data}\\") # Skip any remaining data in the chunk chunk_left = ch.getsize() - len(data) if chunk_left > 0: ch.read(chunk_left) # Close the chunk ch.close() except EOFError: break"},{"question":"# Question: Implementing and Using Custom Pairwise Distance and Kernels You are tasked with implementing custom distance metrics and kernel functions using the scikit-learn library. Specifically, you will: 1. Implement a new custom distance metric. 2. Implement a new custom kernel function. 3. Use these in a simple machine learning application (like clustering or classification). Part 1: Custom Distance Metric Implement a function `custom_euclidean_distance` that computes the Euclidean distance but raises each coordinate difference to the power of 3 before summing. ```python import numpy as np from sklearn.metrics import pairwise_distances def custom_euclidean_distance(X, Y=None): Compute the custom Euclidean distance between each pair of the two collections of inputs. Parameters: - X: array of shape (n_samples_X, n_features) - Y: array of shape (n_samples_Y, n_features), defaults to None Returns: - distances: array of shape (n_samples_X, n_samples_Y) def distance_func(a, b): return np.sum(np.abs(a - b) ** 3) ** (1/3) return pairwise_distances(X, Y, metric=distance_func) ``` Part 2: Custom Kernel Function Implement a function `custom_sigmoid_kernel` that computes a modified sigmoid kernel, with the formula: [ k(x, y) = tanh(gamma x^T y + c_0 + 1) ] ```python import numpy as np from sklearn.metrics.pairwise import pairwise_kernels def custom_sigmoid_kernel(X, Y=None, gamma=0.1, coef0=0): Compute the custom sigmoid kernel between each pair of the two collections of inputs. Parameters: - X: array of shape (n_samples_X, n_features) - Y: array of shape (n_samples_Y, n_features), defaults to None - gamma: float, default is 0.1 - coef0: float, default is 0 Returns: - kernel: array of shape (n_samples_X, n_samples_Y) def kernel_func(a, b): return np.tanh(gamma * np.dot(a, b) + coef0 + 1) return pairwise_kernels(X, Y, metric=kernel_func) ``` Part 3: Application Using the above custom functions, perform clustering using k-means with the custom distance metric as the distance measure and an SVM classifier using the custom kernel for a small dataset. ```python from sklearn.cluster import KMeans from sklearn.svm import SVC # Sample dataset X_train = np.array([[2, 3], [3, 5], [5, 8], [8, 13], [4, 5], [6, 7]]) X_test = np.array([[3, 4], [7, 10]]) # Part 3a: K-Means Clustering with Custom Distance Metric kmeans = KMeans(n_clusters=2, random_state=0) kmeans.fit(X_train) # Assigning clusters to new data points clusters = kmeans.predict(X_test) # Part 3b: SVM Classification with Custom Kernel y_train = np.array([0, 1, 0, 1, 0, 1]) svm = SVC(kernel=custom_sigmoid_kernel) svm.fit(X_train, y_train) # Predicting classes for test data predictions = svm.predict(X_test) print(\\"Clusters assigned to test data:\\", clusters) print(\\"Predictions for test data:\\", predictions) ``` **Note:** - Ensure all necessary imports are included. - Use numpy arrays as the input for all functions. - The clustering and classification outputs should demonstrate a basic understanding of using custom metrics/kernels in scikit-learn. Expected Outputs: For the provided sample data, the specific results can vary due to initialization randomness, but the structure and use of custom metrics and kernels in the k-means clustering and SVM classifier should be clear and correct.","solution":"import numpy as np from sklearn.metrics import pairwise_distances, pairwise_kernels from sklearn.cluster import KMeans from sklearn.svm import SVC def custom_euclidean_distance(X, Y=None): Compute the custom Euclidean distance between each pair of the two collections of inputs. Parameters: - X: array of shape (n_samples_X, n_features) - Y: array of shape (n_samples_Y, n_features), defaults to None Returns: - distances: array of shape (n_samples_X, n_samples_Y) def distance_func(a, b): return np.sum(np.abs(a - b) ** 3) ** (1/3) return pairwise_distances(X, Y, metric=distance_func) def custom_sigmoid_kernel(X, Y=None, gamma=0.1, coef0=0): Compute the custom sigmoid kernel between each pair of the two collections of inputs. Parameters: - X: array of shape (n_samples_X, n_features) - Y: array of shape (n_samples_Y, n_features), defaults to None - gamma: float, default is 0.1 - coef0: float, default is 0 Returns: - kernel: array of shape (n_samples_X, n_samples_Y) def kernel_func(a, b): return np.tanh(gamma * np.dot(a, b) + coef0 + 1) return pairwise_kernels(X, Y, metric=kernel_func) # Sample dataset for application X_train = np.array([[2, 3], [3, 5], [5, 8], [8, 13], [4, 5], [6, 7]]) X_test = np.array([[3, 4], [7, 10]]) y_train = np.array([0, 1, 0, 1, 0, 1]) # Part 3a: K-Means Clustering with Custom Distance Metric kmeans = KMeans(n_clusters=2, random_state=0) kmeans.fit(X_train) # Assigning clusters to new data points clusters = kmeans.predict(X_test) # Part 3b: SVM Classification with Custom Kernel svm = SVC(kernel=custom_sigmoid_kernel) svm.fit(X_train, y_train) # Predicting classes for test data predictions = svm.predict(X_test) print(\\"Clusters assigned to test data:\\", clusters) print(\\"Predictions for test data:\\", predictions)"},{"question":"# Advanced Python Programming Assessment Question: You are tasked with creating a log monitoring system for a server. The system should read logs generated by different applications, filter them based on specified criteria, and categorize them into different severity levels such as INFO, WARNING, and ERROR. To achieve this, implement a Python function `monitor_logs` that performs the following: 1. **Input:** - A list of log entries, where each log entry is a dictionary containing the following keys: - `\'timestamp\'`: A string representing the date and time when the log was created (e.g., `\'2023-10-05 14:23:45\'`). - `\'source\'`: A string representing the source of the log (e.g., `\'app1\'`, `\'app2\'`). - `\'severity\'`: A string representing the severity level of the log (e.g., `\'INFO\'`, `\'WARNING\'`, `\'ERROR\'`). - `\'message\'`: A string representing the log message. 2. **Process:** - Filter the logs based on a given criterion, such as the source or severity level. - Categorize the logs into separate lists based on their severity levels (`INFO`, `WARNING`, `ERROR`). 3. **Output:** - A dictionary with three keys: `\'INFO\'`, `\'WARNING\'`, and `\'ERROR\'`. Each key should correspond to a list of log entries filtered and categorized by their severity levels. Each list should contain the log entries as dictionaries. 4. **Constraints:** - You may assume that the input logs always contain valid data according to the specified structure. - Utilize the `logging` module to implement this feature in an efficient way. 5. **Performance Requirements:** - The solution should be able to handle large lists of log entries efficiently. Example Usage: ```python logs = [ {\'timestamp\': \'2023-10-05 14:23:45\', \'source\': \'app1\', \'severity\': \'INFO\', \'message\': \'Application started\'}, {\'timestamp\': \'2023-10-05 14:24:01\', \'source\': \'app2\', \'severity\': \'ERROR\', \'message\': \'Unhandled exception occurred\'}, {\'timestamp\': \'2023-10-05 14:24:15\', \'source\': \'app1\', \'severity\': \'WARNING\', \'message\': \'Low disk space\'}, {\'timestamp\': \'2023-10-05 14:25:30\', \'source\': \'app3\', \'severity\': \'INFO\', \'message\': \'User login successful\'} ] def monitor_logs(logs): # Your implementation here result = monitor_logs(logs) print(result) # Expected Output: # { # \'INFO\': [ # {\'timestamp\': \'2023-10-05 14:23:45\', \'source\': \'app1\', \'severity\': \'INFO\', \'message\': \'Application started\'}, # {\'timestamp\': \'2023-10-05 14:25:30\', \'source\': \'app3\', \'severity\': \'INFO\', \'message\': \'User login successful\'} # ], # \'WARNING\': [ # {\'timestamp\': \'2023-10-05 14:24:15\', \'source\': \'app1\', \'severity\': \'WARNING\', \'message\': \'Low disk space\'} # ], # \'ERROR\': [ # {\'timestamp\': \'2023-10-05 14:24:01\', \'source\': \'app2\', \'severity\': \'ERROR\', \'message\': \'Unhandled exception occurred\'} # ] # } ``` Implement the `monitor_logs` function and ensure your solution meets the outlined requirements and constraints.","solution":"def monitor_logs(logs, filter_criteria=None): Filters and categorizes log entries based on their severity levels. Args: logs (list): A list of log entries, where each entry is a dictionary. filter_criteria (dict, optional): A dictionary with keys that can filter the logs (e.g., {\'source\': \'app1\', \'severity\': \'ERROR\'}). Returns: dict: A dictionary with keys \'INFO\', \'WARNING\', \'ERROR\' each containing a list of log entries belonging to that category. categorized_logs = {\'INFO\': [], \'WARNING\': [], \'ERROR\': []} for log in logs: if filter_criteria: match = True for key, value in filter_criteria.items(): if log.get(key) != value: match = False break if not match: continue severity = log[\'severity\'] if severity in categorized_logs: categorized_logs[severity].append(log) return categorized_logs"},{"question":"**Objective:** Implement a Python function that dynamically executes a module by its name using the `runpy` module, captures specific global variables after execution, and returns them. **Problem Statement:** You need to write a function `execute_and_capture(module_name: str, global_vars: list[str]) -> dict` that performs the following: 1. Uses the `runpy.run_module` function to execute a module by its name. 2. After execution, captures the values of the specified global variables from the module\'s globals dictionary. 3. Returns a dictionary where the keys are the names of the global variables and the values are these variables\' values in the module\'s context. **Function Signature:** ```python def execute_and_capture(module_name: str, global_vars: list[str]) -> dict: ``` **Parameters:** - `module_name` (str): The absolute name of the module to be executed. - `global_vars` (list of str): A list of global variable names to capture from the module after execution. **Returns:** - `dict`: A dictionary mapping the specified global variable names to their respective values in the module\'s globals dictionary. **Constraints:** - You may assume that the module specified by `module_name` exists and can be executed without errors. - The global variables specified in `global_vars` will be present in the module\'s context after execution. **Example:** ```python # Assume there is a module named \\"sample_module\\" with the following content: # x = 10 # y = 20 # z = 30 result = execute_and_capture(\\"sample_module\\", [\\"x\\", \\"y\\"]) print(result) # Output: {\\"x\\": 10, \\"y\\": 20} result = execute_and_capture(\\"sample_module\\", [\\"z\\"]) print(result) # Output: {\\"z\\": 30} ``` **Additional Requirements:** - Your implementation should account for importing and executing the module safely. - Make sure to handle any necessary imports within your function. - You should not modify the module\'s source code; your function should work purely through the `runpy` module and standard Python operations. **Evaluation Criteria:** - Correctness: The function must correctly execute the module and capture the specified global variables. - Robustness: The function should handle any edge cases as specified in the constraints. - Code Quality: The code should be clean, readable, and well-documented.","solution":"def execute_and_capture(module_name: str, global_vars: list[str]) -> dict: Executes a module by its name and captures specific global variables. Parameters: module_name (str): The name of the module to execute. global_vars (list of str): List of global variable names to capture. Returns: dict: Dictionary mapping global variable names to their values. import runpy # Run the module and capture its global variables module_globals = runpy.run_module(module_name) # Capture the requested global variables captured_vars = {var: module_globals[var] for var in global_vars if var in module_globals} return captured_vars"},{"question":"# Question: Covariance Estimation with Different Methods You are provided with a dataset and your task is to estimate its covariance matrix using different methods available in `sklearn.covariance`. Implement a function `compare_covariance_estimators` that takes a 2D numpy array `data` as input and outputs a dictionary with the covariance matrices estimated by the following methods: 1. **Empirical (Maximum Likelihood) Covariance** 2. **Shrunk Covariance with a shrinkage coefficient of 0.5** 3. **Ledoit-Wolf Shrinkage** 4. **Oracle Approximating Shrinkage (OAS)** 5. **Minimum Covariance Determinant (Robust Estimator)** Input - `data`: 2D numpy array of shape `(n_samples, n_features)` representing the dataset. Output - A dictionary with keys: - `\\"empirical\\"`: Covariance matrix estimated using empirical covariance. - `\\"shrunk\\"`: Covariance matrix estimated using shrunk covariance. - `\\"ledoit_wolf\\"`: Covariance matrix estimated using Ledoit-Wolf shrinkage. - `\\"oas\\"`: Covariance matrix estimated using Oracle Approximating Shrinkage. - `\\"robust\\"`: Covariance matrix estimated using Minimum Covariance Determinant. Constraints - Ensure that the data does not have missing values. - Assume that the dataset is centered (i.e., the mean of each feature is 0). Performance Requirements - The solutions should efficiently handle datasets with up to 10,000 samples and 100 features. Example: ```python import numpy as np # Example dataset with 100 samples and 5 features data = np.random.randn(100, 5) result = compare_covariance_estimators(data) # result should be a dictionary with 5 keys, each containing a covariance matrix of shape (5, 5) ``` Implementation: Make sure to use the appropriate classes and functions from `sklearn.covariance`. Follow good coding practices, including appropriate imports, function definitions, and docstrings.","solution":"import numpy as np from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, MinCovDet def compare_covariance_estimators(data): Estimates covariance matrix of the input data using various methods. Parameters: data (np.ndarray): 2D array of shape (n_samples, n_features) Returns: dict: Dictionary of covariance matrices estimated by different methods results = {} # Empirical Covariance empirical_cov = EmpiricalCovariance().fit(data).covariance_ results[\'empirical\'] = empirical_cov # Shrunk Covariance with a shrinkage coefficient of 0.5 shrunk_cov = ShrunkCovariance(shrinkage=0.5).fit(data).covariance_ results[\'shrunk\'] = shrunk_cov # Ledoit-Wolf Shrinkage lw_cov = LedoitWolf().fit(data).covariance_ results[\'ledoit_wolf\'] = lw_cov # Oracle Approximating Shrinkage (OAS) oas_cov = OAS().fit(data).covariance_ results[\'oas\'] = oas_cov # Minimum Covariance Determinant (Robust Estimator) robust_cov = MinCovDet().fit(data).covariance_ results[\'robust\'] = robust_cov return results"},{"question":"# **Seaborn Configuration Challenge** In this coding assessment, you are required to create a function that generates and customizes a seaborn plot based on given parameters. You need to utilize the seaborn configuration options discussed in the provided documentation to achieve the desired plot settings. **Function Signature** ```python def customized_plot(data, plot_type, theme_params, display_params): Generate and customize a seaborn plot based on given parameters. Args: data (pandas.DataFrame): The input data for the plot. plot_type (str): The type of seaborn plot to create (\'scatter\', \'line\', etc.). theme_params (dict): Dictionary of theme parameters to be updated. display_params (dict): Dictionary of display configuration parameters. Returns: None: The function should display the plot. ``` **Parameters** - **data**: A pandas DataFrame containing the data to be plotted. - **plot_type**: A string specifying the type of seaborn plot to create. Valid options include \'scatter\', \'line\', \'bar\', etc. - **theme_params**: A dictionary where keys are theme configuration parameter names and values are the values to set. - **display_params**: A dictionary where keys are display configuration parameter names and values are the values to set. **Requirements** 1. **Update Plot Theme**: - Apply individual parameters from `theme_params` to the plot theme using `so.Plot.config.theme`. - Use the `axes_style` function from seaborn for overall theme changes if specified. 2. **Configure Display**: - Update the display settings using `so.Plot.config.display`. 3. **Generate Plot**: - Based on `plot_type`, generate the corresponding seaborn plot using `so.Plot` and the provided `data`. 4. **Display the Plot**: - Ensure the plot is displayed with the specified configurations. **Example** ```python import seaborn as sns import pandas as pd # Sample data data = pd.DataFrame({ \'x\': range(10), \'y\': [2, 3, 5, 7, 11, 13, 17, 19, 23, 29] }) # Sample theme and display parameters theme_params = {\'axes.facecolor\': \'lightgrey\'} display_params = {\'format\': \'svg\', \'hidpi\': False, \'scaling\': 0.75} customized_plot(data, \'scatter\', theme_params, display_params) ``` The function will apply the specified theme and display settings, generate a scatter plot with the sample data, and display the customized plot. **Constraints** - Assume the input data is always valid and contains the necessary columns for the specified plot type. - Handle potential conflicts in display settings gracefully. **Evaluation Criteria** - Correctness: The function should correctly apply the specified theme and display configurations. - Usage of seaborn and matplotlib: Proper utilization and combination of seaborn and matplotlib configuration options. - Functionality: The plot should be generated and displayed as expected.","solution":"import matplotlib.pyplot as plt import seaborn as sns import pandas as pd def customized_plot(data, plot_type, theme_params, display_params): Generate and customize a seaborn plot based on given parameters. Args: data (pandas.DataFrame): The input data for the plot. plot_type (str): The type of seaborn plot to create (\'scatter\', \'line\', etc.). theme_params (dict): Dictionary of theme parameters to be updated. display_params (dict): Dictionary of display configuration parameters. Returns: None: The function should display the plot. # Update theme parameters if \'style\' in theme_params: sns.set_style(theme_params.pop(\'style\')) sns.set_theme(**theme_params) # Setting display parameters for param, value in display_params.items(): plt.rcParams[param] = value # Generate the plot based on the plot_type if plot_type == \'scatter\': sns.scatterplot(data=data) elif plot_type == \'line\': sns.lineplot(data=data) elif plot_type == \'bar\': sns.barplot(data=data) elif plot_type == \'hist\': sns.histplot(data=data) else: raise ValueError(\\"Invalid plot type specified\\") # Display the plot plt.show()"},{"question":"# Question: Implementing a Synthetic Data Generator with PyTorch Distributions **Objective:** Write a function `generate_synthetic_data` to generate synthetic data using a combination of various probability distributions provided in the `torch.distributions` module. The function should be capable of producing a structured dataset comprising continuous and categorical features. **Function Signature:** ```python def generate_synthetic_data(n_samples: int, n_continuous: int, n_categorical: int, categorical_levels: List[int]) -> Tuple[torch.Tensor, torch.Tensor]: Generates synthetic data with specified number of continuous and categorical features. Args: n_samples (int): Number of samples to generate. n_continuous (int): Number of continuous features. n_categorical (int): Number of categorical features. categorical_levels (List[int]): A list specifying the number of levels for each categorical feature. Returns: Tuple[torch.Tensor, torch.Tensor]: A tuple containing two tensors: - Continuous features: A tensor of shape (n_samples, n_continuous) - Categorical features: A tensor of shape (n_samples, n_categorical) pass ``` **Details:** 1. **Continuous Features:** - Generate continuous features using the `Normal` distribution. Assume a standard normal distribution with mean 0 and standard deviation 1 for all continuous features. 2. **Categorical Features:** - Generate categorical features using the `Categorical` distribution. The number of levels for each categorical feature is provided in the `categorical_levels` list. Each entry in the list specifies the number of unique categories (levels) for the corresponding categorical feature. **Example:** If `n_samples=100`, `n_continuous=3`, `n_categorical=2`, and `categorical_levels=[4, 3]`, the function should return: - A tensor of shape (100, 3) for continuous features sampled from normal distributions. - A tensor of shape (100, 2) for categorical features where the first categorical feature has 4 levels and the second has 3 levels. **Constraints:** - You should use the `torch.distributions.Normal` and `torch.distributions.Categorical` classes to generate the data. - Ensure that the function is efficient and can handle large values of `n_samples`. **Implementation Tips:** - Utilize the `sample` method of PyTorch distributions to generate random samples. - For computing efficiency, leverage batch operations provided by PyTorch. Test your function thoroughly to ensure it generates data with the correct structure and statistical properties.","solution":"import torch from torch.distributions import Normal, Categorical from typing import List, Tuple def generate_synthetic_data(n_samples: int, n_continuous: int, n_categorical: int, categorical_levels: List[int]) -> Tuple[torch.Tensor, torch.Tensor]: Generates synthetic data with specified number of continuous and categorical features. Args: n_samples (int): Number of samples to generate. n_continuous (int): Number of continuous features. n_categorical (int): Number of categorical features. categorical_levels (List[int]): A list specifying the number of levels for each categorical feature. Returns: Tuple[torch.Tensor, torch.Tensor]: A tuple containing two tensors: - Continuous features: A tensor of shape (n_samples, n_continuous) - Categorical features: A tensor of shape (n_samples, n_categorical) assert len(categorical_levels) == n_categorical, \\"Length of categorical_levels must be equal to n_categorical\\" # Generate continuous features normal_dist = Normal(0, 1) continuous_features = normal_dist.sample((n_samples, n_continuous)) # Generate categorical features categorical_features = [] for levels in categorical_levels: categorical_dist = Categorical(torch.ones(levels)) categorical_features.append(categorical_dist.sample((n_samples,)).unsqueeze(1)) categorical_features = torch.cat(categorical_features, dim=1) return continuous_features, categorical_features"},{"question":"Problem Statement You are tasked with implementing a specialized collection class that behaves like a set but also maintains insertion order, similar to the behavior of Python\'s `OrderedDict`. This collection should support all standard set operations and should be implemented using ABCs from the `collections.abc` module. Requirements 1. Create a class `OrderedSet` that: - Inherits from `collections.abc.MutableSet`. - Maintains the order of elements as they were added. - Provides all methods required by `MutableSet`, using efficient data structures for quick lookups and ordered storage. 2. Implement the following methods: - `__init__(self, iterable=None)`: Initializes the set with an optional iterable of elements. - `__contains__(self, value)`: Checks if the set contains a specific value. - `__iter__(self)`: Returns an iterator over the elements of the set. - `__len__(self)`: Returns the number of elements in the set. - `add(self, value)`: Adds a new element to the set, maintaining the insertion order. - `discard(self, value)`: Removes an element from the set if it exists. - `__repr__(self)`: Returns a string representation of the set. 3. Ensure that the class meets the following constraints: - The class should only use built-in Python data structures. - The complexity of checking membership (`__contains__`) should be O(1). - The insertion and deletion operations (`add` and `discard`) should have average-case O(1) complexity. Expected Input and Output - Instantiation: ```python os = OrderedSet([1, 2, 3]) ``` - Operations: ```python os.add(4) print(os) # Output: OrderedSet([1, 2, 3, 4]) print(2 in os) # Output: True os.discard(3) print(os) # Output: OrderedSet([1, 2, 4]) print(len(os)) # Output: 3 for element in os: print(element) # Output: # 1 # 2 # 4 ``` - The `__repr__` method should provide a clear string representation for diagnostic purposes, such as `OrderedSet([1, 2, 4])`. Your implementation should cover these requirements comprehensively, ensuring compatibility with the `MutableSet` API and maintaining efficient operation.","solution":"from collections.abc import MutableSet class OrderedSet(MutableSet): def __init__(self, iterable=None): self._data = dict() if iterable is not None: for item in iterable: self.add(item) def __contains__(self, value): return value in self._data def __iter__(self): return iter(self._data.keys()) def __len__(self): return len(self._data) def add(self, value): self._data[value] = None def discard(self, value): if value in self._data: del self._data[value] def __repr__(self): return f\\"OrderedSet({list(self._data.keys())})\\""},{"question":"Objective: Your task is to implement a function that reads a binary file, decodes its content according to a specified character encoding, processes the decoded data, and then re-encodes and saves it to another binary file. This exercise will test your comprehension of binary data handling, encoding/decoding principles, and file input/output operations in Python using the `struct` and `codecs` modules. Problem Statement: You need to write a function `process_binary_file(input_file: str, output_file: str, encoding: str) -> None` that performs the following steps: 1. Read the content of the `input_file` as binary data. 2. Decode the binary data into a string using the specified `encoding`. 3. Process the decoded string by reversing its characters. 4. Re-encode the processed string back into binary data using the same `encoding`. 5. Write the re-encoded binary data into the `output_file`. Input and Output Formats: - The function takes three parameters: - `input_file` (str): The path to the input binary file. - `output_file` (str): The path to the output binary file. - `encoding` (str): The character encoding used for decoding and re-encoding the file content. - The function does not return any value. Constraints: - You can assume that the `input_file` exists and is readable. - The `encoding` parameter will be a valid encoding string recognized by the `codecs` module. - The size of the binary data is manageable to be read into memory at once. Example Usage: ```python def process_binary_file(input_file: str, output_file: str, encoding: str) -> None: # Step 1: Read the content of the input_file as binary data with open(input_file, \'rb\') as f: binary_data = f.read() # Step 2: Decode the binary data into a string using the specified encoding decoded_str = binary_data.decode(encoding) # Step 3: Process the decoded string by reversing its characters processed_str = decoded_str[::-1] # Step 4: Re-encode the processed string back into binary data using the same encoding re_encoded_data = processed_str.encode(encoding) # Step 5: Write the re-encoded binary data into the output_file with open(output_file, \'wb\') as f: f.write(re_encoded_data) # Example usage process_binary_file(\'input.bin\', \'output.bin\', \'utf-8\') ``` Ensure to test your function with various encodings such as `\'utf-8\'`, `\'ascii\'`, and `\'utf-16\'` to observe the behavior. You may create sample binary files to test the correctness of your implementation.","solution":"def process_binary_file(input_file: str, output_file: str, encoding: str) -> None: # Step 1: Read the content of the input_file as binary data with open(input_file, \'rb\') as f: binary_data = f.read() # Step 2: Decode the binary data into a string using the specified encoding decoded_str = binary_data.decode(encoding) # Step 3: Process the decoded string by reversing its characters processed_str = decoded_str[::-1] # Step 4: Re-encode the processed string back into binary data using the same encoding re_encoded_data = processed_str.encode(encoding) # Step 5: Write the re-encoded binary data into the output_file with open(output_file, \'wb\') as f: f.write(re_encoded_data)"},{"question":"Objective: Design a class hierarchy with multiple inheritance and implement a custom iterator and generator expressions. This problem will test your understanding of Python classes, inheritance, iterators, and generators. Problem Statement: Design a library system where you have to manage books, members, and librarians. The system should support the following features: 1. **Book**: A class to represent a book with the following attributes: - `title` (string) - `author` (string) - `ISBN` (string) 2. **Member**: A class to represent a library member with the following attributes: - `name` (string) - `member_id` (integer) The `Member` class should have the following methods: - `borrow_book(book: Book)`: A method to borrow a book. - `return_book(book: Book)`: A method to return a borrowed book. - `list_borrowed_books()`: A method to list all borrowed books using an iterator. 3. **Librarian**: A class to represent a librarian with the following attributes: - `name` (string) - `employee_id` (integer) - Inherits from `Member` The `Librarian` class should have the following methods: - `add_book(book: Book)`: A method to add a new book to the library. - `remove_book(book: Book)`: A method to remove a book from the library. 4. **LibrarySystem**: A class to manage the entire library system with the following features: - Maintaining a collection of all books and members. - Methods to add/remove books and members. - A method `find_books_by_author(author_name: string)` that returns a generator expression yielding all books by the given author. Input and Output Requirements: - Books, members, and librarians are added programmatically. - Methods should raise appropriate exceptions when trying to remove or borrow books that do not exist or are already borrowed. - For simplicity, you may assume that books have unique ISBNs and members have unique IDs. Example Usage: ```python # Example book instances book1 = Book(title=\\"1984\\", author=\\"George Orwell\\", ISBN=\\"1234567890\\") book2 = Book(title=\\"Animal Farm\\", author=\\"George Orwell\\", ISBN=\\"1234567891\\") # Example member and librarian instances member = Member(name=\\"John Doe\\", member_id=1) librarian = Librarian(name=\\"Alice Smith\\", employee_id=1001) # Example library system instance library = LibrarySystem() # Librarian adds books to the library librarian.add_book(book1) librarian.add_book(book2) # Member borrows a book member.borrow_book(book1) # Member lists borrowed books for book in member.list_borrowed_books(): print(book.title) # Librarian finds books by George Orwell for book in library.find_books_by_author(\\"George Orwell\\"): print(book.title) ``` Constraints: - The library should not allow borrowing the same book by multiple members simultaneously. - Methods should handle edge cases such as trying to add a book with an existing ISBN or borrowing a book that is not available. Implement the classes and their methods as described above. Ensure your code handles all specified features and constraints effectively.","solution":"class Book: def __init__(self, title: str, author: str, ISBN: str): self.title = title self.author = author self.ISBN = ISBN class Member: def __init__(self, name: str, member_id: int): self.name = name self.member_id = member_id self.borrowed_books = [] def borrow_book(self, book: Book): if book in self.borrowed_books: raise Exception(f\\"{book.title} is already borrowed by {self.name}.\\") self.borrowed_books.append(book) def return_book(self, book: Book): if book not in self.borrowed_books: raise Exception(f\\"{self.name} has not borrowed {book.title}.\\") self.borrowed_books.remove(book) def list_borrowed_books(self): return iter(self.borrowed_books) class Librarian(Member): def __init__(self, name: str, employee_id: int): super().__init__(name, employee_id) self.employee_id = employee_id def add_book(self, book: Book, library_system): library_system.add_book(book) def remove_book(self, book: Book, library_system): library_system.remove_book(book) class LibrarySystem: def __init__(self): self.books = [] self.members = [] def add_book(self, book: Book): if book in self.books: raise Exception(f\\"{book.title} already exists in the library.\\") self.books.append(book) def remove_book(self, book: Book): if book not in self.books: raise Exception(f\\"{book.title} does not exist in the library.\\") self.books.remove(book) def add_member(self, member: Member): if member in self.members: raise Exception(f\\"{member.name} with ID {member.member_id} already exists.\\") self.members.append(member) def remove_member(self, member: Member): if member not in self.members: raise Exception(f\\"{member.name} with ID {member.member_id} does not exist.\\") self.members.remove(member) def find_books_by_author(self, author_name: str): return (book for book in self.books if book.author == author_name)"},{"question":"Objective Implement a Python utility that leverages the `mimetypes` module to perform the following tasks: 1. Guess the MIME type and encoding for a given list of file URLs. 2. Guess the file extension for a given list of MIME types. 3. Manage and update custom MIME type mappings. 4. Optionally initialize the MIME type database with additional files. Use the provided list of filenames and MIME types to demonstrate the functionality of your utility. Requirements 1. Implement a function `guess_file_mime_types(urls: List[str]) -> List[Tuple[str, str]]` that takes a list of URLs or filenames and returns a list of tuples containing the guessed MIME type and encoding for each URL. 2. Implement a function `guess_mime_extensions(types: List[str]) -> List[Optional[str]]` that takes a list of MIME types and returns a list of guessed file extensions for each MIME type. 3. Implement a function `add_custom_mime_type(mime_type: str, extension: str, strict: bool = True) -> None` that adds a custom MIME type mapping. 4. Optionally, implement a function `initialize_mime_database(files: List[str] = None) -> None` that initializes the MIME type database with additional files. Input - A list of URLs or filenames (strings) for MIME type guessing. - A list of MIME types (strings) for file extension guessing. - Custom MIME type and file extension strings for updating the mapping. - Optional: A list of filenames (strings) representing MIME type files for initializing the database. Output - A list of tuples `(MIME type, encoding)` for guessed MIME types and encodings. - A list of guessed file extensions for the provided MIME types. - Updated internal mappings for custom MIME types. - Optionally re-initialized MIME type database with additional files. constraints - Use the default `strict=True` for most functionality unless otherwise specified. - Ensure to test with both standard and non-standard MIME types and extensions. - Consider edge cases such as missing or unknown MIME types and extensions. Example Usage ```python urls = [\\"example.html\\", \\"archive.tar.gz\\", \\"data.csv\\"] mime_types = [\\"text/html\\", \\"application/gzip\\"] # Expected output: [(\'text/html\', None), (\'application/x-tar\', \'gzip\'), (\'text/csv\', None)] print(guess_file_mime_types(urls)) # Expected output: [\'.html\', \'.gz\'] print(guess_mime_extensions(mime_types)) # Adding a custom MIME type add_custom_mime_type(\\"application/x-myapp\\", \\".myapp\\") # Optionally initializing with additional MIME type files initialize_mime_database([\\"/path/to/custom/mime.types\\"]) ``` Your implementation should demonstrate understanding and proper usage of the `mimetypes` module, handling default settings, and managing custom MIME type mappings.","solution":"import mimetypes from typing import List, Tuple, Optional def guess_file_mime_types(urls: List[str]) -> List[Tuple[str, Optional[str]]]: result = [] for url in urls: mime_type, encoding = mimetypes.guess_type(url) result.append((mime_type, encoding)) return result def guess_mime_extensions(types: List[str]) -> List[Optional[str]]: result = [] for mime_type in types: extension = mimetypes.guess_extension(mime_type) result.append(extension) return result def add_custom_mime_type(mime_type: str, extension: str, strict: bool = True) -> None: mimetypes.add_type(mime_type, extension, strict) def initialize_mime_database(files: List[str] = None) -> None: if files: for file in files: mimetypes.init(files=[file]) else: mimetypes.init()"},{"question":"You are given a task to create a series of data visualizations using seaborn. The main objective is to display various statistics of a given dataset with customized color palettes. Input: 1. A Pandas DataFrame containing at least two numerical columns. 2. A list of colors that will be used to create a palette. 3. A boolean flag indicating if the palette should be a continuous colormap. Requirements: 1. You should create a blend palette using the provided list of colors. 2. If the continuous flag is set to `True`, return a continuous colormap. 3. Use this palette/colormap to create the following visualizations: - A scatter plot for the first two numerical columns. - A line plot for the first two numerical columns. - A histogram for the first numerical column. 4. Each plot should use the `palette` or `colormap` created as per the given flag. 5. Display the plots. Constraints: - You must use `seaborn` and `matplotlib`. - Your solution should efficiently handle large datasets with up to 100,000 rows. Function Signature: ```python import pandas as pd from typing import List def create_visualizations(df: pd.DataFrame, colors: List[str], continuous: bool) -> None: pass ``` # Example Input: ```python import pandas as pd # Sample DataFrame data = { \'A\': [1, 2, 3, 4, 5], \'B\': [5, 4, 3, 2, 1] } df = pd.DataFrame(data) # List of colors colors = [\\"#45a872\\", \\".8\\", \\"xkcd:golden\\"] # Flag for continuous colormap continuous = False ``` # Example Output: The function should create and display a scatter plot, line plot, and histogram using the given color palette. # Instructions: 1. Implement the `create_visualizations` function. 2. Ensure the function meets all requirements and constraints. 3. Document your code appropriately.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from typing import List def create_visualizations(df: pd.DataFrame, colors: List[str], continuous: bool) -> None: Create and display scatter plot, line plot, and histogram using the given color palette. Parameters: df : pd.DataFrame A Pandas DataFrame containing at least two numerical columns. colors : List[str] A list of colors that will be used to create a palette. continuous : bool A boolean flag indicating if the palette should be a continuous colormap. Returns: None if len(df.columns) < 2 or df.shape[1] < 2: raise ValueError(\\"The DataFrame must contain at least two numerical columns.\\") # Create the palette if continuous: palette = sns.color_palette(colors, as_cmap=True) else: palette = sns.color_palette(colors) # Scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot(x=df.iloc[:, 0], y=df.iloc[:, 1], palette=palette) plt.title(\\"Scatter Plot\\") plt.xlabel(df.columns[0]) plt.ylabel(df.columns[1]) plt.show() # Line plot plt.figure(figsize=(10, 6)) sns.lineplot(x=df.iloc[:, 0], y=df.iloc[:, 1], palette=palette) plt.title(\\"Line Plot\\") plt.xlabel(df.columns[0]) plt.ylabel(df.columns[1]) plt.show() # Histogram plt.figure(figsize=(10, 6)) sns.histplot(df.iloc[:, 0], palette=palette, bins=20) plt.title(\\"Histogram\\") plt.xlabel(df.columns[0]) plt.show()"},{"question":"Data Processing with Pathlib You are provided with a root directory containing various subdirectories and files. Your task is to write a Python function `process_paths` that performs the following operations using the `pathlib` module: 1. **List Python Files**: Starting from the root directory, recursively find all Python source files (`.py` extension) in the directory tree. 2. **Read and Count Lines**: For each Python file found, read its content and count the number of lines. Store the filename (including its relative path) and the line count in a dictionary. 3. **Summarize Findings**: Calculate the total number of Python files found and the total number of lines across all Python files. 4. **Save Results**: Write the summary information and the file-specific line counts to a new file called `summary.txt` in the root directory. Use the `pathlib` module exclusively to handle all filesystem operations. # Input - `root_dir` (str): The path to the root directory where the search starts. # Output - The function does not return any value. Instead, it creates a file `summary.txt` in the `root_dir` containing: - The total number of Python files found. - The total number of lines across all Python files found. - The relative paths of each Python file and their corresponding line counts. # Constraints - Assume the root directory always exists. - Your solution should handle large directories efficiently. - Consider edge cases such as empty directories or directories with no Python files. # Example Suppose the directory structure is as follows: ``` /root_dir  dir1   file1.py   file2.txt  dir2   file3.py  file4.py ``` The contents of the files are: - `file1.py`: 10 lines - `file3.py`: 20 lines - `file4.py`: 30 lines The `summary.txt` should contain: ``` Total Python files: 3 Total lines: 60 /root_dir/dir1/file1.py: 10 lines /root_dir/dir2/file3.py: 20 lines /root_dir/file4.py: 30 lines ``` Implement the function `process_paths(root_dir: str) -> None`: ```python from pathlib import Path def process_paths(root_dir: str) -> None: # Your code here ```","solution":"from pathlib import Path def process_paths(root_dir: str) -> None: root_path = Path(root_dir) python_files = list(root_path.rglob(\'*.py\')) line_counts = {} total_lines = 0 for py_file in python_files: with py_file.open(\'r\', encoding=\'utf-8\') as file: line_count = sum(1 for _ in file) line_counts[py_file.relative_to(root_path)] = line_count total_lines += line_count summary_file = root_path / \'summary.txt\' with summary_file.open(\'w\', encoding=\'utf-8\') as file: file.write(f\\"Total Python files: {len(python_files)}n\\") file.write(f\\"Total lines: {total_lines}nn\\") for relative_path, line_count in line_counts.items(): file.write(f\\"{relative_path}: {line_count} linesn\\")"},{"question":"Objective: Write a Python function `describe_error(error_number: int) -> str` that takes an error number as input and returns a string description of the error. If the error number is not recognized, the function should return `\\"Unknown error\\"`. Utilize the `errno` module to achieve this. Function Signature: ```python def describe_error(error_number: int) -> str: ``` Input: - An integer `error_number`, representing the system error number. Output: - A string description of the error corresponding to the error number. - If the error number is not recognized in the `errno` module, return `\\"Unknown error\\"`. Examples: ```python print(describe_error(errno.EPERM)) # should return \\"Operation not permitted\\" print(describe_error(errno.ENOENT)) # should return \\"No such file or directory\\" print(describe_error(12345)) # should return \\"Unknown error\\" ``` Constraints: 1. You are only allowed to use the `errno` module to get error names and descriptions. 2. You should handle the case where the `error_number` is not defined in the `errno` module. Guidelines: - Use `errno.errorcode` to find the symbolic name of the error number. - Use `os.strerror()` to get the human-readable error message. ```python import errno import os def describe_error(error_number: int) -> str: Given an error number, return the string description of the error. :param error_number: int - system error number. :return: str - description of the error or \'Unknown error\' if not recognized. # Your implementation here ``` This question assesses the ability of the candidate to: 1. Use the `errno` module effectively. 2. Implement error handling and look-ups in Python. 3. Work with system-level concepts in Python, such as error codes and their interpretations.","solution":"import errno import os def describe_error(error_number: int) -> str: Given an error number, return the string description of the error. :param error_number: int - system error number. :return: str - description of the error or \'Unknown error\' if not recognized. if error_number in errno.errorcode: return os.strerror(error_number) else: return \\"Unknown error\\""},{"question":"**Problem Statement:** You are required to create a Python script that automates the setup of a Python interactive environment with a custom startup file. This will involve reading from environment variables and performing file operations. # Objectives: 1. **Create the startup file:** - Write a function `create_startup_file(commands: list, filename: str) -> None` that takes a list of strings where each string is a Python command, and a filename to save these commands. - Save the commands to the specified file. 2. **Set the environment variable:** - Write a function `set_startup_env_variable(filename: str) -> None` that sets the `PYTHONSTARTUP` environment variable to the provided filename within your script. 3. **Read and execute the startup file:** - Write a function `execute_startup_file() -> None` that reads the file specified by the `PYTHONSTARTUP` environment variable and executes its content. # Input: 1. A list of Python commands (strings). 2. A filename for the startup file. # Output: 1. The startup file is saved with the specified commands. 2. The environment variable `PYTHONSTARTUP` is set to the created file. 3. The commands from the startup file are executed in the current interactive Python session. # Constraints: 1. Assume the provided list of commands is valid Python code. 2. Ensure the environment variable `PYTHONSTARTUP` is set for the duration of the script execution. 3. Handle any potential errors that might arise during file operations gracefully, printing appropriate error messages to standard error output. # Example: ```python # Example commands commands = [ \\"import sys\\", \\"sys.ps1 = \'>>> \'\\", \\"sys.ps2 = \'... \'\\", \\"print(\'Welcome to the customized Python interactive shell!\')\\", ] # Filename for the startup file filename = \\"my_startup.py\\" # Implement the functions create_startup_file(commands, filename) set_startup_env_variable(filename) execute_startup_file() ``` # Expected Outcomes: - The file `my_startup.py` is created with the given commands. - The environment variable `PYTHONSTARTUP` is set to `my_startup.py`. - Upon starting a new interactive session, the custom prompts and welcome message are displayed as defined in the startup file. **Note:** - Ensure to test your implementation in an actual Python interactive shell to verify its behavior.","solution":"import os def create_startup_file(commands: list, filename: str) -> None: Create a startup file with the specified commands. Args: commands (list): A list of strings where each string is a Python command. filename (str): The name of the file to save the commands. try: with open(filename, \'w\') as f: for command in commands: f.write(command + \'n\') except Exception as e: print(f\\"Error creating the startup file: {e}\\") def set_startup_env_variable(filename: str) -> None: Set the PYTHONSTARTUP environment variable to the specified filename. Args: filename (str): The name of the startup file. try: os.environ[\'PYTHONSTARTUP\'] = filename except Exception as e: print(f\\"Error setting the PYTHONSTARTUP environment variable: {e}\\") def execute_startup_file() -> None: Execute the Python commands from the file specified by the PYTHONSTARTUP environment variable. try: startup_file = os.getenv(\'PYTHONSTARTUP\') if startup_file and os.path.exists(startup_file): with open(startup_file) as f: exec(f.read()) else: print(\\"No valid PYTHONSTARTUP file found.\\") except Exception as e: print(f\\"Error executing the startup file: {e}\\") # Example Usage: # commands = [ # \\"import sys\\", # \\"sys.ps1 = \'>>> \'\\", # \\"sys.ps2 = \'... \'\\", # \\"print(\'Welcome to the customized Python interactive shell!\')\\", # ] # filename = \'my_startup.py\' # create_startup_file(commands, filename) # set_startup_env_variable(filename) # execute_startup_file()"},{"question":"**Coding Challenge: File Merging and Line Number Annotation** **Objective:** You are tasked with creating a Python script that uses the `fileinput` module to read from multiple text files, merge their contents, annotate each line with its origin (filename and line number), and write the annotated contents either back to the original files or to a new output file. **Problem Statement:** Implement a function `merge_and_annotate_files(input_files: list[str], output_file: Optional[str] = None, inplace: bool = False, backup_ext: str = \'.bak\') -> None` that performs the following tasks: 1. Reads lines from a list of input files. 2. Annotates each line with the filename and the line number from that file in the format: ``` <filename>:<line_number>: <original_line_content> ``` 3. Writes the annotated lines either back to the original files (if `inplace=True`) or to a new output file (if `output_file` is provided). 4. If `inplace=True`, creates backup files with the specified extension `backup_ext`. **Function Signature:** ```python def merge_and_annotate_files(input_files: list[str], output_file: Optional[str] = None, inplace: bool = False, backup_ext: str = \'.bak\') -> None: pass ``` **Input:** - `input_files`: a list of strings representing the file names to read from. - `output_file`: an optional string representing the name of the output file. If `None`, in-place modification is performed based on the value of `inplace`. - `inplace`: a boolean indicating whether to modify the input files in place. Default is `False`. - `backup_ext`: a string representing the extension for backup files when performing in-place modification. Default is `\'.bak\'`. **Output:** - The function does not return anything. It writes the annotated lines to the specified output. **Example Usage:** ```python # Input files content before running the function: # file1.txt # Hello # World # # file2.txt # Foo # Bar merge_and_annotate_files([\'file1.txt\', \'file2.txt\'], output_file=\'merged_output.txt\') # merged_output.txt # file1.txt:1: Hello # file1.txt:2: World # file2.txt:1: Foo # file2.txt:2: Bar merge_and_annotate_files([\'file1.txt\', \'file2.txt\'], inplace=True) # file1.txt # file1.txt:1: Hello # file1.txt:2: World # # file2.txt # file2.txt:1: Foo # file2.txt:2: Bar # Backup files file1.txt.bak and file2.txt.bak contain original contents. ``` **Constraints:** - You may assume that all input files are encoded in UTF-8. - Handle possible I/O errors gracefully by raising appropriate exceptions. **Notes:** - Ensure that the implementation adheres to the behavior of the `fileinput` module as described in the provided documentation. - This exercise will test your ability to work with file I/O, context managers, and the handling of multiple files using the `fileinput` module.","solution":"import fileinput import os from typing import Optional def merge_and_annotate_files(input_files: list[str], output_file: Optional[str] = None, inplace: bool = False, backup_ext: str = \'.bak\') -> None: if inplace and output_file: raise ValueError(\\"Cannot specify both inplace and output_file\\") if inplace: mode = \'inplace\' else: mode = \'output\' annotated_lines = [] with fileinput.input(files=input_files, inplace=inplace, backup=backup_ext if inplace else False) as f: for line in f: filename = fileinput.filename() linenumber = fileinput.filelineno() annotated_line = f\\"{filename}:{linenumber}: {line}\\" if inplace: print(annotated_line, end=\'\') else: annotated_lines.append(annotated_line) if output_file: with open(output_file, \'w\', encoding=\'utf-8\') as of: of.writelines(annotated_lines)"},{"question":"# Advanced Python Object Management Problem Statement You are required to create a Python class that mimics the behavior of object creation and memory management as described in the provided documentation for `_PyObject_New` and related functions. Your task is to implement this class with the following functionalities: 1. **Allocating and Initializing Objects**: Implement methods to create new instances of the class (`allocate`) and reinitialize existing instances (`initialize`). 2. **Managing Memory**: Implement a method to release memory (`deallocate`). 3. **Creating Variable-Size Objects**: Implement functionality to handle objects of variable sizes. 4. **Reference Counting**: Manage the reference count for each object, incrementing when a new reference is created and decrementing when a reference is deleted. 5. **Special Singleton Object**: Implement a singleton object to represent `None` similar to `_Py_NoneStruct`. Class Definition - `class PyObject`: - `__init__(self, size=1)`: Initialize the object with a given size. - `allocate(cls, size=1)`: Class method to allocate a new object. - `initialize(self, size=1)`: Reinitialize the object. - `deallocate(cls, obj)`: Class method to deallocate the given object. - `increment_ref(self)`: Increment the reference count. - `decrement_ref(self)`: Decrement the reference count and deallocate if it hits zero. Additionally, create a singleton object to represent the `None` object: - `PyNone` Requirements - The `allocate` method should mimic `_PyObject_New` by creating a new instance and initializing its reference count to one. - The `initialize` method should reinitialize any fields as necessary. - The `deallocate` method should mimic `PyObject_Del` by releasing the memory and ensuring the object cannot be accessed afterward. - The reference counting methods should ensure memory management follows the rules of incrementing and decrementing appropriately. - The `PyNone` singleton object should always be the same instance when accessed. Example ```python class PyObject: # Implementation here # Tests obj1 = PyObject.allocate() assert obj1 is not None obj1.increment_ref() obj1.decrement_ref() assert obj1.ref_count == 1 obj2 = PyObject.allocate(size=10) assert obj2.size == 10 PyObject.deallocate(obj2) py_none = PyNone() assert py_none is PyNone() ``` # Constraints - Do not use any external libraries for object allocation. - `size` must be a positive integer. - `ref_count` must not get below zero. This coding assessment should test students\' understanding of object-oriented programming, memory management, and the concept of singletons in Python.","solution":"class PyObject: _instances = [] # List to keep track of allocated instances for deallocation purposes def __init__(self, size=1): self.size = size self.ref_count = 1 PyObject._instances.append(self) @classmethod def allocate(cls, size=1): obj = cls(size) return obj def initialize(self, size=1): self.size = size @classmethod def deallocate(cls, obj): if obj in cls._instances: cls._instances.remove(obj) del obj def increment_ref(self): self.ref_count += 1 def decrement_ref(self): self.ref_count -= 1 if self.ref_count == 0: PyObject.deallocate(self) class PyNone(PyObject): _singleton = None def __new__(cls): if cls._singleton is None: cls._singleton = super(PyNone, cls).__new__(cls) super(PyNone, cls._singleton).__init__(size=1) return cls._singleton def __init__(self): pass # Overriding __init__ to do nothing as singletons should be initialized only once"},{"question":"**Objective:** Write a Python function that demonstrates your understanding of dictionary objects, including type checking, element manipulation, and error handling. **Problem Statement:** Create a function `manage_student_records(operation: str, student_records: dict, student: tuple = None) -> dict:` that performs different operations on a dictionary containing student records. The dictionary has student names as keys and their grades as values. **Function Signature:** ```python def manage_student_records(operation: str, student_records: dict, student: tuple = None) -> dict: ``` **Parameters:** - `operation` (str): The operation to be performed. It can be one of the following: - `\\"add\\"`: Adds a new student record. - `\\"update\\"`: Updates the grade of an existing student. - `\\"delete\\"`: Deletes a student record. - `\\"fetch\\"`: Fetches the grade of a specific student. - `student_records` (dict): The dictionary containing student records. Keys are student names, and values are their grades. - `student` (tuple, optional): A tuple containing student name and grade. Required for `add` and `update` operations. **Returns:** - `dict` : The updated dictionary of student records. **Constraints:** 1. For `add`, if the student name already exists in the dictionary, the function should return the original dictionary without modifications. 2. For `update`, if the student name does not exist in the dictionary, raise a `KeyError` with the message \\"Student not found\\". 3. For `delete`, if the student name does not exist in the dictionary, raise a `KeyError` with the message \\"Student not found\\". 4. For `fetch`, if the student name does not exist in the dictionary, raise a `KeyError` with the message \\"Student not found\\". 5. Ensure appropriate type checks are performed on the inputs. **Goal:** Demonstrate your ability to handle dictionary manipulations, type checking, and exception handling in Python. **Example Usage:** ```python # Initial student records students = {\\"Alice\\": 85, \\"Bob\\": 90, \\"Charlie\\": 78} # Add a new student print(manage_student_records(\\"add\\", students, (\\"Daisy\\", 95))) # {\\"Alice\\": 85, \\"Bob\\": 90, \\"Charlie\\": 78, \\"Daisy\\": 95} # Update an existing student\'s grade print(manage_student_records(\\"update\\", students, (\\"Alice\\", 88))) # {\\"Alice\\": 88, \\"Bob\\": 90, \\"Charlie\\": 78, \\"Daisy\\": 95} # Fetch a student\'s grade print(manage_student_records(\\"fetch\\", students, (\\"Bob\\",))) # 90 # Delete a student record print(manage_student_records(\\"delete\\", students, (\\"Charlie\\",))) # {\\"Alice\\": 88, \\"Bob\\": 90, \\"Daisy\\": 95} ``` **Note:** The focus is on demonstrating correct type handling and error handling practices while working with dictionary objects in Python.","solution":"def manage_student_records(operation: str, student_records: dict, student: tuple = None) -> dict: if not isinstance(operation, str): raise TypeError(\\"Operation must be a string\\") if not isinstance(student_records, dict): raise TypeError(\\"Student records must be a dictionary\\") if operation in {\\"add\\", \\"update\\", \\"delete\\", \\"fetch\\"} and (student is not None and not isinstance(student, tuple)): raise TypeError(\\"Student must be a tuple\\") if operation == \\"add\\": if student[0] not in student_records: student_records[student[0]] = student[1] return student_records elif operation == \\"update\\": if student[0] not in student_records: raise KeyError(\\"Student not found\\") student_records[student[0]] = student[1] return student_records elif operation == \\"delete\\": if student[0] not in student_records: raise KeyError(\\"Student not found\\") del student_records[student[0]] return student_records elif operation == \\"fetch\\": student_name = student[0] if student_name not in student_records: raise KeyError(\\"Student not found\\") return {student_name: student_records[student_name]} else: raise ValueError(\\"Invalid operation\\")"},{"question":"# Unit Testing in Python using the \\"test\\" package Objective: To assess your understanding of unit testing in Python using the \\"unittest\\" module and the utilities provided in the \\"test.support\\" module. Problem Statement: You are required to write a unit test suite for a piece of code that simulates a simple bank account. Your tests should cover the following functionality: - Creating a bank account with an initial balance - Depositing amount into the account - Withdrawing an amount from the account - Ensuring the account balance cannot be negative Your task involves two parts: 1. Implement the `BankAccount` class. 2. Create a comprehensive test suite for the `BankAccount` class using the `unittest` module and the utilities from the `test.support` module. Specifications: **Part 1: Implement BankAccount class** ```python class BankAccount: def __init__(self, initial_balance=0): self.balance = initial_balance def deposit(self, amount): if amount > 0: self.balance += amount else: raise ValueError(\\"Deposit amount must be positive\\") def withdraw(self, amount): if amount > 0 and self.balance >= amount: self.balance -= amount elif amount <= 0: raise ValueError(\\"Withdraw amount must be positive\\") else: raise ValueError(\\"Insufficient balance\\") def get_balance(self): return self.balance ``` **Part 2: Create a Test Suite** 1. **Name the test module `test_bank_account.py`.** 2. **Set up the test module and write the test methods:** - Test account creation (with and without initial balance). - Test depositing a positive amount. - Test depositing a non-positive amount (should raise `ValueError`). - Test withdrawing a positive amount. - Test withdrawing a non-positive amount (should raise `ValueError`). - Test withdrawing more than the available balance (should raise `ValueError`). 3. **Use the `unittest` module and `test.support` utilities for your test suite.** Ensure proper use of setUp() and tearDown() methods if necessary. Expected Functions: - **BankAccount()**: Initializes a new bank account object. - **deposit(amount)**: Adds the specified amount to the account balance. - **withdraw(amount)**: Deducts the specified amount from the account balance if sufficient funds are available. - **get_balance()**: Returns the current balance of the account. **Constraints:** - The initial balance should not be negative. - The deposit amount should be positive. - The withdrawal amount should be positive and less than or equal to the current balance. **Performance Requirements:** - The solution should handle typical use cases efficiently, but performance optimizations are not the primary focus of this assessment. Example: ```python import unittest from test import support class TestBankAccount(unittest.TestCase): def setUp(self): self.account = BankAccount(100) def test_initial_balance(self): self.assertEqual(self.account.get_balance(), 100) def test_deposit(self): self.account.deposit(50) self.assertEqual(self.account.get_balance(), 150) def test_withdraw(self): self.account.withdraw(30) self.assertEqual(self.account.get_balance(), 70) def test_deposit_negative(self): with self.assertRaises(ValueError): self.account.deposit(-10) def test_withdraw_negative(self): with self.assertRaises(ValueError): self.account.withdraw(-10) def test_withdraw_insufficient_balance(self): with self.assertRaises(ValueError): self.account.withdraw(200) if __name__ == \'__main__\': unittest.main() ``` # Notes: - Your test suite should follow the structure and recommendations provided in the documentation. - Ensure each test method\'s name starts with `test_`. - No method documentation strings should be included, use comments if needed.","solution":"class BankAccount: def __init__(self, initial_balance=0): if initial_balance < 0: raise ValueError(\\"Initial balance cannot be negative\\") self.balance = initial_balance def deposit(self, amount): if amount > 0: self.balance += amount else: raise ValueError(\\"Deposit amount must be positive\\") def withdraw(self, amount): if amount > 0 and self.balance >= amount: self.balance -= amount elif amount <= 0: raise ValueError(\\"Withdraw amount must be positive\\") else: raise ValueError(\\"Insufficient balance\\") def get_balance(self): return self.balance"},{"question":"**Problem Statement:** You are tasked with creating a custom function in PyTorch to manipulate tensors by leveraging views. The function should first create a 4-dimensional tensor, apply a series of view transformations, and ensure the tensor is contiguous before performing the final operation. The function should also assess the performance implications of ensuring tensor contiguity. **Function Signature:** ```python def tensor_view_operations() -> torch.Tensor: This function performs a series of transformations on a PyTorch tensor. 1. Create a 4-dimensional tensor of shape (2, 3, 4, 5) with random values. 2. Reshape this tensor to (6, 4, 5) using \'view\'. 3. Transpose the reshaped tensor\'s first two dimensions. 4. Ensure the resulting tensor is contiguous in memory. 5. Return the final contiguous tensor. Returns: torch.Tensor: The transformed and contiguous tensor. pass ``` **Requirements:** 1. **Tensor Initialization**: Use `torch.rand` to create a 4-dimensional tensor with shape (2, 3, 4, 5). 2. **Reshaping**: Apply the `view` method to reshape it to (6, 4, 5). 3. **Transpose**: Transpose the first two dimensions of the reshaped tensor. 4. **Ensure Contiguity**: The transposed tensor must be contiguous in memory. 5. **Return**: Finally, return the contiguous tensor. **Example:** ```python >>> result = tensor_view_operations() >>> result.size() # Expected shape after transformations torch.Size([4, 6, 5]) >>> result.is_contiguous() # The final tensor should be contiguous in memory True ``` **Constraints:** - You should use the appropriate PyTorch functions for each operation. - Ensure the final tensor is contiguous using the `.contiguous()` method when necessary. **Performance Considerations:** - Understanding of memory layout and performance optimization by making the tensor contiguous. - Efficient use of view operations to manipulate tensor shapes without unnecessary data copying. This question will test the students\' ability to manipulate tensors using views, ensure proper memory layouts, and apply consecutive transformations efficiently.","solution":"import torch def tensor_view_operations() -> torch.Tensor: This function performs a series of transformations on a PyTorch tensor. 1. Create a 4-dimensional tensor of shape (2, 3, 4, 5) with random values. 2. Reshape this tensor to (6, 4, 5) using \'view\'. 3. Transpose the reshaped tensor\'s first two dimensions. 4. Ensure the resulting tensor is contiguous in memory. 5. Return the final contiguous tensor. Returns: torch.Tensor: The transformed and contiguous tensor. # Step 1: Create a 4-dimensional tensor of shape (2, 3, 4, 5) with random values tensor = torch.rand((2, 3, 4, 5)) # Step 2: Reshape this tensor to (6, 4, 5) using \'view\' tensor_reshaped = tensor.view(6, 4, 5) # Step 3: Transpose the reshaped tensor\'s first two dimensions tensor_transposed = tensor_reshaped.transpose(0, 1) # Step 4: Ensure the resulting tensor is contiguous in memory tensor_contiguous = tensor_transposed.contiguous() # Step 5: Return the final contiguous tensor return tensor_contiguous"},{"question":"**Objective:** Implement a Python function that processes a dictionary of sequences and verifies specific conditions on these sequences. **Problem Statement:** Write a function `process_dict_sequences(data: dict) -> dict` that takes a dictionary `data` as input, where each key is associated with a sequence (list, tuple, or range). The function should perform the following tasks: 1. Check if the sequence associated with each key is non-empty. 2. Calculate the sum of all numbers in the sequence if it\'s non-empty. 3. If the sequence is empty, replace the sequence with the string `\\"EMPTY\\"` in the output dictionary. 4. Return a new dictionary where each key maps to the sum of its numbers if non-empty, or to the string `\\"EMPTY\\"` if the sequence was empty. **Function Signature:** ```python def process_dict_sequences(data: dict) -> dict: pass ``` # Input - `data`: A dictionary where each key is a string and its value is a sequence (list, tuple, or range) of numbers. # Output - A dictionary where each key maps to the sum of its numbers if the sequence is non-empty, or to the string `\\"EMPTY\\"` if the sequence was empty. # Constraints - Each sequence contains only numeric values. - The input dictionary is not empty. # Example ```python data = { \\"first\\": [1, 2, 3], \\"second\\": (4, 5, 6), \\"third\\": range(7, 10), \\"fourth\\": [] } print(process_dict_sequences(data)) ``` Expected Output: ```python { \\"first\\": 6, \\"second\\": 15, \\"third\\": 24, \\"fourth\\": \\"EMPTY\\" } ``` # Notes: - The function should handle different types of sequences uniformly. - You may assume that the input dictionary only contains sequences or empty lists. - Use appropriate built-in functions and methods discussed in the provided documentation to solve the problem efficiently.","solution":"def process_dict_sequences(data: dict) -> dict: Processes a dictionary of sequences and returns a new dictionary where: - Each key maps to the sum of its sequence if non-empty, - Or \\"EMPTY\\" if the sequence is empty. output = {} for key, sequence in data.items(): if len(sequence) > 0: output[key] = sum(sequence) else: output[key] = \\"EMPTY\\" return output"},{"question":"# Advanced Python Dictionary Manipulation and Integration You are tasked with creating a Python extension module that interfaces with the provided Dictionary Objects API. This extension should expose functionalities to Python which leverage the C-based dictionary manipulations outlined in the documentation above. Problem Statement: Write a C extension module called `py310_dict` that provides the following functions: 1. **create_dict()**: - **Description**: Create and return a new Python dictionary. - **Input**: None - **Output**: A new Python dictionary (empty). 2. **add_item(p, key, value)**: - **Description**: Add a key-value pair to an existing dictionary. - **Input**: - `p`: The dictionary to which the key-value pair will be added. - `key`: The key to add (must be hashable). - `value`: The associated value. - **Output**: Returns `True` on success, `False` on failure. 3. **get_item(p, key)**: - **Description**: Retrieve the value associated with a given key in a dictionary. - **Input**: - `p`: The dictionary from which to retrieve the value. - `key`: The key whose associated value is to be retrieved. - **Output**: The value associated with the key, or `None` if key is not present. 4. **delete_item(p, key)**: - **Description**: Removes a key-value pair from the dictionary. - **Input**: - `p`: The dictionary from which to remove the key-value pair. - `key`: The key to delete. - **Output**: Returns `True` on success, `False` if the key was not present. 5. **merge_dicts(dict1, dict2, override)**: - **Description**: Merge `dict2` into `dict1`. If override is `True`, existing keys in `dict1` will be replaced by keys from `dict2`. - **Input**: - `dict1`: The dictionary to merge into. - `dict2`: The dictionary to merge from. - `override`: Boolean indicating whether to override existing keys in `dict1`. - **Output**: Returns `True` on success, `False` on failure. Constraints and Performance Requirements: - All key parameters should be hashable. - No mutation should occur during iteration of the dictionary. - Ensure proper reference counting to avoid memory leaks. Example Scenario: ```python import py310_dict # Create a new dictionary d = py310_dict.create_dict() # Add items to the dictionary py310_dict.add_item(d, \\"apple\\", 10) py310_dict.add_item(d, \\"banana\\", 20) # Retrieve an item value = py310_dict.get_item(d, \\"apple\\") # Should return 10 # Delete an item py310_dict.delete_item(d, \\"banana\\") # Merge dictionaries d1 = py310_dict.create_dict() d2 = py310_dict.create_dict() py310_dict.add_item(d1, \\"key1\\", \\"value1\\") py310_dict.add_item(d2, \\"key2\\", \\"value2\\") py310_dict.merge_dicts(d1, d2, True) # Override True # d1 should now contain both \\"key1\\":\\"value1\\" and \\"key2\\":\\"value2\\" ``` # Note: Provide complete C code for the `py310_dict` module, including all necessary function definitions and proper handling of Python object references.","solution":"# Create a Python Extension Module in C # Step 1: Define the module and its methods \'\'\' #include <Python.h> static PyObject* create_dict(PyObject* self) { return PyDict_New(); } static PyObject* add_item(PyObject *self, PyObject *args) { PyObject *dict; PyObject *key; PyObject *value; if (!PyArg_ParseTuple(args, \\"OOO\\", &dict, &key, &value)) { return Py_False; } if (PyDict_SetItem(dict, key, value) == 0) { Py_RETURN_TRUE; } else { Py_RETURN_FALSE; } } static PyObject* get_item(PyObject *self, PyObject *args) { PyObject *dict; PyObject *key; if (!PyArg_ParseTuple(args, \\"OO\\", &dict, &key)) { return NULL; } return PyDict_GetItemWithError(dict, key); } static PyObject* delete_item(PyObject *self, PyObject *args) { PyObject *dict; PyObject *key; if (!PyArg_ParseTuple(args, \\"OO\\", &dict, &key)) { Py_RETURN_FALSE; } if (PyDict_DelItem(dict, key) == 0) { Py_RETURN_TRUE; } else { Py_RETURN_FALSE; } } static PyObject* merge_dicts(PyObject *self, PyObject *args) { PyObject *dict1; PyObject *dict2; PyObject *key, *value; PyObject *override_obj; int override = 0; Py_ssize_t pos = 0; if (!PyArg_ParseTuple(args, \\"OOO\\", &dict1, &dict2, &override_obj)) { Py_RETURN_FALSE; } override = PyObject_IsTrue(override_obj); while (PyDict_Next(dict2, &pos, &key, &value)) { if (override || !PyDict_GetItem(dict1, key)) { if (PyDict_SetItem(dict1, key, value) == -1) { Py_RETURN_FALSE; } } } Py_RETURN_TRUE; } static PyMethodDef py310_dict_methods[] = { {\\"create_dict\\", (PyCFunction)create_dict, METH_NOARGS, \\"Create a new dictionary\\"}, {\\"add_item\\", add_item, METH_VARARGS, \\"Add a key-value pair to a dictionary\\"}, {\\"get_item\\", get_item, METH_VARARGS, \\"Get a value for a key from a dictionary\\"}, {\\"delete_item\\", delete_item, METH_VARARGS, \\"Delete a key-value pair from a dictionary\\"}, {\\"merge_dicts\\", merge_dicts, METH_VARARGS, \\"Merge two dictionaries\\"}, {NULL, NULL, 0, NULL} }; static struct PyModuleDef py310_dict_module = { PyModuleDef_HEAD_INIT, \\"py310_dict\\", \\"Python 3.10 Dictionary Extensions\\", -1, py310_dict_methods }; PyMODINIT_FUNC PyInit_py310_dict(void) { return PyModule_Create(&py310_dict_module); } \'\'\' # Step 2: Build the module # To build the module, we would save the above C code in a .c file (e.g. py310_dict_module.c), # and use setup.py to compile it. # Step 3: Use the module in Python code as given in the problem statement def create_dict(): return {} def add_item(d, key, value): d[key] = value return True def get_item(d, key): return d.get(key, None) def delete_item(d, key): if key in d: del d[key] return True return False def merge_dicts(d1, d2, override): if override: d1.update(d2) else: for key, value in d2.items(): if key not in d1: d1[key] = value return True"},{"question":"# Question Your task is to create a Python wrapper module that allows dynamic initialization and configuration of the Python environment using the `python310` package. This wrapper will simulate the process of setting up both a regular Python configuration and an isolated configuration. Implement the following functions in your wrapper module: 1. **initialize_python(config_type: str, command: str, args: list) -> int**: - This function initializes Python based on the provided configuration type (`\\"regular\\"` or `\\"isolated\\"`), executes the given command, and handles the arguments. - **Input**: - `config_type` (str): Specifies the type of configuration (`\\"regular\\"` or `\\"isolated\\"`). - `command` (str): Python code or script to be executed. - `args` (list): List of command-line arguments to be passed to the Python interpreter. - **Output**: - Returns the exit code after the Python command is executed. 2. **append_to_wide_string_list(lst: list, item: str) -> list**: - This function appends an item to a `PyWideStringList` and returns the updated list. - **Input**: - `lst` (list): A list of wide strings (simulated as Python strings). - `item` (str): The wide string item to append. - **Output**: - Returns the updated list. 3. **handle_status(status) -> None**: - This function checks if the provided `PyStatus` indicates an exception and handles it appropriately. - **Input**: - `status`: An instance of `PyStatus`. - **Output**: - None. The function should raise a RuntimeError with the error message if the status indicates an error. Requirements - Make sure to use `PyConfig`, `PyPreConfig`, and their methods adequately based on the configuration type. - Properly handle wide strings and memory management. - Raise appropriate exceptions when required. Example ```python if __name__ == \\"__main__\\": # Simulate initializing a regular Python configuration and running a command exit_code = initialize_python(\\"regular\\", \\"print(\'Hello, World!\')\\", [\\"my_script.py\\"]) print(f\\"Python exited with code {exit_code}\\") # Append an item to a wide string list updated_list = append_to_wide_string_list([\\"str1\\", \\"str2\\"], \\"str3\\") print(f\\"Updated list: {updated_list}\\") ``` # Constraints - Assume the presence of necessary methods to convert between Python strings and `wchar_t` strings. - Handle the preinitialization and initialization steps clearly and correctly. - Ensure proper memory cleanup where necessary, simulating the behavior of the `PyConfig_Clear` and other cleanup routines. **Note:** This question requires a high level of understanding of the initialization process and memory handling as per the provided `python310` package documentation.","solution":"class PyStatus: def __init__(self, code=0, err_msg=None): self.code = code self.err_msg = err_msg def is_error(self): return self.code != 0 class PyConfig: def __init__(self): self.settings = {} def set_default(self): self.settings[\\"default\\"] = True def set_isolated(self): self.settings[\\"isolated\\"] = True class PyPreConfig: def __init__(self): self.settings = {} def init_isolated_config(self): self.settings[\\"isolated\\"] = True def init_python_config(self): self.settings[\\"default\\"] = True def initialize_python(config_type: str, command: str, args: list) -> int: if config_type == \\"regular\\": pre_config = PyPreConfig() pre_config.init_python_config() elif config_type == \\"isolated\\": pre_config = PyPreConfig() pre_config.init_isolated_config() else: raise ValueError(\\"Invalid config type. Use \'regular\' or \'isolated\'.\\") config = PyConfig() config.set_default() if config_type == \\"isolated\\": config.set_isolated() # Simulating the command execution try: exec(command) return 0 # Assume success except Exception as e: print(f\\"Command execution failed: {e}\\") return 1 # Exit with error code def append_to_wide_string_list(lst: list, item: str) -> list: lst.append(item) return lst def handle_status(status) -> None: if status.is_error(): raise RuntimeError(f\\"Error: {status.err_msg}\\")"},{"question":"# Email Address Extraction and Validation Implement a function named `extract_and_validate_emails` that takes a string containing multiple email addresses and performs the following tasks: 1. **Extract all email addresses** from the given string. 2. **Validate** the extracted email addresses using the `parseaddr` function. A valid email address should not return an empty tuple. 3. **Format** the valid email addresses back into a standardized format using the `formataddr` function. **Function Signature**: ```python def extract_and_validate_emails(email_string: str) -> list: pass ``` **Input**: - `email_string` (str): A string containing multiple email addresses, possibly separated by commas, semicolons, or whitespace. **Output**: - Returns a list of tuples, where each tuple contains: - The original email address string. - The formatted email address. **Constraints**: - The function should be case-insensitive. - The input string can be large, containing up to 10,000 characters. **Example**: ```python input_string = \\"John Doe <john.doe@example.com>, jane.doe@sample.com; \'Invalid Email\' <invalid-email>, Michael Smith <michael.smith@work-domain> \\" output = extract_and_validate_emails(input_string) # Expected Output # [ # (\'John Doe <john.doe@example.com>\', \'John Doe <john.doe@example.com>\'), # (\'jane.doe@sample.com\', \'jane.doe@sample.com\'), # (\'Michael Smith <michael.smith@work-domain>\', \'Michael Smith <michael.smith@work-domain>\') # ] ``` **Note**: The string `\'Invalid Email\' <invalid-email>` is not included in the output because it is not a valid email address. **Evaluation Criteria**: 1. Correctness: The function should correctly extract and validate email addresses. 2. Usage of `parseaddr` and `formataddr` functions. 3. Efficiency: The function should handle large input strings efficiently. 4. Code readability and structure.","solution":"from email.utils import parseaddr, formataddr def extract_and_validate_emails(email_string: str) -> list: email_string = email_string.replace(\\";\\", \\",\\").replace(\\"n\\", \\",\\") email_addresses = [addr.strip() for addr in email_string.split(\\",\\") if addr.strip()] valid_emails = [] for email in email_addresses: name, address = parseaddr(email) if address and \\"@\\" in address: # Basic validation. formatted_email = formataddr((name, address)) valid_emails.append((email, formatted_email)) return valid_emails"},{"question":"**Objective**: Assess the understanding of seaborn\'s error bar functionalities, including various types of error bars, their settings, and customization. **Problem Statement**: You are given a dataset containing the following columns: - `group`: A categorical variable representing different groups. - `values`: A numerical variable representing the data values. Using this dataset, your task is to create a visual comparison of the different error bar types provided by seaborn. Specifically, you need to visualize the standard deviation, standard error, percentile interval, and confidence interval error bars. **Requirements**: 1. Load the dataset from a CSV file (`data.csv`). 2. Create a subplot with a 2x2 grid layout. 3. Each subplot should display a point plot using seaborn\'s `pointplot` function for the `values` with respect to `group`, with different error bars shown as follows: - Top-left subplot: Standard deviation error bars. - Top-right subplot: Standard error error bars. - Bottom-left subplot: Percentile interval error bars. - Bottom-right subplot: Confidence interval error bars. 4. Add appropriate titles to each subplot to indicate the type of error bar displayed. 5. Optionally, annotate each subplot to show the mean value of the `values` for each `group`. **Input**: - A CSV file named `data.csv` with columns `group` and `values`. **Output**: - A 2x2 grid of subplots with different types of error bars, displayed using seaborn. **Constraints**: - You must use seaborn for all visualizations. - Ensure that the plots are neatly formatted and easy to read. - Use appropriate scales for the error bars. **Performance**: - Handle datasets with up to 10,000 rows efficiently. **Example Dataset** (`data.csv`): ``` group,values A,5.1 B,6.5 A,5.8 B,7.2 A,6.0 B,6.8 ... ``` **Implementation**: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the dataset data = pd.read_csv(\'data.csv\') # Set the theme for seaborn sns.set_theme(style=\\"darkgrid\\") # Create a 2x2 subplot grid fig, axs = plt.subplots(2, 2, figsize=(14, 10)) # Top-left subplot: Standard deviation error bars sns.pointplot(x=\'group\', y=\'values\', data=data, errorbar=\\"sd\\", ax=axs[0, 0]) axs[0, 0].set_title(\'Standard Deviation Error Bars\') # Top-right subplot: Standard error error bars sns.pointplot(x=\'group\', y=\'values\', data=data, errorbar=\\"se\\", ax=axs[0, 1]) axs[0, 1].set_title(\'Standard Error Error Bars\') # Bottom-left subplot: Percentile interval error bars sns.pointplot(x=\'group\', y=\'values\', data=data, errorbar=(\\"pi\\", 95), ax=axs[1, 0]) axs[1, 0].set_title(\'Percentile Interval Error Bars\') # Bottom-right subplot: Confidence interval error bars sns.pointplot(x=\'group\', y=\'values\', data=data, errorbar=\\"ci\\", ax=axs[1, 1]) axs[1, 1].set_title(\'Confidence Interval Error Bars\') # Adjust the layout fig.tight_layout() # Show the plots plt.show() ``` **Note**: - Make sure your `data.csv` file is in the same directory as your script. - You can adjust the parameters of the error bars (such as the scale or percentage width for percentile intervals) as needed.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_error_bars(file_path): This function takes in the file path of a CSV file, reads the data, and creates a 2x2 grid plot displaying different types of error bars. # Load the dataset data = pd.read_csv(file_path) # Set the theme for seaborn sns.set_theme(style=\\"darkgrid\\") # Create a 2x2 subplot grid fig, axs = plt.subplots(2, 2, figsize=(14, 10)) # Top-left subplot: Standard deviation error bars sns.pointplot(x=\'group\', y=\'values\', data=data, errorbar=\\"sd\\", ax=axs[0, 0]) axs[0, 0].set_title(\'Standard Deviation Error Bars\') # Top-right subplot: Standard error error bars sns.pointplot(x=\'group\', y=\'values\', data=data, errorbar=\\"se\\", ax=axs[0, 1]) axs[0, 1].set_title(\'Standard Error Error Bars\') # Bottom-left subplot: Percentile interval error bars sns.pointplot(x=\'group\', y=\'values\', data=data, errorbar=(\\"pi\\", 95), ax=axs[1, 0]) axs[1, 0].set_title(\'Percentile Interval Error Bars\') # Bottom-right subplot: Confidence interval error bars sns.pointplot(x=\'group\', y=\'values\', data=data, errorbar=\\"ci\\", ax=axs[1, 1]) axs[1, 1].set_title(\'Confidence Interval Error Bars\') # Adjust the layout fig.tight_layout() # Show the plots plt.show()"},{"question":"<|Analysis Begin|> The documentation provided pertains to the `torch.xpu` module in the PyTorch framework, which appears to deal with functionalities related to managing computation on Intel\'s XPU (rather than the more commonly known GPU). This documentation encompasses: - Device management functions like `current_device`, `device_count`, and `set_device`. - Random Number Generator functions such as `get_rng_state`, `initial_seed`, and `manual_seed`. - Streams and events for asynchronous work with `Event` and `Stream`. - Memory management functions like `empty_cache`, `memory_allocated`, and `reset_peak_memory_stats`. Given this context, the documentation is primarily about managing computation resources on an XPU. This encompasses setting devices, managing streams, and tracking memory usage, which are advanced operations in PyTorch. A suitably challenging question should incorporate several of these elements to test the students\' understanding of how to: 1. Efficiently manage XPU devices. 2. Handle memory allocation and reset peaks. 3. Use streams for asynchronous execution. <|Analysis End|> <|Question Begin|> # XPU Device and Memory Management You are tasked with simulating a scenario where computational work needs to be managed on multiple XPU devices using the PyTorch framework. Your task is to implement a function that does the following: 1. Initialize the XPU environment. 2. Checks if any XPU devices are available. If not, return `\\"No XPU Devices available\\"`. 3. Allocate memory for a large tensor on a specific device and perform a simple operation (like element-wise addition). 4. Track the peak memory usage and reset it at the end. 5. Ensure to use streams and synchronize appropriately before measuring peak memory. Your function signature should be: ```python import torch def manage_xpu_computation(): # Initialize XPU torch.xpu.init() # Check for XPU device availability if not torch.xpu.is_available(): return \\"No XPU Devices available\\" # Proceed with the following logic only if devices are available # Get the count of XPU devices device_count = torch.xpu.device_count() # Create a large tensor on the first available device xpu_device = torch.xpu.device(0) tensor_size = (10000, 10000) # Example size, can be adjusted A = torch.rand(tensor_size, device=xpu_device) # Perform a simple element-wise addition B = A + A # Initialize a stream and perform the operation using the stream stream = torch.xpu.Stream() with torch.xpu.stream(stream): C = A + B # Synchronize the stream torch.xpu.synchronize() # Capture peak memory usage and reset memory stats peak_memory_usage = torch.xpu.max_memory_allocated(xpu_device) torch.xpu.reset_peak_memory_stats(xpu_device) return peak_memory_usage ``` # Expected Output The function should return the peak memory usage in bytes after performing the operations. # Constraints - Ensure that the operation is performed only if there are available XPU devices. - Use appropriate memory management functions to track and reset memory peaks. - The function should handle any possible exceptions or errors gracefully. # Example For a machine with one XPU device, the output might look like: ```python >>> manage_xpu_computation() 164840960 ``` Use the documentation provided to ensure you correctly initialize the XPU, manage memory, and use streams for efficient computation.","solution":"import torch def manage_xpu_computation(): # Initialize XPU (this step is generally not required for PyTorch, assuming it relates to another specific init process) # torch.xpu.init() # Uncomment if there is a specific initialization function # Check for XPU device availability if not torch.xpu.is_available(): return \\"No XPU Devices available\\" # Proceed with the following logic only if devices are available # Get the count of XPU devices device_count = torch.xpu.device_count() print(f\\"Number of XPU devices: {device_count}\\") # Create a large tensor on the first available device xpu_device = torch.xpu.device(0) tensor_size = (10000, 10000) # Example size, can be adjusted A = torch.rand(tensor_size, device=xpu_device) # Perform a simple element-wise addition B = A + A # Initialize a stream and perform the operation using the stream stream = torch.xpu.Stream() with torch.xpu.stream(stream): C = A + B # Synchronize the stream torch.xpu.synchronize() # Capture peak memory usage and reset memory stats peak_memory_usage = torch.xpu.max_memory_allocated(xpu_device) torch.xpu.reset_peak_memory_stats(xpu_device) return peak_memory_usage"},{"question":"# Hyper-Parameter Tuning for Support Vector Classifier using GridSearchCV You are given a dataset and your task is to perform hyper-parameter tuning for a Support Vector Classifier (SVC) using GridSearchCV. The goal is to find the best combination of hyper-parameters that results in the highest cross-validation score. # Dataset You will use the well-known Iris dataset, which can be loaded using sklearn\'s datasets module. # Requirements 1. **Import necessary libraries.** 2. **Load the dataset.** 3. **Split the dataset into training and testing sets.** 4. **Set up a parameter grid for hyper-parameter tuning.** 5. **Use GridSearchCV to search for the best hyper-parameters.** 6. **Fit the model on the training data.** 7. **Evaluate the model on the test data.** # Parameter Grid The parameter grid for GridSearchCV should include: - `C`: [0.1, 1, 10, 100] - `kernel`: [\'linear\', \'rbf\'] - `gamma`: [1, 0.1, 0.01, 0.001] # Expected Input and Output Formats **Function Signature:** ```python def tune_svc_model(X: np.ndarray, y: np.ndarray) -> Tuple[dict, float]: pass ``` **Input:** - `X`: A numpy array of shape `(n_samples, n_features)` representing the feature matrix. - `y`: A numpy array of shape `(n_samples,)` representing the target vector. **Output:** - Returns a tuple containing: - A dictionary of the best hyper-parameters found by GridSearchCV. - The accuracy score of the model on the test data. # Constraints and Notes - Use an 80-20 train-test split. - Use the `accuracy_score` for evaluation. - Set the random state to 42 for reproducibility when splitting the dataset. # Example ```python from sklearn.datasets import load_iris # Load Iris dataset data = load_iris() X = data.data y = data.target # Function usage best_params, accuracy = tune_svc_model(X, y) print(\\"Best Parameters: \\", best_params) print(\\"Test Accuracy: \\", accuracy) ``` Implement the function `tune_svc_model` to successfully perform hyper-parameter tuning using GridSearchCV.","solution":"import numpy as np from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.svm import SVC from sklearn.metrics import accuracy_score def tune_svc_model(X: np.ndarray, y: np.ndarray): Perform hyper-parameter tuning for SVC using GridSearchCV. Parameters: X (np.ndarray): Feature matrix y (np.ndarray): Target vector Returns: Tuple[dict, float]: Best hyper-parameters and the accuracy score on the test data # Split the dataset into training and testing sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Set up the parameter grid param_grid = { \'C\': [0.1, 1, 10, 100], \'kernel\': [\'linear\', \'rbf\'], \'gamma\': [1, 0.1, 0.01, 0.001] } # Initialize the Support Vector Classifier svc = SVC() # Initialize GridSearchCV with the SVC and the parameter grid grid_search = GridSearchCV(svc, param_grid, cv=5, scoring=\'accuracy\') # Fit GridSearchCV to the training data grid_search.fit(X_train, y_train) # Get the best parameters from the grid search best_params = grid_search.best_params_ # Predict on the test set using the best found parameters best_model = grid_search.best_estimator_ y_pred = best_model.predict(X_test) # Calculate the accuracy score on the test set accuracy = accuracy_score(y_test, y_pred) return best_params, accuracy"}]'),D={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const n=this.searchQuery.trim().toLowerCase();return n?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(n)||e.solution&&e.solution.toLowerCase().includes(n)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(n=>setTimeout(n,1e3)),this.visibleCount+=4,this.isLoading=!1}}},z={class:"search-container"},R={class:"card-container"},F={key:0,class:"empty-state"},q=["disabled"],N={key:0},O={key:1};function M(n,e,l,m,i,o){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"prompts chat")])],-1)),t("div",z,[e[3]||(e[3]=t("span",{class:"search-icon"},"",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>i.searchQuery=r),placeholder:"Search..."},null,512),[[y,i.searchQuery]]),i.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>i.searchQuery="")},"  ")):d("",!0)]),t("div",R,[(a(!0),s(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),s("div",F,' No results found for "'+c(i.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:i.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[i.isLoading?(a(),s("span",O,"Loading...")):(a(),s("span",N,"See more"))],8,q)):d("",!0)])}const j=p(D,[["render",M],["__scopeId","data-v-69c83a6f"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/63.md","filePath":"chatai/63.md"}'),U={name:"chatai/63.md"},X=Object.assign(U,{setup(n){return(e,l)=>(a(),s("div",null,[x(j)]))}});export{Y as __pageData,X as default};
