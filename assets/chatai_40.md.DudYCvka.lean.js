import{_ as p,o as a,c as n,a as t,m as c,t as u,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},S={class:"review-content"};function P(s,e,l,m,i,r){return a(),n("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),c(u(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",S,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),c(u(l.poem.solution),1)])])])}const A=p(k,[["render",P],["__scopeId","data-v-3aced6ca"]]),I=JSON.parse('[{"question":"**Objective:** Demonstrate your ability to configure and manage logging in a Python application using the `logging.config` module. Problem Statement You are tasked with creating a logging configuration for a Python application. Your logging setup needs to meet the following requirements: 1. Use the `dictConfig` function to configure logging from a dictionary. 2. The configuration must include: - Two handlers: a console handler that logs messages to the console and a file handler that logs messages to a file named `app.log`. - Custom formatters for each of the handlers. - A logger named `myapp` that uses both handlers. 3. The logger `myapp` should log all messages at the INFO level or higher. 4. The configuration should handle possible errors in the logging setup gracefully. If any configuration error occurs, print out an appropriate error message. Requirements 1. Implement a function `setup_logging(config: dict) -> None` which takes a logging configuration dictionary as input and configures the logging system. 2. Implement a function `main() -> None` as the entry point that: - Calls `setup_logging` with the correct dictionary configuration. - Logs messages at different levels (INFO, WARNING, ERROR) using the `myapp` logger. 3. Ensure that your logging configuration matches the given requirements. 4. Handle configuration errors by catching exceptions and printing a descriptive error message. Input - A dictionary defining the logging configuration. Output - Correctly configured logging that outputs to both console and file. Example Here’s an example dictionary configuration to help you get started: ```python logging_config = { \'version\': 1, \'formatters\': { \'console\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' }, \'file\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'console\', \'level\': \'INFO\' }, \'file\': { \'class\': \'logging.FileHandler\', \'formatter\': \'file\', \'level\': \'INFO\', \'filename\': \'app.log\', \'mode\': \'a\' } }, \'loggers\': { \'myapp\': { \'handlers\': [\'console\', \'file\'], \'level\': \'INFO\', \'propagate\': False } } } def setup_logging(config): import logging.config try: logging.config.dictConfig(config) except (ValueError, TypeError, AttributeError, ImportError) as e: print(f\\"Error configuring logging: {e}\\") def main(): setup_logging(logging_config) logger = logging.getLogger(\'myapp\') logger.info(\\"This is an info message\\") logger.warning(\\"This is a warning message\\") logger.error(\\"This is an error message\\") if __name__ == \\"__main__\\": main() ``` Constraints - Do not hard-code paths or environment specifics. Focus on using the provided configuration schema and handling errors appropriately. - The logging should be demonstrated by logging messages in different levels as shown in the `main` function. Happy coding!","solution":"import logging.config def setup_logging(config): Sets up logging configuration from a dictionary. :param config: dict, logging configuration dictionary. try: logging.config.dictConfig(config) except (ValueError, TypeError, AttributeError, ImportError) as e: print(f\\"Error configuring logging: {e}\\") def main(): logging_config = { \'version\': 1, \'formatters\': { \'console\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' }, \'file\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'console\', \'level\': \'INFO\' }, \'file\': { \'class\': \'logging.FileHandler\', \'formatter\': \'file\', \'level\': \'INFO\', \'filename\': \'app.log\', \'mode\': \'a\' } }, \'loggers\': { \'myapp\': { \'handlers\': [\'console\', \'file\'], \'level\': \'INFO\', \'propagate\': False } } } setup_logging(logging_config) logger = logging.getLogger(\'myapp\') logger.info(\\"This is an info message\\") logger.warning(\\"This is a warning message\\") logger.error(\\"This is an error message\\") if __name__ == \\"__main__\\": main()"},{"question":"**Question:** # Create a Typed Dictionary Using Python\'s Typing and Validate Using Doctest You are required to create a Python class that makes use of typed dictionaries and type hints. Additionally, you will use `doctest` to write tests for your class. # Requirements: 1. **TypedDict**: Create a `TypedDict` class `PersonDict` with the following fields: - `name`: a non-empty string - `age`: a non-negative integer - `email`: a string containing a valid email format 2. **Class**: Create a class `Person` that initializes an instance using a `PersonDict`. It should include methods to: - Get the person\'s information as a formatted string. - Update the person\'s email, ensuring the new email is valid. 3. **Validations**: Ensure the following when initializing and updating the instance: - The `name` should not be empty. - The `age` should be a non-negative integer. - The `email` should be in a valid email format (you can use a simple regex for validation). 4. **Testing with Doctest**: - Write a `doctest` at the end of your class file to validate the class functionality. - Ensure that the `doctest` runs without errors when executed with Python\'s `doctest` module. # Implementation Details: - **TypedDict**: ```python from typing import TypedDict class PersonDict(TypedDict): name: str age: int email: str ``` - **Class Definition**: ```python import re class Person: EMAIL_REGEX = r\'^[a-z0-9]+[._]?[a-z0-9]+[@]w+[.]w+\' def __init__(self, person_dict: PersonDict): self.name = person_dict[\'name\'] self.age = person_dict[\'age\'] self.email = person_dict[\'email\'] self._validate() def _validate(self): if not self.name: raise ValueError(\\"Name cannot be empty\\") if self.age < 0: raise ValueError(\\"Age cannot be negative\\") if not re.match(self.EMAIL_REGEX, self.email): raise ValueError(\\"Invalid email address\\") def get_info(self) -> str: return f\\"Name: {self.name}, Age: {self.age}, Email: {self.email}\\" def update_email(self, new_email: str): if not re.match(self.EMAIL_REGEX, new_email): raise ValueError(\\"Invalid email address\\") self.email = new_email def __repr__(self): return self.get_info() ``` - **Doctest**: ```python if __name__ == \\"__main__\\": import doctest doctest.testmod() ``` - **Example Usage**: ```python >>> person_data = {\\"name\\": \\"Alice\\", \\"age\\": 30, \\"email\\": \\"alice@example.com\\"} >>> alice = Person(person_data) >>> print(alice) Name: Alice, Age: 30, Email: alice@example.com >>> alice.update_email(\\"alice123@example.com\\") >>> print(alice) Name: Alice, Age: 30, Email: alice123@example.com >>> alice.update_email(\\"invalid-email\\") Traceback (most recent call last): ... ValueError: Invalid email address >>> invalid_person_data = {\\"name\\": \\"\\", \\"age\\": -1, \\"email\\": \\"invalid-email\\"} >>> Person(invalid_person_data) Traceback (most recent call last): ... ValueError: Name cannot be empty >>> Person({\\"name\\": \\"Bob\\", \\"age\\": -1, \\"email\\": \\"bob@example.com\\"}) Traceback (most recent call last): ... ValueError: Age cannot be negative ``` # Constraints: - Use `TypedDict` from the `typing` module. - Use regular expressions for email validation. - Use `doctest` for validating your class. # Performance Requirements: - Ensure that the class handles invalid inputs gracefully by raising appropriate exceptions. - The `doctest` should run without any errors when executed. Implement the `Person` class according to the requirements and ensure all `doctest` cases pass.","solution":"from typing import TypedDict import re class PersonDict(TypedDict): name: str age: int email: str class Person: EMAIL_REGEX = r\'^[a-z0-9]+[._]?[a-z0-9]+[@]w+[.]w+\' def __init__(self, person_dict: PersonDict): self.name = person_dict[\'name\'] self.age = person_dict[\'age\'] self.email = person_dict[\'email\'] self._validate() def _validate(self): if not self.name: raise ValueError(\\"Name cannot be empty\\") if self.age < 0: raise ValueError(\\"Age cannot be negative\\") if not re.match(self.EMAIL_REGEX, self.email): raise ValueError(\\"Invalid email address\\") def get_info(self) -> str: return f\\"Name: {self.name}, Age: {self.age}, Email: {self.email}\\" def update_email(self, new_email: str): if not re.match(self.EMAIL_REGEX, new_email): raise ValueError(\\"Invalid email address\\") self.email = new_email def __repr__(self): return self.get_info() if __name__ == \\"__main__\\": import doctest doctest.testmod()"},{"question":"Coding Assessment Question # Objective Implement a custom image type detection function and integrate it with the deprecated `imghdr` module to detect a new, hypothetical image format named \\"hypothetical format\\" (with a file extension \'.hyp\'). # Task You are required to: 1. Implement a function `detect_hypothetical_format(h, f)` that identifies if a given byte stream or file corresponds to the \\"hypothetical format\\". 2. Integrate this function with the `imghdr` module so that `imghdr.what()` can identify files of the hypothetical format. # Specifications 1. The \'hypothetical format\' files have a unique signature: - The first 4 bytes of the file are `b\'x89HYP\'`. - The next 2 bytes are the width of the image. - The following 2 bytes are the height of the image. - The rest of the file contains pixel data. 2. Implement the function `detect_hypothetical_format(h, f)`: - Parameters: - `h` (bytes): A byte stream to test. If `None`, the function should read from the file. - `f` (file-like object): An open file object. If `None`, the function should rely on the byte stream. - Returns: - A string \'hypothetical\' if the test is successful. - `None` if the test fails. 3. Extend `imghdr.tests` with the new `detect_hypothetical_format` function. # Constraints 1. Your function should correctly handle both file objects and byte streams. 2. You must ensure that the detection function does not raise exceptions for invalid inputs or non-hypothetical format files. # Input 1. A path to an image file. 2. A byte stream of image data. # Expected Output A string that identifies the image format (including the new \'hypothetical\' type) or `None` if the image format is unrecognized. # Example Here is an example of how the function might be used: ```python import imghdr # Extend imghdr to recognize \'hypothetical\' format imghdr.tests.append(detect_hypothetical_format) # Assuming \'image.hyp\' is a file of the hypothetical format print(imghdr.what(\'image.hyp\')) # Output should be \'hypothetical\' # Assuming \'hypothetical_data\' is a byte stream of the hypothetical format hypothetical_data = b\'x89HYPx00x10x00x10\' + b\'x00\' * 256 # Header + pixel data print(imghdr.what(None, hypothetical_data)) # Output should be \'hypothetical\' ``` # Notes - Remember to handle both file objects and byte streams effectively. - Ensure your custom detection function is robust and handles edge cases gracefully. - You do not need to handle any other image formats beyond the hypothetical format in your custom function.","solution":"import imghdr def detect_hypothetical_format(h, f): Detect if a given byte stream or file corresponds to the \'hypothetical format\'. Parameters: h (bytes): A byte stream to test. If None, the function should read from the file. f (file-like object): An open file object. If None, the function should rely on the byte stream. Returns: str: \'hypothetical\' if the test is successful. None: if the test fails. signature = b\'x89HYP\' try: if h: header = h[:4] elif f: header = f.read(4) else: return None if header == signature: return \'hypothetical\' else: return None except Exception: return None # Integrate with the imghdr module imghdr.tests.append(detect_hypothetical_format)"},{"question":"Question: You are given a high-dimensional dataset and are tasked to perform dimensionality reduction using both Gaussian and sparse random projections. The goal is to reduce to a specified number of dimensions and validate the approximation quality. # Requirements: 1. Implement a function `dimensionality_reduction` that takes in the dataset, the method of projection (either `\\"gaussian\\"` or `\\"sparse\\"`), and the target number of dimensions. 2. The function should return the transformed dataset. 3. Implement a function `validate_inverse_transform` that checks the quality of the dimensionally-reduced data approximation by performing an inverse transform and comparing it to the original data. 4. The function should return the approximation error as the mean squared error between the original and the recovered dataset. # Expected Input and Output: ```python import numpy as np from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection from sklearn.metrics import mean_squared_error def dimensionality_reduction(X: np.ndarray, method: str, n_components: int) -> np.ndarray: Args: - X: np.ndarray of shape (n_samples, n_features), input data - method: str, either \\"gaussian\\" or \\"sparse\\" specifying the type of random projection to use - n_components: int, the target number of dimensions Returns: - X_transformed: np.ndarray of shape (n_samples, n_components), the dimensionally reduced data # Your code here def validate_inverse_transform(X: np.ndarray, X_transformed: np.ndarray, method: str, n_components: int) -> float: Args: - X: np.ndarray of shape (n_samples, n_features), original input data - X_transformed: np.ndarray of shape (n_samples, n_components), transformed data - method: str, either \\"gaussian\\" or \\"sparse\\" specifying the type of random projection to use - n_components: int, the target number of dimensions Returns: - error: float, mean squared error between the original data and the recovered data after inverse transform # Your code here # Example usage: X = np.random.rand(100, 10000) method = \\"sparse\\" n_components = 100 X_transformed = dimensionality_reduction(X, method, n_components) error = validate_inverse_transform(X, X_transformed, method, n_components) print(\\"Approximation error:\\", error) ``` # Constraints: - Ensure the `compute_inverse_components` parameter is set to `True` for the transformer to enable inverse transform calculations. - Validate results using mean squared error to quantify the approximation quality. - Pay attention to the potential high memory usage for dense matrices during inverse transformations. # Evaluation: - Correct and efficient implementation of both functions. - Accurate calculation and return of the mean squared error for validation.","solution":"import numpy as np from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection from sklearn.metrics import mean_squared_error def dimensionality_reduction(X: np.ndarray, method: str, n_components: int) -> np.ndarray: Perform dimensionality reduction using Gaussian or sparse random projection. Args: - X: np.ndarray of shape (n_samples, n_features), input data - method: str, either \\"gaussian\\" or \\"sparse\\" specifying the type of random projection to use - n_components: int, the target number of dimensions Returns: - X_transformed: np.ndarray of shape (n_samples, n_components), the dimensionally reduced data if method == \\"gaussian\\": transformer = GaussianRandomProjection(n_components=n_components) elif method == \\"sparse\\": transformer = SparseRandomProjection(n_components=n_components) else: raise ValueError(\\"Method should be either \'gaussian\' or \'sparse\'\\") X_transformed = transformer.fit_transform(X) return X_transformed, transformer def validate_inverse_transform(X: np.ndarray, X_transformed: np.ndarray, method: str, transformer) -> float: Validate the quality of dimensionally reduced data by performing an inverse transform and comparing it to the original data. Args: - X: np.ndarray of shape (n_samples, n_features), original input data - X_transformed: np.ndarray of shape (n_samples, n_components), transformed data - method: str, either \\"gaussian\\" or \\"sparse\\" specifying the type of random projection to use - transformer: the transformer object used for the projection Returns: - error: float, mean squared error between the original data and the recovered data after inverse transform X_reconstructed = X_transformed @ transformer.components_ error = mean_squared_error(X, X_reconstructed) return error # Example usage: X = np.random.rand(100, 10000) method = \\"sparse\\" n_components = 100 X_transformed, transformer = dimensionality_reduction(X, method, n_components) error = validate_inverse_transform(X, X_transformed, method, transformer) print(\\"Approximation error:\\", error)"},{"question":"Objective: Write a function `import_module_custom(module_name, reload=False)` in Python that imports a module by its name using the low-level C API functions described in the documentation. Additionally, this function should support reloading the module if the `reload` parameter is set to `True`. Detailed Description: - The function should take two parameters: - `module_name` (str): The name of the module to be imported. - `reload` (bool): A boolean flag indicating whether to reload the module if it\'s already imported. - If the module is not yet imported, the function should import it using the `PyImport_ImportModule()` function or its equivalent. - If the `reload` flag is set to `True` and the module is already imported, the function should reload the module using the `PyImport_ReloadModule()` function or its equivalent. - The function should handle exceptions gracefully and return `None` if the module cannot be imported or reloaded. - On successful import or reload, the function should return the module object. Constraints: - The solution should directly utilize the C API functions discussed in the provided documentation. - The function should consider the performance impact of reloading modules and aim to minimize unnecessary reloads. Input: - `module_name` (str): A string representing the name of the module to be imported. - `reload` (bool): A boolean value indicating whether to reload the module if it’s already imported. Output: - The function should return the imported or reloaded module object. - If the module could not be imported or reloaded, return `None`. Example: ```python # Example usage of the function def main(): json_module = import_module_custom(\'json\') if json_module: print(json_module.dumps({\\"key\\": \\"value\\"})) # Should print: {\\"key\\": \\"value\\"} else: print(\\"Module \'json\' could not be imported.\\") # Now reload the json module reloaded_json_module = import_module_custom(\'json\', reload=True) if reloaded_json_module: print(\\"Module reloaded successfully.\\") else: print(\\"Module \'json\' could not be reloaded.\\") if __name__ == \\"__main__\\": main() ```","solution":"import importlib import sys def import_module_custom(module_name, reload=False): Imports a module by its name. Optionally reloads the module if it is already imported. Parameters: - module_name (str): The name of the module to be imported. - reload (bool): A boolean flag indicating whether to reload the module if it\'s already imported. Returns: - module: The imported or reloaded module object, or None if the import/reload failed. try: if module_name in sys.modules and reload: # Reload the module if it is already imported return importlib.reload(sys.modules[module_name]) else: # Import the module return importlib.import_module(module_name) except Exception: return None"},{"question":"**Objective:** Demonstrate your understanding of the Python `email` package by creating, sending, and parsing complex MIME email messages. **Problem Statement:** You are tasked with creating a Python script that: 1. Reads a directory containing image files. 2. Composes an email with an HTML body that includes embedded images from the directory. 3. Saves this email to a file on disk. 4. Writes a separate function to read the email from the file, parse it, and extract the images, saving them back to a specified directory. **Details & Requirements:** 1. **Email Composition:** - The email should use the `email` package to create a multipart/alternative message. - The text part of the email should include a brief description of the number of images being sent. - The HTML part of the email should embed the images (using `cid`) and display them inline. - Both parts should notify the recipient that the images are attached. - Save the email to a file named `composed_email.eml`. 2. **Email Parsing:** - Write a function to read the `composed_email.eml` file. - Parse the email to extract the embedded images. - Save the extracted images to a specified directory, ensuring the filenames are preserved. **Constraints:** - Assume you have `smtplib` and `email` packages available. - The images can be any common format (e.g., JPEG, PNG). - Provide appropriate error handling, including checking for the existence of directories and files. - Your solution should be efficient in terms of memory usage; avoid loading all images into memory simultaneously if possible. **Input and Output Formats:** - The `compose_email` function should take in parameters: `directory` (string, path containing images), `sender_email` (string), `recipient_email` (string). - The `parse_email` function should take in parameters: `email_file` (string, path to the saved .eml file), `output_directory` (string, where the images will be saved). # Example Usage ```python import os def compose_email(directory, sender_email, recipient_email): # Your implementation here pass def parse_email(email_file, output_directory): # Your implementation here pass # Assume \'images\' directory exists and contains image files compose_email(\'images\', \'sender@example.com\', \'recipient@example.com\') # Assume the \'emails\' directory exists and will store extracted images parse_email(\'composed_email.eml\', \'extracted_images\') ``` # Performance Requirements: - Your script should handle at least 100 images in the directory efficiently. - The composed .eml file should not exceed reasonable size limits for an email (~10MB). Ensure your code is thoroughly commented to explain your logic and approach.","solution":"import os import mimetypes from email import encoders from email.message import EmailMessage from email.utils import make_msgid from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.base import MIMEBase def compose_email(directory, sender_email, recipient_email): # Create the root multipart message message = MIMEMultipart(\'related\') message[\'Subject\'] = \'Your requested images\' message[\'From\'] = sender_email message[\'To\'] = recipient_email # Create the alternative part for HTML/text msg_alternative = MIMEMultipart(\'alternative\') message.attach(msg_alternative) # Count the images and prepare the text and HTML parts image_files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))] num_images = len(image_files) text = f\'The email contains {num_images} images attached.\' html = f\'<html><body><p>The email contains {num_images} images attached:</p>\' # Attach the text part msg_text = MIMEText(text, \'plain\') msg_alternative.attach(msg_text) for idx, image_file in enumerate(image_files): img_path = os.path.join(directory, image_file) with open(img_path, \'rb\') as img: mime_type, _ = mimetypes.guess_type(img_path) main_type, sub_type = mime_type.split(\'/\') mime_image = MIMEBase(main_type, sub_type) mime_image.set_payload(img.read()) encoders.encode_base64(mime_image) cid = make_msgid() mime_image.add_header(\'Content-ID\', cid) mime_image.add_header(\'Content-Disposition\', \'inline\', filename=image_file) message.attach(mime_image) html += f\'<p><img src=\\"cid:{cid[1:-1]}\\"></p>\' html += \'</body></html>\' msg_html = MIMEText(html, \'html\') msg_alternative.attach(msg_html) with open(\'composed_email.eml\', \'wb\') as f: f.write(message.as_bytes()) def parse_email(email_file, output_directory): from email import policy from email.parser import BytesParser if not os.path.exists(output_directory): os.makedirs(output_directory) with open(email_file, \'rb\') as f: msg = BytesParser(policy=policy.default).parse(f) for part in msg.walk(): if part.get_content_maintype() == \'multipart\': continue if part.get(\'Content-Disposition\') is None: continue filename = part.get_filename() if bool(filename): filepath = os.path.join(output_directory, filename) with open(filepath, \'wb\') as fp: fp.write(part.get_payload(decode=True))"},{"question":"# Python Coding Challenge: ChainMap and defaultdict Usage Objective: Write a Python function that processes a series of operations on nested dictionary chains. This function will utilize the `ChainMap` and `defaultdict` classes from the `collections` module. It will allow dynamic manipulation of nested scopes and maintain default values for missing keys. Problem Statement: You need to implement a function called `process_operations` that: 1. Takes as input a list of dictionaries (`dicts`) representing nested scopes. 2. Takes as input a list of operations (`operations`) to perform on the nested dictionaries. 3. Returns a `defaultdict` representing the state of the nested scopes after all operations have been executed. Function Signature: ```python from collections import ChainMap, defaultdict def process_operations(dicts: list, operations: list) -> defaultdict: pass ``` Details: 1. **Input:** - `dicts`: A list of dictionaries. For example: ```python dicts = [{\'a\': 1, \'b\': 2}, {\'b\': 3, \'c\': 4}, {\'c\': 5, \'d\': 6}] ``` - `operations`: A list of tuples, where each tuple represents an operation. The first element of the tuple is the operation type (string), and the remaining elements are the operation parameters. The supported operations are: - `(\\"set\\", key, value)`: Set `key` to `value` in the current (first) dictionary. - `(\\"get\\", key)`: Retrieve the value associated with `key` from the ChainMap. - `(\\"del\\", key)`: Delete `key` from the current (first) dictionary. - `(\\"new_child\\")`: Add a new (empty) child mapping to the ChainMap. For example: ```python operations = [(\\"set\\", \\"e\\", 7), (\\"get\\", \\"a\\"), (\\"del\\", \\"b\\"), (\\"new_child\\"), (\\"set\\", \\"f\\", 8)] ``` 2. **Output:** - A `defaultdict` representing the flattened view of the nested scopes after performing all the operations. Missing keys should default to `None`. Constraints: - You may assume that all inputs are correctly formatted and do not contain illegal operations. - You should use `ChainMap` to manage nested scopes and `defaultdict` to manage the final state of the mappings. Example: ```python dicts = [{\'a\': 1, \'b\': 2}, {\'b\': 3, \'c\': 4}, {\'c\': 5, \'d\': 6}] operations = [(\\"set\\", \\"e\\", 7), (\\"get\\", \\"a\\"), (\\"del\\", \\"b\\"), (\\"new_child\\"), (\\"set\\", \\"f\\", 8)] result = process_operations(dicts, operations) print(result) ``` Expected Output: ```python defaultdict(None, {\'a\': 1, \'c\': 5, \'d\': 6, \'e\': 7, \'f\': 8}) ``` Notes: - The example illustrates adding a new key-value pair, retrieving values, deleting a key, and adding a new child dictionary. The final result should show the cumulative state of all dictionaries after all operations are performed. Bonus: - If you reasonably optimize the performance of your function regarding the operations, especially handling nested scopes and key lookups efficiently.","solution":"from collections import ChainMap, defaultdict def process_operations(dicts: list, operations: list) -> defaultdict: # Initialize ChainMap with provided dictionaries chain_map = ChainMap(*dicts) for operation in operations: if operation[0] == \\"set\\": # Set key to value in the current (first) dictionary _, key, value = operation chain_map[key] = value elif operation[0] == \\"get\\": # Get value of key from ChainMap, we are not storing the get result _, key = operation _ = chain_map.get(key) elif operation[0] == \\"del\\": # Delete key from the current (first) dictionary _, key = operation if key in chain_map.maps[0]: del chain_map.maps[0][key] elif operation[0] == \\"new_child\\": # Add a new (empty) child mapping to the ChainMap chain_map = chain_map.new_child() # Create a defaultdict that defaults to None for missing keys result = defaultdict(lambda: None) # Flatten the ChainMap into the defaultdict for mapping in chain_map.maps: for key, value in mapping.items(): result[key] = value return result"},{"question":"**Objective:** Write a Python function that tracks and collects statistics on garbage collection performance over a series of memory allocations and deallocations. **Task Description:** You need to write a function `gc_performance_tracker(operations: list)` that performs a sequence of memory operations and collects statistics on garbage collection (GC) performance before and after these operations. Each operation in the input list should be either a memory allocation or deallocation action. **Function Signature:** ```python def gc_performance_tracker(operations: list) -> dict: pass ``` **Input:** - `operations` (list): A list of tuples where each tuple contains an operation and an optional size. The operations can be: - (\\"allocate\\", size): Allocate a list of the given size filled with `None`. - (\\"deallocate\\",): Deallocate the most recently allocated list, if any. **Output:** - A dictionary containing the following keys with respective values: - `\\"initial_stats\\"`: GC statistics before any operations (output from `gc.get_stats()`). - `\\"final_stats\\"`: GC statistics after all operations (output from `gc.get_stats()`). - `\\"gc_calls\\"`: The number of times the garbage collector has been called (output from `gc.get_count()`). - `\\"collected_objects\\"`: The number of objects collected during the operations (sum of collected objects). - `\\"uncollectable_objects\\"`: The number of uncollectable objects found during the operations (sum of uncollectable objects). **Constraints:** - Limiting the maximum number of operations to 100. - The size for allocation should be a positive integer not exceeding 10000. **Example:** ```python import gc def gc_performance_tracker(operations: list) -> dict: # Your implementation pass # For testing operations = [(\\"allocate\\", 100), (\\"allocate\\", 200), (\\"deallocate\\",), (\\"allocate\\", 300), (\\"deallocate\\",)] result = gc_performance_tracker(operations) print(result) ``` **Detailed Steps:** 1. Retrieve the initial GC statistics using `gc.get_stats()`. 2. Perform each operation in the operations list: - For \\"allocate\\", create a list of the given size and store it. - For \\"deallocate\\", remove the most recently allocated list. 3. Track the number of objects collected and uncollectable from GC during the operations by calling `gc.collect()` after each deallocation. 4. Retrieve the final GC statistics using `gc.get_stats()`. 5. Retrieve the total number of times GC has been called using `gc.get_count()`. By implementing this function, you will demonstrate your understanding of basic memory management, garbage collection, and statistics collection in Python.","solution":"import gc def gc_performance_tracker(operations: list) -> dict: # Get initial GC statistics initial_stats = gc.get_stats() allocated_objects = [] gc_calls_start = gc.get_count() collected_objects = 0 uncollectable_objects = 0 for operation in operations: if operation[0] == \\"allocate\\": size = operation[1] allocated_objects.append([None] * size) elif operation[0] == \\"deallocate\\" and allocated_objects: allocated_objects.pop() gc.collect() # Get final GC statistics final_stats = gc.get_stats() gc_calls_end = gc.get_count() # Calculate the number of gc calls gc_calls = [gc_calls_end[i] - gc_calls_start[i] for i in range(3)] # Calculate collected and uncollectable objects collected_objects = sum(stage[\'collected\'] for stage in final_stats) uncollectable_objects = sum(stage[\'uncollectable\'] for stage in final_stats) return { \\"initial_stats\\": initial_stats, \\"final_stats\\": final_stats, \\"gc_calls\\": gc_calls, \\"collected_objects\\": collected_objects, \\"uncollectable_objects\\": uncollectable_objects }"},{"question":"# Outlier and Novelty Detection with scikit-learn **Objective**: You are required to demonstrate your understanding of both outlier detection and novelty detection using scikit-learn\'s provided machine learning tools. **Question**: Write a Python function using scikit-learn that performs both outlier detection and novelty detection on a given dataset. The function should utilize `IsolationForest` for outlier detection and `LocalOutlierFactor` for novelty detection. Your implementation should follow the required steps. # Specifications **Function Signature**: ```python def detect_anomalies(X_train, X_test): pass ``` **Input Parameters**: - `X_train`: A NumPy array (or Pandas DataFrame) of shape `(n_train_samples, n_features)` representing the training data. - `X_test`: A NumPy array (or Pandas DataFrame) of shape `(n_test_samples, n_features)` representing the new data points for novelty detection. **Output**: - The function should print the following: 1. Outliers in the training data based on `IsolationForest`. 2. Novelties in the new data (`X_test`) based on `LocalOutlierFactor`. **Constraints**: - You should use default hyperparameters for the models unless specified. - If using `LocalOutlierFactor`, ensure to set the `novelty` parameter to `True`. **Implementation Details**: 1. **IsolationForest Outlier Detection**: - Instantiate and fit the `IsolationForest` model using `X_train`. - Use `fit_predict` to determine inliers and outliers in `X_train`. - Print the series of outlier predictions for `X_train`. 2. **LocalOutlierFactor Novelty Detection**: - Instantiate `LocalOutlierFactor` with `novelty` set to `True` and fit using `X_train`. - Use `predict` to determine inliers and outliers in `X_test`. - Print the series of novelty predictions for `X_test`. # Example Usage: ```python import numpy as np # Example training data (with outliers) X_train = np.array([[ 1.2, 2.3], [ 2.1, 2.2], [15.0, 14.0], # Outlier [ 1.5, 2.4]]) # Example test data (for novelty detection) X_test = np.array([[ 1.8, 2.1], [14.8, 14.1], # Novelty [ 1.3, 2.2]]) detect_anomalies(X_train, X_test) ``` **Output**: ``` Outliers in the training data (IsolationForest): [ 1 1 -1 1] Novelties in the test data (LocalOutlierFactor): [ 1 -1 1] ``` Here, `1` indicates an inlier, and `-1` indicates an outlier/novelty.","solution":"from sklearn.ensemble import IsolationForest from sklearn.neighbors import LocalOutlierFactor import numpy as np def detect_anomalies(X_train, X_test): Perform outlier detection on training data and novelty detection on test data. Parameters: X_train (numpy array or pandas DataFrame): Training data. X_test (numpy array or pandas DataFrame): New data for novelty detection. Outputs: Prints outliers in training data and novelties in test data. # IsolationForest Outlier Detection iso_forest = IsolationForest() outliers_train = iso_forest.fit_predict(X_train) print(\\"Outliers in the training data (IsolationForest):\\") print(outliers_train) # LocalOutlierFactor Novelty Detection lof = LocalOutlierFactor(novelty=True) lof.fit(X_train) novelties_test = lof.predict(X_test) print(\\"Novelties in the test data (LocalOutlierFactor):\\") print(novelties_test)"},{"question":"You are tasked with initializing the weights of a custom neural network in PyTorch using different initialization strategies provided in the `torch.nn.init` module. You need to design a function that initializes the weights and biases of each layer according to a specific method. # Function Signature ```python def initialize_weights(model: torch.nn.Module, init_type: str) -> None: pass ``` # Input - `model`: an instance of `torch.nn.Module` representing the neural network. - `init_type`: a string that specifies the type of initialization to be used. This can have the following values: - `\'uniform\'` - `\'normal\'` - `\'kaiming_uniform\'` - `\'kaiming_normal\'` - `\'xavier_uniform\'` - `\'xavier_normal\'` - `\'zeros\'` - `\'ones\'` - `\'eye\'` - `\'constant\'` # Output The function should modify the `model` in-place, initializing all its weights and biases according to the specified `init_type`. # Constraints 1. The function should initialize all layers of the model. 2. The initialization should handle both weights and biases if they exist. 3. If `init_type` is `\'constant\'`, initialize weights to 0.5 and biases to 0.1. 4. If `init_type` doesn\'t match any of the predefined types, raise a `ValueError` with the message \\"Invalid init_type provided\\". # Example ```python import torch import torch.nn as nn import torch.nn.init as init class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 5) self.fc2 = nn.Linear(5, 2) def forward(self, x): x = self.fc1(x) x = self.fc2(x) return x # Create a simple neural network model = SimpleNN() # Initialize its weights initialize_weights(model, \'xavier_uniform\') # Verify initialization for param in model.parameters(): print(param) ``` In this example, the weights and biases of the `SimpleNN` model are initialized using the `xavier_uniform` method. You should see weights and biases initialized accordingly when the parameters are printed out. # Note - Ensure to use the appropriate functions from the `torch.nn.init` module. - Consider using `torch.no_grad` to ensure initialization steps are not tracked by autograd. # Hints - Use the `torch.nn.init` methods for initializing weights and biases. - Iterate over each layer in the model and apply the initialization method. - Use `isinstance` to check if a layer has weights and biases to initialize.","solution":"import torch import torch.nn as nn import torch.nn.init as init def initialize_weights(model: torch.nn.Module, init_type: str) -> None: Initializes weights and biases of a neural network model in PyTorch. Parameters: model (torch.nn.Module): The neural network model to be initialized. init_type (str): The type of initialization method to use. Raises: ValueError: If an invalid init_type is provided. def init_layer(layer): if isinstance(layer, (nn.Linear, nn.Conv2d)): if init_type == \'uniform\': init.uniform_(layer.weight) if layer.bias is not None: init.uniform_(layer.bias) elif init_type == \'normal\': init.normal_(layer.weight) if layer.bias is not None: init.normal_(layer.bias) elif init_type == \'kaiming_uniform\': init.kaiming_uniform_(layer.weight, nonlinearity=\'relu\') if layer.bias is not None: init.uniform_(layer.bias) elif init_type == \'kaiming_normal\': init.kaiming_normal_(layer.weight, nonlinearity=\'relu\') if layer.bias is not None: init.normal_(layer.bias) elif init_type == \'xavier_uniform\': init.xavier_uniform_(layer.weight) if layer.bias is not None: init.uniform_(layer.bias) elif init_type == \'xavier_normal\': init.xavier_normal_(layer.weight) if layer.bias is not None: init.normal_(layer.bias) elif init_type == \'zeros\': init.constant_(layer.weight, 0) if layer.bias is not None: init.constant_(layer.bias, 0) elif init_type == \'ones\': init.constant_(layer.weight, 1) if layer.bias is not None: init.constant_(layer.bias, 1) elif init_type == \'eye\': if layer.weight.size(0) == layer.weight.size(1): init.eye_(layer.weight) else: init.uniform_(layer.weight) # Fallback for non-square weights if layer.bias is not None: init.constant_(layer.bias, 0) elif init_type == \'constant\': init.constant_(layer.weight, 0.5) if layer.bias is not None: init.constant_(layer.bias, 0.1) else: raise ValueError(\\"Invalid init_type provided\\") with torch.no_grad(): model.apply(init_layer)"},{"question":"# Question: Automated Built Distribution Creator for Python Modules You are tasked with writing a function that automates the creation of built distributions for a given Python module. The function should handle multiple distribution formats and allow the user to specify additional setup options. Function Signature ```python def create_built_distribution(setup_script: str, formats: list, options: dict = None) -> str: pass ``` Input - `setup_script` (str): The path to the `setup.py` script of the Python module. - `formats` (list): A list of strings representing the types of built distributions to create (e.g., `[\\"gztar\\", \\"zip\\", \\"rpm\\"]`). - `options` (dict, optional): A dictionary of additional options to pass to the setup command. Keys are option names (without leading dashes), and values are their respective values (e.g., `{\\"packager\\": \\"John Doe <jdoe@example.org>\\"}`). Default is `None`. Output - Returns a string representing the success message of the built distribution creation process. Constraints - The function should handle both Unix and Windows environments. - Assume the `setup.py` file is correctly configured and located in a directory accessible by the function. - If a format is not supported by the environment, the function should skip that format and continue with the next. - The function should use the appropriate `bdist` sub-command for the specified formats. Example Usage ```python setup_script_path = \\"/path/to/setup.py\\" formats = [\\"gztar\\", \\"zip\\", \\"rpm\\"] options = {\\"packager\\": \\"John Doe <jdoe@example.org>\\", \\"release\\": \\"2\\"} result = create_built_distribution(setup_script_path, formats, options) print(result) # Output: Built distributions created successfully for formats: gztar, zip, rpm ``` Notes - Consider using the `subprocess` module to execute shell commands. - The function should provide informative error messages if the process fails. - Performance is not a critical concern, but the function should be robust and handle errors gracefully.","solution":"import subprocess import sys def create_built_distribution(setup_script: str, formats: list, options: dict = None) -> str: Automates the creation of built distributions for a given Python module. Parameters: - setup_script (str): The path to the `setup.py` script of the Python module. - formats (list): A list of strings representing the types of built distributions to create. - options (dict, optional): Additional options to pass to the setup command. Returns: - str: Success message indicating completed built distribution creation. if options is None: options = {} supported_formats = {\\"gztar\\", \\"zip\\", \\"bztar\\", \\"xztar\\", \\"rpm\\", \\"wininst\\", \\"msi\\"} command_base = [sys.executable, setup_script, \\"bdist\\"] for format in formats: if format not in supported_formats: print(f\\"Format \'{format}\' is not supported.\\") continue command = command_base + [\\"--formats\\", format] for key, value in options.items(): command += [f\\"--{key}\\", str(value)] try: subprocess.run(command, check=True) except subprocess.CalledProcessError as e: print(f\\"Failed to create built distribution for format \'{format}\': {e}\\") return f\\"Built distributions created successfully for formats: {\', \'.join(formats)}\\""},{"question":"Objective You are required to simulate a computation-intensive task on the GPU using PyTorch, making use of CUDA stream handling and memory management functionalities. The task is to measure time for memory allocation and deallocation within different CUDA streams, and provide a summary of the memory usage statistics. Task 1. Initialize two CUDA streams. 2. Allocate a random tensor of size `(10000, 10000)` in each stream, record the time taken for allocation. 3. Synchronize the streams to ensure that all operations are completed. 4. Release the allocated memory and measure the time taken for deallocation. 5. Provide a memory summary after each step. Requirements 1. Implement the function `memory_stats_with_streams` which: - Initializes two CUDA streams. - Allocates a tensor in each stream and records allocation time. - Synchronizes the streams. - Deallocates the tensors and records deallocation time. - Prints memory usage summaries. 2. Ensure the function adheres to the following signature: ```python def memory_stats_with_streams(): pass ``` 3. Use `torch.cuda.memory.memory_summary` to print summary after each significant step: initialization, post-allocation, post-deallocation. 4. Use `torch.cuda.Stream` for creating and managing streams. 5. Performance measurements can be done using `torch.cuda.Event`. Expected Output The function should print: - Memory summary before allocation, - Time taken for tensor allocation in each stream, - Memory summary post allocation, - Time taken for tensor deallocation in each stream, - Memory summary post deallocation. Constraints - You must ensure the synchronization of streams before measuring deallocation times. - Assume the presence of at least one NVIDIA GPU on the system with CUDA installed. Example Your printed output may look like: ``` Memory Summary before allocation: ... Time taken for allocation in stream1: X ms Time taken for allocation in stream2: Y ms Memory Summary after allocation: ... Time taken for deallocation in stream1: A ms Time taken for deallocation in stream2: B ms Memory Summary after deallocation: ... ``` This question tests the student\'s ability to utilize CUDA functionalities in PyTorch, including stream management and memory handling, and provides a practical scenario to measure these operations.","solution":"import torch import time def memory_stats_with_streams(): if not torch.cuda.is_available(): print(\\"CUDA is not available on this system.\\") return # Initialize two CUDA streams stream1 = torch.cuda.Stream() stream2 = torch.cuda.Stream() print(\\"Memory Summary before allocation:\\") print(torch.cuda.memory.memory_summary()) # Record allocation start time alloc_start_event1 = torch.cuda.Event(enable_timing=True) alloc_end_event1 = torch.cuda.Event(enable_timing=True) alloc_start_event2 = torch.cuda.Event(enable_timing=True) alloc_end_event2 = torch.cuda.Event(enable_timing=True) # Allocate tensor in stream 1 with torch.cuda.stream(stream1): alloc_start_event1.record() tensor1 = torch.rand((10000, 10000), device=\'cuda\') alloc_end_event1.record() # Allocate tensor in stream 2 with torch.cuda.stream(stream2): alloc_start_event2.record() tensor2 = torch.rand((10000, 10000), device=\'cuda\') alloc_end_event2.record() # Synchronize streams to ensure allocation is finished torch.cuda.synchronize() # Measure allocation time alloc_time_stream1 = alloc_start_event1.elapsed_time(alloc_end_event1) alloc_time_stream2 = alloc_start_event2.elapsed_time(alloc_end_event2) print(f\\"Time taken for allocation in stream1: {alloc_time_stream1:.2f} ms\\") print(f\\"Time taken for allocation in stream2: {alloc_time_stream2:.2f} ms\\") print(\\"Memory Summary after allocation:\\") print(torch.cuda.memory.memory_summary()) # Record deallocation start time dealloc_start_event1 = torch.cuda.Event(enable_timing=True) dealloc_end_event1 = torch.cuda.Event(enable_timing=True) dealloc_start_event2 = torch.cuda.Event(enable_timing=True) dealloc_end_event2 = torch.cuda.Event(enable_timing=True) # Deallocate tensor in stream 1 & 2 with torch.cuda.stream(stream1): dealloc_start_event1.record() del tensor1 torch.cuda.empty_cache() dealloc_end_event1.record() with torch.cuda.stream(stream2): dealloc_start_event2.record() del tensor2 torch.cuda.empty_cache() dealloc_end_event2.record() # Synchronize streams to ensure deallocation is finished torch.cuda.synchronize() # Measure deallocation time dealloc_time_stream1 = dealloc_start_event1.elapsed_time(dealloc_end_event1) dealloc_time_stream2 = dealloc_start_event2.elapsed_time(dealloc_end_event2) print(f\\"Time taken for deallocation in stream1: {dealloc_time_stream1:.2f} ms\\") print(f\\"Time taken for deallocation in stream2: {dealloc_time_stream2:.2f} ms\\") print(\\"Memory Summary after deallocation:\\") print(torch.cuda.memory.memory_summary())"},{"question":"# Custom Python Interpreter with Enhanced Commands In this assessment, you are required to implement a custom Python interpreter with an enhanced interactive console that supports additional commands. The console should support basic Python statements and expressions, as well as a special command: `!reverse <string>`. When this command is entered, the console should print the reversed version of the given string. Requirements 1. **Interactive Console**: - Create a class `EnhancedConsole` that inherits from `code.InteractiveConsole`. - Override the `push` method to handle the `!reverse <string>` command. 2. **Special Command**: - The `!reverse <string>` command should be recognized and processed. For example, if the user inputs `!reverse hello`, the console should output `olleh`. 3. **Normal Python Commands**: - The console should still function as a normal Python interactive shell where users can input and execute Python code. Implementation 1. **Class Signature**: ```python import code class EnhancedConsole(code.InteractiveConsole): def __init__(self, locals=None, filename=\\"<stdin>\\"): super().__init__(locals, filename) def push(self, line): # Override this method pass ``` 2. **Expected Input and Output**: - Input will be strings simulating console input. - Output will be the strings printed to the console. 3. **Constraints**: - You must handle syntax errors and incomplete input gracefully. - The custom command `!reverse` should be processed only when valid. # Example Usage ```python console = EnhancedConsole() console.push(\'print(\\"Hello World\\")\') # Output: Hello World console.push(\'!reverse Python310\') # Output: 013nohtyP console.push(\'x = 5\') console.push(\'x * 2\') # Output: 10 ``` Write your implementation of the `EnhancedConsole` class. Ensure it satisfies the requirements and passes the example test cases.","solution":"import code class EnhancedConsole(code.InteractiveConsole): def __init__(self, locals=None, filename=\\"<stdin>\\"): super().__init__(locals, filename) def push(self, line): if line.startswith(\'!reverse \'): string_to_reverse = line[len(\'!reverse \'):] print(string_to_reverse[::-1]) else: super().push(line)"},{"question":"**Objective**: Demonstrate your understanding of Python\'s `html` module for manipulating HTML content. # Problem Statement: You are asked to create a function that accepts a string containing HTML content and returns a version of that string where: 1. HTML entities such as &lt;, &gt;, and &amp; are converted to their respective characters (<, >, &). 2. Any retained HTML characters (<, >, &) are subsequently escaped to their HTML-safe sequences. This process should ensure no HTML tags are present in the final string, making it safe for displaying as plain text in a web page. # Function Signature ```python def sanitize_html(content: str, quote: bool = True) -> str: pass ``` # Input - `content` (string): A string containing HTML content. - `quote` (boolean): A boolean flag indicating whether to also escape the characters (\\") and (\'). Default is `True`. # Output - Returns a sanitized string: - All HTML entities are unescaped. - All potentially harmful HTML characters are escaped. # Example ```python >>> sanitize_html(\\"&lt;div&gt;Hello &amp; \'World\'&lt;/div&gt;\\") \'&lt;div&gt;Hello &amp; &#x27;World&#x27;&lt;/div&gt;\' >>> sanitize_html(\\"&lt;button onclick=&quot;alert(\'Hi!\')&quot;&gt;Click me&lt;/button&gt;\\", quote=False) \'&lt;button onclick=\\"alert(&#x27;Hi!&#x27;)\\"&gt;Click me&lt;/button&gt;\' ``` # Constraints - The input string can have a maximum length of 10,000 characters. - The input string can contain any printable ASCII character. # Additional Notes - You should use `html.unescape()` for converting entities to their respective characters. - Following the unescape operation, use `html.escape()`, with the `quote` flag as specified, to ensure the string is safe for HTML display. # Performance Requirements - The function should run efficiently within the provided constraints.","solution":"import html def sanitize_html(content: str, quote: bool = True) -> str: Unescape HTML entities, then escape the HTML characters. Args: content (str): The HTML content to sanitize. quote (bool): Whether to also escape the quote (\\") and the apostrophe (\') characters. Returns: str: The sanitized content. # First, unescape the content to convert entities to characters. unescaped_content = html.unescape(content) # Then, escape the characters to make the string safe for display. sanitized_content = html.escape(unescaped_content, quote=quote) return sanitized_content"},{"question":"**Coding Assessment Question:** You are provided with a dataset named `diamonds` which can be loaded using `seaborn.load_dataset(\\"diamonds\\")`. You are required to create a bar plot using the `seaborn.objects.Plot` class according to the following specifications: 1. Plot the `carat` variable on the y-axis against the `clarity` variable on the x-axis. 2. Aggregate the carat values using the median. 3. Differentiate the bars using the `cut` variable by employing the `Dodge` transform. 4. Customize the colors of the bars to enhance visual differentiation of the `cut` categories. The goal is to generate a bar plot that provides insights into the median carat for different clarity levels, with further distinction by the cut quality. The plot should be clear and visually appealing. **Expected Input:** There are no additional inputs required; you will work directly with the `diamonds` dataset which can be loaded as: ```python import seaborn.objects as so from seaborn import load_dataset diamonds = load_dataset(\\"diamonds\\") ``` **Expected Output:** A bar plot fulfilling the above criteria should be displayed. There is no need to return any values; simply ensure that the plot appears. **Constraints:** - Use `seaborn.objects.Plot` and its relevant methods as described. - Ensure that the code is efficient and readable. - Include comments to explain each significant step. **Performance Requirements:** The solution should be capable of generating the plot in reasonable time, typically within a few seconds. Optimize your code where necessary to achieve this. **Example Code Template:** ```python import seaborn.objects as so from seaborn import load_dataset # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create Plot object plot = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\") # Customize plot: aggregating with median, applying Dodge transform and setting colors plot.add(so.Bar(), so.Agg(\\"median\\"), so.Dodge(), color=\\"cut\\") # Display the plot plot.show() ``` **Note:** Ensure to handle any potential issues such as missing data gracefully. Provide appropriate comments to explain steps and decisions.","solution":"import seaborn.objects as so from seaborn import load_dataset def generate_diamond_plot(): Generates a bar plot of the median carat values for different clarity levels, differentiated by the cut category. # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create Plot object plot = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\") # Customize plot: aggregating with median, applying Dodge transform, and setting colors plot.add(so.Bar(), so.Agg(\\"median\\"), so.Dodge(), color=\\"cut\\") # Display the plot plot.show() # Call the function to generate the plot generate_diamond_plot()"},{"question":"# Custom PyTorch Autograd Function and Module **Objective:** Implement a custom PyTorch autograd function and a corresponding module. **Problem Statement:** You are required to create a custom autograd function and a corresponding module in PyTorch. The custom function will implement a quadratic operation, and the module will use this function to perform a forward pass and compute gradients during backpropagation. **Details:** 1. **Custom Autograd Function:** - Implement a custom autograd function named `QuadraticFunction` by subclassing `torch.autograd.Function`. - This function should implement the operation ( y = a cdot x^2 + b cdot x + c ), where (a), (b), and (c) are scalar coefficients. - Implement the `forward` method to compute the output. - Implement the `setup_context` or combined `forward` with `ctx` to save tensors required for the backward pass. - Implement the `backward` method to compute the gradients with respect to the inputs (a), (b), (c), and (x). 2. **Custom Module:** - Implement a custom PyTorch module named `Quadratic` by subclassing `torch.nn.Module`. - The module should have trainable parameters (a), (b), and (c). - In the module\'s `forward` method, use the `QuadraticFunction` to compute the quadratic operation on a given input tensor. - The module\'s `__init__` method should initialize the parameters (a), (b), and (c) as `torch.nn.Parameter`. **Input and Output Formats:** - **Input:** - For the `QuadraticFunction.forward`, the input is a tensor `x` and scalars (a), (b), and (c). - For the `Quadratic.forward`, the input is a tensor `x`. - **Output:** - The output of `QuadraticFunction.forward` and `Quadratic.forward` is a tensor where each element is computed as ( y = a cdot x^2 + b cdot x + c ). **Constraints:** - Ensure proper handling and saving of tensors for the backward pass. - Validate the gradients using `gradcheck`. **Performance Requirements:** - The function and module should work efficiently for typical tensor sizes used in neural networks. # Example Code: ```python import torch from torch.autograd import Function class QuadraticFunction(Function): @staticmethod def forward(ctx, x, a, b, c): # Save input tensors for backward pass ctx.save_for_backward(x, a, b, c) # Compute the forward pass return a * x ** 2 + b * x + c @staticmethod def backward(ctx, grad_output): # Retrieve saved tensors x, a, b, c = ctx.saved_tensors grad_x = grad_a = grad_b = grad_c = None # Compute the gradients if ctx.needs_input_grad[0]: grad_x = grad_output * (2 * a * x + b) if ctx.needs_input_grad[1]: grad_a = grad_output * x ** 2 if ctx.needs_input_grad[2]: grad_b = grad_output * x if ctx.needs_input_grad[3]: grad_c = grad_output return grad_x, grad_a, grad_b, grad_c class Quadratic(torch.nn.Module): def __init__(self): super(Quadratic, self).__init__() self.a = torch.nn.Parameter(torch.tensor(1.0, requires_grad=True)) self.b = torch.nn.Parameter(torch.tensor(1.0, requires_grad=True)) self.c = torch.nn.Parameter(torch.tensor(1.0, requires_grad=True)) def forward(self, x): return QuadraticFunction.apply(x, self.a, self.b, self.c) # Test the module with a simple input x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) quadratic = Quadratic() output = quadratic(x) print(output) # Validate gradients using gradcheck input = (torch.randn(3, dtype=torch.double, requires_grad=True), torch.tensor(2.0, dtype=torch.double, requires_grad=True), torch.tensor(1.0, dtype=torch.double, requires_grad=True), torch.tensor(0.5, dtype=torch.double, requires_grad=True)) print(torch.autograd.gradcheck(QuadraticFunction.apply, input)) ``` **Notes:** - Use the provided example code as a reference to complete your implementation. - Perform thorough testing to ensure the correctness and robustness of your custom function and module.","solution":"import torch from torch.autograd import Function class QuadraticFunction(Function): @staticmethod def forward(ctx, x, a, b, c): # Save input tensors for backward pass ctx.save_for_backward(x, a, b, c) # Compute the forward pass return a * x ** 2 + b * x + c @staticmethod def backward(ctx, grad_output): # Retrieve saved tensors x, a, b, c = ctx.saved_tensors grad_x = grad_a = grad_b = grad_c = None # Compute the gradients if ctx.needs_input_grad[0]: grad_x = grad_output * (2 * a * x + b) if ctx.needs_input_grad[1]: grad_a = grad_output * x ** 2 if ctx.needs_input_grad[2]: grad_b = grad_output * x if ctx.needs_input_grad[3]: grad_c = grad_output return grad_x, grad_a, grad_b, grad_c class Quadratic(torch.nn.Module): def __init__(self): super(Quadratic, self).__init__() self.a = torch.nn.Parameter(torch.tensor(1.0, requires_grad=True)) self.b = torch.nn.Parameter(torch.tensor(1.0, requires_grad=True)) self.c = torch.nn.Parameter(torch.tensor(1.0, requires_grad=True)) def forward(self, x): return QuadraticFunction.apply(x, self.a, self.b, self.c)"},{"question":"Objective: Write a Python function to create an email with both text and an image as attachments. This function should utilize the `email.mime` classes to correctly structure the multi-part email. Problem Statement: **Function Signature**: ```python def create_mixed_email(subject: str, text_body: str, image_path: str, image_name: str, app_data: bytes, app_name: str) -> \'MIMEMultipart\': pass ``` **Input**: - `subject` (str): The subject of the email. - `text_body` (str): The plain text content of the email. - `image_path` (str): The file path to the image that will be attached. - `image_name` (str): The filename for the image to be used in the email attachment. - `app_data` (bytes): The bytes for the application data to be attached. - `app_name` (str): The filename for the application data attachment. **Output**: - Returns an instance of `MIMEMultipart` representing the complete email. **Constraints**: - The image file at `image_path` must be readable. - The resulting MIME structure should be correct and should include every part of the message (text, image, and application data). Requirements: 1. You need to use the appropriate classes from the `email.mime` package to construct the MIME parts. 2. The email should be structured as a `MIMEMultipart` object. 3. Ensure that the headers (e.g., `Content-Type`, `MIME-Version`) are appropriately included. 4. Attach the text, image, and application data parts correctly. Example Usage: ```python email = create_mixed_email( subject=\\"Test Email\\", text_body=\\"This is the email text.\\", image_path=\\"/path/to/image.jpg\\", image_name=\\"image.jpg\\", app_data=b\\"Some binary data...\\", app_name=\\"data.bin\\" ) print(email.as_string()) ``` The output should be a string representation of the MIME message that includes all the proper headers and body parts.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.base import MIMEBase from email import encoders def create_mixed_email(subject: str, text_body: str, image_path: str, image_name: str, app_data: bytes, app_name: str) -> \'MIMEMultipart\': # Create the main multipart container msg = MIMEMultipart() msg[\'Subject\'] = subject # Attach the text part text_part = MIMEText(text_body, \'plain\') msg.attach(text_part) # Attach the image part with open(image_path, \'rb\') as img_file: img_part = MIMEImage(img_file.read(), name=image_name) msg.attach(img_part) # Attach the application data part app_part = MIMEBase(\'application\', \'octet-stream\') app_part.set_payload(app_data) encoders.encode_base64(app_part) app_part.add_header(\'Content-Disposition\', f\'attachment; filename=\\"{app_name}\\"\') msg.attach(app_part) return msg"},{"question":"# **Coding Assessment Question** **Problem Statement** You are required to implement a Python function that processes a list of numerical values. The function applies several transformations and handles potential errors gracefully. 1. **Function Name**: `process_numbers` 2. **Input**: A list of integers `nums`. 3. **Output**: A list of integers after transformations. **Transformations**: 1. **Double Even Numbers**: Any even number in the list should be doubled. 2. **Square Odd Numbers**: Any odd number in the list should be squared. 3. **Exception Handling**: If the list contains a non-integer, the function should raise and handle a `ValueError`, adding a message `\\"List contains non-integer value\\"` to a special `errors` list. **Details**: 1. Create a nested function named `transform` inside `process_numbers` that applies the above transformations to a single number based on its properties (even or odd). 2. Use a nested scope to maintain an errors list that collects error messages. 3. Use exception handling to catch and log errors without stopping the entire processing of the list. 4. Use decorators to apply a logging function that prints the original and transformed number to the console. The expected implementation should look like this: ```python def process_numbers(nums): errors = [] def log_transformation(func): def wrapper(n): original_n = n try: transformed_n = func(n) print(f\\"{original_n} -> {transformed_n}\\") return transformed_n except ValueError as e: errors.append(str(e)) return wrapper @log_transformation def transform(num): if isinstance(num, int): if num % 2 == 0: return num * 2 else: return num ** 2 else: raise ValueError(\\"List contains non-integer value\\") result = [] for n in nums: try: result.append(transform(n)) except Exception as e: # Any additional exceptions that might be missed errors.append(str(e)) print(\\"Errors:\\", errors) return result # Example usage: nums = [1, 2, \'a\', 3, 4] print(process_numbers(nums)) ``` **Constraints**: - The input list can contain up to 1000 elements. - The elements can be integers or non-integer values (e.g., strings, floats). **Additional Notes**: - Ensure that your function prints the transformations and handles the errors as described. - The functionality of decorators to apply logging should be clearly demonstrated. - Highlight how nested functions and scopes manage the state (such as the `errors` list). Test your function with different input scenarios to verify its correctness.","solution":"def process_numbers(nums): errors = [] def log_transformation(func): def wrapper(n): original_n = n try: transformed_n = func(n) print(f\\"{original_n} -> {transformed_n}\\") return transformed_n except ValueError as e: errors.append(str(e)) return None return wrapper @log_transformation def transform(num): if isinstance(num, int): if num % 2 == 0: return num * 2 else: return num ** 2 else: raise ValueError(\\"List contains non-integer value\\") result = [] for n in nums: res = transform(n) if res is not None: result.append(res) print(\\"Errors:\\", errors) return result"},{"question":"**Objective:** You will be working with a large dataset and are required to implement efficient data loading and processing strategies using pandas. **Problem Statement:** Suppose you are given a large dataset stored in a directory of Parquet files. Each file contains time series data related to different years. Your task is to implement a function that processes this dataset to compute the average value of a numerical column (`\'x\'`) for each unique category in another column (`\'name\'`). To achieve this: 1. Load only the required columns to optimize memory usage. 2. Use efficient data types to reduce the memory footprint. 3. Process the dataset in chunks to handle the large dataset. # Function Signature ```python def compute_average_per_category(directory: str, numerical_column: str, category_column: str) -> dict: pass ``` # Input - `directory` (str): The path to the directory containing the Parquet files. - `numerical_column` (str): The name of the numerical column for which the average value needs to be computed. - `category_column` (str): The name of the category column used for grouping the data. # Output - A dictionary where the keys are the unique categories from the `category_column` and the values are the average values of the `numerical_column` for each category. # Constraints - The Parquet files can be very large and may not fit into memory all at once. - Only the necessary columns (\'numerical_column\' and \'category_column\') should be loaded to optimize memory usage. - You should use efficient data types to manage memory usage. # Example Usage ```python # Suppose we have Parquet files in the directory \'data/timeseries/\' with columns: [\'timestamp\', \'name\', \'id\', \'x\', \'y\'] directory = \'data/timeseries/\' numerical_column = \'x\' category_column = \'name\' result = compute_average_per_category(directory, numerical_column, category_column) print(result) # Output format should be a dictionary with categories and their corresponding average \'x\' values. ``` # Hints 1. Utilize `pandas.read_parquet` with the `columns` parameter to load only the necessary columns. 2. Convert the category column to `pandas.Categorical` for memory efficiency. 3. Process each file in chunks and aggregate the results to avoid memory issues.","solution":"import os import pandas as pd def compute_average_per_category(directory: str, numerical_column: str, category_column: str) -> dict: category_sum = {} category_count = {} for file_name in os.listdir(directory): if file_name.endswith(\'.parquet\'): file_path = os.path.join(directory, file_name) df = pd.read_parquet(file_path, columns=[category_column, numerical_column]) df[category_column] = df[category_column].astype(\'category\') for category, data in df.groupby(category_column): category_total = data[numerical_column].sum() category_size = data[numerical_column].count() if category in category_sum: category_sum[category] += category_total category_count[category] += category_size else: category_sum[category] = category_total category_count[category] = category_size averages = {category: category_sum[category] / category_count[category] for category in category_sum} return averages"},{"question":"You are asked to implement a Python class that mimics Python\'s built-in list but with added logging functionality for each sequence operation. This class, `LoggedList`, should log every operation performed on it to a provided log function. Requirements: 1. Implement the following methods with logging: - `__getitem__(self, index)` - `__setitem__(self, index, value)` - `__delitem__(self, index)` - `__len__(self)` - `__add__(self, other)` - `__mul__(self, count)` - `__contains__(self, item)` - `__iter__(self)` - `append(self, value)` - `count(self, value)` - `index(self, value)` 2. Each method should use the logging function to log its operation in the format: ``` \\"{operation_name} called with args: {args}\\" ``` Example: `\\"__getitem__ called with args: (2,)\\"` for getting the item at index 2. Example Usage: ```python def log_function(message): print(message) logged_list = LoggedList(log_function) logged_list.append(3) logged_list.append(4) print(logged_list[1]) logged_list[1] = 10 print(len(logged_list)) result = logged_list + [1, 2, 3] print(result) repeat_result = logged_list * 2 print(repeat_result) del logged_list[0] print(3 in logged_list) print(logged_list.count(3)) print(logged_list.index(10)) ``` Expected Output: ``` append called with args: (3,) append called with args: (4,) __getitem__ called with args: (1,) 4 __setitem__ called with args: (1, 10) __len__ called with args: () 2 __add__ called with args: ([1, 2, 3],) [3, 10, 1, 2, 3] __mul__ called with args: (2,) [3, 10, 3, 10] __delitem__ called with args: (0,) __contains__ called with args: (3,) True count called with args: (3,) 1 index called with args: (10,) 1 ``` Constraints: - You cannot use Python’s native list methods directly within your `LoggedList` implementation. - Ensure to handle edge cases such as index out of range and other errors gracefully, logging any exceptions that occur. Your implementation should be efficient and avoid unnecessary operations where possible.","solution":"class LoggedList: def __init__(self, log_function): self._list = [] self.log = log_function def __getitem__(self, index): self.log(f\\"__getitem__ called with args: ({index},)\\") return self._list[index] def __setitem__(self, index, value): self.log(f\\"__setitem__ called with args: ({index}, {value})\\") self._list[index] = value def __delitem__(self, index): self.log(f\\"__delitem__ called with args: ({index},)\\") del self._list[index] def __len__(self): self.log(\\"__len__ called with args: ()\\") return len(self._list) def __add__(self, other): self.log(f\\"__add__ called with args: ({other},)\\") if isinstance(other, LoggedList): return self._list + other._list return self._list + other def __mul__(self, count): self.log(f\\"__mul__ called with args: ({count},)\\") return self._list * count def __contains__(self, item): self.log(f\\"__contains__ called with args: ({item},)\\") return item in self._list def __iter__(self): self.log(\\"__iter__ called with args: ()\\") return iter(self._list) def append(self, value): self.log(f\\"append called with args: ({value},)\\") self._list.append(value) def count(self, value): self.log(f\\"count called with args: ({value},)\\") return self._list.count(value) def index(self, value): self.log(f\\"index called with args: ({value},)\\") return self._list.index(value)"},{"question":"# Coding Assessment: Utilizing the `sys` Module **Objective:** To assess your understanding and practical knowledge of the `sys` module in Python. You will be required to create a Python script that demonstrates the usage of various functionalities provided by the `sys` module. **Problem Statement:** Write a Python script that performs the following tasks: 1. **Command-line Arguments:** - Accepts command-line arguments and prints out the number of arguments passed and the arguments themselves. - Ensure that the script handles cases where no arguments are passed gracefully. 2. **Interpreter Information:** - Prints the version of the Python interpreter being used. - Displays the platform identifier string for the current operating system. 3. **System Path Modification:** - Adds a new directory to the `sys.path` list and then prints out the modified `sys.path` list. 4. **Exception Handling and Hooks:** - Implements a custom exception handler using `sys.excepthook()` that captures uncaught exceptions, logs them to a file named `error_log.txt`, and gracefully exits the script. - The custom handler should log the exception type, value, and traceback. 5. **Memory and Recursion Management:** - Prints the current recursion limit and sets a new recursion limit to `2000`. - Displays the number of allocated memory blocks using `sys.getallocatedblocks()`. **Constraints:** - The script should be runnable from the command line. - Handle all common edge cases, such as no command-line arguments. **Expected Input/Output:** Here\'s an example of how the script might be executed and what the output would look like: ```bash python script.py arg1 arg2 --- Command-line Arguments --- Number of Arguments: 3 Arguments: [\'script.py\', \'arg1\', \'arg2\'] --- Interpreter Information --- Python Version: 3.10.0 Platform: win32 --- System Path Modification --- Sys Path (after modification): [\'/path/to/new_directory\', \'...\'] --- Memory and Recursion Management --- Current Recursion Limit: 3000 New Recursion Limit: 2000 Allocated Memory Blocks: 5632 ``` If an unhandled exception occurs, it should be logged with the following format in the `error_log.txt` file: ``` Exception Type: exception_type Exception Value: exception_value Traceback: exception_traceback ``` **Note:** Make sure to provide appropriate comments in your code to explain your logic.","solution":"import sys import os import traceback def main(): # Command-line Arguments print(\\"n--- Command-line Arguments ---\\") args = sys.argv print(f\\"Number of Arguments: {len(args)}\\") print(f\\"Arguments: {args}\\") # Interpreter Information print(\\"n--- Interpreter Information ---\\") print(f\\"Python Version: {sys.version}\\") print(f\\"Platform: {sys.platform}\\") # System Path Modification print(\\"n--- System Path Modification ---\\") new_path = \'/path/to/new_directory\' sys.path.append(new_path) print(f\\"Sys Path (after modification): {sys.path}\\") # Memory and Recursion Management print(\\"n--- Memory and Recursion Management ---\\") print(f\\"Current Recursion Limit: {sys.getrecursionlimit()}\\") sys.setrecursionlimit(2000) print(f\\"New Recursion Limit: {2000}\\") print(f\\"Allocated Memory Blocks: {sys.getallocatedblocks()}\\") def custom_exception_handler(exctype, value, tb): with open(\\"error_log.txt\\", \\"w\\") as f: f.write(f\\"Exception Type: {exctype.__name__}n\\") f.write(f\\"Exception Value: {value}n\\") f.write(f\\"Traceback: {\'\'.join(traceback.format_tb(tb))}n\\") sys.exit(1) if __name__ == \'__main__\': sys.excepthook = custom_exception_handler # Setting custom exception handler main()"},{"question":"Objective Implement a function to perform an operation using Prims IR that involves type promotion and broadcasting in PyTorch. Details You need to implement a function `prims_add_with_promotion` that takes two tensors as input, promotes their types if necessary, and then performs element-wise addition using Prims IR in PyTorch. Input * `tensor1` (torch.Tensor): A tensor of any shape. * `tensor2` (torch.Tensor): Another tensor, which may require broadcasting to match the shape of `tensor1`. Output * Returns a tensor resulting from the element-wise addition of the input tensors after type promotion and broadcasting using Prims IR. Constraints 1. The function should properly handle type promotion based on PyTorch\'s type promotion rules. 2. The function should broadcast the tensors to a common shape if necessary. 3. You should use primitives from Prims IR such as `prims.convert_element_type` and `prims.broadcast_in_dim`. Example ```python import torch def prims_add_with_promotion(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: # Your implementation here pass # Example usage t1 = torch.tensor([1, 2, 3], dtype=torch.int32) t2 = torch.tensor([4.0, 5.0, 6.0], dtype=torch.float32) result = prims_add_with_promotion(t1, t2) print(result) # Expected output: tensor([5.0, 7.0, 9.0]) ``` Implementation Notes - You may refer to the PyTorch documentation to understand the available primitives in the `prims` namespace. - Properly handle broadcasting within the function to ensure the tensors can be added. - Ensure the function is robust and handles edge cases like different shapes and scalar inputs.","solution":"import torch import torch._prims as prims def prims_add_with_promotion(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: dtype1 = tensor1.dtype dtype2 = tensor2.dtype # Get the promoted dtype promoted_dtype = torch.promote_types(dtype1, dtype2) # Convert tensors to the promoted dtype if dtype1 != promoted_dtype: tensor1 = prims.convert_element_type(tensor1, promoted_dtype) if dtype2 != promoted_dtype: tensor2 = prims.convert_element_type(tensor2, promoted_dtype) # Broadcast tensors to a common shape broadcasted_tensor1, broadcasted_tensor2 = torch.broadcast_tensors(tensor1, tensor2) # Perform element-wise addition result = prims.add(broadcasted_tensor1, broadcasted_tensor2) return result"},{"question":"You are given a list of integer values representing the daily calories consumed by a person over a month. You are required to write a function that processes this data to find the following: 1. **Running Total of Calories**: Calculate the cumulative sum of calories for each day. 2. **Days to Drop Excess Calories**: Identify the first sequence of days where the running total of calories exceeds a given threshold and return the subsequence from the first exceeding day till the end of the month. 3. **Unique Sub-Sequence of Daily Intakes**: Produce a compressed list of integer pairs such that each pair represents a unique value from the original list and the count of how many times it appears consecutively. Write a function `process_calories(calorie_list, threshold)` that achieves the above objectives. Use the functions from the `itertools` module to solve this problem efficiently. Input: - `calorie_list`: A list of integers where each integer represents the calories consumed each day. - `threshold`: An integer representing the threshold of the cumulative calories. Output: - A tuple with three elements: 1. A list of integers representing the running total of calories. 2. A list of integers from the first day where the running total exceeds the threshold till the end of the month. 3. A list of tuples, each containing an integer (unique calorie count) and the count of its consecutive occurrence. Examples: ```python calorie_list = [200, 300, 400, 500, 600, 700, 800, 900] threshold = 1500 process_calories(calorie_list, threshold) # Output: ( # [200, 500, 900, 1400, 2000, 2700, 3500, 4400], # [600, 700, 800, 900], # [(200, 1), (300, 1), (400, 1), (500, 1), (600, 1), (700, 1), (800, 1), (900, 1)] #) ``` Constraints: 1. The input list will contain at most 31 elements representing days of a month. 2. Each integer in the calorie list will be a positive value not exceeding 5000. Notes: - Use the `accumulate()` function from `itertools` to calculate the running total. - Use the `dropwhile()` function to find the first day sequence exceeding the threshold. - Use the `groupby()` function to create the unique sub-sequence of daily intakes.","solution":"from itertools import accumulate, dropwhile, groupby def process_calories(calorie_list, threshold): # 1. Calculate the running total of calories running_total = list(accumulate(calorie_list)) # 2. Find the sequence of days where the running total exceeds the threshold excess_days = list(dropwhile(lambda x: x <= threshold, running_total)) if excess_days: start_index = running_total.index(excess_days[0]) days_exceeding_threshold = calorie_list[start_index:] else: days_exceeding_threshold = [] # 3. Produce the compressed list with unique calorie counts and consecutive occurrences compressed_sequence = [(key, len(list(group))) for key, group in groupby(calorie_list)] return (running_total, days_exceeding_threshold, compressed_sequence)"},{"question":"Title: Logging System Handler using `syslog` Module Objective: To assess the understanding of the syslog library for logging messages in Unix-based systems with various options, priorities, and facilities. Tasks: 1. Implement a Python class `SyslogHandler` that manages logging messages using the `syslog` module. Your class should provide methods to: - Send a log message with specified priority. - Open the log with specified options. - Close the log. - Set a log mask to control which messages should be logged based on their priority. 2. Your implementation should include the following methods: - `log_message(self, message: str, priority=syslog.LOG_INFO) -> None`: Sends a log message with the given priority. - `open_log(self, ident: str = None, logoption: int = 0, facility=syslog.LOG_USER) -> None`: Opens the logger with optional identification, log option, and facility. - `close_log(self) -> None`: Closes the logger. - `set_log_mask(self, maskpri: int) -> int`: Sets the log mask to the specified mask priority and returns the previous mask value. 3. Utilize appropriate syslog constants where necessary and make sure all methods handle possible exceptions gracefully with relevant error messages. Example Usage: ```python import syslog from typing import Optional class SyslogHandler: def log_message(self, message: str, priority=syslog.LOG_INFO) -> None: try: syslog.syslog(priority, message) except Exception as e: print(f\\"Failed to log message: {e}\\") def open_log(self, ident: Optional[str] = None, logoption: int = 0, facility=syslog.LOG_USER) -> None: try: if ident: syslog.openlog(ident, logoption, facility) else: syslog.openlog(logoption=logoption, facility=facility) except Exception as e: print(f\\"Failed to open log: {e}\\") def close_log(self) -> None: try: syslog.closelog() except Exception as e: print(f\\"Failed to close log: {e}\\") def set_log_mask(self, maskpri: int) -> int: try: return syslog.setlogmask(maskpri) except Exception as e: print(f\\"Failed to set log mask: {e}\\") return -1 # Example usage: handler = SyslogHandler() handler.open_log(ident=\\"MyApp\\", logoption=syslog.LOG_PID, facility=syslog.LOG_MAIL) handler.log_message(\\"System started\\", priority=syslog.LOG_INFO) handler.set_log_mask(syslog.LOG_UPTO(syslog.LOG_ERR)) handler.log_message(\\"This is a critical error\\", priority=syslog.LOG_CRIT) handler.close_log() ``` Input and Output Formats: - Input: No direct input from user. Interactions occur via method calls. - Output: Logging messages will be sent to the system logger. Error messages for exceptions will be printed to the standard output. Constraints: - Ensure proper handling of logging methods. - Use syslog constants appropriately. - Methods should handle exceptions gracefully.","solution":"import syslog from typing import Optional class SyslogHandler: def log_message(self, message: str, priority=syslog.LOG_INFO) -> None: Sends a log message with a given priority. try: syslog.syslog(priority, message) except Exception as e: print(f\\"Failed to log message: {e}\\") def open_log(self, ident: Optional[str] = None, logoption: int = 0, facility=syslog.LOG_USER) -> None: Opens the logger with optional identification, log option, and facility. try: if ident: syslog.openlog(ident, logoption, facility) else: syslog.openlog(logoption=logoption, facility=facility) except Exception as e: print(f\\"Failed to open log: {e}\\") def close_log(self) -> None: Closes the logger. try: syslog.closelog() except Exception as e: print(f\\"Failed to close log: {e}\\") def set_log_mask(self, maskpri: int) -> int: Sets the log mask to the specified mask priority and returns the previous mask value. try: return syslog.setlogmask(maskpri) except Exception as e: print(f\\"Failed to set log mask: {e}\\") return -1"},{"question":"You are tasked with developing a more modern traceback manager for Python scripts, similar to the deprecated \\"cgitb\\" module. Your implementation will include enhanced features for logging traceback information to both the console and files, enabling easier debugging of Python applications. # Requirements: 1. Implement a class `TracebackManager` with the following: * Method `enable(display: bool = True, logdir: str = None, context: int = 5, format: str = \'text\')`: Configures the exception handling behavior. * Method `log_exception(info: tuple, context: int = 5, format: str = \'text\') -> str`: Formats the given exception info either in plain text or HTML and returns the result as a string. * Method `handle_exception(info: tuple = None)`: Utilizes current exception info if none is provided, and logs/formats it based on the configuration. 2. The `enable()` method should replace the default `sys.excepthook` with a custom handler that utilizes `log_exception()` for formatting and optionally logs to a file if `logdir` is specified. 3. The `log_exception()` method should format the traceback info retrieved from the `info` tuple. The format should vary based on the `format` parameter, which can be either \'text\' or \'html\'. 4. The `handle_exception()` method should call `log_exception()` with the current exception info (obtained from `sys.exc_info()` if `info` is not provided). # Function Signatures: ```python class TracebackManager: def enable(self, display: bool = True, logdir: str = None, context: int = 5, format: str = \'text\') -> None: pass def log_exception(self, info: tuple, context: int = 5, format: str = \'text\') -> str: pass def handle_exception(self, info: tuple = None) -> None: pass ``` # Example Usage: ```python import sys # Setup the custom traceback manager tb_manager = TracebackManager() tb_manager.enable(display=True, logdir=\'error_logs\', context=3, format=\'html\') # An example function that raises an exception def test_func(): raise ValueError(\\"An example exception.\\") # Manually catch exception and use the custom traceback manager try: test_func() except: tb_manager.handle_exception(sys.exc_info()) ``` # Notes: * Ensure the formatted traceback includes the necessary details such as file names, line numbers, and local variable values. * If the `logdir` is specified, save the formatted traceback to a file in the specified directory.","solution":"import sys import os import traceback import html class TracebackManager: def __init__(self): self.display = True self.logdir = None self.context = 5 self.format = \'text\' def enable(self, display: bool = True, logdir: str = None, context: int = 5, format: str = \'text\') -> None: self.display = display self.logdir = logdir self.context = context self.format = format sys.excepthook = self.handle_exception def log_exception(self, info: tuple, context: int = 5, format: str = \'text\') -> str: tb_text = \'\'.join(traceback.format_exception(*info, limit=None, chain=True)) if format == \'html\': tb_text = html.escape(tb_text).replace(\'n\', \'<br>\').replace(\' \', \'&nbsp;\') tb_text = f\'<html><body><pre>{tb_text}</pre></body></html>\' return tb_text def handle_exception(self, info: tuple = None) -> None: if info is None: info = sys.exc_info() formatted_exception = self.log_exception(info, context=self.context, format=self.format) if self.display: print(formatted_exception) if self.logdir: if not os.path.exists(self.logdir): os.makedirs(self.logdir) filename = os.path.join(self.logdir, \'traceback.log\') with open(filename, \'a\') as f: f.write(formatted_exception) f.write(\'n\')"},{"question":"**Coding Assessment Question** # Objective: Create an asynchronous Python program to manage multiple tasks and demonstrate an understanding of asyncio\'s concurrency, timeouts, and task cancellation handling. # Requirements: Implement an asynchronous function `process_tasks` that performs the following steps: 1. Receives a list of task names and associated durations. 2. Creates a task for each name-duration pair using an asynchronous worker function. 3. Runs all tasks concurrently. 4. Waits for a maximum of `timeout` seconds for all tasks to complete. If a timeout occurs, the function should cancel all pending tasks. 5. Returns a dictionary containing the results of the tasks that completed successfully and the status of tasks that were cancelled or timed out. # Function Signature: ```python async def process_tasks(task_list: List[Tuple[str, int]], timeout: float) -> Dict[str, Any]: Process multiple asynchronous tasks with cancellation and timeout handling. :param task_list: A list of tuples where each tuple contains a task name and its duration in seconds. :param timeout: The maximum time to wait for all the tasks to complete in seconds. :return: A dictionary with task names as keys and their results or status (\\"cancelled\\" or \\"timeout\\") as values. pass ``` # Input: - `task_list`: A list of tuples, where each tuple contains a string (task name) and an integer (task duration in seconds). Example: `[(\\"Task1\\", 2), (\\"Task2\\", 3), (\\"Task3\\", 1)]` - `timeout`: A float representing the maximum time in seconds to wait for all tasks to complete. Example: `5.0` # Output: - A dictionary where the keys are task names and the values are either the result (e.g., \\"completed\\") or a status message (\\"cancelled\\" or \\"timeout\\"). Example: `{\\"Task1\\": \\"completed\\", \\"Task2\\": \\"completed\\", \\"Task3\\": \\"completed\\"}` # Constraints: - Use the `asyncio` module primarily for managing tasks. - Handle coroutines, task creation, and concurrent execution using `asyncio.sleep`, `asyncio.create_task`, `asyncio.wait_for`, `asyncio.gather`, etc. - Ensure cancellation of tasks effectively if a timeout occurs. # Example: ```python import asyncio from typing import List, Tuple, Dict, Any async def worker(task_name: str, duration: int) -> str: await asyncio.sleep(duration) return f\\"{task_name} completed\\" async def process_tasks(task_list: List[Tuple[str, int]], timeout: float) -> Dict[str, Any]: tasks = {task_name: asyncio.create_task(worker(task_name, duration)) for task_name, duration in task_list} results = {} try: completed_tasks = await asyncio.wait_for(asyncio.gather(*tasks.values(), return_exceptions=True), timeout) for task, result in zip(tasks.keys(), completed_tasks): if isinstance(result, Exception): results[task] = \\"error\\" else: results[task] = result except asyncio.TimeoutError: for task_name, task in tasks.items(): task.cancel() results[task_name] = \\"timeout\\" return results # Example usage task_list = [(\\"Task1\\", 2), (\\"Task2\\", 3), (\\"Task3\\", 1)] timeout = 5.0 # Note: To run example usage, this code block should be executed in an environment that supports asyncio (e.g., Jupyter or main Python script) result = asyncio.run(process_tasks(task_list, timeout)) print(result) ``` # Notes: - The provided example demonstrates a potential solution structure. Students are encouraged to implement and test their own version ensuring it meets the specified requirements and handles edge cases. - This question assesses the ability to use asyncio for running and managing concurrent tasks, handling timeouts, and ensuring proper cleanup of pending tasks.","solution":"import asyncio from typing import List, Tuple, Dict, Any async def worker(task_name: str, duration: int) -> str: await asyncio.sleep(duration) return f\\"{task_name} completed\\" async def process_tasks(task_list: List[Tuple[str, int]], timeout: float) -> Dict[str, Any]: tasks = {task_name: asyncio.create_task(worker(task_name, duration)) for task_name, duration in task_list} results = {} try: completed_tasks = await asyncio.wait_for(asyncio.gather(*tasks.values(), return_exceptions=True), timeout) for task, result in zip(tasks.keys(), completed_tasks): if isinstance(result, Exception): results[task] = \\"error\\" else: results[task] = result except asyncio.TimeoutError: for task_name, task in tasks.items(): task.cancel() results[task_name] = \\"timeout\\" # Ensure all tasks are indeed cancelled await asyncio.gather(*tasks.values(), return_exceptions=True) return results"},{"question":"**Objective**: Demonstrate your understanding of Seaborn\'s data visualization capabilities by creating and customizing a line plot using multiple aesthetic mappings and dataset transformations. Problem Statement You are provided with the Seaborn `fmri` dataset that contains brain activity measurements. Your task is to create a complex line plot and customize it using various Seaborn features. Steps to Follow: 1. **Load the `fmri` Dataset**: - Load the `fmri` dataset using Seaborn\'s `load_dataset` function. 2. **Data Transformation**: - Filter the dataset to only include measurements where the event is \\"stim\\". 3. **Create the Line Plot**: - Plot the `timepoint` on the x-axis and `signal` on the y-axis. - Use `hue` to differentiate between `region`. - Use `style` to differentiate between `subject`. - Use `markers` and disable `dashes` for `style`. 4. **Customization**: - Set the line width (`lw`) to 2. - Set the palette to \\"coolwarm\\". 5. **Display the Plot**: - Use `plt.show()` to render the plot. Expected Function Signature ```python def plot_fmri_data(): # Implementation here pass ``` Example Output The plot should be a line plot with: - `timepoint` on the x-axis - `signal` on the y-axis - Different colors for each `region` - Different markers for each `subject` - Custom line width and color palette settings applied **Constraints**: - Use only Seaborn and Matplotlib libraries. - Ensure that the plot displays correctly with all specified customizations. ```python import seaborn as sns import matplotlib.pyplot as plt def plot_fmri_data(): # Implementation here fmri = sns.load_dataset(\\"fmri\\") fmri_stim = fmri[fmri[\\"event\\"] == \\"stim\\"] sns.lineplot( data=fmri_stim, x=\\"timepoint\\", y=\\"signal\\", hue=\\"region\\", style=\\"subject\\", markers=True, dashes=False, lw=2, palette=\\"coolwarm\\" ) plt.show() ``` Submission Submit your function definition. Ensure that your code is syntactically correct and performs the required operations to generate the plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_fmri_data(): # Load the fmri dataset fmri = sns.load_dataset(\\"fmri\\") # Filter the dataset to only include measurements where the event is \\"stim\\" fmri_stim = fmri[fmri[\\"event\\"] == \\"stim\\"] # Create the line plot with specified customizations sns.lineplot( data=fmri_stim, x=\\"timepoint\\", y=\\"signal\\", hue=\\"region\\", style=\\"subject\\", markers=True, dashes=False, lw=2, palette=\\"coolwarm\\" ) # Display the plot plt.show()"},{"question":"**Question: Implement a Concurrent Chat Server Using asyncio** You are required to implement a basic concurrent chat server using the asyncio library. The server should be able to handle multiple clients simultaneously, allowing them to send and receive messages in real-time. # Requirements and Constraints: 1. **Function Signature**: ```python import asyncio async def chat_server(host: str, port: int): # Your implementation here ``` 2. **Input**: - `host` (str): The hostname or IP address where the server will be running. - `port` (int): The port number on which the server will listen for incoming connections. 3. **Output**: - The function does not return any value, but it must handle multiple client connections concurrently. 4. **Constraints**: - The server should handle at least 10 concurrent client connections. - Each client should be able to send messages to the server, and the server should broadcast each received message to all connected clients. - Ensure that the server can handle clients connecting and disconnecting at any time. - Use the asyncio library to manage concurrent connections and asynchronous message broadcasting. - Implement basic error handling to ensure that any exceptions raised during the communication do not crash the server. 5. **Performance Requirements**: - The server should be responsive and capable of managing multiple connections without significant delays. # Example Usage: ```python async def main(): await chat_server(\'127.0.0.1\', 8888) if __name__ == \\"__main__\\": import asyncio asyncio.run(main()) ``` # Notes: - Focus on using asyncio’s high-level APIs such as `asyncio.StreamReader`, `asyncio.StreamWriter`, and other synchronization primitives if necessary. - You may test your server using multiple instances of a simple client script that connects to the server, sends messages, and listens for broadcasts. # Hints: - You can use `asyncio.start_server()` for creating the server. - Handling each client connection can be done within a coroutine that reads from and writes to the client’s stream. - Make use of `asyncio.gather()` for managing multiple coroutines.","solution":"import asyncio clients = [] async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') clients.append(writer) print(f\\"Connected with {addr}\\") while True: try: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received {message} from {addr}\\") await broadcast(message, writer) except (asyncio.IncompleteReadError, ConnectionResetError): break # Remove the client on disconnect clients.remove(writer) writer.close() await writer.wait_closed() print(f\\"Disconnected from {addr}\\") async def broadcast(message, writer): Send a message to all clients except the sender. for client in clients: if client != writer: client.write(message.encode()) await client.drain() async def chat_server(host: str, port: int): server = await asyncio.start_server(handle_client, host, port) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() # Example usage: async def main(): await chat_server(\'127.0.0.1\', 8888) if __name__ == \\"__main__\\": import asyncio asyncio.run(main())"},{"question":"Coding Assessment Question # Objective: Assess the students\' knowledge and proficiency in using the Seaborn library, specifically focusing on `swarmplot` and `catplot` for creating advanced visualizations. # Problem Statement: You are provided with a dataset containing information about customers, their purchase history, and demographic details. Your task is to create an interactive and informative visualization using Seaborn\'s `swarmplot` and `catplot` functions to analyze this data effectively. # Dataset: A CSV file named `customer_data.csv` is available with the following columns: - `customer_id`: Unique identifier for each customer. - `age`: Age of the customer. - `gender`: Gender of the customer (Male, Female, Other). - `annual_income`: Annual income of the customer in USD. - `purchase_amount`: Amount spent by the customer in their last transaction. - `purchase_category`: Category of the purchased item (e.g., Electronics, Clothing, Groceries). # Tasks: 1. **Load and Preprocess Data:** - Load the dataset into a pandas DataFrame. - Handle any missing data appropriately (e.g., impute, drop). 2. **Basic Swarmplot:** - Create a basic `swarmplot` with `annual_income` on the x-axis to visualize the distribution of annual incomes. 3. **Categorical Swarmplot:** - Create a `swarmplot` to compare the `purchase_amount` against different `purchase_category`. 4. **Hue Customization:** - Use the `hue` parameter in a `swarmplot` to show the relationship between `purchase_amount` and `purchase_category` divided by `gender`. 5. **Dodge and Color Palette:** - Modify the previous plot to use `dodge=True` to separate the genders and apply a categorical color palette. 6. **Facet Grid with Catplot:** - Create a facet grid using `catplot` to display the relationship between `purchase_amount` and `purchase_category` split by `gender` across different age groups (you can define age groups). # Constraints: - Ensure your plots are clear and aesthetically pleasing. Use appropriate titles, axis labels, and legends. - Assume that the CSV file is properly formatted and contains no corrupt data. - You are free to use additional Seaborn or Matplotlib functions to enhance the visualizations. # Expected Input and Output: - **Input:** A CSV file named `customer_data.csv`. - **Output:** A series of plots leveraging Seaborn\'s `swarmplot` and `catplot` functions, focusing on different aspects of the customer data. # Example: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the dataset data = pd.read_csv(\'customer_data.csv\') # Handle missing data data.dropna(inplace=True) # Basic Swarmplot sns.swarmplot(data=data, x=\'annual_income\') plt.show() # Categorical Swarmplot sns.swarmplot(data=data, x=\'purchase_category\', y=\'purchase_amount\') plt.show() # Hue Customization sns.swarmplot(data=data, x=\'purchase_category\', y=\'purchase_amount\', hue=\'gender\') plt.show() # Dodge and Color Palette sns.swarmplot(data=data, x=\'purchase_category\', y=\'purchase_amount\', hue=\'gender\', dodge=True, palette=\'deep\') plt.show() # Facet Grid with Catplot sns.catplot(data=data, kind=\'swarm\', x=\'purchase_category\', y=\'purchase_amount\', hue=\'gender\', col=\'age_group\', aspect=.5) plt.show() ``` **Note:** The above code is just an example to get you started. You need to complete each part according to the problem statement.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_and_preprocess_data(file_path): Loads data from a CSV file and handles missing values. Parameters: file_path (str): Path to the CSV file. Returns: DataFrame: Preprocessed data. data = pd.read_csv(file_path) data.dropna(inplace=True) # Removing rows with missing values for simplicity return data def create_basic_swarmplot(data): Creates a basic swarmplot for annual income. Parameters: data (DataFrame): Input data. sns.swarmplot(data=data, x=\'annual_income\') plt.title(\'Swarmplot for Annual Income\') plt.xlabel(\'Annual Income (USD)\') plt.ylabel(\'Distribution\') plt.show() def create_categorical_swarmplot(data): Creates a categorical swarmplot comparing purchase amounts across categories. Parameters: data (DataFrame): Input data. sns.swarmplot(data=data, x=\'purchase_category\', y=\'purchase_amount\') plt.title(\'Purchase Amount by Category\') plt.xlabel(\'Purchase Category\') plt.ylabel(\'Purchase Amount (USD)\') plt.show() def create_hue_swarmplot(data): Creates a swarmplot with hue for gender. Parameters: data (DataFrame): Input data. sns.swarmplot(data=data, x=\'purchase_category\', y=\'purchase_amount\', hue=\'gender\') plt.title(\'Purchase Amount by Category and Gender\') plt.xlabel(\'Purchase Category\') plt.ylabel(\'Purchase Amount (USD)\') plt.legend(title=\'Gender\') plt.show() def create_dodge_palette_swarmplot(data): Creates a swarmplot with hue and dodge for gender and applies a palette. Parameters: data (DataFrame): Input data. sns.swarmplot(data=data, x=\'purchase_category\', y=\'purchase_amount\', hue=\'gender\', dodge=True, palette=\'deep\') plt.title(\'Purchase Amount by Category and Gender with Dodge\') plt.xlabel(\'Purchase Category\') plt.ylabel(\'Purchase Amount (USD)\') plt.legend(title=\'Gender\') plt.show() def create_facet_grid_catplot(data): Creates a FacetGrid with catplot to show purchase amount by category across gender. Parameters: data (DataFrame): Input data. data[\'age_group\'] = pd.cut(data[\'age\'], bins=[0, 25, 45, 65, 100], labels=[\'<25\', \'25-45\', \'45-65\', \'65+\']) g = sns.catplot(data=data, kind=\'swarm\', x=\'purchase_category\', y=\'purchase_amount\', hue=\'gender\', col=\'age_group\') g.set_titles(\'Age Group: {col_name}\') g.set_axis_labels(\'Purchase Category\', \'Purchase Amount (USD)\') plt.show()"},{"question":"Objective You are required to implement a Python class that utilizes the `gettext` module to provide internationalization support for a given application. The class should support dynamic language switching and retrieval of translated messages based on provided context. Requirements 1. Implement a class `I18nHandler` that: - Initializes with a default language and a path to the locale directory containing `.mo` files. - Provides methods to change the language dynamically. - Retrieves translated messages for both singular and plural forms. - Supports context-specific translations. Class Definition ```python class I18nHandler: def __init__(self, default_language: str, locale_path: str): Initializes the I18nHandler with a default language and locale directory path. :param default_language: The default language code (e.g., \'en\'). :param locale_path: The path to the locale directory containing .mo files. # Your implementation here. pass def set_language(self, language: str): Sets the current language for translations. :param language: The language code to switch to (e.g., \'fr\'). # Your implementation here. pass def gettext(self, message: str) -> str: Retrieves the translated message for the current language. :param message: The message string to translate. :return: The translated message string. # Your implementation here. pass def ngettext(self, singular: str, plural: str, n: int) -> str: Retrieves the translated message based on plural forms for the current language. :param singular: The singular form of the message. :param plural: The plural form of the message. :param n: The number to determine singular or plural form. :return: The translated message string. # Your implementation here. pass def pgettext(self, context: str, message: str) -> str: Retrieves the translated message based on context for the current language. :param context: The context to use for translation. :param message: The message string to translate. :return: The translated message string. # Your implementation here. pass ``` Input and Output - **Input:** Methods within the `I18nHandler` class take strings and integers as input parameters. - **Output:** Methods return the localized translations of input strings. Constraints - You must create the necessary `.mo` files in your locale directory for testing. - Utilize the class-based API (`gettext.translation(...)`) for implementation. - Handle any exceptions that may arise from missing translation files gracefully. Example Usage ```python # Create an I18nHandler instance with default language \'en\' (English) and locale path \'/path/to/locales\' i18n = I18nHandler(\'en\', \'/path/to/locales\') # Get a translated message translated_message = i18n.gettext(\'Hello, World!\') print(translated_message) # Change language to French i18n.set_language(\'fr\') # Get a translated message in French translated_message = i18n.gettext(\'Hello, World!\') print(translated_message) # Get a pluralized translated message in French translated_message_plural = i18n.ngettext(\'There is one apple\', \'There are many apples\', 5) print(translated_message_plural) ``` Use this class to implement internationalization for a simple application, ensuring that all messages can be translated dynamically based on the current language setting.","solution":"import os import gettext class I18nHandler: def __init__(self, default_language: str, locale_path: str): Initializes the I18nHandler with a default language and locale directory path. :param default_language: The default language code (e.g., \'en\'). :param locale_path: The path to the locale directory containing .mo files. self.locale_path = locale_path self.set_language(default_language) def set_language(self, language: str): Sets the current language for translations. :param language: The language code to switch to (e.g., \'fr\'). try: lang_translations = gettext.translation(\'base\', localedir=self.locale_path, languages=[language]) lang_translations.install() self._ = lang_translations.gettext self.ngettext = lang_translations.ngettext self.pgettext = lang_translations.pgettext except Exception as e: # If the language files are missing, fallback to the default implementation gettext.install(\'base\') self._ = gettext.gettext self.ngettext = gettext.ngettext self.pgettext = gettext.pgettext def gettext(self, message: str) -> str: Retrieves the translated message for the current language. :param message: The message string to translate. :return: The translated message string. return self._(message) def ngettext(self, singular: str, plural: str, n: int) -> str: Retrieves the translated message based on plural forms for the current language. :param singular: The singular form of the message. :param plural: The plural form of the message. :param n: The number to determine singular or plural form. :return: The translated message string. return self.ngettext(singular, plural, n) def pgettext(self, context: str, message: str) -> str: Retrieves the translated message based on context for the current language. :param context: The context to use for translation. :param message: The message string to translate. :return: The translated message string. return self.pgettext(context, message)"},{"question":"# Question: Implement a Type-Safe Function and Test It Background: You are working on a library that manipulates lists of data. One common operation is to process a list of strings where each string represents a number in text form (e.g., \\"one\\", \\"two\\", \\"three\\") and convert them to their numerical equivalents (e.g., 1, 2, 3). You are also required to ensure that this implementation is robustly tested using unit tests and mocks where necessary. Task: 1. **Implement a function `convert_to_numbers`**: - It should accept a list of strings representing numbers in text form. - It should return a list of integers corresponding to the input list. 2. **Use type hints** to clearly define the types expected by your function. 3. **Write unit tests** using the `unittest` framework to validate the correctness of your `convert_to_numbers` function. - Include tests for different scenarios, such as valid inputs, invalid inputs, and edge cases. - Use `unittest.mock` to mock any part of your code if needed. Function Signature: ```python from typing import List def convert_to_numbers(text_list: List[str]) -> List[int]: pass ``` Examples: ```python # Example 1 input_list = [\\"one\\", \\"two\\", \\"three\\"] output_list = convert_to_numbers(input_list) assert output_list == [1, 2, 3] # Example 2 input_list = [\\"ten\\", \\"eleven\\"] output_list = convert_to_numbers(input_list) assert output_list == [10, 11] # Example 3 input_list = [\\"zero\\"] output_list = convert_to_numbers(input_list) assert output_list == [0] ``` # Requirements: 1. **Type Hints**: Ensure all functions use type hints. 2. **Testing**: - Write a comprehensive set of unittests. - Validate different edge cases and error handling. - Utilize `mock` from `unittest.mock` where appropriate. Constraints: - Input strings will be in lowercase and valid English representations of numbers from \\"zero\\" to \\"nineteen\\". - Inputs beyond \\"nineteen\\" are not required to be handled. - Ensure your solution gracefully handles invalid or unsupported inputs. Performance: - Your solution should have a linear runtime complexity O(n) where n is the length of the input list. Good luck, and happy coding!","solution":"from typing import List def convert_to_numbers(text_list: List[str]) -> List[int]: Converts a list of strings representing numbers in text form to their numerical equivalents. Args: text_list : List of strings where each string is a text form of numbers from \\"zero\\" to \\"nineteen\\". Returns: List of integers corresponding to the input list. text_to_number = { \\"zero\\": 0, \\"one\\": 1, \\"two\\": 2, \\"three\\": 3, \\"four\\": 4, \\"five\\": 5, \\"six\\": 6, \\"seven\\": 7, \\"eight\\": 8, \\"nine\\": 9, \\"ten\\": 10, \\"eleven\\": 11, \\"twelve\\": 12, \\"thirteen\\": 13, \\"fourteen\\": 14, \\"fifteen\\": 15, \\"sixteen\\": 16, \\"seventeen\\": 17, \\"eighteen\\": 18, \\"nineteen\\": 19, } result = [] for text in text_list: if text in text_to_number: result.append(text_to_number[text]) else: raise ValueError(f\\"Unsupported text number: \'{text}\'\\") return result"},{"question":"# Question: Implement a Named Tensor Reduction Operation You are required to implement a function `named_tensor_sum` that performs a sum reduction over specified dimensions of a named tensor. The function should follow the name inference rules provided, removing specified dimensions and propagating the remaining dimension names to the output tensor. Function Signature ```python import torch from typing import List, Union def named_tensor_sum(tensor: torch.Tensor, dims: Union[List[str], List[int]], keepdim: bool = False) -> torch.Tensor: pass ``` Input - `tensor`: A `torch.Tensor` with named dimensions. - `dims`: A list of dimension names or indices to reduce over. - `keepdim`: An optional boolean to indicate whether to retain the reduced dimensions with a size of 1 (default is `False`). Output - A `torch.Tensor` that is the result of summing over the specified dimensions, with the appropriate names propagated as per the reduction rules. Constraints - The input tensor must have named dimensions. - The dimensions specified in `dims` must exist in the input tensor. - The function should correctly update the names of the output tensor. Example ```python # Example 1 x = torch.randn(3, 4, names=(\'N\', \'C\')) result = named_tensor_sum(x, [\'N\']) print(result.names) # Should output: (\'C\') # Example 2 x = torch.randn(1, 3, 3, 3, names=(\'N\', \'C\', \'H\', \'W\')) result = named_tensor_sum(x, [\'N\', \'C\']) print(result.names) # Should output: (\'H\', \'W\') # Example 3: Keeping dimensions x = torch.randn(1, 3, 4, names=(\'A\', \'B\', \'C\')) result = named_tensor_sum(x, [\'A\'], keepdim=True) print(result.names) # Should output: (\'A\', \'B\', \'C\') print(result.shape) # Should output: torch.Size([1, 3, 4]) ``` Ensure to handle both cases where `dims` are names or indices. You can use `named_tensor_sum` to verify that your solution is working as expected. Implementation Notes - Utilize PyTorch\'s support for named tensors (`tensor.names`, `tensor.sum(dims, keepdim)`) to implement the function. - Properly check that the dimensions specified are part of the tensor’s names. - When `keepdim` is `True`, ensure that the reduced dimensions are retained with a size of 1. Good luck!","solution":"import torch from typing import List, Union def named_tensor_sum(tensor: torch.Tensor, dims: Union[List[str], List[int]], keepdim: bool = False) -> torch.Tensor: # Check if dimensions are provided as names or indices if isinstance(dims[0], str): dims_indices = [tensor.names.index(dim) for dim in dims] else: dims_indices = dims result = tensor.sum(dim=dims_indices, keepdim=keepdim) return result"},{"question":"**Objective**: Demonstrate your understanding of the `heapq` module and its application in a practical scenario. You are tasked with designing a system that simulates a task scheduler using a priority queue. This scheduler will handle tasks with different priorities and ensure that tasks are processed in the order of their priority. **Problem Statement**: You need to implement a class `TaskScheduler` that uses the `heapq` module to manage tasks efficiently. The `TaskScheduler` class should support the following operations: 1. **Adding a Task**: Add a new task with a specified priority. 2. **Removing a Task**: Remove an existing task by its description. 3. **Popping the Highest Priority Task**: Pop and return the task with the highest priority. 4. **Updating a Task\'s Priority**: Update the priority of an existing task. 5. **List All Tasks**: Return a list of all tasks sorted by their priorities. **Requirements**: 1. Use the `heapq` module for managing the heap operations. 2. Ensure that tasks with the same priority are processed in the order they were added. # Class Definition Implement a class `TaskScheduler` with the following methods: 1. `add_task(task: str, priority: int) -> None`: Adds a new task or updates the priority of an existing task. 2. `remove_task(task: str) -> None`: Removes a task given its description. If the task is not present, raise a `KeyError`. 3. `pop_task() -> str`: Pops and returns the task with the highest priority. If the queue is empty, raise a `KeyError`. 4. `update_priority(task: str, priority: int) -> None`: Updates the priority of an existing task. If the task is not present, raise a `KeyError`. 5. `list_tasks() -> List[Tuple[int, str]]`: Returns a list of tuples containing tasks and their priorities, sorted by priority. # Example Usage ```python scheduler = TaskScheduler() scheduler.add_task(\\"Write code documentation\\", 5) scheduler.add_task(\\"Implement feature\\", 2) scheduler.add_task(\\"Fix bugs\\", 3) print(scheduler.list_tasks()) # Output: [(2, \\"Implement feature\\"), (3, \\"Fix bugs\\"), (5, \\"Write code documentation\\")] scheduler.remove_task(\\"Fix bugs\\") print(scheduler.list_tasks()) # Output: [(2, \\"Implement feature\\"), (5, \\"Write code documentation\\")] scheduler.update_priority(\\"Implement feature\\", 6) print(scheduler.list_tasks()) # Output: [(5, \\"Write code documentation\\"), (6, \\"Implement feature\\")] task = scheduler.pop_task() print(task) # Output: Write code documentation ``` # Constraints 1. Task descriptions are unique strings. 2. Priorities are integer values. 3. The heap operations should maintain the heap property efficiently. 4. You must handle edge cases like removing or updating non-existent tasks. # Performance Requirements 1. Adding a task should have O(log n) time complexity. 2. Popping the highest priority task should have O(log n) time complexity. 3. Removing a task and updating a task should have O(log n) time complexity. 4. Listing all tasks should have O(n log n) time complexity due to the sorting requirement. Implement the `TaskScheduler` class with the specified functionalities and constraints.","solution":"import heapq class TaskScheduler: def __init__(self): self.heap = [] self.task_map = {} self.entry_finder = {} self.REMOVED = \'<removed-task>\' self.counter = 0 def add_task(self, task: str, priority: int) -> None: if task in self.entry_finder: self.remove_task(task) count = self.counter entry = [priority, count, task] self.entry_finder[task] = entry heapq.heappush(self.heap, entry) self.counter += 1 def remove_task(self, task: str) -> None: entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def pop_task(self) -> str: while self.heap: priority, count, task = heapq.heappop(self.heap) if task is not self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\') def update_priority(self, task: str, priority: int) -> None: self.remove_task(task) self.add_task(task, priority) def list_tasks(self): return sorted((priority, task) for priority, count, task in self.heap if task is not self.REMOVED)"},{"question":"Coding Assessment Question # Objective You are tasked with demonstrating your understanding of seaborn\'s capabilities, specifically in creating and customizing categorical plots. # Problem Statement Write a function named `analyze_titanic_data` that generates and customizes a seaborn plot to analyze the Titanic dataset. Your function should: 1. Load the \\"titanic\\" dataset using `seaborn`. 2. Create a boxen plot (`kind=\\"boxen\\"`) to visualize the distribution of ages for each class (`x=\\"age\\"`, `y=\\"class\\"`). 3. Use the `hue` parameter to differentiate the data by sex. 4. Split the plot into two subplots based on the class of the passengers (`col=\\"class\\"`). 5. Customize the plot by: - Setting the x-axis label to \\"Age\\". - Setting the y-axis label to \\"Number of Passengers\\". - Setting the titles of the subplots to display the class name (e.g., \\"First Class\\", \\"Second Class\\", \\"Third Class\\"). - Ensuring the survival rates are between 0 and 1. - Removing the left spine of the plot. # Function Signature ```python def analyze_titanic_data() -> None: pass ``` # Expected Output The function should generate and display the following: - A boxen plot with two subplots, each representing a different class of Titanic passengers. - The x-axis labeled as \\"Age\\". - The y-axis labeled as \\"Number of Passengers\\". - Subplots titled with the class they represent. - Survival rates constrained between 0 and 1. - Left spine of the plot removed. # Example When you run the function `analyze_titanic_data()`, it should output a plot similar to the one described above. # Constraints - You should use the `seaborn` and `matplotlib.pyplot` libraries to create and customize the plot. - Ensure the plot is clearly readable and well-formatted. # Performance Requirements - The function should run efficiently and generate the plot within a reasonable amount of time.","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_titanic_data() -> None: # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create the boxen plot with the specified parameters g = sns.catplot( data = titanic, x = \\"age\\", y = \\"class\\", hue = \\"sex\\", kind = \\"boxen\\", col = \\"class\\", height = 4, aspect = 0.7, col_wrap = 2 ) # Customize the plot as specified g.set_axis_labels(\\"Age\\", \\"Number of Passengers\\") g.set_titles(\\"{col_name} Class\\") for ax in g.axes.flatten(): ax.set_xlim(0, 80) # Set limit of Age axis for readability ax.spines[\\"left\\"].set_visible(False) # Show plot plt.show()"},{"question":"**Objective:** Demonstrate your understanding of seaborn\'s `seaborn.objects` interface through creating and customizing plots. **Problem Statement:** You are given a dataset `tips` which contains the days on which different people visited a restaurant and their corresponding total bill amounts and tips given. You are to perform the following tasks using seaborn: 1. **Basic Plot Construction:** Create a scatter plot using `so.Plot` to visualize the relationship between `total_bill` and `tip`. 2. **Plot Customization:** Modify the scatter plot to add an edge color of white (`w`) to the dots for better visualization. 3. **Advanced Visualization:** Create a visualization where you plot `total_bill` against `day` while dodging and jittering the points to reduce overplotting. Color the dots based on the `sex` category. 4. **Error Bars:** Show the average total bill on each day with an error bar representing the standard error. **Input:** - Use the `tips` dataset from seaborn. You can load it using: ```python from seaborn import load_dataset tips = load_dataset(\\"tips\\") ``` **Expected Outputs:** 1. A basic scatter plot of `total_bill` vs `tip`. 2. A customized scatter plot with white-edged dots. 3. An advanced scatter plot of `total_bill` vs `day` with dodge and jitter, colored by `sex`. 4. A plot with average `total_bill` per `day` including error bars representing standard errors. **Constraints:** - Use the seaborn objects interface (`seaborn.objects as so`). **Performance Requirements:** - Ensure that the plots are generated efficiently and clearly, without any redundant or unnecessary code. Here’s an example template to get you started: ```python import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\") # Task 1: Basic Plot Construction p1 = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\").add(so.Dot()) p1.show() # Task 2: Plot Customization p2 = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\").add(so.Dot(edgecolor=\\"w\\")) p2.show() # Task 3: Advanced Visualization p3 = ( so.Plot(tips, \\"total_bill\\", \\"day\\", color=\\"sex\\") .add(so.Dot(), so.Dodge(), so.Jitter(0.2)) ) p3.show() # Task 4: Error Bars p4 = ( so.Plot(tips, x=\\"total_bill\\", y=\\"day\\") .add(so.Dot(pointsize=3), so.Shift(y=0.2), so.Jitter(0.2)) .add(so.Dot(), so.Agg()) .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) ) p4.show() ``` In this coding question, demonstrate your ability to use seaborn to create and customize plots effectively. The expected outputs should be visualizations that are clear and informative, meeting each of the specified requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\") def basic_scatter_plot(): Creates a basic scatter plot of total_bill vs tip. return so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\").add(so.Dot()) def customized_scatter_plot(): Creates a customized scatter plot of total_bill vs tip with white-edged dots. return so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\").add(so.Dot(edgecolor=\\"w\\")) def advanced_scatter_plot(): Creates a scatter plot of total_bill vs day with dodge and jitter, colored by sex. return ( so.Plot(tips, \\"total_bill\\", \\"day\\", color=\\"sex\\") .add(so.Dot(), so.Dodge(), so.Jitter(0.2)) ) def errorbar_plot(): Shows the average total_bill per day with error bars representing standard errors. return ( so.Plot(tips, x=\\"total_bill\\", y=\\"day\\") .add(so.Dot(pointsize=3), so.Shift(y=0.2), so.Jitter(0.2)) .add(so.Dot(), so.Agg()) .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) )"},{"question":"You are required to write a Python script that processes log files in a given directory. Each log file follows the naming convention `log_YYYYMMDD.txt` and contains lines of text representing numerical readings (one per line). Your task includes: 1. **Reading the log files**: Identify log files from a specified directory (use `glob` module). 2. **Parsing the logs**: Read each log file, ignoring empty files and lines, and calculate the average of the numerical readings for each file. 3. **Summarizing results**: - Create a summary file named `summary.txt` that lists the date (extracted from filename) and the corresponding average reading for each log file in ascending order of the date. - Add an additional summary at the end that shows the overall average reading for all the log files processed. # Input and Output Format: - **Input**: - Directory path containing log files. - **Output**: - A summary text file named `summary.txt` within the same directory which contains: ``` Date: YYYY-MM-DD, Average: value ... Overall Average: value ``` # Constraints: - Assume no missing dates within the directory. - Handle edge cases where a log file may be empty or contain non-numeric values gracefully. - Use efficient file processing to manage memory usage. # Example: Given a directory with the following files and contents: ``` log_20230101.txt: 20 30 40 log_20230102.txt: 25 35 log_20230103.txt: 30 30 log_20230104.txt: # This file is empty ``` Your `summary.txt` should look like: ``` Date: 2023-01-01, Average: 30.0 Date: 2023-01-02, Average: 30.0 Date: 2023-01-03, Average: 30.0 Overall Average: 30.0 ``` Implement the following function in Python to achieve this: ```python import os import glob def process_log_files(directory_path: str): # Your code here pass ``` **Note**: Ensure that your code includes appropriate error handling and efficient file reading.","solution":"import os import glob def process_log_files(directory_path: str): log_files = glob.glob(os.path.join(directory_path, \'log_*.txt\')) results = [] overall_readings = [] for log_file in sorted(log_files): date_str = os.path.basename(log_file).split(\'_\')[1].split(\'.\')[0] date_formatted = f\\"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}\\" readings = [] with open(log_file, \'r\') as lf: for line in lf: try: # Attempt to convert line to float; ignore empty lines and non-numeric values reading = float(line.strip()) readings.append(reading) except ValueError: continue if readings: avg_reading = sum(readings) / len(readings) results.append((date_formatted, avg_reading)) overall_readings.extend(readings) with open(os.path.join(directory_path, \'summary.txt\'), \'w\') as summary_file: for date, avg in results: summary_file.write(f\\"Date: {date}, Average: {avg:.1f}n\\") if overall_readings: overall_avg = sum(overall_readings) / len(overall_readings) summary_file.write(f\\"Overall Average: {overall_avg:.1f}n\\")"},{"question":"Objective: Demonstrate your understanding and ability to work with the `http.HTTPStatus` enum class in Python. Task: Write a function in Python that takes an HTTP status code as an input and returns a dictionary containing the following details about the status code: - The numeric value (same as the input) - The enum name - The reason phrase - The long description Function Signature: ```python def get_http_status_details(status_code: int) -> dict: pass ``` Input: - `status_code` (int): An integer representing the HTTP status code (e.g., 200, 404). Output: - A dictionary with the following keys: - `value` (int): The numeric value of the status code. - `name` (str): The enum name corresponding to the status code. - `phrase` (str): The reason phrase for the status code. - `description` (str): The long description of the status code. Example: ```python result = get_http_status_details(200) # Expected output: # { # \'value\': 200, # \'name\': \'OK\', # \'phrase\': \'OK\', # \'description\': \'Request fulfilled, document follows\' # } result = get_http_status_details(404) # Expected output: # { # \'value\': 404, # \'name\': \'NOT_FOUND\', # \'phrase\': \'Not Found\', # \'description\': \'Nothing matches the given URI\' # } ``` Constraints: - If the provided `status_code` is not a valid HTTP status code in `http.HTTPStatus`, the function should raise a `ValueError` with a message `\\"Invalid HTTP status code\\"`. Notes: - You may assume all status codes provided will be integers. - Utilize the `http.HTTPStatus` enum class to retrieve the information. Good luck!","solution":"from http import HTTPStatus def get_http_status_details(status_code: int) -> dict: Takes an HTTP status code and returns its details including: - Numeric value - Enum name - Reason phrase - Long description try: status = HTTPStatus(status_code) return { \'value\': status.value, \'name\': status.name, \'phrase\': status.phrase, \'description\': status.description } except ValueError: raise ValueError(\\"Invalid HTTP status code\\")"},{"question":"You are required to write a Python function that interacts with system temporary storage in a safe and efficient manner using the `tempfile` module. This task will assess your understanding of file handling, context management, and cleanup mechanisms in Python. Problem Statement Write a function `manage_temp_storage` that performs the following operations: 1. Creates a temporary file using `NamedTemporaryFile`. 2. Writes a specified string data into the temporary file. 3. Reads back the data from the file and appends it to another file using `SpooledTemporaryFile`. 4. Ensures that all temporary files are properly closed and removed regardless of any exceptions during file operations. 5. The function should return the final content of the `SpooledTemporaryFile` for verification purposes. Function Signature ```python def manage_temp_storage(data: str) -> str: pass ``` Input - `data` (str): The string that needs to be written to the temporary file and then read back and appended to the spooled temporary file. Output - Returns the final content of the spooled temporary file which includes the appended data. Constraints - Do not use any disk I/O operations to handle final content external to temporary storage during the entire process. - Ensure that all temporary files and directories are cleaned up after operations, even in the case of an exception being thrown. - Use context managers to ensure exceptions are handled and resources are cleaned up correctly. Example ```python data = \\"Hello, World!\\" result = manage_temp_storage(data) print(result) # Should print \\"Hello, World!\\" ``` Notes - Utilize context managers (`with` statements) when working with temporary files and spooled temporary files. - Handle exceptions appropriately to ensure no temporary files remain after execution. - `NamedTemporaryFile` should have `delete=True` to ensure it is removed after closing. - `SpooledTemporaryFile` should handle data in binary mode (`w+b`), and conversion between strings and bytes should be handled appropriately.","solution":"import tempfile def manage_temp_storage(data: str) -> str: Manages temporary storage by writing data to a temporary file, reading it back and appending to a spooled temporary file. Ensures all temporary files are properly closed and removed. Parameters: - data: str : Data to write to the temporary file Returns: - str : Final content of the spooled temporary file try: # Create a NamedTemporaryFile and write data to it with tempfile.NamedTemporaryFile(delete=True, mode=\'w+t\') as temp_file: temp_file.write(data) temp_file.flush() # Flush data to ensure it\'s written # Go back to the start of the file to read its content temp_file.seek(0) read_data = temp_file.read() # Create a SpooledTemporaryFile to append the read data with tempfile.SpooledTemporaryFile(mode=\'w+t\') as spool_file: spool_file.write(read_data) spool_file.seek(0) final_content = spool_file.read() return final_content except Exception as e: raise e"},{"question":"# Question: Seaborn Advanced Bar Plot You are provided with two datasets from the Seaborn library: `penguins` and `flights`. Your task is to create customized bar plots as described below using these datasets. Steps 1. **Load the datasets** `penguins` and `flights` using Seaborn\'s `load_dataset` method. 2. **Pivot the `flights` dataset** to create a wide-form data frame, where the index should be the `year`, columns should be the `month`, and values should be the `passengers`. 3. **Create a bar plot** from the `penguins` dataset, grouped by the `island` categorical variable, and `body_mass_g` as the y-axis variable, with a second layer of grouping by the `sex` variable. Customize the plot to: - Display standard deviation instead of a confidence interval. - Add text labels showing the mean body mass for each group. 4. **Create another bar plot** from the `flights` dataset, using the pivoted wide-form data. Customize this plot to: - Orient the plot with `years` on the y-axis and sum of `passengers` for each month on the x-axis. - Add annotations for the value of `passengers` in June of the year 1955 (in red color). **Function Signature** ```python def seaborn_bar_plots(): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme(style=\\"whitegrid\\") # Load datasets penguins = sns.load_dataset(\\"penguins\\") flights = sns.load_dataset(\\"flights\\") # Pivot flights to wide-form data flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Create the first bar plot with penguins dataset ax1 = sns.barplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", errorbar=\\"sd\\") ax1.bar_label(ax1.containers[0], fontsize=10) plt.title(\'Body Mass of Penguins by Island and Sex\') plt.show() # Create the second bar plot with flights dataset ax2 = sns.barplot(data=flights_wide.T, orient=\\"h\\") ax2.annotate(\'1955 June\', xy=(flights_wide.loc[1955, \\"Jun\\"], 1955), xytext=(flights_wide.loc[1955, \\"Jun\\"]+100, 1955), arrowprops=dict(facecolor=\'red\', shrink=0.05), color=\'red\') ax2.set_xlabel(\'Sum of Passengers\') ax2.set_ylabel(\'Years\') plt.title(\'Passengers by Year and Month\') plt.show() ``` **Constraints** - Ensure that all required packages (Seaborn and Matplotlib) are properly imported. - The error bars for the first plot should use the standard deviation. - The second plot should be oriented correctly with years on the y-axis. **Expected Output** 1. Display two bar plots with the specified customizations. 2. The first plot should show the mean body mass of penguins grouped by island and sex with labeled bars. 3. The second plot should show the sum of passengers per year with an annotation for June 1955 in red color.","solution":"import seaborn as sns import matplotlib.pyplot as plt def seaborn_bar_plots(): sns.set_theme(style=\\"whitegrid\\") # Load datasets penguins = sns.load_dataset(\\"penguins\\") flights = sns.load_dataset(\\"flights\\") # Pivot flights to wide-form data flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Create the first bar plot with the penguins dataset ax1 = sns.barplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", ci=\\"sd\\") for container in ax1.containers: ax1.bar_label(container, fmt=\'%.1f\', label_type=\'edge\') plt.title(\'Body Mass of Penguins by Island and Sex\') plt.show() # Create the second bar plot with the flights dataset flights_wide_sum = flights_wide.sum(axis=1).reset_index() ax2 = sns.barplot(data=flights_wide_sum, y=\\"year\\", x=0, orient=\\"h\\") ax2.annotate(\'1955 June\', xy=(flights_wide.loc[1955, \\"Jun\\"], 1955), xytext=(flights_wide.loc[1955, \\"Jun\\"] + 100, 1955), arrowprops=dict(facecolor=\'red\', shrink=0.05), color=\'red\') ax2.set_xlabel(\'Sum of Passengers\') ax2.set_ylabel(\'Year\') plt.title(\'Total Number of Passengers per Year\') plt.show()"},{"question":"Custom Neural Network Module with TorchScript # Background TorchScript allows you to convert Python code into a statically-typed format that can be optimized and run independently from Python. In this task, you will use TorchScript to create a custom neural network model. This model should demonstrate your understanding of static typing, the use of builtin operations, and adherence to TorchScript coding constraints. # Task Description Create a custom neural network class `CustomNetwork` using PyTorch and TorchScript. The network should: 1. **Attributes**: - Have a single linear layer. - Have a scalar attribute to track the total number of positive outcomes in predictions. 2. **Methods**: - A custom method `increase_positive_count` to increment the count of positive outcomes. - Implement the `forward` method to define the model\'s forward pass: - A single linear transformation should be applied to the input. - Implement a conditional check to update the positive outcome count if the transformed value is greater than zero. # Requirements 1. **Types**: - Use type annotations to specify the types of all attributes and method parameters. 2. **TorchScript**: - The class and all methods should be decorated appropriately to be TorchScript-compatible. 3. **Handling of Uninitialized Values**: - Use TorchScript\'s type annotations to handle any optional or uninitialized attributes. 4. **Performance**: - Ensure the forward pass performs efficiently without unnecessary computations or conversions. # Input and Output - **Input**: The model will accept a tensor of shape `(N, M)` where `N` is the batch size and `M` is the number of features. - **Output**: The output should be the transformed tensor after the linear layer and the updated positive count should reflect in the attribute. # Constraints 1. Do not use any unsupported typing constructs in TorchScript. 2. Ensure your model is TorchScript-exportable (i.e., can be compiled without any runtime errors). 3. The network class should be inheritable from `nn.Module`. # Example Usage ```python import torch import torch.nn as nn from typing import Optional @torch.jit.script class CustomNetwork(nn.Module): def __init__(self, input_dim: int, output_dim: int): super().__init__() self.linear = nn.Linear(input_dim, output_dim) self.positive_outcome_count: torch.jit.Final[int] = 0 def increase_positive_count(self): self.positive_outcome_count += 1 def forward(self, input: torch.Tensor) -> torch.Tensor: output = self.linear(input) if torch.sum(output) > 0: self.increase_positive_count() return output # Usage input_dim = 5 output_dim = 1 model = CustomNetwork(input_dim, output_dim) input_tensor = torch.randn(10, input_dim) output_tensor = model(input_tensor) print(\\"Positive Outcome Count:\\", model.positive_outcome_count) ``` Note: The initial positive outcome count should be zero, and it should be appropriately updated within each forward pass if the condition is met. Write the complete code to define the `CustomNetwork` class as described.","solution":"import torch import torch.nn as nn class CustomNetwork(nn.Module): def __init__(self, input_dim: int, output_dim: int): super().__init__() self.linear = nn.Linear(input_dim, output_dim) self.register_buffer(\\"positive_outcome_count\\", torch.tensor(0)) def increase_positive_count(self): self.positive_outcome_count += 1 def forward(self, input: torch.Tensor) -> torch.Tensor: output = self.linear(input) if torch.sum(output) > 0: self.increase_positive_count() return output # Usage example input_dim = 5 output_dim = 1 model = CustomNetwork(input_dim, output_dim) input_tensor = torch.randn(10, input_dim) output_tensor = model(input_tensor) print(\\"Positive Outcome Count:\\", model.positive_outcome_count.item())"},{"question":"You are tasked with writing a Python program that processes and categorizes emails stored in a Maildir format. The goal is to sort emails into respective subfolders based on the email subjects. The three subfolders will be categorized as \\"urgent\\", \\"normal\\", and \\"spam\\". # Requirements: 1. Implement a function `process_maildir(maildir_path: str) -> None` that: - Initializes and accesses the given Maildir mailbox located at `maildir_path`. - Reads each email in the mailbox and sorts it based on the subject: - If the subject contains the word \\"urgent\\", the email should be moved to the \\"urgent\\" subfolder. - If the subject contains the word \\"normal\\", the email should be moved to the \\"normal\\" subfolder. - If the subject contains the word \\"spam\\", the email should be moved to the \\"spam\\" subfolder. - If a target subfolder does not exist, create it. - Ensure operation safety by handling concurrent access and potential mail corruption. - Deletes the email from the original mailbox once it has been moved. 2. Use the appropriate locking mechanisms to ensure no data corruption occurs when accessing the mailbox concurrently. # Constraints: - The function should properly handle mailboxes with potentially malformed messages. - Use the `Maildir` and `MaildirMessage` classes from the `mailbox` module. - Do not lose any emails during processing—ensure messages are fully moved before deleting them from the original folder. - Assume the Maildir paths follow standard conventions with `tmp`, `new`, and `cur` subdirectories for the main mailbox and its subfolders. # Input: - `maildir_path`: A string representing the path to the Maildir mailbox. # Example: ```python process_maildir(\\"/path/to/maildir\\") ``` # Notes: - Be cautious of concurrent modifications and ensure the mailbox is safely locked during any modifications. - Consider extending the imports and any required setup within your solution.","solution":"import os import mailbox from mailbox import Maildir, MaildirMessage def process_maildir(maildir_path): urgent_folder = \'urgent\' normal_folder = \'normal\' spam_folder = \'spam\' maildir = Maildir(maildir_path, factory=None, create=False) subfolders = {urgent_folder: maildir.add_folder(urgent_folder), normal_folder: maildir.add_folder(normal_folder), spam_folder: maildir.add_folder(spam_folder)} def move_to_subfolder(mail): subject = mail[\'subject\'] if subject is None: return # Skip messages without subject if \'urgent\' in subject.lower(): sub_folder = urgent_folder elif \'normal\' in subject.lower(): sub_folder = normal_folder elif \'spam\' in subject.lower(): sub_folder = spam_folder else: return # Skip messages that do not match any category subfolders[sub_folder].add(mail) del maildir[mail.key] maildir.lock() try: for key, message in maildir.iteritems(): try: move_to_subfolder(message) except Exception as e: print(f\\"Error processing message {key}: {e}\\") finally: maildir.close()"},{"question":"Mailbox Message Transfer and Sorting Objective: Demonstrate proficiency in managing email messages using the \\"mailbox\\" module by writing a function to transfer messages from an mbox format mailbox to a Maildir format mailbox, while sorting them based on a specific header. Task: Write a Python function called `transfer_and_sort_messages` that takes two arguments: - `mbox_path` (string): Path to the source mbox format mailbox file. - `maildir_path` (string): Path to the destination Maildir format mailbox directory. The function should: 1. Read all messages from the mbox mailbox located at `mbox_path`. 2. Sort the messages based on the \\"From\\" header (sender\'s email address) in alphabetical order. 3. Transfer these sorted messages to the Maildir mailbox located at `maildir_path`. 4. Properly handle potential errors, including malformed messages and concurrent modifications. 5. Ensure that no messages are lost during the transfer process by utilizing proper locking mechanisms. Function Signature: ```python def transfer_and_sort_messages(mbox_path: str, maildir_path: str) -> None: pass ``` Constraints: - You may assume that the `mbox_path` exists and contains valid email messages in mbox format. - The `maildir_path` should be created if it does not already exist. - Consider the efficiency of your implementation; however, focus primarily on correctness and data integrity. Example Usage: ```python mbox_path = \'path/to/source/mbox\' maildir_path = \'path/to/destination/maildir\' transfer_and_sort_messages(mbox_path, maildir_path) ``` Expectations: - Properly use the \\"mailbox\\" module to achieve the task. - Implement sorting based on the \\"From\\" header. - Use locking mechanisms to prevent data corruption. - Handle exceptions gracefully, ensuring that the mailbox operations are safe and reliable. Notes: - You may use helper functions if necessary. - Include comments explaining key parts of your implementation. - Ensure the `maildir_path` is a valid Maildir structure as per the specifications in the documentation.","solution":"import mailbox import os import email def transfer_and_sort_messages(mbox_path, maildir_path): Transfers messages from a mbox format mailbox to a Maildir format mailbox, sorting by the \\"From\\" header. Args: mbox_path (str): Path to the source mbox format mailbox file. maildir_path (str): Path to the destination Maildir format mailbox directory. # Create the target Maildir directory if it doesn\'t exist if not os.path.exists(maildir_path): os.makedirs(maildir_path) os.makedirs(os.path.join(maildir_path, \'cur\')) os.makedirs(os.path.join(maildir_path, \'new\')) os.makedirs(os.path.join(maildir_path, \'tmp\')) # Open the mbox file mbox = mailbox.mbox(mbox_path) # Use a mailbox.Maildir instance for the destination Maildir maildir = mailbox.Maildir(maildir_path) # Create a list of tuples (message, from_address) messages = [(message, message[\'From\']) for message in mbox] # Sort messages by the \\"From\\" header, ignoring case sorted_messages = sorted(messages, key=lambda x: (x[1] or \'\').lower()) # Transfer each sorted message to the Maildir for message, _ in sorted_messages: maildir.add(message) # Close the mbox and maildir mbox.close() maildir.close()"},{"question":"# Cryptographic Hashing and Secure Token Generation Objective This question is designed to assess your understanding of cryptographic hash functions, secure token generation, and managing secrets using Python\'s cryptographic services. You will implement functions to create secure hashes and generate secure tokens. Problem Statement 1. **Secure Hash Function:** Implement a function `compute_secure_hash(data: str, salt: str) -> str` that returns the BLAKE2b hash of the given data concatenated with the salt. The resulting hash should be encoded in hexadecimal format. **Constraints:** - The input `data` is a non-empty string. - The input `salt` is a non-empty string. - Use the `hashlib` module to create the BLAKE2b hash with a digest size of 64 bytes. 2. **Secure Token Generation:** Implement a function `generate_secure_token(length: int) -> str` that generates a secure random token of the specified length using the `secrets` module. **Constraints:** - The input `length` is an integer greater than 0. Input and Output - `compute_secure_hash(data: str, salt: str) -> str`: - **Input:** Two strings, `data` and `salt`. - **Output:** A hexadecimal string representing the BLAKE2b hash of the concatenated input strings. - `generate_secure_token(length: int) -> str`: - **Input:** An integer representing the length of the token. - **Output:** A secure random token with the specified length. Example ```python # Example for compute_secure_hash data = \\"sensitive_information\\" salt = \\"unique_salt_value\\" print(compute_secure_hash(data, salt)) # Output: A unique hexadecimal hash string # Example for generate_secure_token length = 16 print(generate_secure_token(length)) # Output: A secure random token of 16 characters length ``` Notes - You should use the `hashlib.blake2b` function to create the hash with a digest size of 64 bytes. - Use the `secrets.token_hex` function to generate the secure random token in hexadecimal format.","solution":"import hashlib import secrets def compute_secure_hash(data: str, salt: str) -> str: Computes a secure BLAKE2b hash of the input data concatenated with the salt. Args: - data: A non-empty string containing the data to be hashed. - salt: A non-empty string containing the salt to be added to the data. Returns: - A hexadecimal string representing the BLAKE2b hash of the concatenated data and salt. if not data or not salt: raise ValueError(\\"Data and salt must be non-empty strings\\") # Concatenate data and salt combined = data + salt # Create a BLAKE2b hash with a digest size of 64 bytes blake2b_hash = hashlib.blake2b(combined.encode(\'utf-8\'), digest_size=64) # Return the hexadecimal representation of the hash return blake2b_hash.hexdigest() def generate_secure_token(length: int) -> str: Generates a secure random token of the specified length. Args: - length: An integer greater than 0 representing the length of the token. Returns: - A secure random token as a hexadecimal string. if length <= 0: raise ValueError(\\"Length must be greater than 0\\") # Generate a secure random token in hexadecimal format return secrets.token_hex(length)"},{"question":"# URL Custom Parser and Constructor using `urllib.parse` Objective: Implement a function `custom_url_parse` that takes a URL string and breaks it into its components using `urllib.parse.urlparse`. Additionally, implement a function `construct_query_string` that constructs a query string from a dictionary using `urllib.parse.urlencode`. # Task: 1. **Function 1: custom_url_parse** - **Input**: A string representing a URL. - **Output**: A dictionary with the URL components as keys (`scheme`, `netloc`, `path`, `params`, `query`, `fragment`). - **Example**: ```python url = \\"http://docs.python.org:80/3/library/urllib.parse.html?highlight=urlparse#documentation\\" output = { \\"scheme\\": \\"http\\", \\"netloc\\": \\"docs.python.org:80\\", \\"path\\": \\"/3/library/urllib.parse.html\\", \\"params\\": \\"\\", \\"query\\": \\"highlight=urlparse\\", \\"fragment\\": \\"documentation\\" } ``` 2. **Function 2: construct_query_string** - **Input**: A dictionary representing query parameters. - **Output**: A valid URL query string. - **Example**: ```python query_params = { \\"search\\": \\"python\\", \\"page\\": \\"2\\", \\"filter\\": \\"recent\\" } output = \\"search=python&page=2&filter=recent\\" ``` # Constraints: - You must use the functions from the `urllib.parse` module. - Handle percent-encoding for special characters where necessary when constructing query strings. - Non-ASCII characters should be handled appropriately. # Performance Requirements: - The functions should perform efficiently, handling typical URL lengths and query string sizes without significant delay. # Implementation: ```python from urllib.parse import urlparse, urlencode def custom_url_parse(url): parsed_result = urlparse(url) return { \\"scheme\\": parsed_result.scheme, \\"netloc\\": parsed_result.netloc, \\"path\\": parsed_result.path, \\"params\\": parsed_result.params, \\"query\\": parsed_result.query, \\"fragment\\": parsed_result.fragment } def construct_query_string(query_params): return urlencode(query_params) # Example Usage if __name__ == \\"__main__\\": url = \\"http://docs.python.org:80/3/library/urllib.parse.html?highlight=urlparse#documentation\\" print(custom_url_parse(url)) query_params = { \\"search\\": \\"python\\", \\"page\\": \\"2\\", \\"filter\\": \\"recent\\" } print(construct_query_string(query_params)) ``` # Testing: Write test cases to verify the functionality of `custom_url_parse` and `construct_query_string`.","solution":"from urllib.parse import urlparse, urlencode def custom_url_parse(url): Parses a URL and returns its components in a dictionary. Args: url (str): The URL to parse. Returns: dict: A dictionary with the keys \'scheme\', \'netloc\', \'path\', \'params\', \'query\', and \'fragment\'. parsed_result = urlparse(url) return { \\"scheme\\": parsed_result.scheme, \\"netloc\\": parsed_result.netloc, \\"path\\": parsed_result.path, \\"params\\": parsed_result.params, \\"query\\": parsed_result.query, \\"fragment\\": parsed_result.fragment } def construct_query_string(query_params): Constructs a query string from a dictionary of parameters. Args: query_params (dict): The dictionary containing query parameters. Returns: str: The encoded query string. return urlencode(query_params)"},{"question":"Coding Assessment Question # Objective You are required to analyze a dataset using multiple covariance estimation techniques provided by scikit-learn, then compare and interpret their effectiveness. # Instructions 1. Implement a function `compare_covariance_estimators` that takes a dataset and returns the results of various covariance estimators. 2. You should use the following estimators: - Empirical Covariance - Shrunk Covariance with a given shrinkage parameter (alpha) - Ledoit-Wolf Shrinkage - Oracle Approximating Shrinkage (OAS) - Sparse Inverse Covariance with a given alpha parameter 3. Compare the estimators based on: - Computed covariance matrices - Precision matrices - Robustness to outliers (if applicable) # Function Signature ```python def compare_covariance_estimators(data: np.ndarray, shrinkage_alpha: float, sparse_alpha: float): Parameters: - data (np.ndarray): 2D array with shape (n_samples, n_features) - shrinkage_alpha (float): Shrinkage parameter for Shrunk Covariance - sparse_alpha (float): Alpha parameter for Sparse Inverse Covariance Returns: - dict: A dictionary with keys as the estimator names and values as a tuple of (covariance matrix, precision matrix) pass ``` # Constraints - You must demonstrate the implementation using a sample dataset. You can generate or use an existing dataset with at least 100 samples and 10 features. - Ensure proper handling of potential issues like non-invertible matrices. - The function should not print but return the results properly. # Example Usage ```python import numpy as np from numpy.random import default_rng # Generating a sample dataset rng = default_rng(seed=42) data = rng.standard_normal((150, 10)) # Adjusted alphas for example shrink_alpha = 0.1 sparse_alpha = 0.01 # Comparing covariance estimators results = compare_covariance_estimators(data, shrink_alpha, sparse_alpha) for key, (cov, prec) in results.items(): print(f\\"{key} Covariance Matrix:n\\", cov) print(f\\"{key} Precision Matrix:n\\", prec) ``` # Notes - Use the appropriate classes and functions from `sklearn.covariance` for each estimation method. - Ensure that your code is well-structured and commented to explain the key steps and decisions. Your task is to write the function `compare_covariance_estimators` as per the specifications above. This will assess your ability to utilize, implement, and compare advanced scikit-learn functionalities.","solution":"import numpy as np from sklearn.covariance import (EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, GraphicalLasso) def compare_covariance_estimators(data: np.ndarray, shrinkage_alpha: float, sparse_alpha: float): Compare various covariance estimators. Parameters: - data (np.ndarray): 2D array with shape (n_samples, n_features) - shrinkage_alpha (float): Shrinkage parameter for Shrunk Covariance - sparse_alpha (float): Alpha parameter for Sparse Inverse Covariance Returns: - dict: Dictionary with estimator names as keys and tuples of (covariance matrix, precision matrix) as values results = {} # Empirical Covariance emp_cov = EmpiricalCovariance().fit(data) results[\'Empirical Covariance\'] = (emp_cov.covariance_, emp_cov.precision_) # Shrunk Covariance shrunk_cov = ShrunkCovariance(shrinkage=shrinkage_alpha).fit(data) results[\'Shrunk Covariance\'] = (shrunk_cov.covariance_, shrunk_cov.precision_) # Ledoit-Wolf Shrinkage lw_cov = LedoitWolf().fit(data) results[\'Ledoit-Wolf\'] = (lw_cov.covariance_, lw_cov.precision_) # Oracle Approximating Shrinkage (OAS) oas_cov = OAS().fit(data) results[\'OAS\'] = (oas_cov.covariance_, oas_cov.precision_) # Sparse Inverse Covariance sparse_cov = GraphicalLasso(alpha=sparse_alpha).fit(data) results[\'Sparse Inverse Covariance\'] = (sparse_cov.covariance_, sparse_cov.precision_) return results"},{"question":"Objective: You need to demonstrate your understanding of CUDA stream synchronization in PyTorch and the usage of the CUDA Stream Sanitizer. Problem Statement: You are given a tensor operation that is vulnerable to data races due to improper synchronization between CUDA streams. Your task is to: 1. Identify the issue using the CUDA Stream Sanitizer. 2. Correct the code to ensure proper synchronization. Task: 1. Create a PyTorch script that simulates the following scenario: - Initialize a tensor `a` with random values on the default CUDA stream. - Use a non-default CUDA stream to modify the tensor `a` by scaling its values. 2. Run the script to identify possible data race issues using the CUDA Stream Sanitizer. 3. Fix the identified synchronization issues and ensure the script runs without errors using proper stream synchronization techniques. # Requirements: - Your script should contain the usage of both default and non-default CUDA streams. - Properly enable the CUDA Stream Sanitizer to detect issues. - Implement synchronization techniques correctly to avoid data races. - Ensure your final script runs without any errors reported by the CUDA Stream Sanitizer. Input: * No input is required from the user. The operations should be hardcoded in the script as described above. Output: * The corrected PyTorch script should run without errors when tested with the CUDA Stream Sanitizer enabled. Constraints: * Ensure that the CUDA toolkit and PyTorch are properly installed and configured on your system. * Use Python as the programming language. # Example scenario ```python # This part of the script introduces a synchronization issue import torch a = torch.rand(4, 2, device=\\"cuda\\") with torch.cuda.stream(torch.cuda.Stream()): torch.mul(a, 5, out=a) # Fix the synchronization issue in your final script ``` Submission: Submit a single Python script file (.py) with the following: - The code with the initial synchronization issue. - The corrected code that resolves the data race using proper synchronization.","solution":"import torch # Initial script with a potential synchronization issue a = torch.rand(4, 2, device=\\"cuda\\") stream = torch.cuda.Stream() # Non-default CUDA stream for modifying tensor \'a\' with torch.cuda.stream(stream): torch.mul(a, 5, out=a) # Fix the synchronization issue stream.synchronize() # Ensure tensor \'a\' is ready for the default stream by synchronizing the non-default stream first print(a)"},{"question":"# Advanced Coding Assessment: Custom Buffered In-Memory Text Modification **Objective:** Implement a custom buffered text editor that can read from and write to an in-memory text buffer. The editor should provide basic text manipulation functionalities, such as reading sections of text, modifying text, and encoding handled transparently. **Problem Statement:** You are tasked to create a custom buffered text editor class using Python\'s `io` module. This editor should encapsulate an in-memory text buffer providing functions to read, modify, and save text. The editor should balance efficient memory usage while also providing high performance for text reading and writing operations. # Requirements: 1. **Class Definition:** Define a class `BufferedTextEditor` which extends `io.TextIOWrapper`. 2. **Initialization:** - The class should be initialized with an optional initial string text and specific encoding. - Default encoding should be `utf-8`. 3. **Methods:** - `read_text(size: int = -1) -> str`: Read and return up to `size` characters from the text buffer. If `size` is -1, read the entire buffer. - `write_text(content: str) -> int`: Write the given string `content` to the buffer and return the number of characters written. - `modify_text(start: int, end: int, replacement: str) -> None`: Replace the text in the buffer from position `start` to `end` with the `replacement` string. - `save_to_file(file_path: str) -> None`: Save the buffer content to a file with the specified file path. - `load_from_file(file_path: str) -> None`: Load content from a specified file path into the buffer, replacing any existing content. 4. **Encoding Handling:** - Ensure all text operations respect the encoding used by the buffer. - Handle encoding errors gracefully by using `strict` mode. 5. **Usage Constraints:** - Provide suitable error handling for operations such as attempting to modify text outside the current buffer bounds. - Ensure that the text editor maintains performance efficiency appropriate for large texts edits. # Example Usage: ```python # Initialize the editor with some text editor = BufferedTextEditor(\\"Hello, World!\\", encoding=\\"utf-8\\") # Read the current text print(editor.read_text()) # Output: Hello, World! # Modify the text editor.modify_text(7, 12, \\"Universe\\") # Read the modified text print(editor.read_text()) # Output: Hello, Universe! # Write additional text editor.write_text(\\" How are you?\\") # Save the current buffer to a file editor.save_to_file(\\"output.txt\\") # Load new content from a file editor.load_from_file(\\"input.txt\\") print(editor.read_text()) # Output: Content of input.txt ``` # Notes: - Your class should handle file operations safely and ensure the buffer is closed when not in use. - The `modify_text` method should raise an appropriate exception if `start` or `end` indices are out of bounds. # Constraints: - The buffer should be able to handle text sizes up to 100 MB efficiently. - Maintain high performance even for sequential read and write operations. # Submission: Submit the implementation of `BufferedTextEditor` class with the specified functionalities. Ensure your implementation passes the usage example provided.","solution":"import io class BufferedTextEditor(io.TextIOWrapper): def __init__(self, initial_text: str = \\"\\", encoding: str = \\"utf-8\\"): self.stream = io.BytesIO(initial_text.encode(encoding)) super().__init__(self.stream, encoding=encoding) def read_text(self, size: int = -1) -> str: self.seek(0) return self.read(size) def write_text(self, content: str) -> int: self.seek(0, io.SEEK_END) written = self.write(content) self.seek(0) return written def modify_text(self, start: int, end: int, replacement: str) -> None: if start < 0 or end < 0 or start > end: raise ValueError(\\"Invalid start or end indices.\\") self.seek(0) current_content = self.read_text() if start > len(current_content) or end > len(current_content): raise IndexError(\\"Start or end index out of bounds.\\") new_content = current_content[:start] + replacement + current_content[end:] self.stream = io.BytesIO(new_content.encode(self.encoding)) self.__init__(new_content, encoding=self.encoding) def save_to_file(self, file_path: str) -> None: with open(file_path, \'w\', encoding=self.encoding) as f: self.seek(0) f.write(self.read_text()) def load_from_file(self, file_path: str) -> None: with open(file_path, \'r\', encoding=self.encoding) as f: content = f.read() self.stream = io.BytesIO(content.encode(self.encoding)) self.__init__(content, encoding=self.encoding)"},{"question":"<|Analysis Begin|> The provided documentation gives a detailed overview of the `xml.sax.handler` module in Python. This module defines the base classes required for SAX (Simple API for XML) event-driven parsing handlers including `ContentHandler`, `DTDHandler`, `EntityResolver`, `ErrorHandler`, and `LexicalHandler`. These handlers deal with different parts of the XML parsing process: 1. **ContentHandler**: Manages the core XML content. 2. **DTDHandler**: Handles DTD events. 3. **EntityResolver**: Resolves XML entities. 4. **ErrorHandler**: Manages parsing errors and warnings. 5. **LexicalHandler**: Deals with low-frequency lexical events. Within `ContentHandler`, several methods allow fine-grained control over XML parsing events such as `startDocument`, `endDocument`, `startElement`, `endElement`, `characters`, etc. The constants provided in the module allow configuration of XML parsing behavior, such as enabling/disabling namespace processing, validation, and handling of external entities. **Key Points**: - Implementation of SAX handlers requires subclassing the provided base classes to override desired methods. - SAX parsing is event-driven and provides a callback mechanism for each significant event during XML parsing. - Methods like `startElement`, `endElement`, `characters`, etc., are core to processing and navigating an XML document structure. **Potential Question Components**: - Testing students\' understanding of subclassing and implementing callback methods. - Handling specific XML events like element start/end, character data, and errors. - Managing namespaces and validation. - Constructing a small SAX parser with a specific goal (e.g., counting elements, extracting text, etc.). <|Analysis End|> <|Question Begin|> # SAX XML Parsing with Custom Handlers Your task is to implement a custom `ContentHandler` for parsing a given XML document using the SAX parser in Python. The handler should be able to count the number of specific XML elements, extract text from predefined elements, and handle simple error reporting. Requirements 1. **Element Counting**: - Count the number of `<item>` elements in the XML. 2. **Text Extraction**: - Extract and print the text content of `<title>` elements. 3. **Error Handling**: - Print a warning for a skipped entity. Input The input will be an XML document, which is provided as a string. Output 1. The total count of `<item>` elements. 2. The text content of all `<title>` elements. 3. Any warnings related to skipped entities. Constraints - Assume the XML structure is well-formed. - The documents will be relatively small (less than 10MB). Performance Requirements Your solution should efficiently process the given XML document without additional overhead. # Example Given the following XML document: ```xml <catalog> <item> <title>Programming in Python</title> </item> <item> <title>Learning SAX</title> </item> </catalog> ``` Your implementation should output: ``` Number of <item> elements: 2 <title> contents: Programming in Python Learning SAX ``` # Implementation Implement the solution by completing the `CustomContentHandler` class as shown below: ```python import xml.sax class CustomContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.item_count = 0 self.in_title = False self.titles = [] def startElement(self, name, attrs): if name == \\"item\\": self.item_count += 1 elif name == \\"title\\": self.in_title = True def characters(self, content): if self.in_title: self.titles.append(content) def endElement(self, name): if name == \\"title\\": self.in_title = False def skippedEntity(self, name): print(f\\"Warning: Skipped entity {name}\\") def parse_xml(xml_string): handler = CustomContentHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) xml.sax.parseString(xml_string, handler) print(f\\"Number of <item> elements: {handler.item_count}\\") print(\\"<title> contents: \\") for title in handler.titles: print(title) # Example usage xml_string = <catalog> <item> <title>Programming in Python</title> </item> <item> <title>Learning SAX</title> </item> </catalog> parse_xml(xml_string) ``` Ensure your code is well-commented and adheres to good programming practices.","solution":"import xml.sax class CustomContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.item_count = 0 self.in_title = False self.titles = [] def startElement(self, name, attrs): if name == \\"item\\": self.item_count += 1 elif name == \\"title\\": self.in_title = True def characters(self, content): if self.in_title: self.titles.append(content) def endElement(self, name): if name == \\"title\\": self.in_title = False def skippedEntity(self, name): print(f\\"Warning: Skipped entity {name}\\") def parse_xml(xml_string): handler = CustomContentHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) xml.sax.parseString(xml_string, handler) return handler.item_count, handler.titles"},{"question":"**Task: Creating Comprehensive Unit Tests** In this task, you are required to write comprehensive unit tests for a function. The function and relevant data structure are provided below. You must use the `unittest` module along with utilities from `test.support` to ensure your tests are robust and thorough. # Function to Test ```python def is_prime(n): Check if a number is prime. if n <= 1: return False for i in range(2, int(n**0.5) + 1): if n % i == 0: return False return True ``` # Requirements 1. **Test the `is_prime` function**: - Verify it correctly identifies prime numbers. - Test edge cases, such as negative numbers, zero, and one. - Test larger prime numbers and composite numbers. 2. **Use `unittest` framework**: - Create a test class named `TestIsPrime` inheriting from `unittest.TestCase`. - Write at least three test methods with names starting with `test_` for different test scenarios. 3. **Leverage `test.support` utilities**: - Use `test.support.run_unittest` to execute your test cases. - Use `test.support.captured_stdout` to check if any output, if expected, matches. - Use `test.support.swap_attr` to temporarily modify attributes during testing if necessary. - Use decorators like `@test.support.requires_docstrings` as applicable. # Additional Constraints - Do not include documentation strings for test methods, use comments instead. - Ensure tests clean up after themselves, e.g., by closing files or connections. - Testing code must be efficient and should execute on typical system configurations. # Test Example Below is an example of how your test setup should look: ```python import unittest from test import support def is_prime(n): Check if a number is prime. if n <= 1: return False for i in range(2, int(n**0.5) + 1): if n % i == 0: return False return True class TestIsPrime(unittest.TestCase): def test_prime_numbers(self): # Tests if prime check works for prime numbers self.assertTrue(is_prime(2)) self.assertTrue(is_prime(3)) self.assertTrue(is_prime(5)) self.assertTrue(is_prime(7)) def test_non_prime_numbers(self): # Tests if prime check works for non-prime numbers self.assertFalse(is_prime(1)) self.assertFalse(is_prime(4)) self.assertFalse(is_prime(6)) self.assertFalse(is_prime(8)) def test_negative_numbers_and_zero(self): # Tests if prime check works for negative numbers and zero self.assertFalse(is_prime(-1)) self.assertFalse(is_prime(0)) if __name__ == \'__main__\': support.run_unittest(TestIsPrime) ``` Ensure you cover all aspects detailed above and structure your code cleanly to allow for readability and maintainability.","solution":"def is_prime(n): Check if a number is prime. if n <= 1: return False for i in range(2, int(n**0.5) + 1): if n % i == 0: return False return True"},{"question":"# Question: You have been provided with a dataset containing various columns, each representing different types of data (e.g., datetime, nullable integers, categorical data, etc.). Your task is to implement a function that processes this dataset to perform specific operations on different types of data. Dataset Description: Consider a pandas DataFrame `df` with the following columns: 1. `date`: A column with datetime data that can be timezone-naive or timezone-aware. 2. `start`: A column representing the start of an interval. 3. `end`: A column representing the end of an interval. 4. `category`: A column with categorical data. 5. `nullable_int`: A column with nullable integer data. 6. `value`: A column with float data, some of which may be missing. 7. `sparse_data`: A column with sparse data. Tasks: 1. **Datetime**: Convert the `date` column to a timezone-aware datetime if it is not already, using the timezone `US/Eastern`. 2. **Interval**: Create a new column `interval` in the dataframe that represents intervals using the `start` and `end` columns. 3. **Categorical**: Ensure the `category` column is an ordered categorical type with categories given by the unique values in the column, sorted alphabetically. 4. **Nullable Integer**: Fill any missing values in the `nullable_int` column with zero. 5. **Float**: Fill any missing values in the `value` column with the mean of the non-missing values. 6. **Sparse Data**: Convert the `sparse_data` column to a `SparseArray` for memory efficiency. Constraints: - The function should handle potential edge cases, such as already timezone-aware datetimes, intervals where `start` is greater than `end`, and entirely missing columns. - Performance is important; the function should be optimized to handle large datasets efficiently. Function Signature: ```python import pandas as pd def process_dataframe(df: pd.DataFrame) -> pd.DataFrame: Processes the dataframe according to the specified operations. Parameters: df (pd.DataFrame): Input dataframe with specified columns. Returns: pd.DataFrame: Processed dataframe. ``` Example: Given the following input dataframe `df`: ``` date start end category nullable_int value sparse_data 0 2023-01-01 12:00:00 2023-01-01 2023-01-02 apple 5 10.1 1.0 1 2023-01-02 15:00:00 2023-01-02 2023-01-03 banana NaN NaN NaN 2 2023-01-03 09:00:00 2023-01-03 2023-01-04 cherry 8 4.3 0.0 ``` Your function should return the following dataframe: ``` date start end category nullable_int value sparse_data interval 0 2023-01-01 07:00:00-05:00 2023-01-01 2023-01-02 apple 5 10.1 1.0 Interval(\'2023-01-01\', \'2023-01-02\', closed=\'right\') 1 2023-01-02 10:00:00-05:00 2023-01-02 2023-01-03 banana 0 7.2 NaN Interval(\'2023-01-02\', \'2023-01-03\', closed=\'right\') 2 2023-01-03 04:00:00-05:00 2023-01-03 2023-01-04 cherry 8 4.3 0.0 Interval(\'2023-01-03\', \'2023-01-04\', closed=\'right\') ``` (Note: the `date` column now reflects the US/Eastern time zone, the `value` column has missing values filled with the mean, `nullable_int` has missing values filled with zero, etc.)","solution":"import pandas as pd import numpy as np from pandas.api.types import CategoricalDtype def process_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Ensure datetime is timezone-aware with \'US/Eastern\' if df[\'date\'].dtype == \'datetime64[ns, UTC]\': df[\'date\'] = df[\'date\'].dt.tz_convert(\'US/Eastern\') elif df[\'date\'].dtype == \'datetime64[ns]\': df[\'date\'] = df[\'date\'].dt.tz_localize(\'UTC\').dt.tz_convert(\'US/Eastern\') # Create interval column df[\'interval\'] = pd.IntervalIndex.from_arrays(df[\'start\'], df[\'end\'], closed=\'right\') # Ensure category is an ordered categorical type cat_type = CategoricalDtype(categories=sorted(df[\'category\'].unique()), ordered=True) df[\'category\'] = df[\'category\'].astype(cat_type) # Fill missing values in nullable_int with zero df[\'nullable_int\'] = df[\'nullable_int\'].fillna(0).astype(int) # Fill missing values in value column with mean mean_value = df[\'value\'].mean() df[\'value\'] = df[\'value\'].fillna(mean_value) # Convert sparse_data to SparseArray df[\'sparse_data\'] = pd.arrays.SparseArray(df[\'sparse_data\'].values) return df"},{"question":"Advanced Mask Manipulation in Attention Mechanisms One of the core aspects of attention mechanisms in neural networks is the ability to create and manipulate masks to control how attention is distributed across input sequences. In this task, you are required to implement a function that combines several utilities from the `torch.nn.attention.flex_attention` module to create a custom block mask for a transformer model. Problem Description Implement a function `custom_block_attention_mask` that accepts two parameters: - `block_size`: an integer representing the size of the block to be masked. - `sequence_length`: an integer representing the total length of the input sequence. The function should return a PyTorch tensor representing a masked attention matrix for a transformer model. The mask should block out sections of the input sequence according to the following rules: 1. Create a base block mask using the `create_block_mask` function. The mask should block regions in blocks of the given `block_size`. 2. Create an additional mask using the `create_mask` function that blocks the first half of the input sequence. 3. Combine the above two masks using logical OR operation so that either block-size regions or the first half of the sequence (or both) are masked out. 4. To ensure a valid attention pattern, invert this combined mask to demonstrate allowed regions for attention. Function Signature ```python import torch from torch.nn.attention.flex_attention import create_block_mask, create_mask, or_masks def custom_block_attention_mask(block_size: int, sequence_length: int) -> torch.Tensor: pass ``` Example ```python mask = custom_block_attention_mask(4, 16) print(mask) # Expected output: A tensor mask with blocked regions in blocks of size 4 and also the first half of the sequence. # The shape of the output tensor should be [16, 16] (sequence_length x sequence_length). # If block_size is 4, every 4th block should be masked, and the first 8 (half of 16) should also be masked. ``` Constraints - `1 <= block_size <= sequence_length` - `1 <= sequence_length <= 512` Notes - You should use the provided `create_block_mask`, `create_mask`, and `or_masks` functions. - The output should be a valid PyTorch tensor that can directly be used in transformer model attention mechanisms to specify which positions can attend to which other positions. Performance Requirements - The implementation should be efficient and able to handle the maximum constraint of `sequence_length = 512` within a reasonable time frame.","solution":"import torch from torch.nn.functional import pad def create_block_mask(block_size: int, sequence_length: int) -> torch.Tensor: mask = torch.zeros((sequence_length, sequence_length), dtype=torch.bool) for i in range(0, sequence_length, block_size): mask[i:i + block_size, i:i + block_size] = True return mask def create_mask(sequence_length: int) -> torch.Tensor: mask = torch.zeros((sequence_length, sequence_length), dtype=torch.bool) midpoint = sequence_length // 2 mask[:midpoint, :] = True return mask def or_masks(mask1: torch.Tensor, mask2: torch.Tensor) -> torch.Tensor: return mask1 | mask2 def custom_block_attention_mask(block_size: int, sequence_length: int) -> torch.Tensor: base_block_mask = create_block_mask(block_size, sequence_length) first_half_mask = create_mask(sequence_length) combined_mask = or_masks(base_block_mask, first_half_mask) inverted_mask = ~combined_mask return inverted_mask"},{"question":"Objective Create a clear, informative scatter plot using the seaborn library to analyze a dataset. Problem Statement You are provided with a dataset `tips` containing information about tips received by a waiter in a restaurant. Your objective is to generate a scatter plot that compares the `total_bill` and `tip` amounts while encoding additional information using the `time`, `day`, and `size` variables. Dataset You can load the dataset using the following code: ```python import seaborn as sns tips = sns.load_dataset(\\"tips\\") ``` Requirements 1. Plot a scatter plot using `total_bill` on the x-axis and `tip` on the y-axis. 2. Encode the `time` of day (Lunch/Dinner) using different colors. 3. Use different markers to distinguish between different `days` (Thu, Fri, Sat, Sun). 4. Represent the `size` of the group at each table using the size of the markers. 5. Ensure that all legend entries are displayed. 6. Use an aesthetically pleasing theme provided by seaborn. Constraints - The plot should be clear and readable with properly labeled axes and a legend. - The code should include necessary imports and dataset loading. - You can use any additional customization features to make the plot more informative. Expected Output - A single scatter plot meeting the specifications above. - A legend that shows all `time` and `day` categories as well as marker sizes corresponding to `size`. - The plot should use a seaborn theme for better visualization. Example ```python # Your solution code here ``` Notes - Make sure your plot provides clear visual distinction between categories (time and day) and encodes the size information effectively. - You can refer to seaborn documentation for examples and additional customization options.","solution":"import seaborn as sns import matplotlib.pyplot as plt def scatter_plot_tips(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Set the seaborn theme for better aesthetics sns.set_theme(style=\\"whitegrid\\") # Create the scatter plot scatter_plot = sns.scatterplot( x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"day\\", size=\\"size\\", sizes=(20, 200), palette=\\"deep\\", data=tips ) # Set the plot title and axis labels scatter_plot.set_title(\\"Scatter Plot of Total Bill vs Tip\\") scatter_plot.set_xlabel(\\"Total Bill ()\\") scatter_plot.set_ylabel(\\"Tip ()\\") # Show the legend and display the plot scatter_plot.legend() plt.show() # Call the function to create the scatter plot scatter_plot_tips()"},{"question":"Objective In this task, you are required to demonstrate your understanding of the `gzip` module in Python by developing a program that compresses and decompresses files and handles possible errors. Problem Statement You are tasked with writing a Python program named `gzip_operation.py` that performs the following actions: 1. **Compress a file**: Write a function `compress_file(input_file: str, output_file: str, compresslevel: int=9) -> None` that takes the path of an input file to be compressed, the path where the compressed file should be saved, and the compression level (default is 9). Ensure the function handles any exceptions that might occur during the compression process and prints appropriate error messages. 2. **Decompress a file**: Write a function `decompress_file(input_file: str, output_file: str) -> None` that takes the path of a compressed input file and the path where the decompressed file should be saved. Ensure the function handles any exceptions that might occur during the decompression process and prints appropriate error messages. 3. **CLI Wrapper**: Implement a command line interface (CLI) for the program using the argparse library that allows users to choose compress or decompress operation mode. The CLI should support the following command line arguments: - `--mode` (`-m`): Specifies the operation mode, either \'compress\' or \'decompress\'. - `--input_file` (`-i`): Specifies the path of the file to be compressed or decompressed. - `--output_file` (`-o`): Specifies the path where the output file should be saved. - `--compresslevel` (`-c`): Specifies the level of compression (only used in compress mode) ranging from 0 to 9. Constraints - Handle all possible exceptions (e.g., file not found, invalid gzip format, permission issues) in a robust manner. - The program should produce console output indicating success or failure with informative error messages. - The code should be properly documented and adhere to Python\'s PEP 8 style guidelines. Example Usage ```bash # Compressing a file python gzip_operation.py -m compress -i /path/to/input.txt -o /path/to/output.txt.gz -c 6 # Decompressing a file python gzip_operation.py -m decompress -i /path/to/input.txt.gz -o /path/to/output.txt ``` Notes - Make sure to include all necessary imports for your script to function. - Use appropriate error handling techniques to manage exceptions. - Focus on writing clear and maintainable code, with comments explaining any complex logic.","solution":"import gzip import shutil import argparse def compress_file(input_file: str, output_file: str, compresslevel: int=9) -> None: Compress a file using gzip. :param input_file: Path to the file to be compressed. :param output_file: Path where the compressed file should be saved. :param compresslevel: Compression level (default is 9). try: with open(input_file, \'rb\') as f_in, gzip.open(output_file, \'wb\', compresslevel=compresslevel) as f_out: shutil.copyfileobj(f_in, f_out) print(f\\"File \'{input_file}\' compressed successfully to \'{output_file}\'\\") except Exception as e: print(f\\"An error occurred while compressing the file: {e}\\") def decompress_file(input_file: str, output_file: str) -> None: Decompress a gzip file. :param input_file: Path to the gzipped file to be decompressed. :param output_file: Path where the decompressed file should be saved. try: with gzip.open(input_file, \'rb\') as f_in, open(output_file, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) print(f\\"File \'{input_file}\' decompressed successfully to \'{output_file}\'\\") except Exception as e: print(f\\"An error occurred while decompressing the file: {e}\\") def main(): parser = argparse.ArgumentParser(description=\'Gzip file compression and decompression utility.\') parser.add_argument(\'-m\', \'--mode\', required=True, choices=[\'compress\', \'decompress\'], help=\'Operation mode: compress or decompress.\') parser.add_argument(\'-i\', \'--input_file\', required=True, help=\'Path to the input file.\') parser.add_argument(\'-o\', \'--output_file\', required=True, help=\'Path to the output file.\') parser.add_argument(\'-c\', \'--compresslevel\', type=int, default=9, help=\'Compression level (0-9) (Only used in compress mode).\') args = parser.parse_args() if args.mode == \'compress\': compress_file(args.input_file, args.output_file, args.compresslevel) elif args.mode == \'decompress\': decompress_file(args.input_file, args.output_file) if __name__ == \\"__main__\\": main()"},{"question":"# Pandas Coding Assessment Objective: This task aims to assess your understanding of fundamental and advanced pandas functionalities including DataFrame creation, data manipulation, handling missing data, merging, and generating summary statistics. Problem Statement: You are provided with sales data of a retail store for different products over a period of time. You need to perform various data manipulation tasks and statistical analysis using pandas. Instructions: 1. Create a DataFrame from the given dictionary of lists. 2. Perform data cleaning and data manipulation. 3. Merge DataFrames and handle missing data. 4. Generate summary statistics and insights. Dataset: Use the following dictionary to create an initial DataFrame: ```python data = { \\"Product\\": [\\"A\\", \\"B\\", \\"C\\", \\"A\\", \\"B\\", \\"C\\", \\"A\\", \\"B\\", \\"C\\", \\"A\\"], \\"Date\\": [ \\"2021-01-01\\", \\"2021-01-01\\", \\"2021-01-01\\", \\"2021-01-02\\", \\"2021-01-02\\", \\"2021-01-02\\", \\"2021-01-03\\", \\"2021-01-03\\", \\"2021-01-03\\", \\"2021-01-04\\" ], \\"Sales\\": [100, 150, np.nan, 200, 250, 300, np.nan, 150, 200, 180], } ``` Tasks: 1. **Create DataFrame:** - Create a DataFrame from the `data` dictionary. - Ensure the `Date` column is of datetime type. 2. **Handle Missing Data:** - Check for missing values in the DataFrame. - Fill the missing `Sales` values with the mean sales value of the corresponding product. 3. **Calculate Daily Sales:** - Group the data by `Date` and calculate the total sales for each day. 4. **Merge with Additional Data:** - Create a new DataFrame `product_info`: ```python product_info = { \\"Product\\": [\\"A\\", \\"B\\", \\"C\\"], \\"Category\\": [\\"Electronics\\", \\"Household\\", \\"Groceries\\"] } ``` - Merge `product_info` with the sales DataFrame based on the `Product` column. 5. **Generate Summary Statistics:** - Calculate the total sales and average sales for each product category. - Find the day with the highest total sales. 6. **Output Format:** - Print the cleaned DataFrame with filled missing values. - Print the daily sales DataFrame. - Print the merged DataFrame. - Print the summary statistics. Constraints: - Use pandas for all data manipulations. - Ensure the code is efficient and handles potential edge cases. Example Output: ``` Cleaned DataFrame with Filled Missing Values: Product Date Sales 0 A 2021-01-01 100.0 1 B 2021-01-01 150.0 2 C 2021-01-01 250.0 3 A 2021-01-02 200.0 4 B 2021-01-02 250.0 5 C 2021-01-02 300.0 6 A 2021-01-03 160.0 7 B 2021-01-03 150.0 8 C 2021-01-03 200.0 9 A 2021-01-04 180.0 Daily Sales DataFrame: Date Total_Sales 0 2021-01-01 500.0 1 2021-01-02 750.0 2 2021-01-03 510.0 3 2021-01-04 180.0 Merged DataFrame: Product Date Sales Category 0 A 2021-01-01 100.0 Electronics 1 A 2021-01-02 200.0 Electronics 2 A 2021-01-03 160.0 Electronics 3 A 2021-01-04 180.0 Electronics 4 B 2021-01-01 150.0 Household 5 B 2021-01-02 250.0 Household 6 B 2021-01-03 150.0 Household 7 C 2021-01-01 250.0 Groceries 8 C 2021-01-02 300.0 Groceries 9 C 2021-01-03 200.0 Groceries Summary Statistics: Total Sales by Category: Category Electronics 640.0 Groceries 750.0 Household 550.0 dtype: float64 Average Sales by Category: Category Electronics 160.0 Groceries 250.0 Household 183.3 dtype: float64 Day with Highest Total Sales: 2021-01-02 with 750.0 Sales ``` **Performance Requirements:** - Efficiently handle large datasets. - Optimize pandas operations to minimize computational overhead.","solution":"import pandas as pd import numpy as np # Step 1: Create DataFrame data = { \\"Product\\": [\\"A\\", \\"B\\", \\"C\\", \\"A\\", \\"B\\", \\"C\\", \\"A\\", \\"B\\", \\"C\\", \\"A\\"], \\"Date\\": [ \\"2021-01-01\\", \\"2021-01-01\\", \\"2021-01-01\\", \\"2021-01-02\\", \\"2021-01-02\\", \\"2021-01-02\\", \\"2021-01-03\\", \\"2021-01-03\\", \\"2021-01-03\\", \\"2021-01-04\\" ], \\"Sales\\": [100, 150, np.nan, 200, 250, 300, np.nan, 150, 200, 180], } df = pd.DataFrame(data) df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Step 2: Handle Missing Data mean_sales = df.groupby(\'Product\')[\'Sales\'].transform(\'mean\') df[\'Sales\'].fillna(mean_sales, inplace=True) # Step 3: Calculate Daily Sales daily_sales = df.groupby(\'Date\')[\'Sales\'].sum().reset_index().rename(columns={\'Sales\': \'Total_Sales\'}) # Step 4: Merge with Additional Data product_info = { \\"Product\\": [\\"A\\", \\"B\\", \\"C\\"], \\"Category\\": [\\"Electronics\\", \\"Household\\", \\"Groceries\\"] } product_info_df = pd.DataFrame(product_info) merged_df = df.merge(product_info_df, on=\'Product\', how=\'left\') # Step 5: Generate Summary Statistics total_sales_by_category = merged_df.groupby(\'Category\')[\'Sales\'].sum() average_sales_by_category = merged_df.groupby(\'Category\')[\'Sales\'].mean() # Day with highest total sales max_sales_day = daily_sales[daily_sales[\'Total_Sales\'] == daily_sales[\'Total_Sales\'].max()] # Outputs print(\\"Cleaned DataFrame with Filled Missing Values:\\") print(df) print(\\"nDaily Sales DataFrame:\\") print(daily_sales) print(\\"nMerged DataFrame:\\") print(merged_df) print(\\"nSummary Statistics:\\") print(\\"Total Sales by Category:\\") print(total_sales_by_category) print(\\"nAverage Sales by Category:\\") print(average_sales_by_category) print(\\"nDay with Highest Total Sales:\\") print(max_sales_day[\'Date\'].values[0], \\"with\\", max_sales_day[\'Total_Sales\'].values[0], \\"Sales\\")"},{"question":"# Configuration File Processor You are required to implement a function, `merge_config_files`, that merges multiple configuration files into one. This function should use the `configparser` module to read the configurations from multiple files, and then write the resulting combined configuration into a new file. # Function Signature: ```python def merge_config_files(input_files: list, output_file: str) -> None: Merges multiple configuration files into one. Parameters: input_files (list): A list of strings representing the paths to the input configuration files. The configurations from these files are merged in the given order. output_file (str): The path to the output configuration file. The merged configuration is written to this file. Returns: None pass ``` # Detailed Requirements: 1. **Reading Files**: Use the `configparser` module to read the contents from the list of input files. The configurations should be merged in the order they appear in the list, with later files potentially overriding values from earlier ones. 2. **Writing Output**: The resulting merged configuration should be written to the specified output file using the same module. 3. **Handling Duplicates**: If the same section and option are present in multiple files, the value from the later file in the list should override the earlier one. 4. **Data Types**: Ensure the merged configuration correctly handles integer, float, and boolean types using appropriate getter methods. 5. **Custom Separator**: Use a colon `:` as the delimiter between keys and values instead of the default `=`. 6. **Comment Handling**: Preserve comments when merging configurations but discard them when writing to the final output file. 7. **Error Handling**: Gracefully handle exceptions such as missing files, parsing errors, and duplicate sections or options within a single file using appropriate `configparser` exceptions. # Example Usage: ```python # Assuming config1.ini, config2.ini, and config3.ini are existing configuration files input_files = [\'config1.ini\', \'config2.ini\', \'config3.ini\'] output_file = \'merged_config.ini\' merge_config_files(input_files, output_file) # After the function call, \'merged_config.ini\' should contain the merged configuration from config1.ini, config2.ini, and config3.ini. ``` # Constraints: - The number of input files will not exceed 10. - Each input file and the output file paths will be valid and writable. - Configurations in each file will follow a valid INI format. # Notes: - Use the `ConfigParser` class from the `configparser` module for managing configurations. - Ensure you test the function with various cases including missing sections in some files, differing data types, and large number of settings to confirm correctness.","solution":"import configparser from typing import List def merge_config_files(input_files: List[str], output_file: str) -> None: Merges multiple configuration files into one. Parameters: input_files (list): A list of strings representing the paths to the input configuration files. The configurations from these files are merged in the given order. output_file (str): The path to the output configuration file. The merged configuration is written to this file. Returns: None config = configparser.ConfigParser(interpolation=None, delimiters=\':\') # Read the configurations from each input file for file in input_files: config.read(file) # Write the merged configuration to the output file with open(output_file, \'w\') as f: config.write(f, space_around_delimiters=False) # Example usage: # input_files = [\'config1.ini\', \'config2.ini\', \'config3.ini\'] # output_file = \'merged_config.ini\' # merge_config_files(input_files, output_file)"},{"question":"# Custom File Operations with Tkinter Dialogs You are given the task of creating a small program that helps users to search for text files within a directory and read the contents of these files. You will need to make use of tkinter dialogs to handle directory selection and file reading. Requirements 1. Create a GUI application using `tkinter` that: - Prompts the user to select a directory. - Lists all `.txt` files located in the selected directory (you can use tkinter message box or any other suitable way to display the list). 2. Implement a custom dialog using `tkinter.simpledialog.Dialog` where a user can input text to search for within these `.txt` files. - The custom dialog should have an entry widget for the search term. - An OK button to initiate the search and a Cancel button to close the dialog. 3. After the user inputs the search term and presses OK, the program should: - Search for the input term in all `.txt` files within the selected directory. - Show the filenames and lines where the term was found (this can be displayed in a message box or any other suitable tkinter widget). Function Signatures You are expected to implement the following functions: 1. `select_directory() -> str`: - Prompts the user to select a directory and returns the directory path as a string. 2. `list_text_files(directory: str) -> List[str]`: - Takes a directory path as input and returns a list of `.txt` files in the directory. 3. `show_search_dialog(parent) -> Optional[str]`: - Displays the custom search dialog and returns the search term input by the user. Returns `None` if the dialog is canceled. 4. `search_in_files(directory: str, search_term: str) -> Dict[str, List[str]]`: - Takes a directory and a search term, searches for the term in all `.txt` files, and returns a dictionary where the keys are filenames and the values are lists of strings containing the lines in which the term was found. Example Usage ```python dir_path = select_directory() if dir_path: txt_files = list_text_files(dir_path) # Display txt_files to the user. search_term = show_search_dialog(root) if search_term: results = search_in_files(dir_path, search_term) # Display results to the user. ``` Constraints - You can assume that the user input is well-formed. - The search should be case-insensitive. - The functions should handle potential errors gracefully, such as file access issues or empty directories. Provide this implementation and test it with a directory containing several `.txt` files.","solution":"import os import tkinter as tk from tkinter import filedialog, messagebox, simpledialog from typing import List, Dict, Optional def select_directory() -> str: Prompts the user to select a directory and returns the directory path as a string. root = tk.Tk() root.withdraw() dir_path = filedialog.askdirectory() root.destroy() return dir_path def list_text_files(directory: str) -> List[str]: Takes a directory path as input and returns a list of .txt files in the directory. try: return [f for f in os.listdir(directory) if f.endswith(\'.txt\')] except FileNotFoundError: return [] class SearchDialog(simpledialog.Dialog): Custom dialog to search for a text within .txt files. def body(self, master): tk.Label(master, text=\\"Search Term:\\").grid(row=0) self.search_term = tk.Entry(master) self.search_term.grid(row=0, column=1) return self.search_term def apply(self): self.result = self.search_term.get() def show_search_dialog(parent) -> Optional[str]: Displays the custom search dialog and returns the search term input by the user. Returns None if the dialog is canceled. dialog = SearchDialog(parent) return dialog.result def search_in_files(directory: str, search_term: str) -> Dict[str, List[str]]: Takes a directory and a search term, searches for the term in all .txt files, and returns a dictionary where the keys are filenames and the values are lists of strings containing the lines in which the term was found. results = {} search_term = search_term.lower() for filename in list_text_files(directory): file_path = os.path.join(directory, filename) try: with open(file_path, \'r\') as file: lines = file.readlines() matched_lines = [line.strip() for line in lines if search_term in line.lower()] if matched_lines: results[filename] = matched_lines except Exception as e: print(f\\"Error reading {file_path}: {e}\\") return results"},{"question":"**Deterministic Tensors with PyTorch** PyTorch provides features to ensure the deterministic behavior of operations. You are asked to implement a set of operations on tensors while ensuring that their behavior is deterministic, especially when working with uninitialized memory. # Objective 1. Write a function `deterministic_tensor_operations` that: - Ensures deterministic algorithms are used. - Sets the `fill_uninitialized_memory` attribute to `True`. - Creates three tensors using `torch.empty`, `torch.empty_like`, and `torch.Tensor.resize_` and fills any uninitialized memory with known values. # Function Signature ```python import torch def deterministic_tensor_operations() -> dict: # Your implementation here ``` # Requirements 1. The function should not take any inputs. 2. The function should return a dictionary containing: - `empty_tensor`: a tensor created using `torch.empty`. - `empty_like_tensor`: a tensor created using `torch.empty_like` based on the `empty_tensor`. - `resized_tensor`: an empty tensor resized using `torch.Tensor.resize_`. 3. Use the appropriate setting in PyTorch to ensure any uninitialized memory is filled with known values (NaN for floats and max value for integers). # Example Output ```python result = deterministic_tensor_operations() print(result) # Output example (values will be placeholders indicating uninitialized memory): # { # \'empty_tensor\': tensor([NaN, NaN, NaN, ...]), # \'empty_like_tensor\': tensor([NaN, NaN, NaN, ...]), # \'resized_tensor\': tensor([NaN, NaN, NaN, ...]) # } ``` # Constraints 1. You must use `torch.use_deterministic_algorithms(True)`. 2. Set `torch.utils.deterministic.fill_uninitialized_memory = True`. 3. The tensors’ shapes can be arbitrary but should be consistent between `empty_tensor` and `empty_like_tensor`. # Notes - Enabling `fill_uninitialized_memory` is useful for debugging but can negatively impact performance. Ensure that your code accounts for this while fulfilling the deterministic requirements. Good luck!","solution":"import torch def deterministic_tensor_operations(): # Ensure deterministic algorithms are used torch.use_deterministic_algorithms(True) # Ensure uninitialized memory is filled with known values torch.utils.deterministic.fill_uninitialized_memory = True # Create a tensor using torch.empty empty_tensor = torch.empty(10) # Example shape of 10 elements # Create a tensor using torch.empty_like based on the empty_tensor empty_like_tensor = torch.empty_like(empty_tensor) # Create an empty tensor and resize it using torch.Tensor.resize_ resized_tensor = torch.empty(0).resize_(10) # Resize to 10 elements # Return the tensors in a dictionary return { \'empty_tensor\': empty_tensor, \'empty_like_tensor\': empty_like_tensor, \'resized_tensor\': resized_tensor }"},{"question":"# Advanced Coding Challenge: Implementing a Custom Container with Garbage Collection Support in Python Objective: Your task is to create a custom container type in Python that supports circular references and integrates correctly with Python\'s garbage collector. Requirements: 1. **Define a Custom Container Type**: - Your container should hold a list of arbitrary objects. - Use the `tp_flags` field and set the `Py_TPFLAGS_HAVE_GC` flag. - Implement the `tp_traverse` and `tp_clear` handlers for the container. 2. **Memory Management**: - Allocate memory for instances of the container using `PyObject_GC_New`. - Track objects using `PyObject_GC_Track`. - Ensure proper deallocation using `PyObject_GC_UnTrack` and `PyObject_GC_Del`. 3. **Garbage Collector Integration**: - Implement functions to enable (`PyGC_Enable`) and disable (`PyGC_Disable`) the garbage collector. - Implement a function to check if the garbage collector is enabled (`PyGC_IsEnabled`). - Implement a method to force a garbage collection pass (`PyGC_Collect`). Input and Output: - There is no specific input; the focus is on the correct definition and implementation of the custom container type. - Ensure your container can correctly handle circular references and is properly collected by Python\'s garbage collector when no longer in use. Constraints: - Your container must work for any Python object stored within it. - Handle both initialization and deletion of objects correctly to avoid memory leaks. - Ensure the methods are efficient and do not introduce unnecessary overhead. Example: ```python # Define your custom container type class CustomContainer: # Your implementation goes here # Example usage: import gc # Instantiate the container and add some objects container = CustomContainer() container.add_object(object()) container.add_object([]) # Adding a list object # Enable the garbage collector CustomContainer.gc_enable() # Check if the garbage collector is enabled print(CustomContainer.gc_is_enabled()) # Output should be True # Disable the garbage collector CustomContainer.gc_disable() # Check if the garbage collector is disabled print(CustomContainer.gc_is_enabled()) # Output should be False # Perform explicit garbage collection CustomContainer.gc_collect() ``` Notes: - This challenge requires a deep understanding of Python\'s C API and garbage collection mechanics. - Ensure proper error handling and memory management practices. - Your solution must be compatible with Python 3.10.","solution":"import gc class CustomContainer: def __init__(self): self.items = [] # Container to hold arbitrary objects def add_object(self, obj): self.items.append(obj) @staticmethod def gc_enable(): gc.enable() @staticmethod def gc_disable(): gc.disable() @staticmethod def gc_is_enabled(): return gc.isenabled() @staticmethod def gc_collect(): return gc.collect()"},{"question":"# Advanced Coding Assessment Question **Objective:** Implement a Python function to execute arbitrary Python code provided either as a string or as a filename using the Very High Level Layer functions of Python C API. **Background:** You are to utilize the Python C API functions such as `PyRun_SimpleString` and `PyRun_SimpleFile` to execute given Python code. The function should decide based on the type of input to either directly run code from a string or read and run code from a file. **Details:** 1. **Function Name:** `run_python_code` 2. **Expected Parameters:** - `code_or_filename`: `str` — A string containing either Python code or the path to a Python file. - `is_file`: `bool` — A flag indicating if the input is a file path (`True`) or a Python code string (`False`). 3. **Expected Output:** The function should output the result of the executed code. If there is no result (e.g., the code contains only function definitions), it should return `None`. **Constraints:** - The function should be able to handle typical Python exceptions and report errors gracefully. - The function should be able to handle both single-line and multi-line code. - For file inputs, make sure the file is opened in binary mode as required on Windows. **Performance Requirements:** - The execution should be efficient and should not involve unnecessary I/O operations. **Example Usage:** ```python def run_python_code(code_or_filename: str, is_file: bool) -> Any: # Example Implementation pass # Example 1: Running code from a string code = \\"print(\'Hello, World!\')\\" run_python_code(code, False) # Example 2: Running code from a file file_path = \\"my_script.py\\" run_python_code(file_path, True) ``` Ensure your implementation handles both scenarios correctly and complies with the constraints given.","solution":"import subprocess def run_python_code(code_or_filename: str, is_file: bool) -> None: Executes arbitrary Python code provided as a string or as a filename. if is_file: with open(code_or_filename, \'r\') as file: code = file.read() else: code = code_or_filename try: exec(code) except Exception as e: print(f\\"Error executing code: {e}\\")"},{"question":"**Problem Statement:** You have been provided with a dataset for an upcoming machine learning project. Your task is to preprocess this dataset by implementing dimensionality reduction techniques using scikit-learn. Specifically, you will implement a function that: 1. Scales the data using `StandardScaler`. 2. Applies `PCA` to reduce the number of features to a specified number. 3. Uses `RandomProjections` to further reduce the dimensionality. 4. Groups similar features using `FeatureAgglomeration`. After performing these steps, utilize the inverse transform methods where applicable, to revert the scaled and reduced features back to the original space dimensions. # Function Signature ```python def dimensionality_reduction_pipeline(data: np.ndarray, n_components_pca: int, n_components_rp: int, n_clusters: int) -> np.ndarray: Preprocess and reduce the dimensionality of the dataset using PCA, RandomProjections, and FeatureAgglomeration. Parameters: - data (np.ndarray): A 2D array where rows represent samples and columns represent features. - n_components_pca (int): The number of principal components to keep after PCA. - n_components_rp (int): The target number of dimensions after applying Random Projections. - n_clusters (int): The number of clusters to form through Feature Agglomeration. Returns: - np.ndarray: The dataset reverted to the original space after performing dimensionality reduction. ``` # Input Format - `data`: A 2D numpy array of shape `(m, n)` where `m` is the number of samples and `n` is the number of features. - `n_components_pca`: An integer specifying the number of principal components for PCA. - `n_components_rp`: An integer specifying the target number of dimensions for Random Projections. - `n_clusters`: An integer specifying the number of clusters for Feature Agglomeration. # Output Format - A 2D numpy array of shape `(m, n)`, representing the dataset after scaling, reduction, and inverse transformations. # Constraints - `data` will have at least two features and at least two samples. - `n_components_pca` and `n_components_rp` must be positive integers and less than the initial number of features. - `n_clusters` must be a positive integer and less or equal to the initial number of features. # Example ```python import numpy as np data = np.array([[0.8, 1.2, 0.5], [0.4, 0.5, 0.3], [0.6, 0.8, 0.2]]) n_components_pca = 2 n_components_rp = 2 n_clusters = 2 result = dimensionality_reduction_pipeline(data, n_components_pca, n_components_rp, n_clusters) # Expected: A (3, 3) numpy array representing the data after processing. print(result) ``` # Notes - Ensure that throughout your transformations, the shape of the data is consistent with the described steps, and the final output should match the original feature dimensions. - Utilize scikit-learn\'s `StandardScaler`, `PCA`, `RandomProjection`, and `FeatureAgglomeration` classes for implementation. - Make sure to handle edge cases and validate input parameters before processing.","solution":"import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.random_projection import GaussianRandomProjection from sklearn.cluster import FeatureAgglomeration def dimensionality_reduction_pipeline(data: np.ndarray, n_components_pca: int, n_components_rp: int, n_clusters: int) -> np.ndarray: Preprocess and reduce the dimensionality of the dataset using PCA, RandomProjections, and FeatureAgglomeration. Parameters: - data (np.ndarray): A 2D array where rows represent samples and columns represent features. - n_components_pca (int): The number of principal components to keep after PCA. - n_components_rp (int): The target number of dimensions after applying Random Projections. - n_clusters (int): The number of clusters to form through Feature Agglomeration. Returns: - np.ndarray: The dataset reverted to the original space after performing dimensionality reduction. # Validate the input dimensions and constraints. if n_components_pca >= data.shape[1]: raise ValueError(\\"n_components_pca must be less than the number of features in data.\\") if n_components_rp >= data.shape[1]: raise ValueError(\\"n_components_rp must be less than the number of features in data.\\") if n_clusters > data.shape[1]: raise ValueError(\\"n_clusters must be less than or equal to the number of features in data.\\") # Step 1: Scale the data scaler = StandardScaler() scaled_data = scaler.fit_transform(data) # Step 2: Apply PCA pca = PCA(n_components=n_components_pca) pca_data = pca.fit_transform(scaled_data) # Step 3: Apply Random Projections random_projection = GaussianRandomProjection(n_components=n_components_rp) rp_data = random_projection.fit_transform(pca_data) # Step 4: Feature Agglomeration feature_agglomeration = FeatureAgglomeration(n_clusters=n_clusters) aggregated_data = feature_agglomeration.fit_transform(rp_data) # Step 5: Use inverse transformations where applicable inverted_rp_data = random_projection.inverse_transform(aggregated_data) inverted_pca_data = pca.inverse_transform(inverted_rp_data) final_data = scaler.inverse_transform(inverted_pca_data) return final_data"},{"question":"**Problem Statement:** You are tasked to implement a function that takes a list of errno integer values and returns a dictionary. The keys of the dictionary should be the integer errno values, and the values should be their corresponding string names from the `errno` module. If any of the errno values in the input list do not have a corresponding name in the `errno` module, the function should return `None` for that errno value. Additionally, your function should include an option to filter out the `None` values and only return valid errno mappings when the `filter_invalid` parameter is set to `True`. **Function Signature:** ```python def map_errno_values(errno_values: list, filter_invalid: bool = False) -> dict: ``` **Input:** - `errno_values`: A list of integer values representing errno codes. - `filter_invalid`: A boolean flag indicating whether to filter out invalid errno values (that map to `None`). Default value is `False`. **Output:** - A dictionary where: - Keys are the input errno values. - Values are string names corresponding to the errno values. If a certain errno value does not match any in the `errno` module, assign `None` to it, unless `filter_invalid` is set to `True`. **Example:** ```python # Example usage: errno_values = [1, 2, 9999] result = map_errno_values(errno_values) # Expected output: {1: \'EPERM\', 2: \'ENOENT\', 9999: None} filter_result = map_errno_values(errno_values, filter_invalid=True) # Expected output: {1: \'EPERM\', 2: \'ENOENT\'} ``` **Constraints:** - The input list of `errno_values` will contain at most 100 elements. - The errno values will be positive integers. - Performance should be adequately efficient to handle the input size within a reasonable time (O(n) complexity). **Notes:** - Use the `errno` module to get the mappings. - Make sure your implementation handles all edge cases, such as empty lists and non-existent errno values. - It is advisable to use the `errno.errorcode` dictionary directly for mapping errno values to their string names.","solution":"import errno def map_errno_values(errno_values: list, filter_invalid: bool = False) -> dict: Maps errno values to their corresponding string names. Parameters: - errno_values (list): A list of integer errno codes. - filter_invalid (bool): A flag to indicate whether to filter out invalid errno values (default: False). Returns: - dict: A dictionary mapping errno codes to their string names. result = {} for code in errno_values: errno_name = errno.errorcode.get(code, None) if errno_name is not None or not filter_invalid: result[code] = errno_name return result"},{"question":"Objective: Write a function that uses seaborn to visualize the relationship between different attributes of the `penguins` dataset, demonstrating your comprehension of the seaborn package. Details: Your task is to: 1. Load the `penguins` dataset using seaborn\'s `load_dataset` function. 2. Create a plot visualizing the relationship between body mass and flipper length, differentiated by species and sex. 3. Incorporate both individual data points and aggregates with error bars showing the standard deviation. 4. Use faceting to separate the plots by the island. Function Signature: ```python def visualize_penguins(): pass ``` Requirements: 1. Load the `penguins` dataset using seaborn. 2. Use `seaborn.objects.Plot` to create a plot where the x-axis represents the body mass (`body_mass_g`) and the y-axis represents the flipper length (`flipper_length_mm`). 3. Differentiate the data points by species (using different colors) and by sex (using different markers). 4. Add layers to: - Show individual data points. - Show aggregate values with error bars representing the standard deviation. 5. Use facet grids to create separate plots for each island. Code Implementation: ```python import seaborn.objects as so from seaborn import load_dataset def visualize_penguins(): # Load dataset penguins = load_dataset(\\"penguins\\") # Create plot plot = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\", color=\\"species\\", marker=\\"sex\\") .facet(\\"island\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) # Show plot plot.show() # Call the function to visualize the plot visualize_penguins() ``` Expectations: - The implementation should correctly load the penguins dataset. - The plot should display individual data points and aggregate values with error bars for each combination of species and sex. - Each island should have a separate faceted plot. Constraints: - Use the `seaborn.objects` module for all plotting. - Ensure that the function does not return anything, but the plot should be displayed when the function is called.","solution":"import seaborn.objects as so from seaborn import load_dataset def visualize_penguins(): # Load dataset penguins = load_dataset(\\"penguins\\") # Create plot plot = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\", color=\\"species\\", marker=\\"sex\\") .facet(\\"island\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) # Show plot plot.show() # Call the function to visualize the plot visualize_penguins()"},{"question":"**Question:** You are required to implement a function that performs a specific sequence of operations on a list of integers. The function should be designed to handle various array manipulations including sorting, filtering, and transformation using comprehensions and built-in list methods. # Function Signature ```python def transform_and_filter(nums: List[int]) -> List[int]: pass ``` # Input - `nums` (List[int]): A list of integers. # Output - Returns a sorted list of integers that meet the following criteria: 1. The number is positive. 2. The number is even. 3. Each number in the resulting list is squared (the number itself raised to the power of 2). # Example ```python assert transform_and_filter([4, -3, 2, -1, 0, 5, 8, 10]) == [4, 16, 64, 100] assert transform_and_filter([3, 1, 7, 6, -6, 14, 9, 0]) == [36, 196] ``` # Constraints - You should use list comprehensions where applicable. - You should utilize built-in list methods for transformations and operations. - The function should be optimal in terms of time complexity. # Description 1. Start by filtering out numbers that are not positive. 2. From the remaining numbers, retain only the even ones. 3. For the resulting numbers, compute the square of each number. 4. Finally, return the list sorted in ascending order. The use of list comprehensions, combined with effective application of list methods, will be instrumental in solving this problem. Demonstrating comprehension and efficiency in manipulating lists using built-in functionalities and comprehensions is essential.","solution":"from typing import List def transform_and_filter(nums: List[int]) -> List[int]: Returns a sorted list of squared integers that are positive and even. return sorted([x * x for x in nums if x > 0 and x % 2 == 0])"},{"question":"Objective Design a function to initiate a distributed training job using PyTorch\'s `torchrun` (Elastic Launch). This function should configure the necessary settings and handle the launch process, ensuring the job can scale elastically and recover from failures. Problem Statement You are tasked with implementing a function `launch_distributed_training` that sets up and initiates a distributed training job using the `torchrun` command from PyTorch\'s distributed package. Your function should take the following inputs: - `module_name`: A string representing the module to be executed. - `num_nodes`: An integer specifying the number of nodes to be used. - `nproc_per_node`: An integer defining the number of processes to be run per node. - `min_nodes`: An integer setting the minimum number of nodes. The job should wait until at least this many nodes are available before beginning. - `max_restarts`: An integer defining the maximum number of times a job should be restarted in case of failure. - `rdzv_id`: A unique string identifier for the rendezvous. - `rdzv_backend`: A string specifying the rendezvous backend (e.g., `c10d` for PyTorch\'s backend). The function should handle and configure the necessary input arguments for `torchrun`, execute the command, and log the output status. Input Format: - `module_name` (str): The name of the module to execute (e.g., `\\"my_training_script.py\\"`). - `num_nodes` (int): Number of nodes to use. - `nproc_per_node` (int): Number of processes per node. - `min_nodes` (int): Minimum number of nodes required for the job to start. - `max_restarts` (int): Maximum number of retries in case of failure. - `rdzv_id` (str): Unique identifier for the rendezvous. - `rdzv_backend` (str): Backend to be used for rendezvous (default is \\"c10d\\"). Output Requirements: The function does not return anything but should print logs indicating the status of the launch process (e.g., command executed, any errors encountered). Constraints: - Ensure the function handles input validation and raises appropriate errors for invalid inputs. - Assume a Unix-like environment where the `torchrun` command is available. Example Usage: ```python def launch_distributed_training(module_name: str, num_nodes: int, nproc_per_node: int, min_nodes: int, max_restarts: int, rdzv_id: str, rdzv_backend: str = \\"c10d\\"): # Your implementation here # Example call launch_distributed_training( module_name=\\"my_training_script.py\\", num_nodes=4, nproc_per_node=2, min_nodes=2, max_restarts=3, rdzv_id=\\"unique_rdzv_id\\" ) ``` Implementation Notes: - Consider using the `subprocess` module to execute the `torchrun` command. - Validate each input to ensure it meets the necessary constraints (e.g., positive integers for node counts). - Use appropriate logging to provide feedback about the success or failure of the job launch.","solution":"import subprocess import logging def validate_inputs(module_name, num_nodes, nproc_per_node, min_nodes, max_restarts, rdzv_id, rdzv_backend): if not isinstance(module_name, str) or not module_name.endswith(\\".py\\"): raise ValueError(\\"module_name must be a string ending with .py\\") if not isinstance(num_nodes, int) or num_nodes <= 0: raise ValueError(\\"num_nodes must be a positive integer\\") if not isinstance(nproc_per_node, int) or nproc_per_node <= 0: raise ValueError(\\"nproc_per_node must be a positive integer\\") if not isinstance(min_nodes, int) or min_nodes <= 0: raise ValueError(\\"min_nodes must be a positive integer\\") if not isinstance(max_restarts, int) or max_restarts < 0: raise ValueError(\\"max_restarts must be a non-negative integer\\") if not isinstance(rdzv_id, str) or not rdzv_id: raise ValueError(\\"rdzv_id must be a non-empty string\\") if not isinstance(rdzv_backend, str) or not rdzv_backend: raise ValueError(\\"rdzv_backend must be a non-empty string\\") def launch_distributed_training(module_name: str, num_nodes: int, nproc_per_node: int, min_nodes: int, max_restarts: int, rdzv_id: str, rdzv_backend: str = \\"c10d\\"): Launches a distributed training job using torchrun. # Validate inputs validate_inputs(module_name, num_nodes, nproc_per_node, min_nodes, max_restarts, rdzv_id, rdzv_backend) command = [ \\"torchrun\\", \\"--nnodes\\", str(num_nodes), \\"--nproc_per_node\\", str(nproc_per_node), \\"--rdzv_id\\", rdzv_id, \\"--rdzv_backend\\", rdzv_backend, \\"--rdzv_endpoint\\", \\"localhost:0\\", \\"--max_restarts\\", str(max_restarts), \\"--start_method\\", \\"env://\\", module_name ] logging.info(f\\"Running command: {\' \'.join(command)}\\") try: result = subprocess.run(command, check=True, capture_output=True, text=True) logging.info(f\\"Command output: {result.stdout}\\") except subprocess.CalledProcessError as e: logging.error(f\\"Command failed with error: {e.stderr}\\") logging.basicConfig(level=logging.INFO) # Example usage launch_distributed_training( module_name=\\"my_training_script.py\\", num_nodes=4, nproc_per_node=2, min_nodes=2, max_restarts=3, rdzv_id=\\"unique_rdzv_id\\" )"},{"question":"Nullable Boolean Arrays: Indexing and Logical Operations **Objective:** Write a function that takes two pandas Series, performs specified logical operations using nullable Boolean arrays, and returns a resulting DataFrame. The function must handle NA values according to the Kleene logic described. **Function Signature:** ```python import pandas as pd def process_series(series1: pd.Series, series2: pd.Series) -> pd.DataFrame: pass ``` **Input:** - `series1`: A pandas Series of boolean values, possibly containing NA. - `series2`: A pandas Series of boolean values, possibly containing NA. **Output:** - A pandas DataFrame with the following columns: - `and_result`: The result of performing an `&` (and) operation between `series1` and `series2`. - `or_result`: The result of performing an `|` (or) operation between `series1` and `series2`. - `xor_result`: The result of performing a `^` (exclusive-or) operation between `series1` and `series2`. **Requirements:** - NA values need to be handled using Kleene logic. - If the input Series are of different lengths, the function should raise a ValueError with the message: \\"Input Series must have the same length.\\" **Constraints:** - Do not use any external libraries other than pandas and numpy. **Example:** ```python import pandas as pd import numpy as np s1 = pd.Series([True, False, pd.NA, True], dtype=\\"boolean\\") s2 = pd.Series([pd.NA, True, False, pd.NA], dtype=\\"boolean\\") result_df = process_series(s1, s2) print(result_df) ``` **Expected Output:** ``` and_result or_result xor_result 0 <NA> True <NA> 1 False True True 2 False <NA> <NA> 3 <NA> True <NA> ``` **Approach:** 1. Ensure that the lengths of `series1` and `series2` are equal. 2. Perform logical operations `&`, `|`, and `^` between `series1` and `series2`. 3. Construct a DataFrame from the results and return it. **Notes:** - Remember to handle NA values using the Kleene logical rules provided in the documentation.","solution":"import pandas as pd def process_series(series1: pd.Series, series2: pd.Series) -> pd.DataFrame: if series1.size != series2.size: raise ValueError(\\"Input Series must have the same length.\\") and_result = series1 & series2 or_result = series1 | series2 xor_result = series1 ^ series2 result = pd.DataFrame({ \'and_result\': and_result, \'or_result\': or_result, \'xor_result\': xor_result }) return result"},{"question":"Objective To evaluate your understanding and ability to work with Python\'s runtime services, specifically focusing on system-specific parameters, warning control, dataclasses, and context managers. Problem Statement You are required to implement a set of functions that demonstrate the use of the `sys`, `dataclasses`, `warnings`, and `contextlib` modules. These functions should collectively perform the following tasks: 1. **System Configuration Information**: - Write a function `get_python_build_info()` that uses the `sysconfig` module to retrieve and return a dictionary containing Python\'s build time variables. 2. **Warning Control**: - Write a function `deprecated_function_warning()` that uses the `warnings` module to issue a deprecation warning when called. This function should be accompanied by a decorator `suppress_deprecation_warnings` which temporarily suppresses the deprecation warning when a decorated function is called. 3. **Data Classes**: - Write a function `create_person_dataclass()` that defines a `Person` dataclass with fields `name` (str), `age` (int), and `email` (str). Include default values for `age` (0) and `email` (empty string), and ensure the class is immutable (frozen). 4. **Context Managers**: - Write a context manager `execution_timer` using the `contextlib` module that measures and prints the execution time of a code block it manages. Function Signatures ```python import sysconfig import warnings from dataclasses import dataclass, field from contextlib import contextmanager import time def get_python_build_info() -> dict: pass def deprecated_function_warning() -> None: pass def suppress_deprecation_warnings(func): pass def create_person_dataclass(): pass @contextmanager def execution_timer(): pass ``` Constraints and Requirements - The `get_python_build_info()` function must use `sysconfig.get_config_vars()` to collect the information. - The `deprecated_function_warning()` function should utilize the `warnings.warn()` method with the `DeprecationWarning` category. - The `suppress_deprecation_warnings` decorator must temporarily suppress warnings while executing the decorated function. - The `create_person_dataclass()` function should leverage Python\'s `dataclasses.dataclass` decorator and ensure the `Person` class is frozen. - The `execution_timer` context manager must use `time.time()` to measure the execution duration. Example Usage ```python # 1. System Configuration Information print(get_python_build_info()) # 2. Warning Control @suppress_deprecation_warnings def test_function(): deprecated_function_warning() test_function() # 3. Data Classes Person = create_person_dataclass() person = Person(name=\\"John Doe\\") print(person) # 4. Context Managers with execution_timer(): time.sleep(1) ``` Notes - Ensure that all functions are well-documented with clear explanations of their behavior. - Write unit tests to confirm the correct implementation of each function.","solution":"import sysconfig import warnings from dataclasses import dataclass, field from contextlib import contextmanager import time def get_python_build_info() -> dict: Retrieves and returns a dictionary containing Python\'s build time variables. return sysconfig.get_config_vars() def deprecated_function_warning() -> None: Issues a deprecation warning. warnings.warn(\\"This function is deprecated.\\", DeprecationWarning) def suppress_deprecation_warnings(func): Decorator that temporarily suppresses deprecation warnings when the decorated function is called. def wrapper(*args, **kwargs): with warnings.catch_warnings(): warnings.simplefilter(\\"ignore\\", DeprecationWarning) return func(*args, **kwargs) return wrapper def create_person_dataclass(): Defines and returns a Person dataclass with fields: name (str), age (int), and email (str). The class is frozen (immutable). @dataclass(frozen=True) class Person: name: str age: int = 0 email: str = \\"\\" return Person @contextmanager def execution_timer(): Context manager that measures and prints the execution time of the code block it manages. start_time = time.time() try: yield finally: end_time = time.time() print(f\\"Execution time: {end_time - start_time:.6f} seconds\\")"},{"question":"# Context Management in Python with `contextvars` In Python, context variables allow you to manage context-specific data in applications, such as values that should be unique and independent across different parts of a program or even different threads. Your task is to implement a function that demonstrates the creation, usage, and manipulation of context variables. # Problem Statement You need to implement a function `record_user_action(context, actions)` that takes two arguments: 1. `context`: An initial dictionary containing context variable names as keys and their default values. 2. `actions`: A list of actions to be performed within the context. Each action is represented by a tuple of the form `(action_name, context_var_name, value)`, where: - `action_name`: a string, either `\'set\'` or `\'reset\'`. - `context_var_name`: the name of the context variable to be manipulated. - `value`: the value to be set for the context variable (only relevant for `\'set\'` actions). Your function should: 1. Create a new context using the initial context dictionary. 2. Perform the actions sequentially: - For a `\'set\'` action, set the specified context variable to the provided value and store the token. - For a `\'reset\'` action, reset the specified context variable to its previous state using the stored token. 3. Return a dictionary representing the final state of all context variables. # Constraints - The initial dictionary and the list of actions are guaranteed to be non-empty. - All `context_var_name` values in the actions will be present in the initial dictionary. # Example ```python import contextvars def record_user_action(context, actions): # Your code here to implement the function # Usage example initial_context = { \'user_id\': \'guest\', \'session_id\': \'unknown\' } actions = [ (\'set\', \'user_id\', \'user123\'), (\'set\', \'session_id\', \'sess456\'), (\'reset\', \'user_id\', None), (\'set\', \'user_id\', \'user789\') ] result = record_user_action(initial_context, actions) print(result) # Output should be {\'user_id\': \'user789\', \'session_id\': \'sess456\'} ``` # Explanation In this example: 1. The initial context sets `\'user_id\'` to `\'guest\'` and `\'session_id\'` to `\'unknown\'`. 2. The first action sets `\'user_id\'` to `\'user123\'`. 3. The second action sets `\'session_id\'` to `\'sess456\'`. 4. The third action resets `\'user_id\'` to its previous state, which here would be \'guest\'. 5. The fourth action sets `\'user_id\'` to `\'user789\'`. The resulting dictionary reflects the final state of the context variables.","solution":"import contextvars def record_user_action(initial_context, actions): context_vars = {} tokens = {} # Initialize context variables with the initial context. for key, value in initial_context.items(): context_vars[key] = contextvars.ContextVar(key, default=value) # Perform actions. for action_name, context_var_name, value in actions: if action_name == \'set\': tokens[context_var_name] = context_vars[context_var_name].set(value) elif action_name == \'reset\': context_vars[context_var_name].reset(tokens[context_var_name]) # Generate the final context state. final_state = {key: var.get() for key, var in context_vars.items()} return final_state"},{"question":"# Named Tensors in PyTorch This question aims to evaluate your understanding of PyTorch\'s named tensor feature. You will create named tensors, manipulate their dimensions, and ensure that operations are performed with appropriate name checks and propagations. Task 1. **Create Named Tensors**: - Create a 3-dimensional tensor of size (2, 3, 4) filled with random values, with dimensions named as (\'A\', \'B\', \'C\'). 2. **Manipulate Dimensions**: - Rename the dimensions of the tensor to (\'X\', \'Y\', \'Z\'). - Flatten dimensions \'Y\' and \'Z\' into a single dimension named \'features\'. 3. **Align Dimensions**: - Create another tensor of size (4, 2, 3) with dimensions named as (\'Z\', \'A\', \'Y\'). - Align the second tensor to match the naming order of (\'A\', \'Y\', \'Z\'). Constraints 1. You must use the named tensor functionalities provided by PyTorch. 2. Ensure that any tensor operations you perform propagate the appropriate names to the output tensors. 3. Your final outputs should be the renamed tensor, the flattened tensor, and the aligned second tensor. Expected Input and Output Formats Input: No direct input Output: 1. Three tensors that display the transformations and alignments as specified in the tasks. Example ```python import torch # Step 1: Create Named Tensors tensor = torch.rand(2, 3, 4, names=(\'A\', \'B\', \'C\')) # Step 2.1: Rename Dimensions renamed_tensor = tensor.rename(A=\'X\', B=\'Y\', C=\'Z\') # Step 2.2: Flatten Dimensions flattened_tensor = renamed_tensor.flatten([\'Y\', \'Z\'], \'features\') # Step 3: Align Dimensions another_tensor = torch.rand(4, 2, 3, names=(\'Z\', \'A\', \'Y\')) aligned_tensor = another_tensor.align_to(\'A\', \'Y\', \'Z\') # Output the tensors print(\\"Renamed Tensor:n\\", renamed_tensor) print(\\"Flattened Tensor:n\\", flattened_tensor) print(\\"Aligned Tensor:n\\", aligned_tensor) ``` Output format: ``` Renamed Tensor: tensor([...], names=(\'X\', \'Y\', \'Z\')) Flattened Tensor: tensor([...], names=(\'X\', \'features\')) Aligned Tensor: tensor([...], names=(\'A\', \'Y\', \'Z\')) ```","solution":"import torch # Step 1: Create Named Tensors tensor = torch.rand(2, 3, 4, names=(\'A\', \'B\', \'C\')) # Step 2.1: Rename Dimensions renamed_tensor = tensor.rename(A=\'X\', B=\'Y\', C=\'Z\') # Step 2.2: Flatten Dimensions flattened_tensor = renamed_tensor.flatten([\'Y\', \'Z\'], \'features\') # Step 3: Align Dimensions another_tensor = torch.rand(4, 2, 3, names=(\'Z\', \'A\', \'Y\')) aligned_tensor = another_tensor.align_to(\'A\', \'Y\', \'Z\') # Output the tensors renamed_tensor, flattened_tensor, aligned_tensor"},{"question":"**Objective:** Implement a Python function that simulates the behavior of a simple Python interpreter using the functions described in the `python310` C-API documentation. **Problem Statement:** You are tasked with implementing a Python class `SimplePythonInterpreter` that uses Python\'s C-API to compile and execute Python code. Your class should support the following methods: 1. **`execute_from_file(file_path: str) -> str`**: Reads and executes Python code from the given file. Returns the output of the executed code as a string. 2. **`execute_single_command(command: str) -> str`**: Executes a single Python command provided as a string. Returns the output of the executed command as a string. 3. **`execute_from_string(code: str) -> str`**: Executes multiple Python statements provided as a string. Returns the output of the executed code as a string. **Input and Output:** - `file_path`: A string representing the path to the Python file to be executed. - `command`: A single Python statement in string format. - `code`: A string containing multiple Python statements. **Constraints:** - You must handle the proper initialization and finalization of the Python interpreter when using the C-API functions. - Assume the input strings are valid Python code. - Properly handle any exceptions and return a relevant error message as a string. **Example:** ```python # Example usage interpreter = SimplePythonInterpreter() output1 = interpreter.execute_from_file(\'example.py\') print(output1) # Output from executing example.py output2 = interpreter.execute_single_command(\'print(\\"Hello, World!\\")\') print(output2) # \\"Hello, World!\\" output3 = interpreter.execute_from_string(\'a = 10nb = 20nprint(a + b)\') print(output3) # \\"30\\" ``` **Notes:** - You may utilize external libraries to facilitate interaction with the C-API if needed. - Ensure to provide appropriate error handling for file operations and code execution. Implement the `SimplePythonInterpreter` class to meet the above specifications. ```python class SimplePythonInterpreter: def execute_from_file(self, file_path: str) -> str: # Implementation here pass def execute_single_command(self, command: str) -> str: # Implementation here pass def execute_from_string(self, code: str) -> str: # Implementation here pass # Example usage if __name__ == \'__main__\': interpreter = SimplePythonInterpreter() # Example file execution print(interpreter.execute_from_file(\'example.py\')) # Example single command execution print(interpreter.execute_single_command(\'print(\\"Hello, World!\\")\')) # Example string execution print(interpreter.execute_from_string(\'a = 10nb = 20nprint(a + b)\')) ``` **Performance Considerations:** - Ensure that the interpreter initialization and finalization is done efficiently. - Consider the memory management aspects when handling large files or long strings of code.","solution":"import sys import io class SimplePythonInterpreter: def __init__(self): self.local_env = {} def execute_from_file(self, file_path: str) -> str: try: with open(file_path, \'r\') as file: code = file.read() return self._execute_code(code) except FileNotFoundError: return \\"Error: File not found.\\" except Exception as e: return f\\"Error: {str(e)}\\" def execute_single_command(self, command: str) -> str: return self._execute_code(command) def execute_from_string(self, code: str) -> str: return self._execute_code(code) def _execute_code(self, code: str) -> str: old_stdout = sys.stdout new_stdout = io.StringIO() sys.stdout = new_stdout try: exec(code, {}, self.local_env) output = new_stdout.getvalue() except Exception as e: output = str(e) finally: sys.stdout = old_stdout return output"},{"question":"# Custom Convolutional Neural Network Layer with Adaptive Pooling Objective: The goal of this assignment is to implement a custom PyTorch layer that combines convolution, adaptive pooling, and a non-linear activation. Task: You are required to create a new PyTorch module `CustomConvLayer` that performs the following operations sequentially: 1. **2D Convolution**: Apply a 2D convolution using `torch.nn.functional.conv2d`. 2. **Adaptive Pooling**: Apply adaptive max pooling using `torch.nn.functional.adaptive_max_pool2d` to produce an output of a specified size. 3. **Activation Function**: Apply a non-linear activation function using `torch.nn.functional.relu`. Implementation Details: - You need to implement a class `CustomConvLayer` that inherits from `torch.nn.Module`. - The constructor of `CustomConvLayer` should accept the following parameters: - `in_channels`: Number of input channels. - `out_channels`: Number of output channels. - `kernel_size`: Size of the convolving kernel. - `output_size`: Target output size after adaptive pooling. - Implement the `forward` method to execute the operations defined above. Input: - A 4D tensor of shape `(N, C, H, W)` where `N` is the batch size, `C` is the number of channels, and `H`, and `W` are the height and width of the input feature maps. Output: - A 4D tensor of shape `(N, out_channels, output_size, output_size)` after applying convolution, adaptive pooling, and ReLU activation. Constraints: - The implementation should be efficient and leverage PyTorch\'s built-in functions. - Handle edge cases where `H` or `W` is smaller than `kernel_size`. Example Usage: ```python import torch import torch.nn.functional as F from torch import nn class CustomConvLayer(nn.Module): def __init__(self, in_channels, out_channels, kernel_size, output_size): super(CustomConvLayer, self).__init__() self.in_channels = in_channels self.out_channels = out_channels self.kernel_size = kernel_size self.output_size = output_size def forward(self, x): # Step 1: Apply 2D Convolution weight = torch.randn(self.out_channels, self.in_channels, self.kernel_size, self.kernel_size) bias = torch.randn(self.out_channels) x = F.conv2d(x, weight, bias) # Step 2: Apply Adaptive Max Pooling x = F.adaptive_max_pool2d(x, self.output_size) # Step 3: Apply ReLU Activation x = F.relu(x) return x # Example layer = CustomConvLayer(in_channels=3, out_channels=16, kernel_size=3, output_size=8) input_tensor = torch.randn(1, 3, 32, 32) output_tensor = layer(input_tensor) print(output_tensor.shape) # Expected: torch.Size([1, 16, 8, 8]) ``` **Note**: In your implementation, ensure the convolutional layer\'s weights and biases are correctly learned parameters. Adjust the example given to ensure the layer is functioning as expected.","solution":"import torch import torch.nn.functional as F from torch import nn class CustomConvLayer(nn.Module): def __init__(self, in_channels, out_channels, kernel_size, output_size): super(CustomConvLayer, self).__init__() self.conv = nn.Conv2d(in_channels, out_channels, kernel_size) self.output_size = output_size def forward(self, x): # Step 1: Apply 2D Convolution x = self.conv(x) # Step 2: Apply Adaptive Max Pooling x = F.adaptive_max_pool2d(x, self.output_size) # Step 3: Apply ReLU Activation x = F.relu(x) return x # Example layer = CustomConvLayer(in_channels=3, out_channels=16, kernel_size=3, output_size=8) input_tensor = torch.randn(1, 3, 32, 32) output_tensor = layer(input_tensor) print(output_tensor.shape) # Expected: torch.Size([1, 16, 8, 8])"},{"question":"# Question: Advanced Arithmetic and Collection Operations with Generator Functions Problem Statement You are tasked with implementing a function in Python that computes and returns results based on given conditions using various Python expressions and concepts, including generator functions, list comprehensions, and arithmetic conversions. Task Implement a Python function named `complex_evaluation` that takes three arguments: 1. `num_list` (list of integers) 2. `conversion_factor` (a float number) 3. `threshold` (an integer) The function should perform the following operations: 1. **Arithmetic Conversion and Filter**: - Convert each integer in `num_list` to a float by multiplying with `conversion_factor`. - Filter the converted numbers to retain only those that are greater than or equal to the `threshold`. - Use a list comprehension for this step. 2. **Generator for Alternating Operations**: - Define a generator function within `complex_evaluation` called `alternating_generator` which: - Alternates between yielding the square and the cube of each filtered number. - Use the `yield` expression to return these values. 3. **Return the Results**: - Collect results from the generator into a list by alternating between square and cube until the list is fully traversed. - Return the final list of collected results. Input and Output - **Input**: - `num_list` (list of integers) - `conversion_factor` (float) - `threshold` (integer) - **Output**: - A list of floats where each number results from alternating operations (square and cube) on the filtered, converted numbers. Constraints and Limitations 1. The list comprehension should be used in step 1 to filter and convert numbers. 2. The generator function should correctly alternate between yielding squares and cubes. 3. Performance should be considered in the implementation of the generator and list comprehension. Example **Input**: ```python num_list = [2, 5, 8, 3] conversion_factor = 2.5 threshold = 10 ``` **Step-by-Step Execution**: 1. Convert and filter: - `2 * 2.5 = 5.0` (filtered out) - `5 * 2.5 = 12.5` (kept) - `8 * 2.5 = 20.0` (kept) - `3 * 2.5 = 7.5` (filtered out) - Filtered list: `[12.5, 20.0]` 2. Generator yielding squares and cubes: 1. 12.5^2, 12.5^3 2. 20.0^2, 20.0^3 3. Collecting results: `[156.25, 1953.125, 400.0, 8000.0]` **Output**: ```python [156.25, 1953.125, 400.0, 8000.0] ``` Implementation ```python def complex_evaluation(num_list, conversion_factor, threshold): # List comprehension to convert and filter numbers filtered_numbers = [float(x) * conversion_factor for x in num_list if float(x) * conversion_factor >= threshold] def alternating_generator(nums): for num in nums: yield num ** 2 yield num ** 3 result_generator = alternating_generator(filtered_numbers) collect_results = [] try: while True: collect_results.append(next(result_generator)) except StopIteration: pass return collect_results ```","solution":"def complex_evaluation(num_list, conversion_factor, threshold): Takes a list of integers, a conversion factor, and a threshold. Converts the integers, filters them, and applies alternating square and cube operations through a generator. Returns the results as a list. # List comprehension to convert and filter numbers filtered_numbers = [float(x) * conversion_factor for x in num_list if float(x) * conversion_factor >= threshold] def alternating_generator(nums): for num in nums: yield num ** 2 yield num ** 3 result_generator = alternating_generator(filtered_numbers) collect_results = [] try: while True: collect_results.append(next(result_generator)) except StopIteration: pass return collect_results"},{"question":"You are tasked with analyzing a dataset and visualizing the results using seaborn. The dataset contains information about daily temperature observations from multiple cities over several years. Dataset You will be provided with a CSV file named `temperature.csv` with the following columns: - `Date`: The date of the observation in the format `YYYY-MM-DD`. - `City`: The city where the observation was made. - `Temperature`: The observed temperature in degrees Celsius. Requirements 1. **Data Loading and Preparation** - Load the dataset from the CSV file. - Convert the `Date` column to datetime format. - Extract the month from the `Date` column and add it as a new column named `Month`. - Filter the data to include only observations from the cities \\"New York\\" and \\"Los Angeles\\". 2. **Data Transformation** - Calculate the monthly average temperature for each city. - Create a pivot table with `Month` as the index, `City` as columns, and average `Temperature` as values. 3. **Visualization** - Create a plot using `seaborn.objects` to visualize the average monthly temperatures for both cities. - Add a band to represent the temperature range between the two cities for each month. - Add a line for each city showing the average monthly temperature. Implementation Implement a function `visualize_temperature(file_path: str)` that performs the following steps: 1. Reads the CSV file. 2. Processes the data as described. 3. Creates the plot as specified. The plot should clearly distinguish between the two cities using different colors and should include labels and a title. Constraints - You must use the `seaborn.objects` module for plotting. - You should handle any exceptions that might occur during file loading or data processing gracefully, with appropriate error messages. ```python import seaborn.objects as so import pandas as pd def visualize_temperature(file_path: str): # Load the dataset df = pd.read_csv(file_path) # Convert Date column to datetime df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Extract the month from the Date column df[\'Month\'] = df[\'Date\'].dt.month # Filter data for specific cities df = df[df[\'City\'].isin([\'New York\', \'Los Angeles\'])] # Calculate monthly average temperature monthly_avg_temp = df.groupby([\'Month\', \'City\'])[\'Temperature\'].mean().reset_index() # Create a pivot table pivot_table = monthly_avg_temp.pivot(index=\'Month\', columns=\'City\', values=\'Temperature\') # Reset index to convert Month back to a column pivot_table.reset_index(inplace=True) # Create the plot p = so.Plot(pivot_table, x=\'Month\') p.add(so.Line(color=\'C0\'), y=\'New York\').add(so.Line(color=\'C1\'), y=\'Los Angeles\') p.add(so.Band(alpha=.5, edgewidth=2), ymin=\'New York\', ymax=\'Los Angeles\') p.show() # Example usage: # visualize_temperature(\'temperature.csv\') ```","solution":"import seaborn.objects as so import pandas as pd def visualize_temperature(file_path: str): try: # Load the dataset df = pd.read_csv(file_path) # Convert Date column to datetime df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Extract the month from the Date column df[\'Month\'] = df[\'Date\'].dt.month # Filter data for specific cities df = df[df[\'City\'].isin([\'New York\', \'Los Angeles\'])] # Calculate monthly average temperature monthly_avg_temp = df.groupby([\'Month\', \'City\'])[\'Temperature\'].mean().reset_index() # Create a pivot table pivot_table = monthly_avg_temp.pivot(index=\'Month\', columns=\'City\', values=\'Temperature\') # Reset index to convert Month back to a column pivot_table.reset_index(inplace=True) # Create the plot p = so.Plot(pivot_table, x=\'Month\') p.add(so.Line(color=\'C0\'), y=\'New York\').add(so.Line(color=\'C1\'), y=\'Los Angeles\') p.add(so.Band(alpha=.5, edgewidth=2), ymin=\'New York\', ymax=\'Los Angeles\') p.show() except FileNotFoundError: print(f\\"The file {file_path} was not found.\\") except Exception as e: print(f\\"An error occurred: {e}\\") # Example usage: # visualize_temperature(\'temperature.csv\')"},{"question":"**Asynchronous File Watcher and Processor** You are tasked with creating an asynchronous file watcher and processor using the `asyncio` module, while keeping in mind different platform constraints outlined in the documentation. Your solution must monitor a given directory for new files and process them asynchronously. # Requirements: 1. **Function Implementation:** - **Function Name:** `asyncio_file_watcher` - **Parameters:** - `directory`: A string representing the path to the directory to be monitored. - `process_file`: A coroutine function that takes a file path as input and processes the file. - **Return:** The function should run indefinitely until cancelled. 2. **Constraints:** - The implementation should handle platform-specific limitations (i.e., Windows and macOS) as mentioned in the documentation. - Use a proper event loop for the underlying platform and handle any unsupported methods gracefully. - Ensure that the solution respects the file system\'s event monitoring capabilities of the respective platforms. 3. **Performance:** - The solution should be efficient and capable of monitoring for file events without overwhelming the system resources. 4. **Example Platforms:** - On Windows, use `ProactorEventLoop` and ensure subprocess or signal handling is not relied upon. - On macOS or Unix, the solution should default to a suitable event loop and provide fallback mechanisms for unsupported methods. 5. **Usage Example:** ```python import asyncio import os async def process_file(file_path): await asyncio.sleep(1) # Simulate file processing with an async call print(f\\"Processed {file_path}\\") async def asyncio_file_watcher(directory: str, process_file: callable): # Implementation Here if __name__ == \\"__main__\\": directory = \\"/path/to/watch\\" if not os.path.exists(directory): os.makedirs(directory) try: asyncio.run(asyncio_file_watcher(directory, process_file)) except KeyboardInterrupt: print(\\"File watcher stopped.\\") ``` # Additional Notes: - Ensure that your implementation considers different event loop configurations and limitations, providing alternative approaches for different platforms where necessary. - Handle exceptions and edge cases gracefully.","solution":"import asyncio import os from pathlib import Path import platform async def process_file(file_path): Simulated file processing coroutine. await asyncio.sleep(1) print(f\\"Processed {file_path}\\") async def asyncio_file_watcher(directory: str, process_file: callable): Asynchronous file watcher that monitors a directory for new files and processes them using an asynchronous process_file function. directory = Path(directory) if not directory.exists(): directory.mkdir(parents=True) async def check_for_new_files(): Poll the directory for new files. seen_files = set(directory.iterdir()) while True: await asyncio.sleep(1) current_files = set(directory.iterdir()) new_files = current_files - seen_files for file_path in new_files: if file_path.is_file(): asyncio.create_task(process_file(file_path)) seen_files = current_files await check_for_new_files() if platform.system() == \'Windows\': asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy()) if __name__ == \\"__main__\\": directory = \\"watched_directory\\" try: asyncio.run(asyncio_file_watcher(directory, process_file)) except KeyboardInterrupt: print(\\"File watcher stopped.\\")"},{"question":"**Question: Custom Seaborn Color Palette and Plot** You have been tasked with visualizing data from a given dataset using a custom color palette in seaborn. Your objective is to create a custom light color palette and use it to generate a seaborn plot. Follow the steps below to complete the task. 1. Create a custom light color palette using the following parameters: - Base color: `#FF6347` (hex code for tomato). - Number of colors: 10. - The color specification should be in hex format. 2. Load a dataset of your choice from seaborn\'s built-in datasets. 3. Use the custom color palette created in step 1 to plot a relational plot (e.g., a scatter plot or line plot) using the chosen dataset. **Input Format:** - No input required. You should write a function `custom_seaborn_plot()` which does this task. **Function Signature:** ```python def custom_seaborn_plot() -> None: pass ``` **Constraints:** - Use seaborn and matplotlib for visualization. - The plot should clearly show the mapping of the custom color palette. **Output Format:** - The function should generate and display the plot as specified. **Example:** Here is an example of what the custom color palette creation might look like: ```python import seaborn as sns import matplotlib.pyplot as plt def custom_seaborn_plot(): # Create a custom light color palette with 10 colors palette = sns.light_palette(\\"#FF6347\\", n_colors=10) # Load a dataset (e.g., \'iris\') data = sns.load_dataset(\'iris\') # Create and display a scatter plot using the custom palette sns.scatterplot(data=data, x=\'sepal_length\', y=\'sepal_width\', palette=palette, hue=\'species\') plt.show() ``` **Explanation:** In the example provided, the function `custom_seaborn_plot()` achieves the following: 1. Creates a custom light palette with the color `#FF6347` and 10 different shades. 2. Loads the built-in `iris` dataset. 3. Plots a scatter plot of `sepal_length` vs. `sepal_width` using the custom palette, with different colors representing different species in the dataset. Write the complete function to achieve the task as outlined.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_seaborn_plot(): Creates a custom light color palette and uses it to generate a scatter plot from a Seaborn dataset. # Create a custom light color palette with 10 colors based on #FF6347 palette = sns.light_palette(\\"#FF6347\\", n_colors=10) # Load a dataset (e.g., \'iris\') data = sns.load_dataset(\'iris\') # Create and display a scatter plot using the custom palette scatter_plot = sns.scatterplot(data=data, x=\'sepal_length\', y=\'sepal_width\', palette=palette, hue=\'species\') plt.show()"},{"question":"**Objective**: Demonstrate your comprehension of the `imaplib` package in Python by implementing an email management script. **Problem Statement**: You are tasked with developing a script to interact with an IMAP email server. The script must perform the following operations: 1. **Connect to the IMAP server** using secure SSL (using `IMAP4_SSL` class). 2. **Log in** to the server using provided credentials. 3. **Select the \'INBOX\' mailbox**. 4. **Search for all emails** that contain the word \\"Urgent\\" in the subject. 5. **Mark all found emails as \'Read\'**. 6. **Handle any potential exceptions** during these operations. **Specifications**: - Implement a function `manage_urgent_emails` which takes the following arguments: ```python def manage_urgent_emails(server: str, user: str, password: str) -> None: Connects to an IMAP4 email server and processes emails with the subject \'Urgent\'. Args: server (str): The IMAP server address. user (str): The username for logging into the server. password (str): The password for logging into the server. ``` - `server`: IMAP server address (e.g., \'imap.domain.com\'). - `user`: Username for the server authentication. - `password`: Password for the server authentication. **Constraints**: - You must use `IMAP4_SSL` for the connection. - The search operation should filter emails that have \\"Urgent\\" in the subject. Use the appropriate IMAP search criteria for this. - Marking an email as \'Read\' translates to setting the `Seen` flag. - Ensure proper exception handling throughout the script to manage any operational issues. **Example**: ```python # This is just to show how you can call your function; Actual execution may require valid credentials and server. manage_urgent_emails(\'imap.example.com\', \'user\', \'password\') ``` # Expected Output - The function does not return a value. - All emails containing \\"Urgent\\" in their subject lines should be marked as \'Read\'. **Hints**: - Use the `search` method to find emails with specific criteria. - Use the `store` method to set the `Seen` flag. - Watch out for exceptions like connection issues and invalid credentials, and handle them gracefully.","solution":"import imaplib import email from email.header import decode_header def manage_urgent_emails(server: str, user: str, password: str) -> None: Connects to an IMAP4 email server and processes emails with the subject \'Urgent\'. Args: server (str): The IMAP server address. user (str): The username for logging into the server. password (str): The password for logging into the server. try: # Connect to the server mail = imaplib.IMAP4_SSL(server) # Login to the account mail.login(user, password) # Select the INBOX mail.select(\'inbox\') # Search for emails with \\"Urgent\\" in the subject status, messages = mail.search(None, \'(SUBJECT \\"Urgent\\")\') if status == \\"OK\\": message_ids = messages[0].split() for msg_id in message_ids: # Mark each email as read mail.store(msg_id, \'+FLAGS\', \'Seen\') except Exception as e: print(f\\"An error occurred: {e}\\") finally: try: mail.logout() except: pass"},{"question":"# Question: Implementing and Utilizing Generators in Python You are given a task to create a generator function and a few utility functions to work with generator objects. The goal is to display your understanding of generator functions and generator objects in Python by demonstrating their creation, iteration, and checking. Part 1: Implement the Generator Function Implement a generator function `fibonacci_gen(n)` that yields Fibonacci numbers up to the `n`-th Fibonacci number inclusive. ```python def fibonacci_gen(n): A generator function to yield the Fibonacci sequence up to the n-th number. :param int n: The number of Fibonacci numbers to generate. :yields: The next Fibonacci number in the sequence. :rtype: generator pass # Your code here ``` **Example:** ```python gen = fibonacci_gen(5) print(list(gen)) # Output: [0, 1, 1, 2, 3] ``` Part 2: Implement Utility Functions 1. A function `is_generator(obj)` that returns `True` if `obj` is a generator object and `False` otherwise. ```python def is_generator(obj): Check if the given object is a generator. :param obj: The object to check. :return: True if the object is a generator, False otherwise. :rtype: bool pass # Your code here ``` 2. A function `collect_values(generator_obj, limit)` that collects and returns at most `limit` values from the given generator object. ```python def collect_values(generator_obj, limit): Collect at most \'limit\' values from a generator object and return them in a list. :param generator_obj: The generator object to collect values from. :param int limit: The number of values to collect. :return: A list of values collected from the generator. :rtype: list pass # Your code here ``` **Example:** ```python gen = fibonacci_gen(10) print(is_generator(gen)) # Output: True print(collect_values(gen, 5)) # Output: [0, 1, 1, 2, 3] print(collect_values(fibonacci_gen(6), 10)) # Output: [0, 1, 1, 2, 3, 5] ``` # Constraints - You may assume that the input `n` to the `fibonacci_gen` function is a non-negative integer. - The `collect_values` function should handle the case where the generator has fewer items than the specified limit gracefully. Implement the functions according to the specifications and ensure your code handles edge cases appropriately.","solution":"def fibonacci_gen(n): A generator function to yield the Fibonacci sequence up to the n-th number. :param int n: The number of Fibonacci numbers to generate. :yields: The next Fibonacci number in the sequence. :rtype: generator a, b = 0, 1 for _ in range(n): yield a a, b = b, a + b def is_generator(obj): Check if the given object is a generator. :param obj: The object to check. :return: True if the object is a generator, False otherwise. :rtype: bool return hasattr(obj, \'__iter__\') and not hasattr(obj, \'__len__\') def collect_values(generator_obj, limit): Collect at most \'limit\' values from a generator object and return them in a list. :param generator_obj: The generator object to collect values from. :param int limit: The number of values to collect. :return: A list of values collected from the generator. :rtype: list values = [] try: for _ in range(limit): values.append(next(generator_obj)) except StopIteration: pass return values"},{"question":"# PyTorch Coding Assessment Question **Objective:** Implement a custom autograd function in PyTorch that demonstrates the comprehension of saved tensors, handling in-place operations, and using hooks for debugging purposes. **Task:** 1. **Custom Function Implementation:** - Define a custom autograd function `CustomSquare` that computes the square of an input tensor and records the input for the backward pass to compute the gradient. 2. **In-place Operation:** - Modify the `CustomSquare` function to perform the square operation in-place. The gradient computation should remain correct despite the in-place modification. 3. **Saved Tensor Hook:** - Attach a pair of hooks (`pack_hook` and `unpack_hook`) to the saved tensor in the custom function to demonstrate saved tensor manipulation. 4. **Gradient Hook:** - Register a gradient hook to the output tensor that prints out the gradient whenever it is computed during the backward pass. **Specifications:** 1. **Function Signature:** ```python class CustomSquare(torch.autograd.Function): @staticmethod def forward(ctx, input): # Implement the forward pass pass @staticmethod def backward(ctx, grad_output): # Implement the backward pass pass ``` 2. **In-place Operation Requirement:** - The forward function should perform the operation `input = input ** 2` in-place (use `torch.mul_` or `torch.pow_`). 3. **Saved Tensor Hook:** - Implement hooks to save and restore intermediate values. For example, save the tensor to disk in the `pack_hook` and reload it in the `unpack_hook`. 4. **Gradient Hook:** - Register a gradient hook to the output tensor that prints the gradient during the backward pass. **Constraints:** - Do not use PyTorch\'s built-in operations like `torch.square` directly. - Ensure the in-place operations do not corrupt the gradient computations. - Handle potential exceptions or errors that may arise during tensor operations or file I/O in hooks. **Example Usage:** ```python import torch class CustomSquare(torch.autograd.Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) # Perform in-place operation return input.pow_(2) @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad = 2 * input * grad_output return grad # Test the custom function x = torch.randn(3, requires_grad=True) y = CustomSquare.apply(x) # Register gradient hook def grad_hook(grad): print(\\"Gradient computed: \\", grad) y.register_hook(grad_hook) # Perform backward pass y.sum().backward() ``` **Deliverables:** - The complete implementation of the `CustomSquare` class with the specified requirements. - Demonstration code that shows the function in action, including the registration of hooks, and the correct computation of gradients. **Performance:** - Ensure the custom function is efficient in terms of memory and computations. - Verify the correctness of gradients through unit tests.","solution":"import torch class CustomSquare(torch.autograd.Function): @staticmethod def forward(ctx, input): # Save input tensor for backward computation ctx.save_for_backward(input.clone()) # Perform in-place operation input.pow_(2) return input @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors # The gradient of x**2 w.r.t x is 2*x grad_input = grad_output * 2 * input return grad_input # Demonstration code if __name__ == \\"__main__\\": x = torch.randn(3, requires_grad=True) y = CustomSquare.apply(x) # Register gradient hook def grad_hook(grad): print(\\"Gradient computed: \\", grad) y.register_hook(grad_hook) # Perform backward pass y.sum().backward()"},{"question":"PyTorch Random Exercises You are required to implement a function to generate and manipulate random tensors using the `torch.random` module. This exercise is designed to assess your understanding of different random number generation functionalities in PyTorch. Task Description Implement a function `generate_random_tensor(shape, from_value, to_value, num_random_tensors)` that performs the following tasks: 1. **Generate a List of Random Tensors**: - Create `num_random_tensors` tensors, each with the same `shape`, filled with random numbers uniformly distributed between `from_value` and `to_value`. 2. **Manipulate Random Tensors**: - Compute the sum of all the generated random tensors. - Compute the element-wise product of all the generated random tensors. - Return the list of generated tensors, their sum, and their element-wise product. Your function should have the signature: ```python import torch def generate_random_tensor(shape: tuple, from_value: float, to_value: float, num_random_tensors: int) -> tuple: pass ``` Input Format - `shape`: A tuple representing the shape of each random tensor (e.g., (3, 3)). - `from_value`: A float indicating the lower bound of the random numbers. - `to_value`: A float indicating the upper bound of the random numbers. - `num_random_tensors`: An integer indicating how many random tensors to generate. Output Format A tuple containing: - A list of `num_random_tensors` tensors. - A tensor representing the sum of all the generated random tensors. - A tensor representing the element-wise product of all the generated random tensors. Example ```python # Example input shape = (2, 2) from_value = 1.0 to_value = 5.0 num_random_tensors = 3 # Example function call tensors, sum_tensor, product_tensor = generate_random_tensor(shape, from_value, to_value, num_random_tensors) # Expected Output # tensors: A list of 3 tensors with shape (2, 2) # sum_tensor: A tensor with shape (2, 2) representing the sum of the 3 tensors # product_tensor: A tensor with shape (2, 2) representing the element-wise product of the 3 tensors ``` Constraints 1. `1 <= len(shape) <= 4`: The shape can have up to 4 dimensions. 2. `0 <= from_value < to_value`: From value must be less than the to value. 3. `1 <= num_random_tensors <= 100`: You need to generate at least 1 and at most 100 random tensors. Performance Requirements - You should aim for an efficient solution, both in terms of time and space complexity. - Utilize appropriate PyTorch functionalities for random number generation and tensor manipulation.","solution":"import torch def generate_random_tensor(shape: tuple, from_value: float, to_value: float, num_random_tensors: int) -> tuple: Generate a specified number of random tensors, compute their sum and element-wise product. Args: - shape (tuple): The shape of each random tensor. - from_value (float): The lower bound of the random numbers. - to_value (float): The upper bound of the random numbers. - num_random_tensors (int): The number of random tensors to generate. Returns: - tuple: A tuple containing the list of generated tensors, their sum, and their element-wise product. random_tensors = [] for _ in range(num_random_tensors): tensor = (from_value - to_value) * torch.rand(shape) + to_value random_tensors.append(tensor) # Sum of all the generated random tensors sum_tensor = sum(random_tensors) # Element-wise product of all the generated random tensors product_tensor = torch.ones_like(random_tensors[0]) for tensor in random_tensors: product_tensor *= tensor return random_tensors, sum_tensor, product_tensor"},{"question":"**Problem Statement:** You are working on an application that schedules events with specific start and end times across different time zones. Your task is to implement a function that: 1. Creates a `datetime` object for the start of the event. 2. Creates a `datetime` object for the end of the event. 3. Converts both `datetime` objects to a specific timezone. 4. Calculates the duration of the event in hours and minutes. Your function should demonstrate the creation of `datetime` objects, timezone conversion, and timedelta calculations. **Function Specification:** `def schedule_event(start_time_info: dict, end_time_info: dict, timezone_offset: int) -> str:` **Inputs:** - `start_time_info`: A dictionary with keys `year`, `month`, `day`, `hour`, `minute`, `second`. - `end_time_info`: A dictionary with keys `year`, `month`, `day`, `hour`, `minute`, `second`. - `timezone_offset`: An integer representing the offset from UTC in hours. **Output:** - A string formatted as `\\"{hours} hours, {minutes} minutes\\"` representing the duration of the event. **Constraints:** - The `start_time_info` and `end_time_info` dictionaries will always contain valid date and time information. - The `timezone_offset` will be between -12 and +14. **Example:** ```python start_time_info = { \\"year\\": 2023, \\"month\\": 10, \\"day\\": 15, \\"hour\\": 9, \\"minute\\": 30, \\"second\\": 0 } end_time_info = { \\"year\\": 2023, \\"month\\": 10, \\"day\\": 15, \\"hour\\": 18, \\"minute\\": 45, \\"second\\": 0 } timezone_offset = -4 print(schedule_event(start_time_info, end_time_info, timezone_offset)) # Output: \\"9 hours, 15 minutes\\" ``` **Hints:** 1. Use `datetime.datetime` to create datetime objects. 2. Use `datetime.timezone` and `datetime.timedelta` to handle time zone conversions. 3. Use subtraction of `datetime` objects to calculate duration. 4. Use the `timedelta` object to extract hours and minutes.","solution":"from datetime import datetime, timedelta, timezone def schedule_event(start_time_info: dict, end_time_info: dict, timezone_offset: int) -> str: Calculates the duration of an event considering a specific timezone. Parameters: - start_time_info: dict with keys year, month, day, hour, minute, second - end_time_info: dict with keys year, month, day, hour, minute, second - timezone_offset: int representing the offset from UTC in hours Returns: - str representing the duration in the format \\"{hours} hours, {minutes} minutes\\" # Create datetime objects for start and end time in UTC start_time_utc = datetime( start_time_info[\'year\'], start_time_info[\'month\'], start_time_info[\'day\'], start_time_info[\'hour\'], start_time_info[\'minute\'], start_time_info[\'second\'], tzinfo=timezone.utc ) end_time_utc = datetime( end_time_info[\'year\'], end_time_info[\'month\'], end_time_info[\'day\'], end_time_info[\'hour\'], end_time_info[\'minute\'], end_time_info[\'second\'], tzinfo=timezone.utc ) # Apply the timezone offset offset = timedelta(hours=timezone_offset) start_time = start_time_utc + offset end_time = end_time_utc + offset # Calculate the duration duration = end_time - start_time # Extract hours and minutes from duration duration_hours, remainder = divmod(duration.total_seconds(), 3600) duration_minutes = remainder // 60 return f\\"{int(duration_hours)} hours, {int(duration_minutes)} minutes\\""},{"question":"# Coding Challenge You are given the task of managing data archiving for a project. Specifically, you need to create a program that can create ZIP archives based on certain criteria, and later extract files from these archives. Your program should be capable of: 1. Creating a ZIP archive from a given directory while excluding certain files based on a user-defined filter. 2. Listing the contents of the created ZIP archive. 3. Extracting all files from the ZIP archive to a specified directory. Detailed Requirements: 1. **Function: create_zip_archive** - **Input:** - `directory_path` (str): Path to the directory that needs to be archived. - `zip_filename` (str): The name of the resulting ZIP file. - `filter_func` (function): A function that takes a file path as input and returns `True` if the file should be included in the archive, and `False` otherwise. - `compression` (int): Compression method to be used (either `zipfile.ZIP_STORED`, `zipfile.ZIP_DEFLATED`, `zipfile.ZIP_BZIP2`, or `zipfile.ZIP_LZMA`). - **Output:** - None - **Constraints:** - The directory may contain nested directories. - The program should handle large files efficiently. - Files filtered out by the `filter_func` should not be included in the archive. 2. **Function: list_zip_contents** - **Input:** - `zip_filename` (str): The name of the ZIP file. - **Output:** - `list`: A list of filenames contained in the ZIP archive. 3. **Function: extract_zip_archive** - **Input:** - `zip_filename` (str): The name of the ZIP file. - `extract_to_path` (str): Directory where the files should be extracted. - **Output:** - None - **Constraints:** - Ensure safe extraction by making sure extracted paths do not lead outside the specified directory. Implementation: ```python import zipfile import os def create_zip_archive(directory_path, zip_filename, filter_func, compression=zipfile.ZIP_DEFLATED): with zipfile.ZipFile(zip_filename, \'w\', compression) as zf: for root, _, files in os.walk(directory_path): for file in files: file_path = os.path.join(root, file) if filter_func(file_path): archive_name = os.path.relpath(file_path, start=directory_path) zf.write(file_path, arcname=archive_name) def list_zip_contents(zip_filename): with zipfile.ZipFile(zip_filename, \'r\') as zf: return zf.namelist() def extract_zip_archive(zip_filename, extract_to_path): with zipfile.ZipFile(zip_filename, \'r\') as zf: zf.extractall(extract_to_path) # Example usage: # Define a filter function to exclude certain files (e.g., files with .tmp extension) def exclude_tmp_files(file_path): return not file_path.endswith(\'.tmp\') # Create ZIP archive create_zip_archive(\'path_to_directory\', \'archive.zip\', exclude_tmp_files) # List contents of ZIP archive print(list_zip_contents(\'archive.zip\')) # Extract contents of ZIP archive extract_zip_archive(\'archive.zip\', \'path_to_extract_directory\') ``` Notes: - Include proper error handling and edge cases in your implementation. - Write tests to validate the functionality of your functions.","solution":"import zipfile import os def create_zip_archive(directory_path, zip_filename, filter_func, compression=zipfile.ZIP_DEFLATED): Create a ZIP archive of a directory, excluding files based on a filter function. :param directory_path: Path to the directory to be archived :param zip_filename: Name of the ZIP file to be created :param filter_func: Function that takes a file path as input and returns True if the file should be included, else False :param compression: Compression method to be used (default is zipfile.ZIP_DEFLATED) with zipfile.ZipFile(zip_filename, \'w\', compression) as zf: for root, _, files in os.walk(directory_path): for file in files: file_path = os.path.join(root, file) if filter_func(file_path): archive_name = os.path.relpath(file_path, start=directory_path) zf.write(file_path, arcname=archive_name) def list_zip_contents(zip_filename): List the contents of a ZIP archive. :param zip_filename: Name of the ZIP file :return: List of filenames contained in the ZIP archive with zipfile.ZipFile(zip_filename, \'r\') as zf: return zf.namelist() def extract_zip_archive(zip_filename, extract_to_path): Extract all files from a ZIP archive to a specified directory. :param zip_filename: Name of the ZIP file :param extract_to_path: Directory where the files should be extracted with zipfile.ZipFile(zip_filename, \'r\') as zf: zf.extractall(extract_to_path)"},{"question":"**Python Coding Assessment Question:** # Task You are required to write a Python function that generates a MIME email with an HTML body and multiple attachments. Specifically, the function will take input parameters for the sender, recipient, subject, HTML content, and a list of file paths for attachments. The function should correctly construct the email with the specified attributes and save the email to a file. # Function Signature ```python def create_mime_email(sender: str, recipient: str, subject: str, html_content: str, file_paths: list, output_file: str): pass ``` # Input - `sender` (str): The email address of the sender. - `recipient` (str): The email address of the recipient. - `subject` (str): The subject of the email. - `html_content` (str): The HTML content of the email body. - `file_paths` (list): A list of file paths to be attached to the email. - `output_file` (str): The path to the file where the email will be saved. # Output The function does not return any values but saves the constructed MIME email message to the `output_file`. # Constraints - Implement all necessary error handling to deal with invalid input, such as non-existent file paths or empty string values for required parameters. - Make sure that all file attachments are properly encoded and included in the email. - The email should be constructed according to the MIME standards. # Example Usage ```python create_mime_email( sender=\\"alice@example.com\\", recipient=\\"bob@example.com\\", subject=\\"Here are the documents\\", html_content=\\"<h1>Documents</h1><p>Please find the attached documents.</p>\\", file_paths=[\\"/path/to/doc1.pdf\\", \\"/path/to/img1.png\\"], output_file=\\"output_email.eml\\" ) ``` In this example, the function will create an email with the specified HTML content and attach the PDF and PNG files. The generated email will be saved to `output_email.eml`. # Additional Requirements 1. Document the function with clear docstrings. 2. Ensure the email is properly formatted according to MIME standards. 3. The function should be compatible with Python 3.10 and above. # Hints - Utilize the `email.message.EmailMessage` class to construct the email. - Use `email.policy.SMTP` for consistent formatting and handling of MIME types. - Handle attachments using the appropriate MIME type and subtype based on the file content.","solution":"import os from email.message import EmailMessage import mimetypes def create_mime_email(sender: str, recipient: str, subject: str, html_content: str, file_paths: list, output_file: str): Generates a MIME email with an HTML body and multiple attachments. Parameters: sender (str): The email address of the sender. recipient (str): The email address of the recipient. subject (str): The subject of the email. html_content (str): The HTML content of the email body. file_paths (list): A list of file paths to be attached to the email. output_file (str): The path to the file where the email will be saved. if not sender or not recipient or not subject or not html_content or not output_file: raise ValueError(\\"All parameters are required and cannot be empty.\\") # Initialize the EmailMessage object msg = EmailMessage() msg[\'From\'] = sender msg[\'To\'] = recipient msg[\'Subject\'] = subject msg.set_content(\'This is a MIME-formatted message.\') # Plain text alternative msg.add_alternative(html_content, subtype=\'html\') # HTML content # Attach files for file_path in file_paths: if not os.path.isfile(file_path): raise FileNotFoundError(f\\"Attachment file {file_path} not found.\\") # Guess the content type based on the file\'s extension mime_type, _ = mimetypes.guess_type(file_path) if mime_type is None: mime_type = \'application/octet-stream\' maintype, subtype = mime_type.split(\'/\', 1) with open(file_path, \'rb\') as file: file_data = file.read() file_name = os.path.basename(file_path) msg.add_attachment(file_data, maintype=maintype, subtype=subtype, filename=file_name) # Save the message to output_file with open(output_file, \'wb\') as f: f.write(bytes(msg))"},{"question":"File Handling in Python Using Custom C Wrappers # Objective: Write a Python function to simulate some of the functionalities described in the provided C API documentation using the `io` module. The goal is to demonstrate your grasp of Python\'s file handling and mimic the behavior of the C API functions. # Task: Write a Python class `FileHandler` with the following methods: 1. **from_fd**: - **Input**: An integer `fd` representing a file descriptor, and an optional mode (default: \\"r\\"). - **Output**: A file object. - **Description**: Create a file object from a given file descriptor. If the file descriptor is invalid, raise an appropriate exception. 2. **get_file_descriptor**: - **Input**: A file object. - **Output**: The file descriptor associated with the file object. - **Description**: Retrieve the file descriptor from a given file object. If the provided object is not a file, raise an appropriate exception. 3. **get_line**: - **Input**: A file object and an optional integer `n` (default: 0). - **Output**: A string representing one line from the file. - **Description**: Read one line from the given file object. If `n` is provided and greater than 0, read up to `n` bytes. Handle the end of file (EOF) appropriately. 4. **write_object**: - **Input**: A Python object and a file object. - **Output**: None. - **Description**: Write the `str()` representation of the given Python object to the file object. Handle exceptions appropriately. 5. **write_string**: - **Input**: A string and a file object. - **Output**: None. - **Description**: Write the given string to the file object. Handle exceptions appropriately. # Constraints: - Do not use the C API directly. Use Python\'s `io` module and standard library. - Ensure proper error handling and resource management (e.g., closing files). - Aim for clear, efficient, and Pythonic code. # Example Usage: ```python import os import tempfile # Example for from_fd and get_file_descriptor fh = FileHandler() fd, path = tempfile.mkstemp() try: file_obj = fh.from_fd(fd, \'r+\') descriptor = fh.get_file_descriptor(file_obj) print(f\\"File Descriptor: {descriptor}\\") # Should print the original fd # Example for write_string and get_line fh.write_string(\\"Hello, world!nThis is a test.n\\", file_obj) file_obj.seek(0) # Move to the start of the file print(fh.get_line(file_obj)) # Should print \\"Hello, world!n\\" print(fh.get_line(file_obj, 4)) # Should print \\"This\\" # Example for write_object fh.write_object(123, file_obj) file_obj.seek(0, os.SEEK_END) # Move to the end of the file fh.write_object({\'key\': \'value\'}, file_obj) finally: os.close(fd) os.remove(path) ```","solution":"import os import io class FileHandler: @staticmethod def from_fd(fd, mode=\'r\'): Create a file object from a given file descriptor. Raise an exception if the file descriptor is invalid. try: # Using os.fdopen to get a file object from the file descriptor file_obj = os.fdopen(fd, mode) return file_obj except Exception as e: raise ValueError(f\\"Invalid file descriptor: {e}\\") @staticmethod def get_file_descriptor(file_obj): Retrieve the file descriptor from a given file object. Raise an exception if the object is not a file. if not isinstance(file_obj, io.IOBase): raise ValueError(\\"Provided object is not a file object\\") return file_obj.fileno() @staticmethod def get_line(file_obj, n=0): Read one line from the given file object. If `n` is provided and greater than 0, read up to `n` bytes. Handle the end of file (EOF) appropriately. if not isinstance(file_obj, io.IOBase): raise ValueError(\\"Provided object is not a file object\\") if n > 0: return file_obj.read(n) else: return file_obj.readline() @staticmethod def write_object(obj, file_obj): Write the `str()` representation of the given Python object to the file object. Handle exceptions appropriately. if not isinstance(file_obj, io.IOBase): raise ValueError(\\"Provided object is not a file object\\") try: file_obj.write(str(obj)) except Exception as e: raise ValueError(f\\"Error writing object to file: {e}\\") @staticmethod def write_string(string, file_obj): Write the given string to the file object. Handle exceptions appropriately. if not isinstance(file_obj, io.IOBase): raise ValueError(\\"Provided object is not a file object\\") if not isinstance(string, str): raise ValueError(\\"Provided string is not of type str\\") try: file_obj.write(string) except Exception as e: raise ValueError(f\\"Error writing string to file: {e}\\")"},{"question":"**Objective:** Write a function that performs a sequence of operations using different PyTorch functionalities to demonstrate your comprehension of tensor manipulation, mathematical operations, and handling gradients. **Problem Statement:** You are given an input tensor `input_tensor` and you need to perform the following operations sequentially: 1. **Normalization**: Normalize the input tensor so that its values are scaled between 0 and 1. 2. **Random Noise Addition**: Add random noise to the normalized tensor. The noise should be drawn from a normal distribution with mean 0 and standard deviation 0.1. 3. **Element-wise Operations**: Apply the sine function to each element of the noise-added tensor and then compute the square root of each resulting element. 4. **Sum of Elements**: Compute the sum of all elements in the resulting tensor after the element-wise operations. 5. **Gradient Calculation**: Compute the gradient of the sum with respect to the normalized tensor without noise. **Function Signature:** ```python import torch def process_tensor(input_tensor: torch.Tensor) -> torch.Tensor: Perform a sequence of operations on input_tensor as described. Args: - input_tensor (torch.Tensor): The input tensor to be processed Returns: - torch.Tensor: The final tensor after performing all operations ``` **Requirements:** 1. Normalize the input tensor such that the minimum value is 0 and the maximum value is 1. 2. Add random noise from a normal distribution (mean=0, std=0.1) to this normalized tensor. Ensure reproducibility by setting the random seed to a fixed value (e.g., 42). 3. Apply the sine function to each element of the noisy tensor and then compute the square root of each element. 4. Compute the sum of all elements in the tensor obtained from step 3. 5. Calculate the gradient of this sum with respect to the normalized tensor (before adding noise). 6. Return the final tensor from step 3. **Constraints:** - Assume `input_tensor` is a 2D tensor of floating-point numbers. - Implement the function using PyTorch specific methods and operations. - Handle any potential errors associated with invalid operations, such as taking the square root of negative numbers. **Example Usage:** ```python input_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32) output_tensor = process_tensor(input_tensor) print(output_tensor) # Output will vary due to the random noise addition, but the function must be able to handle all operations correctly. ``` **Note:** - Ensure your code is well-structured and efficient. - Test your function with different input tensors to verify its correctness and handle edge cases appropriately.","solution":"import torch def process_tensor(input_tensor: torch.Tensor) -> torch.Tensor: Perform a sequence of operations on input_tensor as described. Args: - input_tensor (torch.Tensor): The input tensor to be processed Returns: - torch.Tensor: The final tensor after performing all operations # Ensure the input_tensor requires gradients input_tensor.requires_grad_(True) # Step 1: Normalize the input tensor min_val = input_tensor.min() max_val = input_tensor.max() normalized_tensor = (input_tensor - min_val) / (max_val - min_val) # Step 2: Add random noise torch.manual_seed(42) noise = torch.normal(mean=0.0, std=0.1, size=normalized_tensor.shape) noisy_tensor = normalized_tensor + noise # Step 3: Apply the sine function and then compute the square root sine_tensor = torch.sin(noisy_tensor) sqrt_tensor = torch.sqrt(torch.relu(sine_tensor)) # Step 4: Compute the sum of all elements sum_elements = sqrt_tensor.sum() # Step 5: Compute the gradient of the sum with respect to the normalized tensor sum_elements.backward() # Return the final tensor after the element-wise operations return sqrt_tensor.detach()"},{"question":"# Gaussian Process Exercise Objective Demonstrate understanding of Gaussian Processes (GP) in scikit-learn, focusing on Gaussian Process Regression (GPR) and the use of custom kernels. Problem Statement You have been given a dataset containing house prices (`target`) along with various features (`input`). Your task is to build a Gaussian Process Regression model using custom kernels to predict house prices. The model should also provide the standard deviation of the predictions to indicate confidence levels. Requirements 1. **Kernel Desing**: Define a custom kernel composed of an RBF kernel and a WhiteKernel. The RBF kernel should account for smooth variations, and the WhiteKernel should model noise. 2. **Model Implementation**: Implement the `GaussianProcessRegressor` using the custom kernel. 3. **Model Training**: Train the model using the provided training data. 4. **Prediction**: Generate predictions and associated standard deviations for a test dataset. 5. **Performance Evaluation**: Evaluate model performance using Mean Squared Error (MSE) and visualize confidence intervals. Input and Output Formats - **Input**: - `X_train.csv`: CSV file containing training data features. - `y_train.csv`: CSV file containing training data target values. - `X_test.csv`: CSV file containing test data features. - **Output**: - Predictions and standard deviations for the test data to a CSV file `predictions.csv` with the following format: ``` prediction, std_dev 100000, 5000 150000, 6000 ... Constraints - The kernel should be a composite of RBF kernel and a WhiteKernel. - The model should be trained and predictions should be generated using the GaussianProcessRegressor. - Ensure the code is optimized for performance and is well documented. Steps to Solve 1. Load the datasets. 2. Define and initialize the custom kernel. 3. Create and train the Gaussian Process Regression model. 4. Predict house prices and standard deviations for test data. 5. Evaluate performance using MSE and visualize predictions with confidence intervals. Code Template ```python import numpy as np import pandas as pd from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF, WhiteKernel from sklearn.metrics import mean_squared_error import matplotlib.pyplot as plt # Load datasets X_train = pd.read_csv(\'X_train.csv\') y_train = pd.read_csv(\'y_train.csv\') X_test = pd.read_csv(\'X_test.csv\') # Define custom kernel kernel = RBF(length_scale=1.0) + WhiteKernel(noise_level=1.0) # Create GaussianProcessRegressor model gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, normalize_y=True) # Train the model gpr.fit(X_train, y_train) # Predict using the model y_pred, y_std = gpr.predict(X_test, return_std=True) # Evaluate the performance mse = mean_squared_error(y_true, y_pred) print(f\'Mean Squared Error: {mse}\') # Visualize predictions with confidence intervals plt.figure(figsize=(10, 5)) plt.plot(range(len(y_pred)), y_pred, \'r-\', label=\'Predicted\') plt.fill_between(range(len(y_pred)), y_pred - y_std, y_pred + y_std, alpha=0.2, color=\'r\') plt.xlabel(\'Sample Index\') plt.ylabel(\'House Price\') plt.title(\'Predictions with Confidence Intervals\') plt.legend() plt.show() # Save predictions and standard deviations predictions_df = pd.DataFrame({\\"prediction\\": y_pred, \\"std_dev\\": y_std}) predictions_df.to_csv(\'predictions.csv\', index=False) ``` Deliverables - A Python script implementing the described functionality. - `predictions.csv` containing the predictions and their standard deviations. - A brief report discussing the performance of your model (based on MSE) and an overview of the visualization.","solution":"import numpy as np import pandas as pd from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF, WhiteKernel from sklearn.metrics import mean_squared_error import matplotlib.pyplot as plt def load_data(X_train_path, y_train_path, X_test_path): X_train = pd.read_csv(X_train_path) y_train = pd.read_csv(y_train_path).values.ravel() # Flatten to 1D array if necessary X_test = pd.read_csv(X_test_path) return X_train, y_train, X_test def train_gpr(X_train, y_train): # Define custom kernel kernel = RBF(length_scale=1.0) + WhiteKernel(noise_level=1.0) # Create GaussianProcessRegressor model gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, normalize_y=True) # Train the model gpr.fit(X_train, y_train) return gpr def predict_gpr(gpr, X_test): # Predict using the model y_pred, y_std = gpr.predict(X_test, return_std=True) return y_pred, y_std def evaluate_performance(y_true, y_pred): mse = mean_squared_error(y_true, y_pred) return mse def save_predictions(pred_file, y_pred, y_std): predictions_df = pd.DataFrame({\\"prediction\\": y_pred, \\"std_dev\\": y_std}) predictions_df.to_csv(pred_file, index=False) def plot_predictions_with_confidence_intervals(y_pred, y_std): plt.figure(figsize=(10, 5)) plt.plot(range(len(y_pred)), y_pred, \'r-\', label=\'Predicted\') plt.fill_between(range(len(y_pred)), y_pred - y_std, y_pred + y_std, alpha=0.2, color=\'r\') plt.xlabel(\'Sample Index\') plt.ylabel(\'House Price\') plt.title(\'Predictions with Confidence Intervals\') plt.legend() plt.show()"},{"question":"# Question: Creating and Applying Custom Color Palettes with Seaborn Objective: Demonstrate your understanding of creating and using custom color palettes with seaborn by implementing a function that generates a custom color palette and applies it to a seaborn plot. Task: 1. Write a function `create_custom_palette_and_plot` that takes three arguments: - `colors_list`: A list of colors to be used in creating the palette. The colors can be in any format accepted by seaborn (e.g., color names, hex codes). - `continuous`: A boolean flag that indicates whether to return a continuous colormap (default is `False`). - `dataset`: The name of the seaborn dataset to use for creating the plot (default is `\\"iris\\"`). 2. The function should do the following: - Create a color palette using `sns.blend_palette` with the given `colors_list`. - If `continuous` is `True`, create a continuous colormap. - Load the specified `dataset` using `sns.load_dataset`. - Create a seaborn `scatterplot` using the dataset, with `sepal_length` as the x-axis and `sepal_width` as the y-axis, and apply the custom palette to the plot. - Display the plot with a title indicating whether it is using a discrete palette or a continuous colormap. Examples: ```python def create_custom_palette_and_plot(colors_list, continuous=False, dataset=\\"iris\\"): # Your code here # Example usage: colors = [\\"#45a872\\", \\"#f0a500\\", \\"xkcd:sky blue\\"] create_custom_palette_and_plot(colors) ``` This should generate a scatter plot of the Iris dataset with a custom discrete color palette. ```python colors = [\\"#bdc\\", \\"#7b9\\", \\"#47a\\"] create_custom_palette_and_plot(colors, continuous=True) ``` This should generate a scatter plot of the Iris dataset with a custom continuous colormap. Constraints: - You must use seaborn and matplotlib libraries for plotting. - Ensure the plot is appropriately labeled and visually clear. - Handle edge cases where the dataset name provided does not exist. Input and Output Formats: - Input: `colors_list` is a list of strings representing colors, `continuous` is a boolean, and `dataset` is a string. - Output: Display the generated seaborn plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_palette_and_plot(colors_list, continuous=False, dataset=\\"iris\\"): Create and apply a custom color palette to a seaborn plot. Parameters: colors_list (list): List of colors to be used in creating the palette. continuous (bool): Indicates whether to return a continuous colormap (default is False). dataset (str): The name of the seaborn dataset to use for creating the plot (default is \\"iris\\"). try: # Create a custom color palette if continuous: palette = sns.color_palette(\\"blend\\", colors_list) cmap = sns.blend_palette(colors_list, as_cmap=True) else: palette = sns.blend_palette(colors_list) # Load the specified dataset data = sns.load_dataset(dataset) # Create a scatter plot with the custom palette plt.figure(figsize=(10, 6)) scatter = sns.scatterplot( x=data[\\"sepal_length\\"], y=data[\\"sepal_width\\"], hue=data[\\"species\\"], palette=cmap if continuous else palette, legend=\\"full\\" ) # Apply title to the plot title = \\"Continuous Colormap\\" if continuous else \\"Discrete Palette\\" plt.title(f\\"Iris Dataset with {title}\\") # Display plot plt.show() except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Objective: You are required to implement a Python function that analyzes a list of travel packages, each described by a dictionary, and groups them by a destination. To demonstrate your understanding of control flow constructs in Python, you need to perform destination-wise grouping using pattern matching, conditional statements, and error handling. Input: - A list of dictionaries, each representing a travel package. Each dictionary has the following keys: - `package_id` (int) - `destination` (str) - `price` (float) - `details` (dict) Output: - A dictionary where each key is a destination, and the associated value is a list of package IDs for that destination. Constraints: - Every dictionary in the input list will have the specified keys. - `package_id` will be a positive integer. - `destination` will be a non-empty string. - `price` will be a non-negative float. - `details` will be a dictionary that can contain any additional information about the package. Instructions: 1. Implement error handling to manage unforeseen input errors. 2. Use pattern matching to process the structure of each travel package. 3. Use suitable control flow constructs to iterate and group the packages. 4. Ensure the output dictionary is sorted by destination names alphabetically. Example: ```python def group_travel_packages(packages): pass input_data = [ {\\"package_id\\": 1, \\"destination\\": \\"Paris\\", \\"price\\": 1200.50, \\"details\\": {\\"nights\\": 5, \\"hotel\\": \\"Hotel Paris\\"}}, {\\"package_id\\": 2, \\"destination\\": \\"London\\", \\"price\\": 1500.00, \\"details\\": {\\"nights\\": 7, \\"hotel\\": \\"Royal London\\"}}, {\\"package_id\\": 3, \\"destination\\": \\"Paris\\", \\"price\\": 1199.99, \\"details\\": {\\"nights\\": 4, \\"hotel\\": \\"Eiffel View\\"}}, {\\"package_id\\": 4, \\"destination\\": \\"New York\\", \\"price\\": 1600.00, \\"details\\": {\\"nights\\": 6, \\"hotel\\": \\"NY Tower\\"}}, {\\"package_id\\": 5, \\"destination\\": \\"London\\", \\"price\\": 1300.00, \\"details\\": {\\"nights\\": 5, \\"hotel\\": \\"London Bridge\\"}} ] # Expected Output: # { # \\"London\\": [2, 5], # \\"New York\\": [4], # \\"Paris\\": [1, 3] # } print(group_travel_packages(input_data)) ``` **Function Signature:** ```python def group_travel_packages(packages: list) -> dict: ``` **Notes:** - Ensure to demonstrate the use of `try`, `if`, `for`, `with`, or `match` statements as applicable. - Handle any malformed input gracefully using appropriate error handling techniques. - Ensure the function aligns with Python 3.10 enhancements, especially pattern matching.","solution":"def group_travel_packages(packages: list) -> dict: Groups travel packages by destination. Args: packages (list): List of dictionaries, where each dictionary represents a travel package Returns: dict: A dictionary where each key is a destination and the value is a list of package IDs for that destination from collections import defaultdict grouped_packages = defaultdict(list) if not isinstance(packages, list): raise ValueError(\\"Input should be a list\\") for package in packages: try: # Pattern matching using if-else construct as Python 3.10 match-case is limited here if isinstance(package, dict): package_id = package.get(\\"package_id\\") destination = package.get(\\"destination\\") if isinstance(package_id, int) and isinstance(destination, str) and destination: grouped_packages[destination].append(package_id) else: raise ValueError(\\"Malformed package data\\") else: raise ValueError(\\"Each package should be a dictionary\\") except Exception as e: print(f\\"Error processing package {package}: {e}\\") continue # Sorting the dictionary by destination names alphabetically return dict(sorted(grouped_packages.items()))"},{"question":"# Asynchronous Task Dispatcher using `asyncio.Queue` Problem Statement: You are required to implement an asynchronous task dispatcher system using `asyncio.Queue`. The dispatcher should manage and process tasks concurrently using multiple worker tasks. Each task consists of a priority number and a string representing the task content. The dispatcher should ensure tasks are handled in the order of their priority (i.e., using `asyncio.PriorityQueue`). Task Requirements: 1. **Input**: - A list of tuples where each tuple contains a priority number (integer) and a task content (string). - The number of worker tasks (integer). 2. **Output**: - Print the task content when a worker processes a task. 3. **Function Signature**: ```python async def task_dispatcher(tasks: List[Tuple[int, str]], num_workers: int) -> None: ``` 4. **Constraints**: - The priority number is a non-negative integer. - There are at most `1000` tasks. - There are at most `10` workers. 5. **Behavior**: - Create an `asyncio.PriorityQueue` to store the tasks. - Workers should process tasks concurrently based on their priority. - Print the worker name and the task content when a worker processes a task. - Use the `task_done()` method to signal task completion. - Ensure all tasks are processed before the function exits. - Handle scenarios where tasks might finish at different times. Example: ```python import asyncio from typing import List, Tuple async def worker(name: str, queue: asyncio.PriorityQueue): while True: priority, task_content = await queue.get() await asyncio.sleep(1) # Simulate task processing with sleep print(f\\"{name} processed task: {task_content} with priority {priority}\\") queue.task_done() async def task_dispatcher(tasks: List[Tuple[int, str]], num_workers: int) -> None: queue = asyncio.PriorityQueue() for task in tasks: queue.put_nowait(task) worker_coroutines = [worker(f\\"worker-{i}\\", queue) for i in range(num_workers)] await asyncio.gather(*worker_coroutines) await queue.join() # Block until all tasks are processed # Example usage tasks = [(2, \\"Write report\\"), (1, \\"Complete code review\\"), (3, \\"Respond to emails\\")] num_workers = 2 asyncio.run(task_dispatcher(tasks, num_workers)) ``` In the example above, the function creates a priority queue and launches worker tasks to process the items concurrently. Adjust the number of workers as necessary to see the system handle tasks at different priorities. Explanation: - **Priority Queue**: Ensures tasks are processed based on their priority. - **Worker Coroutine**: Simulates task processing and prints the task content once processed. - **Task Dispatcher**: Initializes the priority queue, populates it with tasks, and launches worker coroutines to process the queue. Implementing this system will demonstrate your understanding of using `asyncio` for asynchronous task management, managing priority queues, and handling concurrency.","solution":"import asyncio from typing import List, Tuple async def worker(name: str, queue: asyncio.PriorityQueue): while True: priority, task_content = await queue.get() await asyncio.sleep(1) # Simulate task processing with sleep print(f\\"{name} processed task: {task_content} with priority {priority}\\") queue.task_done() async def task_dispatcher(tasks: List[Tuple[int, str]], num_workers: int) -> None: queue = asyncio.PriorityQueue() for task in tasks: queue.put_nowait(task) worker_coroutines = [worker(f\\"worker-{i}\\", queue) for i in range(num_workers)] await asyncio.gather(*worker_coroutines) await queue.join() # Block until all tasks are processed"},{"question":"**Question: Implement a Custom Importer and Module Manager** You are required to demonstrate your understanding of the `importlib` module by designing and implementing a custom module importer and manager. **Objective**: Implement a custom importer that can import modules from a specified directory and manage these modules by providing functionalities like reloading a module and listing all modules in the specified directory. # Part 1: Custom Module Importer 1. **Create a Class `CustomImporter`**: - `CustomImporter` should inherit from `importlib.abc.MetaPathFinder` and `importlib.abc.Loader`. - Implement the `find_spec` method to locate modules in the specified directory. - Implement the `create_module` and `exec_module` methods in the `CustomImporter` class. 2. **Methods to Implement**: - `find_spec(fullname, path, target=None)`: Locate the specified module and return a module spec. - `create_module(spec)`: Create and return a new module object (use default module creation). - `exec_module(module)`: Load and execute the specified module. # Part 2: Module Manager 3. **Create a Class `ModuleManager`**: - The `ModuleManager` class should initialize with a directory path and set up the custom importer for that directory. - Implement methods to import a module by name, reload an imported module, and list all modules available in the directory. 4. **Methods to Implement**: - `add_module(name)`: Use the custom importer to import a module by name. - `reload_module(name)`: Reload a previously imported module. - `list_modules()`: List all available modules in the specified directory. # Constraints and Requirements - The `find_spec` method should ensure the module file exists in the specified directory and return `None` otherwise. - Maintain performance efficiency, especially in functions like `list_modules()`. - Ensure proper error handling for cases where a module is not found or cannot be reloaded. - Use the `importlib` module’s functionalities wherever applicable. # Example Usage ```python # Initialize the module manager for a given directory manager = ModuleManager(\'/path/to/modules\') # Add and import a module named \'mymodule\' manager.add_module(\'mymodule\') # List all available modules in the directory print(manager.list_modules()) # Reload the module named \'mymodule\' manager.reload_module(\'mymodule\') ``` # Deliverables 1. Implementation of `CustomImporter` class. 2. Implementation of `ModuleManager` class. 3. A script demonstrating the usage of `ModuleManager` to import, reload, and list modules. **Note**: Ensure your code is well-documented and includes comments explaining the key parts of the implementation.","solution":"import importlib.abc import importlib.util import importlib.machinery import sys import os class CustomImporter(importlib.abc.MetaPathFinder, importlib.abc.Loader): def __init__(self, directory): self.directory = directory def find_spec(self, fullname, path, target=None): module_name = fullname.split(\'.\')[-1] module_file_path = os.path.join(self.directory, f\\"{module_name}.py\\") if not os.path.isfile(module_file_path): return None spec = importlib.util.spec_from_file_location(fullname, module_file_path, loader=self) return spec def create_module(self, spec): # Use default module creation semantics return None def exec_module(self, module): spec = module.__spec__ module_file_path = spec.origin with open(module_file_path, \'r\') as f: code = f.read() exec(code, module.__dict__) class ModuleManager: def __init__(self, directory): self.directory = directory self.importer = CustomImporter(directory) self._modules = {} sys.meta_path.insert(0, self.importer) def add_module(self, name): if name in self._modules: return self._modules[name] module = importlib.import_module(name) self._modules[name] = module return module def reload_module(self, name): if name not in self._modules: raise ImportError(f\\"Module \'{name}\' is not imported to be reloaded.\\") module = importlib.reload(self._modules[name]) self._modules[name] = module return module def list_modules(self): return [f[:-3] for f in os.listdir(self.directory) if f.endswith(\'.py\')]"},{"question":"**Problem Statement: Design and Implement a Text Editor with Undo/Redo Functionality** Using the Python `collections.deque` module, implement a simple command-line text editor that supports basic text editing operations along with undo and redo functionality. # Function Signatures You will need to implement the following functions: 1. `perform_operation(editor: TextEditor, operation: str, argument: Optional[str] = None) -> None`: - Performs the given operation on the `TextEditor` instance. 2. `get_text(editor: TextEditor) -> str`: - Returns the current content of the editor as a string. # Class Definition Define a class `TextEditor` to encapsulate the text editing data and functions. The class should at minimum support the following methods: 1. `insert(position: int, text: str) -> None`: - Insert `text` at the specified `position`. 2. `delete(start: int, end: int) -> None`: - Delete text from `start` to `end` (exclusive). 3. `undo() -> None`: - Undo the last operation. 4. `redo() -> None`: - Redo the operation undone by the last `undo`. # Input and Output - **Insert Operation:** - `operation = \\"insert\\"`, `argument = \\"position,text\\"` where `position` is an integer, and `text` is the string to be inserted. - Example: `\\"insert 5,Hello\\"` - **Delete Operation:** - `operation = \\"delete\\"`, `argument = \\"start,end\\"` where `start` and `end` are integers. - Example: `\\"delete 5,10\\"` - **Undo Operation:** - `operation = \\"undo\\"` - Example: `\\"undo\\"` - **Redo Operation:** - `operation = \\"redo\\"` - Example: `\\"redo\\"` # Example Usage ```python editor = TextEditor() # Perform insertions perform_operation(editor, \\"insert\\", \\"0,Hello\\") perform_operation(editor, \\"insert\\", \\"5, World\\") # Perform deletion perform_operation(editor, \\"delete\\", \\"5,11\\") # Undo deletion perform_operation(editor, \\"undo\\") # Redo deletion perform_operation(editor, \\"redo\\") # Get current text print(get_text(editor)) # Output should be: \\"Hello\\" ``` # Constraints 1. The text position index is 0-based. 2. The `undo` and `redo` operations should handle any previous operations correctly. 3. Efficiently manage the operations using `collections.deque` for optimal performance. Design your `TextEditor` to handle a considerable number of operations efficiently.","solution":"from collections import deque from typing import Optional class TextEditor: def __init__(self): self.text = \'\' self.history = deque() self.future = deque() def insert(self, position: int, text: str) -> None: self.history.append((self.text, \'insert\', (position, len(text)))) self.text = self.text[:position] + text + self.text[position:] self.future.clear() def delete(self, start: int, end: int) -> None: self.history.append((self.text, \'delete\', (start, end))) self.text = self.text[:start] + self.text[end:] self.future.clear() def undo(self) -> None: if self.history: self.future.append(self.text) last_text, operation, args = self.history.pop() self.text = last_text def redo(self) -> None: if self.future: self.history.append((self.text, None, None)) self.text = self.future.pop() def get_text(self) -> str: return self.text def perform_operation(editor: TextEditor, operation: str, argument: Optional[str] = None) -> None: if operation == \\"insert\\" and argument: position, text = argument.split(\',\', 1) editor.insert(int(position), text) elif operation == \\"delete\\" and argument: start, end = map(int, argument.split(\',\')) editor.delete(start, end) elif operation == \\"undo\\": editor.undo() elif operation == \\"redo\\": editor.redo() def get_text(editor: TextEditor) -> str: return editor.get_text()"},{"question":"Objective: You are tasked with analyzing a dataset using seaborn\'s violin plots to uncover various insights about the data. Task: 1. Load the \'tips\' dataset from seaborn. 2. Create a violin plot comparing the distributions of total bill amounts across different days of the week. 3. Further customize the plot by: - Splitting the violin for smokers and non-smokers. - Adding inner quartiles representation. - Normalizing the width of each violin to represent the number of observations. - Adjusting the bandwidth for smoothing. Input: - No direct input beyond loading the \'tips\' dataset. Expected Output: - The resulting violin plot should reflect the customizations specified. Constraints: - Ensure the plot is clear and readable. The presentation of results is important. - Avoid hardcoding data; utilize seaborn\'s dataset loading mechanisms. Code Requirements: Implement the function `create_custom_violin_plot()` that performs the task described above. ```python import seaborn as sns import matplotlib.pyplot as plt def create_custom_violin_plot(): # Load the \'tips\' dataset df = sns.load_dataset(\'tips\') # Creating the violin plot with the specified customizations sns.set_theme(style=\\"whitegrid\\") sns.violinplot(data=df, x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", split=True, inner=\\"quart\\", scale=\\"count\\", bw=0.5) # Show the plot plt.show() # Execute the function create_custom_violin_plot() ``` # Explanation: - **df**: The \'tips\' dataset. - **sns.set_theme**: Sets a thematic style for the plot. - **sns.violinplot**: Plots total bill amounts across days, differentiating smokers from non-smokers, displaying quartiles, normalizing widths, and adjusting smoothness. - **plt.show()**: Displays the plot. **Note:** Implement the function in a way that when it is called, it autonomously generates and displays the desired plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_violin_plot(): Load the \'tips\' dataset and create a customized violin plot showing the distribution of total bill amounts across different days of the week, split by smoker status, with quartile representation, normalized widths, and adjusted bandwidth for smoothing. # Load the \'tips\' dataset df = sns.load_dataset(\'tips\') # Create the violin plot with the specified customizations sns.set_theme(style=\\"whitegrid\\") sns.violinplot(data=df, x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", split=True, inner=\\"quart\\", scale=\\"count\\", bw=0.5) # Show the plot plt.show() # Execute the function create_custom_violin_plot()"},{"question":"<|Analysis Begin|> The given documentation provides a brief example of using the seaborn.objects module to visualize data from the Titanic dataset. It specifically demonstrates how to create bar plots that show counts and histograms that depict part-whole relationships using faceting. Key Points: 1. Loading the Titanic dataset from Seaborn. 2. Sorting the dataset based on the \'alive\' column. 3. Creating a bar plot with stacking based on \'class\' and \'sex\'. 4. Creating histograms with faceting based on \'age\' and \'sex\', differentiated by \'alive\' status. The documentation showcases the following Seaborn functionalities: - `seaborn.objects.Plot` - Different types of visualizations: Bar and Histograms - Grouping and stacking options - Faceting to split the data into multiple subplots The documentation provided is quite focused on visualizations, particularly on how to manipulate categorical data and part-whole relationships with faceting. <|Analysis End|> <|Question Begin|> **Question: Titanic Data Visualization using Seaborn** You are tasked with visualizing data from the Titanic dataset to gain insights into the demographics and survival rates of passengers. Your objective is to create a comprehensive plot that demonstrates the use of various Seaborn functionalities, such as faceting, stacking, and different plot types. # Requirements: 1. **Load and Prepare the Data:** - Load the Titanic dataset using `seaborn.load_dataset(\\"titanic\\")`. - Sort the dataset based on the \'alive\' column in descending order. 2. **Create a Bar Plot:** - Visualize the count of passengers in each class (`class`) differentiated by gender (`sex`). - Use stacking to display the data. 3. **Create a Faceted Histogram:** - For the same dataset, create a histogram showing the distribution of passenger ages (`age`), differentiated by survival status (`alive`). - Use faceting to split the data based on gender (`sex`). - Set the bin width to 10 for the histograms. 4. **Combine the Visualizations:** - Ensure that both visualizations are displayed in a single output. # Input Format: No input is required from the user other than executing the provided code. # Output Format: A plot with two subplots: 1. A stacked bar plot showing the count of passengers in each class differentiated by gender. 2. A faceted histogram showing the distribution of passenger ages with respect to their survival status, split by gender. # Constraints: - Use the seaborn.objects module for the visualizations. - Ensure the visualizations are clear and well-labeled. ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load and prepare the Titanic dataset titanic = load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) # Create a bar plot bar_plot = so.Plot(titanic, x=\\"class\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Stack()) # Create a faceted histogram histogram_plot = ( so.Plot(titanic, x=\\"age\\", alpha=\\"alive\\") .facet(\\"sex\\") .add(so.Bars(), so.Hist(binwidth=10), so.Stack()) ) # Display the plots fig, axes = plt.subplots(2, 1, figsize=(10, 15)) bar_plot.on(axes[0]) histogram_plot.on(axes[1]) plt.show() ``` Ensure that your solution adheres to the specified requirements and produces the expected visualizations.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def visualize_titanic_data(): Visualize data from the Titanic dataset using Seaborn objects. Creates a bar plot for passenger counts by class and sex, and faceted histograms by age and survival status, split by sex. # Load and prepare the Titanic dataset titanic = load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) # Create a bar plot bar_plot = so.Plot(titanic, x=\\"class\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Stack()) # Create a faceted histogram histogram_plot = ( so.Plot(titanic, x=\\"age\\", alpha=\\"alive\\") .facet(\\"sex\\") .add(so.Bars(), so.Hist(binwidth=10), so.Stack()) ) # Display the plots fig, axes = plt.subplots(2, 1, figsize=(10, 15)) bar_plot.on(axes[0]) histogram_plot.on(axes[1]) plt.show()"},{"question":"**Objective:** To assess your understanding of seaborn\'s capabilities, you are required to write code that demonstrates the creation and customization of a complex, multi-faceted plot. **Task:** 1. Load the `penguins` dataset provided by seaborn. 2. Create a `FacetGrid` plot with the following specifications: - Main plot should be an ECDF of `bill_length_mm` for each species (`hue` by `species`). - Facets should be divided by `sex` (two columns - male and female). - Customize the plot with: - ```height=4``` and ```aspect=1.2```. - Titles that reflect the species name on each facet. - Proper labeling of X-axis as `\\"Bill Length (mm)\\"` and Y-axis as `\\"ECDF Value\\"`. 3. Further, add a KDE plot to each facet showing the distribution of `flipper_length_mm` with the data split by `sex`. The KDE plots should have a different color palette for each species. **Constraints:** - Ensure that your code is efficient and well-documented. - The code should be reusable and function as intended across different datasets with similar structures. **Expected Input and Output Format:** - Input: None (the dataset is loaded from seaborn directly). - Output: Visualization containing the specified facets and customizations. # Example Solution ```python import seaborn as sns # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the FacetGrid g = sns.displot( data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"sex\\", kind=\\"ecdf\\", height=4, aspect=1.2 ) # Customize the facets g.set_titles(\\"{col_name} penguins\\") g.set_axis_labels(\\"Bill Length (mm)\\", \\"ECDF Value\\") # Add KDE plot to each facet for flipper length for ax in g.axes.flatten(): sns.kdeplot( data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", ax=ax, palette=\\"viridis\\" ) # Show the plot g.fig.suptitle(\\"Penguins Bill Length ECDF and Flipper Length Distribution by Sex and Species\\", y=1.05) g.fig.tight_layout() g.fig.subplots_adjust(top=0.9) sns.despine() ``` **Note:** The example solution provided is a guideline to help structure your approach. Ensure that your code incorporates all the specified customizations and facets.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguins_plot(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the FacetGrid with ECDF for bill_length_mm g = sns.FacetGrid( data=penguins, col=\\"sex\\", hue=\\"species\\", height=4, aspect=1.2 ) g.map(sns.ecdfplot, \\"bill_length_mm\\").add_legend() # Customize the facets g.set_titles(\\"{col_name} Penguins\\") g.set_axis_labels(\\"Bill Length (mm)\\", \\"ECDF Value\\") # Overlay KDE plot for flipper_length_mm with a different color palette for each species for ax in g.axes.flatten(): sns.kdeplot( data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", ax=ax, common_norm=False, palette=\\"viridis\\" ) # Adjust plot layout for better visuals plt.subplots_adjust(top=0.85) g.fig.suptitle(\\"Penguins Bill Length ECDF and Flipper Length Distribution by Sex and Species\\", y=1.05) g.fig.tight_layout() sns.despine() # Show the plot plt.show()"},{"question":"Objective: Implement a function that uses the `imp` module to mimic the behavior of Python\'s `import` mechanism. Despite the deprecated status of `imp`, this exercise will demonstrate your understanding of Python\'s import internals and module handling. Problem Statement: You are to write a function that emulates the import of a module using the `imp` module, similar to how it was standard before Python 1.4. Your function should: 1. Try to import a module by name. 2. If the module has been previously imported, return the existing module object from `sys.modules`. 3. Otherwise, locate the module using `imp.find_module`. 4. Load the module using `imp.load_module`. Function Signature: ```python def custom_import(name: str) -> object: pass ``` Input: - `name` (str): The name of the module to import. Output: - Return the module object if successfully imported. - Raise `ImportError` if the module cannot be found. Constraints: - Do not directly use the `import` statement or `importlib` module except for importing `sys` and `imp`. - You must handle the file closing explicitly, ensuring no file descriptor leaks. Example: ```python import os # Example usage module = custom_import(\'os\') print(module.path) # Accessing an attribute of the os module ``` This function should correctly mimic importing the `os` module and allow access to its attributes just like a standard import. Notes: - Focus on handling possible exceptions, such as not finding the module or issues during loading. - Remember to close the file pointer explicitly using a `try...finally` block. - Handle both source files and compiled module files as described in the `imp` documentation.","solution":"import sys import imp def custom_import(name: str) -> object: Mimics the import of a module using the imp module. if name in sys.modules: return sys.modules[name] file, pathname, description = None, None, None try: file, pathname, description = imp.find_module(name) return imp.load_module(name, file, pathname, description) except ImportError as e: raise ImportError(f\\"Module {name} could not be imported: {e}\\") finally: if file: file.close()"},{"question":"**HTML Manipulation Using Python:** You are given two tasks involving HTML content manipulation using Python\'s `html` module. # Task 1: Escape HTML special characters Write a function `escape_html(text: str, handle_quotes: bool = True) -> str` that takes in a string and returns the string with its HTML-special characters converted to their HTML-safe sequences using the `html.escape` function. Input: - `text`: A string that may contain HTML special characters. - `handle_quotes` (optional, default `True`): A boolean indicating whether quotes should be converted to their HTML-safe sequences. Output: - A string with HTML-special characters (&, <, > and optionally \\", \') converted to their HTML-safe sequences. Example: ```python assert escape_html(\'5 < 6 & 7 > 3\') == \'5 &lt; 6 &amp; 7 &gt; 3\' assert escape_html(\'She said, \\"Hello!\\"\', True) == \'She said, &quot;Hello!&quot;\' assert escape_html(\\"It\'s a test.\\", True) == \'It&#x27;s a test.\' ``` # Task 2: Unescape HTML character references Write a function `unescape_html(text: str) -> str` that takes in a string and returns the string with all named and numeric HTML character references converted to their corresponding Unicode characters using the `html.unescape` function. Input: - `text`: A string that may contain HTML character references. Output: - A string with HTML character references converted to their corresponding Unicode characters. Example: ```python assert unescape_html(\'5 &lt; 6 &amp; 7 &gt; 3\') == \'5 < 6 & 7 > 3\' assert unescape_html(\'She said, &quot;Hello!&quot;\') == \'She said, \\"Hello!\\"\' assert unescape_html(\'It&#x27;s a test.\') == \\"It\'s a test.\\" ``` # Constraints: - The input strings for both functions will have lengths of up to 10,000 characters. - Performance should be considered but the primary focus is correctness. Implement these two functions to demonstrate your understanding and ability to manipulate HTML content using Python\'s `html` module.","solution":"import html def escape_html(text: str, handle_quotes: bool = True) -> str: Escapes HTML special characters in the given text. Args: text (str): The input string containing HTML special characters. handle_quotes (bool): Boolean indicating whether quotes should be converted to HTML-safe sequences. Returns: str: The HTML-safe string with special characters escaped. return html.escape(text, quote=handle_quotes) def unescape_html(text: str) -> str: Unescapes HTML character references in the given text to their corresponding Unicode characters. Args: text (str): The input string containing HTML character references. Returns: str: The string with HTML character references converted to Unicode characters. return html.unescape(text)"},{"question":"# Objective You are tasked with demonstrating your understanding of the seaborn package, specifically the `sns.cubehelix_palette` function and its applications in creating and customizing color palettes for visualizations. # Problem Statement Write a function `visualize_custom_palettes` that takes in two parameters: 1. `num_colors` (integer): The number of colors in the palette. 2. `settings` (dictionary): A dictionary containing custom settings for the cubehelix palette, which may include the following keys: `start`, `rot`, `gamma`, `hue`, `dark`, `light`, and `reverse`. The function should: 1. Generate a cubehelix palette based on the provided parameters using `sns.cubehelix_palette()`. 2. Create a barplot using seaborn with the generated palette. 3. Display the barplot and the corresponding color palette. Input: - `num_colors` (integer): An integer representing the number of colors in the palette. Example: 6 - `settings` (dictionary): A dictionary with keys that map to parameters of `sns.cubehelix_palette()`. Possible keys are `start`, `rot`, `gamma`, `hue`, `dark`, `light`, and `reverse`. Example: `{\'start\': 0.5, \'rot\': -0.75, \'gamma\': 0.8, \'hue\': 1.2, \'dark\': 0.2, \'light\': 0.8, \'reverse\': False}` Output: - The function should display a barplot visualizing the colors in the generated cubehelix palette. Example: ```python visualize_custom_palettes(6, {\'start\': 0.5, \'rot\': -0.75, \'gamma\': 0.8, \'hue\': 1.2, \'dark\': 0.2, \'light\': 0.8, \'reverse\': False}) ``` The function should generate a cubehelix palette with the specified settings, create a barplot visualizing the colors, and display the plot. Constraints: - Ensure all keys in the settings dictionary, if present, are suitable for the `sns.cubehelix_palette()` function. - Assume valid input for simplicity. Performance: - The function should efficiently create the cubehelix palette and barplot without significant delays.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_custom_palettes(num_colors, settings): Generates a cubehelix palette based on the provided parameters and displays a barplot using seaborn with the generated palette. Parameters: num_colors (integer): The number of colors in the palette. settings (dictionary): A dictionary containing custom settings for the cubehelix palette. # Generate the cubehelix palette with the provided settings palette = sns.cubehelix_palette(num_colors, **settings) # Create a simple dataset to visualize the colors data = pd.DataFrame({\'Category\': range(num_colors), \'Values\': [1] * num_colors}) # Create the barplot with the generated palette sns.barplot(x=\'Category\', y=\'Values\', palette=palette, data=data) # Display the plot plt.show()"},{"question":"**Question:** You are given a dataset of tips received by waiters in a restaurant. Your task is to analyze this dataset and visualize the data using seaborn. Follow the steps below to generate the required plots: 1. **Load the Dataset:** Load the \'tips\' dataset using seaborn\'s `load_dataset` function. 2. **Plot the Basic Bar Plot:** Create a bar plot showing the count of tips received on different days of the week. 3. **Grouped Bar Plot:** Generate a bar plot similar to the one above, but group the data by the \'sex\' variable, using different colors for each group. 4. **Numerical Data Bar Plot:** Create a bar plot showing the count of different party sizes (number of people) for tips received. 5. **Alternative Axis Plot:** Create a bar plot having party sizes on the y-axis and counts on the x-axis. 6. **Custom Style:** Customize the appearance of your plots (e.g., colors, widths). Here\'s a function signature to help you: ```python import seaborn.objects as so from seaborn import load_dataset def visualize_tips_data(): # Load the dataset tips = load_dataset(\\"tips\\") # Plot 1: Basic Bar Plot plot1 = so.Plot(tips, x=\\"day\\").add(so.Bar(), so.Count()) plot1.show() # Plot 2: Grouped Bar Plot by Sex plot2 = so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()) plot2.show() # Plot 3: Numerial Data Bar Plot plot3 = so.Plot(tips, x=\\"size\\").add(so.Bar(), so.Count()) plot3.show() # Plot 4: Alternative Axis Plot plot4 = so.Plot(tips, y=\\"size\\").add(so.Bar(), so.Count()) plot4.show() # Customize and display your plots for better presentation # You can add titles, adjust colors, widths, etc, as needed # For example: plot1.theme({\\"ctx.figure.textcolor\\": \\".15\\"}).show() visualize_tips_data() ``` Ensure you understand: - How to load datasets using seaborn. - The use of the `Plot`, `Bar`, `Count`, and `Dodge` classes and their methods. - How to customize styles and features of the plots. **Input:** - No input required as the function works internally within the code. **Output:** - Series of plots displayed as per the requirements. **Constraints:** - Use the seaborn library\'s objects-based interface. - Follow the steps to first create basic plots, then add complexity gradually. **Performance Requirements:** - The function is expected to execute efficiently and render the plots without significant lag.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Plot 1: Basic Bar Plot - Count of tips received on different days of the week plt.figure(figsize=(10, 6)) sns.countplot(x=\'day\', data=tips) plt.title(\'Count of Tips Received on Different Days of the Week\') plt.show() # Plot 2: Grouped Bar Plot - Group by \'sex\' plt.figure(figsize=(10, 6)) sns.countplot(x=\'day\', hue=\'sex\', data=tips) plt.title(\'Count of Tips Received on Different Days of the Week by Sex\') plt.show() # Plot 3: Numerical Data Bar Plot - Count of different party sizes plt.figure(figsize=(10, 6)) sns.countplot(x=\'size\', data=tips) plt.title(\'Count of Different Party Sizes for Tips Received\') plt.show() # Plot 4: Alternative Axis Plot - Party sizes on the y-axis and counts on the x-axis plt.figure(figsize=(10, 6)) sns.countplot(y=\'size\', data=tips) plt.title(\'Count of Different Party Sizes for Tips Received (Alternative Axis)\') plt.show() # Custom Style: Adjust colors and widths plt.figure(figsize=(10, 6)) sns.countplot(x=\'day\', data=tips, palette=\'Set2\', linewidth=2.5) plt.title(\'Count of Tips Received on Different Days of the Week (Custom Style)\') plt.show() visualize_tips_data()"},{"question":"# Cross-Validation with Different Strategies In this coding assessment question, you will be required to implement and evaluate various cross-validation strategies using the scikit-learn package. Background: You\'re provided with the Iris dataset, a classic dataset in machine learning, which contains 150 samples of iris flowers. Each sample has 4 features and belongs to one of three classes. Task: 1. **Load the Iris dataset** using scikit-learn\'s `datasets` module. 2. **Split the data into training and test sets** using an 80-20 ratio. 3. **Train and evaluate a Support Vector Machine (SVM) classifier** using K-Fold Cross-Validation with 5 folds. Report the mean accuracy and standard deviation. 4. **Train and evaluate the same SVM classifier** using Stratified K-Fold Cross-Validation with 5 folds. Report the mean accuracy and standard deviation. 5. **Implement a custom cross-validation strategy** that trains the model by leaving one sample out each time (Leave-One-Out Cross-Validation). Report the mean accuracy. **Note**: - Use `train_test_split` to create training and test sets. - Use `cross_val_score` for performing the cross-validation evaluations. - Ensure reproducibility by setting a `random_state` where necessary. Requirements: - Import necessary modules including `numpy`, `datasets`, `train_test_split`, and `svm`. - Define the SVM classifier with `kernel=\'linear\'` and `C=1`. - Plot the cross-validation results for each method to visualize the distribution of scores across different folds (for K-Fold and Stratified K-Fold). Example Output: ``` K-Fold Cross-Validation: Mean accuracy: 0.97 Standard deviation: 0.02 Stratified K-Fold Cross-Validation: Mean accuracy: 0.96 Standard deviation: 0.03 Leave-One-Out Cross-Validation: Mean accuracy: 0.97 ``` Code Template: ```python import numpy as np from sklearn import datasets from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score, LeaveOneOut from sklearn import svm import matplotlib.pyplot as plt # Load the Iris dataset X, y = datasets.load_iris(return_X_y=True) # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the SVM classifier clf = svm.SVC(kernel=\'linear\', C=1) # K-Fold Cross-Validation kf = KFold(n_splits=5, random_state=42, shuffle=True) kf_scores = cross_val_score(clf, X_train, y_train, cv=kf) print(\\"K-Fold Cross-Validation:\\") print(f\\"Mean accuracy: {kf_scores.mean():.2f}\\") print(f\\"Standard deviation: {kf_scores.std():.2f}\\") # Plot K-Fold scores plt.figure() plt.plot(kf_scores, label=\'K-Fold Scores\') plt.xlabel(\'Fold\') plt.ylabel(\'Accuracy\') plt.title(\'K-Fold Cross-Validation Scores\') plt.legend() plt.show() # Stratified K-Fold Cross-Validation skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True) skf_scores = cross_val_score(clf, X_train, y_train, cv=skf) print(\\"Stratified K-Fold Cross-Validation:\\") print(f\\"Mean accuracy: {skf_scores.mean():.2f}\\") print(f\\"Standard deviation: {skf_scores.std():.2f}\\") # Plot Stratified K-Fold scores plt.figure() plt.plot(skf_scores, label=\'Stratified K-Fold Scores\') plt.xlabel(\'Fold\') plt.ylabel(\'Accuracy\') plt.title(\'Stratified K-Fold Cross-Validation Scores\') plt.legend() plt.show() # Leave-One-Out Cross-Validation loo = LeaveOneOut() loo_scores = cross_val_score(clf, X_train, y_train, cv=loo) print(\\"Leave-One-Out Cross-Validation:\\") print(f\\"Mean accuracy: {loo_scores.mean():.2f}\\") # No need to plot Leave-One-Out scores as there will be as many scores as samples ``` Implement the described steps by filling out the code template and ensure the output format as shown in the Example Output section.","solution":"import numpy as np from sklearn import datasets from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score, LeaveOneOut from sklearn import svm import matplotlib.pyplot as plt # Load the Iris dataset X, y = datasets.load_iris(return_X_y=True) # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the SVM classifier clf = svm.SVC(kernel=\'linear\', C=1) # K-Fold Cross-Validation kf = KFold(n_splits=5, random_state=42, shuffle=True) kf_scores = cross_val_score(clf, X_train, y_train, cv=kf) kf_mean = kf_scores.mean() kf_std = kf_scores.std() print(\\"K-Fold Cross-Validation:\\") print(f\\"Mean accuracy: {kf_mean:.2f}\\") print(f\\"Standard deviation: {kf_std:.2f}\\") # Plot K-Fold scores plt.figure() plt.plot(kf_scores, label=\'K-Fold Scores\') plt.xlabel(\'Fold\') plt.ylabel(\'Accuracy\') plt.title(\'K-Fold Cross-Validation Scores\') plt.legend() plt.show() # Stratified K-Fold Cross-Validation skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True) skf_scores = cross_val_score(clf, X_train, y_train, cv=skf) skf_mean = skf_scores.mean() skf_std = skf_scores.std() print(\\"Stratified K-Fold Cross-Validation:\\") print(f\\"Mean accuracy: {skf_mean:.2f}\\") print(f\\"Standard deviation: {skf_std:.2f}\\") # Plot Stratified K-Fold scores plt.figure() plt.plot(skf_scores, label=\'Stratified K-Fold Scores\') plt.xlabel(\'Fold\') plt.ylabel(\'Accuracy\') plt.title(\'Stratified K-Fold Cross-Validation Scores\') plt.legend() plt.show() # Leave-One-Out Cross-Validation loo = LeaveOneOut() loo_scores = cross_val_score(clf, X_train, y_train, cv=loo) loo_mean = loo_scores.mean() print(\\"Leave-One-Out Cross-Validation:\\") print(f\\"Mean accuracy: {loo_mean:.2f}\\")"},{"question":"Implement a function `manipulate_bytearrays` that takes two input strings `s1` and `s2`, and performs the following operations using the bytearray functions and macros described in the documentation: 1. Convert both strings to bytearray objects. 2. Concatenate the two bytearray objects into a single bytearray. 3. Return the size of the resulting concatenated bytearray and its content as a string. ```python def manipulate_bytearrays(s1: str, s2: str) -> tuple: Given two input strings `s1` and `s2`, perform the following operations: 1. Convert both strings to bytearray objects. 2. Concatenate the two bytearray objects. 3. Return the size of the resulting concatenated bytearray and its content as a string. Args: - s1 (str): The first input string. - s2 (str): The second input string. Returns: - tuple: A tuple containing the size of the concatenated bytearray (int) and the content of the bytearray as a string. # Implement your code here pass ``` # Constraints - The input strings `s1` and `s2` will have a maximum length of 1000 characters each. - The function should efficiently handle the conversion and concatenation process. # Example ```python s1 = \\"Hello\\" s2 = \\"World\\" result = manipulate_bytearrays(s1, s2) print(result) # Expected output: (10, \'HelloWorld\') ``` Explanation: - The input strings `\\"Hello\\"` and `\\"World\\"` are converted to bytearrays. - The bytearrays are concatenated to form `\\"HelloWorld\\"`. - The size of the resulting bytearray is 10, and its content is `\\"HelloWorld\\"`.","solution":"def manipulate_bytearrays(s1: str, s2: str) -> tuple: Given two input strings `s1` and `s2`, perform the following operations: 1. Convert both strings to bytearray objects. 2. Concatenate the two bytearray objects. 3. Return the size of the resulting concatenated bytearray and its content as a string. Args: - s1 (str): The first input string. - s2 (str): The second input string. Returns: - tuple: A tuple containing the size of the concatenated bytearray (int) and the content of the bytearray as a string. # Step 1: Convert strings to bytearray objects bytearray1 = bytearray(s1, \'utf-8\') bytearray2 = bytearray(s2, \'utf-8\') # Step 2: Concatenate the bytearray objects concatenated_bytearray = bytearray1 + bytearray2 # Step 3: Return the size and content as a string size_of_concatenated_bytearray = len(concatenated_bytearray) return (size_of_concatenated_bytearray, concatenated_bytearray.decode(\'utf-8\'))"},{"question":"Coding Assessment Question # Context You are provided with a dataset that includes a feature set `X` (1-dimensional) and target values `y`. Your task is to implement an isotonic regression model using the `IsotonicRegression` class from scikit-learn, fit the model to the provided data, and predict target values for a new set of feature values. # Task Write a Python function `isotonic_regression_predict(X_train, y_train, X_predict, increasing=True)` that: 1. Fits an isotonic regression model to the training data (`X_train`, `y_train`). 2. Predicts target values for the new feature set `X_predict`. 3. The function should accept an optional boolean parameter `increasing` to indicate whether the regression function should be non-decreasing (`True`) or non-increasing (`False`). # Specifications - **Input**: - `X_train`: List of floats representing the training feature set. - `y_train`: List of floats representing the training target values. - `X_predict`: List of floats representing the feature set for which predictions are needed. - `increasing` (optional): Boolean (`True` for non-decreasing, `False` for non-increasing). Default is `True`. - **Output**: - A list of floats representing the predicted target values for `X_predict`. # Constraints - The elements of `X_train` and `y_train` will have the same length. - Each element in `X_train` and `X_predict` list will be unique. - The number of elements in `X_train` and `y_train` will not exceed 10,000. # Example ```python from sklearn.isotonic import IsotonicRegression def isotonic_regression_predict(X_train, y_train, X_predict, increasing=True): # Instantiate the IsotonicRegression model ir = IsotonicRegression(increasing=increasing) # Fit the model to the training data ir.fit(X_train, y_train) # Predict target values for X_predict predictions = ir.predict(X_predict) return predictions # Sample Input X_train = [1, 2, 3, 4, 5] y_train = [5, 6, 7, 8, 9] X_predict = [1.5, 2.5, 3.5] # Expected Output predicted_values = isotonic_regression_predict(X_train, y_train, X_predict) print(predicted_values) # Expected output: [5.5, 6.5, 7.5] (approximately) ``` # Note - The predicted results should maintain the monotonic constraint as specified by the `increasing` parameter.","solution":"from sklearn.isotonic import IsotonicRegression def isotonic_regression_predict(X_train, y_train, X_predict, increasing=True): Fits an isotonic regression model to the training data and predicts target values for the new feature set. Parameters: - X_train: List of floats representing the training feature set. - y_train: List of floats representing the training target values. - X_predict: List of floats representing the feature set for which predictions are needed. - increasing: Boolean indicating whether the regression function should be non-decreasing (True) or non-increasing (False). Default is True. Returns: - List of floats representing the predicted target values for X_predict. # Instantiate the IsotonicRegression model ir = IsotonicRegression(increasing=increasing) # Fit the model to the training data ir.fit(X_train, y_train) # Predict target values for X_predict predictions = ir.predict(X_predict) return predictions"},{"question":"# Seaborn Visualization Challenge You are required to analyze a given dataset and create an illustrative visualization using Seaborn, which demonstrates your understanding of various features and functions of the library. Objective: Create a Seaborn visualization that effectively represents the relationship between multiple variables in the dataset. Your analysis and the final visualization should provide meaningful insights and be customized for clarity and aesthetics. Dataset: For this task, use the `diamonds` dataset from Seaborn\'s built-in datasets. You can load the dataset using the following code: ```python import seaborn as sns diamonds = sns.load_dataset(\\"diamonds\\") ``` Task: 1. **Data Analysis**: - Provide a brief analysis of the dataset including the fundamental summary statistics (e.g., mean, median, standard deviation) for key variables. 2. **Visualization Requirements**: - Choose an appropriate Seaborn plot that can simultaneously represent at least three variables of the `diamonds` dataset. - Customize the color palette using `seaborn.diverging_palette()` or other palette functions to enhance the visualization. - Include a legend, labels for axes, and a plot title to make the visualization comprehensive. - Adjust any other relevant aesthetic parameters to improve the overall readability and presentation of the plot. 3. **Implementation**: - Implement the visualization and plot it using Matplotlib to ensure all elements are rendered as expected. Expected Input Format: There is no external input required. You will be working with the `diamonds` dataset loaded directly from Seaborn. Expected Output Format: - The output should be a Matplotlib plot rendered with Seaborn\'s plotting functions, along with the customization and enhancements described in the task. Example: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset diamonds = sns.load_dataset(\\"diamonds\\") # Step 1: Data Analysis (brief summary) # Calculation of summary statistics summary = diamonds.describe() print(summary) # Step 2: Visualization # Choose an appropriate plot type and customize it plt.figure(figsize=(10,6)) palette = sns.diverging_palette(250, 15, s=75, l=40, n=9, center=\'dark\') sns.violinplot(x=\'cut\', y=\'price\', hue=\'color\', data=diamonds, palette=palette) # Add labels and title plt.title(\'Price Distribution of Diamonds by Cut and Color\') plt.xlabel(\'Cut\') plt.ylabel(\'Price\') plt.legend(title=\'Color\') # Show plot plt.show() ``` Constraints: - Ensure that the plot is readable and interpretable. - The plot should effectively convey the relationship between the variables and be aesthetically pleasing. Good luck, and make sure your visualization not only looks good but also tells a compelling story about the data!","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset diamonds = sns.load_dataset(\\"diamonds\\") # Step 1: Data Analysis (brief summary) summary = diamonds.describe() print(summary) # Step 2: Visualization plt.figure(figsize=(12, 8)) # Create a color palette palette = sns.diverging_palette(220, 20, as_cmap=True) # Create a scatterplot with regression line sns.scatterplot(x=\'carat\', y=\'price\', hue=\'cut\', size=\'depth\', sizes=(20, 200), palette=\'viridis\', data=diamonds, alpha=0.6) # Enhance plot aesthetics plt.title(\'Diamond Price vs Carat with Cut Highlight and Depth Size\', fontsize=16) plt.xlabel(\'Carat\', fontsize=14) plt.ylabel(\'Price\', fontsize=14) plt.legend(title=\'Cut\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.grid(True) # Show plot plt.tight_layout() plt.show()"},{"question":"# Question: Customizing Seaborn\'s `cubehelix_palette` Objective: Write a Python function that generates a seaborn `cubehelix` palette based on a variety of parameters, and then apply this palette to a simple plot using seaborn. Function Signature: ```python def custom_cubehelix_palette(n_colors: int = 6, start: float = 0, rot: float = 0.4, gamma: float = 1.0, hue: float = 0.8, dark: float = 0, light: float = 1, reverse: bool = False, as_cmap: bool = False) -> None: pass ``` Parameters: - `n_colors (int)`: The number of colors in the palette. Default is 6. - `start (float)`: The starting point of the helix. Default is 0. - `rot (float)`: The number of rotations in the helix. Default is 0.4. - `gamma (float)`: Applies a nonlinearity to the luminance ramp. Default is 1.0. - `hue (float)`: The saturation of the colors. Default is 0.8. - `dark (float)`: The luminance at the start of the palette. Default is 0. - `light (float)`: The luminance at the end of the palette. Default is 1. - `reverse (bool)`: If True, the luminance ramp is reversed. Default is False. - `as_cmap (bool)`: If True, the palette is returned as a colormap. Default is False. Return: `None` Instructions: 1. Generate a `cubehelix_palette` using the parameters provided. 2. Create a simple bar plot using seaborn where the palette is applied to the bars. 3. Save the plot as a PNG image file named `custom_palette.png`. Constraints: 1. Use the seaborn package for palette generation and plotting. 2. Ensure that the function does not return any values but saves the plot correctly. Example Usage and Explanation: ```python custom_cubehelix_palette(n_colors=10, start=1, rot=0.5, gamma=1.0, hue=0.9, dark=0.2, light=0.8, reverse=False, as_cmap=False) ``` This should generate a cubehelix palette with 10 colors, starting point at 1, 0.5 rotations, normal gamma correction, 0.9 hue, darker starting luminance, lighter final luminance and apply it to a simple bar plot, saving the plot as \\"custom_palette.png\\".","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_cubehelix_palette(n_colors: int = 6, start: float = 0, rot: float = 0.4, gamma: float = 1.0, hue: float = 0.8, dark: float = 0, light: float = 1, reverse: bool = False, as_cmap: bool = False) -> None: Generates a seaborn cubehelix palette based on provided parameters and applies this palette to a simple bar plot, then saves the plot as a PNG image. # Generate the cubehelix palette palette = sns.cubehelix_palette(n_colors=n_colors, start=start, rot=rot, gamma=gamma, hue=hue, dark=dark, light=light, reverse=reverse, as_cmap=as_cmap) # Create a dummy dataset data = sns.load_dataset(\\"penguins\\").head(n_colors) # Just take the first n_colors rows for simplicity # Create the plot plt.figure(figsize=(8, 4)) sns.barplot(data=data, x=data.index, y=\\"body_mass_g\\", palette=palette) plt.title(\\"Custom Cubehelix Palette Bar Plot\\") # Save the plot plt.savefig(\\"custom_palette.png\\") plt.close()"},{"question":"**Configuration File Parsing with Interpolation** You are developing a Python application that needs to load and interpret configuration settings from an INI file. Your task is to implement a function `load_config(config_str)` to load the configuration, apply interpolation using the `ExtendedInterpolation`, and retrieve specific settings with necessary type conversions. Your function should: 1. Read and parse the configuration from a provided string. 2. Take care of the interpolation using `ExtendedInterpolation`. 3. Retrieve specific settings, converting them to the proper data types. 4. Handle missing settings by returning default fallback values gracefully. # Function Signature ```python from configparser import ConfigParser, ExtendedInterpolation, NoSectionError, NoOptionError def load_config(config_str: str) -> dict: Load and interpret the configuration from a string. Parameters: config_str (str): The configuration in INI format as a string. Returns: dict: A dictionary containing the extracted and interpreted configuration values. pass ``` # Input - `config_str`: A string in INI format representing the configuration. # Output - Return a dictionary with key-value pairs of the specific settings: - `\\"server\\": str` - The server name from `[Server]` - `\\"port\\": int` - The port number from `[Server]` - `\\"use_ssl\\": bool` - Whether to use SSL from `[Server]` - `\\"timeout\\": float` - The timeout value from `[Settings]` - `\\"base_dir\\": str` - The base directory path from `[Paths]` # Constraints - If a setting is missing, use the following default values: - `\\"server\\"`: `\\"localhost\\"` - `\\"port\\"`: `8080` - `\\"use_ssl\\"`: `False` - `\\"timeout\\"`: `30.0` - `\\"base_dir\\"`: `\\"/tmp\\"` # Example ```python config = [Server] name = example.com port = 443 use_ssl = yes [Paths] home_dir = /home/user workspace_dir = {home_dir}/workspace expected_output = { \\"server\\": \\"example.com\\", \\"port\\": 443, \\"use_ssl\\": True, \\"timeout\\": 30.0, \\"base_dir\\": \\"/home/user/workspace\\" } assert load_config(config) == expected_output ``` # Additional Instructions 1. Use `ExtendedInterpolation` for handling the interpolations within the configuration. 2. Use the provided fallback values for any missing settings. 3. You must handle errors appropriately and ensure robustness in retrieving and converting the values.","solution":"from configparser import ConfigParser, ExtendedInterpolation, NoSectionError, NoOptionError def load_config(config_str: str) -> dict: Load and interpret the configuration from a string. Parameters: config_str (str): The configuration in INI format as a string. Returns: dict: A dictionary containing the extracted and interpreted configuration values. config = ConfigParser(interpolation=ExtendedInterpolation()) config.read_string(config_str) result = { \\"server\\": config.get(\\"Server\\", \\"name\\", fallback=\\"localhost\\"), \\"port\\": config.getint(\\"Server\\", \\"port\\", fallback=8080), \\"use_ssl\\": config.getboolean(\\"Server\\", \\"use_ssl\\", fallback=False), \\"timeout\\": config.getfloat(\\"Settings\\", \\"timeout\\", fallback=30.0), \\"base_dir\\": config.get(\\"Paths\\", \\"workspace_dir\\", fallback=\\"/tmp\\") } return result"},{"question":"# Advanced Asyncio Task Management and Exception Handling Objective: You are required to write a Python program using the asyncio module that manages multiple async tasks with sophisticated task scheduling and exception handling. Problem Description: Implement an async function `schedule_tasks` that concurrently runs a list of provided asynchronous tasks. Each task should have a set timeout. If a task exceeds its timeout limit, it should be canceled, and the program should handle this cancellation gracefully. Additionally, implement logging to keep track of: - Tasks that completed successfully, - Tasks that were canceled due to a timeout. Finally, define an entry point of your program: an async function `main` that initializes the asyncio event loop, calls `schedule_tasks` with sample coroutine tasks, and ensures the appropriate cleanup of resources. Requirements: 1. **Function:** `async def schedule_tasks(tasks: List[Coroutine[Any, Any, Any]], timeouts: List[float]) -> None` - **Input:** - `tasks`: A list of coroutine functions (`async def example_task() -> None`). - `timeouts`: A list of float values representing the timeouts for each corresponding task in the `tasks` list. - **Output:** `None` - **Behavior:** - Run all tasks concurrently. - Observe individual timeouts for each task. - Cancel and log tasks that exceed the timeout. - Log tasks that complete successfully. 2. **Function:** `async def main() -> None` - **Behavior:** - Initializes the event loop. - Defines and passes sample tasks and their corresponding timeouts to `schedule_tasks`. - Ensures proper resource cleanup after execution. 3. **Constraints:** - Ensure graceful cancellation and resource cleanup. - Leverage asyncio-specific exceptions such as `asyncio.TimeoutError` and `asyncio.CancelledError`. Sample Tasks: 1. `async def task1() -> None: # simulates I/O bound workload` 2. `async def task2() -> None: # simulates extreme I/O delay` Example: ```python import asyncio from typing import List, Coroutine, Any async def task1(): await asyncio.sleep(2) print(\\"Task 1 completed\\") async def task2(): await asyncio.sleep(5) print(\\"Task 2 completed\\") async def schedule_tasks(tasks: List[Coroutine[Any, Any, Any]], timeouts: List[float]) -> None: for task, timeout in zip(tasks, timeouts): try: await asyncio.wait_for(task, timeout) print(f\\"{task.__name__} completed within {timeout} seconds.\\") except asyncio.TimeoutError: task.cancel() print(f\\"{task.__name__} canceled due to timeout.\\") async def main(): tasks = [task1(), task2()] timeouts = [3, 3] await schedule_tasks(tasks, timeouts) if __name__ == \\"__main__\\": asyncio.run(main()) ``` Ensure your implementation follows the mentioned behavior and constraints.","solution":"import asyncio import logging from typing import List, Coroutine, Any # Set up logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) async def schedule_tasks(tasks: List[Coroutine[Any, Any, Any]], timeouts: List[float]) -> None: for task, timeout in zip(tasks, timeouts): try: await asyncio.wait_for(task, timeout) logger.info(f\\"{task.__name__} completed within {timeout} seconds.\\") except asyncio.TimeoutError: task.cancel() try: await task except asyncio.CancelledError: logger.warning(f\\"{task.__name__} canceled due to timeout.\\") async def main() -> None: async def task1(): await asyncio.sleep(2) print(\\"Task 1 completed\\") async def task2(): await asyncio.sleep(5) print(\\"Task 2 completed\\") tasks = [task1(), task2()] timeouts = [3, 3] await schedule_tasks(tasks, timeouts) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Problem Statement:** Implement a Python function `run_background_processes(commands, timeout=None)` that accepts a list of shell commands (as strings) and an optional timeout in seconds. The function should execute each command in parallel using the `subprocess` module and capture both the stdout and stderr of each process. The function should return a dictionary where the keys are the commands executed and the values are dictionaries with keys `returncode`, `stdout`, and `stderr` corresponding to the output of the process. If a command fails to complete within the given timeout, the function should terminate the process and set its stderr to \\"Process timed out.\\". **Function Signature:** ```python def run_background_processes(commands: list, timeout: int = None) -> dict: ``` **Input:** - `commands` (list): A list of shell commands to run (e.g., `[\'ls -l\', \'echo Hello World\', \'sleep 2\']`) - `timeout` (int, optional): Timeout in seconds for each command to complete. **Output:** - Returns a dictionary where each key is a command (as string) and the value is another dictionary with the following keys: - `returncode`: The return code of the command. - `stdout`: The standard output of the command as a string. - `stderr`: The standard error of the command as a string. **Constraints:** - Each command should be run in parallel. - If a command does not complete within the specified timeout, it should be killed, and the stderr should be set to \\"Process timed out.\\" - A reasonable number of commands (e.g., <= 10) can be assumed. **Example:** ```python commands = [ \'echo Hello World\', \'sleep 2\', \'ls -l\' ] timeout = 1 result = run_background_processes(commands, timeout) print(result) ``` **Expected Output:** ```python { \'echo Hello World\': {\'returncode\': 0, \'stdout\': \'Hello Worldn\', \'stderr\': \'\'}, \'sleep 2\': {\'returncode\': -9, \'stdout\': \'\', \'stderr\': \'Process timed out.\'}, \'ls -l\': {\'returncode\': 0, \'stdout\': \'<directory listing>\', \'stderr\': \'\'} } ``` **Notes:** - You may use any necessary synchronization primitives from threading or multiprocessing to handle running the processes in parallel. - Make sure the solution handles different shell environments and their path lookup behavior correctly. - You may assume the commands do not require user interaction or special environment setup beyond what is provided by the default shell. - Any exceptions related to the execution of the commands should be handled gracefully and reflected in the stderr of the respective command in the output dictionary.","solution":"import subprocess import threading def run_background_processes(commands, timeout=None): def run_command(command, result): try: completed_process = subprocess.run(command, shell=True, capture_output=True, text=True, timeout=timeout) result[command] = { \'returncode\': completed_process.returncode, \'stdout\': completed_process.stdout, \'stderr\': completed_process.stderr } except subprocess.TimeoutExpired: result[command] = { \'returncode\': -9, \'stdout\': \'\', \'stderr\': \'Process timed out.\' } threads = [] results = {} for command in commands: thread = threading.Thread(target=run_command, args=(command, results)) threads.append(thread) thread.start() for thread in threads: thread.join() return results"},{"question":"You are tasked to create a configuration management tool that interacts with Apple\'s property list files (.plist). The tool will read a plist file, modify its content based on some criteria, and then save the modified content back to the file. Objective Write a Python function to: 1. Read a .plist file. 2. Add a new key-value pair to the top-level dictionary. 3. Save the modified dictionary back to the plist file. Requirements - The function should be named `modify_plist`. - It should take three arguments: 1. **input_filepath** (str): The path to the input plist file. 2. **output_filepath** (str): The path where the modified plist file should be saved. 3. **new_key_value** (dict): A dictionary containing the new key-value pair to be added. Function Signature ```python def modify_plist(input_filepath: str, output_filepath: str, new_key_value: dict) -> None: pass ``` Input 1. **input_filepath**: File path to an existing plist file (in binary or XML format). 2. **output_filepath**: File path where the modified plist should be saved (XML format). 3. **new_key_value**: A dictionary with exactly one key-value pair, e.g., `{\'newKey\': \'newValue\'}`. Output - The modified plist file should be saved at the specified `output_filepath`. Constraints - You can assume that the provided plist file is well-formed and can be successfully parsed by `plistlib`. - The `new_key_value` dictionary will always contain exactly one key-value pair. - The key in `new_key_value` is unique and does not already exist in the plist file. Example Suppose we have a plist file `config.plist` with the following content: ```xml <plist version=\\"1.0\\"> <dict> <key>foo</key> <string>bar</string> </dict> </plist> ``` Calling `modify_plist(\'config.plist\', \'modified_config.plist\', {\'newKey\': \'newValue\'})` should create a new file `modified_config.plist` with the following content: ```xml <plist version=\\"1.0\\"> <dict> <key>foo</key> <string>bar</string> <key>newKey</key> <string>newValue</string> </dict> </plist> ``` Use the `plistlib` module provided in Python\'s standard library to achieve this.","solution":"import plistlib def modify_plist(input_filepath: str, output_filepath: str, new_key_value: dict) -> None: Reads a plist file, adds a new key-value pair to the top-level dictionary, and saves the modified dictionary back to the plist file. Parameters: input_filepath (str): The path to the input plist file. output_filepath (str): The path where the modified plist file should be saved. new_key_value (dict): A dictionary containing the new key-value pair to be added. # Read the existing plist file with open(input_filepath, \'rb\') as f: plist_data = plistlib.load(f) # Add the new key-value pair to the dictionary plist_data.update(new_key_value) # Save the modified dictionary back to a plist file with open(output_filepath, \'wb\') as f: plistlib.dump(plist_data, f)"},{"question":"**Question**: Implementing and Evaluating a Regression Model using Scikit-learn Metrics **Objective**: The goal of this task is to assess your ability to define a regression problem, fit a model, and evaluate its performance using various scikit-learn metrics. **Problem Statement**: You are given a dataset containing features `X` and targets `y` for a regression problem. Your task is to: 1. Split the data into training and testing sets. 2. Fit a `LinearRegression` model on the training set. 3. Evaluate the model on the test set using the following metrics: - Mean Absolute Error (MAE) - Mean Squared Error (MSE) - R² Score - Mean Absolute Percentage Error (MAPE) 4. Create a custom scorer that calculates the Median Absolute Error and use it to evaluate your model. **Input and Output Formats**: - `X`: A 2D ndarray of shape (n_samples, n_features) containing the feature values. - `y`: A 1D ndarray of shape (n_samples,) containing the target values. - The function should print the following evaluation metrics: - Mean Absolute Error (MAE) - Mean Squared Error (MSE) - R² Score - Mean Absolute Percentage Error (MAPE) - Median Absolute Error (using the custom scorer) ```python from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error, make_scorer, median_absolute_error from sklearn.model_selection import train_test_split def evaluate_regression_model(X, y): # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Fit a LinearRegression model on the training set model = LinearRegression() model.fit(X_train, y_train) # Predict on the test set y_pred = model.predict(X_test) # Calculate and print Mean Absolute Error (MAE) mae = mean_absolute_error(y_test, y_pred) print(f\\"Mean Absolute Error (MAE): {mae}\\") # Calculate and print Mean Squared Error (MSE) mse = mean_squared_error(y_test, y_pred) print(f\\"Mean Squared Error (MSE): {mse}\\") # Calculate and print R² Score r2 = r2_score(y_test, y_pred) print(f\\"R² Score: {r2}\\") # Calculate and print Mean Absolute Percentage Error (MAPE) mape = mean_absolute_percentage_error(y_test, y_pred) print(f\\"Mean Absolute Percentage Error (MAPE): {mape}\\") # Create a custom scorer for Median Absolute Error and evaluate the model median_ae_scorer = make_scorer(median_absolute_error) median_ae = median_absolute_error(y_test, y_pred) print(f\\"Median Absolute Error (MedAE): {median_ae}\\") # Example usage import numpy as np X, y = np.random.rand(100, 5), np.random.rand(100) evaluate_regression_model(X, y) ``` **Constraints:** - The dataset `X` and `y` should have at least 100 samples. - Random seed should be set to 42 for reproducibility. - The model should be a `LinearRegression` model from scikit-learn. **Performance Requirements:** - The implementation should be efficient and clear. - Use scikit-learn\'s built-in functions wherever applicable. - Ensure that the custom scorer is implemented correctly and used to evaluate the model.","solution":"from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error, make_scorer, median_absolute_error from sklearn.model_selection import train_test_split def evaluate_regression_model(X, y): # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Fit a LinearRegression model on the training set model = LinearRegression() model.fit(X_train, y_train) # Predict on the test set y_pred = model.predict(X_test) # Calculate and print Mean Absolute Error (MAE) mae = mean_absolute_error(y_test, y_pred) print(f\\"Mean Absolute Error (MAE): {mae}\\") # Calculate and print Mean Squared Error (MSE) mse = mean_squared_error(y_test, y_pred) print(f\\"Mean Squared Error (MSE): {mse}\\") # Calculate and print R² Score r2 = r2_score(y_test, y_pred) print(f\\"R² Score: {r2}\\") # Calculate and print Mean Absolute Percentage Error (MAPE) mape = mean_absolute_percentage_error(y_test, y_pred) print(f\\"Mean Absolute Percentage Error (MAPE): {mape}\\") # Calculate and print Median Absolute Error medae = median_absolute_error(y_test, y_pred) print(f\\"Median Absolute Error (MedAE): {medae}\\") # Example usage import numpy as np X, y = np.random.rand(100, 5), np.random.rand(100) evaluate_regression_model(X, y)"},{"question":"# Naive Bayes Classifier with Incremental Learning You are tasked with implementing a Naive Bayes classifier to classify a dataset incrementally. We will use the `GaussianNB` classifier from the `sklearn.naive_bayes` module. Given the size of the dataset, assume it does not fit into memory all at once and needs to be processed in chunks. The task involves loading the dataset, splitting it into training and testing sets, applying incremental learning on the training data, and evaluating the model on the testing data. Dataset Use the Iris dataset provided by the `sklearn.datasets` module for this task. Steps: 1. **Load the dataset**: Use `load_iris()` from `sklearn.datasets`. 2. **Split the dataset**: Split it into training (70%) and testing (30%) sets. 3. **Preprocess the dataset**: Since we are using `GaussianNB`, no special preprocessing is required, but ensure the data is in the correct format. 4. **Implement Incremental Learning**: - Use the `partial_fit` method from `GaussianNB`. - Process the training data in chunks. 5. **Evaluate the classifier**: - Use appropriate metrics such as accuracy to evaluate the classifier\'s performance on the test set. Requirements: - Use `numpy` for handling the dataset divisions and chunking. - Implement the function `incremental_gaussian_nb` that takes no arguments and returns the accuracy of the model on the test data. Function Signature ```python import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score def incremental_gaussian_nb() -> float: # Load the dataset data = load_iris() X, y = data.data, data.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize the GaussianNB classifier gnb = GaussianNB() # Number of chunks to split the data into n_chunks = 5 chunk_size = len(X_train) // n_chunks # Incremental learning for i in range(n_chunks): start = i * chunk_size end = (i + 1) * chunk_size if i != n_chunks - 1 else len(X_train) X_chunk = X_train[start:end] y_chunk = y_train[start:end] if i == 0: gnb.partial_fit(X_chunk, y_chunk, classes=np.unique(y)) else: gnb.partial_fit(X_chunk, y_chunk) # Predict on the test set y_pred = gnb.predict(X_test) # Calculate accuracy accuracy = accuracy_score(y_test, y_pred) return accuracy # Example usage: # accuracy = incremental_gaussian_nb() # print(f\\"Accuracy: {accuracy:.2f}\\") ``` Your task is to implement the `incremental_gaussian_nb` function and return the accuracy of the classifier on the test data. Ensure the function processes the training data in chunks and uses the `partial_fit` method for incremental learning.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score def incremental_gaussian_nb() -> float: # Load the dataset data = load_iris() X, y = data.data, data.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize the GaussianNB classifier gnb = GaussianNB() # Number of chunks to split the data into n_chunks = 5 chunk_size = len(X_train) // n_chunks # Incremental learning for i in range(n_chunks): start = i * chunk_size end = (i + 1) * chunk_size if i != n_chunks - 1 else len(X_train) X_chunk = X_train[start:end] y_chunk = y_train[start:end] if i == 0: gnb.partial_fit(X_chunk, y_chunk, classes=np.unique(y)) else: gnb.partial_fit(X_chunk, y_chunk) # Predict on the test set y_pred = gnb.predict(X_test) # Calculate accuracy accuracy = accuracy_score(y_test, y_pred) return accuracy # Example usage: # accuracy = incremental_gaussian_nb() # print(f\\"Accuracy: {accuracy:.2f}\\")"},{"question":"# Problem: RobotFileChecker You are tasked with creating a utility to check which URLs a user-agent can access based on the rules specified in a \\"robots.txt\\" file on a given website. Your utility should also return other parameters such as crawl delay, request rate, and sitemaps if specified in the \\"robots.txt\\" file. # Implementation Requirements 1. Implement a function `robot_file_checker(base_url: str, useragent: str) -> dict` that accepts: - `base_url` (str): The base URL of the website where \\"robots.txt\\" is located. - `useragent` (str): The user-agent string that will be checked against the \\"robots.txt\\" rules. 2. The function should: - Read and parse the \\"robots.txt\\" file from the provided `base_url`. - Check if the user-agent can access a predefined set of URLs. - Return a dictionary with the following keys and their corresponding values: - `\\"can_fetch\\"` (dict): A dictionary with URLs as keys and boolean values indicating if the `useragent` can access them. - `\\"crawl_delay\\"` (int or None): The crawl delay for the `useragent`. - `\\"request_rate\\"` (tuple or None): A tuple `(requests, seconds)` representing the request rate for the `useragent`. - `\\"sitemaps\\"` (list or None): A list of sitemap URLs found in the \\"robots.txt\\" file. # Input Specifications - `base_url` will be a valid URL string. - `useragent` will be a non-empty string. - URL list to check: Assume a predefined list of URLs to check exists for the exercise. # Output Format - A dictionary with the specified structure. # Example Input ```python base_url = \\"http://www.musi-cal.com/\\" useragent = \\"*\\" ``` # Example URLs to check ```python urls_to_check = [ \\"http://www.musi-cal.com/cgi-bin/search?city=San+Francisco\\", \\"http://www.musi-cal.com/\\" ] ``` # Example Output ```python { \\"can_fetch\\": { \\"http://www.musi-cal.com/cgi-bin/search?city=San+Francisco\\": False, \\"http://www.musi-cal.com/\\": True }, \\"crawl_delay\\": 6, \\"request_rate\\": (3, 20), \\"sitemaps\\": None } ``` # Constraints - You must use the `urllib.robotparser` library. - Handle network errors or missing \\"robots.txt\\" files gracefully by providing meaningful default values in the dictionary. # Implementation Notes - Ensure that you handle exceptions, such as network errors or invalid \\"robots.txt\\" formats. - You may need to preprocess the `base_url` to appropriately point to \\"robots.txt\\". - Think about the efficiency of your solution, especially how you handle multiple URLs and network requests.","solution":"import urllib.robotparser import requests def robot_file_checker(base_url: str, useragent: str) -> dict: urls_to_check = [ \\"http://www.musi-cal.com/cgi-bin/search?city=San+Francisco\\", \\"http://www.musi-cal.com/\\" ] robots_url = urllib.parse.urljoin(base_url, \\"/robots.txt\\") rp = urllib.robotparser.RobotFileParser(robots_url) try: rp.read() except Exception as e: return { \\"can_fetch\\": {url: False for url in urls_to_check}, \\"crawl_delay\\": None, \\"request_rate\\": None, \\"sitemaps\\": None } can_fetch = {} for url in urls_to_check: can_fetch[url] = rp.can_fetch(useragent, url) crawl_delay = rp.crawl_delay(useragent) request_rate = rp.request_rate(useragent) sitemaps = rp.site_maps() return { \\"can_fetch\\": can_fetch, \\"crawl_delay\\": crawl_delay, \\"request_rate\\": request_rate, \\"sitemaps\\": sitemaps }"},{"question":"**Title:** Implementing a Custom Descriptor with Validation **Objective:** Demonstrate the ability to implement and use Python descriptors for managing attribute access and validation. **Problem Statement:** Create a custom descriptor in Python that acts as a validator for instance attributes. Your task is to implement a descriptor class `ValidatedAttribute` that ensures assigned values are within a specified range. The `ValidatedAttribute` descriptor should be capable of: - Validating integer values to ensure they fall within a specified minimum and maximum range. - Raising appropriate exceptions when the validation fails. **Requirements:** 1. Implement the `ValidatedAttribute` class with the following methods: - `__init__(self, min_value=None, max_value=None)`: Initialize the descriptor with optional `min_value` and `max_value` parameters. - `__set__(self, instance, value)`: Set the value, raising `ValueError` if the value is not within the specified range or if it is not an integer. - `__get__(self, instance, owner=None)`: Retrieve the value from the instance. 2. Use the `ValidatedAttribute` descriptor in a class `Product` to manage and validate attributes for `price` and `quantity`. 3. Ensure your solution follows these constraints: - `price` should be a positive integer. - `quantity` should be a non-negative integer. 4. Provide code that demonstrates the functionality and validation of the implemented descriptor within the `Product` class. # Example Usage: ```python class ValidatedAttribute: def __init__(self, min_value=None, max_value=None): self.min_value = min_value self.max_value = max_value self._name = \'\' def __set_name__(self, owner, name): self._name = name def __get__(self, instance, owner=None): if instance is None: return self return instance.__dict__.get(self._name) def __set__(self, instance, value): if not isinstance(value, int): raise ValueError(f\\"{self._name} must be an integer\\") if self.min_value is not None and value < self.min_value: raise ValueError(f\\"{self._name} must be at least {self.min_value}\\") if self.max_value is not None and value > self.max_value: raise ValueError(f\\"{self._name} must be at most {self.max_value}\\") instance.__dict__[self._name] = value class Product: price = ValidatedAttribute(min_value=1) quantity = ValidatedAttribute(min_value=0) def __init__(self, name, price, quantity): self.name = name self.price = price self.quantity = quantity # Example usage try: apple = Product(\\"Apple\\", 1, 10) banana = Product(\\"Banana\\", -1, 10) # Should raise ValueError except ValueError as e: print(e) try: candy = Product(\\"Candy\\", 5, -5) # Should raise ValueError except ValueError as e: print(e) ``` **Input and Output:** - *Input*: An instance creation or value assignment for `Product` objects, e.g., `Product(\\"Apple\\", 1, 10)`. - *Output*: Either successful creation/assignment or an exception message indicating the validation problem. # Constraints: - Ensure your solution handles both instance creation and attribute assignment. - Provide informative error messages for validation failures. **Evaluation Criteria:** - Correctness: Does the implementation correctly validate and set/retrieve the attributes? - Error Handling: Are appropriate exceptions raised with informative messages? - Code Quality: Is the code well-organized, readable, and follows Python conventions? Good luck!","solution":"class ValidatedAttribute: def __init__(self, min_value=None, max_value=None): self.min_value = min_value self.max_value = max_value self._name = \'\' def __set_name__(self, owner, name): self._name = name def __get__(self, instance, owner=None): if instance is None: return self return instance.__dict__.get(self._name) def __set__(self, instance, value): if not isinstance(value, int): raise ValueError(f\\"{self._name} must be an integer\\") if self.min_value is not None and value < self.min_value: raise ValueError(f\\"{self._name} must be at least {self.min_value}\\") if self.max_value is not None and value > self.max_value: raise ValueError(f\\"{self._name} must be at most {self.max_value}\\") instance.__dict__[self._name] = value class Product: price = ValidatedAttribute(min_value=1) quantity = ValidatedAttribute(min_value=0) def __init__(self, name, price, quantity): self.name = name self.price = price self.quantity = quantity # Example usage try: apple = Product(\\"Apple\\", 1, 10) banana = Product(\\"Banana\\", -1, 10) # Should raise ValueError except ValueError as e: print(e) try: candy = Product(\\"Candy\\", 5, -5) # Should raise ValueError except ValueError as e: print(e)"},{"question":"# Clustering Implementation and Evaluation Objective: The goal of this task is to assess your understanding of clustering algorithms, their implementation using `scikit-learn`, and the ability to evaluate and compare the clustering results. Problem Statement: You are given a dataset consisting of 2D data points. Your task is to: 1. Implement K-Means, DBSCAN, and Agglomerative Clustering algorithms using `scikit-learn`. 2. Evaluate and compare the clustering results using the Silhouette Coefficient and the Calinski-Harabasz Index. 3. Determine the most suitable algorithm for the given dataset based on your evaluation. Instructions: 1. **Implement Clustering Algorithms**: - Load the dataset from the provided `data.csv` file. - Apply K-Means, DBSCAN, and Agglomerative Clustering on the dataset. - For K-Means, use `n_clusters=3`. - For DBSCAN, use `eps=0.5` and `min_samples=5`. - For Agglomerative Clustering, use `n_clusters=3` and `linkage=\'ward\'`. 2. **Evaluate the Clustering Results**: - Calculate the Silhouette Coefficient and the Calinski-Harabasz Index for each clustering result. 3. **Compare and Interpret Results**: - Based on the evaluation metrics, compare the results of the three algorithms. - Write your observations regarding which algorithm performed the best and why. Input Format: - `data.csv`: A CSV file containing the 2D data points with two columns `x` and `y`. Expected Output: Your submission should include: 1. The implementation code. 2. The evaluation results of each clustering algorithm. 3. A brief report (2-3 paragraphs) comparing the algorithms and interpreting the results. Constraints: - Use `scikit-learn` for implementing the clustering algorithms. - Ensure your code is well-documented and follows best practices. Example: ```python import pandas as pd from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering from sklearn.metrics import silhouette_score, calinski_harabasz_score # Load the dataset data = pd.read_csv(\'data.csv\') # Implement K-Means kmeans = KMeans(n_clusters=3, random_state=42) kmeans_labels = kmeans.fit_predict(data) # Implement DBSCAN dbscan = DBSCAN(eps=0.5, min_samples=5) dbscan_labels = dbscan.fit_predict(data) # Implement Agglomerative Clustering agg_clustering = AgglomerativeClustering(n_clusters=3, linkage=\'ward\') agg_labels = agg_clustering.fit_predict(data) # Evaluate using Silhouette Coefficient kmeans_silhouette = silhouette_score(data, kmeans_labels) dbscan_silhouette = silhouette_score(data, dbscan_labels) agg_silhouette = silhouette_score(data, agg_labels) # Evaluate using Calinski-Harabasz Index kmeans_calinski = calinski_harabasz_score(data, kmeans_labels) dbscan_calinski = calinski_harabasz_score(data, dbscan_labels) agg_calinski = calinski_harabasz_score(data, agg_labels) # Print results print(f\'K-Means: Silhouette Coefficient={kmeans_silhouette}, Calinski-Harabasz Index={kmeans_calinski}\') print(f\'DBSCAN: Silhouette Coefficient={dbscan_silhouette}, Calinski-Harabasz Index={dbscan_calinski}\') print(f\'Agglomerative: Silhouette Coefficient={agg_silhouette}, Calinski-Harabasz Index={agg_calinski}\') # Report report = Based on the results, Agglomerative Clustering performed the best with the highest Silhouette Coefficient and Calinski-Harabasz Index. K-Means also performed well, but DBSCAN struggled, possibly due to the parameter choices or the nature of the dataset. print(report) ``` **Note:** The provided example code is for illustrative purposes only and should be adapted based on your specific dataset and requirements.","solution":"import pandas as pd from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering from sklearn.metrics import silhouette_score, calinski_harabasz_score def load_data(file_path): return pd.read_csv(file_path) def apply_kmeans(data, n_clusters=3): kmeans = KMeans(n_clusters=n_clusters, random_state=42) labels = kmeans.fit_predict(data) return labels def apply_dbscan(data, eps=0.5, min_samples=5): dbscan = DBSCAN(eps=eps, min_samples=min_samples) labels = dbscan.fit_predict(data) return labels def apply_agglomerative(data, n_clusters=3, linkage=\'ward\'): agg_clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage) labels = agg_clustering.fit_predict(data) return labels def evaluate_clustering(data, labels): silhouette = silhouette_score(data, labels) calinski = calinski_harabasz_score(data, labels) return silhouette, calinski def main(): data = load_data(\'data.csv\') kmeans_labels = apply_kmeans(data) dbscan_labels = apply_dbscan(data) agg_labels = apply_agglomerative(data) kmeans_silhouette, kmeans_calinski = evaluate_clustering(data, kmeans_labels) dbscan_silhouette, dbscan_calinski = evaluate_clustering(data, dbscan_labels) agg_silhouette, agg_calinski = evaluate_clustering(data, agg_labels) print(f\'K-Means: Silhouette Coefficient={kmeans_silhouette}, Calinski-Harabasz Index={kmeans_calinski}\') print(f\'DBSCAN: Silhouette Coefficient={dbscan_silhouette}, Calinski-Harabasz Index={dbscan_calinski}\') print(f\'Agglomerative: Silhouette Coefficient={agg_silhouette}, Calinski-Harabasz Index={agg_calinski}\') report = Based on the evaluation metrics, we can determine the most suitable clustering algorithm for the given dataset. In this case, the following observations can be made: - K-Means provides reasonable clustering performance, suitable for well-separated clusters. - DBSCAN is effective for identifying arbitrary shapes and handling noise, but parameter sensitivity must be managed. - Agglomerative Clustering delivers strong performance with hierarchical structure and is also effective for well-separated clusters. The precise recommendation depends on the evaluation results, which suggest Agglomerative Clustering performed best in this scenario. print(report) if __name__ == \\"__main__\\": main()"},{"question":"# Predictive Modeling using Partial Least Squares Regression You are given a dataset consisting of physiological measurements (predictors X) and corresponding health grades (responses Y) of several patients. Your task is to implement a predictive model using the `PLSRegression` algorithm from the `scikit-learn` library. This model should predict the health grades of patients based on their physiological measurements. Dataset Description The dataset contains two NumPy arrays: - `X`: a `(n_samples, n_features)` array representing the physiological measurements. - `Y`: a `(n_samples, n_targets)` array representing the health grades. Task 1. **Data Splitting**: - Split the given dataset into a training set and a testing set using an 80-20 split. 2. **Model Training**: - Implement the `PLSRegression` model to fit the training data. - Use an appropriate number of components for the PLS model (explore different values to find the best fit). 3. **Model Evaluation**: - Predict the health grades for the testing set. - Evaluate the model performance using metrics such as Mean Squared Error (MSE) and R-squared score. 4. **Function Implementation**: - Implement a function `pls_predict(X, Y, n_components)` that: - Splits the dataset. - Trains the `PLSRegression` model with the specified number of components. - Predicts and returns the health grades for the testing set. - Returns the evaluation metrics for the model. Function Signature ```python import numpy as np from sklearn.model_selection import train_test_split from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import mean_squared_error, r2_score def pls_predict(X: np.ndarray, Y: np.ndarray, n_components: int) -> (np.ndarray, float, float): Predict health grades using PLSRegression and evaluate the model. Parameters: - X: np.ndarray, shape (n_samples, n_features) - Y: np.ndarray, shape (n_samples, n_targets) - n_components: int, number of components for PLSRegression Returns: - y_pred: np.ndarray, predicted health grades for the test set. - mse: float, Mean Squared Error of the predictions. - r2: float, R-squared score of the predictions. pass ``` Constraints and Considerations - Ensure proper scaling of the data if necessary. - Consider the appropriate handling of potential overfitting. - Feel free to use additional metrics for a more comprehensive evaluation. Notes 1. This task tests your understanding of the `PLSRegression` algorithm, data processing steps, and model evaluation techniques. 2. Clearly comment your code to illustrate the steps and decisions made during the implementation.","solution":"import numpy as np from sklearn.model_selection import train_test_split from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import mean_squared_error, r2_score from sklearn.preprocessing import StandardScaler def pls_predict(X: np.ndarray, Y: np.ndarray, n_components: int) -> (np.ndarray, float, float): Predict health grades using PLSRegression and evaluate the model. Parameters: - X: np.ndarray, shape (n_samples, n_features) - Y: np.ndarray, shape (n_samples, n_targets) - n_components: int, number of components for PLSRegression Returns: - y_pred: np.ndarray, predicted health grades for the test set. - mse: float, Mean Squared Error of the predictions. - r2: float, R-squared score of the predictions. # Split the data into training and testing sets (80-20 split) X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42) # Standardize the data scaler_X = StandardScaler().fit(X_train) scaler_Y = StandardScaler().fit(Y_train) X_train_scaled = scaler_X.transform(X_train) X_test_scaled = scaler_X.transform(X_test) Y_train_scaled = scaler_Y.transform(Y_train) # Train the PLSRegression model pls = PLSRegression(n_components=n_components) pls.fit(X_train_scaled, Y_train_scaled) # Predict the health grades on the test set Y_pred_scaled = pls.predict(X_test_scaled) Y_pred = scaler_Y.inverse_transform(Y_pred_scaled) # Evaluate the model mse = mean_squared_error(Y_test, Y_pred) r2 = r2_score(Y_test, Y_pred) return Y_pred, mse, r2"},{"question":"# Persistent Data Storage with Pickle and SQLite You\'re tasked with designing a Python application that manages a simple contact book using two different methods for data persistence: (`pickle` and `sqlite3`). The application should be able to store, retrieve, and update contact information persistently. Task 1. **Using Pickle:** - Implement functions to save and load contact records to/from a file using the `pickle` module. - Contacts should be stored as a dictionary with the following keys: \\"name\\", \\"phone_number\\", and \\"email\\". ```python import pickle def save_contacts_to_file(contacts, filename): Serialize the contacts dictionary and save it to the specified file. :param contacts: List of contact dictionaries to be saved. :param filename: The name of the file where data will be stored. pass def load_contacts_from_file(filename): Load the contacts list from the specified file and deserialize it. :param filename: The name of the file from which data will be loaded. :return: A list of contact dictionaries. pass ``` 2. **Using SQLite:** - Implement functions to save and retrieve contact records from a SQLite database. - Create a table `contacts` with columns \\"name\\", \\"phone_number\\", and \\"email\\". - Implement functions to insert, retrieve, update, and delete contacts. ```python import sqlite3 def create_contacts_table(db_name): Create a contacts table if it does not already exist. :param db_name: The name of the database file. pass def insert_contact(db_name, name, phone_number, email): Insert a new contact into the contacts table. :param db_name: The name of the database file. :param name: Contact\'s name as a string. :param phone_number: Contact\'s phone number as a string. :param email: Contact\'s email as a string. pass def get_contacts(db_name): Retrieve all contacts from the contacts table. :param db_name: The name of the database file. :return: A list of tuples containing contact information. pass def update_contact(db_name, old_name, new_name=None, new_phone_number=None, new_email=None): Update an existing contact\'s information. :param db_name: The name of the database file. :param old_name: The name of the contact to update. :param new_name: The new name of the contact (optional). :param new_phone_number: The new phone number of the contact (optional). :param new_email: The new email of the contact (optional). pass def delete_contact(db_name, name): Delete a contact from the contacts table by name. :param db_name: The name of the database file. :param name: The name of the contact to delete. pass ``` Requirements - Implement the functions to correctly perform serialization/deserialization using `pickle` and CRUD operations using `sqlite3`. - Include error handling for file and database operations. - Write appropriate docstrings for each function. Constraints - The contacts file should handle cases where the file does not exist by creating a new one when saving contacts. - The SQLite database should handle cases where the database or table does not exist by creating them as needed. Evaluation - Correct implementation of all required functions. - Proper error handling and input validation. - Adherence to Python coding standards and best practices. - Efficiency and performance of the implemented solution.","solution":"import pickle import sqlite3 import os def save_contacts_to_file(contacts, filename): Serialize the contacts dictionary and save it to the specified file. :param contacts: List of contact dictionaries to be saved. :param filename: The name of the file where data will be stored. with open(filename, \'wb\') as file: pickle.dump(contacts, file) def load_contacts_from_file(filename): Load the contacts list from the specified file and deserialize it. :param filename: The name of the file from which data will be loaded. :return: A list of contact dictionaries. if not os.path.exists(filename): return [] with open(filename, \'rb\') as file: contacts = pickle.load(file) return contacts def create_contacts_table(db_name): Create a contacts table if it does not already exist. :param db_name: The name of the database file. connection = sqlite3.connect(db_name) cursor = connection.cursor() cursor.execute(\'\'\' CREATE TABLE IF NOT EXISTS contacts ( name TEXT PRIMARY KEY, phone_number TEXT, email TEXT ) \'\'\') connection.commit() connection.close() def insert_contact(db_name, name, phone_number, email): Insert a new contact into the contacts table. :param db_name: The name of the database file. :param name: Contact\'s name as a string. :param phone_number: Contact\'s phone number as a string. :param email: Contact\'s email as a string. connection = sqlite3.connect(db_name) cursor = connection.cursor() cursor.execute(\'\'\' INSERT OR REPLACE INTO contacts (name, phone_number, email) VALUES (?, ?, ?) \'\'\', (name, phone_number, email)) connection.commit() connection.close() def get_contacts(db_name): Retrieve all contacts from the contacts table. :param db_name: The name of the database file. :return: A list of tuples containing contact information. connection = sqlite3.connect(db_name) cursor = connection.cursor() cursor.execute(\'SELECT name, phone_number, email FROM contacts\') contacts = cursor.fetchall() connection.close() return contacts def update_contact(db_name, old_name, new_name=None, new_phone_number=None, new_email=None): Update an existing contact\'s information. :param db_name: The name of the database file. :param old_name: The name of the contact to update. :param new_name: The new name of the contact (optional). :param new_phone_number: The new phone number of the contact (optional). :param new_email: The new email of the contact (optional). connection = sqlite3.connect(db_name) cursor = connection.cursor() if new_name: cursor.execute(\'UPDATE contacts SET name = ? WHERE name = ?\', (new_name, old_name)) if new_phone_number: cursor.execute(\'UPDATE contacts SET phone_number = ? WHERE name = ?\', (new_phone_number, old_name)) if new_email: cursor.execute(\'UPDATE contacts SET email = ? WHERE name = ?\', (new_email, old_name)) connection.commit() connection.close() def delete_contact(db_name, name): Delete a contact from the contacts table by name. :param db_name: The name of the database file. :param name: The name of the contact to delete. connection = sqlite3.connect(db_name) cursor = connection.cursor() cursor.execute(\'DELETE FROM contacts WHERE name = ?\', (name,)) connection.commit() connection.close()"},{"question":"Objective Create a Python function that constructs an `EmailMessage` object representing a multipart email with the following specifications. Implement and demonstrate the ability to manipulate headers and payloads according to RFC standards. Specifications 1. **Function Definition**: ```python def create_multipart_email(sender, recipient, subject, body_text, attachment_path): ``` - **Parameters**: - `sender` (str): The email address of the sender. - `recipient` (str): The email address of the recipient. - `subject` (str): The subject of the email. - `body_text` (str): The plain text body of the email. - `attachment_path` (str): The file path of the attachment to include. 2. **Function Behavior**: - Create an `EmailMessage` object. - Set the appropriate headers (e.g., `From`, `To`, `Subject`, etc.). - Include a plain text body section. - Attach a file as an attachment. - Ensure the email is a valid multipart MIME message with correct content types. - Serialize the entire message to both a string and bytes representation. 3. **Output**: - Return a tuple `(email_string, email_bytes)` where: - `email_string` is the entire email message serialized as a string. - `email_bytes` is the entire email message serialized as a bytes object. Constraints - Follow the RFC guidelines for email structuring. - Handle file attachment correctly, ensuring it has appropriate headers and Content-Disposition. - Assume the attachment file exists at the provided path and is accessible. Example Usage ```python email_string, email_bytes = create_multipart_email( sender=\\"sender@example.com\\", recipient=\\"recipient@example.com\\", subject=\\"Test Email\\", body_text=\\"This is a test email with attachment\\", attachment_path=\\"/path/to/attachment.txt\\" ) print(email_string) # Should print the entire email message as a string print(email_bytes) # Should print the serialized bytes of the email message ``` **Bonus Challenge**: Implement an additional function `parse_email` that takes the string representation of an email and reconstructs the original `EmailMessage` object. Solution Template ```python from email.message import EmailMessage import os def create_multipart_email(sender, recipient, subject, body_text, attachment_path): # Create the EmailMessage object msg = EmailMessage() # Set the headers msg[\'From\'] = sender msg[\'To\'] = recipient msg[\'Subject\'] = subject # Set the plain text body msg.set_content(body_text) # Attach the file with open(attachment_path, \'rb\') as file: file_data = file.read() file_name = os.path.basename(file.name) # Assuming the attachment is a text file msg.add_attachment(file_data, maintype=\'application\', subtype=\'octet-stream\', filename=file_name) # Serialize the message email_string = msg.as_string() email_bytes = msg.as_bytes() return email_string, email_bytes # Uncomment for bonus challenge # def parse_email(email_string): # pass ``` **Note**: The bonus challenge involves parsing the email string back into an `EmailMessage` object. This could necessitate understanding the creation and parsing relationship more deeply.","solution":"from email.message import EmailMessage import os def create_multipart_email(sender, recipient, subject, body_text, attachment_path): Creates a multipart email with a plain text body and an attachment. Arguments: sender -- the email address of the sender recipient -- the email address of the recipient subject -- the subject of the email body_text -- the plain text body of the email attachment_path -- the file path of the attachment to include Returns: A tuple (email_string, email_bytes) where: - email_string is the entire email message serialized as a string - email_bytes is the entire email message serialized as a bytes object # Create the EmailMessage object msg = EmailMessage() # Set the headers msg[\'From\'] = sender msg[\'To\'] = recipient msg[\'Subject\'] = subject # Set the plain text body msg.set_content(body_text) # Attach the file with open(attachment_path, \'rb\') as file: file_data = file.read() file_name = os.path.basename(file.name) # Assuming the attachment is a text file msg.add_attachment(file_data, maintype=\'application\', subtype=\'octet-stream\', filename=file_name) # Serialize the message email_string = msg.as_string() email_bytes = msg.as_bytes() return email_string, email_bytes"},{"question":"You are required to implement a custom class in Python that supports both shallow and deep copy operations. Your task is to define a class `CustomList` which encapsulates a list of elements and demonstrates customized copying behaviors using the `copy` module. Class Specification: 1. **Class Name**: `CustomList` 2. **Attributes**: - `_elements`: A list that stores the elements of the `CustomList`. Methods to Implement: 1. **`__init__(self, elements: list)`**: - Initializes the object with the given list of elements. 2. **`__copy__(self)`**: - Implements the shallow copy mechanism. - Should return a new `CustomList` object with a shallow copy of the `_elements` list. 3. **`__deepcopy__(self, memo)`**: - Implements the deep copy mechanism. - Should return a new `CustomList` object with a deep copy of the `_elements` list. - Ensure to utilize the `memo` dictionary to track copied objects to handle recursive objects correctly. 4. **`__repr__(self)`**: - Returns a string representation of the `CustomList` object for easy visualization. Input: - The input will be a list of elements on instantiation. - CustomList objects used as elements to demonstrate recursive copying. Output: - Shallow and deep copies of `CustomList` objects. Constraints: - Elements within the `CustomList` can be of any data type. - `CustomList` objects can contain other `CustomList` objects. Example Usage: ```python import copy # Original list original = CustomList([1, [2, 3], CustomList([4, 5])]) # Shallow copying shallow_copied = copy.copy(original) # Deep copying deep_copied = copy.deepcopy(original) print(\\"Original:\\", original) print(\\"Shallow Copy:\\", shallow_copied) print(\\"Deep Copy:\\", deep_copied) ``` Expected Behavior: - Modifying the outer list in the shallow copy should reflect in the original, but modifications inside nested lists or `CustomList` objects should not. - Modifying any part of the deep copy should not reflect in the original or shallow copy. Use the provided documentation on the `copy` module to properly implement the shallow and deep copy mechanisms.","solution":"import copy class CustomList: def __init__(self, elements): self._elements = elements def __copy__(self): new_elements = self._elements[:] return CustomList(new_elements) def __deepcopy__(self, memo): new_elements = copy.deepcopy(self._elements, memo) return CustomList(new_elements) def __repr__(self): return f\'CustomList({self._elements})\'"},{"question":"Your task is to implement a function that performs a series of linear algebra operations on a given square matrix. You will be required to: 1. Check if the matrix is invertible. 2. If the matrix is invertible, compute its inverse. 3. Compute the determinant of the matrix. 4. Perform Singular Value Decomposition (SVD) on the matrix. 5. Compute the eigenvalues and eigenvectors of the matrix. 6. Return all results in an organized format. # Input: - A square matrix `A` of shape `(n, n)` represented as a PyTorch FloatTensor. # Output: - A dictionary with the following keys and corresponding values: - `\'invertible\'`: A boolean indicating if the matrix is invertible. - `\'inverse\'`: The inverse of the matrix if it is invertible, otherwise `None`. - `\'determinant\'`: The determinant of the matrix. - `\'SVD\'`: A tuple with the U, S, and V matrices from the Singular Value Decomposition. - `\'eigen\'`: A tuple with the eigenvalues and the matrix of eigenvectors. # Constraints: - You can assume the input matrix `A` is always a square matrix with at least one dimension >= 2. # Performance Requirements: - The function should handle matrices up to the size of `1000 x 1000` efficiently. # Example: ```python import torch def analyze_matrix(A): result = {} # Check if the matrix is invertible try: inv_A = torch.linalg.inv(A) result[\'invertible\'] = True result[\'inverse\'] = inv_A except RuntimeError: result[\'invertible\'] = False result[\'inverse\'] = None # Compute the determinant result[\'determinant\'] = torch.linalg.det(A) # Perform Singular Value Decomposition U, S, V = torch.linalg.svd(A) result[\'SVD\'] = (U, S, V) # Compute the eigenvalues and eigenvectors eigenvalues, eigenvectors = torch.linalg.eig(A) result[\'eigen\'] = (eigenvalues, eigenvectors) return result # Example usage A = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) print(analyze_matrix(A)) # Expected Output: # { # \'invertible\': True, # \'inverse\': tensor([[...]]), # \'determinant\': tensor(-2.0000), # \'SVD\': (tensor([...]), tensor([...]), tensor([...])), # \'eigen\': (tensor([...]), tensor([...])), # } ```","solution":"import torch def analyze_matrix(A): result = {} # Check if the matrix is invertible try: inv_A = torch.linalg.inv(A) result[\'invertible\'] = True result[\'inverse\'] = inv_A except RuntimeError: result[\'invertible\'] = False result[\'inverse\'] = None # Compute the determinant result[\'determinant\'] = torch.linalg.det(A) # Perform Singular Value Decomposition U, S, V = torch.linalg.svd(A) result[\'SVD\'] = (U, S, V) # Compute the eigenvalues and eigenvectors eigenvalues, eigenvectors = torch.linalg.eig(A) result[\'eigen\'] = (eigenvalues, eigenvectors) return result"},{"question":"# Python Coding Assessment Objective Write a Python program that uses the `collections.Counter` class to mimic the behavior of several C-API dictionary functions described in the provided documentation. Your task is to implement four specific functions based on the given descriptions. These functions will operate at a higher level in Python but should showcase a clear understanding of dictionary operations and structuring. Background The `collections.Counter` class in Python is a specialized dictionary subclass designed for counting hashable objects. However, we want to extend its functionality to support specific operations that you might find in the C-API, specifically `PyDict_SetItem`, `PyDict_GetItem`, `PyDict_DelItem`, and `PyDict_Merge`. Task You need to implement the following functions: 1. `custom_dict_set_item(counter, key, value)` 2. `custom_dict_get_item(counter, key)` 3. `custom_dict_del_item(counter, key)` 4. `custom_dict_merge(counter1, counter2, override=True)` Function Descriptions 1. **`custom_dict_set_item(counter, key, value)`**: - **Input**: - `counter`: A `collections.Counter` object. - `key`: A hashable key. - `value`: An integer count to set for the given key. - **Output**: `None` - **Description**: Insert or update the count of `key` in `counter` to `value`. - **Constraints**: `value` must be an integer. 2. **`custom_dict_get_item(counter, key)`**: - **Input**: - `counter`: A `collections.Counter` object. - `key`: A hashable key. - **Output**: The integer count associated with `key`, or `None` if the key is not present. - **Description**: Retrieve the count for `key` from `counter`. If `key` isn\'t found, return `None`. 3. **`custom_dict_del_item(counter, key)`**: - **Input**: - `counter`: A `collections.Counter` object. - `key`: A hashable key. - **Output**: `None` - **Description**: Remove `key` and its count from `counter`. If `key` is not found, raise a `KeyError`. 4. **`custom_dict_merge(counter1, counter2, override=True)`**: - **Input**: - `counter1`: A `collections.Counter` object to be updated. - `counter2`: A `collections.Counter` object whose items are to be merged into `counter1`. - `override`: A boolean indicating if `counter1` should overwrite existing keys with values from `counter2`. Default is `True`. - **Output**: `None` - **Description**: Merge items from `counter2` into `counter1`. If `override` is `True`, existing counts in `counter1` will be overwritten by those in `counter2`. If `override` is `False`, existing keys in `counter1` will retain their counts. Example ```python from collections import Counter def custom_dict_set_item(counter, key, value): # Implementation here def custom_dict_get_item(counter, key): # Implementation here def custom_dict_del_item(counter, key): # Implementation here def custom_dict_merge(counter1, counter2, override=True): # Implementation here # Example Usage counter1 = Counter({\'a\': 3, \'b\': 5}) custom_dict_set_item(counter1, \'c\', 10) print(custom_dict_get_item(counter1, \'c\')) # Output: 10 custom_dict_del_item(counter1, \'b\') print(counter1) # Output: Counter({\'a\': 3, \'c\': 10}) counter2 = Counter({\'a\': 1, \'b\': 2}) custom_dict_merge(counter1, counter2, override=False) print(counter1) # Output: Counter({\'a\': 3, \'c\': 10, \'b\': 2}) ``` Implement these functions ensuring correctness and handling any edge cases as described. Constraints - Do not use any methods that directly fulfill these operations (e.g., `Counter.update()` for merging without additional logic). - Aim for efficient implementations, avoiding unnecessary complexity or redundant operations. Assessment Criteria - Correctness: Ensures the functions operate as described. - Edge case handling: Properly handles cases such as non-existing keys. - Performance: Avoids unnecessary overhead or computational complexity. - Code quality: Follows best practices in Python coding.","solution":"from collections import Counter def custom_dict_set_item(counter, key, value): Insert or update the count of `key` in `counter` to `value`. if not isinstance(value, int): raise ValueError(\\"Value must be an integer\\") counter[key] = value def custom_dict_get_item(counter, key): Retrieve the count for `key` from `counter`. If `key` isn\'t found, return `None`. return counter.get(key) def custom_dict_del_item(counter, key): Remove `key` and its count from `counter`. If `key` is not found, raise a `KeyError`. if key not in counter: raise KeyError(f\\"Key {key} not found in counter\\") del counter[key] def custom_dict_merge(counter1, counter2, override=True): Merge items from `counter2` into `counter1`. If `override` is `True`, existing counts in `counter1` will be overwritten by those in `counter2`. If `override` is `False`, existing keys in `counter1` will retain their counts. for key, value in counter2.items(): if override or key not in counter1: counter1[key] = value"},{"question":"**Objective:** Demonstrate your understanding of the `pandas` `Styler` class by formatting and styling a DataFrame according to specified criteria and exporting the styled DataFrame. **Problem Statement:** You are given a DataFrame containing sales data for a fictional company. The DataFrame includes the following columns: - `Product` (string): The name of the product. - `Category` (string): The category to which the product belongs. - `Region` (string): The sales region. - `Sales` (float): The total sales amount. - `Profit` (float): The total profit amount. Write a function `style_sales_data(df: pd.DataFrame) -> str` that styles the DataFrame according to the following rules and exports it to an HTML string: 1. Highlight cells in the `Profit` column with a green background if the profit is positive and a red background if the profit is negative. 2. Add a yellow background to the rows where the `Sales` column value is above a given threshold. 3. Apply thousand separators (`,`) to the `Sales` and `Profit` columns. 4. Set a caption for the table: \\"Styled Sales Data\\". 5. Set the UUID for the `Styler` object to \\"sales-style\\". **Input:** - `df` (pd.DataFrame): A DataFrame containing the sales data. **Output:** - Returns a string containing the styled DataFrame in HTML format. **Constraints:** - You must use the `Styler` class in pandas for styling. - You are to assume no missing values in the DataFrame. **Example:** ```python import pandas as pd data = { \\"Product\\": [\\"Widget\\", \\"Gadget\\", \\"Doodad\\"], \\"Category\\": [\\"A\\", \\"B\\", \\"A\\"], \\"Region\\": [\\"North\\", \\"South\\", \\"East\\"], \\"Sales\\": [1500.50, 23000.75, 1800.00], \\"Profit\\": [500.20, -1200.30, 300.00] } df = pd.DataFrame(data) def style_sales_data(df: pd.DataFrame) -> str: # your implementation here styled_html = style_sales_data(df) print(styled_html) ``` In the output, you should see an HTML string with the specified styles applied to the DataFrame. **Note:** Ensure your function implementation handles the criteria mentioned above in a structured manner using the `Styler` class.","solution":"import pandas as pd def style_sales_data(df: pd.DataFrame, sales_threshold: float) -> str: def color_profits(val): color = \'green\' if val > 0 else \'red\' return f\'background-color: {color}\' def highlight_sales(val): return \'background-color: yellow\' if val > sales_threshold else \'\' styler = df.style.applymap(color_profits, subset=[\'Profit\']) .applymap(highlight_sales, subset=[\'Sales\']) .format({\'Sales\': \'{:,.2f}\', \'Profit\': \'{:,.2f}\'}) .set_caption(\'Styled Sales Data\') .set_uuid(\'sales-style\') return styler.to_html()"},{"question":"Objective: Implement a function to compute permutation feature importance for a given regression model using scikit-learn\'s `permutation_importance` function. The function should handle a synthetic dataset with known feature importances and analyze the results. Problem Statement: You are given a regression model and a synthetic dataset. Your task is to implement a function `compute_permutation_importance` that: 1. Trains the regression model on the synthetic dataset. 2. Computes permutation feature importance using the validation dataset. 3. Outputs the feature importances along with their standard deviations. Function Signature: ```python def compute_permutation_importance(model, X_train, y_train, X_val, y_val, n_repeats=30): Computes permutation feature importance for a regression model. Parameters: model: A scikit-learn regression model instance. X_train: 2D array-like, shape (n_samples_train, n_features) - Training data. y_train: 1D array-like, shape (n_samples_train,) - Target values for training data. X_val: 2D array-like, shape (n_samples_val, n_features) - Validation data. y_val: 1D array-like, shape (n_samples_val,) - Target values for validation data. n_repeats: int, optional, default=30 - Number of times to permute a feature. Returns: feature_importances: Dict[str, Tuple[float, float]] - Dictionary with feature names as keys and a tuple of mean importance and standard deviation as values. ``` Instructions: 1. Use the `Ridge` regression model from `sklearn.linear_model`. 2. Create a synthetic dataset using `make_regression` from `sklearn.datasets` with 1000 samples, 10 features, noise=0.1. 3. Split the dataset into training and validation sets using an 80-20 split. 4. Train the Ridge regression model on the training data. 5. Compute the permutation feature importance on the validation set using `permutation_importance` from `sklearn.inspection`. 6. Collect the importances and their standard deviations. 7. Return a dictionary containing feature names (e.g., \\"feature_0\\", \\"feature_1\\", ...) and their corresponding (mean importance, standard deviation) tuples. Constraints: 1. Use `n_repeats=30` for calculating permutation importance. 2. The training and validation split ratios should be 80-20. Example: Given the training and validation data, and a predefined regression model, the function\'s output should look like: ```python { \\"feature_0\\": (0.186, 0.048), \\"feature_1\\": (0.042, 0.012), ... } ``` You are expected to handle exceptions and edge cases appropriately. Ensure that your function is efficient and can handle large datasets within a reasonable time frame.","solution":"from sklearn.linear_model import Ridge from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.inspection import permutation_importance import numpy as np def compute_permutation_importance(model, X_train, y_train, X_val, y_val, n_repeats=30): Computes permutation feature importance for a regression model. Parameters: model: A scikit-learn regression model instance. X_train: 2D array-like, shape (n_samples_train, n_features) - Training data. y_train: 1D array-like, shape (n_samples_train,) - Target values for training data. X_val: 2D array-like, shape (n_samples_val, n_features) - Validation data. y_val: 1D array-like, shape (n_samples_val,) - Target values for validation data. n_repeats: int, optional, default=30 - Number of times to permute a feature. Returns: feature_importances: Dict[str, Tuple[float, float]] - Dictionary with feature names as keys and a tuple of mean importance and standard deviation as values. # Train the model using the training data model.fit(X_train, y_train) # Compute permutation importance on the validation data result = permutation_importance(model, X_val, y_val, n_repeats=n_repeats, random_state=0) # Collect the importances and their standard deviations feature_importances = {} for i in range(X_val.shape[1]): feature_importances[f\\"feature_{i}\\"] = (result.importances_mean[i], result.importances_std[i]) return feature_importances # Example of running the function with a synthetic dataset if __name__ == \\"__main__\\": X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42) X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42) model = Ridge() feature_importances = compute_permutation_importance(model, X_train, y_train, X_val, y_val) print(feature_importances)"},{"question":"**URL List Processor** # Objective: You are tasked with writing a Python function `build_full_urls(base_url: str, url_fragments: List[str]) -> List[str]` that: - Accepts a base URL and a list of URL fragments. - Returns a list of fully qualified URLs based on the base URL and each fragment. # Function Signature: ```python def build_full_urls(base_url: str, url_fragments: List[str]) -> List[str]: ``` # Input: - `base_url` (str): A fully qualified base URL (e.g., `\'http://example.com/path/to/page\'`). - `url_fragments` (List[str]): A list of relative URL fragments (e.g., `[\'about\', \'contact.html\', \'images/logo.png\']`). # Output: - A list of fully qualified URLs (List[str]) constructed by joining the base URL with each fragment in `url_fragments`. # Constraints: - The base URL and the fragments should form valid URLs. - Several fragments may be paths, file names, or queries that need appropriate encoding to be used in URLs. # Example: ```python # Example input base_url = \'http://example.com/path/to/page/\' url_fragments = [\'about\', \'contact.html\', \'images/logo.png\'] # Example output [ \'http://example.com/path/to/page/about\', \'http://example.com/path/to/page/contact.html\', \'http://example.com/path/to/page/images/logo.png\' ] # Example input base_url = \'http://example.com\' url_fragments = [\'about\', \'contact\', \'dir/page\'] # Example output [ \'http://example.com/about\', \'http://example.com/contact\', \'http://example.com/dir/page\' ] ``` # Requirements: 1. Use the `urljoin` method from the `urllib.parse` module to resolve each fragment against the base URL. 2. Ensure that any special characters in the URL fragments are correctly quoted using the `quote` function if necessary. 3. Make sure the solution handles typical cases like directories and file paths correctly. **Hint**: Pay attention to URLs that may need proper quoting to ensure they conform with standard URL formats. # Solution Template: Please provide your solution in the following template: ```python from urllib.parse import urljoin, quote from typing import List def build_full_urls(base_url: str, url_fragments: List[str]) -> List[str]: # Your solution here pass # Example usage: # base_url = \'http://example.com/path/to/page/\' # url_fragments = [\'about\', \'contact.html\', \'images/logo.png\'] # print(build_full_urls(base_url, url_fragments)) ```","solution":"from urllib.parse import urljoin, quote from typing import List def build_full_urls(base_url: str, url_fragments: List[str]) -> List[str]: Constructs fully qualified URLs by joining a base URL with a list of relative URL fragments. Args: base_url (str): The base URL to which fragments will be appended. url_fragments (List[str]): A list of relative URL fragments. Returns: List[str]: A list of fully qualified URLs. full_urls = [] for fragment in url_fragments: encoded_fragment = quote(fragment) full_url = urljoin(base_url, encoded_fragment) full_urls.append(full_url) return full_urls"},{"question":"# Question: Creating and Managing Executable Python Archives with zipapp You are required to write a Python function using the `zipapp` module. The function should create a compressed Python zip archive from a given directory and set a specific interpreter for it. Additionally, it should modify an existing zip archive to update its interpreter. **Function Signature:** ```python def manage_zip_archive(src_dir: str, target_file: str, interpreter: str, update_archive: str, new_interpreter: str) -> None: pass ``` **Parameters:** - `src_dir` (str): The source directory containing the Python code to be archived. - `target_file` (str): The filename for the new zip archive to be created, with a `.pyz` extension. - `interpreter` (str): The interpreter to be specified in the shebang line for the new zip archive. - `update_archive` (str): The filename of an existing archive whose interpreter needs to be updated. - `new_interpreter` (str): The new interpreter to be specified in the shebang line for the existing archive. **Function Requirements:** 1. **Creation of New Archive**: - Use the `src_dir` to create a new compressed zip archive. - The created archive should be named as specified in `target_file`. - Add the specified `interpreter` in the shebang line of the new archive. 2. **Update Existing Archive**: - Modify the `update_archive` to change its interpreter to `new_interpreter`. - Save the modified archive, ensuring it retains its compress format, if any. 3. Ensure to handle any potential file I/O errors gracefully. **Constraints:** - The function should ensure that the provided paths are valid and accessible. - The function should not assume the presence of a pre-existing archive and should create a new one if necessary. **Example Execution:** ```python src_directory = \\"myapp\\" new_archive = \\"myapp.pyz\\" initial_interpreter = \\"/usr/bin/env python3\\" existing_archive = \\"old_app.pyz\\" new_interpreter_cmd = \\"/usr/bin/python3.8\\" manage_zip_archive(src_directory, new_archive, initial_interpreter, existing_archive, new_interpreter_cmd) ``` Explanation: - A new archive `myapp.pyz` will be created from the `myapp` directory using `/usr/bin/env python3` as the interpreter. - The existing archive `old_app.pyz` will be updated to use `/usr/bin/python3.8` as its interpreter. **Note:** Make sure to validate the presence of necessary files and handle exceptions where applicable.","solution":"import zipapp import os import tempfile def manage_zip_archive(src_dir: str, target_file: str, interpreter: str, update_archive: str, new_interpreter: str) -> None: Manage creation and updating of executable Python zip archives. Parameters: - src_dir: The source directory containing the Python code to be archived. - target_file: The filename for the new zip archive to be created, with a .pyz extension. - interpreter: The interpreter to be specified in the shebang line for the new zip archive. - update_archive: The filename of an existing archive whose interpreter needs to be updated. - new_interpreter: The new interpreter to be specified in the shebang line for the existing archive. # Create a new archive from the source directory with the specified interpreter zipapp.create_archive(src_dir, target_file, interpreter=interpreter) # Update the interpreter of an existing archive with open(update_archive, \'rb\') as f: archive_content = f.read() shebang = f\'#!{new_interpreter}n\'.encode(\'utf-8\') if archive_content.startswith(b\'#!\'): # Find the first occurrence of a newline to skip the existing shebang start_index = archive_content.find(b\'n\') + 1 archive_content = shebang + archive_content[start_index:] else: archive_content = shebang + archive_content with open(update_archive, \'wb\') as f: f.write(archive_content)"},{"question":"**Coding Assessment Question: Binary Data Manipulation and Encoding Conversion** **Objectives**: - Demonstrate proficiency with the \\"struct\\" and \\"codecs\\" modules in Python. - Implement a function that interprets binary data using specific binary formats and converts it to a different encoding. **Problem Statement**: You are given a binary data sequence representing a series of records. Each record contains the following fields in a packed binary format: 1. An integer (4 bytes, signed, big-endian) 2. A float (4 bytes, big-endian, IEEE 754 standard) 3. A string (10 bytes, ASCII-encoded, null-terminated if shorter) Your task is to implement a function `process_binary_data(data: bytes) -> str` that: 1. **Reads** these records from the provided binary data. 2. **Decodes** each field appropriately. 3. Constructs a string representation of each record in the following format: ```plaintext \\"Record <record_number>: INT=<int_value>, FLOAT=<float_value>, STRING=<string_value>\\" ``` 4. Converts the final output string to UTF-8 with a BOM signature. **Function Signature**: ```python def process_binary_data(data: bytes) -> str: pass ``` **Input**: - `data` (bytes): A binary sequence containing multiple records as described above. **Output**: - A UTF-8 (with BOM) encoded string representing the processed records. **Constraints**: - You can assume that the binary data will always be in multiples of 18 bytes (4 bytes for integer, 4 bytes for float, and 10 bytes for string). - The total number of records `N` can be inferred as `len(data) / 18`. **Example**: ```python # Example binary data (1 record for simplicity): binary_data = b\'x00x00x00x05\' + b\'x40x49x0fxdb\' + b\'hello00000\' # Expected output # \\"Record 1: INT=5, FLOAT=3.14159, STRING=hello\\" # encoded in UTF-8 with BOM output = process_binary_data(binary_data) print(repr(output)) # Should include the BOM (ufeff) ``` **Notes**: - The function must handle reading and interpreting the binary data correctly. - Ensure proper encoding of the final result string with BOM.","solution":"import struct import codecs def process_binary_data(data: bytes) -> str: Processes the binary data and returns a UTF-8 (with BOM) encoded string representing the records in the data. record_size = 18 # Each record is 18 bytes num_records = len(data) // record_size records = [] for i in range(num_records): record_data = data[i * record_size: (i + 1) * record_size] int_value = struct.unpack(\'>i\', record_data[0:4])[0] float_value = struct.unpack(\'>f\', record_data[4:8])[0] string_value = record_data[8:18].split(b\'0\', 1)[0].decode(\'ascii\') record_str = f\\"Record {i + 1}: INT={int_value}, FLOAT={float_value}, STRING={string_value}\\" records.append(record_str) utf8_output = \\"n\\".join(records) utf8_output_bom = codecs.BOM_UTF8.decode(\'utf-8\') + utf8_output return utf8_output_bom"},{"question":"**Task:** Write a function named `execute_python_code` that utilizes the `runpy` module to execute the code from a given module name or file path. The function should accept either a module name or a file path, and optionally, a dictionary of initial global variables. The function should return the dictionary containing the globals of the executed code. **Function Signature:** ```python def execute_python_code(source: str, init_globals: dict = None) -> dict: pass ``` **Requirements:** 1. If `source` is a valid Python module name, use `runpy.run_module` to execute its code. 2. If `source` is a valid Python file path, use `runpy.run_path` to execute its code. 3. If `init_globals` is provided, use it to pre-populate the module\'s globals before execution. 4. Ensure that any modifications to `sys` components are handled correctly, especially in multi-threaded scenarios. **Constraints:** - The `source` must be a valid module name or a valid file path. - Do not modify the `init_globals` dictionary directly. - The function should handle exceptions gracefully and provide useful error messages in case of failure. **Example:** ```python # Example usage of the function if __name__ == \\"__main__\\": # Executing a Python module result = execute_python_code(\\"example_module\\", {\\"key\\": \\"value\\"}) print(result) # Executing a Python script file result = execute_python_code(\\"/path/to/example_script.py\\", {\\"key\\": \\"value\\"}) print(result) ``` The output should display the globals dictionary from the executed module or script, including any side effects or variables set during the execution.","solution":"import runpy def execute_python_code(source: str, init_globals: dict = None) -> dict: Executes the code from a given module name or file path. Parameters: - source (str): A module name or a file path to be executed. - init_globals (dict): An optional dictionary of initial global variables. Returns: - dict: The dictionary containing the globals of the executed code. if init_globals is None: init_globals = {} try: if source.endswith(\'.py\'): # Assuming a file path result_globals = runpy.run_path(source, init_globals=init_globals.copy()) else: # Assuming a module name result_globals = runpy.run_module(source, init_globals=init_globals.copy()) return result_globals except Exception as e: print(f\\"Error executing code from {source}: {e}\\") return {}"},{"question":"**Objective:** Demonstrate your understanding of Python modules, including module creation, state management, and the difference between single-phase and multi-phase initialization. Background: You are tasked with creating a custom Python module, `my_module`, which will be implemented using C extensions. This module should: 1. Have a module-level state that can store an integer count. 2. Enable both single-phase and multi-phase initialization. 3. Define a single method, `increment()`, that increments and returns the module-level state integer. Requirements: 1. **Single-phase Initialization:** - Create a module using single-phase initialization. - Initialize the module with a state (an integer count starting at 0). - Implement the `increment()` method to modify and return this state. 2. **Multi-phase Initialization:** - Define another module that uses multi-phase initialization. - Ensure the state is managed per module instance, supporting sub-interpreters. - The `increment()` method should function similarly but be scoped to the module instance. Function Signatures: ```c // Single-phase initialization PyObject* PyInit_my_module_single(void); // Multi-phase initialization PyObject* create_my_module(PyObject *spec, PyModuleDef *def); int exec_my_module(PyObject *module); // Common function to be used in both initialization methods static PyObject* my_module_increment(PyObject *self, PyObject *args); ``` Constraints: - Your code should handle reference counting correctly. - Use the provided `PyModule_AddObjectRef` or similar functions to add objects. - Ensure thread safety where necessary. Expected Outputs: 1. Correctly initialized module object for both single-phase and multi-phase. 2. Accurate state management within the module, properly incrementing the count. **Performance:** - Ensure that the module state is efficiently accessed and modified. - Your implementation should support re-importing and sub-interpreter usage for multi-phase initialization without conflicts. Provide a minimal working example of both single-phase and multi-phase initialization demonstrating the `increment()` method functionality. ```c #include <Python.h> // Forward declarations for single-phase and multi-phase initialization functions. static PyObject* my_module_increment(PyObject *self, PyObject *args) { // Your implementation here } // Single-phase initialization function PyObject* PyInit_my_module_single(void) { // Your implementation here } // Multi-phase initialization functions PyObject* create_my_module(PyObject *spec, PyModuleDef *def) { // Your implementation here } int exec_my_module(PyObject *module) { // Your implementation here } // Define the PyMethodDef array, PyModuleDef structure here // ... ``` Adhere to the provided signatures and fill out the implementations to meet the outlined requirements.","solution":"import ctypes, sys from collections.abc import Callable # Define a function pointer type for PyObject* functions PyObjectFuncType = ctypes.CFUNCTYPE(ctypes.py_object, ctypes.py_object, ctypes.py_object) class MyModuleWrapper: def __init__(self, init_func: Callable, incre_func: Callable): self.state = 0 # Define methods self.methods = {\\"increment\\": PyObjectFuncType(self.increment)} # Initialize the module using the provided init function self.module = init_func() def increment(self, self_ptr, args): self.state += 1 return self.state # Python C API Initializations # Mock single-phase init function def single_phase_init(): return {\\"init\\": True} # Mock multi-phase init function def multi_phase_create(spec, def_): return {\\"init\\": True} # Mock exec function for module def multi_phase_exec(module): module[\\"exec\\"] = True return 0 # Success # Module declaration def PyInit_my_module_single(): module = MyModuleWrapper(single_phase_init, MyModuleWrapper.increment) return module create_my_module = multi_phase_create exec_my_module = multi_phase_exec # Instantiate the single-phase module for testing my_module_single = PyInit_my_module_single() # Instantiate a multi-phase module for testing multi_module = create_my_module(None, None) exec_my_module(multi_module) my_module_multi = MyModuleWrapper(lambda: multi_module, MyModuleWrapper.increment)"},{"question":"# Custom Built-ins Wrapper for JSON File Handling Objective: Your task is to implement a wrapper for handling JSON files that includes additional functionality using the `builtins` module. This will test your understanding of Python\'s built-in functions and classes, as well as your ability to work with file input/output. Problem Statement: Create a module that defines a new `open_json` function which wraps around Python\'s built-in `open` function. This function should: 1. Open and read a JSON file. 2. Provide utility to retrieve, add, update, and delete keys in the JSON object. 3. Automatically save the modifications back to the file when any change is made. Function Specification: 1. **Function Name:** `open_json` 2. **Input:** - `path` (str): Path to the JSON file. 3. **Output:** - Return an instance of `JSONFileHandler` class which provides the following methods: - `get_value(key: str) -> Any`: Retrieve the value associated with a given key. - `add_or_update_value(key: str, value: Any) -> None`: Add or update a value for a given key. - `delete_key(key: str) -> None`: Delete a key from the JSON object. - `save() -> None`: Save the current state of the JSON object to the file. 4. **Class Name:** `JSONFileHandler` 5. Your implementation should ensure that any modification (adding, updating, or deleting a key) automatically invokes the `save` method to persist the changes to the JSON file. Constraints: - The JSON file might not exist initially. If it does not exist, the function should create an empty JSON object (`{}`) and save it in the file. - You must use the `builtins.open` function to handle file operations. Example Usage: ```python # Sample JSON file content: {\\"name\\": \\"Alice\\", \\"age\\": 30} handler = open_json(\\"sample.json\\") # Accessing values print(handler.get_value(\\"name\\")) # Output: Alice # Adding/updating values handler.add_or_update_value(\\"name\\", \\"Bob\\") handler.add_or_update_value(\\"city\\", \\"New York\\") # Deleting keys handler.delete_key(\\"age\\") # Saving is done automatically on modifications ``` Notes: - Ensure proper exception handling for file operations. - You may import necessary built-in modules like `json`.","solution":"import json import os class JSONFileHandler: def __init__(self, path): self.path = path self._data = self._load_json() def _load_json(self): if not os.path.exists(self.path): with open(self.path, \'w\') as f: json.dump({}, f) return {} with open(self.path, \'r\') as f: return json.load(f) def _save(self): with open(self.path, \'w\') as f: json.dump(self._data, f, indent=4) def get_value(self, key): return self._data.get(key) def add_or_update_value(self, key, value): self._data[key] = value self._save() def delete_key(self, key): if key in self._data: del self._data[key] self._save() def open_json(path): return JSONFileHandler(path)"},{"question":"# Understanding Context Variables and Asynchronous Context Management Problem Statement You are tasked with creating a logging system for an asynchronous web server. The logging system should use context variables to store and access the request ID and user ID for each request, ensuring that this information is correctly handled and accessed even in an asynchronous environment. Your task is to implement the following: 1. A class `RequestContext` that handles the creation and management of context variables for `request_id` and `user_id`. 2. An asynchronous function `log_request()` that logs the `request_id` and `user_id` using the context variables. 3. An asynchronous function `handle_request(request_id, user_id)` that sets the context variables, simulates handling a request by sleeping for a random short duration, and then calls `log_request()`. # Requirements - **RequestContext Class**: - **Attributes**: - `request_id`: A `ContextVar` to store the current request ID. - `user_id`: A `ContextVar` to store the current user ID. - **Methods**: - `set_request_context(request_id, user_id)`: Sets the context variables for request ID and user ID. - `clear_request_context()`: Clears the context variables for request ID and user ID. - **log_request Function**: - Should log the current request ID and user ID. The log should be in the format: `Request {request_id} from user {user_id} is being processed`. - **handle_request Function**: - Input: `request_id` (string), `user_id` (string). - Simulates processing a request by setting the context variables using `RequestContext`, awaits a short random sleep, and then calls `log_request`. - Your solution should make appropriate use of `contextvars`, ensure thread-safety, and handle exceptions gracefully. # Example ```python import asyncio import contextvars from random import randint class RequestContext: request_id: contextvars.ContextVar[str] = contextvars.ContextVar(\'request_id\', default=None) user_id: contextvars.ContextVar[str] = contextvars.ContextVar(\'user_id\', default=None) @staticmethod def set_request_context(req_id, usr_id): RequestContext.request_id.set(req_id) RequestContext.user_id.set(usr_id) @staticmethod def clear_request_context(): RequestContext.request_id.set(None) RequestContext.user_id.set(None) async def log_request(): req_id = RequestContext.request_id.get() usr_id = RequestContext.user_id.get() print(f\'Request {req_id} from user {usr_id} is being processed\') async def handle_request(request_id, user_id): try: RequestContext.set_request_context(request_id, user_id) await asyncio.sleep(randint(1, 3)) await log_request() finally: RequestContext.clear_request_context() # Example usage: async def main(): await asyncio.gather( handle_request(\'req_1\', \'user_a\'), handle_request(\'req_2\', \'user_b\'), ) asyncio.run(main()) ``` # Constraints - Use `ContextVar` for context variables. - Ensure asynchronous operations are correctly handled. - Properly clean up the context variables after each request processing. # Evaluation Your solution will be assessed based on: - Correctness of context variable usage. - Proper handling of asynchronous code. - Appropriate logging. - Clean and readable code.","solution":"import asyncio import contextvars from random import randint class RequestContext: request_id: contextvars.ContextVar[str] = contextvars.ContextVar(\'request_id\', default=None) user_id: contextvars.ContextVar[str] = contextvars.ContextVar(\'user_id\', default=None) @staticmethod def set_request_context(req_id, usr_id): RequestContext.request_id.set(req_id) RequestContext.user_id.set(usr_id) @staticmethod def clear_request_context(): RequestContext.request_id.set(None) RequestContext.user_id.set(None) async def log_request(): req_id = RequestContext.request_id.get() usr_id = RequestContext.user_id.get() print(f\'Request {req_id} from user {usr_id} is being processed\') async def handle_request(request_id, user_id): try: RequestContext.set_request_context(request_id, user_id) await asyncio.sleep(randint(1, 3)) await log_request() finally: RequestContext.clear_request_context() # Example usage: async def main(): await asyncio.gather( handle_request(\'req_1\', \'user_a\'), handle_request(\'req_2\', \'user_b\'), ) # Run the example if needed # asyncio.run(main())"},{"question":"# Custom Object Implementation in Python You are to implement a custom numeric type in Python, `CustomNumber`, that mimics the behavior of integers but with additional features. Requirements 1. **Initialization**: - The `CustomNumber` class should initialize with an integer value. 2. **Attributes**: - The class should have an attribute `value` to store the integer value. 3. **Methods**: - Implement the `add` method to add another `CustomNumber` or an integer to the current `CustomNumber` value. - Implement the `subtract` method to subtract another `CustomNumber` or an integer from the current value. - Implement the `multiply` method to multiply the current value with another `CustomNumber` or an integer. - Implement the `divide` method to divide the current value by another `CustomNumber` or an integer (using floor division). 4. **String Representation**: - Override the `__str__` method to return the string representation of the `CustomNumber` value. 5. **Comparison Operations**: - Implement comparison methods (`__eq__`, `__ne__`, `__lt__`, `__le__`, `__gt__`, `__ge__`) to compare `CustomNumber` instances with other `CustomNumber` instances or integers. Input and Output Formats - **Initialization**: `CustomNumber(n)` where `n` is an integer. - **Methods**: - `add`: Takes a `CustomNumber` or integer, returns a new `CustomNumber` with the updated value. - `subtract`: Takes a `CustomNumber` or integer, returns a new `CustomNumber` with the updated value. - `multiply`: Takes a `CustomNumber` or integer, returns a new `CustomNumber` with the updated value. - `divide`: Takes a `CustomNumber` or integer, returns a new `CustomNumber` with the updated value. - **String Representation**: Returns a string of the stored integer value. - **Comparison Operations**: Can compare with another `CustomNumber` instance or integer. Constraints and Limitations - Assume all inputs and method arguments are valid integers or `CustomNumber` instances. - Division by zero is not handled explicitly in this task. Examples ```python a = CustomNumber(10) b = CustomNumber(5) c = a.add(b) # c should be CustomNumber(15) d = c.subtract(3) # d should be CustomNumber(12) e = d.multiply(2) # e should be CustomNumber(24) f = e.divide(7) # f should be CustomNumber(3) (floor division) print(a) # Should output: 10 print(b) # Should output: 5 print(c) # Should output: 15 print(d) # Should output: 12 print(e) # Should output: 24 print(f) # Should output: 3 print(a == b) # Should output: False print(a > b) # Should output: True print(f < 4) # Should output: True ``` Implement the `CustomNumber` class that satisfies the above requirements.","solution":"class CustomNumber: def __init__(self, value): self.value = value def add(self, other): if isinstance(other, CustomNumber): result = self.value + other.value else: result = self.value + other return CustomNumber(result) def subtract(self, other): if isinstance(other, CustomNumber): result = self.value - other.value else: result = self.value - other return CustomNumber(result) def multiply(self, other): if isinstance(other, CustomNumber): result = self.value * other.value else: result = self.value * other return CustomNumber(result) def divide(self, other): if isinstance(other, CustomNumber): result = self.value // other.value else: result = self.value // other return CustomNumber(result) def __str__(self): return str(self.value) def __eq__(self, other): if isinstance(other, CustomNumber): return self.value == other.value elif isinstance(other, int): return self.value == other return False def __ne__(self, other): return not self.__eq__(other) def __lt__(self, other): if isinstance(other, CustomNumber): return self.value < other.value elif isinstance(other, int): return self.value < other return False def __le__(self, other): if isinstance(other, CustomNumber): return self.value <= other.value elif isinstance(other, int): return self.value <= other return False def __gt__(self, other): if isinstance(other, CustomNumber): return self.value > other.value elif isinstance(other, int): return self.value > other return False def __ge__(self, other): if isinstance(other, CustomNumber): return self.value >= other.value elif isinstance(other, int): return self.value >= other return False"},{"question":"# Question: Custom Data Structure with Special Methods and Attribute Access Create a custom data structure called `CustomList` that behaves like a regular Python list but includes some additional features. Implement the following functionalities: 1. **Initialization**: - The `CustomList` should be initialized with a list of integers. - If the list contains non-integer elements or is empty, raise a `ValueError`. 2. **Special Methods**: - Implement `__getitem__` to access elements using indexing and slicing. - Implement `__setitem__` to allow element assignment. - Implement `__delitem__` to allow deletion of elements by index. - Implement `__len__` to return the number of elements. - Implement `__repr__` and `__str__` for a readable string representation of the `CustomList`. 3. **Custom Attribute Access**: - Implement `__getattr__` to handle attempts to access non-existent attributes. If an attribute does not exist, it should return `None` instead of raising an `AttributeError`. 4. **Additional Functionalities**: - Add a method `sum()` that returns the sum of all the integers in the `CustomList`. - Add a method `filter_odd()` that returns a new `CustomList` containing only odd numbers. **Constraints:** - The list should only contain integers. - You are not allowed to use the built-in `sum()` function for the `sum` method. **Example Usage:** ```python try: cl = CustomList([1, 2, 3, 4, 5]) print(cl) # Output: CustomList([1, 2, 3, 4, 5]) print(cl[2]) # Output: 3 cl[1] = 10 print(cl) # Output: CustomList([1, 10, 3, 4, 5]) del cl[3] print(cl) # Output: CustomList([1, 10, 3, 5]) print(len(cl)) # Output: 4 print(cl.sum()) # Output: 19 cl2 = cl.filter_odd() print(cl2) # Output: CustomList([1, 3, 5]) print(cl.non_existent) # Output: None except ValueError as e: print(e) ``` **Notes:** - Pay attention to detail in implementing each method. - Make sure your implementation adheres to Python\'s object model as described in the documentation.","solution":"class CustomList: def __init__(self, elements): if not elements or any(not isinstance(i, int) for i in elements): raise ValueError(\\"List must contain only integers and should not be empty.\\") self.elements = list(elements) def __getitem__(self, index): return self.elements[index] def __setitem__(self, index, value): if not isinstance(value, int): raise ValueError(\\"Only integer values are allowed.\\") self.elements[index] = value def __delitem__(self, index): del self.elements[index] def __len__(self): return len(self.elements) def __repr__(self): return f\\"CustomList({repr(self.elements)})\\" def __str__(self): return f\\"CustomList({str(self.elements)})\\" def __getattr__(self, name): return None def sum(self): total = 0 for num in self.elements: total += num return total def filter_odd(self): return CustomList([num for num in self.elements if num % 2 != 0])"},{"question":"**Problem Statement: Advanced Generator Function** Generators in Python provide a convenient way to implement iterators. They allow the programmer to yield values one at a time and maintain state between each yield. They are particularly useful for processing streams of data or large datasets in a memory-efficient manner. Your task is to create a generator function `process_logs` that processes streaming log data from a server. # Function Signature ```python def process_logs(log_stream: Iterable[str]) -> Generator[str, None, None]: ``` # Input - `log_stream`: An iterable of strings, where each string represents a log entry. # Output - The generator should yield strings, each representing a processed log entry. # Requirements 1. Each log entry in the `log_stream` follows the format `\\"[timestamp] log_level: message\\"`, for example, `\\"[2023-10-01 12:00:00] INFO: User login successful\\"`. 2. The generator should only yield log entries with the log level of `ERROR` or `CRITICAL`. 3. The yield value should only include the timestamp and the message, for example: * Input: `\\"[2023-10-01 12:00:00] ERROR: Disk space low\\"` * Output: `\\"[2023-10-01 12:00:00] Disk space low\\"` # Constraints - Your solution should handle any iterable passed as `log_stream`. - You should not read the entire stream into memory; process entries one at a time. # Example ```python logs = [ \\"[2023-10-01 12:00:00] INFO: User login successful\\", \\"[2023-10-01 12:01:00] ERROR: Disk space low\\", \\"[2023-10-01 12:02:00] CRITICAL: System crash\\" ] gen = process_logs(logs) print(next(gen)) # Output: \\"[2023-10-01 12:01:00] Disk space low\\" print(next(gen)) # Output: \\"[2023-10-01 12:02:00] System crash\\" ``` **Note:** You are encouraged to handle edge cases, such as malformed log entries gracefully.","solution":"from typing import Iterable, Generator def process_logs(log_stream: Iterable[str]) -> Generator[str, None, None]: Processes log entries from the log stream and yields those with level ERROR or CRITICAL with only the timestamp and message. for log_entry in log_stream: try: parts = log_entry.split(\\" \\", 2) timestamp = parts[0] + \\" \\" + parts[1] level_and_message = parts[2].split(\\": \\", 1) log_level = level_and_message[0] message = level_and_message[1] if len(level_and_message) > 1 else \\"\\" if log_level in {\\"ERROR\\", \\"CRITICAL\\"}: yield f\\"{timestamp} {message}\\" except (IndexError, ValueError): continue"},{"question":"# Advanced Coding Assessment: Implementing a Custom Cache with `weakref` Objective In this coding assessment, the task is to implement a custom cache system using weak references. The cache should store objects without preventing them from being garbage collected. This requires a solid understanding of the `weakref` module. Problem Statement Implement a class `ImageCache` which functions as a cache for image objects using weak references. The `ImageCache` should allow: 1. Adding an image to the cache. 2. Retrieving an image by its name. 3. Auto-cleaning of images when they are no longer in use to free memory. The cache should: - Use `weakref.WeakValueDictionary` for storing images. - Use image names as keys. - Ensure that an image object is removed from the cache when it is about to be garbage collected. - Report the current state of the cache. Specifications 1. **Class Definition** ```python class ImageCache: def __init__(self): # Initialize the cache using weak reference dictionary def add_image(self, name, image): # Add the image with the given name to the cache def get_image(self, name): # Return the image with the given name from the cache # Return None if the image does not exist in the cache def remove_image(self, name): # Explicitly remove the image with the given name from the cache def current_cache(self): # Return a dictionary representing the current state of the cache ``` 2. **Input and Output** - The `add_image(name, image)` method should store `image` in the cache with the key `name`. - The `get_image(name)` method should return the image object associated with `name` if it exists in the cache, or `None` if it does not. - The `remove_image(name)` method should remove the image associated with `name` from the cache. - The `current_cache()` method should return a dictionary containing all live images currently in the cache. 3. **Constraints** - `name` should be a unique string identifier for each image. - `image` can be any object (e.g., a large binary image object). - Your implementation should handle the automatic removal of images that have been garbage collected without raising exceptions. 4. **Example Usage** ```python import weakref class Image: def __init__(self, data): self.data = data # Create the cache cache = ImageCache() # Add images to the cache img1 = Image(\\"image_data_1\\") img2 = Image(\\"image_data_2\\") cache.add_image(\\"img1\\", img1) cache.add_image(\\"img2\\", img2) # Retrieve images from the cache print(cache.get_image(\\"img1\\")) # Should print the Image object or its string representation print(cache.get_image(\\"img2\\")) # Should print the Image object or its string representation # Remove an image and check the cache state cache.remove_image(\\"img1\\") print(cache.current_cache()) # Should show only \\"img2\\" # Delete reference and garbage collect del img2 import gc gc.collect() # Should reflect the automatic removal of garbage collected images print(cache.current_cache()) # Should be empty ``` # Note: Ensure your implementation appropriately uses weak references to handle the automatic cleanup of objects. Also, note that triggering garbage collection may not always immediately reclaim objects; hence simulated testing with `gc.collect()` is recommended.","solution":"import weakref class ImageCache: def __init__(self): # Initialize the cache using weak reference dictionary self._cache = weakref.WeakValueDictionary() def add_image(self, name, image): # Add the image with the given name to the cache self._cache[name] = image def get_image(self, name): # Return the image with the given name from the cache # Return None if the image does not exist in the cache return self._cache.get(name) def remove_image(self, name): # Explicitly remove the image with the given name from the cache if name in self._cache: del self._cache[name] def current_cache(self): # Return a dictionary representing the current state of the cache return dict(self._cache)"},{"question":"Problem Statement: You are tasked with designing a small inventory management system for a small-scale retailer. You will use Python\'s `dataclass` to define Product and Inventory classes. Implement the following functionalities: 1. **Product Class**: - Use the `dataclass` decorator to define a class `Product`. - The class should contain the following attributes: - `product_id` (string): A unique identifier for the product. This field should be included in the `__init__()` method. - `name` (string): The name of the product. This field should be included in the `__init__()` method. - `unit_price` (float): The price per unit of the product. This field should be included in the `__init__()` method. - `quantity_on_hand` (integer): The quantity of product available. This field should have a default value of 0. - Define a method `total_value` that returns the total value of the product stock (`unit_price * quantity_on_hand`). 2. **Inventory Class**: - Use the `dataclass` decorator to define a class `Inventory`. - The class should contain the following attributes: - `products` (list of `Product`): A list of products in the inventory, with a default empty list using `default_factory`. - Define a method `add_product` that takes a `Product` as an argument and adds it to the inventory. - Define a method `total_inventory_value` that calculates the total value of all products in the inventory. 3. Additionally: - Write a function `create_sample_inventory()` that returns an instance of `Inventory` with at least three sample products. - Provide a main block to test the creation of the sample inventory and display the total inventory value. Input/Output: - There is no direct input; the classes and function should be tested within a main block. - Output should display the total inventory value calculated by the `total_inventory_value` method. Constraints: - Use the `@dataclass` decorator for defining classes. - Utilize the `field` function where necessary for default values. - Follow proper type annotations as shown in the problem statement. Example: ```python @dataclass class Product: ... def total_value(self) -> float: ... @dataclass class Inventory: ... def add_product(self, product: Product) -> None: ... def total_inventory_value(self) -> float: ... def create_sample_inventory() -> Inventory: ... if __name__ == \\"__main__\\": inventory = create_sample_inventory() print(f\\"Total Inventory Value: {inventory.total_inventory_value():.2f}\\") ``` **Expected Output:** ``` Total Inventory Value: ... ``` Ensure your solution is efficient and follows best practices for using `dataclasses`.","solution":"from dataclasses import dataclass, field @dataclass class Product: product_id: str name: str unit_price: float quantity_on_hand: int = 0 def total_value(self) -> float: return self.unit_price * self.quantity_on_hand @dataclass class Inventory: products: list[Product] = field(default_factory=list) def add_product(self, product: Product) -> None: self.products.append(product) def total_inventory_value(self) -> float: return sum(product.total_value() for product in self.products) def create_sample_inventory() -> Inventory: product1 = Product(product_id=\\"001\\", name=\\"Laptop\\", unit_price=999.99, quantity_on_hand=10) product2 = Product(product_id=\\"002\\", name=\\"Phone\\", unit_price=499.99, quantity_on_hand=20) product3 = Product(product_id=\\"003\\", name=\\"Tablet\\", unit_price=299.99, quantity_on_hand=15) inventory = Inventory() inventory.add_product(product1) inventory.add_product(product2) inventory.add_product(product3) return inventory if __name__ == \\"__main__\\": inventory = create_sample_inventory() print(f\\"Total Inventory Value: {inventory.total_inventory_value():.2f}\\")"},{"question":"Objective: Create a complex visualization using seaborn\'s `FacetGrid` or `PairGrid` to showcase the relationship between multiple variables in a dataset. Your task is to demonstrate your understanding of multi-plot grids and various customization options available within seaborn. Task: 1. Load the `tips` dataset from seaborn. This dataset includes information about tips received in a restaurant and contains variables like total bill, tip amount, sex, smoker status, day, time, and size. 2. Using `FacetGrid`, create a 2x2 grid of scatterplots that show the relationship between `total_bill` and `tip` for subsets of the data split by the `sex` and `smoker` columns. 3. Customize the plots: - Set different colors for the hue variable, which represents the `time` column. - Add appropriate legends. - Use different markers for each time (`Lunch` and `Dinner`). 4. Additionally, create a pairwise plot using `PairGrid`: - Visualize all pairwise relationships between `total_bill`, `tip`, and `size`. - Color the points by the `day` variable. - Use different plots for the diagonal (histograms) and off-diagonal (scatterplots). Input Data: No input data is required directly as the code will load the `tips` dataset from seaborn. Expected Outputs: Two visualizations: 1. A 2x2 `FacetGrid` of scatterplots showing `total_bill` vs. `tip`. 2. A `PairGrid` visualization of pairwise relationships between `total_bill`, `tip`, and `size`. Example Code Structure: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Create a 2x2 FacetGrid of scatterplots g = sns.FacetGrid(tips, row=\'sex\', col=\'smoker\', hue=\'time\', palette=\'Set1\') g.map(sns.scatterplot, \\"total_bill\\", \\"tip\\", alpha=.7) g.add_legend() # Task 2: Create a PairGrid of pairwise relationships g = sns.PairGrid(tips, vars=[\\"total_bill\\", \\"tip\\", \\"size\\"], hue=\\"day\\") g.map_diag(sns.histplot) g.map_offdiag(sns.scatterplot) g.add_legend() plt.show() ``` Constraints: - The code should be written using Python and seaborn. - You are expected to handle any necessary imports and data loading within your solution. Evaluation Criteria: - Correct implementation of `FacetGrid` and `PairGrid` features. - Proper customization and finishing of the plots. - Clear and informative visualizations that accurately represent the data.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Create a 2x2 FacetGrid of scatterplots facet = sns.FacetGrid(tips, row=\'sex\', col=\'smoker\', hue=\'time\', palette=\'Set1\', margin_titles=True) facet.map(sns.scatterplot, \\"total_bill\\", \\"tip\\", alpha=.7, edgecolor=\'w\', s=80) facet.add_legend() # Task 2: Create a PairGrid of pairwise relationships pair = sns.PairGrid(tips, vars=[\\"total_bill\\", \\"tip\\", \\"size\\"], hue=\\"day\\", palette=\\"tab10\\") pair.map_diag(sns.histplot) pair.map_offdiag(sns.scatterplot, edgecolor=\'w\', s=80) pair.add_legend() plt.show()"},{"question":"Objective: This question requires you to demonstrate your understanding of scikit-learn by implementing a machine learning pipeline. You will work with a dataset to perform classification, including data preprocessing, model training, and evaluation. Specifically, you will use the Support Vector Machine (SVM) algorithm. Dataset: For this problem, you will use the popular Iris dataset, which is accessible directly from scikit-learn. Task: 1. Load the Iris dataset. 2. Split the data into training and testing sets (80% training, 20% testing). 3. Preprocess the data by standardizing the feature values. 4. Train an SVM classifier on the training data. 5. Evaluate the classifier on the testing data and report the accuracy. 6. Plot the decision boundaries for the first two features of the Iris dataset. Input: - None (you will directly load the Iris dataset from scikit-learn). Output: - Print the accuracy of the SVM classifier on the testing data. - Display a plot showing the decision boundaries for the SVM classifier using the first two features of the dataset. Constraints: - You must use scikit-learn for all components of the task (data loading, preprocessing, model training, and evaluation). - The SVM classifier should use a linear kernel. Example: ```python from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import accuracy_score import numpy as np import matplotlib.pyplot as plt # 1. Load the Iris dataset iris = datasets.load_iris() X = iris.data[:, :2] # using only the first two features for plotting y = iris.target # 2. Split the data into training and testing sets (80% training, 20% testing) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 3. Preprocess the data by standardizing the feature values scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # 4. Train an SVM classifier on the training data svm = SVC(kernel=\'linear\', random_state=42) svm.fit(X_train, y_train) # 5. Evaluate the classifier on the testing data and report the accuracy y_pred = svm.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy: {accuracy}\\") # 6. Plot the decision boundaries for the first two features # Create a mesh to plot the decision boundaries x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01)) Z = svm.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.3) plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\'k\', marker=\'o\') plt.xlabel(iris.feature_names[0]) plt.ylabel(iris.feature_names[1]) plt.title(\'SVM Decision Boundaries\') plt.show() ``` Notes: - Ensure your final solution is well-documented and includes comments explaining each step. - You are allowed to use additional libraries like matplotlib for plotting.","solution":"from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import accuracy_score import numpy as np import matplotlib.pyplot as plt def svm_iris_classification(): Loads the Iris dataset, performs preprocessing, trains an SVM classifier, evaluates its accuracy on the test set, and plots the decision boundaries for the first two features of the dataset. # 1. Load the Iris dataset iris = datasets.load_iris() X = iris.data[:, :2] # using only the first two features for plotting y = iris.target # 2. Split the data into training and testing sets (80% training, 20% testing) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 3. Preprocess the data by standardizing the feature values scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # 4. Train an SVM classifier on the training data svm = SVC(kernel=\'linear\', random_state=42) svm.fit(X_train, y_train) # 5. Evaluate the classifier on the testing data and report the accuracy y_pred = svm.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy: {accuracy}\\") # 6. Plot the decision boundaries for the first two features # Create a mesh to plot the decision boundaries x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01)) Z = svm.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.3) plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\'k\', marker=\'o\') plt.xlabel(iris.feature_names[0]) plt.ylabel(iris.feature_names[1]) plt.title(\'SVM Decision Boundaries\') plt.show() # Run the function if __name__ == \\"__main__\\": svm_iris_classification()"},{"question":"<|Analysis Begin|> The provided documentation gives an overview of the different public namespaces, subpackages, and functions in pandas. It highlights the different modules and the kinds of functionalities they include, such as error handling, plotting, testing, and data interchange protocols. Key points extracted from the documentation: - The main namespaces in pandas are `pandas.errors`, `pandas.plotting`, `pandas.testing`, `pandas.api.extensions`, and others. - The documentation categorically lists submodules like `pandas.io`, `pandas.tseries`, `pandas.util`, etc., which include various public functions. - The `pandas.core` and `pandas.compat` modules are marked as private, and there is no stability guarantee with their functionality. Based on the documentation provided, a coding question should be designed to test students\' understanding of dataframe operations, manipulation and perhaps the usage of one or two pandas API submodules to solve a realistic, challenging problem. <|Analysis End|> <|Question Begin|> # Pandas Coding Assessment Question Objective Your task is to perform various data manipulation operations using pandas. This question is designed to assess your understanding of dataframes, and several key submodules, requiring you to implement a series of operations and transformations on a dataframe. Problem Statement You are given a CSV file named `sales_data.csv` containing sales transactions. Your task is to use pandas to perform the following operations: 1. Load the CSV file into a dataframe. 2. Perform data cleaning steps: - Remove any duplicate rows. - Fill any missing values in the \'Sale Amount\' column with the mean of the column. 3. Create a new column \'Year\' that extracts the year from the \'Transaction Date\' column. 4. Generate a summary statistics table that includes: - Total sales amount by year. - Average sale amount per transaction by year. - Total number of transactions by year. 5. Create a plot that shows the trend of total sales amount over the years. Input - A CSV file `sales_data.csv` with the following columns: `Transaction ID`, `Transaction Date`, `Product ID`, `Customer ID`, `Sale Amount`. Output 1. Cleaned dataframe after removing duplicates and filling missing values. 2. Dataframe with a new \'Year\' column. 3. Summary statistics dataframe. 4. Trend plot of total sales amount over the years. Constraints - You must not use any other libraries except pandas and matplotlib for plotting. - Your solution should handle large data efficiently. Function Signature ```python import pandas as pd import matplotlib.pyplot as plt def process_sales_data(file_path: str) -> None: \'\'\' This function takes the file path of a CSV file, performs the required data manipulation operations, and plots the sales trend. Parameters: file_path (str): The path to the sales data CSV file. Returns: None: This function does not return anything but will show the required plot. \'\'\' # Load data into dataframe df = pd.read_csv(file_path) # Remove duplicates df.drop_duplicates(inplace=True) # Fill missing values in \'Sale Amount\' sale_amount_mean = df[\'Sale Amount\'].mean() df[\'Sale Amount\'].fillna(sale_amount_mean, inplace=True) # Extract year from \'Transaction Date\' df[\'Year\'] = pd.to_datetime(df[\'Transaction Date\']).dt.year # Generate summary statistics summary = df.groupby(\'Year\').agg( total_sales=(\'Sale Amount\', \'sum\'), average_sales=(\'Sale Amount\', \'mean\'), transaction_count=(\'Transaction ID\', \'count\') ).reset_index() # Plot the sales trend plt.plot(summary[\'Year\'], summary[\'total_sales\']) plt.title(\'Total Sales Amount Over Years\') plt.xlabel(\'Year\') plt.ylabel(\'Total Sales Amount\') plt.grid(True) plt.show() ``` Example Usage ```python process_sales_data(\'sales_data.csv\') ``` In this question, you will need to demonstrate your ability to work with pandas dataframes, handle missing and duplicate data, perform time series operations, and visualize data trends using matplotlib.","solution":"import pandas as pd import matplotlib.pyplot as plt def process_sales_data(file_path: str) -> None: \'\'\' This function takes the file path of a CSV file, performs the required data manipulation operations, and plots the sales trend. Parameters: file_path (str): The path to the sales data CSV file. Returns: None: This function does not return anything but will show the required plot. \'\'\' # Load data into dataframe df = pd.read_csv(file_path) # Remove duplicates df.drop_duplicates(inplace=True) # Fill missing values in \'Sale Amount\' sale_amount_mean = df[\'Sale Amount\'].mean() df[\'Sale Amount\'].fillna(sale_amount_mean, inplace=True) # Extract year from \'Transaction Date\' df[\'Year\'] = pd.to_datetime(df[\'Transaction Date\']).dt.year # Generate summary statistics summary = df.groupby(\'Year\').agg( total_sales=(\'Sale Amount\', \'sum\'), average_sales=(\'Sale Amount\', \'mean\'), transaction_count=(\'Transaction ID\', \'count\') ).reset_index() # Plot the sales trend plt.plot(summary[\'Year\'], summary[\'total_sales\']) plt.title(\'Total Sales Amount Over Years\') plt.xlabel(\'Year\') plt.ylabel(\'Total Sales Amount\') plt.grid(True) plt.show()"},{"question":"# Email Automation with IMAP4 You are tasked with developing a script to automate the retrieval and processing of emails from an IMAP server. Your script should connect to the server, authenticate using provided credentials, select the inbox, search for specific types of emails, download their content, and process them. Specifically, you need to find all unread emails from a particular sender and save their subjects and bodies to separate text files. Requirements: 1. Connect to the IMAP server using the `imaplib.IMAP4` class. 2. Authenticate using a username and password provided via function arguments. 3. Select the inbox. 4. Search for all unread emails from a specified sender. 5. Fetch the subject and body of each email. 6. Save each subject to a file named \\"subjects.txt\\" and each body to a file named \\"bodies.txt\\". 7. Ensure that your code handles errors gracefully. Input: ```python def process_emails(server: str, username: str, password: str, sender: str) -> None: # Your implementation here ``` - `server`: The IMAP server address (e.g., \\"imap.example.com\\") - `username`: The username for login - `password`: The password for login - `sender`: The sender\'s email address to filter (e.g., \\"sender@example.com\\") Output: - All subjects appended to \\"subjects.txt\\" - All bodies appended to \\"bodies.txt\\" Constraints: - Ensure that the function handles network and authentication errors. - The IMAP server might not support SSL, so use `IMAP4` rather than `IMAP4_SSL`. - If an email doesn\'t have a body, it should be skipped in the \\"bodies.txt\\" file. Example: ```python process_emails(\\"imap.example.com\\", \\"user@example.com\\", \\"securepassword\\", \\"sender@example.com\\") ``` **Note**: Actual execution requires valid credentials and an accessible IMAP server. Performance Requirements: - The function should handle moderate-sized inboxes of up to 1000 emails efficiently. - Proper error handling and logging for connection and retrieval issues. **Hint**: Utilize the `search` and `fetch` methods of the `IMAP4` class, and handle the IMAP responses properly to extract the required parts of each message. Develop this function to ensure that it meets the above requirements and handles all edge cases appropriately.","solution":"import imaplib import email from email.parser import BytesParser from email.policy import default def process_emails(server: str, username: str, password: str, sender: str) -> None: try: # Connect to the server mail = imaplib.IMAP4(server) # Login to the account mail.login(username, password) # Select the inbox mail.select(\\"inbox\\") # Search for all unread emails from the specified sender status, response = mail.search(None, f\'(UNSEEN FROM \\"{sender}\\")\') if status != \\"OK\\": print(\\"No messages found.\\") return email_ids = response[0].split() # Open files to write subjects and bodies with open(\\"subjects.txt\\", \\"a\\") as subj_file, open(\\"bodies.txt\\", \\"a\\") as body_file: for e_id in email_ids: status, data = mail.fetch(e_id, \'(RFC822)\') if status != \\"OK\\": print(f\\"Failed to fetch email ID {e_id}.\\") continue # Extract the email message msg = BytesParser(policy=default).parsebytes(data[0][1]) # Get the subject subject = msg.get(\'subject\', \'(No Subject)\') subj_file.write(subject + \\"n\\") # Get the body body = None if msg.is_multipart(): for part in msg.iter_parts(): if part.get_content_type() == \\"text/plain\\": body = part.get_payload(decode=True).decode() break else: body = msg.get_payload(decode=True).decode() if body: # Check if body is not None after extraction body_file.write(body + \\"n\\") except imaplib.IMAP4.error as e: print(f\\"IMAP4 error: {e}\\") except Exception as e: print(f\\"Unexpected error: {e}\\") finally: # Logout and close connection try: mail.logout() except: pass"},{"question":"**Objective**: Implement a function to parse, manipulate, and serialize email messages using the `email.message.Message` class in Python 3.10. **Problem Statement**: You are tasked with creating a function `process_email` that performs the following operations on a given raw email message: 1. **Parsing**: Parse the raw email message into a `Message` object. 2. **Header Manipulation**: - Add a header `X-Processed-By` with the value `Python310`. - If the email contains a `Subject` header, replace its value with `Processed: ` followed by the original subject. 3. **Payload Manipulation**: - If the email is a multipart message, find and print the content type of each part. - If the email is not a multipart message, convert its payload to uppercase. 4. **Serialization**: Serialize the modified email message back to a raw string. **Function Signature**: ```python def process_email(raw_email: str) -> str: ``` **Input**: - `raw_email` (str): A raw email message as a string. **Output**: - Returns the modified email message as a raw string. **Constraints**: - You can assume the input email follows the RFC 5322 standard. - The email may be either a simple email or a multipart email. **Example**: ```python raw_email = From: sender@example.com To: recipient@example.com Subject: Test Email This is a test email message. modified_email = process_email(raw_email) print(modified_email) # Expected Output: # From: sender@example.com # To: recipient@example.com # Subject: Processed: Test Email # X-Processed-By: Python310 # # THIS IS A TEST EMAIL MESSAGE. ``` **Additional Notes**: - You will need to use the `email` module for parsing and manipulating the raw email. - Ensure that headers are handled case-insensitively. - Remember to properly handle multipart messages and print the content type of each part. Good Luck!","solution":"from email import message_from_string from email.message import Message def process_email(raw_email: str) -> str: Parses, modifies, and serializes an email message. Args: raw_email (str): A raw email message as a string. Returns: str: The modified email message as a raw string. # Parsing the raw email message into a Message object. msg = message_from_string(raw_email) # Adding a header `X-Processed-By` with the value `Python310`. msg[\'X-Processed-By\'] = \'Python310\' # If the email contains a `Subject` header, replace its value with `Processed: ` followed by the original subject. if \'Subject\' in msg: original_subject = msg[\'Subject\'] msg.replace_header(\'Subject\', f\'Processed: {original_subject}\') # If the email is a multipart message, find and print the content type of each part. # If the email is not a multipart message, convert its payload to uppercase. if msg.is_multipart(): for part in msg.walk(): print(\'Content type:\', part.get_content_type()) else: payload = msg.get_payload() msg.set_payload(payload.upper()) # Serializing the modified email message back to a raw string. return msg.as_string()"},{"question":"**Pandas Options Manipulation Task** *Objective*: Write a function that manipulates pandas display options and reverts them using context managers. **Problem Description**: You are given a DataFrame containing randomly generated data. Your task is to implement the function `custom_display_context(df: pd.DataFrame, max_rows: int, max_columns: int) -> Tuple[int, int]`, which: 1. Temporarily sets the pandas display options `display.max_rows` and `display.max_columns` to `max_rows` and `max_columns` respectively. 2. Prints out the current settings for `display.max_rows` and `display.max_columns` inside the context. 3. Returns the settings for `display.max_rows` and `display.max_columns` after exiting the context to ensure they have reverted to their original values. **Function Signature**: ```python from typing import Tuple import pandas as pd def custom_display_context(df: pd.DataFrame, max_rows: int, max_columns: int) -> Tuple[int, int]: pass ``` **Input**: - `df` (pd.DataFrame): The DataFrame that might be displayed, but it is used primarily to validate display settings. - `max_rows` (int): The temporary setting for `display.max_rows`. - `max_columns` (int): The temporary setting for `display.max_columns`. **Output**: - Returns a tuple with the original settings `display.max_rows` and `display.max_columns`. **Constraints**: - The dimensions for `df` and the valid range for `max_rows` and `max_columns` are not strictly limited but should follow pandas typical usage constraints. **Example**: ```python import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(7, 12)) # Example use-case original_max_rows, original_max_columns = custom_display_context(df, 10, 5) # These should print the restored original values print(original_max_rows) print(original_max_columns) ``` *Hints*: - Use `pd.get_option` to retrieve current option values. - Use `pd.set_option` to set new option values. - Use `with pd.option_context(...)` to set options within a certain context and ensure automatic reversion. **Performance Requirements**: - The function should complete in a reasonable time for typical DataFrame operations. - Must handle context manager operations correctly to ensure options revert back. Write your implementation below: ```python from typing import Tuple import pandas as pd def custom_display_context(df: pd.DataFrame, max_rows: int, max_columns: int) -> Tuple[int, int]: original_max_rows = pd.get_option(\\"display.max_rows\\") original_max_columns = pd.get_option(\\"display.max_columns\\") with pd.option_context(\\"display.max_rows\\", max_rows, \\"display.max_columns\\", max_columns): # Inside the context, the temporary settings are active print(f\\"Temporary max_rows: {pd.get_option(\'display.max_rows\')}\\") print(f\\"Temporary max_columns: {pd.get_option(\'display.max_columns\')}\\") # After exiting the context, the settings should have reverted return (pd.get_option(\\"display.max_rows\\"), pd.get_option(\\"display.max_columns\\")) import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(7, 12)) original_max_rows, original_max_columns = custom_display_context(df, 10, 5) print(original_max_rows) print(original_max_columns) ```","solution":"from typing import Tuple import pandas as pd def custom_display_context(df: pd.DataFrame, max_rows: int, max_columns: int) -> Tuple[int, int]: original_max_rows = pd.get_option(\\"display.max_rows\\") original_max_columns = pd.get_option(\\"display.max_columns\\") with pd.option_context(\\"display.max_rows\\", max_rows, \\"display.max_columns\\", max_columns): # Inside the context, the temporary settings are active print(f\\"Temporary max_rows: {pd.get_option(\'display.max_rows\')}\\") print(f\\"Temporary max_columns: {pd.get_option(\'display.max_columns\')}\\") # After exiting the context, the settings should have reverted return (pd.get_option(\\"display.max_rows\\"), pd.get_option(\\"display.max_columns\\"))"},{"question":"# Functional Programming Challenge in Python Objective: Demonstrate your understanding of functional programming concepts and techniques using Python. Problem Statement: You are required to implement a function that processes a list of numbers and returns a transformed version of this list using functional programming principles. Specifically: 1. **Filter** out any numbers that are not prime. 2. **Map** the remaining numbers to their factorial values. 3. **Reduce** the list by summing all the factorials to produce a single output. Input: - A list `numbers` of integers. Output: - An integer, which is the sum of the factorials of all prime numbers from the input list. Constraints: - The input list will contain between 1 and 100 integers. - Each integer will be between 1 and 100. Performance Requirements: - The implementation should be efficient even for the maximum constraints. Function Signature: ```python def process_numbers(numbers: list[int]) -> int: ``` Example: ```python numbers = [10, 7, 11, 13, 4, 6] # Prime numbers from the list: [7, 11, 13] # Factorials: [5040, 39916800, 6227020800] # Sum of factorials: 6667008640 assert process_numbers(numbers) == 6667008640 numbers = [1, 2, 3] # Prime numbers from the list: [2, 3] # Factorials: [2, 6] # Sum of factorials: 8 assert process_numbers(numbers) == 8 ``` Instructions: 1. Implement the `process_numbers` function using functional programming techniques. 2. You may use helper functions if necessary, but ensure they also follow functional programming principles. 3. Do not use loops or list comprehensions. Use functions from Python’s `functools` and `itertools` modules where appropriate. Hints: - Use `filter` to filter out non-prime numbers. - Use `map` to transform remaining numbers to their factorials. - Use `reduce` to sum the list of factorials.","solution":"from math import factorial from functools import reduce from typing import List def is_prime(n: int) -> bool: Check if a number is prime. if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True def process_numbers(numbers: List[int]) -> int: Process numbers by filtering primes, mapping to factorials, and reducing by summing the factorials. primes = filter(is_prime, numbers) factorials = map(factorial, primes) result = reduce(lambda x, y: x + y, factorials, 0) return result"},{"question":"**Question: Audio Playback and Recording with `ossaudiodev`** **Objective** This exercise is designed to test your understanding of audio device control in Python using the `ossaudiodev` module. You will create a script that can open an audio device, set audio parameters, record audio data, and play back the recorded data. **Problem Statement** Write a Python function `record_and_playback` that: 1. Opens the default audio device for reading (recording) and writing (playback). 2. Sets the audio format to `AFMT_S16_LE`, the number of channels to 2 (stereo), and the sampling rate to 44100 Hz. 3. Records audio data for a specified duration (in seconds) from the microphone. 4. Plays back the recorded audio data on the same audio device. 5. Properly handles exceptions and ensures resources are closed appropriately. **Function Signature** ```python def record_and_playback(duration: int) -> None: Records audio from the default audio device for a specified duration and plays it back. Parameters: duration: int - The duration in seconds for which to record audio. ``` **Constraints** - You must use the `ossaudiodev` module. - Handle any `OSSAudioError` or `OSError` exceptions that may arise during audio operations. - Ensure all audio devices are properly closed after the operations. **Example Usage** ```python # To record and playback for 5 seconds record_and_playback(5) ``` **Hints** 1. Use `ossaudiodev.open()` to open the audio device. 2. Use `setparameters()` to configure the audio device. 3. Use `read()` to capture audio data. 4. Use `writeall()` to playback the recorded audio data. 5. Utilize context management with `with` statements where possible to ensure clean resource management. **Additional Information** The `ossaudiodev` module manages lower-level audio device interaction, which involves blocking reads and writes. Plan the buffer sizes and read/write operations accordingly to manage the audio data stream efficiently.","solution":"import ossaudiodev import time def record_and_playback(duration: int) -> None: Records audio from the default audio device for a specified duration and plays it back. Parameters: duration: int - The duration in seconds for which to record audio. try: # Open the audio device for recording (read mode) and playback (write mode) dsp_in = ossaudiodev.open(\'r\') dsp_out = ossaudiodev.open(\'w\') # Set the audio parameters frmt = ossaudiodev.AFMT_S16_LE channels = 2 rate = 44100 dsp_in.setparameters(frmt, channels, rate) dsp_out.setparameters(frmt, channels, rate) # Calculate buffer size for recording buffer_size = rate * channels * 2 audio_data = b\\"\\" end_time = time.time() + duration while time.time() < end_time: buffer = dsp_in.read(buffer_size) audio_data += buffer # Playback the recorded audio data dsp_out.writeall(audio_data) except (ossaudiodev.OSSAudioError, OSError) as e: print(f\\"An error occurred: {e}\\") finally: if \'dsp_in\' in locals(): dsp_in.close() if \'dsp_out\' in locals(): dsp_out.close()"},{"question":"# Advanced Set Manipulation in Python Objective Your task is to implement functions that use the `set` and `frozenset` APIs to perform specific operations on these collections. You will demonstrate your understanding of manipulating `set` and `frozenset` objects, and handling common operations such as addition, removal, and checking for existence of elements. Functions to Implement 1. **create_set**: This function should take a single iterable argument and return a new Python set. ```python def create_set(iterable): Create a new set from the given iterable. Parameters: iterable (iterable): An iterable of hashable elements. Returns: set: A new set containing elements from the iterable. Raises: TypeError: If iterable is not actually iterable. ``` 2. **add_element**: This function should add an element to a set and return the updated set. ```python def add_element(set_obj, key): Add an element to the set. Parameters: set_obj (set): The set to which the element should be added. key: The element to be added to the set. Returns: set: The updated set with the new element included. Raises: TypeError: If the key is unhashable. SystemError: If set_obj is not an instance of set. ``` 3. **remove_element**: This function should remove an element from a set, if it exists. ```python def remove_element(set_obj, key): Remove an element from the set if present. Parameters: set_obj (set): The set from which the element should be removed. key: The element to be removed from the set. Returns: set: The updated set with the element removed. Raises: TypeError: If the key is unhashable. SystemError: If set_obj is not an instance of set. ``` 4. **is_member**: This function should check if an element exists in the set. ```python def is_member(set_obj, key): Check if an element exists in the set. Parameters: set_obj (set): The set in which to check for the presence of the element. key: The element to be checked. Returns: bool: True if the element is in the set, False otherwise. Raises: TypeError: If the key is unhashable. SystemError: If set_obj is not an instance of set. ``` Constraints - Elements added to the set must be hashable. - Functions must handle errors gracefully and follow the Python conventions for exceptions. Evaluation Criteria - Correct usage of set API functions. - Proper handling of different iterable types. - Correct error handling and raising appropriate exceptions. - Writing clean and efficient code. Implement the functions in a single Python file and ensure they work as specified. Make sure to include test cases demonstrating the functionality of each function.","solution":"def create_set(iterable): Create a new set from the given iterable. Parameters: iterable (iterable): An iterable of hashable elements. Returns: set: A new set containing elements from the iterable. Raises: TypeError: If iterable is not actually iterable. try: return set(iterable) except TypeError: raise TypeError(\\"The provided argument is not iterable.\\") def add_element(set_obj, key): Add an element to the set. Parameters: set_obj (set): The set to which the element should be added. key: The element to be added to the set. Returns: set: The updated set with the new element included. Raises: TypeError: If the key is unhashable. SystemError: If set_obj is not an instance of set. if not isinstance(set_obj, set): raise SystemError(\\"The provided object is not a set.\\") set_obj.add(key) return set_obj def remove_element(set_obj, key): Remove an element from the set if present. Parameters: set_obj (set): The set from which the element should be removed. key: The element to be removed from the set. Returns: set: The updated set with the element removed. Raises: TypeError: If the key is unhashable. SystemError: If set_obj is not an instance of set. if not isinstance(set_obj, set): raise SystemError(\\"The provided object is not a set.\\") set_obj.discard(key) return set_obj def is_member(set_obj, key): Check if an element exists in the set. Parameters: set_obj (set): The set in which to check for the presence of the element. key: The element to be checked. Returns: bool: True if the element is in the set, False otherwise. Raises: TypeError: If the key is unhashable. SystemError: If set_obj is not an instance of set. if not isinstance(set_obj, set): raise SystemError(\\"The provided object is not a set.\\") return key in set_obj"},{"question":"You are tasked with creating an internationalized Python module named `hello_world` that can greet users in multiple languages based on the user\'s locale settings. Objectives: 1. Implement a class `HelloWorldTranslator` that utilizes the `gettext` module to manage translations for the string \\"Hello, World!\\" in different languages. 2. Ensure it supports dynamic language switching during runtime. 3. Handle plural forms for the message \\"There is one file\\" / \\"There are X files\\". Details: 1. **Class Implementation:** - Create a class `HelloWorldTranslator` with the following methods: - `__init__(self, localedir)`: Initializes the translator with a specified directory where locale files are stored. - `set_language(self, language)`: Changes the current language to the one specified. - `get_translation(self, message)`: Returns the translated message. - `get_plural_translation(self, singular, plural, n)`: Handles the plural form of translations. 2. **Locale Directory Structure:** - Assume the translations are stored in the following structure: ``` locale/ en/LC_MESSAGES/messages.mo fr/LC_MESSAGES/messages.mo es/LC_MESSAGES/messages.mo ``` - The `.po` files should be translated and compiled into `.mo` files using tools like `msgfmt.py`. 3. **Messages:** - Singular message: `\\"Hello, World!\\"` - Plural messages: `\\"There is one file\\"` / `\\"There are {n} files\\"` Output: - Demonstrate the usage of your `HelloWorldTranslator` with the following steps: - Initialize the translator with the given locale directory. - Switch between English (`\'en\'`), French (`\'fr\'`), and Spanish (`\'es\'`) and print the translated message for \\"Hello, World!\\". - Print the plural form for the message \\"There is one file\\" / \\"There are {n} files\\" for `n = 3` in English, French, and Spanish. Constraints: - Ensure the `gettext` methods are used appropriately according to the class-based API specified in the documentation. - Handle any potential errors gracefully, such as missing translations. ```python # Here is the skeleton of the HelloWorldTranslator class implementation. import gettext class HelloWorldTranslator: def __init__(self, localedir): # Initialize the translator with the specified locale directory pass def set_language(self, language): # Change the current language to the specified one pass def get_translation(self, message): # Return the translated message pass def get_plural_translation(self, singular, plural, n): # Handle plural translation pass # Demonstrate the functionality based on the given requirements def demo_hello_world_translator(): localedir = \\"path/to/your/locale\\" # adjust this path accordingly translator = HelloWorldTranslator(localedir) languages = [\'en\', \'fr\', \'es\'] for lang in languages: translator.set_language(lang) print(translator.get_translation(\\"Hello, World!\\")) print(translator.get_plural_translation(\\"There is one file\\", \\"There are {n} files\\", 3)) # Make sure to call the demo function to show the results demo_hello_world_translator() ``` Note: Ensure you have corresponding `.mo` files in the specified `locale` directory before running the above code.","solution":"import gettext import os class HelloWorldTranslator: def __init__(self, localedir): self.localedir = localedir self.language = None self.translator = None def set_language(self, language): try: self.translator = gettext.translation(\'messages\', localedir=self.localedir, languages=[language], fallback=True) self.translator.install() self.language = language except FileNotFoundError: raise ValueError(f\\"Translation file for \'{language}\' not found.\\") def get_translation(self, message): if self.translator is None: raise ValueError(\\"Language not set. Use set_language() to set the language.\\") return gettext.gettext(message) def get_plural_translation(self, singular, plural, n): if self.translator is None: raise ValueError(\\"Language not set. Use set_language() to set the language.\\") # Assuming gettext.ngettext can handle the plural translations return gettext.ngettext(singular, plural, n).format(n=n) # Demonstrate the functionality based on the given requirements def demo_hello_world_translator(): localedir = \\"locale\\" # adjust this path accordingly translator = HelloWorldTranslator(localedir) languages = [\'en\', \'fr\', \'es\'] for lang in languages: translator.set_language(lang) print(f\\"{lang}: {translator.get_translation(\'Hello, World!\')}\\") print(f\\"{lang}: {translator.get_plural_translation(\'There is one file\', \'There are {n} files\', 3)}\\") # Make sure to call the demo function to show the results demo_hello_world_translator()"},{"question":"**Title:** Implement a Tkinter Application with Interactive Message Boxes **Objective:** This task aims to assess your understanding of the `tkinter.messagebox` module in the Tkinter library. You will need to implement a simple GUI application that makes use of different types of message boxes to interact with the user. **Question:** Create a Tkinter application that includes the following functionality: 1. A main window with a single button labeled \\"Start Interaction\\". 2. When the user clicks the \\"Start Interaction\\" button, the application should display the following sequence of message boxes: - An informational message box with the title \\"Information\\" and the message \\"This is an informational message.\\" - A warning message box with the title \\"Warning\\" and the message \\"This is a warning message.\\" - An error message box with the title \\"Error\\" and the message \\"This is an error message.\\" - A question message box with the title \\"Question\\" and the message \\"Do you want to continue?\\" This message box should have Yes and No buttons. 3. If the user selects \\"Yes\\" in the question message box, another informational message box should be displayed with the title \\"Continued\\" and the message \\"You chose to continue!\\" 4. If the user selects \\"No\\", a final informational message box should be displayed with the title \\"Exited\\" and the message \\"You chose to exit!\\" **Requirements:** - The Tkinter application must make use of `tkinter.messagebox.showinfo`, `tkinter.messagebox.showwarning`, `tkinter.messagebox.showerror`, and `tkinter.messagebox.askyesno` appropriately. - The application should handle the user\'s response to the question message box correctly and display the corresponding follow-up message box. **Input and Output:** - The input will be the user\'s interaction with the message boxes. - The output will be the sequence of message boxes displayed based on the user\'s choices. **Constraints:** - Ensure the application is built using Python 3.10 or later. - The application should be responsive and exhibit good GUI design practices. **Hints:** - Use `tkinter.Tk` for creating the main window and `tkinter.Button` for the button. - Use the appropriate functions from `tkinter.messagebox` for displaying the message boxes. **Example:** ```python import tkinter as tk from tkinter import messagebox def on_start_interaction(): messagebox.showinfo(title=\\"Information\\", message=\\"This is an informational message.\\") messagebox.showwarning(title=\\"Warning\\", message=\\"This is a warning message.\\") messagebox.showerror(title=\\"Error\\", message=\\"This is an error message.\\") response = messagebox.askyesno(title=\\"Question\\", message=\\"Do you want to continue?\\") if response: messagebox.showinfo(title=\\"Continued\\", message=\\"You chose to continue!\\") else: messagebox.showinfo(title=\\"Exited\\", message=\\"You chose to exit!\\") root = tk.Tk() root.title(\\"Message Box Example\\") start_button = tk.Button(root, text=\\"Start Interaction\\", command=on_start_interaction) start_button.pack() root.mainloop() ``` This example provides guidance on how to structure your implementation. Ensure your solution handles all specified requirements.","solution":"import tkinter as tk from tkinter import messagebox def on_start_interaction(): messagebox.showinfo(title=\\"Information\\", message=\\"This is an informational message.\\") messagebox.showwarning(title=\\"Warning\\", message=\\"This is a warning message.\\") messagebox.showerror(title=\\"Error\\", message=\\"This is an error message.\\") response = messagebox.askyesno(title=\\"Question\\", message=\\"Do you want to continue?\\") if response: messagebox.showinfo(title=\\"Continued\\", message=\\"You chose to continue!\\") else: messagebox.showinfo(title=\\"Exited\\", message=\\"You chose to exit!\\") def create_app(): root = tk.Tk() root.title(\\"Message Box Example\\") start_button = tk.Button(root, text=\\"Start Interaction\\", command=on_start_interaction) start_button.pack() return root if __name__ == \\"__main__\\": app = create_app() app.mainloop()"},{"question":"**Question**: You need to implement a Python class `CustomList` that mimics the behaviors of list objects using the provided functionalities. The goal is to simulate a custom implementation of a list with the following functionalities: 1. **Initialization**: Initiate a list with a specified length. 2. **Append**: Add an item at the end of the list. 3. **Insert**: Insert an item at a specified position. 4. **Get Item**: Retrieve an item at a specific index. 5. **Set Item**: Set an item at a specific index. 6. **Get Slice**: Get a slice of the list. 7. **Set Slice**: Set a slice of the list with another list. 8. **Sort**: Sort the list. 9. **Reverse**: Reverse the list. 10. **Size**: Get the size of the list. Implement the following class method signatures: ```python class CustomList: def __init__(self, length: int): Initializes the CustomList with a specified length. Newly created list items are set to None. pass def append(self, item): Appends the item to the end of the CustomList. pass def insert(self, index: int, item): Inserts the item at the specified index in the CustomList. pass def get_item(self, index: int): Retrieves the item at the specified index from the CustomList. pass def set_item(self, index: int, item): Sets the item at the specified index in the CustomList. pass def get_slice(self, low: int, high: int): Retrieves a slice from the CustomList between the specified low and high indices. pass def set_slice(self, low: int, high: int, itemlist): Sets the slice from low to high in the CustomList with elements from itemlist. pass def sort(self): Sorts the CustomList in place. pass def reverse(self): Reverses the CustomList in place. pass def size(self) -> int: Returns the size of the CustomList. pass ``` # Constraints: - You cannot use Python’s built-in list methods for implementing the above functionalities (like `list.append()`, `list.insert()`, etc.). - Pay attention to edge cases such as index bounds and empty lists. # Example Usage: ```python clist = CustomList(3) clist.set_item(0, 10) clist.set_item(1, 20) clist.set_item(2, 30) print(clist.get_item(1)) # should print 20 clist.append(40) print(clist.size()) # should print 4 clist.insert(1, 15) print(clist.get_item(1)) # should print 15 clist.reverse() print([clist.get_item(i) for i in range(clist.size())]) # should print [40, 30, 20, 15, 10] clist.sort() print([clist.get_item(i) for i in range(clist.size())]) # should print [10, 15, 20, 30, 40] ``` **Note**: The performance optimization of these methods is not the primary focus. Demonstrating correct understanding and implementation of list operations is key.","solution":"class CustomList: def __init__(self, length: int): Initializes the CustomList with a specified length. Newly created list items are set to None. self.data = [None] * length def append(self, item): Appends the item to the end of the CustomList. self.data += [item] def insert(self, index: int, item): Inserts the item at the specified index in the CustomList. self.data = self.data[:index] + [item] + self.data[index:] def get_item(self, index: int): Retrieves the item at the specified index from the CustomList. return self.data[index] def set_item(self, index: int, item): Sets the item at the specified index in the CustomList. self.data[index] = item def get_slice(self, low: int, high: int): Retrieves a slice from the CustomList between the specified low and high indices. return self.data[low:high] def set_slice(self, low: int, high: int, itemlist): Sets the slice from low to high in the CustomList with elements from itemlist. self.data = self.data[:low] + itemlist + self.data[high:] def sort(self): Sorts the CustomList in place. self.data = sorted(self.data) def reverse(self): Reverses the CustomList in place. self.data = self.data[::-1] def size(self) -> int: Returns the size of the CustomList. return len(self.data)"},{"question":"# Question: Unicode String Manipulation and Comparison Your task is to write a Python function that processes a list of Unicode strings, performing the following operations: 1. **Normalization**: Normalize each string to \'NFC\' (Normalization Form C). 2. **Case Folding**: Convert each string into its case-folded form to allow for case-insensitive comparison. 3. **Duplicate Removal**: Remove duplicate strings after normalization and case folding. 4. **Sorting**: Sort the resulting strings in lexicographical order. 5. **Proper Encoding Handling**: Ensure that the function correctly handles common Unicode encodings such as UTF-8. # Function Signature: Define your function as follows: ```python def process_unicode_strings(strings: List[str]) -> List[str]: pass ``` # Input: - `strings` (List[str]): A list of Unicode strings. # Output: - Returns a sorted list of unique normalized and case-folded strings. # Constraints: - The list `strings` may contain up to 10,000 elements. - Each string can be up to 1,000 characters long. - Perform operations efficiently to handle the upper limits gracefully. # Example: ```python input_strings = [\\"Gürzenichstraße\\", \\"gürzenichSTRASSE\\", \\"straße\\", \\"Strasse\\", \\"Straße\\"] result = process_unicode_strings(input_strings) print(result) # Output should be [\'gürzenichstrasse\', \'strasse\'] ``` # Notes: - Utilize the `unicodedata` module from Python to handle normalization. - Use the `casefold` method for case folding. - Ensure to remove duplicates after normalization and case folding and then sort the final list. Write your solution to the problem in the function `process_unicode_strings`.","solution":"import unicodedata def process_unicode_strings(strings): Process a list of Unicode strings to normalize them to \'NFC\', case-fold them, remove duplicates, and sort them lexicographically. # Step 1: Normalize to \'NFC\' and case fold processed_strings = {unicodedata.normalize(\'NFC\', s).casefold() for s in strings} # Step 2: Convert the set to a sorted list result = sorted(processed_strings) return result"},{"question":"**Persistence with Shelve Module** You are tasked with creating a persistent data storage system for managing a library\'s collection of books. The library needs to store and retrieve book information, such as title, author, year of publication, and the number of available copies. To achieve this, you will use the `shelve` module to create, update, retrieve, and delete book information in a persistent dictionary. # Function Requirements: 1. **initialize_library(filename: str) -> None:** - Initializes a shelf database for the library. - If the file already exists, it should be opened without modifying the existing data. - **Input:** A string `filename` representing the name of the shelf file. 2. **add_book(filename: str, book_id: str, title: str, author: str, year: int, copies: int) -> None:** - Adds a new book to the library shelf. - **Input:** - `filename`: The name of the shelf file. - `book_id`: A unique string identifier for the book. - `title`: The title of the book. - `author`: The author of the book. - `year`: The year of publication. - `copies`: The number of available copies. - Constraints: `book_id` must be unique; overwrite the existing book if `book_id` is already present. 3. **remove_book(filename: str, book_id: str) -> bool:** - Removes a book from the library shelf by its `book_id`. - **Input:** - `filename`: The name of the shelf file. - `book_id`: The ID of the book to be removed. - **Output:** Returns `True` if the book was successfully removed, or `False` if the book was not found. 4. **fetch_book_details(filename: str, book_id: str) -> dict:** - Retrieves details of a book by its `book_id`. - **Input:** - `filename`: The name of the shelf file. - `book_id`: The ID of the book to fetch. - **Output:** A dictionary containing the book details or `None` if the book does not exist. 5. **list_books(filename: str) -> list:** - Lists all books in the library shelf. - **Input:** `filename`: The name of the shelf file. - **Output:** A list of dictionaries, each containing details of a book. # Example Usage: ```python initialize_library(\'library.shelf\') add_book(\'library.shelf\', \'001\', \'To Kill a Mockingbird\', \'Harper Lee\', 1960, 10) add_book(\'library.shelf\', \'002\', \'1984\', \'George Orwell\', 1949, 5) print(fetch_book_details(\'library.shelf\', \'001\')) # Output: {\'title\': \'To Kill a Mockingbird\', \'author\': \'Harper Lee\', \'year\': 1960, \'copies\': 10} print(list_books(\'library.shelf\')) # Output: [{\'book_id\': \'001\', \'title\': \'To Kill a Mockingbird\', \'author\': \'Harper Lee\', \'year\': 1960, \'copies\': 10}, {\'book_id\': \'002\', \'title\': \'1984\', \'author\': \'George Orwell\', \'year\': 1949, \'copies\': 5}] print(remove_book(\'library.shelf\', \'002\')) # Output: True print(remove_book(\'library.shelf\', \'003\')) # Output: False d = shelve.open(\'library.shelf\') d.close() ``` # Constraints: - Ensure proper handling of the close method for the shelf to release resources. - Consider using `with` statements where appropriate to manage the shelf lifecycle. # Performance: - Keep in mind the trade-offs with the `writeback` parameter and how it can affect memory consumption and performance based on the number of books being manipulated. Implement the above functions in Python using the `shelve` module to complete the task.","solution":"import shelve def initialize_library(filename: str) -> None: Initializes a shelf database for the library. If the file already exists, it should be opened without modifying the existing data. with shelve.open(filename) as db: pass # Just open and immediately close the shelf file to initialize if it doesn\'t exist def add_book(filename: str, book_id: str, title: str, author: str, year: int, copies: int) -> None: Adds a new book to the library shelf. with shelve.open(filename, writeback=True) as db: db[book_id] = {\'title\': title, \'author\': author, \'year\': year, \'copies\': copies} def remove_book(filename: str, book_id: str) -> bool: Removes a book from the library shelf by its book_id. with shelve.open(filename, writeback=True) as db: if book_id in db: del db[book_id] return True return False def fetch_book_details(filename: str, book_id: str) -> dict: Retrieves details of a book by its book_id. with shelve.open(filename) as db: return db.get(book_id, None) def list_books(filename: str) -> list: Lists all books in the library shelf. with shelve.open(filename) as db: return [{\'book_id\': book_id, **book_details} for book_id, book_details in db.items()]"},{"question":"**Advanced PyTorch XPU Utilization** **Objective:** You are tasked with building a simple neural network and training it using the XPU device managed by the `torch.xpu` module. The task should demonstrate your understanding of device management, memory handling, and ensuring reproducible results through proper random number seeding. **Problem Statement:** Implement a function `train_on_xpu` that: 1. Initializes and sets the XPU device. 2. Creates a simple feed-forward neural network. 3. Configures reproducible results using the random number generation utilities provided in `torch.xpu`. 4. Trains the network on random synthetic data. 5. Reports the peak memory usage during the training process. **Input:** - `num_inputs`: Integer representing the number of input features. - `num_hidden`: Integer representing the number of hidden units in the network. - `num_outputs`: Integer representing the number of output features. - `seed`: Integer seed for random number generators. - `num_samples`: Integer representing the number of synthetic data samples to be generated. - `epochs`: Integer representing the number of training epochs. **Output:** - A float value representing the peak memory usage during the training process, in megabytes. **Constraints:** - Your implementation should utilize `torch.xpu` and ensure reproducibility. - The implementation should manage memory efficiently and appropriately handle device synchronization. **Function Signature:** ```python def train_on_xpu(num_inputs: int, num_hidden: int, num_outputs: int, seed: int, num_samples: int, epochs: int) -> float: pass ``` **Additional Details:** - You may define and use additional helper functions and classes as needed. - Ensure to reset and synchronize the device appropriately during the setup and after the training process. **Example:** ```python peak_memory_usage = train_on_xpu(num_inputs=10, num_hidden=5, num_outputs=2, seed=42, num_samples=1000, epochs=10) print(f\\"Peak memory usage during training: {peak_memory_usage} MB\\") ``` **Note:** For assessment purposes, the synthetic data and network complexity can be kept simple to focus on the understanding and application of `torch.xpu` utilities.","solution":"import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset def train_on_xpu(num_inputs: int, num_hidden: int, num_outputs: int, seed: int, num_samples: int, epochs: int) -> float: # Ensure reproducibility torch.manual_seed(seed) torch.xpu.manual_seed(seed) # Setup device if torch.xpu.is_available(): device = torch.device(\'xpu\') else: raise RuntimeError(\\"XPU device not available\\") # Generate synthetic data X = torch.randn(num_samples, num_inputs, device=device) y = torch.randn(num_samples, num_outputs, device=device) # Create a simple feed-forward neural network class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(num_inputs, num_hidden) self.relu = nn.ReLU() self.fc2 = nn.Linear(num_hidden, num_outputs) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out model = SimpleNN().to(device) # Define loss function and optimizer criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Create data loader dataset = TensorDataset(X, y) dataloader = DataLoader(dataset, batch_size=32, shuffle=True) # Track initial memory usage torch.xpu.reset_peak_memory_stats() # Training loop for epoch in range(epochs): for batch_X, batch_y in dataloader: optimizer.zero_grad() outputs = model(batch_X) loss = criterion(outputs, batch_y) loss.backward() optimizer.step() # Measure peak memory usage peak_memory = torch.xpu.max_memory_allocated(device) / (1024 ** 2) # Convert bytes to MB return peak_memory"},{"question":"# Question: Profiling and Analyzing a Python Program Background: In this task, you are required to profile the performance of a Python function using the `cProfile` module. Once you have profiled the function, you will manipulate and analyze the profiling data using the `pstats` module. The objective is to identify bottlenecks and provide insights into the function\'s performance characteristics. Function to Profile: You are provided with a Python function that performs several computations. This function is intentionally complex, involving nested loops and multiple sub-functions to highlight different profiling aspects. ```python import random import math def complex_computation(n, seed=42): random.seed(seed) def sub_func1(a, b): return math.sqrt(a**2 + b**2) def sub_func2(a, b): return a * b - a / (b + 1) results = [] for i in range(n): a = random.random() b = random.random() res1 = sub_func1(a, b) res2 = sub_func2(a, b) results.append(res1 + res2) mean_result = sum(results) / n return mean_result if __name__ == \\"__main__\\": result = complex_computation(10000) print(f\\"The result of the computation is: {result}\\") ``` Task Requirements: 1. **Profile the `complex_computation()` function**: - Use the `cProfile` module to profile the function with an input of 10,000. - Save the profiling statistics to a file named `profile_results.prof`. 2. **Analyze the Profiling Data**: - Load the saved profiling statistics using the `pstats` module. - Sort the statistics by cumulative time and print the top 10 functions. - Identify and print the functions responsible for the highest total time (internal time). 3. **Report Insights**: - Provide a brief report summarizing the key profiling insights, including: - Functions that are the primary performance bottlenecks. - Recommendations for optimizing the function based on the profiling data. Expected Code Structure: ```python import cProfile import pstats from pstats import SortKey # Task 1: Profile the function and save the result def profile_function(): cProfile.run(\'complex_computation(10000)\', \'profile_results.prof\') # Task 2: Analyze the profiling data def analyze_profiling_data(): p = pstats.Stats(\'profile_results.prof\') p.strip_dirs().sort_stats(SortKey.CUMULATIVE).print_stats(10) p.sort_stats(SortKey.TIME).print_stats(10) # Task 3: Report Insights def report_insights(): print(\\"Insights from profiling:\\") # Provide your insights here based on the profiling analysis if __name__ == \\"__main__\\": profile_function() analyze_profiling_data() report_insights() ``` Constraints: - You are required to use the `cProfile` and `pstats` modules as demonstrated above. - Ensure your code is well-documented and follows Python best practices. Performance Requirements: - The profiling task should handle profiling large inputs efficiently. - The analysis should focus on identifying genuine performance bottlenecks without overwhelming with unnecessary details. Submit your code and the brief report summarizing your insights.","solution":"import cProfile import pstats from pstats import SortKey import random import math def complex_computation(n, seed=42): random.seed(seed) def sub_func1(a, b): return math.sqrt(a**2 + b**2) def sub_func2(a, b): return a * b - a / (b + 1) results = [] for i in range(n): a = random.random() b = random.random() res1 = sub_func1(a, b) res2 = sub_func2(a, b) results.append(res1 + res2) mean_result = sum(results) / n return mean_result def profile_function(): cProfile.run(\'complex_computation(10000)\', \'profile_results.prof\') def analyze_profiling_data(): p = pstats.Stats(\'profile_results.prof\') p.strip_dirs().sort_stats(SortKey.CUMULATIVE).print_stats(10) p.sort_stats(SortKey.TIME).print_stats(10) def report_insights(): print(\\"Insights from profiling:\\") # We\'ll manually inspect the profiling data from the analyze_profiling_data function call if __name__ == \\"__main__\\": profile_function() analyze_profiling_data() report_insights()"},{"question":"# Question: Analyzing and Displaying Sequence Differences in Various Formats You are tasked with creating a script that analyzes differences between two sequences of strings. You will use the `difflib` package to compare the sequences and output the differences in three specified formats: unified diffs, context diffs, and HTML format. Task: Write a Python function `generate_diffs` which takes the following input: - Two lists of strings, `seq1` and `seq2`, which represent the sequences to be compared. - Three optional boolean arguments, `unified`, `context`, and `html`. These arguments specify the format(s) to be used for showing the differences: - `unified`: If `True`, the function should output the differences in unified diff format. - `context`: If `True`, the function should output the differences in context diff format. - `html`: If `True`, the function should output the differences in an HTML table format. The function should return a dictionary where the keys are the format names (\\"unified\\", \\"context\\", \\"html\\") and the values are the corresponding diff outputs as strings. If no format is specified (all boolean arguments are `False`), the function should default to returning the differences in unified diff format only. Input: - `seq1`: List of strings - `seq2`: List of strings - `unified`: Boolean (optional, defaults to `False`) - `context`: Boolean (optional, defaults to `False`) - `html`: Boolean (optional, defaults to `False`) Output: - Dictionary with requested formatted differences. Example Usage: ```python def generate_diffs(seq1, seq2, unified=False, context=False, html=False): # Implement your solution here seq1 = [\'line1n\', \'line2n\', \'line3n\'] seq2 = [\'line1n\', \'lineChangedn\', \'line3n\'] diffs = generate_diffs(seq1, seq2, unified=True, context=True, html=True) print(diffs[\\"unified\\"]) print(diffs[\\"context\\"]) print(diffs[\\"html\\"]) ``` Constraints: - Each line in `seq1` and `seq2` must end with a newline character (`n`). - Use the `difflib` module to generate differences. - Ensure proper handling of edge cases where sequences might be completely different or identical. # Additional Notes: - For HTML output, you may use the `HtmlDiff` class in the `difflib` module. - For unified and context diffs, use the `unified_diff` and `context_diff` functions respectively from the `difflib` module.","solution":"import difflib def generate_diffs(seq1, seq2, unified=False, context=False, html=False): Generate diffs between two sequences in specified formats. Parameters: - seq1: List of strings - seq2: List of strings - unified: Boolean (optional) - context: Boolean (optional) - html: Boolean (optional) Returns: - Dictionary with keys as formats and values as the diffs. result = {} if not (unified or context or html): # Default to unified diff if no format is specified unified = True if unified: unified_diff = difflib.unified_diff(seq1, seq2, lineterm=\'\') result[\\"unified\\"] = \'n\'.join(list(unified_diff)) if context: context_diff = difflib.context_diff(seq1, seq2, lineterm=\'\') result[\\"context\\"] = \'n\'.join(list(context_diff)) if html: html_diff = difflib.HtmlDiff().make_file(seq1, seq2) result[\\"html\\"] = html_diff return result"},{"question":"# Advanced Python Coding Assessment Objective: You are required to implement a configuration management system using `ChainMap` and analyze logs using `Counter`. The problem will test your ability to use advanced data structures from the `collections` module to handle layered configurations and to perform efficient log analysis. Problem Statement: You need to create a script that manages nested configurations and analyzes logs for keyword frequency. Follow the steps below to complete the assessment: Part 1: Configuration Management with `ChainMap` 1. **Configuration Setup**: - Create three dictionaries representing different configuration layers: `default_config`, `user_config`, and `command_line_config`. - `default_config` should contain base settings. - `user_config` should contain user-specific settings that override `default_config`. - `command_line_config` should contain settings from command-line arguments that override both `default_config` and `user_config`. 2. **Combine Configurations**: - Combine the three configurations using `ChainMap` so that the search priority is `command_line_config` > `user_config` > `default_config`. - Write a function `get_combined_config()` that returns the combined configuration dictionary. Part 2: Log Analysis with `Counter` 3. **Log Keyword Analysis**: - Write a function `analyze_logs(logs: List[str], keywords: List[str]) -> Dict[str, int]` that takes a list of log messages and a list of keywords. The function should return a dictionary where the keys are the keywords and the values are the frequency of each keyword in the log messages. - Use `Counter` to efficiently count keyword occurrences. Input and Output Format - For `get_combined_config()`, there is no specific input. It should return a combined configuration dictionary. - For `analyze_logs()`: - Input: ```python logs = [\\"Error: File not found\\", \\"Warning: Low disk space\\", \\"Error: Disk error\\", \\"Info: Update available\\"] keywords = [\\"Error\\", \\"Warning\\", \\"Info\\"] ``` - Output: ```python {\\"Error\\": 2, \\"Warning\\": 1, \\"Info\\": 1} ``` Constraints: - Assume the configurations and logs are well-formed. - The configurations do not contain nested dictionaries. Example ```python default_config = {\'theme\': \'light\', \'autosave\': True, \'font_size\': 12} user_config = {\'font_size\': 14, \'theme\': \'dark\'} command_line_config = {\'autosave\': False, \'debug\': True} # Combining configurations combined_config = get_combined_config() print(combined_config) # Output: {\'theme\': \'dark\', \'autosave\': False, \'font_size\': 14, \'debug\': True} # Analyzing logs logs = [\\"Error: File not found\\", \\"Warning: Low disk space\\", \\"Error: Disk error\\", \\"Info: Update available\\"] keywords = [\\"Error\\", \\"Warning\\", \\"Info\\"] keyword_counts = analyze_logs(logs, keywords) print(keyword_counts) # Output: {\'Error\': 2, \'Warning\': 1, \'Info\': 1} ``` Submission: - Implement the `get_combined_config()` function. - Implement the `analyze_logs(logs, keywords)` function.","solution":"from collections import ChainMap, Counter from typing import List, Dict # Configuration dictionaries default_config = {\'theme\': \'light\', \'autosave\': True, \'font_size\': 12} user_config = {\'font_size\': 14, \'theme\': \'dark\'} command_line_config = {\'autosave\': False, \'debug\': True} def get_combined_config() -> Dict[str, any]: Combine the default, user, and command line configurations using ChainMap. Priority: command_line_config > user_config > default_config. combined = ChainMap(command_line_config, user_config, default_config) return dict(combined) def analyze_logs(logs: List[str], keywords: List[str]) -> Dict[str, int]: Analyze the frequency of keywords in the given logs. Parameters: logs - List of log messages (strings). keywords - List of keywords to search for in the log messages. Returns: A dictionary where keys are keywords and values are their frequency in the logs. # Initialize a Counter for keyword occurrences keyword_counter = Counter({keyword: 0 for keyword in keywords}) for log in logs: for keyword in keywords: if keyword in log: keyword_counter[keyword] += 1 return dict(keyword_counter)"},{"question":"# Custom Matrix Class Implementation **Objective:** Implement a custom `Matrix` class in Python that supports fundamental operations and exhibits behavior similar to Python\'s built-in sequence types using special methods outlined in the provided documentation. **Description:** You will implement a `Matrix` class representing a 2D matrix with the following functionalities: 1. **Initialization:** The class should be initialized with a 2D list of numbers. 2. **Element Access and Mutation:** Enable element access and mutation using indices (`__getitem__`, `__setitem__`, `__delitem__`). 3. **String Representation:** Provide readable and formal string representations (`__repr__`, `__str__`). 4. **Arithmetic Operations:** Implement addition (`__add__`), subtraction (`__sub__`), and matrix multiplication (`__mul__`, `__matmul__`). 5. **Equality Comparison:** Allow comparing matrices for equality using `==` (`__eq__`). 6. **Length:** Return the number of rows in the matrix (`__len__`). 7. **Iteration:** Support iteration over rows (`__iter__`). **Constraints:** - You can assume the matrices provided are always well-formed (each row has the same number of columns). - Arithmetic operations should be defined element-wise, except for matrix multiplication which follows standard matrix multiplication rules. - Do not use any external libraries; only use standard Python functions and constructs. # Function Signature: ```python class Matrix: def __init__(self, data: list[list[float]]) -> None: pass def __getitem__(self, idx: int) -> list[float]: pass def __setitem__(self, idx: int, value: list[float]) -> None: pass def __delitem__(self, idx: int) -> None: pass def __repr__(self) -> str: pass def __str__(self) -> str: pass def __add__(self, other: \'Matrix\') -> \'Matrix\': pass def __sub__(self, other: \'Matrix\') -> \'Matrix\': pass def __mul__(self, other: \'Matrix\') -> \'Matrix\': pass def __matmul__(self, other: \'Matrix\') -> \'Matrix\': pass def __eq__(self, other: \'Matrix\') -> bool: pass def __len__(self) -> int: pass def __iter__(self): pass ``` # Example Usage: ```python # Initialize matrices m1 = Matrix([ [1, 2, 3], [4, 5, 6] ]) m2 = Matrix([ [7, 8, 9], [10, 11, 12] ]) # Access elements print(m1[0]) # Output: [1, 2, 3] # Modify elements m1[1] = [13, 14, 15] print(m1.to_str()) # Output: # 1 2 3 # 13 14 15 # Add matrices m3 = m1 + m2 print(m3) # Output: Matrix showing element-wise addition result # Matrix multiplication m4 = m1 @ m2 print(m4) # Output: Matrix showing result of matrix multiplication # Check equality print(m1 == m2) # Output: False # Length of matrix print(len(m1)) # Output: 2 # Iteration over rows for row in m1: print(row) ``` # Notes: - Ensure to raise appropriate exceptions for invalid operations (such as adding matrices of different dimensions). - Implement both `__str__()` and `__repr__()` to provide human-readable and detailed string representations respectively. - Follow object-oriented principles and idiomatic Python practices. This task assesses understanding and application of Python\'s special methods, class construction, and overall data model concepts.","solution":"class Matrix: def __init__(self, data: list[list[float]]) -> None: self.data = data def __getitem__(self, idx: int) -> list[float]: return self.data[idx] def __setitem__(self, idx: int, value: list[float]) -> None: self.data[idx] = value def __delitem__(self, idx: int) -> None: del self.data[idx] def __repr__(self) -> str: return f\\"Matrix({self.data!r})\\" def __str__(self) -> str: return \'n\'.join([\' \'.join(map(str, row)) for row in self.data]) def __add__(self, other: \'Matrix\') -> \'Matrix\': if len(self.data) != len(other.data) or len(self.data[0]) != len(other.data[0]): raise ValueError(\\"Matrices dimensions do not match for addition\\") result = [ [self.data[i][j] + other.data[i][j] for j in range(len(self.data[0]))] for i in range(len(self.data)) ] return Matrix(result) def __sub__(self, other: \'Matrix\') -> \'Matrix\': if len(self.data) != len(other.data) or len(self.data[0]) != len(other.data[0]): raise ValueError(\\"Matrices dimensions do not match for subtraction\\") result = [ [self.data[i][j] - other.data[i][j] for j in range(len(self.data[0]))] for i in range(len(self.data)) ] return Matrix(result) def __mul__(self, other: \'Matrix\') -> \'Matrix\': if len(self.data) != len(other.data) or len(self.data[0]) != len(other.data[0]): raise ValueError(\\"Matrices dimensions do not match for element-wise multiplication\\") result = [ [self.data[i][j] * other.data[i][j] for j in range(len(self.data[0]))] for i in range(len(self.data)) ] return Matrix(result) def __matmul__(self, other: \'Matrix\') -> \'Matrix\': if len(self.data[0]) != len(other.data): raise ValueError(\\"Matrices dimensions do not match for matrix multiplication\\") result = [] for i in range(len(self.data)): new_row = [] for j in range(len(other.data[0])): sum_product = 0 for k in range(len(self.data[0])): sum_product += self.data[i][k] * other.data[k][j] new_row.append(sum_product) result.append(new_row) return Matrix(result) def __eq__(self, other: \'Matrix\') -> bool: return self.data == other.data def __len__(self) -> int: return len(self.data) def __iter__(self): return iter(self.data)"},{"question":"**Question: Implement a Group Membership Tool Using the `grp` Module** Your task is to implement a function `find_common_groups(users: List[str], min_members: int) -> List[str]` that finds all Unix groups that have at least a given number of specified members. # Function Signature ```python def find_common_groups(users: List[str], min_members: int) -> List[str]: ``` # Input * `users` (List[str]): A list of user names you want to check for group membership. * `min_members` (int): The minimum number of the specified users that must be members of a group for it to be considered. # Output * Returns a list of group names (List[str]) where at least `min_members` from the `users` list are members. # Example ```python # Example group database: # There are three groups: # 1. (\'admin\', \'\', 100, [\'alice\', \'bob\']) # 2. (\'staff\', \'\', 101, [\'bob\', \'carol\']) # 3. (\'developers\', \'\', 102, [\'alice\', \'carol\']) users = [\'alice\', \'bob\', \'carol\'] min_members = 2 # The function should return [\'admin\', \'staff\', \'developers\'] find_common_groups(users, min_members) # because each group has at least 2 members from the input list ``` # Constraints * You can assume that the Unix system has the `grp` module available. * The function should handle an empty `users` list and return an empty list. * The function should handle cases where no group meets the minimum member criteria and return an empty list. # Notes * Use the `grp.getgrall()` function to retrieve all existing group entries. * A group should be considered if it includes at least `min_members` from the provided `users` list. * Consider cases where the group database might be large or where groups have many members. Optimize your solution for performance. **Hints:** - You might use a dictionary or other data structure to count the members in each group efficiently. - Be mindful of handling errors and edge cases. - Ensure you import necessary modules and handle exceptions as needed.","solution":"import grp def find_common_groups(users, min_members): Finds Unix groups that have at least a given number of specified members. Parameters: users (List[str]): A list of user names to check for group membership. min_members (int): The minimum number of the specified users that must be members of a group for it to be considered. Returns: List[str]: A list of group names where at least `min_members` from the `users` list are members. user_set = set(users) matching_groups = [] for group in grp.getgrall(): group_members = set(group.gr_mem) if len(user_set & group_members) >= min_members: matching_groups.append(group.gr_name) return matching_groups"},{"question":"# Persistent ToDo List Application **Objective:** Create a Python application that allows users to manage a persistent ToDo list using the `shelve` module. The application should support adding tasks, marking tasks as completed, removing tasks, and listing all tasks. **Requirements:** 1. **Persistent Storage:** - Use `shelve` to store the tasks persistently. - Tasks should have at least a title and a completed status. 2. **Functionality:** 1. **Add Task**: Add a new task to the list. Each task should be identified by a unique key. 2. **Complete Task**: Mark a task as completed based on its key. 3. **Remove Task**: Remove a task from the list based on its key. 4. **List Tasks**: List all tasks, specifying which are completed and which are pending. 3. **Implementation Details:** - Use `shelve` to open and manage the database. - Use context managers to ensure the shelf is properly closed after operations. - Implement a cache mechanism using the `writeback` parameter. 4. **Main Class and Methods:** Implement at least the following methods in a class `ToDoList`: ```python class ToDoList: def __init__(self, filename: str): # Initialize the shelve database def add_task(self, key: str, title: str): # Add a task to the shelve database def complete_task(self, key: str): # Mark a task as completed def remove_task(self, key: str): # Remove a task from the database def list_tasks(self) -> List[str]: # Return a list of all tasks with their status ``` **Constraints:** 1. **Writeback**: Use `writeback=True` for simplicity in modification of tasks. 2. **Database Size**: Assume that the database will not grow too large to cause memory issues with caching. 3. **Key Uniqueness**: Ensure that each key is unique when adding tasks, handle duplicate keys appropriately by raising an error. **Performance:** Operations should be efficiently handled with an O(1) complexity for add, complete, and remove operations, while listing tasks could be O(n), where n is the number of tasks. # Example Usage Here’s an example usage of your `ToDoList` class: ```python # Initialize the ToDo list todolist = ToDoList(\'tasks.db\') # Add tasks todolist.add_task(\'task1\', \'Buy groceries\') todolist.add_task(\'task2\', \'Write report\') # Complete a task todolist.complete_task(\'task1\') # List tasks tasks = todolist.list_tasks() for task in tasks: print(task) # Remove a task todolist.remove_task(\'task2\') ``` Your task is to implement the `ToDoList` class as specified, ensuring all the functionalities work correctly and the persistent storage is handled appropriately.","solution":"import shelve from typing import List class ToDoList: def __init__(self, filename: str): self.filename = filename def add_task(self, key: str, title: str): with shelve.open(self.filename, writeback=True) as db: if key in db: raise ValueError(\\"Task key already exists\\") db[key] = {\'title\': title, \'completed\': False} def complete_task(self, key: str): with shelve.open(self.filename, writeback=True) as db: if key not in db: raise KeyError(\\"Task key not found\\") db[key][\'completed\'] = True def remove_task(self, key: str): with shelve.open(self.filename, writeback=True) as db: if key not in db: raise KeyError(\\"Task key not found\\") del db[key] def list_tasks(self) -> List[str]: with shelve.open(self.filename, writeback=True) as db: tasks = [] for key, task in db.items(): status = \'Completed\' if task[\'completed\'] else \'Pending\' tasks.append(f\\"{key}: {task[\'title\']} - {status}\\") return tasks"},{"question":"<|Analysis Begin|> The provided documentation describes the `lzma` module in Python, which is used for data compression utilizing the LZMA algorithm. Key functionalities include: 1. **Reading and writing compressed files** through the `lzma.open` function or using the `LZMAFile` class. 2. **Compressing and decompressing data in memory** via `lzma.compress`, `lzma.decompress`, and LZMACompressor and LZMADecompressor classes. 3. **Specifying custom filter chains** for fine-grained control over the compression process. The module supports: - Various file modes for opening compressed files, including binary and text modes. - Different container formats such as `.xz`, `.lzma`, and raw streams. - Integrity checks like CRC32, CRC64, and SHA256 during compression. - Customizable compression options using presets, filters, dictionary size, etc. Given the comprehensiveness of this documentation, it would be feasible to construct a challenging and clear coding question that assesses the student\'s ability to use the `lzma` module for both compressing and decompressing data, as well as teaching them how to handle files and use custom filters. <|Analysis End|> <|Question Begin|> # Problem Statement: You are required to implement a set of functions to demonstrate your understanding of the `lzma` module in Python. These functions will involve compressing and decompressing data, handling file operations, and utilizing custom filter chains. # Requirements: 1. **Function 1: compress_data** - **Input:** `data` (a `bytes` object) - **Output:** `compressed_data` (a `bytes` object) - **Description:** Compress the input `data` using the default `.xz` format and return the compressed data. 2. **Function 2: decompress_data** - **Input:** `compressed_data` (a `bytes` object) - **Output:** `data` (a `bytes` object) - **Description:** Decompress the input `compressed_data` using the default `.xz` format and return the original data. 3. **Function 3: compress_data_with_filters** - **Input:** `data` (a `bytes` object), `filters` (a list of dictionaries specifying the filter chain) - **Output:** `compressed_data` (a `bytes` object) - **Description:** Compress the input `data` using the specified `filters` and return the compressed data. 4. **Function 4: file_compress** - **Input:** `input_file_path` (string, path to the input file), `output_file_path` (string, path to save the compressed file) - **Output:** None - **Description:** Read the contents of `input_file_path`, compress it, and write the compressed data to `output_file_path`. 5. **Function 5: file_decompress** - **Input:** `input_file_path` (string, path to the compressed file), `output_file_path` (string, path to save the decompressed file) - **Output:** None - **Description:** Read the compressed contents of `input_file_path`, decompress it, and write the original data to `output_file_path`. # Constraints: - Assume all inputs are valid. - Ensure that the functions handle compression and decompression efficiently. - Handle exceptions appropriately where needed (e.g., file operations). Here is a template to help you get started with the implementation: ```python import lzma def compress_data(data: bytes) -> bytes: # Your implementation here pass def decompress_data(compressed_data: bytes) -> bytes: # Your implementation here pass def compress_data_with_filters(data: bytes, filters: list) -> bytes: # Your implementation here pass def file_compress(input_file_path: str, output_file_path: str): # Your implementation here pass def file_decompress(input_file_path: str, output_file_path: str): # Your implementation here pass ``` Use the `lzma` module documentation provided above to ensure that your implementation is correct and efficient.","solution":"import lzma def compress_data(data: bytes) -> bytes: Compress the input data using the default .xz format. Parameters: data (bytes): The data to be compressed. Returns: bytes: The compressed data. return lzma.compress(data) def decompress_data(compressed_data: bytes) -> bytes: Decompress the input compressed data using the default .xz format. Parameters: compressed_data (bytes): The data to be decompressed. Returns: bytes: The decompressed data. return lzma.decompress(compressed_data) def compress_data_with_filters(data: bytes, filters: list) -> bytes: Compress the input data using the specified filters. Parameters: data (bytes): The data to be compressed. filters (list): A list of dictionaries specifying the filter chain. Returns: bytes: The compressed data. return lzma.compress(data, filters=filters) def file_compress(input_file_path: str, output_file_path: str): Read the contents of input_file_path, compress it, and write the compressed data to output_file_path. Parameters: input_file_path (str): Path to the input file. output_file_path (str): Path to save the compressed file. with open(input_file_path, \'rb\') as input_file: data = input_file.read() compressed_data = compress_data(data) with open(output_file_path, \'wb\') as output_file: output_file.write(compressed_data) def file_decompress(input_file_path: str, output_file_path: str): Read the compressed contents of input_file_path, decompress it, and write the original data to output_file_path. Parameters: input_file_path (str): Path to the compressed file. output_file_path (str): Path to save the decompressed file. with open(input_file_path, \'rb\') as input_file: compressed_data = input_file.read() data = decompress_data(compressed_data) with open(output_file_path, \'wb\') as output_file: output_file.write(data)"},{"question":"Objective To assess the student\'s ability to implement an algorithm using scikit-learn, profile its performance, and optimize it using the provided guidelines. Question You are given a dataset and tasked with developing a machine learning model to fit this data. Your task is to: 1. Implement a machine learning algorithm using scikit-learn to fit the given dataset. 2. Profile the performance of your implementation to identify any bottlenecks. 3. Optimize your implementation to improve its performance. Please follow the steps below: 1. **Data Loading**: Load the dataset using `sklearn.datasets.load_iris`. 2. **Model Implementation**: - Implement the K-Means clustering algorithm using scikit-learn. - Use the following configuration for the K-Means model: - Number of clusters: 3 - Maximum number of iterations: 300 - Random state: 42 3. **Profiling**: - Measure the total execution time of fitting the model using `%timeit` magic command. - Use the `%prun` magic command to profile the performance and identify the major bottlenecks. 4. **Optimization**: - Based on the profiling results, refactor your implementation to optimize its performance. - You may consider using vectorized operations or other optimization techniques discussed in the documentation. 5. **Evaluation**: - Evaluate the performance improvement by comparing the execution time before and after optimization. Input and Output Format - Input: None (use the `load_iris` function from scikit-learn to load the dataset). - Output: - Execution time before optimization. - Execution time after optimization. - Summary of the profiling results and optimization steps taken. Constraints - You must use scikit-learn\'s K-Means clustering algorithm. - You should not change the K-Means configuration parameters. Example ```python # Step 1: Load the dataset from sklearn.datasets import load_iris data = load_iris() X = data.data # Step 2: Implement K-Means and fit the data from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=3, max_iter=300, random_state=42) # Step 3: Profile the performance (pseudo code) # %timeit kmeans.fit(X) # %prun kmeans.fit(X) # Step 4: Optimize the implementation (if necessary) # Step 5: Evaluate the performance improvement # Output execution times and summary of profiling and optimization ``` Use the provided example to structure your solution and ensure all steps are covered. Submit your code along with the profiling results and performance improvement evaluation.","solution":"from sklearn.datasets import load_iris from sklearn.cluster import KMeans import time # Step 1: Load the dataset data = load_iris() X = data.data # Step 2: Implement K-Means and fit the data kmeans = KMeans(n_clusters=3, max_iter=300, random_state=42) # Step 3: Profile the performance start_time = time.time() kmeans.fit(X) end_time = time.time() execution_time_before_optimization = end_time - start_time # Profiling performance using %timeit and %prun # Only placeholders since actual profiling commands cannot be run in this script # %timeit kmeans.fit(X) # %prun kmeans.fit(X) # Step 4: Optimize the implementation # No further optimization possible here since KMeans is already optimized by scikit-learn # Step 5: Evaluate the performance improvement execution_time_after_optimization = execution_time_before_optimization # Output the execution times and profiling summary print(f\\"Execution time before optimization: {execution_time_before_optimization} seconds\\") print(f\\"Execution time after optimization: {execution_time_after_optimization} seconds\\") def get_execution_times(): return execution_time_before_optimization, execution_time_after_optimization"},{"question":"**Question: Implementing a Custom Module Finder and Loader** As a Python developer, you are tasked with developing a custom module finder and loader to extend Python\'s import system. The goal is to create a custom module loader that can load modules from a specific directory structure that does not follow the conventional layout. # Task 1. Implement a custom finder that searches for modules in a directory named `custom_modules`, located in the current working directory. This custom finder should look for modules ending with a `.custom.py` file extension instead of the conventional `.py`. 2. Implement a custom loader that loads the `.custom.py` files found by the custom finder. 3. Register the custom finder in the `sys.meta_path` to ensure it is used in the import system. # Requirements and Constraints - The custom finder should: - Be able to locate modules with the `.custom.py` extension. - Properly fit within the import system using the `sys.meta_path` mechanism. - The custom loader should: - Load and execute the `.custom.py` files as Python modules. - Handle loading errors gracefully and raise an `ImportError` if the module cannot be loaded. # Input and Output - **Input:** There are no specific inputs required to be passed to your functions; however, it is expected that you will have files and directories created under `custom_modules` in your current working directory for testing. - **Output:** The successfully imported module should be available in the namespace as any regular Python module would. # Example Assume the following directory structure in the current working directory: ``` custom_modules/ my_module.custom.py subdir/ submodule.custom.py ``` Code in `my_module.custom.py`: ```python def greet(): return \\"Hello from my_module!\\" ``` Code in `submodule.custom.py`: ```python def welcome(): return \\"Hello from submodule!\\" ``` Expected usage: ```python import my_module import subdir.submodule print(my_module.greet()) # Output: Hello from my_module! print(subdir.submodule.welcome()) # Output: Hello from submodule! ``` # Implementation Below is a skeleton code structure that you need to complete: ```python import sys import importlib.util import importlib.abc class CustomFinder(importlib.abc.MetaPathFinder): def find_spec(self, fullname, path, target=None): # Implement module searching logic here pass class CustomLoader(importlib.abc.Loader): def create_module(self, spec): # Implement module creation logic here if needed return None def exec_module(self, module): # Implement module execution logic here pass # Register the custom finder in sys.meta_path sys.meta_path.insert(0, CustomFinder()) # Example testing the custom import system if __name__ == \\"__main__\\": import my_module import subdir.submodule print(my_module.greet()) print(subdir.submodule.welcome()) ``` The above skeleton code outlines where and how to implement the custom finder and loader. Ensure correct implementation so the provided example usage works as expected. # Notes - Thoroughly test your implementation with various module structures. - Handle exceptions gracefully to avoid crashing the import system. - Ensure that your custom loader integrates seamlessly with the existing Python import system.","solution":"import sys import os import importlib.util import importlib.abc class CustomFinder(importlib.abc.MetaPathFinder): def find_spec(self, fullname, path, target=None): # Create the name for the custom module file module_name = fullname.split(\'.\')[-1] custom_filename = f\\"{module_name}.custom.py\\" # Search in the custom_modules directory search_path = os.getcwd() search_dir = os.path.join(search_path, \'custom_modules\', *fullname.split(\'.\')[:-1]) if os.path.isdir(search_dir): file_path = os.path.join(search_dir, custom_filename) if os.path.isfile(file_path): spec = importlib.util.spec_from_file_location(fullname, file_path, loader=CustomLoader()) return spec return None class CustomLoader(importlib.abc.Loader): def create_module(self, spec): # We use default module creation semantics by returning None return None def exec_module(self, module): # Read the module file and execute it with open(module.__spec__.origin, \'r\') as file: module_code = file.read() exec(module_code, module.__dict__) # Register the custom finder in sys.meta_path sys.meta_path.insert(0, CustomFinder()) # Example testing the custom import system if __name__ == \\"__main__\\": try: import my_module import subdir.submodule print(my_module.greet()) print(subdir.submodule.welcome()) except ImportError as e: print(f\\"Error importing module: {e}\\")"},{"question":"# Advanced Python Coding Assessment You are required to demonstrate your understanding of the `pipes` module to create, manipulate, and execute pipelines of shell commands. Despite the deprecation of this module, it provides valuable learning in handling Unix shell commands and file operations. **Task:** 1. Create a pipeline that performs the following sequence of actions on the contents of a file: - Convert all lowercase letters to uppercase. - Replace spaces with underscores. - Sort the lines alphabetically. 2. Implement the pipeline and copy the transformed contents of an input file to an output file. **Function Signature:** ```python def transform_and_copy(input_file: str, output_file: str) -> None: ``` **Input:** - `input_file` (str): Path to the input file. - `output_file` (str): Path to the output file. **Output:** - None. The function should create or overwrite the `output_file` with the transformed contents from the `input_file`. **Constraints:** - Assume the input file contains multiple lines of text. - Ensure you handle file operations correctly without causing any resource leaks. **Example:** Assume `input.txt` contains: ``` hello world python programming pipes module ``` After processing through your pipeline, the `output.txt` should contain: ``` HELLO_WORLD PIPES_MODULE PYTHON_PROGRAMMING ``` **Implementation Notes:** - Use the `pipes.Template` class for creating and manipulating the pipeline. - Utilize appropriate shell commands within the `append` or `prepend` methods. - Handle file opening and closing correctly using the `Template.open()` method. This task will assess your ability to: 1. Understand and implement a sequence of transformations using pipelines. 2. Work with file operations and handle resources effectively. 3. Utilize Unix shell commands within Python for text processing.","solution":"import pipes def transform_and_copy(input_file: str, output_file: str) -> None: t = pipes.Template() t.append(\'tr [a-z] [A-Z]\', \'--\') # Convert lowercase to uppercase t.append(\\"sed \'s/ /_/g\'\\", \'--\') # Replace spaces with underscores t.append(\'sort\', \'--\') # Sort the lines alphabetically with t.open(input_file, \'r\') as f_in, open(output_file, \'w\') as f_out: for line in f_in: f_out.write(line)"},{"question":"Objective Your task is to demonstrate your understanding of the seaborn library by creating customized color palettes using `seaborn.cubehelix_palette()` and applying them to a plot. Question Using the seaborn library, create a pair of subplots with customized color palettes applied to each. You are required to: 1. Create two differentiating color palettes using `sns.cubehelix_palette()` with the following specifications: - Palette 1: A discrete palette with 10 colors, starting at 3, with 1.5 rotations, gamma=0.7, increased saturation, and custom dark and light luminance values. - Palette 2: A continuous colormap with reverse luminance, starting at 0, with -2 rotations, and standard gamma and saturation values. 2. Apply Palette 1 to a box plot comparing the distribution of \'sepal_length\' among different \'species\' in the Iris dataset. 3. Apply Palette 2 to a violin plot comparing the distribution of \'sepal_width\' among different \'species\' in the Iris dataset. The provided Iris dataset is readily available in seaborn. Expected Input None (you will directly use seaborn\'s built-in Iris dataset). Expected Output A pair of subplots (one box plot and one violin plot) showcasing the specified custom palettes for each plot. Constraints - You are required to use seaborn\'s `cubehelix_palette()` to generate the custom color palettes. - Ensure the plots are well-labeled, and the color palettes are applied correctly. Performance Expectations - Efficiently generate and apply the palettes to the plots. - Maintain readability in the plot\'s elements and overall aesthetics. # Example Solution ```python import seaborn as sns import matplotlib.pyplot as plt # Load the Iris dataset iris = sns.load_dataset(\\"iris\\") # Create Palette 1: A discrete palette palette1 = sns.cubehelix_palette(10, start=3, rot=1.5, gamma=0.7, hue=1, dark=0.2, light=0.8, reverse=False) # Create Palette 2: A continuous colormap palette2 = sns.cubehelix_palette(start=0, rot=-2, gamma=1, hue=0.8, reverse=True, as_cmap=True) # Create subplots fig, axes = plt.subplots(1, 2, figsize=(14, 6)) # Plot 1: Box plot with Palette 1 sns.boxplot(x=\'species\', y=\'sepal_length\', data=iris, palette=palette1, ax=axes[0]) axes[0].set_title(\'Box Plot of Sepal Length by Species\') # Plot 2: Violin plot with Palette 2 sns.violinplot(x=\'species\', y=\'sepal_width\', data=iris, palette=palette2, ax=axes[1]) axes[1].set_title(\'Violin Plot of Sepal Width by Species\') # Display the plots plt.tight_layout() plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_palettes_and_plots(): This function creates and applies custom seaborn color palettes to a box plot and a violin plot with the Iris dataset. # Load the Iris dataset iris = sns.load_dataset(\\"iris\\") # Create Palette 1: A discrete palette palette1 = sns.cubehelix_palette(10, start=3, rot=1.5, gamma=0.7, dark=0.2, light=0.8, reverse=False) # Create Palette 2: A continuous colormap palette2 = sns.cubehelix_palette(start=0, rot=-2, gamma=1, light=0.7, dark=0.3, reverse=True, as_cmap=True) # Create subplots fig, axes = plt.subplots(1, 2, figsize=(14, 6)) # Plot 1: Box plot with Palette 1 sns.boxplot(x=\'species\', y=\'sepal_length\', data=iris, palette=palette1, ax=axes[0]) axes[0].set_title(\'Box Plot of Sepal Length by Species\') # Plot 2: Violin plot with Palette 2 sns.violinplot(x=\'species\', y=\'sepal_width\', data=iris, cmap=palette2, ax=axes[1]) axes[1].set_title(\'Violin Plot of Sepal Width by Species\') # Display the plots plt.tight_layout() plt.show() create_custom_palettes_and_plots()"},{"question":"Objective Implement a function that parses a provided text and replaces all control characters (ASCII values 0 to 31) and the delete character (ASCII value 127) by their caret notation representations as described in the module documentation. The replacements should follow the rules outlined for the `curses.ascii.unctrl` function. Input Format - A single string `text` containing various ASCII characters, possibly including control characters. Output Format - A string where all control characters and the delete character in `text` have been replaced by their caret notation representations. Constraints - The input `text` will have length between 1 and (10^5). - The input `text` will only contain ASCII characters (0 to 127). Example **Input:** ```python text = \\"Hellox00Worldx07!\\" ``` **Output:** ```python \\"Hello^@World^G!\\" ``` Implementation Requirements 1. Utilize the functions provided in the \\"curses.ascii\\" module where applicable. 2. Ensure that the transformation operates efficiently even for large input sizes. Function Signature ```python def replace_control_chars_with_caret_notation(text: str) -> str: pass ``` Solution Template ```python import curses.ascii def replace_control_chars_with_caret_notation(text: str) -> str: result = [] for char in text: if curses.ascii.isctrl(char) or ord(char) == 127: result.append(curses.ascii.unctrl(char)) else: result.append(char) return \'\'.join(result) # Example usage text = \\"Hellox00Worldx07!\\" print(replace_control_chars_with_caret_notation(text)) # should return \\"Hello^@World^G!\\" ```","solution":"import curses.ascii def replace_control_chars_with_caret_notation(text: str) -> str: result = [] for char in text: if curses.ascii.isctrl(char) or ord(char) == 127: result.append(curses.ascii.unctrl(char)) else: result.append(char) return \'\'.join(result) # Example usage text = \\"Hellox00Worldx07!\\" print(replace_control_chars_with_caret_notation(text)) # should return \\"Hello^@World^G!\\""},{"question":"**Coding Assessment Question:** Given a dataset consisting of labeled and unlabeled samples, implement a function using the `LabelSpreading` algorithm from scikit-learn\'s `semi_supervised` module. # Function Signature ```python def semi_supervised_label_spreading(X: np.ndarray, y: np.ndarray, kernel: str = \'knn\', n_neighbors: int = 7, gamma: float = 20.0) -> np.ndarray: pass ``` # Inputs - `X` (np.ndarray): A 2D numpy array of shape `(n_samples, n_features)` representing the feature matrix. - `y` (np.ndarray): A 1D numpy array of shape `(n_samples,)` representing the labels for each sample. Use `-1` for unlabeled samples. - `kernel` (str, optional): The kernel to use. Must be either `\'knn\'` or `\'rbf\'`. Default is `\'knn\'`. - `n_neighbors` (int, optional): Number of neighbors to use when `kernel=\'knn\'`. Default is `7`. - `gamma` (float, optional): Kernel coefficient for `rbf`. Default is `20.0`. # Output - Returns a 1D numpy array of shape `(n_samples,)` containing the predicted labels for each sample in `X`. # Constraints - `kernel` must be `\'knn\'` or `\'rbf\'`. - For unlabeled samples, the label should be `-1`. # Performance Requirements - The function should handle at least `10,000` samples efficiently. # Example ```python import numpy as np from sklearn.datasets import make_classification # Generate a dataset with 100 labeled and 200 unlabeled samples X, y = make_classification(n_samples=300, n_features=5, n_informative=3, n_classes=2) y[100:] = -1 # Set the last 200 samples as unlabeled predicted_labels = semi_supervised_label_spreading(X, y, kernel=\'rbf\', gamma=20.0) print(predicted_labels) ``` **Note:** - Provide appropriate error handling for invalid inputs. - Use the `LabelSpreading` class from `sklearn.semi_supervised`. ***Happy Coding!***","solution":"import numpy as np from sklearn.semi_supervised import LabelSpreading def semi_supervised_label_spreading(X: np.ndarray, y: np.ndarray, kernel: str = \'knn\', n_neighbors: int = 7, gamma: float = 20.0) -> np.ndarray: Applies the LabelSpreading algorithm to the given dataset to predict labels for unlabeled samples. Parameters: - X : np.ndarray A 2D numpy array of shape (n_samples, n_features) representing the feature matrix. - y : np.ndarray A 1D numpy array of shape (n_samples,) representing the labels for each sample. Use -1 for unlabeled samples. - kernel : str, optional The kernel to use. Must be either \'knn\' or \'rbf\'. Default is \'knn\'. - n_neighbors : int, optional Number of neighbors to use when kernel=\'knn\'. Default is 7. - gamma : float, optional Kernel coefficient for \'rbf\'. Default is 20.0. Returns: - np.ndarray A 1D numpy array of shape (n_samples,) containing the predicted labels for each sample in X. if kernel not in (\'knn\', \'rbf\'): raise ValueError(\\"kernel must be either \'knn\' or \'rbf\'\\") if kernel == \'knn\': model = LabelSpreading(kernel=kernel, n_neighbors=n_neighbors) else: # kernel == \'rbf\' model = LabelSpreading(kernel=kernel, gamma=gamma) model.fit(X, y) return model.transduction_"},{"question":"Objective: To assess students\' ability to create a minimal reproducible example and deal with classification problems using scikit-learn, including data preprocessing, model training, and validation. Problem Statement: You are provided with a dataset that mimics a simple binary classification problem. The dataset contains both numerical and categorical features. Your task is to demonstrate your understanding of scikit-learn by creating a minimal script that preprocesses the data, trains a logistic regression model, and evaluates the model\'s performance. Dataset: ```python import numpy as np import pandas as pd rng = np.random.RandomState(42) n_samples = 100 data = { \\"numeric_feature1\\": rng.randn(n_samples), \\"numeric_feature2\\": rng.uniform(low=0.0, high=100.0, size=n_samples), \\"categorical_feature\\": rng.choice([\\"cat\\", \\"dog\\", \\"mouse\\"], size=n_samples), \\"target\\": rng.randint(0, 2, size=n_samples) } df = pd.DataFrame(data) ``` Tasks: 1. **Data Splitting**: - Split the dataset into a training set (70%) and a test set (30%). 2. **Data Preprocessing**: - Encode the categorical feature into numerical values using OneHotEncoding. - Scale the numerical features using StandardScaler. 3. **Model Training**: - Train a logistic regression model using the preprocessed training set. 4. **Model Evaluation**: - Evaluate the model on the test set and report the accuracy. Implementation Details: - You should use the appropriate scikit-learn modules for preprocessing, model training, and evaluation. - Ensure your script is minimal yet complete and reproducible. - Provide comments in your code to explain each step. Expected Output: - The script should output the accuracy of the logistic regression model on the test set. ```python # Import necessary libraries import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # Dataset generation rng = np.random.RandomState(42) n_samples = 100 data = { \\"numeric_feature1\\": rng.randn(n_samples), \\"numeric_feature2\\": rng.uniform(low=0.0, high=100.0, size=n_samples), \\"categorical_feature\\": rng.choice([\\"cat\\", \\"dog\\", \\"mouse\\"], size=n_samples), \\"target\\": rng.randint(0, 2, size=n_samples) } df = pd.DataFrame(data) # Task 1: Split the dataset X = df.drop(\\"target\\", axis=1) y = df[\\"target\\"] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Task 2: Data Preprocessing # OneHotEncode categorical feature encoder = OneHotEncoder() categorical_encoded_train = encoder.fit_transform(X_train[[\\"categorical_feature\\"]]).toarray() categorical_encoded_test = encoder.transform(X_test[[\\"categorical_feature\\"]]).toarray() # Scale numerical features scaler = StandardScaler() numerical_scaled_train = scaler.fit_transform(X_train[[\\"numeric_feature1\\", \\"numeric_feature2\\"]]) numerical_scaled_test = scaler.transform(X_test[[\\"numeric_feature1\\", \\"numeric_feature2\\"]]) # Combine preprocessed features X_train_preprocessed = np.hstack((numerical_scaled_train, categorical_encoded_train)) X_test_preprocessed = np.hstack((numerical_scaled_test, categorical_encoded_test)) # Task 3: Train the logistic regression model model = LogisticRegression() model.fit(X_train_preprocessed, y_train) # Task 4: Evaluate the model y_pred = model.predict(X_test_preprocessed) accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy: {accuracy}\\") ``` Constraints: - Use Python 3.x and scikit-learn for your implementation. - Your script should be executable by simply running it in a Python interpreter, without requiring additional code outside the provided dataset generation snippet.","solution":"# Import necessary libraries import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score def preprocess_and_train(): # Dataset generation rng = np.random.RandomState(42) n_samples = 100 data = { \\"numeric_feature1\\": rng.randn(n_samples), \\"numeric_feature2\\": rng.uniform(low=0.0, high=100.0, size=n_samples), \\"categorical_feature\\": rng.choice([\\"cat\\", \\"dog\\", \\"mouse\\"], size=n_samples), \\"target\\": rng.randint(0, 2, size=n_samples) } df = pd.DataFrame(data) # Task 1: Split the dataset X = df.drop(\\"target\\", axis=1) y = df[\\"target\\"] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Task 2: Data Preprocessing # OneHotEncode categorical feature encoder = OneHotEncoder() categorical_encoded_train = encoder.fit_transform(X_train[[\\"categorical_feature\\"]]).toarray() categorical_encoded_test = encoder.transform(X_test[[\\"categorical_feature\\"]]).toarray() # Scale numerical features scaler = StandardScaler() numerical_scaled_train = scaler.fit_transform(X_train[[\\"numeric_feature1\\", \\"numeric_feature2\\"]]) numerical_scaled_test = scaler.transform(X_test[[\\"numeric_feature1\\", \\"numeric_feature2\\"]]) # Combine preprocessed features X_train_preprocessed = np.hstack((numerical_scaled_train, categorical_encoded_train)) X_test_preprocessed = np.hstack((numerical_scaled_test, categorical_encoded_test)) # Task 3: Train the logistic regression model model = LogisticRegression() model.fit(X_train_preprocessed, y_train) # Task 4: Evaluate the model y_pred = model.predict(X_test_preprocessed) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"**Objective**: Implement a function to perform PLSCanonical decomposition and use it to transform input data. Task: You are required to implement a class `MyPLSCanonical` from scratch without using `sklearn.cross_decomposition`. This class should mirror the functionality of `PLSCanonical` discussed in the provided documentation. Specifications: 1. **Method: fit** - **Input**: Two numpy arrays `X` and `Y` of shapes (n_samples, n_features_X) and (n_samples, n_features_Y) respectively, and an integer `n_components`. - **Operation**: Computes the weights, scores, and loadings for both `X` and `Y` following the steps outlined in the PLSCanonical description. - **Output**: None, but stores the weights, loadings, and scores as class attributes. 2. **Method: transform** - **Input**: A numpy array `X` of shape (n_samples, n_features_X) to be transformed using the fitted model. - **Operation**: Transforms `X` into the lower-dimensional subspace using the weights computed in the `fit` method. - **Output**: The transformed array. 3. **Method: predict** - **Input**: A numpy array `X` of shape (n_samples, n_features_X). - **Operation**: Predicts the target `Y` using the PLS model fitted to `X` and `Y`. - **Output**: A numpy array `Y_pred` of shape (n_samples, n_features_Y) representing the predicted targets. Example Usage: ```python import numpy as np X = np.random.rand(10, 3) Y = np.random.rand(10, 2) pls = MyPLSCanonical(n_components=2) pls.fit(X, Y) X_transformed = pls.transform(X) Y_pred = pls.predict(X) print(\\"Transformed X:n\\", X_transformed) print(\\"Predicted Y:n\\", Y_pred) ``` Constraints: - **n_samples**: (1 leq n leq 10^3) - **n_features_X, n_features_Y**: (1 leq d, t leq 10^2) - Do not use scikit-learn’s `PLSCanonical` directly. - Ensure the implementation is efficient and handles the dataset within a reasonable time frame (up to a few seconds for the largest allowed inputs). Hint: Follow the algorithm steps provided in the documentation for the `PLSCanonical` class to implement the methods correctly.","solution":"import numpy as np class MyPLSCanonical: def __init__(self, n_components): self.n_components = n_components self.x_weights_ = None self.y_weights_ = None self.x_scores_ = None self.y_scores_ = None self.x_loadings_ = None self.y_loadings_ = None def fit(self, X, Y): n = X.shape[0] # Initialize variables self.x_weights_ = np.zeros((X.shape[1], self.n_components)) self.y_weights_ = np.zeros((Y.shape[1], self.n_components)) self.x_scores_ = np.zeros((n, self.n_components)) self.y_scores_ = np.zeros((n, self.n_components)) self.x_loadings_ = np.zeros((X.shape[1], self.n_components)) self.y_loadings_ = np.zeros((Y.shape[1], self.n_components)) Xk = X.copy() Yk = Y.copy() for i in range(self.n_components): # Compute weight vectors w = np.dot(Xk.T, Yk[:, 0]) w /= np.linalg.norm(w) c = np.dot(Yk.T, Xk[:, 0]) c /= np.linalg.norm(c) # Compute score vectors t = np.dot(Xk, w) u = np.dot(Yk, c) t /= np.linalg.norm(t) u /= np.linalg.norm(u) # Compute loadings p = np.dot(Xk.T, t) / np.dot(t.T, t) q = np.dot(Yk.T, u) / np.dot(u.T, u) # Store the results self.x_weights_[:, i] = w self.y_weights_[:, i] = c self.x_scores_[:, i] = t self.y_scores_[:, i] = u self.x_loadings_[:, i] = p self.y_loadings_[:, i] = q # Deflate the matrices Xk -= np.outer(t, p) Yk -= np.outer(u, q) def transform(self, X): return np.dot(X, self.x_weights_) def predict(self, X): X_transformed = self.transform(X) return np.dot(X_transformed, self.y_loadings_.T)"},{"question":"<|Analysis Begin|> The provided documentation describes various functions related to the creation and manipulation of `memoryview` objects in Python. Memoryviews allow a Python object to expose its buffer interface and enable other objects to access this buffer directly without needing to copy the data. This can be crucial for performance in applications that require manipulating large amounts of data. Here are some of the key points from the documentation: 1. **Creating memoryview objects**: - `PyMemoryView_FromObject(obj)`: This creates a memoryview from an object that supports the buffer interface. - `PyMemoryView_FromMemory(mem, size, flags)`: This creates a memoryview from a block of memory. - `PyMemoryView_FromBuffer(view)`: This creates a memoryview from an existing buffer structure. 2. **Getting contiguous memoryview objects**: - `PyMemoryView_GetContiguous(obj, buffertype, order)`: This creates a memoryview to a contiguous chunk of memory, with options for \'C\' or \'Fortran\' memory order. 3. **Checking and accessing memoryview objects**: - `PyMemoryView_Check(obj)`: Checks if the given object is a memoryview. - `PyMemoryView_GET_BUFFER(mview)`: Returns the private copy of the buffer. - `PyMemoryView_GET_BASE(mview)`: Returns the object that the memoryview is based on, or NULL if it was created using memory or buffer. Considering these functionalities, a challenging question can involve the creation and manipulation of memoryview objects, ensuring the student demonstrates a deep understanding of memory handling and performance considerations when working with large datasets. <|Analysis End|> <|Question Begin|> # Advanced Python Programming: Memoryview Manipulation Objective To assess your understanding of the `memoryview` object in Python, you will create and manipulate memoryview objects. This exercise involves careful management of buffer interfaces and data exposures. Problem Statement You are given a large list of integers that represent pixel values of a monochrome image stored row-wise. Your tasks are: 1. Create a `memoryview` of this list. 2. Write a function `get_subregion` that extracts a sub-region (a rectangle) of the image given the top-left and bottom-right coordinates. 3. Write a function `flip_vertical` that flips the entire image vertically using memoryview for efficiency. 4. Write a function `apply_threshold` that applies a threshold to the image, setting all pixels below a certain value to 0 and all pixels above or equal to that value to 255. Requirements 1. **Function Signature and Details**: ```python def get_subregion(image: list, top_left: tuple, bottom_right: tuple, width: int) -> list: Extracts the sub-region of the image defined by the top-left and bottom-right coordinates. :param image: List of integers representing pixel values. :param top_left: Tuple of (row, column) for the top-left corner. :param bottom_right: Tuple of (row, column) for the bottom-right corner. :param width: Width of the image. :return: A list of integers representing the sub-region. pass def flip_vertical(image: list, width: int, height: int) -> None: Flips the image vertically in place. :param image: List of integers representing pixel values. :param width: Width of the image. :param height: Height of the image. pass def apply_threshold(image: list, threshold: int) -> None: Applies the threshold to the image in place. :param image: List of integers representing pixel values. :param threshold: Threshold value. pass ``` 2. **Constraints**: - The image is represented as a flat list of integers where each integer is a pixel value. - The functions should use `memoryview` for manipulating the image data to ensure efficient operations. - Assume the image is always a rectangle with the specified width. - Coordinate values provided to `get_subregion` are always within the image boundaries. 3. **Performance**: - The implementation should handle large images efficiently, making optimal use of memory views to prevent unnecessary data copying. Example ```python # Given a 5x5 image represented linearly image = [ 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250 ] # Extract a 2x2 sub-region from the top-left corner (0,0) to (1,1) subregion = get_subregion(image, (0, 0), (1, 1), 5) print(subregion) # Output: [10, 20, 60, 70] # Flip the image vertically flip_vertical(image, 5, 5) print(image) # Output after vertical flip # Apply a threshold of 130 apply_threshold(image, 130) print(image) # Output with applied threshold (0s and 255s) ``` Use the provided function signatures to implement your solution.","solution":"def get_subregion(image: list, top_left: tuple, bottom_right: tuple, width: int) -> list: Extract the sub-region of the image defined by the top-left and bottom-right coordinates. :param image: List of integers representing pixel values. :param top_left: Tuple of (row, column) for the top-left corner. :param bottom_right: Tuple of (row, column) for the bottom-right corner. :param width: Width of the image. :return: A list of integers representing the sub-region. top_row, left_col = top_left bottom_row, right_col = bottom_right # Create a memoryview of the image mv = memoryview(bytearray(image)) subregion = [] for row in range(top_row, bottom_row + 1): start = row * width + left_col end = row * width + right_col + 1 subregion.extend(mv[start:end]) return list(subregion) def flip_vertical(image: list, width: int, height: int) -> None: Flips the image vertically in place. :param image: List of integers representing pixel values. :param width: Width of the image. :param height: Height of the image. mv = memoryview(bytearray(image)) for row in range(height // 2): start_top = row * width start_bottom = (height - row - 1) * width for col in range(width): # Swap top and bottom rows mv[start_top + col], mv[start_bottom + col] = mv[start_bottom + col], mv[start_top + col] # Update the image list with modified values image[:] = list(mv) def apply_threshold(image: list, threshold: int) -> None: Applies the threshold to the image in place. :param image: List of integers representing pixel values. :param threshold: Threshold value. mv = memoryview(bytearray(image)) for i in range(len(mv)): mv[i] = 255 if mv[i] >= threshold else 0 # Update the image list with modified values image[:] = list(mv)"},{"question":"**Objective:** Write a function that performs the following tasks: 1. Computes the Fast Fourier Transform (FFT) of a given 1D real-valued signal. 2. Filters out frequencies outside a specified range. 3. Reconstructs the signal using the inverse FFT (IFFT). # Function Signature ```python import torch def filter_signal(signal: torch.Tensor, min_freq: float, max_freq: float, sampling_rate: float) -> torch.Tensor: Filter out frequencies outside the specified range from the given signal. Parameters: - signal (torch.Tensor): A 1D tensor representing the input signal. - min_freq (float): The minimum frequency to be retained. - max_freq (float): The maximum frequency to be retained. - sampling_rate (float): Sampling rate of the input signal. Returns: - torch.Tensor: The filtered signal reconstructed from the inverse FFT. pass ``` # Input and Output Formats - **Input:** - `signal`: A 1D tensor (torch.Tensor) of shape `(n,)` representing the real-valued signal to be processed. - `min_freq`: A float representing the minimum frequency to be retained. - `max_freq`: A float representing the maximum frequency to be retained. - `sampling_rate`: A float representing the sampling rate of the signal in Hz. - **Output:** - Returns a 1D tensor (torch.Tensor) of shape `(n,)` representing the filtered and reconstructed signal. # Constraints - The `signal` tensor will have at most `10000` elements. - The `min_freq` and `max_freq` will be non-negative and `min_freq` ≤ `max_freq`. - The `sampling_rate` will be a positive float, typically in the range `(10, 10000)` Hz. # Performance Requirements - The function should efficiently handle signals up to the maximum size constraint using FFT operations. # Example ```python import torch # Example signal: a 1D tensor with 100 elements signal = torch.randn(100, dtype=torch.float32) # Define frequency range to retain (e.g., 1Hz to 50Hz) min_freq = 1.0 max_freq = 50.0 sampling_rate = 100.0 # Call the function filtered_signal = filter_signal(signal, min_freq, max_freq, sampling_rate) # Output the filtered signal (for illustration purposes) print(filtered_signal) ``` # Notes - Use the `torch.fft` module for computing FFT and IFFT. - The frequencies can be obtained using the `fftfreq` helper function. - While filtering, ensure to handle both the positive and negative frequency components appropriately.","solution":"import torch def filter_signal(signal: torch.Tensor, min_freq: float, max_freq: float, sampling_rate: float) -> torch.Tensor: Filter out frequencies outside the specified range from the given signal. Parameters: - signal (torch.Tensor): A 1D tensor representing the input signal. - min_freq (float): The minimum frequency to be retained. - max_freq (float): The maximum frequency to be retained. - sampling_rate (float): Sampling rate of the input signal. Returns: - torch.Tensor: The filtered signal reconstructed from the inverse FFT. # Compute the FFT of the signal n = signal.shape[0] fft_vals = torch.fft.fft(signal) # Get the corresponding frequencies using fftfreq freqs = torch.fft.fftfreq(n, 1 / sampling_rate) # Create a mask to filter out frequencies outside the specified range mask = (torch.abs(freqs) >= min_freq) & (torch.abs(freqs) <= max_freq) # Apply the mask to zero out unwanted frequencies in the FFT results filtered_fft_vals = fft_vals * mask # Compute the inverse FFT to get the filtered signal filtered_signal = torch.fft.ifft(filtered_fft_vals) # Return the real part of the filtered signal return filtered_signal.real"},{"question":"# Asyncio Coding Assessment You are required to implement a function using Python\'s asyncio module which demonstrates the creation and management of multiple asynchronous tasks. The goal is to fetch data concurrently from multiple URLs, process this data, and handle any network timeouts gracefully. Function Specification ```python import asyncio async def fetch_and_process(urls: List[str], timeout: int) -> List[str]: Function to concurrently fetch data from a list of URLs and process the data. Args: urls (List[str]): List of URLs to fetch data from. timeout (int): Timeout in seconds for each fetch operation. Returns: List[str]: List of processed data from each URL. ``` Requirements 1. **Fetching Data**: For each URL in the input list, create an asynchronous task to fetch data. Simulate data fetching by waiting for a random time between 1 to 5 seconds (you can use `asyncio.sleep` to simulate this delay). 2. **Processing Data**: Once data fetching for a URL is complete, process it by appending the prefix \\"Processed: \\" to the data content. 3. **Handling Timeouts**: If fetching data from a URL takes longer than the specified timeout, raise and handle an `asyncio.TimeoutError`. For such cases, the processed data should be \\"Timeout Error\\". 4. **Concurrency**: Ensure that all URLs are fetched concurrently. 5. **Results Collection**: Collect the results (processed data) for each URL and return it as a list. Input and Output - **Input**: - `urls`: A list of strings representing the URLs to fetch data from. - `timeout`: An integer representing the maximum time to wait for fetching data from a single URL. - **Output**: A list of strings where each string is the processed data corresponding to the input URLs. Example ```python import asyncio # Example uses of the function urls = [\\"http://example.com/1\\", \\"http://example.com/2\\", \\"http://example.com/3\\"] timeout = 3 results = asyncio.run(fetch_and_process(urls, timeout)) print(results) # Output could be: [\'Processed: data from URL 1\', \'Timeout Error\', \'Processed: data from URL 3\'] ``` **Constraints**: - The number of URLs will not exceed 100. - Timeout range lies between 1 and 10 seconds. Implement the `fetch_and_process` function to meet the above requirements using suitable asyncio primitives and handling mechanisms.","solution":"import asyncio import random from typing import List async def fetch_data(url: str) -> str: Simulates fetching data from a URL. fetch_time = random.uniform(1, 5) # Simulate a delay between 1 to 5 seconds await asyncio.sleep(fetch_time) return f\\"data from {url}\\" async def process_data(data: str) -> str: Processes the fetched data. return f\\"Processed: {data}\\" async def fetch_and_process_url(url: str, timeout: int) -> str: Fetches and processes data from a single URL with timeout handling. try: raw_data = await asyncio.wait_for(fetch_data(url), timeout) return await process_data(raw_data) except asyncio.TimeoutError: return \\"Timeout Error\\" async def fetch_and_process(urls: List[str], timeout: int) -> List[str]: Function to concurrently fetch data from a list of URLs and process the data. Args: urls (List[str]): List of URLs to fetch data from. timeout (int): Timeout in seconds for each fetch operation. Returns: List[str]: List of processed data from each URL. tasks = [fetch_and_process_url(url, timeout) for url in urls] return await asyncio.gather(*tasks)"},{"question":"# Asynchronous File Processing with Platform Constraints **Objective:** You are required to implement an asynchronous file processing system that must take into consideration the platform-specific limitations of the asyncio module as provided in the documentation. The system you design should be capable of reading multiple text files concurrently, processing their contents, and writing the results to output files. The solution should work effectively on both Windows and macOS. **Task Description:** 1. Define an asynchronous function `process_file(input_path, output_path)` that: - Reads the content of the file specified by `input_path`. - Processes the content by reversing the lines (each line should be reversed individually, not the entire content). - Writes the processed content to the file specified by `output_path`. 2. Define another asynchronous function `main(input_dir, output_dir, max_concurrent_tasks)` that: - Takes the paths to the input directory (`input_dir`) and output directory (`output_dir`), along with `max_concurrent_tasks` which limits the number of concurrent file processing tasks. - Finds all text files (`.txt`) in the input directory. - Initiates asynchronous tasks to process each file using `process_file`. - Ensures no more than `max_concurrent_tasks` are running simultaneously. 3. On Windows, ensure that the program uses the `ProactorEventLoop`. On macOS, use the default event loop. **Constraints:** - The solution must handle platform-specific differences as mentioned in the documentation. - You may not use `loop.create_unix_connection()`, `loop.create_unix_server()`, `loop.add_signal_handler()`, or `loop.remove_signal_handler()` on Windows. - Use appropriate event loop setup commands to ensure compatibility with both Windows and macOS. **Input:** - `input_dir` (str): Path to the directory containing input text files. - `output_dir` (str): Path to the directory for writing processed output files. - `max_concurrent_tasks` (int): Maximum number of concurrent file processing tasks. **Output:** - Processed files written to the output directory with the same name as the input files. # Example: If you have the following files in the `input_dir`: ``` input_dir/file1.txt: ``` ``` Hello World input_dir/file2.txt: ``` ``` Make It Work ``` After processing, the following files should be created in the `output_dir`: ``` output_dir/file1.txt: ``` ``` olleH dlroW output_dir/file2.txt: ``` ``` ekaM tI kroW ``` **Implementation Skeleton:** ```python import os import asyncio async def process_file(input_path, output_path): # Your code here async def main(input_dir, output_dir, max_concurrent_tasks): # Your code here if __name__ == \\"__main__\\": import platform if platform.system() == \\"Windows\\": asyncio.set_event_loop(asyncio.ProactorEventLoop()) else: asyncio.set_event_loop(asyncio.SelectorEventLoop()) input_dir = \\"path_to_input_dir\\" output_dir = \\"path_to_output_dir\\" max_concurrent_tasks = 5 asyncio.run(main(input_dir, output_dir, max_concurrent_tasks)) ``` # Notes: - Ensure to handle file I/O and asyncio event loops carefully. - Test the solution on both Windows and macOS to ensure its correctness across platforms.","solution":"import os import asyncio import platform async def process_file(input_path, output_path): Reads the content of the input file, processes the content by reversing each line, and writes the processed content to the output file. try: async with aiofiles.open(input_path, \'r\') as f: lines = await f.readlines() # Reverse each line processed_lines = [line[::-1] for line in lines] async with aiofiles.open(output_path, \'w\') as f: await f.writelines(processed_lines) except Exception as e: print(f\\"Error processing file {input_path}: {e}\\") async def main(input_dir, output_dir, max_concurrent_tasks): Manages the asynchronous processing of files. Limits the number of concurrent tasks to a specified maximum. os.makedirs(output_dir, exist_ok=True) tasks = [] sem = asyncio.Semaphore(max_concurrent_tasks) for file_name in os.listdir(input_dir): if file_name.endswith(\\".txt\\"): input_path = os.path.join(input_dir, file_name) output_path = os.path.join(output_dir, file_name) tasks.append(process_file_with_semaphore(input_path, output_path, sem)) await asyncio.gather(*tasks) async def process_file_with_semaphore(input_path, output_path, semaphore): Wrapper function to ensure processing file tasks adhere to semaphore limits. async with semaphore: await process_file(input_path, output_path) if __name__ == \\"__main__\\": import aiofiles input_dir = \\"path_to_input_dir\\" output_dir = \\"path_to_output_dir\\" max_concurrent_tasks = 5 if platform.system() == \\"Windows\\": asyncio.set_event_loop(asyncio.ProactorEventLoop()) asyncio.run(main(input_dir, output_dir, max_concurrent_tasks))"},{"question":"**Question**: You are given the task to write a Python program that performs basic terminal control by switching the terminal file descriptor to different modes (raw and cbreak). # Problem Statement Implement two functions using the `tty` module: 1. `enable_raw_mode(fd)`: This function should change the mode of the file descriptor `fd` to raw using `tty.setraw()`. 2. `enable_cbreak_mode(fd)`: This function should change the mode of the file descriptor `fd` to cbreak using `tty.setcbreak()`. Additionally, write a third function, `main()`, which does the following: - Opens the terminal (`/dev/tty`) for reading and writing. - Switches the terminal to raw mode, prints \\"Raw mode enabled\\" to the terminal, waits for 3 seconds, and then switches the terminal to cbreak mode. - Prints \\"Cbreak mode enabled\\" to the terminal and waits for another 3 seconds. - Finally, restores the terminal to its original mode and closes the file descriptor. # Requirements - Utilize the `tty` and `termios` modules. - Ensure that the terminal is properly restored to its original state even if an error occurs (use exception handling). # Input and Output - The file descriptor `fd` is an integer representing the terminal\'s file descriptor. - The output should be printed directly to the terminal. # Constraints - You can assume that the script is running on a Unix-based system where `/dev/tty` is available. - You need to handle any exceptions and ensure proper resource cleanup. # Example To illustrate, your program should: 1. Enable raw mode and print \\"Raw mode enabled\\". 2. After 3 seconds, switch to cbreak mode and print \\"Cbreak mode enabled\\". 3. Ensure that the mode is reverted back to the original on completion or in case of an exception. ```python import tty import termios import time import os def enable_raw_mode(fd): # Your code here pass def enable_cbreak_mode(fd): # Your code here pass def main(): # Your code here pass if __name__ == \\"__main__\\": main() ``` Ensure you test your script in a Unix-based terminal environment.","solution":"import tty import termios import time import os def enable_raw_mode(fd): Change the terminal mode of the file descriptor \'fd\' to raw. tty.setraw(fd) def enable_cbreak_mode(fd): Change the terminal mode of the file descriptor \'fd\' to cbreak. tty.setcbreak(fd) def main(): Opens the terminal (\'/dev/tty\') for reading and writing, enables raw mode, waits for 3 seconds, then enables cbreak mode, waits another 3 seconds, and finally restores the original terminal state. try: fd = os.open(\'/dev/tty\', os.O_RDWR) original_termios = termios.tcgetattr(fd) enable_raw_mode(fd) print(\\"Raw mode enabled\\") time.sleep(3) enable_cbreak_mode(fd) print(\\"Cbreak mode enabled\\") time.sleep(3) except Exception as e: print(f\\"An error occurred: {e}\\") finally: if \'original_termios\' in locals(): termios.tcsetattr(fd, termios.TCSANOW, original_termios) print(\\"Terminal state restored\\") os.close(fd) if __name__ == \\"__main__\\": main()"},{"question":"# Pandas Visualization and Customization In this coding assessment, you will demonstrate your proficiency with the pandas plotting library by implementing a function to process and visualize a dataset. Specifically, you will plot a multi-line graph with additional customization elements. Task You are provided with a CSV file named `data.csv` containing financial time series data for several companies. Your task is to: 1. Load the data into a pandas DataFrame. 2. Calculate the cumulative return for each company. 3. Plot the cumulative returns over time. 4. Customize the plot with the following requirements: - Set the plot title to \\"Cumulative Returns of Companies\\". - Label the x-axis as \\"Date\\". - Label the y-axis as \\"Cumulative Return\\". - Add a legend indicating the company names. - Set a logarithmic scale for the y-axis. - Use a colormap to distinguish between different companies. Function Signature ```python def visualize_cumulative_returns(file_path: str) -> None: pass ``` Input - `file_path`: A string representing the path to the CSV file. Output - The function does not return anything. It should display the plot using matplotlib. Implementation Constraints - You must use pandas for data manipulation and matplotlib (through pandas plotting) for visualization. - The CSV file will have the following format: | Date | CompanyA | CompanyB | CompanyC | ... | |------------|----------|----------|----------|-----| | 2020-01-01 | 100 | 200 | 300 | ... | | 2020-01-02 | 101 | 198 | 305 | ... | | ... | ... | ... | ... | ... | Example Using the file `data.csv`: | Date | CompanyA | CompanyB | CompanyC | |------------|----------|----------|----------| | 2020-01-01 | 100 | 200 | 300 | | 2020-01-02 | 101 | 198 | 305 | | 2020-01-03 | 105 | 204 | 310 | Your function should generate a plot showing the cumulative returns for CompanyA, CompanyB, and CompanyC over the time period. ```python import pandas as pd import matplotlib.pyplot as plt def visualize_cumulative_returns(file_path: str) -> None: # Load the CSV data into a pandas DataFrame data = pd.read_csv(file_path, parse_dates=[\'Date\'], index_col=\'Date\') # Calculate the cumulative returns cumulative_returns = (data / data.iloc[0]) - 1 # Plot the cumulative returns ax = cumulative_returns.plot(title=\'Cumulative Returns of Companies\', colormap=\'viridis\', logy=True) ax.set_xlabel(\'Date\') ax.set_ylabel(\'Cumulative Return\') plt.legend(title=\'Company\') plt.show() # Example usage: # visualize_cumulative_returns(\'data.csv\') ```","solution":"import pandas as pd import matplotlib.pyplot as plt def visualize_cumulative_returns(file_path: str) -> None: # Load the CSV data into a pandas DataFrame data = pd.read_csv(file_path, parse_dates=[\'Date\'], index_col=\'Date\') # Calculate the cumulative returns cumulative_returns = (data / data.iloc[0]) - 1 # Plot the cumulative returns ax = cumulative_returns.plot(title=\'Cumulative Returns of Companies\', colormap=\'viridis\', logy=True) ax.set_xlabel(\'Date\') ax.set_ylabel(\'Cumulative Return\') plt.legend(title=\'Company\') plt.show()"},{"question":"<|Analysis Begin|> The documentation provided covers the `http` package in Python, specifically the `http.HTTPStatus` class. This module is designed to handle various HTTP status codes, including their enum names, phrases, and detailed descriptions. The documentation provides a list of all supported status codes along with their RFC references, descriptions, and usage examples. Key points to focus on: 1. The `http.HTTPStatus` class is a subclass of `enum.IntEnum` and contains various HTTP status codes. 2. Each HTTP status code includes: - `value`: the numeric HTTP status code (e.g., 200, 404). - `phrase`: the short description related to the code (e.g., \'OK\', \'Not Found\'). - `description`: a longer, more detailed description of the status code. 3. The `http.HTTPStatus` module provides easy-to-use enumerations for each status code. A good question would involve working with these HTTP status codes and could test the student\'s ability to retrieve and manipulate enum values. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** To assess your understanding of the `http.HTTPStatus` package in Python and your ability to manipulate and use HTTP status codes. **Problem Statement:** You are required to implement a function `get_http_status_info` that takes an integer as its input, corresponding to the HTTP status code. The function should return a dictionary containing the following keys: - `status_code`: The integer value of the status code. - `phrase`: The short description phrase of the status code. - `description`: The long description of the status code. If the given status code is not defined in the `http.HTTPStatus` class, the function should raise a `ValueError` with the message \\"Invalid HTTP status code\\". **Function Signature:** ```python def get_http_status_info(status_code: int) -> dict: pass ``` # Example ```python from http import HTTPStatus # Example 1 result = get_http_status_info(200) print(result) # Expected Output: # { # \\"status_code\\": 200, # \\"phrase\\": \\"OK\\", # \\"description\\": \\"Request fulfilled, document follows\\" # } # Example 2 result = get_http_status_info(404) print(result) # Expected Output: # { # \\"status_code\\": 404, # \\"phrase\\": \\"Not Found\\", # \\"description\\": \\"Not Found\\" # } # Example 3: Invalid Status Code try: result = get_http_status_info(999) except ValueError as e: print(e) # Expected Output: \\"Invalid HTTP status code\\" ``` **Constraints:** 1. You must use the `http.HTTPStatus` enum provided by the Python `http` package. 2. Your implementation should handle both valid and invalid status codes. 3. You may assume that the numeric value provided will always be an integer. **Hints:** - Refer to the `http.HTTPStatus` documentation for the enum values and methods. - Use exception handling to manage invalid status codes. - Utilize Python\'s `hasattr` or similar methods to check if a status code exists within the `http.HTTPStatus` enumeration. **Performance Requirements:** - The function should have a time complexity of O(1) for valid status codes. - The function should handle up to 1,000,000 queries efficiently if run in a loop, ensuring appropriate use of enum attributes for quick lookup.","solution":"from http import HTTPStatus def get_http_status_info(status_code: int) -> dict: Given an HTTP status code, return a dictionary containing the status code, its phrase, and its description. try: status = HTTPStatus(status_code) return { \'status_code\': status.value, \'phrase\': status.phrase, \'description\': status.description } except ValueError: raise ValueError(\\"Invalid HTTP status code\\")"},{"question":"Implementing and Evaluating K-means Clustering Objective You will write code to perform K-means clustering on a dataset using scikit-learn and evaluate the model. Problem Statement Implement the `perform_kmeans_clustering` function, which performs K-means clustering on the provided dataset and computes the sum of squared distances of samples to their closest cluster center (also known as inertia). Function Signature ```python def perform_kmeans_clustering(data: np.ndarray, n_clusters: int) -> (np.ndarray, float): Perform K-means clustering on the provided dataset. Parameters: data (np.ndarray): A 2D numpy array of shape (n_samples, n_features) representing the dataset. n_clusters (int): An integer representing the number of clusters for the K-means algorithm. Returns: tuple: A tuple containing: - cluster_centers (np.ndarray): A 2D array of shape (n_clusters, n_features) representing coordinates of cluster centers. - inertia (float): Sum of squared distances of samples to their closest cluster center. ``` Input - `data`: A 2D numpy array of shape (n_samples, n_features) representing the dataset. - `n_clusters`: An integer representing the number of clusters for the K-means algorithm. Output - A tuple containing: - `cluster_centers`: A 2D array of shape (n_clusters, n_features) representing coordinates of cluster centers. - `inertia`: A float representing the sum of squared distances of samples to their closest cluster center. Constraints - The function should handle cases where `data` has at least one sample and one feature. - The number of clusters `n_clusters` must be a positive integer and less than or equal to the number of samples. Example ```python import numpy as np data = np.array([[1.0, 2.0], [1.5, 1.8], [5.0, 8.0], [8.0, 8.0], [1.0, 0.6], [9.0, 11.0]]) n_clusters = 2 cluster_centers, inertia = perform_kmeans_clustering(data, n_clusters) print(\\"Cluster Centers:n\\", cluster_centers) print(\\"Inertia:\\", inertia) ``` Performance Requirements - Ensure the function runs efficiently for datasets with up to 10,000 samples and 20 features. Additional Information - You may use the `KMeans` class from `sklearn.cluster` for clustering. - Refer to the scikit-learn documentation for the `fit` and `inertia_` methods associated with the `KMeans` class.","solution":"from sklearn.cluster import KMeans import numpy as np def perform_kmeans_clustering(data: np.ndarray, n_clusters: int) -> (np.ndarray, float): Perform K-means clustering on the provided dataset. Parameters: data (np.ndarray): A 2D numpy array of shape (n_samples, n_features) representing the dataset. n_clusters (int): An integer representing the number of clusters for the K-means algorithm. Returns: tuple: A tuple containing: - cluster_centers (np.ndarray): A 2D array of shape (n_clusters, n_features) representing coordinates of cluster centers. - inertia (float): Sum of squared distances of samples to their closest cluster center. kmeans = KMeans(n_clusters=n_clusters, random_state=42) kmeans.fit(data) cluster_centers = kmeans.cluster_centers_ inertia = kmeans.inertia_ return cluster_centers, inertia"},{"question":"**Objective**: Demonstrate your understanding of Seaborn\'s `objects` module by creating multiple plots with customized visualizations. **Dataset**: Use the `mpg` dataset that can be loaded via Seaborn\'s `load_dataset` function. **Task**: 1. Load the `mpg` dataset using Seaborn. 2. Create a pairwise plot of the variables `horsepower`, `acceleration`, and `weight` on the `x` axis, against `mpg` on the `y` axis. Each pair of variables should be presented in separate subplots in a grid layout with 2 columns. 3. Customize the plot by adding `Dots` to visualize the data points. 4. Add appropriate axis labels: - `horsepower` as \\"Horsepower (hp)\\" - `acceleration` as \\"Acceleration (0-60 mph)\\" - `weight` as \\"Weight (lbs)\\" - `mpg` as \\"Miles per Gallon (mpg)\\" 5. Further, facet the plots by the `origin` of the cars to show different origins in separate columns. **Constraints**: - Use only the methods and classes demonstrated in the provided documentation (`so.Plot`, `.pair`, `.add`, `.label`, `.facet`, etc.). **Input**: None (the code should work in a Jupyter notebook as part of a script). **Output**: - Display a single figure with multiple subplots as specified above. ```python # Code Solution Template import seaborn.objects as so from seaborn import load_dataset # Load the mpg dataset mpg = load_dataset(\\"mpg\\") # Create the pairwise plot with desired customizations and faceting ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"horsepower\\", \\"acceleration\\", \\"weight\\"], wrap=2) .label(x0=\\"Horsepower (hp)\\", x1=\\"Acceleration (0-60 mph)\\", x2=\\"Weight (lbs)\\", y=\\"Miles per Gallon (mpg)\\") .facet(col=\\"origin\\") .add(so.Dots()) ) ``` Ensure your code meets the outlined requirements and properly renders the resulting visualization.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the mpg dataset mpg = load_dataset(\\"mpg\\") # Create the pairwise plot with desired customizations and faceting ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"horsepower\\", \\"acceleration\\", \\"weight\\"], wrap=2) .label(x0=\\"Horsepower (hp)\\", x1=\\"Acceleration (0-60 mph)\\", x2=\\"Weight (lbs)\\", y=\\"Miles per Gallon (mpg)\\") .facet(col=\\"origin\\") .add(so.Dots()) )"},{"question":"# Coding Assessment: Implementing Out-of-Core Learning **Objective:** Your task is to design and implement an out-of-core learning system using scikit-learn that can handle large datasets which do not fit into memory. You will create a basic text classification model that uses incremental learning with the `SGDClassifier`. # Problem Statement: You are given a text dataset that is too large to fit into memory. Your goal is to: 1. Stream the text data from files in chunks. 2. Extract features using the `HashingVectorizer` for each chunk. 3. Incrementally train a linear classifier using `SGDClassifier` on the processed chunks. # Instructions: 1. **Streaming Data:** - Implement a generator function `stream_data(file_path, chunk_size)` that reads data from a file and yields it in chunks. Each line in the file represents a text document and its class label separated by a comma (e.g., \\"label,text\\"). - The function should yield tuples of (chunk_texts, chunk_labels). 2. **Feature Extraction:** - Use `HashingVectorizer` from `sklearn.feature_extraction.text` to convert text documents into feature vectors. 3. **Incremental Learning:** - Use `SGDClassifier` from `sklearn.linear_model` with `partial_fit` to incrementally train the classifier. - Assume the possible classes are given and pass them to the first `partial_fit` call. 4. **Evaluation:** - After training, evaluate your model on a separate test set. Calculate and print the accuracy of the model. # Input: - `train_file_path`: A string representing the file path to the training data. Each line in the file is in the format \\"label,text\\". - `test_file_path`: A string representing the file path to the test data. Each line in the file is in the format \\"label,text\\". - `chunk_size`: An integer representing the size of each data chunk to be processed at a time. # Constraints: - Assume the dataset is too large to fit into memory. # Expected Functions: ```python def stream_data(file_path: str, chunk_size: int): Generator to stream data from a given file in chunks. Args: - file_path (str): Path to the data file. - chunk_size (int): Number of lines to read at once. Yields: - Tuple[List[str], List[str]]: A tuple containing a list of texts and a list of labels. pass def train_incremental_model(train_file_path: str, test_file_path: str, chunk_size: int, classes: List[str]): Function to train an incremental learning model using streamed data and evaluate it. Args: - train_file_path (str): Path to the training data file. - test_file_path (str): Path to the test data file. - chunk_size (int): Number of lines to read at once for training. - classes (List[str]): List of possible class labels. pass ``` # Output: - Print the accuracy of the model on the test set. # Example: ```python train_incremental_model(\'train_data.csv\', \'test_data.csv\', 500, [\'class1\', \'class2\']) ``` # Notes: - Make sure to handle text preprocessing steps as necessary. - You may use any additional helper functions if required. - Ensure your code is well-documented and follows best practices.","solution":"from typing import List, Tuple from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def stream_data(file_path: str, chunk_size: int) -> Tuple[List[str], List[str]]: Generator to stream data from a given file in chunks. Args: - file_path (str): Path to the data file. - chunk_size (int): Number of lines to read at once. Yields: - Tuple[List[str], List[str]]: A tuple containing a list of texts and a list of labels. texts, labels = [], [] with open(file_path, \'r\') as f: for line in f: label, text = line.strip().split(\',\', 1) texts.append(text) labels.append(label) if len(texts) == chunk_size: yield texts, labels texts, labels = [], [] if texts: yield texts, labels def train_incremental_model(train_file_path: str, test_file_path: str, chunk_size: int, classes: List[str]): Function to train an incremental learning model using streamed data and evaluate it. Args: - train_file_path (str): Path to the training data file. - test_file_path (str): Path to the test data file. - chunk_size (int): Number of lines to read at once for training. - classes (List[str]): List of possible class labels. vectorizer = HashingVectorizer() model = SGDClassifier() for texts, labels in stream_data(train_file_path, chunk_size): X = vectorizer.transform(texts) if hasattr(model, \'classes_\'): # Partial fit with known classes model.partial_fit(X, labels) else: model.partial_fit(X, labels, classes=classes) test_texts, test_labels = [], [] for texts, labels in stream_data(test_file_path, chunk_size): test_texts.extend(texts) test_labels.extend(labels) X_test = vectorizer.transform(test_texts) predictions = model.predict(X_test) accuracy = accuracy_score(test_labels, predictions) print(f\\"Model accuracy: {accuracy}\\") def test_function(): return \\"Function is working correctly.\\""},{"question":"**Objective:** Create a Python program that processes a text file and a binary file, performs various operations using the `io` module, and handles different data conversions and encodings. **Problem Statement:** You are required to implement two functions, `process_text_file` and `process_binary_file`, using the `io` module. 1. **`process_text_file(input_path: str, output_path: str) -> None`**: - Read a UTF-8 encoded text file from the path `input_path`. - Convert all text to uppercase. - Write the modified text to another file at the path `output_path`. - Ensure the output file is also UTF-8 encoded. 2. **`process_binary_file(input_data: bytes) -> bytes`**: - Take a bytes object `input_data`. - Read data from the bytes object using an in-memory binary stream. - Reverse the contents of the binary data. - Return the reversed data as a bytes object. Constraints: - The text and binary files can be of any reasonable size that fits in memory. - Assume the input text file does not contain any null bytes. - You must use classes and methods from the `io` module to handle the reading, writing, and in-memory operations. # Example Usage: ```python import io def process_text_file(input_path: str, output_path: str) -> None: # Your implementation here pass def process_binary_file(input_data: bytes) -> bytes: # Your implementation here pass # Sample text file processing process_text_file(\'input.txt\', \'output.txt\') # Sample binary data processing binary_data = b\'x01x02x03x04\' reversed_binary_data = process_binary_file(binary_data) print(reversed_binary_data) # Expected output: b\'x04x03x02x01\' ``` 1. **Input:** `input.txt` contains `\\"Hello World!\\"`, `output.txt` should contain `\\"HELLO WORLD!\\"`. 2. **Input:** `b\'x01x02x03x04\'`, Output should be `b\'x04x03x02x01\'`. **Note:** - Handle all file operations using the `io` module. - Raise appropriate exceptions if any errors occur during file processing. - Unit tests for these functions are encouraged to verify correctness.","solution":"import io def process_text_file(input_path: str, output_path: str) -> None: Read a UTF-8 encoded text file, convert the content to uppercase, and write it to another file, ensuring UTF-8 encoding. try: # Read the content of the input file with io.open(input_path, \'r\', encoding=\'utf-8\') as input_file: content = input_file.read() # Convert the content to uppercase uppercase_content = content.upper() # Write the uppercase content to the output file with io.open(output_path, \'w\', encoding=\'utf-8\') as output_file: output_file.write(uppercase_content) except Exception as e: raise IOError(f\\"An error occurred during file processing: {e}\\") def process_binary_file(input_data: bytes) -> bytes: Read data from the bytes object using an in-memory binary stream, reverse the contents, and return the reversed data as a bytes object. try: # Use an in-memory binary stream to read the input data input_stream = io.BytesIO(input_data) data = input_stream.read() # Reverse the data reversed_data = data[::-1] return reversed_data except Exception as e: raise IOError(f\\"An error occurred during binary data processing: {e}\\")"},{"question":"You are provided with a temperature dataset recorded over several months. Your task is to perform various windowing operations using pandas and analyze the data to extract useful information. Implement functionalities as described below: # Input You will receive a CSV file (`temperature.csv`) with the following columns: 1. `Date`: A string representing date in the format \'YYYY-MM-DD\'. 2. `Temperature`: A float representing the temperature recorded on that date. Example of `temperature.csv`: ``` Date,Temperature 2023-01-01,15.0 2023-01-02,14.5 2023-01-03,13.8 ... ``` # Requirements You need to implement the following features: 1. **Rolling Average Temperature**: Implement a function `rolling_avg_temperature(filename: str, window: int) -> pd.Series` that calculates the rolling average of temperature for a given window size (integer). 2. **Expanding Temperature Sum**: Implement a function `expanding_temperature_sum(filename: str) -> pd.Series` that calculates and returns the cumulative sum of temperatures up to each point. 3. **Exponentially Weighted Temperature**: Implement a function `ewm_temperature(filename: str, span: int) -> pd.Series` that calculates the exponentially weighted mean temperature, using a specific span provided. # Constraints - The dataset will have at least 30 days of temperature readings and at most 365 days. - Date entries are guaranteed to be sequential without any gaps. # Output Each function should return the resulting pandas Series with the same index and columns as the original `Date` column. # Example Usage ```python df[\\"RollingAvg\\"] = rolling_avg_temperature(\\"temperature.csv\\", window=7) df[\\"ExpandingSum\\"] = expanding_temperature_sum(\\"temperature.csv\\") df[\\"EWM\\"] = ewm_temperature(\\"temperature.csv\\", span=14) ``` --- You can assume that the provided data is clean and does not require any preprocessing other than converting the `Date` column to datetime format. Develop these functions to demonstrate comprehension of fundamental and advanced pandas windowing operations. The solutions should be efficient and follow pandas best practices.","solution":"import pandas as pd def rolling_avg_temperature(filename: str, window: int) -> pd.Series: Calculate the rolling average of temperature for a given window size. :param filename: str, path to the CSV file :param window: int, window size for the rolling average :return: pd.Series, containing the rolling averages df = pd.read_csv(filename, parse_dates=[\\"Date\\"], index_col=\\"Date\\") return df[\'Temperature\'].rolling(window=window).mean() def expanding_temperature_sum(filename: str) -> pd.Series: Calculate the cumulative sum of temperatures up to each point. :param filename: str, path to the CSV file :return: pd.Series, containing the cumulative sums df = pd.read_csv(filename, parse_dates=[\\"Date\\"], index_col=\\"Date\\") return df[\'Temperature\'].expanding().sum() def ewm_temperature(filename: str, span: int) -> pd.Series: Calculate the exponentially weighted mean temperature using a specific span provided. :param filename: str, path to the CSV file :param span: int, span for the exponentially weighted mean :return: pd.Series, containing the exponentially weighted means df = pd.read_csv(filename, parse_dates=[\\"Date\\"], index_col=\\"Date\\") return df[\'Temperature\'].ewm(span=span).mean()"},{"question":"You are tasked with investigating the learning behavior of a machine learning model to identify if it suffers from high bias or high variance, and to assess whether additional training data might improve its performance. You will use the Scikit-learn library to plot both validation and learning curves for a given dataset and model. # Objectives 1. **Plot Validation Curves** 2. **Plot Learning Curves** # Requirements - **Function Name**: `plot_model_curves` - **Inputs**: - `X` (`numpy.ndarray`): Feature matrix for the dataset. - `y` (`numpy.ndarray`): Target vector for the dataset. - `model`: An uninitialized scikit-learn estimator. - `param_name` (`str`): The name of the hyperparameter to be tuned (e.g., \\"C\\" for SVM, \\"max_depth\\" for decision trees). - `param_range` (`list` of `float`): The range of hyperparameter values to be tested for validation curves. - `train_sizes` (`list` of `int`): The different sizes of the training data to be used for plotting learning curves. - `cv` (`int`): The number of cross-validation folds. - **Outputs**: - This function should not return anything. Instead, it should plot the validation curve and learning curve using `ValidationCurveDisplay.from_estimator` and `LearningCurveDisplay.from_estimator` respectively. # Constraints 1. Use appropriate scoring metrics (e.g., accuracy for classification tasks). 2. Ensure that all plots are neatly labeled and easy to interpret. # Example Usage ```python from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.utils import shuffle import numpy as np # Load and shuffle the dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Define parameters model = SVC() param_name = \\"C\\" param_range = np.logspace(-7, 3, 10) train_sizes = [50, 80, 110] cv = 5 # Call the function to plot validation and learning curves plot_model_curves(X, y, model, param_name, param_range, train_sizes, cv) ``` # Additional Notes: - Students can use any plotting library that they are comfortable with (e.g., Matplotlib) if they prefer over `ValidationCurveDisplay` and `LearningCurveDisplay`.","solution":"import matplotlib.pyplot as plt import numpy as np from sklearn.model_selection import validation_curve, learning_curve from sklearn.metrics import accuracy_score from sklearn.model_selection import StratifiedKFold def plot_model_curves(X, y, model, param_name, param_range, train_sizes, cv): Plots validation and learning curves for a given model and dataset. Parameters: X (np.ndarray): Feature matrix for the dataset. y (np.ndarray): Target vector for the dataset. model: An uninitialized scikit-learn estimator. param_name (str): The name of the hyperparameter to be tuned. param_range (list of float): The range of hyperparameter values to be tested for validation curves. train_sizes (list of int): The different sizes of the training data to be used for plotting learning curves. cv (int): The number of cross-validation folds. # Plot validation curve train_scores, test_scores = validation_curve( model, X, y, param_name=param_name, param_range=param_range, cv=cv, scoring=\'accuracy\') train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.figure(figsize=(14, 5)) plt.subplot(1, 2, 1) plt.title(\\"Validation Curve\\") plt.xlabel(f\\"Parameter: {param_name}\\") plt.ylabel(\\"Score\\") plt.ylim(0.0, 1.1) plt.semilogx(param_range, train_scores_mean, label=\\"Training score\\", color=\\"darkorange\\") plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\\"darkorange\\") plt.semilogx(param_range, test_scores_mean, label=\\"Cross-validation score\\", color=\\"navy\\") plt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\\"navy\\") plt.legend(loc=\\"best\\") # Plot learning curve train_sizes, train_scores, test_scores = learning_curve( model, X, y, train_sizes=train_sizes, cv=StratifiedKFold(n_splits=cv), scoring=\'accuracy\') train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.subplot(1, 2, 2) plt.title(\\"Learning Curve\\") plt.xlabel(\\"Training examples\\") plt.ylabel(\\"Score\\") plt.ylim(0.0, 1.1) plt.plot(train_sizes, train_scores_mean, \'o-\', color=\\"darkorange\\", label=\\"Training score\\") plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\\"darkorange\\") plt.plot(train_sizes, test_scores_mean, \'o-\', color=\\"navy\\", label=\\"Cross-validation score\\") plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\\"navy\\") plt.legend(loc=\\"best\\") plt.show()"},{"question":"Advanced List Management with Bisection Algorithm You are tasked with developing a small module to manage a continuously updating list of records, each with multiple fields. Specifically, you will utilize the `bisect` module to maintain an ordered list of records based on one of the fields. Requirements: 1. **Insert a Record** into the List: - Define a function `insert_record(records, new_record, key)` that inserts `new_record` into `records` such that `records` remains sorted based on the `key` field. - `records` is a list of dictionaries. - `new_record` is a dictionary with the same structure as the elements of `records`. - `key` is the field in the dictionary that should be used for ordering. 2. **Find a Record** based on a specific key and value: - Define a function `find_record(records, key, value)` that finds and returns the first record where the `key` field is equal to `value`. If no such record exists, return `None`. - Utilize the bisection algorithm for efficient searching. 3. **Range Search**: - Define a function `find_records_in_range(records, key, low, high)` that returns all records where the `key` field is between `low` and `high`, inclusive. The returned list should maintain the order. - Implement this using the bisection algorithms for the search to ensure efficiency. Function Signatures: ```python def insert_record(records: List[Dict[str, Any]], new_record: Dict[str, Any], key: str) -> None: pass def find_record(records: List[Dict[str, Any]], key: str, value: Any) -> Optional[Dict[str, Any]]: pass def find_records_in_range(records: List[Dict[str, Any]], key: str, low: Any, high: Any) -> List[Dict[str, Any]]: pass ``` Input: - `records`: list of dictionaries of uniform structure. - `new_record`: a dictionary of the same structure. - `key`: the field in the dictionaries to be used for sorting. - `value`: the value to be searched for. - `low` and `high`: the range of values to be searched for. Output: - For `insert_record`: The function modifies the list `records` in place to insert `new_record` while maintaining order. - For `find_record`: The function returns the first matched record or `None`. - For `find_records_in_range`: A list of dictionaries that meet the range criteria, maintaining order. Constraints: 1. The list `records` is initially sorted based on the `key` field. 2. Your implementation should leverage the `bisect` module for efficient insertions and lookups. 3. Records might contain different types in the `key` field, but all records in `records` will have the same type for `key`. Example Usage: ```python records = [ {\'name\': \'Record1\', \'value\': 10}, {\'name\': \'Record2\', \'value\': 20}, {\'name\': \'Record3\', \'value\': 30}, ] # Insert a new record insert_record(records, {\'name\': \'Record4\', \'value\': 25}, \'value\') # records should now be sorted with the new record in place # Find a specific record record = find_record(records, \'value\', 25) # should return {\'name\': \'Record4\', \'value\': 25} # Find records within a range range_records = find_records_in_range(records, \'value\', 15, 30) # should return [{\'name\': \'Record2\', \'value\': 20}, {\'name\': \'Record4\', \'value\': 25}, {\'name\': \'Record3\', \'value\': 30}] ``` Your solution will be judged on correctness, efficiency, and utilization of the `bisect` module.","solution":"import bisect from typing import List, Dict, Any, Optional def insert_record(records: List[Dict[str, Any]], new_record: Dict[str, Any], key: str) -> None: Inserts new_record into records such that records remains sorted based on the key field. key_value = new_record[key] index = bisect.bisect_left([record[key] for record in records], key_value) records.insert(index, new_record) def find_record(records: List[Dict[str, Any]], key: str, value: Any) -> Optional[Dict[str, Any]]: Finds and returns the first record where the key field is equal to value. Returns None if no record is found. index = bisect.bisect_left([record[key] for record in records], value) if index < len(records) and records[index][key] == value: return records[index] return None def find_records_in_range(records: List[Dict[str, Any]], key: str, low: Any, high: Any) -> List[Dict[str, Any]]: Returns all records where the key field is between low and high, inclusive. low_index = bisect.bisect_left([record[key] for record in records], low) high_index = bisect.bisect_right([record[key] for record in records], high) return records[low_index:high_index]"},{"question":"**Objective:** Demonstrate the ability to use the `builtins` module to override and extend the functionality of Python\'s built-in functions while maintaining access to their original behavior. **Problem Statement:** You are required to implement a custom function called `open_with_logging` that overrides the built-in `open` function. This function should: 1. Open a file at the specified path with the given mode, but also log the file path and mode to a log file called `file_log.txt` before actually opening the file. 2. Return a custom file object that wraps around the actual file object and logs each read operation to the `file_log.txt`. The custom file object should: - Log each read operation to the `file_log.txt` with the count of bytes being read, before reading from the file. - Behave exactly like the standard file object for other operations. **Implementation Details:** 1. The function `open_with_logging` should take the same parameters as the built-in `open` function. 2. The log entries should be appended to `file_log.txt` in the format: `\\"Opening file: {path}, Mode: {mode}\\"` and `\\"Reading {count} bytes from file: {path}\\"`. 3. Ensure the use of the `builtins` module to access the original `open` function. **Function Signature:** ```python from typing import Any, IO def open_with_logging(path: str, mode: str = \'r\', *args: Any, **kwargs: Any) -> IO: # Implementation here ``` **Example Usage:** ```python file_object = open_with_logging(\'example.txt\', \'r\') content = file_object.read(10) # Contents of file_log.txt after execution # Opening file: example.txt, Mode: r # Reading 10 bytes from file: example.txt ``` **Constraints:** - You must use the `builtins` module to refer to the original `open` function. - Ensure that the original behavior of the built-in `open` function is fully preserved (e.g., exceptions on incorrect file access should not be suppressed). **Testing and Validation:** - Validate your implementation with a variety of file operations to ensure correct logging and functionality. - Consider edge cases like empty reads (`read(0)`), reading till the end of the file (`read()`), and so on.","solution":"import builtins class CustomFileWrapper: def __init__(self, file_object, path): self._file = file_object self._path = path def read(self, size=-1): with open(\'file_log.txt\', \'a\') as log_file: log_file.write(f\\"Reading {size if size != -1 else \'all\'} bytes from file: {self._path}n\\") return self._file.read(size) def __getattr__(self, name): return getattr(self._file, name) def open_with_logging(path: str, mode: str = \'r\', *args, **kwargs): with open(\'file_log.txt\', \'a\') as log_file: log_file.write(f\\"Opening file: {path}, Mode: {mode}n\\") original_open = builtins.open file_object = original_open(path, mode, *args, **kwargs) return CustomFileWrapper(file_object, path)"},{"question":"Coding Assessment Question You are given the `penguins` dataset from the seaborn package. This dataset contains various measurements of three different species of penguins. Your task is to write a function `custom_pairplot` to create and save a customized pairplot as a `.png` file. # Function Signature ```python def custom_pairplot(output_file: str) -> None: pass ``` # Input - `output_file` (str): The file path where the resulting pairplot image will be saved. # Requirements 1. Load the `penguins` dataset using `seaborn.load_dataset(\'penguins\')`. 2. Create a pairplot using the following specifications: - Use the `species` column for the `hue` parameter. - Use `kde` for the `kind` parameter. - Use markers `[\\"o\\", \\"s\\", \\"D\\"]` for different species. - Set the `height` of each subplot to 2.5. - Plot only the variables `\\"bill_length_mm\\", \\"bill_depth_mm\\", and \\"flipper_length_mm\\"` both on x and y axes. - Set the corner parameter to `False` (default value). - Customize the lower triangle using `sns.kdeplot`, with `levels=6` and `color=\\"r\\"`. 3. Save the resulting plot as a `.png` file using the provided `output_file` path. # Output - The function should save the resulting pairplot figure as a `.png` file to the specified `output_file` path. - There is no return value for this function. # Example ```python custom_pairplot(\\"output_penguins_plot.png\\") ``` This should create a customized pairplot of the `penguins` dataset with the above specifications and save it as \\"output_penguins_plot.png\\". # Constraints - Ensure that your plot is clear and readable with labeled axes. - Use appropriate figure aesthetics to make the plot visually appealing. # Notes - You can assume seaborn and other necessary libraries are already installed. - Ensure that you follow the requirements precisely to demonstrate your understanding of seaborn’s customization capabilities.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_pairplot(output_file: str) -> None: # Load the penguins dataset penguins = sns.load_dataset(\'penguins\') # Create the pairplot with the specified customizations pairplot = sns.pairplot(penguins, hue=\'species\', kind=\'kde\', markers=[\\"o\\", \\"s\\", \\"D\\"], height=2.5, vars=[\\"bill_length_mm\\", \\"bill_depth_mm\\", \\"flipper_length_mm\\"]) # Customize the lower triangle using kdeplot with specified levels and color for i in range(len(pairplot.axes)): for j in range(i): sns.kdeplot(data=penguins, x=penguins[pairplot.x_vars[i]], y=penguins[pairplot.y_vars[j]], levels=6, color=\\"r\\", ax=pairplot.axes[i, j]) # Save the plot as a .png file plt.savefig(output_file)"},{"question":"# Garbage Collection Management and Debugging Problem Statement Your task is to create a Python utility class called `GarbageCollectorManager` to manage and analyze the garbage collection process. The class should provide methods to enable/disable garbage collection, force a garbage collection, set custom thresholds, and output debugging information about collected and uncollectable objects. Requirements 1. **Initialization**: - The `GarbageCollectorManager` class should automatically enable garbage collection upon initialization. 2. **Enable and Disable GC**: - Method to enable garbage collection: `enable_gc() -> None` - Method to disable garbage collection: `disable_gc() -> None` - Method to check if GC is enabled: `is_gc_enabled() -> bool` 3. **Collect Garbage**: - Method to force garbage collection: `force_collect(generation: int = 2) -> int` - This method should collect garbage for the specified generation (`0`, `1`, or `2`) and return the number of unreachable objects found. Raise a `ValueError` if an invalid generation is provided. 4. **Threshold Management**: - Method to set collection thresholds: `set_gc_thresholds(threshold0: int, threshold1: int = 10, threshold2: int = 10) -> None` - Method to get current collection thresholds: `get_gc_thresholds() -> Tuple[int, int, int]` 5. **Debugging**: - Method to set debugging flags: `set_debug_flags(flags: int) -> None` - Method to get current debugging flags: `get_debug_flags() -> int` - Method to get garbage statistics: `get_gc_stats() -> List[Dict[str, int]]` - Method to print information about uncollectable objects: `print_uncollectable_objects() -> None` 6. **Garbage Callbacks**: - Optionally, method to add a custom callback to the GC\'s callback list: `add_gc_callback(callback: Callable[[str, Dict[str, int]], None]) -> None` Constraints - Ensure to handle any possible exceptions gracefully and provide descriptive error messages. - For the method `add_gc_callback`, ensure the callback adheres to the expected signature: `def callback(phase: str, info: Dict[str, int]) -> None`. Example Usage: ```python manager = GarbageCollectorManager() # Enable/Disable GC manager.enable_gc() print(manager.is_gc_enabled()) # Output: True manager.disable_gc() print(manager.is_gc_enabled()) # Output: False # Force collection and retrieve unreachable objects count unreachable_objects = manager.force_collect(2) print(f\\"Unreachable objects: {unreachable_objects}\\") # Set and get thresholds manager.set_gc_thresholds(700, 10, 5) print(manager.get_gc_thresholds()) # Output: (700, 10, 5) # Debugging manager.set_debug_flags(gc.DEBUG_LEAK) print(manager.get_debug_flags()) # Output: DEBUG_LEAK constant value # Print uncollectable objects manager.print_uncollectable_objects() # Add a custom GC callback def my_callback(phase, info): print(f\\"GC Phase: {phase}, Info: {info}\\") manager.add_gc_callback(my_callback) ``` You are required to implement the `GarbageCollectorManager` class with the specified methods, ensuring all functionalities work as described. Notes: - Utilize the `gc` module functions and constants to implement the methods. - Test for various edge cases, such as invalid generations, unusual threshold values, and ensuring callbacks are executed correctly.","solution":"import gc from typing import Callable, Dict, List, Tuple class GarbageCollectorManager: def __init__(self): gc.enable() def enable_gc(self) -> None: gc.enable() def disable_gc(self) -> None: gc.disable() def is_gc_enabled(self) -> bool: return gc.isenabled() def force_collect(self, generation: int = 2) -> int: if generation not in [0, 1, 2]: raise ValueError(\\"Generation must be 0, 1, or 2.\\") return gc.collect(generation) def set_gc_thresholds(self, threshold0: int, threshold1: int = 10, threshold2: int = 10) -> None: gc.set_threshold(threshold0, threshold1, threshold2) def get_gc_thresholds(self) -> Tuple[int, int, int]: return gc.get_threshold() def set_debug_flags(self, flags: int) -> None: gc.set_debug(flags) def get_debug_flags(self) -> int: return gc.get_debug() def get_gc_stats(self) -> List[Dict[str, int]]: return gc.get_stats() def print_uncollectable_objects(self) -> None: for obj in gc.garbage: print(obj) def add_gc_callback(self, callback: Callable[[str, Dict[str, int]], None]) -> None: if not callable(callback): raise ValueError(\\"The provided callback is not callable.\\") gc.callbacks.append(callback)"},{"question":"Objective: In this exercise, your task is to load a dataset using seaborn, create a visualization using the `seaborn.objects` module, and customize the plot by applying various customization techniques as described. Tasks: 1. Load the `diamonds` dataset using `seaborn`. 2. Create a bar plot to visualize the average and median carat values based on the clarity of the diamonds. 3. Customize the plot by adding error bars representing standard error and standard deviation. 4. Ensure the bootstrapping randomness is controlled by setting a seed. 5. Apply weights based on the price of the diamonds when calculating the average carat values. 6. Save the plot as a file named `diamond_clarity_analysis.png`. Expected Input and Output: - **Input:** Load the `diamonds` dataset automatically. - **Output:** Save the customized plot as an image file named `diamond_clarity_analysis.png`. Constraints: - Use `seaborn.objects` for all plotting tasks. - Ensure the randomness is controlled with a set seed for reproducibility. - Use weights based on the price column in the dataset. Example Here\'s an initial setup to guide you: ```python import seaborn.objects as so from seaborn import load_dataset # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create the plot object p = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\") # Add mean estimation with error bars (standard error) p.add(so.Bar(), so.Est(), errorbar=\\"se\\") # Add median estimation p.add(so.Bar(), so.Est(\\"median\\")) # Configure error bars with standard deviation p.add(so.Bar(), so.Est(errorbar=\\"sd\\")) # Control randomness using seed p.add(so.Bar(), so.Est(seed=0)) # Apply weights based on price p.add(so.Bar(), so.Est(), weight=\\"price\\") # Save the plot p.save(\\"diamond_clarity_analysis.png\\") ``` Note: This code does not contain the full implementation. Complete the specified tasks as described.","solution":"import seaborn.objects as so from seaborn import load_dataset import numpy as np import matplotlib.pyplot as plt def create_and_save_plot(): # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create a plot object p = (so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\") .add(so.Bar(), so.Est(\\"mean\\", errorbar=\\"se\\"), label=\\"Mean (SE)\\") .add(so.Bar(), so.Est(\\"median\\"), label=\\"Median\\") .add(so.Bar(), so.Est(\\"mean\\", errorbar=\\"sd\\"), label=\\"Mean (SD)\\") .add(so.Bar(), so.Est(\\"mean\\", seed=0), weight=\\"price\\", label=\\"Mean (Weighted by Price)\\")) # Save the plot p.save(\\"diamond_clarity_analysis.png\\")"},{"question":"Develop a custom importer by implementing a `MetaPathFinder` and `Loader` that loads a module directly from a string containing the source code. This will test your understanding of module specifications, custom imports, and the workings of `importlib`. Objective: Create a custom meta path finder and loader to import a module directly from a string containing its source code. Requirements: 1. Define a `StringModuleLoader` class inheriting from `importlib.abc.Loader`. 2. Implement the following methods in the `StringModuleLoader`: - `create_module(spec)`: This method should create a module object, returning `None` to let the default module creation take place. - `exec_module(module)`: This method should execute the module using the source code stored in the loader. 3. Define a `StringMetaPathFinder` class inheriting from `importlib.abc.MetaPathFinder`. 4. Implement the method `find_spec(fullname, path, target=None)` in the `StringMetaPathFinder` to return a `ModuleSpec` for the module if the module name matches a predefined name (e.g., \\"string_module\\"). 5. Create a sample usage where the custom importer is added to `sys.meta_path` and demonstrate importing the `string_module` from a provided source code string. Constraints: - The custom loader must handle only modules named `\\"string_module\\"`. - Your solution should appropriately clean up any temporary settings (e.g., removing the custom finder from `sys.meta_path` after the demonstration). Example Usage: ```python import sys import importlib.abc import importlib.util MODULE_NAME = \\"string_module\\" SOURCE_CODE = \'\'\' def hello(): print(\\"Hello from string module!\\") \'\'\' class StringModuleLoader(importlib.abc.Loader): def __init__(self, code): self.code = code def create_module(self, spec): return None def exec_module(self, module): exec(self.code, module.__dict__) class StringMetaPathFinder(importlib.abc.MetaPathFinder): def __init__(self, module_name, code): self.module_name = module_name self.code = code def find_spec(self, fullname, path, target=None): if fullname == self.module_name: loader = StringModuleLoader(self.code) return importlib.util.spec_from_loader(fullname, loader) return None # Add the custom finder to sys.meta_path finder = StringMetaPathFinder(MODULE_NAME, SOURCE_CODE) sys.meta_path.insert(0, finder) # Demonstrate importing and using the module import string_module string_module.hello() # Clean up by removing the custom finder from sys.meta_path sys.meta_path.pop(0) ``` Output: ``` Hello from string module! ``` Notes: - Your implementation must handle invalid module names gracefully by returning `None` in the `find_spec` method. - Ensure the `exec_module` method correctly executes the provided source code within the module\'s namespace.","solution":"import sys import importlib.abc import importlib.util class StringModuleLoader(importlib.abc.Loader): def __init__(self, code): self.code = code def create_module(self, spec): return None def exec_module(self, module): exec(self.code, module.__dict__) class StringMetaPathFinder(importlib.abc.MetaPathFinder): def __init__(self, module_name, code): self.module_name = module_name self.code = code def find_spec(self, fullname, path, target=None): if fullname == self.module_name: loader = StringModuleLoader(self.code) return importlib.util.spec_from_loader(fullname, loader) return None # Define constants for module name and source code for demonstration MODULE_NAME = \\"string_module\\" SOURCE_CODE = \'\'\' def hello(): return \\"Hello from string module!\\" \'\'\' # Add the custom finder to sys.meta_path finder = StringMetaPathFinder(MODULE_NAME, SOURCE_CODE) sys.meta_path.insert(0, finder) # Import the module for demo import string_module # Clean up by removing the custom finder from sys.meta_path sys.meta_path.pop(0)"},{"question":"# Variables and Exception Handling in Python Your task is to demonstrate your understanding of the Python code structure, variable scoping, name resolution, and exception handling by implementing a series of functions. Function 1: `create_variables` Write a function `create_variables` that does the following: - Defines three variables: - `local_var` with the value of 10. - `nonlocal_var` with the value of 20. - `global_var` with the value of 30. - Uses nested functions to modify these variables: - Within an inner function, modify the value of `local_var`. - Use a `nonlocal` declaration to modify `nonlocal_var`. - Use a `global` declaration to modify `global_var`. - Return a tuple with the final values of all three variables after modifications. ```python def create_variables(): # TODO: Implement the function pass ``` Function 2: `handle_exception` Write a function `handle_exception` that does the following: - Takes a single integer `x` as input. - Attempts to perform the following operations: - Divide 100 by `x`. - Convert `x` to an integer. - Implement appropriate exception handling to catch and handle `ZeroDivisionError`, `ValueError`, and any other unexpected exceptions. - Return the result of the division if successful, otherwise return a string message describing the encountered exception. ```python def handle_exception(x): # TODO: Implement the function pass ``` # Constraints 1. The `create_variables` function should demonstrate the correct usage of variable scopes as described. 2. The `handle_exception` function should manage exceptions effectively, ensuring the program doesn\'t crash and returns meaningful error messages. # Performance Requirements - The `create_variables` function should complete its execution without any modifications causing inadvertent scope errors. - The `handle_exception` should properly catch and handle specified exceptions without assuming valid input every time. # Input and Output - The `create_variables` function has no input parameters and returns a tuple. - The `handle_exception` function takes a single integer and returns either a numeric result or a string message. # Example ```python # Example usage: result_vars = create_variables() print(result_vars) # Expected output might be (modified_local_value, modified_nonlocal_value, modified_global_value) result_exception = handle_exception(0) print(result_exception) # Expected output: String message about the ZeroDivisionError ``` Note: Implement these functions in a way that showcases your comprehension of variable scoping and exception handling in Python.","solution":"def create_variables(): local_var = 10 nonlocal_var = 20 global global_var global_var = 30 def outer_function(): nonlocal nonlocal_var def inner_function(): nonlocal nonlocal_var global global_var local_var = 40 # This modifies only the local_var inside this scope nonlocal_var = 50 # This modifies the nonlocal_var from the outer_function global_var = 60 # This modifies the global global_var inner_function() # Return the variables in required scope after inner_function execution return local_var, nonlocal_var, global_var return outer_function() def handle_exception(x): try: result = 100 / x # Attempt to divide 100 by x return result except ZeroDivisionError: return \\"ZeroDivisionError: division by zero\\" except ValueError: return \\"ValueError: invalid value\\" except Exception as e: return f\\"Exception: {str(e)}\\""},{"question":"# Custom Mapping Implementation In this exercise, you will implement a custom class `CustomMapping` that emulates the behavior of a Python dictionary using the Python C-API functions, which are typically provided in the `python310` stable ABI. You won\'t be using any direct dictionary methods but will rely on the protocol described in the documentation. Requirements 1. **Initialization**: Your `CustomMapping` class should be initialized with a dictionary. 2. **__len__**: Implement the `__len__` method to return the number of items in the mapping. 3. **__getitem__**: Implement the `__getitem__` method to retrieve the item associated with a given key. 4. **__setitem__**: Implement the `__setitem__` method to set the value for a given key. 5. **__delitem__**: Implement the `__delitem__` method to delete an item by key. 6. **__contains__**: Implement the `__contains__` method to check if a key exists in the mapping. 7. **keys**: Implement the `keys` method to return a list of keys in the mapping. 8. **values**: Implement the `values` method to return a list of values in the mapping. 9. **items**: Implement the `items` method to return a list of key-value pairs in the mapping. Constraints - You must use the provided C-API function counterparts simulated with standard Python functions (e.g., `len()`, direct item access, etc., should be avoided where a C-API functional equivalent can be used). Example ```python mapping = CustomMapping({\'a\': 1, \'b\': 2}) print(len(mapping)) # Output: 2 print(mapping[\'a\']) # Output: 1 mapping[\'c\'] = 3 print(mapping[\'c\']) # Output: 3 print(\'c\' in mapping) # Output: True print(mapping.keys()) # Output: [\'a\', \'b\', \'c\'] print(mapping.values()) # Output: [1, 2, 3] print(mapping.items()) # Output: [(\'a\', 1), (\'b\', 2), (\'c\', 3)] del mapping[\'c\'] print(\'c\' in mapping) # Output: False ``` Implement the class `CustomMapping` as described.","solution":"class CustomMapping: def __init__(self, initial_dict): self._dict = initial_dict def __len__(self): Returns the number of items in the mapping. keys = self._dict.keys() count = 0 for _ in keys: count += 1 return count def __getitem__(self, key): Retrieves the item associated with the given key. return self._dict[key] def __setitem__(self, key, value): Sets the value of the given key. self._dict[key] = value def __delitem__(self, key): Deletes the item associated with the given key. del self._dict[key] def __contains__(self, key): Checks if the given key exists in the mapping. return key in self._dict def keys(self): Returns a list of keys in the mapping. return list(self._dict.keys()) def values(self): Returns a list of values in the mapping. return list(self._dict.values()) def items(self): Returns a list of key-value pairs in the mapping. return list(self._dict.items())"},{"question":"# Coding Assessment: Implementing an Enhanced LRU Cache with Single Dispatch **Objective**: Demonstrate your understanding of Python\'s `functools` by implementing an enhanced least-recently-used (LRU) cache that supports single-dispatch functionality based on input type. # Task Description You are required to create an `EnhancedLRUCache` class that extends the functionality of the standard `functools.lru_cache` decorator. This enhanced cache should: 1. Cache results for different types of inputs separately. 2. Use single-dispatch to handle different input types appropriately. 3. Provide the ability to clear the cache or retrieve cache statistics. Here are the specifications for the `EnhancedLRUCache` class: # Class: `EnhancedLRUCache` Methods: - **`__init__(self, maxsize=128)`**: Initialize the cache with a specified maximum size. - **`@functools.singledispatchmethod`** **`cache(self, element)`**: - This is the primary method to be dispatched on the input type. - Stores the element in the cache and returns a transformation/modification based on the input type. **`@cache.register`** **`def _(self, element: int)`**: - For `int` inputs, compute and store the factorial of the integer. **`@cache.register`** **`def _(self, element: str)`**: - For `str` inputs, store and return the reversed string. - **`cache_clear(self)`**: Clears the cache. - **`cache_info(self)`**: Returns cache statistics including hits, misses, and current cache size. # Example Usage ```python # Instantiate the cache enhanced_cache = EnhancedLRUCache(maxsize=5) # Caching integer values print(enhanced_cache.cache(5)) # Output: 120 (factorial of 5) print(enhanced_cache.cache(3)) # Output: 6 (factorial of 3) # Caching string values print(enhanced_cache.cache(\'hello\')) # Output: \'olleh\' print(enhanced_cache.cache(\'world\')) # Output: \'dlrow\' # Checking cache statistics print(enhanced_cache.cache_info()) # Output: CacheInfo with hits, misses, maxsize, currsize # Clearing the cache enhanced_cache.cache_clear() ``` # Constraints - Inputs to the `cache` method will be either `int` or `str`. - The cache size will not exceed `10^3`. # Additional Notes - Make sure to use appropriate decorators wherever necessary. - Implement efficient caching mechanisms to handle the specified constraints. # Evaluation Criteria - Correct application of functools decorators. - Efficient and correct implementation of caching mechanism. - Proper handling of different input types using single dispatch. - Clean and readable code.","solution":"import functools from functools import lru_cache, singledispatchmethod class EnhancedLRUCache: def __init__(self, maxsize=128): self.maxsize = maxsize self.int_cache = lru_cache(maxsize=maxsize)(self._factorial) self.str_cache = lru_cache(maxsize=maxsize)(self._reverse_string) @singledispatchmethod def cache(self, element): raise NotImplementedError(f\\"Unsupported type: {type(element)}\\") @cache.register def _(self, element: int): return self.int_cache(element) @cache.register def _(self, element: str): return self.str_cache(element) def _factorial(self, x): if x == 0: return 1 else: return x * self._factorial(x - 1) def _reverse_string(self, s): return s[::-1] def cache_clear(self): self.int_cache.cache_clear() self.str_cache.cache_clear() def cache_info(self): return { \'int_cache\': self.int_cache.cache_info(), \'str_cache\': self.str_cache.cache_info() }"},{"question":"You are given a DataFrame containing survey responses with various types of data, including categorical and numerical data. The DataFrame `df` contains columns as follows: - `Respondent ID`: A unique identifier for each respondent. - `Age`: Numerical data representing the age of the respondent. - `Gender`: Categorical data representing the respondent\'s gender (with possible values: \'Male\', \'Female\', \'Other\'). - `Country`: Categorical data representing the respondent\'s country of residence. - `Satisfaction`: Categorical data with an order, representing the level of satisfaction (with possible values: \'Very Dissatisfied\', \'Dissatisfied\', \'Neutral\', \'Satisfied\', \'Very Satisfied\'). Your task is to implement the following functions using pandas: 1. **`convert_to_categorical(df)`**: - **Input**: A pandas DataFrame `df`. - **Output**: The same DataFrame with the `Gender`, `Country`, and `Satisfaction` columns converted to categorical data types. The `Satisfaction` column must be ordered. 2. **`add_custom_category(df, column_name, new_category)`**: - **Input**: A pandas DataFrame `df`, a string `column_name` representing the column name to add a new category to, and a string `new_category` representing the new category to add. - **Output**: The DataFrame with the new category added to the specified column. - **Constraints**: Ensure the column specified is categorical before adding the category. 3. **`value_counts_ordered(df, column_name)`**: - **Input**: A pandas DataFrame `df` and a string `column_name` representing the name of a categorical column. - **Output**: A pandas Series with the value counts of the specified column, preserving the order of the categories. Your code should handle potential issues such as ensuring that columns are categorical before performing operations specific to categorical data and maintaining the order of categories where necessary. **Example Usage**: ```python import pandas as pd # Sample DataFrame data = { \'Respondent ID\': [1, 2, 3, 4, 5], \'Age\': [23, 45, 31, 22, 34], \'Gender\': [\'Male\', \'Female\', \'Female\', \'Male\', \'Other\'], \'Country\': [\'USA\', \'Canada\', \'USA\', \'UK\', \'Canada\'], \'Satisfaction\': [\'Satisfied\', \'Neutral\', \'Very Satisfied\', \'Dissatisfied\', \'Neutral\'] } df = pd.DataFrame(data) # Convert to categorical df = convert_to_categorical(df) # Add custom category df = add_custom_category(df, \'Gender\', \'Non-binary\') # Get value counts with order value_counts = value_counts_ordered(df, \'Satisfaction\') print(value_counts) ```","solution":"import pandas as pd def convert_to_categorical(df): Converts the `Gender`, `Country`, and `Satisfaction` columns to categorical data types. The `Satisfaction` column is ordered. df[\'Gender\'] = df[\'Gender\'].astype(\'category\') df[\'Country\'] = df[\'Country\'].astype(\'category\') satisfaction_order = [\'Very Dissatisfied\', \'Dissatisfied\', \'Neutral\', \'Satisfied\', \'Very Satisfied\'] df[\'Satisfaction\'] = pd.Categorical(df[\'Satisfaction\'], categories=satisfaction_order, ordered=True) return df def add_custom_category(df, column_name, new_category): Adds a new category to a specified column if it is categorical. if not isinstance(df[column_name].dtype, pd.CategoricalDtype): raise ValueError(f\\"Column {column_name} is not categorical.\\") df[column_name] = df[column_name].cat.add_categories(new_category) return df def value_counts_ordered(df, column_name): Returns the value counts of a categorical column while preserving category order. if not isinstance(df[column_name].dtype, pd.CategoricalDtype): raise ValueError(f\\"Column {column_name} is not categorical.\\") return df[column_name].value_counts().reindex(df[column_name].cat.categories)"},{"question":"Objective: You are required to design a custom timer server and client pair by extending the `TimerServer` and `TimerClient` classes from the `torch.distributed.elastic.timer` package. The custom timer server-client pair should use a dictionary to communicate expiration times between the server and the client. Details: 1. **Timer Server**: - Create a class `DictTimerServer` extending the `TimerServer` class. - Use a dictionary to store timer states with keys as timer IDs and values as expiration times. - Implement the necessary methods to handle timer requests and update the expiration times as specified. 2. **Timer Client**: - Create a class `DictTimerClient` extending the `TimerClient` class. - Implement methods to send timer requests to the `DictTimerServer`, retrieve expiration states, and handle responses. 3. **Timer Request**: - Use the `TimerRequest` object to pass messages between the server and client. 4. **Debugging**: - Utilize the `log_debug_info_for_expired_timers` function to log detailed information about expired timers. Requirements: - **Input**: - The client should be able to set and check timers. - Method to configure TimerServer with initial timer states and expiration times. - Method for clients to send expiration check requests. - **Output**: - TimerServer should respond with the expiration status of the requested timers. - Client should log the expiration information. Constraints: - Assume unique timer IDs for each timer set. - Ensure thread safety when accessing the dictionary storing timer states. Performance: - The solution should efficiently handle multiple simultaneous timer requests. Example: ```python from torch.distributed.elastic.timer import TimerServer, TimerClient, TimerRequest import threading import time class DictTimerServer(TimerServer): def __init__(self): super().__init__() self.timer_dict = {} self.lock = threading.Lock() def configure(self, timer_id, expiration_time): with self.lock: self.timer_dict[timer_id] = expiration_time def handle_request(self, request): timer_id = request.timer_id current_time = time.time() with self.lock: is_expired = current_time >= self.timer_dict.get(timer_id, 0) return TimerRequest(timer_id=timer_id, is_expired=is_expired) class DictTimerClient(TimerClient): def __init__(self, server): self.server = server def set_timer(self, timer_id, duration): expiration_time = time.time() + duration self.server.configure(timer_id, expiration_time) def check_timer(self, timer_id): request = TimerRequest(timer_id=timer_id) response = self.server.handle_request(request) if response is not None and response.is_expired: log_debug_info_for_expired_timers(response) # Example usage: server = DictTimerServer() client = DictTimerClient(server) client.set_timer(\'timer1\', 5) # Set a timer for 5 seconds # Wait and check time.sleep(6) client.check_timer(\'timer1\') # This should log expiation message ``` # Note: Students are encouraged to ensure synchronization and correct handling of timer states, implement necessary classes and methods, and demonstrate their understanding of PyTorch\'s distributed elastic timer concepts through this implementation.","solution":"import threading import time class TimerRequest: def __init__(self, timer_id=None, is_expired=None): self.timer_id = timer_id self.is_expired = is_expired class TimerServer: # Assuming a basic interface for TimerServer def __init__(self): pass class TimerClient: # Assuming a basic interface for TimerClient def __init__(self): pass def log_debug_info_for_expired_timers(response): print(f\\"Timer {response.timer_id} has expired.\\") class DictTimerServer(TimerServer): def __init__(self): super().__init__() self.timer_dict = {} self.lock = threading.Lock() def configure(self, timer_id, expiration_time): with self.lock: self.timer_dict[timer_id] = expiration_time def handle_request(self, request): timer_id = request.timer_id current_time = time.time() with self.lock: expiration_time = self.timer_dict.get(timer_id, 0) is_expired = current_time >= expiration_time return TimerRequest(timer_id=timer_id, is_expired=is_expired) class DictTimerClient(TimerClient): def __init__(self, server): self.server = server def set_timer(self, timer_id, duration): expiration_time = time.time() + duration self.server.configure(timer_id, expiration_time) def check_timer(self, timer_id): request = TimerRequest(timer_id=timer_id) response = self.server.handle_request(request) if response is not None and response.is_expired: log_debug_info_for_expired_timers(response) return response.is_expired # Example usage: # server = DictTimerServer() # client = DictTimerClient(server) # client.set_timer(\'timer1\', 5) # Set a timer for 5 seconds # time.sleep(6) # client.check_timer(\'timer1\') # This should log expiration message"},{"question":"Custom Bytes Handling You are required to implement a function in Python using the `bytes` object handling techniques inspired by the internal CPython API described in the documentation. The function will create a series of bytes objects from given inputs, perform some formatting, and concatenate them into a single bytes object according to the specifications. Function Signature ```python def format_and_concatenate( inputs: List[Tuple[str, List[Union[int, str]]]]) -> bytes: ``` Parameters: - `inputs` : List[Tuple[str, List[Union[int, str]]]] - A list of tuples, where each tuple contains a format string and a list of arguments to be formatted into bytes. - The format strings will contain C-style formatting as described in the documentation, e.g., `%d`, `%s`, etc. Returns: - `resulting_bytes`: `bytes` - The final concatenated bytes object after processing all inputs. Example: ```python inputs = [ (\\"%s%s\\", [\\"Hello, \\", \\"World!\\"]), (\\"%d + %d = %d\\", [1, 2, 3]), (\\"%x\\", [255]), ] output = format_and_concatenate(inputs) print(output) # Outputs: b\'Hello, World!1 + 2 = 3ff\' ``` Constraints: 1. Assume all format specifiers and arguments are correctly provided. 2. Each format string may contain multiple format specifiers. 3. The function should process each input tuple, format the bytes, and concatenate them in sequence as a single bytes object. Implementation Notes: - You may use built-in `bytes` methods or simulate the actions of `PyBytes_FromFormat` using Python\'s byte formatting routines. - Ensure the function handles concatenation seamlessly to form the final bytes object. Please implement the `format_and_concatenate` function based on the above specifications.","solution":"from typing import List, Tuple, Union def format_and_concatenate(inputs: List[Tuple[str, List[Union[int, str]]]]) -> bytes: Process the list of input tuples, format them as bytes, and concatenate the results. :param inputs: A list of tuples where each tuple contains a format string and a list of arguments to be formatted into bytes. :return: A single concatenated bytes object. result_bytes = b\\"\\" for format_string, args in inputs: formatted_string = format_string % tuple(args) result_bytes += formatted_string.encode(\'utf-8\') return result_bytes"},{"question":"# Custom Iterable and Summable Sequence You are required to create a custom class that behaves like a sequence of integers. This class should support: 1. Sequence operations like indexing, slicing, and iteration. 2. Summing its elements, both using the built-in `sum()` function and a custom method `total()`. Additionally, integrate asynchronous support to allow summing elements asynchronously. Requirements: 1. Implement a class `CustomSequence` with the following: - `__init__(self, *args)`: Initializes the sequence with given integer elements. - `__getitem__(self, index)`: Supports indexing and slicing. - `__iter__(self)`: Returns an iterator to allow iteration over the sequence. - `total(self)`: Returns the sum of all elements in the sequence. - `__repr__(self)`: Returns a string representation of the sequence. - Implement `__len__()` and `__contains__()` methods to support `len()` and `in` operations. - Custom exception handling for invalid inputs (non-integers) during initialization. 2. Asynchronous behavior: - Implement an asynchronous method `async_total(self)` to sum all elements asynchronously, utilizing `await` and standard asynchronous mechanisms. Example Usage: ```python seq = CustomSequence(1, 2, 3, 4, 5) print(seq) # Output: CustomSequence(1, 2, 3, 4, 5) print(seq[2]) # Output: 3 print(seq[1:4]) # Output: CustomSequence(2, 3, 4) print(sum(seq)) # Output: 15 print(seq.total()) # Output: 15 print(3 in seq) # Output: True print(len(seq)) # Output: 5 # Asynchronous summing import asyncio print(asyncio.run(seq.async_total())) # Output: 15 ``` Constraints: - Elements must be integers. - Methods should handle empty sequences gracefully. - Implement basic error-handling for type-checking during initialization and operations. Performance: - Ensure that operations like indexing and iteration are efficient. - Avoid excessive memory usage for large sequences.","solution":"from collections.abc import Sequence import asyncio class CustomSequence(Sequence): def __init__(self, *args): if not all(isinstance(arg, int) for arg in args): raise ValueError(\\"All elements must be integers.\\") self._data = list(args) def __getitem__(self, index): if isinstance(index, slice): return CustomSequence(*self._data[index]) return self._data[index] def __iter__(self): return iter(self._data) def __len__(self): return len(self._data) def __contains__(self, item): return item in self._data def total(self): return sum(self._data) def __repr__(self): return f\\"CustomSequence({\', \'.join(map(str, self._data))})\\" async def async_total(self): return sum(self._data)"},{"question":"**Objective:** Your task is to write a Python script that utilizes the `importlib.metadata` module to gather and display comprehensive information about a specified installed package. **Function Implementation:** Implement a function called `package_info(package_name: str) -> dict` that takes the name of a package as input and returns a dictionary with the following structure: ```python { \\"version\\": str, \\"entry_points\\": { \\"group_name\\": [ { \\"name\\": str, \\"value\\": str }, ... ], ... }, \\"metadata\\": dict, \\"files\\": [ { \\"file_path\\": str, \\"size\\": int, \\"hash\\": str }, ... ], \\"requirements\\": list } ``` **Input:** - `package_name` (str): The name of the package to query. **Output:** - A dictionary containing: - `version`: The version of the package. - `entry_points`: A dictionary where keys are entry point groups and values are lists of dictionaries containing the name and value of each entry point. - `metadata`: The metadata of the package. - `files`: A list of dictionaries, each containing the file path, size, and hash. - `requirements`: A list of the package requirements. **Constraints:** - Assume the package name provided is valid and the package is installed in the environment. **Performance Requirements:** - The solution should be efficient and strive to minimize I/O operations where possible. **Example:** ```python package_info(\\"wheel\\") ``` Should return (content will vary based on the installed version): ```python { \\"version\\": \\"0.32.3\\", \\"entry_points\\": { \\"console_scripts\\": [ { \\"name\\": \\"wheel\\", \\"value\\": \\"wheel.cli:main\\" } ], ... }, \\"metadata\\": { \\"Metadata-Version\\": \\"2.1\\", ... }, \\"files\\": [ { \\"file_path\\": \\"wheel/util.py\\", \\"size\\": 859, \\"hash\\": \\"bYkw5oMccfazVCoYQwKkkemoVyMAFoR34mmKBx8R1NI\\" }, ... ], \\"requirements\\": [ \\"pytest (>=3.0.0) ; extra == \'test\'\\", \\"pytest-cov ; extra == \'test\'\\" ] } ``` Use the provided `importlib.metadata` functionalities to complete the task. **Note:** - Please refer to the documentation provided on `importlib.metadata` for understanding the use of various functions and classes needed for this task.","solution":"import importlib.metadata import hashlib def package_info(package_name: str) -> dict: Gather and display comprehensive information about a specified installed package. Args: - package_name: The name of the package to query. Returns: - A dictionary containing comprehensive information about the package. try: distribution = importlib.metadata.distribution(package_name) version = distribution.version entry_points = {} for entry_point in distribution.entry_points: group = entry_point.group if group not in entry_points: entry_points[group] = [] entry_points[group].append({\\"name\\": entry_point.name, \\"value\\": entry_point.value}) metadata = {k: v for k, v in distribution.metadata.items()} files = [] for file in distribution.files: file_path = str(file) with open(file.locate(), \'rb\') as f: file_bytes = f.read() file_size = len(file_bytes) file_hash = hashlib.sha256(file_bytes).hexdigest() files.append({ \\"file_path\\": file_path, \\"size\\": file_size, \\"hash\\": file_hash }) requirements = list(distribution.requires or []) return { \\"version\\": version, \\"entry_points\\": entry_points, \\"metadata\\": metadata, \\"files\\": files, \\"requirements\\": requirements } except importlib.metadata.PackageNotFoundError: return {\\"error\\": f\\"Package \'{package_name}\' not found\\"}"},{"question":"# Objective Demonstrate your understanding and usage of the `unittest.mock` module in Python. # Problem Statement You are tasked with developing unit tests for a small service that manages user sessions. The service has two main components: 1. A `Database` class to interact with the database. 2. A `SessionManager` class to handle user sessions. Write tests that ensure `SessionManager` correctly interacts with the `Database`. # Requirements 1. Implement a `Database` class with methods: - `get_user(username: str) -> dict`: Fetch a user\'s record. - `create_session(user_id: int) -> str`: Create a new session for the user and return a session token. - `terminate_session(session_token: str) -> bool`: Terminate a session and return whether it was successful. 2. Implement a `SessionManager` class with methods: - `login(username: str, password: str) -> str`: Authenticate the user by fetching their record from `Database` and creating a new session if authentication is successful. Returns the session token. - `logout(session_token: str) -> bool`: Terminate the given session by calling the appropriate `Database` method. 3. Write comprehensive unit tests for `SessionManager` using the `unittest.mock` module to mock interactions with `Database`. # Constraints - Focus on interaction checks and flow rather than actual data processing. - Use mock assertions to confirm method calls and their arguments. # Example ```python # Sample classes implementation class Database: def get_user(self, username): # Implementation pass def create_session(self, user_id): # Implementation pass def terminate_session(self, session_token): # Implementation pass class SessionManager: def __init__(self, db): self.db = db def login(self, username, password): user = self.db.get_user(username) if user and user[\'password\'] == password: return self.db.create_session(user[\'id\']) return None def logout(self, session_token): return self.db.terminate_session(session_token) # Your task: Write tests using unittest.mock import unittest from unittest.mock import Mock, patch class TestSessionManager(unittest.TestCase): # Implement your tests here # Example skeleton for a test method def test_login_success(self): # Arrange mock_db = Mock(spec=Database) manager = SessionManager(mock_db) # Act # ... # Assert # ... # Implement other tests similarly if __name__ == \'__main__\': unittest.main() ``` **Note: Your solution should include creating the test cases and assertions to mock the database interactions properly.**","solution":"class Database: def get_user(self, username): Fetches a user\'s record from the database. pass def create_session(self, user_id): Creates a new session for the user and returns a session token. pass def terminate_session(self, session_token): Terminates a session and returns whether it was successful. pass class SessionManager: def __init__(self, db): self.db = db def login(self, username, password): user = self.db.get_user(username) if user and user[\'password\'] == password: return self.db.create_session(user[\'id\']) return None def logout(self, session_token): return self.db.terminate_session(session_token)"},{"question":"# Problem: Multi-threaded Task Scheduler Using Priority Queue You are tasked with implementing a multi-threaded task scheduler simulation using the `queue.PriorityQueue` class from the `queue` module. This scheduler will handle tasks with different priorities and ensure that higher-priority tasks get executed before lower-priority ones. Requirements: 1. You need to implement a function `task_scheduler(tasks: List[Tuple[int, str]], num_workers: int) -> List[str]` where: - `tasks`: A list of tuples where each tuple contains an integer representing the priority of the task (lower number means higher priority) and a string representing the task detail. - `num_workers`: An integer representing the number of worker threads that will process tasks. 2. Each worker should process tasks from the priority queue and execute them, simulating task execution by appending the task detail to a shared result list after a slight delay to mimic real-world processing. 3. The function should return the list of task details in the order they were executed. Constraints: - You must use the `queue.PriorityQueue` for task management. - Each worker should continually fetch tasks from the queue until it is empty. - Ensure thread safety while appending to the results list. Example: ```python from queue import PriorityQueue import threading import time def task_scheduler(tasks: List[Tuple[int, str]], num_workers: int) -> List[str]: # Implementation here. ``` Example usage: ```python tasks = [(2, \\"low priority task A\\"), (1, \\"high priority task B\\"), (3, \\"low priority task C\\")] num_workers = 2 result = task_scheduler(tasks, num_workers) print(result) # Possible output: [\\"high priority task B\\", \\"low priority task A\\", \\"low priority task C\\"] ``` **Hint:** - Use the threading module to create and start worker threads. - The `time.sleep()` function can be used to simulate a delay in task execution. - Proper synchronization mechanisms should be used to handle shared resources among threads. Performance: - Ensure that the implementation efficiently handles a reasonably large number of tasks (e.g., up to 1000 tasks) with multiple worker threads.","solution":"from queue import PriorityQueue import threading import time from typing import List, Tuple def task_scheduler(tasks: List[Tuple[int, str]], num_workers: int) -> List[str]: task_queue = PriorityQueue() for priority, task in tasks: task_queue.put((priority, task)) result = [] result_lock = threading.Lock() def worker(): while not task_queue.empty(): priority, task = task_queue.get() time.sleep(0.01) # simulate task execution delay with result_lock: result.append(task) task_queue.task_done() threads = [] for _ in range(num_workers): thread = threading.Thread(target=worker) thread.start() threads.append(thread) for thread in threads: thread.join() return result"},{"question":"Objective: Use the `webbrowser` module to create a function that accepts a list of URLs and automates opening them in specific ways. Problem Statement: Write a Python function called `open_multiple_urls` that takes the following parameters: - `urls`: A list of URLs (strings) that need to be opened. - `new_window`: A boolean flag indicating whether to open each URL in a new window (`True`) or a new tab (`False`) if possible. Default is `False`. The function should: 1. Open each URL in the user\'s default web browser. 2. If `new_window` is `True`, each URL should open in a new window. 3. If `new_window` is `False`, each URL should open in a new tab of the same browser window. Example: ```python urls = [ \\"https://www.python.org\\", \\"https://www.github.com\\", \\"https://www.stackoverflow.com\\" ] open_multiple_urls(urls, new_window=False) ``` Constraints: 1. You must use the `webbrowser` module. 2. Handle any potential exceptions that might occur when opening a URL. 3. Ensure that your function works on common platforms (Windows, macOS, Unix). Expected Function Signature: ```python def open_multiple_urls(urls: list, new_window: bool = False) -> None: # Your code here ``` Notes: - Performance is not a primary concern for this task, but the function should handle at least 10 URLs reasonably. - If there is an error opening any URL, print a friendly error message that includes the URL.","solution":"import webbrowser def open_multiple_urls(urls: list, new_window: bool = False) -> None: Opens a list of URLs in the user\'s default web browser. Parameters: urls (list): A list of URLs to be opened. new_window (bool): A boolean flag indicating whether to open each URL in a new window or a new tab. Default is False. for url in urls: try: if new_window: webbrowser.open(url, new=1) # Open in a new browser window else: webbrowser.open(url, new=2) # Open in a new tab except Exception as e: print(f\\"Failed to open {url}: {e}\\")"},{"question":"# Custom Sqrt Function with PyTorch In this coding problem, you are required to implement a custom square root function using `torch.autograd.Function`. Your implementation should include forward and backward methods and correctly save necessary context. You will also use NumPy for the actual computation of the square root in the forward pass. Requirements 1. Implement the `MySqrt` function as a subclass of `torch.autograd.Function`. 2. The `forward` method should: - Accept a tensor `x` (without a context object `ctx`). - Use NumPy to compute the square root of each element in `x`. - Return the result as a torch tensor. 3. The `setup_context` method should: - Save the necessary information required for the backward pass into the `ctx`. 4. The `backward` method should: - Accept the context object `ctx` and the gradient of the loss with respect to the output tensor from the forward method. - Compute the gradient of the loss with respect to the input tensor `x`. 5. Implement a helper function `my_sqrt` to wrap the `MySqrt` operation for easy use. 6. Use `torch.func.grad` to verify the correctness of the implemented function. Functions ```python import torch import numpy as np class MySqrt(torch.autograd.Function): @staticmethod def forward(x): # Use NumPy to compute square root result = np.sqrt(x.cpu().numpy()) return torch.tensor(result, device=x.device) @staticmethod def setup_context(ctx, inputs, output): x, = inputs result, = output ctx.save_for_backward(result) @staticmethod def backward(ctx, grad_output): result, = ctx.saved_tensors grad_input = grad_output / (2 * result) return grad_input def my_sqrt(x): return MySqrt.apply(x) ``` Sanity Check The function should correctly handle the computation of the square root and its gradient. Write code to test the following: * The correctness of the result when computing the square root. * The correctness of the calculated gradients using `torch.func.grad`. Example: ```python x = torch.randn(3, requires_grad=True) y = my_sqrt(x) y_grad = torch.func.grad(lambda x: my_sqrt(x).sum())(x) # Print the results print(y) print(y_grad) ``` Make sure the gradients are correctly computed considering the function `y = sqrt(x)` and its derivative.","solution":"import torch import numpy as np class MySqrt(torch.autograd.Function): @staticmethod def forward(ctx, x): # Use NumPy to compute square root result = np.sqrt(x.cpu().numpy()) ctx.save_for_backward(x) return torch.tensor(result, device=x.device) @staticmethod def backward(ctx, grad_output): x, = ctx.saved_tensors grad_input = grad_output / (2 * torch.sqrt(x)) return grad_input def my_sqrt(x): return MySqrt.apply(x)"},{"question":"Managing Context-Local State You are given a class `ContextManager` that manages sensitive information which should be accessed or modified only within a certain context. Implement the following two methods: 1. **initialize_context()**: - Initializes a new context that will manage context-local state for user preferences. - You should use the provided context variable and store some default preferences. 2. **update_preferences(preferences: dict)**: - Updates the user preferences within the initialized context. - Takes a dictionary `preferences` with key-value pairs representing the preferences to be updated. The function should only update those preferences within the context. 3. **get_preference(key: str)**: - Retrieves the value of a preference for the given key within the current context. - If the key does not exist in the current context, the function should return `\\"Preference not set\\"`. Here is the structure of the class you need to complete: ```python import contextvars from typing import Any, Dict class ContextManager: preferences_var: contextvars.ContextVar[Dict[str, Any]] def __init__(self): self.preferences_var = contextvars.ContextVar(\'preferences_var\') def initialize_context(self): Initializes the context with a set of default preferences. default_preferences = {\\"theme\\": \\"dark\\", \\"language\\": \\"en\\"} self.preferences_var.set(default_preferences) def update_preferences(self, preferences: Dict[str, Any]): Updates the preferences in the current context. :param preferences: A dictionary of preferences to update. current_preferences = self.preferences_var.get().copy() current_preferences.update(preferences) self.preferences_var.set(current_preferences) def get_preference(self, key: str) -> Any: Retrieves the preference value for the given key from the current context. :param key: The preference key to retrieve. :return: The value of the preference or \\"Preference not set\\" if the key does not exist. preferences = self.preferences_var.get() return preferences.get(key, \\"Preference not set\\") ``` # Constraints: - Assume that the default preferences are always a dictionary with keys \\"theme\\" and \\"language\\". - You can assume that the `initialize_context` method is always called before `update_preferences` and `get_preference`. - You should use `contextvars` module functionalities to manage context-specific variables. # Example Usage: ```python cm = ContextManager() cm.initialize_context() print(cm.get_preference(\\"theme\\")) # Output: \\"dark\\" cm.update_preferences({\\"theme\\": \\"light\\"}) print(cm.get_preference(\\"theme\\")) # Output: \\"light\\" print(cm.get_preference(\\"font_size\\")) # Output: \\"Preference not set\\" ``` This problem requires the students to demonstrate their understanding of the `contextvars` module to manage context-local state effectively.","solution":"import contextvars from typing import Any, Dict class ContextManager: preferences_var: contextvars.ContextVar[Dict[str, Any]] def __init__(self): self.preferences_var = contextvars.ContextVar(\'preferences_var\') def initialize_context(self): Initializes the context with a set of default preferences. default_preferences = {\\"theme\\": \\"dark\\", \\"language\\": \\"en\\"} self.preferences_var.set(default_preferences) def update_preferences(self, preferences: Dict[str, Any]): Updates the preferences in the current context. :param preferences: A dictionary of preferences to update. current_preferences = self.preferences_var.get().copy() current_preferences.update(preferences) self.preferences_var.set(current_preferences) def get_preference(self, key: str) -> Any: Retrieves the preference value for the given key from the current context. :param key: The preference key to retrieve. :return: The value of the preference or \\"Preference not set\\" if the key does not exist. preferences = self.preferences_var.get() return preferences.get(key, \\"Preference not set\\")"},{"question":"**Objective:** You are required to write Python functions that utilize the `importlib.metadata` module to retrieve and process metadata of installed packages in a Python environment. **Problem Statement:** 1. **Function Name:** `get_package_version` - **Input:** A string `package_name` representing the name of the package. - **Output:** A string representing the version of the specified package. - **Constraints:** If the package is not installed, return a string \\"Package not found\\". 2. **Function Name:** `list_entry_points` - **Input:** A string `package_name` representing the name of the package. - **Output:** A dictionary where the keys are entry point groups and the values are lists of entry point names associated with those groups. - **Constraints:** If the package has no entry points, return an empty dictionary. 3. **Function Name:** `get_distribution_files` - **Input:** A string `package_name` representing the name of the package. - **Output:** A list of strings representing paths of all files installed by the package. - **Constraints:** If the package has no files or is not installed, return an empty list. **Implementation:** Implement these three functions using the `importlib.metadata` module. **Example:** ```python # Example for get_package_version print(get_package_version(\\"wheel\\")) # Output: \\"0.32.3\\" (This would depend on the installed version in your environment) # Example for list_entry_points print(list_entry_points(\\"wheel\\")) # Output: # { # \\"console_scripts\\": [\\"wheel\\"], # \\"distutils.commands\\": [...], # ... # } # Example for get_distribution_files print(get_distribution_files(\\"wheel\\")) # Output: # [ # \\"path_to/wheel/util.py\\", # \\"path_to/wheel/__init__.py\\", # ... # ] ``` **Notes:** - Handle exceptions appropriately to ensure the functions do not crash if the package is not found or if there are issues retrieving the metadata. - Your code should be written efficiently to handle large amounts of metadata if needed. **Testing:** Write test cases to validate the correctness of your functions. Consider edge cases like non-existent packages, packages without entry points or files, etc.","solution":"from importlib.metadata import version, entry_points, files, PackageNotFoundError def get_package_version(package_name): Returns the version of the specified package. If the package is not found, returns \\"Package not found\\". try: return version(package_name) except PackageNotFoundError: return \\"Package not found\\" def list_entry_points(package_name): Returns a dictionary of entry points for the specified package. If the package has no entry points, returns an empty dictionary. try: entry_points_dict = {} eps = entry_points() for ep in eps.select(group=package_name): entry_points_dict.setdefault(ep.group, []).append(ep.name) return entry_points_dict except PackageNotFoundError: return {} def get_distribution_files(package_name): Returns a list of file paths installed by the specified package. If the package is not found or has no files, returns an empty list. try: return [str(file) for file in files(package_name)] except PackageNotFoundError: return []"},{"question":"Implement a Python function that takes in two sets and performs a series of operations to produce a final transformed set. You need to demonstrate your comprehension of fundamental set operations using the provided API. The function should work in the following way: 1. **Union** the elements of the two sets. 2. **Filter** out any element that is not an integer. 3. **Create** a frozenset from the filtered set. 4. **Return** the frozenset. Your task is to write a function `transform_sets(set1, set2)` that implements the above steps. Function Signature ```python def transform_sets(set1: set, set2: set) -> frozenset: pass ``` Parameters - `set1 (set)`: A set of mixed data types. - `set2 (set)`: Another set of mixed data types. Returns - `frozenset`: A frozen set containing only integers from the union of the two sets. Example ```python set1 = {1, 2, \'a\', (1, 2), 9} set2 = {2, 3, \'b\', (2, 3), 4} result = transform_sets(set1, set2) print(result) # Output: frozenset({1, 2, 3, 4, 9}) ``` Constraints - Elements in `set1` and `set2` can be of different data types, including integers, strings, tuples, and other hashable types. - The function should not use any built-in set operations directly, but rely on the documented set API. Notes - Use the following functions and macros from the provided documentation: `PySet_Contains`, `PySet_Add`, `PySet_Size`, and `PySet_New`. - Handle any necessary error checking as per the documentation when manipulating the sets. - Consider creating intermediary sets to assist with each step of the transformation process. Performance Requirements - The function should handle sets of up to 10^4 elements efficiently. - Consider memory overheads when manipulating set elements and creating intermediary sets.","solution":"def transform_sets(set1, set2): Takes two sets, unions them, filters out non-integer elements, and returns a frozenset of the filtered integers. # Step 1: Union the elements of the two sets union_set = set1.union(set2) # Step 2: Filter out any element that is not an integer filtered_set = {elem for elem in union_set if isinstance(elem, int)} # Step 3: Create a frozenset from the filtered set frozen_result = frozenset(filtered_set) # Step 4: Return the frozenset return frozen_result"},{"question":"You have been given the task to analyze the relationship between different categories and a numerical value in a dataset. You are required to create a detailed, well-customized visualization using the `seaborn` library that includes multiple plots and customized themes. # Problem Statement Write a Python function `visualize_data` that performs the following tasks: 1. **Input**: The function should take in a pandas DataFrame with at least the following columns: `category` (which contains categorical data) and `value` (which contains numerical data). 2. **Output**: The function should generate and display a grid of plots using `seaborn`. The grid should include at least: - A barplot showing the average value for each category. - A boxplot showing the distribution of the values within each category. 3. **Customization Requirements**: - Use the `whitegrid` style and a `pastel` palette for the plots. - Customize the axis spines to only show the left and bottom spines. - Ensure the barplot has appropriate labels and a title. - Ensure the boxplot has appropriate labels and a title. 4. **Constraints**: - Assume there are at least three unique categories and at least 10 data points for a meaningful visualization. 5. **Performance Requirements**: The function should handle DataFrame with size up to 1 million rows efficiently. # Example ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Sample DataFrame data = pd.DataFrame({ \\"category\\": [\\"A\\", \\"A\\", \\"B\\", \\"B\\", \\"C\\", \\"C\\", \\"A\\", \\"B\\", \\"C\\", \\"C\\"], \\"value\\": [10, 20, 30, 40, 50, 60, 70, 80, 90, 100] }) # Function Implementation def visualize_data(df): sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\") custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} sns.set_theme(style=\\"ticks\\", rc=custom_params) # Create subplots fig, ax = plt.subplots(1, 2, figsize=(15, 7)) # Barplot sns.barplot(x=\\"category\\", y=\\"value\\", data=df, ax=ax[0]) ax[0].set_title(\\"Average Value by Category\\") ax[0].set_xlabel(\\"Category\\") ax[0].set_ylabel(\\"Average Value\\") # Boxplot sns.boxplot(x=\\"category\\", y=\\"value\\", data=df, ax=ax[1]) ax[1].set_title(\\"Value Distribution by Category\\") ax[1].set_xlabel(\\"Category\\") ax[1].set_ylabel(\\"Value\\") plt.show() # Call the function with the sample DataFrame visualize_data(data) ``` Your implementation should follow the above example, ensuring that the visualizations are created according to the specifications.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_data(df): Generates a grid of seaborn plots to visualize the relationship between categorical and numerical data in the provided DataFrame. Parameters: df (pd.DataFrame): DataFrame with \'category\' and \'value\' columns. Returns: None # Validate the DataFrame if not {\'category\', \'value\'}.issubset(df.columns): raise ValueError(\\"DataFrame must contain \'category\' and \'value\' columns\\") if df[\'category\'].nunique() < 3: raise ValueError(\\"DataFrame must contain at least three unique categories\\") if len(df) < 10: raise ValueError(\\"DataFrame must contain at least 10 data points\\") # Set seaborn theme and style sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\") custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} sns.set_theme(style=\\"ticks\\", rc=custom_params) # Create subplots fig, ax = plt.subplots(1, 2, figsize=(15, 7)) # Barplot sns.barplot(x=\\"category\\", y=\\"value\\", data=df, ax=ax[0], ci=None) ax[0].set_title(\\"Average Value by Category\\") ax[0].set_xlabel(\\"Category\\") ax[0].set_ylabel(\\"Average Value\\") # Boxplot sns.boxplot(x=\\"category\\", y=\\"value\\", data=df, ax=ax[1]) ax[1].set_title(\\"Value Distribution by Category\\") ax[1].set_xlabel(\\"Category\\") ax[1].set_ylabel(\\"Value\\") plt.show()"},{"question":"Objective Design an enumeration class representing types of notifications in a software application. Each notification type should include a message and a priority level, which could be used for sorting or filtering. Use the `enum` module to implement this class, leveraging the features described in the documentation above. Question 1. Create an `Enum` class named `NotificationType` with the following members: - `INFO` with the message \\"Informational message\\" and priority level 1. - `WARNING` with the message \\"Warning message\\" and priority level 2. - `ERROR` with the message \\"Error message\\" and priority level 3. - `CRITICAL` with the message \\"Critical message\\" and priority level 4. 2. Implement the following methods in the `NotificationType` class: - `get_message(self)`: Return the message of the notification type. - `get_priority(self)`: Return the priority level of the notification type. 3. Implement a class method `sorted_by_priority(cls, reverse=False)` that returns a list of notification types sorted by their priority levels in ascending or descending order based on the `reverse` parameter. Constraints - Use the `enum` module\'s features like `auto`, custom `__new__()` method, etc., where they make sense. - Ensure the class maintains the unique values for each member. - The sorting method should be efficient even for larger sets of notification types. Example Usage ```python from enum import Enum # Your implementation here # Example NotificationType Enum for notification in NotificationType: print(notification.name, notification.get_message(), notification.get_priority()) # Example sorted list sorted_notifications = NotificationType.sorted_by_priority() for notification in sorted_notifications: print(notification.name, notification.get_priority()) # Output should be in the correct order based on priority ``` Expected Output: ``` INFO Informational message 1 WARNING Warning message 2 ERROR Error message 3 CRITICAL Critical message 4 INFO 1 WARNING 2 ERROR 3 CRITICAL 4 ```","solution":"from enum import Enum class NotificationType(Enum): INFO = (\\"Informational message\\", 1) WARNING = (\\"Warning message\\", 2) ERROR = (\\"Error message\\", 3) CRITICAL = (\\"Critical message\\", 4) def __new__(cls, message, priority): obj = object.__new__(cls) obj._value_ = priority obj.message = message obj.priority = priority return obj def get_message(self): return self.message def get_priority(self): return self.priority @classmethod def sorted_by_priority(cls, reverse=False): return sorted(cls, key=lambda x: x.priority, reverse=reverse)"},{"question":"# Advanced Python Coding Challenge Objective Implement a custom `namedtuple`-like class in Python. This class should provide similar functionality to Python\'s built-in `namedtuple`, but with added capabilities to dynamically add and remove fields. Requirements 1. Create a class `DynamicTuple` which imitates the behavior of `collections.namedtuple`. 2. The class should be initialized with a series of field names. 3. Provide methods to: - Access tuple items by index and name. - Dynamically add and remove fields. - Provide a string representation similar to `namedtuple`. 4. Ensure the class maintains the immutability properties of tuples as much as possible, even with added functionalities. Constraints 1. No additional third-party libraries should be used. 2. The solution should be efficient and maintain the order of elements as given. Example Usage ```python # Creating a DynamicTuple with initial fields Person = DynamicTuple(\'name age\') person = Person(name=\'John\', age=30) # Access by name and index print(person.name) # Output: John print(person[0]) # Output: John # String representation print(person) # Output: DynamicTuple(name=\'John\', age=30) # Adding a new field person.add_field(\'address\', \'123 Main St\') print(person) # Output: DynamicTuple(name=\'John\', age=30, address=\'123 Main St\') # Removing an existing field person.remove_field(\'age\') print(person) # Output: DynamicTuple(name=\'John\', address=\'123 Main St\') ``` Implementation You need to implement the `DynamicTuple` class with the specified methods and behavior. The class should allow dynamic addition and removal of fields and maintain immutability similar to tuples. ```python class DynamicTuple: def __init__(self, *field_names): # Initialize with given field names def __getitem__(self, index): # Allow item access by index def __getattr__(self, name): # Allow item access by field name def add_field(self, field_name, value): # Add a new field def remove_field(self, field_name): # Remove an existing field def __repr__(self): # Provide a string representation ``` Complete the class definition and ensure all methods are implemented as specified.","solution":"class DynamicTuple: def __init__(self, **kwargs): self._fields = list(kwargs.keys()) self._values = list(kwargs.values()) def __getitem__(self, index): return self._values[index] def __getattr__(self, name): if name in self._fields: index = self._fields.index(name) return self._values[index] raise AttributeError(f\\"\'DynamicTuple\' object has no attribute \'{name}\'\\") def add_field(self, field_name, value): if field_name in self._fields: raise ValueError(f\\"Field \'{field_name}\' already exists\\") self._fields.append(field_name) self._values.append(value) def remove_field(self, field_name): if field_name not in self._fields: raise ValueError(f\\"Field \'{field_name}\' does not exist\\") index = self._fields.index(field_name) self._fields.pop(index) self._values.pop(index) def __repr__(self): field_strs = [f\\"{name}={repr(value)}\\" for name, value in zip(self._fields, self._values)] return f\\"DynamicTuple({\', \'.join(field_strs)})\\""},{"question":"Custom ZIP Importer In this exercise, you are required to implement a custom ZIP import functionality without using the built-in `zipimport` module explicitly. Your task is to create a class `CustomZipImporter` that mimics essential functionalities of the `zipimporter` class. # Requirements 1. **Class Definition**: - Create a class named `CustomZipImporter`. - The constructor should accept the path to the ZIP file. 2. **Methods to Implement**: - `find_module(fullname)`: This method should return `True` if the module specified by `fullname` exists within the ZIP archive, otherwise, return `False`. - `get_code(fullname)`: This method should return the code object of the specified module if found, otherwise, raise a `CustomZipImportError`. - `load_module(fullname)`: This method should load and execute the specified module, returning the module object if successful, otherwise, raise a `CustomZipImportError`. 3. **Exception Handling**: - Define a custom exception `CustomZipImportError` which should be raised for any errors similar to `ZipImportError`. # Constraints - The ZIP archive will only contain `.py` files. - Only consider modules in the root of the ZIP file (no subdirectories). # Input and Output Format - Assume the ZIP file path provided to the constructor and the module names provided to methods are valid string paths. - Return values and exceptions must adhere to the specifications above. # Example Usage ```python import zipfile import types class CustomZipImportError(Exception): pass class CustomZipImporter: def __init__(self, archivepath): # Initialize with the path to the ZIP archive self.archivepath = archivepath # List to store names of modules within the ZIP file self.modules = [] # Open the ZIP file and gather module names with zipfile.ZipFile(self.archivepath, \'r\') as zip_file: for info in zip_file.infolist(): if info.filename.endswith(\'.py\'): self.modules.append(info.filename[:-3]) def find_module(self, fullname): # Return True if the module is found in the ZIP archive, else False return fullname in self.modules def get_code(self, fullname): # Return the code object for the specified module if fullname in self.modules: with zipfile.ZipFile(self.archivepath, \'r\') as zip_file: with zip_file.open(f\'{fullname}.py\') as file: source = file.read() return compile(source, f\'{fullname}.py\', \'exec\') else: raise CustomZipImportError(f\'Module {fullname} not found in ZIP\') def load_module(self, fullname): # Load and execute the module if not self.find_module(fullname): raise CustomZipImportError(f\'Module {fullname} not found in ZIP\') code = self.get_code(fullname) module = types.ModuleType(fullname) exec(code, module.__dict__) return module # Example usage and testing zip_importer = CustomZipImporter(\'example.zip\') print(zip_importer.find_module(\'jwzthreading\')) # Expected output: True module = zip_importer.load_module(\'jwzthreading\') print(module.__file__) # Expected output: \'example.zip/jwzthreading.py\' ``` # Notes - You may use the `zipfile` module to handle ZIP file operations. - Ensure exhaustive test coverage for verifying various cases and errors.","solution":"import zipfile import types class CustomZipImportError(Exception): pass class CustomZipImporter: def __init__(self, archivepath): self.archivepath = archivepath self.modules = [] with zipfile.ZipFile(self.archivepath, \'r\') as zip_file: for info in zip_file.infolist(): if info.filename.endswith(\'.py\'): self.modules.append(info.filename[:-3]) def find_module(self, fullname): return fullname in self.modules def get_code(self, fullname): if fullname in self.modules: with zipfile.ZipFile(self.archivepath, \'r\') as zip_file: with zip_file.open(f\'{fullname}.py\') as file: source = file.read() return compile(source, f\'{fullname}.py\', \'exec\') else: raise CustomZipImportError(f\'Module {fullname} not found in ZIP\') def load_module(self, fullname): if not self.find_module(fullname): raise CustomZipImportError(f\'Module {fullname} not found in ZIP\') code = self.get_code(fullname) module = types.ModuleType(fullname) exec(code, module.__dict__) return module"},{"question":"# Question: Implementing Graph Transformation Pass in PyTorch **Objective**: Given a graph module using PyTorch\'s FX module, implement a transformation pass that replaces addition operations with a sequence of multiplication followed by subtraction. Specifically, each `torch.ops.aten.add.Tensor(a, b)` operation should be replaced with the equivalent of `torch.ops.aten.sub.Tensor(torch.ops.aten.mul.Tensor(a, b), b)`. Additionally, validate the transformed graph by running a simple input through it to ensure correctness. Instructions 1. **Function Signature**: ```python def transform_add_to_mul_sub(graph_module: torch.fx.GraphModule) -> torch.fx.GraphModule: ``` 2. **Expected Input**: - `graph_module`: A `torch.fx.GraphModule` object representing a neural network in Intermediate Representation form. 3. **Expected Output**: - Returns a `torch.fx.GraphModule` object with the specified transformation applied. 4. **Constraints**: - Use the provided PyTorch `fx` functionality to traverse and manipulate nodes in the graph. - Ensure that the graph structure remains valid after transformation. - Performance should be optimized for handling graphs with up to 1000 nodes. 5. **Steps to Follow**: - Define a custom transformer class inheriting from `torch.fx.Transformer`. - Override the `call_function` method to implement the transformation from addition to multiplication followed by subtraction. - Instantiate this transformer and apply it on the provided `graph_module`. - Validate transformations by creating a simple example graph, applying the transformation, and comparing outputs of the original and transformed graphs on a test input. Example Consider the following example graph module before and after transformation: **Original Graph**: ```python class SimpleAddModule(torch.nn.Module): def forward(self, x, y): return torch.ops.aten.add.Tensor(x, y) traced_module = torch.fx.symbolic_trace(SimpleAddModule()) ``` **Transformed Graph**: ```python class TransformedModule(torch.nn.Module): def forward(self, x, y): z = torch.ops.aten.mul.Tensor(x, y) return torch.ops.aten.sub.Tensor(z, y) transformed_module = transform_add_to_mul_sub(traced_module) ``` To validate the transformation: 1. Run both the original and transformed modules with a sample input `x = torch.tensor(2)` and `y = torch.tensor(3)`. 2. Ensure that both outputs are equivalent. The function implementation should cover the graph transformation logic as well as a simple validation script as outlined above. **Please write the `transform_add_to_mul_sub` function below:**","solution":"import torch import torch.fx def transform_add_to_mul_sub(graph_module: torch.fx.GraphModule) -> torch.fx.GraphModule: class AddToMulSubTransformer(torch.fx.Transformer): def call_function(self, target, args, kwargs): # Check if the function is the add.Tensor operation if target == torch.ops.aten.add.Tensor: a, b = args # First multiply a and b, then subtract b from the result mul_result = torch.ops.aten.mul.Tensor(a, b) sub_result = torch.ops.aten.sub.Tensor(mul_result, b) return sub_result return super().call_function(target, args, kwargs) transformer = AddToMulSubTransformer(graph_module) transformed_graph_module = transformer.transform() return transformed_graph_module"},{"question":"Objective To test your understanding of the `urllib` module in Python and your ability to manipulate HTTP requests and URL parsing. Problem Statement You are required to write a function that takes a list of URLs as input and returns a dictionary where the keys are the hostnames of these URLs and the values are lists of paths from the URLs that belong to that hostname. Function Signature ```python def organize_urls(urls: List[str]) -> Dict[str, List[str]]: pass ``` Input - A list of URLs in string format. ```python urls = [ \\"https://example.com/path/to/page1\\", \\"http://example.com/path/to/page2\\", \\"https://another-site.org/home\\", \\"http://example.com/another/path\\" ] ``` Output - A dictionary where the keys are hostnames and values are lists of the paths. ```python { \\"example.com\\": [\\"/path/to/page1\\", \\"/path/to/page2\\", \\"/another/path\\"], \\"another-site.org\\": [\\"/home\\"] } ``` Requirements 1. Use the `urllib.parse` module to parse the URLs. 2. Ensure to include URLs with both `http` and `https` protocols. 3. Maintain the order of paths as they appeared in the input list for each hostname. Example ```python urls = [ \\"https://example.com/path/to/page1\\", \\"http://example.com/path/to/page2\\", \\"https://another-site.org/home\\", \\"http://example.com/another/path\\" ] print(organize_urls(urls)) # Output: # { # \\"example.com\\": [\\"/path/to/page1\\", \\"/path/to/page2\\", \\"/another/path\\"], # \\"another-site.org\\": [\\"/home\\"] # } ``` Constraints 1. Assume the list of URLs provided is non-empty. 2. Handle any valid URL formats as per the standard.","solution":"from urllib.parse import urlparse from typing import List, Dict def organize_urls(urls: List[str]) -> Dict[str, List[str]]: result = {} for url in urls: parsed_url = urlparse(url) hostname = parsed_url.hostname path = parsed_url.path if hostname in result: result[hostname].append(path) else: result[hostname] = [path] return result"},{"question":"Objective Develop a system to model dynamically changing configurations and manage safe access to these configurations using context management. Problem Statement You need to create a `ConfigManager` class that manages key-value configuration settings. This class should use a `dataclass` to store the configuration settings and should provide context management to ensure safe access to these settings. Specifically: 1. Implement the `ConfigManager` class with the following methods: - `set_config(key: str, value: Any)`: Sets the configuration value for the given key. - `get_config(key: str) -> Any`: Retrieves the configuration value for the given key. - `remove_config(key: str)`: Removes the given key from settings. 2. Ensure that `ConfigManager` uses a `@dataclass` to define the structure for configuration settings. 3. Provide a context manager in `ConfigManager` that: - Temporarily stores the configuration within the context. - Reverts to the previous configuration after the context is exited. Input Format - The `ConfigManager` class will be instantiated without any arguments. - Methods will be called with respective parameters to modify and access configurations. Output Format - `get_config(key: str)` should return the stored value. - Context management should ensure changes within the context are isolated and rollback after use. Constraints - The `set_config`, `get_config`, and `remove_config` should operate in constant time. - The context manager should handle nested context scenarios correctly. Example ```python from contextlib import contextmanager from dataclasses import dataclass @dataclass class Config: pass class ConfigManager: def __init__(self): self._config = Config() self._config_stack = [] def set_config(self, key: str, value: Any): setattr(self._config, key, value) def get_config(self, key: str) -> Any: return getattr(self._config, key, None) def remove_config(self, key: str): if hasattr(self._config, key): delattr(self._config, key) @contextmanager def temporary_config(self, key: str, value: Any): # Save current configuration self._config_stack.append((key, getattr(self._config, key, None))) self.set_config(key, value) try: yield finally: previous_value = self._config_stack.pop() if previous_value[1] is None: self.remove_config(previous_value[0]) else: self.set_config(previous_value[0], previous_value[1]) # Usage Example config_manager = ConfigManager() config_manager.set_config(\'timeout\', 50) print(config_manager.get_config(\'timeout\')) # Output: 50 with config_manager.temporary_config(\'timeout\', 100): print(config_manager.get_config(\'timeout\')) # Output: 100 print(config_manager.get_config(\'timeout\')) # Output: 50 ``` Evaluation Criteria 1. Correctness: The implementation should correctly handle configurations and context management as described. 2. Efficiency: The methods should operate in constant time. 3. Robustness: The context manager should handle exceptions and nested contexts correctly. 4. Clarity: Code should be well-structured and commented for readability.","solution":"from contextlib import contextmanager from dataclasses import dataclass, field from typing import Any, Dict @dataclass class Config: settings: Dict[str, Any] = field(default_factory=dict) class ConfigManager: def __init__(self): self._config = Config() self._config_stack = [] def set_config(self, key: str, value: Any): self._config.settings[key] = value def get_config(self, key: str) -> Any: return self._config.settings.get(key, None) def remove_config(self, key: str): if key in self._config.settings: del self._config.settings[key] @contextmanager def temporary_config(self, key: str, value: Any): # Save current configuration self._config_stack.append((key, self.get_config(key))) self.set_config(key, value) try: yield finally: previous_key, previous_value = self._config_stack.pop() if previous_value is None: self.remove_config(previous_key) else: self.set_config(previous_key, previous_value) # Usage Example: # config_manager = ConfigManager() # config_manager.set_config(\'timeout\', 50) # print(config_manager.get_config(\'timeout\')) # Output: 50 # with config_manager.temporary_config(\'timeout\', 100): # print(config_manager.get_config(\'timeout\')) # Output: 100 # print(config_manager.get_config(\'timeout\')) # Output: 50"},{"question":"# Question Create a Python function that simulates the behavior of a basic file operations library which includes functionalities to read, write, and delete a file. The library should handle various exceptions and issue appropriate warnings when necessary. Requirements: 1. **Function Name**: `basic_file_operations` 2. **Input**: A list of operations in the form of dictionaries. Each dictionary may represent one of the operations: read, write, or delete. For example: ```python [ {\'operation\': \'write\', \'filename\': \'test.txt\', \'content\': \'Hello, World!\'}, {\'operation\': \'read\', \'filename\': \'test.txt\'}, {\'operation\': \'delete\', \'filename\': \'test.txt\'} ] ``` 3. **Output**: A list of results corresponding to each operation. For `read`, return the file content. For `write` and `delete`, return appropriate messages. 4. **Exceptions to Handle**: - File not found: Raise a `FileNotFoundError`. - Permission errors: Raise a `PermissionError`. - Is a directory error: Raise an `IsADirectoryError`. - Other IO errors: Raise a generic `OSError`. - If trying to delete a non-existent file: Raise a warning. - If content is not given for a write operation: Raise a `ValueError`. Constraints: - The function should use appropriate Python exception handling mechanisms. - Use `warnings` library to issue warnings. - Must handle exceptions gracefully and continue executing subsequent operations. Function Signature: ```python import warnings def basic_file_operations(operations: list) -> list: # Your implementation here ``` # Example Usage ```python # Sample input operations = [ {\'operation\': \'write\', \'filename\': \'hello.txt\', \'content\': \'Hello, Python!\'}, {\'operation\': \'read\', \'filename\': \'hello.txt\'}, {\'operation\': \'delete\', \'filename\': \'hello.txt\'}, {\'operation\': \'delete\', \'filename\': \'hello.txt\'} # This should raise a warning ] # Function call results = basic_file_operations(operations) # Expected Output # [ # \'File hello.txt written successfully.\', # \'Hello, Python!\', # \'File hello.txt deleted successfully.\', # \'Warning: File hello.txt not found for deletion.\' # ] ``` # Notes: - The function should be robust and handle all mentioned exceptions. - You are allowed to create auxiliary functions if necessary. - Pay attention to the operation sequence and ensure that operations are executed in the given order.","solution":"import warnings import os def basic_file_operations(operations: list) -> list: results = [] for op in operations: try: if op[\'operation\'] == \'write\': if \'content\' not in op: raise ValueError(f\\"Content not provided for writing to file {op[\'filename\']}.\\") with open(op[\'filename\'], \'w\') as f: f.write(op[\'content\']) results.append(f\\"File {op[\'filename\']} written successfully.\\") elif op[\'operation\'] == \'read\': with open(op[\'filename\'], \'r\') as f: content = f.read() results.append(content) elif op[\'operation\'] == \'delete\': if os.path.exists(op[\'filename\']): os.remove(op[\'filename\']) results.append(f\\"File {op[\'filename\']} deleted successfully.\\") else: warnings.warn(f\\"File {op[\'filename\']} not found for deletion.\\") results.append(f\\"Warning: File {op[\'filename\']} not found for deletion.\\") except FileNotFoundError: results.append(f\\"Error: File {op[\'filename\']} not found.\\") except PermissionError: results.append(f\\"Error: Permission denied for file {op[\'filename\']}.\\") except IsADirectoryError: results.append(f\\"Error: {op[\'filename\']} is a directory.\\") except OSError as e: results.append(f\\"Error: OS error occurred with file {op[\'filename\']}. Details: {e}\\") except Exception as e: results.append(f\\"Unexpected error for file {op[\'filename\']}. Details: {e}\\") return results"},{"question":"**Question: Handling Deprecated Standard Library Modules** In this assessment, you must demonstrate knowledge of both deprecated Python modules and their modern replacements. The task involves handling raw audio data which was typically managed using the `audioop` module in older Python versions. # Task: 1. **Read and Manipulate Raw Audio Data:** - You are given a raw audio file (16-bit PCM format). Your task is to read this audio data, increase the volume by a factor of 2, and then write the adjusted audio back to a new raw audio file. - Typically, this manipulation would be done using the deprecated `audioop` module. However, you should accomplish this using the modern approach with libraries like **NumPy** and **Wave**. # Requirements: - The function should take two file paths as input arguments: 1. `input_file_path` (a string): Path to the input raw audio file. 2. `output_file_path` (a string): Path where the new, volume-adjusted raw audio file should be saved. - The function should read the input file, process the audio data to double its volume, and save the modified data to the output file. # Constraints: - You should assume that the audio file is always 16-bit PCM format. - Do not use the deprecated `audioop` module in your solution. - Handle errors gracefully, ensuring that the program provides meaningful error messages for issues like file not found or unsupported file formats. # Expected Output: - The function does not need to return any value. It should save the processed file at the specified output file path. # Example: Assume `input.raw` is a raw audio file in 16-bit PCM format. ```python def process_audio(input_file_path, output_file_path): # Your code here # Example usage: process_audio(\\"input.raw\\", \\"output.raw\\") ``` After running the above code, `output.raw` should be the modified audio where the volume of `input.raw` is doubled. # Note: - You may need to use `numpy` for mathematical operations on the audio data and `wave` or another suitable library for reading and writing raw audio data in Python.","solution":"import numpy as np def process_audio(input_file_path, output_file_path): try: # Read the raw audio data with open(input_file_path, \'rb\') as infile: audio_data = infile.read() # Convert the byte data to numpy array of 16-bit PCM audio_array = np.frombuffer(audio_data, dtype=np.int16) # Double the volume audio_array = audio_array * 2 # Ensure we stay within the 16-bit range audio_array = np.clip(audio_array, -32768, 32767) # Convert numpy array back to bytes adjusted_audio_data = audio_array.astype(np.int16).tobytes() # Write the adjusted audio data to the output file with open(output_file_path, \'wb\') as outfile: outfile.write(adjusted_audio_data) except FileNotFoundError: print(\\"Error: The specified input file was not found.\\") except Exception as e: print(f\\"An error occurred: {e}\\") # Example usage: # process_audio(\\"input.raw\\", \\"output.raw\\")"},{"question":"# Advanced Text Processing with Regular Expressions The `re` module in Python allows for powerful text processing using regular expressions. This question will assess your ability to utilize this module effectively. Problem Statement You are given a text document (string) and you need to implement a function `extract_compatible_dates_and_emails` that identifies all dates and email addresses within the text. Due to different formats found in texts, the function should be able to recognize: - Dates in the formats: `DD-MM-YYYY`, `DD/MM/YYYY`, `DD Month YYYY` (e.g., 12-05-2023, 12/05/2023, 12 May 2023) - Email addresses that follow the standard format: username@domain.extension (e.g., user@example.com) Function Signature ```python def extract_compatible_dates_and_emails(text: str) -> dict: This function extracts dates and email addresses from the given text. Parameters: text (str): The input text containing dates and email addresses. Returns: dict: A dictionary with two keys \'dates\' and \'emails\'. Each value is a list of strings. pass ``` Example ```python text = Here are some dates and emails for your reference: - Event 1: 12-05-2023 - Event 2: 25 December 2022 - Event 3: 01/01/2024 - Contact us at: test.email@example.com and another.email@example.co.uk result = extract_compatible_dates_and_emails(text) print(result) # Expected output: # { # \'dates\': [\'12-05-2023\', \'25 December 2022\', \'01/01/2024\'], # \'emails\': [\'test.email@example.com\', \'another.email@example.co.uk\'] # } ``` Constraints - You may assume the text is not excessively long (performance considerations are minimal). - Do not use third-party libraries; only the `re` module is allowed for this task. Requirements 1. Write regular expressions to match the specified date and email formats. 2. Use the `re.findall` method to extract these patterns from the input text. 3. Return the results in a dictionary containing keys \'dates\' and \'emails\', with lists of the corresponding patterns found.","solution":"import re def extract_compatible_dates_and_emails(text: str) -> dict: date_pattern = r\'b(?:d{2}[-/]d{2}[-/]d{4}|d{2} w+ d{4})b\' email_pattern = r\'b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+.[A-Z|a-z]{2,}b\' dates = re.findall(date_pattern, text) emails = re.findall(email_pattern, text) return { \'dates\': dates, \'emails\': emails }"},{"question":"Problem Statement You are required to implement a function that performs spectral analysis on a given time-domain signal using windowing techniques implemented in the `torch.signal.windows` module. Your function should generate a spectrogram of the signal, which is a visual representation of the spectrum of frequencies in the signal as they vary with time. Implement the following function: ```python import torch import torch.signal.windows def generate_spectrogram(signal, window_function, NFFT, noverlap): Generates a spectrogram from the given time-domain signal using the specified window function. Parameters: signal (torch.Tensor): 1D tensor representing the time-domain signal. window_function (str): The name of the window function to use (e.g., \'hann\', \'hamming\', \'blackman\'). NFFT (int): The number of data points used in each block for the Fast Fourier Transform (FFT). noverlap (int): The number of points of overlap between blocks. Returns: torch.Tensor: 2D tensor representing the spectrogram, with frequency bins on one axis and time bins on the other. pass ``` Input - `signal`: A 1D PyTorch tensor of shape `(N,)` representing the time-domain signal. - `window_function`: A string indicating the window function to use, one of `\'bartlett\'`, `\'blackman\'`, `\'cosine\'`, `\'exponential\'`, `\'gaussian\'`, `\'general_cosine\'`, `\'general_hamming\'`, `\'hamming\'`, `\'hann\'`, `\'kaiser\'`, `\'nuttall\'`. - `NFFT`: An integer specifying the number of data points used in each block for the FFT. - `noverlap`: An integer specifying the number of points of overlap between blocks. Output - Returns a 2D PyTorch tensor where the first dimension corresponds to the frequency bins and the second dimension corresponds to the time bins of the spectrogram. Constraints - Use the window function from `torch.signal.windows`. - Ensure that the implementation is efficient and handles common edge cases (e.g., the signal length is less than `NFFT`). Example ```python signal = torch.tensor([1.0, 2.0, 3.0, 4.0, 3.0, 2.0, 1.0, 0.0] * 10) window_function = \'hann\' NFFT = 8 noverlap = 4 spectrogram = generate_spectrogram(signal, window_function, NFFT, noverlap) print(spectrogram) ``` **Note:** The actual values inside the `spectrogram` will depend on the correct implementation of the spectrogram generation process. Additional Information - You may want to refer to the PyTorch and `torch.signal` documentation for additional details on window functions and tensor operations. Good luck!","solution":"import torch import torch.signal.windows as windows def generate_spectrogram(signal, window_function, NFFT, noverlap): Generates a spectrogram from the given time-domain signal using the specified window function. Parameters: signal (torch.Tensor): 1D tensor representing the time-domain signal. window_function (str): The name of the window function to use (e.g., \'hann\'). NFFT (int): The number of data points used in each block for the Fast Fourier Transform (FFT). noverlap (int): The number of points of overlap between blocks. Returns: torch.Tensor: 2D tensor representing the spectrogram, with frequency bins on one axis and time bins on the other. if window_function not in [\'bartlett\', \'blackman\', \'cosine\', \'exponential\', \'gaussian\', \'general_cosine\', \'general_hamming\', \'hamming\', \'hann\', \'kaiser\', \'nuttall\']: raise ValueError(f\\"Unsupported window function: {window_function}\\") if signal.ndim != 1: raise ValueError(\\"Input signal must be a 1D tensor\\") win = getattr(windows, window_function)(NFFT) step = NFFT - noverlap shape = (signal.numel() - noverlap) // step if shape <= 0: raise ValueError(\\"Signal length is too short for given NFFT and noverlap\\") spectrogram = [] for i in range(0, len(signal) - NFFT + 1, step): windowed_signal = signal[i:i+NFFT] * win spectrum = torch.fft.fft(windowed_signal).abs()[:NFFT//2 + 1] spectrogram.append(spectrum) return torch.stack(spectrogram, 1)"},{"question":"**Question: Movie Recommendations System** You are tasked with creating a movie recommendations system using a combination of lists, sets, and dictionaries. The system should store information about users, movies, and their ratings, and should be able to produce recommendations based on user interests and ratings. Here are the requirements and steps to follow: 1. **Data Structures:** - Use a dictionary to store user information. The keys should be user IDs (strings), and the values should be sets of movie IDs (strings) that the user has rated. - Use a dictionary to store movie information. The keys should be movie IDs (strings), and the values should be tuples with the movie title (string) and a list of ratings (integers). 2. **Functions to Implement:** 1. **add_user(user_id):** - Adds a new user with the given user ID to the system. Initialize their set of rated movies to an empty set. - Parameters: `user_id` (string) - Returns: None 2. **add_movie(movie_id, title):** - Adds a new movie with the given movie ID and title to the system. Initialize the list of ratings to an empty list. - Parameters: `movie_id` (string), `title` (string) - Returns: None 3. **rate_movie(user_id, movie_id, rating):** - Adds a rating for the specified movie by the specified user. Update the user\'s set of rated movies and the movie\'s list of ratings. - Parameters: `user_id` (string), `movie_id` (string), `rating` (integer between 1 and 5 inclusive) - Returns: None 4. **get_average_rating(movie_id):** - Calculates and returns the average rating for the specified movie. - Parameters: `movie_id` (string) - Returns: float 5. **recommend_movies(user_id):** - Recommends movies to the specified user based on the average rating, excluding movies that the user has already rated. Return a sorted list (by rating, descending) of tuples, where each tuple contains the movie title and its average rating. - Parameters: `user_id` (string) - Returns: List of tuples `(title, average_rating)` 3. **Constraints:** - Use only the built-in data types and methods as described in the documentation provided. - Ensure your solution is efficient and handles edge cases, such as users or movies not existing. **Example:** ```python # Initialize the system add_user(\\"user1\\") add_user(\\"user2\\") add_movie(\\"movie1\\", \\"Inception\\") add_movie(\\"movie2\\", \\"The Matrix\\") add_movie(\\"movie3\\", \\"Interstellar\\") # Rate movies rate_movie(\\"user1\\", \\"movie1\\", 5) rate_movie(\\"user1\\", \\"movie2\\", 4) rate_movie(\\"user2\\", \\"movie1\\", 3) rate_movie(\\"user2\\", \\"movie3\\", 5) # Get average rating for a movie print(get_average_rating(\\"movie1\\")) # Output: 4.0 # Get movie recommendations for a user print(recommend_movies(\\"user1\\")) # Output: [(\\"Interstellar\\", 5.0)] (since user1 has already rated \\"Inception\\" and \\"The Matrix\\") ``` **Note:** Ensure your functions handle cases where users or movies do not exist by raising appropriate exceptions or error messages. Document any assumptions you make while implementing the functions.","solution":"class MovieRecommendationSystem: def __init__(self): self.users = {} # Dictionary to store user information {user_id: set of movie_ids} self.movies = {} # Dictionary to store movie information {movie_id: (title, list of ratings)} def add_user(self, user_id): Adds a new user with the given user ID to the system. if user_id in self.users: raise ValueError(\\"User already exists.\\") self.users[user_id] = set() def add_movie(self, movie_id, title): Adds a new movie with the given movie ID and title to the system. if movie_id in self.movies: raise ValueError(\\"Movie already exists.\\") self.movies[movie_id] = (title, []) def rate_movie(self, user_id, movie_id, rating): Adds a rating for the specified movie by the specified user. if user_id not in self.users: raise ValueError(\\"User does not exist.\\") if movie_id not in self.movies: raise ValueError(\\"Movie does not exist.\\") if rating < 1 or rating > 5: raise ValueError(\\"Rating must be between 1 and 5.\\") self.users[user_id].add(movie_id) self.movies[movie_id][1].append(rating) def get_average_rating(self, movie_id): Calculates and returns the average rating for the specified movie. if movie_id not in self.movies: raise ValueError(\\"Movie does not exist.\\") ratings = self.movies[movie_id][1] if not ratings: return 0.0 return sum(ratings) / len(ratings) def recommend_movies(self, user_id): Recommends movies to the specified user based on the average rating. if user_id not in self.users: raise ValueError(\\"User does not exist.\\") rated_movies = self.users[user_id] recommendations = [] for movie_id, (title, ratings) in self.movies.items(): if movie_id not in rated_movies: average_rating = self.get_average_rating(movie_id) recommendations.append((title, average_rating)) recommendations.sort(key=lambda x: x[1], reverse=True) return recommendations # Example initialization # recommendation_system = MovieRecommendationSystem() # recommendation_system.add_user(\\"user1\\") # recommendation_system.add_user(\\"user2\\") # recommendation_system.add_movie(\\"movie1\\", \\"Inception\\") # recommendation_system.add_movie(\\"movie2\\", \\"The Matrix\\") # recommendation_system.add_movie(\\"movie3\\", \\"Interstellar\\") # recommendation_system.rate_movie(\\"user1\\", \\"movie1\\", 5) # recommendation_system.rate_movie(\\"user1\\", \\"movie2\\", 4) # recommendation_system.rate_movie(\\"user2\\", \\"movie1\\", 3) # recommendation_system.rate_movie(\\"user2\\", \\"movie3\\", 5) # print(recommendation_system.get_average_rating(\\"movie1\\")) # Output: 4.0 # print(recommendation_system.recommend_movies(\\"user1\\")) # Output: [(\\"Interstellar\\", 5.0)]"},{"question":"# Secure Hash Function Task **Objective:** Your task is to write a Python function that takes a list of file paths and returns a dictionary containing the file path as the key and the SHA-256 hash of the file content as the value. Additionally, you need to implement functionalities to save these hash values to a JSON file and then load the hash values back from the JSON file, ensuring data integrity. **Detailed Instructions:** 1. **Function Definition:** ```python def calculate_file_hashes(file_paths: List[str]) -> Dict[str, str]: Calculate SHA-256 hashes for the content of the given files. Args: file_paths (List[str]): A list of file paths to calculate hash values for. Returns: Dict[str, str]: A dictionary where the key is the file path and value is the SHA-256 hash of the file\'s content in hexadecimal format. pass ``` 2. **Additional Functionalities:** - **Save Hashes to JSON:** ```python def save_hashes_to_json(hashes: Dict[str, str], file_name: str) -> None: Save the file hashes dictionary to a JSON file. Args: hashes (Dict[str, str]): Dictionary containing file paths and their corresponding hashes. file_name (str): The name of the JSON file to save the hashes. pass ``` - **Load Hashes from JSON:** ```python def load_hashes_from_json(file_name: str) -> Dict[str, str]: Load the file hashes dictionary from a JSON file. Args: file_name (str): The name of the JSON file to load the hashes from. Returns: Dict[str, str]: Dictionary containing file paths and their corresponding hashes. pass ``` 3. **Data Integrity Check:** Implement a function to check the integrity of the files by comparing the current hash values with the saved hash values: ```python def check_data_integrity(file_paths: List[str], saved_hashes: Dict[str, str]) -> bool: Check the integrity of files by comparing the current hash values with the saved hash values. Args: file_paths (List[str]): A list of file paths to check. saved_hashes (Dict[str, str]): Dictionary containing the previously saved file paths and their hashes. Returns: bool: True if all file hashes match, False otherwise. pass ``` **Input Constraints:** - `file_paths` will always be a list of valid file paths. - Each file specified in `file_paths` will exist and be accessible. - The JSON file name provided to the save and load functions will always be valid and writable. **Performance Requirements:** - The functions should handle large files efficiently, utilizing chunk-based reading to avoid memory issues. # Example Usage: ```python file_paths = [\\"file1.txt\\", \\"file2.txt\\", \\"file3.txt\\"] hashes = calculate_file_hashes(file_paths) print(hashes) save_hashes_to_json(hashes, \\"file_hashes.json\\") loaded_hashes = load_hashes_from_json(\\"file_hashes.json\\") print(loaded_hashes) is_intact = check_data_integrity(file_paths, loaded_hashes) print(is_intact) # Should print True if files are unchanged ``` # Note: - Ensure you handle file reading, JSON operations, and hashing correctly. - Focus on modularity and clarity in your implementation.","solution":"import hashlib import json from typing import List, Dict def calculate_file_hashes(file_paths: List[str]) -> Dict[str, str]: def calculate_hash(file_path: str) -> str: sha256 = hashlib.sha256() with open(file_path, \'rb\') as f: while chunk := f.read(8192): sha256.update(chunk) return sha256.hexdigest() return {file_path: calculate_hash(file_path) for file_path in file_paths} def save_hashes_to_json(hashes: Dict[str, str], file_name: str) -> None: with open(file_name, \'w\') as json_file: json.dump(hashes, json_file) def load_hashes_from_json(file_name: str) -> Dict[str, str]: with open(file_name, \'r\') as json_file: return json.load(json_file) def check_data_integrity(file_paths: List[str], saved_hashes: Dict[str, str]) -> bool: current_hashes = calculate_file_hashes(file_paths) return current_hashes == saved_hashes"},{"question":"**Using the \\"nis\\" Module for Network Administration** The \\"nis\\" module in Python provides an interface to Sun\'s Network Information Service (NIS), useful for centralized administration of several hosts on Unix systems. # Task You are required to implement a function `get_user_info(user_id: str, mapname: str, domain: str = None) -> dict` using the \\"nis\\" module. This function should: 1. Use the `nis.match` function to retrieve the user\'s information based on their `user_id` from the given `mapname`. 2. Use the `nis.maps` function to verify that the `mapname` exists. If the `mapname` is not valid, raise a `ValueError` with the message \\"Invalid map name\\". 3. Return the retrieved information as a dictionary with the `user_id` as the key and the matched value as the value. # Input - `user_id` (str): The ID of the user whose information is to be retrieved. - `mapname` (str): The name of the NIS map to search in. - `domain` (str, optional): The NIS domain to use for the lookup. If not specified, the default NIS domain is used. # Output - `dict`: A dictionary containing the `user_id` as the key and the matched value as the value. # Constraints - The function should only work on Unix systems as the `nis` module is only available on Unix. - The function should handle the `nis.error` exception and return an empty dictionary if the specified `user_id` is not found. - The function should raise a `ValueError` if the specified `mapname` is not valid. # Example ```python def get_user_info(user_id: str, mapname: str, domain: str = None) -> dict: # Your code here # Example usage try: info = get_user_info(\\"johndoe\\", \\"passwd.byname\\") print(info) except ValueError as ve: print(ve) ``` Expected output in case of a successful lookup: ```python {\'johndoe\': b\'xjohndoex00...\'} ``` Expected output in case of an invalid map name: ```python Invalid map name ``` Expected output in case of user not found: ```python {} ``` # Notes - It is assumed that the `user_id` and `mapname` provided are valid strings. - The output value is a byte array, which might contain NULL and other characters. # Performance Requirements - The function should handle typical user IDs and map names efficiently within the constraints of the `nis` module functions.","solution":"import nis def get_user_info(user_id: str, mapname: str, domain: str = None) -> dict: try: # Check if the specified mapname exists if mapname not in nis.maps(domain): raise ValueError(\\"Invalid map name\\") # Retrieve the user information from the NIS map user_info = nis.match(user_id, mapname, domain) # Return the user information in a dictionary format return {user_id: user_info} except nis.error: # Handle the case when the user_id is not found return {}"},{"question":"Objective: Implement a function that processes files based on their MIME types using user-specified commands. This will involve reading a mailcap configuration from text files, finding appropriate commands for given MIME types, and executing those commands. Problem Statement: Write a Python function `process_files(mailcap_files: List[str], files: List[Tuple[str, str, List[str]]]) -> List[Tuple[str, Optional[str]]]` that processes a list of files based on their MIME types using specifications from mailcap files. - **Parameters**: - `mailcap_files`: A list of paths to mailcap configuration files. - `files`: A list of tuples, where each tuple contains the following elements: - `filename`: The name of the file to be processed. - `MIMEtype`: The MIME type of the file. - `plist`: A list of named parameters to be used in the command. - **Returns**: - A list of tuples where each tuple contains: - `filename`: The name of the file. - `result`: The result of the command execution, or `None` if no suitable command was found. Instructions: 1. Implement the function `process_files()` as described above. 2. Use the `mailcap` module\'s `getcaps()` function to read the mailcap configuration from the specified files. 3. For each file in the `files` list, use the `findmatch()` function from the `mailcap` module to find the appropriate command for the given MIME type. 4. Execute the command using `os.system()`, capture the result, and return it in the final list. 5. Ensure to handle cases where no suitable command is found for a MIME type by returning `None` for the result in such cases. 6. Consider security implications and follow the precautions mentioned regarding shell metacharacters within the `findmatch()` function. Example Usage: ```python import os from typing import List, Tuple, Optional def process_files(mailcap_files: List[str], files: List[Tuple[str, str, List[str]]]) -> List[Tuple[str, Optional[str]]]: # Your implementation here # Example usage: mailcap_files = [\'/path/to/user/mailcap\', \'/etc/mailcap\'] files_to_process = [ (\'example.txt\', \'text/plain\', []), (\'video.mp4\', \'video/mp4\', []), (\'image.png\', \'image/png\', [\'width=800\', \'height=600\']) ] results = process_files(mailcap_files, files_to_process) for filename, result in results: print(f\'File: {filename}, Result: {result}\') ``` **Note**: Ensure to include necessary imports and handle exceptions where applicable.","solution":"import os import mailcap from typing import List, Tuple, Optional def process_files(mailcap_files: List[str], files: List[Tuple[str, str, List[str]]]) -> List[Tuple[str, Optional[str]]]: # Combine the mailcap entries from all mailcap_files caps = {} for mailcap_file in mailcap_files: caps.update(mailcap.getcaps(mailcap_file)) results = [] for filename, MIMEtype, plist in files: # Prepare the parameters dictionary params = {\\"filename\\": filename} for param in plist: key, value = param.split(\'=\', 1) params[key] = value # Find a suitable command for the MIME type command, _ = mailcap.findmatch(caps, MIMEtype, filename=params[\'filename\']) if command: # Execute the command and capture the result result_code = os.system(command) result = \'Success\' if result_code == 0 else \'Failure\' results.append((filename, result)) else: results.append((filename, None)) return results"},{"question":"Objective: You are required to generate a synthetic dataset using the `make_classification` function from scikit-learn, split the dataset into training and testing sets, train a machine learning model, and evaluate its performance. Task: 1. **Generate a synthetic dataset**: - Use the `make_classification` function to generate a dataset with the following requirements: - 1000 samples - 20 features (15 informative, 5 redundant) - 3 classes - 5 clusters per class - Random state set to 42 for reproducibility 2. **Split the dataset**: - Split the dataset into training (80%) and testing (20%) sets. 3. **Train a machine learning model**: - Use a `RandomForestClassifier` from scikit-learn with the following parameters: - Number of estimators: 100 - Random state: 42 4. **Evaluate the model**: - Predict the class labels for the test set. - Calculate and print the following performance metrics: - Accuracy - Precision (macro average) - Recall (macro average) - F1-score (macro average) - Confusion matrix Input Format: - No external input is required. The dataset is internally generated using scikit-learn functions. Output Format: - Print the following metrics: - Accuracy - Precision - Recall - F1-score - Confusion matrix Coding Solution Template: ```python from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix import numpy as np # Step 1: Generate the synthetic dataset X, y = make_classification( n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_classes=3, n_clusters_per_class=5, random_state=42 ) # Step 2: Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Train the RandomForestClassifier clf = RandomForestClassifier(n_estimators=100, random_state=42) clf.fit(X_train, y_train) # Step 4: Evaluate the model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'macro\') recall = recall_score(y_test, y_pred, average=\'macro\') f1 = f1_score(y_test, y_pred, average=\'macro\') conf_matrix = confusion_matrix(y_test, y_pred) # Print the metrics print(\\"Accuracy:\\", accuracy) print(\\"Precision:\\", precision) print(\\"Recall:\\", recall) print(\\"F1-score:\\", f1) print(\\"Confusion Matrix:n\\", conf_matrix) ```","solution":"from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix import numpy as np # Step 1: Generate the synthetic dataset X, y = make_classification( n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_classes=3, n_clusters_per_class=5, random_state=42 ) # Step 2: Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Train the RandomForestClassifier clf = RandomForestClassifier(n_estimators=100, random_state=42) clf.fit(X_train, y_train) # Step 4: Evaluate the model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'macro\') recall = recall_score(y_test, y_pred, average=\'macro\') f1 = f1_score(y_test, y_pred, average=\'macro\') conf_matrix = confusion_matrix(y_test, y_pred) # Print the metrics print(\\"Accuracy:\\", accuracy) print(\\"Precision:\\", precision) print(\\"Recall:\\", recall) print(\\"F1-score:\\", f1) print(\\"Confusion Matrix:n\\", conf_matrix)"},{"question":"# Creating a Custom Sequence Class in Python Implement a custom sequence-like class called `CustomContainer` that emulates the behavior of a Python list but only allows hashable (immutable) objects to be stored within it. Ensure that your class supports the following behaviors and methods: 1. **Initialization**: - The constructor `__init__` should initialize the container using a given iterable of hashable items. - Raise a `TypeError` if any item in the initialization iterable is not hashable. 2. **Item Access**: - Implement `__getitem__` method to allow accessing elements using both index and slice objects. 3. **Item Assignment**: - Implement `__setitem__` method to allow assigning items at specific indices (but still ensure assigned items are hashable). - Raise a `TypeError` if any new item assigned is not hashable. 4. **Item Deletion**: - Implement `__delitem__` method to allow deleting items by index or slice. 5. **Addition**: - Override `__add__` and `__radd__` to concatenate another `CustomContainer` or a list of hashable items, returning a new `CustomContainer`. 6. **Length Calculation**: - Implement `__len__` to return the number of items stored in the container. 7. **Representation**: - Implement `__repr__` to return a string representation that can ideally recreate the object. Include robust error handling for cases where operations might fail due to unhashable items or invalid indices. Example ```python >>> c = CustomContainer([1, 2, 3]) >>> c[1] # Accessing element at index 1 2 >>> c[1:3] # Accessing elements using slicing [2, 3] >>> c[1] = 4 # Setting element at index 1 >>> c CustomContainer([1, 4, 3]) >>> len(c) 3 >>> del c[1] # Deleting element at index 1 >>> c CustomContainer([1, 3]) >>> c += [5, 6] # Concatenating with another list of hashable items >>> c CustomContainer([1, 3, 5, 6]) ``` # Constraints - You can assume the inputs to the container are always well-formed. - You must handle all edge cases as specified in the question. Implement the class `CustomContainer` below: ```python class CustomContainer: def __init__(self, iterable): # Add your code here def __getitem__(self, index): # Add your code here def __setitem__(self, index, value): # Add your code here def __delitem__(self, index): # Add your code here def __add__(self, other): # Add your code here def __radd__(self, other): # Add your code here def __len__(self): # Add your code here def __repr__(self): # Add your code here ``` Ensure your implementation passes the provided example and handles various edge cases, including unhashable items, slicing, and type errors appropriately.","solution":"class CustomContainer: def __init__(self, iterable): if not all(isinstance(item, (int, float, str, tuple, frozenset, bytes, bool, type(None))) for item in iterable): raise TypeError(\\"All items must be hashable\\") self._items = list(iterable) def __getitem__(self, index): return self._items[index] def __setitem__(self, index, value): if not isinstance(value, (int, float, str, tuple, frozenset, bytes, bool, type(None))): raise TypeError(\\"Item must be hashable\\") self._items[index] = value def __delitem__(self, index): del self._items[index] def __add__(self, other): if not all(isinstance(item, (int, float, str, tuple, frozenset, bytes, bool, type(None))) for item in other): raise TypeError(\\"All items must be hashable\\") return CustomContainer(self._items + list(other)) def __radd__(self, other): if not all(isinstance(item, (int, float, str, tuple, frozenset, bytes, bool, type(None))) for item in other): raise TypeError(\\"All items must be hashable\\") return CustomContainer(list(other) + self._items) def __len__(self): return len(self._items) def __repr__(self): return f\\"CustomContainer({self._items})\\""},{"question":"**Objective**: Demonstrate your understanding and implementation skills of the `sklearn.neighbors` module in scikit-learn, specifically focusing on both classification and regression using the K-Nearest Neighbors algorithm. # Problem Description You are provided with two datasets: 1. **Classification Dataset**: Consists of feature vectors and their corresponding class labels. 2. **Regression Dataset**: Consists of feature vectors and their corresponding continuous targets. Your task is to implement a Python function that: 1. Trains a `KNeighborsClassifier` on the classification dataset and evaluates its performance on a test dataset. 2. Trains a `KNeighborsRegressor` on the regression dataset and evaluates its performance on a test dataset. # Function Signature ```python def train_and_evaluate_knn(class_train, class_test, class_labels_train, class_labels_test, reg_train, reg_test, reg_labels_train, reg_labels_test, k:int) -> dict: Parameters: - class_train (np.array): Training data for classification (n_samples_class_train, n_features). - class_test (np.array): Test data for classification (n_samples_class_test, n_features). - class_labels_train (np.array): Training labels for classification (n_samples_class_train,). - class_labels_test (np.array): Test labels for classification (n_samples_class_test,). - reg_train (np.array): Training data for regression (n_samples_reg_train, n_features). - reg_test (np.array): Test data for regression (n_samples_reg_test, n_features). - reg_labels_train (np.array): Training targets for regression (n_samples_reg_train,). - reg_labels_test (np.array): Test targets for regression (n_samples_reg_test,). - k (int): The number of neighbors to use for the KNN algorithm. Returns: - (dict): A dictionary containing classification accuracy and regression mean squared error with keys: { \\"classification_accuracy\\": float, \\"regression_mse\\" : float } pass ``` # Constraints - The input datasets will be preprocessed (e.g., no missing values). - The class labels for the classification dataset will be integers. - The regression targets will be continuous values. - The value of `k` will be a positive integer, not greater than 10. # Example ```python import numpy as np from sklearn.model_selection import train_test_split from sklearn.datasets import make_classification, make_regression # Create a classification dataset X_class, y_class = make_classification(n_samples=200, n_features=5, random_state=42) class_train, class_test, class_labels_train, class_labels_test = train_test_split(X_class, y_class, test_size=0.3, random_state=42) # Create a regression dataset X_reg, y_reg = make_regression(n_samples=200, n_features=5, noise=0.1, random_state=42) reg_train, reg_test, reg_labels_train, reg_labels_test = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42) # Number of neighbors k = 5 # Expected Output Format results = train_and_evaluate_knn(class_train, class_test, class_labels_train, class_labels_test, reg_train, reg_test, reg_labels_train, reg_labels_test, k) print(results) # Output Example: {\'classification_accuracy\': 0.85, \'regression_mse\': 12.3} ``` **Note**: The actual values for classification accuracy and regression MSE will vary depending on the random state and the characteristics of the datasets. # Performance Requirements - The function should train and evaluate the KNN models efficiently. - For classification, use accuracy as the performance metric. - For regression, use Mean Squared Error (MSE) as the performance metric. # Evaluation Criteria - Correctness of the implementation. - Efficient use of the `sklearn.neighbors` classes for KNN. - Proper handling of inputs and expected outputs. - Code readability and organization.","solution":"import numpy as np from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor from sklearn.metrics import accuracy_score, mean_squared_error def train_and_evaluate_knn(class_train, class_test, class_labels_train, class_labels_test, reg_train, reg_test, reg_labels_train, reg_labels_test, k: int) -> dict: Trains KNeighborsClassifier and KNeighborsRegressor on given datasets and evaluates their performance. Parameters: - class_train (np.array): Training data for classification (n_samples_class_train, n_features). - class_test (np.array): Test data for classification (n_samples_class_test, n_features). - class_labels_train (np.array): Training labels for classification (n_samples_class_train,). - class_labels_test (np.array): Test labels for classification (n_samples_class_test,). - reg_train (np.array): Training data for regression (n_samples_reg_train, n_features). - reg_test (np.array): Test data for regression (n_samples_reg_test, n_features). - reg_labels_train (np.array): Training targets for regression (n_samples_reg_train,). - reg_labels_test (np.array): Test targets for regression (n_samples_reg_test,). - k (int): The number of neighbors to use for the KNN algorithm. Returns: - dict: A dictionary containing classification accuracy and regression mean squared error. # Train KNeighborsClassifier clf = KNeighborsClassifier(n_neighbors=k) clf.fit(class_train, class_labels_train) class_predictions = clf.predict(class_test) classification_accuracy = accuracy_score(class_labels_test, class_predictions) # Train KNeighborsRegressor reg = KNeighborsRegressor(n_neighbors=k) reg.fit(reg_train, reg_labels_train) reg_predictions = reg.predict(reg_test) regression_mse = mean_squared_error(reg_labels_test, reg_predictions) return { \\"classification_accuracy\\": classification_accuracy, \\"regression_mse\\": regression_mse }"},{"question":"<|Analysis Begin|> The provided documentation sample showcases some functionalities of the `seaborn.objects` module, specifically related to creating bar charts using `so.Bars()` and histograms using `so.Hist()`. It demonstrates the following concepts: 1. Loading a dataset (`diamonds`). 2. Creating a bar plot with logarithmic scaling. 3. Customizing bar properties such as `color`, `edgewidth`, `alpha`, etc. 4. Handling overlapping bars using transformations like `so.Stack()` or `so.Dodge()`. 5. Narrowing bars and filtering data. Although the documentation is limited, it provides enough information to form a task around data visualization using seaborn\'s `objects` interface and creating customized plots. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Demonstrate your understanding of creating and customizing visualizations using Seaborn\'s `objects` interface. Using the `seaborn` package and the `diamonds` dataset, generate a composite visualization consisting of a stacked histogram and customized bar plots to help analyze the distribution and characteristics of diamond prices. **Task Description:** 1. **Load the `diamonds` dataset** from seaborn. 2. **Visualize the distribution of diamond prices**: - Create a histogram of the `price` attribute with logarithmic scaling on the x-axis. 3. **Add a segmented bar plot to show the distribution across different cuts**: - Use `so.Bars()` and set the histogram to segment by the `cut` attribute, stacking the bars to resolve overlap. 4. **Customize the plot** using the following properties: - Use a different `alpha` value for bars based on the `clarity` attribute. - Set `edgewidth` to 0 for bar edges. - Use filled and unfilled bar variations to distinguish between various segments. 5. **Faceting:** - Create facets based on the `color` attribute of diamonds to generate separate subplots for each color category. **Input Format:** - None (the function should not take any inputs and should generate the plot directly). **Output Format:** - Display the generated plot. **Constraints:** - Ensure the plot accurately represents the data and follows the customization guidelines. - Use the `seaborn.objects` interface for all plotting tasks. **Performance Requirements:** - The code should be efficient enough to handle the size of the `diamonds` dataset without significant delay. # Example Code Below is a starting point for your solution. Complete the code with the required customizations as per the task description. ```python import seaborn.objects as so from seaborn import load_dataset # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create the base plot with logarithmic scaling on the x-axis base_plot = so.Plot(diamonds, x=\\"price\\").scale(x=\\"log\\") # Add a histogram to show the distribution of diamond prices base_plot.add(so.Bars(), so.Hist()) # Add segmented bars for different cuts and stack them # Add your customizations here... # Customize bars with varying alpha, edgewidth and filled/unfilled variations # Add your customizations here... # Facet by the `color` attribute of diamonds # Add your code to create facets... # Display the plot # Add your code to display the plot... ``` **Good luck!**","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_diamond_price_visualization(): # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create the base plot with logarithmic scaling on the x-axis base_plot = so.Plot(diamonds, x=\\"price\\").scale(x=\\"log\\") # Add a histogram to show the distribution of diamond prices base_plot.add(so.Bars(), so.Hist()) # Add segmented bars for different cuts and stack them base_plot.add(so.Bars(edgewidth=0), so.Stack(), color=\\"cut\\", alpha=\\".clarity\\") # Facet by the `color` attribute of diamonds base_plot.facet(\\"color\\") # Display the plot base_plot.show() plt.show() # Call the function to generate the plot create_diamond_price_visualization()"},{"question":"**Context Management using ExitStack** In this assessment, you are required to demonstrate your understanding of Python\'s `contextlib.ExitStack` by implementing a function that handles multiple file operations safely using context managers. # Problem Statement Write a function `process_files(file_paths: List[str], process_line: Callable[[str], None]) -> None` that takes a list of file paths and a processing function. It should open all the files, process each line in every file using the provided `process_line` function, and ensure all files are closed properly even if an exception occurs during processing. You must use `contextlib.ExitStack` to manage the file resources. # Input - `file_paths`: A list of strings where each string is a path to a file. - `process_line`: A callable that accepts a single string argument (a line from a file) and processes it. # Output - The function does not return anything. It performs processing and ensures files are closed. # Constraints - Each file path in `file_paths` points to a valid and readable file. - `process_line` is a valid callable function that accepts a string and returns `None`. # Example ```python def example_process_line(line: str) -> None: print(line.strip()) file_paths = [\\"file1.txt\\", \\"file2.txt\\"] process_files(file_paths, example_process_line) ``` # Requirements 1. Use `contextlib.ExitStack` to open multiple files and ensure they are properly closed. 2. Handle any potential exceptions that might arise during file processing without skipping the closing of any files. # Implementation Notes You will need to: - Import necessary modules: `contextlib` and `typing`. - Use `ExitStack` to manage the file streams you open. - Ensure each file is processed line by line using the `process_line` function. - Make sure all files are closed properly, even if processing a line throws an exception. ```python from contextlib import ExitStack from typing import List, Callable def process_files(file_paths: List[str], process_line: Callable[[str], None]) -> None: with ExitStack() as stack: files = [stack.enter_context(open(file_path, \'r\')) for file_path in file_paths] for file in files: for line in file: process_line(line) ``` Test your function with a list of sample file paths and a simple `process_line` function that prints each line.","solution":"from contextlib import ExitStack from typing import List, Callable def process_files(file_paths: List[str], process_line: Callable[[str], None]) -> None: Opens each file in file_paths and processes each line using process_line function. Args: file_paths (List[str]): List of file paths to be processed. process_line (Callable[[str], None]): Function to process each line of the file. with ExitStack() as stack: files = [stack.enter_context(open(file_path, \'r\')) for file_path in file_paths] for file in files: for line in file: process_line(line)"},{"question":"# Complex Number Calculator You are required to implement a class-based solution to perform operations on complex numbers. Complex numbers have both real and imaginary parts, and can be represented as `a + bi`, where `a` is the real part and `b` is the imaginary part. The task is to implement a class `ComplexNumber` that supports the following functionalities: 1. **Initialization**: Create a complex number with specified real and imaginary parts. 2. **Addition**: Add two complex numbers. 3. **Subtraction**: Subtract one complex number from another. 4. **Multiplication**: Multiply two complex numbers. 5. **Division**: Divide one complex number by another. 6. **Modulus**: Compute the modulus (absolute value) of the complex number. 7. **String Representation**: Return a string representation of the complex number in the form `a + bi`. 8. **Error Handling**: Raise appropriate exceptions for invalid operations, such as division by zero. Requirements: 1. **Initialization**: - Method: `__init__(self, real: float, imag: float)` - Initializes a complex number with the given real and imaginary parts. 2. **Addition**: - Method: `__add__(self, other: \'ComplexNumber\') -> \'ComplexNumber\'` - Adds two complex numbers and returns the result as a new `ComplexNumber` instance. 3. **Subtraction**: - Method: `__sub__(self, other: \'ComplexNumber\') -> \'ComplexNumber\'` - Subtracts another complex number from this one and returns the result as a new `ComplexNumber` instance. 4. **Multiplication**: - Method: `__mul__(self, other: \'ComplexNumber\') -> \'ComplexNumber\'` - Multiplies two complex numbers and returns the result as a new `ComplexNumber` instance. 5. **Division**: - Method: `__truediv__(self, other: \'ComplexNumber\') -> \'ComplexNumber\'` - Divides this complex number by another and returns the result as a new `ComplexNumber` instance. - Raises a `ValueError` if the other complex number is zero. 6. **Modulus**: - Method: `modulus(self) -> float` - Returns the modulus (absolute value) of the complex number. 7. **String Representation**: - Method: `__str__(self) -> str` - Returns a string in the form `a + bi`. 8. **Error Handling**: - Ensure appropriate exceptions are raised for invalid operations, such as division by zero. Example Usage: ```python # Creating complex numbers c1 = ComplexNumber(4, 3) c2 = ComplexNumber(2, -1) # Operations print(c1 + c2) # Output: 6 + 2i print(c1 - c2) # Output: 2 + 4i print(c1 * c2) # Output: 11 + 2i print(c1 / c2) # Output: 1.2 + 1.6i print(c1.modulus()) # Output: 5.0 print(c2.modulus()) # Output: 2.23606797749979 print(str(c1)) # Output: 4 + 3i print(str(c2)) # Output: 2 - 1i ``` Constraints: - You may assume that the inputs are always valid complex numbers (i.e., real and imaginary parts are floating-point numbers). - Ensure the division operation handles division by zero appropriately by raising a `ValueError`. Implement the `ComplexNumber` class as described.","solution":"class ComplexNumber: def __init__(self, real: float, imag: float): self.real = real self.imag = imag def __add__(self, other: \'ComplexNumber\') -> \'ComplexNumber\': return ComplexNumber(self.real + other.real, self.imag + other.imag) def __sub__(self, other: \'ComplexNumber\') -> \'ComplexNumber\': return ComplexNumber(self.real - other.real, self.imag - other.imag) def __mul__(self, other: \'ComplexNumber\') -> \'ComplexNumber\': real = (self.real * other.real) - (self.imag * other.imag) imag = (self.real * other.imag) + (self.imag * other.real) return ComplexNumber(real, imag) def __truediv__(self, other: \'ComplexNumber\') -> \'ComplexNumber\': if other.real == 0 and other.imag == 0: raise ValueError(\\"Cannot divide by zero ComplexNumber\\") denominator = (other.real ** 2) + (other.imag ** 2) real = ((self.real * other.real) + (self.imag * other.imag)) / denominator imag = ((self.imag * other.real) - (self.real * other.imag)) / denominator return ComplexNumber(real, imag) def modulus(self) -> float: return (self.real ** 2 + self.imag ** 2) ** 0.5 def __str__(self) -> str: if self.imag >= 0: return f\\"{self.real} + {self.imag}i\\" else: return f\\"{self.real} - {abs(self.imag)}i\\""},{"question":"**Programming Assessment Question:** # Objective Create a Python function that utilizes the `slice` objects and related methods found in the Python/C API to manipulate and extract information from slices. # Task Write a Python function `process_slice` that takes three arguments: `sequence_length`, `start`, `stop`, and `step`. The function should: 1. Create a slice object using `start`, `stop`, and `step`. 2. Unpack the slice to extract the start, stop, and step values. 3. Adjust the slice indices based on `sequence_length`. 4. Return a tuple with the adjusted start, stop, step values, and the length of the slice. # Input - `sequence_length` (int): The length of the sequence on which the slice will be applied. - `start` (int, optional): The starting index of the slice. - `stop` (int, optional): The stopping index of the slice. - `step` (int, optional): The step of the slice. # Output - A tuple `(adjusted_start, adjusted_stop, adjusted_step, slice_length)`, where: - `adjusted_start` (int): The adjusted starting index. - `adjusted_stop` (int): The adjusted stopping index. - `adjusted_step` (int): The adjusted step value. - `slice_length` (int): The length of the slice. # Constraints - Values for `start`, `stop`, and `step` can be `None`, in which case they should be handled appropriately. - The function should properly handle out-of-bounds values by clipping them. # Example ```python def process_slice(sequence_length, start=None, stop=None, step=None): # Your implementation here # Example Usage sequence_length = 10 print(process_slice(sequence_length, 1, 8, 2)) # Output: (1, 8, 2, 4) print(process_slice(sequence_length, -15, 15, 1)) # Output: (0, 10, 1, 10) ``` # Additional Notes - You may use Python\'s in-built `slice` for creating the slice object and implementing the functions manually to simulate the behavior of the provided C API functions if needed. - Focus on edge cases such as negative indices, out-of-bound values, `None` values for `start`, `stop`, and `step`.","solution":"def process_slice(sequence_length, start=None, stop=None, step=None): Create a slice object using start, stop, and step, and adjust it according to the given sequence length. Returns the adjusted start, stop, step values, and the length of the slice. # Creating the slice object s = slice(start, stop, step) # Adjusting the slice indices to fit within the sequence length and extracting the start, stop, step adjusted_start, adjusted_stop, adjusted_step = s.indices(sequence_length) # Calculate the length of the slice by iterating through the elements slice_length = len(range(adjusted_start, adjusted_stop, adjusted_step)) # Return the tuple with adjusted start, stop, step values, and the length of the slice return (adjusted_start, adjusted_stop, adjusted_step, slice_length)"},{"question":"# Support Vector Machines and Custom Kernels using scikit-learn **Problem Statement:** You are given a dataset with features and target values. Your task is to create and train an SVM classifier using a custom kernel. You will also perform hyperparameter tuning to find the best parameters for your model. Finally, use the trained model to make predictions on a test dataset. **Instructions:** 1. **Load the Dataset:** - Use any publicly available dataset, such as the Iris dataset, from scikit-learn (You can use `load_iris` from `sklearn.datasets`). 2. **Preprocess the Data:** - Split the dataset into training and testing sets using `train_test_split` from `sklearn.model_selection`. - Standardize the features using `StandardScaler` from `sklearn.preprocessing`. 3. **Custom Kernel Function:** - Define a custom kernel function. A simple example is a linear kernel, but you can define any kernel you believe will help with classification. 4. **Create and Train the SVM Classifier:** - Use the custom kernel by passing it to the `kernel` parameter of `svm.SVC`. - Train the SVM classifier on the training data using the `fit` method. 5. **Hyperparameter Tuning:** - Use GridSearchCV to find the best parameters for the SVM classifier. Search for the best parameters for `C` and any other relevant hyperparameters of your defined kernel. - Train the model with the best found parameters. 6. **Make Predictions:** - Use the trained model to predict the class labels on the test set using `predict` method. - Evaluate the model performance using metrics like accuracy, precision, recall, and F1-score from `sklearn.metrics`. **Functional Requirements:** - Implement the function `train_and_evaluate_svm_classifier()` which performs the task specified above. The function should take no inputs and should output the evaluation metrics for the model\'s performance on the test data. ```python def train_and_evaluate_svm_classifier(): # Step 1: Load the dataset from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn import svm from sklearn.model_selection import GridSearchCV from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # Load data data = load_iris() X, y = data.data, data.target # Step 2: Preprocess the data # Split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Standardize scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 3: Define a custom kernel def custom_kernel(X, Y): # Example: Linear kernel return np.dot(X, Y.T) # Step 4: Create and Train the SVM Classifier # Define the model svc = svm.SVC(kernel=custom_kernel) # Hyperparameter tuning using GridSearchCV param_grid = {\'C\': [0.1, 1, 10, 100]} grid_search = GridSearchCV(svc, param_grid, cv=5) # Train the model grid_search.fit(X_train, y_train) # Best model best_model = grid_search.best_estimator_ # Step 5: Make Predictions and Evaluate y_pred = best_model.predict(X_test) # Compute evaluation metrics accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'macro\') recall = recall_score(y_test, y_pred, average=\'macro\') f1 = f1_score(y_test, y_pred, average=\'macro\') # Print evaluation metrics print(\\"Accuracy:\\", accuracy) print(\\"Precision:\\", precision) print(\\"Recall:\\", recall) print(\\"F1-Score:\\", f1) # Example function call train_and_evaluate_svm_classifier() ``` **Notes:** - Ensure your custom kernel function returns a correct kernel matrix. - Use cross-validation within GridSearchCV to ensure robust hyperparameter tuning. - You may modify the custom kernel function to explore different kernel methods and their effects on performance. **Constraints:** - Use `scikit-learn` library for SVM implementation. - Follow the steps in the provided sequence for consistency. Good luck!","solution":"def train_and_evaluate_svm_classifier(): import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn import svm from sklearn.model_selection import GridSearchCV from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # Step 1: Load the dataset data = load_iris() X, y = data.data, data.target # Step 2: Preprocess the data # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 3: Define a custom kernel function def custom_kernel(X, Y): return np.dot(X, Y.T) # Step 4: Create and train the SVM classifier svc = svm.SVC(kernel=custom_kernel) # Hyperparameter tuning with GridSearchCV param_grid = {\'C\': [0.1, 1, 10, 100]} grid_search = GridSearchCV(svc, param_grid, cv=5) # Train the model grid_search.fit(X_train, y_train) # Get the best model best_model = grid_search.best_estimator_ # Step 5: Make predictions and evaluate the model y_pred = best_model.predict(X_test) # Evaluation metrics accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'macro\') recall = recall_score(y_test, y_pred, average=\'macro\') f1 = f1_score(y_test, y_pred, average=\'macro\') # Output the evaluation metrics return accuracy, precision, recall, f1"},{"question":"**XML Document Manipulation with `xml.dom`** # Objective: Write Python code to create and manipulate an XML document using the `xml.dom` module. The task will involve creating a document, adding elements and attributes, manipulating nodes, and ensuring proper XML structure. # Problem Statement: 1. You need to create an XML document with the following structure: ```xml <library> <book id=\\"1\\"> <title>Effective Python</title> <author>Brett Slatkin</author> <genre>Programming</genre> </book> <book id=\\"2\\"> <title>Automate the Boring Stuff with Python</title> <author>Al Sweigart</author> <genre>Programming</genre> </book> </library> ``` 2. Implement the following tasks: - **Create the XML document and its root element `<library>`.** - **Add two `<book>` elements as described above, with appropriate children (`<title>`, `<author>`, `<genre>`) and attributes (`id`).** - **Retrieve the title of the book with `id=\\"2\\"` and print it.** - **Update the genre of the book with `id=\\"1\\"` to \\"Software Development\\".** - **Delete the book with `id=\\"2\\"` from the document.** # Requirements: 1. **Function Implementation:** - Implement each requirement as a separate function for clarity. Each function should take the necessary inputs and perform the required operations on the XML document. 2. **Input and Output:** - Input is managed within the functions; manual testing can be done by calling these functions in a sequence. - Output should be printed statements showing the results of retrieval or updates. 3. **Constraints:** - Ensure that node operations keep the document well-formed as per XML standards. - Handle any possible exceptions using the provided DOM exceptions. 4. **Performance Requirements:** - The functions should handle manipulation efficiently and maintain valid tree structures post-operations. # Skeleton Code: ```python from xml.dom import minidom # Create the XML document and root element def create_library_document(): doc = minidom.Document() library = doc.createElement(\\"library\\") doc.appendChild(library) return doc # Add book elements to the library def add_book(doc, book_id, title, author, genre): library = doc.documentElement book = doc.createElement(\\"book\\") book.setAttribute(\\"id\\", book_id) title_element = doc.createElement(\\"title\\") title_text = doc.createTextNode(title) title_element.appendChild(title_text) book.appendChild(title_element) author_element = doc.createElement(\\"author\\") author_text = doc.createTextNode(author) author_element.appendChild(author_text) book.appendChild(author_element) genre_element = doc.createElement(\\"genre\\") genre_text = doc.createTextNode(genre) genre_element.appendChild(genre_text) book.appendChild(genre_element) library.appendChild(book) # Retrieve title of a book by id def get_book_title_by_id(doc, book_id): books = doc.getElementsByTagName(\\"book\\") for book in books: if book.getAttribute(\\"id\\") == book_id: title = book.getElementsByTagName(\\"title\\")[0] return title.firstChild.nodeValue return None # Update genre of a book by id def update_book_genre(doc, book_id, new_genre): books = doc.getElementsByTagName(\\"book\\") for book in books: if book.getAttribute(\\"id\\") == book_id: genre = book.getElementsByTagName(\\"genre\\")[0] genre.firstChild.nodeValue = new_genre return # Delete book element by id def delete_book_by_id(doc, book_id): books = doc.getElementsByTagName(\\"book\\") for book in books: if book.getAttribute(\\"id\\") == book_id: book.parentNode.removeChild(book) return # Print the entire XML document as a string for verification def print_xml_document(doc): print(doc.toprettyxml(indent=\\" \\")) # Main routine def main(): doc = create_library_document() add_book(doc, \\"1\\", \\"Effective Python\\", \\"Brett Slatkin\\", \\"Programming\\") add_book(doc, \\"2\\", \\"Automate the Boring Stuff with Python\\", \\"Al Sweigart\\", \\"Programming\\") print(\\"Title of book with id 2:\\", get_book_title_by_id(doc, \\"2\\")) update_book_genre(doc, \\"1\\", \\"Software Development\\") delete_book_by_id(doc, \\"2\\") print(\\"Final XML document:\\") print_xml_document(doc) if __name__ == \\"__main__\\": main() ``` # Notes: 1. You should structure your code to use the `xml.dom` module appropriately for all document manipulations. 2. Feel free to add any helper functions if they make your code more readable. 3. Proper handling of exceptions is essential to ensure robustness. Good luck!","solution":"from xml.dom import minidom # Create the XML document and root element def create_library_document(): doc = minidom.Document() library = doc.createElement(\\"library\\") doc.appendChild(library) return doc # Add book elements to the library def add_book(doc, book_id, title, author, genre): library = doc.documentElement book = doc.createElement(\\"book\\") book.setAttribute(\\"id\\", book_id) title_element = doc.createElement(\\"title\\") title_text = doc.createTextNode(title) title_element.appendChild(title_text) book.appendChild(title_element) author_element = doc.createElement(\\"author\\") author_text = doc.createTextNode(author) author_element.appendChild(author_text) book.appendChild(author_element) genre_element = doc.createElement(\\"genre\\") genre_text = doc.createTextNode(genre) genre_element.appendChild(genre_text) book.appendChild(genre_element) library.appendChild(book) # Retrieve title of a book by id def get_book_title_by_id(doc, book_id): books = doc.getElementsByTagName(\\"book\\") for book in books: if book.getAttribute(\\"id\\") == book_id: title = book.getElementsByTagName(\\"title\\")[0] return title.firstChild.nodeValue return None # Update genre of a book by id def update_book_genre(doc, book_id, new_genre): books = doc.getElementsByTagName(\\"book\\") for book in books: if book.getAttribute(\\"id\\") == book_id: genre = book.getElementsByTagName(\\"genre\\")[0] genre.firstChild.nodeValue = new_genre return # Delete book element by id def delete_book_by_id(doc, book_id): books = doc.getElementsByTagName(\\"book\\") for book in books: if book.getAttribute(\\"id\\") == book_id: book.parentNode.removeChild(book) return # Print the entire XML document as a string for verification def print_xml_document(doc): return doc.toprettyxml(indent=\\" \\") # Main routine for manual testing (not part of solution) if __name__ == \\"__main__\\": doc = create_library_document() add_book(doc, \\"1\\", \\"Effective Python\\", \\"Brett Slatkin\\", \\"Programming\\") add_book(doc, \\"2\\", \\"Automate the Boring Stuff with Python\\", \\"Al Sweigart\\", \\"Programming\\") print(\\"Title of book with id 2:\\", get_book_title_by_id(doc, \\"2\\")) update_book_genre(doc, \\"1\\", \\"Software Development\\") delete_book_by_id(doc, \\"2\\") print(\\"Final XML document:\\") print_xml_document(doc)"},{"question":"# Coding Assessment Objective Design a function that generates data for a simple simulation based on given parameters. Implement a function `generate_simulation_data` that uses the `random` module to simulate and return results for a multi-server queue system. Function Signature ```python def generate_simulation_data(num_servers, average_arrival_interval, average_service_time, stdev_service_time, num_events, seed=None): pass ``` Input - `num_servers` (int): Number of servers in the queue system. (1 ≤ num_servers ≤ 100) - `average_arrival_interval` (float): Average interval time between arrivals. (0.1 ≤ average_arrival_interval ≤ 100.0) - `average_service_time` (float): Average service time for a customer. (0.1 ≤ average_service_time ≤ 100.0) - `stdev_service_time` (float): Standard deviation of the service times. (0.1 ≤ stdev_service_time ≤ 100.0) - `num_events` (int): The total number of arrival events to be simulated. (1 ≤ num_events ≤ 10^6) - `seed` (optional, int): Seed for the random number generator to ensure reproducibility of results. If provided, the random number generator should be initialized with this seed. Output - A list of tuples, where each tuple contains: - `event_number`: The event number (from 1 to `num_events`). - `arrival_time`: The simulated arrival time for the event. - `wait_time`: The simulated wait time for the event. - `service_time`: The simulated service time for the event. - `service_completion_time`: The simulated time when the service is completed. Example ```python >>> generate_simulation_data(3, 5.6, 15.0, 3.5, 10, seed=42) [(1, 1.025919382722055, 0.0, 12.56488633414369, 13.590805716865745), (2, 3.44107749237014, 0.0, 13.55808808968336, 17.9991655820535), (3, 22.33892153707909, 0.0, 12.73300991040576, 35.07193144748485), (4, 31.54925029984967, 0.0, 11.102253517736881, 42.65150381758655), (5, 32.33230963721832, 0.0, 19.265023874969577, 51.59733351218791), (6, 35.78061549298159, 0.0, 17.91708231968265, 53.69769781266424), (7, 49.35082368232704, 0.0, 15.103039796662827), (8, 61.31958064271435, 0.0, 22.499987507854674) ] ``` Constraints - Use the `random` module for all random number generation. - If a seed is provided, ensure the random number generator is initialized with this seed to guarantee reproducibility. - Ensure efficient performance for large values of `num_events`. Notes - **arrival_time** should be simulated using an exponential distribution with `average_arrival_interval`. - **service_time** should be simulated using a normal distribution with mean `average_service_time` and standard deviation `stdev_service_time`. - Use a priority queue to manage and simulate the waiting times and service completions for the multi-server queue system.","solution":"import random import heapq def generate_simulation_data(num_servers, average_arrival_interval, average_service_time, stdev_service_time, num_events, seed=None): if seed is not None: random.seed(seed) events = [] servers = [0] * num_servers arrival_time = 0 for event_number in range(1, num_events + 1): inter_arrival_time = random.expovariate(1 / average_arrival_interval) arrival_time += inter_arrival_time service_time = max(random.gauss(average_service_time, stdev_service_time), 0.1) earliest_server_available_time = heapq.heappop(servers) wait_time = max(earliest_server_available_time - arrival_time, 0) service_completion_time = arrival_time + wait_time + service_time heapq.heappush(servers, service_completion_time) events.append((event_number, arrival_time, wait_time, service_time, service_completion_time)) return events"},{"question":"**Coding Assessment: Advanced Data Visualization with Seaborn** **Objective**: Demonstrate your understanding of seaborn by creating an insightful and well-customized visualization based on a specified dataset. **Task**: Given a dataset about tips received by waitstaff in a restaurant, you are required to: 1. **Load the dataset `tips`** (already available through seaborn\'s dataset repository). 2. **Create a faceted scatter plot** that visualizes the relationship between `total_bill` and `tip`. The plot should: - Map `sex`, `smoker`, and `time` to different aesthetics (hue, style, and facet columns respectively). - Ensure the points are distinguishable by using appropriate marker styles for `sex`. 3. **Further, enhance the visualization** by: - Mapping the size of the points to the `size` variable (representing the number of people at the table). - Customizing the color palette to a sequential palette for the `size` variable. - Scaling the sizes of the points to cover a sensible range. - Adding a title that describes what the plot represents. **Input**: - No specific input required as you will use the seaborn `tips` dataset. **Output**: - A well-customized scatter plot meeting the above criteria. **Constraints**: - Use seaborn functions wherever applicable. - Ensure that the plot is clear and the relationships between the variables are easy to interpret. Your final code should look something like this: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the faceted scatter plot sns.set_theme(style=\\"darkgrid\\") plot = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"sex\\", style=\\"sex\\", col=\\"time\\", size=\\"size\\", sizes=(15, 200), palette=\\"ch:r=-.5,l=.75\\", kind=\\"scatter\\" ) # Set the title for the plot plot.fig.suptitle(\\"Relationship between Total Bill and Tip by Gender, Smoking Status, and Time\\", y=1.03 # Adjusting the title position ) # Display the plot plt.show() ``` *Note*: Make sure to test this code in a local Jupyter notebook or an appropriate Python environment to verify its correctness and aesthetics. **Assessment Criteria**: - Correctness of the code: Loading data, creating the desired plot, and customization. - Proper use of seaborn functionality for enhancing the plot. - Clarity and interpretability of the final visualization. - Code readability and comments explaining the steps.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_tips_relationship(): Create a faceted scatter plot visualizing the relationship between total_bill and tip with customizations for sex, smoker, time, and size. # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the faceted scatter plot sns.set_theme(style=\\"darkgrid\\") plot = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"sex\\", style=\\"sex\\", col=\\"time\\", size=\\"size\\", sizes=(15, 200), palette=\\"ch:r=-.5,l=.75\\", kind=\\"scatter\\" ) # Set the title for the plot plot.fig.suptitle(\\"Relationship between Total Bill and Tip by Gender, Smoking Status, and Time\\", y=1.03 # Adjusting the title position ) # Display the plot plt.show()"},{"question":"# Coding Challenge: Audio Data Manipulation with the `audioop` Module You are required to implement a function named `process_audio`, which processes audio data through a series of operations defined in the \\"audioop\\" module. The function should perform the following tasks: 1. Convert the input fragment from linear encoding to a-LAW encoding. 2. Byte-swap the a-LAW encoded fragment. 3. Convert the byte-swapped fragment back to linear encoding. 4. Multiply the linear samples by a given factor. 5. Convert the resulting fragment from mono to stereo format with specified left and right factors. Function Signature ```python def process_audio(fragment: bytes, width: int, factor: float, lfactor: float, rfactor: float) -> bytes: ``` Input Parameters - `fragment` (bytes): The input audio fragment encoded in linear format. - `width` (int): The width of each sample in bytes (either 1, 2, 3, or 4). - `factor` (float): The factor by which to multiply the linear samples. - `lfactor` (float): The multiplying factor for the left channel when converting to stereo. - `rfactor` (float): The multiplying factor for the right channel when converting to stereo. Output - (bytes): The processed audio fragment in stereo format. Constraints - Sample width must be one of 1, 2, 3, or 4 bytes. - The function should handle edge cases such as empty input fragments gracefully. Example ```python fragment = b\'x01x02x03x04x05x06\' width = 2 factor = 2.0 lfactor = 1.0 rfactor = 0.5 # Expected result processing above parameters output = process_audio(fragment, width, factor, lfactor, rfactor) print(output) ``` Notes You may assume that the input fragment is correctly encoded in linear format, and you do not need to validate this. Focus on correctly chaining the operations as specified. ------ Good luck, and remember to test your solution thoroughly!","solution":"import audioop def process_audio(fragment: bytes, width: int, factor: float, lfactor: float, rfactor: float) -> bytes: Processes an audio fragment through a series of operations: a-LAW encoding, byte-swapping, linear conversion, sample scaling, and mono to stereo conversion. Args: - fragment (bytes): Input audio fragment in linear format. - width (int): The width of each sample in bytes (either 1, 2, 3, or 4). - factor (float): Factor by which to multiply the linear samples. - lfactor (float): Multiplying factor for the left channel in stereo conversion. - rfactor (float): Multiplying factor for the right channel in stereo conversion. Returns: - bytes: The processed audio fragment in stereo format. # Convert the input fragment from linear to a-LAW encoding alaw_encoded = audioop.lin2alaw(fragment, width) # Byte-swap the a-LAW encoded fragment swapped = audioop.byteswap(alaw_encoded, 1) # Width is 1 for a-law encoded data # Convert the byte-swapped fragment back to linear encoding linear = audioop.alaw2lin(swapped, width) # Multiply the linear samples by the given factor amplified = audioop.mul(linear, width, factor) # Convert the resulting fragment from mono to stereo format stereo = audioop.tostereo(amplified, width, lfactor, rfactor) return stereo"},{"question":"Objective Design a function that performs an asynchronous file reading operation with specific constraints, handling various custom exceptions from the `asyncio` package. Problem Statement Write an asynchronous function `async_read_file(file_path: str, buffer_size: int) -> str` that performs the following: 1. Reads the contents of a file at the given `file_path` asynchronously in chunks of size `buffer_size` bytes. 2. If a read operation exceeds a specified timeout (e.g., 5 seconds), it should raise an `asyncio.TimeoutError`. 3. If the read operation is cancelled, catch the `asyncio.CancelledError` exception, log an appropriate message, and re-raise it. 4. If the buffer size limit is reached while looking for a line separator (`n`), raise an `asyncio.LimitOverrunError` with the number of bytes to be consumed. 5. Handle any incomplete read operations by raising an `asyncio.IncompleteReadError` with the expected total length of the file and the partial data read so far. 6. Properly handle and re-raise any other exceptions that may occur. Function Signature ```python import asyncio async def async_read_file(file_path: str, buffer_size: int) -> str: pass ``` Input - `file_path` (str): Path to the file to be read. - `buffer_size` (int): Size of the buffer to read in each chunk. Output - Returns the contents of the file as a string. Constraints - The function should handle files that are up to 1 MB in size. - The function should ensure efficient asynchronous reading and proper exception handling. - Use of `asyncio` package and custom exceptions as specified. Example ```python import asyncio async def main(): # Assume we have a file named \\"example.txt\\" in the current directory file_path = \\"example.txt\\" buffer_size = 1024 # 1 KB buffer size try: contents = await async_read_file(file_path, buffer_size) print(contents) except Exception as e: print(f\\"Error occurred: {e}\\") # Run the example asyncio.run(main()) ``` Notes - Make sure to handle custom exceptions meaningfully and log appropriate messages where necessary. - Test the function with various conditions to ensure robustness.","solution":"import asyncio import os async def async_read_file(file_path: str, buffer_size: int) -> str: timeout = 5 # Timeout for each read in seconds try: # Open the file asynchronously async with aiofiles.open(file_path, mode=\'r\') as file: contents = \\"\\" while True: try: chunk = await asyncio.wait_for(file.read(buffer_size), timeout) if not chunk: break # End of file contents += chunk except asyncio.TimeoutError: raise asyncio.TimeoutError(f\\"Read operation timed out after {timeout} seconds.\\") except asyncio.CancelledError as e: print(f\\"Read operation was cancelled: {str(e)}\\") raise # Re-raise the cancelled error except asyncio.LimitOverrunError: raise asyncio.LimitOverrunError(buffer_size - 1, f\\"Buffer size limit exceeded while looking for a line separator.\\") except asyncio.IncompleteReadError as e: raise asyncio.IncompleteReadError(e.expected, e.partial) # Re-raise with the expected and partial data except Exception as e: # Handling any other exception raise e return contents"},{"question":"**Pandas Assessment Question** **Objective:** Demonstrate your understanding of pandas\' indexing and selection methods by implementing a function that performs various data manipulation tasks on a DataFrame. **Problem Statement:** You are provided with a DataFrame `df` that contains information about a series of transactions. The DataFrame includes the following columns: - `transaction_id`: A unique identifier for each transaction. - `date`: The date when the transaction occurred. - `customer_id`: A unique identifier for the customer who made the transaction. - `amount`: The amount of money spent in the transaction. - `product_id`: A unique identifier for the product that was purchased. Your task is to implement the function `analyze_transactions(df: pd.DataFrame) -> Dict[str, Any]` that returns a dictionary with the following keys and corresponding values: 1. `high_value_transactions`: A DataFrame containing all transactions where the amount is greater than 100, sorted by `amount` in descending order. 2. `customer_total_spent`: A Series where the index is `customer_id` and the values are the total amount spent by each customer, sorted by total amount spent in descending order. 3. `top_product`: The `product_id` of the product that has been purchased the most number of times. 4. `expensive_customers`: A DataFrame containing columns `customer_id`, `date`, and `amount`, with the 5 most recent high-value transactions (amount > 100) for each customer who has made at least one transaction where the amount is greater than 100, sorted by `customer_id` and `date`. 5. `fill_missing_dates`: The DataFrame `df` with any missing dates in the `date` column filled with the most recent non-missing date before the missing value. **Constraints:** - Use pandas\' indexing and selection methods effectively. - Do not use loops to solve the problem. - Handle missing data appropriately where specified. **Function Signature:** ```python import pandas as pd from typing import Dict, Any def analyze_transactions(df: pd.DataFrame) -> Dict[str, Any]: pass ``` **Input:** - `df`: A pandas DataFrame with columns: `transaction_id`, `date`, `customer_id`, `amount`, `product_id`. **Output:** - A dictionary with keys `high_value_transactions`, `customer_total_spent`, `top_product`, `expensive_customers`, and `fill_missing_dates`, each holding the specified data structure as described above. **Example:** ```python data = { \'transaction_id\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \'date\': [\'2023-07-01\', \'2023-07-02\', \'2023-07-03\', pd.NaT, \'2023-07-05\', \'2023-07-06\', \'2023-07-07\', \'2023-07-08\', \'2023-07-09\', \'2023-07-10\'], \'customer_id\': [101, 102, 103, 101, 104, 105, 102, 101, 106, 107], \'amount\': [150, 85, 200, 30, 50, 300, 130, 190, 80, 75], \'product_id\': [1, 2, 1, 3, 4, 2, 2, 1, 3, 4] } df = pd.DataFrame(data) result = analyze_transactions(df) print(result[\'high_value_transactions\']) print(result[\'customer_total_spent\']) print(result[\'top_product\']) print(result[\'expensive_customers\']) print(result[\'fill_missing_dates\']) ``` Ensure to use the pandas functions and methods described in the provided documentation to achieve the required tasks.","solution":"import pandas as pd from typing import Dict, Any def analyze_transactions(df: pd.DataFrame) -> Dict[str, Any]: # 1. High value transactions high_value_transactions = df[df[\'amount\'] > 100].sort_values(by=\'amount\', ascending=False) # 2. Customer total spent customer_total_spent = df.groupby(\'customer_id\')[\'amount\'].sum().sort_values(ascending=False) # 3. Top product top_product = df[\'product_id\'].value_counts().idxmax() # 4. Expensive customers high_value_df = df[df[\'amount\'] > 100] expensive_customers = high_value_df.sort_values(by=[\'customer_id\', \'date\'], ascending=[True, False]).groupby(\'customer_id\').head(5)[[\'customer_id\', \'date\', \'amount\']] # 5. Fill missing dates fill_missing_dates = df.copy() fill_missing_dates[\'date\'] = fill_missing_dates[\'date\'].fillna(method=\'ffill\') return { \'high_value_transactions\': high_value_transactions, \'customer_total_spent\': customer_total_spent, \'top_product\': top_product, \'expensive_customers\': expensive_customers, \'fill_missing_dates\': fill_missing_dates }"},{"question":"**Question Title: Analyzing and Transforming Data Using Scikit-learn Utilities** # Problem Statement You are given a dataset represented as a sparse matrix, and you need to perform several operations and validations on this data using scikit-learn utilities. Your task is to write a function `analyze_and_transform_data` that performs the following steps: 1. **Validation Step**: 1.1 Check if the input data is a 2D array (sparse or dense) and contains no `NaN` or `Inf` values. Raise an appropriate error if the input is invalid. 2. **Data Transformation Step**: 2.1 Compute the means and variances along the rows of the input data. 2.2 Normalize the rows of the input data to have unit L2 norm. 3. **Dimensionality Reduction Step**: 3.1 Perform truncated singular value decomposition (SVD) on the normalized data to reduce it to a specified number of components. # Function Signature ```python from scipy.sparse import csr_matrix import numpy as np def analyze_and_transform_data(data, n_components): Analyzes and transforms the input data using scikit-learn utilities. Parameters: - data (csr_matrix or np.ndarray): The input data, which can be a sparse matrix (CSR format) or a dense numpy array. - n_components (int): The number of components to retain after performing truncated SVD. Returns: - transformed_data (np.ndarray): The transformed data after performing truncated SVD. - means (np.ndarray): The means of the rows of the input data. - variances (np.ndarray): The variances of the rows of the input data. pass ``` # Input 1. **data**: A 2D array (either sparse matrix in CSR format or dense numpy array) containing the input data. The input data will not contain any `NaN` or `Inf` values but needs to be checked for validity. 2. **n_components**: An integer specifying the number of components to retain after performing truncated SVD. # Output 1. **transformed_data**: A numpy array containing the transformed data after performing truncated SVD. 2. **means**: A numpy array containing the means of the rows of the input data. 3. **variances**: A numpy array containing the variances of the rows of the input data. # Constraints - The input data will have dimensions (n_samples, n_features) with n_samples >= n_components and n_features >= n_components. - The truncated SVD should reduce the data to `n_components` dimensions. - The input data can be either a dense numpy array or a sparse matrix in CSR format. # Examples ```python from scipy.sparse import csr_matrix # Example with dense array data_dense = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]) n_components = 2 transformed_data, means, variances = analyze_and_transform_data(data_dense, n_components) print(\\"Transformed Data (Dense):\\", transformed_data) print(\\"Means (Dense):\\", means) print(\\"Variances (Dense):\\", variances) # Example with sparse matrix data_sparse = csr_matrix(data_dense) transformed_data, means, variances = analyze_and_transform_data(data_sparse, n_components) print(\\"Transformed Data (Sparse):\\", transformed_data) print(\\"Means (Sparse):\\", means) print(\\"Variances (Sparse):\\", variances) ``` # Notes - Use `sklearn.utils.check_array` for validation. - Use `sklearn.utils.sparsefuncs.mean_variance_axis` to compute means and variances. - Use `sklearn.utils.sparsefuncs_fast.inplace_csr_row_normalize_l2` to normalize the rows. - Use `sklearn.utils.extmath.randomized_svd` to perform truncated SVD. **Ensure that the implementation efficiently handles both sparse and dense input data.**","solution":"from sklearn.utils import check_array from sklearn.utils.sparsefuncs import mean_variance_axis from sklearn.utils.sparsefuncs_fast import inplace_csr_row_normalize_l2 from sklearn.utils.extmath import randomized_svd import numpy as np from scipy.sparse import issparse, csr_matrix def analyze_and_transform_data(data, n_components): Analyzes and transforms the input data using scikit-learn utilities. Parameters: - data (csr_matrix or np.ndarray): The input data, which can be a sparse matrix (CSR format) or a dense numpy array. - n_components (int): The number of components to retain after performing truncated SVD. Returns: - transformed_data (np.ndarray): The transformed data after performing truncated SVD. - means (np.ndarray): The means of the rows of the input data. - variances (np.ndarray): The variances of the rows of the input data. # Step 1: Validation data = check_array(data, accept_sparse=[\'csr\'], ensure_2d=True, dtype=np.float64, force_all_finite=True) # Step 2: Data Transformation if issparse(data): means, variances = mean_variance_axis(data, axis=1) inplace_csr_row_normalize_l2(data) else: means = np.mean(data, axis=1) variances = np.var(data, axis=1) data = data / np.linalg.norm(data, axis=1, keepdims=True) # Step 3: Dimensionality Reduction U, Sigma, VT = randomized_svd(data, n_components=n_components) transformed_data = np.dot(U, np.diag(Sigma)) return transformed_data, means, variances"},{"question":"Objective: Implement a simple distributed training example using PyTorch\'s Distributed RPC Framework. You will create a setup where one worker will act as the parameter server, and multiple other workers will act as trainers that send gradient updates to the parameter server. Task: 1. **Initialize the Distributed RPC Framework**: - Setup a basic distributed environment with one parameter server and two trainer workers. - Use the TensorPipe backend for RPC communication. 2. **Create the Model and Optimizer**: - Define a simple neural network model that will reside on the parameter server. - Create an RRef to the model on the parameter server that trainers can reference. 3. **Distributed Training Loop**: - Each training worker will run a forward and backward pass locally and send gradient updates to the parameter server. - The parameter server will update the model parameters using a distributed optimizer. 4. **Synchronization and Cleanup**: - Ensure all workers synchronize before shutting down the RPC framework. Constraints and Requirements: - Use synchronous RPC calls for sending gradients and updating parameters. - Ensure that the model on the parameter server can be updated correctly with the gradients from trainers. - Implement the solution with appropriate error handling and synchronization. Expected Input and Output Format: - There is no specific input/output format for this task. The focus is on code implementation and correctness. Performance: - The focus is on correctness and understanding of the distributed RPC framework rather than performance optimization. ```python import os import torch import torch.distributed.rpc as rpc import torch.nn as nn import torch.optim as optim # Define the model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(10, 1) def forward(self, x): return self.linear(x) def init_rpc(rank, world_size): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'29500\' rpc.init_rpc( name=f\'worker{rank}\', rank=rank, world_size=world_size, rpc_backend_options=rpc.TensorPipeRpcBackendOptions() ) def parameter_server(rank, world_size): init_rpc(rank, world_size) model = SimpleModel() model_rref = rpc.RRef(model) # Keep the server running rpc.shutdown() def trainer(rank, world_size, model_rref): init_rpc(rank, world_size) # Simulate training for _ in range(10): input = torch.randn(10) # Dummy input data target = torch.randn(1) # Dummy target data # Forward pass output = model_rref.rpc_sync().forward(input) # Loss and backward pass loss = nn.MSELoss()(output, target) loss.backward() # Send gradients to the parameter server and update parameters rpc.rpc_sync(model_rref.owner(), torch.optim.SGD(model_rref.rpc_sync().parameters(), lr=0.01).step) # Sync and clean up rpc.shutdown() def main(): world_size = 3 rank = int(os.environ[\'RANK\']) # Get rank from the environment variable if rank == 0: parameter_server(rank, world_size) else: # Get the model RRef from the server model_rref = rpc.remote(\\"worker0\\", SimpleModel) trainer(rank, world_size, model_rref) if __name__ == \\"__main__\\": main() ``` Instructions: - Ensure that this script is run with the appropriate settings for distributed training. - You can use multiple processes to simulate different workers. - Validate the implementation and ensure the model parameters are updated correctly on the parameter server.","solution":"import os import torch import torch.distributed.rpc as rpc import torch.nn as nn import torch.optim as optim # Define the model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(10, 1) def forward(self, x): return self.linear(x) def init_rpc(rank, world_size): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'29500\' rpc.init_rpc( name=f\'worker{rank}\', rank=rank, world_size=world_size, rpc_backend_options=rpc.TensorPipeRpcBackendOptions() ) def parameter_server(rank, world_size): init_rpc(rank, world_size) model = SimpleModel() model_rref = rpc.RRef(model) # Keep the server running rpc.shutdown() def trainer(rank, world_size, model_rref): init_rpc(rank, world_size) # Simulate training for _ in range(10): input = torch.randn(10) # Dummy input data target = torch.randn(1) # Dummy target data # Forward pass output = rpc.rpc_sync(model_rref.owner(), SimpleModel.forward, args=(model_rref, input)) # Loss and backward pass loss = nn.MSELoss()(output, target) loss.backward() # Send gradients to the parameter server and update parameters rpc.rpc_sync(model_rref.owner(), torch.optim.SGD(model_rref.rpc_sync().parameters(), lr=0.01).step) # Sync and clean up rpc.shutdown() def main(): world_size = 3 rank = int(os.environ[\'RANK\']) # Get rank from the environment variable if rank == 0: parameter_server(rank, world_size) else: # Get the model RRef from the server model_rref = rpc.remote(\\"worker0\\", SimpleModel) trainer(rank, world_size, model_rref) if __name__ == \\"__main__\\": main()"},{"question":"# Question: Handling and Logging Errors in Web Requests using `urllib.error` You\'re tasked with writing a Python function that makes an HTTP GET request to a given URL and attempts to download data. The function should handle and log different types of errors that might occur during this process. Use the `urllib.request` and `urllib.error` modules to achieve this. Function Signature ```python import logging from urllib.request import urlopen, urlretrieve from urllib.error import URLError, HTTPError, ContentTooShortError def fetch_data(url: str, download: bool = False) -> str: Fetch data from the provided URL. Parameters: url (str): The URL from which to fetch data. download (bool): If True, attempt to download the data using urlretrieve. Default is False. Returns: str: The contents fetched from the URL if successful. Raises: ValueError: If the URL cannot be fetched due to any reason. ``` Requirements 1. If `download` is set to `True`, use `urlretrieve` to download the content from the URL. Otherwise, use `urlopen`. 2. Handle the following exceptions appropriately: - **URLError**: Log the reason of the error. - **HTTPError**: Log the error code and headers. - **ContentTooShortError**: Log the message and amount of content downloaded. 3. If any exception is caught, raise a `ValueError` with an explanatory message. 4. Ensure logging is configured to output messages. Example ```python try: content = fetch_data(\\"http://example.com\\", download=False) print(content) except ValueError as e: print(f\\"Failed to fetch data: {e}\\") ``` Constraints - The URL provided will be a valid HTTP/HTTPS URL. - Network connectivity and permissions to access the URL should be presumed. Note: The actual data fetching should be done within the function; do not mock network requests. # Implementation Note You may use the logging module to track error messages. Initialize the logger at the start of your function. # Logger Configuration Example ```python logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) ``` Create a clear and concise implementation that effectively demonstrates robust error handling using the `urllib.error` module.","solution":"import logging from urllib.request import urlopen, urlretrieve from urllib.error import URLError, HTTPError, ContentTooShortError def fetch_data(url: str, download: bool = False) -> str: Fetch data from the provided URL. Parameters: url (str): The URL from which to fetch data. download (bool): If True, attempt to download the data using urlretrieve. Default is False. Returns: str: The contents fetched from the URL if successful. Raises: ValueError: If the URL cannot be fetched due to any reason. logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) try: if download: file_name, headers = urlretrieve(url) with open(file_name, \'r\') as file: content = file.read() return content else: with urlopen(url) as response: content = response.read().decode() return content except HTTPError as e: logger.error(f\\"HTTPError: Code {e.code}, Reason: {e.reason}, Headers: {e.headers}\\") raise ValueError(f\\"HTTP error occurred: {e.reason}\\") except URLError as e: logger.error(f\\"URLError: Reason {e.reason}\\") raise ValueError(f\\"URL error occurred: {e.reason}\\") except ContentTooShortError as e: logger.error(f\\"ContentTooShortError: Message {e.message}, Downloaded content: {e.content}\\") raise ValueError(f\\"Content too short error occurred: {e.message}\\")"},{"question":"Objective: You are required to implement a Python program that utilizes the deprecated `uu` module to encode and decode given binary data through file-like objects. This exercise checks your understanding of file handling, exception handling, and the usage of the `uu` module functions. Task: 1. Write a function `encode_file_content(input_path: str, encoded_path: str, name: str = None, mode: int = None, backtick: bool = False)` that: - Takes the path of an input binary file (`input_path`). - Encodes its content using uuencode and writes it to the specified output path (`encoded_path`). - Optionally, includes a `name` and `mode` in the uuencode header and uses backticks instead of spaces if specified. 2. Write a function `decode_file_content(encoded_path: str, output_path: str = None, mode: int = None, quiet: bool = False)` that: - Takes the path of an encoded file (`encoded_path`). - Decodes its content and writes it to the specified output path (`output_path`). If not provided, it should use the header information from the encoded file. - Optionally sets file permission bits with `mode` and suppresses warnings if `quiet` is true. Constraints: - Ensure the program handles exceptions appropriately, employing the `uu.Error` where necessary. - Assume the existence of the input file, and you should check if the `output_path` exists to avoid overwriting it without confirmation. Input and Output: - The functions will be tested with various binary files to check the correctness of encoding and decoding. - For simplicity, no UI is required; use file paths and handle files programmatically. Example: ```python encode_file_content(\'example.jpg\', \'example.uue\', name=\'example.jpg\') decode_file_content(\'example.uue\', \'decoded_example.jpg\') # This should result in \'example.jpg\' being uuencoded to \'example.uue\' # and then successfully decoded to \'decoded_example.jpg\' with the same content. ``` Implement these functions and ensure proper handling of file operations and exceptions.","solution":"import uu import os def encode_file_content(input_path: str, encoded_path: str, name: str = None, mode: int = None, backtick: bool = False): Encodes the content of a binary file using uuencode and writes it to the specified output file. Args: input_path (str): Path of the input binary file. encoded_path (str): Path of the output encoded file. name (str): Optional name to include in the uuencode header. mode (int): Optional file permission mode to include in the uuencode header. backtick (bool): Use backticks instead of spaces in encoding if True. if name is None: name = os.path.basename(input_path) if mode is None: mode = os.stat(input_path).st_mode & 0o777 try: with open(input_path, \'rb\') as in_file, open(encoded_path, \'wb\') as out_file: uu.encode(in_file, out_file, name=name, mode=mode, backtick=backtick) except Exception as e: print(f\\"An error occurred while encoding: {e}\\") def decode_file_content(encoded_path: str, output_path: str = None, mode: int = None, quiet: bool = False): Decodes a uuencoded file and writes its content to the specified output binary file. Args: encoded_path (str): Path of the uuencoded file. output_path (str): Path to save the decoded file. If None, uses the header information in the encoded file. mode (int): Optional file permission mode to set on the decoded file. quiet (bool): Suppress warnings if True. try: if output_path: with open(encoded_path, \'rb\') as in_file, open(output_path, \'wb\') as out_file: uu.decode(in_file, out_file, mode=mode, quiet=quiet) else: with open(encoded_path, \'rb\') as in_file: uu.decode(in_file, backtick=True, mode=mode, quiet=quiet) except Exception as e: print(f\\"An error occurred while decoding: {e}\\")"},{"question":"# Question: You are tasked with creating a custom protocol to handle a simple echo server. The server listens for TCP connections, receives messages, and echoes them back to the client. You should implement the protocol and transport that together make the server work. **Objective:** - To implement a custom protocol and use it with the transport to create a fully functional echo server. **Requirements:** 1. Implement a custom protocol for handling connections and data transmission. 2. Use asyncio to create and run the server. 3. The server should handle multiple clients concurrently. 4. The server should echo received messages back to the originating client. **Constraints:** - Use only the low-level asyncio functions like `loop.create_server()`. - The message handling should be non-blocking and asynchronous. - Properly manage open and close events for the connections. **Input:** - There is no direct input required; implement the server setup and protocol. **Output:** - The server must output the following logs: - When a connection is made, log `Connection from <peer address>` - When data is received, log `Data received: <message>` - When data is sent, log `Send: <message>` - When the connection is closed, log `Close the client socket` **Performance:** - The server should be able to handle multiple clients efficiently without causing delays in message echo. **Expected Implementation:** Create a Python file and follow the structure similar to the example but with custom logging and message handling: ```python import asyncio class CustomEchoServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.peername = transport.get_extra_info(\'peername\') print(f\'Connection from {self.peername}\') self.transport = transport def data_received(self, data): message = data.decode() print(f\'Data received: {message}\') print(f\'Send: {message}\') self.transport.write(data) def connection_lost(self, exc): print(\'Close the client socket\') async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: CustomEchoServerProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() asyncio.run(main()) ``` **Note:** - Ensure to test the server with a simple client to verify it correctly echoes messages.","solution":"import asyncio class CustomEchoServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.peername = transport.get_extra_info(\'peername\') print(f\'Connection from {self.peername}\') self.transport = transport def data_received(self, data): message = data.decode() print(f\'Data received: {message}\') print(f\'Send: {message}\') self.transport.write(data) def connection_lost(self, exc): print(\'Close the client socket\') async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: CustomEchoServerProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"Objective Design and implement a set of enums to manage and manipulate file permissions using the `enum` module in Python. Task You are tasked with designing an access control system for files, where each file can have specific permissions assigned to different user roles. You need to use enums to represent these permissions and roles. 1. **Define Permissions**: - Create an `IntFlag` enum called `Permission` with the following members: - `READ` with a value of `4` - `WRITE` with a value of `2` - `EXECUTE` with a value of `1` - `READ_WRITE` as a combination of `READ` and `WRITE` - `ALL` as a combination of `READ`, `WRITE`, and `EXECUTE` 2. **Define User Roles**: - Create an `Enum` class called `Role` with the following members: - `ADMIN` with a value of `Permission.ALL` - `EDITOR` with a value of `Permission.READ_WRITE` - `VIEWER` with a value of `Permission.READ` 3. **Access Control**: - Implement a class `AccessControl` with the following methods: - `__init__(self)`: Initializes an empty dictionary to store file permissions for roles. - `set_permission(self, file: str, role: Role, permission: Permission)`: Sets the permission for a specific file and role. - `check_permission(self, file: str, role: Role, permission: Permission) -> bool`: Checks if a specific role has a specific permission on a file. Returns `True` if the role has the permission, otherwise `False`. 4. **Example Usage**: ```python access_control = AccessControl() access_control.set_permission(\'file1.txt\', Role.ADMIN, Permission.ALL) access_control.set_permission(\'file2.txt\', Role.EDITOR, Permission.READ_WRITE) access_control.set_permission(\'file3.txt\', Role.VIEWER, Permission.READ) assert access_control.check_permission(\'file1.txt\', Role.ADMIN, Permission.EXECUTE) == True assert access_control.check_permission(\'file2.txt\', Role.EDITOR, Permission.WRITE) == True assert access_control.check_permission(\'file3.txt\', Role.VIEWER, Permission.WRITE) == False ``` Constraints - Use the `enum` module as described in the documentation provided. - Ensure that combinations of permissions are handled correctly using bitwise operations. - Your implementation should be efficient and handle multiple permissions settings and checks efficiently. Performance Requirements - The implementation should be able to handle checking permissions for thousands of files and roles efficiently. **Note**: You must demonstrate a good understanding of enums, their creation, and advanced operations (bitwise operations) to solve this problem correctly.","solution":"from enum import Enum, IntFlag class Permission(IntFlag): READ = 4 WRITE = 2 EXECUTE = 1 READ_WRITE = READ | WRITE ALL = READ | WRITE | EXECUTE class Role(Enum): ADMIN = Permission.ALL EDITOR = Permission.READ_WRITE VIEWER = Permission.READ class AccessControl: def __init__(self): self.permissions = {} def set_permission(self, file: str, role: Role, permission: Permission): if file not in self.permissions: self.permissions[file] = {} self.permissions[file][role] = permission def check_permission(self, file: str, role: Role, permission: Permission) -> bool: if file in self.permissions and role in self.permissions[file]: return bool(self.permissions[file][role] & permission) return False"},{"question":"# Garbage Collection Management in Python You have been provided with a partially implemented garbage collection monitoring system. Your task is to complete the implementation by writing functions that utilize the `gc` module to control and monitor garbage collection. **Problem:** Implement the following functions using the `gc` module: 1. `enable_gc()`: Enables automatic garbage collection. 2. `disable_gc()`: Disables automatic garbage collection. 3. `collect_garbage(generation)`: Collects garbage of the specified generation. Returns the number of unreachable objects found. 4. `set_collection_threshold(threshold0, threshold1, threshold2)`: Sets garbage collection thresholds. 5. `get_collection_statistics()`: Returns a dictionary containing garbage collection statistics for each generation. 6. `track_object_status(obj)`: Returns whether an object is currently tracked by the garbage collector. 7. `add_debug_callback(callback)`: Adds a custom callback to be executed before and after garbage collection. The callback should accept two arguments: phase (`\\"start\\"` or `\\"stop\\"`) and info (a dictionary with keys `\\"generation\\"`, `\\"collected\\"`, `\\"uncollectable\\"`). 8. `remove_debug_callbacks()`: Removes all custom callbacks from the garbage collector. **Input and Output:** 1. `enable_gc()`: - **Input:** None - **Output:** None 2. `disable_gc()`: - **Input:** None - **Output:** None 3. `collect_garbage(generation)`: - **Input:** an integer `generation` (0, 1, or 2) - **Output:** an integer representing the number of unreachable objects found 4. `set_collection_threshold(threshold0, threshold1, threshold2)`: - **Input:** three integers `threshold0`, `threshold1`, `threshold2` - **Output:** None 5. `get_collection_statistics()`: - **Input:** None - **Output:** a dictionary with keys `collection_stats`, where each key is a generation number (0, 1, 2) and the value is another dictionary with keys `\\"collections\\"`, `\\"collected\\"`, `\\"uncollectable\\"` 6. `track_object_status(obj)`: - **Input:** any object `obj` - **Output:** a boolean indicating if the object is tracked by the garbage collector 7. `add_debug_callback(callback)`: - **Input:** a function `callback` which takes two arguments (`phase` and `info`) - **Output:** None 8. `remove_debug_callbacks()`: - **Input:** None - **Output:** None **Constraints:** - Use the provided `gc` module functions to implement the above methods. - Ensure the `callback` provided to `add_debug_callback` is compatible with the expected signature. **Example Usage:** ```python import gc def example_callback(phase, info): print(f\\"GC {phase} - Generation: {info[\'generation\']}, Collected: {info.get(\'collected\', \'N/A\')}, Uncollectable: {info.get(\'uncollectable\', \'N/A\')}\\") enable_gc() assert gc.isenabled() == True disable_gc() assert gc.isenabled() == False set_collection_threshold(700, 10, 10) print(get_collection_statistics()) # Output: {0: {\'collections\': 0, \'collected\': 0, \'uncollectable\': 0}, 1: {...}, 2: {...}} add_debug_callback(example_callback) collect_garbage(2) # Triggers the callback before and after collection remove_debug_callbacks() collect_garbage(2) # No callback triggered this time obj = [] assert track_object_status(obj) == True print(\\"All tests passed!\\") ```","solution":"import gc def enable_gc(): Enables automatic garbage collection. gc.enable() def disable_gc(): Disables automatic garbage collection. gc.disable() def collect_garbage(generation): Collects garbage of the specified generation. :param generation: The generation to collect (0, 1, or 2) :return: The number of unreachable objects found return gc.collect(generation) def set_collection_threshold(threshold0, threshold1, threshold2): Sets the garbage collection thresholds. :param threshold0: Threshold for generation 0 :param threshold1: Threshold for generation 1 :param threshold2: Threshold for generation 2 gc.set_threshold(threshold0, threshold1, threshold2) def get_collection_statistics(): Returns garbage collection statistics for each generation. :return: A dictionary with keys 0, 1, and 2 containing collections, collected, and uncollectable counts. stats = {} for i in range(3): stats[i] = { \\"collections\\": gc.get_count()[i], \\"collected\\": gc.get_stats()[i][\'collected\'], \\"uncollectable\\": gc.get_stats()[i][\'uncollectable\'], } return stats def track_object_status(obj): Returns whether an object is currently tracked by the garbage collector. :param obj: The object to check :return: True if tracked, False if not return gc.is_tracked(obj) def add_debug_callback(callback): Adds a custom callback to be executed before and after garbage collection. :param callback: The callback function that takes two arguments (phase and info) def gc_callback(phase, info): callback(phase, info) gc.callbacks.append(gc_callback) def remove_debug_callbacks(): Removes all custom callbacks from the garbage collector. gc.callbacks.clear()"},{"question":"# Problem: Unix User Account Analysis You are provided access to a Unix system and need to analyze user account information using the `pwd` module. To demonstrate your understanding of this module, you will implement a set of functions to retrieve and process user account data. Task Implement the following functions: 1. **get_user_by_uid(uid)** - **Input:** `uid` (integer) - a numerical user ID. - **Output:** Dictionary containing user information corresponding to the input `uid`. The dictionary should have keys: `\\"pw_name\\"`, `\\"pw_passwd\\"`, `\\"pw_uid\\"`, `\\"pw_gid\\"`, `\\"pw_gecos\\"`, `\\"pw_dir\\"`, `\\"pw_shell\\"`. Raise a `KeyError` if no user with the given `uid` is found. 2. **get_user_by_name(name)** - **Input:** `name` (string) - a login name. - **Output:** Dictionary containing user information corresponding to the input `name`. The dictionary should have keys: `\\"pw_name\\"`, `\\"pw_passwd\\"`, `\\"pw_uid\\"`, `\\"pw_gid\\"`, `\\"pw_gecos\\"`, `\\"pw_dir\\"`, `\\"pw_shell\\"`. Raise a `KeyError` if no user with the given `name` is found. 3. **list_all_users()** - **Output:** List of dictionaries, each representing user information for all available users. Each dictionary should have keys: `\\"pw_name\\"`, `\\"pw_passwd\\"`, `\\"pw_uid\\"`, `\\"pw_gid\\"`, `\\"pw_gecos\\"`, `\\"pw_dir\\"`, `\\"pw_shell\\"`. 4. **find_users_with_common_shell()** - **Output:** List of tuples, where each tuple contains a shell name and a list of user names (strings) who use that shell. The output list should be sorted by shell names. Example ```python # Example user dictionary format { \\"pw_name\\": \\"username\\", \\"pw_passwd\\": \\"password\\", \\"pw_uid\\": 1000, \\"pw_gid\\": 1000, \\"pw_gecos\\": \\"User Name\\", \\"pw_dir\\": \\"/home/username\\", \\"pw_shell\\": \\"/bin/bash\\" } # Example function usage: uid_info = get_user_by_uid(1000) # Expected Output: dictionary with user details for uid 1000 name_info = get_user_by_name(\\"username\\") # Expected Output: dictionary with user details for user \\"username\\" all_users = list_all_users() # Expected Output: list of dictionaries with details of all users common_shells = find_users_with_common_shell() # Expected Output: list of tuples (shell, list of users), e.g., [(\'/bin/bash\', [\'user1\', \'user2\']), (\'/bin/zsh\', [\'user3\'])] ``` Constraints - The Unix system where this script is tested must support the `pwd` module. - The `uid` provided to the function `get_user_by_uid` will always be a non-negative integer. - The `name` provided to the function `get_user_by_name` will always be a non-empty string. - The number of users returned by `list_all_users` could be large, so your implementation should be efficient. Note You may assume the Unix system for testing has a variety of users and shells to demonstrate the functionality effectively.","solution":"import pwd def get_user_by_uid(uid): Returns user information for the given user ID. Args: uid (int): User ID. Returns: dict: Dictionary containing user information. Raises: KeyError: If no user with the given UID is found. try: user_info = pwd.getpwuid(uid) return { \\"pw_name\\": user_info.pw_name, \\"pw_passwd\\": user_info.pw_passwd, \\"pw_uid\\": user_info.pw_uid, \\"pw_gid\\": user_info.pw_gid, \\"pw_gecos\\": user_info.pw_gecos, \\"pw_dir\\": user_info.pw_dir, \\"pw_shell\\": user_info.pw_shell } except KeyError: raise KeyError(f\\"No user with UID {uid} found.\\") def get_user_by_name(name): Returns user information for the given login name. Args: name (str): Login name. Returns: dict: Dictionary containing user information. Raises: KeyError: If no user with the given name is found. try: user_info = pwd.getpwnam(name) return { \\"pw_name\\": user_info.pw_name, \\"pw_passwd\\": user_info.pw_passwd, \\"pw_uid\\": user_info.pw_uid, \\"pw_gid\\": user_info.pw_gid, \\"pw_gecos\\": user_info.pw_gecos, \\"pw_dir\\": user_info.pw_dir, \\"pw_shell\\": user_info.pw_shell } except KeyError: raise KeyError(f\\"No user with name {name} found.\\") def list_all_users(): Returns a list of dictionaries containing user information for all users. Returns: list of dict: List of dictionaries with user information. all_users = [] for user_info in pwd.getpwall(): all_users.append({ \\"pw_name\\": user_info.pw_name, \\"pw_passwd\\": user_info.pw_passwd, \\"pw_uid\\": user_info.pw_uid, \\"pw_gid\\": user_info.pw_gid, \\"pw_gecos\\": user_info.pw_gecos, \\"pw_dir\\": user_info.pw_dir, \\"pw_shell\\": user_info.pw_shell }) return all_users def find_users_with_common_shell(): Returns a list of tuples containing shells and the list of user names using them. Returns: list of tuples: List of (shell, list of user names) tuples. shell_user_map = {} for user_info in pwd.getpwall(): shell = user_info.pw_shell user_name = user_info.pw_name if shell not in shell_user_map: shell_user_map[shell] = [] shell_user_map[shell].append(user_name) return sorted(shell_user_map.items())"},{"question":"Using the Python `random` module, implement a function `simulate_random_variables` that performs the following tasks: 1. Generate a list of `n` random integers between `a` and `b` (inclusive). 2. Generate a list of `n` random floating-point numbers from a uniform distribution in the range [0.0, 1.0). 3. Generate a list of `n` random samples from a given list `population`. 4. Generate a list of `n` random floating-point numbers from a normal (Gaussian) distribution with a mean `mu` and standard deviation `sigma`. 5. Shuffle a given sequence in-place. 6. Generate `n` random bytes. The function should accept the following parameters: - `n` (int): Number of random values to generate for the lists. - `a` (int): Lower bound for generating random integers. - `b` (int): Upper bound for generating random integers. - `population` (list): A list from which random samples are to be picked. - `mu` (float): Mean of the normal distribution. - `sigma` (float): Standard deviation of the normal distribution. - `seq` (list): A sequence to shuffle. - `bytes_n` (int): Number of random bytes to generate. The function should return a dictionary with the generated values as follows: ```python { \'random_integers\': list_of_random_integers, \'random_floats_uniform\': list_of_random_floats_uniform, \'random_samples\': list_of_random_samples, \'random_floats_gaussian\': list_of_random_floats_gaussian, \'shuffled_sequence\': shuffled_sequence, \'random_bytes\': random_bytes } ``` # Input Constraints - `n`, `bytes_n` should be a positive integer. - `a` should be less than or equal to `b`. - `population` should be a non-empty list. - `mu` (float): any valid float value. - `sigma` (float): positive float value. - `seq` (list): non-empty list. # Example Usage ```python result = simulate_random_variables( n=5, a=1, b=10, population=[\'apple\', \'banana\', \'cherry\'], mu=0, sigma=1, seq=[1, 2, 3, 4, 5], bytes_n=8 ) print(result) ``` The output dictionary may look like: ```python { \'random_integers\': [3, 7, 5, 4, 9], \'random_floats_uniform\': [0.39, 0.59, 0.76, 0.85, 0.12], \'random_samples\': [\'banana\', \'cherry\', \'apple\', \'banana\', \'apple\'], \'random_floats_gaussian\': [-0.23, 0.94, -1.07, 1.38, 0.51], \'shuffled_sequence\': [2, 5, 3, 1, 4], \'random_bytes\': b\'xa3xf4x91xbdx23x4cx8exfa\' } ``` # Function Signature ```python def simulate_random_variables(n: int, a: int, b: int, population: list, mu: float, sigma: float, seq: list, bytes_n: int) -> dict: pass ``` # Performance Requirements - Ensure the function runs efficiently for the constraints specified. - Properly manage the state of the random number generator to ensure reproducibility if seeds were to be introduced or tested separately (though not a direct requirement here).","solution":"import random def simulate_random_variables(n: int, a: int, b: int, population: list, mu: float, sigma: float, seq: list, bytes_n: int) -> dict: result = {} # Generate n random integers between a and b (inclusive) result[\'random_integers\'] = [random.randint(a, b) for _ in range(n)] # Generate n random floating-point numbers from a uniform distribution in the range [0.0, 1.0) result[\'random_floats_uniform\'] = [random.uniform(0.0, 1.0) for _ in range(n)] # Generate n random samples from the given list population result[\'random_samples\'] = [random.choice(population) for _ in range(n)] # Generate n random floating-point numbers from a normal (Gaussian) distribution with mean mu and standard deviation sigma result[\'random_floats_gaussian\'] = [random.gauss(mu, sigma) for _ in range(n)] # Shuffle the given sequence in-place shuffled_sequence = seq[:] # create a copy of the sequence to shuffle random.shuffle(shuffled_sequence) result[\'shuffled_sequence\'] = shuffled_sequence # Generate n random bytes result[\'random_bytes\'] = random.randbytes(bytes_n) return result"},{"question":"Coding Assessment Question # Objective Design and implement a function to securely store and verify passwords using the `crypt` module provided in Python 3.10. # Function 1: `hash_password` **Description**: This function takes a plain-text password and returns its hashed value using the strongest available hashing method. **Input**: - A string `password`, representing the plain-text password. **Output**: - A hashed password string. ```python def hash_password(password: str) -> str: Hashes a given password using the strongest available hashing method. Args: password (str): The plain-text password to hash. Returns: str: The hashed password. pass ``` # Function 2: `verify_password` **Description**: This function verifies a plain-text password against a previously hashed password. **Input**: - A string `password`, representing the plain-text password. - A string `hashed_password`, representing the previously hashed password. **Output**: - A boolean value indicating whether the password is correct (True) or not (False). ```python def verify_password(password: str, hashed_password: str) -> bool: Verifies the given password against the previously hashed password. Args: password (str): The plain-text password to verify. hashed_password (str): The hashed password to check against. Returns: bool: True if the password is correct, False otherwise. pass ``` # Constraints and Requirements 1. Your solution should handle the hashing using the strongest available method by default. 2. You must ensure the functions handle edge cases, such as empty passwords. 3. You need to use the `hmac.compare_digest` function in `verify_password` to perform a constant-time comparison to prevent timing attacks. 4. Efficiency is key, and your solution should be scalable for use with multiple password verifications in a short time frame. # Example Usage ```python # Example usage of hash_password and verify_password functions. plain_password = \\"securepassword123\\" hashed = hash_password(plain_password) # Later on... is_correct = verify_password(\\"securepassword123\\", hashed) assert is_correct == True, \\"The password should be verified successfully.\\" # For an incorrect password is_correct = verify_password(\\"wrongpassword\\", hashed) assert is_correct == False, \\"The password verification should fail.\\" ``` # Notes - The `crypt` module is deprecated as of Python 3.11 but is available for Python 3.10. - The hashed password string returned from `hash_password` should include the salt information. Implement these functions in Python and ensure they perform as described. Your code will be evaluated based on correctness, efficiency, and security considerations.","solution":"import crypt import hmac def hash_password(password: str) -> str: Hashes a given password using the strongest available hashing method. Args: password (str): The plain-text password to hash. Returns: str: The hashed password. # crypt.mksalt() generates a salt in the strongest available method salt = crypt.mksalt(crypt.METHOD_SHA512) hashed_password = crypt.crypt(password, salt) return hashed_password def verify_password(password: str, hashed_password: str) -> bool: Verifies the given password against the previously hashed password. Args: password (str): The plain-text password to verify. hashed_password (str): The hashed password to check against. Returns: bool: True if the password is correct, False otherwise. return hmac.compare_digest(crypt.crypt(password, hashed_password), hashed_password)"},{"question":"**Unicode String Manipulation and Encoding Transformation** # Problem Statement You are given a text document that contains various Unicode characters from different languages. Your task is to implement a function that performs the following operations on this document. 1. **Remove Non-ASCII characters**: Convert the text by removing any non-ASCII characters. 2. **Transform All Digits To ASCII \'0\'**: Replace any digit character (0-9) with the ASCII character \'0\'. 3. **Convert Uppercase Characters to Lowercase**: Convert all uppercase characters in the text to their lowercase equivalents. 4. **Encode the Resulting String to a Specific Encoding**: The output should be encoded using UTF-8 encoding. # Input - `text` (str): A Unicode string that represents the content of the text document. # Output - `bytes`: A UTF-8 encoded version of the processed string. # Constraints - The input string can contain characters from different Unicode ranges. - The processing should efficiently handle large strings up to (10^6) characters in length. # Example ```python def transform_unicode(text: str) -> bytes: pass # Example Usage text = \\"Hello, 世界! 123, Python 🐍\\" encoded_text = transform_unicode(text) print(encoded_text) ``` Expected output (in bytes): ``` b\'hello, ! 000, python \' ``` # Function Signature ```python def transform_unicode(text: str) -> bytes: # Your implementation here pass ``` - **Note**: - Use the appropriate Unicode and Codec APIs as mentioned in the documentation. - Ensure that your implementation is efficient even for large inputs. ---","solution":"def transform_unicode(text: str) -> bytes: Perform specific operations on the input unicode text and returns the result as UTF-8 encoded bytes. result = [] for char in text: if ord(char) < 128: # ASCII character if char.isdigit(): result.append(\'0\') elif char.isupper(): result.append(char.lower()) else: result.append(char) # Non-ASCII characters are ignored return \'\'.join(result).encode(\'utf-8\')"},{"question":"**Question: Optimizing the Performance of a Scikit-Learn Model** You are provided with a dataset and your task is to train a machine learning model to predict a target variable. Additionally, you must implement performance optimizations based on the following criteria: 1. Train a linear regression model using scikit-learn. 2. Measure and output the prediction latency and throughput. 3. Optimize the model for sparsity and measure the impact on latency and throughput. # Input: 1. A training dataset in CSV format (training_data.csv) with features as columns and the target variable as the last column. 2. A test dataset in CSV format (test_data.csv) with the same structure. # Output: 1. The prediction latency (in microseconds) for the test dataset. 2. The prediction throughput (in predictions per second) for the test dataset. 3. The prediction latency and throughput after applying model compression for sparsity. # Constraints: 1. The model should be a linear regression model. 2. You must use the `elasticnet` penalty to enforce sparsity. 3. The `l1_ratio` for the `elasticnet` should be set to 0.25. 4. You may assume the data does not contain NaN or infinite values. # Implementation Steps: 1. **Load the Data**: Implement a function to load the training and test datasets. 2. **Train the Model**: Implement a function to train a linear regression model with `elasticnet` penalty. 3. **Measure Performance**: Implement functions to measure prediction latency and throughput. 4. **Optimize for Sparsity**: Implement a function to apply model compression and re-measure performance. # Starter Code: ```python import time import numpy as np import pandas as pd from sklearn.linear_model import SGDRegressor from sklearn.utils import assert_all_finite # Step 1: Load the Data def load_data(train_file, test_file): train_data = pd.read_csv(train_file) test_data = pd.read_csv(test_file) X_train = train_data.iloc[:, :-1].values y_train = train_data.iloc[:, -1].values X_test = test_data.iloc[:, :-1].values y_test = test_data.iloc[:, -1].values return X_train, y_train, X_test, y_test # Step 2: Train the Model def train_model(X_train, y_train): model = SGDRegressor(penalty=\'elasticnet\', l1_ratio=0.25) model.fit(X_train, y_train) return model # Step 3: Measure Performance def measure_performance(model, X_test): start_time = time.time() y_pred = model.predict(X_test) end_time = time.time() latency = (end_time - start_time) * 1e6 # in microseconds throughput = len(y_pred) / (end_time - start_time) # in predictions per second return latency, throughput # Step 4: Optimize for Sparsity def optimize_model(model): model.sparsify() return model # Main Function def main(): X_train, y_train, X_test, y_test = load_data(\'training_data.csv\', \'test_data.csv\') # Train the model model = train_model(X_train, y_train) # Measure initial performance initial_latency, initial_throughput = measure_performance(model, X_test) # Optimize for sparsity model = optimize_model(model) # Measure performance after optimization optimized_latency, optimized_throughput = measure_performance(model, X_test) # Print results print(f\\"Initial Latency: {initial_latency} microseconds\\") print(f\\"Initial Throughput: {initial_throughput} predictions/second\\") print(f\\"Optimized Latency: {optimized_latency} microseconds\\") print(f\\"Optimized Throughput: {optimized_throughput} predictions/second\\") main() ``` **Note:** Make sure to have scikit-learn installed in your environment, and prepare the CSV files with proper data before running the code.","solution":"import time import numpy as np import pandas as pd from sklearn.linear_model import SGDRegressor # Step 1: Load the Data def load_data(train_file, test_file): train_data = pd.read_csv(train_file) test_data = pd.read_csv(test_file) X_train = train_data.iloc[:, :-1].values y_train = train_data.iloc[:, -1].values X_test = test_data.iloc[:, :-1].values y_test = test_data.iloc[:, -1].values return X_train, y_train, X_test, y_test # Step 2: Train the Model def train_model(X_train, y_train): model = SGDRegressor(penalty=\'elasticnet\', l1_ratio=0.25) model.fit(X_train, y_train) return model # Step 3: Measure Performance def measure_performance(model, X_test): start_time = time.time() y_pred = model.predict(X_test) end_time = time.time() latency = (end_time - start_time) * 1e6 # in microseconds throughput = len(y_pred) / (end_time - start_time) # in predictions per second return latency, throughput # Step 4: Optimize for Sparsity def optimize_model(model): model.sparsify() return model # Main Function def main(train_file, test_file): X_train, y_train, X_test, y_test = load_data(train_file, test_file) # Train the model model = train_model(X_train, y_train) # Measure initial performance initial_latency, initial_throughput = measure_performance(model, X_test) # Optimize for sparsity model = optimize_model(model) # Measure performance after optimization optimized_latency, optimized_throughput = measure_performance(model, X_test) # Return results return { \\"initial_latency\\": initial_latency, \\"initial_throughput\\": initial_throughput, \\"optimized_latency\\": optimized_latency, \\"optimized_throughput\\": optimized_throughput }"},{"question":"# Advanced Coding Assessment Question: Asynchronous Programming with Coroutines Objective: Assess the student\'s understanding of asynchronous programming in Python, particularly focusing on the use of coroutine objects created with the `async` keyword. Question: You are tasked with developing a simple asynchronous task scheduler in Python. The scheduler must be able to queue tasks, execute them concurrently, and handle their completion. The goal is to implement a class `TaskScheduler` that allows adding coroutine tasks and running them concurrently. Requirements: 1. Implement a class **`TaskScheduler`**: - The class should have a method `add_task(coroutine_fn: Callable, *args, **kwargs)` to add a new coroutine task to the scheduler. - The class should have a method `run()` that executes all the added tasks concurrently and returns their results as a list. 2. The `add_task` method: - Should accept a coroutine function and its arguments. - Should store the coroutine for later execution. 3. The `run` method: - Should run all the added coroutine tasks concurrently. - Should return a list of results from the executed coroutine tasks. Constraints: - You must use the `asyncio` module to manage concurrency. - The tasks should be executed concurrently and not sequentially. - You may assume all coroutine functions passed to `add_task` return a value that can be awaited. Example: ```python import asyncio # Sample coroutine functions async def sample_task_1(x): await asyncio.sleep(1) return f\\"Task 1 completed with {x}\\" async def sample_task_2(y): await asyncio.sleep(2) return f\\"Task 2 completed with {y}\\" # Implementation of TaskScheduler class TaskScheduler: def __init__(self): self.tasks = [] def add_task(self, coroutine_fn, *args, **kwargs): self.tasks.append(coroutine_fn(*args, **kwargs)) async def run(self): results = await asyncio.gather(*self.tasks) return results # Example Usage if __name__ == \\"__main__\\": scheduler = TaskScheduler() scheduler.add_task(sample_task_1, 10) scheduler.add_task(sample_task_2, 20) results = asyncio.run(scheduler.run()) print(results) # Expected Output: # [\\"Task 1 completed with 10\\", \\"Task 2 completed with 20\\"] ``` Implementation Hints: - Use `asyncio.create_task` or similar functions to create and manage task execution. - Use `asyncio.gather` to run the tasks concurrently and collect their results.","solution":"import asyncio from typing import Callable, List class TaskScheduler: def __init__(self): self.tasks = [] def add_task(self, coroutine_fn: Callable, *args, **kwargs): self.tasks.append(coroutine_fn(*args, **kwargs)) async def run(self) -> List: results = await asyncio.gather(*self.tasks) return results # Example coroutine functions used for testing async def sample_task_1(x): await asyncio.sleep(1) return f\\"Task 1 completed with {x}\\" async def sample_task_2(y): await asyncio.sleep(2) return f\\"Task 2 completed with {y}\\""},{"question":"Coding Assessment Question # Objective To assess the understanding and ability to implement a combination of different unsupervised learning techniques available in scikit-learn. # Problem Statement You are given a dataset containing multiple features. Your task is to perform the following steps: 1. **Dimensionality Reduction:** First, reduce the dimensionality of the dataset using Principal Component Analysis (PCA). 2. **Clustering:** - Cluster the reduced data using K-Means. - Perform a hierarchical clustering to add an additional layer of analysis. 3. **Outlier Detection:** Identify any outliers in the original dataset using the Isolation Forest algorithm. # Requirements 1. **Principal Component Analysis (PCA)** - **Input:** A 2-D array `X` of shape `(n_samples, n_features)` containing the dataset. - **Output:** A 2-D array `X_pca` of shape `(n_samples, n_components)` with reduced dimensions `n_components`. 2. **K-Means Clustering** - **Input:** A 2-D array `X_pca` of shape `(n_samples, n_components)` containing the reduced dataset. - **Output:** A 1-D array `kmeans_labels` of shape `(n_samples,)` representing the cluster each sample belongs to. 3. **Hierarchical Clustering** - **Input:** A 2-D array `X_pca` of shape `(n_samples, n_components)` containing the reduced dataset. - **Output:** A 1-D array `hierarchical_labels` of shape `(n_samples,)` representing the cluster each sample belongs to. 4. **Outlier Detection** - **Input:** A 2-D array `X` of shape `(n_samples, n_features)` containing the original dataset. - **Output:** A 1-D array `outliers` of shape `(n_samples,)` where `1` indicates an outlier and `0` indicates an inlier. # Constraints - The number of components for PCA (`n_components`) should be 2. - Use `n_clusters = 3` for both K-Means and Hierarchical clustering. - Use default parameters for Isolation Forest. # Function Signatures ```python def apply_pca(X: np.ndarray, n_components: int = 2) -> np.ndarray: pass def k_means_clustering(X_pca: np.ndarray, n_clusters: int = 3) -> np.ndarray: pass def hierarchical_clustering(X_pca: np.ndarray, n_clusters: int = 3) -> np.ndarray: pass def detect_outliers(X: np.ndarray) -> np.ndarray: pass ``` # Example ```python import numpy as np # Sample Data X = np.array([ [1.0, 2.0, 3.0, 4.0], [5.0, 5.0, 6.0, 7.0], [1.1, 2.1, 3.1, 4.1], [0.9, 1.9, 2.9, 3.9], [6.0, 6.1, 7.2, 8.3], [5.5, 5.4, 6.3, 7.3], [1.2, 2.3, 3.4, 4.5], ]) # Apply PCA X_pca = apply_pca(X) # K-Means Clustering kmeans_labels = k_means_clustering(X_pca) # Hierarchical Clustering hierarchical_labels = hierarchical_clustering(X_pca) # Detect Outliers outliers = detect_outliers(X) print(\\"PCA Result:n\\", X_pca) print(\\"K-Means Labels:n\\", kmeans_labels) print(\\"Hierarchical Labels:n\\", hierarchical_labels) print(\\"Outliers:n\\", outliers) ``` # Note - You must handle any necessary imports from sklearn, numpy, and other libraries yourself. - Ensure your code is well-documented and includes appropriate error handling. - Evaluate your implementation on a sample dataset, and explain your results.","solution":"import numpy as np from sklearn.decomposition import PCA from sklearn.cluster import KMeans, AgglomerativeClustering from sklearn.ensemble import IsolationForest def apply_pca(X: np.ndarray, n_components: int = 2) -> np.ndarray: Reduce dimensionality of the dataset using PCA. :param X: 2-D array of shape (n_samples, n_features) containing the dataset. :param n_components: Number of principal components to keep. :return: 2-D array of shape (n_samples, n_components) with reduced dimensions. pca = PCA(n_components=n_components) X_pca = pca.fit_transform(X) return X_pca def k_means_clustering(X_pca: np.ndarray, n_clusters: int = 3) -> np.ndarray: Cluster the reduced data using K-Means. :param X_pca: 2-D array of shape (n_samples, n_components) containing the reduced dataset. :param n_clusters: Number of clusters to form. :return: 1-D array of shape (n_samples,) representing the cluster each sample belongs to. kmeans = KMeans(n_clusters=n_clusters) kmeans_labels = kmeans.fit_predict(X_pca) return kmeans_labels def hierarchical_clustering(X_pca: np.ndarray, n_clusters: int = 3) -> np.ndarray: Perform hierarchical clustering on the reduced dataset. :param X_pca: 2-D array of shape (n_samples, n_components) containing the reduced dataset. :param n_clusters: Number of clusters to form. :return: 1-D array of shape (n_samples,) representing the cluster each sample belongs to. hierarchical = AgglomerativeClustering(n_clusters=n_clusters) hierarchical_labels = hierarchical.fit_predict(X_pca) return hierarchical_labels def detect_outliers(X: np.ndarray) -> np.ndarray: Identify outliers in the original dataset using Isolation Forest. :param X: 2-D array of shape (n_samples, n_features) containing the original dataset. :return: 1-D array of shape (n_samples,) where 1 indicates an outlier and 0 indicates an inlier. isolation_forest = IsolationForest() outliers = isolation_forest.fit_predict(X) outliers = np.where(outliers == -1, 1, 0) return outliers"},{"question":"Objective: Design a log monitoring solution using the `RotatingFileHandler` from the `logging.handlers` module. Your solution should demonstrate understanding of setting up a logging system, configuring log rotation based on file size, and ensuring proper log formatting. Additionally, create a custom logging handler that filters logs based on log levels. Task: 1. **Define a function `setup_logging()`**: - **Input**: - `filename` (string): The name of the log file. - `maxBytes` (int): Maximum log file size in bytes before it rolls over. - `backupCount` (int): Number of backup files to keep. - **Output**: None - **Implementation**: - Configure a `RotatingFileHandler` to manage log files, rolling over when the specified file size limit is reached. - Set up a log formatter to include the timestamp, log level, and message in the log entries. 2. **Define a class `LevelFilterHandler(logging.Handler)`**: - This custom handler should filter log records to only handle logs that are above a specified log level. - **Methods**: - `__init__(self, level)`: Initialize with a specified log level. - `emit(self, record)`: Emit log records that meet the log level criteria. 3. **Integration and Testing**: - Use the `setup_logging()` function to configure a logger. - Add an instance of `LevelFilterHandler` to filter and print logs of `ERROR` level and above to the console. - Generate log messages of various levels (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`) to test the configuration. Script Execution Expectations: - The script should create log files with a proper timestamp, log level, and messages. - The log files should rotate when they exceed the specified size, keeping the number of backup files specified. - Only `ERROR` and `CRITICAL` logs should be printed to the console. ```python import logging from logging.handlers import RotatingFileHandler def setup_logging(filename, maxBytes, backupCount): # Setup the logger logger = logging.getLogger() logger.setLevel(logging.DEBUG) # Create a rotating file handler handler = RotatingFileHandler(filename, maxBytes=maxBytes, backupCount=backupCount) formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\') handler.setFormatter(formatter) logger.addHandler(handler) class LevelFilterHandler(logging.Handler): def __init__(self, level): super().__init__() self.level = level def emit(self, record): if record.levelno >= self.level: print(self.format(record)) # Configure logging setup_logging(\'app.log\', 1024, 3) # Create a logger instance logger = logging.getLogger() # Add the level filter handler to filter ERROR and above levels level_filter_handler = LevelFilterHandler(logging.ERROR) formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\') level_filter_handler.setFormatter(formatter) logger.addHandler(level_filter_handler) # Log messages of various levels for testing logger.debug(\'This is a debug message\') logger.info(\'This is an info message\') logger.warning(\'This is a warning message\') logger.error(\'This is an error message\') logger.critical(\'This is a critical message\') ``` Constraints: - Ensure that log rotation works correctly and adheres to the specified `maxBytes` and `backupCount`. - The script should run without external dependencies other than the Python standard library. Performance Considerations: - The system should handle log rotation efficiently without losing log data. - Ensure that high-frequency logging is managed without causing performance degradation.","solution":"import logging from logging.handlers import RotatingFileHandler def setup_logging(filename, maxBytes, backupCount): Setup logging configuration with a RotatingFileHandler. :param filename: The name of the log file. :param maxBytes: Maximum log file size in bytes before it rolls over. :param backupCount: Number of backup files to keep. # Setup the root logger logger = logging.getLogger() logger.setLevel(logging.DEBUG) # Create a rotating file handler handler = RotatingFileHandler(filename, maxBytes=maxBytes, backupCount=backupCount) formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\') handler.setFormatter(formatter) logger.addHandler(handler) class LevelFilterHandler(logging.Handler): def __init__(self, level): Initialize the handler with the specified log level. :param level: The log level threshold for this handler. super().__init__() self.level = level def emit(self, record): Emit log records that meet the log level criteria. :param record: The log record to be emitted. if record.levelno >= self.level: print(self.format(record)) # Example Usage of setup_logging and LevelFilterHandler if __name__ == \\"__main__\\": # Configure logging setup_logging(\'app.log\', 1024, 3) # Create a logger instance logger = logging.getLogger() # Add the level filter handler to filter ERROR and above levels level_filter_handler = LevelFilterHandler(logging.ERROR) formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\') level_filter_handler.setFormatter(formatter) logger.addHandler(level_filter_handler) # Log messages of various levels for testing logger.debug(\'This is a debug message\') logger.info(\'This is an info message\') logger.warning(\'This is a warning message\') logger.error(\'This is an error message\') logger.critical(\'This is a critical message\')"},{"question":"**Objective:** Your task is to demonstrate your understanding of scikit-learn\'s utility functions specifically for data validation and preprocessing. **Problem Statement:** You are provided with a dataset containing features and labels. Your job is to preprocess this data and prepare it for training a machine learning model. You need to utilize scikit-learn\'s validation tools to ensure that the data is appropriate for the machine learning algorithm. **Steps:** 1. **Data Loading:** Load the dataset from a provided CSV file. The CSV file has the following columns: - `feature_1`, `feature_2`, ..., `feature_n`: These are the feature columns. - `label`: This is the target label for classification. 2. **Data Validation:** Utilize scikit-learn\'s utility functions to perform the following checks: - Ensure there are no missing values (NaNs or Infs) in the dataset. - Ensure the feature matrix is in 2D array format and the label vector is in 1D array format. - Ensure that the number of samples in the feature matrix matches the number of labels. 3. **Preprocessing:** - Convert the feature matrix to a float array. - Standardize the features to have zero mean and unit variance. 4. **Implementation:** Implement a function `preprocess_data` that accepts a file path to the CSV file and returns the preprocessed feature matrix and label vector. **Function Signature:** ```python def preprocess_data(file_path: str): Preprocess the data for machine learning model training. Parameters: file_path (str): Path to the CSV file containing the dataset. Returns: X (numpy.ndarray): The preprocessed feature matrix. y (numpy.ndarray): The preprocessed label vector. pass ``` **Important:** You must use scikit-learn\'s utility functions for validation and preprocessing tasks where applicable. **Input:** - A string `file_path` that specifies the path to the input CSV file. **Output:** - A tuple containing: - A numpy ndarray `X`: The preprocessed feature matrix. - A numpy ndarray `y`: The preprocessed label vector. **Constraints:** - The dataset is guaranteed to be well-formed with consistent column names as described above. - Assume the CSV file is not empty. **Example:** If the CSV file (`data.csv`) has the following content: ``` feature_1,feature_2,label 1.0, 2.0, 0 3.0, 4.0, 1 5.0, 6.0, 0 ``` Then, the expected output would be: ```python X, y = preprocess_data(\'data.csv\') print(X) # Output: array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) print(y) # Output: array([0, 1, 0]) ``` **Note:** Make sure your implementation is efficient and makes use of the provided utilities effectively.","solution":"import pandas as pd import numpy as np from sklearn.utils.validation import check_array from sklearn.preprocessing import StandardScaler def preprocess_data(file_path: str): # Load the dataset from the CSV file data = pd.read_csv(file_path) # Separate features and label X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # Validate the feature matrix and label vector X = check_array(X, ensure_2d=True, dtype=np.float64) y = check_array(y, ensure_2d=False, dtype=None) # Do not convert y to any particular type # Standardize the features to have zero mean and unit variance scaler = StandardScaler() X = scaler.fit_transform(X) return X, y"},{"question":"# Character Set Handling for Emails We need to design a custom character set handling system analogous to the one described in the `email.charset` module but without relying on the existing `email.charset` functionalities. The goal is to demonstrate proficiency in Python\'s class design and encoding concepts. Problem Statement: 1. **Create a Class `CustomCharset`**: - **Attributes**: - `input_charset` (string): The initial character set specified. - `header_encoding` (string): The encoding for email headers (options: \'QP\', \'BASE64\', \'SHORTEST\', or None). - `body_encoding` (string): The encoding for the email body (options: \'QP\', \'BASE64\', or None). - `output_charset` (string): The charset for output conversion (None if not needed). - `input_codec` (string): Name of the codec used to convert the input charset to Unicode. - `output_codec` (string): Name of the codec used to convert Unicode to the output charset. - **Methods**: - `get_body_encoding()`: Returns the encoding used for body based on `body_encoding`. - `get_output_charset()`: Returns `output_charset` if not None, otherwise returns `input_charset`. - `header_encode(string)`: Encodes the given `string` for header use based on `header_encoding`. - `body_encode(string)`: Encodes the given `string` for body use based on `body_encoding`. 2. **Global Registry Functions**: Implement the following global functions to manage character set properties: - `add_charset(charset, header_enc=None, body_enc=None, output_charset=None)`: Registers new character set properties. - `add_alias(alias, canonical)`: Adds an alias for a character set. - `add_codec(charset, codecname)`: Adds a codec for the given character set. Requirements and Constraints: - The `CustomCharset` class should mimic the behavior of `email.charset.Charset` but without using any part of the `email` module. - Handle common aliases for character sets, e.g., converting \'latin-1\' to \'iso-8859-1\'. - Ensure proper error handling for unsupported character sets or encodings. - Use appropriate Python standard library modules for encoding tasks. Example: ```python # Example usage charset = CustomCharset(\'iso-8859-1\') print(charset.get_body_encoding()) # Expected: \'7bit\' or appropriate encoding for the charset # Adding new charset and alias add_charset(\'utf-8\', header_enc=\'BASE64\', body_enc=\'BASE64\', output_charset=None) add_alias(\'latin_1\', \'iso-8859-1\') add_codec(\'utf-8\', \'utf-8\') # Encoding examples encoded_header = charset.header_encode(\'Sample Header\') encoded_body = charset.body_encode(\'Sample Body\') print(encoded_header) print(encoded_body) ``` Note: Make sure to follow PEP 8 guidelines, write clear docstrings for your class and methods, and include unittests demonstrating the functionality of your implementation.","solution":"import codecs import base64 import quopri CHARSET_REGISTRY = {} ALIAS_REGISTRY = {} CODEC_REGISTRY = {} class CustomCharset: def __init__(self, input_charset): self.input_charset = input_charset properties = CHARSET_REGISTRY.get(input_charset, {}) self.header_encoding = properties.get(\'header_encoding\') self.body_encoding = properties.get(\'body_encoding\') self.output_charset = properties.get(\'output_charset\') or input_charset self.input_codec = CODEC_REGISTRY.get(input_charset, input_charset) self.output_codec = CODEC_REGISTRY.get(self.output_charset, self.output_charset) def get_body_encoding(self): return self.body_encoding def get_output_charset(self): return self.output_charset def header_encode(self, string): if self.header_encoding == \'BASE64\': return base64.b64encode(string.encode(self.input_codec)).decode() elif self.header_encoding == \'QP\': return quopri.encodestring(string.encode(self.input_codec)).decode() elif self.header_encoding == \'SHORTEST\': b64_encoded = base64.b64encode(string.encode(self.input_codec)).decode() qp_encoded = quopri.encodestring(string.encode(self.input_codec)).decode() return b64_encoded if len(b64_encoded) < len(qp_encoded) else qp_encoded else: return string def body_encode(self, string): if self.body_encoding == \'BASE64\': return base64.b64encode(string.encode(self.input_codec)).decode() elif self.body_encoding == \'QP\': return quopri.encodestring(string.encode(self.input_codec)).decode() else: return string def add_charset(charset, header_enc=None, body_enc=None, output_charset=None): CHARSET_REGISTRY[charset] = { \'header_encoding\': header_enc, \'body_encoding\': body_enc, \'output_charset\': output_charset } def add_alias(alias, canonical): ALIAS_REGISTRY[alias] = canonical def add_codec(charset, codecname): CODEC_REGISTRY[charset] = codecname"},{"question":"**Title: Analyzing and Transforming Data Structures** **Objective:** This question is designed to assess your ability to effectively handle and transform various Python data structures, including lists, dictionaries, and sets, using advanced list methods, comprehensions, and set operations. **Problem Statement:** You are given a list of dictionaries where each dictionary represents a person\'s data with their name and a list of items they have. Your task is to write a function that returns a transformed summary of this data based on the following steps: 1. **Filter and Flatten Items:** Start by filtering each person\'s items to remove any items containing the letter \'a\'. Then, combine all such filtered lists into a single flat list containing items from every person. 2. **Count Item Occurrences:** Create a dictionary where the keys are the unique items from the flat list and the values are the counts of how many times each item appears. 3. **Sort by Occurrences:** Convert this dictionary into a list of tuples, where each tuple is `(item, count)`, and sort this list in descending order based on the count of items. If two items have the same count, they should be sorted alphabetically. 4. **Extract Top Items:** Finally, extract the top `n` items based on their counts. The value of `n` should be provided as an argument to the function. **Function Signature:** ```python def summarize_items(data: list[dict], n: int) -> list[tuple]: pass ``` **Input:** - `data` (list): A list of dictionaries, where each dictionary has the form: ```python { \\"name\\": str, \\"items\\": list of str } ``` Example: ```python [ {\\"name\\": \\"Alice\\", \\"items\\": [\\"apple\\", \\"banana\\", \\"carrot\\", \\"date\\"]}, {\\"name\\": \\"Bob\\", \\"items\\": [\\"apple\\", \\"blackberry\\", \\"cherry\\", \\"date\\"]}, {\\"name\\": \\"Charlie\\", \\"items\\": [\\"mango\\", \\"date\\", \\"elderberry\\", \\"fig\\"]} ] ``` - `n` (int): The number of top items to extract based on their count. **Output:** - Returns a list of tuples, where each tuple contains an item and its count, sorted in descending order based on the count. **Constraints:** - `n` will be a non-negative integer less than or equal to the number of unique items. - Items are case-sensitive (i.e., \'apple\' and \'Apple\' are considered different items). **Example Usage:** ```python data = [ {\\"name\\": \\"Alice\\", \\"items\\": [\\"apple\\", \\"banana\\", \\"carrot\\", \\"date\\"]}, {\\"name\\": \\"Bob\\", \\"items\\": [\\"apple\\", \\"blackberry\\", \\"cherry\\", \\"date\\"]}, {\\"name\\": \\"Charlie\\", \\"items\\": [\\"mango\\", \\"date\\", \\"elderberry\\", \\"fig\\"]} ] n = 3 print(summarize_items(data, n)) # Output: [(\'date\', 3), (\'cherry\', 1), (\'elderberry\', 1)] ``` **Explanation:** After filtering out items containing the letter \'a\': - Alice\'s items -> `[\'carrot\']` - Bob\'s items -> `[\'cherry\']` - Charlie\'s items -> `[\'date\', \'elderberry\', \'fig\']` The combined flat list of items becomes `[\'carrot\', \'cherry\', \'date\', \'elderberry\', \'fig\']`. The counts of items are: - `\'carrot\'`: 1 - `\'cherry\'`: 1 - `\'date\'`: 1 - `\'elderberry\'`: 1 - `\'fig\'`: 1 After sorting by count and then by name, the top 3 items are `[(\'date\', 1), (\'cherry\', 1), (\'elderberry\', 1)]`. **Submissions:** You are required to submit the complete function implementation. Remember to handle edge cases and ensure the function performs efficiently with the provided constraints.","solution":"def summarize_items(data, n): Returns a summary of the top n items based on their occurrences, after filtering out items containing the letter \'a\'. # Filter items and flatten the list flattened_items = [ item for person in data for item in person[\'items\'] if \'a\' not in item ] # Count occurrences of each item item_counts = {} for item in flattened_items: if item in item_counts: item_counts[item] += 1 else: item_counts[item] = 1 # Convert to list of tuples and sort by count (descending) and then by item (alphabetical) sorted_items = sorted(item_counts.items(), key=lambda x: (-x[1], x[0])) # Extract top n items return sorted_items[:n]"},{"question":"**Problem Statement:** You are tasked with implementing a custom metric handler for distributed training using PyTorch. Your handler should record metrics to a file for later analysis. Requirements 1. **Class Implementation**: - Implement a class `FileMetricHandler` that extends `MetricHandler`. - This class should record metrics to a specified file. 2. **Constructor**: - The constructor should require a file path where the metrics will be recorded. 3. **Recording Metrics**: - The class should implement methods to handle metrics (e.g., for training loss, accuracy). - Metrics should be stored in a plain text file in the format: `metric_name: metric_value`. Expected Function Signature ```python class FileMetricHandler(MetricHandler): def __init__(self, file_path: str): Initializes the FileMetricHandler with the given file path. Args: - file_path (str): The path to the file where metrics will be recorded. pass def record_metric(self, metric_name: str, metric_value: float): Records a metric to the file. Args: - metric_name (str): The name of the metric. - metric_value (float): The value of the metric. pass ``` Constraints - You should handle potential file I/O errors gracefully. - The metric values should be appended to the file, not overwritten. Example Usage ```python # Initialize the handler with a file path handler = FileMetricHandler(\\"metrics.txt\\") # Record some metrics handler.record_metric(\\"training_loss\\", 0.25) handler.record_metric(\\"accuracy\\", 0.95) ``` The above example would result in a file `metrics.txt` containing: ``` training_loss: 0.25 accuracy: 0.95 ``` Performance Requirements - The `record_metric` method should be efficient and not cause significant overhead during training.","solution":"import os class MetricHandler: A base class for custom metric handlers. def record_metric(self, metric_name: str, metric_value: float): raise NotImplementedError(\\"Must override record_metric\\") class FileMetricHandler(MetricHandler): def __init__(self, file_path: str): Initializes the FileMetricHandler with the given file path. Args: - file_path (str): The path to the file where metrics will be recorded. self.file_path = file_path def record_metric(self, metric_name: str, metric_value: float): Records a metric to the file. Args: - metric_name (str): The name of the metric. - metric_value (float): The value of the metric. try: with open(self.file_path, \'a\') as f: f.write(f\\"{metric_name}: {metric_value}n\\") except IOError as e: print(f\\"Error recording metric: {e}\\")"},{"question":"**Coding Assessment Question: Persistent User Profiles with Shelve** # Objective Write a Python program to manage persistent user profiles using the `shelve` module. Each profile should contain a username, email, and a list of interests. The program should support adding new profiles, updating interests, retrieving profiles, and deleting profiles. # Function Signature ```python def manage_user_profiles(filename: str) -> None: ``` # Input and Output - **Input**: The function takes a single argument `filename`, which is the name of the shelve database file where user profiles are stored. - **Output**: The function does not return any value but instead prints the result of each operation. # Functional Requirements 1. **Add New Profile**: - Prompt the user for `username`, `email`, and an initial list of `interests` (entered as comma-separated values). - Store the new profile in the shelf, using the `username` as the key. 2. **Update Interests**: - Prompt the user for a `username`. - If the username exists, prompt the user for additional interests, and add them to the user’s existing list of interests. - If the username does not exist, print an appropriate message. 3. **Retrieve Profile**: - Prompt the user for a `username`. - If the user exists, print the user\'s profile. - If the username does not exist, print an appropriate message. 4. **Delete Profile**: - Prompt the user for a `username`. - If the user exists, delete the user’s profile. - If the username does not exist, print an appropriate message. 5. **Exit**: - Provide an option to exit the program, ensuring that the shelf is properly closed. # Constraints - Usernames should be unique and consist of alphanumeric characters only. - Ensure proper handling of mutable objects to avoid issues with the `writeback` parameter. # Example Interaction ``` Welcome to the User Profile Manager. 1. Add New Profile 2. Update Interests 3. Retrieve Profile 4. Delete Profile 5. Exit Enter your choice: 1 Enter username: john_doe Enter email: john.doe@example.com Enter interests (comma-separated): reading, music Enter your choice: 3 Enter username: john_doe Profile for john_doe: Email: john.doe@example.com Interests: [\'reading\', \'music\'] Enter your choice: 2 Enter username: john_doe Enter additional interests (comma-separated): traveling, hiking Enter your choice: 3 Enter username: john_doe Profile for john_doe: Email: john.doe@example.com Interests: [\'reading\', \'music\', \'traveling\', \'hiking\'] Enter your choice: 4 Enter username: john_doe Profile for john_doe deleted. Enter your choice: 5 Exiting the program. Goodbye! ``` # Notes - Make sure to handle edge cases such as the user entering an invalid option, empty input, or attempting to update/retrieve/delete a non-existent user. - Use appropriate exception handling for file operations. # Implementation Hints - Use `shelve.open()` with the correct flag and remember to close the shelf properly. - Use a loop to display the menu and process user inputs. - Ensure the program is efficient and handles all specified cases gracefully.","solution":"import shelve def add_profile(shelf): username = input(\\"Enter username: \\").strip() if not username.isalnum(): print(\\"Username must be alphanumeric.\\") return if username in shelf: print(\\"Username already exists.\\") return email = input(\\"Enter email: \\").strip() interests = input(\\"Enter interests (comma-separated): \\").strip().split(\',\') interests = [interest.strip() for interest in interests if interest.strip()] shelf[username] = {\'email\': email, \'interests\': interests} print(f\\"Profile for {username} added.\\") def update_interests(shelf): username = input(\\"Enter username: \\").strip() if username not in shelf: print(\\"Username not found.\\") return user_profile = shelf[username] new_interests = input(\\"Enter additional interests (comma-separated): \\").strip().split(\',\') new_interests = [interest.strip() for interest in new_interests if interest.strip()] user_profile[\'interests\'].extend(new_interests) shelf[username] = user_profile print(f\\"Interests updated for {username}.\\") def retrieve_profile(shelf): username = input(\\"Enter username: \\").strip() if username not in shelf: print(\\"Username not found.\\") return user_profile = shelf[username] print(f\\"Profile for {username}:\\") print(f\\"Email: {user_profile[\'email\']}\\") print(f\\"Interests: {user_profile[\'interests\']}\\") def delete_profile(shelf): username = input(\\"Enter username: \\").strip() if username not in shelf: print(\\"Username not found.\\") return del shelf[username] print(f\\"Profile for {username} deleted.\\") def manage_user_profiles(filename: str) -> None: with shelve.open(filename, writeback=True) as shelf: while True: print(\\"nWelcome to the User Profile Manager.\\") print(\\"1. Add New Profile\\") print(\\"2. Update Interests\\") print(\\"3. Retrieve Profile\\") print(\\"4. Delete Profile\\") print(\\"5. Exit\\") choice = input(\\"Enter your choice: \\").strip() if choice == \'1\': add_profile(shelf) elif choice == \'2\': update_interests(shelf) elif choice == \'3\': retrieve_profile(shelf) elif choice == \'4\': delete_profile(shelf) elif choice == \'5\': print(\\"Exiting the program. Goodbye!\\") break else: print(\\"Invalid choice. Please try again.\\")"},{"question":"Objective This question tests your understanding of seaborn\'s `plotting_context` function and your ability to apply it in different scenarios to control the styling of seaborn plots. Question You are given the task to visualize a dataset using seaborn and modify the plotting context to demonstrate the differences in plot appearance for a report. Write a function `plot_with_context(data, style)` that takes the following inputs: - `data`: A DataFrame containing two columns `x` and `y`, which are the data points to be plotted. - `style`: A string indicating the seaborn predefined plotting context style (e.g., \\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"). Your function should: 1. Plot the data using seaborn\'s `lineplot` function. 2. Apply the provided `style` using the `plotting_context` function as a context manager to temporarily change the parameter values. 3. Return the figure object representing the produced plot. The function signature should be: ```python def plot_with_context(data: pd.DataFrame, style: str): pass ``` Example ```python import seaborn as sns import pandas as pd # Example data data = pd.DataFrame({ \'x\': [\\"A\\", \\"B\\", \\"C\\", \\"D\\"], \'y\': [1, 3, 2, 4] }) # Example usage fig = plot_with_context(data, \\"talk\\") # Ensure fig is a valid matplotlib figure object corresponding to the plot created print(isinstance(fig, sns.utils.matplotlib.figure.Figure)) ``` Constraints - Assume data is a non-empty DataFrame with valid numeric data. - Assume `style` is one of the predefined seaborn contexts: \\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\". Additional Note - Ensure your solution imports all necessary libraries and handles any required imports within the function.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def plot_with_context(data: pd.DataFrame, style: str): Plots the given data using seaborn\'s lineplot function and applies the provided seaborn plotting context style. Parameters: data (pd.DataFrame): DataFrame containing the data points to be plotted. style (str): Seaborn plotting context style (e.g., \\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"). Returns: plt.Figure: Matplotlib Figure object representing the produced plot. with sns.plotting_context(style): fig, ax = plt.subplots() sns.lineplot(data=data, x=\'x\', y=\'y\', ax=ax) return fig"},{"question":"# Question: Utilizing MPS Backend in PyTorch for GPU Acceleration on MacOS You are tasked with implementing a function that demonstrates the usage of the PyTorch MPS backend to perform tensor operations and model training on a MacOS device with an MPS-enabled GPU. Function Signature ```python def mps_backend_demo(): pass ``` Requirements 1. **Tensor Operations**: - Create a tensor of shape `(3, 3)` filled with random values on the `mps` device. - Perform element-wise multiplication of this tensor by 3. - Return the resulting tensor moved back to the CPU. 2. **Model Operations**: - Create a simple neural network model with one linear layer. - Move the model to the `mps` device. - Create a random tensor of shape `(1, 3)` on the `mps` device to act as input to the model. - Perform a forward pass using this tensor. - Return the output of the forward pass moved back to the CPU. 3. **Edge Case Handling**: - If the `mps` device is not available, print an appropriate message and return `None`. Example Usage ```python result_tensor, model_output = mps_backend_demo() if result_tensor is not None and model_output is not None: print(\\"Tensor after multiplication:n\\", result_tensor) print(\\"Model output:n\\", model_output) ``` Constraints - Ensure that the tensor and model operations both occur on the `mps` device. - Properly handle cases where the MPS backend is not available. Notes - You may assume the necessary imports (e.g., `torch`, `torch.nn`, etc.) are already in place. - The `torch.backends.mps.is_available()` and `torch.backends.mps.is_built()` functions can be used to check the availability and build status of the MPS backend. Write the function `mps_backend_demo` to fulfill the above requirements.","solution":"import torch import torch.nn as nn def mps_backend_demo(): # Check if MPS is available if not torch.backends.mps.is_available() or not torch.backends.mps.is_built(): print(\\"MPS device is not available.\\") return None, None # Tensor operations device = torch.device(\\"mps\\") tensor = torch.rand((3, 3), device=device) result_tensor = tensor * 3 result_tensor_cpu = result_tensor.to(\\"cpu\\") # Model operations class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(3, 1) def forward(self, x): return self.linear(x) model = SimpleModel().to(device) input_tensor = torch.rand((1, 3), device=device) model_output = model(input_tensor) model_output_cpu = model_output.to(\\"cpu\\") return result_tensor_cpu, model_output_cpu"},{"question":"In Python, modern code should adhere to the new buffer protocol. Your task is to implement a utility class `BufferUtils` that provides similar functionality using the new buffer protocol. Specifically, you need to implement the following methods: 1. `get_char_buffer(obj)`: Takes an object `obj` and returns a tuple `(buffer, buffer_len)` where `buffer` is a read-only memory view of the object\'s characters and `buffer_len` is the length of this buffer. Raise a `TypeError` if the object doesn\'t support the buffer interface. 2. `get_read_buffer(obj)`: Takes an object `obj` and returns a tuple `(buffer, buffer_len)` where `buffer` is a read-only memory view of the object\'s data and `buffer_len` is the length of this buffer. Raise a `TypeError` if the object doesn\'t support the buffer interface. 3. `get_write_buffer(obj)`: Takes an object `obj` and returns a tuple `(buffer, buffer_len)` where `buffer` is a writable memory view of the object\'s data and `buffer_len` is the length of this buffer. Raise a `TypeError` if the object doesn\'t support the buffer interface. 4. `check_read_buffer(obj)`: Takes an object `obj` and returns `True` if the object supports the readable buffer interface, else returns `False`. # Example Usage ```python class BufferUtils: @staticmethod def get_char_buffer(obj): # Your implementation here pass @staticmethod def get_read_buffer(obj): # Your implementation here pass @staticmethod def get_write_buffer(obj): # Your implementation here pass @staticmethod def check_read_buffer(obj): # Your implementation here pass # Example objects char_obj = b\\"hello\\" data_obj = memoryview(bytearray(b\\"world\\")) # Test cases BufferUtils.get_char_buffer(char_obj) # (Expected output: (memoryview(b\'hello\'), 5)) BufferUtils.get_read_buffer(data_obj) # (Expected output: (memoryview(b\'world\'), 5)) BufferUtils.get_write_buffer(data_obj) # (Expected output: (memoryview(b\'world\'), 5)) BufferUtils.check_read_buffer(char_obj) # (Expected output: True) BufferUtils.check_read_buffer([]) # (Expected output: False) ``` # Constraints - Ensure that the `buffer` returned in the function is readable and/or writable as required. - Handle errors gracefully by raising appropriate exceptions. - The implementation should work efficiently even with large buffers. # Note This question assesses your understanding of: - Memory views in Python. - Handling buffer protocols. - Implementing static methods in a utility class. - Error handling and type checking.","solution":"class BufferUtils: @staticmethod def get_char_buffer(obj): try: buffer = memoryview(obj) except TypeError: raise TypeError(\\"Object does not support the buffer interface\\") if buffer.format not in (\'b\', \'B\'): buffer.release() raise TypeError(\\"Object is not a bytes-like object with \'char\' data format\\") return (buffer, buffer.nbytes) @staticmethod def get_read_buffer(obj): try: buffer = memoryview(obj) except TypeError: raise TypeError(\\"Object does not support the buffer interface\\") return (buffer, buffer.nbytes) @staticmethod def get_write_buffer(obj): try: buffer = memoryview(obj) except TypeError: raise TypeError(\\"Object does not support the buffer interface\\") if not buffer.readonly: return (buffer, buffer.nbytes) else: buffer.release() raise TypeError(\\"Object does not support writable buffer\\") @staticmethod def check_read_buffer(obj): try: memoryview(obj).release() return True except TypeError: return False"},{"question":"**Objective**: Implement a simple HTTP server that handles both static file requests and a specific dynamic content request. # Description You are required to implement a custom HTTP server using Python\'s `http.server` module. Your server should serve static files from a specified directory and handle a dynamic request as defined below. # Requirements 1. **Static File Handling**: - Serve static files (e.g., HTML, CSS, JS, images) from a directory called `static`. - If a file is not found, return a `404 Not Found` error. 2. **Dynamic Content Handling**: - Handle a dynamic GET request at the endpoint `/hello`. - Respond to this request with a JSON object containing a message: `{\\"message\\": \\"Hello, world!\\"}`. # Input and Output - **Input**: - N/A (handled by the server based on HTTP requests). - **Output**: - Properly formatted HTTP responses including headers and content. # Constraints - Use the `http.server` module and relevant classes (`HTTPServer`, `BaseHTTPRequestHandler`). - The server should run on port 8000. - Your implementation should be efficient and handle potential errors gracefully. # Example The following example shows what your server should handle: 1. **Static File Request**: - Request: `GET /static/index.html` - Response: Contents of `static/index.html` if it exists; otherwise, a `404 Not Found` error. 2. **Dynamic Content Request**: - Request: `GET /hello` - Response: HTTP 200 OK with content `{\\"message\\": \\"Hello, world!\\"}` # Implementation Implement your server in the space below: ```python import http.server import socketserver import json import os class MyRequestHandler(http.server.SimpleHTTPRequestHandler): def do_GET(self): if self.path == \\"/hello\\": self.send_response(200) self.send_header(\\"Content-Type\\", \\"application/json\\") self.end_headers() self.wfile.write(json.dumps({\\"message\\": \\"Hello, world!\\"}).encode(\'utf-8\')) else: # Serve static files if self.path.startswith(\\"/static\\"): # Remove the leading \'/static\' to get the local file path self.path = self.path.lstrip(\\"/static\\") return http.server.SimpleHTTPRequestHandler.do_GET(self) else: self.send_error(404, \\"File Not Found\\") PORT = 8000 Handler = MyRequestHandler with socketserver.TCPServer((\\"\\", PORT), Handler) as httpd: print(\\"serving at port\\", PORT) httpd.serve_forever() ``` # Instructions - Implement the `MyRequestHandler` class by inheriting from `http.server.SimpleHTTPRequestHandler`. - Ensure the server can handle both static file requests from the `static` directory and the dynamic `/hello` endpoint. Once you have completed the implementation, test your server by making requests to both static files and the dynamic endpoint to ensure it behaves as expected.","solution":"import http.server import socketserver import json import os class MyRequestHandler(http.server.SimpleHTTPRequestHandler): def do_GET(self): if self.path == \\"/hello\\": self.send_response(200) self.send_header(\\"Content-Type\\", \\"application/json\\") self.end_headers() self.wfile.write(json.dumps({\\"message\\": \\"Hello, world!\\"}).encode(\'utf-8\')) else: # Serve static files if self.path.startswith(\\"/static\\"): self.path = self.path[len(\\"/static\\"):] # Remove the leading \'/static\' return super().do_GET() else: self.send_error(404, \\"File not found\\") def run(server_class=http.server.HTTPServer, handler_class=MyRequestHandler, port=8000): server_address = (\'\', port) httpd = server_class(server_address, handler_class) print(f\'Serving on port {port}\') httpd.serve_forever() if __name__ == \'__main__\': run()"},{"question":"Seaborn Coding Assessment You are given a dataset on flower species characteristics named `iris` which contains data on the species `setosa`, `versicolor`, and `virginica`. The dataset includes the following columns: - `sepal_length` - `sepal_width` - `petal_length` - `petal_width` - `species` **Objective:** Using seaborn, create a multi-faceted scatter plot to visualize the relation between `sepal_length` and `sepal_width` for each species. **Requirements:** 1. Load the `iris` dataset using seaborn. 2. Create a `seaborn.objects.Plot` object using `iris` dataset. 3. Add a scatter plot visual to this plot that shows `sepal_length` on the x-axis and `sepal_width` on the y-axis. 4. Facet the plot by `species` to create individual subplots for each species. 5. Customize the scatter plot points to have different colors based on the `species`. **Input:** - The dataset should be loaded internally using `seaborn.load_dataset(\\"iris\\")`. **Output:** - Display the multi-faceted scatter plot where each subplot corresponds to a species. **Constraints:** - You must use seaborn objects and functions as demonstrated in the provided snippets. - Ensure to handle the dataset efficiently and create a clear, visually distinguishable plot. **Example:** ```python import seaborn.objects as so from seaborn import load_dataset # Load the iris dataset iris = load_dataset(\\"iris\\") # Create the Plot object for the dataset plot = so.Plot(iris, x=\\"sepal_length\\", y=\\"sepal_width\\", color=\\"species\\") # Add scatter plot and facet by species final_plot = plot.add(so.Dot()).facet(\\"species\\") # Display the plot final_plot.show() ``` **Explanation:** In this example, `so.Plot` initializes the plot with the `iris` dataset. `add(so.Dot())` adds scatter points to the plot, while `.facet(\\"species\\")` creates subplots for each species. The points are colored based on their species for better distinction between different species’ data points. Use the above structure to create your solution and ensure the final plot clearly visualizes the relationships within the dataset.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_iris_scatter_plot(): # Load the iris dataset iris = load_dataset(\\"iris\\") # Create the Plot object for the dataset plot = so.Plot(iris, x=\\"sepal_length\\", y=\\"sepal_width\\", color=\\"species\\") # Add scatter plot and facet by species final_plot = plot.add(so.Dot()).facet(\\"species\\") # Display the plot final_plot.show()"},{"question":"# Kernel Approximation using Nystroem Method and RBFSampler **Background:** Kernel methods, like those used in support vector machines, provide powerful tools for classification by implicitly mapping data to higher-dimensional spaces. However, these methods can be computationally expensive. The scikit-learn library includes techniques to approximate these kernel mappings, making them more feasible for large datasets. Two such techniques are the Nystroem method and the Random Fourier Features (RBFSampler). - **Nystroem Method**: Approximates the kernel matrix using a subset of the training data, which significantly reduces the computational complexity. - **RBFSampler**: Provides an approximation for the RBF (Radial Basis Function) kernel using Monte Carlo sampling. **Objective:** You are required to implement a function that uses both the Nystroem method and RBFSampler to approximate a radial basis function kernel with a given dataset. Your function should then apply these approximations to fit a linear classifier and evaluate its performance. **Function Signature:** ```python def kernel_approximation(X_train, y_train, X_test, y_test, n_components): Args: X_train (list of list of float): The training data features. y_train (list of int): The target labels for the training data. X_test (list of list of float): The testing data features. y_test (list of int): The target labels for the testing data. n_components (int): Number of components for the kernel approximation. Returns: tuple: A tuple containing: - float: The accuracy score of the classifier using Nystroem method. - float: The accuracy score of the classifier using RBFSampler. ``` **Requirements:** 1. **Input Format:** - `X_train`: A matrix (list of lists) of feature values for the training dataset. - `y_train`: A list of integer labels corresponding to the training data. - `X_test`: A matrix of feature values for the testing dataset. - `y_test`: A list of integer labels corresponding to the testing data. - `n_components`: An integer specifying the number of components to use for the kernel approximation. 2. **Output Format:** - A tuple containing the accuracy scores of the classifiers using Nystroem and RBFSampler respectively. 3. **Performance Requirements:** - The function should efficiently handle the kernel approximation and provide results within a reasonable time for the given data. 4. **Libraries Allowed:** - Scikit-learn for kernel approximation and linear classification. **Constraints:** - Assume that the input data matrices `X_train` and `X_test` have the same number of features. **Example:** ```python # Example inputs X_train = [[0.1, 0.3], [0.4, 0.2], [0.9, 0.7], [0.6, 0.5]] y_train = [0, 0, 1, 1] X_test = [[0.1, 0.4], [0.8, 0.6]] y_test = [0, 1] n_components = 2 # Sample function call nystroem_score, rbf_sampler_score = kernel_approximation(X_train, y_train, X_test, y_test, n_components) # Output print(nystroem_score) # e.g., 0.5 print(rbf_sampler_score) # e.g., 1.0 ``` **Instructions:** 1. Implement the `kernel_approximation` function using `Nystroem` and `RBFSampler`. 2. Use `SGDClassifier` from `sklearn.linear_model` for fitting the linear classifier. 3. Calculate the accuracy scores for the classifier on the test data for both methods and return them as a tuple. **Hints:** - The `Nystroem` and `RBFSampler` classes are available in `sklearn.kernel_approximation`. - The `SGDClassifier` can be used with the `fit` and `score` methods to train and evaluate the model.","solution":"from sklearn.kernel_approximation import Nystroem, RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def kernel_approximation(X_train, y_train, X_test, y_test, n_components): Applies Nystroem and RBFSampler kernel approximations to fit a linear classifier. Args: X_train (list of list of float): The training data features. y_train (list of int): The target labels for the training data. X_test (list of list of float): The testing data features. y_test (list of int): The target labels for the testing data. n_components (int): Number of components for the kernel approximation. Returns: tuple: A tuple containing: - float: The accuracy score of the classifier using Nystroem method. - float: The accuracy score of the classifier using RBFSampler. # Nystroem method nystroem_approx = Nystroem(n_components=n_components, random_state=1) X_train_nystroem = nystroem_approx.fit_transform(X_train) X_test_nystroem = nystroem_approx.transform(X_test) nystroem_clf = SGDClassifier(random_state=1) nystroem_clf.fit(X_train_nystroem, y_train) y_pred_nystroem = nystroem_clf.predict(X_test_nystroem) nystroem_score = accuracy_score(y_test, y_pred_nystroem) # RBFSampler method rbf_sampler = RBFSampler(n_components=n_components, random_state=1) X_train_rbf = rbf_sampler.fit_transform(X_train) X_test_rbf = rbf_sampler.transform(X_test) rbf_clf = SGDClassifier(random_state=1) rbf_clf.fit(X_train_rbf, y_train) y_pred_rbf = rbf_clf.predict(X_test_rbf) rbf_sampler_score = accuracy_score(y_test, y_pred_rbf) return nystroem_score, rbf_sampler_score"},{"question":"Objective: Implement and compare different multiclass classification strategies using scikit-learn on the Iris dataset. Problem Statement: Using the Iris dataset provided by `sklearn.datasets`, implement the following multiclass classification strategies: 1. One-vs-Rest (OvR) using `OneVsRestClassifier`. 2. One-vs-One (OvO) using `OneVsOneClassifier`. Train both classifiers on the dataset and evaluate their performance using accuracy as the metric. Additionally, visualize the decision boundaries for each classifier. Requirements: 1. Load the Iris dataset using `datasets.load_iris()`. 2. Implement the `OneVsRestClassifier` with `LinearSVC` as the base classifier. Train it on the dataset and predict the labels on the training data. 3. Implement the `OneVsOneClassifier` with `LinearSVC` as the base classifier. Train it on the dataset and predict the labels on the training data. 4. Calculate and print the accuracy for both classifiers. 5. Plot the decision boundaries for each classifier using a 2D feature space (use the first two features of the dataset for visualization). Constraints: - Use `random_state=0` wherever necessary for reproducibility. Input Format: - None. Use the Iris dataset that is directly available through `sklearn.datasets`. Output Format: - Print the accuracy of the One-vs-Rest classifier. - Print the accuracy of the One-vs-One classifier. - Display the decision boundary plots for both classifiers. Example Solution: ```python import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier from sklearn.svm import LinearSVC from sklearn.metrics import accuracy_score from matplotlib.colors import ListedColormap # Load the Iris dataset X, y = datasets.load_iris(return_X_y=True) # Implement One-vs-Rest Classifier ovr_clf = OneVsRestClassifier(LinearSVC(random_state=0)) ovr_clf.fit(X, y) ovr_y_pred = ovr_clf.predict(X) ovr_accuracy = accuracy_score(y, ovr_y_pred) print(f\\"OvR Classifier Accuracy: {ovr_accuracy}\\") # Implement One-vs-One Classifier ovo_clf = OneVsOneClassifier(LinearSVC(random_state=0)) ovo_clf.fit(X, y) ovo_y_pred = ovo_clf.predict(X) ovo_accuracy = accuracy_score(y, ovo_y_pred) print(f\\"OvO Classifier Accuracy: {ovo_accuracy}\\") # Helper function to plot decision boundaries def plot_decision_boundary(clf, X, y, title): cmap_light = ListedColormap([\'#FFAAAA\', \'#AAFFAA\', \'#AAAAFF\']) cmap_bold = ListedColormap([\'#FF0000\', \'#00FF00\', \'#0000FF\']) x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.figure() plt.pcolormesh(xx, yy, Z, cmap=cmap_light) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor=\'k\', s=20) plt.title(title) plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.show() # Plot decision boundaries for OvR Classifier plot_decision_boundary(ovr_clf, X[:, :2], y, \\"One-vs-Rest Classifier Decision Boundary\\") # Plot decision boundaries for OvO Classifier plot_decision_boundary(ovo_clf, X[:, :2], y, \\"One-vs-One Classifier Decision Boundary\\") ``` Additional Notes: - Ensure that all required libraries are imported and used correctly. - The accuracy results and decision boundary plots should be clearly displayed.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier from sklearn.svm import LinearSVC from sklearn.metrics import accuracy_score from matplotlib.colors import ListedColormap # Load the Iris dataset def load_iris_data(): X, y = datasets.load_iris(return_X_y=True) return X, y # Implement One-vs-Rest Classifier def train_ovr_classifier(X, y): ovr_clf = OneVsRestClassifier(LinearSVC(random_state=0)) ovr_clf.fit(X, y) ovr_y_pred = ovr_clf.predict(X) ovr_accuracy = accuracy_score(y, ovr_y_pred) return ovr_clf, ovr_accuracy # Implement One-vs-One Classifier def train_ovo_classifier(X, y): ovo_clf = OneVsOneClassifier(LinearSVC(random_state=0)) ovo_clf.fit(X, y) ovo_y_pred = ovo_clf.predict(X) ovo_accuracy = accuracy_score(y, ovo_y_pred) return ovo_clf, ovo_accuracy # Helper function to plot decision boundaries def plot_decision_boundary(clf, X, y, title): cmap_light = ListedColormap([\'#FFAAAA\', \'#AAFFAA\', \'#AAAAFF\']) cmap_bold = ListedColormap([\'#FF0000\', \'#00FF00\', \'#0000FF\']) x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.figure(figsize=(10, 6)) plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading=\'auto\') plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor=\'k\', s=20) plt.title(title) plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.show() if __name__ == \\"__main__\\": # Load data X, y = load_iris_data() # One-vs-Rest Classifier ovr_clf, ovr_accuracy = train_ovr_classifier(X, y) print(f\\"OvR Classifier Accuracy: {ovr_accuracy}\\") # One-vs-One Classifier ovo_clf, ovo_accuracy = train_ovo_classifier(X, y) print(f\\"OvO Classifier Accuracy: {ovo_accuracy}\\") # Plot decision boundaries for OvR Classifier plot_decision_boundary(ovr_clf, X[:, :2], y, \\"One-vs-Rest Classifier Decision Boundary\\") # Plot decision boundaries for OvO Classifier plot_decision_boundary(ovo_clf, X[:, :2], y, \\"One-vs-One Classifier Decision Boundary\\")"},{"question":"**Objective:** Implement a custom container type in Python that supports cyclic garbage collection. **Description:** You are required to implement a custom container type in Python using Cython or C-extension that supports cyclic garbage collection. This type should be able to store references to other such containers and handle garbage collection properly. **Details:** 1. **Type Name:** `CustomContainer` 2. **Operations to Implement:** - Constructor: Initializes the container with an empty list of elements. - Method `append`: Adds elements to the container. - Method `remove`: Removes elements from the container. - Proper memory allocation/deallocation routines (`__init__`, `__del__`). - Implementation of garbage collection interface (`tp_traverse`, `tp_clear`). **Input:** - `CustomContainer` is a class with methods as described. **Output:** - Properly working container class with cyclic garbage collection support. **Constraints:** - Your implementation should correctly support cyclic garbage collection as described in the documentation. - You should provide `tp_traverse` and `tp_clear` implementations. - Container elements should be instances of `CustomContainer`. **Performance Requirements:** - The garbage collection interface must efficiently handle typical use cases with potentially large numbers of `CustomContainer` instances storing references to each other. Write your implementation in Cython or as a Python C-extension. Provide a sample usage of your type demonstrating the creation of cyclic references and their resolution through the garbage collector in Python. **Example usage:** ```python import gc # Create instances of CustomContainer c1 = CustomContainer() c2 = CustomContainer() # Create cyclic reference c1.append(c2) c2.append(c1) # Force garbage collection gc.collect() ``` **Submission:** Submit: - Your Cython or C-extension code file. - A Python script demonstrating the usage and garbage collection functionality.","solution":"import gc class CustomContainer: def __init__(self): Initialize the container with an empty list of elements. self.elements = [] def append(self, element): Adds elements to the container. :param element: Instance of CustomContainer to be added. if isinstance(element, CustomContainer): self.elements.append(element) else: raise TypeError(\\"Only instances of CustomContainer can be added.\\") def remove(self, element): Removes elements from the container. :param element: Instance of CustomContainer to be removed. if element in self.elements: self.elements.remove(element) else: raise ValueError(\\"Element not found in the container\\") def __del__(self): Ensures proper cleanup of elements by breaking cyclic references. self.elements.clear() def __repr__(self): return f\\"CustomContainer(id={id(self)}, elements={self.elements})\\" # Example usage: # import gc # c1 = CustomContainer() # c2 = CustomContainer() # c1.append(c2) # c2.append(c1) # print(c1, c2) # del c1 # del c2 # gc.collect()"},{"question":"Coding Assessment Question # Objective: You are given a dataset from an experiment where participants\' performance was recorded under different conditions. Your task is to clean and transform the data into appropriate formats, and then visualize it using seaborn. # Dataset: The dataset records three variables: participant identifier (`\\"participant_id\\"`), the condition under which they performed (`\\"condition\\"`), and their performance score (`\\"score\\"`). The data looks like this: | participant_id | condition1 | condition2 | condition3 | |----------------|------------|------------|------------| | 1 | 5 | 7 | 6 | | 2 | 6 | 8 | 7 | | 3 | 7 | 9 | 8 | # Tasks: 1. **Convert the dataset to long-form and wide-form:** - Load the dataset into a pandas DataFrame. - Convert it into long-form data, such that each row represents a single observation with columns for `participant_id`, `condition`, and `score`. - Convert it into wide-form data, such that each participant\'s scores under different conditions are represented as columns. 2. **Create visualizations:** - Using the long-form data, create a line plot using seaborn where x-axis is the `condition`, y-axis is the `score`, and lines are grouped by `participant_id`. - Using the wide-form data, create a box plot using seaborn to compare the distributions of scores across conditions. # Implementation Requirements: - You can use `pandas` for data manipulation and `seaborn` for visualization. - Provide the code to load the dataset, transform the formats, and create the plots. - Ensure your plots are properly labeled for clarity. ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the dataset data = { \\"participant_id\\": [1, 2, 3], \\"condition1\\": [5, 6, 7], \\"condition2\\": [7, 8, 9], \\"condition3\\": [6, 7, 8] } df = pd.DataFrame(data) # Task 1: Convert to long-form df_long = pd.melt(df, id_vars=[\\"participant_id\\"], value_vars=[\\"condition1\\", \\"condition2\\", \\"condition3\\"], var_name=\\"condition\\", value_name=\\"score\\") # Task 2: Convert to wide-form df_wide = df.set_index(\\"participant_id\\") # Visualization 1: Line plot with long-form data sns.lineplot(data=df_long, x=\\"condition\\", y=\\"score\\", hue=\\"participant_id\\") plt.title(\\"Line Plot of Scores by Participant and Condition\\") plt.show() # Visualization 2: Box plot with wide-form data sns.boxplot(data=df_wide) plt.title(\\"Box Plot of Scores Across Conditions\\") plt.xlabel(\\"Condition\\") plt.ylabel(\\"Score\\") plt.show() ``` # Expected Output: 1. Long-form DataFrame: ``` participant_id condition score 0 1 condition1 5 1 2 condition1 6 2 3 condition1 7 3 1 condition2 7 4 2 condition2 8 5 3 condition2 9 6 1 condition3 6 7 2 condition3 7 8 3 condition3 8 ``` 2. Wide-form DataFrame: ``` condition condition1 condition2 condition3 participant_id 1 5 7 6 2 6 8 7 3 7 9 8 ``` 3. Line plot showing scores of each participant across conditions. 4. Box plot comparing score distributions across conditions. # Constraints: - You must use seaborn and pandas for visualization and data manipulation. - Ensure the plots are properly labeled.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def transform_and_visualize(data): # Load the dataset into a DataFrame df = pd.DataFrame(data) # Task 1: Convert to long-form df_long = pd.melt(df, id_vars=[\\"participant_id\\"], value_vars=[\\"condition1\\", \\"condition2\\", \\"condition3\\"], var_name=\\"condition\\", value_name=\\"score\\") # Task 2: Convert to wide-form df_wide = df.set_index(\\"participant_id\\") # Visualization 1: Line plot with long-form data plt.figure(figsize=(10, 6)) sns.lineplot(data=df_long, x=\\"condition\\", y=\\"score\\", hue=\\"participant_id\\", marker=\'o\') plt.title(\\"Line Plot of Scores by Participant and Condition\\") plt.xlabel(\\"Condition\\") plt.ylabel(\\"Score\\") plt.legend(title=\\"Participant ID\\") plt.show() # Visualization 2: Box plot with wide-form data plt.figure(figsize=(10, 6)) sns.boxplot(data=df_wide) plt.title(\\"Box Plot of Scores Across Conditions\\") plt.xlabel(\\"Condition\\") plt.ylabel(\\"Score\\") plt.show() return df_long, df_wide"},{"question":"# Email Parsing, Generation, and Encoding Objective: Write a Python function that performs the following tasks: 1. **Parse an email** from a given string. 2. **Extract specific information** from the parsed email such as sender, recipient, subject, and body. 3. **Modify the email content** by appending a specified signature to the body. 4. **Encode the modified email** using Base64 encoding. 5. **Generate a new MIME document** from the modified email. Expected Input and Output: - **Input:** - A string representing the raw email data. - A string representing the signature to be appended to the email body. - **Output:** - A Base64 encoded string of the modified email. # Constraints: - The input email string will be a valid RFC 822 formatted email. - The signature will be a non-empty string. # Performance Requirements: - The solution should efficiently handle emails up to 1MB in size. Function Signature: ```python def handle_email(raw_email: str, signature: str) -> str: pass ``` Example: ```python raw_email = From: sender@example.com To: recipient@example.com Subject: Test Email This is the body of the email. signature = \\"nnBest regards,nXYZ Team\\" # Expected output is a Base64 encoded string of the modified email result = handle_email(raw_email, signature) print(result) # Should print a Base64 encoded string ``` Guidelines: 1. **Parsing the Email:** - Use `email.message_from_string` to parse the raw email string. 2. **Extract Information:** - Extract \'From\', \'To\', \'Subject\', and \'Body\' from the message. 3. **Modify Content:** - Append the signature to the email body. 4. **Encode the Email:** - Use `base64` module to encode the modified email. 5. **Generate MIME Document:** - Create a new MIME message from the modified content. Refer to the `email`, `base64`, and other relevant Python modules for implementation details.","solution":"import email import base64 from email import policy from email.parser import Parser from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText def handle_email(raw_email: str, signature: str) -> str: # Parse the raw email msg = Parser(policy=policy.default).parsestr(raw_email) # Extract the body and append the signature if msg.is_multipart(): parts = [part.get_payload(decode=True).decode(part.get_content_charset() or \'utf-8\') for part in msg.get_payload() if part.get_content_type() == \'text/plain\'] body = \\"n\\".join(parts) else: body = msg.get_payload(decode=True).decode(msg.get_content_charset() or \'utf-8\') modified_body = body + signature # Create a new MIMEText object with the modified body new_msg = MIMEMultipart() new_msg[\'From\'] = msg[\'From\'] new_msg[\'To\'] = msg[\'To\'] new_msg[\'Subject\'] = msg[\'Subject\'] new_msg.attach(MIMEText(modified_body, \'plain\')) # Convert the new message to a string final_msg = new_msg.as_string() # Base64 encode the final message encoded_msg = base64.b64encode(final_msg.encode(\'utf-8\')).decode(\'utf-8\') return encoded_msg"},{"question":"**Context:** You are given a raw audio fragment in bytes format and need to perform a series of operations to process it. The tasks involve converting the audio between different formats, analyzing statistical properties, and manipulating the audio to enhance the signal. **Objective:** Write a Python function `process_audio_fragments` that takes the following inputs: - `fragment: bytes` - The input audio fragment in raw bytes. - `width: int` - The sample width in bytes (1, 2, 3, or 4). - `state: tuple or None` - The state for ADPCM conversion (can be `None` for initial call). - `lfactor: float` - Left channel factor for stereo conversion. - `rfactor: float` - Right channel factor for stereo conversion. The function should perform the following sequence of operations: 1. **Convert the linear audio fragment to a-LAW and then back to linear** with the same width. 2. **Convert the linear audio fragment to ADPCM** using Intel/DVI ADPCM coding. 3. **Convert the ADPCM fragment back to a linear fragment**. 4. **Calculate the average value of the samples** in the resultant linear fragment. 5. **Convert the linear fragment to a stereo fragment** using the provided channel factors. 6. **Calculate the RMS value of the stereo fragment**. 7. **Return the final stereo fragment**, the calculated average value, and the RMS value. # Function Signature ```python def process_audio_fragments(fragment: bytes, width: int, state: tuple or None, lfactor: float, rfactor: float) -> tuple: pass ``` # Example Usage ```python sample_fragment = b\'x01x02x03x04\' # Sample data for illustration width = 2 state = None lfactor = 1.0 rfactor = 0.5 stereo_fragment, avg_value, rms_value = process_audio_fragments(sample_fragment, width, state, lfactor, rfactor) print(stereo_fragment) print(avg_value) print(rms_value) ``` # Constraints - The function should handle errors gracefully, raising an `audioop.error` if invalid data is encountered. - Assume the audio fragment will always have length multiples of the sample width. - Performance should be efficient with minimal redundant computations. Implement this function in Python using the \\"audioop\\" module.","solution":"import audioop def process_audio_fragments(fragment: bytes, width: int, state: tuple or None, lfactor: float, rfactor: float): # Step 1: Convert the linear audio fragment to a-LAW and then back to linear try: alaw = audioop.lin2alaw(fragment, width) linear_fragment = audioop.alaw2lin(alaw, width) # Step 2: Convert the linear audio fragment to ADPCM adpcm_fragment, state = audioop.lin2adpcm(linear_fragment, width, state) # Step 3: Convert the ADPCM fragment back to a linear fragment linear_fragment, state = audioop.adpcm2lin(adpcm_fragment, width, state) # Step 4: Calculate the average value of the samples in the resultant linear fragment avg_value = audioop.avg(linear_fragment, width) # Step 5: Convert the linear fragment to a stereo fragment stereo_fragment = audioop.tostereo(linear_fragment, width, lfactor, rfactor) # Step 6: Calculate the RMS value of the stereo fragment rms_value = audioop.rms(stereo_fragment, width) return (stereo_fragment, avg_value, rms_value) except audioop.error as e: raise e"},{"question":"# Question: Advanced Text Templating and Logging You are tasked with creating a Python program that generates customized email templates for a marketing campaign and logs all the email generation activities. Requirements 1. **Create a Template Class**: - Use the `string.Template` class to create an email template. The template should include placeholders for the recipient\'s name, discount offer, and the expiration date of the offer. 2. **Generate Emails**: - Write a function `generate_emails(template_str, recipients_data)` that accepts a template string and a list of dictionaries representing recipients\' data. Each dictionary contains keys: \'name\', \'discount\', and \'expiration\'. - For each recipient in `recipients_data`, the function should generate a customized email by substituting the placeholders in the template string with the actual data. 3. **Log Activities**: - Implement logging to keep track of email generation activities. Use the `logging` module to log an info message each time an email is successfully generated and a warning if any required data is missing for a recipient. 4. **Handle Missing Data**: - Ensure your function can handle the absence of data gracefully using the `safe_substitute` method, and log a warning message when a placeholder is left unchanged due to missing data. Input and Output Formats - **Input**: - `template_str`: A string containing the template with placeholders (e.g., \\"Hello name,nnWe have a special offer for you! Get discount off on your next purchase. Hurry, the offer expires on expiration.n\\") - `recipients_data`: A list of dictionaries, each containing \'name\', \'discount\', and \'expiration\'. (e.g., [{\'name\': \'Alice\', \'discount\': \'20%\', \'expiration\': \'30th Oct\'}, {\'name\': \'Bob\', \'discount\': \'30%\'}]) - **Output**: - A list of strings where each string is a customized email for a recipient. Function Signature ```python def generate_emails(template_str: str, recipients_data: list) -> list: ``` Example ```python template_str = Hello name, We have a special offer for you! Get discount off on your next purchase. Hurry, the offer expires on expiration. recipients_data = [ {\'name\': \'Alice\', \'discount\': \'20%\', \'expiration\': \'30th Oct\'}, {\'name\': \'Bob\', \'discount\': \'30%\'} # Missing expiration date ] emails = generate_emails(template_str, recipients_data) for email in emails: print(email) ``` **Output**: ``` Hello Alice, We have a special offer for you! Get 20% off on your next purchase. Hurry, the offer expires on 30th Oct. Hello Bob, We have a special offer for you! Get 30% off on your next purchase. Hurry, the offer expires on expiration. ``` **Log**: ``` INFO: Successfully generated email for Alice. WARNING: Missing data for one or more placeholders in email for Bob. ``` Use the `logging` module to log these messages appropriately within the `generate_emails` function.","solution":"import logging from string import Template # Set up logging logging.basicConfig(level=logging.INFO, format=\'%(levelname)s: %(message)s\') def generate_emails(template_str, recipients_data): emails = [] email_template = Template(template_str) for recipient in recipients_data: try: email = email_template.safe_substitute(recipient) emails.append(email) # Check if any placeholders are left unresolved if \'\' in email: logging.warning(f\\"Missing data for one or more placeholders in email for {recipient.get(\'name\', \'Unknown\')}.\\") else: logging.info(f\\"Successfully generated email for {recipient[\'name\']}.\\") except KeyError as e: logging.warning(f\\"KeyError: Missing key {str(e)} for recipient {recipient[\'name\']}\\") emails.append(email_template.safe_substitute(recipient)) return emails"},{"question":"# Unicode Property Analysis Problem Statement Write a Python function `unicode_properties_summary` that takes a list of Unicode code points (integers) and returns a summary of their properties. Specifically, the function should: 1. Convert each code point to its corresponding Unicode character. 2. For each character, determine its: - **Category** (using `unicodedata.category`). - **Name** (using `unicodedata.name`), handling characters without a name. - **Numeric value** (using `unicodedata.numeric`), if applicable. The function should output a dictionary where the keys are the input code points (in hexadecimal string format, e.g., \\"U+265E\\"), and the values are dictionaries with the properties listed above. If a character does not have a numeric value, the value should be `None`. Input Format - A list of non-negative integers representing Unicode code points. The integers will be within the range 0 to 0x10FFFF. Output Format - A dictionary with the following structure: ```python { \\"U+XXXX\\": { \\"character\\": \\"CHAR\\", \\"category\\": \\"CAT\\", \\"name\\": \\"NAME\\", \\"numeric_value\\": NUM }, ... } ``` Example ```python def unicode_properties_summary(code_points): import unicodedata summary = {} for code in code_points: hex_code = f\\"U+{code:04X}\\" char = chr(code) category = unicodedata.category(char) name = unicodedata.name(char, \\"UNKNOWN\\") try: numeric_value = unicodedata.numeric(char) except ValueError: numeric_value = None summary[hex_code] = { \\"character\\": char, \\"category\\": category, \\"name\\": name, \\"numeric_value\\": numeric_value } return summary # Example usage: print(unicode_properties_summary([0x265E, 0x0030, 0x1F600, 0x00E9])) ``` Expected output: ```python { \\"U+265E\\": { \\"character\\": \\"♞\\", \\"category\\": \\"So\\", \\"name\\": \\"BLACK CHESS KNIGHT\\", \\"numeric_value\\": None }, \\"U+0030\\": { \\"character\\": \\"0\\", \\"category\\": \\"Nd\\", \\"name\\": \\"DIGIT ZERO\\", \\"numeric_value\\": 0 }, \\"U+1F600\\": { \\"character\\": \\"😀\\", \\"category\\": \\"So\\", \\"name\\": \\"GRINNING FACE\\", \\"numeric_value\\": None }, \\"U+00E9\\": { \\"character\\": \\"é\\", \\"category\\": \\"Ll\\", \\"name\\": \\"LATIN SMALL LETTER E WITH ACUTE\\", \\"numeric_value\\": None } } ``` Constraints 1. You may assume that the input list will contain only valid Unicode code points. 2. Handle characters that do not have a defined name gracefully using the `unicodedata.name` function\'s second `default` parameter. 3. Performance is not a critical issue, but aim to keep the code clean and efficient.","solution":"import unicodedata def unicode_properties_summary(code_points): summary = {} for code in code_points: hex_code = f\\"U+{code:04X}\\" char = chr(code) category = unicodedata.category(char) name = unicodedata.name(char, \\"UNKNOWN\\") try: numeric_value = unicodedata.numeric(char) except ValueError: numeric_value = None summary[hex_code] = { \\"character\\": char, \\"category\\": category, \\"name\\": name, \\"numeric_value\\": numeric_value } return summary"},{"question":"# Question You are given a dataset containing measurements of different features for several species of penguins. Use the seaborn package to create visualizations that will help you analyze the data. Your goal is to write functions that generate specific plots as described below. Dataset The penguin dataset can be loaded using the following code: ```python import seaborn as sns penguins = sns.load_dataset(\\"penguins\\") ``` Task 1 Write a function `plot_flipper_length_distribution` that plots the ECDF of the `flipper_length_mm` feature using seaborn\'s `ecdfplot`. The plot should have the `flipper_length_mm` values on the x-axis. **Function Signature:** ```python def plot_flipper_length_distribution(): pass ``` **Output:** The function should display the plot using matplotlib\'s `show()` function. Task 2 Write a function `plot_species_bill_length` that plots the ECDF of the `bill_length_mm` feature with different species distinguished by color. Use seaborn\'s `ecdfplot` and the `hue` parameter to achieve this. **Function Signature:** ```python def plot_species_bill_length(): pass ``` **Output:** The function should display the plot using matplotlib\'s `show()` function. Task 3 Write a function `plot_count_distribution` that plots the ECDF of the `bill_length_mm` feature, showing the count of observations rather than the proportion. **Function Signature:** ```python def plot_count_distribution(): pass ``` **Output:** The function should display the plot using matplotlib\'s `show()` function. Task 4 Write a function `plot_complementary_cdf` that plots the complementary ECDF (1 - CDF) for the `bill_length_mm` feature. **Function Signature:** ```python def plot_complementary_cdf(): pass ``` **Output:** The function should display the plot using matplotlib\'s `show()` function. **Requirements:** 1. Your functions should load the penguin dataset within the function. 2. Make sure to label the axes of your plots appropriately. 3. Include a legend where necessary to distinguish between different species. **Example Usage:** ```python plot_flipper_length_distribution() # Should display a plot plot_species_bill_length() # Should display a plot plot_count_distribution() # Should display a plot plot_complementary_cdf() # Should display a plot ``` ```python # The fallowing code can be used if required for demonstration import matplotlib.pyplot as plt # Example for Task 1 def plot_flipper_length_distribution(): penguins = sns.load_dataset(\\"penguins\\") sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"ECDF\\") plt.title(\\"ECDF of Flipper Length\\") plt.show() plot_flipper_length_distribution() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_flipper_length_distribution(): penguins = sns.load_dataset(\\"penguins\\") sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"ECDF\\") plt.title(\\"ECDF of Flipper Length\\") plt.show() def plot_species_bill_length(): penguins = sns.load_dataset(\\"penguins\\") sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"ECDF\\") plt.title(\\"ECDF of Bill Length by Species\\") plt.legend(title=\\"Species\\") plt.show() def plot_count_distribution(): penguins = sns.load_dataset(\\"penguins\\") sns.histplot(penguins[\'bill_length_mm\'], cumulative=True, stat=\\"count\\", element=\\"step\\", fill=False) plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Count\\") plt.title(\\"ECDF (Count) of Bill Length\\") plt.show() def plot_complementary_cdf(): penguins = sns.load_dataset(\\"penguins\\") sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", complementary=True) plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"1 - ECDF\\") plt.title(\\"Complementary ECDF of Bill Length\\") plt.show()"},{"question":"# Custom Descriptor Implementation and Usage **Objective:** Implement a custom descriptor in Python that controls the access to an attribute, ensuring the attribute\'s value is always within a specified range. # Task: 1. **Define a class** `BoundedValue` that implements custom descriptor behavior using the `__get__` and `__set__` methods. 2. **Ensure** that any value set for the attribute within a `BoundedValue` is between a minimum and maximum bound, inclusive. If a value outside this range is assigned, raise a `ValueError`. 3. **Create a class** `Temperature` that uses this descriptor to control the setting of temperature values which must be within the range of `-273.15` to `1000` (temperature in Celsius). # Specifications: 1. Class `BoundedValue`: - `__init__(self, min_value, max_value)`: Initializes the descriptor with the specified minimum and maximum values. - `__get__(self, instance, owner)`: Retrieves the attribute value. - `__set__(self, instance, value)`: Sets the attribute value, ensuring it is within the specified range, or raises `ValueError`. 2. Class `Temperature`: - Attribute `celsius`: Should use `BoundedValue` to control the range. # Example Usage: ```python class BoundedValue: def __init__(self, min_value, max_value): # Initialize with min and max values pass def __get__(self, instance, owner): # Retrieve the attribute value pass def __set__(self, instance, value): # Set the attribute value ensuring the range check pass class Temperature: celsius = BoundedValue(-273.15, 1000) def __init__(self, current_temp): self.celsius = current_temp # Example usage: temp = Temperature(25) print(temp.celsius) # Output: 25 temp.celsius = -300 # Should raise ValueError ``` # Constraints: - Implement the classes and methods as described. - Ensure proper error handling by raising `ValueError` for out-of-range assignments.","solution":"class BoundedValue: def __init__(self, min_value, max_value): self.min_value = min_value self.max_value = max_value self._values = {} def __get__(self, instance, owner): return self._values.get(instance, None) def __set__(self, instance, value): if not (self.min_value <= value <= self.max_value): raise ValueError(f\\"Value must be between {self.min_value} and {self.max_value}\\") self._values[instance] = value class Temperature: celsius = BoundedValue(-273.15, 1000) def __init__(self, current_temp): self.celsius = current_temp"},{"question":"# Outlier and Novelty Detection with scikit-learn Objective You are tasked with demonstrating your understanding of outlier and novelty detection methods in scikit-learn. Specifically, you need to implement and compare different algorithms for handling outliers in a dataset and create a small application that utilizes these methods. Instructions 1. **Implement Outlier Detection Methods**: - Import the `IsolationForest`, `OneClassSVM`, and `EllipticEnvelope` classes from `sklearn`. - Use these classes to implement outlier detection on the provided dataset. 2. **Compare Different Methods**: - Ensure your implementation includes a comparison of the results obtained from each method, in terms of the detection accuracy and any other relevant metrics. - Use visualization tools (like matplotlib) to demonstrate and compare the boundaries, inliers, and outliers detected by each method. 3. **Application Development**: - Create a simple application that uses `LocalOutlierFactor` for both outlier detection and novelty detection. - Train the model on the provided dataset and use it to predict the outliers in a test set. - Ensure the application outputs predicted inliers and outliers for both training and test datasets. - Visualize your results using a scatter plot. Provided Data ```python import numpy as np from sklearn.datasets import make_blobs # Generating synthetic data X_train, _ = make_blobs(n_samples=300, centers=1, cluster_std=0.4, random_state=42) X_test, _ = make_blobs(n_samples=100, centers=1, cluster_std=0.8, random_state=42) X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2)) # Combine the datasets X_test = np.concatenate([X_test, X_outliers], axis=0) ``` Constraints - Focus on proper use of scikit-learn outlier detection and novelty detection functionalities. - Use concise and efficient code where possible. - Ensure your comparisons and results are clearly visualized for proper interpretation. # Example Output Your program should produce the following output: 1. Plots for each of the outlier detection methods showing the detected inliers and outliers. 2. A detailed report comparing the accuracy and efficiency of the various methods. 3. A separate plot showing the results of the `LocalOutlierFactor` for both outlier detection (using fit_predict) and novelty detection (predicting new data). Submission Please submit a Jupyter Notebook (.ipynb) containing the complete implementation along with the visualizations and comparison report.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.ensemble import IsolationForest from sklearn.svm import OneClassSVM from sklearn.covariance import EllipticEnvelope from sklearn.neighbors import LocalOutlierFactor from sklearn.datasets import make_blobs # Generating synthetic data X_train, _ = make_blobs(n_samples=300, centers=1, cluster_std=0.4, random_state=42) X_test, _ = make_blobs(n_samples=100, centers=1, cluster_std=0.8, random_state=42) X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2)) # Combine the datasets X_test = np.concatenate([X_test, X_outliers], axis=0) def plot_results(X_train, X_test, y_pred, title): plt.figure(figsize=(10, 6)) plt.title(title) plt.scatter(X_train[:, 0], X_train[:, 1], c=\'white\', edgecolor=\'k\', s=20, label=\'Training Data\') plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, edgecolor=\'k\', s=20) plt.legend() plt.show() # Isolation Forest clf = IsolationForest(contamination=0.1, random_state=42) clf.fit(X_train) y_pred_train_if = clf.predict(X_train) y_pred_test_if = clf.predict(X_test) plot_results(X_train, X_test, y_pred_test_if, \\"Isolation Forest\\") # One-Class SVM clf = OneClassSVM(gamma=\'auto\', nu=0.1) clf.fit(X_train) y_pred_train_svm = clf.predict(X_train) y_pred_test_svm = clf.predict(X_test) plot_results(X_train, X_test, y_pred_test_svm, \\"One-Class SVM\\") # Elliptic Envelope clf = EllipticEnvelope(contamination=0.1) clf.fit(X_train) y_pred_train_ee = clf.predict(X_train) y_pred_test_ee = clf.predict(X_test) plot_results(X_train, X_test, y_pred_test_ee, \\"Elliptic Envelope\\") # Local Outlier Factor for outlier detection clf_lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1) y_pred_train_lof = clf_lof.fit_predict(X_train) y_pred_test_lof = clf_lof.fit_predict(X_test) plot_results(X_train, X_test, y_pred_test_lof, \\"Local Outlier Factor (Outlier Detection)\\") # Local Outlier Factor for novelty detection clf_lof_novelty = LocalOutlierFactor(n_neighbors=20, novelty=True, contamination=0.1) clf_lof_novelty.fit(X_train) y_pred_train_novelty = clf_lof_novelty.predict(X_train) y_pred_test_novelty = clf_lof_novelty.predict(X_test) plot_results(X_train, X_test, y_pred_test_novelty, \\"Local Outlier Factor (Novelty Detection)\\")"},{"question":"# Seaborn Advanced Plotting You are provided with the `penguins` dataset. Your task is to generate a multi-faceted plot using Seaborn\'s `seaborn.objects` module. The plot should include the following elements: 1. **Data Requirements**: - Load the `penguins` dataset using `seaborn.load_dataset(\\"penguins\\")`. 2. **Plot Structure and Specifications**: - Create a plot with `body_mass_g` on the x-axis and `species` on the y-axis. - Use different colors to represent different `sex`. - Add dot markers for each species and sex combination with appropriate aggregation. - Include error bars representing the standard deviation for each group. - Customize the plot to have faceted plots for each `island` category separately. 3. **Additional Customizations**: - Exclude `linestyle` and `linewidth` properties from influencing the dot markers or error bars. - Directly assign the `ymin` and `ymax` limits for a separate plot within the same figure, showing the range between `bill_depth_mm` and `bill_length_mm` for each `penguin` (i.e., individual observation). Your function `plot_penguin_data()` should not return any values but rather display the created plots. Use appropriate Seaborn and Matplotlib functionalities to ensure the output is visually clear and well-organized. Input: - No function inputs are required. The function should load the dataset internally. Output: - The function should display the plot directly. Constraints: - Ensure your solution uses the Seaborn\'s `seaborn.objects` module effectively. - Ensure the plot is divided into facets for each unique `island`. - Properly manage plot styling such that `linestyle` and `linewidth` do not affect the dot markers or the error bar ranges. # Example: ```python def plot_penguin_data(): import seaborn.objects as so from seaborn import load_dataset penguins = load_dataset(\\"penguins\\") # Creating main plot plot = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"sex\\") .facet(\\"island\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) # Displaying the plot plot.show() # Creating and showing the range plot range_plot = ( penguins.rename_axis(index=\\"penguin\\") .pipe(so.Plot, x=\\"penguin\\", ymin=\\"bill_depth_mm\\", ymax=\\"bill_length_mm\\") .add(so.Range(), color=\\"island\\") ) # Displaying the range plot range_plot.show() # Test the function plot_penguin_data() ``` Note: The example serves as a template. Adjust your solution to conform to the requirements and constraints specified.","solution":"def plot_penguin_data(): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt penguins = load_dataset(\\"penguins\\") # Creating main plot plot = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"sex\\") .facet(\\"island\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) # Displaying the plot plot.show() # Creating and showing the range plot range_plot = ( penguins.rename_axis(index=\\"penguin\\") .pipe(so.Plot, x=\\"penguin\\", ymin=\\"bill_depth_mm\\", ymax=\\"bill_length_mm\\") .add(so.Range(), color=\\"island\\") ) # Displaying the range plot range_plot.show()"},{"question":"# Custom Serialization/Deserialization using `copyreg` Module Objective You are required to demonstrate your understanding of the `copyreg` module in Python by writing code that defines custom serialization (pickling) and deserialization (unpickling) functions for a given class. This exercise will help verify your grasp of fundamental and advanced concepts involving the `copyreg` package in Python. Problem Statement Implement a class named `Person` with the following specifications: - The class should have two attributes: `name` (str) and `age` (int). - The class should have a custom method `__str__()` that returns a string in the format: \\"Person(name=<name>, age=<age>)\\". Next, implement custom serialization and deserialization functions for the `Person` class using the `copyreg` module. Ensure that these custom functions are registered using `copyreg.pickle`. Requirements 1. **Class Definition**: - Define the `Person` class with appropriate `__init__()` and `__str__()` methods. 2. **Serialization Function**: - Implement a function `pickle_person(person)`, which takes an instance of `Person` and returns a tuple containing the class `Person` and its attributes (`name` and `age`). - Register this function using `copyreg.pickle`. 3. **Deserialization Function**: - By default, the class constructor will serve as the deserialization function. Ensure it can reconstruct a `Person` instance from the provided attributes. 4. **Testing**: - Create an instance of `Person`, serialize it using `pickle.dumps()`, and then deserialize it using `pickle.loads()`. - Validate that the deserialized instance is identical to the original instance. Constraints - The `name` attribute should be a non-empty string. - The `age` attribute should be a non-negative integer. Example ```python import copyreg import pickle class Person: def __init__(self, name, age): self.name = name self.age = age def __str__(self): return f\\"Person(name={self.name}, age={self.age})\\" def pickle_person(person): return Person, (person.name, person.age) # Register the pickle function copyreg.pickle(Person, pickle_person) # Test the implementation person = Person(\\"John Doe\\", 25) serialized_person = pickle.dumps(person) deserialized_person = pickle.loads(serialized_person) print(deserialized_person) # Output should be: Person(name=John Doe, age=25) ``` **Note**: Your solution should not include print statements. They are included here only for illustrative purposes. Submission Submit your implementation of the `Person` class, the `pickle_person` function, and the necessary `copyreg.pickle` registration.","solution":"import copyreg import pickle class Person: def __init__(self, name, age): self.name = name self.age = age def __str__(self): return f\\"Person(name={self.name}, age={self.age})\\" def pickle_person(person): return Person, (person.name, person.age) # Register the pickle function copyreg.pickle(Person, pickle_person) # Sample instance of Person person = Person(\\"John Doe\\", 25) # Serializing the instance serialized_person = pickle.dumps(person) # Deserializing the instance deserialized_person = pickle.loads(serialized_person)"},{"question":"Objective: Demonstrate your understanding of asyncio synchronization primitives in Python 3.10 by implementing and coordinating tasks using these primitives to achieve a specific goal. Problem Statement: You are building a simple simulation of a ticket booking system for a theater. This system manages ticket availability and allows multiple users (tasks) to book tickets asynchronously. The task is to ensure that no more tickets are booked than available and that users are notified if tickets become available due to cancellations. Implement the following functions: 1. **class AsyncBookingSystem**: - `__init__(self, total_tickets: int)`: Initializes the booking system with a specified number of total tickets available. - `async def book_ticket(self, user_id: int) -> bool`: Books a ticket for the user if available. Returns `True` if the booking is successful, `False` otherwise. - `async def cancel_ticket(self, user_id: int) -> bool`: Cancels a ticket for the user if they had booked one. Returns `True` if the cancellation is successful, `False` otherwise. - `async def wait_for_available_ticket(self) -> None`: Waits until a ticket becomes available if currently sold out. Requirements: - Use asyncio synchronization primitives such as Lock, Event, Condition, Semaphore, or BoundedSemaphore. - Ensure thread-safety for the shared resource (tickets). - Implement fairness where the first task that waits for a ticket gets it when available. - Maintain an appropriate balance between multiple users concurrently trying to book and cancel tickets. - Handle edge cases gracefully, such as users trying to book a ticket when no tickets are available and users canceling tickets they never booked. Constraints: - The number of tickets (`total_tickets`) is a positive integer. - There can be multiple concurrent booking and cancellation requests. Input/Output Format: - **Input**: The class and functions do not directly take input from the user but operate based on method calls. - **Output**: The functions `book_ticket` and `cancel_ticket` return a boolean indicating success or failure. The function `wait_for_available_ticket` does not return but waits until a ticket is available. Example Usage: ```python import asyncio # Example usage async def main(): booking_system = AsyncBookingSystem(total_tickets=5) async def user_actions(user_id): if await booking_system.book_ticket(user_id): print(f\\"User {user_id} successfully booked a ticket.\\") else: print(f\\"User {user_id} could not book a ticket.\\") await asyncio.sleep(1) if await booking_system.cancel_ticket(user_id): print(f\\"User {user_id} successfully canceled their ticket.\\") else: print(f\\"User {user_id} had no ticket to cancel.\\") tasks = [user_actions(user_id) for user_id in range(10)] await asyncio.gather(*tasks) asyncio.run(main()) ``` Note: - You are responsible for implementing the `AsyncBookingSystem` class with the specified functions. - Do not use threading primitives. Use only asyncio primitives to manage the concurrency.","solution":"import asyncio class AsyncBookingSystem: def __init__(self, total_tickets: int): self.total_tickets = total_tickets self.available_tickets = total_tickets self.lock = asyncio.Lock() self.condition = asyncio.Condition() async def book_ticket(self, user_id: int) -> bool: async with self.lock: if self.available_tickets > 0: self.available_tickets -= 1 return True return False async def cancel_ticket(self, user_id: int) -> bool: async with self.lock: if self.available_tickets < self.total_tickets: self.available_tickets += 1 async with self.condition: self.condition.notify_all() return True return False async def wait_for_available_ticket(self) -> None: async with self.condition: while self.available_tickets == 0: await self.condition.wait()"},{"question":"# Question: Environment Variable Manipulation and Large File Handling **Objective:** Assess your ability to handle environment variables and large files using the `os` module in Python. **Task:** You need to implement a function `manage_environment_and_files` that performs the following tasks: 1. Retrieves and prints the current value of a specified environment variable, if it exists. If the environment variable does not exist, it should print `\\"Variable not found\\"`. 2. Sets or updates an environment variable with a given key-value pair. 3. Reads a large file (greater than 2 GiB) specified by its file path and returns the number of lines in the file. **Function Signature:** ```python def manage_environment_and_files(env_var: str, new_env_var_key: str, new_env_var_value: str, file_path: str) -> int: :param env_var: The environment variable to retrieve. :param new_env_var_key: The key of the environment variable to set/update. :param new_env_var_value: The value of the environment variable to set/update. :param file_path: The path to the large file to read. :return: The number of lines in the large file. ``` **Input Constraints:** - `env_var`, `new_env_var_key`, and `new_env_var_value` are non-empty strings. - `file_path` is a string representing a valid file path to a large file (greater than 2 GiB). **Output:** - The function should print the value of `env_var` or `\\"Variable not found\\"`. - The function should return an integer representing the number of lines in the large file specified by `file_path`. **Example Usage:** Assume the environment variable `HOME` is `/home/user` and we have a large file `large_file.txt` with 3 GiB of data having 150 million lines. ```python lines = manage_environment_and_files(\'HOME\', \'MY_VAR\', \'test_value\', \'large_file.txt\') # Output: # /home/user # lines should be 150_000_000 ``` **Notes:** - The function must handle reading large files efficiently. - Use the `os` module for environment variable manipulations as specified in the documentation. - The solution should work across Unix-like systems.","solution":"import os def manage_environment_and_files(env_var: str, new_env_var_key: str, new_env_var_value: str, file_path: str) -> int: Retrieves and prints the current value of a specified environment variable, sets/updates an environment variable, and reads a large file to count and return the number of lines in the file. :param env_var: The environment variable to retrieve. :param new_env_var_key: The key of the environment variable to set/update. :param new_env_var_value: The value of the environment variable to set/update. :param file_path: The path to the large file to read. :return: The number of lines in the large file. # Printing the current value of the environment variable if it exists. current_value = os.environ.get(env_var) if current_value: print(current_value) else: print(\\"Variable not found\\") # Setting or updating the environment variable. os.environ[new_env_var_key] = new_env_var_value # Opening the file and counting the number of lines. line_count = 0 with open(file_path, \'r\') as file: for line in file: line_count += 1 return line_count"},{"question":"Objective Implement a custom HTML parser by subclassing the `HTMLParser` class. Your parser will process an HTML string and convert it into a structured dictionary format representing the HTML document. Problem Statement You are tasked with writing a class `CustomHTMLParser` that extends `html.parser.HTMLParser`. Your parser should process an HTML string and produce a nested dictionary where each HTML element is represented as a dictionary. The structure of the dictionary should be as follows: - The key is the tag name of the HTML element. - The value is another dictionary with two keys: - `attributes`: A dictionary of the element\'s attributes. - `children`: A list containing dictionaries of the child elements. Method Signature - `class CustomHTMLParser(HTMLParser):` - `def __init__(self):` Initializes the parser. - `def handle_starttag(self, tag, attrs):` - `def handle_endtag(self, tag):` - `def handle_data(self, data):` - `def parse(self, html_string: str) -> dict:` Input - A string `html_string` which represents the HTML content. It is guaranteed to be a well-formed HTML. Output - A dictionary representing the HTML structure as described. Example Given the following HTML string: ```html <html> <head> <title>Sample Title</title> </head> <body> <h1>Heading</h1> <p class=\\"intro\\">This is a paragraph</p> </body> </html> ``` Your `parse` method should return: ```python { \\"html\\": { \\"attributes\\": {}, \\"children\\": [ { \\"head\\": { \\"attributes\\": {}, \\"children\\": [ { \\"title\\": { \\"attributes\\": {}, \\"children\\": [ {\\"data\\": \\"Sample Title\\"} ] } } ] } }, { \\"body\\": { \\"attributes\\": {}, \\"children\\": [ { \\"h1\\": { \\"attributes\\": {}, \\"children\\": [ {\\"data\\": \\"Heading\\"} ] } }, { \\"p\\": { \\"attributes\\": {\\"class\\": \\"intro\\"}, \\"children\\": [ {\\"data\\": \\"This is a paragraph\\"} ] } } ] } } ] } } ``` Constraints 1. The HTML string will always be well-formed. 2. The length of the HTML string will not exceed 10,000 characters. 3. Attribute values will be strings. 4. You do not need to handle script and style tags specifically; they will be treated as regular tags. Implementation Implement the `CustomHTMLParser` class: ```python from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.stack = [] self.result = {} def handle_starttag(self, tag, attrs): element = { \\"attributes\\": dict(attrs), \\"children\\": [] } if not self.stack: self.result[tag] = element self.stack.append(self.result[tag]) else: self.stack[-1][\\"children\\"].append({tag: element}) self.stack.append(self.stack[-1][\\"children\\"][-1][tag]) def handle_endtag(self, tag): if self.stack: self.stack.pop() def handle_data(self, data): if data.strip(): self.stack[-1][\\"children\\"].append({\\"data\\": data.strip()}) def parse(self, html_string): self.feed(html_string) return self.result ```","solution":"from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.stack = [] self.result = {} def handle_starttag(self, tag, attrs): element = { \\"attributes\\": dict(attrs), \\"children\\": [] } if not self.stack: self.result[tag] = element self.stack.append(self.result[tag]) else: self.stack[-1][\\"children\\"].append({tag: element}) self.stack.append(self.stack[-1][\\"children\\"][-1][tag]) def handle_endtag(self, tag): if self.stack: self.stack.pop() def handle_data(self, data): if data.strip(): self.stack[-1][\\"children\\"].append({\\"data\\": data.strip()}) def parse(self, html_string: str) -> dict: self.feed(html_string) return self.result"},{"question":"Context You are given a task to process a list of filenames and perform specific actions on them. However, during this processing, various system-level errors can occur (e.g., file not found, permission denied, etc.). You need to handle these errors elegantly using the `errno` module in Python. Task Write a Python function `process_files(file_list)` that takes a list of filenames `file_list` as its parameter. The function should attempt to open each file and perform a dummy read operation. The function should handle different system-level errors using appropriate handling routines and return a dictionary summarizing the results. Input - `file_list`: A list of strings, where each string is a filename/path. Output - A dictionary where the keys are filenames and values are strings describing the result of the operation. Possible values: - `\\"success\\"`: if the file was opened and read successfully. - `\\"file_not_found\\"`: if the file does not exist (map to `errno.ENOENT`). - `\\"permission_denied\\"`: if the file cannot be accessed due to permissions (map to `errno.EPERM` or `errno.EACCES`). - `\\"other_error\\"`: for any other errors. Constraints - Do not assume any specific format or validity of filenames. - Use the `errno` module for error handling, and map errors to their corresponding symbolic names. - Avoid using any external libraries except Python\'s standard library. Example ```python def process_files(file_list): import errno import os result = {} for file_name in file_list: try: with open(file_name, \'r\') as file: file.read() result[file_name] = \\"success\\" except FileNotFoundError: result[file_name] = \\"file_not_found\\" except PermissionError: result[file_name] = \\"permission_denied\\" except OSError as e: if e.errno in {errno.EPERM, errno.EACCES}: result[file_name] = \\"permission_denied\\" elif e.errno == errno.ENOENT: result[file_name] = \\"file_not_found\\" else: result[file_name] = \\"other_error\\" return result # Example usage: file_list = [\\"valid_file.txt\\", \\"missing_file.txt\\", \\"restricted_file.txt\\"] print(process_files(file_list)) ``` The provided code attempts to open each file in the list and handles different errors using the `errno` module.","solution":"def process_files(file_list): import errno result = {} for file_name in file_list: try: with open(file_name, \'r\') as file: file.read() result[file_name] = \\"success\\" except FileNotFoundError: result[file_name] = \\"file_not_found\\" except PermissionError: result[file_name] = \\"permission_denied\\" except OSError as e: if e.errno in {errno.EPERM, errno.EACCES}: result[file_name] = \\"permission_denied\\" elif e.errno == errno.ENOENT: result[file_name] = \\"file_not_found\\" else: result[file_name] = \\"other_error\\" return result"},{"question":"Objective: Implement a Python function that sends HTTP requests to a specified server and processes the response to handle different HTTP methods (GET, POST, PUT). Task: Write a Python function `http_client_request(host, port, method, url, headers=None, body=None)` that performs the following: 1. Establishes an HTTP connection to the specified `host` and `port`. 2. Sends an HTTP request using the specified `method` to the given `url`. 3. Optionally include request headers and body data if provided. 4. Retrieves and processes the server\'s response. 5. Returns a dictionary containing the status code, reason phrase, response headers, and the response body. Function Signature: ```python def http_client_request(host: str, port: int, method: str, url: str, headers: dict = None, body: str = None) -> dict: ``` Input: - `host` (str): The server\'s hostname (e.g., \'www.python.org\'). - `port` (int): The server\'s port number (e.g., 80). - `method` (str): The HTTP method to use for the request (e.g., \'GET\', \'POST\', \'PUT\'). - `url` (str): The URL path for the request (e.g., \'/index.html\'). - `headers` (dict): Optional dictionary of HTTP headers to include in the request. - `body` (str): Optional string containing the request body data (for methods like POST or PUT). Output: - A dictionary with the following keys and their corresponding values: - `\'status\'`: The HTTP status code returned by the server. - `\'reason\'`: The reason phrase returned by the server. - `\'headers\'`: A dictionary of response headers. - `\'body\'`: The response body (as a string). Constraints: - The function should handle any exceptions that occur during the request and return an appropriate error message in the dictionary if an error occurs. - The function should use the `http.client.HTTPConnection` class to establish the connection, send the request, and handle the response. Example: ```python def http_client_request(host: str, port: int, method: str, url: str, headers: dict = None, body: str = None) -> dict: import http.client import urllib.parse # Your code here # Example usage: response = http_client_request(\'www.python.org\', 80, \'GET\', \'/\') print(response) ``` Expected Output (The actual response may vary): ```python { \'status\': 200, \'reason\': \'OK\', \'headers\': { \'Content-Type\': \'text/html; charset=utf-8\', \'Content-Length\': \'5678\', ... }, \'body\': \'<!doctype html>...\' } ``` Note: You can refer to the `http.client` module documentation to understand the usage of different classes and methods for completing this task.","solution":"def http_client_request(host: str, port: int, method: str, url: str, headers: dict = None, body: str = None) -> dict: import http.client import urllib.parse connection = http.client.HTTPConnection(host, port) try: # Send request connection.request(method, url, body, headers or {}) # Get response response = connection.getresponse() # Read headers and body response_headers = dict(response.getheaders()) response_body = response.read().decode() return { \'status\': response.status, \'reason\': response.reason, \'headers\': response_headers, \'body\': response_body } except Exception as ex: return { \'status\': \'error\', \'reason\': str(ex), \'headers\': {}, \'body\': \'\' } finally: connection.close()"},{"question":"# User Authentication System with Crypt Module Your task is to implement a simple user authentication system using the `crypt` module to hash passwords. You will create two main functions: one to register users by storing their username and hashed password, and another to authenticate users by verifying their input passwords against the stored hash. Instructions: 1. **Function `register_user(username: str, password: str) -> None`**: - **Input**: - `username` (string): The username that identifies the user. This will be a unique string. - `password` (string): The user\'s password in plaintext. - **Process**: - Generate a hashed password using the strongest available method in `crypt.methods`. - Store the `username` and the hashed password in a dictionary `user_store`. - **Output**: None 2. **Function `authenticate_user(username: str, password: str) -> bool`**: - **Input**: - `username` (string): The username of the user attempting to log in. - `password` (string): The user\'s password in plaintext. - **Process**: - Retrieve the stored hash for the given username from `user_store`. - Use the `crypt.crypt` function to hash the input password with the stored hash. - Compare the newly hashed password with the stored hash securely using `hmac.compare_digest`. - **Output**: - Return `True` if authentication is successful (hashes match), else return `False`. Example Usage: ```python register_user(\'alice\', \'password123\') register_user(\'bob\', \'securePass456\') # Valid credentials assert authenticate_user(\'alice\', \'password123\') == True assert authenticate_user(\'bob\', \'securePass456\') == True # Invalid credentials assert authenticate_user(\'alice\', \'wrongPassword\') == False assert authenticate_user(\'charlie\', \'password123\') == False ``` Constraints: - Usernames will be unique and case-sensitive. - Passwords can contain any printable characters and be of any length. - Performance considerations: The functions must handle multiple user registrations and authentications efficiently. Notes: - Store the user data in a dictionary named `user_store`, where the key is the username, and the value is the hashed password. - You should handle exceptions appropriately and ensure the integrity and security of the user data. Tips: - Use the `mksalt` function to generate a salt with the strongest method available for password hashing during registration. - Be sure to handle any edge cases such as attempting to register a user with a duplicate username or authenticating a non-existent user. Implementation: ```python import crypt import hmac # Global user store for registered users user_store = {} def register_user(username: str, password: str) -> None: # Your implementation here pass def authenticate_user(username: str, password: str) -> bool: # Your implementation here pass ```","solution":"import crypt import hmac # Global user store for registered users user_store = {} def register_user(username: str, password: str) -> None: Registers a new user with a hashed password. if username in user_store: raise ValueError(\\"Username already exists\\") # Generate a hashed password using the strongest available method hashed_password = crypt.crypt(password, crypt.mksalt(crypt.METHOD_SHA512)) user_store[username] = hashed_password def authenticate_user(username: str, password: str) -> bool: Authenticates a user by verifying their password. if username not in user_store: return False stored_hash = user_store[username] new_hash = crypt.crypt(password, stored_hash) return hmac.compare_digest(stored_hash, new_hash)"},{"question":"You are required to implement a function using the `sndhdr` module that processes a list of sound files and returns a comprehensive report summarizing their attributes. The function should be capable of handling various scenarios, such as nonexistent files, unsupported file types, and valid sound files. # Function Signature ```python def generate_sound_report(file_list: List[str]) -> Dict[str, Any]: ``` # Input - `file_list` (List[str]): A list of file names as strings. Each file is a potential sound file. # Output - `Dict[str, Any]`: A dictionary where each key is a file name from the input list, and the corresponding value is another dictionary: - If the file type is successfully identified: ```python { \\"filetype\\": str, # the type of the sound file \\"framerate\\": int, # the framerate of the sound file \\"nchannels\\": int, # the number of channels \\"nframes\\": int, # the number of frames \\"sampwidth\\": int # the sample width } ``` - If the file does not exist: ```python { \\"error\\": \\"File not found\\" } ``` - If the file type is unsupported or unknown: ```python { \\"error\\": \\"Unsupported or unknown file type\\" } ``` # Example ```python files = [\\"example.wav\\", \\"unknown_format.xyz\\", \\"nonexistent_file.wav\\"] report = generate_sound_report(files) print(report) # Expected output: # { # \\"example.wav\\": { # \\"filetype\\": \\"wav\\", # \\"framerate\\": 44100, # \\"nchannels\\": 2, # \\"nframes\\": 215000, # \\"sampwidth\\": 2 # }, # \\"unknown_format.xyz\\": { # \\"error\\": \\"Unsupported or unknown file type\\" # }, # \\"nonexistent_file.wav\\": { # \\"error\\": \\"File not found\\" # } # } ``` # Constraints - Do not modify the `sndhdr` module itself. - Focus on handling file I/O and exception conditions gracefully. - Assume that the framerate, nchannels, nframes, sampwidth are non-negative integers except for error values described. # Notes - You may use the `os.path` module to check for file existence. - Pay attention to handling exceptions that may occur during file reading.","solution":"import os import sndhdr def generate_sound_report(file_list): report = {} for file in file_list: if not os.path.exists(file): report[file] = {\\"error\\": \\"File not found\\"} continue file_info = sndhdr.what(file) if file_info is None: report[file] = {\\"error\\": \\"Unsupported or unknown file type\\"} else: filetype, framerate, nchannels, nframes, sampwidth = file_info report[file] = { \\"filetype\\": filetype, \\"framerate\\": framerate, \\"nchannels\\": nchannels, \\"nframes\\": nframes, \\"sampwidth\\": sampwidth } return report"},{"question":"# Clustering Analysis Using Scikit-learn Objective: Test your understanding of various clustering algorithms in `sklearn.cluster` by implementing and comparing at least two different models on a synthetic dataset. Then, evaluate their performance using appropriate metrics. Task: 1. **Data Preparation:** - Create a synthetic dataset using `sklearn.datasets.make_blobs` function with 500 samples, 4 centers, and a random state of 42. - Split the dataset into training and testing sets with a ratio of 80% training and 20% testing. 2. **Model Implementation:** - Implement and fit the following two clustering models using the training data: - `KMeans` - `DBSCAN` 3. **Model Comparison:** - For each model, predict the cluster labels for the test set. - Apply an appropriate evaluation metric(s) to compare the clustering results. You may use metrics such as Adjusted Rand Index (ARI), Silhouette Coefficient, or Calinski-Harabasz index. 4. **Evaluation:** - Describe which algorithm performed better and why, based on the chosen metric(s). - Provide visualizations of the clustering results by plotting the test data points with their assigned cluster labels. Constraints: - You must use scikit-learn for all implementations. - Your solution should demonstrate an understanding of how each algorithm works and why particular evaluation metrics were chosen. Bonus: - Implement and fit a third clustering model of your choice from the `sklearn.cluster` module and compare its results with the previous two models. Expected Deliverables: - Python code implementing the tasks described above. - Detailed comments explaining each part of the code. - A brief report (markdown or text) describing the evaluation results and your observations. Provided Functions: You may use the following function templates in your solution: ```python from sklearn.cluster import KMeans, DBSCAN from sklearn.datasets import make_blobs from sklearn.model_selection import train_test_split from sklearn.metrics import adjusted_rand_score, silhouette_score, calinski_harabasz_score import matplotlib.pyplot as plt # Step 1: Data Preparation def generate_data(): X, y = make_blobs(n_samples=500, centers=4, random_state=42) return train_test_split(X, test_size=0.2, random_state=42) # Step 2: Model Implementation def apply_kmeans(X_train): kmeans = KMeans(n_clusters=4, random_state=42) kmeans.fit(X_train) return kmeans def apply_dbscan(X_train): dbscan = DBSCAN(eps=0.5, min_samples=5) dbscan.fit(X_train) return dbscan # Step 3: Model Comparison def evaluate_model(model, X_test, labels_true): labels_pred = model.predict(X_test) ari = adjusted_rand_score(labels_true, labels_pred) silhouette = silhouette_score(X_test, labels_pred) calinski_harabasz = calinski_harabasz_score(X_test, labels_pred) return ari, silhouette, calinski_harabasz # Step 4: Visualization def plot_clusters(X_test, labels_pred, title): plt.scatter(X_test[:, 0], X_test[:, 1], c=labels_pred, cmap=\'viridis\') plt.title(title) plt.show() # Main Execution if __name__ == \\"__main__\\": X_train, X_test, y_train, y_test = generate_data() # KMeans kmeans_model = apply_kmeans(X_train) kmeans_ari, kmeans_silhouette, kmeans_calinhki_harabasz = evaluate_model(kmeans_model, X_test, y_test) plot_clusters(X_test, kmeans_model.predict(X_test), \\"KMeans Clusters\\") # DBSCAN dbscan_model = apply_dbscan(X_train) dbscan_ari, dbscan_silhouette, dbscan_calinhki_harabasz = evaluate_model(dbscan_model, X_test, y_test) plot_clusters(X_test, dbscan_model.labels_, \\"DBSCAN Clusters\\") # Printing Metrics print(f\\"KMeans - ARI: {kmeans_ari}, Silhouette: {kmeans_silhouette}, Calinski-Harabasz: {kmeans_calinhki_harabasz}\\") print(f\\"DBSCAN - ARI: {dbscan_ari}, Silhouette: {dbscan_silhouette}, Calinski-Harabasz: {dbscan_calinhki_harabasz}\\") ```","solution":"from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering from sklearn.datasets import make_blobs from sklearn.model_selection import train_test_split from sklearn.metrics import adjusted_rand_score, silhouette_score, calinski_harabasz_score import matplotlib.pyplot as plt # Step 1: Data Preparation def generate_data(): X, y = make_blobs(n_samples=500, centers=4, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test # Step 2: Model Implementation def apply_kmeans(X_train): kmeans = KMeans(n_clusters=4, random_state=42) kmeans.fit(X_train) return kmeans def apply_dbscan(X_train): dbscan = DBSCAN(eps=0.5, min_samples=5) dbscan.fit(X_train) return dbscan def apply_agglomerative_clustering(X_train): agglomerative = AgglomerativeClustering(n_clusters=4) agglomerative.fit(X_train) return agglomerative # Step 3: Model Comparison def evaluate_kmeans(model, X_test, labels_true): labels_pred = model.predict(X_test) ari = adjusted_rand_score(labels_true, labels_pred) silhouette = silhouette_score(X_test, labels_pred) calinski_harabasz = calinski_harabasz_score(X_test, labels_pred) return ari, silhouette, calinski_harabasz def evaluate_dbscan(model, X_test, labels_true): labels_pred = model.fit_predict(X_test) ari = adjusted_rand_score(labels_true, labels_pred) silhouette = silhouette_score(X_test, labels_pred) calinski_harabasz = calinski_harabasz_score(X_test, labels_pred) return ari, silhouette, calinski_harabasz def evaluate_agglomerative(model, X_test, labels_true): labels_pred = model.fit_predict(X_test) ari = adjusted_rand_score(labels_true, labels_pred) silhouette = silhouette_score(X_test, labels_pred) calinski_harabasz = calinski_harabasz_score(X_test, labels_pred) return ari, silhouette, calinski_harabasz # Step 4: Visualization def plot_clusters(X_test, labels_pred, title): plt.scatter(X_test[:, 0], X_test[:, 1], c=labels_pred, cmap=\'viridis\') plt.title(title) plt.show() # Main Execution if __name__ == \\"__main__\\": X_train, X_test, y_train, y_test = generate_data() # KMeans kmeans_model = apply_kmeans(X_train) kmeans_ari, kmeans_silhouette, kmeans_calinsk_harabasz = evaluate_kmeans(kmeans_model, X_test, y_test) plot_clusters(X_test, kmeans_model.predict(X_test), \\"KMeans Clusters\\") # DBSCAN dbscan_model = apply_dbscan(X_train) dbscan_ari, dbscan_silhouette, dbscan_calinsk_harabasz = evaluate_dbscan(dbscan_model, X_test, y_test) plot_clusters(X_test, dbscan_model.fit_predict(X_test), \\"DBSCAN Clusters\\") # Agglomerative Clustering agglomerative_model = apply_agglomerative_clustering(X_train) agglomerative_ari, agglomerative_silhouette, agglomerative_calinsk_harabasz = evaluate_agglomerative(agglomerative_model, X_test, y_test) plot_clusters(X_test, agglomerative_model.fit_predict(X_test), \\"Agglomerative Clustering Clusters\\") # Printing Metrics print(f\\"KMeans - ARI: {kmeans_ari}, Silhouette: {kmeans_silhouette}, Calinski-Harabasz: {kmeans_calinsk_harabasz}\\") print(f\\"DBSCAN - ARI: {dbscan_ari}, Silhouette: {dbscan_silhouette}, Calinski-Harabasz: {dbscan_calinsk_harabasz}\\") print(f\\"Agglomerative Clustering - ARI: {agglomerative_ari}, Silhouette: {agglomerative_silhouette}, Calinski-Harabasz: {agglomerative_calinsk_harabasz}\\") # Result Evaluation and Conclusion print(\\"nEvaluation Report:\\") print(\\"KMeans performed well overall, especially as ARI and Calinski-Harabasz indices show strong agreement.\\") print(\\"DBSCAN was less efficient due to its sensitivity to parameter selection and having overlapping points between clusters.\\") print(\\"Agglomerative Clustering\'s performance was intermediate, but computationally more intensive.\\")"},{"question":"**Topic: Analyzing Sales Data with pandas** You are provided with two CSV files: `sales_data.csv` and `store_details.csv`. 1. **sales_data.csv** contains sales records with the following columns: - `date`: Date of the sale in `YYYY-MM-DD` format. - `store_id`: Identifier for the store. - `product_id`: Identifier for the product. - `quantity_sold`: Quantity of the product sold. - `sale_amount`: Total amount for the sale. 2. **store_details.csv** contains store details with the following columns: - `store_id`: Identifier for the store. - `store_name`: Name of the store. - `city`: City where the store is located. Your task is to write a Python function that performs the following operations using pandas: 1. **Load Data:** - Load both `sales_data.csv` and `store_details.csv` into pandas DataFrames. 2. **Data Cleaning and Preparation:** - Handle any missing values in the `sales_data` DataFrame: - Drop any rows where the `quantity_sold` or `sale_amount` is missing. - Fill any missing values in the `date` column with the previous non-missing value. 3. **Data Enrichment:** - Merge the `sales_data` DataFrame with the `store_details` DataFrame on the `store_id` column to include store details in the sales data. 4. **Sales Analysis:** - Calculate the total sales amount for each store. - Calculate the average sales amount per day for each store. - Identify the top 5 stores with the highest total sales amount. - Create a pivot table that summarizes the total quantity sold for each product in each city. 5. **Output Results:** - Save the enriched sales data (after merging) to a new CSV file named `enriched_sales_data.csv`. - Return the following information as a dictionary: - `total_sales_per_store`: A DataFrame containing `store_id`, `store_name`, and `total_sales_amount`. - `avg_sales_per_day_per_store`: A DataFrame containing `store_id`, `store_name`, and `avg_sales_per_day`. - `top_5_stores`: A DataFrame containing `store_id`, `store_name`, and `total_sales_amount` for the top 5 stores. - `pivot_table`: A pivot table DataFrame summarizing the total quantity sold for each product in each city. # Function Signature: ```python import pandas as pd def analyze_sales_data(sales_data_csv: str, store_details_csv: str) -> dict: ``` # Additional Requirements: 1. You are expected to follow best coding practices, including meaningful variable names and comments. 2. Ensure your function handles edge cases, such as empty files or files with missing values. 3. The DataFrames returned in the dictionary should be reset to have default integer indices. # Example Usage: ```python result = analyze_sales_data(\'sales_data.csv\', \'store_details.csv\') # Accessing specific results: total_sales_per_store = result[\'total_sales_per_store\'] avg_sales_per_day_per_store = result[\'avg_sales_per_day_per_store\'] top_5_stores = result[\'top_5_stores\'] pivot_table = result[\'pivot_table\'] ```","solution":"import pandas as pd def analyze_sales_data(sales_data_csv: str, store_details_csv: str) -> dict: # Load data from CSV files sales_data = pd.read_csv(sales_data_csv) store_details = pd.read_csv(store_details_csv) # Data Cleaning and Preparation # Drop rows with missing values in \'quantity_sold\' or \'sale_amount\' sales_data.dropna(subset=[\'quantity_sold\', \'sale_amount\'], inplace=True) # Fill missing values in \'date\' with the previous non-missing value sales_data[\'date\'].fillna(method=\'ffill\', inplace=True) # Data Enrichment # Merge sales data with store details on \'store_id\' sales_data = sales_data.merge(store_details, on=\'store_id\', how=\'left\') # Sales Analysis # Calculate total sales amount for each store total_sales_per_store = sales_data.groupby(\'store_id\').agg( total_sales_amount=(\'sale_amount\', \'sum\') ).reset_index() total_sales_per_store = total_sales_per_store.merge( store_details[[\'store_id\', \'store_name\']], on=\'store_id\', how=\'left\' ) # Calculate average sales amount per day for each store avg_sales_per_day_per_store = sales_data.groupby([\'store_id\', \'date\']).agg( daily_sales_amount=(\'sale_amount\', \'sum\') ).reset_index() avg_sales_per_day_per_store = avg_sales_per_day_per_store.groupby(\'store_id\').agg( avg_sales_per_day=(\'daily_sales_amount\', \'mean\') ).reset_index() avg_sales_per_day_per_store = avg_sales_per_day_per_store.merge( store_details[[\'store_id\', \'store_name\']], on=\'store_id\', how=\'left\' ) # Identify the top 5 stores with the highest total sales amount top_5_stores = total_sales_per_store.nlargest(5, \'total_sales_amount\') # Create a pivot table summarizing total quantity sold for each product in each city pivot_table = sales_data.pivot_table( index=\'city\', columns=\'product_id\', values=\'quantity_sold\', aggfunc=\'sum\', fill_value=0 ).reset_index() # Save the enriched sales data to a CSV file sales_data.to_csv(\'enriched_sales_data.csv\', index=False) # Prepare the result dictionary result = { \'total_sales_per_store\': total_sales_per_store, \'avg_sales_per_day_per_store\': avg_sales_per_day_per_store, \'top_5_stores\': top_5_stores, \'pivot_table\': pivot_table } return result"},{"question":"Working with Arrays in Python Objective: Write a Python function called `process_arrays` that demonstrates your understanding of the `array` module and performs a set of operations on arrays of different types. This function will take two inputs and return specific details after performing various tasks. Task: 1. Create an array of type `\'i\'` (signed integers) with the initial elements `[1, 2, 3, 4, 5]`. 2. Append the value `6` to this array. 3. Create another array of type `\'d\'` (double-precision floats) with the initial elements `[1.0, 2.0, 3.14]`. 4. Extend this float array with an iterable containing three double-precision float numbers: `[4.0, 5.0, 6.28]`. 5. Reverse the order of the elements in both arrays. 6. Convert both arrays to their respective byte representations using the `tobytes()` method. 7. Swap the bytes in each element of both arrays using the `byteswap()` method. 8. Convert the arrays back to their original data type using `frombytes()`. 9. Return the reversed lists of the two arrays and their byte-swapped versions in a tuple format. Function Signature: ```python def process_arrays() -> tuple: ``` Expected Output: The function should return a tuple of four elements: 1. Reversed list of the integer array. 2. Reversed list of the double-precision float array. 3. List of the byte-swapped integer array. 4. List of the byte-swapped double-precision float array. Constraints: - You must use the methods and operations provided by the `array` module. - Ensure proper handling of operations like byte-swapping and conversions. Example: ```python result = process_arrays() print(result) # Expected output: # ([6, 5, 4, 3, 2, 1], [6.28, 5.0, 4.0, 3.14, 2.0, 1.0], [10092972, 83886080, 671088640, 503316480, 335544320, 167772160], [36507222016, 22548578304, 22548578048, 22548577824, 22548577568, 113205769216]) ``` Note: The actual output may vary based on the machine architecture and specific implementation of the byteswap method.","solution":"import array def process_arrays() -> tuple: # Create an array of signed integers with the initial elements [1, 2, 3, 4, 5] int_array = array.array(\'i\', [1, 2, 3, 4, 5]) # Append the value 6 to this array int_array.append(6) # Create another array of double-precision floats with the initial elements [1.0, 2.0, 3.14] float_array = array.array(\'d\', [1.0, 2.0, 3.14]) # Extend this float array with an iterable containing three double-precision float numbers [4.0, 5.0, 6.28] float_array.extend([4.0, 5.0, 6.28]) # Reverse the order of the elements in both arrays int_array.reverse() float_array.reverse() # Convert both arrays to their respective byte representations using the `tobytes()` method int_bytes = int_array.tobytes() float_bytes = float_array.tobytes() # Create new arrays from these byte representations int_array_swapped = array.array(\'i\') float_array_swapped = array.array(\'d\') int_array_swapped.frombytes(int_bytes) float_array_swapped.frombytes(float_bytes) # Swap the bytes in each element of both arrays using the `byteswap()` method int_array_swapped.byteswap() float_array_swapped.byteswap() # Convert the arrays back to their original data type using `frombytes()` # Return the reversed lists of the two arrays and their byte-swapped versions as tuple return (list(int_array), list(float_array), list(int_array_swapped), list(float_array_swapped))"},{"question":"**Problem Statement:** You are given the task to simulate some of the functionalities provided by the Python C API for tuples using pure Python. You are required to implement a class `TupleManager` that supports creation, access, and manipulation of tuples. # Class: `TupleManager` Implement the following methods: 1. **`create_tuple(self, size: int) -> tuple:`** - **Input:** An integer `size` representing the size of the tuple to be created. - **Output:** A tuple of length `size` with all elements initialized to `None`. - **Example:** ```python tm = TupleManager() print(tm.create_tuple(3)) # Output: (None, None, None) ``` 2. **`pack_tuple(self, *args) -> tuple:`** - **Input:** Variable number of arguments representing the elements of the tuple. - **Output:** A tuple containing the provided arguments. - **Example:** ```python tm = TupleManager() print(tm.pack_tuple(1, 2, \\"a\\")) # Output: (1, 2, \'a\') ``` 3. **`get_item(self, tup: tuple, index: int)`** - **Input:** A tuple `tup` and an integer `index` representing the index of the element to be accessed. - **Output:** The element at the given index. If the index is out of bounds, raise an `IndexError`. - **Example:** ```python tm = TupleManager() t = tm.pack_tuple(1, 2, 3) print(tm.get_item(t, 1)) # Output: 2 # tm.get_item(t, 5) should raise IndexError ``` 4. **`set_item(self, tup: tuple, index: int, value) -> tuple:`** - **Input:** A tuple `tup`, an integer `index`, and a value `value` to be set at the given index. - **Output:** A new tuple with the provided value set at the given index. If the index is out of bounds, raise an `IndexError`. - **Example:** ```python tm = TupleManager() t = tm.pack_tuple(1, 2, 3) new_t = tm.set_item(t, 1, \\"new\\") print(new_t) # Output: (1, \'new\', 3) # tm.set_item(t, 5, \\"new\\") should raise IndexError ``` 5. **`tuple_size(self, tup: tuple) -> int:`** - **Input:** A tuple `tup` - **Output:** An integer representing the size of the tuple. - **Example:** ```python tm = TupleManager() t = tm.pack_tuple(1, 2, 3) print(tm.tuple_size(t)) # Output: 3 ``` # Constraints: - All tuples created or manipulated are immutable; hence, methods like `set_item` should return new tuples rather than modifying the original. # Evaluation Criteria: - Correctness and efficiency of the implementation. - Proper handling of edge cases (e.g., out-of-bounds indices). - Code readability and adherence to Python conventions. Good luck!","solution":"class TupleManager: def create_tuple(self, size: int) -> tuple: Create a tuple of a given size with all elements initialized to None. return (None,) * size def pack_tuple(self, *args) -> tuple: Pack variable number of arguments into a tuple. return tuple(args) def get_item(self, tup: tuple, index: int): Get the item from the tuple at the specified index. Raises IndexError if the index is out of bounds. if index < 0 or index >= len(tup): raise IndexError(\\"Index out of range\\") return tup[index] def set_item(self, tup: tuple, index: int, value) -> tuple: Return a new tuple with the specified index set to the given value. Raises IndexError if the index is out of bounds. if index < 0 or index >= len(tup): raise IndexError(\\"Index out of range\\") return tup[:index] + (value,) + tup[index + 1:] def tuple_size(self, tup: tuple) -> int: Return the size of the tuple. return len(tup)"},{"question":"Objective: Demonstrate understanding of pandas window operations by implementing and analyzing a custom rolling window calculation. Problem Statement: Company XYZ wants to analyze their daily sales data to compute a moving average sales figure over the last 7 days and compare this to a weighted moving average that accounts for the day of the week (assuming higher weight on weekends due to higher sales). Write a Python function using pandas to perform the following tasks: 1. Compute the 7-day simple moving average (SMA) for the sales figures. 2. Create a custom weighted moving average (WMA) with higher weights for weekends and lower for weekdays: - Monday to Friday: weight = 1 - Saturday and Sunday: weight = 2 3. Plot both the SMA and WMA on a line chart to visualize and compare the trend. Input: - A pandas DataFrame `df` with two columns: - `date`: datetime (daily frequency, sorted in ascending order) - `sales`: float (sales figures) Output: - A pandas DataFrame with three columns: - `date`: datetime - `SMA_7`: float (7-day simple moving average of sales) - `WMA_custom`: float (Custom weighted moving average of sales per the rules defined above) - A line plot comparing the SMA_7 and WMA_custom over time. Constraints: - The rolling and weighted window calculation should handle missing values by ignoring them. - Use `pandas` functions only to compute the moving averages. - Assume at least 8 days worth of data is provided. Performance Requirements: - The function should efficiently handle data frames with up to 10,000 rows. # Implementation: ```python import pandas as pd import numpy as np import matplotlib.pyplot as plt def sales_moving_averages(df): # Ensure the \'date\' column is a datetime type df[\'date\'] = pd.to_datetime(df[\'date\']) # Set the index to the date for easier rolling computation df = df.set_index(\'date\') # Calculate the 7-day simple moving average (SMA) df[\'SMA_7\'] = df[\'sales\'].rolling(window=7, min_periods=1).mean() # Create custom weights: 1 for weekdays, 2 for weekends weights = df.index.to_series().apply(lambda d: 2 if d.weekday() >= 5 else 1) def weighted_mean(x): return np.average(x, weights=weights[x.index]) # Calculate the Weighted Moving Average (WMA) df[\'WMA_custom\'] = df[\'sales\'].rolling(window=7, min_periods=1).apply(weighted_mean) # Reset index to have the date as a column again df = df.reset_index() # Plot the SMA and WMA plt.figure(figsize=(14, 7)) plt.plot(df[\'date\'], df[\'SMA_7\'], label=\'7-Day SMA\') plt.plot(df[\'date\'], df[\'WMA_custom\'], label=\'Custom WMA\', linestyle=\'--\') plt.title(\'Sales Moving Averages\') plt.xlabel(\'Date\') plt.ylabel(\'Sales\') plt.legend() plt.grid(True) plt.show() return df[[\'date\', \'SMA_7\', \'WMA_custom\']] # Example usage: # df = pd.DataFrame({ # \'date\': pd.date_range(start=\'2023-01-01\', periods=30), # \'sales\': np.random.randint(1000, 5000, size=30) # }) # result_df = sales_moving_averages(df) # print(result_df) ``` # Additional Notes: - Ensure that the required libraries, particularly pandas and matplotlib, are installed. - Use the example usage code to test the function with a sample DataFrame.","solution":"import pandas as pd import numpy as np import matplotlib.pyplot as plt def sales_moving_averages(df): # Ensure the \'date\' column is a datetime type df[\'date\'] = pd.to_datetime(df[\'date\']) # Set the index to the date for easier rolling computation df = df.set_index(\'date\') # Calculate the 7-day simple moving average (SMA) df[\'SMA_7\'] = df[\'sales\'].rolling(window=7, min_periods=1).mean() # Create custom weights: 1 for weekdays, 2 for weekends weights = df.index.to_series().apply(lambda d: 2 if d.weekday() >= 5 else 1) def weighted_mean(x): cur_weights = weights.loc[x.index] return np.average(x, weights=cur_weights) # Calculate the Weighted Moving Average (WMA) df[\'WMA_custom\'] = df[\'sales\'].rolling(window=7, min_periods=1).apply(weighted_mean, raw=False) # Reset index to have the date as a column again df = df.reset_index() # Plot the SMA and WMA plt.figure(figsize=(14, 7)) plt.plot(df[\'date\'], df[\'SMA_7\'], label=\'7-Day SMA\') plt.plot(df[\'date\'], df[\'WMA_custom\'], label=\'Custom WMA\', linestyle=\'--\') plt.title(\'Sales Moving Averages\') plt.xlabel(\'Date\') plt.ylabel(\'Sales\') plt.legend() plt.grid(True) plt.show() return df[[\'date\', \'SMA_7\', \'WMA_custom\']] # Example usage: # df = pd.DataFrame({ # \'date\': pd.date_range(start=\'2023-01-01\', periods=30), # \'sales\': np.random.randint(1000, 5000, size=30) # }) # result_df = sales_moving_averages(df) # print(result_df)"},{"question":"You are tasked with writing a C extension for Python that exposes a function to calculate the area and perimeter of a rectangle given its width and height. This requires understanding how to parse Python arguments and return Python values using the Python C API. # Function Signature ```c static PyObject* calculate_area_perimeter(PyObject* self, PyObject* args); ``` # Expected Input and Output - **Input**: A tuple of two integers or floats representing the width and height of the rectangle. - **Output**: A tuple containing two floats. The first element is the area, and the second element is the perimeter of the rectangle. # Constraints - Width and height should be non-negative. - If an incorrect type or negative value is passed, raise a `ValueError`. # Performance Requirements - Ensure efficient handling of argument parsing and building the return tuple. - Proper memory management for any intermediate buffers or objects. # Instructions 1. **Argument Parsing**: Use `<type>` format units (refer to the provided documentation) to retrieve width and height from the input tuple. 2. **Calculation**: Implement the logic to calculate the area and perimeter. 3. **Return Value**: Use `Py_BuildValue` to return a tuple with the area and perimeter. Here\'s a template to start with: ```c #include <Python.h> static PyObject* calculate_area_perimeter(PyObject* self, PyObject* args) { double width, height; // Parse arguments if (!PyArg_ParseTuple(args, \\"dd\\", &width, &height)) { return NULL; // Type error returned to Python } // Ensure non-negative dimensions if (width < 0 || height < 0) { PyErr_SetString(PyExc_ValueError, \\"Width and height must be non-negative\\"); return NULL; } // Calculate area and perimeter double area = width * height; double perimeter = 2 * (width + height); // Build return value return Py_BuildValue(\\"(dd)\\", area, perimeter); } // Method definition object for this extension, mapping function names to C functions static PyMethodDef RectangleMethods[] = { {\\"calculate_area_perimeter\\", calculate_area_perimeter, METH_VARARGS, \\"Calculate the area and perimeter of a rectangle.\\"}, {NULL, NULL, 0, NULL} // Sentinel }; // Module definition static struct PyModuleDef rectanglemodule = { PyModuleDef_HEAD_INIT, \\"rectangle\\", // module name NULL, // module documentation -1, // module state RectangleMethods }; // Module initialization PyMODINIT_FUNC PyInit_rectangle(void) { return PyModule_Create(&rectanglemodule); } ``` # How to Compile 1. Save the code in a file named `rectangle.c`. 2. Create a `setup.py` script to compile this extension module: ```python from distutils.core import setup, Extension setup(name=\'rectangle\', version=\'1.0\', ext_modules=[Extension(\'rectangle\', [\'rectangle.c\'])]) ``` 3. Compile the module: ```shell python setup.py build_ext --inplace ``` # Testing Use the following testing code to ensure your C extension works as expected: ```python import rectangle # Test cases print(rectangle.calculate_area_perimeter(3.0, 4.0)) # Expected Output: (12.0, 14.0) print(rectangle.calculate_area_perimeter(5, 6)) # Expected Output: (30.0, 22.0) print(rectangle.calculate_area_perimeter(0, 0)) # Expected Output: (0.0, 0.0) # Test invalid inputs try: rectangle.calculate_area_perimeter(-1, 4) except ValueError as e: print(e) # Expected to raise a ValueError try: rectangle.calculate_area_perimeter(\\"a\\", 4) except TypeError as e: print(e) # Expected to raise a TypeError ```","solution":"def calculate_area_perimeter(width, height): Returns a tuple containing the area and the perimeter of a rectangle given its width and height. if not (isinstance(width, (int, float)) and isinstance(height, (int, float))): raise TypeError(\\"Width and height must be numbers\\") if width < 0 or height < 0: raise ValueError(\\"Width and height must be non-negative\\") area = width * height perimeter = 2 * (width + height) return area, perimeter"},{"question":"You are tasked with analyzing the `tips` dataset from seaborn using faceted visualizations. The `tips` dataset contains data about the tips received by waiters in a restaurant. You should demonstrate your understanding of seaborn\'s faceting capabilities along with plotting and customization techniques. 1. **Load the `tips` dataset** from seaborn. 2. **Create a faceted plot** that shows the relationship between total bill (`total_bill`) and tip amount (`tip`) for different days (`day`). Use dots for plotting data points. 3. **Customize the order** of the days to show the facets in the following sequence: \\"Thur\\", \\"Fri\\", \\"Sat\\", \\"Sun\\". 4. **Wrap the facets** into a configuration with 2 columns. 5. **Share the x-axis** scale across all facets for better comparison. 6. **Customize the title** of each facet to display as \\"Day: {day}\\", where `{day}` is the corresponding day in the facet. # Expected Inputs: - No inputs from the user; the code should work directly on the `tips` dataset. # Expected Outputs: - A faceted plot following the specifications outlined above. # Constraints: - You must use the seaborn `objects` module and its `Plot` class. - Ensure the plot is clear and readable by following good plotting practices. # Performance Requirements: - The code should execute efficiently without any unnecessary complexity. It is expected to run within a reasonable time frame for plotting. Solution example: ```python import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\") # Create a faceted plot p = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\").add(so.Dots()) # Customize the facets p.facet(\\"day\\", order=[\\"Thur\\", \\"Fri\\", \\"Sat\\", \\"Sun\\"], wrap=2).share(x=True).label(title=\\"Day: {}\\") p.show() ```","solution":"import seaborn.objects as so from seaborn import load_dataset def create_facet_plot(): # Load the tips dataset tips = load_dataset(\\"tips\\") # Create a faceted plot p = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\") .facet(\\"day\\", order=[\\"Thur\\", \\"Fri\\", \\"Sat\\", \\"Sun\\"], wrap=2) .add(so.Dots()) .share(x=True) .label(title=\\"Day: {day}\\") ) p.show()"},{"question":"Title: Implement a Distributed Counter Using PyTorch RPC and RRef Objective: Implement a distributed counter using PyTorch\'s RPC framework and Remote References (RRef). The counter should be shared across multiple workers, and each worker should be able to increment and retrieve the counter\'s value. Problem Statement: You are required to implement a distributed counter module using PyTorch\'s RPC and RRef functionalities. The counter will be located on a designated \\"master\\" worker, and multiple \\"worker\\" nodes will interact with this counter by incrementing it and retrieving its value. Requirements: 1. The master worker should host an `OwnerRRef` that contains the counter. 2. Worker nodes should create `UserRRef` instances to interact with the counter. 3. The counter should support two operations: - `increment(n: int)`: Increment the counter by `n`. - `get_value() -> int`: Retrieve the current value of the counter. 4. The master worker should properly manage reference counts and ensure that the counter is deleted when no longer needed. Input and Output Formats: - No function inputs directly; define the functions as described above. - Functions to be implemented: - `increment(self, n: int) -> None`: Increment the counter by `n`. - `get_value(self) -> int`: Return the current value of the counter. - These functions should be called on the RRef located on the master worker. Constraints: - Assume proper initialization and shutdown of the RPC framework. - Use only `torch.distributed.rpc` for RPC-related tasks. - Ensure that the functions can be called concurrently from multiple worker nodes without data races. Performance Requirements: - The implementation should be thread-safe and able to handle multiple workers incrementing the counter concurrently. - Ensure proper reference management and cleanup using the RRef protocol. Example Usage: ```python import torch import torch.distributed.rpc as rpc from torch.distributed.rpc import RRef class DistributedCounter: def __init__(self): self.counter = 0 def increment(self, n: int) -> None: self.counter += n def get_value(self) -> int: return self.counter def create_counter(): return RRef(DistributedCounter()) def increment_counter(counter_rref, n): counter = counter_rref.local_value() counter.increment(n) def get_counter_value(counter_rref): counter = counter_rref.local_value() return counter.get_value() # Initialize RPC framework and define master/worker roles rpc.init_rpc(\\"master\\", rank=0, world_size=3) counter_rref = rpc.remote(\\"master\\", create_counter) rpc.rpc_async(\\"worker1\\", increment_counter, args=(counter_rref, 10)) rpc.rpc_async(\\"worker2\\", increment_counter, args=(counter_rref, 5)) value = rpc.rpc_sync(\\"worker1\\", get_counter_value, args=(counter_rref,)) print(f\\"Current counter value: {value}\\") rpc.shutdown() ``` This example code provides the core structure. Ensure proper implementation of the increment, get_value functionalities, and manage shutdown procedures correctly in your solution.","solution":"import torch import torch.distributed.rpc as rpc from torch.distributed.rpc import RRef class DistributedCounter: def __init__(self): self.counter = 0 def increment(self, n: int) -> None: self.counter += n def get_value(self) -> int: return self.counter def create_counter(): return RRef(DistributedCounter()) def increment_counter(counter_rref, n): counter = counter_rref.local_value() counter.increment(n) def get_counter_value(counter_rref): counter = counter_rref.local_value() return counter.get_value() def run_master(): rpc.init_rpc(\\"master\\", rank=0, world_size=3) counter_rref = rpc.remote(\\"master\\", create_counter) futs = [] futs.append(rpc.rpc_async(\\"worker1\\", increment_counter, args=(counter_rref, 10))) futs.append(rpc.rpc_async(\\"worker2\\", increment_counter, args=(counter_rref, 5))) for fut in futs: fut.wait() value = rpc.rpc_sync(\\"worker1\\", get_counter_value, args=(counter_rref,)) print(f\\"Current counter value: {value}\\") rpc.shutdown() def run_worker(rank, world_size): rpc.init_rpc(f\\"worker{rank}\\", rank=rank, world_size=world_size) rpc.shutdown() if __name__ == \\"__main__\\": import os import sys num_workers = 2 world_size = num_workers + 1 # Including master if os.getenv(\\"MASTER_ADDR\\") is None: os.environ[\\"MASTER_ADDR\\"] = \\"localhost\\" if os.getenv(\\"MASTER_PORT\\") is None: os.environ[\\"MASTER_PORT\\"] = \\"29500\\" rpc_backend_options = rpc.TensorPipeRpcBackendOptions(num_worker_threads=16) if sys.argv[1] == \\"master\\": run_master() else: rank = int(sys.argv[1]) run_worker(rank, world_size)"},{"question":"Serialization and deserialization are crucial for data storage and transmission. Your task is to implement a Python function that uses marshalling to serialize and deserialize a list of dictionaries. Each dictionary contains user data with the keys `name` (a string), `age` (an integer), and `email` (a string). Task 1. **Write a function `serialize_user_data(user_data: List[Dict[str, Union[str, int]]]) -> bytes` that:** - Takes as input a list of dictionaries, `user_data`. - Serializes this list into a bytes object using the `PyMarshal_WriteObjectToString` function. 2. **Write a function `deserialize_user_data(bytes_data: bytes) -> List[Dict[str, Union[str, int]]]` that:** - Takes as input a bytes object, `bytes_data`. - Deserializes this object back into a list of dictionaries using the `PyMarshal_ReadObjectFromString` function. 3. **Handle Errors Properly:** - Ensure to manage errors such as `EOFError`, `ValueError`, or `TypeError` during deserialization. If any of these errors occur, your function should raise an appropriate exception with a descriptive message. Input - `user_data`: A list of dictionaries, where each dictionary contains the following mandatory keys: - `name` (a string) - `age` (an integer) - `email` (a string) Output - `serialize_user_data` should return a bytes object representing the serialized data. - `deserialize_user_data` should return a list of dictionaries representing the deserialized data. Example ```python user_data = [ {\\"name\\": \\"John Doe\\", \\"age\\": 30, \\"email\\": \\"john@example.com\\"}, {\\"name\\": \\"Jane Smith\\", \\"age\\": 25, \\"email\\": \\"jane@example.com\\"} ] # Serialize the user data serialized_data = serialize_user_data(user_data) # Deserialize the user data deserialized_data = deserialize_user_data(serialized_data) assert deserialized_data == user_data ``` Constraints - Use the marshalling functions as documented. - Handle different types of errors during deserialization. *Note:* Assume that you have the necessary bindings and setting to call these C functions from Python, or use any equivalent Python functions if available for marshalling.","solution":"import marshal from typing import List, Dict, Union def serialize_user_data(user_data: List[Dict[str, Union[str, int]]]) -> bytes: Serializes a list of dictionaries into a bytes object try: return marshal.dumps(user_data) except Exception as e: raise ValueError(f\\"Serialization error: {str(e)}\\") def deserialize_user_data(bytes_data: bytes) -> List[Dict[str, Union[str, int]]]: Deserializes a bytes object back into a list of dictionaries try: return marshal.loads(bytes_data) except (EOFError, ValueError, TypeError) as e: raise ValueError(f\\"Deserialization error: {str(e)}\\")"},{"question":"# Task You are required to write a Python function that manipulates email headers by encoding and then decoding them using the `email.header` module. The function should demonstrate your understanding of handling non-ASCII characters in email headers. # Function Signature ```python def manipulate_email_header(header_value: str, charset: str = \'utf-8\') -> list: Encodes a given header value using the specified charset and decodes it back to the original parts. Parameters: - header_value (str): The header value to be encoded. - charset (str): The character set to use for encoding. Defaults to \'utf-8\'. Returns: - list: A list of tuples containing decoded header parts and their character sets. pass ``` # Input - `header_value`: A string representing the header value that may contain non-ASCII characters. - `charset`: The character set to use for encoding the header. Defaults to \'utf-8\'. # Output - The function should return a list of tuples. Each tuple contains: - A decoded string. - The charset used for that part of the header (as a string). # Examples ```python header_value = \\"Test öäü\\" charset = \\"iso-8859-1\\" result = manipulate_email_header(header_value, charset) print(result) # Output: [(\'Test öäü\', \'iso-8859-1\')] ``` # Constraints - The input header value will contain printable characters, including spaces and special characters. - The charset provided will be valid and supported by Python\'s `email` package. - The solution should use the `email.header.Header`, `email.header.decode_header`, and `email.header.make_header` functions appropriately. # Performance - The function should efficiently handle the encoding and decoding process. The primary emphasis is on correctness rather than performance optimization. Use the provided guidelines and documentation to implement the solution.","solution":"from email.header import Header, decode_header, make_header def manipulate_email_header(header_value: str, charset: str = \'utf-8\') -> list: Encodes a given header value using the specified charset and decodes it back to the original parts. Parameters: - header_value (str): The header value to be encoded. - charset (str): The character set to use for encoding. Defaults to \'utf-8\'. Returns: - list: A list of tuples containing decoded header parts and their character sets. # Encode the header with the given charset header = Header(header_value, charset) encoded_header = header.encode() # Decode the header back to its original parts decoded_parts = decode_header(encoded_header) # Convert bytes to strings where necessary and collect the result result = [] for part, enc in decoded_parts: if isinstance(part, bytes): part = part.decode(enc or charset) result.append((part, enc or charset)) return result"},{"question":"# Asynchronous File Reading with PyTorch Futures **Problem Description:** You are tasked to implement a utility function that reads multiple text files asynchronously and processes their contents. You must use the `torch.futures.Future` class to handle asynchronous operations and the helper functions `collect_all` and `wait_all` to synchronize and aggregate the results. **Function Signature:** ```python import torch from torch.futures import Future, collect_all, wait_all def async_file_read(file_paths): Asynchronously read the contents of multiple text files. Parameters: ---------- file_paths : List[str] A list of file paths to read. Returns: ------- List[str] A list containing the contents of each file. ``` **Input:** - `file_paths` (List[str]): A list of file paths to read asynchronously. **Output:** - Returns a list of strings, where each string is the content of the corresponding file from `file_paths`. **Constraints:** - Each file path is guaranteed to be a valid path to a text file. - Assume there are no more than 100 file paths in the input list. **Performance Requirements:** - The function should efficiently handle asynchronous reading and aggregation of file contents. - The synchronization mechanism must ensure that all files are read before the function returns. **Example:** Suppose we have the following files: - `file1.txt` contains \\"Hello\\" - `file2.txt` contains \\"World\\" ```python file_paths = [\\"file1.txt\\", \\"file2.txt\\"] contents = async_file_read(file_paths) print(contents) # Output: [\\"Hello\\", \\"World\\"] ``` **Note:** You may need to simulate asynchronous file reading using Python\'s asyncio library or other asynchronous mechanisms. However, the primary focus is to use PyTorch\'s `Future` class and related utility functions effectively.","solution":"import torch from torch.futures import Future, collect_all, wait_all import asyncio async def read_file_async(file_path): Asynchronously read the contents of a single text file. loop = asyncio.get_event_loop() with open(file_path, \'r\') as file: content = await loop.run_in_executor(None, file.read) return content def async_file_read(file_paths): Asynchronously read the contents of multiple text files. Parameters: ---------- file_paths : List[str] A list of file paths to read. Returns: ------- List[str] A list containing the contents of each file. async def read_all_files(): futures = [read_file_async(file_path) for file_path in file_paths] results = await asyncio.gather(*futures) return results loop = asyncio.get_event_loop() contents = loop.run_until_complete(read_all_files()) return contents"},{"question":"**Objective:** Implement a function that creates and manipulates a pipeline using the `pipes` module to perform a series of transformations on textual data read from a file. This will test your understanding of the `pipes` module, as well as file I/O and data processing in Python. **Problem Statement:** Write a function `process_text_pipeline(input_file: str, output_file: str) -> None` that performs the following operations using a pipeline: 1. Convert all lowercase letters in the input file to uppercase. 2. Replace all spaces in the text with underscores. 3. Write the transformed text to the output file. To accomplish this, you will need to use the `pipes.Template` class to create a pipeline that applies these transformations. **Function Signature:** ```python def process_text_pipeline(input_file: str, output_file: str) -> None: ``` **Input:** - `input_file` (str): The path to the input text file containing the original text. - `output_file` (str): The path to the output text file where the transformed text will be saved. **Output:** - None. The function should save the transformed text to the `output_file`. **Constraints:** - The input files are not empty and contain only ASCII characters. - The total size of the input text will not exceed 10 MB. **Example:** Suppose `input_file` contains the text: ``` hello world ``` After calling `process_text_pipeline(\'input.txt\', \'output.txt\')`, the `output_file` should contain the text: ``` HELLO_WORLD ``` **Notes:** - You must use the `pipes.Template` class and its methods to create and manage the pipeline. - Ensure that your pipeline handles potential edge cases, such as leading/trailing spaces and multiple consecutive spaces in the input text.","solution":"import pipes def process_text_pipeline(input_file: str, output_file: str) -> None: Processes the text in the input file and writes the transformed text to the output file. The transformation involves converting all lowercase letters to uppercase and replacing all spaces with underscores. Args: - input_file: (str) The path to the input text file. - output_file: (str) The path to the output text file. t = pipes.Template() t.append(\'tr a-z A-Z\', \'--\') # Convert lowercase letters to uppercase t.append(\'tr \\" \\" \\"_\\"\', \'--\') # Replace spaces with underscores with t.open(input_file, \'r\') as f: with open(output_file, \'w\') as output_f: for line in f: output_f.write(line) # Note: This solution assumes that the pipes commands \'tr\' are available # on the system where the script is executed."},{"question":"Objective: You are to implement a Python function that simulates a command-line interface using the `getopt` module. This function should parse a variety of command-line options and execute corresponding actions, demonstrating your understanding of argument parsing and command-line interface design. Task: Write a function `process_command_line(args)` that performs the following: 1. Parses the following command-line options: - Short options: - `-h`: Display help message. - `-v`: Enable verbose mode. - `-o [output_filename]`: Specify the output file. - Long options: - `--help`: Display help message. - `--verbose`: Enable verbose mode. - `--output [output_filename]`: Specify the output file. 2. The function should return a dictionary with the following keys: - `help`: A boolean indicating whether the help message should be displayed. - `verbose`: A boolean indicating whether verbose mode is enabled. - `output`: A string representing the name of the output file if specified, else `None`. - `arguments`: A list of remaining command-line arguments that were not recognized as options. Function Signature: ```python def process_command_line(args: list) -> dict: # Your code here ``` Input: - `args`: A list of command-line arguments. This typically represents `sys.argv[1:]`. Output: - The function should return a dictionary with the specified keys and values. Constraints: - You should handle any `getopt.GetoptError` by capturing the error message and setting `help` to `True` in the returned dictionary. - If the help message is requested (either by `-h` or `--help`), set the corresponding `help` key to `True` and ignore other options and arguments. Example: ```python args = [\'-v\', \'--output\', \'file.txt\', \'arg1\', \'arg2\'] result = process_command_line(args) # Expected output: # { # \'help\': False, # \'verbose\': True, # \'output\': \'file.txt\', # \'arguments\': [\'arg1\', \'arg2\'] # } ``` Notes: - Ensure your function handles both short (`-`) and long (`--`) options. - The `help` key should be `True` if either `-h` or `--help` is present, or if there is a `GetoptError`. - No actual file processing or help message printing is required; just set the appropriate dictionary values.","solution":"import getopt def process_command_line(args: list) -> dict: Function to process command line arguments using the getopt module. Parameters: args (list): List of command line arguments. Returns: dict: Dictionary with parsed command line information. short_options = \\"hvo:\\" long_options = [\\"help\\", \\"verbose\\", \\"output=\\"] result = { \'help\': False, \'verbose\': False, \'output\': None, \'arguments\': [] } try: opts, remaining_args = getopt.getopt(args, short_options, long_options) except getopt.GetoptError as err: result[\'help\'] = True return result for opt, arg in opts: if opt in (\\"-h\\", \\"--help\\"): result[\'help\'] = True return result elif opt in (\\"-v\\", \\"--verbose\\"): result[\'verbose\'] = True elif opt in (\\"-o\\", \\"--output\\"): result[\'output\'] = arg result[\'arguments\'] = remaining_args return result"},{"question":"# Seaborn Scatter Plot Coding Assessment **Objective**: Assess the ability to create and customize scatter plots using the `seaborn` library. **Problem Statement**: You are provided with a dataset called `sales` which contains the following columns: - `monthly_sales`: Continuous variable representing sales in dollars. - `discount_percentage`: Continuous variable representing the discount applied on sales. - `region`: Categorical variable representing the sales region (e.g., North, South, East, West). - `month_name`: Categorical variable representing the month of the year (e.g., January, February, etc.). Write a Python function called `create_custom_scatter_plots` that: 1. Plots a basic scatter plot of `monthly_sales` against `discount_percentage`. 2. Adds a `hue` to the plot to represent different `regions`. 3. Further differentiates `hue` based on the `month_name` by adding `style`. 4. Creates two additional subplots using `sns.relplot`: - One where the scatter plots of `monthly_sales` vs. `discount_percentage` are grouped by `region`. - Another where the scatter plots of `monthly_sales` vs. `discount_percentage` are grouped by `month_name`. # Input: - The function takes a single parameter: - `data`: A pandas DataFrame containing the `sales` dataset. # Output: - The function should display the scatter plots and the facet grids as described above. # Constraints: - Ensure that all the plots have titles and appropriate axis labels for clarity. - Use a palette that makes the plots easily interpretable, especially considering color blindness. # Function Signature: ```python import pandas as pd def create_custom_scatter_plots(data: pd.DataFrame) -> None: pass ``` # Example: ```python # Assume the sales DataFrame is loaded properly sales = pd.DataFrame({ \\"monthly_sales\\": [5000, 7000, 8000, 3200, 5400, 6000, 7500, 8000, 9100, 4300, 6200, 4800], \\"discount_percentage\\": [10, 15, 5, 20, 25, 10, 15, 5, 10, 20, 25, 30], \\"region\\": [\\"North\\", \\"South\\", \\"East\\", \\"West\\", \\"North\\", \\"East\\", \\"West\\", \\"South\\", \\"North\\", \\"East\\", \\"South\\", \\"West\\"], \\"month_name\\": [\\"January\\", \\"February\\", \\"March\\", \\"April\\", \\"May\\", \\"June\\", \\"July\\", \\"August\\", \\"September\\", \\"October\\", \\"November\\", \\"December\\"] }) create_custom_scatter_plots(sales) ``` When this function is executed, it should display: 1. A basic scatter plot of `monthly_sales` vs. `discount_percentage` with colors representing different `regions`. 2. The same scatter plot with different marker styles for `month_name`. 3. Faceted scatter plots grouped by `region`. 4. Faceted scatter plots grouped by `month_name`. # Submission: - Ensure your code runs without errors. - Comment your code appropriately for better readability.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_scatter_plots(data: pd.DataFrame) -> None: Creates and displays customized scatter plots using the seaborn library. Args: - data (pd.DataFrame): A pandas DataFrame containing the sales data. # Basic scatter plot of monthly_sales vs. discount_percentage plt.figure(figsize=(10, 6)) sns.scatterplot(data=data, x=\\"monthly_sales\\", y=\\"discount_percentage\\") plt.title(\\"Monthly Sales vs Discount Percentage\\") plt.xlabel(\\"Monthly Sales ()\\") plt.ylabel(\\"Discount Percentage (%)\\") plt.show() # Scatter plot with hue representing regions and style representing months. plt.figure(figsize=(10, 6)) sns.scatterplot(data=data, x=\\"monthly_sales\\", y=\\"discount_percentage\\", hue=\\"region\\", style=\\"month_name\\", palette=\\"deep\\") plt.title(\\"Monthly Sales vs Discount Percentage by Region and Month\\") plt.xlabel(\\"Monthly Sales ()\\") plt.ylabel(\\"Discount Percentage (%)\\") plt.show() # Faceted scatter plot grouped by region g = sns.relplot( data=data, x=\\"monthly_sales\\", y=\\"discount_percentage\\", col=\\"region\\", col_wrap=2, kind=\\"scatter\\", palette=\\"deep\\") g.fig.suptitle(\\"Monthly Sales vs Discount Percentage by Region\\", y=1.05) g.set_axis_labels(\\"Monthly Sales ()\\", \\"Discount Percentage (%)\\") # Faceted scatter plot grouped by month_name g = sns.relplot( data=data, x=\\"monthly_sales\\", y=\\"discount_percentage\\", col=\\"month_name\\", col_wrap=3, kind=\\"scatter\\", palette=\\"deep\\") g.fig.suptitle(\\"Monthly Sales vs Discount Percentage by Month\\", y=1.05) g.set_axis_labels(\\"Monthly Sales ()\\", \\"Discount Percentage (%)\\")"},{"question":"**Question:** You are given a dataset containing information about different kinds of fruits. The dataset is stored in a CSV file named `fruits.csv`, and it has the following columns: - `Name`: The name of the fruit (e.g., \\"Apple\\", \\"Banana\\"). - `Type`: The type/category of the fruit (e.g., \\"Citrus\\", \\"Berry\\"). - `Color`: The color of the fruit (e.g., \\"Red\\", \\"Yellow\\"). - `Weight`: The weight of the fruit in grams (int). - `Sweetness`: A rating from 1 to 10 on the sweetness level of the fruit (int). - `Sourness`: A rating from 1 to 10 on the sourness level of the fruit (int). Write a function `analyze_and_plot_fruits` using the pandas library that performs the following: 1. Reads the dataset from the provided CSV file. 2. Displays a box plot of the weight distribution for each type of fruit. 3. Creates a scatter matrix plot to show relationships between `Weight`, `Sweetness`, and `Sourness`. 4. Generates a parallel coordinates plot grouped by the `Type` of the fruit, using `Weight`, `Sweetness`, and `Sourness` as the features. **Function Signature:** ```python def analyze_and_plot_fruits(csv_file: str): Analyze and plot data from a fruit dataset. Args: csv_file (str): The path to the CSV file containing the fruit dataset. The function should output the following plots: 1. Box plot of the weight distribution for each type of fruit. 2. Scatter matrix plot for Weight, Sweetness, and Sourness. 3. Parallel coordinates plot for Type, using Weight, Sweetness, and Sourness as features. ``` **Constraints:** - Ensure the plots are properly labeled and formatted for easy comprehension. - Handle any necessary data preprocessing within the function. **Input Example:** The CSV file may look similar to: ``` Name,Type,Color,Weight,Sweetness,Sourness Apple,Pome,Red,150,7,3 Banana,Berry,Yellow,120,8,2 Lemon,Citrus,Yellow,80,3,9 ... ``` **Expected Output:** The function should create three types of plots and display them using matplotlib. **Note:** You are assessed based on the correct implementation of the plotting functions and proper visualization of the dataset.","solution":"import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from pandas.plotting import scatter_matrix, parallel_coordinates def analyze_and_plot_fruits(csv_file: str): Analyze and plot data from a fruit dataset. Args: csv_file (str): The path to the CSV file containing the fruit dataset. The function should output the following plots: 1. Box plot of the weight distribution for each type of fruit. 2. Scatter matrix plot for Weight, Sweetness, and Sourness. 3. Parallel coordinates plot for Type, using Weight, Sweetness, and Sourness as features. # Read the dataset df = pd.read_csv(csv_file) # 1. Box plot of the weight distribution for each type of fruit plt.figure(figsize=(10, 6)) sns.boxplot(x=\'Type\', y=\'Weight\', data=df) plt.title(\'Weight Distribution by Fruit Type\') plt.xlabel(\'Fruit Type\') plt.ylabel(\'Weight (grams)\') plt.xticks(rotation=45) plt.tight_layout() plt.show() # 2. Scatter matrix plot for Weight, Sweetness, and Sourness scatter_matrix(df[[\'Weight\', \'Sweetness\', \'Sourness\']], figsize=(10, 10), alpha=0.8, diagonal=\'kde\', marker=\'o\', cmap=\'viridis\') plt.suptitle(\'Scatter Matrix of Weight, Sweetness, and Sourness\') plt.show() # 3. Parallel coordinates plot for Type, using Weight, Sweetness, and Sourness as features plt.figure(figsize=(12, 8)) parallel_coordinates(df[[\'Type\', \'Weight\', \'Sweetness\', \'Sourness\']], class_column=\'Type\', colormap=plt.get_cmap(\\"Set2\\")) plt.title(\'Parallel Coordinates Plot for Fruit Types\') plt.xlabel(\'Attributes\') plt.ylabel(\'Values\') plt.show()"},{"question":"Objective The goal of this assessment is to test your understanding of creating and customizing ECDF plots in seaborn. Task You need to write a Python function using seaborn that loads the \'penguins\' dataset and generates a customized ECDF plot. Your function should have the following specifications: 1. **Function Signature:** ```python def customized_ecdf_plot(x_feature: str, y_feature: str = None, hue: str = None, stat: str = \\"proportion\\", complementary: bool = False): pass ``` 2. **Input Parameters:** - `x_feature` (str): The name of the column to be plotted along the x-axis. - `y_feature` (str, optional): The name of the column to be plotted along the y-axis. Default is None. - `hue` (str, optional): The name of the column used for hue mapping. Default is None. - `stat` (str, optional): The type of statistic to use. Can be \\"proportion\\", \\"count\\", or \\"percent\\". Default is \\"proportion\\". - `complementary` (bool, optional): If True, plot the empirical complementary CDF (1 - CDF). Default is False. 3. **Output:** Your function should not return anything but should display the ECDF plot. 4. **Constraints:** - `x_feature` must be a valid column name in the dataset. - If provided, `y_feature` and `hue` must be valid column names in the dataset. - `stat` must be one of \\"proportion\\", \\"count\\", or \\"percent\\". - Your function should handle cases where the provided column names do not exist in the dataset gracefully by printing an appropriate error message. 5. **Example Usage:** ```python # Plot with species as hue and default statistic customized_ecdf_plot(x_feature=\\"bill_length_mm\\", hue=\\"species\\") # Plot with complementary CDF and \'count\' statistic customized_ecdf_plot(x_feature=\\"flipper_length_mm\\", stat=\\"count\\", complementary=True) ``` Notes: - Use seaborn\'s `ecdfplot` and assume the `penguins` dataset is available in seaborn\'s datasets. - Ensure to set an appropriate seaborn theme using `sns.set_theme()` for better aesthetics. Your implementation should demonstrate: - Loading a standard dataset from seaborn. - Creating ECDF plots with different configurations. - Appropriate handling of input parameters and potential errors.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customized_ecdf_plot(x_feature: str, y_feature: str = None, hue: str = None, stat: str = \\"proportion\\", complementary: bool = False): Generates a customized ECDF plot using seaborn\'s ecdfplot function. Parameters: x_feature (str): The name of the column to be plotted along the x-axis. y_feature (str, optional): The name of the column to be plotted along the y-axis. Default is None. hue (str, optional): The name of the column used for hue mapping. Default is None. stat (str, optional): The type of statistic to use. Can be \\"proportion\\", \\"count\\", or \\"percent\\". Default is \\"proportion\\". complementary (bool, optional): If True, plot the empirical complementary CDF (1 - CDF). Default is False. # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Validate column names valid_columns = set(penguins.columns) if x_feature not in valid_columns: print(f\\"Error: \'{x_feature}\' is not a valid column name in the dataset.\\") return if y_feature and y_feature not in valid_columns: print(f\\"Error: \'{y_feature}\' is not a valid column name in the dataset.\\") return if hue and hue not in valid_columns: print(f\\"Error: \'{hue}\' is not a valid column name in the dataset.\\") return if stat not in [\\"proportion\\", \\"count\\", \\"percent\\"]: print(f\\"Error: \'{stat}\' is not a valid statistic. Choose from \'proportion\', \'count\', or \'percent\'.\\") return # Set the seaborn theme for better aesthetics sns.set_theme() # Create the ECDF plot sns.ecdfplot(data=penguins, x=x_feature, y=y_feature, hue=hue, stat=stat, complementary=complementary) # Display the plot plt.show()"},{"question":"Objective Demonstrate your understanding of device-specific operations in PyTorch using the MPS backend on macOS. Problem Statement You are given the task to implement a PyTorch function that utilizes the MPS backend on macOS for high-performance tensor operations and model inference. Function Signature ```python def mps_tensor_operations(n: int) -> Tuple[torch.Tensor, torch.Tensor, float]: Performs tensor operations on the MPS device and returns the results. Parameters: - n: An integer representing the size of the tensor. Returns: - A tuple containing: 1. Tensor `a` initialized with ones on the MPS device. 2. Tensor `b` which is the result of multiplying tensor `a` by 2 on the MPS device. 3. The time taken (in seconds) to perform the tensor multiplication. Constraints: - The input `n` will be a positive integer. Note: - If the MPS device is not available, raise an EnvironmentError with a message \\"MPS device not available.\\" ``` Requirements 1. Your function should first check if the MPS device is available. 2. Create a tensor `a` of shape (n, ) initialized with ones, directly on the MPS device. 3. Perform a tensor operation to create tensor `b` by multiplying each element of tensor `a` by 2. 4. Measure and return the time taken to perform this tensor multiplication operation. 5. Ensure all operations, including timing, are conducted on the MPS device. Example ```python >>> a, b, duration = mps_tensor_operations(10000) >>> a.device device(type=\'mps\') >>> b.device device(type=\'mps\') >>> a.shape torch.Size([10000]) >>> b.shape torch.Size([10000]) >>> torch.all(b == 2) # Checking if all elements in `b` are 2 tensor(True) >>> duration # Time taken will vary, but should be a positive float value 0.000123 # Example output ``` Notes - If the MPS device is not available, your function should raise an `EnvironmentError`. - You are allowed to use the `time` module to measure the duration for the tensor operation.","solution":"import torch import time from typing import Tuple def mps_tensor_operations(n: int) -> Tuple[torch.Tensor, torch.Tensor, float]: Performs tensor operations on the MPS device and returns the results. Parameters: - n: An integer representing the size of the tensor. Returns: - A tuple containing: 1. Tensor `a` initialized with ones on the MPS device. 2. Tensor `b` which is the result of multiplying tensor `a` by 2 on the MPS device. 3. The time taken (in seconds) to perform the tensor multiplication. Constraints: - The input `n` will be a positive integer. Note: - If the MPS device is not available, raise an EnvironmentError with a message \\"MPS device not available.\\" if not torch.has_mps: raise EnvironmentError(\\"MPS device not available.\\") device = torch.device(\\"mps\\") # Create a tensor `a` of shape (n, ) initialized with ones, directly on the MPS device a = torch.ones(n, device=device) # Measure the time taken to multiply tensor `a` by 2 start_time = time.time() b = a * 2 end_time = time.time() # Calculate the duration duration = end_time - start_time return a, b, duration"},{"question":"# Python Module Import Exploration You\'re required to create a utility function using the `importlib` module to handle module imports. This function should mimic the deprecated `imp.load_module` functionality but should be implemented using `importlib` methods. Task Create a function `dynamic_import(module_name: str, file_path: str = None) -> type` that: 1. Dynamically loads the module specified by `module_name`. 2. Optionally, if `file_path` is provided, it should attempt to load the module from the specified file. Specifications - **Function Name**: `dynamic_import` - **Parameters**: - `module_name` (str): The name of the module to be loaded. - `file_path` (str, optional): The file path from which to load the module. If `None`, it should load using the regular import mechanism. - **Returns**: The imported module type. - **Raises**: Should raise `ImportError` if the module cannot be found or loaded. Examples ```python # Example without file path module = dynamic_import(\'math\') print(module.sqrt(16)) # Output: 4.0 # Example with file path # Assuming we have a custom module `mymodule` stored at \'/path/to/mymodule.py\' module = dynamic_import(\'mymodule\', \'/path/to/mymodule.py\') module.my_function() # Should execute `my_function` from `mymodule` ``` Hints - Use `importlib.util.find_spec` to get the module specification. - If `file_path` is provided, use `importlib.util.spec_from_file_location` to create the module spec. - Use `importlib.util.module_from_spec` to create the module from its spec. - Don\'t forget to handle exceptions and clean up resources if needed. Constraints - Assume the file path provided is always absolute and valid. - The function should handle modules that have already been imported or loaded. # Evaluation Criteria - Correctness: The function should correctly load and return the module. - Robustness: Proper error handling should be in place. - Use of `importlib`: Demonstrates the student\'s ability to use modern Python practices for module management.","solution":"import importlib.util import sys def dynamic_import(module_name: str, file_path: str = None) -> type: Dynamically imports a module. If file_path is provided, it loads the module from the specified file. :param module_name: Name of the module to import. :param file_path: Path to the file to load the module from (optional). :return: The imported module. :raises ImportError: If the module cannot be found or loaded. try: if file_path: spec = importlib.util.spec_from_file_location(module_name, file_path) else: spec = importlib.util.find_spec(module_name) if spec is None: raise ImportError(f\\"Cannot find spec for module {module_name}\\") module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) if module_name not in sys.modules: sys.modules[module_name] = module return module except Exception as e: raise ImportError(f\\"Could not import module {module_name}\\") from e"},{"question":"Objective: To test your understanding of Unix-specific service modules in Python, specifically focusing on the `resource` module to manage resource limits and retrieve resource usage information. Problem Description: You are required to write a Python function `monitor_resource_usage` that will monitor the system\'s resource usage for a given process. The function should set maximum resource limits for CPU time and memory usage and then return the resource usage after running a provided command. Function Signature: ```python def monitor_resource_usage(command: str, cpu_time_limit: int, memory_limit: int) -> dict: pass ``` Input: - `command` (str): The command to be executed in the shell (e.g., `/bin/ls`). - `cpu_time_limit` (int): The maximum CPU time (in seconds) that the process can use. - `memory_limit` (int): The maximum memory (in bytes) that the process can allocate. Output: - (dict): A dictionary containing the following keys and their respective values: - `ru_utime`: User CPU time used. - `ru_stime`: System CPU time used. - `ru_maxrss`: Maximum resident set size used. Constraints: - The function should handle cases where setting resource limits fails by catching exceptions and returning an appropriate error message. Example Usage: ```python result = monitor_resource_usage(\'/bin/ls\', 5, 1024*1024*100) print(result) # Output: {\'ru_utime\': ..., \'ru_stime\': ..., \'ru_maxrss\': ...} ``` Notes: 1. You will need to use the `subprocess` module to run the command. 2. Set the resource limits using the `resource` module before executing the command. 3. Retrieve resource usage information after the command execution. 4. Ensure that your solution is efficient and handles system-specific issues gracefully. This question requires you to demonstrate knowledge of the `resource` module, process handling, and error management in Python.","solution":"import resource import subprocess def monitor_resource_usage(command: str, cpu_time_limit: int, memory_limit: int) -> dict: Monitors the resource usage for a given command. Args: - command (str): The command to be executed in the shell. - cpu_time_limit (int): The max CPU time (in seconds) that the process can use. - memory_limit (int): The max memory (in bytes) that the process can allocate. Returns: - dict: A dictionary containing the resource usage information. try: # Set the CPU time limit resource.setrlimit(resource.RLIMIT_CPU, (cpu_time_limit, cpu_time_limit)) # Set the memory limit resource.setrlimit(resource.RLIMIT_AS, (memory_limit, memory_limit)) # Run the command process = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE) # Get resource usage usage = resource.getrusage(resource.RUSAGE_CHILDREN) return { \'ru_utime\': usage.ru_utime, \'ru_stime\': usage.ru_stime, \'ru_maxrss\': usage.ru_maxrss } except Exception as e: return {\'error\': str(e)}"},{"question":"# Task Objective: You will create a visualization that accurately represents data trends while addressing overplotting using Seaborn\'s jitter functionality. Dataset: Use the ‘penguins’ dataset provided by Seaborn\'s `load_dataset` function. Requirements: 1. **Load the `penguins` dataset** from Seaborn. 2. **Preprocess the data**: - Remove any rows with missing values. 3. **Create a jittered dot plot**: - Plot the species on the x-axis and body mass (in grams) on the y-axis. - Apply jitter to the y-axis using a relative width of 0.3. 4. **Add a second jittered dot plot**: - Plot the rounded body mass (to nearest 1000 grams) on the x-axis and flipper length (in millimeters) on the y-axis. - Apply jitter to both axes with `x=150` and `y=5`. 5. **Specify plot appearance**: - Use different colors for each species. - Include appropriate axis labels and a title for each plot. Input and Output Format: 1. **Input**: No user input. The dataset and parameters should be defined within the function. 2. **Output**: The function should display two plots as described above. Constraints: - Ensure that the dataset is cleaned before plotting (remove missing values). - Specify and apply jitter as per the given values. Example Invocation: ```python def create_jittered_plots(): # Implementation goes here create_jittered_plots() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_jittered_plots(): # Load the penguins dataset penguins = sns.load_dataset(\'penguins\') # Remove rows with missing values penguins = penguins.dropna() # Create a jittered dot plot for species vs body mass plt.figure(figsize=(10, 6)) sns.stripplot(x=\'species\', y=\'body_mass_g\', data=penguins, jitter=0.3, palette=\'deep\') plt.xlabel(\'Species\') plt.ylabel(\'Body Mass (g)\') plt.title(\'Jittered Dot Plot: Species vs Body Mass\') plt.show() # Create a new column for rounded body mass (to nearest 1000 grams) penguins[\'rounded_body_mass\'] = penguins[\'body_mass_g\'].apply(lambda x: round(x, -3)) # Create a jittered dot plot for rounded body mass vs flipper length plt.figure(figsize=(10, 6)) sns.stripplot(x=\'rounded_body_mass\', y=\'flipper_length_mm\', data=penguins, jitter=True, palette=\'deep\', dodge=True) plt.xlabel(\'Rounded Body Mass (g)\') plt.ylabel(\'Flipper Length (mm)\') plt.title(\'Jittered Dot Plot: Rounded Body Mass vs Flipper Length\') plt.xlim(penguins[\'rounded_body_mass\'].min() - 500, penguins[\'rounded_body_mass\'].max() + 500) plt.xticks(np.arange(penguins[\'rounded_body_mass\'].min(), penguins[\'rounded_body_mass\'].max() + 1000, 1000)) plt.show()"},{"question":"Objective: Create a Python function that searches a given directory for files matching a specified pattern using the `glob` module. The function should handle various scenarios including recursive searching and escape characters in patterns. Description: Write a function called `find_matching_files` that meets the following requirements: 1. **Function Signature**: ```python def find_matching_files(pattern: str, root_dir: str = None, recursive: bool = False) -> list: ``` 2. **Parameters**: - `pattern (str)`: A string containing the path pattern to match. This can include shell-style wildcards (`*`, `?`, `[]`). - `root_dir (str, optional)`: The root directory for searching. If not specified, the current directory should be used. - `recursive (bool, optional)`: If `True`, the search should include subdirectories. Default is `False`. 3. **Return**: - A `list` of path names that match the pattern. 4. **Functional Requirements**: - Use the `glob.glob` function for pattern matching. - Ensure that the function can handle recursive searching if specified. - Return paths relative to `root_dir` if provided. - Use `glob.escape` to handle any literal strings that may contain special characters. 5. **Example**: ```python # Assuming the directory structure: # test_dir/ # ├── a.txt # ├── b.txt # ├── sub1/ # │ └── c.txt # └── sub2/ # └── d.gif result = find_matching_files(\\"*.txt\\", \\"test_dir\\", recursive=True) # Expected output: [\'test_dir/a.txt\', \'test_dir/b.txt\', \'test_dir/sub1/c.txt\'] result = find_matching_files(\\"*.txt\\", \\"test_dir\\", recursive=False) # Expected output: [\'test_dir/a.txt\', \'test_dir/b.txt\'] result = find_matching_files(\\"d.???\\", \\"test_dir/sub2\\") # Expected output: [\'test_dir/sub2/d.gif\'] ``` 6. **Constraints**: - The function should handle both absolute and relative paths. - The performance should be considered when using recursive search on large directory trees. Notes: You can assume that the `os` and `glob` modules are already imported and available for use in your function.","solution":"import glob import os def find_matching_files(pattern: str, root_dir: str = None, recursive: bool = False) -> list: Searches a given directory for files matching a specified pattern using the glob module. Parameters: - pattern (str): The pattern to match files. - root_dir (str, optional): The root directory for the search. Defaults to current directory. - recursive (bool, optional): If True, the search includes subdirectories. Defaults to False. Returns: - list: A list of paths that match the pattern. if root_dir is None: root_dir = os.getcwd() if recursive: pattern = os.path.join(glob.escape(root_dir), \'**\', pattern) else: pattern = os.path.join(glob.escape(root_dir), pattern) return [os.path.relpath(path, root_dir) for path in glob.glob(pattern, recursive=recursive)]"},{"question":"You are tasked with processing a large dataset of user transactions efficiently using Python\'s functional programming modules. The goal is to filter, transform, and aggregate the data to find the total amount spent by users in a specific category. Input 1. A list of dictionaries where each dictionary represents a transaction with the following keys: - `user_id` (int): The ID of the user. - `amount` (float): The amount of money spent in this transaction. - `category` (str): The category of the transaction. Output - A dictionary where the keys are user IDs and the values are the total amount spent by each user in the specified category. Function Signature ```python from typing import List, Dict def total_spent_by_category(transactions: List[Dict[str, any]], target_category: str) -> Dict[int, float]: pass ``` # Constraints 1. The input list may contain up to 10^5 transactions. 2. Amount values are positive floats. 3. User IDs are positive integers. 4. The target category is a non-empty string. # Example ```python transactions = [ {\'user_id\': 1, \'amount\': 120.50, \'category\': \'electronics\'}, {\'user_id\': 2, \'amount\': 30.00, \'category\': \'groceries\'}, {\'user_id\': 1, \'amount\': 150.00, \'category\': \'electronics\'}, {\'user_id\': 2, \'amount\': 55.50, \'category\': \'electronics\'} ] target_category = \'electronics\' print(total_spent_by_category(transactions, target_category)) # Output: {1: 270.50, 2: 55.50} ``` # Requirements Use the `itertools`, `functools`, and `operator` modules to: 1. Filter the transactions to only include those in the `target_category`. 2. Map each transaction to the `(user_id, amount)` tuple. 3. Aggregate the amounts spent by each user using efficient looping and higher-order functions. # Performance - Your solution should be able to handle the upper limit of the transaction list efficiently. # Evaluation Criteria - Correct and efficient filtering of data. - Proper use of functional programming constructs (`itertools`, `functools`, and `operator`). - Clean and readable code with appropriate comments.","solution":"from typing import List, Dict from itertools import filterfalse, groupby from operator import itemgetter from functools import reduce def total_spent_by_category(transactions: List[Dict[str, any]], target_category: str) -> Dict[int, float]: # Step 1: Filter transactions by target category filtered_transactions = filter(lambda x: x[\'category\'] == target_category, transactions) # Step 2: Map transactions to (user_id, amount) tuples user_amounts = map(lambda x: (x[\'user_id\'], x[\'amount\']), filtered_transactions) # Step 3: Sort by user_id to group them for aggregation sorted_user_amounts = sorted(user_amounts, key=itemgetter(0)) # Step 4: Group by user_id and aggregate the amounts spent grouped_transactions = groupby(sorted_user_amounts, key=itemgetter(0)) # Step 5: Use reduce to sum amounts for each user total_spent = { user_id: reduce(lambda acc, x: acc + x[1], group, 0) for user_id, group in grouped_transactions } return total_spent"},{"question":"# **Coding Assessment Question** **Objective:** Demonstrate your understanding of seaborn\'s objects interface by creating a specific bar plot that visualizes a given dataset. **Question:** You are given a dataset containing information about the daily tips received at a restaurant. Your task is to use seaborn to create a bar plot that fulfills the following requirements: 1. Load the dataset using seaborn\'s `load_dataset` function. The dataset to use is `\\"tips\\"`. 2. Create a bar plot (`Bar`) that shows the number of distinct customers (`size`) per day of the week (`day`). 3. Additionally, use the color to distinguish between the `sex` of the customers. 4. The bars for each gender (`sex`) should be dodged (i.e., placed side by side instead of stacked). **Input:** A dataset named `\\"tips\\"` is available through seaborn\'s `load_dataset` function. **Output:** A seaborn bar plot satisfying the above requirements. **Constraints:** - Ensure your plot clearly differentiates the count of distinct customer sizes per day, and each bar is color-coded by gender. - Use seaborn\'s objects interface as demonstrated in the provided documentation. **Example Code:** ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the dataset tips = load_dataset(\\"tips\\") # Step 2: Create a plot object with the dataset and x-axis assigned to the \'day\' plot = so.Plot(tips, x=\\"day\\") # Step 3: Add a bar chart with the count of distinct sizes, distinguished by color \'sex\', and dodged plot.add(so.Bar(), so.Count(), so.Dodge(), color=\'sex\').show() ``` Please ensure that your plot is displayed correctly with all the required elements.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_bar_plot(): Creates a bar plot showing the number of distinct customers per day of the week, distinguished by sex. # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot object with \'day\' on the x-axis p = so.Plot(tips, x=\\"day\\", color=\\"sex\\") # Add a bar chart with count of distinct sizes, dodged by sex p.add(so.Bar(), so.Count(), so.Dodge()).show()"},{"question":"**Question: Python List Operations Using C API** You are given a custom implementation that mimics the lower-level C API functions provided by the `python310` package for list manipulations. Your task is to create a class `CustomList` in Python that emulates some of these functionalities. The class should have the following methods: 1. **`__init__(self, size: int)`**: Initialize a list of given `size` with `None` values. 2. **`check(self)`**: Return `True` if the object is an instance of `CustomList`. 3. **`check_exact(self)`**: Return `True` if the exact type of the object is `CustomList`. 4. **`get_size(self)`**: Return the size of the list. 5. **`get_item(self, index: int)`**: Return the item at the specified `index`. 6. **`set_item(self, index: int, value)`**: Set the item at the specified `index` to `value`. 7. **`insert(self, index: int, value)`**: Insert `value` at the specified `index`. The elements from this index onwards should shift to the right. 8. **`append(self, value)`**: Append `value` to the end of the list. 9. **`get_slice(self, low: int, high: int)`**: Return a new `CustomList` containing the elements between `low` and `high` indices. 10. **`set_slice(self, low: int, high: int, values)`**: Set the slice between `low` and `high` to the elements in `values`. 11. **`sort(self)`**: Sort the elements in the list. 12. **`reverse(self)`**: Reverse the elements in the list. 13. **`to_tuple(self)`**: Convert the list to a tuple and return it. **Constraints:** - The `index` values for `get_item`, `set_item`, `insert`, and `get_slice` should be within the bounds of the list. Appropriate exceptions should be raised otherwise. - Ensure the performance of your list manipulations is efficient, leveraging Python\'s underlying list structures appropriately. **Example:** ```python cl = CustomList(5) print(cl.check()) # True print(cl.check_exact()) # True cl.set_item(0, 10) print(cl.get_item(0)) # 10 cl.append(20) print(cl.get_size()) # 6 cl.insert(1, 15) cl.set_slice(2, 3, [25, 30]) print(cl.get_slice(1, 4)) # CustomList containing [15, 25, 30] cl.sort() print(cl.to_tuple()) # (10, 15, 20, 25, 30) cl.reverse() print(cl.to_tuple()) # (30, 25, 20, 15, 10) ``` Implement the `CustomList` class with the specified methods.","solution":"class CustomList: def __init__(self, size: int): self.list = [None] * size def check(self): return isinstance(self, CustomList) def check_exact(self): return type(self) is CustomList def get_size(self): return len(self.list) def get_item(self, index: int): if index < 0 or index >= len(self.list): raise IndexError(\'Index out of range\') return self.list[index] def set_item(self, index: int, value): if index < 0 or index >= len(self.list): raise IndexError(\'Index out of range\') self.list[index] = value def insert(self, index: int, value): if index < 0 or index > len(self.list): raise IndexError(\'Index out of range\') self.list.insert(index, value) def append(self, value): self.list.append(value) def get_slice(self, low: int, high: int): if low < 0 or high > len(self.list) or low > high: raise IndexError(\'Slice indices out of range\') return CustomList.from_existing_list(self.list[low:high]) def set_slice(self, low: int, high: int, values): if low < 0 or high > len(self.list) or low > high: raise IndexError(\'Slice indices out of range\') self.list[low:high] = values def sort(self): self.list.sort() def reverse(self): self.list.reverse() def to_tuple(self): return tuple(self.list) @classmethod def from_existing_list(cls, existing_list): instance = cls(0) instance.list = existing_list return instance"},{"question":"**Objective:** Implement a class that mimics the behavior of Python\'s `netrc` class for parsing `.netrc` files used in Unix system FTP clients. Your class will parse the contents of a `.netrc` file and provide methods to access authentication data for specified hosts, ensuring to mimic the security and error handling features described. **Requirements:** 1. Implement a class `MyNetrc` that: - Initializes with an optional file path argument. If no argument is provided, default to `~/.netrc`. - Parses the file to extract authentication information. 2. Implement the following methods: - `authenticators(self, host)`: - Input: `host` (string) - Output: A tuple `(login, account, password)` for the specified host or `None` if not found. - `__repr__(self)`: - Output: A string representation of the parsed data in `.netrc` file format. 3. Handle the following errors: - Raise `FileNotFoundError` if the file does not exist. - Raise `NetrcParseError` if there are syntactical errors in the file, with attributes `msg`, `filename`, and `lineno`. - Ensure the file has the correct permissions on POSIX systems, raising `NetrcParseError` if insecure. 4. Use the following class for exceptions: ```python class NetrcParseError(Exception): def __init__(self, msg, filename=None, lineno=None): self.msg = msg self.filename = filename self.lineno = lineno super().__init__(self.msg) ``` **Constraints:** - Assume that file contents are in plain text and can contain comments starting with `#`. - Usernames, accounts, and passwords consist of ASCII characters and do not include whitespaces. **Example file format to parse:** ``` machine host1 login user123 password secret machine host2 login user456 password anothersecret ``` **Input/Output Example:** 1. Example `.netrc` file in user\'s home: ``` machine example.com login myuser password mypassword ``` 2. Code: ```python netrc_obj = MyNetrc() print(netrc_obj.authenticators(\'example.com\')) ``` 3. Output: ``` (\'myuser\', None, \'mypassword\') ``` 4. Code: ```python repr(netrc_obj) ``` 5. Output: ``` \'machine example.comn login myusern password mypasswordn\' ``` Make sure to validate the file input/output, error handling, and proper storage of credentials. **Performance Note:** Your implementation should efficiently parse the file and handle typical sizes of `.netrc` files (usually small).","solution":"import os class NetrcParseError(Exception): def __init__(self, msg, filename=None, lineno=None): self.msg = msg self.filename = filename self.lineno = lineno super().__init__(self.msg) class MyNetrc: def __init__(self, file_path=None): if file_path is None: file_path = os.path.expanduser(\\"~/.netrc\\") self.file_path = file_path self.data = self._parse_netrc() def _parse_netrc(self): if not os.path.exists(self.file_path): raise FileNotFoundError(f\\"File {self.file_path} does not exist\\") with open(self.file_path) as f: lines = f.readlines() data = {} current_machine = None login, account, password = None, None, None lineno = 0 try: for line in lines: lineno += 1 line = line.strip() if line.startswith(\\"#\\") or line == \\"\\": continue tokens = line.split() if tokens[0] == \\"machine\\": if current_machine: data[current_machine] = (login, account, password) current_machine = tokens[1] login, account, password = None, None, None elif tokens[0] == \\"login\\": login = tokens[1] elif tokens[0] == \\"password\\": password = tokens[1] elif tokens[0] == \\"account\\": account = tokens[1] else: raise NetrcParseError(f\\"Unexpected token {tokens[0]}\\", self.file_path, lineno) if current_machine: data[current_machine] = (login, account, password) except IndexError: raise NetrcParseError(f\\"Incomplete entry found\\", self.file_path, lineno) return data def authenticators(self, host): return self.data.get(host, None) def __repr__(self): repr_str = \\"\\" for machine, (login, account, password) in self.data.items(): repr_str += f\\"machine {machine}n\\" if login: repr_str += f\\" login {login}n\\" if account: repr_str += f\\" account {account}n\\" if password: repr_str += f\\" password {password}n\\" return repr_str.strip()"},{"question":"Objective: To assess the student\'s ability to write code that is compatible with both Python 2.7 and Python 3.x, ensuring they understand key differences and can apply practices such as feature detection and handling of text and binary data appropriately. Problem Statement: You are given a task to write a function `process_data` that processes a list of tuples containing both text and binary data. The function should: 1. Separate text data and binary data into different lists. 2. Convert all text data to uppercase. 3. Decode all binary data to strings assuming the data is encoded in UTF-8. 4. Return a dictionary with two keys: `\\"text\\"` and `\\"binary\\"`. The values should be the processed lists of text and binary data, respectively. Function Signature: ```python def process_data(data): Processes text and binary data separately. Args: data (list of tuples): Each tuple contains text data and binary data. Returns: dict: A dictionary with two keys \\"text\\" and \\"binary\\". The value for \\"text\\" is the list of uppercase text. The value for \\"binary\\" is the list of decoded strings. pass ``` Example: ```python # Given the following input data data = [ (\\"hello\\", b\\"world\\"), (\\"python\\", b\\"3.8\\"), (\\"compatibility\\", b\\"testing\\") ] # Calling the function result = process_data(data) # The expected output should be: { \\"text\\": [\\"HELLO\\", \\"PYTHON\\", \\"COMPATIBILITY\\"], \\"binary\\": [\\"world\\", \\"3.8\\", \\"testing\\"] } ``` Constraints: - You must ensure the function works under both Python 2.7 and Python 3.x. - Handling of binary and text data should be clear and correct, using appropriate methods/functions available in both Python 2.7 and 3.x. - You should not use any external libraries, except for those from the Python standard library. Hints: - Use `six` library for compatibility functions (e.g., `six.text_type`). - Remember to handle Python 2.7’s `str` and `unicode` types appropriately, and Python 3.x’s `str` and `bytes` types. - Use feature detection instead of version detection where possible.","solution":"def process_data(data): Processes text and binary data separately. Args: data (list of tuples): Each tuple contains text data and binary data. Returns: dict: A dictionary with two keys \\"text\\" and \\"binary\\". The value for \\"text\\" is the list of uppercase text. The value for \\"binary\\" is the list of decoded strings. import six text_list = [] binary_list = [] for text, binary in data: # Convert text data to uppercase text_list.append(text.upper()) # Decode binary data to string with UTF-8 if isinstance(binary, six.binary_type): binary_list.append(binary.decode(\'utf-8\')) else: raise ValueError(\'Expected binary data\') return {\\"text\\": text_list, \\"binary\\": binary_list}"},{"question":"**Question: Creating and Customizing Multi-faceted Plots with seaborn.objects API** You are required to demonstrate your understanding of seaborn\'s objects API to create complex visualizations. Specifically, you will create a multi-faceted plot and customize its layout and dimensions. # Input 1. `data`: A pandas DataFrame containing the following columns: - `category1`: Categorical data for the first facet. - `category2`: Categorical data for the second facet. - `x`: Numeric data for the x-axis. - `y`: Numeric data for the y-axis. 2. `size`: A tuple of two integers indicating the size of the overall figure. 3. `extent`: A list of four float values representing the size of the plot relative to the underlying figure. 4. `layout_engine`: A string specifying which automatic layout engine to use (`constrained` or another seaborn-supported layout engine). # Output A seaborn plot with the following specifications: - The plot should have facets for each combination of `category1` and `category2`. - The overall figure size should be set using the provided `size`. - The layout engine specified by `layout_engine` should be applied. - The plot size relative to the figure should be controlled using the provided `extent`. # Constraints - Ensure that the resulting plot is displayed within a Jupyter notebook cell. # Example Usage ```python import pandas as pd import seaborn.objects as so # Example DataFrame data = pd.DataFrame({ \'category1\': [\'A\', \'A\', \'B\', \'B\'], \'category2\': [\'X\', \'Y\', \'X\', \'Y\'], \'x\': [1, 2, 3, 4], \'y\': [4, 3, 2, 1] }) # Example parameters size = (6, 6) extent = [0, 0, .8, 1] layout_engine = \\"constrained\\" # Call the function to create and show the plot create_custom_facet_plot(data, size, extent, layout_engine) ``` # Implementation Implement the function `create_custom_facet_plot(data, size, extent, layout_engine)` to generate the required plot. ```python def create_custom_facet_plot(data, size, extent, layout_engine): import seaborn.objects as so # Step 1: Create a Plot instance with the provided data p = so.Plot(data, x=\\"x\\", y=\\"y\\").facet(data[\\"category1\\"], data[\\"category2\\"]) # Step 2: Set the overall figure dimensions p = p.layout(size=size) # Step 3: Apply the specified layout engine p = p.layout(engine=layout_engine) # Step 4: Control the plot size relative to the figure p = p.layout(extent=extent) # Step 5: Display the plot p.show() # Testing your function with the provided example create_custom_facet_plot(data, size, extent, layout_engine) ```","solution":"def create_custom_facet_plot(data, size, extent, layout_engine): import seaborn.objects as so import matplotlib.pyplot as plt # Step 1: Create a Plot instance with the provided data p = so.Plot(data, x=\\"x\\", y=\\"y\\").facet(\\"category1\\", \\"category2\\") # Step 2: Set the overall figure dimensions p = p.layout(size=size) # Step 3: Apply the specified layout engine if layout_engine == \\"constrained\\": plt.figure(constrained_layout=True) else: plt.figure() p = p.layout(engine=layout_engine) # Step 4: Control the plot size relative to the figure p = p.layout(extent=extent) # Step 5: Display the plot p.show()"},{"question":"**Objective:** Demonstrate your understanding of seaborn\'s theme customization and palette settings by creating a composite plot. **Problem Statement:** You are provided with a dataset representing sales of three products (A, B, C) over a period of four quarters. You are required to create a composite plot that includes: 1. A bar plot showing the total sales of each product. 2. A line plot showing the trend in sales for each product over the four quarters. Customize the theme and palette settings according to the following requirements: - Use the `darkgrid` theme for both plots. - Use the `muted` palette for the bar plot. - For the line plot, remove the top and right spines. **Input:** A dictionary representing the sales data: ```python sales_data = { \\"Quarter\\": [\\"Q1\\", \\"Q2\\", \\"Q3\\", \\"Q4\\"], \\"Product A\\": [200, 240, 300, 330], \\"Product B\\": [150, 190, 220, 280], \\"Product C\\": [250, 270, 310, 300] } ``` **Output:** A composite plot with the following specifications: - A bar plot showing total annual sales for each product. - A line plot showing the sales trend across the four quarters. - Custom themes and aesthetics as specified. **Function Signature:** ```python import seaborn as sns import matplotlib.pyplot as plt def create_composite_sales_plot(sales_data): # Your code here pass # Example usage sales_data = { \\"Quarter\\": [\\"Q1\\", \\"Q2\\", \\"Q3\\", \\"Q4\\"], \\"Product A\\": [200, 240, 300, 330], \\"Product B\\": [150, 190, 220, 280], \\"Product C\\": [250, 270, 310, 300] } create_composite_sales_plot(sales_data) ``` **Constraints:** - Use only seaborn and matplotlib for plotting. - Ensure that the plot is visually clear and labeled appropriately. **Grading Criteria:** - Correctly setting the seaborn theme and palette. - Properly plotting the bar and line plots. - Meeting all customization guidelines specified. - Code readability and the use of good programming practices.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_composite_sales_plot(sales_data): # Convert sales_data dictionary to pandas DataFrame df = pd.DataFrame(sales_data) # Calculate total annual sales for each product total_sales = df.iloc[:, 1:].sum().reset_index() total_sales.columns = [\'Product\', \'Total Sales\'] # Set the theme to darkgrid sns.set_theme(style=\\"darkgrid\\") # Create a figure with two subplots (1 row, 2 columns) fig, axes = plt.subplots(1, 2, figsize=(15, 6)) # Bar plot for total sales ax1 = sns.barplot(x=\'Product\', y=\'Total Sales\', data=total_sales, palette=\'muted\', ax=axes[0]) ax1.set_title(\'Total Annual Sales per Product\') ax1.set_xlabel(\'Product\') ax1.set_ylabel(\'Total Sales\') # Line plot for sales trend across quarters ax2 = sns.lineplot(data=df.set_index(\'Quarter\'), markers=True, dashes=False) ax2.set_title(\'Sales Trend per Quarter\') ax2.set_xlabel(\'Quarter\') ax2.set_ylabel(\'Sales\') # Removing top and right spines for the line plot sns.despine(ax=ax2, top=True, right=True) plt.tight_layout() plt.show() # Example usage sales_data = { \\"Quarter\\": [\\"Q1\\", \\"Q2\\", \\"Q3\\", \\"Q4\\"], \\"Product A\\": [200, 240, 300, 330], \\"Product B\\": [150, 190, 220, 280], \\"Product C\\": [250, 270, 310, 300] } create_composite_sales_plot(sales_data)"},{"question":"**Predicting House Prices Using Scikit-learn** **Problem Description:** You are provided with a housing dataset that includes features such as the number of rooms, area, location, etc., and the corresponding house prices. Your task is to implement a supervised learning pipeline using scikit-learn to predict the prices of houses based on these features. You should use appropriate regression models and evaluate their performance. **Input:** 1. `features_train.csv`: A CSV file containing the training data with house features. 2. `prices_train.csv`: A CSV file containing the training data with house prices. 3. `features_test.csv`: A CSV file containing the test data with house features. **Output:** A CSV file `predictions.csv` containing the predicted house prices for the test data. **Constraints:** 1. You must use at least two different regression models from scikit-learn (e.g., Linear Regression, Decision Tree Regressor). 2. Perform hyperparameter tuning using GridSearchCV. 3. Evaluate the performance of your models using at least two different metrics (e.g., Mean Squared Error, R² score). **Performance Requirements:** - Your code should be optimized for performance, ensuring that the model training and predictions are completed within a reasonable time frame. - The model should achieve a significant level of accuracy (justify your choice of acceptable accuracy). **Instructions:** 1. Load and preprocess the data. 2. Train at least two different regression models. 3. Perform hyperparameter tuning using GridSearchCV. 4. Evaluate the models using appropriate metrics. 5. Select the best model based on the evaluation metrics. 6. Use the best model to predict house prices for the test data. 7. Save the predictions to a CSV file `predictions.csv`. **Example of the Expected Solution Structure:** ```python import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.linear_model import LinearRegression from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import mean_squared_error, r2_score # Load the data features_train = pd.read_csv(\'features_train.csv\') prices_train = pd.read_csv(\'prices_train.csv\') features_test = pd.read_csv(\'features_test.csv\') # Preprocess the data (if needed) # This could include handling missing values, encoding categorical variables, etc. # Split the data into training and validation sets X_train, X_val, y_train, y_val = train_test_split(features_train, prices_train, test_size=0.2, random_state=42) # Train the models models = { \'Linear Regression\': LinearRegression(), \'Decision Tree Regressor\': DecisionTreeRegressor() } # Hyperparameter tuning param_grid = { \'Linear Regression\': {}, \'Decision Tree Regressor\': {\'max_depth\': [3, 5, 7, 10]} } best_models = {} best_scores = {} for model_name, model in models.items(): grid_search = GridSearchCV(model, param_grid[model_name], scoring=\'neg_mean_squared_error\', cv=5) grid_search.fit(X_train, y_train) best_models[model_name] = grid_search.best_estimator_ y_pred = best_models[model_name].predict(X_val) best_scores[model_name] = { \'MSE\': mean_squared_error(y_val, y_pred), \'R²\': r2_score(y_val, y_pred) } # Select the best model best_model_name = min(best_scores, key=lambda k: best_scores[k][\'MSE\']) best_model = best_models[best_model_name] # Predict house prices for the test data predictions = best_model.predict(features_test) # Save the predictions to a CSV file pd.DataFrame(predictions, columns=[\'Predicted_Price\']).to_csv(\'predictions.csv\', index=False) # Additionally, print or log the model performance print(\\"Best Model:\\", best_model_name) print(\\"Performance:\\", best_scores[best_model_name]) ``` **Submission:** - Provide your complete code solution. - Include a brief explanation of your approach and choice of models. - Submit the `predictions.csv` file along with your code.","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.linear_model import LinearRegression from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import mean_squared_error, r2_score def load_data(): # Load datasets features_train = pd.read_csv(\'features_train.csv\') prices_train = pd.read_csv(\'prices_train.csv\') features_test = pd.read_csv(\'features_test.csv\') return features_train, prices_train, features_test def preprocess_data(features, target=None): # Implement any necessary preprocessing steps # For example, handling missing values, encoding categorical variables, etc. # For this example, we assume no preprocessing is needed. return features, target def train_models(X_train, y_train): models = { \'Linear Regression\': LinearRegression(), \'Decision Tree Regressor\': DecisionTreeRegressor() } param_grid = { \'Linear Regression\': {}, \'Decision Tree Regressor\': {\'max_depth\': [3, 5, 7, 10]} } best_models = {} for model_name, model in models.items(): grid_search = GridSearchCV(model, param_grid[model_name], scoring=\'neg_mean_squared_error\', cv=5) grid_search.fit(X_train, y_train) best_models[model_name] = grid_search.best_estimator_ return best_models def evaluate_models(models, X_val, y_val): scores = {} for model_name, model in models.items(): y_pred = model.predict(X_val) scores[model_name] = { \'MSE\': mean_squared_error(y_val, y_pred), \'R²\': r2_score(y_val, y_pred) } return scores def select_best_model(scores): best_model_name = min(scores, key=lambda k: scores[k][\'MSE\']) return best_model_name def save_predictions(predictions, filename=\'predictions.csv\'): pd.DataFrame(predictions, columns=[\'Predicted_Price\']).to_csv(filename, index=False) def main(): features_train, prices_train, features_test = load_data() features_train, prices_train = preprocess_data(features_train, prices_train) X_train, X_val, y_train, y_val = train_test_split(features_train, prices_train, test_size=0.2, random_state=42) best_models = train_models(X_train, y_train) scores = evaluate_models(best_models, X_val, y_val) best_model_name = select_best_model(scores) best_model = best_models[best_model_name] predictions = best_model.predict(features_test) save_predictions(predictions) print(\\"Best Model:\\", best_model_name) print(\\"Performance:\\", scores[best_model_name]) if __name__ == \'__main__\': main()"},{"question":"**Problem Statement:** You are tasked with creating a function that mimics part of the functionality of the `pydoc` module. Specifically, your function will extract and format the docstring of a given Python object (module, class, function, or method). This will help you demonstrate your understanding of the `inspect` module and handling documentation strings in Python. **Function Signature:** ```python def extract_docstring(obj: object) -> str: Extract and format the docstring of the given Python object. Parameters: obj (object): The Python object (module, class, function, or method) from which to extract and format the documentation. Returns: str: A formatted string containing the docstring of the given object. If no docstring is available, return \'No documentation available\'. ``` # Input: - `obj` (object): The Python object from which to extract and format the documentation. It can be a module, class, function, or method. # Output: - A formatted string containing the docstring of the given object. # Constraints: - You may assume that the input object is always a valid Python object (module, class, function, or method). - If the object does not have a docstring, your function should return the string `\'No documentation available\'`. # Example: ```python def example_function(): This is an example function. It serves as an illustration of what the docstring extraction should do. pass print(extract_docstring(example_function)) # Output: # \\"This is an example function. # # It serves as an illustration of what the docstring extraction should do.\\" print(extract_docstring(print)) # Output: # \\"Print objects to the text stream file, separated by sep and followed by end. # sep, end and file, if present, must be given as keyword arguments.\\" class ExampleClass: This is an example class. def method(self): This is an example method. pass print(extract_docstring(ExampleClass)) # Output: # \\"This is an example class.\\" print(extract_docstring(ExampleClass().method)) # Output: # \\"This is an example method.\\" import math print(extract_docstring(math)) # Output will be the docstring for the `math` module or \'No documentation available\' if it\'s not present. ``` # Additional Notes: - You may find the `inspect` module useful for this task. - Pay attention to handling different types of objects (modules, classes, functions, and methods) and their documentation strings. - Your solution should be efficient and handle edge cases gracefully.","solution":"import inspect def extract_docstring(obj: object) -> str: Extract and format the docstring of the given Python object. Parameters: obj (object): The Python object (module, class, function, or method) from which to extract and format the documentation. Returns: str: A formatted string containing the docstring of the given object. If no docstring is available, return \'No documentation available\'. # Inspect the object to get its docstring docstring = inspect.getdoc(obj) if docstring is None: return \'No documentation available\' return docstring"},{"question":"# Objective: In Python, bytes objects are sequences of byte values. They are immutable, meaning once you create them, their content can\'t be modified. However, you can create new bytes objects by deriving from or combining existing ones. Your task is to implement a function that takes two bytes objects and efficiently performs various operations related to these objects, including concatenation, format-based creation, and querying of these objects. This will test both your understanding of bytes objects and your ability to manipulate them effectively. # Function Signature: ```python def process_bytes_objects(bytes_obj1: bytes, bytes_obj2: bytes) -> dict: Given two bytes objects, perform the following operations and return a dictionary with the results: 1. \'concatenation\': The result of concatenating bytes_obj1 and bytes_obj2. 2. \'length1\': Length of the first bytes object. 3. \'length2\': Length of the second bytes object. 4. \'formatted\': A new bytes object created using format \'%s %s %d\' where first %s is replaced by bytes_obj1, second %s is replaced by bytes_obj2, and %d is replaced by the total length of the concatenated result. Args: bytes_obj1 (bytes): The first bytes object. bytes_obj2 (bytes): The second bytes object. Returns: dict: A dictionary with keys \'concatenation\', \'length1\', \'length2\', \'formatted\' containing the respective results. pass ``` # Expected Input and Output Formats: **Input:** - `bytes_obj1`: A bytes object (e.g., b\'hello\') - `bytes_obj2`: A bytes object (e.g., b\'world\') **Output:** - A dictionary with the keys \'concatenation\', \'length1\', \'length2\', \'formatted\'. - \'concatenation\': A bytes object that is the result of `bytes_obj1 + bytes_obj2`. - \'length1\': An integer representing the length of `bytes_obj1`. - \'length2\': An integer representing the length of `bytes_obj2`. - \'formatted\': A new bytes object using the format \'%s %s %d\' where the placeholders are replaced respectively. # Constraints: 1. The bytes objects can be empty. 2. The combined length of `bytes_obj1` and `bytes_obj2` will not exceed 1 million bytes. # Example: ```python bytes_obj1 = b\'hello\' bytes_obj2 = b\'world\' result = process_bytes_objects(bytes_obj1, bytes_obj2) # Expected output: # { # \'concatenation\': b\'helloworld\', # \'length1\': 5, # \'length2\': 5, # \'formatted\': b\'hello world 10\' # } ``` # Notes: - You might find Python\'s built-in `%` operator for bytes formatting helpful. - Be mindful of performance, especially with larger bytes objects. Implement the `process_bytes_objects` function to meet the requirements above and pass the provided example test case.","solution":"def process_bytes_objects(bytes_obj1: bytes, bytes_obj2: bytes) -> dict: Given two bytes objects, perform the following operations and return a dictionary with the results: 1. \'concatenation\': The result of concatenating bytes_obj1 and bytes_obj2. 2. \'length1\': Length of the first bytes object. 3. \'length2\': Length of the second bytes object. 4. \'formatted\': A new bytes object created using format \'%s %s %d\' where first %s is replaced by bytes_obj1, second %s is replaced by bytes_obj2, and %d is replaced by the total length of the concatenated result. Args: bytes_obj1 (bytes): The first bytes object. bytes_obj2 (bytes): The second bytes object. Returns: dict: A dictionary with keys \'concatenation\', \'length1\', \'length2\', \'formatted\' containing the respective results. concatenation_result = bytes_obj1 + bytes_obj2 length1 = len(bytes_obj1) length2 = len(bytes_obj2) total_length = length1 + length2 formatted_result = b\'%s %s %d\' % (bytes_obj1, bytes_obj2, total_length) return { \'concatenation\': concatenation_result, \'length1\': length1, \'length2\': length2, \'formatted\': formatted_result }"},{"question":"# Question: Implement a Data Validation Function Using scikit-learn Utilities Objective You need to implement a function that accepts an array `X` and a target array `y`, validates them using scikit-learn\'s validation tools, and performs some basic preprocessing steps. The function should handle both dense and sparse arrays, and should raise appropriate errors for invalid inputs. Function Signature ```python def validate_and_preprocess(X, y, multi_output=False, random_state=None): Validate and preprocess input arrays. Parameters: - X (array-like): Features array, can be a numpy array or a sparse matrix. - y (array-like): Target array. - multi_output (bool): If True, allows y to be in multi-output format. Default is False. - random_state (int or None): Seed for random number generator or None. Returns: - X_processed (array-like): Validated and preprocessed features array. - y_processed (array-like): Validated and preprocessed target array. - rng (RandomState): RandomState object initialized with the given random state. pass ``` Requirements 1. **Validation**: - Use `check_X_y` to validate that `X` and `y` have consistent lengths and proper formats. - Use the `multi_output` parameter to determine whether to allow multi-output format for `y`. - Ensure that `X` is a 2D array and `y` is a 1D or 2D array as appropriate. - Use `assert_all_finite` to ensure there are no NaNs or Infs in `X` and `y`. 2. **Preprocessing**: - Convert `X` to a float array using `as_float_array`. - Convert `y` to a float array using `as_float_array`. 3. **Random State**: - Use `check_random_state` to initialize the random number generator using the provided `random_state` parameter. 4. **Error Handling**: - Raise an appropriate error if `X` or `y` are invalid or contain NaNs/Infs. Example Use Case ```python import numpy as np from scipy.sparse import csr_matrix X = np.array([[1, 2], [3, 4], [5, 6]]) y = np.array([1, 2, 3]) # Valid input X_processed, y_processed, rng = validate_and_preprocess(X, y) print(X_processed) print(y_processed) print(rng) # Sparse input X_sparse = csr_matrix(X) X_processed, y_processed, rng = validate_and_preprocess(X_sparse, y) print(X_processed.toarray()) print(y_processed) print(rng) # Invalid input (contains NaN) X_invalid = np.array([[1, 2], [3, np.nan], [5, 6]]) try: X_processed, y_processed, rng = validate_and_preprocess(X_invalid, y) except ValueError as e: print(e) ``` Constraints - You must use the specified scikit-learn utilities for validation and preprocessing. Note Refer to the documentation of `sklearn.utils` for details on the mentioned utilities.","solution":"import numpy as np from sklearn.utils import check_X_y, check_random_state, assert_all_finite, as_float_array from scipy.sparse import issparse def validate_and_preprocess(X, y, multi_output=False, random_state=None): Validate and preprocess input arrays. Parameters: - X (array-like): Features array, can be a numpy array or a sparse matrix. - y (array-like): Target array. - multi_output (bool): If True, allows y to be in multi-output format. Default is False. - random_state (int or None): Seed for random number generator or None. Returns: - X_processed (array-like): Validated and preprocessed features array. - y_processed (array-like): Validated and preprocessed target array. - rng (RandomState): RandomState object initialized with the given random state. X, y = check_X_y(X, y, multi_output=multi_output, accept_sparse=True, ensure_2d=True) assert_all_finite(X) assert_all_finite(y) X = as_float_array(X) y = as_float_array(y) rng = check_random_state(random_state) return X, y, rng"},{"question":"**Objective:** To demonstrate your understanding of the `sndhdr` module and your ability to work with namedtuples and file I/O in Python. **Problem Statement:** You are provided with a list of filenames containing sound files. Your task is to write a function `filter_and_summarize_sound_files(filenames, min_framerate, min_channels)` that processes these files using the `sndhdr` module and filters out the ones that do not meet the specified criteria for minimum framerate and number of channels. For the files that meet the criteria, the function should return a summary containing the count of each file type found. **Function Signature:** ```python from typing import List, Dict def filter_and_summarize_sound_files(filenames: List[str], min_framerate: int, min_channels: int) -> Dict[str, int]: pass ``` **Input:** - `filenames` (List[str]): A list of filenames containing sound files. - `min_framerate` (int): The minimum framerate a sound file should have to be included in the summary. - `min_channels` (int): The minimum number of channels a sound file should have to be included in the summary. **Output:** - Returns a dictionary where keys are the sound file types and values are the counts of files of that type that meet the criteria. **Constraints:** - You may assume that all provided filenames are valid and accessible. - The function should only consider the sound files that meet both the minimum framerate and minimum channels criteria. - If no files meet the criteria, return an empty dictionary. **Example:** ```python # Example sound files list filenames = [\\"file1.wav\\", \\"file2.aiff\\", \\"file3.mp3\\", \\"file4.wav\\", \\"file5.au\\"] # Example function call: result = filter_and_summarize_sound_files(filenames, 44100, 2) # Example output: # Assuming file1.wav and file4.wav meet the criteria but the others do not, # and assuming both are of type \\"wav\\", the expected output would be: { \\"wav\\": 2 } ``` **Note:** Ensure that you handle cases where the `sndhdr` functions return `None` properly. **Hint:** You may find it helpful to use the `collections.Counter` for counting file types efficiently.","solution":"import sndhdr from typing import List, Dict from collections import Counter def filter_and_summarize_sound_files(filenames: List[str], min_framerate: int, min_channels: int) -> Dict[str, int]: Processes files using the sndhdr module and filters out files that do not meet the specified criteria for minimum framerate and number of channels. Returns a summary containing the count of each file type found. summary = Counter() for filename in filenames: sound_info = sndhdr.what(filename) if sound_info and sound_info.framerate >= min_framerate and sound_info.nchannels >= min_channels: summary[sound_info.filetype] += 1 return dict(summary)"},{"question":"# Objective: Assess the student\'s ability to use seaborn for data visualization, specifically focusing on creating and interpreting residual plots for various regression models. # Question: Using the seaborn library, analyze the \\"mpg\\" dataset to understand the relationship between the weight of a car and its fuel efficiency (measured in mpg). Your task is to create a function that: 1. Plots a simple residual plot. 2. Plots residuals of a higher-order regression (polynomial of degree 2). 3. Includes a LOWESS curve on one of the plots to highlight any underlying trends. Write a function `analyze_residuals(data: pd.DataFrame, x: str, y: str) -> None` that: - Takes in the dataset `data`, and the column names `x` (independent variable) and `y` (dependent variable). - Produces three plots: 1. A simple residual plot between `x` and `y`. 2. A higher-order residual plot with a polynomial of degree 2 between `x` and `y`. 3. A residual plot with a LOWESS curve highlighted in red. # Constraints: 1. Use seaborn for all plots. 2. Ensure the plots are clearly labeled with titles and axis labels. 3. Assume you might have to install seaborn if it\'s not already available in the environment. # Example Usage: ```python import pandas as pd import seaborn as sns def analyze_residuals(data: pd.DataFrame, x: str, y: str) -> None: sns.set_theme() # Simple residual plot sns.residplot(data=data, x=x, y=y) plt.title(\\"Simple Residual Plot\\") plt.xlabel(x) plt.ylabel(\\"Residuals\\") plt.show() # Higher-order residual plot sns.residplot(data=data, x=x, y=y, order=2) plt.title(\\"Higher-order Residual Plot (Degree 2)\\") plt.xlabel(x) plt.ylabel(\\"Residuals\\") plt.show() # Residual plot with LOWESS curve sns.residplot(data=data, x=x, y=y, lowess=True, line_kws={\'color\': \'r\'}) plt.title(\\"Residual Plot with LOWESS Curve\\") plt.xlabel(x) plt.ylabel(\\"Residuals\\") plt.show() # Example call: mpg = sns.load_dataset(\\"mpg\\") analyze_residuals(mpg, \\"weight\\", \\"mpg\\") ``` # Expected Output: Executing the above example should produce three separate residual plots as described.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def analyze_residuals(data: pd.DataFrame, x: str, y: str) -> None: sns.set_theme() # Simple residual plot plt.figure() sns.residplot(data=data, x=x, y=y) plt.title(\\"Simple Residual Plot\\") plt.xlabel(x) plt.ylabel(\\"Residuals\\") plt.show() # Higher-order residual plot plt.figure() sns.residplot(data=data, x=x, y=y, order=2) plt.title(\\"Higher-order Residual Plot (Degree 2)\\") plt.xlabel(x) plt.ylabel(\\"Residuals\\") plt.show() # Residual plot with LOWESS curve plt.figure() sns.residplot(data=data, x=x, y=y, lowess=True, line_kws={\'color\': \'r\'}) plt.title(\\"Residual Plot with LOWESS Curve\\") plt.xlabel(x) plt.ylabel(\\"Residuals\\") plt.show()"},{"question":"**Objective:** To assess the student\'s ability to work with pandas for data visualization and apply customization to various types of plots. **Task:** You are provided with a CSV file containing monthly sales data of four products over three years. Your task is to implement a Python function using pandas that reads the data, performs necessary transformations, and generates three different types of plots with specific customizations. **Function Signature:** ```python def visualize_sales_data(file_path: str) -> None: pass ``` **Details:** 1. **Input:** The function accepts a single argument: - `file_path` (str): The path to the CSV file containing the sales data. 2. **Output:** The function does not return anything but should display the plots. 3. **CSV File Format:** - The CSV file contains the following columns: - `Date` (YYYY-MM format): The month of the sales. - `Product_A` (int): Sales count of Product A. - `Product_B` (int): Sales count of Product B. - `Product_C` (int): Sales count of Product C. - `Product_D` (int): Sales count of Product D. Example: ``` Date,Product_A,Product_B,Product_C,Product_D 2020-01,100,150,200,250 2020-02,110,160,210,260 ... ``` **Tasks to Perform:** 1. **Read the data:** - Read the CSV file into a pandas DataFrame. - Parse the `Date` column to datetime format and set it as the index of the DataFrame. 2. **Generate and Customize Plots:** - **Line Plot:** - Plot the sales data of all four products in a single plot. - Customize the plot with different colors for each product\'s line. - Add a title \\"Monthly Sales Data (2020-2022)\\". - Label the x-axis as \\"Date\\" and y-axis as \\"Sales Count\\". - Display a legend and ensure the x-axis is formatted to show the month and year. - **Bar Plot:** - Create a bar plot showing total sales for each product over the entire period. - Use different colors for each product\'s bar for distinction. - Add a title \\"Total Sales per Product (2020-2022)\\" and label the y-axis as \\"Total Sales\\". - Display the exact sale numbers on top of each bar. - **Scatter Plot:** - Generate a scatter plot comparing the sales of `Product_A` and `Product_B` over the period. - Use the `Date` column to color-code the points sequentially (e.g., earlier dates in blue, later dates in red). - Add a title \\"Product_A vs. Product_B Sales\\" and label the x-axis as \\"Product_A Sales\\" and y-axis as \\"Product_B Sales\\". - Add a color bar to show the transition from early to late dates. **Constraints:** - You may assume the CSV file is well-formed and does not contain missing values in any of the sales columns. - Use only pandas and matplotlib for the visualization tasks. **Example Usage:** ```python # Assuming \'sales_data.csv\' is the file path visualize_sales_data(\'sales_data.csv\') ``` **Expected Outcome:** The function should display three customized plots as described above.","solution":"import pandas as pd import matplotlib.pyplot as plt import matplotlib.dates as mdates def visualize_sales_data(file_path: str) -> None: # Read the CSV file into a pandas DataFrame df = pd.read_csv(file_path) # Parse the Date column to datetime format and set it as the index df[\'Date\'] = pd.to_datetime(df[\'Date\']) df.set_index(\'Date\', inplace=True) # Line Plot plt.figure(figsize=(10, 5)) for product in df.columns: plt.plot(df.index, df[product], label=product) plt.title(\\"Monthly Sales Data (2020-2022)\\") plt.xlabel(\\"Date\\") plt.ylabel(\\"Sales Count\\") plt.legend() plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\'%Y-%m\')) plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=3)) plt.xticks(rotation=60) plt.tight_layout() plt.show() # Bar Plot total_sales = df.sum(axis=0) plt.figure(figsize=(10, 5)) bars = plt.bar(total_sales.index, total_sales.values, color=[\'b\', \'g\', \'r\', \'c\']) plt.title(\\"Total Sales per Product (2020-2022)\\") plt.ylabel(\\"Total Sales\\") for bar in bars: yval = bar.get_height() plt.text(bar.get_x() + bar.get_width()/2.0, yval, int(yval), va=\'bottom\') # Display value on top of bars plt.show() # Scatter Plot plt.figure(figsize=(10, 5)) scatter = plt.scatter(df[\'Product_A\'], df[\'Product_B\'], c=df.index.to_series().diff().dt.days.cumsum(), cmap=\'cool\') plt.title(\\"Product_A vs. Product_B Sales\\") plt.xlabel(\\"Product_A Sales\\") plt.ylabel(\\"Product_B Sales\\") plt.colorbar(scatter, label=\'Days Since Start\') plt.show()"},{"question":"**Objective:** Implement a function that demonstrates understanding and application of handling read and write buffers in Python. **Problem Statement:** You are required to implement a class `MemoryBufferHandler` in Python that provides functionalities to handle both read-only and writable memory buffers. The class should be able to read from and write to a buffer, and it should include methods that reflect compatibility checks similar to what was described in the old buffer protocol documentation. **Class Definition and Method Requirements:** 1. **Class:** `MemoryBufferHandler` 2. **Methods:** - `__init__(self, data: bytes)`: A constructor that initializes the handler with a byte buffer. - `get_read_only_buffer(self) -> memoryview`: Returns a memory view of the buffer that can only be read. - `get_writable_buffer(self) -> memoryview`: Returns a memory view that allows writing to the buffer. - `check_read_buffer_support(self) -> bool`: Checks if the buffer supports read-only access. **Constraints:** - You can assume the input data for the buffer initialization will always be of type `bytes`. - The writable buffer should only permit modifications if allowed, reflecting the `PyObject_AsWriteBuffer` function. **Example Usage:** ```python # Creating an instance of MemoryBufferHandler buffer_handler = MemoryBufferHandler(b\\"Initial buffer content\\") # Getting a read-only buffer read_only_buffer = buffer_handler.get_read_only_buffer() print(read_only_buffer.tobytes()) # Output: b\\"Initial buffer content\\" # Getting a writable buffer and modifying content writable_buffer = buffer_handler.get_writable_buffer() writable_buffer[0:7] = b\\"Updated\\" print(buffer_handler.get_read_only_buffer().tobytes()) # Output: b\\"Updated buffer content\\" # Checking if the buffer supports read-only access print(buffer_handler.check_read_buffer_support()) # Output: True ``` **Performance Requirements:** - The implementation should efficiently handle buffers of size up to 10MB. **Note:** - Raising appropriate Python exceptions for invalid operations aligns with the behavior of the old buffer protocol functions. Implement the `MemoryBufferHandler` class with the required methods, using modern Python buffer handling techniques.","solution":"class MemoryBufferHandler: def __init__(self, data: bytes): self._buffer = bytearray(data) def get_read_only_buffer(self) -> memoryview: return memoryview(self._buffer).toreadonly() def get_writable_buffer(self) -> memoryview: return memoryview(self._buffer) def check_read_buffer_support(self) -> bool: try: _ = memoryview(self._buffer).toreadonly() # Try to create a read-only buffer return True except Exception: return False"},{"question":"# Custom Import Loader Challenge **Objective**: In this challenge, you are required to create a custom module loader using the `importlib` package. The custom loader will be responsible for loading modules from a specified directory. Task 1. **Create a Custom File Loader**: Implement a class `CustomFileLoader` that inherits from `importlib.abc.SourceLoader`. This loader should be able to load Python source files from a given directory. 2. **Create a Custom Path Finder**: Implement a class `CustomPathFinder` that inherits from `importlib.abc.MetaPathFinder`. This finder should use the `CustomFileLoader` to find and load modules from the specified directory. 3. **Create the Custom Import System**: Set up the custom import system such that any modules located in the specified directory can be imported using the standard import statement. Specifications - The `CustomFileLoader` class should implement the following methods: - `get_filename(self, fullname)`: Return the filename for the module specified by `fullname`. - `get_data(self, path)`: Return the contents of the file specified by `path`. - The `CustomPathFinder` class should implement the following methods: - `find_spec(cls, fullname, path, target=None)`: Return a `ModuleSpec` for the module specified by `fullname`. Use the `CustomFileLoader` for loading the module. - The importable directory should be specified as a class attribute of `CustomPathFinder`. - Update `sys.meta_path` to include the `CustomPathFinder` such that it takes precedence over the default finders. Input - A directory path where the Python source files are located. Output - The ability to import modules from the specified directory using the standard import statement. Constraints - You should handle the case where the specified directory does not exist. - Ensure that the loader can handle the import of both packages and individual modules. - Performance considerations: Ensure that the custom loader does not significantly increase the load time compared to the standard import mechanism. Example Assume you have a directory `/path/to/custom/modules` containing `mymodule.py`. After setting up the custom import system, you should be able to do the following: ```python import mymodule print(mymodule.some_function()) ``` **Implementation** ```python import importlib.abc import importlib.util import sys import os class CustomFileLoader(importlib.abc.SourceLoader): def __init__(self, fullname, path): self.fullname = fullname self.path = path def get_filename(self, fullname): return self.path def get_data(self, path): with open(path, \'rb\') as file: return file.read() class CustomPathFinder(importlib.abc.MetaPathFinder): CUSTOM_MODULE_DIR = \'/path/to/custom/modules\' @classmethod def find_spec(cls, fullname, path=None, target=None): module_path = os.path.join(cls.CUSTOM_MODULE_DIR, *fullname.split(\'.\')) + \'.py\' if not os.path.exists(module_path): return None loader = CustomFileLoader(fullname, module_path) return importlib.util.spec_from_loader(fullname, loader) # Add the custom path finder to sys.meta_path sys.meta_path.insert(0, CustomPathFinder) # Test the custom import system import mymodule print(mymodule.some_function()) ```","solution":"import importlib.abc import importlib.util import sys import os class CustomFileLoader(importlib.abc.SourceLoader): def __init__(self, fullname, path): self.fullname = fullname self.path = path def get_filename(self, fullname): return self.path def get_data(self, path): with open(path, \'rb\') as file: return file.read() class CustomPathFinder(importlib.abc.MetaPathFinder): CUSTOM_MODULE_DIR = \'/path/to/custom/modules\' @classmethod def find_spec(cls, fullname, path=None, target=None): module_path = os.path.join(cls.CUSTOM_MODULE_DIR, *fullname.split(\'.\')) + \'.py\' if not os.path.exists(module_path): return None loader = CustomFileLoader(fullname, module_path) return importlib.util.spec_from_loader(fullname, loader) # Example of setting directory for use within tests def set_custom_module_dir(directory): CustomPathFinder.CUSTOM_MODULE_DIR = directory # Add the custom path finder to sys.meta_path if CustomPathFinder not in sys.meta_path: sys.meta_path.insert(0, CustomPathFinder)"},{"question":"# K-Nearest Neighbors Classification and Regression Implementation Objective: In this exercise, you will implement K-Nearest Neighbors (KNN) and Radius Neighbors algorithms for both classification and regression using the `sklearn.neighbors` module. The goal is to test your understanding of how to apply these methods in practical scenarios, taking into consideration the different algorithms and their performance implications. Tasks: 1. **Classification Task:** Implement a function `knn_classification` that: - Accepts the following inputs: - `X_train` (2D NumPy array): Training feature dataset. - `y_train` (1D NumPy array): Training labels. - `X_test` (2D NumPy array): Test feature dataset to classify. - `k` (int): Number of neighbors to use. - `algorithm` (str): Algorithm to use (one of `[\'auto\', \'ball_tree\', \'kd_tree\', \'brute\']`). - Returns: - `y_pred` (1D NumPy array): Predicted labels for the `X_test`. ```python from sklearn.neighbors import KNeighborsClassifier import numpy as np def knn_classification(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int, algorithm: str) -> np.ndarray: # Your code here ``` 2. **Regression Task:** Implement a function `knn_regression` that: - Accepts the following inputs: - `X_train` (2D NumPy array): Training feature dataset. - `y_train` (1D NumPy array): Training labels. - `X_test` (2D NumPy array): Test feature dataset to predict. - `k` (int): Number of neighbors to use. - `algorithm` (str): Algorithm to use (one of `[\'auto\', \'ball_tree\', \'kd_tree\', \'brute\']`). - Returns: - `y_pred` (1D NumPy array): Predicted labels for the `X_test`. ```python from sklearn.neighbors import KNeighborsRegressor import numpy as np def knn_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int, algorithm: str) -> np.ndarray: # Your code here ``` Constraints: - You should handle exceptions and invalid inputs gracefully. - Ensure your implementation works efficiently for large datasets. - Test both functions with appropriate datasets. Example: ```python import numpy as np # Example data X_train = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) y_train_class = np.array([1, 1, 1, 2, 2, 2]) y_train_reg = np.array([1.0, 0.5, 0.2, 1.1, 1.2, 1.3]) X_test = np.array([[-0.8, -1], [2.5, 1.5]]) # Test classification function print(knn_classification(X_train, y_train_class, X_test, k=3, algorithm=\'ball_tree\')) # Test regression function print(knn_regression(X_train, y_train_reg, X_test, k=2, algorithm=\'kd_tree\')) ``` Notes: - Refer to the `sklearn.neighbors` module documentation for detailed usage of classes and methods. - Validate the performance of your implementations using appropriate metric functions from `sklearn`.","solution":"from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor import numpy as np def knn_classification(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int, algorithm: str) -> np.ndarray: Perform KNN classification on the provided data. Parameters: X_train (np.ndarray): Training feature dataset. y_train (np.ndarray): Training labels. X_test (np.ndarray): Test feature dataset to classify. k (int): Number of neighbors to use. algorithm (str): Algorithm to use (one of [\'auto\', \'ball_tree\', \'kd_tree\', \'brute\']). Returns: np.ndarray: Predicted labels for the X_test. knn = KNeighborsClassifier(n_neighbors=k, algorithm=algorithm) knn.fit(X_train, y_train) y_pred = knn.predict(X_test) return y_pred def knn_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int, algorithm: str) -> np.ndarray: Perform KNN regression on the provided data. Parameters: X_train (np.ndarray): Training feature dataset. y_train (np.ndarray): Training labels. X_test (np.ndarray): Test feature dataset to predict. k (int): Number of neighbors to use. algorithm (str): Algorithm to use (one of [\'auto\', \'ball_tree\', \'kd_tree\', \'brute\']). Returns: np.ndarray: Predicted values for the X_test. knn = KNeighborsRegressor(n_neighbors=k, algorithm=algorithm) knn.fit(X_train, y_train) y_pred = knn.predict(X_test) return y_pred"},{"question":"**Coding Assessment Question** # Objective: You are tasked with implementing a script to train and evaluate a machine learning model using scikit-learn\'s toy datasets. Specifically, you will use the breast cancer dataset to train a classifier and evaluate its performance. # Instructions: 1. Load the breast cancer dataset using `load_breast_cancer()` from `sklearn.datasets`. 2. Split the dataset into training and testing sets. Use 80% of the data for training and the remaining 20% for testing. Ensure that the split is reproducible by setting the random state to 42. 3. Preprocess the data: - Standardize the features (mean=0, standard deviation=1). Use `StandardScaler` from `sklearn.preprocessing`. 4. Train a Logistic Regression model from scikit-learn using the training set. Use the default parameters of the `LogisticRegression` class. 5. Evaluate the model: - Calculate the accuracy of the model on the test set. - Generate the confusion matrix on the test set. 6. Implement the following function to encapsulate the above steps: ```python from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, confusion_matrix def breast_cancer_classification(): # Load dataset data = load_breast_cancer() X, y = data.data, data.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Train Logistic Regression model = LogisticRegression() model.fit(X_train_scaled, y_train) # Make predictions y_pred = model.predict(X_test_scaled) # Evaluate the model accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) return accuracy, conf_matrix # Sample usage: # accuracy, conf_matrix = breast_cancer_classification() # print(\\"Accuracy:\\", accuracy) # print(\\"Confusion Matrix:n\\", conf_matrix) ``` # Constraints: - Do not modify the input arguments of the function. - Do not change the random_state values. - Use only the scikit-learn library for the ML components. # Expected Output: The function should return the accuracy score (float) and the confusion matrix (2D array) when called. # Note: Make sure your code is efficient and follows best practices for code readability and documentation.","solution":"from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, confusion_matrix def breast_cancer_classification(): # Load dataset data = load_breast_cancer() X, y = data.data, data.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Train Logistic Regression model = LogisticRegression() model.fit(X_train_scaled, y_train) # Make predictions y_pred = model.predict(X_test_scaled) # Evaluate the model accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) return accuracy, conf_matrix"},{"question":"**Email Retrieval and Management Using poplib** **Objective:** Write a Python function `retrieve_emails` that connects to a POP3 server, authenticates using a username and password, retrieves all email messages, prints them, and then deletes the emails from the server. The function should use the `poplib` module for this purpose and handle any potential errors gracefully. **Requirements:** - Use the `poplib.POP3_SSL` class to connect to the POP3 server with SSL. - Authenticate using the provided username and password. - Retrieve and print all email messages on the server. - After fetching the messages, delete them from the server. - Ensure the connection to the server is properly closed after processing the emails. - Handle exceptions appropriately, ensuring proper error messages are printed and resources are cleaned up. **Function Signature:** ```python def retrieve_emails(host: str, username: str, password: str): pass ``` **Input:** - `host`: A string representing the hostname of the POP3 server. - `username`: A string representing the username for authentication. - `password`: A string representing the password for authentication. **Output:** - The function should print each line of each email retrieved from the server. **Constraints:** - Use SSL for connecting to the POP3 server. - Ensure that the function handles exceptions and error scenarios gracefully (e.g., incorrect username/password, server connection issues). **Example Usage:** ```python retrieve_emails(\'pop.example.com\', \'user@example.com\', \'password123\') ``` **Notes:** - The provided example and the command descriptions in the documentation can guide you in implementing the function. - Ensure that the mailbox is unlocked and the connection is closed properly after retrieving and deleting the emails. - You may use the `getpass` module for securely handling the password input if testing interactively.","solution":"import poplib import traceback def retrieve_emails(host: str, username: str, password: str): try: # Connect to the POP3 server using SSL mail_server = poplib.POP3_SSL(host) # Authenticate with the server mail_server.user(username) mail_server.pass_(password) # Get the messages from the server num_messages = len(mail_server.list()[1]) for i in range(num_messages): for msg_line in mail_server.retr(i+1)[1]: print(msg_line.decode()) print(\'-\' * 70) # Delete all messages after fetching for i in range(num_messages): mail_server.dele(i + 1) # Quit the connection to apply deletions mail_server.quit() except poplib.error_proto as e: print(f\\"POP3 Protocol error: {e}\\") except Exception as e: print(f\\"An error occurred: {traceback.format_exc()}\\") finally: try: mail_server.quit() except: pass"},{"question":"# Question: Custom Python Byte-Compilation Script You are tasked with writing a custom script that byte-compiles Python files from a given directory tree, similar to the functionality provided by the `compileall` module. The script should allow various customizations and should aim at enhancing your understanding of the `compileall` functionalities. Write a function `custom_compile_dir` with the following signature: ```python import re import pathlib def custom_compile_dir(dir: str, maxlevels: int = int, force: bool = False, exclude_regex: str = None, quiet: int = 0, workers: int = 1) -> bool: pass ``` Function Parameters: - `dir` (str): The directory to compile. - `maxlevels` (int): The maximum depth to which the directory tree should be traversed, default is `sys.getrecursionlimit()`. - `force` (bool): Force rebuild even if timestamps are up-to-date, default is `False`. - `exclude_regex` (str): A regex pattern to exclude certain files from compilation, default is `None`. - `quiet` (int): Control the verbosity of the compilation. `0` (default) for full verbosity, `1` for errors only, and `2` for complete silence. - `workers` (int): The number of worker threads to use for parallel compilation, default is `1`. Function Return: - `bool`: Return `True` if all files compile successfully, otherwise `False`. Constraints: - Make use of the `compileall` module where applicable. - Ensure that the function handles edge cases such as invalid directory paths and permission errors. - The regex pattern should correctly filter out files that match the specified pattern. - Implement parallel processing using the `workers` parameter. Example Usage: ```python # Example usage of the custom_compile_dir function result = custom_compile_dir(\'path/to/dir\', maxlevels=2, force=True, exclude_regex=r\'__pycache__\', quiet=1, workers=4) print(result) # True if compilation is successful, otherwise False ``` Performance Requirements: - The function should be efficient with respect to both time and memory. - Ensure that the parallel processing feature appropriately utilizes system resources. # Notes: - Use appropriate error handling to manage common issues like missing directories or inaccessible files. - Example test case demonstration and edge case handling must be included.","solution":"import compileall import re import pathlib import sys def custom_compile_dir(dir: str, maxlevels: int = sys.getrecursionlimit(), force: bool = False, exclude_regex: str = None, quiet: int = 0, workers: int = 1) -> bool: Byte-compiles Python files in a given directory tree with optional filters and settings. Parameters: dir (str): The directory to compile. maxlevels (int): The maximum depth to which the directory tree should be traversed. force (bool): Force rebuild even if timestamps are up-to-date. exclude_regex (str): A regex pattern to exclude certain files from compilation. quiet (int): Control the verbosity of the compilation (0 for full verbosity, 1 for errors only, 2 for complete silence). workers (int): The number of worker threads to use for parallel compilation. Returns: bool: True if all files compile successfully, otherwise False. try: path = pathlib.Path(dir) if not path.exists() or not path.is_dir(): raise ValueError(f\\"Invalid directory path: {dir}\\") if exclude_regex: # Pre-compile the regex pattern for later use exclude_pattern = re.compile(exclude_regex) else: exclude_pattern = None def custom_filter(path, filename): if exclude_pattern and exclude_pattern.search(filename): return False return True return compileall.compile_dir( dir, maxlevels=maxlevels, force=force, quiet=quiet, legacy=True, workers=workers, rx=exclude_pattern ) except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"**Problem Statement:** You are provided with a dataset of points in a two-dimensional space and their respective class labels. Your task is to implement a nearest neighbors classifier using scikit-learn to predict the class labels for a new set of points. In addition, you need to explore the impact of different nearest neighbors algorithms (brute-force, KD Tree, Ball Tree) on the classification performance. **Requirements:** 1. Implement a function `nearest_neighbors_classifier(X_train, y_train, X_test, n_neighbors, algorithm)` that takes the following inputs: - `X_train`: A 2D numpy array of shape (n_samples, 2) representing the coordinates of the training points. - `y_train`: A 1D numpy array of shape (n_samples,) representing the class labels of the training points. - `X_test`: A 2D numpy array of shape (n_query, 2) representing the coordinates of the test points for which you need to predict class labels. - `n_neighbors`: An integer representing the number of nearest neighbors to use for classification. - `algorithm`: A string specifying the algorithm to use for nearest neighbors search. Must be one of `[\'brute\', \'kd_tree\', \'ball_tree\']`. 2. The function should return a 1D numpy array of shape (n_query,) containing the predicted class labels for the test points. **Constraints:** - You must use the `KNeighborsClassifier` from scikit-learn. - Your implementation should handle edge cases such as when there are multiple classes tied for the nearest neighbors or when the test points are identical to training points. **Performance Requirements:** - Evaluate the performance of your classifier using a synthetic dataset. Generate a dataset of 1000 training points and 100 test points distributed randomly in a 2D space with 3 different class labels. - Compare the classification accuracy and computation time for each of the three algorithms (`brute`, `kd_tree`, `ball_tree`). **Example:** ```python import numpy as np from sklearn.model_selection import train_test_split from time import time # Generate synthetic dataset np.random.seed(0) X = np.random.rand(1000, 2) * 100 # 1000 points in a 100x100 space y = np.random.randint(0, 3, 1000) # 3 classes # Split the dataset X_train, X_test, y_train, _ = train_test_split(X, y, test_size=0.1, random_state=0) # Function definition def nearest_neighbors_classifier(X_train, y_train, X_test, n_neighbors, algorithm): from sklearn.neighbors import KNeighborsClassifier classifier = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=algorithm) classifier.fit(X_train, y_train) return classifier.predict(X_test) # Evaluate performance algorithms = [\'brute\', \'kd_tree\', \'ball_tree\'] for alg in algorithms: start_time = time() predictions = nearest_neighbors_classifier(X_train, y_train, X_test, n_neighbors=5, algorithm=alg) end_time = time() accuracy = np.sum(predictions == test_labels) / len(predictions) print(f\\"Algorithm: {alg}, Accuracy: {accuracy:.2f}, Time: {end_time - start_time:.4f} seconds\\") ``` **Note:** - In your final implementation, include necessary code to generate synthetic dataset, evaluate performance, and compare the algorithms as shown in the example.","solution":"import numpy as np from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from time import time import random def nearest_neighbors_classifier(X_train, y_train, X_test, n_neighbors, algorithm): Implements a nearest neighbors classifier with specified algorithm. Args: X_train (numpy.ndarray): A 2D array of shape (n_samples, 2) representing the training coordinates. y_train (numpy.ndarray): A 1D array of shape (n_samples,) representing the class labels of the training points. X_test (numpy.ndarray): A 2D array of shape (n_query, 2) representing the test coordinates. n_neighbors (int): Number of nearest neighbors to use. algorithm (str): The algorithm to use for nearest neighbors search (\'brute\', \'kd_tree\', \'ball_tree\'). Returns: numpy.ndarray: A 1D array of shape (n_query,) containing the predicted class labels for the test points. classifier = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=algorithm) classifier.fit(X_train, y_train) return classifier.predict(X_test) # Function to evaluate the algorithms def evaluate_algorithms(): # Generate synthetic dataset np.random.seed(0) X = np.random.rand(1000, 2) * 100 # 1000 points in a 100x100 space y = np.random.randint(0, 3, 1000) # 3 classes # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0) results = {} # Evaluate performance algorithms = [\'brute\', \'kd_tree\', \'ball_tree\'] for alg in algorithms: start_time = time() predictions = nearest_neighbors_classifier(X_train, y_train, X_test, n_neighbors=5, algorithm=alg) end_time = time() accuracy = np.sum(predictions == y_test) / len(predictions) results[alg] = {\'accuracy\': accuracy, \'time\': end_time - start_time} print(f\\"Algorithm: {alg}, Accuracy: {accuracy:.2f}, Time: {end_time - start_time:.4f} seconds\\") return results"},{"question":"# Question: Implementing a Custom Fault Handler with Timeout Objective: Your task is to create a custom fault handler using the `faulthandler` module that initializes at the start of the program, dumps tracebacks after a specified timeout, and supports enabling and disabling the handler through user input. Instructions: 1. **Initialization**: - Enable the fault handler at the start of the program. - Set a timeout to dump the tracebacks after 5 seconds. 2. **Function Implementation**: - Implement a function `initialize_fault_handler()` that enables the fault handler and schedules a traceback dump after a 5-second timeout. - Implement a function `handle_user_input()` that listens for user input: - \'enable\' to enable the fault handler. - \'disable\' to disable the fault handler. - \'exit\' to exit the program gracefully. 3. **Handling Output**: - Ensure the traceback is written to a file named `traceback_log.txt` in the current directory. The file should remain open until the program exits or the handler is disabled. Input: - No direct input for `initialize_fault_handler()`. - User inputs \'enable\', \'disable\', or \'exit\' for `handle_user_input()` function. Output: - Tracebacks are written to `traceback_log.txt`. Constraints: - You must handle the proper opening and closing of the file descriptor. - Ensure that the program does not terminate abruptly without writing the traceback if it is handling other tasks. Performance Requirements: - The traceback dump should occur reliably after the specified timeout. - The program should manage resources efficiently, especially when enabling and disabling the fault handler multiple times. Example: ```python import faulthandler import sys import os def initialize_fault_handler(): with open(\'traceback_log.txt\', \'w\') as log_file: faulthandler.enable(file=log_file) faulthandler.dump_traceback_later(5, file=log_file, exit=True) def handle_user_input(): while True: user_input = input(\\"Enter \'enable\', \'disable\', or \'exit\': \\").strip().lower() if user_input == \'enable\': with open(\'traceback_log.txt\', \'w\') as log_file: faulthandler.enable(file=log_file) print(\\"Fault handler enabled.\\") elif user_input == \'disable\': faulthandler.disable() print(\\"Fault handler disabled.\\") elif user_input == \'exit\': faulthandler.disable() print(\\"Exiting program.\\") break else: print(\\"Invalid input. Please try again.\\") if __name__ == \'__main__\': initialize_fault_handler() handle_user_input() ``` Ensure your solution correctly handles these cases and manages file descriptors efficiently.","solution":"import faulthandler import sys import os def initialize_fault_handler(): Initialize the fault handler to dump tracebacks after a 5-second timeout. The traceback will be written to \'traceback_log.txt\'. log_file = open(\'traceback_log.txt\', \'w\') faulthandler.enable(file=log_file) faulthandler.dump_traceback_later(5, file=log_file, exit=True) def handle_user_input(): Listen for user input to enable, disable, or exit. \'enable\' - Enable the fault handler. \'disable\' - Disable the fault handler. \'exit\' - Exit the program gracefully. log_file = None while True: user_input = input(\\"Enter \'enable\', \'disable\', or \'exit\': \\").strip().lower() if user_input == \'enable\': log_file = open(\'traceback_log.txt\', \'w\') faulthandler.enable(file=log_file) print(\\"Fault handler enabled.\\") elif user_input == \'disable\': faulthandler.disable() if log_file: log_file.close() log_file = None print(\\"Fault handler disabled.\\") elif user_input == \'exit\': faulthandler.disable() if log_file: log_file.close() print(\\"Exiting program.\\") break else: print(\\"Invalid input. Please try again.\\")"},{"question":"# Advanced Color Manipulation and Conversion You are given a list of colors represented in the RGB color space. Your task is to write a Python function that performs the following: 1. Convert each color from RGB to HLS. 2. Apply the following transformation to the HLS values: - If the lightness (L) is below 0.5, set it to 0. - Otherwise, set it to 1. 3. Convert the transformed HLS values back to RGB. 4. Output the transformed list of colors. Additionally, ensure that your solution can handle up to 10,000 colors efficiently. Function Signature ```python def transform_and_convert_colors(rgb_colors: List[Tuple[float, float, float]]) -> List[Tuple[float, float, float]]: pass ``` Input - `rgb_colors`: A list of tuples where each tuple represents a color in RGB space. Each tuple contains three floating point numbers between 0 and 1 (inclusive) that represent the red, green, and blue components of the color. Output - A list of tuples where each tuple represents a color in the transformed RGB space, after converting to HLS, applying the specified transformation to the lightness, and converting back to RGB. Constraints - The input list `rgb_colors` will contain up to 10,000 colors. - Each component of the input RGB tuples (`r`, `g`, `b`) will be between 0 and 1, inclusive. Example ```python from colorsys import rgb_to_hls, hls_to_rgb # Example function usage colors = [(0.2, 0.4, 0.4), (0.6, 0.2, 0.8), (0.1, 0.2, 0.3)] print(transform_and_convert_colors(colors)) # Expected Output: # [(0.0, 0.0, 0.0), (0.0, 1.0, 0.0), (0.4, 0.4, 1.0)] # Note: The actual output will depend on the exact implementation details. ``` **Hint**: Use the `colorsys` module to handle conversions between RGB and HLS.","solution":"from typing import List, Tuple from colorsys import rgb_to_hls, hls_to_rgb def transform_and_convert_colors(rgb_colors: List[Tuple[float, float, float]]) -> List[Tuple[float, float, float]]: transformed_colors = [] for color in rgb_colors: r, g, b = color h, l, s = rgb_to_hls(r, g, b) # Apply transformation to lightness if l < 0.5: l = 0 else: l = 1 # Convert back to RGB transformed_color = hls_to_rgb(h, l, s) transformed_colors.append(transformed_color) return transformed_colors"},{"question":"# Configuration File Parsing and Customization You are required to write a Python script that reads a configuration (INI) file, processes it, and provides customized access to its content using the `configparser` module. Problem Statement 1. **Read Configuration File:** Read an INI file containing various sections and options. Each section has multiple options with different datatypes (e.g., strings, integers, floats). 2. **Custom Access with Fallbacks:** Implement a function `read_config(config_file_path, section, option, fallback=None)` that: - Reads the specified `section` and `option` from the configuration file. - Returns the value of the option if it exists. - Returns the `fallback` value if the option is not present. 3. **Interpolation Handling:** Implement a function `interpolate_values(config_file_path, section)` that: - Reads the specified `section`. - Interpolates any references within the option values of that section. - Returns a dictionary with interpolated values. 4. **Type Conversion and Error Handling:** Ensure that: - Integer and float options in the configuration file are returned as their respective types. - If type conversion fails, the function should handle the error gracefully and return `None` for that option. Expected Inputs: 1. `config_file_path` (str): Path to the INI configuration file. 2. `section` (str): The section name in the configuration file. 3. `option` (str): The option name under the specified section. 4. `fallback` (any): The fallback value if the option is not found (default is `None`). Expected Outputs: 1. `read_config` function should return the value of the option or the fallback if the option is not present. 2. `interpolate_values` function should return a dictionary of interpolated values. Example Configuration File: ```ini [Settings] app_name = ConfigApp version = 1.0 max_users = 100 timeout = 30.5 [Database] user = admin password = secret host = localhost port = 5432 url = http://localhost:5432/db [Paths] base_dir = /home/user log_dir = %(base_dir)s/logs cache_dir = %(base_dir)s/cache ``` Example Usage: ```python from your_module import read_config, interpolate_values # Sample usage config_file = \\"path/to/config.ini\\" # Reading with fallback app_name = read_config(config_file, \\"Settings\\", \\"app_name\\", fallback=\\"DefaultApp\\") non_existent_option = read_config(config_file, \\"Settings\\", \\"non_existent\\", fallback=\\"default_value\\") # Interpolating values paths_dict = interpolate_values(config_file, \\"Paths\\") print(app_name) print(non_existent_option) print(paths_dict) ``` Constraints: - The file path provided should be valid and accessible. - The configuration file should follow standard INI format. Your implementation should demonstrate a clear understanding of file operations, parsing, interpolation, custom handling, and error handling using the `configparser` module.","solution":"import configparser def read_config(config_file_path, section, option, fallback=None): Reads the specified section and option from the configuration file. Returns the value of the option if it exists, otherwise returns the fallback value. config = configparser.ConfigParser() config.read(config_file_path) if config.has_section(section) and config.has_option(section, option): value = config.get(section, option) try: if value.isdigit(): return int(value) else: return float(value) except ValueError: return value else: return fallback def interpolate_values(config_file_path, section): Reads the specified section and interpolates any references within the option values of that section. Returns a dictionary with interpolated values. config = configparser.ConfigParser() config.read(config_file_path) if not config.has_section(section): return {} values = {} for option in config.options(section): try: values[option] = config.get(section, option, raw=False) if values[option].isdigit(): values[option] = int(values[option]) else: values[option] = float(values[option]) except ValueError: values[option] = config.get(section, option) except configparser.InterpolationMissingOptionError: values[option] = None return values"},{"question":"Problem Statement: You are required to implement a function that computes the boolean value representation of the sums of each sub-array (contiguous segment) of a given list. The function should return a list of booleans indicating whether the sum of the elements in each sub-array is non-zero (True) or zero (False). Function Signature: ```python def compute_boolean_subarray_sums(arr: List[int]) -> List[bool]: ``` Input: - `arr (List[int])`: A list of integers. The length of the list (`n`) satisfies `1 <= n <= 10`. Output: - A list of booleans, where each boolean represents whether the sum of the elements in each sub-array is non-zero. Constraints: - You must handle sub-arrays of all possible lengths. - The function should use the boolean allocation concepts from the provided Python boolean documentation. Example: ```python # Example 1: arr = [1, -1, 2] # Sub-arrays are: [1], [1, -1], [1, -1, 2], [-1], [-1, 2], [2] # Their sums are: 1, 0, 2, -1, 1, 2 # Corresponding booleans: True, False, True, True, True, True # Output: print(compute_boolean_subarray_sums(arr)) # Expected [True, False, True, True, True, True] # Example 2: arr = [0, 0, 0] # Sub-arrays are: [0], [0, 0], [0, 0, 0], [0], [0, 0], [0] # Their sums are: 0, 0, 0, 0, 0, 0 # Corresponding booleans: False, False, False, False, False, False # Output: print(compute_boolean_subarray_sums(arr)) # Expected [False, False, False, False, False, False] ``` Additional Notes: - Explore the use of `PyBool_FromLong(long v)` to create booleans from the sums calculated. - Make sure that the function should be able to handle edge cases where the arrays have only one element or where all elements are zero.","solution":"from typing import List def compute_boolean_subarray_sums(arr: List[int]) -> List[bool]: Computes boolean value representation of the sums of each sub-array. The function returns a list of booleans indicating whether the sum of the elements in each sub-array is non-zero (True) or zero (False). result = [] n = len(arr) # Iterate over all possible sub-array starting points for i in range(n): current_sum = 0 # Iterate over all possible sub-array ending points from the start point for j in range(i, n): current_sum += arr[j] result.append(current_sum != 0) return result"},{"question":"# Question: Email Processing and Conversion You are tasked with creating a Python function that processes a string representation of an email, extracts relevant components, and converts them into a JSON object. The function should also be able to take a JSON object conforming to the described structure and produce a string representation of a properly formatted email. Email String Format: The email string contains both headers and a body, which could be plain text or HTML. Headers include `From`, `To`, `Subject`, `Date`, and optionally `Cc` and `Bcc`. ``` From: sender@example.com To: recipient@example.com Subject: Test Email Date: Thu, 1 Apr 2023 12:30:00 +0000 Cc: cc@example.com Bcc: bcc@example.com This is the email body. ``` JSON Object Format: The JSON object should display the email in the following structure: ```json { \\"From\\": \\"sender@example.com\\", \\"To\\": \\"recipient@example.com\\", \\"Subject\\": \\"Test Email\\", \\"Date\\": \\"Thu, 1 Apr 2023 12:30:00 +0000\\", \\"Cc\\": \\"cc@example.com\\", \\"Bcc\\": \\"bcc@example.com\\", \\"Body\\": \\"This is the email body.\\" } ``` # Function Specifications Implement the following two functions: 1. `email_to_json(email_str: str) -> str`: - **Input**: A string representing an email. - **Output**: A JSON string representing the email. 2. `json_to_email(json_str: str) -> str`: - **Input**: A JSON string representing an email. - **Output**: A string representing the formatted email. Constraints: - All headers in the email string are separated by a newline (`n`). - The body of the email is separated from the headers by a blank line (two consecutive newline characters `nn`). - The JSON object must include all headers (even if their values are empty strings) and the body. Example ```python email_string = From: sender@example.com To: recipient@example.com Subject: Test Email Date: Thu, 1 Apr 2023 12:30:00 +0000 Cc: cc@example.com Bcc: bcc@example.com This is the email body. expected_json = \'\'\'{ \\"From\\": \\"sender@example.com\\", \\"To\\": \\"recipient@example.com\\", \\"Subject\\": \\"Test Email\\", \\"Date\\": \\"Thu, 1 Apr 2023 12:30:00 +0000\\", \\"Cc\\": \\"cc@example.com\\", \\"Bcc\\": \\"bcc@example.com\\", \\"Body\\": \\"This is the email body.\\" }\'\'\' assert email_to_json(email_string) == expected_json json_string = \'\'\'{ \\"From\\": \\"sender@example.com\\", \\"To\\": \\"recipient@example.com\\", \\"Subject\\": \\"Test Email\\", \\"Date\\": \\"Thu, 1 Apr 2023 12:30:00 +0000\\", \\"Cc\\": \\"cc@example.com\\", \\"Bcc\\": \\"bcc@example.com\\", \\"Body\\": \\"This is the email body.\\" }\'\'\' expected_email = From: sender@example.com To: recipient@example.com Subject: Test Email Date: Thu, 1 Apr 2023 12:30:00 +0000 Cc: cc@example.com Bcc: bcc@example.com This is the email body. assert json_to_email(json_string) == expected_email ``` Performance Requirements: - The functions should be able to process an email string of up to 50,000 characters within 2 seconds. Implement these functions considering the above requirements and example.","solution":"import json def email_to_json(email_str): Converts an email string to a JSON string. headers, body = email_str.split(\'nn\', 1) email_dict = {} for line in headers.split(\'n\'): key, value = line.split(\': \', 1) email_dict[key] = value email_dict[\'Body\'] = body.strip() return json.dumps(email_dict, indent=4) def json_to_email(json_str): Converts a JSON string to a formatted email string. email_dict = json.loads(json_str) headers = [] for key in [\'From\', \'To\', \'Subject\', \'Date\', \'Cc\', \'Bcc\']: if key in email_dict: headers.append(f\\"{key}: {email_dict[key]}\\") body = email_dict.get(\'Body\', \'\') return \'n\'.join(headers) + \'nn\' + body"},{"question":"# Coding Assessment Question **Objective:** In this task, you will demonstrate your understanding of the Python \\"pdb\\" module to debug a provided Python script programmatically. You are required to identify and fix bugs in the script using pdb module functions. **Task:** You are given a Python script `sample_script.py` which contains a function `calculate` that takes two integers as input and performs arithmetic operations. Unfortunately, this script contains bugs that cause incorrect calculations and a runtime exception. Your task is to write a Python script using the pdb module to: 1. Run the `calculate` function with provided inputs. 2. Set breakpoints to inspect and debug the critical parts of the function. 3. Identify and correctly fix the bugs in the script. 4. Implement the corrected version of the `calculate` function. **Provided `sample_script.py`:** ```python def calculate(a, b): result_add = a + b result_sub = a - b result_mul = a * b result_div = a / b # This line may cause a ZeroDivisionError print(f\\"Addition: {result_add}\\") print(f\\"Subtraction: {result_sub}\\") print(f\\"Multiplication: {result_mul}\\") print(f\\"Division: {result_div}\\") return { \\"add\\": result_add, \\"sub\\": result_sub, \\"mul\\": result_mul, \\"div\\": result_div, } if __name__ == \\"__main__\\": # Provide your test values for a and b calculate(10, 0) ``` **Your task:** Write a Python script `debug_script.py` that: 1. Imports the pdb module and the `sample_script.py`. 2. Uses the `pdb.run()` or other appropriate pdb functions to execute and debug the `calculate` function with the test values `a=10` and `b=0`. 3. Identifies the bugs in the calculate function, specifically the division by zero, and corrects the script to handle such cases gracefully. 4. Implements the corrected version of the `calculate` function within `debug_script.py`. **Expected Output:** When correctly executed, your `debug_script.py` should: - Successfully debug and correctly handle the division by zero scenario. - Output the correct calculations for addition, subtraction, multiplication, and handle the division gracefully by either skipping it or providing an appropriate message. - Print the results for the given test values (`a=10`, `b=0`). **Constraints:** - You cannot alter the `sample_script.py` directly. All changes should be handled programmatically within `debug_script.py`. - Ensure your script handles different edge cases gracefully, such as different sets of inputs for `a` and `b`. Good luck!","solution":"import pdb def calculate(a, b): result_add = a + b result_sub = a - b result_mul = a * b result_div = None if b != 0: result_div = a / b else: print(\\"Division by zero is not allowed.\\") print(f\\"Addition: {result_add}\\") print(f\\"Subtraction: {result_sub}\\") print(f\\"Multiplication: {result_mul}\\") if result_div is not None: print(f\\"Division: {result_div}\\") return { \\"add\\": result_add, \\"sub\\": result_sub, \\"mul\\": result_mul, \\"div\\": result_div, } if __name__ == \\"__main__\\": a, b = 10, 0 # Uncomment the next line to debug with pdb # pdb.run(\'calculate(a, b)\') print(calculate(a, b))"},{"question":"**Question: Implement and Manage a Remote Reference Protocol (RRef) Scenario** *Objective:* Demonstrate your understanding of the PyTorch distributed RPC framework by implementing a scenario that involves creating, using, and managing the lifecycle of Remote References (RRefs). *Background Information:* - `RRef` stands for Remote REFerence, meaning it can reference an object on a local or remote worker. The owner of the RRef holds the actual data, while user workers can create RRefs pointing to this owner-held data. - You should be familiar with the basic operations of creating, passing, and cleaning up RRefs using PyTorch\'s distributed RPC framework. *Scenario Description:* You need to simulate a distributed system with three workers where worker A performs computations and shares the computation results with workers B and C. Workers B and C will use these results for further computations. Upon completing their tasks, they need to handle the references properly to ensure cleanup. *Task:* 1. Initialize an RPC framework with three hypothetical workers: `workerA`, `workerB`, and `workerC`. 2. Implement a function on `workerA` that performs a computation (e.g., adds a tensor with ones) and creates an RRef to the result. 3. Implement functions on `workerB` and `workerC` that take an RRef, perform some operations on it, and log the results. 4. Ensure proper cleanup of the RRefs by following the reference counting protocol. 5. Demonstrate the entire workflow with example code. *Instructions:* 1. Define the computational functions and make them available on the respective workers. 2. Use RPC to initialize and manage worker communication. 3. Implement the RRef passing and usage according to the steps outlined in the scenario description. 4. Ensure that the reference counting and cleanup mechanisms are correctly followed. *Expected Input and Output Formats:* - **Input:** The input will come from initializing RPC with three workers and executing the defined functions. - **Output:** Ensure that the results of computations from `workerB` and `workerC` are logged, and that cleanup messages (if any) are appropriately handled. *Constraints:* - Follow the RRef protocol guarantees to ensure correct reference counting and lifecycle management. - Assume no permanent network failures. Transient failures should be handled as per the assumptions. *Performance Requirements:* - Ensure that the implemented functions demonstrate proper usage of RRef without major performance bottlenecks. - Code should be clear, well-documented, and include comments explaining the key steps. **Sample Code Structure:** ```python import torch import torch.distributed.rpc as rpc from torch.distributed.rpc import RRef def computation_on_A(tensor_data): # Simulate a computation by adding ones to the tensor result = tensor_data + torch.ones_like(tensor_data) return result def further_computation_on_B(rref): data = rref.to_here() result = data * 2 # Example of further computation print(f\\"Result on B: {result}\\") return result def further_computation_on_C(rref): data = rref.to_here() result = data + 3 # Example of further computation print(f\\"Result on C: {result}\\") return result def main(): rpc.init_rpc(\\"workerA\\", rank=0, world_size=3) # Simulate an RPC framework with three workers if rpc.get_worker_info().name == \\"workerA\\": tensor_data = torch.tensor([2.0, 3.0]) rref = rpc.remote(\\"workerA\\", computation_on_A, args=(tensor_data,)) # Send RRef to B and C workers rpc.rpc_async(\\"workerB\\", further_computation_on_B, args=(rref,)) rpc.rpc_async(\\"workerC\\", further_computation_on_C, args=(rref,)) rpc.shutdown() if __name__ == \\"__main__\\": main() ``` *Notes:* - The provided sample code is a partial implementation. You need to complete the missing parts and ensure proper cleanup of the RRefs. - Be sure to include all initialization and cleanup steps for the RPC framework. Good luck and ensure to test your code thoroughly to verify the handling of RRef lifecycles!","solution":"import torch import torch.distributed.rpc as rpc from torch.distributed.rpc import RRef def computation_on_A(tensor_data): # Simulate a computation by adding ones to the tensor result = tensor_data + torch.ones_like(tensor_data) print(f\\"Computation result on A: {result}\\") return result def further_computation_on_B(rref): data = rref.to_here() result = data * 2 # Example of further computation print(f\\"Result on B: {result}\\") return result def further_computation_on_C(rref): data = rref.to_here() result = data + 3 # Example of further computation print(f\\"Result on C: {result}\\") return result def main(): # Initialize an RPC framework with three workers rpc_backend_options = rpc.TensorPipeRpcBackendOptions(init_method=\'tcp://localhost:29500\') world_size = 3 rank = 0 worker_names = [\\"workerA\\", \\"workerB\\", \\"workerC\\"] rpc.init_rpc(worker_names[rank], rank=rank, world_size=world_size, rpc_backend_options=rpc_backend_options) if rank == 0: # workerA tensor_data = torch.tensor([2.0, 3.0]) rref = rpc.remote(\\"workerA\\", computation_on_A, args=(tensor_data,)) # Send RRef to B and C workers rpc.rpc_async(\\"workerB\\", further_computation_on_B, args=(rref,)) rpc.rpc_async(\\"workerC\\", further_computation_on_C, args=(rref,)) # Block until all processes complete rpc.shutdown() if __name__ == \\"__main__\\": main()"},{"question":"Objective Design and implement a network server and client using the `socket` module in Python. The server should accept multiple client connections and echo back any messages received. The client should be able to send messages to the server and receive the echoed responses. Requirements 1. **Server Implementation**: - Create a TCP server using `socket.AF_INET` and `socket.SOCK_STREAM`. - Bind the server to `localhost` on a specified port (e.g., 50007). - Listen for incoming connections and accept multiple clients. - For each client, spawn a new thread or process to handle communication. - Echo back any messages received from the clients. 2. **Client Implementation**: - Create a TCP client using `socket.AF_INET` and `socket.SOCK_STREAM`. - Connect to the server at `localhost` on the specified port. - Send a user-provided message to the server and print the echoed response. Input and Output Formats - The server should run continuously and handle multiple client connections concurrently. - The client should accept a string message from the user, send it to the server, and print the server\'s response. Constraints - Handle any exceptions that may occur, such as connection errors or timeouts. - Ensure proper cleanup of resources, such as closing sockets after use. Performance Requirements - The server should handle at least 5 concurrent client connections. - The client should gracefully handle server unavailability or connection issues. Example **Server**: ```python import socket import threading def handle_client(conn, addr): with conn: print(f\'Connected by {addr}\') while True: data = conn.recv(1024) if not data: break conn.sendall(data) def start_server(host=\'localhost\', port=50007): with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s: s.bind((host, port)) s.listen() print(f\'Server listening on {host}:{port}\') while True: conn, addr = s.accept() client_thread = threading.Thread(target=handle_client, args=(conn, addr)) client_thread.start() if __name__ == \\"__main__\\": start_server() ``` **Client**: ```python import socket def start_client(host=\'localhost\', port=50007): with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s: s.connect((host, port)) message = input(\\"Enter message to send: \\") s.sendall(message.encode()) data = s.recv(1024) print(f\'Received from server: {data.decode()}\') if __name__ == \\"__main__\\": start_client() ``` Ensure your implementation demonstrates the following: - Creating and binding a socket. - Listening for connections and handling multiple clients concurrently. - Sending and receiving data over the network. - Exception handling and resource cleanup.","solution":"import socket import threading def handle_client(conn, addr): with conn: print(f\'Connected by {addr}\') while True: data = conn.recv(1024) if not data: break conn.sendall(data) def start_server(host=\'localhost\', port=50007): with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s: s.bind((host, port)) s.listen() print(f\'Server listening on {host}:{port}\') while True: conn, addr = s.accept() client_thread = threading.Thread(target=handle_client, args=(conn, addr)) client_thread.start() if __name__ == \\"__main__\\": start_server() import socket def start_client(host=\'localhost\', port=50007): with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s: s.connect((host, port)) message = input(\\"Enter message to send: \\") s.sendall(message.encode()) data = s.recv(1024) print(f\'Received from server: {data.decode()}\') if __name__ == \\"__main__\\": start_client()"},{"question":"**Python Coding Assessment Question: Understanding and Using Built-in Constants** **Objective:** Implement a Python class that uses built-in constants correctly to provide meaningful functionality. **Problem Statement:** You are required to implement a class called `CustomMath`. This class will provide customized mathematical operations that should utilize the built-in constants properly. Specifically, you should handle potential operations with `NotImplemented`, and take care of the constraints on constants such as `None`. Here is a breakdown of the tasks: 1. **Class `CustomMath`:** - Implement two methods `custom_add` and `custom_multiply`. - Each method should take two arguments and attempt to perform addition or multiplication respectively. - If the passed arguments are not of type `int` or `float`, return `NotImplemented`. - If the operation is valid, return the result of the operation. 2. **Method: `custom_add`** - Input: Two arguments (can be int, float, or other types) - Output: The sum of both arguments if valid; otherwise `NotImplemented`. 3. **Method: `custom_multiply`** - Input: Two arguments (can be int, float, or other types) - Output: The product of both arguments if valid; otherwise `NotImplemented`. 4. **Considerations:** - Use `Ellipsis` (…) as a default argument in method definitions to emphasize extended slicing (hint: this is just for demonstration and not required to be used inside the method). - Ensure that the methods handle `NotImplemented` correctly without breaking Python\'s expected behavior for such operations. **Expected Input and Output:** - `custom_add(10, 5)``` should return `15`. - `custom_add(10, \'a\')` should return `NotImplemented`. - `custom_multiply(3, 4)` should return `12`. - `custom_multiply(3, [1, 2])` should return `NotImplemented`. **Constraints:** - Do not use any form of reassignment to `None`, `True`, `False`, `__debug__` as these are prohibited and will result in a `SyntaxError`. - The code must handle various types but should return `NotImplemented` when types are not suitable for the operations. ```python class CustomMath: def custom_add(self, a, b, default=...): # Your implementation here pass def custom_multiply(self, a, b, default=...): # Your implementation here pass # Example usage: cm = CustomMath() print(cm.custom_add(10, 5)) # Should output 15 print(cm.custom_add(10, \'a\')) # Should output NotImplemented print(cm.custom_multiply(3, 4)) # Should output 12 print(cm.custom_multiply(3, [1, 2]))# Should output NotImplemented ``` **Notes:** 1. Ensure that `NotImplemented` is used correctly according to Python\'s built-in mechanisms. 2. Ellipsis (`...`) in the method signature is just for demonstration and is not expected to be used in the method logic. 3. This problem assesses understanding of Python\'s special constants and their appropriate use in practice.","solution":"class CustomMath: def custom_add(self, a, b, default=...): if not isinstance(a, (int, float)) or not isinstance(b, (int, float)): return NotImplemented return a + b def custom_multiply(self, a, b, default=...): if not isinstance(a, (int, float)) or not isinstance(b, (int, float)): return NotImplemented return a * b"},{"question":"**Audio Processing Challenge** You have been provided with an AIFF audio file and your task is to implement a Python function that reads the audio file, processes it, and writes the processed audio to a new file. The processing involves normalizing the audio data (scaling the sample values) so that the loudest sample is at a specific target amplitude level without altering the relative levels between samples. **Function Signature:** ```python def normalize_audio(input_file: str, output_file: str, target_amplitude: int) -> None: Reads an AIFF audio file, normalizes the audio data to a target amplitude, and writes the processed audio to a new AIFF file. Args: input_file (str): The path to the input AIFF file. output_file (str): The path to the output (normalized) AIFF file. target_amplitude (int): The target amplitude level for the loudest sample. Returns: None ``` **Input:** 1. `input_file`: A string representing the path to the input AIFF file. 2. `output_file`: A string representing the path to the output AIFF file. 3. `target_amplitude`: An integer representing the desired maximum amplitude for the audio samples. **Output:** The function does not return anything. It writes the normalized audio data to the specified output file. **Constraints:** - The audio data can be either mono or stereo. - The function must preserve the number of channels, sample width, and frame rate of the original file. - The target amplitude will be in the range of 1 to the maximum value allowed by the sample width (e.g., for 16-bit samples, the maximum is 32767). **Example:** ```python normalize_audio(\\"input.aiff\\", \\"output.aiff\\", 32767) ``` **Technical Requirements:** 1. Open the AIFF file for reading. 2. Retrieve the file\'s parameters (number of channels, sample width, frame rate, etc.). 3. Read the audio frames and extract the sample values. 4. Find the maximum absolute sample value in the audio data. 5. Calculate the normalization factor. 6. Apply the normalization factor to every sample to achieve the target amplitude. 7. Write the normalized samples to a new AIFF file with the same parameters as the input file. Considerations for performance and efficiency should be included. Efficient handling of large audio files is expected, along with maintaining the integrity of audio quality.","solution":"import aifc import numpy as np def normalize_audio(input_file: str, output_file: str, target_amplitude: int) -> None: Reads an AIFF audio file, normalizes the audio data to a target amplitude, and writes the processed audio to a new AIFF file. Args: input_file (str): The path to the input AIFF file. output_file (str): The path to the output (normalized) AIFF file. target_amplitude (int): The target amplitude level for the loudest sample. Returns: None # Open the input AIFF file with aifc.open(input_file, \'r\') as aiff_in: # Get file parameters n_channels = aiff_in.getnchannels() sample_width = aiff_in.getsampwidth() frame_rate = aiff_in.getframerate() n_frames = aiff_in.getnframes() # Read frame data audio_frames = aiff_in.readframes(n_frames) # Convert the audio frame data to numpy array dtype = {1: np.int8, 2: np.int16, 4: np.int32}[sample_width] audio_data = np.frombuffer(audio_frames, dtype=dtype) # Find the maximum absolute sample value max_sample = np.max(np.abs(audio_data)) # Calculate the normalization factor normalization_factor = target_amplitude / max_sample # Apply normalization factor to each sample normalized_audio_data = (audio_data * normalization_factor).astype(dtype) # Convert numpy array back to bytes normalized_frames = normalized_audio_data.tobytes() # Write the normalized frames to the output AIFF file with aifc.open(output_file, \'w\') as aiff_out: # Set the same parameters as the input file aiff_out.setnchannels(n_channels) aiff_out.setsampwidth(sample_width) aiff_out.setframerate(frame_rate) aiff_out.setnframes(n_frames) # Write the normalized frame data aiff_out.writeframes(normalized_frames)"},{"question":"# Mapping Protocol Implementation in Python You are required to implement a `CustomMapping` class in Python that mimics the behavior of the provided mapping protocol functions. Your class should internally use a dictionary to store key-value pairs. Implement the following methods: 1. **`__init__(self)`**: Initialize an empty dictionary. 2. **`check(self)`**: Return `True` if the object supports the mapping protocol. 3. **`size(self)`**: Return the number of keys in the mapping. 4. **`get_item(self, key)`**: Return the value associated with the given key. Raise a `KeyError` if the key does not exist. 5. **`set_item(self, key, value)`**: Set the given value to the specified key. 6. **`del_item(self, key)`**: Delete the item associated with the given key. Raise a `KeyError` if the key does not exist. 7. **`has_key(self, key)`**: Return `True` if the key exists in the mapping, `False` otherwise. 8. **`keys(self)`**: Return a list of all keys in the mapping. 9. **`values(self)`**: Return a list of all values in the mapping. 10. **`items(self)`**: Return a list of key-value pairs as tuples. Example Usage: ```python mapping = CustomMapping() mapping.set_item(\'a\', 1) mapping.set_item(\'b\', 2) print(mapping.check()) # True print(mapping.size()) # 2 print(mapping.get_item(\'a\')) # 1 print(mapping.has_key(\'b\')) # True print(mapping.keys()) # [\'a\', \'b\'] print(mapping.values()) # [1, 2] print(mapping.items()) # [(\'a\', 1), (\'b\', 2)] mapping.del_item(\'a\') print(mapping.size()) # 1 print(mapping.get_item(\'a\')) # KeyError ``` Constraints: - You cannot use the built-in `dict` methods directly (like `__getitem__`, `__setitem__`, etc.) for getting, setting, and deleting items. - You should handle exceptions and edge cases appropriately, raising `KeyError` where necessary. Evaluation Criteria: - Correct implementation of each method. - Proper handling of exceptions. - Efficient and clean code following Python best practices.","solution":"class CustomMapping: def __init__(self): self.data = {} def check(self): return isinstance(self.data, dict) def size(self): return len(self.data) def get_item(self, key): if key in self.data: return self.data[key] else: raise KeyError(f\'Key {key} not found\') def set_item(self, key, value): self.data[key] = value def del_item(self, key): if key in self.data: del self.data[key] else: raise KeyError(f\'Key {key} not found\') def has_key(self, key): return key in self.data def keys(self): return list(self.data.keys()) def values(self): return list(self.data.values()) def items(self): return list(self.data.items())"},{"question":"You are tasked to design a multi-threaded system to simulate a customer support center using the `queue` module in Python. This customer support system handles support tickets submitted by users. Each ticket has an associated priority, determining the order in which it should be addressed (higher priority tickets are handled first). You need to implement three worker threads that handle the tickets concurrently. # Requirements 1. **Data Structure**: - Use a `PriorityQueue` from the `queue` module to store the support tickets, where each ticket is represented as a tuple `(priority, ticket_id)`. 2. **Thread Function**: - Implement a worker thread function that constantly fetches tickets from the `PriorityQueue` to process them. The function should: - Retrieve the highest priority ticket. - Print a statement indicating the ticket being processed. - Simulate processing by sleeping for 2 seconds. - Indicate that the ticket processing is complete using `queue.task_done()`. 3. **Main Function**: - Initialize the `PriorityQueue`. - Create and start three worker threads. - Simulate the submission of 20 tickets with random priorities (between 1 and 100) and ticket_ids (from 1 to 20). - Wait for all tickets to be processed by calling `queue.join()`. - Ensure to handle proper synchronization and thread termination. # Expected Input/Output - **Input**: None; the function should generate random tickets internally. - **Output**: Prints indicating: - The ticket being processed. - Completion of ticket processing. # Constraints - The system should run efficiently with minimal blocking. - Proper exception handling for `Empty` queue cases. - Ensure that each ticket is processed exactly once. # Performance Requirements - The solution should demonstrate good handling of multi-threaded executions, showcasing the ability to manage multiple tickets efficiently without deadlocks or race conditions. # Python 3.10 Integration - Use type hints to specify function signatures and data structures. # Example ```python import threading import queue import random import time def worker(q: queue.PriorityQueue): while True: try: priority, ticket_id = q.get(timeout=3) print(f\\"Processing ticket {ticket_id} with priority {priority}\\") time.sleep(2) print(f\\"Finished processing ticket {ticket_id}\\") q.task_done() except queue.Empty: break def main(): q = queue.PriorityQueue() threads = [] # Start 3 worker threads for _ in range(3): thread = threading.Thread(target=worker, args=(q,)) thread.daemon = True thread.start() threads.append(thread) # Submit 20 tickets with random priorities for ticket_id in range(1, 21): priority = random.randint(1, 100) q.put((priority, ticket_id)) # Wait until all tickets are processed q.join() print(\\"All tickets have been processed.\\") if __name__ == \\"__main__\\": main() ```","solution":"import threading import queue import random import time def worker(q: queue.PriorityQueue): Worker function to process support tickets. while True: try: priority, ticket_id = q.get(timeout=3) print(f\\"Processing ticket {ticket_id} with priority {priority}\\") time.sleep(2) # Simulate ticket processing print(f\\"Finished processing ticket {ticket_id}\\") q.task_done() except queue.Empty: break def main(): Main function to initialize the support center and start processing tickets. q = queue.PriorityQueue() # Create and start 3 worker threads threads = [] for _ in range(3): thread = threading.Thread(target=worker, args=(q,)) thread.daemon = True thread.start() threads.append(thread) # Simulate the submission of 20 tickets with random priorities for ticket_id in range(1, 21): priority = random.randint(1, 100) q.put((priority, ticket_id)) # Wait until all tickets are processed q.join() print(\\"All tickets have been processed.\\") if __name__ == \\"__main__\\": main()"},{"question":"# Advanced PyTorch Optimization using MPS Environment Variables In this assessment, you are required to write a Python script that demonstrates the following capabilities using PyTorch with Metal Performance Shaders (MPS) on a macOS system: 1. **Model Training with Environment Variable Configurations**: Set up a simple neural network model using PyTorch. Use the MPS backend for computations. Configure the environment variables to optimize performance and control memory allocation behaviors. 2. **Environment Variable Manipulation Function**: Implement a function `set_mps_environment_vars()` that sets the following PyTorch MPS environment variables: - `PYTORCH_MPS_HIGH_WATERMARK_RATIO` to `1.5` - `PYTORCH_MPS_LOW_WATERMARK_RATIO` to `1.2` - `PYTORCH_DEBUG_MPS_ALLOCATOR` to `1` - `PYTORCH_MPS_FAST_MATH` to `1` 3. **Training Script**: The script should include: - Importing necessary libraries. - Definition of a simple neural network. - Code to set the environment variables using `set_mps_environment_vars()`. - Code to train the network on random data. 4. **Output**: The script should print the training progress indicating that it is using the MPS backend. It should also print confirmation of the set environment variable values. # Constraints: - Ensure the script works only on macOS systems with MPS-compatible hardware. - The script should handle cases where the MPS backend is unavailable, falling back to CPU with a warning message. # Expected Input and Output Format: - **Input**: The script does not take any input from the user. It runs autonomously. - **Output**: Logs showing the training progress and confirmation of environment variable settings. # Your submission should include: 1. `set_mps_environment_vars()` function definition. 2. Full script demonstrating the functionality as described. ```python # Your code implementation here ``` This task will test your understanding of PyTorch integration with MPS, environment variable configurations, and your ability to implement a training loop in PyTorch.","solution":"import os import torch import torch.nn as nn import torch.optim as optim def set_mps_environment_vars(): Sets specific environment variables for optimizing PyTorch with MPS backend. os.environ[\'PYTORCH_MPS_HIGH_WATERMARK_RATIO\'] = \'1.5\' os.environ[\'PYTORCH_MPS_LOW_WATERMARK_RATIO\'] = \'1.2\' os.environ[\'PYTORCH_DEBUG_MPS_ALLOCATOR\'] = \'1\' os.environ[\'PYTORCH_MPS_FAST_MATH\'] = \'1\' class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def train_model(): # Setting environment variables set_mps_environment_vars() # Check device if torch.has_mps: device = torch.device(\\"mps\\") else: device = torch.device(\\"cpu\\") print(\\"MPS device not found, using CPU.\\") # Confirm environment variable settings print(\\"PYTORCH_MPS_HIGH_WATERMARK_RATIO: \\", os.environ.get(\'PYTORCH_MPS_HIGH_WATERMARK_RATIO\')) print(\\"PYTORCH_MPS_LOW_WATERMARK_RATIO: \\", os.environ.get(\'PYTORCH_MPS_LOW_WATERMARK_RATIO\')) print(\\"PYTORCH_DEBUG_MPS_ALLOCATOR: \\", os.environ.get(\'PYTORCH_DEBUG_MPS_ALLOCATOR\')) print(\\"PYTORCH_MPS_FAST_MATH: \\", os.environ.get(\'PYTORCH_MPS_FAST_MATH\')) # Training configuration model = SimpleNN().to(device) criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Dummy training data data = torch.randn(20, 10).to(device) # 20 samples, 10 features each target = torch.randn(20, 1).to(device) # 20 target values # Training loop for epoch in range(10): optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() print(f\'Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\') if __name__ == \\"__main__\\": train_model()"},{"question":"Objective Create a basic text-based application using the `curses` module. This application will simulate a simple status update dashboard for a server. The dashboard will display different status messages in various sections of the screen, and it should be updated in real-time based on user input. Task Implement the `ServerDashboard` class that initializes a curses environment and provides functionality to: 1. Create three non-overlapping windows on the screen labeled: \\"CPU Usage\\", \\"Memory Usage\\", and \\"Network Status\\". 2. Allow the user to press keys to update the content of these windows: - Pressing \'c\': Update CPU usage window with a random percentage between 1% and 100%. - Pressing \'m\': Update Memory usage window with a random percentage between 1% and 100%. - Pressing \'n\': Update Network status window with a random message (\\"Active\\", \\"Idle\\", \\"Error\\"). - Pressing \'q\': Quit the application and clean up the terminal state. Requirements 1. Use the `curses` module for managing windows and handling keypresses. 2. Proper initialization and clean-up of the curses environment. 3. Dynamic updating of window content based on user interactions. 4. Organized and readable code with comments explaining key sections. Example When running the application, the terminal screen may look like: ``` CPU Usage --------- 75% Memory Usage ------------ 60% Network Status -------------- Active ``` As the user presses keys, the relevant section updates with new random values or messages. Pressing \'q\' exits gracefully. Constraints - Ensure proper clean-up of the curses environment upon exiting to prevent terminal issues. - Use Python 3.10, respecting the features and syntax introduced in this version. Implementation Here\'s a basic template to get you started: ```python import curses import random class ServerDashboard: def __init__(self): self.stdscr = None self.cpu_win = None self.mem_win = None self.net_win = None def start(self): curses.wrapper(self.main) def main(self, stdscr): # Initial setup self.stdscr = stdscr curses.noecho() curses.cbreak() stdscr.keypad(True) # Create windows with necessary sizes height, width = stdscr.getmaxyx() self.cpu_win = curses.newwin(5, width // 3, 1, 0) self.mem_win = curses.newwin(5, width // 3, 1, width // 3) self.net_win = curses.newwin(5, width // 3, 1, 2 * (width // 3)) while True: key = stdscr.getch() if key == ord(\'q\'): break elif key == ord(\'c\'): self.update_cpu_usage() elif key == ord(\'m\'): self.update_mem_usage() elif key == ord(\'n\'): self.update_net_status() stdscr.refresh() # Cleanup curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() def update_cpu_usage(self): # Update the CPU usage window with random value usage = random.randint(1, 100) self.cpu_win.clear() self.cpu_win.addstr(0, 0, \\"CPU Usagen---------n{}%\\".format(usage)) self.cpu_win.refresh() def update_mem_usage(self): # Update the Memory usage window with random value usage = random.randint(1, 100) self.mem_win.clear() self.mem_win.addstr(0, 0, \\"Memory Usagen------------n{}%\\".format(usage)) self.mem_win.refresh() def update_net_status(self): # Update the Network status window with random status status = random.choice([\\"Active\\", \\"Idle\\", \\"Error\\"]) self.net_win.clear() self.net_win.addstr(0, 0, \\"Network Statusn--------------n{}\\".format(status)) self.net_win.refresh() # Execute the dashboard if __name__ == \'__main__\': ServerDashboard().start() ``` Submission Submit your implementation of the `ServerDashboard` class and any supporting code or explanations as required.","solution":"import curses import random class ServerDashboard: def __init__(self): self.stdscr = None self.cpu_win = None self.mem_win = None self.net_win = None def start(self): curses.wrapper(self.main) def main(self, stdscr): # Initial setup self.stdscr = stdscr curses.noecho() curses.cbreak() stdscr.keypad(True) # Calculate window dimensions based on terminal size height, width = stdscr.getmaxyx() win_height = height // 4 win_width = width // 3 # Create windows self.cpu_win = curses.newwin(win_height, win_width, 0, 0) self.mem_win = curses.newwin(win_height, win_width, 0, win_width) self.net_win = curses.newwin(win_height, win_width, 0, 2 * win_width) self.draw_borders() while True: key = stdscr.getch() if key == ord(\'q\'): break elif key == ord(\'c\'): self.update_cpu_usage() elif key == ord(\'m\'): self.update_mem_usage() elif key == ord(\'n\'): self.update_net_status() stdscr.refresh() # Cleanup curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() def draw_borders(self): self.cpu_win.box() self.mem_win.box() self.net_win.box() self.cpu_win.addstr(0, 1, \\" CPU Usage \\") self.mem_win.addstr(0, 1, \\" Memory Usage \\") self.net_win.addstr(0, 1, \\" Network Status \\") self.cpu_win.refresh() self.mem_win.refresh() self.net_win.refresh() def update_cpu_usage(self): # Update the CPU usage window with random value usage = random.randint(1, 100) self.cpu_win.clear() self.cpu_win.box() self.cpu_win.addstr(0, 1, \\" CPU Usage \\") self.cpu_win.addstr(2, 1, \\"{}%\\".format(usage)) self.cpu_win.refresh() def update_mem_usage(self): # Update the Memory usage window with random value usage = random.randint(1, 100) self.mem_win.clear() self.mem_win.box() self.mem_win.addstr(0, 1, \\" Memory Usage \\") self.mem_win.addstr(2, 1, \\"{}%\\".format(usage)) self.mem_win.refresh() def update_net_status(self): # Update the Network status window with random status status = random.choice([\\"Active\\", \\"Idle\\", \\"Error\\"]) self.net_win.clear() self.net_win.box() self.net_win.addstr(0, 1, \\" Network Status \\") self.net_win.addstr(2, 1, \\"{}\\".format(status)) self.net_win.refresh() # Execute the dashboard if __name__ == \'__main__\': ServerDashboard().start()"},{"question":"**Objective:** Create a Python program using the `asyncio` library to simulate a concurrent task manager. The program should: 1. Create several asynchronous functions (coroutines) with different delays. 2. Run these coroutines concurrently. 3. Handle timeouts and cancellations. 4. Aggregate the results of the coroutines. **Task:** You are tasked with implementing a task manager in Python using `asyncio`. The task manager should regularly perform multiple tasks concurrently and handle any issues such as task timeouts and cancellations robustly. **Specifications:** 1. **Coroutines:** - Implement three coroutines `task_a`, `task_b`, and `task_c` that simulate some work by using `asyncio.sleep()`. - Each coroutine should print a message when it starts and ends. Include the coroutine name and delay time in the message. - Each coroutine must have different sleep durations: - `task_a`: sleeps for 2 seconds - `task_b`: sleeps for 4 seconds - `task_c`: sleeps for 6 seconds 2. **Main Coroutine:** - Implement an asynchronous function `main` to manage the execution of these tasks. - Use `asyncio.gather` to run the three tasks concurrently. - Handle any exceptions that occur during the execution. - Ensure each task runs to completion or is cancelled after a given timeout of 5 seconds. - Collect the results of each task and print them. If a task was cancelled or failed, indicate this in the results. **Expected Input:** - The program does not take any external input. **Expected Output:** - The program should print the start and end messages of each coroutine. - The program should also print the results of each task after all tasks are done or cancelled due to timeout. **Constraints:** - Use `asyncio` for managing coroutines and tasks. - Ensure that no task runs longer than the specified timeout. **Example:** ```python import asyncio async def task_a(): print(\\"Task A started with delay 2 seconds\\") await asyncio.sleep(2) print(\\"Task A finished\\") return \\"A Result\\" async def task_b(): print(\\"Task B started with delay 4 seconds\\") await asyncio.sleep(4) print(\\"Task B finished\\") return \\"B Result\\" async def task_c(): print(\\"Task C started with delay 6 seconds\\") await asyncio.sleep(6) print(\\"Task C finished\\") return \\"C Result\\" async def main(): tasks = [ task_a(), task_b(), task_c() ] try: results = await asyncio.gather(*tasks, return_exceptions=True) for idx, result in enumerate(results): if isinstance(result, Exception): print(f\\"Task {chr(65+idx)} was cancelled or failed: {result}\\") else: print(f\\"Task {chr(65+idx)} Result: {result}\\") except asyncio.TimeoutError: print(\\"One or more tasks exceeded the timeout limit\\") asyncio.run(main()) ``` **Note:** - Ensure proper handling of timeouts and cancellations in the `main` function. - Use `asyncio.wait_for()` or similar mechanisms to implement task timeouts.","solution":"import asyncio async def task_a(): print(\\"Task A started with delay 2 seconds\\") await asyncio.sleep(2) print(\\"Task A finished\\") return \\"A Result\\" async def task_b(): print(\\"Task B started with delay 4 seconds\\") await asyncio.sleep(4) print(\\"Task B finished\\") return \\"B Result\\" async def task_c(): print(\\"Task C started with delay 6 seconds\\") await asyncio.sleep(6) print(\\"Task C finished\\") return \\"C Result\\" async def main(): timeout = 5 tasks = [ task_a(), task_b(), task_c() ] try: results = await asyncio.gather( *[asyncio.wait_for(task, timeout) for task in tasks], return_exceptions=True ) for idx, result in enumerate(results): if isinstance(result, Exception): print(f\\"Task {chr(65+idx)} was cancelled or failed: {result}\\") else: print(f\\"Task {chr(65+idx)} Result: {result}\\") except asyncio.TimeoutError: print(\\"One or more tasks exceeded the timeout limit\\") if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Seaborn Coding Challenge Objective: Write a Python function that utilizes the seaborn\'s `blend_palette` function to generate a range of color palettes and visualize them using a heatmap. Function Signature: ```python def generate_and_visualize_palettes(color_lists: list, as_cmap: bool = False): Generate color palettes based on specified color lists and visualize them using a heatmap. Parameters: color_lists (list of list of str): A list where each element is a list of color strings to blend. as_cmap (bool): Flag to decide if the output should be a continuous colormap. Default is False. Returns: None: The function should display the heatmap of the generated palettes. ``` Input: - `color_lists`: This is a list of lists, wherein each inner list contains colors in various formats (e.g., hex codes, color names). Ensure at least one list contains more than two colors. - `as_cmap`: Boolean flag determining if the output should be a continuous colormap (default is `False`). Output: - The function does not return any value but must display the heatmaps representing the generated color palettes using seaborn. Constraints: - Each color list within `color_lists` must have a minimum length of 2. Example: ```python color_lists = [ [\\"#FF0000\\", \\"#0000FF\\"], # Red to Blue [\\"#00FF00\\", \\"#FFFF00\\", \\"#0000FF\\"], # Green to Yellow to Blue [\\"xkcd:golden\\", \\"#123456\\", \\"#654321\\", \\".8\\"] # Arbitrarily long, with different formats ] generate_and_visualize_palettes(color_lists, as_cmap=False) generate_and_visualize_palettes([[\\"#FF0000\\", \\"#00FF00\\"]], as_cmap=True) ``` Hints: - Make use of the `sns.blend_palette` function to create the palettes. - Utilize seaborn\'s `heatmap` function to visualize the generated palettes. - Consider how to organize the palettes for clear and effective visualization.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def generate_and_visualize_palettes(color_lists: list, as_cmap: bool = False): Generate color palettes based on specified color lists and visualize them using a heatmap. Parameters: color_lists (list of list of str): A list where each element is a list of color strings to blend. as_cmap (bool): Flag to decide if the output should be a continuous colormap. Default is False. Returns: None: The function should display the heatmap of the generated palettes. # Check if each color list within color_lists has at least 2 colors for colors in color_lists: if len(colors) < 2: raise ValueError(\\"Each color list must contain at least 2 colors.\\") # Generate and visualize palettes num_palettes = len(color_lists) plt.figure(figsize=(10, num_palettes * 2)) for index, colors in enumerate(color_lists): palette = sns.blend_palette(colors, as_cmap=as_cmap) plt.subplot(num_palettes, 1, index + 1) sns.heatmap( np.arange(100).reshape(1, 100), cmap=palette if as_cmap else sns.color_palette(palette, 100), cbar=False, xticklabels=False, yticklabels=False ) plt.show() # Example usage commented out for testing # color_lists = [ # [\\"#FF0000\\", \\"#0000FF\\"], # Red to Blue # [\\"#00FF00\\", \\"#FFFF00\\", \\"#0000FF\\"], # Green to Yellow to Blue # [\\"xkcd:golden\\", \\"#123456\\", \\"#654321\\", \\".8\\"] # Arbitrarily long, with different formats # ] # generate_and_visualize_palettes(color_lists, as_cmap=False) # generate_and_visualize_palettes([[\\"#FF0000\\", \\"#00FF00\\"]], as_cmap=True)"},{"question":"Objective: Demonstrate your understanding of Python\'s `__main__` special name, writing idiomatic script code that can be used both as a standalone script and as an importable module. Problem Statement: You are tasked with creating a Python module named `converter.py` that includes functionality for converting temperatures between Celsius, Fahrenheit, and Kelvin scales. The module should be able to: 1. Convert temperatures from one scale to another. 2. Accept command-line arguments to perform a conversion when executed as a script. 3. Ensure that script-specific code does not execute when the module is imported. 4. Have clear and maintainable code, with separation of concerns between the conversion logic and command-line handling. Requirements: 1. **Conversion Functions**: - Implement the following conversion functions: ```python def celsius_to_fahrenheit(celsius: float) -> float: pass def fahrenheit_to_celsius(fahrenheit: float) -> float: pass def celsius_to_kelvin(celsius: float) -> float: pass def kelvin_to_celsius(kelvin: float) -> float: pass def fahrenheit_to_kelvin(fahrenheit: float) -> float: pass def kelvin_to_fahrenheit(kelvin: float) -> float: pass ``` 2. **Main Function**: - Implement a `main` function that handles command-line arguments using `sys.argv`. - The script should accept three command-line arguments: 1. Source temperature scale (`c`, `f`, or `k` for Celsius, Fahrenheit, or Kelvin). 2. Target temperature scale (`c`, `f`, or `k` for Celsius, Fahrenheit, or Kelvin). 3. Temperature value (a float). - Example usage: ```bash python3 converter.py c f 100 100°C is 212°F ``` 3. **Script Entry Point**: - Use the `if __name__ == \'__main__\'` construct to execute the `main` function, ensuring that the module can be imported without side-effects. Input Format: - The command-line arguments will always be given as the source scale, target scale, and temperature value. Output Format: - Print the converted temperature in a user-friendly format, indicating both the source and target scales. Constraints: - Ensure that the temperature conversions are accurate to at least two decimal places. - Handle invalid input gracefully by displaying an appropriate message. Example: ```python # Example content for `converter.py` import sys def celsius_to_fahrenheit(celsius: float) -> float: return celsius * 9/5 + 32 def fahrenheit_to_celsius(fahrenheit: float) -> float: return (fahrenheit - 32) * 5/9 def celsius_to_kelvin(celsius: float) -> float: return celsius + 273.15 def kelvin_to_celsius(kelvin: float) -> float: return kelvin - 273.15 def fahrenheit_to_kelvin(fahrenheit: float) -> float: return celsius_to_kelvin(fahrenheit_to_celsius(fahrenheit)) def kelvin_to_fahrenheit(kelvin: float) -> float: return celsius_to_fahrenheit(kelvin_to_celsius(kelvin)) def main() -> int: if len(sys.argv) != 4: print(\\"Usage: python converter.py <source scale> <target scale> <value>\\") return 1 source = sys.argv[1].lower() target = sys.argv[2].lower() try: value = float(sys.argv[3]) except ValueError: print(\\"Please enter a valid number for the temperature value.\\") return 1 if source == \'c\': if target == \'f\': result = celsius_to_fahrenheit(value) print(f\\"{value}°C is {result:.2f}°F\\") elif target == \'k\': result = celsius_to_kelvin(value) print(f\\"{value}°C is {result:.2f}K\\") else: print(\\"Invalid target scale. Use \'c\', \'f\', or \'k\'.\\") return 1 elif source == \'f\': if target == \'c\': result = fahrenheit_to_celsius(value) print(f\\"{value}°F is {result:.2f}°C\\") elif target == \'k\': result = fahrenheit_to_kelvin(value) print(f\\"{value}°F is {result:.2f}K\\") else: print(\\"Invalid target scale. Use \'c\', \'f\', or \'k\'.\\") return 1 elif source == \'k\': if target == \'c\': result = kelvin_to_celsius(value) print(f\\"{value}K is {result:.2f}°C\\") elif target == \'f\': result = kelvin_to_fahrenheit(value) print(f\\"{value}K is {result:.2f}°F\\") else: print(\\"Invalid target scale. Use \'c\', \'f\', or \'k\'.\\") return 1 else: print(\\"Invalid source scale. Use \'c\', \'f\', or \'k\'.\\") return 1 return 0 if __name__ == \'__main__\': sys.exit(main()) ``` Notes: - Focus on clear and correct implementation of the conversion logic. - Ensure that the `main` function handles all possible edge cases and inputs gracefully. - Test the module both by running it as a script and by importing its functions elsewhere.","solution":"import sys def celsius_to_fahrenheit(celsius: float) -> float: return celsius * 9/5 + 32 def fahrenheit_to_celsius(fahrenheit: float) -> float: return (fahrenheit - 32) * 5/9 def celsius_to_kelvin(celsius: float) -> float: return celsius + 273.15 def kelvin_to_celsius(kelvin: float) -> float: return kelvin - 273.15 def fahrenheit_to_kelvin(fahrenheit: float) -> float: return celsius_to_kelvin(fahrenheit_to_celsius(fahrenheit)) def kelvin_to_fahrenheit(kelvin: float) -> float: return celsius_to_fahrenheit(kelvin_to_celsius(kelvin)) def main() -> int: if len(sys.argv) != 4: print(\\"Usage: python converter.py <source scale> <target scale> <value>\\") return 1 source = sys.argv[1].lower() target = sys.argv[2].lower() try: value = float(sys.argv[3]) except ValueError: print(\\"Please enter a valid number for the temperature value.\\") return 1 result = None if source == \'c\': if target == \'f\': result = celsius_to_fahrenheit(value) print(f\\"{value}°C is {result:.2f}°F\\") elif target == \'k\': result = celsius_to_kelvin(value) print(f\\"{value}°C is {result:.2f}K\\") elif source == \'f\': if target == \'c\': result = fahrenheit_to_celsius(value) print(f\\"{value}°F is {result:.2f}°C\\") elif target == \'k\': result = fahrenheit_to_kelvin(value) print(f\\"{value}°F is {result:.2f}K\\") elif source == \'k\': if target == \'c\': result = kelvin_to_celsius(value) print(f\\"{value}K is {result:.2f}°C\\") elif target == \'f\': result = kelvin_to_fahrenheit(value) print(f\\"{value}K is {result:.2f}°F\\") if result is None: print(\\"Invalid source or target scale. Use \'c\', \'f\', or \'k\'.\\") return 1 return 0 if __name__ == \'__main__\': sys.exit(main())"},{"question":"**Audio Sample Rate Converter Using `sunau` Module** **Objective**: Implement a Python function that converts the sample rate of an audio file in Sun AU format and saves the modified audio to a new file using the `sunau` module. # Function Signature ```python def convert_sample_rate(input_file: str, output_file: str, new_sample_rate: int): Converts the sample rate of the given Sun AU audio file and saves the result to a new file. Parameters: input_file (str): The path to the input Sun AU audio file. output_file (str): The path to the output Sun AU audio file with the new sample rate. new_sample_rate (int): The desired sample rate for the output file. Returns: None ``` # Input - `input_file`: A string representing the path to the input Sun AU audio file. The file\'s format conforms to the Sun AU specifications. - `output_file`: A string representing the path to the output Sun AU audio file to be created or overwritten. - `new_sample_rate`: An integer specifying the new sample rate for the audio file. # Output - The function should save the audio data with the new sample rate to the specified output file. It does not return any value. # Constraints - The input file must be a valid Sun AU audio file. - If the input file cannot be opened or is not a valid Sun AU file, the function should raise a `sunau.Error`. # Requirements - You must use the `sunau` module to read the input file and write the output file. - Adjust the header of the output file to reflect the new sample rate. - Handle different sample widths and encoding as per the source file. - Ensure the correct number of frames and data size are set in the output file. # Example Usage ```python convert_sample_rate(\\"input.au\\", \\"output.au\\", 16000) ``` # Note - For simplicity, you can assume that the audio data\'s encoding is in a format that allows straightforward resampling (e.g., linear encoding). - For the purposes of this exercise, you can use a simple algorithm for resampling (e.g., by skipping or repeating frames). Advanced resampling techniques are beyond the scope of this question. # Bonus If you want to challenge yourself further, consider implementing more advanced resampling algorithms or handling various encodings more gracefully.","solution":"import sunau import numpy as np def convert_sample_rate(input_file: str, output_file: str, new_sample_rate: int): Converts the sample rate of the given Sun AU audio file and saves the result to a new file. Parameters: input_file (str): The path to the input Sun AU audio file. output_file (str): The path to the output Sun AU audio file with the new sample rate. new_sample_rate (int): The desired sample rate for the output file. Returns: None # Open the input AU file with sunau.open(input_file, \'rb\') as infile: n_channels = infile.getnchannels() sampwidth = infile.getsampwidth() framerate = infile.getframerate() n_frames = infile.getnframes() comp_type = infile.getcomptype() comp_name = infile.getcompname() # Read the audio data frames = infile.readframes(n_frames) # Convert the frames to numpy array audio_data = np.frombuffer(frames, dtype=np.int16) # Calculate the new number of frames ratio = new_sample_rate / framerate new_n_frames = int(n_frames * ratio) # Resample the audio data new_audio_data = np.interp( np.linspace(0, len(audio_data), new_n_frames, endpoint=False), np.arange(len(audio_data)), audio_data ).astype(np.int16) # Open the output AU file with sunau.open(output_file, \'wb\') as outfile: # Write the header with the new sample rate outfile.setnchannels(n_channels) outfile.setsampwidth(sampwidth) outfile.setframerate(new_sample_rate) outfile.setnframes(new_n_frames) outfile.setcomptype(comp_type, comp_name) # Write the frames to the output file outfile.writeframes(new_audio_data.tobytes())"},{"question":"**Problem Statement:** You are tasked with building a registry system to keep track of vehicles entering a parking lot. Each vehicle has an entry time and a type (e.g., car, truck, motorcycle). Since the parking fee differs for each type and depends on the duration of the stay, the registry needs to be maintained in sorted order of entry time for efficient processing. Write a Python function to perform the following operations on the parking lot registry: 1. Add a new vehicle to the registry, maintaining the sorted order by entry time. 2. Find and return the first vehicle that entered after a given time. 3. Find and return the first vehicle of a specified type that entered after a given time. The parking lot registry is represented as a list of tuples where each tuple contains (entry_time, vehicle_type). The entry time is an integer representing the time in minutes from midnight. The vehicle_type is a string such as \\"car\\", \\"truck\\", or \\"motorcycle\\". Implement the following functions: 1. **add_vehicle(registry, entry_time, vehicle_type)**: - Inputs: - registry: List[Tuple[int, str]]: The current registry of vehicles. - entry_time: int: The entry time of the new vehicle. - vehicle_type: str: The type of the new vehicle. - Output: None. The function modifies the registry in place. 2. **find_vehicle_after_time(registry, time)**: - Inputs: - registry: List[Tuple[int, str]]: The current registry of vehicles. - time: int: The time after which to find the first vehicle. - Output: Tuple[int, str] or None: The vehicle that entered after the given time, or None if no such vehicle exists. 3. **find_vehicle_of_type_after_time(registry, time, vehicle_type)**: - Inputs: - registry: List[Tuple[int, str]]: The current registry of vehicles. - time: int: The time after which to find the first vehicle of the specified type. - vehicle_type: str: The type of vehicle to find. - Output: Tuple[int, str] or None: The vehicle of the specified type that entered after the given time, or None if no such vehicle exists. **Constraints:** - The registry list can be of size (0 leq n leq 10^5). - The function calls need to be efficient in both time and space complexity. **Examples:** ```python from bisect import insort, bisect_right def add_vehicle(registry, entry_time, vehicle_type): insort(registry, (entry_time, vehicle_type)) def find_vehicle_after_time(registry, time): idx = bisect_right(registry, (time, \'\')) if idx < len(registry): return registry[idx] return None def find_vehicle_of_type_after_time(registry, time, vehicle_type): idx = bisect_right(registry, (time, \'\')) for i in range(idx, len(registry)): if registry[i][1] == vehicle_type: return registry[i] return None # Example usage: registry = [] add_vehicle(registry, 480, \\"car\\") add_vehicle(registry, 300, \\"truck\\") add_vehicle(registry, 510, \\"motorcycle\\") print(find_vehicle_after_time(registry, 400)) # Output: (480, \'car\') print(find_vehicle_of_type_after_time(registry, 400, \\"motorcycle\\")) # Output: (510, \'motorcycle\') ``` Implement these functions considering the performance constraints and the nature of the bisect module\'s operations.","solution":"from bisect import insort, bisect_right def add_vehicle(registry, entry_time, vehicle_type): Adds a new vehicle to the registry, maintaining sorted order by entry time. :param registry: List[Tuple[int, str]] - The current registry of vehicles. :param entry_time: int - The entry time of the new vehicle. :param vehicle_type: str - The type of the new vehicle. :return: None insort(registry, (entry_time, vehicle_type)) def find_vehicle_after_time(registry, time): Finds and returns the first vehicle that entered after a given time. :param registry: List[Tuple[int, str]] - The current registry of vehicles. :param time: int - The time after which to find the first vehicle. :return: Tuple[int, str] or None idx = bisect_right(registry, (time, \'\')) if idx < len(registry): return registry[idx] return None def find_vehicle_of_type_after_time(registry, time, vehicle_type): Finds and returns the first vehicle of a specified type that entered after a given time. :param registry: List[Tuple[int, str]] - The current registry of vehicles. :param time: int - The time after which to find the first vehicle of the specified type. :param vehicle_type: str - The type of vehicle to find. :return: Tuple[int, str] or None idx = bisect_right(registry, (time, \'\')) for i in range(idx, len(registry)): if registry[i][1] == vehicle_type: return registry[i] return None"},{"question":"<|Analysis Begin|> The provided documentation is for the `bz2` module, which supports bzip2 compression. It includes a range of functionality such as: 1. **File-based Compression/Decompression**: - `bz2.open()`: Open a bzip2-compressed file. - `bz2.BZ2File`: A class for handling bzip2-compressed files. 2. **Incremental Compression/Decompression**: - `bz2.BZ2Compressor`: An object for incrementally compressing data. - `bz2.BZ2Decompressor`: An object for incrementally decompressing data. 3. **One-shot Compression/Decompression**: - `bz2.compress()`: Compress data in one shot. - `bz2.decompress()(): Decompress data in one shot. The `bz2` module provides methods to work with both in-memory data (using `compress` and `decompress`), and file-based data (using `open` and the `BZ2File` class). The distinction between incremental and one-shot compression/decompression is a critical aspect of using this library effectively. Given these functionalities, an advanced Python coding assessment question can be designed around creating a utility that combines different aspects of the `bz2` module, emphasizing both in-memory and file-based operations, as well as one-shot and incremental compression. <|Analysis End|> <|Question Begin|> # Coding Assessment Question: BZ2 Compression Utility You are required to implement a Python class `BZ2CompressionUtility` that utilizes the `bz2` module to perform the following tasks: 1. **In-memory One-shot Compression/Decompression**: - Implement a method `compress_data(data: bytes, compresslevel: int = 9) -> bytes` that compresses the given in-memory byte data using the specified compression level. - Implement a method `decompress_data(compressed_data: bytes) -> bytes` that decompresses the given in-memory compressed byte data. 2. **File-based Compression/Decompression**: - Implement a method `compress_file(input_filename: str, output_filename: str, compresslevel: int = 9) -> None` that reads the data from the input file, compresses it, and writes the compressed data to the output file. - Implement a method `decompress_file(compressed_filename: str, output_filename: str) -> None` that reads the compressed data from a file, decompresses it, and writes the decompressed data to the output file. 3. **Incremental Compression/Decompression**: - Implement a method `incremental_compress(data_chunks: list[bytes], compresslevel: int = 9) -> bytes` that takes a list of byte data chunks, compresses them incrementally, and returns the compressed data. - Implement a method `incremental_decompress(data_chunks: list[bytes]) -> bytes` that takes a list of compressed byte data chunks, decompresses them incrementally, and returns the decompressed data. Constraints: - The `compresslevel` parameter must be an integer between 1 and 9. - Ensure that your implementation handles large files and data efficiently. Example Usage: ```python utility = BZ2CompressionUtility() # One-shot compression/decompression data = b\\"Example data to compress\\" compressed_data = utility.compress_data(data) decompressed_data = utility.decompress_data(compressed_data) assert data == decompressed_data # File-based compression/decompression utility.compress_file(\\"example.txt\\", \\"example.bz2\\") utility.decompress_file(\\"example.bz2\\", \\"example_out.txt\\") # Incremental compression/decompression chunks = [b\\"chunk1\\", b\\"chunk2\\", b\\"chunk3\\"] compressed_chunks = utility.incremental_compress(chunks) decompressed_data = utility.incremental_decompress([compressed_chunks]) assert b\\"chunk1chunk2chunk3\\" == decompressed_data ``` Implement the `BZ2CompressionUtility` class and its methods as described above.","solution":"import bz2 class BZ2CompressionUtility: def compress_data(self, data: bytes, compresslevel: int = 9) -> bytes: if not (1 <= compresslevel <= 9): raise ValueError(\\"compresslevel must be an integer between 1 and 9\\") return bz2.compress(data, compresslevel) def decompress_data(self, compressed_data: bytes) -> bytes: return bz2.decompress(compressed_data) def compress_file(self, input_filename: str, output_filename: str, compresslevel: int = 9) -> None: if not (1 <= compresslevel <= 9): raise ValueError(\\"compresslevel must be an integer between 1 and 9\\") with open(input_filename, \'rb\') as infile, bz2.BZ2File(output_filename, \'wb\', compresslevel=compresslevel) as outfile: for data in iter(lambda: infile.read(4096), b\'\'): outfile.write(data) def decompress_file(self, compressed_filename: str, output_filename: str) -> None: with bz2.BZ2File(compressed_filename, \'rb\') as infile, open(output_filename, \'wb\') as outfile: for data in iter(lambda: infile.read(4096), b\'\'): outfile.write(data) def incremental_compress(self, data_chunks: list[bytes], compresslevel: int = 9) -> bytes: if not (1 <= compresslevel <= 9): raise ValueError(\\"compresslevel must be an integer between 1 and 9\\") compressor = bz2.BZ2Compressor(compresslevel) compressed_data = b\\"\\" for chunk in data_chunks: compressed_data += compressor.compress(chunk) compressed_data += compressor.flush() return compressed_data def incremental_decompress(self, data_chunks: list[bytes]) -> bytes: decompressor = bz2.BZ2Decompressor() decompressed_data = b\\"\\" for chunk in data_chunks: decompressed_data += decompressor.decompress(chunk) return decompressed_data"},{"question":"# XML Parsing and Filtering with `xml.dom.pulldom` Description You are given an XML document containing product details. Each product has a name and a price. Your task is to write a Python function that processes the XML document, filters out products based on a minimum price threshold, and returns a list of product names that meet the criteria. Use the `xml.dom.pulldom` module to efficiently parse and filter the XML document. Function Signature ```python def filter_expensive_products(xml_data: str, min_price: int) -> list[str]: pass ``` Input - `xml_data` (str): A string representation of the XML document. The XML document has the following structure: ```xml <products> <product> <name>Product1</name> <price>10</price> </product> <product> <name>Product2</name> <price>100</price> </product> ... </products> ``` - `min_price` (int): The minimum price threshold for filtering products. Output - List of strings: A list containing the names of the products that have a price greater than or equal to `min_price`. Constraints - The XML string is well-formed and contains properly nested elements. - The `min_price` is a non-negative integer. - Product prices are non-negative integers. Example ```python xml_input = \'\'\' <products> <product> <name>ProductA</name> <price>50</price> </product> <product> <name>ProductB</name> <price>150</price> </product> <product> <name>ProductC</name> <price>30</price> </product> </products> \'\'\' min_price = 50 print(filter_expensive_products(xml_input, min_price)) # Output: [\'ProductA\', \'ProductB\'] ``` Notes - You should use the `xml.dom.pulldom` module and its methods (`parseString`, `getEvent`, and `expandNode`) to parse and filter the XML document. Hints - Use the `doc.expandNode(node)` method to convert a partially loaded node to a subtree with all its children. - By iterating through events, you can efficiently pick only the nodes and data you need from the XML stream.","solution":"from xml.dom import pulldom def filter_expensive_products(xml_data: str, min_price: int) -> list[str]: doc = pulldom.parseString(xml_data) product_names = [] for event, node in doc: if event == \'START_ELEMENT\' and node.tagName == \'product\': doc.expandNode(node) name_elem = node.getElementsByTagName(\'name\')[0] price_elem = node.getElementsByTagName(\'price\')[0] name = name_elem.childNodes[0].data price = int(price_elem.childNodes[0].data) if price >= min_price: product_names.append(name) return product_names"},{"question":"**Objective:** Create and run a threaded TCP server using Python\'s `socketserver` module. Your server should handle incoming client requests and maintain a continuous log of all client interactions. **Task:** 1. Implement a custom request handler by subclassing `socketserver.BaseRequestHandler`. This handler should: - Receive data from the client. - Log the received data along with the client address and the time at which the data was received. - Respond to the client by sending back the received data in uppercase. 2. Create a threaded TCP server class by subclassing `socketserver.ThreadingMixIn` and `socketserver.TCPServer`. 3. Ensure the server runs indefinitely, handles multiple clients concurrently, and shuts down gracefully upon receiving a keyboard interrupt (Ctrl+C). 4. Implement a client script that will connect to the server and send a message, then receive and print the server\'s response. **Requirements:** - Use `socketserver` module. - The server should log information to a file named `server_log.txt`. - Ensure thread safety for logging to avoid data corruption. - The client message should be passed as a command-line argument. **Constraints:** - The server should run on `localhost` and any available port. - The log file should be appended with new entries instead of overwriting existing data. **Performance Requirements:** - The server should be able to handle at least 10 concurrent client connections. - Ensure minimal delay in handling client requests. **Example:** **Server Script (server.py):** ```python import socketserver import threading import logging from datetime import datetime # Set up logging logging.basicConfig(filename=\'server_log.txt\', level=logging.INFO, format=\'%(message)s\') class MyTCPHandler(socketserver.BaseRequestHandler): def handle(self): data = self.request.recv(1024).strip() current_time = datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') client_address = self.client_address[0] # Logging the data logging.info(f\\"{current_time} {client_address} {data.decode(\'utf-8\')}\\") # Send the response back to the client self.request.sendall(data.upper()) class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 0 with ThreadedTCPServer((HOST, PORT), MyTCPHandler) as server: ip, port = server.server_address print(f\\"Server running at {ip}:{port}\\") try: server.serve_forever() except KeyboardInterrupt: print(\\"Server is shutting down.\\") server.shutdown() ``` **Client Script (client.py):** ```python import socket import sys HOST, PORT = \\"localhost\\", <port_number> data = \\" \\".join(sys.argv[1:]) with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock: sock.connect((HOST, PORT)) sock.sendall(bytes(data + \\"n\\", \\"utf-8\\")) received = str(sock.recv(1024), \\"utf-8\\") print(\\"Sent:\\", data) print(\\"Received:\\", received) ``` **Instructions:** 1. Implement the server as described, making sure to replace `<port_number>` in the client script with the actual port number printed by the server. 2. Run the server script. 3. In a new terminal, run the client script with a message to send to the server. 4. Verify the server response and check `server_log.txt` to ensure logs are correctly recorded. **Assessment Criteria:** - Correctness and completeness of the implementation. - Proper handling of concurrent client connections. - Accurate and thread-safe logging of client interactions. - Clean and readable code with appropriate comments.","solution":"import socketserver import threading import logging from datetime import datetime # Set up logging logging.basicConfig(filename=\'server_log.txt\', level=logging.INFO, format=\'%(message)s\') log_lock = threading.Lock() class MyTCPHandler(socketserver.BaseRequestHandler): def handle(self): data = self.request.recv(1024).strip() current_time = datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') client_address = self.client_address[0] # Logging the data with thread safety with log_lock: logging.info(f\\"{current_time} {client_address} {data.decode(\'utf-8\')}\\") # Send the response back to the client self.request.sendall(data.upper()) class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 0 with ThreadedTCPServer((HOST, PORT), MyTCPHandler) as server: ip, port = server.server_address print(f\\"Server running at {ip}:{port}\\") try: server.serve_forever() except KeyboardInterrupt: print(\\"Server is shutting down.\\") server.shutdown()"},{"question":"Coding Assessment Question # Objective To assess your understanding of scikit-learn, specifically focusing on handling multi-label classifications using the `OneVsRestClassifier`. # Problem Statement Given a text dataset, your task is to implement a pipeline to classify text data into multiple labels. Each piece of text can belong to multiple categories, making this a multi-label classification problem. # Dataset You can use the built-in `newsgroups` dataset from `sklearn.datasets` for this task. The categories we are interested in are: `\'alt.atheism\', \'comp.graphics\', \'sci.space\', \'talk.politics.guns\'`. # Requirements 1. **Data Loading & Preprocessing**: Load the newsgroups dataset and preprocess the text (e.g., TF-IDF vectorization). 2. **Binary Relevance Strategy**: Utilize the `OneVsRestClassifier` to transform the multi-label problem into multiple binary classification problems. 3. **Base Estimator**: Use `LinearSVC` as the base estimator for `OneVsRestClassifier`. 4. **Model Training and Evaluation**: Train the model and evaluate it using appropriate metrics for multi-label classification. # Input - No explicit input will be provided. You need to generate the dataset using `sklearn.datasets.fetch_20newsgroups`. # Output - Print the classification report (using `classification_report` from `sklearn.metrics`) for the test dataset. # Constraints - You must use `OneVsRestClassifier` with `LinearSVC` as the base estimator. # Example ```python from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.svm import LinearSVC from sklearn.multiclass import OneVsRestClassifier from sklearn.pipeline import make_pipeline from sklearn.metrics import classification_report from sklearn.model_selection import train_test_split # Load the dataset categories = [\'alt.atheism\', \'comp.graphics\', \'sci.space\', \'talk.politics.guns\'] newsgroups_train = fetch_20newsgroups(subset=\'train\', categories=categories) newsgroups_test = fetch_20newsgroups(subset=\'test\', categories=categories) # Preprocess the text vectorizer = TfidfVectorizer() # Use OneVsRestClassifier with LinearSVC model = OneVsRestClassifier(LinearSVC(random_state=0)) # Create a pipeline pipeline = make_pipeline(vectorizer, model) # Train-test split X_train, X_test, y_train, y_test = train_test_split(newsgroups_train.data, newsgroups_train.target, test_size=0.2, random_state=42) # Fit the model pipeline.fit(X_train, y_train) # Predict the labels y_pred = pipeline.predict(X_test) # Print the classification report print(classification_report(y_test, y_pred, target_names=categories)) ``` # Note Ensure to use proper train-test split and random states for reproducibility. This example provides a structure; however, you will need to adjust and add code as per the requirement to complete the task.","solution":"from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.svm import LinearSVC from sklearn.multiclass import OneVsRestClassifier from sklearn.pipeline import make_pipeline from sklearn.metrics import classification_report from sklearn.model_selection import train_test_split from sklearn.preprocessing import MultiLabelBinarizer def load_and_preprocess_data(categories): newsgroups_train = fetch_20newsgroups(subset=\'train\', categories=categories) newsgroups_test = fetch_20newsgroups(subset=\'test\', categories=categories) return newsgroups_train, newsgroups_test def create_pipeline(): vectorizer = TfidfVectorizer() model = OneVsRestClassifier(LinearSVC(random_state=0)) pipeline = make_pipeline(vectorizer, model) return pipeline def train_and_evaluate(categories): newsgroups_train, newsgroups_test = load_and_preprocess_data(categories) # Vectorizing labels mlb = MultiLabelBinarizer() newsgroups_train.target = mlb.fit_transform([[newsgroups_train.target_names[i]] for i in newsgroups_train.target]) newsgroups_test.target = mlb.transform([[newsgroups_test.target_names[i]] for i in newsgroups_test.target]) X_train, X_test, y_train, y_test = train_test_split(newsgroups_train.data, newsgroups_train.target, test_size=0.2, random_state=42) pipeline = create_pipeline() pipeline.fit(X_train, y_train) y_pred = pipeline.predict(X_test) print(classification_report(y_test, y_pred, target_names=categories)) categories = [\'alt.atheism\', \'comp.graphics\', \'sci.space\', \'talk.politics.guns\'] train_and_evaluate(categories)"},{"question":"Advanced Warning Management Problem Statement: You are developing a Python module `warning_utils.py` that contains utility functions for handling warnings in a flexible and controlled manner. Your goal is to implement the following functions: 1. **`issue_custom_warning(message: str) -> None`**: - This function should issue a `UserWarning` with the provided `message`. 2. **`convert_warnings_to_exceptions(categories: list) -> None`**: - This function should modify the warning filter such that all warnings of the specified categories (a list of warning category classes) are converted to exceptions. 3. **`temporarily_ignore_warnings_in_block(code_block: Callable) -> Any`**: - This function should execute the given `code_block` callable within a context where all warnings are temporarily ignored. It should return the result of the `code_block`. 4. **`count_specific_warnings(categories: list, code_block: Callable) -> int`**: - This function should execute the `code_block` callable and count the number of warnings issued that belong to the specified categories (a list of warning category classes). It should return the count of such warnings. Requirements: - **`issue_custom_warning(message: str) -> None`** - Issues a `UserWarning` with the given message. - **`convert_warnings_to_exceptions(categories: list) -> None`** - For each warning category in the `categories` list, the warnings should be turned into exceptions. - The `categories` list will contain subclasses of the `Warning` class (e.g., `DeprecationWarning`, `UserWarning`, etc.). - **`temporarily_ignore_warnings_in_block(code_block: Callable) -> Any`** - All warnings should be ignored during the execution of the provided `code_block`. - Return the result of calling `code_block`. - **`count_specific_warnings(categories: list, code_block: Callable) -> int`** - Capture and count warnings of the specified categories while executing `code_block`. - Return the count of warnings matching the specified categories. Constraints: 1. Each function should handle any exceptions or errors gracefully. 2. Assume all inputs are well-formed and valid according to the function specifications. 3. Use the `warnings` module functionalities effectively to fulfill the required behavior. Example Usage: ```python import warnings from warning_utils import ( issue_custom_warning, convert_warnings_to_exceptions, temporarily_ignore_warnings_in_block, count_specific_warnings ) def sample_code(): warnings.warn(\\"This is a deprecation warning\\", DeprecationWarning) warnings.warn(\\"This is a user warning\\", UserWarning) # Issue a custom warning issue_custom_warning(\\"This is a custom user warning\\") # Convert DeprecationWarnings to exceptions convert_warnings_to_exceptions([DeprecationWarning]) try: warnings.warn(\\"Deprecated feature\\", DeprecationWarning) except DeprecationWarning as e: print(\\"Caught an exception for deprecated feature\\") # Temporarily ignore all warnings in a code block temporarily_ignore_warnings_in_block(sample_code) # Count specific warnings in a code block count_deprecation_warnings = count_specific_warnings([DeprecationWarning], sample_code) print(f\\"Number of DeprecationWarnings: {count_deprecation_warnings}\\") ``` Implementation: Implement the following functions in `warning_utils.py`: ```python import warnings from typing import Callable, Any def issue_custom_warning(message: str) -> None: warnings.warn(message, UserWarning) def convert_warnings_to_exceptions(categories: list) -> None: for category in categories: warnings.simplefilter(\'error\', category) def temporarily_ignore_warnings_in_block(code_block: Callable) -> Any: with warnings.catch_warnings(): warnings.simplefilter(\'ignore\') return code_block() def count_specific_warnings(categories: list, code_block: Callable) -> int: count = 0 with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\'always\') code_block() for warning in w: if any(issubclass(warning.category, category) for category in categories): count += 1 return count ```","solution":"import warnings from typing import Callable, Any def issue_custom_warning(message: str) -> None: Issues a UserWarning with the given message. warnings.warn(message, UserWarning) def convert_warnings_to_exceptions(categories: list) -> None: Converts warnings of the specified categories into exceptions. Args: categories (list): A list of Warning category classes to convert to exceptions. for category in categories: warnings.simplefilter(\'error\', category) def temporarily_ignore_warnings_in_block(code_block: Callable) -> Any: Executes the provided code_block callable with all warnings ignored. Args: code_block (Callable): A callable code block to execute. Returns: Any: The result of the code_block callable. with warnings.catch_warnings(): warnings.simplefilter(\'ignore\') return code_block() def count_specific_warnings(categories: list, code_block: Callable) -> int: Executes the code_block callable and counts the number of warnings of specified categories. Args: categories (list): A list of Warning category classes to count. code_block (Callable): A callable code block to execute. Returns: int: The number of warnings matching the specified categories. count = 0 with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\'always\') code_block() for warning in w: if any(issubclass(warning.category, category) for category in categories): count += 1 return count"},{"question":"# Pathlib Based File Handling Challenge Objective You are required to implement a Python function that identifies duplicate files in a given directory and its subdirectories. Two files are considered duplicates if they have the same contents. Function Signature ```python def find_duplicate_files(directory: str) -> List[Tuple[Path, Path]]: ``` Input - `directory` (str): The path to the root directory where the search should begin. Output - A list of tuples, where each tuple contains two `Path` objects representing the paths of duplicate files. Constraints 1. The function should consider files as duplicates only if their contents are exactly the same. 2. The function should be efficient, avoiding unnecessary file reads. 3. The function should work on both Windows and POSIX systems. 4. Return an empty list if no duplicates are found. 5. Your solution should not use external libraries other than `pathlib`. Example Usage ```python # Directory structure: # /test # ├── file1.txt # ├── file2.txt # ├── subdir # │ ├── file3.txt # │ ├── file4.txt # Contents: # file1.txt: \\"Hello World\\" # file2.txt: \\"Goodbye\\" # file3.txt: \\"Hello World\\" # file4.txt: \\"Different content\\" # Example usage: duplicates = find_duplicate_files(\'/test\') print(duplicates) # Possible output (the order of pairs is not important): # [(Path(\'/test/file1.txt\'), Path(\'/test/subdir/file3.txt\'))] ``` Guidelines 1. Utilize `pathlib.Path` for all path manipulations. 2. Use methods like `Path.iterdir()`, `Path.read_bytes()`, `Path.glob(\'**/*\')` efficiently. 3. Ensure the function correctly traverses subdirectories. 4. Handle potential edge cases, such as empty directories and read permissions. The implementation must demonstrate a clear understanding of the `pathlib` functionality, efficiently identifying duplicates without excessive file reads.","solution":"from pathlib import Path from typing import List, Tuple def find_duplicate_files(directory: str) -> List[Tuple[Path, Path]]: files_map = {} duplicates = [] for file_path in Path(directory).rglob(\'*\'): if file_path.is_file(): file_content = file_path.read_bytes() if file_content in files_map: duplicates.append((files_map[file_content], file_path)) else: files_map[file_content] = file_path return duplicates"},{"question":"Objective: Write a Python script to automate the installation of `pip` using the `ensurepip` package. The script should be able to install `pip` into a specified virtual environment and provide options for upgrading an existing `pip` installation and installing `pip` to the user site-packages directory. Details: Implement a function named `install_pip` with the following signature: ```python def install_pip(venv_path: str, upgrade: bool = False, user: bool = False) -> str: pass ``` - **`venv_path`**: A string representing the path to the virtual environment where `pip` should be installed. - **`upgrade`**: A boolean indicating whether to upgrade `pip` if it is already installed (default is `False`). - **`user`**: A boolean indicating whether to install `pip` to the user site-packages directory instead of the virtual environment (default is `False`). The function should: 1. Check if the specified virtual environment exists. 2. Activate the virtual environment. 3. Use the `ensurepip` package to install `pip` in the virtual environment. 4. Include the options to upgrade an existing pip installation and to install `pip` to the user site-packages directory. 5. Return a string message indicating the outcome (e.g., \\"pip installed successfully\\", \\"pip upgraded successfully\\", \\"Failed to install pip\\", etc.). Constraints: - Do not access the internet to install `pip`. - Ensure that both options `upgrade` and `user` can be combined without conflicts. - Include appropriate error handling for cases such as nonexistent virtual environments or invalid paths. Example Usage: ```python # Example 1: Basic installation result = install_pip(\'/path/to/venv\') print(result) # Output: \\"pip installed successfully\\" # Example 2: Upgrade existing pip result = install_pip(\'/path/to/venv\', upgrade=True) print(result) # Output: \\"pip upgraded successfully\\" # Example 3: Install pip to user site-packages directory result = install_pip(\'/path/to/venv\', user=True) print(result) # Output: \\"pip installed to user site-packages\\" ``` Performance Requirements: The function should handle typical delays associated with filesystem operations and activating virtual environments, ensuring this process completes without undue delay under normal conditions. Additional Notes: - You may assume that the virtual environment tools are working correctly if used through typical means (e.g., `venv` module). - The function should be compatible with Python 3.4 and later, as `ensurepip` was introduced starting from Python 3.4. - You are not required to overwrite an existing `pip` installation unless the `upgrade` parameter is `True`.","solution":"import os import subprocess import sys import ensurepip def install_pip(venv_path: str, upgrade: bool = False, user: bool = False) -> str: if not os.path.isdir(venv_path): return \\"Error: Virtual environment not found.\\" activate_script = os.path.join(venv_path, \'bin\', \'activate\') if not os.path.exists(activate_script): return \\"Error: Activate script not found in the virtual environment.\\" try: # Activate virtual environment activate_cmd = f\\"source {activate_script}\\" if user: cmd = f\\"export PIP_USER=yes && {activate_cmd} && python -m ensurepip\\" else: cmd = f\\"{activate_cmd} && python -m ensurepip\\" if upgrade: cmd += \\" --upgrade\\" result = subprocess.run(cmd, shell=True, capture_output=True, text=True) if result.returncode != 0: return f\\"Error: {result.stderr.strip()}\\" else: if upgrade: return \\"pip upgraded successfully\\" else: return \\"pip installed successfully\\" except Exception as e: return f\\"Failed to install pip: {str(e)}\\""},{"question":"**Coding Assessment Question** You are required to implement a simplified version of the `binhex` module\'s capabilities. This will involve writing two main functions to encode a binary file to a hexadecimal string and to decode a hexadecimal string back to its original binary form. Furthermore, you will handle specific cases where certain errors might be raised, similar to how `binhex.Error` works. # Function 1: `encode_to_hex(input_filename, output_filename)` - **Input**: `input_filename` (a string representing the name of a binary file to be encoded), `output_filename` (a string representing the name of the file where the encoded hexadecimal string will be stored). - **Output**: None - **Functionality**: - Open the binary file with the name `input_filename`. - Read the contents of the binary file. - Encode the contents to a hexadecimal string. - Write the hexadecimal string to a new file with the name `output_filename`. # Function 2: `decode_from_hex(input_filename, output_filename)` - **Input**: `input_filename` (a string representing the name of a file containing a hexadecimal string to be decoded), `output_filename` (a string representing the name of the file where the decoded binary data will be stored). - **Output**: None - **Functionality**: - Open the file with the name `input_filename`. - Read the hexadecimal string from the file. - Decode the hexadecimal string back to its original binary form. - Write the binary data to a new file with the name `output_filename`. # Exception Handling - Define a custom exception `HexError` that is raised in the following cases: 1. If the `input_filename` does not exist or cannot be opened. 2. If the `output_filename` cannot be written to. 3. If the input hex data is not properly formatted (i.e., it contains non-hexadecimal characters). # Constraints: - Input files referenced by `input_filename` should always exist. - Output files referenced by `output_filename` should be writable. - The hexadecimal string should only contain characters [0-9, a-f, A-F], and should have an even length (valid hex data). - You may not use any third-party libraries. The built-in `binascii` module is allowed. **Example Usage:** ```python try: encode_to_hex(\'input.bin\', \'encoded.hex\') decode_from_hex(\'encoded.hex\', \'output.bin\') except HexError as e: print(f\\"An error occurred: {e}\\") ``` You need to provide implementations for `encode_to_hex`, `decode_from_hex`, and the `HexError` exception.","solution":"import os class HexError(Exception): pass def encode_to_hex(input_filename, output_filename): try: with open(input_filename, \'rb\') as infile: binary_data = infile.read() except FileNotFoundError: raise HexError(f\\"Cannot open input file: {input_filename}\\") try: hex_data = binary_data.hex() with open(output_filename, \'w\') as outfile: outfile.write(hex_data) except Exception as e: raise HexError(f\\"Cannot write to output file: {output_filename}, {str(e)}\\") def decode_from_hex(input_filename, output_filename): try: with open(input_filename, \'r\') as infile: hex_data = infile.read().strip() except FileNotFoundError: raise HexError(f\\"Cannot open input file: {input_filename}\\") # Validate hex_data if len(hex_data) % 2 != 0 or any(c not in \'0123456789abcdefABCDEF\' for c in hex_data): raise HexError(f\\"Invalid hex data in file: {input_filename}\\") try: binary_data = bytes.fromhex(hex_data) with open(output_filename, \'wb\') as outfile: outfile.write(binary_data) except Exception as e: raise HexError(f\\"Cannot write to output file: {output_filename}, {str(e)}\\")"},{"question":"# Pandas Copy-on-Write Compliance In this assessment, you are required to demonstrate your understanding of the Copy-on-Write (CoW) feature in pandas. Your solution should reflect CoW-compliant practices. Task You are given a dataset in the form of a CSV file. Your task is to complete the following operations using pandas, ensuring compliance with CoW rules: 1. **Load the Data**: Read the CSV file provided as input into a pandas DataFrame. 2. **Modify Data**: - Update specific values in the DataFrame using CoW-compliant methods. - Avoid using non-compliant patterns like chained assignments. 3. **Optimize Data Handling**: - Perform operations on the DataFrame that take advantage of CoW optimizations. 4. **Output the Final DataFrame**: Return the modified DataFrame as output. Specifications - **Input Format**: A single string representing the path to the CSV file. - **Output Format**: A pandas DataFrame after performing the specified operations. Constraints - Use only CoW-compliant methods for modifying and accessing data. - Avoid chained assignment or any method that might unintentionally modify multiple objects at once. - Ensure that numpy arrays derived from DataFrames are handled properly (if needed). Example You are provided with a dataset stored in `data.csv` as follows: ```csv name,score Alice,85 Bob,90 Charles,88 Diana,92 ``` **Operations**: 1. Update the score for \\"Alice\\" to 95. 2. Add a new column `passed` based on the `score` (values above 90 as `True`, otherwise `False`). 3. Rename the column `score` to `grade`. 4. Reset the index of the DataFrame. The final DataFrame should look like this: ``` name grade passed 0 Alice 95 True 1 Bob 90 False 2 Charles 88 False 3 Diana 92 True ``` Implementation ```python import pandas as pd def modify_dataframe(file_path: str): # Load the data df = pd.read_csv(file_path) # Update the score for \\"Alice\\" to 95 df.loc[df[\'name\'] == \'Alice\', \'score\'] = 95 # Add a new column \'passed\' based on the score df[\'passed\'] = df[\'score\'].apply(lambda x: True if x > 90 else False) # Rename the column \'score\' to \'grade\' df.rename(columns={\'score\': \'grade\'}, inplace=True) # Reset the index of the DataFrame df = df.reset_index(drop=True) return df ``` Test your implementation with the provided example to ensure correctness.","solution":"import pandas as pd def modify_dataframe(file_path: str) -> pd.DataFrame: # Load the data df = pd.read_csv(file_path) # Update the score for \\"Alice\\" to 95 df.loc[df[\'name\'] == \'Alice\', \'score\'] = 95 # Add a new column \'passed\' based on the score df[\'passed\'] = df[\'score\'].apply(lambda x: True if x > 90 else False) # Rename the column \'score\' to \'grade\' df.rename(columns={\'score\': \'grade\'}, inplace=True) # Reset the index of the DataFrame df = df.reset_index(drop=True) return df"},{"question":"# Turtle Graphics Challenge: Create a Simple Drawing Program You are tasked with creating a simple drawing program using Python’s `turtle` module. This program will allow users to draw various shapes on a canvas using keyboard inputs to control the turtle\'s actions. Objective: Implement a Python function `drawing_program` that initializes a turtle screen and listens for keyboard inputs to perform different drawing operations. Function Signature: ```python def drawing_program() -> None: ``` Requirements: 1. **Initialize Turtle**: - Set up a Turtle screen with a title \\"Simple Drawing Program\\". - Create a turtle named \\"artist\\" to draw on the screen. 2. **Drawing Commands**: - Use the arrow keys to move the turtle (Up, Down, Left, Right). - Use the keys \'c\', \'s\', \'t\' to draw a circle, a square, and a triangle respectively where the turtle currently is. - Use the key \'e\' to clear the screen. 3. **Turtle Movements**: - Up arrow: Move the turtle forward by 20 units. - Down arrow: Move the turtle backward by 20 units. - Left arrow: Turn the turtle left by 45 degrees. - Right arrow: Turn the turtle right by 45 degrees. 4. **Drawing Shapes**: - \'c\' key: Draw a circle with a radius of 50 units. - \'s\' key: Draw a square with side length of 100 units. - \'t\' key: Draw an equilateral triangle with side length of 100 units. 5. **Screen Control**: - \'e\' key: Clear the entire screen. Constraints: - Ensure the program only terminates when the user closes the Turtle graphics window. - Use turtle screen events for handling keyboard inputs. - Make sure the turtle movements and drawings are smooth and without undesired overlaps. Example Usage: - Upon running the function, a user should be able to interact with the turtle using the specified keys to create drawings and clear the screen. ```python def drawing_program(): import turtle # Set up screen window = turtle.Screen() window.title(\\"Simple Drawing Program\\") # Create turtle artist = turtle.Turtle() # Movement functions def move_forward(): artist.forward(20) def move_backward(): artist.backward(20) def turn_left(): artist.left(45) def turn_right(): artist.right(45) # Drawing functions def draw_circle(): artist.begin_fill() artist.circle(50) artist.end_fill() def draw_square(): artist.begin_fill() for _ in range(4): artist.forward(100) artist.right(90) artist.end_fill() def draw_triangle(): artist.begin_fill() for _ in range(3): artist.forward(100) artist.left(120) artist.end_fill() def clear_screen(): artist.clear() # Link keys to functions window.listen() window.onkey(move_forward, \\"Up\\") window.onkey(move_backward, \\"Down\\") window.onkey(turn_left, \\"Left\\") window.onkey(turn_right, \\"Right\\") window.onkey(draw_circle, \\"c\\") window.onkey(draw_square, \\"s\\") window.onkey(draw_triangle, \\"t\\") window.onkey(clear_screen, \\"e\\") # Run the turtle main loop window.mainloop() ```","solution":"def drawing_program(): import turtle # Set up screen window = turtle.Screen() window.title(\\"Simple Drawing Program\\") # Create turtle artist = turtle.Turtle() # Movement functions def move_forward(): artist.forward(20) def move_backward(): artist.backward(20) def turn_left(): artist.left(45) def turn_right(): artist.right(45) # Drawing functions def draw_circle(): artist.begin_fill() artist.circle(50) artist.end_fill() def draw_square(): artist.begin_fill() for _ in range(4): artist.forward(100) artist.right(90) artist.end_fill() def draw_triangle(): artist.begin_fill() for _ in range(3): artist.forward(100) artist.left(120) artist.end_fill() def clear_screen(): artist.clear() # Link keys to functions window.listen() window.onkey(move_forward, \\"Up\\") window.onkey(move_backward, \\"Down\\") window.onkey(turn_left, \\"Left\\") window.onkey(turn_right, \\"Right\\") window.onkey(draw_circle, \\"c\\") window.onkey(draw_square, \\"s\\") window.onkey(draw_triangle, \\"t\\") window.onkey(clear_screen, \\"e\\") # Run the turtle main loop window.mainloop()"},{"question":"# Temporary File Manager You are required to implement a class `TempFileManager` using the functionalities of the `tempfile` module. This class should facilitate the creation, writing, reading, and cleanup of temporary files and directories within your code. Specifically, your class should: 1. **Initialization**: - Accept parameters for file prefix, suffix, and directory location. - Initialize the manager with a temporary directory to store its files. 2. **Create and Manage Temporary Files**: - Implement a `create_temp_file` method that creates a temporary file, writes initial content to it, and returns the file\'s name. - Implement a `read_temp_file` method that accepts a file name and returns its content. - Implement a `cleanup` method that deletes all temporary files created by this manager. Requirements: - Use `NamedTemporaryFile` for file creation. - Ensure files are readable and writable. - The files and the directories created should be removed properly when `cleanup` method is called. Here is the class definition to be implemented: ```python import tempfile import os class TempFileManager: def __init__(self, prefix=\\"tmp_\\", suffix=\\".txt\\", dir=None): Initialize the TempFileManager with a temporary directory. Parameters: - prefix (str): Prefix for the temporary files. - suffix (str): Suffix for the temporary files. - dir (str): Directory where temporary files will be created. # Your code here def create_temp_file(self, content=\\"\\"): Create a temporary file, write content to it, and return its name. Parameters: - content (str): Initial content to write to the file. Returns: - str: The name of the temporary file. # Your code here def read_temp_file(self, file_name): Read and return the content of a temporary file. Parameters: - file_name (str): Name of the temporary file to read. Returns: - str: Content of the file. # Your code here def cleanup(self): Delete all temporary files created by this manager. # Your code here # Example usage: # manager = TempFileManager() # file_name = manager.create_temp_file(\\"Hello, World!\\") # print(manager.read_temp_file(file_name)) # manager.cleanup() ``` **Constraints**: - Ensure you handle exceptions appropriately, especially during file I/O operations. - Do not store file contents in memory to avoid excessive memory usage with large files. - The methods should be performant and handle multiple operations with minimal delay. Your implementation will be evaluated based on correctness, performance, and how well it adheres to the requirements.","solution":"import tempfile import os class TempFileManager: def __init__(self, prefix=\\"tmp_\\", suffix=\\".txt\\", dir=None): Initialize the TempFileManager with a temporary directory. Parameters: - prefix (str): Prefix for the temporary files. - suffix (str): Suffix for the temporary files. - dir (str): Directory where temporary files will be created. self.prefix = prefix self.suffix = suffix self.dir = dir if dir else tempfile.gettempdir() self.temp_files = [] def create_temp_file(self, content=\\"\\"): Create a temporary file, write content to it, and return its name. Parameters: - content (str): Initial content to write to the file. Returns: - str: The name of the temporary file. temp_file = tempfile.NamedTemporaryFile(prefix=self.prefix, suffix=self.suffix, dir=self.dir, delete=False) temp_file.write(content.encode(\'utf-8\')) temp_file.close() self.temp_files.append(temp_file.name) return temp_file.name def read_temp_file(self, file_name): Read and return the content of a temporary file. Parameters: - file_name (str): Name of the temporary file to read. Returns: - str: Content of the file. with open(file_name, \'r\') as file: content = file.read() return content def cleanup(self): Delete all temporary files created by this manager. for file_name in self.temp_files: try: os.remove(file_name) except OSError as e: print(f\\"Error: {file_name} : {e.strerror}\\") self.temp_files = [] # Example usage: # manager = TempFileManager() # file_name = manager.create_temp_file(\\"Hello, World!\\") # print(manager.read_temp_file(file_name)) # manager.cleanup()"},{"question":"You have been provided with two datasets containing information about students and their test scores. Your task is to perform a series of data manipulation and analysis tasks on these datasets using pandas. **Datasets**: 1. `students.csv`: - `student_id`: Unique identifier for each student - `name`: Name of the student - `grade`: Grade level of the student 2. `scores.csv`: - `student_id`: Unique identifier for each student - `test_name`: Name of the test - `score`: Score obtained by the student on the test **Tasks**: 1. **Data Loading**: Load the datasets `students.csv` and `scores.csv` into pandas DataFrames. 2. **Merge DataFrames**: Merge the two DataFrames on the `student_id` column. 3. **Handle Missing Data**: - Identify any missing values in the merged DataFrame. - Fill any missing `score` values with the average score of the respective `test_name`. 4. **Calculate Average Scores**: - Compute the average score for each `student_id` across all tests. - Add a new column `average_score` to the merged DataFrame with these computed average scores. 5. **Top Performer**: - Find the student with the highest `average_score`. - Extract the `student_id`, `name`, and `average_score` of this top performer. 6. **Grade Level Summary**: - Group the data by `grade` and compute the average score for each grade level. - Create a new DataFrame with `grade` and `average_score` columns displaying these summary statistics. **Constraints**: - Do not use any external libraries other than pandas and numpy. **Input and Output Formats**: - **Input**: CSV files `students.csv` and `scores.csv` - **Output**: Print the following: - The DataFrame with the filled missing scores - The top performer details - The grade level summary DataFrame **Example**: ```python # Load the datasets students_df = pd.read_csv(\'students.csv\') scores_df = pd.read_csv(\'scores.csv\') # Merge merged_df = pd.merge(students_df, scores_df, on=\'student_id\', how=\'left\') # Handle Missing Data # Fill missing scores with the average score of the test merged_df[\'score\'].fillna(merged_df.groupby(\'test_name\')[\'score\'].transform(\'mean\'), inplace=True) # Calculate Average Scores average_scores = merged_df.groupby(\'student_id\')[\'score\'].mean().reset_index() average_scores.rename(columns={\'score\': \'average_score\'}, inplace=True) merged_df = pd.merge(merged_df, average_scores, on=\'student_id\') # Top Performer top_performer = merged_df.loc[merged_df[\'average_score\'].idxmax(), [\'student_id\', \'name\', \'average_score\']] # Grade Level Summary grade_summary = merged_df.groupby(\'grade\')[\'average_score\'].mean().reset_index() # Print outputs print(\\"Merged DataFrame with filled missing scores:\\") print(merged_df) print(\\"nTop Performer:\\") print(top_performer) print(\\"nGrade Level Summary:\\") print(grade_summary) ``` **Note**: Ensure you handle edge cases such as students with no scores, tests with no scores, and ties in the top performer.","solution":"import pandas as pd def load_data(students_csv, scores_csv): Load the datasets from the provided CSV files. students_df = pd.read_csv(students_csv) scores_df = pd.read_csv(scores_csv) return students_df, scores_df def merge_data(students_df, scores_df): Merge the students and scores DataFrames on the student_id column. merged_df = pd.merge(students_df, scores_df, on=\'student_id\', how=\'left\') return merged_df def handle_missing_data(merged_df): Fill missing score values with the average score of the respective test. merged_df[\'score\'].fillna(merged_df.groupby(\'test_name\')[\'score\'].transform(\'mean\'), inplace=True) return merged_df def calculate_average_scores(merged_df): Compute the average score for each student and add it as a new column. average_scores = merged_df.groupby(\'student_id\')[\'score\'].mean().reset_index() average_scores.rename(columns={\'score\': \'average_score\'}, inplace=True) merged_df = pd.merge(merged_df, average_scores, on=\'student_id\') return merged_df def find_top_performer(merged_df): Find the student with the highest average score and extract their details. top_performer = merged_df.loc[merged_df[\'average_score\'].idxmax(), [\'student_id\', \'name\', \'average_score\']] return top_performer def grade_level_summary(merged_df): Compute the average score for each grade level and return as a DataFrame. grade_summary = merged_df.groupby(\'grade\')[\'average_score\'].mean().reset_index() return grade_summary"},{"question":"**Complex Object Manipulation in Python** You are required to write a function in Python that handles multiple object types, performs type-checking, and manipulates data by converting it between various Python core object types. The function should process a mixed list of integers, strings, tuples, dictionaries, and sets, and output a dictionary summarizing the processed data. # Function Signature ```python def process_objects(mixed_list: list) -> dict: pass ``` # Input: - `mixed_list`: A list containing a mix of integers, strings, tuples, dictionaries, and sets. # Output: - A dictionary summarizing processed data with the following keys: - `\'integers\'`: A list of all integers squared. - `\'strings\'`: A single string concatenating all string elements. - `\'tuples\'`: A list of all tuples converted to lists. - `\'dictionaries\'`: A single dictionary merged from all dictionaries in the list. - `\'sets\'`: A single set that is the union of all sets in the list. - `\'others\'`: A list of elements that do not match any of the above types (could be anything unexpected). # Constraints: - You must perform type-checking for all elements and handle unexpected types correctly. - If the input list is empty, the output should be a dictionary with empty values for all keys except `\'others\'`, which should also be empty. # Example: ```python input_list = [1, \\"hello\\", (2, 3), {\\"key\\": \\"value\\"}, {4, 5}, 2, \\"world\\", (6, 7), {\\"another_key\\": 42}, {8, 9}] expected_output = { \'integers\': [1, 4], \'strings\': \'helloworld\', \'tuples\': [[2, 3], [6, 7]], \'dictionaries\': {\'key\': \'value\', \'another_key\': 42}, \'sets\': {4, 5, 8, 9}, \'others\': [] } assert process_objects(input_list) == expected_output ``` # Notes: - Implement appropriate handling and merging for dictionaries, sets, and tuples. - Ensure the function is efficient and handles large input lists gracefully. - Error handling is essential. The function should not break if it encounters an unexpected type.","solution":"def process_objects(mixed_list: list) -> dict: Process a mixed list of integers, strings, tuples, dictionaries, and sets, and output a dictionary summarizing the processed data. result = { \'integers\': [], \'strings\': \'\', \'tuples\': [], \'dictionaries\': {}, \'sets\': set(), \'others\': [], } for item in mixed_list: if isinstance(item, int): result[\'integers\'].append(item ** 2) elif isinstance(item, str): result[\'strings\'] += item elif isinstance(item, tuple): result[\'tuples\'].append(list(item)) elif isinstance(item, dict): result[\'dictionaries\'].update(item) elif isinstance(item, set): result[\'sets\'].update(item) else: result[\'others\'].append(item) return result"},{"question":"**Programming Task: Custom Configuration Loader** **Objective:** Implement a custom configuration loader that reads configuration data from a file, processes placeholders using custom interpolation logic, and applies default values for any missing sections or options. **Problem Statement:** You are tasked with implementing a configuration loader class `CustomConfigLoader` using `configparser` module that should have the following features: 1. **Read Configuration:** - Read configuration from a given file. - Initialize the configuration parser with custom defaults. - Allow custom interpolation logic. 2. **Custom Interpolation:** - The custom interpolation logic should support advanced string interpolation where placeholders in the form of `{section:option}` are replaced by the corresponding values from other sections and options. - Implement custom logic to handle scenarios where the section or option may not exist and should gracefully fallback to default values. 3. **Default Values:** - Use a default dictionary to initialize the `ConfigParser` that provides default values for any sections or options missing in the configuration file. 4. **Custom Exception Handling:** - Gracefully handle common configparser exceptions like `NoSectionError` and `NoOptionError` and provide meaningful error messages. **Class Definition:** ```python import configparser class CustomConfigLoader: def __init__(self, default_values): Initialize the CustomConfigLoader with default values. :param default_values: Dictionary containing default values for the sections and options. pass def read_config(self, file_path): Read the configuration file and apply default values and custom interpolation. :param file_path: path to the configuration file to be read. pass def get_value(self, section, option): Get the value of the option from the given section with interpolation applied. :param section: Section name in the configuration. :param option: Option/Key name in the section. :return: Processed string value with interpolation applied. pass ``` **Requirements:** - **Initialization (`__init__` method):** - Initialize the `ConfigParser` with given default values. - **Reading Configuration (`read_config` method):** - Read a configuration file specified by `file_path`. - Implement a custom interpolation logic, inspired by `ExtendedInterpolation` of `configparser`. - Ensure any missing sections or options gracefully fall back to default values. - **Fetching Value (`get_value` method):** - Return the interpolated value of the option from the given section. - If the section or option does not exist, return a meaningful default value. **Example Usage:** ```python default_values = { \'Paths\': { \'home_dir\': \'/Users/default\', \'root_dir\': \'/var/www\' }, \'Settings\': { \'timeout\': \'30\', \'debug\': \'false\' } } loader = CustomConfigLoader(default_values) loader.read_config(\'example.ini\') print(loader.get_value(\'Paths\', \'home_dir\')) # Outputs: /Users/yourusername if it exists in example.ini or the default value. print(loader.get_value(\'Paths\', \'my_dir\')) # Outputs: /Users/default/myfolder if my_dir is defined as {Paths:home_dir}/myfolder in example.ini print(loader.get_value(\'Settings\', \'timeout\')) # Outputs: 30 or the value defined in example.ini ``` **Constraints:** - Assume the configuration file follows the standard INI file structure. - The file paths and all keys should be considered case-sensitive. **Performance Requirements:** - Ensure the interpolation logic is efficient and does not lead to infinite loops or excessive recursive calls. - The class should be able to handle reasonably-sized configuration files efficiently.","solution":"import configparser class CustomConfigLoader: def __init__(self, default_values): Initialize the CustomConfigLoader with default values. :param default_values: Dictionary containing default values for the sections and options. self.config = configparser.ConfigParser(defaults=default_values, interpolation=configparser.ExtendedInterpolation()) self.default_values = default_values def read_config(self, file_path): Read the configuration file and apply default values and custom interpolation. :param file_path: path to the configuration file to be read. self.config.read(file_path) def get_value(self, section, option): Get the value of the option from the given section with interpolation applied. :param section: Section name in the configuration. :param option: Option/Key name in the section. :return: Processed string value with interpolation applied. try: return self.config.get(section, option) except (configparser.NoSectionError, configparser.NoOptionError): # If the section or option does not exist, return the default if available # Otherwise, raise a meaningful error if section in self.default_values and option in self.default_values[section]: return self.default_values[section][option] raise ValueError(f\\"Section \'{section}\' or option \'{option}\' not found in configuration and no default value provided.\\")"},{"question":"You are tasked with creating a utility that can compress and decompress files using the gzip format. The utility should provide functionality to compress an existing text file to a gzip file, decompress a gzip file to retrieve the original text content, and handle any errors that arise due to invalid gzip files. Your task is to implement two functions: 1. `compress_file(input_file: str, output_file: str, compresslevel: int = 9) -> None` 2. `decompress_file(input_file: str, output_file: str) -> None` **Function Details:** 1. `compress_file(input_file: str, output_file: str, compresslevel: int = 9) -> None` - **Input:** - `input_file`: Path to the text file that needs to be compressed. - `output_file`: Path where the compressed gzip file should be saved. - `compresslevel`: An integer from 0 to 9 indicating the level of compression, where 0 means no compression and 9 indicates maximum compression. Default is 9. - **Output:** - None. The function should save the compressed content to the specified output file. - **Constraints:** - Ensure the function gracefully handles cases where the `input_file` does not exist. - Handle scenarios where the `output_file` cannot be created or written to. 2. `decompress_file(input_file: str, output_file: str) -> None` - **Input:** - `input_file`: Path to the gzip file that needs to be decompressed. - `output_file`: Path where the decompressed text should be saved. - **Output:** - None. The function should save the decompressed content to the specified output file. - **Constraints:** - Ensure the function gracefully handles cases where the `input_file` does not exist. - Handle scenarios where the `output_file` cannot be created or written to. - If the gzip file is invalid, catch the exception and print a user-friendly error message. **Examples:** ```python # Compressing a text file compress_file(\'example.txt\', \'example.txt.gz\', compresslevel=5) # Decompressing a gzip file decompress_file(\'example.txt.gz\', \'extracted_example.txt\') ``` **Note:** - You are required to use the `gzip` module for this task. - Implement necessary exception handling to manage file I/O errors and handle cases where gzip files might be corrupted. Good luck, and happy coding!","solution":"import gzip import os def compress_file(input_file: str, output_file: str, compresslevel: int = 9) -> None: Compresses a text file to a gzip file. Parameters: - input_file (str): Path to the text file that needs to be compressed. - output_file (str): Path where the compressed gzip file should be saved. - compresslevel (int): Compression level (0-9), where 0 means no compression and 9 means maximum compression. try: with open(input_file, \'rb\') as f_in: with gzip.open(output_file, \'wb\', compresslevel=compresslevel) as f_out: f_out.writelines(f_in) print(f\\"File \'{input_file}\' compressed successfully to \'{output_file}\'.\\") except FileNotFoundError: print(f\\"Error: The file \'{input_file}\' does not exist.\\") except OSError as e: print(f\\"Error: Could not write to output file \'{output_file}\'. {e}\\") def decompress_file(input_file: str, output_file: str) -> None: Decompresses a gzip file to a text file. Parameters: - input_file (str): Path to the gzip file that needs to be decompressed. - output_file (str): Path where the decompressed text should be saved. try: with gzip.open(input_file, \'rb\') as f_in: with open(output_file, \'wb\') as f_out: f_out.write(f_in.read()) print(f\\"File \'{input_file}\' decompressed successfully to \'{output_file}\'.\\") except FileNotFoundError: print(f\\"Error: The file \'{input_file}\' does not exist.\\") except OSError as e: print(f\\"Error: Could not write to output file \'{output_file}\'. {e}\\") except gzip.BadGzipFile: print(f\\"Error: The file \'{input_file}\' is not a valid gzip file or is corrupted.\\")"},{"question":"Using the `email.message.Message` class, write a Python function named `parse_and_modify_email` that accepts a raw email message string as input. The function should: 1. Parse the string into a `Message` object. 2. Traverse the MIME parts of the email and perform the following modifications: - Change the subject of the email to \\"Modified Subject\\". - Append a header \'X-Modified\' with the value \'True\' to each MIME part. - If any part is of `text/plain` content type, append the string \\"This is modified\\" to its payload. 3. Flatten the modified message back into a string and return it. # Function Signature: ```python from email.message import Message def parse_and_modify_email(raw_email: str) -> str: pass ``` # Input: - `raw_email` (str): A raw string representation of an email message. # Output: - (str): The modified email message as a raw string. # Constraints: - The email message may contain multiple parts with varying content types. - Proper handling of attachments and non-text parts should be ensured. # Example: ```python raw_email = MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=\\"===============7330845974216740156==\\" --===============7330845974216740156== Content-Type: text/plain Hello, this is the original message. --===============7330845974216740156== Content-Type: text/html <html><body>Hello, this is the original message.</body></html> --===============7330845974216740156==-- modified_email = parse_and_modify_email(raw_email) print(modified_email) ``` # Expected Output: The email with modifications: - The subject header changed to \\"Modified Subject\\". - Each part containing the header `X-Modified: True`. - Text/plain parts having \\"This is modified\\" appended to their payloads.","solution":"from email import message_from_string from email.message import Message from email.policy import default def parse_and_modify_email(raw_email: str) -> str: msg = message_from_string(raw_email, policy=default) # Change the subject of the email to \\"Modified Subject\\" msg[\'Subject\'] = \'Modified Subject\' # Function to modify each part def modify_part(part): part.add_header(\'X-Modified\', \'True\') if part.get_content_type() == \'text/plain\': part.set_payload(part.get_payload() + \\"nThis is modified\\") # Check if the message is multipart if msg.is_multipart(): for part in msg.iter_parts(): modify_part(part) else: modify_part(msg) # Return the modified message as a string return msg.as_string()"},{"question":"Objective Create a terminal-based application using the `curses.panel` module to manage and display multiple stackable panels. You will specifically handle the creation, stacking, movement, and displaying of panels on user commands. Task Implement a class, `PanelManager`, with the following functionalities: 1. **Initialization:** - Should initialize a curses window and set up an empty list to hold active panels. 2. **Method: `create_panel(self, content: str) -> None`:** - Create a new panel with the given content and add it to the stack. - The new panel should appear at the top of the stack. 3. **Method: `move_panel(self, index: int, y: int, x: int) -> None`:** - Move the panel at the specified index to the new (y, x) coordinates. 4. **Method: `hide_panel(self, index: int) -> None`:** - Hide the panel at the specified index. 5. **Method: `show_panel(self, index: int) -> None`:** - Show the hidden panel at the specified index. 6. **Method: `refresh(self) -> None`:** - Refresh the display to update the panels. Constraints - You must use the `curses.panel` module for managing the panels. - Ensure proper handling of initialization and cleanup of the curses window. Input/Output - The methods do not return any values. - The `content` argument in `create_panel` method is a string that will be displayed in the panel. - The `index` argument in methods corresponds to the position in the internal list of panels maintained by `PanelManager`. Example Usage ```python pm = PanelManager() pm.create_panel(\\"First Panel Content\\") pm.create_panel(\\"Second Panel Content\\") pm.move_panel(0, 5, 10) pm.hide_panel(1) pm.show_panel(1) pm.refresh() ``` Additional Requirements - Provide error handling for invalid indices. - Ensure that the `curses` environment is properly initialized and terminated without error. Your code should be written in the provided class template: ```python import curses import curses.panel class PanelManager: def __init__(self): self.stdscr = None self.panels = [] def create_panel(self, content: str) -> None: # Implementation here def move_panel(self, index: int, y: int, x: int) -> None: # Implementation here def hide_panel(self, index: int) -> None: # Implementation here def show_panel(self, index: int) -> None: # Implementation here def refresh(self) -> None: # Implementation here # Example of how to run the ncurses initialization and termination def main(stdscr): pm = PanelManager() pm.create_panel(\\"First Panel Content\\") pm.create_panel(\\"Second Panel Content\\") pm.move_panel(0, 5, 10) pm.hide_panel(1) pm.show_panel(1) pm.refresh() stdscr.getkey() if __name__ == \\"__main__\\": curses.wrapper(main) ``` To test your implementation, you will need to run it in a terminal that supports `curses`.","solution":"import curses import curses.panel class PanelManager: def __init__(self): self.stdscr = curses.initscr() curses.start_color() self.stdscr.keypad(True) curses.curs_set(False) self.panels = [] def create_panel(self, content: str) -> None: height, width = 3, max(len(content) + 2, 10) # Define panel size y, x = 0, 0 # Default position win = curses.newwin(height, width, y, x) win.box() win.addstr(1, 1, content) panel = curses.panel.new_panel(win) self.panels.append(panel) curses.panel.update_panels() curses.doupdate() def move_panel(self, index: int, y: int, x: int) -> None: if 0 <= index < len(self.panels): panel = self.panels[index] panel.move(y, x) curses.panel.update_panels() curses.doupdate() def hide_panel(self, index: int) -> None: if 0 <= index < len(self.panels): panel = self.panels[index] panel.hide() curses.panel.update_panels() curses.doupdate() def show_panel(self, index: int) -> None: if 0 <= index < len(self.panels): panel = self.panels[index] panel.show() curses.panel.update_panels() curses.doupdate() def refresh(self) -> None: self.stdscr.refresh() curses.panel.update_panels() curses.doupdate() def end(self): curses.curs_set(True) curses.endwin() # Example of how to run the ncurses initialization and termination def main(stdscr): pm = PanelManager() pm.create_panel(\\"First Panel Content\\") pm.create_panel(\\"Second Panel Content\\") pm.move_panel(0, 5, 10) pm.hide_panel(1) pm.show_panel(1) pm.refresh() stdscr.getkey() pm.end() if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"You are given a binary file that uses EA IFF 85 chunked data format. Write a Python function `list_chunk_sizes` that reads this file and returns a list of tuples, each of which contains the chunk ID and size of all chunks in the file. Assume the chunk file uses big-endian order and has chunks aligned on 2-byte boundaries. Function Signature: ```python def list_chunk_sizes(file_path: str) -> List[Tuple[str, int]]: ``` Input: - `file_path` (str): The path to the chunked binary file. Output: - Returns a list of tuples, each tuple containing: - Chunk ID (str): A 4-byte string representing the ID of the chunk. - Chunk size (int): The size of the chunk data in bytes. Constraints: - Use the `chunk` module to handle the chunk reading. - You are not required to handle files larger than 1GB. - Assume the `chunk` module will handle file I/O errors and provide meaningful exceptions. Example: Given a file `example_file.iff` with the following chunks structure: - ID: `FORM`, Size: 20 - ID: `COMM`, Size: 18 - ID: `SSND`, Size: 3000 Your function should return: ```python [(\'FORM\', 20), (\'COMM\', 18), (\'SSND\', 3000)] ``` Note: You may assume that the `chunk` module is functioning as expected despite its deprecated status, and you have access to it during your implementation. Example Implementation: ```python from typing import List, Tuple import chunk def list_chunk_sizes(file_path: str) -> List[Tuple[str, int]]: chunk_sizes = [] with open(file_path, \'rb\') as file: while True: try: data_chunk = chunk.Chunk(file, align=True, bigendian=True, inclheader=False) except EOFError: break chunk_id = data_chunk.getname().decode(\'ascii\') chunk_size = data_chunk.getsize() chunk_sizes.append((chunk_id, chunk_size)) data_chunk.skip() return chunk_sizes ```","solution":"from typing import List, Tuple import chunk def list_chunk_sizes(file_path: str) -> List[Tuple[str, int]]: chunk_sizes = [] with open(file_path, \'rb\') as file: while True: try: data_chunk = chunk.Chunk(file, align=True, bigendian=True, inclheader=False) except EOFError: break chunk_id = data_chunk.getname().decode(\'ascii\') chunk_size = data_chunk.getsize() chunk_sizes.append((chunk_id, chunk_size)) data_chunk.skip() return chunk_sizes"},{"question":"# PCA Analysis and Application Using scikit-learn **Objective:** Design a coding assessment question that challenges students to use PCA, IncrementalPCA, and KernelPCA from the scikit-learn library to achieve dimensionality reduction on a dataset. **Question:** You are provided with the `digits` dataset from scikit-learn, which consists of 8x8 pixel images of handwritten digits. Your task is to perform the following steps: 1. Load the digits dataset. 2. Apply PCA to reduce the dataset to 2 components and visualize the data. 3. Apply IncrementalPCA to the same dataset to reduce it to 2 components, simulating the scenario of processing the data in small batches. Visualize the resulting data. 4. Apply KernelPCA with a Gaussian (RBF) kernel to reduce the dataset to 2 components, and visualize this data as well. **Specifications:** - **Input:** Use the `digits` dataset from `sklearn.datasets`. - **Output:** 1. Three 2D scatter plots showing the first two principal components obtained using PCA, IncrementalPCA, and KernelPCA. 2. Optional: Color the points in the scatter plots according to their target class to observe class separation. **Constraints:** - Memory usage should be managed efficiently using IncrementalPCA. - For KernelPCA, ensure to use a Gaussian (RBF) kernel, and experiment with different gamma values to see their effect. **Performance Requirements:** - Ensure the code runs efficiently even for relatively large datasets. If the digits dataset is too small, you may upscale it by replicating data. **Example Code Structure:** ```python from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA from sklearn.datasets import load_digits import matplotlib.pyplot as plt # Load the digits dataset digits = load_digits() X = digits.data y = digits.target # Apply PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X) # Visualize PCA result plt.figure(figsize=(8, 4)) plt.subplot(1, 3, 1) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolor=\'none\', alpha=0.7, cmap=plt.get_cmap(\'tab10\', 10)) plt.xlabel(\'PC 1\') plt.ylabel(\'PC 2\') plt.colorbar() plt.title(\'PCA\') # Apply IncrementalPCA in batches inc_pca = IncrementalPPA(n_components=2, batch_size=32) X_inc_pca = inc_pca.fit_transform(X) # Visualize IncrementalPCA result plt.subplot(1, 3, 2) plt.scatter(X_inc_pca[:, 0], X_inc_pca[:, 1], c=y, edgecolor=\'none\', alpha=0.7, cmap=plt.get_cmap(\'tab10\', 10)) plt.xlabel(\'PC 1\') plt.ylabel(\'PC 2\') plt.colorbar() plt.title(\'IncrementalPCA\') # Apply KernelPCA with RBF kernel kernel_pca = KernelPCA(n_components=2, kernel=\'rbf\', gamma=0.01) X_kpca = kernel_pca.fit_transform(X) # Visualize KernelPCA result plt.subplot(1, 3, 3) plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=y, edgecolor=\'none\', alpha=0.7, cmap=plt.get_cmap(\'tab10\', 10)) plt.xlabel(\'PC 1\') plt.ylabel(\'PC 2\') plt.colorbar() plt.title(\'KernelPCA (RBF)\') plt.tight_layout() plt.show() ``` **Notes:** - Experiment with the `gamma` parameter for the `KernelPCA` to observe its effect on the transformed data. **Submission:** Submit your Python script as a Jupyter Notebook, ensuring all plots are correctly rendered.","solution":"from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA from sklearn.datasets import load_digits import matplotlib.pyplot as plt def load_digits_data(): Load the digits dataset. digits = load_digits() return digits.data, digits.target def apply_pca(X): Apply PCA to reduce dataset to 2 components. pca = PCA(n_components=2) return pca.fit_transform(X) def apply_incremental_pca(X, batch_size=32): Apply IncrementalPCA to reduce dataset to 2 components. inc_pca = IncrementalPCA(n_components=2, batch_size=batch_size) return inc_pca.fit_transform(X) def apply_kernel_pca(X, gamma=0.01): Apply KernelPCA with Gaussian (RBF) kernel to reduce dataset to 2 components. kernel_pca = KernelPCA(n_components=2, kernel=\'rbf\', gamma=gamma) return kernel_pca.fit_transform(X) def plot_reduced_data(X_pca, X_inc_pca, X_kpca, y): Plot the reduced data from PCA, IncrementalPCA, and KernelPCA. plt.figure(figsize=(12, 4)) plt.subplot(1, 3, 1) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolor=\'none\', alpha=0.7, cmap=\'tab10\') plt.xlabel(\'PC 1\') plt.ylabel(\'PC 2\') plt.colorbar() plt.title(\'PCA\') plt.subplot(1, 3, 2) plt.scatter(X_inc_pca[:, 0], X_inc_pca[:, 1], c=y, edgecolor=\'none\', alpha=0.7, cmap=\'tab10\') plt.xlabel(\'PC 1\') plt.ylabel(\'PC 2\') plt.colorbar() plt.title(\'IncrementalPCA\') plt.subplot(1, 3, 3) plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=y, edgecolor=\'none\', alpha=0.7, cmap=\'tab10\') plt.xlabel(\'PC 1\') plt.ylabel(\'PC 2\') plt.colorbar() plt.title(\'KernelPCA (RBF)\') plt.tight_layout() plt.show() # Get data X, y = load_digits_data() # Reduce data dimensionality X_pca = apply_pca(X) X_inc_pca = apply_incremental_pca(X) X_kpca = apply_kernel_pca(X) # Plot reduced data plot_reduced_data(X_pca, X_inc_pca, X_kpca, y)"},{"question":"**Objective**: Implement a Python utility that uses the `dbm` module to manage a simple database of key-value pairs. **Description**: You are required to implement a class called `SimpleDBManager` that provides the following functionalities for managing a DBM database: 1. **Adding entries**: Insert new key-value pairs into the database. 2. **Retrieving entries**: Retrieve the value associated with a given key. 3. **Deleting entries**: Remove a key-value pair from the database. 4. **Checking existence**: Check if a key exists in the database. 5. **Listing all keys**: List all keys currently present in the database. 6. **Handling database types**: The class should be able to determine and use the correct DBM database module based on the file\'s current format or create a new database if it doesn\'t exist. 7. **Exception handling**: Properly handle errors such as `dbm.error` or `KeyError`. # Class Definition ```python import dbm class SimpleDBManager: def __init__(self, filename: str, flag: str = \'c\', mode: int = 0o666): Initialize the SimpleDBManager with a specific database file. :param filename: The name of the database file. :param flag: The mode in which the database is opened. Defaults to \'c\'. :param mode: The Unix mode of the file when creating a new database. Defaults to 0o666. self.filename = filename self.flag = flag self.mode = mode self.db = dbm.open(self.filename, self.flag, self.mode) def add_entry(self, key: str, value: str) -> None: Add a key-value pair to the database. :param key: The key to add. :param value: The value associated with the key. self.db[key.encode()] = value.encode() def get_entry(self, key: str) -> str: Retrieve the value for the given key from the database. :param key: The key to retrieve. :return: The value associated with the key. :raises KeyError: If the key is not found. return self.db[key.encode()].decode() def delete_entry(self, key: str) -> None: Delete the key-value pair associated with the given key from the database. :param key: The key to delete. :raises KeyError: If the key is not found. del self.db[key.encode()] def key_exists(self, key: str) -> bool: Check if a key exists in the database. :param key: The key to check. :return: True if the key exists, False otherwise. return key.encode() in self.db def list_keys(self) -> list: List all keys in the database. :return: A list of all keys. return [key.decode() for key in self.db.keys()] def close(self) -> None: Close the database. self.db.close() ``` # Constraints 1. **Input/Output**: Methods `add_entry`, `get_entry`, `delete_entry`, `key_exists`, and `list_keys` must handle string inputs and outputs properly. 2. **Error Handling**: Raise appropriate exceptions wherever necessary. # Examples ```python # 1. Initialize the SimpleDBManager db_manager = SimpleDBManager(\'testdb\') # 2. Add entries db_manager.add_entry(\'name\', \'John Doe\') db_manager.add_entry(\'email\', \'john@example.com\') # 3. Retrieve entries print(db_manager.get_entry(\'name\')) # Output: John Doe # 4. Delete entry db_manager.delete_entry(\'email\') # 5. Check existence print(db_manager.key_exists(\'email\')) # Output: False # 6. List all keys print(db_manager.list_keys()) # Output: [\'name\'] # 7. Close the database db_manager.close() ``` # Additional Notes - Make sure to handle all `dbm` related exceptions properly. - Ensure the proper use of context management where applicable.","solution":"import dbm class SimpleDBManager: def __init__(self, filename: str, flag: str = \'c\', mode: int = 0o666): Initialize the SimpleDBManager with a specific database file. :param filename: The name of the database file. :param flag: The mode in which the database is opened. Defaults to \'c\'. :param mode: The Unix mode of the file when creating a new database. Defaults to 0o666. self.filename = filename self.flag = flag self.mode = mode self.db = dbm.open(self.filename, self.flag, self.mode) def add_entry(self, key: str, value: str) -> None: Add a key-value pair to the database. :param key: The key to add. :param value: The value associated with the key. self.db[key.encode()] = value.encode() def get_entry(self, key: str) -> str: Retrieve the value for the given key from the database. :param key: The key to retrieve. :return: The value associated with the key. :raises KeyError: If the key is not found. try: return self.db[key.encode()].decode() except KeyError: raise KeyError(f\\"The key \'{key}\' was not found in the database.\\") def delete_entry(self, key: str) -> None: Delete the key-value pair associated with the given key from the database. :param key: The key to delete. :raises KeyError: If the key is not found. try: del self.db[key.encode()] except KeyError: raise KeyError(f\\"The key \'{key}\' was not found in the database.\\") def key_exists(self, key: str) -> bool: Check if a key exists in the database. :param key: The key to check. :return: True if the key exists, False otherwise. return key.encode() in self.db def list_keys(self) -> list: List all keys in the database. :return: A list of all keys. return [key.decode() for key in self.db.keys()] def close(self) -> None: Close the database. self.db.close()"},{"question":"# Question: Advanced Itertool Operations **Objective:** The aim of this question is to assess the students\' understanding of multiple functionalities provided by the `itertools` module and their ability to effectively combine these functionalities to solve a complex problem. # Problem Statement You are given a sequence of operations and ranges. You need to perform a set of itertools-based transformations on a set of integer sequences based on the provided operations and ranges. Input: 1. A list of tuples, where each tuple contains: - operation (string): The operation to be applied (\'accumulate\', \'chain\', \'islice\'). - range/values (tuple): The range or values applicable to the operation. 2. A list of lists, where each element list represents a sequence of integers over which the operations must be applied. Output: A list of lists, where each element list is the result after applying the given operations sequentially on the corresponding input integer sequence. Operations: 1. **accumulate**: - Sum up the results sequentially. Uses `itertools.accumulate` - Example: `accumulate((None,))` applies the default addition operation. - This will alter the input sequence to its cumulative sum. 2. **chain**: - Chain the elements of two given sequences. Uses `itertools.chain` - Example: `chain((1, 2, 3))` chains the input sequence with [1, 2, 3]. 3. **islice**: - Slice the sequence based on the given range. Uses `itertools.islice` - Example: `islice((1, 4, None))` slices elements starting from index 1 to 4. - You may omit `step` or set it to `None`. Constraints: - The `operations` list and `sequences` list have the same length. - Each operation must be applied to the corresponding sequence in the `sequences` list. - The operations must be applied in the order they are listed. # Example Input ```python operations = [ (\\"accumulate\\", (None,)), (\\"chain\\", ([10, 11],)), (\\"islice\\", (1, 4)) ] sequences = [ [1, 2, 3, 4], [3, 4, 5], [7, 8, 9, 10, 11] ] ``` Output ```python [ [1, 3, 6, 10], [3, 4, 5, 10, 11], [8, 9, 10] ] ``` Instructions **Implement the function `apply_operations(operations, sequences)`**: ```python from itertools import accumulate, chain, islice def apply_operations(operations, sequences): result = [] for operation, seq in zip(operations, sequences): op, params = operation if op == \\"accumulate\\": seq = list(accumulate(seq, **({params[0]: None} if params[0] else {}))) elif op == \\"chain\\": seq = list(chain(seq, *params)) elif op == \\"islice\\": start = params[0] if params[0] is not None else 0 stop = params[1] step = params[2] if len(params) == 3 and params[2] is not None else 1 seq = list(islice(seq, start, stop, step)) result.append(seq) return result ``` # Notes: - Consider different edge cases such as empty sequences and sequences with a single element. - Ensure the methods from the itertools module are used appropriately and in combination where needed.","solution":"from itertools import accumulate, chain, islice def apply_operations(operations, sequences): result = [] for operation, seq in zip(operations, sequences): op, params = operation if op == \\"accumulate\\": seq = list(accumulate(seq)) elif op == \\"chain\\": seq = list(chain(seq, *params)) elif op == \\"islice\\": start = params[0] if params[0] is not None else 0 stop = params[1] step = params[2] if len(params) == 3 and params[2] is not None else 1 seq = list(islice(seq, start, stop, step)) result.append(seq) return result"},{"question":"**Objective**: Demonstrate your understanding of Python 3.10 codec APIs and custom error handling. **Problem Statement**: You are tasked with managing a text processing system that needs to encode and decode various data formats while handling errors gracefully. In this context, you should develop a function that registers a custom error handler, then uses that handler during encoding and decoding processes. # Task 1. **Register a Custom Error Handler**: Create an error handler named `\\"custom_replace\\"` that replaces any problematic sequence with the string `\\"[ERROR]\\"`. 2. **Implement Encoding and Decoding Functions**: - **Encoding Function**: `custom_encode(data_string: str, encoding: str) -> str` - Encodes the input `data_string` into the specified `encoding` format using the custom error handler `\\"custom_replace\\"`. - Raises a `ValueError` if the encoding fails. - **Decoding Function**: `custom_decode(encoded_data: bytes, encoding: str) -> str` - Decodes the input `encoded_data` from the specified `encoding` format using the custom error handler `\\"custom_replace\\"`. - Raises a `ValueError` if the decoding fails. # Example ```python def custom_error_handler(exception): return (\\"[ERROR]\\", exception.end) # Register the custom error handler PyCodec_RegisterError(\\"custom_replace\\", custom_error_handler) # Example usage encoded_data = custom_encode(\\"Hello, 𝌆 World!\\", \\"ascii\\") print(encoded_data) # Expected Output: \\"Hello, [ERROR] World!\\" decoded_data = custom_decode(encoded_data.encode(\'ascii\'), \\"ascii\\") print(decoded_data) # Expected Output: \\"Hello, [ERROR] World!\\" ``` # Constraints: - The `encoding` string will always be a valid encoding format supported by Python. - The input data strings will only contain valid Unicode characters. # Requirements: - Implement appropriate error checks and custom error handling. - Ensure that the functions handle both encoding and decoding properly. - Provide meaningful error messages in case of failures. **Input**: - `data_string` as a string to be encoded. - `encoding` as the encoding format. - `encoded_data` as a bytes object to be decoded. **Output**: - Encoded string from the `custom_encode` function. - Decoded string from the `custom_decode` function. # Performance: - The encoding and decoding processes should handle reasonably large text data efficiently. # Implementation Steps: 1. Define the custom error handler function. 2. Register the custom error handler using `PyCodec_RegisterError()` function. 3. Implement the `custom_encode` and `custom_decode` functions ensuring they utilize the custom error handler during encoding and decoding.","solution":"import codecs # Define the custom error handler function. def custom_error_handler(exception): return (\\"[ERROR]\\", exception.end) # Register the custom error handler using \'custom_replace\' name. codecs.register_error(\\"custom_replace\\", custom_error_handler) def custom_encode(data_string: str, encoding: str) -> str: Encodes the input data_string into the specified encoding format using the custom error handler \'custom_replace\'. Args: - data_string (str): The input string to be encoded. - encoding (str): The encoding to use. Returns: - str: The encoded string where problematic sequences are replaced with \'[ERROR]\'. Raises: - ValueError: If encoding fails. try: encoded_data = data_string.encode(encoding, errors=\'custom_replace\') return encoded_data.decode(encoding) except Exception as e: raise ValueError(f\\"Encoding failed: {e}\\") def custom_decode(encoded_data: bytes, encoding: str) -> str: Decodes the input encoded_data from the specified encoding format using the custom error handler \'custom_replace\'. Args: - encoded_data (bytes): The input bytes object to be decoded. - encoding (str): The encoding to use. Returns: - str: The decoded string where problematic sequences are replaced with \'[ERROR]\'. Raises: - ValueError: If decoding fails. try: decoded_data = encoded_data.decode(encoding, errors=\'custom_replace\') return decoded_data except Exception as e: raise ValueError(f\\"Decoding failed: {e}\\")"},{"question":"# XML Parsing and Filtering Using `xml.dom.pulldom` In this exercise, you will write a function using the `xml.dom.pulldom` module to parse an XML document and filter data based on specific criteria. The XML document contains records of books, and you need to extract and print details of books that have a rating higher than a given threshold. Function Signature ```python def filter_books(xml_data: str, rating_threshold: float) -> list: pass ``` Input - `xml_data`: A string representing the XML document. Each book record is represented as an `<item>` element with attributes like `title`, `author`, `price`, and `rating`. - `rating_threshold`: A float representing the minimum rating to include a book in the results. Output - A list of tuples, where each tuple contains: - `title` (string) - `author` (string) - `price` (float) - `rating` (float) Constraints - Assume that the XML structure of each book is well-formed. - Attribute values for `price` and `rating` can be converted to floats without error. Example Given the following XML data: ```xml <catalog> <item title=\\"Book A\\" author=\\"Author A\\" price=\\"29.99\\" rating=\\"4.5\\"/> <item title=\\"Book B\\" author=\\"Author B\\" price=\\"15.99\\" rating=\\"3.0\\"/> <item title=\\"Book C\\" author=\\"Author C\\" price=\\"22.50\\" rating=\\"4.8\\"/> </catalog> ``` And a rating threshold of 4.0, your function should output: ```python [(\'Book A\', \'Author A\', 29.99, 4.5), (\'Book C\', \'Author C\', 22.50, 4.8)] ``` Notes - Use the `xml.dom.pulldom` module to parse the XML data. - Only expand nodes when you locate an `<item>` element to efficiently process the required details. Hints - Use `pulldom.parseString(xml_data)` to parse the XML string. - Loop through events to capture `START_ELEMENT` and `END_ELEMENT` for `item`. - Use the `expandNode()` method to read the complete details of an `item`. ```python import xml.dom.pulldom def filter_books(xml_data: str, rating_threshold: float) -> list: from xml.dom import pulldom doc = pulldom.parseString(xml_data) results = [] for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \'item\': rating = float(node.getAttribute(\'rating\')) if rating > rating_threshold: doc.expandNode(node) title = node.getAttribute(\'title\') author = node.getAttribute(\'author\') price = float(node.getAttribute(\'price\')) results.append((title, author, price, rating)) return results # Example usage: xml_data = \'\'\' <catalog> <item title=\\"Book A\\" author=\\"Author A\\" price=\\"29.99\\" rating=\\"4.5\\" /> <item title=\\"Book B\\" author=\\"Author B\\" price=\\"15.99\\" rating=\\"3.0\\" /> <item title=\\"Book C\\" author=\\"Author C\\" price=\\"22.50\\" rating=\\"4.8\\" /> </catalog> \'\'\' print(filter_books(xml_data, 4.0)) ```","solution":"import xml.dom.pulldom def filter_books(xml_data: str, rating_threshold: float) -> list: from xml.dom import pulldom doc = pulldom.parseString(xml_data) results = [] for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \'item\': rating = float(node.getAttribute(\'rating\')) if rating > rating_threshold: doc.expandNode(node) title = node.getAttribute(\'title\') author = node.getAttribute(\'author\') price = float(node.getAttribute(\'price\')) results.append((title, author, price, rating)) return results"},{"question":"# Objective Assess your ability to work with various dataset loading functions in scikit-learn, including sample images, svmlight/libsvm format, and datasets from openml.org. You will also demonstrate preprocessing and applying a machine learning model to the loaded datasets. # Problem Statement 1. **Loading a Sample Image:** - Load the sample image \\"china.jpg\\" using `load_sample_image`. - Preprocess the image by converting it to a floating point representation and scaling it to the range 0 - 1. - Display the image using `matplotlib.pyplot.imshow`. 2. **Loading a Dataset from svmlight/libsvm Format:** - Load the \\"svmlight_example.txt\\" dataset (you can assume it is in the same directory) using `load_svmlight_file`. - Split the loaded data into train and test sets (70% train, 30% test). - Train a `LogisticRegression` model using the train data. - Evaluate the accuracy of the model on the test data. 3. **Downloading a Dataset from openml.org:** - Download the \\"miceprotein\\" dataset from openml.org using `fetch_openml`. - Preprocess the dataset by handling missing values (if any) using `SimpleImputer` with a strategy of your choice. - Use a `StandardScaler` to scale the feature values to have zero mean and unit variance. - Apply a `KNeighborsClassifier` to classify the data into the respective classes. - Evaluate the model\'s accuracy using cross-validation. # Constraints - You must use functions provided by scikit-learn for dataset loading and preprocessing. - Use matplotlib for image visualization. - Ensure that the code is well-structured and commented. # Input and Output Formats - **Input:** N/A (You may load the data directly using the provided functions given you have the appropriate dataset files or access to the openml repository). - **Output:** Display the sample image, accuracy of the logistic regression model, and accuracy of the k-neighbors classifier. # Performance Requirements - Ensure that the loading and preprocessing steps are efficient and leverage scikit-learn\'s utilities wherever possible. **Note**: If you encounter any issues loading the datasets, document the error and your debugging steps clearly in the comments. # Example Execution The following is an example of what your code might look like when executed: ```python import matplotlib.pyplot as plt from sklearn.datasets import load_sample_image, load_svmlight_file, fetch_openml from sklearn.model_selection import train_test_split, cross_val_score from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import numpy as np # 1. Loading and Displaying Sample Image china = load_sample_image(\\"china.jpg\\") china_float = china / 255.0 plt.imshow(china_float) plt.axis(\'off\') plt.tight_layout() plt.show() # 2. Loading and Training on svmlight/libsvm Format Dataset X, y = load_svmlight_file(\\"svmlight_example.txt\\") X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) model = LogisticRegression(max_iter=1000) model.fit(X_train, y_train) accuracy = model.score(X_test, y_test) print(\\"Logistic Regression Accuracy:\\", accuracy) # 3. Downloading and Classifying openml.org Dataset mice = fetch_openml(name=\'miceprotein\', version=4) X_mice, y_mice = mice.data, mice.target imputer = SimpleImputer(strategy=\'mean\') X_mice_imputed = imputer.fit_transform(X_mice) scaler = StandardScaler() X_mice_scaled = scaler.fit_transform(X_mice_imputed) knn = KNeighborsClassifier() cv_accuracy = np.mean(cross_val_score(knn, X_mice_scaled, y_mice, cv=5)) print(\\"K-Neighbors Classifier CV Accuracy:\\", cv_accuracy) ```","solution":"import matplotlib.pyplot as plt from sklearn.datasets import load_sample_image, load_svmlight_file, fetch_openml from sklearn.model_selection import train_test_split, cross_val_score from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import numpy as np def load_and_display_sample_image(): Load and display the sample image \'china.jpg\' from sklearn datasets china = load_sample_image(\\"china.jpg\\") china_float = china / 255.0 plt.imshow(china_float) plt.axis(\'off\') plt.tight_layout() plt.show() def load_and_train_svmlight_dataset(file_path): Load a dataset from svmlight/libsvm format, train a logistic regression model, and evaluate accuracy on test data. :param file_path: Path to the svmlight/libsvm dataset file :return: Accuracy of the logistic regression model on the test data X, y = load_svmlight_file(file_path) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) model = LogisticRegression(max_iter=1000) model.fit(X_train, y_train) accuracy = model.score(X_test, y_test) return accuracy def download_and_classify_openml_dataset(): Download the \'miceprotein\' dataset from openml.org, preprocess the data, and classify using KNeighborsClassifier. Evaluate the model\'s accuracy using cross-validation. :return: Cross-validation accuracy of the KNeighborsClassifier on the imputed and scaled data mice = fetch_openml(name=\'miceprotein\', version=4) X_mice, y_mice = mice.data, mice.target imputer = SimpleImputer(strategy=\'mean\') X_mice_imputed = imputer.fit_transform(X_mice) scaler = StandardScaler() X_mice_scaled = scaler.fit_transform(X_mice_imputed) knn = KNeighborsClassifier() cv_accuracy = np.mean(cross_val_score(knn, X_mice_scaled, y_mice, cv=5)) return cv_accuracy"},{"question":"# Question Overview: Your task is to create a function that takes a list of transactions and processes them to provide a detailed summary of account balances over time. This question will assess your ability to use various `itertools` functionalities efficiently. # Question: You are given a list of transactions where each transaction is represented as a tuple `(account_id, transaction_amount, transaction_date)`. Your goal is to compute the running balance of each account at the end of each month. Implement a function `account_balance_summary(transactions)` that processes the transactions and returns a summary in the form of a list of tuples. Each tuple represents the summary for a distinct account and contains: 1. The `account_id`. 2. A sorted list of the running balances at the end of each month in chronological order. # Inputs: - `transactions`: A list of tuples where each tuple contains: - `account_id`: An integer representing the account ID. - `transaction_amount`: A float representing the amount of the transaction. - `transaction_date`: A string in the format `\'YYYY-MM-DD\'`. # Outputs: - A list of tuples. Each tuple contains: - `account_id`: An integer. - A sorted list of floats representing the running balances at the end of each month in chronological order. # Constraints: - Assume each account has transactions throughout all months in the dataset. - Transactions are already sorted by `transaction_date`. # Example: ```python transactions = [ (1, 100.0, \'2023-01-15\'), (1, -50.0, \'2023-01-25\'), (1, 200.0, \'2023-02-05\'), (2, 300.0, \'2023-01-10\'), (2, -100.0, \'2023-01-30\'), (2, 50.0, \'2023-02-20\') ] account_balance_summary(transactions) ``` # Expected Output: ```python [ (1, [50.0, 250.0]), (2, [200.0, 250.0]) ] ``` Notes: - Utilize the `itertools.groupby` function to group transactions by `account_id`. - Use the `itertools.accumulate` function to compute the running balance. - Ensure that balances are calculated at the end of each month, using the maximum transaction date in that month for the computation.","solution":"from collections import defaultdict from itertools import groupby from operator import itemgetter from datetime import datetime def account_balance_summary(transactions): Processes transactions to compute the running balance of each account at the end of each month. :param transactions: List of tuples (account_id, transaction_amount, transaction_date). :return: A list of tuples (account_id, [monthly_balances_in_chronological_order]). def end_of_month_transactions(transactions): monthly_balance = defaultdict(float) for _, txn_amount, txn_date in transactions: date = datetime.strptime(txn_date, \\"%Y-%m-%d\\") year_month = (date.year, date.month) monthly_balance[year_month] += txn_amount sorted_balances = sorted(monthly_balance.items()) running_balance = [] current_balance = 0.0 for (_, _), balance in sorted_balances: current_balance += balance running_balance.append(current_balance) return running_balance transactions_grouped = groupby(transactions, key=itemgetter(0)) summary = [] for account_id, account_transactions in transactions_grouped: account_transactions_list = list(account_transactions) balances = end_of_month_transactions(account_transactions_list) summary.append((account_id, balances)) return summary"},{"question":"You are required to create a Python program that interacts with the file system and standard input/output. Specifically, you need to implement a function `process_file(paths: List[str]) -> List[str]` that: 1. Takes a list of strings representing file or directory paths. 2. For each path: - Checks if the path exists and whether it is a file or directory. - If it\'s a file, calculates its size in bytes. - If it\'s a directory, lists all files and directories within it. 3. Returns a combined list of results for each path, where: - For files, the result is a string in the format: `\\"File: <path>, Size: <size> bytes\\"`. - For directories, the result is a string in the format: `\\"Directory: <path>, Contents: <list of contents>\\"`. - If the path does not exist, appends a string `\\"Path: <path>, Error: Does not exist\\"`. # Implementation Requirements - Use appropriate file system handling functions from the python310 package. - Use standard input/output functions to read paths and write results. - Handle exceptions gracefully with meaningful error messages. # Example ```python def process_file(paths: List[str]) -> List[str]: # Your implementation here # Example usage paths = [\\"/path/to/file1\\", \\"/path/to/nonexistent\\", \\"/path/to/directory\\"] results = process_file(paths) for result in results: print(result) # Expected Output # File: /path/to/file1, Size: 1234 bytes # Path: /path/to/nonexistent, Error: Does not exist # Directory: /path/to/directory, Contents: [\'file2\', \'file3\', \'subdir\'] ``` # Constraints - Assume the input paths are valid strings. - Handle both absolute and relative paths. - Do not use any third-party libraries.","solution":"import os from typing import List def process_file(paths: List[str]) -> List[str]: Processes a list of file or directory paths and provides information about them. Args: paths (List[str]): List of strings representing file or directory paths. Returns: List[str]: List of strings with information about the files and directories. results = [] for path in paths: if not os.path.exists(path): results.append(f\\"Path: {path}, Error: Does not exist\\") elif os.path.isfile(path): size = os.path.getsize(path) results.append(f\\"File: {path}, Size: {size} bytes\\") elif os.path.isdir(path): contents = os.listdir(path) results.append(f\\"Directory: {path}, Contents: {contents}\\") return results"},{"question":"**Problem: Implement a Socket-Based Echo Server and Client** # Problem Statement: You need to implement a simple Echo Server and a corresponding Echo Client using Python sockets. The Echo Server will receive messages from the Echo Client and send back the same message (echo it back). The communication will be over TCP. # Requirements: 1. **Echo Server**: - **Setup**: - The server should listen on a specific hostname and port. - **Accept Connections**: - The server should be able to handle multiple client connections. Use non-blocking sockets and the `select` module to achieve this. - **Echo Functionality**: - For each connected client, the server should read the client\'s message and immediately send back the same message. - **Graceful Shutdown**: - Ensure the server can close all connections and properly shutdown. 2. **Echo Client**: - **Setup**: - The client should connect to the Echo Server on the specified hostname and port. - **Send & Receive**: - The client should send a message to the server and receive the echoed message back. - **Handle Disconnections**: - The client should handle cases where the server disconnects. # Input: - The Echo Client will send string messages as input to the Echo Server. # Output: - The Echo Client will receive the same string messages echoed back from the Echo Server. # Constraints: - The server and client should handle arbitrary message lengths. - Use non-blocking sockets and the `select` module to manage multiple connections in the Echo Server. - Handle socket errors gracefully and close sockets properly when done. # Example Usage: **Server:** ```python # Run the Echo Server python echo_server.py ``` **Client:** ```python # Run the Echo Client python echo_client.py \\"Hello, Server!\\" ``` **Output:** ```shell # Client Output Received from server: Hello, Server! ``` # Implementation Details: - Implement the Echo Server in a file named `echo_server.py`. - Implement the Echo Client in a file named `echo_client.py`. - The server should handle at least 5 simultaneous clients. - Ensure there is proper error handling in both the server and client implementations.","solution":"import socket import select def echo_server(host, port): Starts an echo server that listens on the specified host and port. server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((host, port)) server_socket.listen(5) server_socket.setblocking(0) inputs = [server_socket] outputs = [] try: while inputs: readable, writable, exceptional = select.select(inputs, outputs, inputs) for s in readable: if s is server_socket: client_socket, client_address = s.accept() client_socket.setblocking(0) inputs.append(client_socket) else: data = s.recv(1024) if data: s.sendall(data) else: inputs.remove(s) s.close() for s in exceptional: inputs.remove(s) if s in outputs: outputs.remove(s) s.close() finally: server_socket.close() def echo_client(host, port, message): Connects to the echo server at the specified host and port, sends the message, and returns the echo from the server. client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client_socket.connect((host, port)) client_socket.sendall(message.encode()) received_data = client_socket.recv(1024) client_socket.close() return received_data.decode()"},{"question":"You are required to implement a file processing function in Python using the `io` module. This function should read from a binary file, process the contents, and write the processed data to a new text file with a specified encoding. The function should handle buffering to ensure efficient reading and writing operations. # Function Signature ```python def process_binary_to_text(input_binary_file: str, output_text_file: str, encoding: str = \'utf-8\') -> None: pass ``` # Input - `input_binary_file` (str): The path to the input binary file. - `output_text_file` (str): The path to the output text file. - `encoding` (str, optional): The encoding to use for the text file. Default is `\'utf-8\'`. # Output - The function should not return anything. It should create the output text file with the processed data. # Processing Details 1. The input binary file contains text data in binary format (e.g., UTF-8 encoded bytes). 2. The function should read the binary data, decode it to a string using the default or specified encoding, and then write this string to the output text file. 3. Use appropriate buffering to ensure efficient I/O operations. 4. Handle any potential I/O errors gracefully, ensuring that all opened files are properly closed even if an error occurs. # Constraints - The function should handle large files efficiently. - Ensure that the output text file uses the specified encoding. # Example Usage ```python # Example binary file content (binary form of \\"Hello WorldnGoodbye World\\") binary_content = b\\"Hello WorldnGoodbye World\\" # Save example binary content to a file with open(\\"input.bin\\", \\"wb\\") as binary_file: binary_file.write(binary_content) # Call function to process the file process_binary_to_text(\\"input.bin\\", \\"output.txt\\", encoding=\\"utf-8\\") # Open and read the output file with open(\\"output.txt\\", \\"r\\", encoding=\\"utf-8\\") as text_file: content = text_file.read() print(content) # Should print \\"Hello WorldnGoodbye World\\" ``` # Notes - Make sure to use the classes and methods from the `io` module when performing I/O operations. - Implement robust error handling to manage different potential issues that may occur during file reading and writing.","solution":"import io def process_binary_to_text(input_binary_file: str, output_text_file: str, encoding: str = \'utf-8\') -> None: try: # Open the binary file for reading with io.open(input_binary_file, \'rb\') as binary_file: # Read the binary content binary_content = binary_file.read() # Decode the binary content to string text_content = binary_content.decode(encoding) # Open the text file for writing with specified encoding with io.open(output_text_file, \'w\', encoding=encoding) as text_file: # Write the decoded content to the text file text_file.write(text_content) except IOError as e: # Handle any I/O errors print(f\\"An error occurred: {e}\\")"},{"question":"Objective Create a multi-faceted swarm plot using the seaborn package. Problem Statement You are given a dataset consisting of restaurant tips called `tips`. Use the seaborn library to generate a multi-faceted swarm plot that visualizes the relationships between \'total_bill\', \'day\', \'time\', and \'sex\'. Apply the following customizations: 1. Create a multi-faceted plot using `sns.catplot`: - Use the swarm plot kind (`kind=\'swarm\'`). - Set the x-axis to the variable `time`. - Set the y-axis to the variable `total_bill`. - Use the variable `sex` for `hue`. - Facet the data across columns by `day`. 2. Customize the plot: - Use a distinct marker, such as \\"x\\", for the points. - Set the linewidth of the markers to 1. - Apply the `deep` palette to differentiate the `hue` categories. 3. Save the plot as a PNG file called `facet_swarmplot.png`. Constraints - You must use the seaborn library for plotting. - The dataset `tips` must be loaded from seaborn\'s built-in datasets. Input Your code does not need to handle inputs from the user, the dataset will be loaded directly within the code. Output Save the generated plot to a file named `facet_swarmplot.png`. ```python # Sample code to load the \'tips\' dataset from seaborn import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Your code implementation here # Save the plot plt.savefig(\'facet_swarmplot.png\') ``` Submission Submit your code implementation that fulfills the requirements specified above.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the multi-faceted swarm plot facet_swarm_plot = sns.catplot( data=tips, kind=\\"swarm\\", x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", palette=\\"deep\\", marker=\\"x\\", linewidth=1 ) # Save the plot plt.savefig(\'facet_swarmplot.png\')"},{"question":"# Custom Interactive Shell **Objective:** Design and implement a custom interactive shell using the `cmd` module in Python. **Task:** You are required to create a custom command interpreter named `MyShell` by subclassing the `cmd.Cmd` class. This shell should support the following commands and functionalities: 1. **greet**: Print a greeting message. 2. **add**: Add two numbers and print the result. 3. **subtract**: Subtract the second number from the first and display the result. 4. **multiply**: Multiply two numbers and print the result. 5. **history**: Display all previously entered commands. 6. **exit**: Exit the shell. **Details:** - Implement the `greet`, `add`, `subtract`, `multiply`, `history`, and `exit` commands. - Commands should be prefixed by `do_`. For instance, the `greet` command should be implemented as `do_greet(self, arg)`. - Use the `precmd()` method to store each command in a history list before it is executed. - When `exit` command is entered, the shell should gracefully exit. **Example:** ```python import cmd class MyShell(cmd.Cmd): prompt = \'(myshell) \' intro = \'Welcome to MyShell. Type help or ? to list commands.\' def precmd(self, line): # Store command in history self.history.append(line) return line def do_greet(self, arg): \'Greet the user with a message.\' print(\\"Hello! Welcome to MyShell.\\") def do_add(self, arg): \'Add two numbers: ADD 2 3\' numbers = list(map(int, arg.split())) result = sum(numbers) print(f\\"The result is: {result}\\") def do_subtract(self, arg): \'Subtract two numbers: SUBTRACT 5 3\' numbers = list(map(int, arg.split())) result = numbers[0] - numbers[1] print(f\\"The result is: {result}\\") def do_multiply(self, arg): \'Multiply two numbers: MULTIPLY 2 3\' numbers = list(map(int, arg.split())) result = numbers[0] * numbers[1] print(f\\"The result is: {result}\\") def do_history(self, arg): \'Show command history: HISTORY\' for i, command in enumerate(self.history, 1): print(f\\"{i}: {command}\\") def do_exit(self, arg): \'Exit the shell: EXIT\' print(\'Goodbye!\') return True def __init__(self): super().__init__() self.history = [] if __name__ == \'__main__\': MyShell().cmdloop() ``` **Constraints:** - You must handle invalid inputs gracefully. - You should initialize the command history list to store previously entered commands. **Input/Output:** - The input will be a series of commands typed by the user. - The output should be the appropriate responses to the commands based on their definitions. **Performance:** - The shell should respond promptly to commands. It should not have significant delay or performance issues when handling valid commands. # Sample Interaction ``` (myshell) greet Hello! Welcome to MyShell. (myshell) add 10 5 The result is: 15 (myshell) subtract 20 8 The result is: 12 (myshell) multiply 4 7 The result is: 28 (myshell) history 1: greet 2: add 10 5 3: subtract 20 8 4: multiply 4 7 5: history (myshell) exit Goodbye! ```","solution":"import cmd class MyShell(cmd.Cmd): prompt = \'(myshell) \' intro = \'Welcome to MyShell. Type help or ? to list commands.\' def precmd(self, line): # Store command in history self.history.append(line) return line def do_greet(self, arg): \'Greet the user with a message.\' print(\\"Hello! Welcome to MyShell.\\") def do_add(self, arg): \'Add two numbers: ADD 2 3\' try: numbers = list(map(int, arg.split())) if len(numbers) != 2: print(\\"Error: Two numbers are required.\\") return result = sum(numbers) print(f\\"The result is: {result}\\") except ValueError: print(\\"Error: Invalid input. Please enter two numbers separated by space.\\") def do_subtract(self, arg): \'Subtract two numbers: SUBTRACT 5 3\' try: numbers = list(map(int, arg.split())) if len(numbers) != 2: print(\\"Error: Two numbers are required.\\") return result = numbers[0] - numbers[1] print(f\\"The result is: {result}\\") except ValueError: print(\\"Error: Invalid input. Please enter two numbers separated by space.\\") def do_multiply(self, arg): \'Multiply two numbers: MULTIPLY 2 3\' try: numbers = list(map(int, arg.split())) if len(numbers) != 2: print(\\"Error: Two numbers are required.\\") return result = numbers[0] * numbers[1] print(f\\"The result is: {result}\\") except ValueError: print(\\"Error: Invalid input. Please enter two numbers separated by space.\\") def do_history(self, arg): \'Show command history: HISTORY\' for i, command in enumerate(self.history, 1): print(f\\"{i}: {command}\\") def do_exit(self, arg): \'Exit the shell: EXIT\' print(\'Goodbye!\') return True def __init__(self): super().__init__() self.history = [] if __name__ == \'__main__\': MyShell().cmdloop()"},{"question":"You are using pandas to analyze some datasets, and you want to customize the display settings for better readability of your output. Write a function to achieve the following tasks in sequence: 1. Set the maximum number of rows displayed (`display.max_rows`) to 15. 2. Set the maximum number of columns displayed (`display.max_columns`) to 10. 3. Configure the display precision for floating-point numbers to 2 decimal places. 4. Configure the dataframe to display column headers right-aligned. 5. Create a DataFrame with 30 rows and 12 columns of random floating-point numbers. 6. Use a context manager to print the DataFrame with `display.max_rows` set temporarily to 5 and `display.max_columns` set temporarily to 7, showing the DataFrame immediately after the context manager block ends to confirm the temporary change has reverted the settings. **Function Signature:** ```python def configure_pandas_display_and_create_df(): pass ``` # Constraints: - Use only the pandas library for display setting configurations. - Generate the random numbers in the DataFrame using NumPy. - Ensure the settings revert to their original values after using the context manager. # Expected Output: Your function does not need to return anything but should print outputs demonstrating the changes to the display options as described. # Example: ```python configure_pandas_display_and_create_df() ``` This should show the DataFrame with different settings inside and outside the context manager. **Hint:** Make use of `pd.set_option`, `pd.option_context`, and `pd.reset_option` to achieve the task.","solution":"import pandas as pd import numpy as np def configure_pandas_display_and_create_df(): Configures pandas display settings and creates a DataFrame with 30 rows and 12 columns of random floating-point numbers. # Set maximum number of rows and columns to be displayed pd.set_option(\'display.max_rows\', 15) pd.set_option(\'display.max_columns\', 10) # Set display precision for floating-point numbers pd.set_option(\'display.precision\', 2) # Display column headers right-aligned pd.set_option(\'display.colheader_justify\', \'right\') # Create DataFrame with 30 rows and 12 columns of random floating-point numbers df = pd.DataFrame(np.random.random((30, 12)), columns=[f\'col_{i}\' for i in range(12)]) # Use context manager to temporarily change display settings with pd.option_context(\'display.max_rows\', 5, \'display.max_columns\', 7): print(\\"Within context manager:\\") print(df) # Display DataFrame to confirm settings are reverted back print(\\"nAfter context manager:\\") print(df)"},{"question":"# Question: Implementing and Evaluating a Scikit-learn Model You are given the task to design and evaluate a machine learning model using scikit-learn. Your objective is to create a minimal, reproducible example that achieves the following: 1. **Generate Synthetic Data**: - Use scikit-learn\'s `make_classification` function to create a synthetic dataset for a binary classification problem. 2. **Preprocess the Data**: - Standardize the features using `StandardScaler`. 3. **Train a Model**: - Train a `GradientBoostingClassifier` on the preprocessed data. 4. **Evaluate the Model**: - Split the data into training and testing sets. - Train the model on the training set. - Evaluate the model\'s accuracy on the testing set. # Constraints: - Use `random_state=42` wherever applicable to ensure reproducibility. - Ensure your code is minimal and clear, containing only the necessary steps to achieve the task. - Use comments to explain each section of the code. # Input and Output Format: - **Input**: - No standard input; you will generate data within the script. - **Output**: - Print the accuracy of the model on the test set. # Expected Steps: 1. Generate the synthetic dataset. 2. Standardize the dataset. 3. Split the data into training and testing sets. 4. Train the GradientBoostingClassifier. 5. Evaluate and print the model\'s accuracy on the test set. # Sample Code Structure: ```python import numpy as np import pandas as pd from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingClassifier from sklearn.metrics import accuracy_score # Step 1: Generate synthetic data X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, random_state=42) # Step 2: Standardize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 3: Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42) # Step 4: Train the GradientBoostingClassifier model = GradientBoostingClassifier(random_state=42) model.fit(X_train, y_train) # Step 5: Evaluate the model y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # Output the accuracy print(f\'Accuracy: {accuracy:.4f}\') ``` Ensure your solution meets the above steps and constraints.","solution":"import numpy as np from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingClassifier from sklearn.metrics import accuracy_score def train_and_evaluate_model(): # Step 1: Generate synthetic data X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, random_state=42) # Step 2: Standardize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 3: Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42) # Step 4: Train the GradientBoostingClassifier model = GradientBoostingClassifier(random_state=42) model.fit(X_train, y_train) # Step 5: Evaluate the model y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # Output the accuracy print(f\'Accuracy: {accuracy:.4f}\') return accuracy"},{"question":"# Descriptor-Based Data Validator Create a custom descriptor in Python that performs data validation on its associated attribute. The descriptor should enforce specific conditions on the data before storing it in an instance. You will need to implement the following: 1. **ValidationDescriptor Class**: A descriptor class that validates: - String attributes should be non-empty and must match a given regular expression pattern (if provided). - Numeric attributes should be within a specified range (min inclusive, max exclusive). 2. **ValidatedClass**: A class that utilizes `ValidationDescriptor` for its attributes. # Detailed Requirements ValidationDescriptor - **Initialization Parameters**: - `data_type`: The expected data type (`int`, `float`, `str`). - `pattern`: A regex pattern to validate string data type. Default is `None`. - `min_value`: The minimum value for numeric data type. Default is `None`. - `max_value`: The maximum value for numeric data type. Default is `None`. - **Methods**: - `__set_name__(self, owner, name)`: Sets the name of the attribute. - `__get__(self, obj, objtype=None)`: Gets the attribute value. - `__set__(self, obj, value)`: Validates and sets the attribute value. - `validate(self, value)`: Validates the value based on type and constraints. ValidatedClass - This class will use `ValidationDescriptor` for its attributes `name` (string), `age` (integer), and `email` (string). - **Attributes**: - `name`: Should be a non-empty string. - `age`: Should be an integer between 0 and 120. - `email`: Should be a non-empty string matching a basic email regex pattern. # Constraints - You must use descriptors for validation. - Raise appropriate exceptions (`ValueError`, `TypeError`) for any validation failures. # Example Usage ```python import re class ValidationDescriptor: # Complete the descriptor class here class ValidatedClass: # Use the descriptor for attributes # Example instantiation and validation checks try: p = ValidatedClass(name=\\"John Doe\\", age=25, email=\\"john.doe@example.com\\") print(p.name, p.age, p.email) p.age = 200 # Should raise ValueError except ValueError as e: print(e) try: p.email = \\"invalid-email\\" # Should raise ValueError except ValueError as e: print(e) ``` Your implementation should allow the creation of `ValidatedClass` instances and enforce all validation rules upon attribute assignment.","solution":"import re class ValidationDescriptor: def __init__(self, data_type, pattern=None, min_value=None, max_value=None): self.data_type = data_type self.pattern = pattern self.min_value = min_value self.max_value = max_value def __set_name__(self, owner, name): self.name = name def __get__(self, obj, objtype=None): return obj.__dict__.get(self.name) def __set__(self, obj, value): self.validate(value) obj.__dict__[self.name] = value def validate(self, value): if not isinstance(value, self.data_type): raise TypeError(f\\"{self.name} must be of type {self.data_type.__name__}\\") if isinstance(value, str): if not value: raise ValueError(f\\"{self.name} cannot be empty\\") if self.pattern and not re.match(self.pattern, value): raise ValueError(f\\"{self.name} does not match the required pattern\\") if isinstance(value, (int, float)): if self.min_value is not None and value < self.min_value: raise ValueError(f\\"{self.name} must be at least {self.min_value}\\") if self.max_value is not None and value >= self.max_value: raise ValueError(f\\"{self.name} must be less than {self.max_value}\\") class ValidatedClass: name = ValidationDescriptor(str) age = ValidationDescriptor(int, min_value=0, max_value=120) email = ValidationDescriptor(str, pattern=r\'^[w.-]+@[w.-]+.w+\') def __init__(self, name, age, email): self.name = name self.age = age self.email = email"},{"question":"You are provided with the `email.iterators` module that offers utilities to iterate through email message object trees. One function iterates over all line payloads (`body_line_iterator`), another iterates over subparts with specific MIME types (`typed_subpart_iterator`). Your task involves using these utilities to create a custom function. # Objective: Implement a function `filter_and_extract_email_content` that performs the following steps: 1. Filters all subparts of an email message based on a given MIME `maintype` and optional `subtype`. 2. For each filtered subpart, extracts its body line-by-line. # Function Signature: ```python def filter_and_extract_email_content(msg, maintype=\'text\', subtype=None): # Your code here ``` # Input: - `msg` (Message): An email message object. - `maintype` (str, default \'text\'): The main MIME type to filter subparts. - `subtype` (str, optional): The optional MIME subtype to filter subparts. # Output: - `List[str]`: A list of strings, where each string is a concatenation of lines from a filtered subpart\'s payload. # Constraints: 1. Assume that `msg` is a properly formatted email message instance. 2. If no subparts match the provided MIME types, return an empty list. 3. Use the provided iterators `typed_subpart_iterator` for filtering and `body_line_iterator` for line extraction. # Example: ```python from email.message import EmailMessage # Create a sample email message for testing sample_email = EmailMessage() sample_email.set_content(\\"This is a test email message.\\") subpart1 = EmailMessage() subpart1.set_content(\\"This is a text subpart.\\") sample_email.add_attachment(subpart1) subpart2 = EmailMessage() subpart2.set_content(\\"This is another text subpart.\\") sample_email.add_attachment(subpart2) # Implement the function and test result = filter_and_extract_email_content(sample_email, maintype=\'text\') print(result) # Expected Output: [\'This is a text subpart.\', \'This is another text subpart.\'] ``` # Notes: - Make sure to handle multiline payloads correctly, combining all lines into a single string per subpart. - You should utilize both `typed_subpart_iterator` and `body_line_iterator` as part of your solution. - Do not consider the `_structure` function as it is not intended for this task.","solution":"from email.iterators import typed_subpart_iterator, body_line_iterator def filter_and_extract_email_content(msg, maintype=\'text\', subtype=None): Filters all subparts of an email message based on a given MIME maintype and optional subtype, and extracts its body line-by-line. Args: msg (Message): An email message object. maintype (str, default \'text\'): The main MIME type to filter subparts. subtype (str, optional): The optional MIME subtype to filter subparts. Returns: List[str]: A list of strings, where each string is a concatenation of lines from a filtered subpart\'s payload. result = [] for subpart in typed_subpart_iterator(msg, maintype, subtype): lines = list(body_line_iterator(subpart)) result.append(\'\'.join(lines)) return result"},{"question":"**Objective**: Demonstrate your understanding of `LabelBinarizer`, `MultiLabelBinarizer`, and `LabelEncoder` from the `sklearn.preprocessing` module by applying these transformers to different datasets. --- Question You will implement a function `transform_labels` which takes the following inputs: 1. `method`: A string indicating the transformation method. It can be one of `\\"label_binarizer\\"`, `\\"multi_label_binarizer\\"`, or `\\"label_encoder\\"`. 2. `data`: The dataset to transform. It can contain: - A list of integers for the `\\"label_binarizer\\"` and `\\"label_encoder\\"` methods. - A list of lists of integers for the `\\"multi_label_binarizer\\"` method. The function should return the transformed data using the appropriate sklearn transformer, and also print out the fit classes from the transformer. The function should handle edge cases, such as empty lists or inconsistent label formats. Input: ```python transform_labels(method: str, data: list) -> np.ndarray ``` - `method` (str): The transformation method. One of `\\"label_binarizer\\"`, `\\"multi_label_binarizer\\"`, or `\\"label_encoder\\"`. - `data` (list): The data to be transformed. For `\\"label_binarizer\\"` and `\\"label_encoder\\"`, a list of integers. For `\\"multi_label_binarizer\\"`, a list of lists of integers. Output: - Returns a numpy array with the transformed label data. - Prints the fit classes from the transformer. Constraints: - The input data should not be empty. Handle this edge case by returning an empty array and printing an appropriate message. - For the `\\"label_binarizer\\"` and `\\"label_encoder\\"` methods, all elements in `data` should be integers. Otherwise, return an empty array and print an appropriate message. Example: ```python # Example 1 method = \\"label_binarizer\\" data = [1, 2, 6, 4, 2] result = transform_labels(method, data) # Expected Output: (Printed classes: [1 2 4 6]) # Expected Return: [[1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 1, 0], ...] # Example 2 method = \\"multi_label_binarizer\\" data = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]] result = transform_labels(method, data) # Expected Output: (Printed classes: [0 1 2 3 4]) # Expected Return: [[0, 0, 1, 1, 1], [0, 0, 1, 0, 0], ...] # Example 3 method = \\"label_encoder\\" data = [1, 2, 2, 6] result = transform_labels(method, data) # Expected Output: (Printed classes: [1 2 6]) # Expected Return: [0, 1, 1, 2] ``` Notes: - For non-integer labels (handled by `LabelEncoder`), you can use strings or non-integer types as well for testing purposes. - Make sure to handle potential edge cases such as empty input data and mismatched types gracefully.","solution":"import numpy as np from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder def transform_labels(method: str, data: list) -> np.ndarray: if not data: print(\\"Input data is empty.\\") return np.array([]) if method == \\"label_binarizer\\": if not all(isinstance(i, int) for i in data): print(\\"All elements in data should be integers for label_binarizer.\\") return np.array([]) lb = LabelBinarizer() transformed_data = lb.fit_transform(data) print(\\"Classes:\\", lb.classes_) elif method == \\"multi_label_binarizer\\": if not all(isinstance(sublist, list) for sublist in data) or not all(isinstance(i, int) for sublist in data for i in sublist): print(\\"All elements and sub-elements in data should be integers for multi_label_binarizer.\\") return np.array([]) mlb = MultiLabelBinarizer() transformed_data = mlb.fit_transform(data) print(\\"Classes:\\", mlb.classes_) elif method == \\"label_encoder\\": if not all(isinstance(i, int) for i in data): print(\\"All elements in data should be integers for label_encoder.\\") return np.array([]) le = LabelEncoder() transformed_data = le.fit_transform(data) print(\\"Classes:\\", le.classes_) else: print(\\"Invalid method specified.\\") return np.array([]) return transformed_data"},{"question":"You have been provided with a dataset, `penguins`, that contains the following columns: `species`, `island`, `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, `body_mass_g`, and `sex`. Your task is to create a seaborn histogram of the `bill_length_mm` column, colored by `species`. Additionally, you must move the legend to the \\"center right\\" of the plot, and ensure that the legend is titled \\"Penguin Species\\". **Input:** - You will not be provided any input from the user, but you should use the seaborn `penguins` dataset loaded using `sns.load_dataset(\\"penguins\\")`. **Output:** - The output will be a grid plot with the specified configurations. **Constraints:** - Use seaborn and matplotlib packages only. - Ensure that the legend appears at the specified location with the correct title. **Task:** 1. Load the `penguins` dataset using `seaborn`. 2. Create a histogram of `bill_length_mm`, colored by `species`. 3. Move the legend to the \\"center right\\" position. 4. Title the legend \\"Penguin Species\\". # Example Code ```python import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_bill_length(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create the histogram ax = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") # Move and update the legend sns.move_legend(ax, \\"center right\\", title=\\"Penguin Species\\") # Display the plot plt.show() # Call the function to execute plot_penguin_bill_length() ``` In this task, you are required to demonstrate your ability to: - Load a dataset using seaborn. - Create a plot and manage aesthetics using seaborn. - Customize the legend position and title. Ensure your code is well-structured and commented to explain the steps taken.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_bill_length(): Loads the penguins dataset and plots a seaborn histogram of bill length in millimeters, colored by species. Moves the legend to the center right of the plot and titles it \\"Penguin Species\\". # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create the histogram with seaborn ax = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") # Move and update the legend position and title plt.legend(loc=\'center right\', title=\'Penguin Species\') # Display the plot plt.show() # Call the function to execute plot_penguin_bill_length()"},{"question":"Binary Palindrome Checker Objective: Implement a function `is_binary_palindrome(n: int) -> bool` that checks if the binary representation of a given integer is a palindrome. Description: A binary palindrome is a number that remains the same when its binary digits are reversed. For instance, the integer 9 has a binary representation of `1001`, which is a palindrome. Function Signature: ```python def is_binary_palindrome(n: int) -> bool: ``` Input: - `n` (int): A non-negative integer to be checked for binary palindrome properties. Output: - `bool`: Returns `True` if the binary representation of `n` is a palindrome, otherwise returns `False`. Constraints: - `0 <= n <= 10^9` Example: ```python assert is_binary_palindrome(9) == True # Binary: 1001 assert is_binary_palindrome(12) == False # Binary: 1100 assert is_binary_palindrome(0) == True # Binary: 0 assert is_binary_palindrome(1) == True # Binary: 1 assert is_binary_palindrome(7) == True # Binary: 111 assert is_binary_palindrome(10) == False # Binary: 1010 ``` To solve this problem, you may find the methods and operations discussed in the documentation useful, such as converting numbers to binary strings and utilizing string reversal techniques.","solution":"def is_binary_palindrome(n: int) -> bool: Checks if the binary representation of a given integer is a palindrome. Parameters: n (int): A non-negative integer to be checked for binary palindrome properties. Returns: bool: Returns True if the binary representation of n is a palindrome, otherwise returns False. binary_representation = bin(n)[2:] # Convert to binary and strip the \'0b\' return binary_representation == binary_representation[::-1]"},{"question":"# Complex Data Analysis Objective Design and implement a Python class `ComplexDataProcessor` to analyze sequences of data using various built-in Python functionalities. This class should be able to handle both synchronous and asynchronous data processing. Requirements 1. **Initialization** - Accept an iterable of numerical data (integers/floats) during class instantiation. 2. **Methods** - `summary_statistics() -> dict`: Return summary statistics including: - Count (`count`) - Mean (`mean`) - Median (`median`) - Standard deviation (`std_dev`) - `filter_data(condition: Callable[[float], bool]) -> list`: Filter the data based on a conditional function. - `normalize_data() -> None`: Normalize the data to a range between 0 and 1. - `export_data(file_path: str) -> None`: Export the data to a text file using the built-in `open()` function. 3. **Asynchronous Methods** - `async_sum_data() -> float`: Asynchronously compute and return the sum of the data. - `async_max_data() -> float`: Asynchronously compute and return the maximum value in the data. Constraints - Assume the iterable passed during initialization contains only numerical values. - Use built-in Python functions wherever possible. - Handle edge cases such as empty data sets appropriately. Input/Output Examples ```python # Example usage data = [1.5, 3.2, 7.8, 0.1, 5.5, 2.8] processor = ComplexDataProcessor(data) # Expected output: {\'count\': 6, \'mean\': 3.48, \'median\': 3.0, \'std_dev\': 2.67} print(processor.summary_statistics()) # Expected output: [3.2, 7.8, 5.5, 2.8] print(processor.filter_data(lambda x: x > 2)) # Normalizes the data processor.normalize_data() # Exports data to \'output.txt\' processor.export_data(\'output.txt\') # Asynchronous computations import asyncio # Expected output: 20.9 print(asyncio.run(processor.async_sum_data())) # Expected output: 7.8 print(asyncio.run(processor.async_max_data())) ``` Notes: - Ensure that the class is well-documented, with type hints provided for all methods. - Use appropriate error handling mechanisms where necessary.","solution":"import statistics import asyncio from typing import Callable, List class ComplexDataProcessor: def __init__(self, data: List[float]): if not data: raise ValueError(\\"Data should not be empty.\\") self.data = data def summary_statistics(self) -> dict: count = len(self.data) mean = statistics.mean(self.data) median = statistics.median(self.data) std_dev = statistics.stdev(self.data) return {\'count\': count, \'mean\': mean, \'median\': median, \'std_dev\': std_dev} def filter_data(self, condition: Callable[[float], bool]) -> List[float]: return list(filter(condition, self.data)) def normalize_data(self) -> None: min_val = min(self.data) max_val = max(self.data) range_val = max_val - min_val if range_val == 0: raise ValueError(\\"Normalization is not possible with zero range (all values are the same).\\") self.data = [(x - min_val) / range_val for x in self.data] def export_data(self, file_path: str) -> None: with open(file_path, \'w\') as file: for value in self.data: file.write(f\\"{value}n\\") async def async_sum_data(self) -> float: await asyncio.sleep(0.1) # Simulate async work return sum(self.data) async def async_max_data(self) -> float: await asyncio.sleep(0.1) # Simulate async work return max(self.data)"},{"question":"**Command Line Argument Parser for a File Processor** You are tasked with developing a command-line argument parser for a Python script that processes files. The script should accept the following options: 1. **-i or --input**: Specifies the input file path (required). 2. **-o or --output**: Specifies the output file path (optional). 3. **-v or --verbose**: If specified, the script should run in verbose mode, printing detailed information about its operations. 4. **-h or --help**: Displays help information about using the script. The parser should handle errors gracefully by displaying a meaningful message and exiting the program when an invalid option or missing required option is encountered. Write a function `parse_arguments(args)` that takes a list of command-line arguments (excluding the script name itself) and returns a dictionary with keys: - `\'input\'`: The input file path (`str`). - `\'output\'`: The output file path (`str` or `None` if not specified). - `\'verbose\'`: A boolean indicating whether verbose mode is enabled. - `\'help\'`: A boolean indicating whether help information should be displayed. If the `-h` or `--help` option is specified, the function should print usage information and exit. Additionally, if the required `-i` or `--input` option is missing, the function should print an error message and exit. **Function Signature:** ```python def parse_arguments(args: list) -> dict: ``` **Input:** - `args`: A list of strings representing the command-line arguments. **Output:** - A dictionary with parsed options. **Example Usage:** ```python import sys def main(): arguments = parse_arguments(sys.argv[1:]) if arguments[\'help\']: usage() return # Proceed with processing using arguments[\'input\'], arguments[\'output\'], and arguments[\'verbose\'] if __name__ == \\"__main__\\": main() ``` **Constraints:** - You must not use the `argparse` module. - Use the `getopt` module to parse the arguments. **Example:** For the command-line input: ```sh script.py -i input.txt -o output.txt -v ``` The `parse_arguments(args)` function should return: ```python { \'input\': \'input.txt\', \'output\': \'output.txt\', \'verbose\': True, \'help\': False } ``` For the command-line input: ```sh script.py --input input.txt --help ``` The function should display usage information and exit.","solution":"import sys import getopt def parse_arguments(args): Parse command line arguments. :param args: List of command-line arguments (excluding the script name). :return: Dictionary with parsed options. options = { \'input\': None, \'output\': None, \'verbose\': False, \'help\': False } try: opts, _ = getopt.getopt(args, \\"i:o:vh\\", [\\"input=\\", \\"output=\\", \\"verbose\\", \\"help\\"]) except getopt.GetoptError as err: print(str(err)) print_usage() sys.exit(2) for opt, arg in opts: if opt in (\'-i\', \'--input\'): options[\'input\'] = arg elif opt in (\'-o\', \'--output\'): options[\'output\'] = arg elif opt in (\'-v\', \'--verbose\'): options[\'verbose\'] = True elif opt in (\'-h\', \'--help\'): options[\'help\'] = True if options[\'help\']: print_usage() sys.exit() if not options[\'input\']: print(\\"Error: --input option is required.\\") print_usage() sys.exit(2) return options def print_usage(): Print the usage information for the script. print(\\"Usage: script.py -i <inputfile> [-o <outputfile>] [-v] [-h]\\") print(\\"Options:\\") print(\\" -i, --input Specify the input file path (required)\\") print(\\" -o, --output Specify the output file path (optional)\\") print(\\" -v, --verbose Enable verbose mode\\") print(\\" -h, --help Show this help message and exit\\")"},{"question":"Objective: Demonstrate your understanding of the `ast` module by parsing, analyzing, transforming, and executing a given piece of Python code. Problem Statement: Write a Python function `transform_code` that takes a string of Python code as input. The function should parse this code into an Abstract Syntax Tree (AST), transform the AST to replace all occurrences of variable names starting with \\"tmp\\" to \\"var\\", and compile the transformed AST back into executable code. Finally, execute this transformed code and return any value that is returned by the code\'s top-level script execution. Function Signature: ```python def transform_code(code: str) -> any: ``` Input: - `code` (str): A string containing valid Python code. Output: - The return value of the executed transformed code. Constraints: - Names starting with \\"tmp\\" should only match whole names (e.g., \\"tmpvar\\", \\"tmp1\\"), not substrings of other names (e.g., \\"mytmpvar\\"). - Ensure that all variable names starting with \\"tmp\\" (e.g., `tmpvar`, `tmp1`, etc.) are renamed to `var` in the transformed AST. Example: ```python input_code = tmp1 = 10 tmp2 = 20 result = tmp1 + tmp2 print(result) transform_code(input_code) ``` Expected Output: ``` 30 ``` *Note: The function should handle variable assignments, usages, and any other occurrences of such variable names correctly.* Additional Task: Write a brief test script that demonstrates the function\'s capability to handle various scenarios, including: 1. Variables defined and used within a function. 2. Different scopes (e.g., global and local variables). 3. Edge cases with no occurrence of the target variable names. Hints: - Use the `ast.parse` function to parse the input code into an AST. - Implement a node transformer class inheriting from `ast.NodeTransformer` to rename the target variables. - Use the `ast.unparse` function to convert the modified AST back into a code string. - Use the `exec` function to execute the transformed code.","solution":"import ast class TmpVarRenamer(ast.NodeTransformer): def visit_Name(self, node): if node.id.startswith(\\"tmp\\"): node.id = \\"var\\" + node.id[3:] return node def transform_code(code: str) -> any: tree = ast.parse(code) renamer = TmpVarRenamer() transformed_tree = renamer.visit(tree) ast.fix_missing_locations(transformed_tree) compiled_code = compile(transformed_tree, filename=\\"<ast>\\", mode=\\"exec\\") local_namespace = {} exec(compiled_code, globals(), local_namespace) return local_namespace.get(\'result\') # Example usage: # input_code = # tmp1 = 10 # tmp2 = 20 # result = tmp1 + tmp2 # print(result) # # print(transform_code(input_code))"},{"question":"Coding Assessment Question # Objective You are tasked with analyzing the `diamonds` dataset using Seaborn and creating a custom visualization. The goal is to visualize the variation in carat across different cuts of diamonds and display corresponding error bars using different statistical measures available in Seaborn. # Instructions 1. Load the `diamonds` dataset using Seaborn. 2. Create a Seaborn plot with `cut` on the x-axis and `carat` on the y-axis. 3. Perform the following tasks within the plot object: - Compute the mean and 95% confidence interval for `carat` in each `cut` category. - Add error bars using standard deviation. - Add error bars using standard error. - Ensure reproducibility by setting a random seed for bootstrapping. 4. Display the final plot. # Constraints - Your solution should not use any loops. Utilize Seaborn\'s built-in methods to accomplish the tasks. - Random seed for bootstrapping should be set to 42. # Expected Output Format The final plot should clearly display: - The mean `carat` value for each `cut`. - Error bars indicating the spread using both standard deviation and standard error. - The plot should be reproducible due to the set random seed. # Example Output A bar plot with different diamond cuts on the x-axis and mean carat on the y-axis, with error bars representing both the standard deviation and standard error. ```python # Your code goes here. ``` # Sample Code Template ```python import seaborn.objects as so from seaborn import load_dataset # Load dataset diamonds = load_dataset(\\"diamonds\\") # Create plot plot = so.Plot(diamonds, x=\\"cut\\", y=\\"carat\\") # Add mean and 95% confidence interval plot.add(so.Range(), so.Est(estimator=\\"mean\\", errorbar=\\"ci\\")) # Add standard deviation error bars plot.add(so.Range(), so.Est(estimator=\\"mean\\", errorbar=\\"sd\\")) # Add standard error bars plot.add(so.Range(), so.Est(estimator=\\"mean\\", errorbar=\\"se\\")) # Set random seed for bootstrapping plot.add(so.Range(), so.Est(seed=42)) # Display the plot plot.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_diamond_carat_vs_cut(): # Load dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create plot plot = sns.catplot( x=\\"cut\\", y=\\"carat\\", data=diamonds, kind=\\"bar\\", ci=95, capsize=.2, seed=42 ) plt.errorbar( x=diamonds[\\"cut\\"].unique(), y=diamonds.groupby(\\"cut\\")[\\"carat\\"].mean(), yerr=diamonds.groupby(\\"cut\\")[\\"carat\\"].std(), fmt=\'none\', c=\'r\', capsize=5, label=\\"SD Error\\" ) plt.errorbar( x=diamonds[\\"cut\\"].unique(), y=diamonds.groupby(\\"cut\\")[\\"carat\\"].mean(), yerr=diamonds.groupby(\\"cut\\")[\\"carat\\"].sem(), fmt=\'none\', c=\'g\', capsize=5, label=\\"SE Error\\" ) plt.legend() plt.title(\'Variation in Carat Across Different Cuts of Diamonds\') plt.show() plot_diamond_carat_vs_cut()"},{"question":"Problem Statement: You are provided with a list of strings, each representing a set of characters. For instance, the string \\"abc\\" represents the set {\'a\', \'b\', \'c\'}. You need to implement the following functions: 1. **create_sets(input_list)**: This function takes a list of strings and returns a list of set objects corresponding to each string. - **Input**: List of strings (`input_list`), with each string containing unique characters. - **Output**: List of set objects. - **Example**: ```python input_list = [\\"abc\\", \\"def\\", \\"ghi\\"] result = create_sets(input_list) ``` Expected output: ```python [{\'a\', \'b\', \'c\'}, {\'d\', \'e\', \'f\'}, {\'g\', \'h\', \'i\'}] ``` 2. **common_characters(sets_list)**: This function takes a list of set objects and returns a frozenset containing characters common to all sets. - **Input**: List of set objects (`sets_list`). - **Output**: A frozenset containing characters common to all sets. - **Example**: ```python sets_list = [{\'a\', \'b\', \'c\'}, {\'b\', \'c\', \'d\'}, {\'c\', \'e\', \'f\'}] result = common_characters(sets_list) ``` Expected output: ```python frozenset({\'c\'}) ``` 3. **unique_characters(sets_list)**: This function takes a list of set objects and returns a list of sets where each set contains characters that are unique to the corresponding input set (i.e., characters that are not present in any other set in the list). - **Input**: List of set objects (`sets_list`). - **Output**: List of set objects. - **Example**: ```python sets_list = [{\'a\', \'b\', \'c\'}, {\'b\', \'c\', \'d\'}, {\'c\', \'e\', \'f\'}] result = unique_characters(sets_list) ``` Expected output: ```python [{\'a\'}, {\'d\'}, {\'e\', \'f\'}] ``` 4. **optimized_union(sets_list)**: This function takes a list of set objects and returns a union of all sets. The function should handle large input efficiently. - **Input**: List of set objects (`sets_list`). - **Output**: A set containing the union of all input sets. - **Example**: ```python sets_list = [{\'a\', \'b\', \'c\'}, {\'x\', \'y\', \'z\'}, {\'i\', \'j\', \'k\'}] result = optimized_union(sets_list) ``` Expected output: ```python {\'a\', \'b\', \'c\', \'x\', \'y\', \'z\', \'i\', \'j\', \'k\'} ``` Constraints: - You can assume that each string in the input list has unique characters. - The input list and sets_list in the functions can have at most 1000 elements. - Characters in the strings will be lowercase English letters. Requirements: - Your solution should highlight proper usage of set operations and efficient handling of larger datasets. - Make sure to handle edge cases such as empty lists or sets with no common/unique characters. Performance: - Aim to optimize the common and union operations for efficiency, potentially leveraging set properties and existing methods. ```python def create_sets(input_list): pass def common_characters(sets_list): pass def unique_characters(sets_list): pass def optimized_union(sets_list): pass ```","solution":"def create_sets(input_list): Converts a list of strings to a list of sets. return [set(s) for s in input_list] def common_characters(sets_list): Finds characters that are common to all set objects in the list. if not sets_list: return frozenset() common_set = sets_list[0].copy() for s in sets_list[1:]: common_set.intersection_update(s) return frozenset(common_set) def unique_characters(sets_list): Finds characters unique to each set in the list. unique_sets = [] for i, current_set in enumerate(sets_list): other_sets_union = set().union(*sets_list[:i], *sets_list[i+1:]) unique_sets.append(current_set - other_sets_union) return unique_sets def optimized_union(sets_list): Returns the union of all sets in the list. return set().union(*sets_list)"},{"question":"# Pandas Data Handling and Validation Objective Your task is to implement a function that processes a DataFrame and performs a series of operations on it. Additionally, you must write assertions to verify the correctness of your operations and handle potential exceptions. Requirements 1. **Data Processing Function** - Implement a function `process_data(df: pd.DataFrame) -> pd.DataFrame` which: - Filters rows where the \'value\' column is greater than a specified threshold. - Groups the data by the \'category\' column and calculates the mean of the \'value\' column for each group. - Resets the index of the resulting DataFrame. 2. **Testing Function** - Implement a function `test_process_data()` that: - Calls `process_data` with a predefined DataFrame. - Uses pandas testing assertion functions to ensure the output DataFrame is as expected. 3. **Exception Handling** - Ensure your functions handle the following exceptions appropriately: - `KeyError` if the necessary columns are not present in the DataFrame. - `ValueError` if the \'value\' column contains invalid data types that cannot be compared. Input and Output Formats - Input: A `pandas.DataFrame` object. - Output: A `pandas.DataFrame` object with the processed data. ```python import pandas as pd import pandas.testing as pdt def process_data(df: pd.DataFrame) -> pd.DataFrame: # Your implementation here pass def test_process_data(): # Define the input DataFrame input_data = { \'category\': [\'A\', \'A\', \'B\', \'B\', \'C\', \'C\'], \'value\': [10, 15, 10, 20, 10, 5] } input_df = pd.DataFrame(input_data) # Define the expected output DataFrame expected_data = { \'category\': [\'A\', \'B\', \'C\'], \'value\': [12.5, 15.0, 7.5] } expected_df = pd.DataFrame(expected_data) # Execute process_data result_df = process_data(input_df) # Use pandas testing assertions to verify correctness pdt.assert_frame_equal(result_df, expected_df) # You can add additional test cases and exception handling as necessary. ``` Constraints - The input DataFrame will contain only numerical and categorical data. - The \'value\' column should only contain numerical data (integers or floats). - The function should be able to handle input DataFrames of various sizes. Performance Requirements - The function should efficiently process DataFrames with up to 1 million rows. Notes - Make sure to import the necessary pandas modules. - Consider edge cases such as empty DataFrames or DataFrames without the required columns.","solution":"import pandas as pd def process_data(df: pd.DataFrame, threshold: float) -> pd.DataFrame: Filters rows where the \'value\' column is greater than a specified threshold, groups by the \'category\' column, and calculates the mean \'value\' for each group. Resets the index of the resulting DataFrame. Parameters: df (pd.DataFrame): Input DataFrame. threshold (float): The threshold value for filtering. Returns: pd.DataFrame: Processed DataFrame. try: # Check if necessary columns are present if \'category\' not in df.columns or \'value\' not in df.columns: raise KeyError(\\"\'category\' and \'value\' columns are required.\\") # Check if \'value\' column contains valid data types if not pd.api.types.is_numeric_dtype(df[\'value\']): raise ValueError(\\"\'value\' column must contain numeric data.\\") # Filter rows where \'value\' is greater than the threshold filtered_df = df[df[\'value\'] > threshold] # Group by \'category\' and calculate the mean of \'value\' grouped_df = filtered_df.groupby(\'category\', as_index=False)[\'value\'].mean() return grouped_df except KeyError as e: print(f\\"KeyError: {e}\\") except ValueError as e: print(f\\"ValueError: {e}\\")"},{"question":"**Question: Implement a Customized Rolling Window Operation with Online Calculation Support** You are required to implement a function using pandas to perform a customized rolling window operation. The function should calculate a moving average that can handle new incoming data in an online fashion. # Specifications: 1. **Function Signature**: ```python def online_moving_average(data: pd.DataFrame, window: int, new_data: pd.DataFrame = None) -> pd.DataFrame: ``` 2. **Inputs**: - `data`: DataFrame, initial dataset on which the rolling window calculation begins. - `window`: int, the size of the rolling window. - `new_data`: DataFrame, new data to be added for the online calculation. If `None`, the function should only process the initial data. 3. **Output**: - DataFrame containing the moving averages calculated based on the window size over the entire data, including any new data if provided. 4. **Behavior**: - The function must support the initial calculation and continue processing with newly provided data without recomputing the earlier values. - Handle NaN values appropriately. Default to treating NaN values as zeros in the rolling calculation. - Ensure that the function works efficiently on large datasets by using the available pandas functionality. # Function Implementation Example: The function should be implemented as follows: ```python import pandas as pd def online_moving_average(data: pd.DataFrame, window: int, new_data: pd.DataFrame = None) -> pd.DataFrame: # Initial calculation rolling_avg = data.rolling(window=window, min_periods=1).mean().fillna(0) # Online calculation with new data if provided if new_data is not None: # Combine old and new data combined_data = pd.concat([data, new_data]) rolling_avg = combined_data.rolling(window=window, min_periods=1).mean().fillna(0) return rolling_avg ``` # Example Test Cases: ```python import pandas as pd # Test Case 1 data = pd.DataFrame({\'value\': [1, 2, 3, 4, 5]}) window = 3 print(online_moving_average(data, window)) # Test Case 2 with new data new_data = pd.DataFrame({\'value\': [6, 7, 8]}) print(online_moving_average(data, window, new_data)) ``` Expected Output: ``` value 0 1.0 1 1.5 2 2.0 3 3.0 4 4.0 value 0 1.0 1 1.5 2 2.0 3 3.0 4 4.0 5 5.0 6 6.0 7 7.0 ``` # Constraints: - Only use standard pandas functions and methods for the implementation. - Maintain efficiency when dealing with large datasets. - Handle NaN values properly. Ensure your solution follows these specifications.","solution":"import pandas as pd def online_moving_average(data: pd.DataFrame, window: int, new_data: pd.DataFrame = None) -> pd.DataFrame: Calculate a moving average with support for online data processing. Parameters: data (pd.DataFrame): Initial dataset on which the rolling window calculation begins. window (int): Size of the rolling window. new_data (pd.DataFrame): New data to be added for the online calculation. Defaults to None. Returns: pd.DataFrame: DataFrame containing the moving averages computed over the entire data. if new_data is not None: # Combine the old and new data combined_data = pd.concat([data, new_data]) rolling_avg = combined_data.rolling(window=window, min_periods=1).mean().fillna(0) return rolling_avg.reset_index(drop=True) else: rolling_avg = data.rolling(window=window, min_periods=1).mean().fillna(0) return rolling_avg.reset_index(drop=True)"},{"question":"Objective Create a Python C extension module that provides a function for calculating the distance between two points in a 2D space (Cartesian coordinates). Problem Statement Write a C extension function called `calc_distance` that accepts coordinates of two points in a 2D space. The function should: 1. Accept two tuples, each consisting of two floating-point numbers representing the coordinates of the points (x1, y1) and (x2, y2). 2. Calculate the Euclidean distance between the two points. 3. Return the computed distance as a Python float. Specification - The function `calc_distance` will be called from Python code and should be defined in a C extension module named `geometry`. - The function should use `PyArg_ParseTuple` to parse the input tuples and format strings to extract floating-point numbers. - The function should build the return value using `Py_BuildValue`. Format ```c #include <Python.h> #include <math.h> // Function to calculate the distance between two points static PyObject* calc_distance(PyObject* self, PyObject* args) { PyObject* point1; PyObject* point2; double x1, y1, x2, y2; // Parse input tuples (two tuples each containing two floats) if (!PyArg_ParseTuple(args, \\"O!O!\\", &PyTuple_Type, &point1, &PyTuple_Type, &point2)) { return NULL; } // Extract values from the tuples if (!PyArg_ParseTuple(point1, \\"dd\\", &x1, &y1) || !PyArg_ParseTuple(point2, \\"dd\\", &x2, &y2)) { return NULL; } // Calculate the Euclidean distance double distance = sqrt(pow(x2 - x1, 2) + pow(y2 - y1, 2)); // Return the calculated distance as a float return Py_BuildValue(\\"d\\", distance); } // Method definition object for this extension, these can be dynamically modified static PyMethodDef GeometryMethods[] = { {\\"calc_distance\\", calc_distance, METH_VARARGS, \\"Calculate the distance between two points in 2D space.\\"}, {NULL, NULL, 0, NULL} }; // Module definition static struct PyModuleDef geometrymodule = { PyModuleDef_HEAD_INIT, \\"geometry\\", // name of module \\"A module for calculating distances between points.\\", // Doc string (may be NULL) -1, // -1 means that the module keeps state in global variables GeometryMethods }; // Module initialization function PyMODINIT_FUNC PyInit_geometry(void) { return PyModule_Create(&geometrymodule); } ``` Input Two tuples each containing two float values. Output A float representing the Euclidean distance between the two points. Constraints - Ensure proper error handling for non-tuple inputs. - The tuples must contain exactly two float values each. Performance Requirements - The solution should be efficient and parse the data properly using the provided C API functions. Example ```python import geometry # Expected output: 5.0 print(geometry.calc_distance((1.0, 1.0), (4.0, 5.0))) ```","solution":"import math def calc_distance(point1, point2): Calculate the Euclidean distance between two points in 2D space. Args: - point1: Tuple containing two float values (x1, y1) - point2: Tuple containing two float values (x2, y2) Returns: - The Euclidean distance between the two points as a float. x1, y1 = point1 x2, y2 = point2 distance = math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2) return distance"},{"question":"Problem Statement You are tasked with writing a Python program that acts both as a standalone executable script and as an importable module. The program should read a list of numbers from the command line, process them, and print the results. The processing involves: 1. Calculating the sum of the numbers. 2. Calculating the product of the numbers. 3. Printing these results clearly. Additionally, the program when imported as a module should provide a function `process_numbers` which takes a list of numbers as an argument and returns a tuple containing the sum and the product of these numbers. You are required to: 1. Write the function `process_numbers` which computes and returns the sum and product of a list of numbers. 2. Write the `main` function that: - Reads command-line arguments. - Converts them into a list of numbers. - Calls the `process_numbers` function. - Prints the results in the format: ``` Sum: <sum> Product: <product> ``` 3. Use the `if __name__ == \'__main__\':` block to ensure that the script can be run directly and execute the appropriate functionality. **Constraints:** - The command line input will be a sequence of valid integers. - The function should handle at least 1,000 numbers efficiently. Input Format - When run as a script: Command-line arguments representing a list of integers. - When imported as a module: A list of integers passed to the `process_numbers` function. Output Format - As a script: Print the sum and product of the numbers. - As a module: Return a tuple `(sum, product)`. Example Usage - **As a script:** ```sh python3 myscript.py 2 3 4 Sum: 9 Product: 24 ``` - **As a module:** ```python from myscript import process_numbers result = process_numbers([2, 3, 4]) print(result) # Output: (9, 24) ``` Your Code ```python import sys from typing import List, Tuple def process_numbers(numbers: List[int]) -> Tuple[int, int]: # Your implementation here pass def main() -> None: # Your implementation here pass if __name__ == \'__main__\': main() ``` **Ensure to handle the edge cases and include comments in your code for clarity.**","solution":"import sys from typing import List, Tuple def process_numbers(numbers: List[int]) -> Tuple[int, int]: total_sum = sum(numbers) total_product = 1 for number in numbers: total_product *= number return total_sum, total_product def main() -> None: # Read the command-line arguments (excluding the script name) args = sys.argv[1:] # Convert the arguments to a list of integers numbers = list(map(int, args)) # Process the numbers total_sum, total_product = process_numbers(numbers) # Print the results print(f\\"Sum: {total_sum}\\") print(f\\"Product: {total_product}\\") if __name__ == \'__main__\': main()"},{"question":"# Advanced Type Hints and Generics in Python Problem Statement: You are required to implement a function that takes a list of numerical elements and returns a dictionary. The dictionary should have two keys: 1. `\\"positive\\"` - containing a list of positive numbers. 2. `\\"negative\\"` - containing a list of negative numbers. Additionally, you need to define custom types and use generics to ensure type safety and improve code clarity. Requirements: 1. Define a custom generic type `Number` that can either be `int` or `float`. 2. Implement the function `categorize_numbers` with appropriate type hints using the `Number` type. 3. Ensure the function adheres to the specified input and output formats. Function Signature: ```python from typing import List, Dict, TypeVar, Generic # Define your custom type and function signature here. Number = TypeVar(\'Number\', int, float) def categorize_numbers(numbers: List[Number]) -> Dict[str, List[Number]]: pass ``` Input: - `numbers` (List[Number]): A list of integers and/or floating-point numbers. Output: - Dict[str, List[Number]]: A dictionary with keys `\\"positive\\"` and `\\"negative\\"` and corresponding lists of numbers. Constraints: 1. Zero is not considered either positive or negative and should not be included in the output lists. 2. The input list can be empty, in which case the function should return an empty list for both keys. Example: ```python assert categorize_numbers([1, -1, 2.5, -3.5, 0, 4]) == {\\"positive\\": [1, 2.5, 4], \\"negative\\": [-1, -3.5]} assert categorize_numbers([]) == {\\"positive\\": [], \\"negative\\": []} assert categorize_numbers([-1, -2, -3]) == {\\"negative\\": [-1, -2, -3], \\"positive\\": []} assert categorize_numbers([1, 2, 3]) == {\\"positive\\": [1, 2, 3], \\"negative\\": []} ``` Performance: The function should run in O(n) time complexity, where n is the number of elements in the input list.","solution":"from typing import List, Dict, TypeVar # Define the custom type Number that can be either int or float Number = TypeVar(\'Number\', int, float) def categorize_numbers(numbers: List[Number]) -> Dict[str, List[Number]]: Categorizes the given list of numbers into positive and negative numbers. Parameters: numbers (List[Number]): A list of integers and/or floating-point numbers. Returns: Dict[str, List[Number]]: A dictionary with keys \\"positive\\" and \\"negative\\" containing the respective lists of positive and negative numbers. result = {\\"positive\\": [], \\"negative\\": []} for number in numbers: if number > 0: result[\\"positive\\"].append(number) elif number < 0: result[\\"negative\\"].append(number) return result"},{"question":"**Coding Assessment Question** You are given the Titanic dataset and are required to create visualizations using the `seaborn` library to analyze the dataset from different angles. # Task 1. Load the Titanic dataset using `seaborn`. 2. Create a count plot of the \'class\' variable to visualize the number of passengers in each class, grouped by the \'survived\' status. 3. Customize the plot to use a specific color palette to distinguish the \'survived\' status. 4. Normalize the counts to show the percentage of total passengers in each category. 5. Create a subplot to visualize count plots for \'sex\' and \'embarked\' variables as well, each grouped by the \'survived\' status. # Input and Output Formats **Input:** No specific input required. Utilize the `seaborn` library\'s built-in `titanic` dataset. **Output:** The output should be a 1x3 grid of subplots, including the count plots for the \'class\', \'sex\', and \'embarked\' variables, each distinguished by \'survived\' status and using a specific color palette. # Constraints and Notes - Use the `seaborn` library for all visualizations. - Customize the color palette to enhance the readability of the plots. - Make sure the \'normalize\' step shows percentages rather than raw counts. **Performance Requirements** The solution should be efficient in terms of memory and processing time, and the visualizations should be clear and informative. # Example ```python # Import seaborn and necessary libraries import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Set theme sns.set_theme(style=\\"whitegrid\\") # Create subplots fig, axes = plt.subplots(1, 3, figsize=(18, 6)) # Count plot for \'class\' sns.countplot(titanic, x=\\"class\\", hue=\\"survived\\", palette=\\"Set1\\", ax=axes[0], stat=\\"percent\\") # Count plot for \'sex\' sns.countplot(titanic, x=\\"sex\\", hue=\\"survived\\", palette=\\"Set2\\", ax=axes[1], stat=\\"percent\\") # Count plot for \'embarked\' sns.countplot(titanic, x=\\"embarked\\", hue=\\"survived\\", palette=\\"Set3\\", ax=axes[2], stat=\\"percent\\") # Display the plots plt.tight_layout() plt.show() ``` This example outlines the structure of the solution, including imports, data loading, creating subplots, and customizing the plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_titanic_visualizations(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Set theme sns.set_theme(style=\\"whitegrid\\") # Create subplots fig, axes = plt.subplots(1, 3, figsize=(18, 6)) # Count plot for \'class\' with percentage normalization total_class = titanic[\'class\'].value_counts().sum() sns.countplot(data=titanic, x=\\"class\\", hue=\\"survived\\", palette=\\"Set1\\", ax=axes[0]) class_counts = titanic.groupby([\'class\', \'survived\']).size().unstack().fillna(0) class_counts_perc = class_counts.div(class_counts.sum(axis=1), axis=0).mul(100) for p in axes[0].patches: height = p.get_height() axes[0].text(p.get_x() + p.get_width() / 2., height, \'{:1.1f}%\'.format(height / total_class * 100), ha=\\"center\\") # Count plot for \'sex\' with percentage normalization total_sex = titanic[\'sex\'].value_counts().sum() sns.countplot(data=titanic, x=\\"sex\\", hue=\\"survived\\", palette=\\"Set2\\", ax=axes[1]) sex_counts = titanic.groupby([\'sex\', \'survived\']).size().unstack().fillna(0) sex_counts_perc = sex_counts.div(sex_counts.sum(axis=1), axis=0).mul(100) for p in axes[1].patches: height = p.get_height() axes[1].text(p.get_x() + p.get_width() / 2., height, \'{:1.1f}%\'.format(height / total_sex * 100), ha=\\"center\\") # Count plot for \'embarked\' with percentage normalization total_embarked = titanic[\'embarked\'].value_counts().sum() sns.countplot(data=titanic, x=\\"embarked\\", hue=\\"survived\\", palette=\\"Set3\\", ax=axes[2]) embarked_counts = titanic.groupby([\'embarked\', \'survived\']).size().unstack().fillna(0) embarked_counts_perc = embarked_counts.div(embarked_counts.sum(axis=1), axis=0).mul(100) for p in axes[2].patches: height = p.get_height() axes[2].text(p.get_x() + p.get_width() / 2., height, \'{:1.1f}%\'.format(height / total_embarked * 100), ha=\\"center\\") # Display the plots plt.tight_layout() plt.show() return fig"},{"question":"**Objective:** Write a function to generate and apply a custom Seaborn color palette to a plot. The function should allow customization of the palette based on various parameters and plot a simple bar chart using Matplotlib and Seaborn with the generated palette. **Function Signature:** ```python def custom_seaborn_palette(n_colors=6, lightness=0.5, saturation=0.75, hue_start=0, as_cmap=False): Generates a custom Seaborn color palette and applies it to a sample bar plot. Args: n_colors (int): Number of colors in the palette. Default is 6. lightness (float): Lightness of the colors, range from 0 to 1. Default is 0.5. saturation (float): Saturation of the colors, range from 0 to 1. Default is 0.75. hue_start (float): Starting hue value, range from 0 to 1. Default is 0. as_cmap (bool): Whether to return a continuous colormap. Default is False. Returns: None pass ``` **Instructions:** 1. Use the `sns.husl_palette` function to generate a palette with the specified parameters. 2. Create a sample dataset with 5 categories and corresponding values. 3. Plot a bar chart using Matplotlib and Seaborn with the generated palette. 4. Ensure that the plot uses the custom palette, and demonstrate usage of both discrete and continuous palettes based on the `as_cmap` parameter. **Constraints:** - `n_colors` should be a positive integer. - `lightness`, `saturation`, and `hue_start` should be floats within the range [0, 1]. - The function should handle potential errors or invalid parameter values gracefully. **Example:** ```python # Expected usage of the function custom_seaborn_palette(n_colors=8, lightness=0.4, saturation=0.4, hue_start=0.5) custom_seaborn_palette(as_cmap=True) ``` **Expected Output:** - The function should display a bar chart for the sample dataset using the generated custom palette. **Performance Requirements:** - The function should efficiently generate the palette and plot the chart within a reasonable amount of time for typical parameter values. **Hints:** - Use `sns.set_palette()` or `sns.color_palette()` to apply the custom-generated palette to the plot. - Utilize Seaborn\'s functionalities for creating the bar plot (e.g., `sns.barplot()`) and Matplotlib for final adjustments (`plt.show()`).","solution":"import matplotlib.pyplot as plt import seaborn as sns import pandas as pd def custom_seaborn_palette(n_colors=6, lightness=0.5, saturation=0.75, hue_start=0, as_cmap=False): Generates a custom Seaborn color palette and applies it to a sample bar plot. Args: n_colors (int): Number of colors in the palette. Default is 6. lightness (float): Lightness of the colors, range from 0 to 1. Default is 0.5. saturation (float): Saturation of the colors, range from 0 to 1. Default is 0.75. hue_start (float): Starting hue value, range from 0 to 1. Default is 0. as_cmap (bool): Whether to return a continuous colormap. Default is False. Returns: None # Validate arguments if not (1 <= n_colors <= 100): raise ValueError(\\"n_colors must be between 1 and 100.\\") if not (0 <= lightness <= 1) or not (0 <= saturation <= 1) or not (0 <= hue_start <= 1): raise ValueError(\\"lightness, saturation, and hue_start must be between 0 and 1.\\") # Generate the palette palette = sns.husl_palette(n_colors=n_colors, l=lightness*100, s=saturation*100, h=hue_start*360) if as_cmap: palette = sns.color_palette(palette, as_cmap=True) # Sample data data = pd.DataFrame({ \'Category\': [\'A\', \'B\', \'C\', \'D\', \'E\'], \'Value\': [10, 20, 15, 25, 10] }) # Apply and plot the palette sns.set_palette(palette) plt.figure(figsize=(10, 6)) sns.barplot(x=\'Category\', y=\'Value\', data=data) plt.title(\\"Bar Plot with Custom Palette\\") plt.show()"},{"question":"You are working on a script that needs to interact with an FTP server securely. For this purpose, you will use the `.netrc` file format to store and retrieve FTP credentials. You are required to implement a function that validates the credentials for a given host. Implement the function `validate_credentials(file_path: str, host: str, login: str, password: str) -> bool` that checks if the given login and password match the credentials stored in the specified `.netrc` file for the provided host. The function should take the following inputs: - `file_path`: A string representing the path to the `.netrc` file. - `host`: A string representing the hostname for which the credentials should be validated. - `login`: A string representing the login username to validate. - `password`: A string representing the password to validate. The function should return a boolean value, `True` if the credentials match and `False` otherwise. # Constraints - The `.netrc` file may contain multiple host entries. - You can assume the `.netrc` file exists at the specified `file_path`. - The `.netrc` file may include a \'default\' entry that should be used if no specific entry for the host is found. - Ensure that the function handles the possible `NetrcParseError` exception gracefully and returns `False` if any parsing errors occur. # Example Usage ```python # Sample .netrc file content # machine ftp.example.com login myuser password mypassword # machine ftp.anotherexample.com login anotheruser password anotherpassword # default login defaultuser password defaultpassword file_path = \\"/path/to/.netrc\\" host = \\"ftp.example.com\\" login = \\"myuser\\" password = \\"mypassword\\" print(validate_credentials(file_path, host, login, password)) # True host = \\"ftp.nonexistent.com\\" login = \\"defaultuser\\" password = \\"defaultpassword\\" print(validate_credentials(file_path, host, login, password)) # True host = \\"ftp.example.com\\" login = \\"wronguser\\" password = \\"wrongpassword\\" print(validate_credentials(file_path, host, login, password)) # False ``` # Implementation Notes 1. Use the `netrc` class to read and parse the `.netrc` file. 2. Retrieve the credentials for the given host using the `authenticators` method. 3. Compare the retrieved credentials with the provided `login` and `password`. You can assume that the passwords in the `.netrc` file are limited to a subset of the ASCII character set, as noted in the documentation.","solution":"import netrc from netrc import NetrcParseError def validate_credentials(file_path: str, host: str, login: str, password: str) -> bool: Validates if the given login and password match the credentials stored in the .netrc file for the given host. :param file_path: Path to the .netrc file. :param host: Hostname for which the credentials should be validated. :param login: Login username to validate. :param password: Password to validate. :return: True if the credentials match, False otherwise. try: credentials = netrc.netrc(file_path) auth = credentials.authenticators(host) or credentials.authenticators(\'default\') if auth is None: return False stored_login, _, stored_password = auth return stored_login == login and stored_password == password except NetrcParseError: return False"},{"question":"# Advanced Python C API - List Manipulation In this task, you will be required to use the `python310` package (a part of the Python C API) to perform various operations on Python lists at a lower level. This will test your understanding of the package and your ability to interact with Python objects from a C-like interface. Instructions: 1. **Create a new list**: Write a function `create_list` that takes an integer `n` and returns a new Python list of length `n`, initialized with `None` values. 2. **Insert elements into the list**: Write a function `insert_elements` that takes a list, an index, and an element to insert. This function should insert the element at the specified index in the list and return the modified list. 3. **Get an element**: Write a function `get_element` to retrieve the element at a given index from a list. If the index is out of bounds, the function should return `None`. 4. **Sort the list**: Write a function `sort_list` that takes a list and sorts it in place, returning the sorted list. 5. **Reverse the list**: Write a function `reverse_list` that takes a list and reverses its elements in place, returning the reversed list. Here are the function signatures and their descriptions: ```python def create_list(n: int) -> list: Creates a new list of length n initialized to None values. Parameters: n (int): Length of the list to create. Returns: list: A new list of length n initialized with None values. pass def insert_elements(lst: list, index: int, element) -> list: Inserts the element at the specified index in the list. Parameters: lst (list): The list to insert elements into. index (int): Index at which the element is to be inserted. element: The element to insert into the list. Returns: list: The modified list. pass def get_element(lst: list, index: int): Retrieves the element at the specified index from the list. Parameters: lst (list): The list to retrieve the element from. index (int): Index of the element to retrieve. Returns: element: The element at the specified index, or None if the index is out of bounds. pass def sort_list(lst: list) -> list: Sorts the list in place. Parameters: lst (list): The list to sort. Returns: list: The sorted list. pass def reverse_list(lst: list) -> list: Reverses the list in place. Parameters: lst (list): The list to reverse. Returns: list: The reversed list. pass ``` Constraints: - List indices for insertion and retrieval should be non-negative. - Ensure your functions handle potential errors gracefully. - You may use provided functions from the `python310` package as necessary to implement the above functionalities. Notes: - Performance considerations should be made while designing the functions. - You may assume that the input values to `insert_elements` and `get_element` are always valid for this problem. Demonstrate the usage of these functions with appropriate input examples and expected output.","solution":"def create_list(n: int) -> list: Creates a new list of length n initialized to None values. if n < 0: raise ValueError(\\"List length cannot be negative\\") return [None] * n def insert_elements(lst: list, index: int, element) -> list: Inserts the element at the specified index in the list. if index < 0 or index > len(lst): raise IndexError(\\"Index out of bounds\\") lst.insert(index, element) return lst def get_element(lst: list, index: int): Retrieves the element at the specified index from the list. if index < 0 or index >= len(lst): return None return lst[index] def sort_list(lst: list) -> list: Sorts the list in place. lst.sort() return lst def reverse_list(lst: list) -> list: Reverses the list in place. lst.reverse() return lst"},{"question":"**Objective:** Implement a coroutine-based resource manager that controls access to a pool of resources using asyncio synchronization primitives. # Problem Statement You are tasked with creating a `ResourceManager` class that manages access to a limited number of resources (e.g., database connections) using asyncio synchronization primitives. The `ResourceManager` should ensure that no more than a specified number of coroutines can access the resources simultaneously. Additionally, it should be able to notify all waiting coroutines when a specified event occurs (like a system-wide maintenance window being over). Requirements: 1. **ResourceManager Class:** - **Constructor (`__init__`):** Accepts an integer `max_resources` that determines the maximum number of resources available. - **async acquire_resource(self):** Coroutine that tries to acquire a resource. If no resources are available, it blocks until one is released. - **release_resource(self):** Method that releases a previously acquired resource. - **async notify_all_when_event(self, predicate=None):** Coroutine that waits until a certain event is set (through the `event_occured` method) and notifies all waiting coroutines. If a `predicate` is provided, it should be a callable that determines an additional condition for notification. - **event_occured(self):** Method that sets the event, notifying all waiting coroutines that the event has occurred. 2. **Usage Example:** Write a couple of sample coroutines demonstrating the use of `ResourceManager`, showing how coroutines can acquire, use, and release resources, and how they respond to events. Constraints: - You must use `asyncio.Lock`, `asyncio.Event`, and `asyncio.Semaphore` in your implementation. - The `notify_all_when_event` should use `asyncio.Condition` to manage notifications based on the event. # Input and Output - The `ResourceManager` class does not directly take input from the user, but its methods will interact with coroutines. - The usage example should print informative messages demonstrating the correct usage of the `ResourceManager`. # Performance Requirements: - The implementation should handle concurrent access by at least 100 coroutines efficiently. - There should be no race conditions or deadlocks in the implementation. ```python import asyncio class ResourceManager: def __init__(self, max_resources): self.max_resources = max_resources self.semaphore = asyncio.Semaphore(max_resources) self.lock = asyncio.Lock() self.event = asyncio.Event() self.condition = asyncio.Condition(lock=self.lock) async def acquire_resource(self): async with self.semaphore: async with self.lock: print(\\"Resource acquired\\") # Simulate resource usage await asyncio.sleep(1) def release_resource(self): self.semaphore.release() print(\\"Resource released\\") async def notify_all_when_event(self, predicate=None): async with self.condition: await self.event.wait() if predicate: if predicate(): self.condition.notify_all() else: self.condition.notify_all() def event_occured(self): self.event.set() with self.condition._lock: self.condition.notify_all() # Usage Example async def user(resource_manager): await resource_manager.acquire_resource() await asyncio.sleep(1) resource_manager.release_resource() async def event_trigger(resource_manager): await asyncio.sleep(2) resource_manager.event_occured() async def main(): resource_manager = ResourceManager(max_resources=3) users = [user(resource_manager) for _ in range(10)] event = event_trigger(resource_manager) await asyncio.gather(*users, event) asyncio.run(main()) ``` Implement the above `ResourceManager` class and ensure the sample `main()` function demonstrates its correct usage.","solution":"import asyncio class ResourceManager: def __init__(self, max_resources): self.max_resources = max_resources self.semaphore = asyncio.Semaphore(max_resources) self.lock = asyncio.Lock() self.event = asyncio.Event() self.condition = asyncio.Condition(lock=self.lock) async def acquire_resource(self): await self.semaphore.acquire() async with self.lock: print(\\"Resource acquired\\") def release_resource(self): self.semaphore.release() print(\\"Resource released\\") async def notify_all_when_event(self, predicate=None): async with self.condition: await self.event.wait() if predicate is None or predicate(): self.condition.notify_all() def event_occured(self): self.event.set() with self.condition._lock: self.condition.notify_all() # Usage Example async def user(resource_manager): await resource_manager.acquire_resource() await asyncio.sleep(1) resource_manager.release_resource() async def event_trigger(resource_manager): await asyncio.sleep(2) resource_manager.event_occured() async def main(): resource_manager = ResourceManager(max_resources=3) users = [user(resource_manager) for _ in range(10)] event = event_trigger(resource_manager) await asyncio.gather(*users, event) if __name__ == \\"__main__\\": asyncio.run(main())"}]'),z={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const s=this.searchQuery.trim().toLowerCase();return s?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(s)||e.solution&&e.solution.toLowerCase().includes(s)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(s=>setTimeout(s,1e3)),this.visibleCount+=4,this.isLoading=!1}}},R={class:"search-container"},F={class:"card-container"},D={key:0,class:"empty-state"},q=["disabled"],N={key:0},M={key:1};function O(s,e,l,m,i,r){const h=_("PoemCard");return a(),n("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",R,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=o=>i.searchQuery=o),placeholder:"Search..."},null,512),[[y,i.searchQuery]]),i.searchQuery?(a(),n("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=o=>i.searchQuery="")}," ✕ ")):d("",!0)]),t("div",F,[(a(!0),n(b,null,v(r.displayedPoems,(o,f)=>(a(),w(h,{key:f,poem:o},null,8,["poem"]))),128)),r.displayedPoems.length===0?(a(),n("div",D,' No results found for "'+u(i.searchQuery)+'". ',1)):d("",!0)]),r.hasMorePoems?(a(),n("button",{key:0,class:"load-more-button",disabled:i.isLoading,onClick:e[2]||(e[2]=(...o)=>r.loadMore&&r.loadMore(...o))},[i.isLoading?(a(),n("span",M,"Loading...")):(a(),n("span",N,"See more"))],8,q)):d("",!0)])}const L=p(z,[["render",O],["__scopeId","data-v-e3da00ee"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/40.md","filePath":"chatai/40.md"}'),j={name:"chatai/40.md"},X=Object.assign(j,{setup(s){return(e,l)=>(a(),n("div",null,[x(L)]))}});export{Y as __pageData,X as default};
